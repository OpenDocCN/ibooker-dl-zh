<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="chap-linalg-tools-ml">4 Linear algebraic tools in machine learning</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Quadratic forms</li>
<li class="co-summary-bullet">Applying principal component analysis (PCA) in data science</li>
<li class="co-summary-bullet">Retrieving documents with a machine learning application</li>
</ul>
<p class="body"><a id="marker-115"/>Finding patterns in large volumes of high-dimensional data is the name of the game in machine learning and data science. Data often appears in the form of large matrices (a toy example of this is shown in section <a class="url" href="02.xhtml#sec-matrices">2.3</a> and also in equation <a class="url" href="02.xhtml#eq-cat-brain-toy-training-dataset">2.1</a>). The rows of the data matrix represent feature vectors for individual input instances. Hence, the number of rows matches the count of observed input instances, and the number of columns matches the size of the feature vector—that is, the number of dimensions in the feature space. Geometrically speaking, each feature vector (that is, row of the data matrix) represents a point in feature space. These points are not distributed uniformly over the space. Rather, the set of points belonging to a specific class occupies a small subregion of that space. This leads to certain structures in the data matrices. Linear algebra provides us the tools needed to study matrix structures.</p>
<p class="body">In this chapter, we study linear algebraic tools to analyze matrix structures. The chapter presents some intricate mathematics, and we encourage you to persevere through it, including the theorem proofs. An intuitive understanding the proofs will give you significantly better insights into the rest of the chapter.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this chapter is available at <a class="url" href="http://mng.bz/aoYz">http://mng.bz/aoYz</a> in the form of fully functional and executable Jupyter notebooks.</p>
<h2 class="fm-head" id="sec-truedim">4.1 Distribution of feature data points and true dimensionality</h2>
<p class="body"><a id="marker-116"/>For instance, consider the problem of determining the similarity between documents. This is an important problem for document search companies like Google. Given a query document, the system needs to retrieve from an archive—in ranked order of similarity—documents that match the query document. To do this, we typically create a vector representation of each document. Then the dot product of the vectors representing a pair of documents can be used as a quantitative estimate of the similarity between the documents. Thus, each document is represented by a document descriptor vector in which every word in the vocabulary is associated with a fixed index in the vector. The value stored in that index position is the frequency (number of occurrences) of that word in the document.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Prepositions and conjunctions are typically excluded and singular; plural and other variants of words originating from the same stem are usually collapsed into one word.</p>
<p class="body">Every word in the vocabulary gets its own dimension in the document space. If a word does not occur in a document, we put a zero at that word’s index location in the descriptor vector for that document. We store one descriptor vector for every document in the archive. In theory, the document descriptor is an extremely long vector: its length matches the size of the vocabulary of the documents in the system. But this vector only exists notionally. In practice, we do not explicitly store descriptor vectors in their entirety. We store a &lt;word, frequency&gt; pair for every unique word that occurs in a document—<i class="fm-italics">but we do not explicitly store words that do not occur</i>. This is a <i class="fm-italics">sparse representation</i> of a document vector. The corresponding <i class="fm-italics">full representation</i> can be constructed from the sparse one whenever necessary. In documents, certain words often occur together (for example, <i class="fm-italics">Michael</i> and <i class="fm-italics">Jackson</i>, or <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i>). For example, in a given set of documents, the number of occurrences of <i class="fm-italics">gun</i> will more or less match the number of occurrences of <i class="fm-italics">violence</i>: if one appears, the other also appears most of the time. For a descriptor vector or, equivalently, a point in a feature space representing a document, the value at the index position corresponding to the word <i class="fm-italics">gun</i> will be more or less equal to that for the word <i class="fm-italics">violence</i>. If we project those points on the hyperplane formed by the axes for these correlated words, all the points fall around a 45-degree straight line (whose equation is <span class="math"><i class="fm-italics">x</i> = <i class="fm-italics">y</i></span>), as shown in figure <a class="url" href="#fig-lsa-dimred">4.1</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="774" id="fig-lsa-dimred" src="../../OEBPS/Images/CH04_F01_Chaudhury.png" width="742"/></p>
<p class="figurecaption">Figure 4.1 Document descriptor space. Each word in the vocabulary corresponds to a separate dimension. Dots show projections of document feature vectors on the plane formed by the axes corresponding to the terms <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i>.</p>
</div>
<p class="body">In figure <a class="url" href="#fig-lsa-dimred">4.1</a>, all the points representing documents are concentrated near the 45-degree line, and the rest of the plane is unpopulated. Can we collapse the two axes defining that plane and replace them with the single line around which most data is concentrated? It turns out that yes, we can do this. Doing so reduces the number of dimensions in the data representation—we are replacing a pair of correlated dimensions with a single one—thereby simplifying the representation. This leads to lower storage costs and, more importantly, provides additional insights. We have effectively discovered a new topic called <i class="fm-italics">gun-violence</i> from the documents.</p>
<p class="body"><a id="marker-117"/>as another example, consider a set of points in <span class="math">3</span>D, represented by coordinates <span class="math"><i class="fm-italics">X</i>, <i class="fm-italics">Y</i>, <i class="fm-italics">Z</i></span>. If the <i class="timesitalic">Z</i> coordinate is near zero for all the points, the data is concentrated around the <span class="math"><i class="fm-italics">X</i>, <i class="fm-italics">Y</i></span> plane. We can (and should) represent these points in two dimensions by projecting them onto the <span class="math"><i class="fm-italics">Z</i> = 0</span> plane. Doing so approximates the positions of the points only slightly (they are projected onto a plane that they were close to in the first place). In a more realistic example, the data points may be clustered around an arbitrary plane in the <span class="math">3</span>D space (as opposed to the <span class="math"><i class="fm-italics">Z</i> = 0</span> plane). We can still reduce the dimensionality of these data points to 2D by projecting on the plane they are close to.</p>
<p class="body">In general, if a set of data points is distributed in a space so that the points are clustered around a lower-dimensional subspace within that space (such as a plane or line), we can project the points onto the subspace and perform a <i class="fm-italics">dimensionality reduction</i> on the data. We effectively approximate the distances from the subspace with zero: since these distances are small by definition, the approximation is not too bad. Viewed another way, we eliminate smaller <i class="fm-italics">from-subspace</i> variations and retain the larger <i class="fm-italics">in-subspace</i> variations. The resulting representation is more compressed and also lends itself more easily to better analysis and insights as we have eliminated unimportant perturbations and are focusing on the main pattern.</p>
<p class="body">These ideas form the basis of the technique called <i class="fm-italics">principal component analysis</i> (PCA). It is one of the most important tools in the repertoire of a data scientist and machine learning practitioner. These ideas also underlie the <i class="fm-italics">latent semantic analysis</i> (LSA) technique for document retrieval—a fundamental approach for solving natural language processing (NLP) problems in machine learning. This chapter is dedicated to studying a set of methods leading to PCA and LSA. We examine a basic document retrieval system along with Python code.</p>
<h2 class="fm-head" id="sec-quadratic-form">4.2 Quadratic forms and their minimization</h2>
<p class="body"><a id="marker-118"/>Given a square symmetric matrix <i class="timesitalic">A</i>, the scalar quantity <span class="math"><i class="fm-italics">Q</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> is called a <i class="fm-italics">quadratic form</i>. These are seen in various situations in machine learning.</p>
<p class="body">For instance, recall the equation for a circle that we learned in high school</p>
<p class="fm-equation"><span class="math">(<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>−<i class="fm-italics">α</i><sub class="fm-subscript">0</sub>)<sup class="fm-superscript">2</sup> + (<i class="fm-italics">x</i><sub class="fm-subscript">1</sub>−<i class="fm-italics">α</i><sub class="fm-subscript">1</sub>)<sup class="fm-superscript">2</sup> = <i class="fm-italics">r</i><sup class="fm-superscript">2</sup></span></p>
<p class="body">where the center of the circle is <span class="math">(<i class="fm-italics">α</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">α</i><sub class="fm-subscript">1</sub>)</span> and the radius is <i class="timesitalic">r</i>. This equation can be rewritten as</p><!--<p class="FM-Equation"><span class="times">$$\begin{bmatrix}
\left(x_{0} - \alpha_{0}\right)  &amp; \left(x_{1} - \alpha_{1}\right)
\end{bmatrix}
\begin{bmatrix} 1 &amp; 0\\ 0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
\left(x_{0} - \alpha_{0}\right)\\
\left(x_{1} - \alpha_{1}\right)
\end{bmatrix}
= r^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="69" src="../../OEBPS/Images/eq_04-00-a.png" width="350"/></p>
</div>
<p class="body">If we denote the position vector <!--<span class="times">$\begin{bmatrix}
x_{0} \\ x_{1}
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="36" src="../../OEBPS/Images/eq_04-00-b.png" width="27"/></span> as <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and the center of the circle as <!--<span class="times">$\begin{bmatrix}
\alpha_{0} \\ \alpha_{1}
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="36" src="../../OEBPS/Images/eq_04-00-c.png" width="31"/></span> as <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_alpha.png" width="14"/></span>, the previous equation can be written compactly as</p>
<p class="fm-equation"><span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_alpha.png" width="14"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i><b class="fm-bold">I</b>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>−<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_alpha.png" width="14"/></span>) = <i class="fm-italics">r</i><sup class="fm-superscript">2</sup></span></p>
<p class="body">Note that left hand side of this equation is a quadratic form. The original <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span>-based equation only works for two dimensions. The matrix based equation is dimension agnostic: it represents a hypersphere in an arbitrary-dimensional space. For a two-dimensional space, the two equations become identical.</p>
<p class="body">Now, consider the equation for an ellipse:</p><!--<p class="FM-Equation"><span class="times">$$\frac{\left(x_{0} -
\alpha_{0}\right)^{2}}{\beta_{0}^{2}} + \frac{\left(x_{1} -
\alpha_{1}\right)^{2}}{\beta_{1}^{2}} = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_04-00-d.png" width="204"/></p>
</div>
<p class="body">You can verify that this can be written compactly in matrix form as</p><!--<p class="FM-Equation"><span class="times">$$\begin{bmatrix}
\left(x_{0} - \alpha_{0}\right) &amp; \left(x_{1} - \alpha_{1}\right)
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\beta_{0}^{2}} &amp; 0\\ 0 &amp; \frac{1}{\beta_{1}^{2}}
\end{bmatrix}
\begin{bmatrix}
\left(x_{0} - \alpha_{0}\right)\\
\left(x_{1} - \alpha_{1}\right)
\end{bmatrix}
= 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_04-00-e.png" width="363"/></p>
</div>
<p class="body">or, equivalently,</p><!--<p class="FM-Equation"><span class="times">(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span>−<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_alpha.png" /></span>)<i class="fm-italics"><sup class="FM-Superscript">T</sup>A</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span>−<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_alpha.png" /></span>) = 1</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="30" src="../../OEBPS/Images/eq_04-01.png" width="173"/></p>
</div>
<p class="fm-equation-caption">Equation 4.1 <span class="calibre" id="eq-hyper-ellipse-again"/></p>
<p class="body">where <!--<span class="times">$A = \begin{bmatrix}
\frac{1}{\beta_{0}^{2}} &amp; 0\\ 0 &amp; \frac{1}{\beta_{1}^{2}}
\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="58" src="../../OEBPS/Images/eq_04-01-a.png" width="104"/></span>. Once again, the matrix representation is dimension independent. In other words, equation <a class="url" href="../Text/04.xhtml#eq-hyper-ellipse-again">4.1</a> represents a hyperellipsoid. Note that if the ellipse axes are aligned with the coordinate axes, matrix <i class="timesitalic">A</i> is diagonal. If we rotate the coordinate system, each position vector is rotated by an orthogonal matrix <i class="timesitalic">R</i>. Equation <a class="url" href="../Text/04.xhtml#eq-hyper-ellipse-again">4.1</a> is transformed as follows (we have used the rules for transposing the products of matrices from equation <a class="url" href="02.xhtml#eq-mat-prod-transpose">2.10</a>):</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
&amp;\left(R\left(\vec{x} - \vec{\alpha}\right)\right)^{T} A
\left(R\left(\vec{x} - \vec{\alpha}\right)\right) =  1\\
&amp;\left(\vec{x} - \vec{\alpha}\right)^{T} \left(R^{T} A R \right)
\left(\vec{x} - \vec{\alpha}\right) =  1\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_04-01-b.png" width="226"/></p>
</div>
<p class="body">Replacing <i class="timesitalic">R<sup class="fm-superscript">T</sup>AR</i> with <i class="timesitalic">A</i>, we get the same equation as equation <a class="url" href="../Text/04.xhtml#eq-hyper-ellipse-again">4.1</a>, but <i class="timesitalic">A</i> is no longer a diagonal matrix.</p>
<p class="body">For a generic ellipsoid with arbitrary axes, <i class="timesitalic">A</i> has nonzero off-diagonal terms but is still symmetric. Thus, the multidimensional hyperellipsoid is represented by a quadratic form. The hypersphere is a special case of this.</p>
<p class="body">Quadratic forms are also found in the second term of the multidimensional Taylor expansion shown in equation <a class="url" href="../Text/03.xhtml#eq-taylor-multidim">3.8</a>: <!--<span class="times">$\frac{1}{2!}\left(\vec{\delta x}\right)^{T} H\left(\vec{x}\right)\left(\vec{\delta x}\right)$</span>--><span class="infigure"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_04-01-c.png" width="153"/></span> is a quadratic form in the Hessian matrix. Another huge application of quadratic forms is PCA, which is so important that we devote a whole section to it section <a class="url" href="../Text/04.xhtml#sec-pca">4.4</a>).</p>
<h3 class="fm-head1" id="minimizing-quadratic-forms">4.2.1 Minimizing quadratic forms</h3>
<p class="body"><a id="marker-119"/>An important question is, what choice of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> maximizes or minimizes the quadratic form? For instance, because the quadratic form is part of the multidimensional Taylor series, we need to minimize quadratic forms when we want to determine the best direction to move in to minimize the loss <span class="math"><i class="fm-italics">L</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>. Later, we will see that this question also lies at the heart of PCA computation.</p>
<p class="body">If <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is a vector with arbitrary length, we can make <i class="timesitalic">Q</i> arbitrarily big or small by simply changing the length of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. Consequently, optimizing <i class="timesitalic">Q</i> with arbitrary length <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is not a very interesting problem: rather, we want to know which <i class="fm-italics">direction</i> of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> optimizes <i class="timesitalic">Q</i>. For the rest of this section, we discuss quadratic forms with unit vectors <span class="math"><i class="fm-italics">Q</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>Ax̂</i></span> recall that <i class="timesitalic">x̂</i> denotes a unit-length vector satisfying <span class="math"><i class="fm-italics">x̂<sup class="fm-superscript">T</sup>x̂</i> = ||<i class="fm-italics">x̂</i>||<sup class="fm-superscript">2</sup> = 1</span>). Equivalently, we could use a different flavor, <span class="math"><i class="fm-italics">Q</i> = <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>/<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>, but we will use the former expression here. We are essentially searching over all possible directions <i class="timesitalic">x̂</i>, examining which direction minimizes <span class="math"><i class="fm-italics">Q</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>Ax̂</i></span>.</p>
<p class="body">Using matrix diagonalization (section <a class="url" href="02.xhtml#sec-mat-diagonalization">2.15</a>),</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">Q</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>Ax̂</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>SΛS<sup class="fm-superscript">T</sup>x̂</i></span></p>
<p class="body">where <span class="math"><i class="fm-italics">S</i> = [<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub>   <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">2</sub>  …  <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">n</sub></i>]</span> is the matrix with eigenvectors of <i class="timesitalic">A</i> as its columns and <span class="math">Λ</span> is a diagonal matrix with the eigenvalues of <i class="timesitalic">A</i> on the diagonal and <span class="math">0</span> everywhere else. Substituting</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ŷ</i> = <i class="fm-italics">S<sup class="fm-superscript">T</sup>x̂</i></span></p>
<p class="body">we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">Q</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>Ax̂</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>S</i>Λ<i class="fm-italics">S<sup class="fm-superscript">T</sup>x̂</i> = <i class="fm-italics">ŷ<sup class="fm-superscript">T</sup></i>Λ<i class="fm-italics">ŷ</i></span></p>
<p class="fm-equation-caption">Equation 4.2 <span class="calibre" id="eq-quad-form"/></p>
<p class="body"><a id="marker-120"/>Note that since <i class="timesitalic">A</i> is symmetric, its eigenvectors are orthogonal. This implies that <i class="timesitalic">S</i> is an orthogonal matrix: that is, <span class="math"><i class="fm-italics">S<sup class="fm-superscript">T</sup>S</i> = <i class="fm-italics">SS<sup class="fm-superscript">T</sup></i> = <b class="fm-bold">I</b></span>. Recall from section <a class="url" href="02.xhtml#orth-len-preserv">2.14.2.1</a> that for an orthogonal matrix <i class="timesitalic">S</i>, the transformation <i class="timesitalic">S<sup class="fm-superscript">T</sup>x̂</i> is length preserving. Consequently, <span class="math"><i class="fm-italics">ŷ</i> = <i class="fm-italics">S<sup class="fm-superscript">T</sup>x̂</i></span> is a unit-length vector. To be precise,</p>
<p class="fm-equation"><span class="math">||<i class="fm-italics">ŷ</i>||<sup class="fm-superscript">2</sup> = ||<i class="fm-italics">S<sup class="fm-superscript">T</sup>x̂</i>||<sup class="fm-superscript">2</sup> = (<i class="fm-italics">S<sup class="fm-superscript">T</sup>x̂</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i>(<i class="fm-italics">S<sup class="fm-superscript">T</sup>x̂</i>) = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>SS<sup class="fm-superscript">T</sup>x̂</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>x̂</i> = 1 since <i class="fm-italics">SS<sup class="fm-superscript">T</sup></i> = <b class="fm-bold">I</b></span></p>
<p class="body">So, expanding the right-hand side of equation <a class="url" href="#eq-quad-form">4.2</a>, we get</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned} Q  &amp;=
\begin{bmatrix}
  y_{1} &amp;y_{2} &amp;\cdots &amp;y_{n}\\
\end{bmatrix}\nonumber
\begin{bmatrix}
\lambda_{1} &amp; 0 &amp; \cdots &amp; 0\\ 0 &amp; \lambda_{2} &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 0 &amp; 0  &amp; \cdots &amp;  \lambda_{n}
\end{bmatrix}\begin{bmatrix}
  &amp;y_{1} \\
  &amp;y_{2} \\
  &amp;\vdots \\
  &amp;y_{n}
\end{bmatrix}\nonumber\\
  &amp;= \sum_{i = 1}^{n}\lambda_{i} _{i}^{2}\label{quad-form-trans}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="136" src="../../OEBPS/Images/eq_04-03.png" width="468"/></p>
</div>
<p class="fm-equation-caption">Equation 4.3 <span class="calibre" id="quad-form-trans"/></p>
<p class="body">We can assume that the eigenvalues are sorted in decreasing order of magnitude (if not, we can always renumber them).</p>
<p class="body">Consider this <i class="fm-italics">lemma</i> (small proof): The quantity <span class="math">Σ<i class="fm-italics1"><sub class="fm-subscript">i</sub><sup class="fm-superscript">n</sup></i><sub class="fm-subscript">= 1</sub> <i class="fm-italics">λ<sub class="fm-subscript">i</sub></i> <i class="fm-italics">y<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup></span>, where <span class="math">Σ<i class="fm-italics1"><sub class="fm-subscript">i</sub><sup class="fm-superscript">n</sup></i><sub class="fm-subscript">= 1</sub> <i class="fm-italics">y<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup> = 1</span> and <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≥ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub> ≥ ⋯ <i class="fm-italics">λ<sub class="fm-subscript">n</sub></i></span>, attains its maximum value when <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript">1</sub> = 1, <i class="fm-italics">y</i><sub class="fm-subscript">2</sub> = ⋯ <i class="fm-italics">y<sub class="fm-subscript">n</sub></i> = 0</span>.</p>
<p class="body">An <i class="fm-italics">intuitive proof</i> follows. If possible, let that the maximum value occur at some other value of <i class="timesitalic">ŷ</i>. We are constrained by the fact that <i class="timesitalic">ŷ</i> is an unit vector, so we must maintain <span class="math">Σ<i class="fm-italics1"><sub class="fm-subscript">i</sub><sup class="fm-superscript">n</sup></i><sub class="fm-subscript">= 1</sub> <i class="fm-italics">y<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup> = 1</span>.</p>
<p class="body">In particular, none of the elements of <i class="timesitalic">ŷ</i> can exceed <span class="math">1</span>. If we reduce the first term from <span class="math">1</span> to a smaller value, say <span class="math">√1-<i class="fm-italics">ϵ</i></span>, some other element(s) must go up by an equivalent amount to compensate (i.e., maintain the unit length property). Accordingly, suppose the hypothesized <i class="timesitalic">ŷ</i> maximizing <i class="timesitalic">Q</i> is given by</p><!--<p class="FM-Equation"><span class="times">$$\hat{y} =
\begin{bmatrix}
\sqrt{1-\epsilon}\\
\vdots\\
\sqrt{\delta}\\
\vdots
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="144" src="../../OEBPS/Images/eq_04-03-a.png" width="97"/></p>
</div>
<p class="body">where <span class="math"><i class="fm-italics">δ</i> &gt; 0</span>.</p>
<p class="body">What happens if we transfer the entire mass from the later term to the first term so that</p><!--<p class="FM-Equation"><span class="times">$$\hat{y} =
\begin{bmatrix}
\sqrt{1-\epsilon + \delta}\\
\vdots\\ 0\\
\vdots
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="145" src="../../OEBPS/Images/eq_04-03-b.png" width="119"/></p>
</div>
<p class="body">Doing this does not alter the length of <i class="timesitalic">ŷ</i> as the sum of the squares of the first and the other term remains <span class="math">1 − <i class="fm-italics">ϵ</i> + <i class="fm-italics">δ</i></span>. But the value of <span class="math"><i class="fm-italics">Q</i> = Σ<i class="fm-italics1"><sub class="fm-subscript">i</sub><sup class="fm-superscript">n</sup></i><sub class="fm-subscript">= 1</sub> <i class="fm-italics">λ<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup></span> is higher in the second case (where <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span> has been beefed up at the expense of another term), since <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub>(1−<i class="fm-italics">ϵ</i> + <i class="fm-italics">δ</i>) &gt; <i class="fm-italics">λ</i><sub class="fm-subscript">1</sub>(1−<i class="fm-italics">ϵ</i>) + <i class="fm-italics">λ<sub class="fm-subscript">j</sub>δ</i></span> for any <span class="math"><i class="fm-italics">j</i> &gt; 1</span> (since, <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> &gt; <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub>⋯</span> by assumption). Thus, whenever we have less than <span class="math">1</span> in the first term and greater than zero in some other term, we can increase <i class="timesitalic">Q</i> without losing the unit length property of <i class="timesitalic">ŷ</i> by transferring the entire mass to the first term.</p>
<p class="body">This means to maximize the right hand side of equation <a class="url" href="#quad-form-trans">4.3</a>, we must have <span class="math">1</span> as the first element (corresponding to the largest eigenvalue) of the unit vector <i class="timesitalic">ŷ</i> and zeros everywhere else. Anything else violates the condition that the corresponding quadratic form <span class="math"><i class="fm-italics">Q</i> = Σ<i class="fm-italics1"><sub class="fm-subscript">i</sub><sup class="fm-superscript">n</sup></i><sub class="fm-subscript">= 1</sub> <i class="fm-italics">λ<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup></span> is a maximum.</p>
<p class="body"><a id="marker-121"/>Thus we have established that the maximum of <i class="timesitalic">Q</i> occurs at <!--<span class="times">$\hat{y} = \begin{bmatrix} 1\\0\\ \vdots \\ 0\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_04-03-c.png" width="51"/></span>. The corresponding <span class="math"><i class="fm-italics">x̂</i> = <i class="fm-italics">Sŷ</i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_e.png" width="15"/></span><sub class="fm-subscript">1</sub></span> - the eigenvector corresponding to the largest eigenvalue of <i class="timesitalic">A</i>.</p>
<p class="body">Thus, the quadratic form <span class="math"><i class="fm-italics">Q</i> = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>Ax̂</i></span> attains its maximum when <i class="timesitalic">x̂</i> is along the eigenvector corresponding to the largest eigenvalue of <i class="timesitalic">A</i>. The corresponding maximum <i class="timesitalic">Q</i> is equal to the largest eigenvalue of <i class="timesitalic">A</i>. Similarly, the minimum of the quadratic form occurs when <i class="timesitalic">x̂</i> is along the eigenvector corresponding to the smallest eigenvalue.</p>
<p class="body">As stated above, many machine learning problems boil down to minimizing a quadratic form. We will study a few of them in later sections.</p>
<h3 class="fm-head1" id="subsec-sym-pos-sd">4.2.2 Symmetric positive (semi)definite matrices</h3>
<p class="body">A square symmetric <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i> is positive semidefinite if and only if</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> ≥ 0 <span class="cambria">∀</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span></p>
<p class="body">In other words, a positive semidefinite matrix yields a non-negative quadratic form with all <span class="math"><i class="fm-italics">n</i> × 1</span> vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. If we disallow the equality, we get symmetric positive definite matrices. Thus a square symmetric <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i> is positive definite if and only if</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> &gt; 0 <span class="cambria">∀</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> <span class="cambria">∈</span> ℝ<i class="fm-italics"><sup class="fm-superscript">n</sup></i></span></p>
<p class="body">From equations <a class="url" href="#eq-quad-form">4.2</a> and <a class="url" href="#quad-form-trans">4.3</a>, <i class="timesitalic">Q</i> is positive or zero if all <i class="timesitalic">λ<sub class="fm-subscript">i</sub></i>s are positive or zero (since the <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i><sup class="fm-superscript">2</sup></span>s are non-negative). Hence, symmetric positive (semi)definiteness is equivalent to the condition that all eigenvalues of the matrix are greater than (or equal to) zero.</p>
<h2 class="fm-head" id="sec-mat-spctral-frob-norm">4.3 Spectral and Frobenius norms of a matrix</h2>
<p class="body"><a id="marker-122"/>A vector is an entity with a magnitude and direction. The norm <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||</span> of a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> represents its magnitude. Is there an equivalent notion for matrices? The answer is yes, and we will study two such ideas.</p>
<h3 class="fm-head1" id="spectral-norms">4.3.1 Spectral norms</h3>
<p class="body">In section <a class="url" href="02.xhtml#subsection-vector_length">2.5.4</a>, we saw that the length (aka magnitude) of a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is <span class="math">||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|| = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>. Is there an equivalent notion of magnitude for a matrix <i class="timesitalic">A</i>?</p>
<p class="body">Well, a matrix can be viewed as an amplifier of a vector. The matrix <i class="timesitalic">A</i> amplifies the vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> to <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span>. So we can take the maximum possible value of <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||</span> over all possible <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; that is a measure for the magnitude of <i class="timesitalic">A</i>. Of course, if we consider arbitrary-length vectors, we can make <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> arbitrarily large by simply scaling <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> for any <i class="timesitalic">A</i>. That is uninteresting. Rather, we want to examine which direction of <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is amplified most and by how much.</p>
<p class="body">We examine this question with unit vectors <i class="timesitalic">x̂</i>: what is the maximum (or minimum) value of <span class="math">||<i class="fm-italics">Ax̂</i>||</span>, and what direction <i class="timesitalic">x̂</i> materializes it? The quantity</p><!--<p class="Body"><span class="times">$$\|A\|_{2} =
\underset{\hat{x}}{\max}\;\;\|A\hat{x}\| _{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="35" src="../../OEBPS/Images/eq_04-03-d.png" width="150"/></p>
</div>
<p class="body">is known as the <i class="fm-italics">spectral norm</i> of the matrix <i class="timesitalic">A</i>. Note that <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span></span> is a vector and <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||<sub class="fm-subscript">2</sub></span> is its length. (We will sometimes drop the subscript <span class="math">2</span> and denote the spectral norm as <span class="math">||<i class="fm-italics">A</i>||</span>.)</p>
<p class="body">Now consider the vector <i class="timesitalic">Ax̂</i>. Its magnitude is</p>
<p class="fm-equation"><span class="math">||<i class="fm-italics">Ax̂</i>|| = (<i class="fm-italics">Ax̂</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i>(<i class="fm-italics">Ax̂</i>) = <i class="fm-italics">x̂<sup class="fm-superscript">T</sup>A<sup class="fm-superscript">T</sup>Ax̂</i></span></p>
<p class="body">This is a quadratic form. From section <a class="url" href="../Text/04.xhtml#sec-quadratic-form">4.2</a>, we know it will be maximized (minimized) when <i class="timesitalic">x̂</i> s aligned with the largest smallest) eigenvalue of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>. Thus the spectral norm is given by the largest eigenvalue of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i></p><!--<p class="Body"><span class="times">$$\|A\|_{2} =
\underset{\hat{x}}{\max}\;\;\|A\hat{x}\|= \sigma_{1}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="32" src="../../OEBPS/Images/eq_04-04.png" width="179"/></p>
</div>
<p class="fm-equation-caption">Equation 4.4 <span class="calibre" id="eq-mat-spectral-norm"/></p>
<p class="body">where <span class="math"><i class="fm-italics">σ</i><sub class="fm-subscript">1</sub></span> is the largest eigenvalue of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>. It is also (the square of) the largest singular value of <i class="timesitalic">A</i>. We will see <span class="math"><i class="fm-italics">σ</i><sub class="fm-subscript">1</sub></span> again in section <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>, when we study singular value decomposition (SVD).</p>
<h3 class="fm-head1" id="frobenius-norms">4.3.2 Frobenius norms</h3>
<p class="body">An alternative measure for the magnitude of a matrix is the Frobenius norm, defined as</p><!--<p class="Body"><span class="times">$$\|A\|_{F} =
\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} \|a_{ij}\|^{2}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="72" src="../../OEBPS/Images/eq_04-05.png" width="181"/></p>
</div>
<p class="fm-equation-caption">Equation 4.5</p>
<p class="body">In other words, it is the root mean square of all the matrix elements.</p>
<p class="body">It can be proved that the Frobenius norm is equal to the root mean square of the sum of all the singular values (eigenvalues of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>) of the matrix</p><!--<p class="Body"><span class="times">$$\|A\|_{F} =
\sqrt{\sum_{i=1}^{min(m,n)}\sigma_{i}^{2}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_04-06.png" width="160"/></p>
</div>
<p class="fm-equation-caption">Equation 4.6</p>
<h2 class="fm-head" id="sec-pca">4.4 Principal component analysis</h2>
<p class="body"><a id="marker-123"/>Suppose we have a set of numbers, <span class="math"><i class="fm-italics">X</i> = {<i class="fm-italics">x</i><sup class="fm-superscript">(0)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>,⋯, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>. We want to get a sense of how tightly packed these points are. In other words, we want to measure the <i class="fm-italics">spread</i> of these numbers. Figure <a class="url" href="#fig-pca-1d">4.2</a> shows such a distribution.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="332" id="fig-pca-1d" src="../../OEBPS/Images/CH04_F02_Chaudhury.png" width="654"/></p>
<p class="figurecaption">Figure 4.2 A <span class="math">1</span>D distribution of points. The distance between extreme points is <i class="fm-italics">not</i> a fair representation of the spread of points: the distribution is not uniform, and the extreme points are far from the others. Most points are within a more tightly packed region.</p>
</div>
<p class="body">Note that the points need not be uniformly distributed. In particular, the extreme points (<i class="timesitalic">x<sub class="fm-subscript">max</sub></i>, <i class="timesitalic">x<sub class="fm-subscript">min</sub></i>) may be far from most other points (as in figure <a class="url" href="#fig-pca-1d">4.2</a>). Thus, <span class="math">(<i class="timesitalic">x<sub class="fm-subscript">max</sub></i>– <i class="timesitalic">x<sub class="fm-subscript">min</sub></i>)/(<i class="timesitalic">n</i>+1)</span> is not a fair representation of the average spread of points here. Most points are within a more tightly packed region. The statistically sensible way to obtain the spread is to first obtain the mean:</p><!--<p class="FM-Equation"><span class="times">$$\mu = \frac{1}{n} \sum_{i=0}^{n}
x^{\left(i\right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_04-06-a.png" width="104"/></p>
</div>
<p class="body">Then obtain the average distance of the numbers from the mean:</p><!--<p class="FM-Equation"><span class="times">$$\sigma^{2} = \frac{1}{n} \sum_{i=0}^{n}
\left(x^{\left(i\right)}-\mu\right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_04-06-b.png" width="173"/></p>
</div>
<p class="body">(If we want to, we can take the square root and use <i class="timesitalic">σ</i>, but it is often not necessary to incur that extra computational burden). This scalar quantity, <i class="timesitalic">σ</i>, is a good measure of the mean packing density or spread of the points in <span class="math">1</span>D. You may recognize that the previous equation is nothing but the famous variance formula from statistics. Can we extend the notion to higher-dimensional data?</p>
<p class="body">Let’s first examine the idea in two dimensions. As usual, we name our coordinate axes <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, and so on, instead of <i class="timesitalic">X</i>, <i class="timesitalic">Y</i>, to facilitate the extension to multiple dimensions. An individual <span class="math">2</span>D data point is denoted <!--<span class="times">$\vec{x}^{\left(i\right)}=\begin{bmatrix}x^{\left(i\right)}_{0}\\x^{\left(i\right)}_{1}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_04-06-c.png" width="87"/></span>. The dataset is <span class="math">{<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(0)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, ⋯, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>.</p>
<p class="body">The mean is straightforward. Instead of one means, we have two:</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
\mu_{0} &amp;= \frac{1}{n} \sum_{i=0}^{n} x^{\left(i\right)}_{0}\\
\mu_{1} &amp;= \frac{1}{n} \sum_{i=0}^{n}
x^{\left(i\right)}_{1}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="117" src="../../OEBPS/Images/eq_04-06-d.png" width="118"/></p>
</div>
<p class="body">Thus we now have a mean <i class="fm-italics">vector</i>:</p><!--<p class="FM-Equation"><span class="times">$$\vec{\mu} =
\begin{bmatrix}
\mu_{0}\\\mu_{1}
\end{bmatrix} = \frac{1}{n} \sum_{i=0}^{n}
\vec{x}^{\left(i\right)}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_04-06-e.png" width="158"/></p>
</div>
<p class="body"><a id="marker-124"/>Now let’s do the variance. The immediate problem we face is that there are infinite possible directions in the <span class="math">2</span>D plane. We can measure variance along any of them, and it will be different for each choice. We can, of course, find the variance along the <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span> and <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span> axes:</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
\sigma_{00}^{2} &amp;= \frac{1}{n} \sum_{i=0}^{n}
\left(x^{\left(i\right)}_{0}-\mu_{0}\right)^{2}\\[4pt]
\sigma_{11}^{2} &amp;= \frac{1}{n} \sum_{i=0}^{n}
\left(x^{\left(i\right)}_{1}-\mu_{1}\right)^{2}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="124" src="../../OEBPS/Images/eq_04-06-f.png" width="188"/></p>
</div>
<p class="body"><span class="math"><i class="fm-italics">σ</i><sub class="fm-subscript">00</sub></span> and <span class="math"><i class="fm-italics">σ</i><sub class="fm-subscript">11</sub></span> tells us the variance along <i class="fm-italics">only one</i> of the axes <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">0</sub></span> and <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript">1</sub></span>, respectively. But in general, there will be joint variation along both axes. To deal with joint variation, let’s introduce a cross term:</p><!--<p class="FM-Equation"><span class="times">$$\sigma_{01}^{2} = \sigma_{10}^{2} = \frac{1}{n}
\sum_{i=0}^{n} \left(x^{\left(i\right)}_{0} -
\mu_{0}\right)\left(x^{\left(i\right)}_{1} - \mu_{1}\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_04-06-g.png" width="310"/></p>
</div>
<p class="body">These equations can be written compactly in matrix vector notation:</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
\vec{\mu} &amp;= \frac{1}{n} \sum_{i=0}^{n} \vec{x}^{\left(i\right)}\\ C &amp;= \begin{bmatrix}
\sigma_{00} &amp; \sigma_{01}\\
\sigma_{10} &amp; \sigma_{11}
\end{bmatrix} =
\frac{1}{n} \sum_{i=0}^{n} \left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)^{T}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="129" src="../../OEBPS/Images/eq_04-06-h.png" width="348"/></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In the expression for <i class="timesitalic">C</i>, we are <i class="fm-italics">not</i> taking the dot product of the vectors <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)</span> and <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)</span>. The dot product would be <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>−<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>)</span>. Here, the second element of the product is transposed, not the first. Consequently, the result is a matrix. The dot product would yield a scalar.)</p>
<p class="body">The previous equations are general, meaning they can be extended to any dimension. To be precise, given a set of <i class="timesitalic">n</i> multidimensional data points <span class="math"><i class="fm-italics">X</i> = {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(0)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>,⋯, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, we can define</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{\mu} &amp;= \frac{1}{n} \sum_{i=0}^{n} \vec{x}^{\left(i\right)}\\ C &amp;= \frac{1}{n} \sum_{i=0}^{n} \left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)^{T}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_04-07.png" width="109"/></p>
</div>
<p class="fm-equation-caption">Equation 4.7</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_04-08.png" width="241"/></p>
</div>
<p class="fm-equation-caption">Equation 4.8 <span class="calibre" id="eq-covar-mat-0"/></p>
<p class="body"><a id="marker-125"/>Note how the mean has become a vector (it was a scalar for <span class="math">1</span>D data) and the scalar variance of <span class="math">1</span>D, <i class="timesitalic">σ</i>, has become a matrix <i class="timesitalic">C</i>. This matrix is called the <i class="fm-italics">covariance matrix</i>. The <span class="math">(<i class="fm-italics">n</i>+1)</span>-dimensional mean and covariance matrix can also be defined a</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{\mu} &amp;= \begin{bmatrix}
\mu_{0}\\
\mu_{1}\\
\cdots\\
\mu_{n}\\
\end{bmatrix}\nonumber\\ C &amp;= \begin{bmatrix}
\sigma_{00} &amp;\sigma_{01} &amp;\cdots &amp;\sigma_{0n}\\
\sigma_{10} &amp;\sigma_{11} &amp;\cdots &amp;\sigma_{1n}\\
\vdots &amp;\vdots &amp;\vdots &amp;\vdots\\
\sigma_{n0} &amp;\sigma_{n1} &amp;\cdots &amp;\sigma_{nn}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="279" src="../../OEBPS/Images/eq_04-09.png" width="207"/></p>
</div>
<p class="fm-equation-caption">Equation 4.9 <span class="calibre" id="eq-covar-mat"/></p>
<p class="body">where</p><!--<p class="Body"><span class="times">$$\sigma_{ij} = \sum_{k=0}^{n}
\left(x_{i}^{\left(k\right)} -
\mu_{i}\right)\left(x_{j}^{\left(k\right)} - \mu_{j}\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_04-10.png" width="240"/></p>
</div>
<p class="fm-equation-caption">Equation 4.10 <span class="calibre" id="eq-varij"/></p>
<p class="body">For <span class="math"><i class="fm-italics">i</i> = <i class="fm-italics">j</i></span>, <i class="timesitalic">σ<sub class="fm-subscript">ii</sub></i> is essentially the variance of the data along the <i class="timesitalic">i</i>th dimension. Thus the diagonal elements of matrix <i class="timesitalic">C</i> contain the variance along the coordinate axes. Off-diagonal elements correspond to cross-covariances.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Equations <a class="url" href="#eq-covar-mat-0">4.8</a> and <a class="url" href="#eq-covar-mat">4.9</a> are equivalent.</p>
<h3 class="fm-head1" id="direction-of-maximum-spread">4.4.1 Direction of maximum spread</h3>
<p class="body">What is the direction of maximum spread/variance? Let’s first consider an arbitrary direction specified by the unit vector <i class="timesitalic">l̂</i>. Recalling that the component of any vector along a direction is yielded by the dot product of the vector with the unit direction vector, the components of the data points along <i class="timesitalic">l̂</i> are given by</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">X</i> <sup class="fm-superscript">′</sup> = {<i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(0)</sup>, <i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>,⋯, <i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Remember figure <a class="url" href="02.xhtml#fig-multi-dim-lineeq">2.8b</a>, which showed that the component of one vector along another is given by the dot product between them? <span class="math"><i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> are dot products and hence scalar values.</p>
<p class="body">The spread along direction <i class="timesitalic">l̂</i> is given by the variance of the scalar values in <span class="math"><i class="fm-italics">X</i> <sup class="fm-superscript">′</sup></span>. The mean of the values in <span class="math"><i class="fm-italics">X</i> <sup class="fm-superscript">′</sup></span> is given by</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
\mu^{'} &amp;= \frac{1}{n} \sum_{i=0}^{n}
\hat{l}^{\,T}\vec{x}^{\left(i\right)}\\
        &amp;= \hat{l}^{\,T} \left(\frac{1}{n} \sum_{i=0}^{n}
\vec{x}^{\left(i\right)}\right)\\
        &amp;= \hat{l}^{\,T}\vec{\mu}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="146" src="../../OEBPS/Images/eq_04-10-b.png" width="151"/></p>
</div>
<p class="body">and the variance is</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned} C^{\,'} &amp;= \frac{1}{n} \sum_{i=0}^{n}
\left(\hat{l}^{\,T}\vec{x}^{\left(i\right)} -
\hat{l}^{\,T}\vec{\mu}\right)\left(\hat{l}^{\,T}\vec{x}^{\left(i\right)}
- \hat{l}^{\,T}\vec{\mu}\right)^{T}\\
&amp;= \frac{1}{n} \sum_{i=0}^{n}
\hat{l}^{\,T}\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\left(\hat{l}^{\,T}\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\right)^{T}\\
&amp;= \frac{1}{n} \sum_{i=0}^{n}
\hat{l}^{\,T}\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)^{T}\hat{l}\\
&amp;= \hat{l}^{\,T}\left(\frac{1}{n} \sum_{i=0}^{n}
\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)^{T}\right)\hat{l}\\
&amp;= \hat{l}^{\,T} C \hat{l}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="266" src="../../OEBPS/Images/eq_04-10-c.png" width="329"/></p>
</div>
<p class="body"><a id="marker-126"/>Note that <span class="math"><i class="fm-italics">C</i> <sup class="fm-superscript">′</sup> = <i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><i class="fm-italics">Cl̂</i></span> is the variance of the data components along the direction <i class="timesitalic">l̂</i>. As such, it represents the spread of the data along that direction. What is the direction <i class="timesitalic">l̂</i> along which this spread <span class="math"><i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><i class="fm-italics">Cl̂</i></span> is maximal? It is the direction <i class="timesitalic">l̂</i> that maximizes <span class="math"><i class="fm-italics">C</i> <sup class="fm-superscript">′</sup> = <i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><i class="fm-italics">Cl̂</i></span>. This maximizing direction can be identified using the quadratic form optimization technique we discussed in <a class="url" href="../Text/04.xhtml#sec-quadratic-form">4.2</a>. Applying that, we have the following results:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Variance is maximal when <i class="timesitalic">l̂</i> is along the eigenvector corresponding to the largest eigenvalue of the covariance matrix <i class="timesitalic">C</i>. This direction is called the <i class="fm-italics">first principal axis</i> of the multidimensional data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The components of the data vectors along the principal axis are known as <i class="fm-italics">first principal components</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The value of the variance along the first principal axis, given by the corresponding eigenvalue of the covariance matrix, is called the <i class="fm-italics">first principal value</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The second principal axis is the eigenvector of the covariance matrix corresponding to the second largest eigenvalue of the covariance matrix. Second principal components and values are defined likewise.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The principal axes are orthogonal to each other because they are eigenvectors of the symmetric covariance matrix.</p>
</li>
</ul>
<p class="body">What is the practical significance of PCA? Why would we like to know the direction along which the spread is maximum for a point distribution? Sections <a class="url" href="#subsec-pca-app-dimred">4.4.2</a> through <a class="url" href="#subsec-pca-app-datacompress">4.4.5</a> are devoted to answering this question.</p>
<h3 class="fm-head1" id="subsec-pca-app-dimred">4.4.2 PCA and dimensionality reduction</h3>
<p class="body"><a id="marker-127"/>In section <a class="url" href="#sec-truedim">4.1</a>, we saw that when data points are clustered around a lower-dimensional subspace, it is beneficial to project them onto the subspace and reduce the dimensionality of the data representation. The dimensionally reduced data is more compactly representable and more amenable to deriving insights and analysis. In the particular case where the data points are clustered around a straight line or hyperplane, PCA can be used to generate a lower-dimensional data representation by getting rid of the principal components corresponding to relatively small principal values. The technique is agnostic to the dimensionality of the data. The line or hyperplane can be anywhere in the space, with arbitrary orientation.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="470" id="fig-pca-2d" src="../../OEBPS/Images/CH04_F03a_Chaudhury.png" width="561"/></p>
<p class="figurecaption">(a) Dimensionality reduction from <span class="math">2</span>D to <span class="math">1</span>D</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="432" id="fig-pca-3d" src="../../OEBPS/Images/CH04_F03b_Chaudhury.png" width="557"/></p>
<p class="figurecaption">(b) Dimensionality reduction from <span class="math">3</span>D to <span class="math">2</span>D</p>
</div>
<p class="figurecaption" id="fig-pca-3and2d">Figure 4.3 Dimensionality reduction via PCA. Original data points are shown with filled little circles, and hollow circles represent lower-dimensional representations.</p>
<p class="body">For instance, consider the <span class="math">2</span>D distribution shown in figure <a class="url" href="#fig-pca-3and2d">4.3a</a>. Here, the data is <span class="math">2</span>D and plotted on a plane, but the main spread of the data is along a <span class="math">1</span>D line shown by the thick two-arrowed line in the figure). There is very little spread in the direction orthogonal to that line (indicated by the little perpendiculars from the data points to the line in the figure). PCA reveals this internal structure. There are two principal values because the data is <span class="math">2</span>D), but one of them is much smaller than the other: this reveals that dimensionality reduction is possible. The principal axis corresponding to the larger principal value is along the line of maximum spread. The small perturbations along the other principal axis can be eliminated with little loss of information. Replacing each data point with its projection on the first principal axis converts the <span class="math">2</span>D dataset into a <span class="math">1</span>D dataset, brings out the true underlying pattern in the data (straight line), eliminates noise (little perpendiculars), and reduces storage costs.</p>
<p class="body">In figure <a class="url" href="#fig-pca-3and2d">4.3b</a>, the data is <span class="math">3</span>D, but the data points are clustered around a plane in <span class="math">3</span>D space (shown as the rectangle in the figure). The main spread of the data is <i class="fm-italics">along</i> the plane, while the spread in the direction normal to that plane (shown with little perpendiculars from data points to the plane) is small. PCA reveals this: there are three principal values (because the data is <span class="math">3</span>D), but one of them is much smaller than the other two, revealing that dimensionality reduction is possible. The principal axis corresponding to the small principal value is normal to the plane. We can ignore these perturbations (perpendiculars in figure <a class="url" href="#fig-pca-3and2d">4.3b</a>) with little loss of information. This is equivalent to projecting the data onto the plane formed by the first two principal axes. Doing so brings out the underlying data pattern (plane), eliminates noise (little perpendiculars), and reduces storage costs.</p>
<h3 class="fm-head1" id="pytorch-code-pca-and-dimensionality-reduction">4.4.3 PyTorch code: PCA and dimensionality reduction</h3>
<p class="body"><a id="marker-128"/>In this section, we provide a PyTorch code sample for PCA computation in listing <a class="url" href="#code-pca-computation">4.1</a>. Then we provide PyTorch code for applying PCA on a correlated dataset and an uncorrelated dataset in listings <a class="url" href="#code-pca-correlated">4.2</a> and <a class="url" href="#code-pca-uncorrelated">4.3</a>, respectively. The results are plotted in figure <a class="url" href="#fig-pca-results">4.4</a>.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="419" id="fig-pca-correlated-data" src="../../OEBPS/Images/CH04_F04a_Chaudhury.png" width="605"/></p>
<p class="figurecaption">(a) PCA on correlated data</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="428" id="fig-pca-uncorrelated-data" src="../../OEBPS/Images/CH04_F04b_Chaudhury.png" width="605"/></p>
<p class="figurecaption">(b) PCA on uncorrelated data</p>
</div>
<p class="figurecaption" id="fig-pca-results">Figure 4.4 PCA results. In (a), the data points are around the straight line <span class="math"><b class="fm-bold">y</b> = 2<i class="fm-italics">x</i></span>. Consequently, one principal value is much larger than the other, indicating that dimensionality reduction will work. In (b), both principal values are large. Dimensionality reduction will not work.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this section is available at <a class="url" href="http://mng.bz/aoYz">http://mng.bz/aoYz</a> in the form of fully functional and executable Jupyter notebooks.</p>
<p class="fm-code-listing-caption" id="code-pca-computation">Listing 4.1 PCA computation</p>
<pre class="programlisting">def pca(X):                                  <span class="fm-combinumeral">①</span>
    covariance_matrix = torch.cov(X.T)
    l, e = torch.linalg.eig(covariance_matrix)
    return l, e</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Returns principal values and vectors</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the PCA computation in listing 4.1 is available at <a class="url" href="http://mng.bz/DRYR">http://mng.bz/DRYR</a>.</p>
<p class="fm-code-listing-caption" id="code-pca-correlated">Listing 4.2 PCA on synthetic correlated data</p>
<pre class="programlisting">x_0 = torch.normal(0, 100, (N,))              <span class="fm-combinumeral">①</span>

x_1 = 2 * x_0 + torch.normal(0, 20, (N,))     <span class="fm-combinumeral">②</span>

  <span class="fm-combinumeral">③</span>
X = torch.column_stack((x_0, x_1))

  <span class="fm-combinumeral">④</span>
principal_values, principal_vectors = pca(X)  <span class="fm-combinumeral">⑤</span>

X_proj = torch.matmul(X, first_princpal_vec)  <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Random feature vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Correlated feature vector + minor noise</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Data matrix spread mostly along y = 2x</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> One large principal value and one small</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> First principal vector along y = 2x</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Dimensionality reduction by projecting on the first principal vector</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">Principal values are: [62.6133, 48991.0469]
First Principal Vector is: [-0.44, -0.89]</pre>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the PCA computation in listing 4.2 is available at <a class="url" href="http://mng.bz/gojl">http://mng.bz/gojl</a>.<a id="marker-129"/></p>
<p class="fm-code-listing-caption" id="code-pca-uncorrelated">Listing 4.3 PCA on synthetic uncorrelated data</p>
<pre class="programlisting">x_0 = torch.normal(0, 100, (N,))
x_1 = torch.normal(0, 100, (N,))             <span class="fm-combinumeral">①</span>
X = torch.column_stack((x_0, x_1))

principal_values, principal_vectors = pca(X) <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Random uncorrelated feature-vector pair</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Principal values close to each other. The spread of the data points is comparable in both directions.</p>
<p class="body">Here is the output:</p>
<pre class="programlisting">Principal values are [ 9736.4033, 7876.6592]</pre>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the PCA computation in listing 4.3 is available at <a class="url" href="http://mng.bz/e5Kz">http://mng.bz/e5Kz</a>.</p>
<h3 class="fm-head1" id="limitations-of-pca">4.4.4 Limitations of PCA</h3>
<p class="body">PCA assumes that the underlying pattern is linear in nature. Where this is not true, PCA will not capture the correct underlying pattern. This is illustrated schematically in figure <a class="url" href="#fig-non-linear-pca">4.5a</a> and via experimental results from listing <a class="url" href="#code-pca-uncorrelated">4.3</a>. Figure <a class="url" href="#fig-non-linear-pca">4.5b</a> shows the results of running listing <a class="url" href="#code-synthetic-nonlin-correlated">4.4</a>, where we synthetically generate non-linearly correlated data and perform PCA. The straight line at the base shows the first principal axis. Projecting data on this axis results in a large error in the data positions (loss of information). Linear PCA will not do well.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="204" id="fig-curved-underlying-pattern" src="../../OEBPS/Images/CH04_F05a_Chaudhury.png" width="540"/></p>
<p class="figurecaption">(a) Schematic <span class="math">2</span>D data distribution with a curved underlying pattern</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="456" id="fig-pca-nonlinear-data" src="../../OEBPS/Images/CH04_F05b_Chaudhury.png" width="674"/></p>
<p class="figurecaption">(b) PCA results on synthetic (computer generated) non-linearly correlated data. The line at the base shows the first principal axis.</p>
</div>
<p class="figurecaption" id="fig-non-linear-pca">Figure 4.5 Non-linearly correlated data. The points are distributed around a curve as opposed to a straight line. It is impossible to find a straight line such that all points are near it.</p>
<p class="fm-code-listing-caption" id="code-synthetic-nonlin-correlated">Listing 4.4 PCA on synthetic nonlinearly correlated data</p>
<pre class="programlisting">x_0 = torch.normal(0, 100, (N,))
x_1 = 2 * (x_0 ** 2) + torch.normal(0, 5, (N,))
X = torch.column_stack((x_0, x_1))

principal_values, principal_vectors = pca(X) <span class="fm-combinumeral">①</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Principal vectors fail to capture the underlying distribution.</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting" id="code-correlated-pca-result">Principal values are [9.3440e+03, 5.3373e+08] 
Mean loss in information: 68.0108526887 - high</pre>
<h3 class="fm-head1" id="subsec-pca-app-datacompress">4.4.5 PCA and data compression</h3>
<p class="body"><a id="marker-130"/>If we want to represent a large multidimensional dataset within a fixed byte budget, what information can we can get rid of with the least loss of accuracy? Clearly, the answer is the principal components along the smaller principal values—getting rid of them actually helps, as described in section <a class="url" href="#subsec-pca-app-dimred">4.4.2</a>. To compress data, we often perform PCA and then replace the data points with their projections on first few principal axes; doing so reduces the number of data components to store. This is the underlying principle in JPEG 98 image compression techniques.</p>
<h2 class="fm-head" id="sec-svd">4.5 Singular value decomposition</h2>
<p class="body">Singular value decomposition (SVD) may be the most important linear algebraic tool in machine learning. Among other things, PCA and LSA implementations are built based on SVD. We illustrate the basic idea in this section.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> There are several slightly different forms of SVD. We have chosen the one that seems intuitively simplest.</p>
<p class="body">The SVD theorem states that any matrix <i class="timesitalic">A</i>, singular or nonsingular, rectangular or square, can be decomposed as the product of three matrices</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">U</i>Σ<i class="fm-italics">V<sup class="fm-superscript">T</sup></i></span></p>
<p class="fm-equation-caption">Equation 4.11 <span class="calibre" id="eq-svd"/></p>
<p class="body">where (assuming that the matrix <i class="timesitalic">A</i> is <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span>)</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><span class="math">Σ</span> is an <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> diagonal matrix. Its diagonal elements contain the square roots of the eigenvalues of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>. These are also known as the singular values of <i class="timesitalic">A</i>. The singular values appear in decreasing order in the diagonal of <span class="math">Σ</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">V</i> is an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> orthogonal matrix containing eigenvectors of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> in its columns.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">U</i> is an <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">m</i></span> orthogonal matrix containing eigenvectors of <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i> in its columns.</p>
</li>
</ul>
<h3 class="fm-head1" id="informal-proof-of-the-svd-theorem">4.5.1 Informal proof of the SVD theorem</h3>
<p class="body"><a id="marker-131"/>We will provide an informal proof of the SVD theorem through a series of lemmas. Going through these will provide additional insights.</p>
<p class="fm-head2" id="lemma-1">Lemma 1</p>
<p class="body"><i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> is symmetric positive semidefinite. Its eigenvalues—aka singular values—are non-negative. Its eigenvectors—aka singular vectors—are orthogonal.</p>
<p class="fm-head2" id="proof-of-lemma-1">Proof of lemma 1</p>
<p class="body">Let’s say <i class="timesitalic">A</i> has <i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns. Then <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> is an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> square matrix</p>
<p class="fm-equation"><span class="math">(<i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">A<sup class="fm-superscript">T</sup></i>(<i class="fm-italics">A<sup class="fm-superscript">T</sup></i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = <i class="fm-italics">A<sup class="fm-superscript">T</sup>A</i></span></p>
<p class="body">which proves that <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> is symmetric. Also, for any <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>,</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A<sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = (<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i>(<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = ||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>||<sup class="fm-superscript">2</sup> &gt; 0</span></p>
<p class="body">which, as per section <a class="url" href="#subsec-sym-pos-sd">4.2.2</a>, proves that the matrix <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> is symmetric and positive semidefinite. Hence, its eigenvalues are all positive or zero.</p>
<p class="body">We proved in section <a class="url" href="02.xhtml#sec-eigen-values-vectors">2.13</a> that symmetric matrices have orthogonal eigenvectors. That proves that singular vectors are orthogonal.</p>
<p class="body">Let <span class="math">(<i class="fm-italics">λ<sub class="fm-subscript">i</sub>, v̂</i><sub class="fm-subscript">1</sub>)</span>, for <span class="math"><i class="fm-italics">i</i> <span class="cambria">∈</span> [1, <i class="fm-italics">n</i>]</span> be the set of eigenvalue, eigenvector pairs of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>—aka the singular value, singular vector pair of <i class="timesitalic">A</i>. Note that without loss of generality, we can assume <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≥ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub> ≥ ⋯ <i class="fm-italics">λ<sub class="fm-subscript">n</sub></i></span> because if not, we can always renumber the eigenvalues and eigenvectors).</p>
<p class="body">Now, by definition,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup>Av̂<sub class="fm-subscript">i</sub></i> = <i class="fm-italics">λ<sub class="fm-subscript">i</sub>v̂<sub class="fm-subscript">i</sub></i> <span class="cambria">∀</span><i class="fm-italics">i</i> <span class="cambria">∈</span> [1, <i class="fm-italics">n</i>]</span></p>
<p class="body">From lemma 1, singular vectors are orthogonal, and hence</p><!--<p class="FM-Equation"><span class="times">$$\hat{v}_{i}^{T}\hat{v}_{j}=
\begin{cases} 0 &amp; i \neq j\\ 1 &amp; i = j
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_04-12.png" width="130"/></p>
</div>
<p class="fm-equation-caption">Equation 4.12 <span class="calibre" id="eq-ortho-eigvec"/></p>
<p class="body">Note that <i class="timesitalic">v̂<sub class="fm-subscript">i</sub></i>s are unit vectors (that is why we are using the hat sign as opposed to the overhead arrow). As described in section <a class="url" href="02.xhtml#sec-eigen-values-vectors">2.13</a>, eigenvectors remain eigenvectors if we change their length. We are free to choose any length for eigenvectors as long as we choose it consistently. We are choosing unit-length eigenvectors here.</p>
<p class="fm-head2" id="lemma-2">Lemma 2</p>
<p class="body"><i class="timesitalic">AA<sup class="fm-superscript">T</sup></i> is symmetric positive semidefinite. Its eigenvalues are non-negative and eigenvectors are orthogonal.<a id="marker-132"/></p>
<p class="fm-head2" id="proof-of-lemma-2">Proof of lemma 2</p>
<p class="fm-equation"><span class="math">(<i class="fm-italics">AA<sup class="fm-superscript">T</sup></i>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> = (<i class="fm-italics">A<sup class="fm-superscript">T</sup></i>)<i class="fm-italics"><sup class="fm-superscript">T</sup>A<sup class="fm-superscript">T</sup></i> = <i class="fm-italics">AA<sup class="fm-superscript">T</sup></i></span></p>
<p class="body">Also,</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>AA<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = (<i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i>(<i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = ||(<i class="fm-italics">A<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)|| ≥ 0</span></p>
<p class="body">and so on.</p>
<p class="fm-head2" id="lemma-3">Lemma 3</p>
<p class="body"><span class="math">1/√<i class="fm-italics">λ<sub class="fm-subscript">i</sub> ⋅ Av̂</i><sub class="fm-subscript">1</sub>, <span class="cambria">∀</span><i class="fm-italics">i</i> <span class="cambria">∈</span> [1, <i class="fm-italics">n</i>]</span> is a set of orthogonal unit vectors.</p>
<p class="fm-head2" id="proof-of-lemma-3">Proof of lemma 3</p>
<p class="body">Let’s take the dot product of a pair of these vectors:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="119" src="../../OEBPS/Images/eq_04-12-a.png" width="332"/></p>
</div>
<p class="body">Since <span class="math"><i class="fm-italics">λ<sub class="fm-subscript">j</sub></i>, <i class="fm-italics">v̂<sub class="fm-subscript">j</sub></i></span> are eigenvalue, eigenvector pairs of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>, the previous equation can be rewritten as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_04-12-b.png" width="109"/></p>
</div>
<p class="body">which, using equation <a class="url" href="#eq-ortho-eigvec">4.12</a>, can be rewritten as</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_04-12-c.png" width="164"/></p>
</div>
<p class="fm-head2" id="lemma-4">Lemma 4</p>
<p class="body">If <span class="math">(<i class="fm-italics">λ<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">v̂<sub class="fm-subscript">i</sub></i>)</span> is an eigenvalue, eigenvector pair of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>, then <span class="math"><i class="fm-italics">λ<sub class="fm-subscript">i</sub>, û<sub class="fm-subscript">i</sub></i> = 1/√<i class="fm-italics">λ<sub class="fm-subscript">i</sub></i> ⋅ <i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i></span> is an eigenvalue, eigenvector pair of <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i>.</p>
<p class="fm-head2" id="proof-of-lemma-4">Proof of lemma 4</p>
<p class="body">Given</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A<sup class="fm-superscript">T</sup>Av̂<sub class="fm-subscript">i</sub></i> = <i class="fm-italics">λ<sub class="fm-subscript">i</sub>v̂<sub class="fm-subscript">i</sub></i></span></p>
<p class="body">left-multiplying both sides of the equation by <i class="timesitalic">A</i>, we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">AA<sup class="fm-superscript">T</sup></i><i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i> = <i class="fm-italics">λ<sub class="fm-subscript">i</sub></i> <i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">AA<sup class="fm-superscript">T</sup></i>(<i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i>) = <i class="fm-italics">λ<sub class="fm-subscript">i</sub></i> (<i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i>)</span></p>
<p class="body">Substituting <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span><sub class="fm-subscript">i</sub></i> = <i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i></span> in the last equation, we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">AA<sup class="fm-superscript">T</sup><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span><sub class="fm-subscript">i</sub></i> = <i class="fm-italics">λ<sub class="fm-subscript">i</sub><span class="infigure"><img alt="" class="calibre19" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span><sub class="fm-subscript">i</sub></i></span></p>
<p class="body">which proves that <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_f.png" width="22"/></span><sub class="fm-subscript">i</sub></i> = <i class="fm-italics">Av̂<sub class="fm-subscript">i</sub></i></span> is an eigenvector of <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i> with <i class="timesitalic">λ<sub class="fm-subscript">i</sub></i> as a corresponding eigenvalue. Multiplying by <span class="math">1/√<i class="fm-italics">λ<sub class="fm-subscript">i</sub></i></span> converts it into a unit vector as per lemma 3. This completes the proof of the lemma.</p>
<h3 class="fm-head1" id="proof-of-the-svd-theorem">4.5.2 Proof of the SVD theorem</h3>
<p class="body"><a id="marker-133"/>Now we are ready to examine the proof of the SVD theorem.</p>
<p class="fm-head2" id="case-1-more-rows-than-columns-in-boldsymbol-a">Case 1: More rows than columns in <span class="timesbold">A</span></p>
<p class="body">If <i class="timesitalic">m</i>, the number of rows in <i class="timesitalic">A</i>, is greater than or equal to <i class="timesitalic">n</i>, the number of columns in <i class="timesitalic">A</i>, we define</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
&amp;U =
\begin{bmatrix}
\hat{u}_{1} &amp; \hat{u}_{2} &amp; \cdots &amp; \hat{u}_{n} &amp;
\hat{u}_{n+1} &amp; \cdots \hat{u}_{m}
\end{bmatrix}\\[6pt]
&amp;\Sigma =
\begin{bmatrix}
\sqrt{\lambda_{1}} &amp; 0                  &amp;  \cdots &amp; 0\\[2pt] 0                  &amp; \sqrt{\lambda_{2}} &amp;  \cdots &amp; 0\\[2pt]
&amp;\vdots\\[2pt] 0 &amp; 0 &amp; \cdots &amp; \sqrt{\lambda_{n}}\\[2pt] 0 &amp; 0 &amp; \cdots &amp; 0\\[2pt]
&amp;\vdots\\[2pt] 0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix}\\[6pt]
&amp;V = \begin{bmatrix}
\hat{v}_{1} &amp; \hat{v}_{2} &amp; \cdots &amp; \hat{v}_{n}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="376" src="../../OEBPS/Images/eq_04-12-d.png" width="294"/></p>
</div>
<p class="body">Note the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">From lemma 1, we know that the eigenvalues of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> are positive. This makes the square roots, <span class="math">√<i class="fm-italics">λ<sub class="fm-subscript">i</sub>s</i></span>, real.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">U</i> is an <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">m</i></span> orthogonal matrix whose columns are the eigenvectors of <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i>. Since, <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i> is <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">m</i></span>, it has <i class="timesitalic">m</i> eigenvalues and eigenvectors. The first <i class="timesitalic">n</i> of them are <span class="math"><i class="fm-italics">û</i><sub class="fm-subscript">1</sub> = 1/√<i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ⋅ <i class="fm-italics">Av̂</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">û</i><sub class="fm-subscript">2</sub> = 1/√<i class="fm-italics">λ</i><sub class="fm-subscript">2</sub> ⋅ <i class="fm-italics">Av̂</i><sub class="fm-subscript">2</sub>, … , <i class="fm-italics">û<sub class="fm-subscript">n</sub></i> = 1/√<i class="fm-italics">λ<sub class="fm-subscript">i</sub></i> ⋅ <i class="fm-italics">Av̂<sub class="fm-subscript">n</sub></i></span> from lemma 4, we know these are eigenvectors of <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i>). In this case, by our initial assumption, <span class="math"><i class="fm-italics">n</i> &lt; <i class="fm-italics">m</i></span>. Thus <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i> has <span class="math">(<i class="fm-italics">m</i> − <i class="fm-italics">n</i>)</span> more eigenvectors, <span class="math"><i class="fm-italics">û</i><sub class="fm-subscript"><i class="fm-italics1">n</i> + 1</sub>, ⋯ <i class="fm-italics">û<sub class="fm-subscript">m</sub></i></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">V</i> is an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> orthogonal matrix with the eigenvectors of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> that is, <span class="math"><i class="fm-italics">v̂</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">v̂</i><sub class="fm-subscript">2</sub></span>, <span class="math">⋯</span>, <i class="timesitalic">v̂<sub class="fm-subscript">n</sub></i>) as its columns.</p>
</li>
</ul>
<p class="body">Consider the matrix product <span class="math"><i class="fm-italics">U</i>Σ</span>. From basic matrix multiplication rules (section <a class="url" href="02.xhtml#sec-misc_mat-vec">2.5</a>, we can see that</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned} U\Sigma &amp;=
\begin{bmatrix}
\hat{u}_{1} &amp; \hat{u}_{2} &amp; \cdots &amp; \hat{u}_{n} &amp;
\hat{u}_{n+1} &amp; \cdots \hat{u}_{m}
\end{bmatrix}
\begin{bmatrix}
\sqrt{\lambda_{1}} &amp; 0                  &amp;  \cdots &amp; 0\\[-2pt] 0                  &amp; \sqrt{\lambda_{2}} &amp;  \cdots &amp; 0\\[-2pt]
&amp;\vdots\\[-2pt] 0 &amp; 0 &amp; \cdots &amp; \sqrt{\lambda_{n}}\\[-2pt] 0 &amp; 0 &amp; \cdots &amp; 0\\[-2pt]
&amp;\vdots\\[-2pt] 0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix}\\
&amp;=
\begin{bmatrix}
\sqrt{\lambda_{1}}\hat{u}_{1} &amp; \sqrt{\lambda_{2}}\hat{u}_{2} &amp;
\cdots &amp; \sqrt{\lambda_{n}}\hat{u}_{n}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="274" src="../../OEBPS/Images/eq_04-12-e.png" width="495"/></p>
</div>
<p class="body"><a id="marker-134"/>Note that the last columns of <i class="timesitalic">U</i>, <span class="math"><i class="fm-italics">û</i><sub class="fm-subscript"><i class="fm-italics1">n</i> + 1</sub>, ⋯, <i class="fm-italics">û<sub class="fm-subscript">m</sub></i></span>, are multiplied by all zeros in <span class="math">Σ</span> and vanishing. Thus,</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned} U\Sigma &amp;=
\begin{bmatrix}
\sqrt{\lambda_{1}}\hat{u}_{1} &amp; \sqrt{\lambda_{2}}\hat{u}_{2} &amp;
\cdots &amp; \sqrt{\lambda_{n}}\hat{u}_{n}
\end{bmatrix}\\
&amp;=
\begin{bmatrix} A\hat{v}_{1} &amp; A\hat{v}_{2} &amp; \cdots &amp; A\hat{v}_{n}
\end{bmatrix}\\
&amp;= A\begin{bmatrix}
\hat{v}_{1} &amp; \hat{v}_{2} &amp; \cdots &amp; \hat{v}_{n}
\end{bmatrix}\\
&amp;= A V\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="142" src="../../OEBPS/Images/eq_04-12-f.png" width="286"/></p>
</div>
<p class="body">The later columns of <i class="timesitalic">U</i>—those named with <i class="timesitalic">u</i>s—fail to survive because they are multiplied by the zeros at the bottom of <span class="math">Σ</span>.</p>
<p class="body">Thus we have proved that</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">AV</i> = <i class="fm-italics">U</i>Σ</span></p>
<p class="body">Then</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">AVV<sup class="fm-superscript">T</sup></i> = <i class="fm-italics">U</i>Σ<i class="fm-italics">V<sup class="fm-superscript">T</sup></i></span></p>
<p class="body">Since <i class="timesitalic">V</i> is orthogonal, <span class="math"><i class="fm-italics">VV<sup class="fm-superscript">T</sup></i> = <b class="fm-bold">I</b></span>. Hence</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">U</i>Σ<i class="fm-italics">V<sup class="fm-superscript">T</sup></i></span></p>
<p class="body">which completes the proof of the singular value theorem.</p>
<p class="fm-head2" id="case-2-fewer-rows-than-columns-in-boldsymbol-a">Case 2: Fewer rows than columns in <span class="timesbold">A</span></p>
<p class="body">If <i class="timesitalic">m</i>, the number of rows in <i class="timesitalic">A</i>, is less than or equal to <i class="timesitalic">n</i>, the number of columns in <i class="timesitalic">A</i>, we have</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
&amp;U =
\begin{bmatrix}
\hat{u}_{1} &amp; \hat{u}_{2} &amp; \cdots &amp; \cdots \hat{u}_{m}
\end{bmatrix}\\
&amp;\Sigma =
\begin{bmatrix}
\sqrt{\lambda_{1}} &amp; 0                  &amp;  \cdots &amp; 0
&amp;  \cdots &amp; 0\\[-2pt] 0                  &amp; \sqrt{\lambda_{2}} &amp;  \cdots &amp; 0
&amp;  \cdots &amp; 0\\[-2pt]
&amp;\vdots\\[-2pt] 0 &amp; 0 &amp; \cdots &amp; \sqrt{\lambda_{n}}     &amp;  \cdots &amp; 0
\end{bmatrix}\\
&amp;V = \begin{bmatrix}
\hat{v}_{1} &amp; \hat{v}_{2} &amp; \cdots &amp; \hat{v}_{n}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="210" src="../../OEBPS/Images/eq_04-12-g.png" width="296"/></p>
</div>
<p class="body">The proof follows along similar lines.</p>
<h3 class="fm-head1" id="subsec-svd-app-pca-computation">4.5.3 Applying SVD: PCA computation</h3>
<p class="body">We will illustrate the idea first with a toy dataset. Consider a <span class="math">3</span>D dataset with five points. We use a superscript to denote the index of the data instance and a subscript to denote the component. Thus the <i class="timesitalic">i</i>th data instance vector is denoted as <span class="math">[<i class="fm-italics">x</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>   <i class="fm-italics">x</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>   <i class="fm-italics">x</i><sub class="fm-subscript">2</sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>]</span>. We denote the entire data set with a matrix in which each feature instance appears as a row vector. The data matrix is<a id="marker-135"/></p><!--<p class="FM-Equation"><span class="times">$$X =
\begin{bmatrix}
x^{\left(0\right)}_{0} &amp; x^{\left(0\right)}_{1} &amp;
x^{\left(0\right)}_{2}\\
x^{\left(1\right)}_{0} &amp; x^{\left(1\right)}_{1} &amp;
x^{\left(1\right)}_{2}\\
x^{\left(2\right)}_{0} &amp; x^{\left(2\right)}_{1} &amp;
x^{\left(2\right)}_{2}\\
x^{\left(3\right)}_{0} &amp; x^{\left(3\right)}_{1} &amp;
x^{\left(3\right)}_{2}\\
x^{\left(4\right)}_{0} &amp; x^{\left(4\right)}_{1} &amp;
x^{\left(4\right)}_{2}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="169" src="../../OEBPS/Images/eq_04-12-h.png" width="176"/></p>
</div>
<p class="body">We will assume that the data is already mean-subtracted. Now examine the matrix product <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i>, using ordinary rules of matrix multiplication:</p><!--<p class="FM-Equation"><span class="times">$$X^{T}X =
\begin{bmatrix}
\sum_{i=0}^{4}\left(x^{\left(i\right)}_{0}\right)^{2} &amp;
\sum_{i=0}^{4}x^{\left(i\right)}_{0}x^{\left(i\right)}_{1} &amp;
\sum_{i=0}^{4} x^{\left(i\right)}_{0}x^{\left(i\right)}_{2}\\[3pt]
\sum_{i=0}^{4}x^{\left(i\right)}_{1}x^{\left(i\right)}_{0} &amp;
\sum_{i=0}^{4}\left(x^{\left(i\right)}_{1}\right)^{2} &amp;
\sum_{i=0}^{4} x^{\left(i\right)}_{1}x^{\left(i\right)}_{2}\\[3pt]
\sum_{i=0}^{4}x^{\left(i\right)}_{2}x^{\left(i\right)}_{0} &amp;
\sum_{i=0}^{4}x^{\left(i\right)}_{2}x^{\left(i\right)}_{1} &amp;
\sum_{i=0}^{4}\left(x^{\left(i\right)}_{2}\right)^{2}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="118" src="../../OEBPS/Images/eq_04-12-i.png" width="392"/></p>
</div>
<p class="body">From equations <a class="url" href="#eq-varij">4.10</a> and <a class="url" href="#eq-covar-mat">4.9</a>,</p><!--<p class="FM-Equation"><span class="times">$$X^{T}X =
\begin{bmatrix}
\sigma_{00} &amp; \sigma_{01} &amp; \sigma_{02}\\
\sigma_{10} &amp; \sigma_{11} &amp; \sigma_{12}\\
\sigma_{20} &amp; \sigma_{21} &amp; \sigma_{22}\\
\end{bmatrix} = C$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="102" src="../../OEBPS/Images/eq_04-12-j.png" width="224"/></p>
</div>
<p class="body">Thus <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> is the covariance matrix of the dataset <i class="timesitalic">X</i>. This holds for arbitrary dimensions and arbitrary feature instance counts.</p>
<p class="body">If we create a data matrix <i class="timesitalic">X</i> with each data instance forming a row, <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> ields the covariance matrix of the dataset. The eigenvalues and eigenvectors of this matrix are the principal components. Hence, performing SVD on <i class="timesitalic">X</i> yields PCA of the data (assuming prior mean subtraction).</p>
<h3 class="fm-head1" id="subsec-svd-app-linsyssolve">4.5.4 Applying SVD: Solving arbitrary linear systems</h3>
<p class="body">A linear system is a system of simultaneous linear equations</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">We first encountered a linear system in section <a class="url" href="02.xhtml#sec-lin_systems">2.12</a>. It is possible to use matrix inversion to solve such a system:</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">A</i><sup class="fm-superscript">-1</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">However, solving a linear system with matrix inversion is undesirable for many reasons. To begin with, it is numerically unstable. The matrix inverse contains the determinant of the matrix in its denominator. If the determinant is near zero, the inverse will contain very large numbers. Minor noise in <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> will be multiplied by these large numbers and cause large errors in the computed solution <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. In this case, the inverse-based solution can be very inaccurate. Furthermore, the determinant can be zero: this can happen when one row of the matrix is a linear combination of others, indicating that we have fewer equations than we think. And what if the matrix is not square to begin with? This can happen when we have more equations than unknowns overdetermined system) or fewer equations than unknowns underdetermined system). In these cases, the inverse is not computable, and the system cannot be solved fully.</p>
<p class="body">Even in these cases, we would like to obtain a solution that is the best approximation in some sense; and in the case of a square matrix, we would like to get the exact solution. How do we do this? Answer: we use SVD. The steps are as follows:<a id="marker-136"/></p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> can be rewritten as <span class="math"><i class="fm-italics">U</i>(Σ<i class="fm-italics">V<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>. We then solve <span class="math"><i class="fm-italics">U</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>. This can be easily done using orthogonality of <i class="timesitalic">U</i>, as <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <i class="fm-italics">U<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Now we have <span class="math">Σ(<i class="fm-italics">V<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span> Solve <span class="math">Σ<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span>. This can be easily done because for any diagonal matrix <!--<span class="times">$\Sigma =
{\arraycolsep=5pt\begin{bmatrix} d_{1}  &amp; 0      &amp; \cdots  &amp; 0\\[-2pt] 0      &amp; d_{2}  &amp; \cdots  &amp; 0\\[-2pt]
\vdots &amp; \vdots &amp; \vdots  &amp; \vdots\\[-2pt] 0      &amp; \cdots &amp; \cdots  &amp; d_{n}
\end{bmatrix}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="90" src="../../OEBPS/Images/eq_04-12-k.png" width="172"/></span> we can easily compute <!--<span class="times">$\Sigma^{-1} =
{\arraycolsep=5pt\begin{bmatrix}
\frac{1}{d_{1}} &amp; 0                &amp; \cdots  &amp; 0\\ 0               &amp; \frac{1}{d_{2}}   &amp; \cdots  &amp; 0\\
\vdots &amp; \vdots &amp; \vdots  &amp; \vdots\\ 0      &amp; \cdots &amp; \cdots  &amp; \frac{1}{d_{n}}
\end{bmatrix}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="113" src="../../OEBPS/Images/eq_04-12-l.png" width="191"/></span></p>
<p class="list">Hence, <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = Σ<sup class="fm-superscript">−1</sup><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Now we have <span class="math"><i class="fm-italics">V<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span>. This too can be solved easily using the orthogonality of <i class="timesitalic">V</i>:</p>
<p class="body"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">V</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span></p>
</li>
</ol>
<p class="body">Thus we have solved for <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> without inverting the matrix <i class="timesitalic">A</i>:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">For invertible square matrices <i class="timesitalic">A</i>, this method yields the same solution as the matrix-inverse-based method.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For nonsquare matrices, this boils down to the Moore-Penrose inverse and yields the best-effort solution.</p>
</li>
</ul>
<h3 class="fm-head1" id="subsec-mat-rank">4.5.5 Rank of a matrix</h3>
<p class="body">In section <a class="url" href="02.xhtml#sec-lin_systems">2.12</a>, we studied linear systems of equations. Such a system can be represented in matrix-vector form:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="body">Each row of <i class="timesitalic">A</i> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> contributes one equation. If we have as many independent equations as unknowns, the system is solvable. This is the simplest case; matrix <i class="timesitalic">A</i> is square and invertible. <span class="math"><i class="fm-italics">det</i>(<i class="fm-italics">A</i>)</span> is nonzero, and <span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> exists.</p>
<p class="body">Sometimes the situation is misleading. Consider the following system:</p><!--<p class="FM-Equation"><span class="times">$$\begin{bmatrix} 1 &amp; 0 &amp; 0\\[-2pt] 0 &amp; 1 &amp; 0\\[-2pt] 1 &amp; 1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
x_{0}\\[-2pt]x_{1}\\[-2pt]x_{2}
\end{bmatrix}
=
\begin{bmatrix} 5\\[-2pt]7\\[-2pt]12
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="89" src="../../OEBPS/Images/eq_04-12-m.png" width="166"/></p>
</div>
<p class="body">Although there are three rows and apparently three equations, the equations are not independent. For instance, the third equation can be obtained by adding the first two. We really have only two equations, not three. We say this linear system is <i class="fm-italics">degenerate</i>. All of the following statements are true for such a system <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>:<a id="marker-137"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The linear system is degenerate.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">det</i>(<i class="fm-italics">A</i>) = 0</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">A</i><sup class="fm-superscript">−1</sup></span> cannot be computed, and <i class="timesitalic">A</i> is not invertible.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Rows of <i class="timesitalic">A</i> are linearly dependent. There exists a linear combination of the rows that sum to zero. For example, in the previous example, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_r.png" width="14"/></span><sub class="fm-subscript">0</sub> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_r.png" width="14"/></span><sub class="fm-subscript">1</sub> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_r.png" width="14"/></span><sub class="fm-subscript">2</sub> = 0</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">At least one of the singular values of <i class="timesitalic">A</i> (eigenvalues of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i>) is zero. The number of linearly independent rows is equal to the number of nonzero eigenvalues.</p>
</li>
</ul>
<p class="body">The number of linearly independent rows in a matrix is called its <i class="fm-italics">rank</i>. It can be proved that a matrix has as many nonzero singular values as its rank. It can also be proved that the number of linearly independent columns in a matrix matches the number of linearly independent rows. Hence, rank can also be defined as the number of linearly independent columns in a matrix.</p>
<p class="body">A nonsquare rectangular matrix with <i class="timesitalic">m</i> rows and <i class="timesitalic">n</i> columns has a rank <span class="math"><i class="fm-italics">r</i> = <i class="fm-italics">min</i>(<i class="fm-italics">m</i>, <i class="fm-italics">n</i>)</span>. Such matrices are never invertible. We usually resort to SVD to solve them.</p>
<p class="body">A square matrix with <i class="timesitalic">n</i> rows and <i class="timesitalic">n</i> columns is invertible nonzero determinant) if and only if it has rank <i class="timesitalic">n</i>. Such a matrix is said to have <i class="fm-italics">full rank</i>. Full-rank matrices are invertible. They can be solved via matrix inverse computation, but inverse computation is not always numerically stable. SVD can be applied here as well, with good numerical properties.</p>
<p class="body">Non-full-rank matrices are degenerate. So, rank is a measure of the non-degeneracy of the matrix.</p>
<h3 class="fm-head1" id="pytorch-code-for-solving-linear-systems-with-svd">4.5.6 PyTorch code for solving linear systems with SVD</h3>
<p class="body">The listings in this section show a PyTorch-based implementation of SVD and demonstrate an application that solves a linear system via SVD.</p>
<p class="fm-code-listing-caption" id="listing-4.5-solving-an-invertible-linear-system-with-matrix-inversion-and-svd">Listing 4.5 Solving an invertible linear system with matrix inversion and SVD</p>
<pre class="programlisting">A = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]]).float()
b = torch.tensor([8, 15, 16]).float()      <span class="fm-combinumeral">①</span>

x_0 = torch.matmul(torch.linalg.inv(A), b) <span class="fm-combinumeral">②</span>

U, S, V_t = torch.linalg.svd(A)            <span class="fm-combinumeral">③</span>

y1 = torch.matmul(U.T, b)                  <span class="fm-combinumeral">④</span>

S_inv = torch.diag(1 / S)

y2 = torch.matmul(S_inv, y1)               <span class="fm-combinumeral">⑤</span>

x_1 = torch.matmul(V_t.T, y2)              <span class="fm-combinumeral">⑥</span>

assert torch.allclose(x_0, x_1)            <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Simple test linear system of equations</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Matrix inversion is numerically unstable; SVD is better.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">USV<sup class="fm-superscript">T</sup></i> <span class="cambria">⟹</span> <i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> <span class="cambria">≜</span> <i class="fm-italics">USV<sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Solves <span class="math"><i class="fm-italics">U</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>. Remember <span class="math"><i class="fm-italics">U</i><sup class="fm-superscript">−1</sup> = <i class="fm-italics">U<sup class="fm-superscript">T</sup></i></span> as U is orthogonal.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Solves <span class="math"><i class="fm-italics">S</i><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub></span>.  Remember <span class="math"><i class="fm-italics">S</i><sup class="fm-superscript">−1</sup></span> is easy as <i class="timesitalic">S</i> is diagonal.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Solves <span class="math"><i class="timesitalic">V<sup class="fm-superscript">T</sup></i> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">2</sub></span>. Remember <i class="timesitalic">V<sup class="fm-superscript">−T</sup></i> = <i class="timesitalic">V</i> as <i class="timesitalic">V</i> is orthogonal.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> The two solutions are the same.</p>
<p class="body">Here is the output:<a id="marker-138"/></p>
<pre class="programlisting">Solution via inverse: [1.0, 2.0, 3.0] 
Solution via SVD: [1.0, 2.0, 3.0]</pre>
<p class="fm-code-listing-caption" id="listing-4.6-solving-an-overdetermined-linear-system-by-pseudo-inverse-and-svd">Listing 4.6 Solving an overdetermined linear system by pseudo-inverse and SVD</p>
<pre class="programlisting">A = torch.tensor([[0.11, 0.09], [0.01, 0.02],
              [0.98, 0.91], [0.12, 0.21],
              [0.98, 0.99], [0.85, 0.87],
              [0.03, 0.14], [0.55, 0.45],
              [0.49, 0.51], [0.99, 0.01],
              [0.02, 0.89], [0.31, 0.47],
              [0.55, 0.29], [0.87, 0.76],
              [0.63, 0.24]])                         <span class="fm-combinumeral">①</span>
A = torch.column_stack((A, torch.ones(15)))
b = torch.tensor([-0.8, -0.97, 0.89, -0.67,
              0.97, 0.72, -0.83, 0.00,
              0.00, 0.00, -0.09, -0.22,
              -0.16, 0.63, 0.37])

x_0 = torch.matmul(torch.linalg.pinv(A), b)          <span class="fm-combinumeral">②</span>

U, S, V_t = torch.linalg.svd(A, full_matrices=False) <span class="fm-combinumeral">③</span>

y1 = torch.matmul(U.T, b)
S_inv = torch.diag(1 / S)
y2 = torch.matmul(S_inv, y1)
x_1 = torch.matmul(V_t.T, y2)


assert torch.allclose(x_0, x_1)                      <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Cat-brain dataset: nonsquare matrix</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Solution via pseudo-inverse</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Solution via SVD</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The two solutions are the same.</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">Solution via pseudo-inverse: [ 1.0766, 0.8976, -0.9582] 
Solution via SVD: [ 1.0766, 0.8976, -0.9582]</pre>
<p class="body">Fully functional code for solving the SVD-based linear system can be found at <a class="url" href="http://mng.bz/OERn">http://mng.bz/OERn</a>.</p>
<h3 class="fm-head1" id="pytorch-code-for-pca-computation-via-svd">4.5.7 PyTorch code for PCA computation via SVD</h3>
<p class="body">The following listing demonstrates PCA computations using SVD.<a id="marker-139"/></p>
<p class="fm-code-listing-caption" id="listing-4.7-computing-pca-directly-and-using-svd">Listing 4.7 Computing PCA directly and using SVD</p>
<pre class="programlisting">  <span class="fm-combinumeral">①</span>                <span class="fm-combinumeral">②</span>               <span class="fm-combinumeral">③</span>
principal_values, principal_vectors = pca(X) <span class="fm-combinumeral">④</span>

X_mean = X - torch.mean(X, axis=0)

  <span class="fm-combinumeral">⑤</span>
U, S, V_t = torch.linalg.svd(X_mean)         <span class="fm-combinumeral">⑥</span>

V = V_t.T                                    <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Eigenvalues of the covariance matrix yield principal values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Eigenvectors of the covariance matrix yield principal vectors.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Direct PCA computation from a covariance matrix</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Data matrix</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Diagonal elements of matrix <i class="timesitalic">S</i> yield principal values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> PCA from SVD</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Columns of matrix <i class="timesitalic">V</i> yield principal vectors.</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">Principal components obtained via PCA: 
[[-0.44588404 -0.89509073]
 [-0.89509073 0.44588404]] 
Principal components obtained via SVD:
[[-0.44588404 0.89509073] 
 [-0.89509073 -0.44588404]]</pre>
<h3 class="fm-head1" id="subsec-svd-app-bestrankkapprox">4.5.8 Applying SVD: Best low-rank approximation of a matrix</h3>
<p class="body">Given a matrix <i class="timesitalic">A</i> of some rank <i class="timesitalic">p</i>, we sometimes want to approximate it with a matrix of lower rank <i class="timesitalic">r</i>, where <span class="math"><i class="fm-italics">r</i> &lt; <i class="fm-italics">p</i></span>. How do we obtain the best rank <i class="timesitalic">r</i> approximation of <i class="timesitalic">A</i>?</p>
<p class="fm-head2" id="motivation">Motivation</p>
<p class="body">Why would we want to do this? Well, consider a data matrix <i class="timesitalic">X</i> as shown in section <a class="url" href="#subsec-svd-app-pca-computation">4.5.3</a>. As explained in section <a class="url" href="#subsec-pca-app-dimred">4.4.2</a>, we often want to eliminate small variances in the data (likely due to noise) and get the pattern underlying large variations. Replacing the data matrix with a lower-rank matrix often achieves this. However, we must bear in mind that this does not work when the underlying pattern is nonlinear (such as in figure <a class="url" href="#fig-non-linear-pca">4.5a</a>).</p>
<p class="fm-head2" id="approximation-error">Approximation error</p>
<p class="body">What do we mean by <i class="fm-italics">best approximation</i>? The Frobenius norm can be taken as the magnitude of the matrix. Accordingly, given a matrix <i class="timesitalic">A</i> and its rank <i class="timesitalic">r</i> approximation <i class="timesitalic">A<sub class="fm-subscript">r</sub></i>, the approximation error is <span class="math"><i class="fm-italics">e</i> = ||<i class="fm-italics">A</i> − <i class="fm-italics">A<sub class="fm-subscript">r</sub></i>||<i class="fm-italics"><sub class="fm-subscript">F</sub></i></span>.</p>
<p class="fm-head2" id="method">Method</p>
<p class="body"><a id="marker-140"/>To solidify our ideas, let’s consider an <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i>. From section <a class="url" href="../Text/04.xhtml#sec-svd">4.5</a>, we know it will have <span class="math"><i class="fm-italics">min</i>(<i class="fm-italics">m</i>, <i class="fm-italics">n</i>)</span> singular values. Let its rank be <span class="math"><i class="fm-italics">p</i> ≤ <i class="fm-italics">min</i>(<i class="fm-italics">m</i>, <i class="fm-italics">n</i>)</span>. We want to approximate this matrix with a rank <span class="math"><i class="fm-italics">r</i>(&lt;<i class="fm-italics">p</i>)</span> matrix.</p>
<p class="body">Let’s rewrite the SVD expression. We will assume <span class="math"><i class="fm-italics">m</i> &gt; <i class="fm-italics">n</i></span>. Also, as usual, we have the singular values sorted in decreasing order: <span class="math"><i class="fm-italics">λ</i><sub class="fm-subscript">1</sub> ≥ <i class="fm-italics">λ</i><sub class="fm-subscript">2</sub> ≥ <i class="fm-italics">λ<sub class="fm-subscript">n</sub></i></span>. We will partition <span class="math"><i class="fm-italics">U</i>, Σ, <i class="fm-italics">V</i></span>:</p><!--<p class="FM-Equation"><span class="times">$$\begin{aligned} A
&amp;= U \Sigma V^{T}\\
&amp;
\includegraphics{imgs/CH04_UN01_Chaudhury}\\
&amp;=
\begin{bmatrix} U_{1} &amp; U_{2}
\end{bmatrix}
\begin{bmatrix}
\Sigma_{1} &amp; 0\\ 0          &amp; \Sigma_{2}
\end{bmatrix}
\begin{bmatrix} V_{1}^{T}\\ V_{2}^{T}
\end{bmatrix}\\
&amp;= U_{1} \Sigma_{1} V_{1}^{T} + U_{2} \Sigma_{2} V_{2}^{T}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="376" src="../../OEBPS/Images/eq_04-12-n.png" width="469"/></p>
</div>
<p class="body">It can be proved that <span class="math"><i class="fm-italics">U</i><sub class="fm-subscript">1</sub>Σ<sub class="fm-subscript">1</sub><i class="fm-italics">V</i><sub class="fm-subscript">1</sub><i class="fm-italics"><sup class="fm-superscript">T</sup></i></span> is a rank <i class="timesitalic">r</i> matrix. Furthermore, it is the best rank <i class="timesitalic">r</i> approximation of <i class="timesitalic">A</i>.</p>
<h2 class="fm-head" id="sec-lsa">4.6 Machine learning application: Document retrieval</h2>
<p class="body">We will now bring together several of the concepts we have discussed in this chapter with an illustrative toy example: the document retrieval problem we first encountered in section <a class="url" href="02.xhtml#sec-vectors">2.1</a>. Briefly recapping, we have a set of documents <span class="math">{<i class="fm-italics">d</i><sub class="fm-subscript">0</sub>,⋯, <i class="fm-italics">d</i><sub class="fm-subscript">6</sub>}</span>. Given an incoming query phrase, we have to retrieve documents that match the query phrase. We will use the <i class="fm-italics">bag of words</i> model: that is, our matching approach does not pay attention to <i class="fm-italics">where</i> a word appears in a document; it simply pays attention to <i class="fm-italics">how many times</i> the word appears in the document. Although this technique is not the most sophisticated, it is popular because of its conceptual simplicity.</p>
<p class="body">Our documents are as follows:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">0</sub></span></i>: Roses are lovely. Nobody hates roses.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">1</sub></span></i>: <i class="fm-italics">Gun</i> <i class="fm-italics">violence</i> has reached epidemic proportions in America.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">2</sub></span></i>: The issue of <i class="fm-italics">gun</i> <i class="fm-italics">violence</i> is really over-hyped. One can find many instances of <i class="fm-italics">violence</i> where no <i class="fm-italics">guns</i> were involved.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">3</sub></span></i>: <i class="fm-italics">Guns</i> are for <i class="fm-italics">violence</i> prone people. <i class="fm-italics">Violence</i> begets <i class="fm-italics">guns</i>. <i class="fm-italics">Guns</i> beget <i class="fm-italics">violence</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">4</sub></span></i>: I like <i class="fm-italics">guns</i> but I hate <i class="fm-italics">violence</i>. I have never been involved in <i class="fm-italics">violence</i>. But I own many <i class="fm-italics">guns</i>. <i class="fm-italics">Gun violence</i> is incomprehensible to me. I do believe <i class="fm-italics">gun</i> owners are the most anti <i class="fm-italics">violence</i> people on the planet. He who never uses a <i class="fm-italics">gun</i> will be prone to senseless <i class="fm-italics">violence</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">5</sub></span></i>: <i class="fm-italics">Guns</i> were used in an armed robbery in San Francisco last night.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics"><span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span></i>: Acts of <i class="fm-italics">violence</i> usually involve a weapon.</p>
</li>
</ul>
<h3 class="fm-head1" id="sec-tfidf-cosinesimilarity">4.6.1 Using TF-IDF and cosine similarity</h3>
<p class="body"><a id="marker-141"/>Before discussing PCA, let’s look at some more elementary techniques for document retrieval. These are based on term frequency-inverse document frequency (TF-IDF) and cosine similarity.</p>
<p class="fm-head2" id="term-frequency">Term frequency</p>
<p class="body"><i class="fm-italics">Term frequency</i> (TF) is defined as the number of occurrences of a particular term in a document. (In this context, note that in this book, we use <i class="fm-italics">term</i> and <i class="fm-italics">word</i> somewhat interchangeably.) In a slightly looser definition, any quantity proportional to the number of occurrences of the term is also known as term frequency. For example, the TF of the word <i class="fm-italics">gun</i> in <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span> is 0, in <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">1</sub></span> is <span class="math">1</span>, in <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">3</sub></span> is <span class="math">3</span>, and so on. Note that we are being case independent. Also, singular/plural (<i class="fm-italics">gun</i> and <i class="fm-italics">guns</i>) and various flavors of the words originating from the same stem (such as <i class="fm-italics">violence</i> and <i class="fm-italics">violent</i>) are typically mapped to the same term.</p>
<p class="fm-head2" id="inverse-document-frequency">Inverse document frequency</p>
<p class="body">Certain terms, such as <i class="fm-italics">the</i>, appear in pretty much all documents. These should be ignored during document retrieval. How do we down-weight them?</p>
<p class="body">The IDF is obtained by inverting and then taking the absolute value of the logarithm of the fraction of all documents in which the term occurs. For terms that occur in most documents, the IDF weight is very low. It is high for relatively esoteric terms.</p>
<p class="fm-head2" id="document-feature-vectors">Document feature vectors</p>
<p class="body">Each document is represented by a document feature vector. It has as many elements as the size of the vocabulary (that is, the number of distinct words over all the documents). Every word has a fixed index position in the vector. Given a specific document, the value at the index position corresponding to a specific word contains the TF of the corresponding word multiplied by that word’s IDF. Thus, every document is a point in a space that has as many dimensions as the vocabulary size. The coordinate value along a specific dimension is proportional to the number of times the word is repeated in the document, with a weigh-down factor for common words.</p>
<p class="body">For real-life document retrieval systems like Google, this vector is extremely long. But not to worry: this vector is notional—it is never explicitly stored in the computer’s memory. We store a sparse version of the document feature vector: a list of unique words along with their TF<span class="math">×</span>IDF scores.</p>
<p class="fm-head2" id="cosine-similarity">Cosine similarity</p>
<p class="body">In section <a class="url" href="02.xhtml#subsubsec-dotproduct_as_agreement">2.5.6.2</a>, we saw that the dot product between two vectors measures the agreement between them. Given two vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, we know <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span> ⋅ <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>|| ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||<i class="fm-italics">cos</i>(<i class="fm-italics">θ</i>)</span>, where the operator <span class="math">|| ⋅ ||</span> implies the length of a vector and <i class="timesitalic">θ</i> is the angle between the two vectors (see figure <a class="url" href="02.xhtml#ch2fig-vec_component">2.7b</a>). The cosine is at its maximum possible value, <span class="math">1</span>, when the vectors are pointing in the same direction and the angle between them is zero. It becomes progressively smaller as the angle between the vectors increases until the two vectors are perpendicular to each other and the cosine is zero, implying no correlation: the vectors are independent of each other.</p>
<p class="body">The magnitude of the dot product is also proportional to the length of the two vectors. We do not want to use the full dot product as a measure of similarity between the vectors because two long vectors would have a high similarity score even if they were not aligned in direction. Rather, we want to use the cosine, defined as</p><!--<p class="Body"><span class="times">$$cosine\_similarity\left(\vec{a},
\vec{b}\right) =\frac{\vec{a}^{T}\vec{b}}{\|\vec{a}\|\|\vec{b}\|}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_04-13.png" width="266"/></p>
</div>
<p class="fm-equation-caption">Equation 4.13</p>
<p class="body">The cosine similarity between document vectors is a principled way of measuring the degree of term sharing between the documents. It is higher if many repeated words are shared between the two documents.</p>
<h3 class="fm-head1" id="latent-semantic-analysis">4.6.2 Latent semantic analysis</h3>
<p class="body"><a id="marker-142"/>Cosine similarity and similar techniques suffer from a significant drawback. To see this, examine the cosine similarity between <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">5</sub></span> and <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span>. It is zero. But it is obvious to a human that the documents are similar.</p>
<p class="body">What went wrong? Answer: we are measuring only the direct overlap between terms in documents. The words <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i> occur together in many of the other documents, indicating some degree of similarity between them. Hence, documents containing only <i class="fm-italics">gun</i> have some similarity with documents containing only <i class="fm-italics">violence</i>—but cosine similarity between document vectors does not look at such secondary evidence. This is the blind spot that LSA tries to overcome.</p>
<p class="body"><i class="fm-italics">Words are known by the company they keep</i>. That is, if terms appear together in many documents (like <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i> in the previous examples), they are likely to share some semantic similarity. Such terms should be grouped together into a common pool of semantically similar terms. Such a pool is called a <i class="fm-italics">topic</i>. Document similarity should be measured in terms of common topics rather than explicit common terms. We are particularly interested in topics that discriminate the documents in our corpus: that is, there should be a high variation in the degree to which different documents subscribe to the topic.</p>
<p class="body">Geometrically, a topic is a subspace in the document feature space. In classical latent semantic analysis, we only look at linear subspaces, and a topic can be visualized as a direction or linear combination of directions (hyperplane) in the document feature space. In particular, any direction line in the space is a topic: it is a subspace representing a weighted combination of the coordinate axis directions, which means it is a weighted combination of vocabulary terms. We are, of course, interested in topics with high variance. These correspond to a direction along which the document vectors are well spread, which means the document vectors are well discriminated over this topic. We typically prune the set of topics, eliminating those with insufficient variance.</p>
<p class="body">From this discussion, a mathematical definition of <i class="fm-italics">topic</i> begins to emerge. Topics are principal components of the matrix of document vectors with individual document descriptor vectors along its rows. Measuring document similarity in terms of topic has the advantage that two documents may not have many exact words in common but may still have a common topic. This happens when they share words belonging to the same topic. Essentially, they share a lot of words that occur together in other documents. So even if the number of common words is low, we can have high document similarity.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="765" id="fig-lsa" src="../../OEBPS/Images/CH04_F06_Chaudhury.png" width="734"/></p>
<p class="figurecaption">Figure 4.6 Document vectors from our toy dataset <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">0</sub>, ⋯ <i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span>. Each word in the vocabulary corresponds to a separate dimension. Dots show projections of document feature vectors on the plane formed by the axes corresponding to the terms <i class="fm-italics">gun(s)</i> and <i class="fm-italics">violence</i>.</p>
</div>
<p class="body"><a id="marker-143"/>For instance, in our toy document corpus, <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i> are very correlated (both or neither is likely to occur in a document). <i class="fm-italics">Gun-violence</i> emerges as a topic. If we express the document vector in terms of this topic instead of the individual words, we see similarities that otherwise would have escaped us. That is, we see <i class="fm-italics">latent semantic</i> similarities. For instance, the cosine similarity between <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">5</sub></span> and <span class="math"><i class="fm-italics">d</i><sub class="fm-subscript">6</sub></span> is nonzero. This is the core idea of latent semantic analysis and is illustrated in figure <a class="url" href="#fig-lsa">4.6</a>.</p>
<p class="body">Let’s revisit our example document-retrieval problem in light of topic extraction. The document matrix (with document vectors as rows) looks like table <a class="url" href="#tab-doc_vector_dataset">4.1</a>. Rows correspond to documents, and columns correspond to terms. Each cell contains the term frequency. The terms <i class="fm-italics">gun</i> and <i class="fm-italics">violence</i> occur an equal number of times in most documents, indicating clear correlation. Hence <i class="fm-italics">gun-violence</i> is a topic. The principal components right eigenvectors) identify topics. As usual, we have omitted prepositions, conjunctions, commas, and so on. The overall steps are as follows (see listing <a class="url" href="#list-svd-lsa-toy">4.8</a> for the Python code):<a id="marker-144"/></p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Create a document term <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span>. Its rows correspond to documents (<i class="timesitalic">m</i> documents), and its columns correspond to terms (<i class="timesitalic">n</i> terms).</p>
</li>
<li class="fm-list-bullet">
<p class="list">Perform SVD on the matrix. This yields <i class="timesitalic">U</i>, <i class="timesitalic">S</i>, and <i class="timesitalic">V</i> matrices. <i class="timesitalic">V</i> is an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> orthogonal matrix, and <i class="timesitalic">S</i> is a diagonal matrix.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The columns of matrix <i class="timesitalic">V</i> yield topics. These are principal vectors for the rows of <i class="timesitalic">X</i>: that is, eigenvectors of <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> or, equivalently, the covariance matrix of <i class="timesitalic">X</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The successive elements of each topic vector (column in matrix <i class="timesitalic">V</i>) tell us the contribution of corresponding terms to that topic. Each column is <span class="math"><i class="fm-italics">n</i> × 1</span>, depicting the contributions of the <i class="timesitalic">n</i> terms in the system.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The diagonal elements of <i class="timesitalic">S</i> tell us the weights (importance) of corresponding topics. These are the eigenvalues of <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i>: that is, principal values of the row vectors of <i class="timesitalic">X</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Inspect the weights, and choose a cutoff. All topics below that weight are discarded—the corresponding columns of <i class="timesitalic">V</i> are thrown away. This yields a matrix <i class="timesitalic">V</i> with fewer columns but the same number of rows); these are the topic vectors of interest to us. We have reduced the dimensionality of the problem. If the number of retained topics is <i class="timesitalic">t</i>, the reduced <i class="timesitalic">V</i> is <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">t</i></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">By projecting (multiplying) the original matrix <i class="timesitalic">X</i> of document terms to this new matrix <i class="timesitalic">V</i>, we get an <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">t</i></span> matrix of document topics (it has same number of rows as <i class="timesitalic">X</i> but fewer columns). This is the projection of <i class="timesitalic">X</i> to the topic space: that is, a topic-based representation of the document vectors.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Rows of the document topic matrix will henceforth be taken as document representations. Document similarities will be computed by taking the cosine similarity of these rows rather than the rows of the original document term matrix. This cosine similarity, in the topic space, will capture many indirect connections that were not visible in the original input space.</p>
</li>
</ol>
<p class="fm-table-caption">Table 4.1 Document matrix for the toy example dataset</p>
<table border="1" class="contenttable-1-table" id="tab-doc_vector_dataset" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="16.66%"/>
<col class="contenttable-0-col" span="1" width="16.66%"/>
<col class="contenttable-0-col" span="1" width="16.66%"/>
<col class="contenttable-0-col" span="1" width="16.66%"/>
<col class="contenttable-0-col" span="1" width="16.66%"/>
<col class="contenttable-0-col" span="1" width="16.66%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">  </p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Violence</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Gun</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">America</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head"><span class="math">⋯</span></p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Roses</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">0</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">1</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">2</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">3</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">3</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">3</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">4</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">5</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">5</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">5</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math"><b class="fm-bold">d</b><sub class="subscript-bold">6</sub></span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">⋯</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">0</p>
</td>
</tr>
</tbody>
</table>
<h3 class="fm-head1" id="pytorch-code-to-perform-lsa">4.6.3 PyTorch code to perform LSA</h3>
<p class="body"><a id="marker-145"/>The following listing demonstrates how to compute the LSA for our toy dataset from table <a class="url" href="#tab-doc_vector_dataset">4.1</a>. Fully functional code for this section can be found at <a class="url" href="http://mng.bz/E2Gd">http://mng.bz/E2Gd</a>.</p>
<p class="fm-code-listing-caption" id="listing-4.8-computing-lsa">Listing 4.8 Computing LSA</p>
<pre class="programlisting" id="list-svd-lsa-toy">terms = ["violence", "gun", "america", "roses"]         <span class="fm-combinumeral">①</span>
X = torch.tensor([[0, 0, 0, 2],
              [1, 1, 1, 0],
              [2, 2, 0, 0],
              [3, 3, 0, 0],
              [5, 5, 0, 0],
              [0, 1, 0, 0],
              [1, 0, 0, 0]]).float()                    <span class="fm-combinumeral">②</span>

U, S, V_t = torch.linalg.svd(X)                         <span class="fm-combinumeral">③</span>

V = V_t.T

rank = 1
U = U[:, :rank]
V = V[:, :rank]                                         <span class="fm-combinumeral">④</span>

topic0_term_weights = list(zip(terms, V[:, 0]))         <span class="fm-combinumeral">⑤</span>

def cosine_similarity(vec_1, vec_2):
    vec_1_norm = torch.linalg.norm(vec_1)
    vec_2_norm = torch.linalg.norm(vec_2)
    return torch.dot(vec_1, vec_2) / (vec_1_norm * vec_2_norm)

d5_d6_cosine_similarity = cosine_similarity(X[5], X[6]) <span class="fm-combinumeral">⑥</span>

doc_topic_projection = torch.dot(X, V)
d5_d6_lsa_similarity = cosine_similarity(doc_topic_projection[5],
                                         doc_topic_projection[6])</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Considers only four terms for simplicity</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Document term matrix. Each row describes a document. Each column contains TF scores for one term. IDF is ignored for simplicity.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Performs SVD on the doc-term matrix. Columns of the resulting matrix <i class="timesitalic">V</i> correspond to topics. These are eigenvectors of <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i>: principal vectors of the doc-term matrix. A topic corresponds to the direction of maximum variance in the doc feature space.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> <i class="timesitalic">S</i> indicates the diagonal matrix of principal values. These signify topic weights (importance). We choose a cut-off and discard all topics below that weight (dimensionality reduction). Only the first few columns of <i class="timesitalic">V</i> are retained. Principal values (topic weights) for this dataset are shown in the output. Only one topic is retained in this example.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Elements of the topic vector show the contributions of corresponding terms to the topic.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Cosine similarity in the feature space fails to capture d, d6 similarity. LSA succeeds.</p>
<p class="body">The output is as follows:</p>
<pre class="programlisting">Principal Values from S matrix: 8.89, 2.00, 1.00, 0.99
(Topic 0 has disproportionately high weight. We discard others)

topic0_term_weights (Topic zero is about "gun" and "violence"):
[
 ('violence', -0.706990662151775)
 ('gun', -0.7069906621517749)
 ('america', -0.018122010384881156)
 ('roses', 2.9413274625621952e-18)
]
Document 5, document 6 Cosine similarity in original space: 0.0
Document 5, document 6 Cosine similarity in topic space: 1.0</pre>
<h3 class="fm-head1" id="pytorch-code-to-compute-lsa-and-svd-on-a-large-dataset">4.6.4 PyTorch code to compute LSA and SVD on a large dataset</h3>
<p class="body"><a id="marker-146"/>Suppose we have a set of <span class="math">500</span> documents over a vocabulary of <span class="math">3</span> terms. This is an unrealistically short vocabulary, but it allows us to easily visualize the space of document vectors. Each document vector is a <span class="math">3 × 1</span> vector, and there are <span class="math">500</span> such vectors. Together they form a <span class="math">500 × 3</span> data matrix <i class="timesitalic">X</i>. In this dataset, the terms <span class="math"><i class="fm-italics">x</i>0</span> and <span class="math"><i class="fm-italics">x</i>1</span> are correlated: <span class="math"><i class="fm-italics">x</i>0</span> occurs randomly between <span class="math">0</span> and <span class="math">100</span> times in a document, and <span class="math"><i class="fm-italics">x</i>1</span> occurs twice as many times as <span class="math"><i class="fm-italics">x</i>0</span> except for small random fluctuations. The third term’s frequency varies between <span class="math">0</span> and <span class="math">5</span>. From section <a class="url" href="../Text/04.xhtml#sec-lsa">4.6</a>, we know that <span class="math"><i class="fm-italics">x</i>0</span>, <span class="math"><i class="fm-italics">x</i>1</span> together form a single topic, while <span class="math"><i class="fm-italics">x</i>2</span> by itself forms another topic. We expect a principal component along each topic.</p>
<p class="body">Listing 4.9 creates the dataset, computes the SVD, plots the dataset, and shows the first two principal components. The third singular value is small compared to the first. We can ignore that dimension—it corresponds to the small random variation within the <span class="math"><i class="fm-italics">x</i>0 − <i class="fm-italics">x</i>1</span> topic. The singular values are printed out and also shown graphically along with the data points in figure <a class="url" href="#fig-svd-lsa">4.7</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="494" id="fig-svd-lsa" src="../../OEBPS/Images/CH04_F07_Chaudhury.png" width="683"/></p>
<p class="figurecaption">Figure 4.7 Latent semantic analysis. Note that the vertical axis line is actually much smaller than it appears to be in the diagram.</p>
</div>
<p class="fm-code-listing-caption" id="listing-4.9-lsa-using-svd">Listing 4.9 LSA using SVD</p>
<pre class="programlisting">num_examples = 500
x0 = torch.normal(0, 100, (num_examples,)).round()
random_noise = torch.normal(0, 2, (num_examples,)).round()
x1 = 2*x0 + random_noise
x2 = torch.normal(0, 5, (num_examples,)).round()
X = torch.column_stack((x0, x1, x2))                       <span class="fm-combinumeral">①</span>

                <span class="fm-combinumeral">②</span>
U, S, V_t = torch.linalg.svd(X)                            <span class="fm-combinumeral">③</span>
V = V_t.T                                                  <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> 3D dataset: the first two axes are linearly correlated; the third axis has small near-zero random values.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The third singular value is relatively small; we ignore it</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The first two principal vectors represent topics. Projecting data points on them yields document descriptors in terms of the two topics.</p>
<p class="body">Here is the output:</p>
<pre class="programlisting">Singular values are: 4867.56982, 118.05858, 19.68604</pre>
<h2 class="fm-head" id="summary-3">Summary</h2>
<p class="body"><a id="marker-147"/>In this chapter, we studied several linear algebraic tools used in machine learning and data science:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The direction (unit vector) that maximizes (minimizes) the quadratic form <i class="timesitalic">x̂<sup class="fm-superscript">T</sup>Ax̂</i> is the eigenvector corresponding to the largest (smallest) eigenvalue of matrix <i class="timesitalic">A</i>. The magnitude of the quadratic form when <i class="timesitalic">x̂</i> is along those directions is the largest (smallest) eigenvalue of <i class="timesitalic">A</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Given a set of points <span class="math"><i class="fm-italics">X</i> = {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(0)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>, ⋯, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span> in an <span class="math"><i class="fm-italics">n</i> + 1</span>-dimensional space, we can define the mean vector and covariance matrix as</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{\mu} &amp;= \frac{1}{n} \sum_{i=0}^{n} \vec{x}^{\left(i\right)}\\ C &amp;= \frac{1}{n} \sum_{i=0}^{n} \left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)\left(\vec{x}^{\left(i\right)} -
\vec{\mu}\right)^{T}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="120" src="../../OEBPS/Images/eq_04-13-a.png" width="244"/></p>
</div>
<p class="body-ind">The variance along an arbitrary direction (unit vector) <i class="timesitalic">l̂</i> is <span class="math"><i class="fm-italics">l̂</i> <sup class="superscript-italic">T</sup><i class="fm-italics">Cl̂</i></span>. This is a quadratic form. Consequently, the maximum (minimum) variance of a set of data points in multidimensional space occurs along the eigenvector corresponding to the largest (smallest) eigenvalue of the covariance matrix. This direction is called the first principal axis of the data. The subsequent eigenvectors, sorted in order of decreasing eigenvalues, are mutually orthogonal (perpendicular) and yield the subsequent direction of maximum variance. This technique is known as principal component analysis (PCA).</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In many real-life cases, larger variances correspond to the true underlying pattern of the data, while smaller variances correspond to noise (such as measurement error). Projecting the data on the principal axes corresponding to the larger eigenvalues yields lower-dimensional data that is relatively noise-free. The projected data points also match the true underlying pattern more closely, yielding better insights. This is known as dimensionality reduction.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Singular value decomposition (SVD) allows us to decompose an arbitrary <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> matrix <i class="timesitalic">A</i> as a product of three matrices: <span class="math"><i class="fm-italics">A</i> = <i class="fm-italics">U</i>Σ<i class="fm-italics">V<sup class="fm-superscript">T</sup></i></span>, where <span class="math"><i class="fm-italics">U</i>, <i class="fm-italics">V</i></span> are orthogonal and <span class="math">Σ</span> is diagonal. Matrix <i class="timesitalic">V</i> has the eigenvectors of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> as its columns. <i class="timesitalic">U</i> has eigenvectors of <i class="timesitalic">AA<sup class="fm-superscript">T</sup></i> as columns. <span class="math">Σ</span> has the eigenvalues of <i class="timesitalic">A<sup class="fm-superscript">T</sup>A</i> sorted in decreasing order) in its diagonal.</p>
</li>
<li class="fm-list-bullet">
<p class="list">SVD provides a numerically stable way to solve the linear system of equations <span class="math"><i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>. In particular, for nonsquare matrices, it provides the closest approximations: that is, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> that minimizes <span class="math">||<i class="fm-italics">A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> − <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Given a dataset <i class="timesitalic">X</i> whose rows are data vectors corresponding to individual instances and columns correspond to feature values, <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> ields the covariance matrix. Thus eigenvectors of <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> ield the data’s principal components. Since the SVD of <i class="timesitalic">X</i> has eigenvectors of <i class="timesitalic">X<sup class="fm-superscript">T</sup>X</i> as columns of the matrix <i class="timesitalic">V</i>, SVD is an effective way to compute PCA.</p>
</li>
<li class="fm-list-bullet">
<p class="list">When using machine learning data science for document retrieval, the bag-of-words model represents documents with document vectors that contain the term frequency (number of occurrences) of each term in the document.</p>
</li>
<li class="fm-list-bullet">
<p class="list">TF-IDF is a cosine similarity technique for document matching and retrieval.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Latent semantic analysis (LSA) does topic modeling: we perform PCA on the document vectors to identify topics. Projecting document vectors onto topic axes allows LSA to see latent (indirect) similarities beyond the direct overlapping of terms.<a id="marker-148"/></p>
</li>
</ul>
</div></body></html>