- en: 15 Diffusion models and text-to-image Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: How forward diffusion and reverse diffusion work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build and train a denoising U-Net model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the trained U-Net to generate flower images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts behind text-to-image Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a Python program to generate an image through text with DALL-E 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recent years, multimodal large language models (LLMs) have gained significant
    attention for their ability to handle various content formats, such as text, images,
    video, audio, and code. A notable example of this is text-to-image Transformers,
    such as OpenAI’s DALL-E 2, Google’s Imagen, and Stability AI’s Stable Diffusion.
    These models are capable of generating high-quality images based on textual descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'These text-to-image models comprise three essential components: a text encoder
    that compresses text into a latent representation, a method to incorporate text
    information into the image generation process, and a diffusion mechanism to gradually
    refine an image to produce realistic output. Understanding the diffusion mechanism
    is particularly crucial for comprehending text-to-image Transformers, as diffusion
    models form the foundation of all leading text-to-image Transformers. For this
    reason, you will start by building and training a diffusion model to generate
    flower images in this chapter. This will provide you with a deep understanding
    of the forward diffusion process, where noise is incrementally added to images
    until they become random noise. Subsequently, you will train a model to reverse
    the diffusion process by gradually removing noise from images until the model
    can generate a new, clean image from random noise, resembling those in the training
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models have become the go-to choice for generating high-resolution
    images. The success of diffusion models lies in their ability to simulate and
    reverse a complex noise addition process, which mimics a deep understanding of
    how images are structured and how to construct them from abstract patterns. This
    method not only ensures high quality but also maintains a balance between diversity
    and accuracy in the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we’ll explain how a text-to-image Transformer works conceptually.
    We’ll focus on the contrastive language–image pretraining (CLIP) model developed
    by OpenAI, which is designed to comprehend and link visual and textual information.
    CLIP processes two types of inputs: images and text (typically in the form of
    captions or descriptions). These inputs are handled separately through two encoders
    in the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The image branch of CLIP employs a Vision Transformer (ViT) to encode images
    into a high-dimensional vector space, extracting visual features in the process.
    Meanwhile, the text branch uses a Transformer-based language model to encode textual
    descriptions into the same vector space, capturing semantic features from the
    text. CLIP has been trained on many pairs of matching images and text descriptions
    to closely align the representations of matching pairs in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s text-to-image Transformers, such as DALL-E 2, incorporate CLIP as a
    core component. In this chapter, you’ll learn to obtain an OpenAI API key and
    write a Python program to generate images using DALL-E 2 based on text descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1 Introduction to denoising diffusion models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of diffusion-based models can be illustrated using the following
    example. Consider the goal of generating high-resolution flower images using a
    diffusion-based model. To do that, you first acquire a set of high-quality flower
    images for training. The model is then instructed to incrementally introduce small
    amounts of random noise into these images, a process known as forward diffusion.
    After many steps of adding noise, the training images eventually become random
    noise. The next phase involves training the model to reverse this process, starting
    with pure noise images and progressively reducing the noise until the images are
    indistinguishable from those in the original training set.
  prefs: []
  type: TYPE_NORMAL
- en: Once trained, the model is given random noise images to work with. It systematically
    eliminates noise from the image over many iterations until it generates a high-resolution
    flower image that resembles those in the training set. This is the underlying
    principle of diffusion-based models.^([1](#footnote-003))
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will first explore the mathematical foundations of diffusion-based
    models. Then you will dive into the architecture of U-Nets, the type of model
    used for denoising images and producing high-resolution flower images. Specifically,
    the U-Net employs a scaled dot product attention (SDPA) mechanism, similar to
    what you have seen in Transformer models in chapters 9 to 12\. Finally, you will
    learn the training process of diffusion-based models and the image-generation
    process of the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.1 The forward diffusion process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several papers have proposed diffusion-based models with similar underlying
    mechanisms.^([2](#footnote-002)) Let’s use the flower images as a concrete example
    to explain the idea behind denoising diffusion models. Figure 15.1 is a diagram
    of how the forward diffusion process works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 A diagram of the forward diffusion process. We start with a clean
    image from the training set, *x*[0], and add noise є[0] to it to form a noisy
    image *x*[١] = √(1 – *β*[1])*x*[0] + √(*β*[١])є[0]. We repeat this process for
    1,000 time steps until the image *x*[1000] becomes random noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that flower images, *x*[0] (illustrated in the left image in figure
    15.1), follow a distribution of q(x). In the forward diffusion process, we’ll
    add small amounts of noise to the images in each of the T = 1,000 steps. The noise
    tensor is normally distributed and has the same shape as the flower images: (3,
    64, 64), meaning three color channels, with a height and width of 64 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: Time steps in diffusion models
  prefs: []
  type: TYPE_NORMAL
- en: In diffusion models, time steps refer to the discrete stages during the process
    of gradually adding noise to data and subsequently reversing this process to generate
    samples. The forward phase of a diffusion model progressively adds noise over
    a series of time steps, transforming data from its original, clean state into
    a noisy distribution. During the reverse phase, the model operates over a similar
    series of time steps but in a reverse order. It systematically removes noise from
    the data to reconstruct the original or generate new, high-fidelity samples. Each
    time step in this reverse process involves predicting the noise that was added
    in the corresponding forward step and subtracting it, thereby gradually denoising
    the data until reaching a clean state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In time step 1, we add noise є[0] to the image *x*[0], so that we obtain a
    noisy image *x*[1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ01.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (15.1) |'
  prefs: []
  type: TYPE_TB
- en: That is, *x*[1] is a weighted sum of *x*[0] and є[0], where *β*[1] measures
    the weight placed on the noise. The value of *β* changes in different time steps—hence
    the subscript in *β*[1]. If we assume *x*[0] and є[0] are independent of each
    other and follow a standard normal distribution (i.e., with mean 0 and variance
    1), the noisy image *x*[1] will also follow a standard normal distribution. This
    is easy to prove since
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ02.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ03.png)'
  prefs: []
  type: TYPE_IMG
- en: We can keep adding noise to the image for the next T–1 time steps so that
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ04.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (15.2) |'
  prefs: []
  type: TYPE_TB
- en: We can use a reparameterization trick and define *α[t]* = 1 − *β[t]* and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ05.png)'
  prefs: []
  type: TYPE_IMG
- en: to allow us to sample *x[t]* at any arbitrary time step t, where t can take
    any value in [1, 2, . . ., T−1, T]. Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F01_Liu-EQ06.png)'
  prefs: []
  type: TYPE_IMG
- en: '| (15.3) |'
  prefs: []
  type: TYPE_TB
- en: Where є is a combination of є[0], є[1], . . ., and є*[t]*[–1], using the fact
    that we can add two normal distributions to obtain a new normal distribution.
    See, for example, the blog of Lilian Weng at [https://mng.bz/Aalg](https://mng.bz/Aalg)
    for proof.
  prefs: []
  type: TYPE_NORMAL
- en: The farther left of figure 15.1 shows a clean flower, *x*[0], from the training
    set. In the first time step, we inject noise є[0] to it to form a noisy image
    *x*[1] (second image in figure 15.1). We repeat this process for 1,000 time steps,
    until the image becomes random noise (the rightmost image).
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.2 Using the U-Net model to denoise images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you understand the forward diffusion process, let’s discuss the reverse
    diffusion process (i.e., the denoising process). If we can train a model to reverse
    the forward diffusion process, we can feed the model with random noise and ask
    it to produce a noisy flower image. We can then feed the noisy image to the trained
    model again and produce a clearer, though still noisy, image. We can iteratively
    repeat the process for many time steps until we obtain a clean image, indistinguishable
    from images from the training set. The use of multiple inference steps in the
    reverse diffusion process, rather than just a single step, is crucial for gradually
    reconstructing high-quality data from a noisy distribution. It allows for a more
    controlled, stable, and high-quality generation of data.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, we’ll create a denoising U-Net model. The U-Net architecture, which
    was originally designed for biomedical image segmentation, is characterized by
    its symmetric shape, with a contracting path (encoder) and an expansive path (decoder),
    connected by a bottleneck layer. In the context of denoising, U-Net models are
    adapted to remove noise from images while preserving important details. U-Nets
    outperform simple convolutional networks in denoising tasks due to their efficient
    capturing of local and global features in images.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.2 is a diagram of the structure of the denoising U-Net we use in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The model takes a noisy image and the time step the noisy image is in (*x[t]*
    and t in equation 15.3) as input and predicts the noise in the image (i.e., є).
    Since the noisy image is a weighted sum of the original clean image and noise
    (see equation 15.3), knowing the noise allows us to deduce and reconstruct the
    original image.
  prefs: []
  type: TYPE_NORMAL
- en: The contracting path (i.e., the encoder; left side of figure 15.2) consists
    of multiple convolutional layers and pooling layers. It progressively downsamples
    the image, extracting and encoding features at different levels of abstraction.
    This part of the network learns to recognize patterns and features that are relevant
    for denoising.
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck layer (bottom of figure 15.2) connects the encoder and decoder
    paths. It consists of convolutional layers and is responsible for capturing the
    most abstract representations of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The expansive path (i.e., the decoder; right side of figure 15.2) consists of
    upsampling layers and convolutional layers. It progressively upsamples the feature
    maps, reconstructing the image while incorporating features from the encoder through
    skip connections. Skip connections (denoted by dashed lines in figure 15.2) are
    crucial in U-Net models, as they allow the model to retain fine-grained details
    from the input image by combining low-level and high-level features. Next, I briefly
    explain how skip connections work.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 The architecture of the denoising U-Net model. The U-Net architecture
    is characterized by its symmetric shape, with a contracting path (encoder) and
    an expansive path (decoder), connected by a bottleneck layer. The model is designed
    to remove noise from images while preserving important details. The input to the
    model is a noisy image, along with which time step the image is in, and the output
    is the predicted noise in the image.
  prefs: []
  type: TYPE_NORMAL
- en: In a U-Net model, skip connections are implemented by concatenating feature
    maps from the encoder path with corresponding feature maps in the decoder path.
    These feature maps are typically of the same spatial dimensions but may have been
    processed differently due to the separate paths they have traversed. During the
    encoding process, the input image is progressively downsampled, and some spatial
    information (such as edges and textures) may be lost. Skip connections help preserve
    this information by directly passing feature maps from the encoder to the decoder,
    bypassing the information bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the dashed line at the top of figure 15.2 indicates that the model
    concatenates the *output* from the Conv2D layer in the encoder, which has a shape
    of (128, 64, 64), with the *input* to the Conv2D layer in the decoder, which also
    has a shape of (128, 64, 64). As a result, the final input to the Conv2D layer
    in the decoder has a shape of (256, 64, 64).
  prefs: []
  type: TYPE_NORMAL
- en: By combining high-level, abstract features from the decoder with low-level,
    detailed features from the encoder, skip connections enable the model to better
    reconstruct fine details in the denoised image. This is particularly important
    in denoising tasks, where retaining subtle image details is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: The scaled dot product attention (SDPA) mechanism is implemented in both the
    final block of the contracting path and the final block of the expansive path
    in our denoising U-Net model, accompanied by layer normalization and residual
    connections (as shown in figure 15.2 with the label Attn/Norm/Add). This SDPA
    mechanism is essentially the same as the one we developed in chapter 9; the key
    difference is its application to image pixels rather than text tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The use of skip connections and the model’s size lead to redundant feature extractions
    in our denoising U-Net, ensuring that no important feature is lost during the
    denoising process. However, the large size of the model also complicates the identification
    of relevant features, akin to searching for a needle in a haystack. The attention
    mechanism empowers the model to emphasize significant features while disregarding
    irrelevant ones, thereby enhancing the effectiveness of the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1.3 A blueprint to train the denoising U-Net model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of the denoising U-Net is the noise injected into the noisy image.
    The model is trained to minimize the difference between the output (predicted
    noise) and the ground truth (actual noise).
  prefs: []
  type: TYPE_NORMAL
- en: The denoising U-Net model uses the U-Net architecture’s ability to capture both
    local and global context, making it effective for removing noise while preserving
    important details such as edges and textures. These models are widely used in
    various applications, including medical image denoising, photographic image restoration,
    and more. Figure 15.3 is a diagram of the training process of our denoising U-Net
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 The training process of the denoising U-Net model. We first obtain
    clean flower images as our training set. We add noise to clean flower images and
    present them to the U-Net model. The model predicts the noise in the noisy images.
    We compare the predicted noise with the actual noise injected into the flower
    images and tweak the model weights to minimize the mean absolute error.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to gather a dataset of flower images. We’ll use the Oxford
    102 Flower dataset as our training set. We’ll resize all images to a fixed resolution
    of 64 × 64 pixels and normalize pixel values to the range [–1, 1]. For denoising,
    we need pairs of clean and noisy images. We’ll synthetically add noise to the
    clean flower images to create noisy counterparts (step 2 in figure 15.3) based
    on the formula specified in equation 15.3.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then build a denoising U-Net model with a structure as outlined in figure
    15.2\. During each epoch of training, we iterate over the dataset in batches.
    We add noise to the flower images and present the noisy images to the U-Net model
    (step 3), along with the time steps the noisy images are in, t. The U-Net model
    predicts the noise in the noisy images (step 4) based on current parameters in
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: We compare the predicted noise with the actual noise and calculate the L1 loss
    (i.e., mean absolute error) at the pixel level (step 5). L1 loss is usually preferred
    in such situations because it’s less sensitive to outliers compared to the L2
    loss (mean squared error). We then tweak the model parameters to minimize the
    L1 loss (step 6) so that in the next iteration, the model makes better predictions.
    We repeat this process for many iterations until the model parameters converge.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2 Preparing the training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use the Oxford 102 Flower dataset, which is freely available on Hugging
    Face, as our training data. The dataset contains about 8,000 flower images and
    can be downloaded directly by using the *datasets* library you installed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: To save space, we’ll place most helper functions and classes in two local modules,
    ch15util.py and unet_util.py. Download these two files from the book’s GitHub
    repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and place them in the /utils/ folder on your computer. The Python programs in
    this chapter are adapted from Hugging Face’s GitHub repository ([https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers))
    and Filip Basara’s GitHub repository ([https://github.com/filipbasara0/simple-diffusion](https://github.com/filipbasara0/simple-diffusion)).
  prefs: []
  type: TYPE_NORMAL
- en: You’ll use Python to download the dataset to your computer. After that, we’ll
    demonstrate the forward diffusion process by gradually adding noise to clean images
    in the training dataset until they become random noise. Finally, you’ll place
    the training data in batches so that we can use them to train the denoising U-Net
    model later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use the following Python libraries in this chapter: datasets, einops,
    diffusers, and openai. To install these libraries, execute the following line
    of code in a new cell in your Jupyter Notebook application on your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Follow the on-screen instructions to finish the installation.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.1 Flower images as the training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `load_dataset()` method from the *datasets* library you installed earlier
    allows you to directly download the Oxford 102 Flower dataset from Hugging Face.
    We’ll then use the *matplotlib* library to show some flower images in the dataset
    so that we have an idea of what the images in the training dataset look like.
  prefs: []
  type: TYPE_NORMAL
- en: Run the lines of code shown in the following listing in a cell in Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.1 Downloading and visualizing flower images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Downloads the images from Hugging Face
  prefs: []
  type: TYPE_NORMAL
- en: ② Plots the first 16 images
  prefs: []
  type: TYPE_NORMAL
- en: After running the preceding code listing, you’ll see the first 16 flower images
    in the dataset, as displayed in figure 15.4\. These are high-resolution color
    images of various types of flowers. We have standardized the size of each image
    to (3, 64, 64).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 The first 16 Images from the Oxford 102 Flower dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We place the dataset in batches of 4 so that we can use them to train the denoising
    U-Net model later. We choose a batch size of 4 to keep the memory size small enough
    to fit on a GPU during training. Adjust the batch size to 2 or even 1 if your
    GPU memory is small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll code in and visualize the forward diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.2 Visualizing the forward diffusion process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have defined a class `DDIMScheduler()` in the local module ch15util.py you
    just downloaded. Take a look at the class in the file; we’ll use it to add noise
    to images. We’ll also use the class to produce clean images later, along with
    the trained denoising U-Net model. The `DDIMScheduler()` class manages the step
    sizes and sequence of denoising steps, enabling deterministic inference that can
    produce high-quality samples through the denoising process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first select four clean images from the training set and generate noise
    tensors that have the same shape as these images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Obtains four clean images
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates a tensor, noise, which has the same shape as the clean images; each
    value in the noise follows an independent standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Both the images and the noise tensors have a shape of (4, 3, 64, 64), meaning
    4 images in the batch and 3 color channels per image, and the height and width
    of the images are 64 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: During the forward diffusion process, there are 999 transitional noisy images
    between the clean images (*x*[0] as we explained in the first section) and random
    noise (*x[T]*). The transitional noisy images are a weighted sum of the clean
    image and the noise. As *t* goes from 0 to 1,000, the weight on the clean image
    gradually decreases, and the weight on the noise gradually increases, as specified
    in equation 15.3.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we generate and visualize some transitional noisy images.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.2 Visualizing the forward diffusion process
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Instantiates the DDIMScheduler() class with 1,000 time steps
  prefs: []
  type: TYPE_NORMAL
- en: ② Looks at time steps 200, 400, 600, 800, and 1,000
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates noisy images at these time steps
  prefs: []
  type: TYPE_NORMAL
- en: ④ Concatenates noisy images with clean images
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Displays all images
  prefs: []
  type: TYPE_NORMAL
- en: 'The `add_noise()` method in the `DDIMScheduler()` class takes three arguments:
    `clean_images`, `noise`, and `timesteps`. It produces a weighted sum of the clean
    image and the noise, which is a noisy image. Further, the weight is a function
    of the time step, t. As the time step, t, moves from 0 to 1,000, the weight on
    the clean image decreases and that on the noise increases. If you run the previous
    code listing, you’ll see an image similar to figure 15.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.5 The forward diffusion process. The four images in the first column
    are clean images from the training dataset. We then gradually add noise to these
    images from time step 1 to time step 1,000\. As the time step increases, more
    and more noise is injected into the images. The four images in the second column
    are images after 200 time steps. The third column contains images after 400 time
    steps, and they have more noise than those in the second column. The last column
    contains images after 1,000 time steps, and they are 100% random noise.
  prefs: []
  type: TYPE_NORMAL
- en: The first column contains the four clean images without noise. As we move to
    the right, we gradually add more and more noise to the images. The very last column
    contains pure random noise.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3 Building a denoising U-Net model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, we discussed the architecture of the denoising U-Net
    model. In this section, I will guide you through implementing it using Python
    and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net model we are going to construct is quite large, containing over 133
    million parameters, reflecting the complexity of its intended task. It is engineered
    to capture both local and global features within an image through a process of
    downsampling and upsampling the input. The model uses multiple convolutional layers
    interconnected by skip connections, which combine features from various levels
    of the network. This architecture helps maintain spatial information, facilitating
    more effective learning.
  prefs: []
  type: TYPE_NORMAL
- en: Given the substantial size of the denoising U-Net model and its redundant feature
    extraction, the SDPA attention mechanism is employed to enable the model to concentrate
    on the most relevant aspects of the input for the task at hand. To compute SDPA
    attention, we will flatten the image and treat its pixels as a sequence. We will
    then use SDPA to learn the dependencies among different pixels in the image in
    a manner akin to how we learned dependencies among different tokens in text in
    chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.1 The attention mechanism in the denoising U-Net model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement the attention mechanism, we have defined an `Attention()` class
    in the local module ch15util.py, as shown in the following code listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.3 The attention mechanism in the denoising U-Net model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Passes the input through three linear layers to obtain query, key, and value
  prefs: []
  type: TYPE_NORMAL
- en: ② Splits query, key, and value into four heads
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates attention weights
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calculates the attention vector in each head
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Concatenates the four attention vectors into one
  prefs: []
  type: TYPE_NORMAL
- en: The output after running the preceding code listing is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The attention mechanism used here, SDPA, is the same as the one we utilized
    in chapter 9, where we applied SDPA to a sequence of indices representing tokens
    in text. Here, we apply it to pixels in an image. We treat the flattened pixels
    of an image as a sequence and use SDPA to extract dependencies among different
    areas of the input image, enhancing the efficiency of the denoising process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 15.3 demonstrates how SDPA operates in our context. To give you a concrete
    example, we have created a hypothetical image, x, with dimensions (1, 128, 64,
    64), indicating one image in the batch, 128 feature channels, and a size of 64
    × 64 pixels in each channel. The input x is then processed through the attention
    layer. Specifically, each feature channel in the image is flattened into a sequence
    of 64 × 64 = 4,096 pixels. This sequence is then passed through three distinct
    neural network layers to produce the query Q, key K, and value V, which are subsequently
    split into four heads. The attention vector in each head is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F05_Liu-EQ07.png)'
  prefs: []
  type: TYPE_IMG
- en: where *d[k]* represents the dimension of the key vector K. The attention vectors
    from the four heads are concatenated back into a single attention vector.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.2 The denoising U-Net model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the local module unet_util.py you just downloaded, we have defined a `UNet()`
    class to represent the denoising U-Net model. Take a look at the definition in
    the file, and I’ll provide a brief explanation of how it works later. The following
    code listing presents a portion of the `UNet()` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.4 Defining the `UNet()` class
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① The model takes a batch of noisy images and the time steps as input.
  prefs: []
  type: TYPE_NORMAL
- en: ② The embedded time steps are added to the images as inputs in various stages.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Passes the input through the contracting path
  prefs: []
  type: TYPE_NORMAL
- en: ④ Passes the input through the bottleneck path
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Passes the input through the expansive path, with skip connections
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ The output is the predicted noise in the input images.
  prefs: []
  type: TYPE_NORMAL
- en: The job of the denoising U-Net is to predict the noise in the input images based
    on the time steps these images are in. As described in equation 15.3, a noisy
    image at any time step t, *x[t]*, can be represented as a weighted sum of the
    clean image, *x*[o], and standard normally distributed random noise, є. The weight
    assigned to the clean image decreases, and the weight assigned to the random noise
    increases as the time step t progresses from 0 to T. Therefore, to deduce the
    noise in noisy images, the denoising U-Net needs to know which time step a noisy
    image is in.
  prefs: []
  type: TYPE_NORMAL
- en: Time steps are embedded using sine and cosine functions in a manner akin to
    positional encoding in Transformers (discussed in chapters 9 and 10), resulting
    in a 128-value vector. These embeddings are then expanded to match the dimensions
    of the image features at various layers within the model. For instance, in the
    first down block, the time embeddings are broadcasted to a shape of (128, 64,
    64) before being added to the image features, which also have dimensions of (128,
    64, 64).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a denoising U-Net model by instantiating the `UNet()` class
    in the local module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The model has more than 133 million parameters, as you can see from the previous
    output. Given the large number of parameters, the training process in this chapter
    will be time-consuming, requiring approximately 3 to 4 hours of GPU training.
    However, for those who do not have access to GPU training, the trained weights
    are also available on my website. The link to these weights will be provided in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 15.4 Training and using the denoising U-Net model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have both the training data and the denoising U-Net model, we’re
    ready to train the model using the training data.
  prefs: []
  type: TYPE_NORMAL
- en: During each training epoch, we’ll cycle through all the batches in the training
    data. For each image, we’ll randomly select a time step and add noise to the clean
    images in the training data based on this time step value, resulting in a noisy
    image. These noisy images and their corresponding time step values are then fed
    into the denoising U-Net model to predict the noise in each image. We compare
    the predicted noise to the ground truth (the actual noise added to the image)
    and adjust the model parameters to minimize the mean absolute error between the
    predicted and actual noise.
  prefs: []
  type: TYPE_NORMAL
- en: After training, we’ll use the trained model to generate flower images. We’ll
    perform this generation in 50 inference steps (i.e., we’ll set time step values
    to 980, 960, . . ., 20, and 0). Starting with random noise, we’ll input it into
    the trained model to obtain a noisy image. This noisy image is then fed back into
    the trained model to denoise it. We repeat this process for 50 inference steps,
    resulting in an image that is indistinguishable from the flowers in the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 15.4.1 Training the denoising U-Net model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’ll first define the optimizer and the learning rate scheduler for the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the AdamW optimizer, a variant of the Adam optimizer that we have
    been using throughout this book. The AdamW optimizer, first proposed by Ilya Loshchilov
    and Frank Hutter, decouples weight decay (a form of regularization) from the optimization
    steps.^([3](#footnote-001)) Instead of applying weight decay directly to the gradients,
    AdamW applies weight decay directly to the parameters (weights) after the optimization
    step. This modification helps achieve better generalization performance by preventing
    the decay rate from being adapted along with the learning rates. Interested readers
    can learn more about the AdamW optimizer in the original paper by Loshchilov and
    Hutter.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use a learning rate scheduler from the diffusers library to adjust
    the learning rate during the training process. Initially using a higher learning
    rate can help the model escape local minima, while gradually lowering the learning
    rate in later stages of training can help the model converge more steadily and
    accurately towards a global minimum. The learning rate scheduler is defined as
    shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.5 Choosing the optimizer and learning rate in training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Will train the model for 100 epochs
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the AdamW optimizer
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the learning rate scheduler in the diffusers library to control the learning
    rate
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact definition of the `get_scheduler()` function is defined on GitHub
    by Hugging Face: [https://mng.bz/ZVo5](https://mng.bz/ZVo5). In the first 300
    training steps (warmup steps), the learning rate increases linearly from 0 to
    0.0001 (the learning rate we set in the AdamW optimizer). After 300 steps, the
    learning rate decreases following the values of the cosine function between 0.0001
    and 0\. We train the model for 100 epochs in the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.6 Training the denoising U-Net model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Adds noise to clean images in the training set
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the denoising U-Net to predict noise in noisy images
  prefs: []
  type: TYPE_NORMAL
- en: ③ Compares the predicted noise with the actual noise to calculate the loss
  prefs: []
  type: TYPE_NORMAL
- en: ④ Tweaks model parameters to minimize the mean absolute error
  prefs: []
  type: TYPE_NORMAL
- en: During each epoch, we cycle through all batches of clean flower images in the
    training set. We introduce noise to these clean images and feed them to the denoising
    U-Net to predict the noise in these images. We then compare the predicted noise
    to the actual noise and adjust the model parameters to minimize the mean absolute
    error (pixel-wise) between the two.
  prefs: []
  type: TYPE_NORMAL
- en: The training process described here takes several hours with GPU training. After
    training, the trained model weights are saved on your computer. Alternatively,
    you can download the trained weights from my website at [https://mng.bz/RNlD](https://mng.bz/RNlD).
    Unzip the file after downloading.
  prefs: []
  type: TYPE_NORMAL
- en: 15.4.2 Using the trained model to generate flower images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate flower images, we’ll use 50 inference steps. This means we’ll look
    at 50 equally spaced time steps between t = 0 and t = T, with T = 1,000 in our
    case. Therefore, the 50 inference time steps are t = 980, 960, 940, . . . , 20,
    and 0. We’ll start with pure random noise, which corresponds to the image at t
    = 1000. We use the trained denoising U-Net model to denoise it and create a noisy
    image at t = 980. We then present the noisy image at t = 980 to the trained model
    to denoise it and obtain the noisy image at t = 960. We repeat the process for
    many iterations until we obtain an image at t = 0, which is a clean image. This
    process is implemented through the `generate()` method in the `DDIMScheduler()`
    class within the local module ch15util.py.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.7 Defining a `generate()` method in the `DDIMScheduler()` class
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Uses random noise as the starting point (i.e., image at t = 1,000)
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses 50 inference time steps (t = 980, 960, 940, . . , 20, 0)
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the trained denoising U-Net model to predict noise
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates an image based on predicted noise
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Saves intermediate images in a list, imgs
  prefs: []
  type: TYPE_NORMAL
- en: In this `generate()` method, we have also created a list, imgs, to store all
    intermediate images at time steps t = 980, 960,. . . , 20, and 0. We’ll use them
    to visualize the denoising process later. The `generate()` method returns a dictionary
    with the generated images and the list, imgs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use the previous `generate()` method to create 10 clean images.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.8 Image generation with the trained denoising U-Net model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the random seed to 1 so results are reproducible
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses the defined generate() method to create 10 clean images
  prefs: []
  type: TYPE_NORMAL
- en: ③ Plots the generated images
  prefs: []
  type: TYPE_NORMAL
- en: We set the random seed to 1\. As a result, if you use the trained model from
    my website, you’ll get identical results as shown in figure 15.6\. We use the
    `generate()` method defined earlier to create 10 clean images, using 50 inference
    steps. We then plot the 10 images in a 2 × 5 grid, as shown in figure 15.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.6 Flower images created by the trained denoising U-Net model.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from figure 15.6, the generated flower images look real and resemble
    those in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 15.1
  prefs: []
  type: TYPE_NORMAL
- en: Modify code listing 15.8 and change the random seed to 2\. Keep the rest of
    the code the same. Rerun the code listing and see what the generated images look
    like.
  prefs: []
  type: TYPE_NORMAL
- en: The `generate()` method also returns a list, imgs, which contains all the images
    in the 50 intermediate steps. We’ll use them to visualize the denoising process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.9 Visualizing the denoising process
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Keeps time steps 800, 600, 400, 200, and 0
  prefs: []
  type: TYPE_NORMAL
- en: ② Selects 4 sets of flowers out of 10
  prefs: []
  type: TYPE_NORMAL
- en: ③ Plots the 20 images in a 4 × 5 grid
  prefs: []
  type: TYPE_NORMAL
- en: The list, imgs, contains 10 sets of images in all 50 inference steps, t = 980,
    960, . . . , 20, 0. So there are a total of 500 images in the list. We select
    five time steps (t = 800, 600, 400, 200, and 0) for four different flowers (the
    2^(nd), 4^(th), 7^(th), and 10^(th) images in figure 15.6). We then plot the 20
    images in a 4 × 5 grid, as shown in figure 15.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.7 How the trained denoising U-Net model gradually converts random
    noise into clean flower images. We feed random noise to the trained model to obtain
    the image at time step 980\. We then feed the noisy image at t = 980 to the model
    to obtain the image at t = 960. We repeat this process 50 inference steps until
    we obtain the image at t = 0. The first column in this figure shows the four flowers
    at t = 800; the second column shows the same four flowers at t = 600 . . . ; the
    last column shows the four flowers at t = 0 (i.e., clean flower images).
  prefs: []
  type: TYPE_NORMAL
- en: The first column in figure 15.7 shows the four flower images at t = 800. They
    are close to random noise. The second column shows the flowers at t = 600, and
    they start to look like flowers. As we move to the right, the images become clearer
    and clearer. The rightmost column shows the four clean flower images at t = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand how diffusion models work, we’ll discuss text-to-image
    generation. The image generation process of text-to-image Transformers such as
    DALL-E 2, Imagen, and Stable Diffusion is very much like the reverse diffusion
    process we discussed earlier in the chapter, except that the model takes the text
    embedding as a conditioning signal when generating an image.
  prefs: []
  type: TYPE_NORMAL
- en: 15.5 Text-to-image Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-image Transformers such as OpenAI’s DALL-E 2, Google’s Imagen, and Stability
    AI’s Stable Diffusion use diffusion models to generate images from textual descriptions.
    An important component of these text-to-image Transformers is a diffusion model.
    The process of text-to-image generation involves encoding the text input into
    a latent representation, which is then used as a conditioning signal for the diffusion
    model. These Transformers learn to generate lifelike images that correspond to
    the textual description by iteratively denoising a random noise vector, guided
    by the encoded text.
  prefs: []
  type: TYPE_NORMAL
- en: The key to all these text-to-image Transformers is a model to understand content
    in different modalities. In this case, the model must understand the text descriptions
    and link them to images and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll use OpenAI’s CLIP model as an example. CLIP is a key
    component in DALL-E 2\. We’ll discuss how CLIP was trained to understand the connection
    between text descriptions and images. We then use a short Python program to generate
    an image from a text prompt by using OpenAI’s DALL-E 2.
  prefs: []
  type: TYPE_NORMAL
- en: '15.5.1 CLIP: A multimodal Transformer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, the intersection of computer vision and natural language processing
    (NLP) has witnessed significant advancements, one of which is the creation of
    the CLIP model by OpenAI. This innovative model is designed to understand and
    interpret images in the context of natural language, a capability that holds immense
    potential for various applications such as image generation and image classification.
  prefs: []
  type: TYPE_NORMAL
- en: The CLIP model is a multimodal Transformer that bridges the gap between visual
    and textual data. It is trained to understand images by associating them with
    corresponding textual descriptions. Unlike traditional models that require explicit
    labeling of images, CLIP uses a vast dataset of images and their natural language
    descriptions to learn a more generalizable representation of visual concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F08_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.8 How OpenAI’s CLIP model is trained. A large-scale training dataset
    of text–image pairs is collected. The text encoder of the model compresses the
    text description into a D-value text embedding. The image encoder converts the
    corresponding image into an image embedding also with D values. During training,
    a batch of N text–image pairs are converted to N text embeddings and N image embeddings.
    CLIP uses a contrastive learning approach to maximize the similarity between paired
    embeddings (the sum of diagonal values in the figure) while minimizing the similarity
    between embeddings from nonmatching text–image pairs (the sum of off-diagonal
    values in the figure).
  prefs: []
  type: TYPE_NORMAL
- en: The training of the CLIP model, which is illustrated in figure 15.8, begins
    with the collection of a large-scale dataset comprising images and their associated
    textual descriptions. OpenAI utilizes a diverse set of sources, including publicly
    available datasets and web-crawled data, to ensure a wide variety of visual and
    textual content. The dataset is then preprocessed to standardize the images so
    they all have the same shape and to tokenize the text, preparing them for input
    into the model.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP employs a dual-encoder architecture, consisting of an image encoder and
    a text encoder. The image encoder processes the input images while the text encoder
    processes the corresponding textual descriptions. These encoders project the images
    and text into a shared embedding space where they can be compared and aligned.
  prefs: []
  type: TYPE_NORMAL
- en: The core of CLIP’s training lies in its contrastive learning approach. For each
    batch of N image–text pairs in the dataset, the model aims to maximize the similarity
    between paired embeddings (measured by the sum of diagonal values in figure 15.8)
    while minimizing the similarity between embeddings from nonmatching text-image
    pairs (the sum of off-diagonal values). Figure 15.9 is a diagram of how text-to-image
    Transformers such as DALL-E 2 generate realistic images based on text prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F09_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.9 How text-to-image Transformers such as DALL-E 2 create images based
    on text prompts. The text encoder in the trained text-to-image Transformer first
    converts the text description in the prompt into text embedding. The text embedding
    is fed to the CLIP model to obtain a prior vector that represents the image in
    the latent space. The text embedding and the prior are concatenated into a conditioning
    vector. To generate an image, the U-Net denoiser first takes a random noise vector
    as input to generate a noisy image using the conditioning vector. It then takes
    the noisy image and the conditioning vector as input and generates another image,
    which is less noisy. The process is repeated for many iterations until the final
    output, a clean image, is generated.
  prefs: []
  type: TYPE_NORMAL
- en: The image generation process of text-to-image Transformers is similar to the
    reverse diffusion process we discussed earlier in the chapter. Let’s take DALL-E
    2, for example, which was proposed by OpenAI researchers in 2022.^([4](#footnote-000))
    The text encoder in the model first converts the text description in the prompt
    into a text embedding. The text embedding is fed to the CLIP model to obtain a
    prior vector that represents the image in the latent space. The text embedding
    and the prior are concatenated into a conditioning vector. In the first iteration,
    we feed a random noise vector to the U-Net denoiser in the model and ask it to
    generate a noisy image based on the conditioning vector. In the second iteration,
    we feed the noisy image from the previous iteration to the U-Net denoiser and
    ask it to generate another noisy image based on the conditioning vector. We repeat
    this process for many iterations, and the final output is a clean image.
  prefs: []
  type: TYPE_NORMAL
- en: 15.5.2 Text-to-image generation with DALL-E 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you understand how text-to-image Transformers work, let’s write a Python
    program to interact with DALL-E 2 to create an image based on a text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to apply for an OpenAI API key. OpenAI offers various pricing
    tiers that vary based on the number of tokens processed and the type of models
    used. Go to [https://chat.openai.com/auth/login](https://chat.openai.com/auth/login)
    and click on the Sign up button to create an account. After that, log in to your
    account, and go to [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
    to view your API key. Save it in a secure place for later use. We can generate
    an image by using OpenAI’s DALL-E 2.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 15.10 Image generation with DALL-E 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① Makes sure you provide your actual OpenAI API key here, in quotes
  prefs: []
  type: TYPE_NORMAL
- en: ② Instantiates the OpenAI() class to create an agent
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the images.generate() method to generate image based on the text prompt
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints out the image URL
  prefs: []
  type: TYPE_NORMAL
- en: You should place the OpenAI API key you obtained earlier in listing 15.10\.
    We create an agent by instantiating the `OpenAI()` class. To generate an image,
    we need to specify the model, a text prompt, and the size of the image. We have
    used “an astronaut in a space suit riding a unicorn” as the prompt, and the code
    listing provides a URL for us to visualize and download the image. The URL expires
    in an hour, and the resulting image is shown in figure 15.10\.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH15_F10_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.10 An image generated by DALL-E 2 with the text prompt “an astronaut
    in a space suit riding a unicorn”
  prefs: []
  type: TYPE_NORMAL
- en: Run listing 15.10 yourself and see what image DALLE-2 generates for you. Note
    that your result will be different since the output from DALLE-2 (and all LLMs)
    is stochastic rather than deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 15.2
  prefs: []
  type: TYPE_NORMAL
- en: Apply for an OpenAI API key. Then modify code listing 15.10 to generate an image
    using the text prompt “a cat in a suit working on a computer.”
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned the inner workings of diffusion-based models and
    their significance in text-to-image Transformers, such as OpenAI’s CLIP model.
    You also discovered how to obtain your OpenAI API key and used a brief Python
    script to generate images from text descriptions with DALL-E 2, which incorporates
    CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will continue to use the OpenAI API key obtained earlier
    to use pretrained LLMs for generating diverse content, including text, audio,
    and images. Additionally, you will integrate the LangChain Python library with
    other APIs, enabling you to create a know-it-all personal assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In forward diffusion, we gradually add small amounts of random noise to clean
    images until they transform into pure noise. Conversely, in reverse diffusion,
    we begin with random noise and employ a denoising model to progressively eliminate
    noise from the images, transforming the noise back into a clean image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The U-Net architecture, originally designed for biomedical image segmentation,
    has a symmetric shape with a contracting encoder path and an expansive decoder
    path, connected by a bottleneck layer. In denoising, U-Nets are adapted to remove
    noise while preserving details. Skip connections link encoder and decoder feature
    maps of the same spatial dimensions, helping to preserve spatial information like
    edges and textures that may be lost during downsampling in the encoding process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating an attention mechanism into a denoising U-Net model enables it
    to concentrate on important features and disregard irrelevant ones. By treating
    image pixels as a sequence, the attention mechanism learns pixel dependencies,
    similar to how it learns token dependencies in NLP. This enhances the model’s
    ability to identify relevant features effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-to-image Transformers like OpenAI’s DALL-E 2, Google’s Imagen, and Stability
    AI’s Stable Diffusion use diffusion models to create images from textual descriptions.
    They encode the text into a latent representation that conditions the diffusion
    model, which then iteratively denoises a random noise vector, guided by the encoded
    text, to generate lifelike images matching the textual description.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-003-backlink))  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli, 2015, “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.”
    International Conference on Machine Learning, [http://arxiv.org/abs/1503.03585](http://arxiv.org/abs/1503.03585).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](#footnote-002-backlink))  Sohl-Dickstein et al., 2015, “Deep Unsupervised
    Learning Using Nonequilibrium Thermodynamics,” h[ttps://arxiv.org/abs/1503.03585](https://arxiv.org/abs/1503.03585).
    Yang Song and Stefano Ermon, 2019, “Generative Modeling by Estimating Gradients
    of the Data Distribution.” [https://arxiv.org/abs/1907.05600](https://arxiv.org/abs/1907.05600).
    Jonathan Ho, Ajay Jain, and Pieter Abbeel, 2020, “Denoising Diffusion Probabilistic
    Models,” [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](#footnote-001-backlink))  Ilya Loshchilov and Frank Hutter, 2017, “Decoupled
    Weight Decay Regularization.” [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](#footnote-000-backlink))  Aditya Rames, Prafulla Dhariwal, Alex Nichol,
    Casey Chu, and Mark Chen, 2022, “Hierarchical Text-Conditional Image Generation
    with CLIP Latents.” [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125).
  prefs: []
  type: TYPE_NORMAL
