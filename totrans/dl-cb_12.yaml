- en: Chapter 12\. Image Style
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ll explore some techniques to visualize what convolutional
    networks see when they classify images. We’ll do this by running the networks
    in reverse—rather than giving the network an image and asking it what it is, we
    tell the network what to see and ask it to modify the image in a way that makes
    it see the detected item more exaggeratedly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by doing this for a single neuron. This will show us what sorts
    of patterns that neuron reacts to. We’ll then expand on this by introducing the
    concept of octaves, where we zoom in while we optimize the image to get more detail.
    Finally, we will look at applying this technique to existing images and visualize
    what the network “almost” sees in an image, a technique known as deep dreaming.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then switch gears and look at how combinations of “lower” layers of a
    network determine the artistic style of an image and how we can visualize just
    the style of an image. This uses the concept of gram matrices and how they represent
    the style of a painting.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Next, we look at how we can combine this notion with a way to stabilize an image,
    which allows us to generate an image that only copies the style of an image. We
    then move on to apply this technique to existing images, which makes it possible
    to render a recent photograph in the style of Vincent van Gogh’s *Starry Skies*.
    Finally, we’ll use two style images and render versions of the same picture somewhere
    between the two styles.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The following notebooks contain the code for this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 12.1 Visualizing CNN Activations
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to see what is actually happening inside the image recognition network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maximize the activation of a neuron to see which pixels it reacts to most strongly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter we saw that convolutional neural networks are the networks
    of choice when it comes to image recognition. The lowest layers work directly
    on the pixels of the image, and as we go up in the stack of layers we speculate
    that the abstraction level of the features recognized goes up. The final layers
    are capable of actually recognizing things in the image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes intuitive sense. These networks are designed this way analogously
    to how we think the human visual cortex works. Let’s take a look at what the individual
    neurons are doing to see if this is actually the case. We’ll start by loading
    the network up as before. We use the VGG16 here because of its simpler architecture:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll now run the network backwards. That is, we’ll define a loss function
    that optimizes the activation for one specific neuron and then ask the network
    to calculate in what direction to change an image to optimize the value for that
    neuron. In this case we randomly pick the layer `block3_conv` and the neuron at
    index 1:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To run the network backwards, we need to define a Keras function called `iterate`.
    It will take an image and return the loss and the gradient (the changes we need
    to make to the network). We also need to normalize the gradient:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We’ll start with a random noise image and feed it repeatedly into the `iterate`
    function we just defined, and then add the returned gradient to our image. This
    changes the image step by step in the direction where the neuron and layer we
    picked will have a maximum activation—20 steps should do the trick:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before we can display the resulting image, it needs normalization and clipping
    of the values to the usual RGB range:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once we have done that, we can display the image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![One activated neuron](assets/dlcb_12in01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: This is cool. It gives us a glimpse of what the network is doing at this particular
    level. The overall network, though, has millions of neurons; inspecting them one
    by one is not a very scalable strategy to get an insight into what is going on.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'A good way to get an impression is to pick some layers of increasing abstraction:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For each of those layers we’ll find eight representative neurons and add them
    to a grid:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Converting the grid and displaying it in the notebook is similar to what we
    did in [Recipe 3.3](ch03.html#visualizing-word-embeddings):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将网格转换并在笔记本中显示与我们在[Recipe 3.3](ch03.html#visualizing-word-embeddings)中所做的类似：
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Grid of activated neurons](assets/dlcb_12in02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![激活的神经元网格](assets/dlcb_12in02.png)'
- en: Discussion
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Maximizing the activation of a neuron in a neural network is a good way to visualize
    the function of that neuron in the overall task of the network. By sampling neurons
    from different layers we can even visualize the increasing complexity of the features
    that the neurons detect as we go up in the stack.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化神经网络中神经元的激活是可视化该神经元在网络整体任务中功能的好方法。通过从不同层中抽样神经元，我们甚至可以可视化随着我们在堆栈中上升，神经元检测的特征的复杂性的增加。
- en: The results we see contain mostly small patterns. The way that we update the
    pixels makes it hard for larger objects to emerge, since a group of pixels has
    to move in unison and they all are optimized against their local contents. This
    means that it is harder for the more abstract layers to “get what they want” since
    the patterns that they recognize are of a larger size. We can see this in the
    grid image we generated. In the next recipe we’ll explore a technique to help
    with this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的结果主要包含小图案。我们更新像素的方式使得更大的对象难以出现，因为一组像素必须协同移动，它们都针对其局部内容进行了优化。这意味着更抽象的层次更难“得到它们想要的”，因为它们识别的模式具有更大的尺寸。我们可以在我们生成的网格图像中看到这一点。在下一个配方中，我们将探索一种技术来帮助解决这个问题。
- en: You might wonder why we only try to activate neurons in low and middle layers.
    Why not try to activate the final predictions layer? We could find the prediction
    for “cat” and tell the network to activate that, and we’d expect to end up with
    a picture of a cat.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么我们只尝试激活低层和中间层的神经元。为什么不尝试激活最终预测层？我们可以找到“猫”的预测，并告诉网络激活它，我们期望最终得到一张猫的图片。
- en: Sadly, this doesn’t work. It turns out that the universe of all images that
    a network will classify as a “cat” is staggeringly large, but only very few of
    those images would be recognizable to us as cats. So, the resulting image almost
    always looks like noise to us, but the network thinks it is a cat.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，这并不起作用。事实证明，网络将分类为“猫”的所有图像的宇宙是惊人地庞大的，但只有极少数的图像对我们来说是可识别的猫。因此，生成的图像几乎总是对我们来说像噪音，但网络认为它是一只猫。
- en: In [Chapter 13](ch13.html#autoencoders) we’ll look at some techniques to generate
    more realistic images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第13章](ch13.html#autoencoders)中，我们将探讨一些生成更真实图像的技术。
- en: 12.2 Octaves and Scaling
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.2 Octaves和Scaling
- en: Problem
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you visualize larger structures that activate a neuron?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如何可视化激活神经元的较大结构？
- en: Solution
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Zoom in while optimizing the image for maximum neuron activation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化图像以最大化神经元激活的同时进行缩放。
- en: 'In the previous step we saw that we can create images that maximize the activation
    of a neuron, but the patterns remain rather local. An interesting way to get around
    this is to start with a small image and then do a series of steps where we optimize
    it using the algorithm from the previous recipe followed by an enlargement of
    the image. This allows the activation step to first set out the overall structure
    of the image before filling in the details. Starting with a 64×64 image:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们看到我们可以创建最大化神经元激活的图像，但模式仍然相当局部。一个有趣的方法来解决这个问题是从一个小图像开始，然后通过一系列步骤来优化它，使用前一个配方的算法，然后对图像进行放大。这允许激活步骤首先勾勒出图像的整体结构，然后再填充细节。从一个64×64的图像开始：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'we can now do the zoom/optimize thing 20 times:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行20次缩放/优化操作：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using the `block5_conv1` layer and neuron 4 gives a nice organic-looking result:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`block5_conv1`层和神经元4会得到一个看起来很有机的结果：
- en: '![Ocatave activated neuron](assets/dlcb_12in03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Octave激活的神经元](assets/dlcb_12in03.png)'
- en: Discussion
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Octaves and scaling are a great way to let a network produce images that somehow
    represent what it can see.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Octaves和scaling是让网络生成某种程度上代表它所看到的东西的图像的好方法。
- en: There’s a lot to explore here. In the code in the Solution we only optimize
    the activation for one neuron, but we can optimize multiple neurons at the same
    time for a more mixed picture. We can assign different weights to them or even
    negative weights to some of them, forcing the network to stay away from certain
    activations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多可以探索的地方。在解决方案中的代码中，我们只优化一个神经元的激活，但我们可以同时优化多个神经元，以获得更混合的图片。我们可以为它们分配不同的权重，甚至为其中一些分配负权重，迫使网络远离某些激活。
- en: The current algorithm sometimes produces too many high frequencies, especially
    in the first octaves. We can counteract this by applying a Gaussian blur to the
    first octaves to produce a less sharp result.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当前算法有时会产生太多高频率，特别是在第一个octave中。我们可以通过对第一个octave应用高斯模糊来抵消这一点，以产生一个不那么锐利的结果。
- en: And why stop resizing when the image has reached our target size? Instead we
    could continue resizing, but also crop the image to keep it the same size. This
    would create a video sequence where we keep zooming while new patterns unfold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当图像达到我们的目标大小时，为什么要停止调整大小呢？相反，我们可以继续调整大小，但同时裁剪图像以保持相同的大小。这将创建一个视频序列，我们在不断缩放的同时，新的图案不断展开。
- en: Once we’re making movies, we could also change the set of neurons that we activate
    and explore the network that way. The *movie_dream.py* script combines some of
    these ideas and produces mesmerizing movies, an example of which you can find
    on [YouTube](https://youtu.be/rubLdCdfDSk).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始制作电影，我们还可以改变我们激活的神经元集合，并通过这种方式探索网络。*movie_dream.py*脚本结合了其中一些想法，并生成了令人着迷的电影，你可以在[YouTube](https://youtu.be/rubLdCdfDSk)上找到一个示例。
- en: 12.3 Visualizing What a Neural Network Almost Sees
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.3 可视化神经网络几乎看到了什么
- en: Problem
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Can you exaggerate what a network detects, to get a better idea of what it’s
    seeing?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你能夸大网络检测到的东西，以更好地了解它看到了什么吗？
- en: Solution
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Expand the code from the previous recipe to operate on existing images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展前一个配方的代码以操作现有图像。
- en: There are two things we need to change to make the existing algorithm work.
    First, upscaling an existing image would make it rather blocky. Second, we want
    to keep some similarity with the original image, as otherwise we might as well
    start out with a random image. Fixing these two issues reproduces Google’s famous
    DeepDream experiment, where eerie pictures appear out of skies and mountain landscapes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有两件事情我们需要改变才能使现有的算法工作。首先，对现有图像进行放大会使其变得相当块状。其次，我们希望保持与原始图像的某种相似性，否则我们可能会从一个随机图像开始。修复这两个问题会重现谷歌著名的DeepDream实验，其中出现了怪异的图片，如天空和山脉景观。
- en: 'We can achieve those two goals by keeping track of the loss of detail because
    of the upscaling and injecting that lost detail back into the generated image;
    this way we undo the scaling artifacts, plus we “steer” the image back to the
    original at every octave. In the following code, we get all the shapes we want
    to go through and then upscale the image step by step, optimize the image for
    our loss function, and then add the lost detail by comparing what gets lost between
    upscaling and downscaling:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过跟踪由于放大而丢失的细节来实现这两个目标，并将丢失的细节注入生成的图像中；这样我们就可以消除缩放造成的伪影，同时在每个八度将图像“引导”回原始状态。在以下代码中，我们获取所有想要经历的形状，然后逐步放大图像，优化图像以适应我们的损失函数，然后通过比较放大和缩小之间丢失的内容来添加丢失的细节：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives a pretty nice result:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了一个相当不错的结果：
- en: '![Deep dream one neuron](assets/dlcb_12in04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Deep Dream一个神经元](assets/dlcb_12in04.png)'
- en: The original Google algorithm for deep dreaming was slightly different, though.
    What we just did was tell the network to optimize the image to maximize the activation
    for a particular neuron. What Google did instead was to have the network exaggerate
    what it already was seeing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的谷歌Deep Dream算法略有不同。我们刚刚告诉网络优化图像以最大化特定神经元的激活。而谷歌所做的是让网络夸大它已经看到的东西。
- en: It turns out we can optimize the image to increase the current activations by
    adjusting the loss function that we previously defined. Instead of taking into
    account one neuron, we are going to use entire layers. For this to work, we have
    to modify our loss function such that it maximizes activations that are already
    high. We do this by taking the sum of the squares of the activations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们可以通过调整我们先前定义的损失函数来优化图像，以增加当前激活。我们不再考虑一个神经元，而是要使用整个层。为了使其工作，我们必须修改我们的损失函数，使其最大化已经高的激活。我们通过取激活的平方和来实现这一点。
- en: 'Let’s start by specifying the three layers we want to optimize and their respective
    weights:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先指定我们要优化的三个层及其相应的权重：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we define the loss as a sum of those, avoiding border artifacts by only
    involving nonborder pixels in the loss:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将损失定义为这些的总和，通过仅涉及损失中的非边界像素来避免边界伪影：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `iterate` function remains the same, as does the function to generate the
    image. The only change we make is that where we add the gradient to the image,
    we slow down the speed by multiplying the `grad_value` by 0.1:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`iterate`函数保持不变，生成图像的函数也是如此。我们唯一做出的改变是，在将梯度添加到图像时，通过将`grad_value`乘以0.1来减慢速度：'
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running this code, we see eyes and something animal face–like appear in the
    landscape:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码，我们看到眼睛和类似动物脸的东西出现在风景中：
- en: '![Deep dream using an entire image](assets/dlcb_12in05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![使用整个图像的Deep Dream](assets/dlcb_12in05.png)'
- en: You can play around with the layers, their weights, and the speed factor to
    get different images.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试调整层、它们的权重和速度因子以获得不同的图像。
- en: Discussion
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Deep dreaming seems like a playful way to generate hallucinogenic images, and
    it certainly allows for endless exploring and experimentation. But it is also
    a way to understand what neural networks see in an image. Ultimately this is a
    reflection on the images that the networks were trained on: a network trained
    on cats and dogs will “see” cats and dogs in an image of a cloud.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Deep Dream似乎是一种生成致幻图像的有趣方式，它确实允许无休止地探索和实验。但它也是一种了解神经网络在图像中看到的内容的方式。最终，这反映了网络训练的图像：一个训练有关猫和狗的网络将在云的图像中“看到”猫和狗。
- en: We can exploit this by using the techniques from [Chapter 9](ch09.html#transfer_learning).
    If we have a large set of images that we use for retraining an existing network,
    but we set only one layer of that network to trainable, the network has to put
    all its “prejudices” into that layer. When we then run the deep dreaming step
    with that layer as the optimized layer, those “prejudices” should be visualized
    quite nicely.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用来自[第9章](ch09.html#transfer_learning)的技术。如果我们有一组大量的图像用于重新训练现有网络，但我们只将该网络的一个层设置为可训练，那么网络必须将其所有“偏见”放入该层中。然后，当我们以该层作为优化层运行深度梦想步骤时，这些“偏见”应该被很好地可视化。
- en: It is always tempting to draw parallels between how neural networks function
    and how human brains work. Since we don’t really know a lot about the latter,
    this is of course rather speculative. Still, in this case, the activation of certain
    neurons seems close to brain experiments where a researcher artificially activates
    a bit of the human brain by sticking an electrode in it and the subject experiences
    a certain image, smell, or memory.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总是很诱人地将神经网络的功能与人类大脑的工作方式进行类比。由于我们对后者了解不多，这当然是相当推测性的。然而，在这种情况下，某些神经元的激活似乎接近于大脑实验，研究人员通过在其中插入电极来人为激活人脑的一部分，受试者会体验到某种图像、气味或记忆。
- en: Similarly, humans have an ability to recognize faces and animals in the shapes
    of clouds. Some mind-altering substances increase this ability. Maybe these substances
    artificially increase the activation of neural layers in our brains?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，人类有能力在云的形状中识别出脸部和动物。一些改变思维的物质增加了这种能力。也许这些物质在我们的大脑中人为增加了神经层的激活？
- en: 12.4 Capturing the Style of an Image
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.4 捕捉图像的风格
- en: Problem
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you capture the style of an image?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如何捕捉图像的风格？
- en: Solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Calculate the gram matrix of the convolutional layers of the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图像的卷积层的格拉姆矩阵。
- en: In the previous recipe we saw how we can visualize what a network has learned
    by asking it to optimize an image such that it maximizes the activation of a specific
    neuron. The gram matrix of a layer captures the style of that layer, so if we
    start with an image filled with random noise and optimize it such that the gram
    matrices of its layers match the gram matrices of a target image, we’d expect
    it to start mimicking that target image’s style.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个教程中，我们看到了如何通过要求网络优化图像以最大化特定神经元的激活来可视化网络学到的内容。一层的格拉姆矩阵捕捉了该层的风格，因此如果我们从一个充满随机噪声的图像开始优化它，使其格拉姆矩阵与目标图像的格拉姆矩阵匹配，我们期望它开始模仿目标图像的风格。
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The gram matrix is the flattened version of the activations, multiplied by itself
    transposed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 格拉姆矩阵是激活的扁平化版本，乘以自身的转置。
- en: 'We can then define a loss function between two sets of activations by subtracting
    the gram matrices from each, squaring the results, and then summing it all up:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过从每个激活集中减去格拉姆矩阵，平方结果，然后将所有结果相加来定义两组激活之间的损失函数：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As before, we want a pretrained network to do the work. We’ll use it on two
    images, the image we are generating and the image that we want to capture the
    style from—in this case Claude Monet’s *Water* *Lilies* from 1912\. So, we’ll
    create an input tensor that contains both and load a network without the final
    layers that takes this tensor as its input. We’ll use `VGG16` because it is simple,
    but any pretrained network would do:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们希望一个预训练网络来完成工作。我们将在两个图像上使用它，一个是我们生成的图像，另一个是我们想要从中捕捉风格的图像——在这种情况下是克劳德·莫奈1912年的《睡莲》。因此，我们将创建一个包含两者的输入张量，并加载一个没有最终层的网络，该网络以此张量作为输入。我们将使用`VGG16`，因为它很简单，但任何预训练网络都可以：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have the model loaded, we can define our loss variable. We’ll go through
    all layers of the model, and for the ones that have `_conv` in their name (the
    convolutional layers), collect the `style_loss` between the `style_image` and
    the `result_image`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们加载了模型，我们就可以定义我们的损失变量。我们将遍历模型的所有层，并对其中名称中包含`_conv`的层（卷积层）收集`style_image`和`result_image`之间的`style_loss`：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have a loss, we can start to optimize. We’ll use `scipy`’s `fmin_l_bfgs_b`
    optimizer. That method needs a gradient and a loss value to do its job. We can
    get them with one call, so we need to cache the values. We do this using a handy
    helper class, `Evaluator`, that takes a loss and an image:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个损失，我们可以开始优化。我们将使用`scipy`的`fmin_l_bfgs_b`优化器。该方法需要一个梯度和一个损失值来完成其工作。我们可以通过一次调用获得它们，因此我们需要缓存这些值。我们使用一个方便的辅助类`Evaluator`来做到这一点，它接受一个损失和一个图像：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can now optimize an image by calling repeatedly:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过重复调用来优化图像：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The resulting image starts looking quite reasonable after 50 steps or so.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 经过大约50步后，生成的图像开始看起来相当合理。
- en: Discussion
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this recipe we’ve seen that the gram matrix captures the style of an image
    effectively. Naively, we might think that the best way to match the style of an
    image would be to match the activations of all layers directly. But that approach
    is too literal.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们看到了格拉姆矩阵有效地捕捉了图像的风格。天真地，我们可能会认为匹配图像风格的最佳方法是直接匹配所有层的激活。但这种方法太过直接了。
- en: It might not be obvious that the gram matrix approach would work better. The
    intuition behind it is that by multiplying every activation with every other activation
    for a given layer, we capture the correlations between the neurons. Those correlations
    encode the style as it is a measure of the activation distribution, rather than
    the absolute activations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 也许不明显格拉姆矩阵方法会更好。其背后的直觉是，通过将给定层的每个激活与其他每个激活相乘，我们捕捉了神经元之间的相关性。这些相关性编码了风格，因为它是激活分布的度量，而不是绝对激活。
- en: 'With this in mind, there are a couple of things we can experiment with. One
    thing to consider is zero values. Taking the dot product of a vector with itself
    transposed will produce a zero if either multiplicand is zero. That makes it impossible
    to spot correlations with zeros. Since zeros appear quite often, this is rather
    unwanted. A simple fix is to add a delta to the features before doing the dot
    operation. A value of `–1` works well:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们可以尝试一些事情。一个要考虑的事情是零值。将一个向量与其自身转置的点积将在任一乘数为零时产生零。这使得无法发现与零的相关性。由于零值经常出现，这是不太理想的。一个简单的修复方法是在进行点操作之前向特征添加一个增量。值为`-1`效果很好：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can also experiment with adding a constant factor to the expression. This
    can smooth or exaggerate the results. Again, `–1` works well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试向表达式添加一个常数因子。这可以使结果更加平滑或夸张。同样，`-1`效果很好。
- en: 'A final consideration is that we’re taking the gram matrix of all the activations.
    This might seem odd—shouldn’t we do this just for the channels per pixel? What
    really is happening is that we calculate the gram matrix for the channels for
    each pixel and then look at how they correlate over the entire image. This allows
    for a shortcut: we can calculate the mean channels and use that as the gram matrix.
    This gives us an image that captures the average style and is therefore more regular.
    It also runs a bit faster:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要考虑的是我们正在获取所有激活的格拉姆矩阵。这可能看起来有点奇怪——难道我们不应该只为每个像素的通道执行此操作吗？实际上正在发生的是，我们为每个像素的通道计算格拉姆矩阵，然后查看它们在整个图像上的相关性。这允许一种快捷方式：我们可以计算平均通道并将其用作格拉姆矩阵。这给我们一个捕捉平均风格的图像，因此更加规则。它也运行得更快：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The total variation loss we added in this recipe tells the network to keep the
    difference between neighboring pixels in check. Without this, the result will
    be more pixelated and more jumpy. In a way this approach is very similar to the
    regularization we use to keep the weights or output of a network layer in check.
    The overall effect is comparable to applying a slight blur filter on the output
    pixel.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中添加的总变差损失告诉网络保持相邻像素之间的差异。如果没有这个，结果将更加像素化和跳跃。在某种程度上，这种方法与我们用来控制网络层的权重或输出的正则化非常相似。总体效果类似于在输出像素上应用轻微模糊滤镜。
- en: 12.5 Improving the Loss Function to Increase Image Coherence
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.5 改进损失函数以增加图像的连贯性
- en: Problem
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you make the resulting image from the captured style less pixelated?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使从捕捉到的风格生成的图像不那么像素化？
- en: Solution
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Add a loss component to control for the local coherence of the image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个损失组件来控制图像的局部连贯性。
- en: 'The image from the previous recipe already looks quite reasonable. However,
    if we look closely it seems somewhat pixelated. We can guide the algorithm away
    from this by adding a loss function that makes sure that the image is locally
    coherent. We compare each pixel with its neighbor to the left and down. By trying
    to minimize that difference, we introduce a sort of blurring of the image:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个配方中的图像看起来已经相当合理。然而，如果我们仔细看，似乎有些像素化。我们可以通过添加一个损失函数来引导算法，确保图像在局部上是连贯的。我们将每个像素与其左侧和下方的邻居进行比较。通过试图最小化这种差异，我们引入了一种对图像进行模糊处理的方法：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The 1.25 exponent determines how much we punish outliers. Adding this to our
    loss gives:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 1.25指数确定了我们惩罚异常值的程度。将这个添加到我们的损失中得到：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we run this evaluator for 100 steps we get a pretty convincing-looking picture:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行这个评估器100步，我们会得到一个非常令人信服的图片：
- en: '![Neural style without an image](assets/dlcb_12in06.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![没有图像的神经风格](assets/dlcb_12in06.png)'
- en: Discussion
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this recipe we added the final component to our loss function that keeps
    the picture globally looking like the content image. Effectively what we’re doing
    here is optimizing the generated image such that the activations in the upper
    layers correspond to the content image and the activations of the lower layers
    to the style image. Since the lower layers correspond to style and the higher
    layers to content, we can accomplish style transfer this way.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们向我们的损失函数添加了最终组件，使图片在全局上看起来像内容图像。实际上，我们在这里所做的是优化生成的图像，使得上层的激活对应于内容图像，而下层的激活对应于风格图像。由于下层对应于风格，上层对应于内容，我们可以通过这种方式实现风格转移。
- en: The results can be quite striking, to the point where people new to the field
    think that computers can now do art. But tuning is still required as some styles
    are a lot wilder than others, as we’ll see in the next recipe.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能非常引人注目，以至于新手可能认为计算机现在可以创作艺术。但仍然需要调整，因为有些风格比其他风格更狂野，我们将在下一个配方中看到。
- en: 12.6 Transferring the Style to a Different Image
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.6 将风格转移到不同的图像
- en: Problem
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you apply the captured style from one image to another image?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将从一幅图像捕捉到的风格应用到另一幅图像上？
- en: Solution
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a loss function that balances the content from one image with the style
    from another.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个损失函数，平衡一幅图像的内容和另一幅图像的风格。
- en: It would be easy to run the code from the previous recipe over an existing image,
    rather than a noise image, but the results aren’t that great. At first it seems
    it is applying the style to the existing image, but with each step the original
    image dissolves a little. If we keep applying the algorithm the end result will
    be more or less the same, independent of the starting image.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有图像上运行前一个配方中的代码会很容易，而不是在噪声图像上运行，但结果并不那么好。起初似乎是将风格应用到现有图像上，但随着每一步，原始图像都会逐渐消失一点。如果我们继续应用算法，最终结果将基本相同，不管起始图像如何。
- en: 'We can fix this by adding a third component to our loss function, one that
    takes into account the difference between the generated image and our reference
    image:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向我们的损失函数添加第三个组件来修复这个问题，考虑生成的图像与我们参考图像之间的差异：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We’ll now need to add the reference image to our input tensor:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将参考图像添加到我们的输入张量中：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We load the network as before and define our content loss on the last layer
    of our network. The last layer contains the best approximation of what the network
    sees, so this is really what we want to keep the same:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样加载网络，并在网络的最后一层定义我们的内容损失。最后一层包含了网络所看到的最佳近似，所以这确实是我们想要保持不变的：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We’re going to slightly change the style loss by taking into account the position
    of the layer in the network. We want lower layers to carry more weight, since
    the lower layers capture more of the texture/style of an image, while the higher
    layers are more involved in the content of the image. This makes it easier for
    the algorithm to balance the content of the image (which uses the last layer)
    and the style (which uses mostly the lower layers):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将稍微改变风格损失，考虑网络中的层的位置。我们希望较低的层承载更多的权重，因为较低的层捕捉更多的纹理/风格，而较高的层更多地涉及图像的内容。这使得算法更容易平衡图像的内容（使用最后一层）和风格（主要使用较低的层）：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we balance the three components:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们平衡这三个组件：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Running this on a picture of the Oude Kerk (the Old Church) in Amsterdam with
    van Gogh’s *Starry Skies* as the style input gives us:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在阿姆斯特丹的Oude Kerk（古教堂）的图片上运行这个算法，以梵高的《星夜》作为风格输入，我们得到：
- en: '![Oude Kerk by Van Gogh](assets/dlcb_12in07.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![梵高的Oude Kerk](assets/dlcb_12in07.png)'
- en: 12.7 Style Interpolation
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.7 风格插值
- en: Problem
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’ve captured the styles of two images, and want to apply a style somewhere
    between the two to another image. How can you blend them?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经捕捉到两幅图像的风格，并希望将这两者之间的风格应用到另一幅图像中。如何混合它们？
- en: Solution
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a loss function that takes an extra float indicating what percentage of
    each style to apply.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个损失函数，带有额外的浮点数，指示应用每种风格的百分比。
- en: 'We can easily extend our input tensor to take two style images, say one for
    summer and one for winter. After we load the model as before, we can now create
    a loss for each of the styles:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地扩展我们的输入张量，以接受两种风格图像，比如夏季和冬季各一种。在我们像以前一样加载模型之后，我们现在可以为每种风格创建一个损失：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then introduce a placeholder, `summerness`, that we can feed in to get the
    desired `summerness` loss:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们引入一个占位符`summerness`，我们可以输入以获得所需的`summerness`损失：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our `Evaluator` class doesn’t have a way of passing in `summerness`. We could
    create a new class or subclass the existing one, but in this case we can get away
    with “monkey patching”:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Evaluator`类没有一种传递`summerness`的方法。我们可以创建一个新类或者子类现有类，但在这种情况下，我们可以通过“猴子补丁”来解决：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will create an image that is 50% summer, but we can specify any value.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个图像，其中有50%的夏季风格，但我们可以指定任何值。
- en: Discussion
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Adding yet another component to the loss variable allows us to specify the weights
    between two different styles. Nothing is stopping us, of course, from adding even
    more style images and varying their weights. It’s also worth exploring varying
    the relative weights of the style images; van Gogh’s *Starry Skies* image is very
    stark and its style will easily overpower that of more subtle paintings.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失变量中添加另一个组件允许我们指定两种不同风格之间的权重。当然，没有任何阻止我们添加更多风格图像并改变它们的权重。值得探索的是改变风格图像的相对权重；梵高的*星空*图像非常鲜明，其风格很容易压倒更加微妙的绘画风格。
