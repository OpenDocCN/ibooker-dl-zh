- en: Chapter 12\. Image Style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ll explore some techniques to visualize what convolutional
    networks see when they classify images. We’ll do this by running the networks
    in reverse—rather than giving the network an image and asking it what it is, we
    tell the network what to see and ask it to modify the image in a way that makes
    it see the detected item more exaggeratedly.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by doing this for a single neuron. This will show us what sorts
    of patterns that neuron reacts to. We’ll then expand on this by introducing the
    concept of octaves, where we zoom in while we optimize the image to get more detail.
    Finally, we will look at applying this technique to existing images and visualize
    what the network “almost” sees in an image, a technique known as deep dreaming.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then switch gears and look at how combinations of “lower” layers of a
    network determine the artistic style of an image and how we can visualize just
    the style of an image. This uses the concept of gram matrices and how they represent
    the style of a painting.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we look at how we can combine this notion with a way to stabilize an image,
    which allows us to generate an image that only copies the style of an image. We
    then move on to apply this technique to existing images, which makes it possible
    to render a recent photograph in the style of Vincent van Gogh’s *Starry Skies*.
    Finally, we’ll use two style images and render versions of the same picture somewhere
    between the two styles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following notebooks contain the code for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 12.1 Visualizing CNN Activations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to see what is actually happening inside the image recognition network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maximize the activation of a neuron to see which pixels it reacts to most strongly.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter we saw that convolutional neural networks are the networks
    of choice when it comes to image recognition. The lowest layers work directly
    on the pixels of the image, and as we go up in the stack of layers we speculate
    that the abstraction level of the features recognized goes up. The final layers
    are capable of actually recognizing things in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes intuitive sense. These networks are designed this way analogously
    to how we think the human visual cortex works. Let’s take a look at what the individual
    neurons are doing to see if this is actually the case. We’ll start by loading
    the network up as before. We use the VGG16 here because of its simpler architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll now run the network backwards. That is, we’ll define a loss function
    that optimizes the activation for one specific neuron and then ask the network
    to calculate in what direction to change an image to optimize the value for that
    neuron. In this case we randomly pick the layer `block3_conv` and the neuron at
    index 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the network backwards, we need to define a Keras function called `iterate`.
    It will take an image and return the loss and the gradient (the changes we need
    to make to the network). We also need to normalize the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll start with a random noise image and feed it repeatedly into the `iterate`
    function we just defined, and then add the returned gradient to our image. This
    changes the image step by step in the direction where the neuron and layer we
    picked will have a maximum activation—20 steps should do the trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can display the resulting image, it needs normalization and clipping
    of the values to the usual RGB range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have done that, we can display the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![One activated neuron](assets/dlcb_12in01.png)'
  prefs: []
  type: TYPE_IMG
- en: This is cool. It gives us a glimpse of what the network is doing at this particular
    level. The overall network, though, has millions of neurons; inspecting them one
    by one is not a very scalable strategy to get an insight into what is going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good way to get an impression is to pick some layers of increasing abstraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of those layers we’ll find eight representative neurons and add them
    to a grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting the grid and displaying it in the notebook is similar to what we
    did in [Recipe 3.3](ch03.html#visualizing-word-embeddings):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Grid of activated neurons](assets/dlcb_12in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maximizing the activation of a neuron in a neural network is a good way to visualize
    the function of that neuron in the overall task of the network. By sampling neurons
    from different layers we can even visualize the increasing complexity of the features
    that the neurons detect as we go up in the stack.
  prefs: []
  type: TYPE_NORMAL
- en: The results we see contain mostly small patterns. The way that we update the
    pixels makes it hard for larger objects to emerge, since a group of pixels has
    to move in unison and they all are optimized against their local contents. This
    means that it is harder for the more abstract layers to “get what they want” since
    the patterns that they recognize are of a larger size. We can see this in the
    grid image we generated. In the next recipe we’ll explore a technique to help
    with this.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why we only try to activate neurons in low and middle layers.
    Why not try to activate the final predictions layer? We could find the prediction
    for “cat” and tell the network to activate that, and we’d expect to end up with
    a picture of a cat.
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, this doesn’t work. It turns out that the universe of all images that
    a network will classify as a “cat” is staggeringly large, but only very few of
    those images would be recognizable to us as cats. So, the resulting image almost
    always looks like noise to us, but the network thinks it is a cat.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 13](ch13.html#autoencoders) we’ll look at some techniques to generate
    more realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Octaves and Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you visualize larger structures that activate a neuron?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zoom in while optimizing the image for maximum neuron activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous step we saw that we can create images that maximize the activation
    of a neuron, but the patterns remain rather local. An interesting way to get around
    this is to start with a small image and then do a series of steps where we optimize
    it using the algorithm from the previous recipe followed by an enlargement of
    the image. This allows the activation step to first set out the overall structure
    of the image before filling in the details. Starting with a 64×64 image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'we can now do the zoom/optimize thing 20 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `block5_conv1` layer and neuron 4 gives a nice organic-looking result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ocatave activated neuron](assets/dlcb_12in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Octaves and scaling are a great way to let a network produce images that somehow
    represent what it can see.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot to explore here. In the code in the Solution we only optimize
    the activation for one neuron, but we can optimize multiple neurons at the same
    time for a more mixed picture. We can assign different weights to them or even
    negative weights to some of them, forcing the network to stay away from certain
    activations.
  prefs: []
  type: TYPE_NORMAL
- en: The current algorithm sometimes produces too many high frequencies, especially
    in the first octaves. We can counteract this by applying a Gaussian blur to the
    first octaves to produce a less sharp result.
  prefs: []
  type: TYPE_NORMAL
- en: And why stop resizing when the image has reached our target size? Instead we
    could continue resizing, but also crop the image to keep it the same size. This
    would create a video sequence where we keep zooming while new patterns unfold.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’re making movies, we could also change the set of neurons that we activate
    and explore the network that way. The *movie_dream.py* script combines some of
    these ideas and produces mesmerizing movies, an example of which you can find
    on [YouTube](https://youtu.be/rubLdCdfDSk).
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Visualizing What a Neural Network Almost Sees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Can you exaggerate what a network detects, to get a better idea of what it’s
    seeing?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expand the code from the previous recipe to operate on existing images.
  prefs: []
  type: TYPE_NORMAL
- en: There are two things we need to change to make the existing algorithm work.
    First, upscaling an existing image would make it rather blocky. Second, we want
    to keep some similarity with the original image, as otherwise we might as well
    start out with a random image. Fixing these two issues reproduces Google’s famous
    DeepDream experiment, where eerie pictures appear out of skies and mountain landscapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can achieve those two goals by keeping track of the loss of detail because
    of the upscaling and injecting that lost detail back into the generated image;
    this way we undo the scaling artifacts, plus we “steer” the image back to the
    original at every octave. In the following code, we get all the shapes we want
    to go through and then upscale the image step by step, optimize the image for
    our loss function, and then add the lost detail by comparing what gets lost between
    upscaling and downscaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives a pretty nice result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep dream one neuron](assets/dlcb_12in04.png)'
  prefs: []
  type: TYPE_IMG
- en: The original Google algorithm for deep dreaming was slightly different, though.
    What we just did was tell the network to optimize the image to maximize the activation
    for a particular neuron. What Google did instead was to have the network exaggerate
    what it already was seeing.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out we can optimize the image to increase the current activations by
    adjusting the loss function that we previously defined. Instead of taking into
    account one neuron, we are going to use entire layers. For this to work, we have
    to modify our loss function such that it maximizes activations that are already
    high. We do this by taking the sum of the squares of the activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by specifying the three layers we want to optimize and their respective
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define the loss as a sum of those, avoiding border artifacts by only
    involving nonborder pixels in the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `iterate` function remains the same, as does the function to generate the
    image. The only change we make is that where we add the gradient to the image,
    we slow down the speed by multiplying the `grad_value` by 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code, we see eyes and something animal face–like appear in the
    landscape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep dream using an entire image](assets/dlcb_12in05.png)'
  prefs: []
  type: TYPE_IMG
- en: You can play around with the layers, their weights, and the speed factor to
    get different images.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep dreaming seems like a playful way to generate hallucinogenic images, and
    it certainly allows for endless exploring and experimentation. But it is also
    a way to understand what neural networks see in an image. Ultimately this is a
    reflection on the images that the networks were trained on: a network trained
    on cats and dogs will “see” cats and dogs in an image of a cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: We can exploit this by using the techniques from [Chapter 9](ch09.html#transfer_learning).
    If we have a large set of images that we use for retraining an existing network,
    but we set only one layer of that network to trainable, the network has to put
    all its “prejudices” into that layer. When we then run the deep dreaming step
    with that layer as the optimized layer, those “prejudices” should be visualized
    quite nicely.
  prefs: []
  type: TYPE_NORMAL
- en: It is always tempting to draw parallels between how neural networks function
    and how human brains work. Since we don’t really know a lot about the latter,
    this is of course rather speculative. Still, in this case, the activation of certain
    neurons seems close to brain experiments where a researcher artificially activates
    a bit of the human brain by sticking an electrode in it and the subject experiences
    a certain image, smell, or memory.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, humans have an ability to recognize faces and animals in the shapes
    of clouds. Some mind-altering substances increase this ability. Maybe these substances
    artificially increase the activation of neural layers in our brains?
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Capturing the Style of an Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you capture the style of an image?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calculate the gram matrix of the convolutional layers of the image.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous recipe we saw how we can visualize what a network has learned
    by asking it to optimize an image such that it maximizes the activation of a specific
    neuron. The gram matrix of a layer captures the style of that layer, so if we
    start with an image filled with random noise and optimize it such that the gram
    matrices of its layers match the gram matrices of a target image, we’d expect
    it to start mimicking that target image’s style.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The gram matrix is the flattened version of the activations, multiplied by itself
    transposed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then define a loss function between two sets of activations by subtracting
    the gram matrices from each, squaring the results, and then summing it all up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we want a pretrained network to do the work. We’ll use it on two
    images, the image we are generating and the image that we want to capture the
    style from—in this case Claude Monet’s *Water* *Lilies* from 1912\. So, we’ll
    create an input tensor that contains both and load a network without the final
    layers that takes this tensor as its input. We’ll use `VGG16` because it is simple,
    but any pretrained network would do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the model loaded, we can define our loss variable. We’ll go through
    all layers of the model, and for the ones that have `_conv` in their name (the
    convolutional layers), collect the `style_loss` between the `style_image` and
    the `result_image`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a loss, we can start to optimize. We’ll use `scipy`’s `fmin_l_bfgs_b`
    optimizer. That method needs a gradient and a loss value to do its job. We can
    get them with one call, so we need to cache the values. We do this using a handy
    helper class, `Evaluator`, that takes a loss and an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now optimize an image by calling repeatedly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The resulting image starts looking quite reasonable after 50 steps or so.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we’ve seen that the gram matrix captures the style of an image
    effectively. Naively, we might think that the best way to match the style of an
    image would be to match the activations of all layers directly. But that approach
    is too literal.
  prefs: []
  type: TYPE_NORMAL
- en: It might not be obvious that the gram matrix approach would work better. The
    intuition behind it is that by multiplying every activation with every other activation
    for a given layer, we capture the correlations between the neurons. Those correlations
    encode the style as it is a measure of the activation distribution, rather than
    the absolute activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, there are a couple of things we can experiment with. One
    thing to consider is zero values. Taking the dot product of a vector with itself
    transposed will produce a zero if either multiplicand is zero. That makes it impossible
    to spot correlations with zeros. Since zeros appear quite often, this is rather
    unwanted. A simple fix is to add a delta to the features before doing the dot
    operation. A value of `–1` works well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can also experiment with adding a constant factor to the expression. This
    can smooth or exaggerate the results. Again, `–1` works well.
  prefs: []
  type: TYPE_NORMAL
- en: 'A final consideration is that we’re taking the gram matrix of all the activations.
    This might seem odd—shouldn’t we do this just for the channels per pixel? What
    really is happening is that we calculate the gram matrix for the channels for
    each pixel and then look at how they correlate over the entire image. This allows
    for a shortcut: we can calculate the mean channels and use that as the gram matrix.
    This gives us an image that captures the average style and is therefore more regular.
    It also runs a bit faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The total variation loss we added in this recipe tells the network to keep the
    difference between neighboring pixels in check. Without this, the result will
    be more pixelated and more jumpy. In a way this approach is very similar to the
    regularization we use to keep the weights or output of a network layer in check.
    The overall effect is comparable to applying a slight blur filter on the output
    pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Improving the Loss Function to Increase Image Coherence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you make the resulting image from the captured style less pixelated?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Add a loss component to control for the local coherence of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image from the previous recipe already looks quite reasonable. However,
    if we look closely it seems somewhat pixelated. We can guide the algorithm away
    from this by adding a loss function that makes sure that the image is locally
    coherent. We compare each pixel with its neighbor to the left and down. By trying
    to minimize that difference, we introduce a sort of blurring of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The 1.25 exponent determines how much we punish outliers. Adding this to our
    loss gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this evaluator for 100 steps we get a pretty convincing-looking picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural style without an image](assets/dlcb_12in06.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we added the final component to our loss function that keeps
    the picture globally looking like the content image. Effectively what we’re doing
    here is optimizing the generated image such that the activations in the upper
    layers correspond to the content image and the activations of the lower layers
    to the style image. Since the lower layers correspond to style and the higher
    layers to content, we can accomplish style transfer this way.
  prefs: []
  type: TYPE_NORMAL
- en: The results can be quite striking, to the point where people new to the field
    think that computers can now do art. But tuning is still required as some styles
    are a lot wilder than others, as we’ll see in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6 Transferring the Style to a Different Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you apply the captured style from one image to another image?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a loss function that balances the content from one image with the style
    from another.
  prefs: []
  type: TYPE_NORMAL
- en: It would be easy to run the code from the previous recipe over an existing image,
    rather than a noise image, but the results aren’t that great. At first it seems
    it is applying the style to the existing image, but with each step the original
    image dissolves a little. If we keep applying the algorithm the end result will
    be more or less the same, independent of the starting image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fix this by adding a third component to our loss function, one that
    takes into account the difference between the generated image and our reference
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll now need to add the reference image to our input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the network as before and define our content loss on the last layer
    of our network. The last layer contains the best approximation of what the network
    sees, so this is really what we want to keep the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to slightly change the style loss by taking into account the position
    of the layer in the network. We want lower layers to carry more weight, since
    the lower layers capture more of the texture/style of an image, while the higher
    layers are more involved in the content of the image. This makes it easier for
    the algorithm to balance the content of the image (which uses the last layer)
    and the style (which uses mostly the lower layers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we balance the three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this on a picture of the Oude Kerk (the Old Church) in Amsterdam with
    van Gogh’s *Starry Skies* as the style input gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Oude Kerk by Van Gogh](assets/dlcb_12in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 12.7 Style Interpolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve captured the styles of two images, and want to apply a style somewhere
    between the two to another image. How can you blend them?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a loss function that takes an extra float indicating what percentage of
    each style to apply.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily extend our input tensor to take two style images, say one for
    summer and one for winter. After we load the model as before, we can now create
    a loss for each of the styles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We then introduce a placeholder, `summerness`, that we can feed in to get the
    desired `summerness` loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `Evaluator` class doesn’t have a way of passing in `summerness`. We could
    create a new class or subclass the existing one, but in this case we can get away
    with “monkey patching”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This will create an image that is 50% summer, but we can specify any value.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding yet another component to the loss variable allows us to specify the weights
    between two different styles. Nothing is stopping us, of course, from adding even
    more style images and varying their weights. It’s also worth exploring varying
    the relative weights of the style images; van Gogh’s *Starry Skies* image is very
    stark and its style will easily overpower that of more subtle paintings.
  prefs: []
  type: TYPE_NORMAL
