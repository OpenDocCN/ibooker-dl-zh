- en: Chapter 2\. Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start with a basic definition of deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a class of machine learning algorithms that uses *multiple
    stacked layers of processing units* to learn high-level representations from *unstructured*
    data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To understand deep learning fully, we need to delve into this definition a bit
    further. First, we’ll take a look at the different types of unstructured data
    that deep learning can be used to model, then we’ll dive into the mechanics of
    building multiple stacked layers of processing units to solve classification tasks.
    This will provide the foundation for future chapters where we focus on deep learning
    for generative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Data for Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many types of machine learning algorithms require *structured*, tabular data
    as input, arranged into columns of features that describe each observation. For
    example, a person’s age, income, and number of website visits in the last month
    are all features that could help to predict if the person will subscribe to a
    particular online service in the coming month. We could use a structured table
    of these features to train a logistic regression, random forest, or XGBoost model
    to predict the binary response variable—did the person subscribe (1) or not (0)?
    Here, each individual feature contains a nugget of information about the observation,
    and the model would learn how these features interact to influence the response.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unstructured* data refers to any data that is not naturally arranged into
    columns of features, such as images, audio, and text. There is of course spatial
    structure to an image, temporal structure to a recording or passage of text, and
    both spatial and temporal structure to video data, but since the data does not
    arrive in columns of features, it is considered unstructured, as shown in [Figure 2-1](#structured_unstructured).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. The difference between structured and unstructured data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When our data is unstructured, individual pixels, frequencies, or characters
    are almost entirely uninformative. For example, knowing that pixel 234 of an image
    is a muddy shade of brown doesn’t really help identify if the image is of a house
    or a dog, and knowing that character 24 of a sentence is an *e* doesn’t help predict
    if the text is about football or politics.
  prefs: []
  type: TYPE_NORMAL
- en: Pixels or characters are really just the dimples of the canvas into which higher-level
    informative features, such as an image of a chimney or the word *striker*, are
    embedded. If the chimney in the image were placed on the other side of the house,
    the image would still contain a chimney, but this information would now be carried
    by completely different pixels. If the word *striker* appeared slightly earlier
    or later in the text, the text would still be about football, but different character
    positions would provide this information. The granularity of the data combined
    with the high degree of spatial dependence destroys the concept of the pixel or
    character as an informative feature in its own right.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, if we train logistic regression, random forest, or XGBoost
    models on raw pixel values, the trained model will often perform poorly for all
    but the simplest of classification tasks. These models rely on the input features
    to be informative and not spatially dependent. A deep learning model, on the other
    hand, can learn how to build high-level informative features by itself, directly
    from the unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning can be applied to structured data, but its real power, especially
    with regard to generative modeling, comes from its ability to work with unstructured
    data. Most often, we want to generate unstructured data such as new images or
    original strings of text, which is why deep learning has had such a profound impact
    on the field of generative modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of deep learning systems are *artificial neural networks* (ANNs,
    or just *neural networks* for short) with multiple stacked hidden layers. For
    this reason, *deep learning* has now almost become synonymous with *deep neural
    networks*. However, any system that employs many layers to learn high-level representations
    of the input data is also a form of deep learning (e.g., deep belief networks).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by breaking down exactly what we mean by a neural network and then
    see how they can be used to learn high-level features from unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Neural Network?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network consists of a series of stacked *layers*. Each layer contains
    *units* that are connected to the previous layer’s units through a set of *weights*.
    As we shall see, there are many different types of layers, but one of the most
    common is the *fully connected* (or *dense*) layer that connects all units in
    the layer directly to every unit in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks where all adjacent layers are fully connected are called *multilayer
    perceptrons* (MLPs). This is the first type of neural network that we will study.
    An example of an MLP is shown in [Figure 2-2](#deep_learning_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. An example of a multilayer perceptron that predicts if a face is
    smiling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input (e.g., an image) is transformed by each layer in turn, in what is
    known as a *forward pass* through the network, until it reaches the output layer.
    Specifically, each unit applies a nonlinear transformation to a weighted sum of
    its inputs and passes the output through to the subsequent layer. The final output
    layer is the culmination of this process, where the single unit outputs a probability
    that the original input belongs to a particular category (e.g., *smiling*).
  prefs: []
  type: TYPE_NORMAL
- en: The magic of deep neural networks lies in finding the set of weights for each
    layer that results in the most accurate predictions. The process of finding these
    weights is what we mean by *training* the network.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, batches of images are passed through the network
    and the predicted outputs are compared to the ground truth. For example, the network
    might output a probability of 80% for an image of someone who really is smiling
    and a probability of 23% for an image of someone who really isn’t smiling. A perfect
    prediction would output 100% and 0% for these examples, so there is a small amount
    of error. The error in the prediction is then propagated backward through the
    network, adjusting each set of weights a small amount in the direction that improves
    the prediction most significantly. This process is appropriately called *backpropagation*.
    Gradually, each unit becomes skilled at identifying a particular feature that
    ultimately helps the network to make better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Learning High-Level Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The critical property that makes neural networks so powerful is their ability
    to learn features from the input data, without human guidance. In other words,
    we do not need to do any feature engineering, which is why neural networks are
    so useful! We can let the model decide how it wants to arrange its weights, guided
    only by its desire to minimize the error in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s walk through the network shown in [Figure 2-2](#deep_learning_diagram),
    assuming it has already been trained to accurately predict if a given input face
    is smiling:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit A receives the value for an individual channel of an input pixel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit B combines its input values so that it fires strongest when a particular
    low-level feature such as an edge is present.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit C combines the low-level features so that it fires strongest when a higher-level
    feature such as *teeth* are seen in the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit D combines the high-level features so that it fires strongest when the
    person in the original image is smiling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Units in each subsequent layer are able to represent increasingly sophisticated
    aspects of the original input, by combining lower-level features from the previous
    layer. Amazingly, this arises naturally out of the training process—we do not
    need to *tell* each unit what to look for, or whether it should look for high-level
    features or low-level features.
  prefs: []
  type: TYPE_NORMAL
- en: The layers between the input and output layers are called *hidden* layers. While
    our example only has two hidden layers, deep neural networks can have many more.
    Stacking large numbers of layers allows the neural network to learn progressively
    higher-level features by gradually building up information from the lower-level
    features in previous layers. For example, ResNet,^([1](ch02.xhtml#idm45387028957520))
    designed for image recognition, contains 152 layers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll dive straight into the practical side of deep learning and get set
    up with TensorFlow and Keras so that you can start building your own deep neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*TensorFlow*](https://www.tensorflow.org) is an open source Python library
    for machine learning, developed by Google. TensorFlow is one of the most utilized
    frameworks for building machine learning solutions, with particular emphasis on
    the manipulation of tensors (hence the name). It provides the low-level functionality
    required to train neural networks, such as computing the gradient of arbitrary
    differentiable expressions and efficiently executing tensor operations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Keras*](https://keras.io) is a high-level API for building neural networks,
    built on top of TensorFlow ([Figure 2-3](#tf_keras_logos)). It is extremely flexible
    and very user-friendly, making it an ideal choice for getting started with deep
    learning. Moreover, Keras provides numerous useful building blocks that can be
    plugged together to create highly complex deep learning architectures through
    its functional API.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. TensorFlow and Keras are excellent tools for building deep learning
    solutions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are just getting started with deep learning, I can highly recommend using
    TensorFlow and Keras. This setup will allow you to build any network that you
    can think of in a production environment, while also giving you an easy-to-learn
    API that enables rapid development of new ideas and concepts. Let’s start by seeing
    how easy it is to build a multilayer perceptron using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Perceptron (MLP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will train an MLP to classify a given image using *supervised
    learning*. Supervised learning is a type of machine learning algorithm in which
    the computer is trained on a labeled dataset. In other words, the dataset used
    for training includes input data with corresponding output labels. The goal of
    the algorithm is to learn a mapping between the input data and the output labels,
    so that it can make predictions on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: The MLP is a discriminative (rather than generative) model, but supervised learning
    will still play a role in many types of generative models that we will explore
    in later chapters of this book, so it is a good place to start our journey.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/02_deeplearning/01_mlp/mlp.ipynb*
    in the book repository.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example we will be using the [CIFAR-10](https://oreil.ly/cNbFG) dataset,
    a collection of 60,000 32 × 32–pixel color images that comes bundled with Keras
    out of the box. Each image is classified into exactly one of 10 classes, as shown
    in [Figure 2-4](#cifar).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-4\. Example images from the CIFAR-10 dataset (source: [Krizhevsky,
    2009](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf))^([2](ch02.xhtml#idm45387033163216))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, the image data consists of integers between 0 and 255 for each pixel
    channel. We first need to preprocess the images by scaling these values to lie
    between 0 and 1, as neural networks work best when the absolute value of each
    input is less than 1.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to change the integer labeling of the images to one-hot encoded
    vectors, because the neural network output will be a probability that the image
    belongs to each class. If the class integer label of an image is <math alttext="i"><mi>i</mi></math>
    , then its one-hot encoding is a vector of length 10 (the number of classes) that
    has 0s in all but the <math alttext="i"><mi>i</mi></math> th element, which is
    1\. These steps are shown in [Example 2-1](#preprocessing-cifar-10).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. Preprocessing the CIFAR-10 dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Load the CIFAR-10 dataset. `x_train` and `x_test` are `numpy` arrays of shape
    `[50000, 32, 32, 3]` and `[10000, 32, 32, 3]`, respectively. `y_train` and `y_test`
    are `numpy` arrays of shape `[50000, 1]` and `[10000, 1]`, respectively, containing
    the integer labels in the range 0 to 9 for the class of each image.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Scale each image so that the pixel channel values lie between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deep_learning_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encode the labels—the new shapes of `y_train` and `y_test` are `[50000,
    10]` and `[10000, 10]`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the training image data (`x_train`) is stored in a *tensor*
    of shape `[50000, 32, 32, 3]`. There are no *columns* or *rows* in this dataset;
    instead, this is a tensor with four dimensions. A tensor is just a multidimensional
    array—it is the natural extension of a matrix to more than two dimensions. The
    first dimension of this tensor references the index of the image in the dataset,
    the second and third relate to the size of the image, and the last is the channel
    (i.e., red, green, or blue, since these are RGB images).
  prefs: []
  type: TYPE_NORMAL
- en: For example, [Example 2-2](#pixel-value) shows how we can find the channel value
    of a specific pixel in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. The green channel (1) value of the pixel in the (12,13) position
    of image 54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Building the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Keras you can either define the structure of a neural network as a `Sequential`
    model or using the functional API.
  prefs: []
  type: TYPE_NORMAL
- en: A `Sequential` model is useful for quickly defining a linear stack of layers
    (i.e., where one layer follows on directly from the previous layer without any
    branching). We can define our MLP model using the `Sequential` class as shown
    in [Example 2-3](#sequential_functional).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. Building our MLP using a `Sequential` model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Many of the models in this book require that the output from a layer is passed
    to multiple subsequent layers, or conversely, that a layer receives input from
    multiple preceding layers. For these models, the `Sequential` class is not suitable
    and we would need to use the functional API instead, which is a lot more flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I recommend that even if you are just starting out building linear models with
    Keras, you still use the functional API rather than `Sequential` models, since
    it will serve you better in the long run as your neural networks become more architecturally
    complex. The functional API will give you complete freedom over the design of
    your deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 2-4](#sequential_functional-2) shows the same MLP coded using the
    functional API. When using the functional API, we use the `Model` class to define
    the overall input and output layers of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. Building our MLP using the functional API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Both methods give identical models—a diagram of the architecture is shown in
    [Figure 2-5](#cifar_nn).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. A diagram of the MLP architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s now look in more detail at the different layers and activation functions
    used within the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To build our MLP, we used three different types of layers: `Input`, `Flatten`,
    and `Dense`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Input` layer is an entry point into the network. We tell the network the
    shape of each data element to expect as a tuple. Notice that we do not specify
    the batch size; this isn’t necessary as we can pass any number of images into
    the `Input` layer simultaneously. We do not need to explicitly state the batch
    size in the `Input` layer definition.
  prefs: []
  type: TYPE_NORMAL
- en: Next we flatten this input into a vector, using a `Flatten` layer. This results
    in a vector of length 3,072 (= 32 × 32 × 3). The reason we do this is because
    the subsequent `Dense` layer requires that its input is flat, rather than a multidimensional
    array. As we shall see later, other layer types require multidimensional arrays
    as input, so you need to be aware of the required input and output shape of each
    layer type to understand when it is necessary to use `Flatten`.
  prefs: []
  type: TYPE_NORMAL
- en: The `Dense` layer is one of the most fundamental building blocks of a neural
    network. It contains a given number of units that are densely connected to the
    previous layer—that is, every unit in the layer is connected to every unit in
    the previous layer, through a single connection that carries a weight (which can
    be positive or negative). The output from a given unit is the weighted sum of
    the inputs it receives from the previous layer, which is then passed through a
    nonlinear *activation function* before being sent to the following layer. The
    activation function is critical to ensure the neural network is able to learn
    complex functions and doesn’t just output a linear combination of its inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many kinds of activation function, but three of the most important
    are ReLU, sigmoid, and softmax.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *ReLU* (rectified linear unit) activation function is defined to be 0 if
    the input is negative and is otherwise equal to the input. The *LeakyReLU* activation
    function is very similar to ReLU, with one key difference: whereas the ReLU activation
    function returns 0 for input values less than 0, the LeakyReLU function returns
    a small negative number proportional to the input. ReLU units can sometimes die
    if they always output 0, because of a large bias toward negative values pre-activation.
    In this case, the gradient is 0 and therefore no error is propagated back through
    this unit. LeakyReLU activations fix this issue by always ensuring the gradient
    is nonzero. ReLU-based functions are among the most reliable activations to use
    between the layers of a deep network to encourage stable training.'
  prefs: []
  type: TYPE_NORMAL
- en: The *sigmoid* activation is useful if you wish the output from the layer to
    be scaled between 0 and 1—for example, for binary classification problems with
    one output unit or multilabel classification problems, where each observation
    can belong to more than one class. [Figure 2-6](#activations) shows ReLU, LeakyReLU,
    and sigmoid activation functions side by side for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. The ReLU, LeakyReLU, and sigmoid activation functions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The *softmax* activation function is useful if you want the total sum of the
    output from the layer to equal 1; for example, for multiclass classification problems
    where each observation only belongs to exactly one class. It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals StartFraction e Superscript x Super
    Subscript i Superscript Baseline Over sigma-summation Underscript j equals 1 Overscript
    upper J Endscripts e Superscript x Super Subscript j Superscript Baseline EndFraction"
    display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <msub><mi>x</mi> <mi>i</mi></msub></msup> <mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>J</mi></munderover> <msup><mi>e</mi> <msub><mi>x</mi> <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, *J* is the total number of units in the layer. In our neural network,
    we use a softmax activation in the final layer to ensure that the output is a
    set of 10 probabilities that sum to 1, which can be interpreted as the likelihood
    that the image belongs to each class.
  prefs: []
  type: TYPE_NORMAL
- en: In Keras, activation functions can be defined within a layer ([Example 2-5](#activation-function-together))
    or as a separate layer ([Example 2-6](#activation-function-separate)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. A ReLU activation function defined as part of a `Dense` layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Example 2-6\. A ReLU activation function defined as its own layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we pass the input through two `Dense` layers, the first with
    200 units and the second with 150, both with ReLU activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use the `model.summary()` method to inspect the shape of the network
    at each layer, as shown in [Table 2-1](#first_nn_shape).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Output from the `model.summary()` method
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 32, 32, 3) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten | (None, 3072) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 200) | 614,600 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 150) | 30,150 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 10) | 1,510 |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 646,260 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 646,260 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Notice how the shape of our `Input` layer matches the shape of `x_train` and
    the shape of our `Dense` output layer matches the shape of `y_train`. Keras uses
    `None` as a marker for the first dimension to show that it doesn’t yet know the
    number of observations that will be passed into the network. In fact, it doesn’t
    need to; we could just as easily pass 1 observation through the network at a time
    as 1,000\. That’s because tensor operations are conducted across all observations
    simultaneously using linear algebra—this is the part handled by TensorFlow. It
    is also the reason why you get a performance increase when training deep neural
    networks on GPUs instead of CPUs: GPUs are optimized for large tensor operations
    since these calculations are also necessary for complex graphics manipulation.'
  prefs: []
  type: TYPE_NORMAL
- en: The `summary` method also gives the number of parameters (weights) that will
    be trained at each layer. If ever you find that your model is training too slowly,
    check the summary to see if there are any layers that contain a huge number of
    weights. If so, you should consider whether the number of units in the layer could
    be reduced to speed up training.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Make sure you understand how the number of parameters is calculated in each
    layer! It’s important to remember that by default, each unit within a given layer
    is also connected to one additional *bias* unit that always outputs 1\. This ensures
    that the output from the unit can still be nonzero even when all inputs from the
    previous layer are 0.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the number of parameters in the 200-unit `Dense` layer is 200 * (3,072
    + 1) = 614,600.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we compile the model with an optimizer and a loss function, as
    shown in [Example 2-7](#optimizer-loss).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-7\. Defining the optimizer and the loss function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now look in more detail at what we mean by loss functions and optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *loss function* is used by the neural network to compare its predicted output
    to the ground truth. It returns a single number for each observation; the greater
    this number, the worse the network has performed for this observation.
  prefs: []
  type: TYPE_NORMAL
- en: Keras provides many built-in loss functions to choose from, or you can create
    your own. Three of the most commonly used are mean squared error, categorical
    cross-entropy, and binary cross-entropy. It is important to understand when it
    is appropriate to use each.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your neural network is designed to solve a regression problem (i.e., the
    output is continuous), then you might use the *mean squared error* loss. This
    is the mean of the squared difference between the ground truth <math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> and predicted value <math
    alttext="p Subscript i"><msub><mi>p</mi> <mi>i</mi></msub></math> of each output
    unit, where the mean is taken over all <math alttext="n"><mi>n</mi></math> output
    units:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper M upper S upper E equals StartFraction 1 Over n EndFraction
    sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis
    y Subscript i Baseline minus p Subscript i Baseline right-parenthesis squared"
    display="block"><mstyle scriptlevel="0" displaystyle="true"><mrow><mo form="prefix">MSE</mo>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are working on a classification problem where each observation only
    belongs to one class, then *categorical cross-entropy* is the correct loss function.
    This is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="minus sigma-summation Underscript i equals 1 Overscript n Endscripts
    y Subscript i Baseline log left-parenthesis p Subscript i Baseline right-parenthesis"
    display="block"><mrow><mo>-</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>y</mi> <mi>i</mi></msub> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you are working on a binary classification problem with one output
    unit, or a multilabel problem where each observation can belong to multiple classes
    simultaneously, you should use *binary cross-entropy*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *optimizer* is the algorithm that will be used to update the weights in
    the neural network based on the gradient of the loss function. One of the most
    commonly used and stable optimizers is *Adam* (Adaptive Moment Estimation).^([3](ch02.xhtml#idm45387032147088))
    In most cases, you shouldn’t need to tweak the default parameters of the Adam
    optimizer, except the *learning rate*. The greater the learning rate, the larger
    the change in weights at each training step. While training is initially faster
    with a large learning rate, the downside is that it may result in less stable
    training and may not find the global minimum of the loss function. This is a parameter
    that you may want to tune or adjust during training.
  prefs: []
  type: TYPE_NORMAL
- en: Another common optimizer that you may come across is *RMSProp* (Root Mean Squared
    Propagation). Again, you shouldn’t need to adjust the parameters of this optimizer
    too much, but it is worth reading the [Keras documentation](https://keras.io/optimizers)
    to understand the role of each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: We pass both the loss function and the optimizer into the `compile` method of
    the model, as well as a `metrics` parameter where we can specify any additional
    metrics that we would like to report on during training, such as accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, we haven’t shown the model any data. We have just set up the architecture
    and compiled the model with a loss function and optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: To train the model against the data, we simply call the `fit` method, as shown
    in [Example 2-8](#training-mlp).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-8\. Calling the `fit` method to train the model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The raw image data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The one-hot encoded class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_deep_learning_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `batch_size` determines how many observations will be passed to the network
    at each training step.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_deep_learning_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The `epochs` determine how many times the network will be shown the full training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_deep_learning_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: If `shuffle = True`, the batches will be drawn randomly without replacement
    from the training data at each training step.
  prefs: []
  type: TYPE_NORMAL
- en: This will start training a deep neural network to predict the category of an
    image from the CIFAR-10 dataset. The training process works as follows.
  prefs: []
  type: TYPE_NORMAL
- en: First, the weights of the network are initialized to small random values. Then
    the network performs a series of training steps. At each training step, one *batch*
    of images is passed through the network and the errors are backpropagated to update
    the weights. The `batch_size` determines how many images are in each training
    step batch. The larger the batch size, the more stable the gradient calculation,
    but the slower each training step.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It would be far too time-consuming and computationally intensive to use the
    entire dataset to calculate the gradient at each training step, so generally a
    batch size between 32 and 256 is used. It is also now recommended practice to
    increase the batch size as training progresses.^([4](ch02.xhtml#idm45387032068928))
  prefs: []
  type: TYPE_NORMAL
- en: This continues until all observations in the dataset have been seen once. This
    completes the first *epoch*. The data is then passed through the network again
    in batches as part of the second epoch. This process repeats until the specified
    number of epochs have elapsed.
  prefs: []
  type: TYPE_NORMAL
- en: During training, Keras outputs the progress of the procedure, as shown in [Figure 2-7](#first_nn_fit).
    We can see that the training dataset has been split into 1,563 batches (each containing
    32 images) and it has been shown to the network 10 times (i.e., over 10 epochs),
    at a rate of approximately 2 milliseconds per batch. The categorical cross-entropy
    loss has fallen from 1.8377 to 1.3696, resulting in an accuracy increase from
    33.69% after the first epoch to 51.67% after the tenth epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. The output from the `fit` method
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluating the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know the model achieves an accuracy of 51.9% on the training set, but how
    does it perform on data it has never seen?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question we can use the `evaluate` method provided by Keras,
    as shown in [Example 2-9](#evaluate-mlp).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-9\. Evaluating the model performance on the test set
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-8](#first_nn_evaluate) shows the output from this method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. The output from the `evaluate` method
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output is a list of the metrics we are monitoring: categorical cross-entropy
    and accuracy. We can see that model accuracy is still 49.0% even on images that
    it has never seen before. Note that if the model were guessing randomly, it would
    achieve approximately 10% accuracy (because there are 10 classes), so 49.0% is
    a good result, given that we have used a very basic neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: We can view some of the predictions on the test set using the `predict` method,
    as shown in [Example 2-10](#predict-mlp).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-10\. Viewing predictions on the test set using the `predict` method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_deep_learning_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`preds` is an array of shape `[10000, 10]`—i.e., a vector of 10 class probabilities
    for each observation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_deep_learning_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We convert this array of probabilities back into a single prediction using `numpy`’s
    `argmax` function. Here, `axis = –1` tells the function to collapse the array
    over the last dimension (the classes dimension), so that the shape of `preds_single`
    is then `[10000, 1]`.
  prefs: []
  type: TYPE_NORMAL
- en: We can view some of the images alongside their labels and predictions with the
    code in [Example 2-11](#display-mlp). As expected, around half are correct.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-11\. Displaying predictions of the MLP against the actual labels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-9](#first_nn_preds) shows a randomly chosen selection of predictions
    made by the model, alongside the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Some predictions made by the model, alongside the actual labels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You’ve just built a multilayer perceptron using Keras and used
    it to make predictions on new data. Even though this is a supervised learning
    problem, when we come to building generative models in future chapters many of
    the core ideas from this chapter (such as loss functions, activation functions,
    and understanding layer shapes) will still be extremely important. Next we’ll
    look at ways of improving this model, by introducing a few new layer types.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network (CNN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the reasons our network isn’t yet performing as well as it might is because
    there isn’t anything in the network that takes into account the spatial structure
    of the input images. In fact, our first step is to flatten the image into a single
    vector, so that we can pass it to the first `Dense` layer!
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this we need to use a *convolutional layer*.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to understand what is meant by a *convolution* in the context
    of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-10](#simple_conv) shows two different 3 × 3 × 1 portions of a grayscale
    image being convoluted with a 3 × 3 × 1 *filter* (or *kernel*). The convolution
    is performed by multiplying the filter pixelwise with the portion of the image,
    and summing the results. The output is more positive when the portion of the image
    closely matches the filter and more negative when the portion of the image is
    the inverse of the filter. The top example resonates strongly with the filter,
    so it produces a large positive value. The bottom example does not resonate much
    with the filter, so it produces a value near zero.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. A 3 × 3 convolutional filter applied to two portions of a grayscale
    image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we move the filter across the entire image from left to right and top to
    bottom, recording the convolutional output as we go, we obtain a new array that
    picks out a particular feature of the input, depending on the values in the filter.
    For example, [Figure 2-11](#conv_layer_2d) shows two different filters that highlight
    horizontal and vertical edges.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can see this convolutional process worked through manually in the Jupyter
    notebook located at *notebooks/02_deeplearning/02_cnn/convolutions.ipynb* in the
    book repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. Two convolutional filters applied to a grayscale image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A convolutional layer is simply a collection of filters, where the values stored
    in the filters are the weights that are learned by the neural network through
    training. Initially these are random, but gradually the filters adapt their weights
    to start picking out interesting features such as edges or particular color combinations.
  prefs: []
  type: TYPE_NORMAL
- en: In Keras, the `Conv2D` layer applies convolutions to an input tensor with two
    spatial dimensions (such as an image). For example, the code shown in [Example 2-12](#conv-layer)
    builds a convolutional layer with two filters, to match the example in [Figure 2-11](#conv_layer_2d).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-12\. A `Conv2D` layer applied to grayscale input images
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s look at two of the arguments to the `Conv2D` layer in more detail—`strides`
    and `padding`.
  prefs: []
  type: TYPE_NORMAL
- en: Stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `strides` parameter is the step size used by the layer to move the filters
    across the input. Increasing the stride therefore reduces the size of the output
    tensor. For example, when `strides = 2`, the height and width of the output tensor
    will be half the size of the input tensor. This is useful for reducing the spatial
    size of the tensor as it passes through the network, while increasing the number
    of channels.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `padding = "same"` input parameter pads the input data with zeros so that
    the output size from the layer is exactly the same as the input size when `strides
    = 1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-12](#padding_example) shows a 3 × 3 kernel being passed over a 5
    × 5 input image, with `padding = "same"` and `strides = 1`. The output size from
    this convolutional layer would also be 5 × 5, as the padding allows the kernel
    to extend over the edge of the image, so that it fits five times in both directions.
    Without padding, the kernel could only fit three times along each direction, giving
    an output size of 3 × 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0212.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-12\. A 3 × 3 × 1 kernel (gray) being passed over a 5 × 5 × 1 input
    image (blue), with `padding = "same"` and `strides = 1`, to generate the 5 × 5
    × 1 output (green) (source: [Dumoulin and Visin, 2018](https://arxiv.org/abs/1603.07285))^([5](ch02.xhtml#idm45387031545152))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Setting `padding = "same"` is a good way to ensure that you are able to easily
    keep track of the size of the tensor as it passes through many convolutional layers.
    The shape of the output from a convolutional layer with `padding = "same"` is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="left-parenthesis StartFraction i n p u t h e i g h t Over s t
    r i d e EndFraction comma StartFraction i n p u t w i d t h Over s t r i d e EndFraction
    comma f i l t e r s right-parenthesis" display="block"><mrow><mo>(</mo> <mfrac><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow>
    <mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi></mrow></mfrac>
    <mo>,</mo> <mfrac><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow>
    <mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi></mrow></mfrac>
    <mo>,</mo> <mi>f</mi> <mi>i</mi> <mi>l</mi> <mi>t</mi> <mi>e</mi> <mi>r</mi> <mi>s</mi>
    <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Stacking convolutional layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of a `Conv2D` layer is another four-dimensional tensor, now of shape
    `(batch_size, height, width, filters)`, so we can stack `Conv2D` layers on top
    of each other to grow the depth of our neural network and make it more powerful.
    To demonstrate this, let’s imagine we are applying `Conv2D` layers to the CIFAR-10
    dataset and wish to predict the label of a given image. Note that this time, instead
    of one input channel (grayscale) we have three (red, green, and blue).
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 2-13](#conv-network) shows how to build a simple convolutional neural
    network that we could train to succeed at this task.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-13\. Code to build a convolutional neural network model using Keras
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This code corresponds to the diagram shown in [Figure 2-13](#conv_2d_complex).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0213.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. A diagram of a convolutional neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that now that we are working with color images, each filter in the first
    convolutional layer has a depth of 3 rather than 1 (i.e., each filter has shape
    4 × 4 × 3, rather than 4 × 4 × 1). This is to match the three channels (red, green,
    blue) of the input image. The same idea applies to the filters in the second convolutional
    layer that have a depth of 10, to match the 10 channels output by the first convolutional
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, the depth of the filters in a layer is always equal to the number
    of channels output by the preceding layer.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s really informative to look at how the shape of the tensor changes as data
    flows through from one convolutional layer to the next. We can use the `model.summary()`
    method to inspect the shape of the tensor as it passes through the network ([Table 2-2](#conv_net_example_summary)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. CNN model summary
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 32, 32, 3) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 16, 16, 10) | 490 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 8, 8, 20) | 1,820 |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten | (None, 1280) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 10) | 12,810 |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 15,120 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 15,120 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s walk through our network layer by layer, noting the shape of the tensor
    as we go:'
  prefs: []
  type: TYPE_NORMAL
- en: The input shape is `(None, 32, 32, 3)`—Keras uses `None` to represent the fact
    that we can pass any number of images through the network simultaneously. Since
    the network is just performing tensor algebra, we don’t need to pass images through
    the network individually, but instead can pass them through together as a batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shape of each of the 10 filters in the first convolutional layer is 4 ×
    4 × 3\. This is because we have chosen each filter to have a height and width
    of 4 (`kernel_size = (4,4)`) and there are three channels in the preceding layer
    (red, green, and blue). Therefore, the number of parameters (or weights) in the
    layer is (4 × 4 × 3 + 1) × 10 = 490, where the + 1 is due to the inclusion of
    a bias term attached to each of the filters. The output from each filter will
    be the pixelwise multiplication of the filter weights and the 4 × 4 × 3 section
    of the image it is covering. As `strides = 2` and `padding = "same"`, the width
    and height of the output are both halved to 16, and since there are 10 filters
    the output of the first layer is a batch of tensors each having shape `[16, 16,
    10]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second convolutional layer, we choose the filters to be 3 × 3 and they
    now have depth 10, to match the number of channels in the previous layer. Since
    there are 20 filters in this layer, this gives a total number of parameters (weights)
    of (3 × 3 × 10 + 1) × 20 = 1,820\. Again, we use `strides = 2 and` `padding =
    "same"`, so the width and height both halve. This gives us an overall output shape
    of `(None, 8, 8, 20)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now flatten the tensor using the Keras `Flatten` layer. This results in a
    set of 8 × 8 × 20 = 1,280 units. Note that there are no parameters to learn in
    a `Flatten` layer as the operation is just a restructuring of the tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We finally connect these units to a 10-unit `Dense` layer with softmax activation,
    which represents the probability of each category in a 10-category classification
    task. This creates an extra 1,280 × 10 = 12,810 parameters (weights) to learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This example demonstrates how we can chain convolutional layers together to
    create a convolutional neural network. Before we see how this compares in accuracy
    to our densely connected neural network, we’ll examine two more techniques that
    can also improve performance: batch normalization and dropout.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common problem when training a deep neural network is ensuring that the
    weights of the network remain within a reasonable range of values—if they start
    to become too large, this is a sign that your network is suffering from what is
    known as the *exploding gradient* problem. As errors are propagated backward through
    the network, the calculation of the gradient in the earlier layers can sometimes
    grow exponentially large, causing wild fluctuations in the weight values.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your loss function starts to return `NaN`, chances are that your weights
    have grown large enough to cause an overflow error.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t necessarily happen immediately as you start training the network.
    Sometimes it can be happily training for hours when suddenly the loss function
    returns `NaN` and your network has exploded. This can be incredibly annoying.
    To prevent it from happening, you need to understand the root cause of the exploding
    gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: Covariate shift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the reasons for scaling input data to a neural network is to ensure a
    stable start to training over the first few iterations. Since the weights of the
    network are initially randomized, unscaled input could potentially create huge
    activation values that immediately lead to exploding gradients. For example, instead
    of passing pixel values from 0–255 into the input layer, we usually scale these
    values to between –1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Because the input is scaled, it’s natural to expect the activations from all
    future layers to be relatively well scaled as well. Initially this may be true,
    but as the network trains and the weights move further away from their random
    initial values, this assumption can start to break down. This phenomenon is known
    as *covariate shift*.
  prefs: []
  type: TYPE_NORMAL
- en: Covariate Shift Analogy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you’re carrying a tall pile of books, and you get hit by a gust of wind.
    You move the books in a direction opposite to the wind to compensate, but as you
    do so, some of the books shift, so that the tower is slightly more unstable than
    before. Initially, this is OK, but with every gust the pile becomes more and more
    unstable, until eventually the books have shifted so much that the pile collapses.
    This is covariate shift.
  prefs: []
  type: TYPE_NORMAL
- en: Relating this to neural networks, each layer is like a book in the pile. To
    remain stable, when the network updates the weights, each layer implicitly assumes
    that the distribution of its input from the layer beneath is approximately consistent
    across iterations. However, since there is nothing to stop any of the activation
    distributions shifting significantly in a certain direction, this can sometimes
    lead to runaway weight values and an overall collapse of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Training using batch normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Batch normalization* is a technique that drastically reduces this problem.
    The solution is surprisingly simple. During training, a batch normalization layer
    calculates the mean and standard deviation of each of its input channels across
    the batch and normalizes by subtracting the mean and dividing by the standard
    deviation. There are then two learned parameters for each channel, the scale (gamma)
    and shift (beta). The output is simply the normalized input, scaled by gamma and
    shifted by beta. [Figure 2-14](#batch_norm) shows the whole process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0214.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-14\. The batch normalization process (source: [Ioffe and Szegedy,
    2015](https://arxiv.org/abs/1502.03167))^([6](ch02.xhtml#idm45387025136368))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can place batch normalization layers after dense or convolutional layers
    to normalize the output.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Referring to our previous example, it’s a bit like connecting the layers of
    books with small sets of adjustable springs that ensure there aren’t any overall
    huge shifts in their positions over time.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction using batch normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might be wondering how this layer works at prediction time. When it comes
    to prediction, we may only want to predict a single observation, so there is no
    *batch* over which to calculate the mean and standard deviation. To get around
    this problem, during training a batch normalization layer also calculates the
    moving average of the mean and standard deviation of each channel and stores this
    value as part of the layer to use at test time.
  prefs: []
  type: TYPE_NORMAL
- en: 'How many parameters are contained within a batch normalization layer? For every
    channel in the preceding layer, two weights need to be learned: the scale (gamma)
    and shift (beta). These are the *trainable* parameters. The moving average and
    standard deviation also need to be calculated for each channel, but since they
    are derived from the data passing through the layer rather than trained through
    backpropagation, they are called *nontrainable* parameters. In total, this gives
    four parameters for each channel in the preceding layer, where two are trainable
    and two are nontrainable.'
  prefs: []
  type: TYPE_NORMAL
- en: In Keras, the `BatchNormalization` layer implements the batch normalization
    functionality, as shown in [Example 2-14](#batchnorm-layer).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-14\. A `BatchNormalization` layer in Keras
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `momentum` parameter is the weight given to the previous value when calculating
    the moving average and moving standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When studying for an exam, it is common practice for students to use past papers
    and sample questions to improve their knowledge of the subject material. Some
    students try to memorize the answers to these questions, but then come unstuck
    in the exam because they haven’t truly understood the subject matter. The best
    students use the practice material to further their general understanding, so
    that they are still able to answer correctly when faced with new questions that
    they haven’t seen before.
  prefs: []
  type: TYPE_NORMAL
- en: The same principle holds for machine learning. Any successful machine learning
    algorithm must ensure that it generalizes to unseen data, rather than simply *remembering*
    the training dataset. If an algorithm performs well on the training dataset, but
    not the test dataset, we say that it is suffering from *overfitting*. To counteract
    this problem, we use *regularization* techniques, which ensure that the model
    is penalized if it starts to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to regularize a machine learning algorithm, but for deep
    learning, one of the most common is by using *dropout* layers. This idea was introduced
    by Hinton et al. in 2012^([7](ch02.xhtml#idm45387025089232)) and presented in
    a 2014 paper by Srivastava et al.^([8](ch02.xhtml#idm45387025086976))
  prefs: []
  type: TYPE_NORMAL
- en: Dropout layers are very simple. During training, each dropout layer chooses
    a random set of units from the preceding layer and sets their output to 0, as
    shown in [Figure 2-15](#dropout).
  prefs: []
  type: TYPE_NORMAL
- en: Incredibly, this simple addition drastically reduces overfitting by ensuring
    that the network doesn’t become overdependent on certain units or groups of units
    that, in effect, just remember observations from the training set. If we use dropout
    layers, the network cannot rely too much on any one unit and therefore knowledge
    is more evenly spread across the whole network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0215.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. A dropout layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This makes the model much better at generalizing to unseen data, because the
    network has been trained to produce accurate predictions even under unfamiliar
    conditions, such as those caused by dropping random units. There are no weights
    to learn within a dropout layer, as the units to drop are decided stochastically.
    At prediction time, the dropout layer doesn’t drop any units, so that the full
    network is used to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout Analogy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Returning to our analogy, it’s a bit like a math student practicing past papers
    with a random selection of key formulae missing from their formula book. This
    way, they learn how to answer questions through an understanding of the core principles,
    rather than always looking up the formulae in the same places in the book. When
    it comes to test time, they will find it much easier to answer questions that
    they have never seen before, due to their ability to generalize beyond the training
    material.
  prefs: []
  type: TYPE_NORMAL
- en: The `Dropout` layer in Keras implements this functionality, with the `rate`
    parameter specifying the proportion of units to drop from the preceding layer,
    as shown in [Example 2-15](#dropout-layer).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-15\. A `Dropout` layer in Keras
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Dropout layers are used most commonly after dense layers since these are the
    most prone to overfitting due to the higher number of weights, though you can
    also use them after convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Batch normalization also has been shown to reduce overfitting, and therefore
    many modern deep learning architectures don’t use dropout at all, relying solely
    on batch normalization for regularization. As with most deep learning principles,
    there is no golden rule that applies in every situation—the only way to know for
    sure what’s best is to test different architectures and see which performs best
    on a holdout set of data.
  prefs: []
  type: TYPE_NORMAL
- en: Building the CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You’ve now seen three new Keras layer types: `Conv2D`, `BatchNormalization`,
    and `Dropout`. Let’s put these pieces together into a CNN model and see how it
    performs on the CIFAR-10 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can run the following example in the Jupyter notebook in the book repository
    called *notebooks/02_deeplearning/02_cnn/cnn.ipynb*.
  prefs: []
  type: TYPE_NORMAL
- en: The model architecture we shall test is shown in [Example 2-16](#conv-network-2).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-16\. Code to build a CNN model using Keras
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We use four stacked `Conv2D` layers, each followed by a `BatchNormalization`
    and a `LeakyReLU` layer. After flattening the resulting tensor, we pass the data
    through a `Dense` layer of size 128, again followed by a `BatchNormalization`
    and a `LeakyReLU` layer. This is immediately followed by a `Dropout` layer for
    regularization, and the network is concluded with an output `Dense` layer of size
    10.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The order in which to use the batch normalization and activation layers is a
    matter of preference. Usually batch normalization layers are placed before the
    activation, but some successful architectures use these layers the other way around.
    If you do choose to use batch normalization before activation, you can remember
    the order using the acronym *BAD* (batch normalization, activation, then dropout)!
  prefs: []
  type: TYPE_NORMAL
- en: The model summary is shown in [Table 2-3](#cnn_model_summary).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-3\. Model summary of the CNN for CIFAR-10
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 32, 32, 3) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 32, 32, 32) | 896 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 32, 32, 32) | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 32, 32, 32) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 16, 16, 32) | 9,248 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 16, 16, 32) | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 16, 16, 32) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 16, 16, 64) | 18,496 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 16, 16, 64) | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 16, 16, 64) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 8, 8, 64) | 36,928 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 8, 8, 64) | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 8, 8, 64) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten | (None, 4096) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 128) | 524,416 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 128) | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 10) | 1290 |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 592,554 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 591,914 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 640 |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before moving on, make sure you are able to calculate the output shape and number
    of parameters for each layer by hand. It’s a good exercise to prove to yourself
    that you have fully understood how each layer is constructed and how it is connected
    to the preceding layer! Don’t forget to include the bias weights that are included
    as part of the `Conv2D` and `Dense` layers.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Evaluating the CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We compile and train the model in exactly the same way as before and call the
    `evaluate` method to determine its accuracy on the holdout set ([Figure 2-16](#cnn_model_evaluate)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0216.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. CNN performance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, this model is now achieving 71.5% accuracy, up from 49.0% previously.
    Much better! [Figure 2-17](#cnn_preds) shows some predictions from our new convolutional
    model.
  prefs: []
  type: TYPE_NORMAL
- en: This improvement has been achieved simply by changing the architecture of the
    model to include convolutional, batch normalization, and dropout layers. Notice
    that the number of parameters is actually fewer in our new model than the previous
    model, even though the number of layers is far greater. This demonstrates the
    importance of being experimental with your model design and being comfortable
    with how the different layer types can be used to your advantage. When building
    generative models, it becomes even more important to understand the inner workings
    of your model since it is the middle layers of your network that capture the high-level
    features that you are most interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0217.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. CNN predictions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the core deep learning concepts that you will need to
    start building deep generative models. We started by building a multilayer perceptron
    (MLP) using Keras and trained the model to predict the category of a given image
    from the CIFAR-10 dataset. Then, we improved upon this architecture by introducing
    convolutional, batch normalization, and dropout layers to create a convolutional
    neural network (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: A really important point to take away from this chapter is that deep neural
    networks are completely flexible by design, and there really are no fixed rules
    when it comes to model architecture. There are guidelines and best practices,
    but you should feel free to experiment with layers and the order in which they
    appear. Don’t feel constrained to only use the architectures that you have read
    about in this book or elsewhere! Like a child with a set of building blocks, the
    design of your neural network is only limited by your own imagination.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall see how we can use these building blocks to design
    a network that can generate images.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.xhtml#idm45387028957520-marker)) Kaiming He et al., “Deep Residual
    Learning for Image Recognition,” December 10, 2015, [*https://arxiv.org/abs/1512.03385*](https://arxiv.org/abs/1512.03385).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.xhtml#idm45387033163216-marker)) Alex Krizhevsky, “Learning Multiple
    Layers of Features from Tiny Images,” April 8, 2009, [*https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf*](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch02.xhtml#idm45387032147088-marker)) Diederik Kingma and Jimmy Ba, “Adam:
    A Method for Stochastic Optimization,” December 22, 2014, [*https://arxiv.org/abs/1412.6980v8*](https://arxiv.org/abs/1412.6980v8).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.xhtml#idm45387032068928-marker)) Samuel L. Smith et al., “Don’t Decay
    the Learning Rate, Increase the Batch Size,” November 1, 2017, [*https://arxiv.org/abs/1711.00489*](https://arxiv.org/abs/1711.00489).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch02.xhtml#idm45387031545152-marker)) Vincent Dumoulin and Francesco Visin,
    “A Guide to Convolution Arithmetic for Deep Learning,” January 12, 2018, [*https://arxiv.org/abs/1603.07285*](https://arxiv.org/abs/1603.07285).
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch02.xhtml#idm45387025136368-marker)) Sergey Ioffe and Christian Szegedy,
    “Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift,” February 11, 2015, [*https://arxiv.org/abs/1502.03167*](https://arxiv.org/abs/1502.03167).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch02.xhtml#idm45387025089232-marker)) Hinton et al., “Networks by Preventing
    Co-Adaptation of Feature Detectors,” July 3, 2012, [*https://arxiv.org/abs/1207.0580*](https://arxiv.org/abs/1207.0580).
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch02.xhtml#idm45387025086976-marker)) Nitish Srivastava et al., “Dropout:
    A Simple Way to Prevent Neural Networks from Overfitting,” *Journal of Machine
    Learning Research* 15 (2014): 1929–1958, [*http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf*](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).'
  prefs: []
  type: TYPE_NORMAL
