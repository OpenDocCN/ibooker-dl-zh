<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Tips and Tricks for Deep Learning in Biology"><div class="chapter" id="tips-and-tricks-for-deep-learning-in-biology">
 <h1><span class="label">Chapter 7. </span>Tips and Tricks for Deep Learning in Biology</h1>
 <p><a contenteditable="false" data-primary="debugging" data-type="indexterm" id="ch07_tips.html0"/>This final chapter brings together common themes from earlier chapters and distills practical strategies for applying deep learning techniques to biological problems. In machine learning, it’s rare for things to work perfectly on the first try—or even the tenth. Debugging is an expected part of the process, not a sign of failure. Don’t get discouraged.
 </p>
 <p>Here, we share a collection of tips that have helped us (and others) navigate the challenges of deep learning in biology. Some were learned the hard way, and others emerged from writing this book. This list isn’t exhaustive, but we hope it shortens your path to developing working models—and sharpens your instincts for when things go wrong.</p>
 <div data-type="warning" epub:type="warning"><h6>Warning</h6>
  <p>Don’t expect steady, incremental improvements in your project. Progress in deep learning—especially with biological data—is often highly nonlinear. You might spend weeks debugging with no clear gains, only to make one small change that suddenly unlocks everything. This is normal—and not a cause for concern.</p>
 </div>
 
 <section data-type="sect1" data-pdf-bookmark="Simplify"><div class="sect1" id="simplify">
  <h1>Simplify</h1>
  <p><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-type="indexterm" id="ch07_tips.html1"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-type="indexterm" id="ch07_tips.html2"/>When things stop making sense, simplify. Strip your problem back to the bare minimum—a smaller dataset, a shallower model, or a simpler loss function. It’s easy to get lost in complex pipelines, but debugging is much easier when you can isolate one thing at a time.
  </p>
  <p>Once you’ve got something working again, you can reintroduce complexity gradually. Think of this as turning the knobs one by one instead of all at once. It’s not glamorous, but it’s one of the most reliable strategies for making progress.</p>
  <section data-type="sect2" data-pdf-bookmark="Simplify Your Model"><div class="sect2" id="simplify-your-model">
   <h2>Simplify Your Model</h2>
   <p>
    <a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="models" data-type="indexterm" id="ch07_tips.html3"/><a contenteditable="false" data-primary="models, debugging" data-secondary="poor model performance" data-tertiary="simplifying as tactic" data-type="indexterm" id="ch07_tips.html4"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="models" data-type="indexterm" id="ch07_tips.html5"/>When your model has too many bells and whistles, it becomes difficult to pinpoint where things are going wrong. Often, the most effective debugging strategy is to strip it down to the bare essentials.
   </p>
   <dl>
    <dt>Simplify your architecture</dt>
    <dd>Complex architectures can make it difficult to reason about what’s going wrong. To help with this:
     <dl>
      <dt>Eliminate unnecessary layers</dt>
      <dd><p>If a layer doesn’t contribute directly to the input–output mapping—such as layers that simply add model capacity without altering dimensions—it’s best to remove it during debugging.</p></dd>
      <dt>Call basic layers directly</dt>
      <dd><p>Use layers like <code>nn.Conv</code> and <code>nn.Dense</code> instead of custom blocks, which can obscure bugs and internal behavior.</p></dd>
      <dt>Reduce depth and width</dt>
      <dd><p>If your model has many layers or units per layer, consider reducing both. A shallower model is easier to debug and understand, especially in the early stages of development.</p></dd>
      <dt>Remove residual connections</dt>
      <dd><p>These can complicate debugging by introducing dependencies between layers and by masking issues in the layers they connect (like poor initialization or gradient problems).</p>

      <div data-type="tip"><h6>Tip</h6>
    <p>The ultimate simplification is to reduce your model to a direct mapping from inputs to outputs. For example, pass your input through a single <code>nn.Dense</code> layer and see if it can overfit a tiny batch of data. If that fails, the issue is likely not with the architecture, but with your data pipeline, loss function, or optimizer.</p>
   </div>
   </dd>
         </dl>
      </dd>
      <dt>Turn off extras, like normalization and dropout</dt>
      <dd>These add complexity that’s often unnecessary during early debugging. In Flax, you must explicitly manage both model state (e.g., batch norm statistics) and random number generators (RNGs), which can easily lead to subtle bugs.
      <dl>
      <dt>Don’t use batch norm</dt>
      <dd>The complexity of batch norm is threefold. First, it behaves differently during training and inference. Second, it introduces additional state (running mean and variance) that must be updated outside standard gradient updates. Third, it breaks a key assumption: most layers operate independently on each batch element, but batch norm computes statistics across the batch. This makes it incompatible with tools like <code>vmap</code> or sharded training (<code>pmap</code>, <code>pjit</code>) unless you take special care to synchronize statistics across devices.</dd>
      <dt>Skip dropout</dt>
      <dd>Dropout introduces stochasticity, making it harder to determine whether poor performance is due to randomness or a deeper issue.</dd>
     </dl>
    </dd>
    <dt>Simplify your optimizer</dt>
    <dd>Don’t worry about experimenting with different optimizers or learning rate schedules until the basics are working. Pick a sensible default—like good old Adam with a learning rate of <code>1e-3</code>, and focus on solving more fundamental issues first.</dd>
    <dt>Avoid mixed precision</dt>
    <dd>While lower-precision data types like <code>bfloat16</code>, <code>float16</code>, or <code>TensorFloat32</code> can improve performance and memory usage (out of scope for this book), they can lead to subtle numerical instability that’s extremely hard to debug. Note that even if you pass in <code>float32</code> inputs, JAX may default to lower-precision matmuls. To force full <code>float32</code> precision globally (especially during debugging), add <code>jax.config.update("jax_default_matmul_precision", "float32")</code>.</dd>
   </dl>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>Although turning off these more advanced features may lead to worse performance (due to underfitting or overfitting) and feel like you’re going backward, once the basic setup is functioning correctly, you can systematically reenable features to assess their impact on model performance.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html5" data-type="indexterm" id="id1036"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html4" data-type="indexterm" id="id1037"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html3" data-type="indexterm" id="id1038"/>
    </p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Simplify and Control Your Environment"><div class="sect2" id="simplify-and-control-your-environment">
   <h2>Simplify and Control Your Environment</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="environment" data-type="indexterm" id="ch07_tips.html6"/><a contenteditable="false" data-primary="environment, simplifying as debugging tactic" data-type="indexterm" id="ch07_tips.html7"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="environment" data-type="indexterm" id="ch07_tips.html8"/>Your model might be fine, but your technical environment could be introducing unexpected issues. Many bugs that look like deep learning failures are really just environmental gremlins—this means that cleaning up your setup is often the fastest way forward. Here are some tips to keep in mind:
   </p>
   <dl>
    <dt>Sort out determinism and reproducibility</dt>
    <dd>It’s easier to isolate issues if your experiments are reproducible. In addition to turning off stochastic components like dropout, consider:
      <dl>
     <dt>Setting explicit random seeds</dt>
     <dd>For reproducibility in JAX, you need to set the seed for <code>jax.random.PRNGKey(...)</code>—this controls all randomness in model initialization, dropout, and other JAX-based operations. Note that you don’t need to set Python’s <code>random.seed(...)</code> or NumPy’s <code>np.random.seed(...)</code> unless you’re also using them separately (e.g., in your data preprocessing or non-JAX components).</dd>
     <dt>Turning off dataset shuffling</dt>
     <dd>Don’t shuffle your training data, or shuffle with a fixed random seed, to maintain a consistent order of examples across runs.</dd>
     <dt>Keeping the environment constant</dt>
     <dd>Avoid inconsistencies caused by external factors (e.g., use the same hardware, library versions, and configurations).</dd></dl>
    </dd>
    
    <dt>Strip down your training loop</dt>
    <dd>Especially in JAX and Flax, training loops require manual control over RNGs, state, and updates—making them powerful but easy to get wrong.
    <dl>
     <dt>Train on a single batch for just a few steps</dt>
     <dd>This is often enough to catch major issues.</dd>
     <dt>Disable extras like logging, metrics, or learning rate schedules</dt>
     <dd>These can obscure what’s actually happening during training.</dd>
     <dt>Use fixed inputs and random seeds</dt>
     <dd>Run your training step on the same batch every time. This eliminates variability due to changing data and makes bugs easier to isolate.</dd>
     <dt>Avoid high-level abstractions temporarily</dt>
     <dd>If you’re using tools like <code>TrainState</code>, try replacing them with raw variable updates until the core logic works.</dd>
    </dl>
    </dd>
    <dt>Make your code self-contained</dt>
    <dd>Especially when working in interactive environments like Colab or Jupyter notebooks, it’s easy for your environment to get cluttered with old variables or states without you realizing it. Restarting the kernel and organizing your code into self-contained functions can really help with debugging.</dd>
    <dt>Turn off JIT compilation (with caution)</dt>
    <dd>Disabling <code>@jax.jit</code> can help identify issues in your training step more easily by making stack traces clearer and behavior more explicit. However, be aware: turning off JIT can cause huge memory usage spikes and drastically slower execution, making this approach impractical for larger runs.</dd>
    <dt>Train on a single GPU instead of multiple GPUs</dt>
    <dd>If you are using GPUs, start with just one. Multi-GPU training via sharding (e.g., with <code>jax.pjit</code>) introduces a lot of additional complexity. (Note: <code>pmap</code> is being deprecated in favor of <code>pjit</code>, so it’s best not to rely on it.)</dd>
    <dt>Use CPU for simpler debugging setups</dt>
    <dd>If your model is small and you’re trying to isolate a bug, using CPU can remove the complexity of device placement and driver issues. It’s slower, but often easier to reason about. Just be aware that some bugs may appear only on accelerators.</dd>
    <dt>Go back to NumPy</dt>
    <dd>While <code>jax.numpy</code> mimics NumPy closely, the original NumPy library has simpler internals and often better error messages. You can’t train models or use autodiff, but for testing data transformations or verifying calculations, it can be useful to isolate and debug numpy-only code outside of JAX.
</dd>
   </dl>
   <p>These steps may feel tedious, but a clean and controlled environment is often the difference between spinning your wheels for days and finding a bug in 10 minutes.</p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>A quick note: different hardware backends behave slightly differently when running JAX code, especially when it comes to reproducibility.</p>
    <ul>
     <li><p><em>TPUs</em> are fully deterministic in JAX. If you use the same random seed, you’ll get the same results every time.</p></li>
     <li><p><em>GPUs</em> are mostly deterministic, but some operations (like convolutions or matrix multiplications) may vary slightly across runs unless you disable certain performance optimizations.</p></li>
     <li><p><em>CPUs</em> are generally deterministic, but subtle sources of non-determinism (like thread scheduling) can still appear.</p></li></ul>
     <p>For early debugging in JAX, running on CPU can simplify things—but just be aware that bugs might show up differently when you move to GPU or TPU.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html8" data-type="indexterm" id="id1039"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html7" data-type="indexterm" id="id1040"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html6" data-type="indexterm" id="id1041"/></p>
    
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Simplify the Data and Problem"><div class="sect2" id="simplify-the-data-and-problem">
   <h2>Simplify the Data and Problem</h2>
   <p><a contenteditable="false" data-primary="data" data-secondary="simplifying as debugging tactic" data-type="indexterm" id="ch07_tips.html9"/><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="data and problem" data-type="indexterm" id="ch07_tips.html10"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="data and problem" data-type="indexterm" id="ch07_tips.html11"/>Make your dataset and prediction task easier to simplify debugging. Here are some ways to make the data more manageable during early experimentation:
   </p>
   <dl>
   <dt>Visualize individual examples</dt>
   <dd>Actually plot or print raw inputs and labels—not just summaries. You’ll often catch issues like incorrect encodings, off-by-one errors, or mismatched image-label pairs this way. It’s surprisingly tempting to skip this step—don’t. Simply looking at the raw data can reveal issues that save you hours later.</dd>
   <dt>Check class balance</dt>
   <dd>An imbalanced dataset can make the model appear broken when it’s just doing the naive thing (predicting the majority class). Consider subsampling or rebalancing during debugging.</dd>
   <dt>Remove data augmentation</dt>
   <dd>Augmentations like cropping, flipping, or adding noise can hide underlying issues or make the task unnecessarily hard. Turn them off until you’re confident the core pipeline works.</dd>
   <dt>Reduce the number of classes</dt>
   <dd>Instead of predicting many categories, reframe your task as binary classification to focus on the clearest signal first.
</dd>
<dt>Simplify the output space</dt>
<dd>On a similar note, if your target is complex (e.g., a regression label or structured output), try reducing it to something simpler. For instance, predict a binary class or thresholded version of the label instead. This can help verify the pipeline before tackling the full problem.</dd>
<dt>Make your dataset smaller</dt>
<dd>Large datasets slow everything down. Use a small, representative subset that captures the key structure.</dd>
<dt>Limit the scope of your data</dt>
<dd>Use a natural slice of your dataset. For example, restrict to a single species, tissue type, year, or patient group. This cuts variability and helps isolate bugs.</dd>
<dt>Check for label leakage</dt>
<dd>Especially in biological datasets, label leakage can creep in through metadata like patient ID, batch number, or experiment date. This can cause your model to perform suspiciously well by learning shortcuts. Double-check that no features or splits accidentally leak target-related information.</dd>
<dt>Work with synthetic or simulated data</dt>
<dd>If feasible, start with synthetic data that mimics key characteristics of your real dataset but is easier to understand and trace. You can also add controlled noise (e.g., <code>np.random.normal(0, 1)</code>) to test the model’s robustness.</dd>
   </dl>
   <p>Simplifying the data isn’t about giving up—it’s about creating a controlled testbed where you can debug quickly, eliminate uncertainty, and build confidence before scaling back up.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html11" data-type="indexterm" id="id1042"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html10" data-type="indexterm" id="id1043"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html9" data-type="indexterm" id="id1044"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Overfit to a Single Batch of Data"><div class="sect2" id="overfit-to-a-single-batch-of-data">
   <h2>Overfit to a Single Batch of Data</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="overfitting to single batch of data" data-type="indexterm" id="ch07_tips.html12"/><a contenteditable="false" data-primary="overfitting" data-secondary="debugging" data-type="indexterm" id="ch07_tips.html13"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="overfitting to single batch of data" data-type="indexterm" id="ch07_tips.html14"/>We’ve mentioned this briefly before, but it’s worth stating explicitly: if your model doesn’t seem to be learning much at all, a classic debugging strategy is to try to overfit to a single batch—meaning, rerun the training loop on the same batch repeatedly and see if the model can memorize it. This test helps confirm that the training loop, loss function, and optimizer are wired up correctly.
   </p>
   <p>Instead of the usual training loop:
   </p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">):</code>
  <code class="n">batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">training_data</code><code class="p">)</code>
  <code class="n">state</code><code class="p">,</code> <code class="n">loss</code> <code class="o">=</code> <code class="n">update_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
</pre>
    
   
   <p>you can try this:
   </p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="n">num_debugging_steps</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">training_data</code><code class="p">)</code>

<code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_debugging_steps</code><code class="p">):</code>
  <code class="n">state</code><code class="p">,</code> <code class="n">loss</code> <code class="o">=</code> <code class="n">update_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Loss at step </code><code class="si">{</code><code class="n">step</code><code class="si">}</code><code class="s2">: </code><code class="si">{</code><code class="n">loss</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>
    
   
   <p>If your setup is working correctly, the model should rapidly memorize the batch, and the loss should decrease significantly after a few steps. If not, consider checking for:
   </p>
   <dl>
   <dt>Learning rate issues:</dt>
    <dd>
     <p>The learning rate might be too high (causing divergence) or too low (causing no learning).
     </p>
    </dd>
    <dt>Frozen parameters or bad gradients</dt>
    <dd>
     <p>Sometimes parameters are not being updated at all—for example, if they were accidentally excluded from the <code>params</code> dict due to naming mismatches or scoping issues. Also inspect gradients—if they’re all zero or NaN, that’s a clue.
     </p>
    </dd>
    <dt>Loss function bugs</dt>
    <dd>
     <p>Make sure you’re using the correct loss function for your task (e.g., cross-entropy for classification) and that it’s behaving numerically as expected. Prefer standard implementations over custom home-made ones, at least during debugging.
     </p>
    </dd>
    <dt>Model initialization problems</dt>
    <dd><p>Poor or inconsistent weight initialization can prevent learning, especially in deeper networks. If you’re using custom modules, double-check their initializations.</p></dd>
    <dt>Batch size too small</dt>
    <dd>Very small batches (e.g., 1–2 examples) can lead to noisy gradients and unstable updates. For debugging, use a small but reasonable batch size like 8–32.</dd>
    <dt>Silent shape mismatches or broadcasting errors</dt>
    <dd>These won’t always crash your code, but they can silently mess up your loss or gradients. Print tensor shapes and inspect intermediate outputs to confirm everything lines up as expected.</dd>
   </dl>
   <p>This is one of the fastest and most informative debugging steps—if your model can’t learn a tiny batch, don’t bother scaling up yet.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html14" data-type="indexterm" id="id1045"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html13" data-type="indexterm" id="id1046"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html12" data-type="indexterm" id="id1047"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Go Back to Basics"><div class="sect2" id="go-back-to-basics">
   <h2>Go Back to Basics</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="going back to basics" data-type="indexterm" id="id1048"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="going back to basics" data-type="indexterm" id="id1049"/>If none of these debugging tips works and you are rapidly losing your mind, one of the most effective strategies is to revert to a simple, well-understood example that you know works:
   </p>
   <dl>
    <dt>Start with an established example</dt>
    <dd>
     <p>Train a simple model on a well-known dataset—like a basic CNN on MNIST for image problems. These examples are widely used and well-documented, making them a reliable way to get something working end-to-end.
     </p>
    </dd>
    <dt>Reproduce known results</dt>
    <dd>
     <p>Make sure your setup can successfully train the model and reach the expected performance (e.g., ~99% accuracy on MNIST). This confirms your training loop, model, and loss function are functioning correctly.
     </p>
    </dd>
    <dt>Swap in your dataset</dt>
    <dd>
     <p>Once the baseline works, begin replacing the dataset with your own. Proceed gradually and check that everything still functions as expected.
     </p>
    </dd>
    <dt>Iteratively add complexity</dt>
    <dd>With your data integrated, incrementally introduce more complex components—like deeper architectures or new training strategies. Watch for breakage after each change.</dd>
  </dl>
  <p>There’s absolutely nothing wrong with going back to a tutorial and training a simple linear model—sometimes that’s the fastest way to confirm your setup and get your bearings.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Log Everything"><div class="sect2" id="log-everything">

<h2>Log Everything</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="logging everything" data-type="indexterm" id="id1050"/><a contenteditable="false" data-primary="logging" data-type="indexterm" id="id1051"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="logging everything" data-type="indexterm" id="id1052"/>Good logging is the difference between productive debugging and blindly guessing. When something goes wrong, clear logs can help you trace back exactly what happened—and when things go right, they help you understand why.</p>
   <dl>
   <dt>Log training loss and key metrics over time</dt>
   <dd>At a minimum, track loss, accuracy, and any relevant task-specific metrics (like auROC or auPRC). This makes it easier to spot overfitting, instability, or underperformance.</dd>
   <dt>Log validation performance at regular intervals</dt>
   <dd>Seeing how your model generalizes during training helps detect overfitting early and can catch bugs where validation performance diverges for no obvious <span class="keep-together">reason</span>.</dd>
   <dt>Log inputs, predictions, and errors</dt>
   <dd>Save a few input samples, predicted outputs, and errors at each step (or epoch). This is especially useful for spotting systematic failures (e.g., always misclassifying a certain class).</dd>
   <dt>Record configuration and hyperparameters</dt>
   <dd>Save the learning rate, batch size, optimizer type, and model architecture along with each run. You will forget. Everyone forgets.</dd>
   <dt>Use a structured logger or tracking tool</dt>
   <dd>Tools like TensorBoard, Weights &amp; Biases, or even just structured JSON logs can make it easier to compare runs and understand what changed.</dd>
   </dl>
   <div data-type="tip"><h6>Tip</h6>
    <p>Logging might feel like overhead in the moment, but it’s one of the best time investments you can make. Even minimal logs can help you debug faster and avoid retraining unnecessarily.</p>
    <p>There’s a rule of thumb: every new log reveals a bug you didn’t know you had.</p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Ask for Help"><div class="sect2" id="ask-for-help">
   <h2>Ask for Help</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="simplifying" data-tertiary="asking for help" data-type="indexterm" id="id1053"/><a contenteditable="false" data-primary="help, asking for" data-type="indexterm" id="id1054"/><a contenteditable="false" data-primary="simplifying, as debugging strategy" data-secondary="asking for help" data-type="indexterm" id="id1055"/>If you’re still stuck, don’t be afraid to reach out. Community forums like Stack Overflow or GitHub Discussions are valuable resources. Talking to a colleague or friend can also help—sometimes just explaining the problem out loud (rubber ducking) leads to breakthroughs.
   </p>
   <p>You can also use LLMs like ChatGPT, Gemini, or Claude to help troubleshoot or explore ideas. Just remember that while these models can be very helpful, their suggestions aren’t always correct and can introduce new bugs—so double-check any code they generate.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html2" data-type="indexterm" id="id1056"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html1" data-type="indexterm" id="id1057"/></p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Common Data Issues"><div class="sect1" id="common-data-issues">
  <h1>Common Data Issues</h1>
  <p><a contenteditable="false" data-primary="data" data-secondary="debugging common data issues" data-type="indexterm" id="ch07_tips.html15"/><a contenteditable="false" data-primary="debugging" data-secondary="common data issues" data-type="indexterm" id="ch07_tips.html16"/>Often, it’s not your model that has a bug—it’s the data. Subtle issues in your dataset can quietly break your learning pipeline, and you may find yourself spending hours debugging the model setup when the problem is actually upstream. This section covers common data pitfalls that are worth checking before tearing your architecture apart.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Data Leakage"><div class="sect2" id="data-leakage">
   <h2>Data Leakage</h2>
   <p><a contenteditable="false" data-primary="data" data-secondary="debugging common data issues" data-tertiary="data leakage" data-type="indexterm" id="ch07_tips.html17"/><a contenteditable="false" data-primary="data leakage, debugging" data-type="indexterm" id="ch07_tips.html18"/><a contenteditable="false" data-primary="debugging" data-secondary="common data issues" data-tertiary="data leakage" data-type="indexterm" id="ch07_tips.html19"/>As physicist Richard Feynman famously said, “The first principle is that you must not fool yourself—and you are the easiest person to fool.” It’s all too easy to believe your model is doing well when it’s actually cheating. Data leakage happens when information that should be hidden during training is inadvertently accessible to the model, leading to overly optimistic performance metrics.
   </p>
   <ul>
    <li>Obvious cases
     <ul>
      <li>Evaluating the model on the training set itself. This sounds slightly silly, but it’s surprisingly common—especially in informal settings like Kaggle notebooks.</li>
      <li>Evaluating on a validation or test set where some examples overlap with the training set.</li>
     </ul>
    </li>
    <li>Subtle cases
     <ul>
      <li>Leakage through preprocessing, for example, normalizing the full dataset before splitting into train/valid/test sets.</li>
      <li>Features that leak future information—correlated with the target only because they wouldn’t be available at prediction time.</li>
     </ul>
    </li>
  </ul>
   <p>Here are some real-world examples of subtle leakage:</p>
   <ul>
    <li>You want to classify skin lesions as malignant or benign. Patients with cancer are photographed in one clinic and healthy patients in a different clinic. If one clinic uses brighter lighting, your model may learn to use brightness as a proxy for cancer.</li>
    <li>You’re predicting protein binding to a gene, and you include gene expression level as an input to the model. Since a gene’s expression may be affected by binding, your model might learn to rely on this proxy instead of the DNA sequence features you intended it to learn.</li>
   </ul>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>If your model performs well on a test set but fails to generalize to new datasets or real-world settings, data leakage is one of the first things to investigate.
    </p>
   </div>
   <p>
    To avoid data leakage:
   </p>
   <ul class="simple">
    <li>
     <p>Always ensure that test data is fully isolated and untouched by the training pipeline.
     </p>
    </li>
    <li>
     <p>When adding new training data partway through a project, check whether it already appears in your validation or test sets.
     </p>
    </li>
    <li class="pagebreak-before less_space">
     <p>Ask yourself: <em>Would this feature be available at inference time?</em> If not, don’t use it.
     </p>
    </li>
    <li>
     <p>Use model interpretation tools to see what aspects of your data your model is relying on when making its predictions. Does what you see match your expectations, or is the model picking up on artifacts?<a contenteditable="false" data-primary="" data-startref="ch07_tips.html19" data-type="indexterm" id="id1058"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html18" data-type="indexterm" id="id1059"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html17" data-type="indexterm" id="id1060"/>
     </p>
    </li>
   </ul>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Incorrect Data Labels"><div class="sect2" id="incorrect-data-labels">
   <h2>Incorrect Data Labels</h2>
   <p><a contenteditable="false" data-primary="data" data-secondary="debugging common data issues" data-tertiary="incorrect data labels" data-type="indexterm" id="id1061"/><a contenteditable="false" data-primary="debugging" data-secondary="common data issues" data-tertiary="incorrect data labels" data-type="indexterm" id="id1062"/><a contenteditable="false" data-primary="incorrect data labels, debugging" data-type="indexterm" id="id1063"/>It seems obvious, but incorrectly labeled data is one of the most common and frustrating causes of model underperformance. Some high-risk scenarios include:
   </p>
   <dl>
    <dt>Labels stored separately from inputs</dt>
    <dd>
     <p>If labels are in a separate file (e.g., a CSV with filenames and classes), they can easily be mismatched or misjoined during preprocessing. We certainly have several shameful anecdotes along these lines.
     </p>
    </dd>
    <dt>Shuffling inputs and labels independently</dt>
    <dd>
     <p>If you shuffle data and labels separately, they’ll fall out of sync—silently.
     </p>
    </dd>
    <dt>Shape mismatches in TensorFlow datasets:</dt>
    <dd>
     <p><code>tf.data.Dataset</code> won’t necessarily complain if your labels and inputs have mismatched shapes like data of shape <code>(100, 10)</code> but labels of shape <code>(43,)</code>. This can result in silent failures that only manifest much later.
     </p>
    </dd>
    <dt>Merging tabular datasets incorrectly</dt>
    <dd>
     <p>Joining datasets without verifying alignment (e.g., via <code>merge</code> in pandas) can mislabel data rows without throwing errors.
     </p>
    </dd>
    <dt>Data augmentation pipelines modifying labels incorrectly</dt>
    <dd>
     <p>Augmentation effectively increases your dataset—so it’s also a high-risk area for introducing label corruption.
     </p>
    </dd>
  </dl>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>A common warning sign of label issues: the training loss does goes down slightly during training but plateaus early at a high value, and accuracy (or other metrics) stays near random chance level.
    </p>
    <p>For example, if the labels are scrambled, your accuracy would hover around random baseline values like 50% in balanced binary classification.</p>
   </div>
   <p>The best antidote to label issues is simply spending time inspecting your input-label pairs, both at the beginning as raw data, and at different points in your data pipeline. Check a few batches by hand. Plot examples and verify the label. It may feel tedious—but it’s one of the fastest ways to catch silent bugs.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Imbalanced Classes"><div class="sect2" id="imbalanced-classes">
  <h2>Imbalanced Classes</h2>
   <p><a contenteditable="false" data-primary="data" data-secondary="debugging common data issues" data-tertiary="imbalanced classes" data-type="indexterm" id="id1064"/><a contenteditable="false" data-primary="debugging" data-secondary="common data issues" data-tertiary="imbalanced classes" data-type="indexterm" id="id1065"/><a contenteditable="false" data-primary="imbalanced classes, debugging" data-type="indexterm" id="id1066"/>In biology, it’s common for one class to vastly outnumber others—like detecting rare mutations or identifying diseased cells. This class imbalance is not an issue in itself, but a model trained on imbalanced data might just learn to always predict the majority class, achieving deceptively high accuracy.</p>
<p>Warning signs:</p>
<ul>
 <li>Accuracy is high, but precision or recall on the minority class is poor.</li>
 <li>Confusion matrix shows the model rarely predicts the minority class.</li>
</ul>
<p>To address this:</p>
<ul>
 <li>Use class weighting or focal loss to penalize the model more for mistakes on the minority class. Focal loss down-weights easy examples and focuses learning on hard, misclassified ones—especially useful when the rare class is easily overwhelmed.</li>
 <li>Resample the data—either oversample the minority class or undersample the majority class—to reduce imbalance. Oversampling is often safer when data is scarce but can lead to overfitting if not done carefully.</li>
 <li>Use stratified sampling to ensure class balance is preserved across your train, validation, and test splits. This means splitting the data so each subset maintains the original class proportions, avoiding skewed performance estimates.</li>
</ul>
   </div></section>
   <section data-type="sect2" data-pdf-bookmark="Distribution Shifts"><div class="sect2" id="distribution-shifts">
    <h2>Distribution Shifts</h2>
    <p><a contenteditable="false" data-primary="data" data-secondary="debugging common data issues" data-tertiary="distribution shifts" data-type="indexterm" id="id1067"/><a contenteditable="false" data-primary="debugging" data-secondary="common data issues" data-tertiary="distribution shifts" data-type="indexterm" id="id1068"/><a contenteditable="false" data-primary="distribution shifts, debugging" data-type="indexterm" id="id1069"/>Training and test data can often come from different sources—different labs, species, sequencing protocols, or imaging setups. These shifts can cause models to learn dataset-specific artifacts instead of generalizable biology.</p>
    <p>Warning signs:</p>
    <ul><li>Strong validation performance doesn’t transfer to real-world or external datasets.</li>
    <li>A model trained to predict dataset labels (e.g., lab or batch ID) performs surprisingly well.</li>
    </ul>
    <p>To catch and correct for this:</p>
    <ul>
     <li>Visualize embeddings (e.g., via PCA or UMAP) colored by data source to spot clustering.</li>
     <li>Use batch effect correction or domain adaptation methods if needed.</li>
     <li>Be cautious when mixing data from different sources—explicitly test generalization to new settings<a contenteditable="false" data-primary="" data-startref="ch07_tips.html16" data-type="indexterm" id="id1070"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html15" data-type="indexterm" id="id1071"/>.</li>
    </ul>

  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Biology-Specific Gotchas"><div class="sect1" id="biology-specific-gotchas">
  <h1>Biology-Specific Gotchas</h1>
  <p><a contenteditable="false" data-primary="debugging" data-secondary="biology-specific gotchas" data-type="indexterm" id="ch07_tips.html20"/>Biology is a vast field, filled with complex systems and ever-evolving datasets. We can’t cover every pitfall here, but the following are some common sources of bugs and errors that we’ve encountered repeatedly—and that are worth keeping in mind when building models on biological data:
  </p>
  <dl>
   <dt>Versioning issues</dt>
    <dd>Many biological datasets are tied to reference versions (e.g., genome builds, gene IDs, transcript annotations). It’s dangerously easy to mix up genome versions (e.g., GRCh37 vs GRCh38), gene ID versions, or even organism accessions. Make sure all parts of your pipeline are using consistent versions—or explicitly map between them.</dd>
    <dt>Data integration challenges</dt>
    <dd>Combining datasets from different sources is common in biology but can lead to subtle inconsistencies: mismatched identifiers, differing file formats, or incompatible measurement units (e.g., read counts versus TPM versus RPKM). Carefully check alignment before merging datasets.</dd>
    <dt>Biological heterogeneity</dt>
    <dd>Biological systems vary widely—across individuals, cell types, populations, and species. A model trained on European ancestry samples may not generalize to other ancestries. Likewise, models trained on data from immortalized cancer cell lines can fail when applied to normal primary cells. Always consider the scope and limitations of your training data.</dd>
    <dt>Ambiguous or soft labels</dt>
    <dd>Biological categories are often not cleanly defined: cell types can be graded or transitional, and protein binding is often a continuous score, not a binary yes/no. Hard labels, where present, may oversimplify what is actually a spectrum. In these cases, performance ceilings may reflect label ambiguity, not model failure.
</dd>
<dt>Experimental noise</dt>
<dd>Just adding more data isn’t always better—low-quality experimental data can introduce noise that overwhelms signal. Look for ways to filter or denoise.
 <dl>
  <dt>Use quality metrics</dt>
  <dd>Many experiments include built-in quality scores. Filtering on these can help.</dd>
  <dt>Leverage replicates</dt>
  <dd>Use experimental replicates (same exact setup, multiple times) or biological replicates (same protocol, but different samples) to reduce variance. You can average replicate signals or use them to quantify uncertainty.</dd>
 </dl>
</dd>
   <dt>Batch effects</dt>
   <dd>Differences in lab conditions, reagent lots, sequencing machines, or protocols can introduce strong confounding signals. These technical artifacts often dominate the true biological signal if not accounted for. Visualize your data (e.g., with PCA or UMAP colored by batch) to assess how much batches cluster apart. You can also train a model to predict batch labels—if it performs better on this than your actual task, your model is probably learning batch-specific noise. If needed, apply normalization techniques like quantile normalization to mitigate these effects.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html20" data-type="indexterm" id="id1072"/></dd>
  </dl>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Common Model Issues"><div class="sect1" id="common-model-issues">
  <h1>Common Model Issues</h1>
  <p><a contenteditable="false" data-primary="debugging" data-secondary="common model issues" data-type="indexterm" id="ch07_tips.html21"/><a contenteditable="false" data-primary="models, debugging" data-secondary="common issues" data-type="indexterm" id="ch07_tips.html22"/>Not all training failures come from bad data—sometimes, the model itself is the problem. In this section, we highlight common issues that arise during model training, from overfitting to gradient instability.</p>
<p>There are more detailed deep learning debugging guides out there—for example, the <a href="https://oreil.ly/-t96r">Deep Learning Tuning Playbook</a> by Google. But here we’ll recap some of the most common and practical failure modes, with a focus on how to identify and fix them efficiently.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Overfitting and Poor Generalization"><div class="sect2" id="overfitting-and-poor-generalization">
   <h2>Overfitting and Poor Generalization</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="common model issues" data-tertiary="overfitting/poor generalization" data-type="indexterm" id="id1073"/><a contenteditable="false" data-primary="generalization" data-secondary="debugging poor generalization" data-type="indexterm" id="id1074"/><a contenteditable="false" data-primary="models, debugging" data-secondary="common issues" data-tertiary="overfitting/poor generalization" data-type="indexterm" id="id1075"/><a contenteditable="false" data-primary="overfitting" data-secondary="debugging" data-type="indexterm" id="id1076"/>Overfitting is one of the most common issues in deep learning. Deep neural networks typically have high capacity and are only told to minimize training loss—without any built-in notion of generalization. As a result, they often perform well on training data but poorly on unseen data.
   </p>
   <p>Fortunately, there’s a well-established set of regularization techniques to help reduce overfitting—many of which we’ve already discussed throughout this book:</p>
   <dl>
    <dt>Dropout</dt>
    <dd>Randomly disables units in the network during training to prevent over-reliance on any one path.</dd>
    <dt>Weight decay</dt>
    <dd>Penalizes large weights (L1 or L2 regularization) to encourage simpler models that generalize better.</dd>
    <dt>Early stopping</dt>
    <dd>Monitors validation performance and stop training when performance starts to degrade, even if training loss continues to drop.</dd>
    <dt>Data augmentation</dt>
    <dd>Expands your dataset by applying small, meaningful transformations (e.g., rotations, flips in images; jittering or cropping in sequences).</dd>
    <dt>Ensembling</dt>
    <dd>Combines predictions from multiple models trained with different seeds or splits as a form of error correction. Even ensembling the same architecture can significantly improve robustness.</dd>
   </dl>
<p>Also, check whether your validation or test data differs fundamentally from your training data (see the previous section). If the distribution has genuinely shifted, then the model might not be overfitting so much as encountering data it was never trained to handle.</p>
<div data-type="note" epub:type="note"><h6>Note</h6><p>A model that performs well on training data but poorly on validation is likely overfitting. A model that performs poorly on both might be underfitting or struggling with a broken setup.</p></div>

  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Vanishing or Exploding Gradients"><div class="sect2" id="vanishing-or-exploding-gradients">
   <h2>Vanishing or Exploding Gradients</h2>
   <p><em><a contenteditable="false" data-primary="debugging" data-secondary="common model issues" data-tertiary="vanishing/exploding gradients" data-type="indexterm" id="ch07_tips.html23"/><a contenteditable="false" data-primary="exploding gradients, debugging" data-type="indexterm" id="ch07_tips.html24"/><a contenteditable="false" data-primary="models, debugging" data-secondary="common issues" data-tertiary="vanishing/exploding gradients" data-type="indexterm" id="ch07_tips.html25"/><a contenteditable="false" data-primary="vanishing gradients" data-type="indexterm" id="ch07_tips.html26"/>Vanishing gradients</em>, where gradient values approach zero, and exploding gradients, where they become excessively large, can severely disrupt training. Fortunately, these issues are relatively easy to detect.</p>
<p>A simple way to monitor gradients is to compute their L2 norm (also called the Euclidean norm), which summarizes the overall magnitude of the gradient as a single scalar. You can log this value alongside the loss during training.
   </p>
   <p>Here’s how to compute the L2 norm of gradients in Flax:</p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">compute_gradients_l2_norm</code><code class="p">(</code><code class="n">grads</code><code class="p">):</code>
  <code class="sd">"""Compute L2 norm of gradients."""</code>
  <code class="n">grads_flat</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_util</code><code class="o">.</code><code class="n">tree_leaves</code><code class="p">(</code><code class="n">grads</code><code class="p">)</code>  <code class="c1"># Flatten.</code>
  <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="nb">sum</code><code class="p">([</code><code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">g</code><code class="p">))</code> <code class="k">for</code> <code class="n">g</code> <code class="ow">in</code> <code class="n">grads_flat</code><code class="p">]))</code>

<code class="c1"># Example usage inside a training step:</code>
<code class="n">loss</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">loss_fn</code><code class="p">)(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">)</code>
<code class="n">grad_norm</code> <code class="o">=</code> <code class="n">compute_gradients_l2_norm</code><code class="p">(</code><code class="n">grads</code><code class="p">)</code>
</pre>
    
   
   <p class="pagebreak-before">You can log this <code>grad_norm</code> over time and visualize it alongside the loss to examine gradient behavior:
   </p>
   <ul>
    <li><p>If <code>grad_norm</code> is close to 0, gradients are likely vanishing.</p></li>
   <li><p>If it grows rapidly or spikes erratically, you may be seeing exploding gradients.</p></li>
   </ul>
   <p>Common fixes to try out include:</p>
  <ul>
    <li><p>Lower the learning rate or use a learning rate schedule.</p></li>
    <li><p>Use better weight initializers: try Xavier (Glorot) or He initialization, depending on your activation function.</p></li>
    <li><p>Normalize activations: Batch normalization or layer normalization helps stabilize the flow of gradients.</p></li>
    <li><p>Add residual connections: These help gradients propagate through deep networks without degradation.</p></li>
    <li><p>Clip gradients: This is a blunt but effective tool to cap extreme values and prevent instability.</p></li>
    </ul>
   <p>The fixes to this issue tend to be ones we’ve already mentioned, like reducing the learning rate or using a learning schedule, using different weight initializers, and adding either batch normalization or layer normalization. Adding residual connections between blocks can also be helpful. Finally, explicitly clipping gradients to a fixed threshold to avoid excessive values might sound like a really crude approach but is common and very effective. Here is an example implementation:
   </p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">clip_gradients</code><code class="p">(</code><code class="n">grads</code><code class="p">,</code> <code class="n">threshold</code><code class="p">):</code>
  <code class="sd">"""Clip gradients."""</code>
  <code class="k">return</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">g</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">clip</code><code class="p">(</code><code class="n">g</code><code class="p">,</code> <code class="o">-</code><code class="n">threshold</code><code class="p">,</code> <code class="n">threshold</code><code class="p">),</code> <code class="n">grads</code><code class="p">)</code>
</pre>
    
   
   <p>Or, if you are using <code>optax</code>, you can also clip gradients with:<a contenteditable="false" data-primary="" data-startref="ch07_tips.html26" data-type="indexterm" id="id1077"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html25" data-type="indexterm" id="id1078"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html24" data-type="indexterm" id="id1079"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html23" data-type="indexterm" id="id1080"/>
   </p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="n">tx</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">chain</code><code class="p">(</code><code class="n">optax</code><code class="o">.</code><code class="n">clip</code><code class="p">(</code><code class="n">threshold</code><code class="p">),</code> <code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">))</code>
</pre>
    
   
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Training Instability"><div class="sect2" id="training-instability">
   <h2>Training Instability</h2>
   <p>
    <a contenteditable="false" data-primary="debugging" data-secondary="common model issues" data-tertiary="training instability" data-type="indexterm" id="id1081"/><a contenteditable="false" data-primary="models, debugging" data-secondary="common issues" data-tertiary="training instability" data-type="indexterm" id="id1082"/><a contenteditable="false" data-primary="training" data-secondary="debugging training instability" data-type="indexterm" id="id1083"/><a contenteditable="false" data-primary="training instability, debugging" data-type="indexterm" id="id1084"/>A related issue to gradient issues is <em>training instability</em>, which can manifest in several ways, including erratic training losses, sudden spikes in validation loss, or even full-blown divergence. By <em>divergence</em>, we mean that the model fails to <em>converge</em> toward a stable solution; instead, the loss may oscillate wildly or become <code>NaN</code>.</p> 
     <p class="pagebreak-before">Training instability typically arises from a few common causes:
   </p>
   <dl>
    <dt>Learning rate is too high</dt>
    <dd>
     <p>A high learning rate can cause the optimizer to overshoot minima, leading to instability. Try lowering the learning rate or using a warmup schedule that starts small and ramps up gradually.
     </p>
    </dd>
    <dt>Using a nonadaptive optimizer</dt>
    <dd>
     <p>Adaptive optimizers like Adam, RMSProp, or Adagrad adjust learning rates per parameter and tend to be more robust out-of-the-box. While vanilla stochastic gradient descent (SGD) can be effective, it typically requires more careful tuning, especially with larger models or noisy data.
     </p>
    </dd>
    <dt>Exploding gradients</dt>
    <dd>
     <p>In deep networks, gradients can grow too large and destabilize updates. As discussed earlier, apply <em>gradient clipping</em> or use normalization layers (like batch norm or layer norm) to control this.
     </p>
    </dd>
    <dt>Inappropriate batch size</dt>
    <dd>
     <p>Very small batches can lead to noisy gradient estimates that make training unstable. Larger batches offer more stable gradients—generally, try using the largest size your hardware allows, especially during early debugging.
     </p>
    </dd>
    <dt>Poor weight initialization</dt>
    <dd>
     <p>Improper initialization can cause gradients to vanish or explode. Flax uses LeCun normal as the default initializer for <code>nn.Dense</code> and <code>nn.Conv</code>, which works well with ReLU activations. But for very deep networks or specific architectures, Xavier or He initialization may perform better.
     </p>
    </dd>
    <dt>Activation blowup</dt>
    <dd>
     <p>As networks deepen, intermediate activations can grow excessively large, especially with ReLUs or unnormalized inputs. To prevent this, keep activations centered and bounded, most commonly by applying batch normalization.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html22" data-type="indexterm" id="id1085"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html21" data-type="indexterm" id="id1086"/>
     </p>
    </dd>
  </dl>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Poor Model Performance"><div class="sect1" id="poor-model-performance">
  <h1>Poor Model Performance</h1>
  <p><a contenteditable="false" data-primary="debugging" data-secondary="poor model performance" data-type="indexterm" id="ch07_tips.html27"/><a contenteditable="false" data-primary="models, debugging" data-secondary="poor model performance" data-type="indexterm" id="ch07_tips.html28"/>The model trains and the dataset looks good. You’ve squashed overfitting. Everything generally looks sane. The only problem is that the model is just not that good.
  </p>
  <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="How Well Should You Do?"><div class="sect2" id="how-well-should-you-do">
   <h2>How Well Should You Do?</h2>
   <p><a contenteditable="false" data-primary="debugging" data-secondary="poor model performance" data-tertiary="determining realistic performance" data-type="indexterm" id="id1087"/><a contenteditable="false" data-primary="models, debugging" data-secondary="poor model performance" data-tertiary="determining realistic performance" data-type="indexterm" id="id1088"/>We touched on this point in the introduction, but it’s worth restating: to judge performance, you need context. Here are some ways to anchor your expectations:
   </p>
   <dl>
    <dt>Random chance performance</dt>
    <dd>
     <p>What would random guessing achieve? For regression, how well would you do by always predicting the mean or median of the training set?
     </p>
    </dd>
    <dt>Baseline models</dt>
    <dd>
     <p>Try a simple linear regression or logistic regression. Sometimes these models perform surprisingly well—and if your deep model doesn’t beat them, something’s wrong.
     </p>
    </dd>
    <dt>Other published models</dt>
    <dd>
     <p>If others have worked on this task, check what performance they report. You can often get architectural or preprocessing ideas from their work. But beware—published metrics aren’t always trustworthy, and they may not be directly comparable to your setup.
     </p>
    </dd>
    <dt>Human performance</dt>
    <dd>
     <p>Can a human do this task? How well would an expert do it? This can help you calibrate expectations.
     </p>
    </dd>
    <dt>Experimental replicates</dt>
    <dd><p>Biological measurements often contain noise due to sampling variability, measurement error, or biological variability itself. One way to estimate the ceiling for your model’s performance is to check how consistent the raw signal is across replicate experiments. If two replicates have a correlation of 0.85, your model is unlikely to exceed that. Don’t expect your model to be more consistent than biology itself.</p></dd>
  </dl>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Addressing Poor Model Performance"><div class="sect2" id="addressing-poor-model-performance">
   <h2>Addressing Poor Model Performance</h2>
   <p>
    <a contenteditable="false" data-primary="debugging" data-secondary="poor model performance" data-tertiary="addressing" data-type="indexterm" id="id1089"/><a contenteditable="false" data-primary="models, debugging" data-secondary="poor model performance" data-tertiary="addressing" data-type="indexterm" id="id1090"/>While there’s no universal fix for a model that just isn’t performing well, the following strategies can help identify what’s wrong and suggest paths forward:
   </p>
   <dl>
    <dt>Check data quality</dt>
    <dd>
     <p>Many model issues are actually data issues. Dive into specific examples—especially ones the model gets wrong—and look for inconsistencies, noise, or labeling errors. If the data is highly domain specific and you’re not an expert, ask someone who is.
     </p>
    </dd>
    <dt>Run error analysis</dt>
    <dd>
     <p>Where is the model doing well? Where does it consistently fail? Are there patterns to its mistakes—specific classes, edge cases, or confounding conditions? Systematic errors often point to missing features or broken assumptions.
     </p>
    </dd>
    <dt>Add more data</dt>
    <dd>
     <p>More data can help if the model is underfitting or struggling with rare cases. You can also try synthetic augmentation, bootstrapping from known examples, or generating simulations. Watch how performance scales with dataset size—plateaus may indicate other bottlenecks.
     </p>
    </dd>
    <dt>Tune hyperparameters</dt>
    <dd>
     <p>Some hyperparameters matter more than others—start with learning rate, batch size, model depth, and regularization strength. Use grid or random search over a small range to find better-performing settings.
     </p>
    </dd>
    <dt>Try transfer learning</dt>
    <dd>
     <p>If similar datasets or tasks exist, use pretrained models as a starting point. You can either fine-tune the whole model or freeze its feature extractor and train a smaller model on top. Alternatively, use learned embeddings from a related model as input features<a contenteditable="false" data-primary="" data-startref="ch07_tips.html28" data-type="indexterm" id="id1091"/><a contenteditable="false" data-primary="" data-startref="ch07_tips.html27" data-type="indexterm" id="id1092"/>.<a contenteditable="false" data-primary="" data-startref="ch07_tips.html0" data-type="indexterm" id="id1093"/>
     </p>
    </dd>
  </dl>

  <div data-type="tip"><h6>Tip</h6>
<p>As always, if you’re stuck, revisit the basics: simplify the model, overfit a single batch, sanity-check your labels, and compare against baseline performance. Many of the strategies that help fix broken models can also clarify why a working model isn’t yet a good one.</p>
</div>
  </div></section>
 </div></section>
<section data-type="sect1" data-pdf-bookmark="Final Thoughts"><div class="sect1" id="Final_thoughts">
   <h1>Final Thoughts</h1>
   <p>Deep learning in biology is hard. The data is messy, the goals are often open-ended, and training useful models can be finicky. But that’s exactly what makes it exciting. With every experiment, you’re not just solving a technical challenge—you’re helping push the boundaries of how we understand life itself.</p>
<p>The journey won’t always be smooth—models will fail in surprising ways, you’ll write catastrophic bugs, and the data will contain monumental errors. But if you stay curious, keep things modular, simplify when in doubt, and stay patient, you’ll find your way through.</p>
<p>Whether you’re building models to decode genomes, predict protein structures, or interpret microscopy images, we hope this book has helped you approach the work more confidently—and maybe even enjoy the process a little more.</p>
<p>Good luck, and keep going!</p>
   </div></section>
</div></section></div></div></body></html>