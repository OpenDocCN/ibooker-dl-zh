- en: 10 Reducing complexity with generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Designing and improving process flows with generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing disambiguation dialogue flows with LLM judgments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing static dialogue flows with generative AI as the “user”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s difficult to design a process-oriented bot that meets all the needs and
    desires of all stakeholders. Competing priorities may lead to a “design by committee”
    that introduces complexity. And well-meaning people can design edge cases that
    hamper the main dialogue flow. These complexities burden your users and make them
    more likely to quit or fail when using the bot. Generative AI can help you detect
    and improve these scenarios, helping you remove complexity and increase the successfulness
    of your bot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Process flow builders often ask for too much information from the user. (More
    information is better, right? Not if it causes the chatbot to fail!) There are
    several ways to improve process flows with generative AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Use generative AI to make suggestions about how to build a process flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your process flow is built, use generative AI to suggest improvements. It
    can also test the flow by acting as the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace some static process flows with a large language mode (LLM)–driven process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll start by exploring a claim status process flow for a medical insurance
    provider. Then we’ll see how generative AI can help us design and improve this
    process flow and others.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 AI-assisted process flows at build time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Figure 10.1 shows the simplest possible view of a process flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F01_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 A high-level view of a process flow. It is initiated by the recognition
    of a specific intent, it includes one or more sequential steps, and it ends with
    completion of the process flow (satisfying the intent).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Our example process flow involves medical insurance customers who call into
    a chatbot to find the status of a claim. At first, this process sounds like a
    simple lookup, but it has several criteria to meet:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Intent detection*—Figure out that the user’s intent is “claim status.” This
    initiates a process flow with multiple steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Beginning of process flow*—Gather the information required to complete the
    claim status process: in this case, the information needed to search for a claim.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Middle of process flow*—Use the gathered information to perform some action.
    In this example, that is searching for the user’s claim.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*End of process flow*—Complete the flow by providing the claim status to the
    user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall claims process flow is shown in figure 10.2.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 5, we showed how you could improve a chatbot’s intent classifier
    to *detect* and *understand* the user’s intent. In this chapter, we’ll focus on
    improving the rest of the process flow to successfully *fulfill* the user’s intent.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F02_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Visualizing a claim status process flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 10.1.1 Generating dialogue flows with generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conversational AI process flows are often based on an existing workflow. That
    flow could be copied from another channel, from a web application, or from a call
    center script. For our claim status example, let’s assume there was no existing
    process to work from. We can use an LLM to help us design the target workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows an example LLM prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Prompt to design a medical insurance claims status flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Scenario and background'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Detailed instructions'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output cue'
  prefs: []
  type: TYPE_NORMAL
- en: 'We had to give the LLM several pieces of information for the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scenario/background*—The LLM should know the scenario behind the task (“you
    are a conversational designer”). The LLM should also be given background assumptions,
    such as what information is available on medical claims.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instructions*—The LLM must design a dialogue flow that achieves a user goal
    (finding the claim) while being as brief and easy as possible. Further, the LLM
    should describe its “reasoning,” which will help us evaluate the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cue*—The cue “Output” lets the LLM know the instruction is finished.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small changes may cause big differences
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs may give significantly different answers to very similar prompts. Even
    formatting changes, like adding a space or including or omitting newline characters,
    can cause major output changes. The exact prompts used in this book are included
    on the book’s GitHub site at [https://github.com/andrewrfreed/EffectiveConversationalAI](https://github.com/andrewrfreed/EffectiveConversationalAI).
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows the output when the prompt uses three lines in the instruction
    (the lines starting with “Instruction,” “Design,” and “Describe”).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 Output from mixtral-8x7b-instruct-01 for claim status flow (less
    newlines)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sample conversation, including the chatbot and user messages'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Explanation of the design process'
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM designed a two-sided sample conversation demonstrating what both the
    bot and the user would say. This is nice—it is helpful to visualize what a complete
    conversation looks like. We should be aware that this is only a sample—users may
    respond in many ways to these questions. Let’s recap what happened in the exchange
    with the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: We told the LLM that claims had a member ID, date, amount, and status. It inferred
    that the status was an output and the other three data points were inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM designed a process flow that collects all three data points in sequential
    order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated dialogue is polite and useful. The bot acknowledges user input
    with “thank you.” It also gives clear instructions to the user about what is expected
    in each step of the flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly, we get very different output depending on how we use newline
    characters in the prompt. The following listing shows the output from a prompt
    using six lines (a new line after every period in the instruction).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 Output from mixtral-8x7b-instruct-01 for claim status flow (more
    newlines)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output in listing 10.3 is quite different. First, the sample conversation
    and rationale are interleaved. Every step of the conversation has a description,
    an example chatbot message, and a detailed rationale. Second, the sample conversation
    includes only the bot messages. We don’t see user responses. Third, this process
    flow implies confirmation statements after each piece of data is received from
    the user. Finally, the sample dialogue contains minor errors. Instruction 3 says
    it will ask for the claim amount, but the dialogue shows it confirming a claim
    amount without collecting it.
  prefs: []
  type: TYPE_NORMAL
- en: Together, the two prompts give us plenty of ideas for constructing a dialogue
    flow to implement a claim status process. Since listing 10.2 is more fully formed,
    we will use that as our baseline. Though it is pretty good, the process generated
    seems a little lengthy. Can we improve the process flow? Of course we can! Let’s
    ask the LLM for improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Improving dialogue flow with generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can ask an LLM to improve process flows whether they were generated by LLMs
    or humans. Let’s improve the process flow in listing 10.2, which we generated
    via LLM to help users learn their claim status.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process flow, as currently constructed, requires three pieces of information:
    a member ID, a date, and a claim amount. This meets some basic requirements, like
    authenticating the caller (by member ID) and providing search criteria (member
    ID plus date and amount). However, this could be burdensome to the user. By intuition,
    it seems that the member ID plus one more piece of information could uniquely
    identify the claim. Let’s ask the LLM how to simplify the process flow.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Asking the LLM to improve the dialogue flow from listing 10.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The full content of listing 10.1 is in the prompt but omitted here for brevity.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The full content of listing 10.2 is in the prompt but omitted here for brevity.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A lightweight instruction to the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Output cue'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “improvement” prompt is simple in nature. We simulate a conversation with
    the LLM by including the full history of our past conversation: the original prompt
    and its response. We then provide a basic instruction (“simplify the dialogue
    flow above”) with a definition of simplification (“only require the user to provide
    two pieces of information”). The output from this prompt is shown in the following
    listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 Simplified dialogue flow and justification provided by LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Step 2 now collects all information in a single turn.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Step 5 converts the claim amount from an input to an output parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see two suggestions from the LLM: first, to not ask the claim amount, and
    second, to combine two questions into one. Let’s review them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Removing the claim amount*—Member ID and claim date are the two most “unique”
    data elements and the ones the user is most likely to have in hand. They are also
    the easiest to enter in web or phone bots, with most chat platforms having libraries
    to accept IDs and dates. The chatbot also informs the caller of the claim amount
    during the readout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Asking two questions at once*—The new flow combines member ID and claim date
    into a single question (step 2). This is excellent for power users who want as
    few steps as possible. This may be more challenging for users who only have one
    piece of information available and need help finding the second. It is good for
    the chatbot to accept both pieces of information in one turn, but it may not be
    optimal to require it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subject matter experts or LLMs?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We advise using subject matter expert (SME) advice before taking any solution
    to production. LLMs are great for generating ideas and testing ideas quickly.
    Use LLMs to explore the art of the possible and quickly draft potential solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In one simple prompt, we generated two suggestions for how to improve the dialogue
    flow. Can you think of other ways to improve the dialogue flow? What instructions
    would you give the LLM?
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Take listing 10.4 and try some alternative instructions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only ask the user for one piece of information at a time.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Guide a user who says “I don’t have it” for one of the questions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce additional parameters, such as a claim ID, and see how the bot generates
    additional process flow variations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use an LLM to generate a process flow for a different scenario, such as these:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Booking a flight
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Buying a movie ticket
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending a vacation destination
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Or use a scenario from a chatbot you are building!
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 AI-assisted process flows at run time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s been great using generative AI to build process flow designs. So far, these
    have been somewhat static flows, usable in traditional conversational AI solutions.
    Claims status is an example of a “slot-filling” search, where we use a conversational
    process to collect information required to complete a task. This often takes the
    form of collecting required parameters for an API call. It requires careful mapping
    of questions and answer responses to an API. Then the answers are slotted into
    API parameters until the API can be executed. Slot-filling is one of the most
    popular conversational process flow patterns.
  prefs: []
  type: TYPE_NORMAL
- en: What about deferring more control to the LLM in these flows?
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Executing dialogue flows with generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our previous process flow was designed statically. Let’s try something different.
    We will just describe the process and let the LLM decide what questions to ask
    during the live conversation. Figure 10.3 shows how we’ll incorporate an LLM into
    the process of gathering information for the claim search API.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F03_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 How a conversational AI can use an LLM to decide what question to
    ask next
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We are assuming some logic in the chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: When it detects a claim status intent, it lets the LLM decide what question
    to ask next.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it detects the LLM responding with a list of variables, it takes back control
    and executes a claim search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses guardrails like a preclassifier to ensure data sent to the LLM is not
    malicious, like “ignore all previous instructions and <do something nasty>.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following listing demonstrates an LLM generating the conversation one step
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Letting the LLM decide what questions to ask for claim status
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Similar instruction to the previous prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Description of new task'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Assuming a static greeting to the bot, we feed the bot’s initial greeting
    and user’s first utterance.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Output cue'
  prefs: []
  type: TYPE_NORMAL
- en: This simple prompt is enough to get the bot generating some dialogue for us.
    (We wrote this prompt in just a few minutes.) The next few listings show the output
    from each consecutive iteration of the prompt. The following listing shows the
    first turn, using the output from listing 10.6.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Conversational turn 2 output (LLM generating next question to ask)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Previous listing is included here'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Output from first prompt'
  prefs: []
  type: TYPE_NORMAL
- en: The bot has done a nice job so far. It decided to ask for the member ID first.
    Let’s see what it does when the user responds.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Conversational turn 2 output (LLM generating next question to ask)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Previous listing is included here'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 User’s response is passed from chatbot to LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 LLM response'
  prefs: []
  type: TYPE_NORMAL
- en: This is another reasonable response. The LLM is responding well without any
    guidance besides the initial prompt. The responses are a bit repetitive—both questions
    took the form of “What is your <X>.” Let’s see the rest of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Conversational output showing turns 3 and 4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Previous listing is included here'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 User’s response is passed from chatbot to LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 LLM asks the final question'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 User’s final response is passed from chatbot to LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 LLM generates a payload that the chatbot can use for search'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the dialogue flow, the LLM has done everything we needed. It asked
    questions without using hardcoded dialogue, and it generated a parameter set we
    could pass to the claim search API. (It did not generate valid JSON, but it could
    probably learn to do that from a better prompt.)
  prefs: []
  type: TYPE_NORMAL
- en: It could be tempting to give full control to the LLM at this point. Before we
    do, let’s see how the LLM reacts in a different scenario. This time, the user
    won’t know everything the system needs. How will the LLM react? The next listing
    demonstrates this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.10 LLM generated responses for when user doesn’t have all the information
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Same instruction as in previous examples'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Same initial conversation as in previous examples'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 User does not know some information'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The LLM gets stuck!'
  prefs: []
  type: TYPE_NORMAL
- en: Uh-oh! The LLM has no sense of error handling in this prompt. It looks like
    the LLM will perpetually ask questions until the user ends the chat in frustration.
    The user probably can’t opt out of this chat either. Clearly this approach has
    some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Asking multiple questions to fulfill a search process was hit and miss. Let’s
    try something else. What if we let the LLM do the search?
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Using LLM for a search process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In scenarios like medical insurance, a careful search is critical. A healthcare
    provider may have hundreds of open claims (or more) across their patient population.
    Strict search criteria are critical to successful searches, not to mention being
    required by law. Let’s imagine a different scenario where there are far fewer
    options to search for.
  prefs: []
  type: TYPE_NORMAL
- en: Isn’t this retrieval-augmented generation (RAG)?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sort of. We are creating textual “passages” based on the output of structured
    APIs and letting the LLM reason over them. Purists may not call it RAG, but it
    has similarities. And most importantly, it is a useful tool in your toolbox, whatever
    you call it.
  prefs: []
  type: TYPE_NORMAL
- en: Our scenario for this example is consumers checking their bank account balances.
    A consumer generally has between one and four accounts at one bank. The chatbot
    will need to know which account the user is asking about. Only a few pieces of
    metadata are relevant to the accounts, including type (checking or savings), owner
    (solo or joint), and ID (though owners may not remember it).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume the user is logged in to our chatbot (we know who they are from
    their logged-in user ID or their verified phone number). They ask for an account
    balance, and the chatbot asks the LLM for help. The flow diagram is shown in figure
    10.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F04_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Using an LLM to handle user responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can imagine the user asking the following questions of the assistant:'
  prefs: []
  type: TYPE_NORMAL
- en: How much money is in my account?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much money is in my savings account?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much money is our joint savings account?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much money is in my son’s account?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much money is in the account I just opened?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt and example output are shown in the following listing. This prompt
    is executed with stopping criteria of any whitespace character (space or newline).
    Otherwise, the LLM continues the output with a justification of its choice.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 Using an LLM to perform a search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Basic instruction provided as a prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 User’s input is passed directly to the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 LLM receives the context of the logged-in user and the metadata for all
    accounts'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Output cue and output'
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! The LLM can answer all five questions. Table 10.1 shows the LLM responses.
    Recall that we are only asking the LLM to pick the account ID. The chatbot will
    still invoke the final “check balance” API call and formulate the final response.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.1 Responses from listing 10.11 for several different input questions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Question | Response (Account ID) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| How much money is in my account?  | 12345  |'
  prefs: []
  type: TYPE_TB
- en: '| How much money is in my savings account?  | 23456  |'
  prefs: []
  type: TYPE_TB
- en: '| How much money is in our joint savings account?  | 23456  |'
  prefs: []
  type: TYPE_TB
- en: '| How much money is in my son’s account?  | 34567  |'
  prefs: []
  type: TYPE_TB
- en: '| How much money is in the account I just opened?  | 34567  |'
  prefs: []
  type: TYPE_TB
- en: 'We can make several observations :'
  prefs: []
  type: TYPE_NORMAL
- en: '*Variability*—We handled several different search criteria, including dates,
    types, and owners, without asking any disambiguation questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flexibility*—Criteria like “my son” or “the account I just opened” were handled
    without a strict API parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Default choice*—For the two ambiguous questions (“my account”) and (“our joint
    savings account”), the bot chose the first matching choice. This implies that
    sort order is important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM offers incredible flexibility! If the stakes are low enough, letting
    the LLM search is an excellent strategy. Assuming that our output message is something
    like “Your <type> account with ID <id> has <balance>,” it may be okay that the
    LLM did not ask a clarifying question. The bot is always responding with accurate
    information and supporting evidence. The user may still ask follow-up questions
    like “No, I meant my savings account balance” if they need different information.
  prefs: []
  type: TYPE_NORMAL
- en: Is letting an LLM pick an account ID safe? What about hallucinations?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the example of consumers checking their bank account balances, we introduce
    safety by separating out the API call from the LLM judgment. A typical “get balance”
    API will have two parameters: a user ID and an account ID. In this scenario, we
    only let the LLM pick the account ID. Thus, we are protected from the LLM hallucinating
    a user ID and account ID combination that leaks someone else’s account information.
    If the LLM hallucinates an account ID, the API will fail the call; if the LLM
    picks the wrong account for this user, at least they will hear about one of their
    own accounts. Be sure to test your design and implementation thoroughly before
    assuming it is safe.'
  prefs: []
  type: TYPE_NORMAL
- en: This kind of safety-driven design should be used when letting LLMs execute API
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI with LLMs offers us interesting possibilities in augmenting our
    chatbots. We need to carefully balance the trade-offs between implementation speed
    and control. But LLMs support things that would otherwise be difficult or impossible
    in traditional chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Update the prompt in listing 10.6 to give more varied responses (not just “what
    is your <X>”).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the prompt in listing 10.11 so that the LLM gives a sentinel value like
    “n/a” if the user’s question is ambiguous. You can give additional instructions
    in the prompt or add few-shot examples for the LLM to learn from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 10.3 AI-assisted flows at test time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we used generative AI to design or implement the chat
    solution by having the LLM act as the chatbot. In this section, we will turn that
    paradigm on its head. We will use the LLM to generate typical or “creative” responses
    and see how the chatbot handles them in our insurance claims scenario. This conceptualized
    flow is shown in figure 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F05_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Flow diagram of how the test script invokes an LLM as a "user" of
    the chatbot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We need three things to put this test script together: a generalized prompt
    for the LLM to act as a user, a test script to invoke both the chatbot and the
    LLM, and a methodology for reviewing the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Setting up generative AI to be the user
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The LLM will need three pieces of information to be an effective user: general
    instructions for the task, a description of the scenario we need to test, and
    the conversation so far.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s provide some simple background telling the LLM we want it to mimic
    a user in an ongoing conversation. The instruction can start quite simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This instruction describes the basics of what we want the LLM to do. We are
    telling the LLM to respond as the user, not the system. We give no further guidance
    to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we want the LLM to be able to handle different scenarios. We need an
    adaptable prompt. Here are a few scenarios we’d like to test:'
  prefs: []
  type: TYPE_NORMAL
- en: The user has all the information they need (member ID, claim date, claim amount).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user is missing some necessary information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user is missing some necessary information but has alternatives (a claim
    ID).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each scenario, we would give slightly different guidance to the prompt.
    Table 10.2 maps some scenarios to the detailed guidance we could give the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10.2 Scenario descriptions and prompt-able guidance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Description | Guidance |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| User has all the information they need  | You are trying to find out if one
    of your medical claims was paid. You know your member ID is 123456, the claim
    date is May 4, 2024, and the claim amount of $1000\.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User is missing some necessary information  | You are trying to find out
    if your most recent medical claim was paid. You know your member ID is 123456
    but don’t know anything else.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| User is missing some necessary information but has alternatives  | You are
    trying to find out if your most recent medical claim was paid. You know your member
    ID is 123456 and that the claim ID is 987654321987654\.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'The guidance in table 10.2 has the following information the LLM can use in
    the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scenario*—What the LLM should try to do, such as find out if a claim was paid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Test data*—We know the chatbot can call APIs, so we need the LLM to provide
    data that exists in our system. We explicitly give the LLM the information we
    want it to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Boundaries*—We tell the LLM what it does not know. This should prevent the
    LLM from “inventing” (hallucinating) information that will cause our later API
    calls to fail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don’t provide the LLM any other guidance. We want to see how it tries to
    achieve these outcomes in the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to provide the conversational transcript to the LLM to inform
    how it responds next (and what it has already responded with). The test script
    will be able to keep track of the transcript because it is invoking both the chatbot
    and LLM. (There are many ways to gather the chat transcript, and chapter 12 will
    demonstrate a few more.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now build a Python function to generate a prompt for a given scenario.
    The function takes two arguments: the guidance for the scenario (as seen in table
    10.2) and the conversational transcript. The next listing shows the function.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.12 Python function to build a prompt for a test scenario
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generic description of the task'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Scenario-specific guidance and test details'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Injects the conversational transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Cue for LLM response'
  prefs: []
  type: TYPE_NORMAL
- en: This code’s function dynamically builds a prompt for a given scenario and conversation
    transcript.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 demonstrates how we can call the `get_prompt()` function. It assumes
    a `call_llm()` function whose implementation will vary based on the LLM platform
    (assume it is initiated with an API key, it lets you pick a model and configuration
    settings, and it then provides a function that receives a prompt and returns output).
    Be sure to use sampling decoding in your `call_llm()` function so that you get
    variety in your responses.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 Python code to use a dynamic prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Full text of the test scenario to guide the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Full conversational transcript to date'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Builds the prompt dynamically'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gets LLM response (e.g., “Sure, my member ID is 123456”)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Updates the conversational transcript'
  prefs: []
  type: TYPE_NORMAL
- en: We now have the first half of our test script. Let’s set up the other half.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Setting up the conversational test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, the test script must call the chatbot. The script will take the LLM-generated
    “user” input and pass it to the bot. Then the script will take the bot’s response,
    append it to the transcript, and call the LLM again. We will again depend on a
    function not implemented here (the implementation will vary by platform)—in this
    case, that function is `call_chatbot()`. This function is expected to configure
    a connection to a chatbot, authenticate with an API key, and manage a user conversation.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows this part of our test script.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 Python code to call the chatbot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sends a message to the chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Stores the chatbot response in the transcript'
  prefs: []
  type: TYPE_NORMAL
- en: We can now put all the pieces together. In the next listing, we combine all
    the elements into a single test script.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 Python code combining LLM-as-user and chatbot calls
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A conversation is often initiated with “blank” input.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tests a few turns of the conversation'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sends the LLM response to the chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Stores the chatbot response in the transcript'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Prints the transcript at the end of the test'
  prefs: []
  type: TYPE_NORMAL
- en: The script initiates a connection to the chatbot and runs through a fixed number
    of turns (four). Depending on our test needs, we could increase or decrease that
    number or put in additional logic to detect when the conversation has ended (or
    failed).
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows some example output from running this script on one of
    our test scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 Test script example output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This script sets up the basic mechanics of having an LLM act as a user of your
    chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of test is an excellent supplement to your other testing efforts.
    LLMs may generate user inputs that you never thought to handle in your chatbot,
    and it is good to find out how your chatbot responds to them. Remember that the
    LLM is only simulating a human—real humans may never act or “speak” the way an
    LLM does. But then again, they might.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Play the role of the bot. Implement the function `call_chatbot(user_ response)`
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This lets you test how the LLM responds (as a user) to the messages you (as
    a chatbot) send. This saves you from having to implement a chatbot just to see
    how this test script works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Connect the `call_chatbot(user_response)` function to an actual chatbot
    you are building. Connect the `call_llm(prompt)` function to your AI platform
    of choice. Update the `get_prompt` function to be more appropriate for your scenario.
    Does the LLM stretch the capabilities of your chatbot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can design a process flow for you from scratch. With a little prompting,
    they can generate example conversational flows and justify their design choices.
    This process flow can then be implemented in traditional conversational AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can also take an existing process flow and improve it. A typical improvement
    is simplifying the process flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use generative AI to execute an entire conversation. There is a trade-off
    between implementation speed and control. This is especially noticeable on error
    paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can replace some slot-filling process flows with an LLM-driven process.
    This can be much more flexible than strictly matching to API parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the cost of being wrong when letting LLMs make judgments. Look for
    cases where “mistakes” are not critical. Be careful about which APIs the LLM is
    allowed to influence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can simulate users of your conversational AI. Use them to generate test
    conversations that show how your system may behave in certain scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
