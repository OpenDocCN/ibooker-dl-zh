- en: 12 Training a Transformer to generate text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Building a scaled-down version of the GPT-2XL model tailored to your needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for training a GPT-style Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a GPT-style Transformer from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating text using the trained GPT model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 11, we developed the GPT-2XL model from scratch but were unable to
    train it due to its vast number of parameters. Training a model with 1.5 billion
    parameters requires supercomputing facilities and an enormous amount of data.
    Consequently, we loaded pretrained weights from OpenAI into our model and then
    used the GPT-2XL model to generate text.
  prefs: []
  type: TYPE_NORMAL
- en: However, learning how to train a Transformer model from scratch is crucial for
    several reasons. First, while this book doesn’t directly cover fine-tuning a pretrained
    model, understanding how to train a Transformer equips you with the skills needed
    for fine-tuning. Training a model involves initializing parameters randomly, whereas
    fine-tuning involves loading pretrained weights and further training the model.
    Second, training or fine-tuning a Transformer enables you to customize the model
    to meet your specific needs and domain, which can significantly enhance its performance
    and relevance for your use case. Finally, training your own Transformer or fine-tuning
    an existing one provides greater control over data and privacy, which is particularly
    important for sensitive applications or handling proprietary data. In summary,
    mastering the training and fine-tuning of Transformers is essential for anyone
    looking to harness the power of language models for specific applications while
    maintaining privacy and control.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this chapter, we’ll construct a scaled-down version of the GPT
    model with approximately 5 million parameters. This smaller model follows the
    architecture of the GPT-2XL model; the significant differences are its composition
    of only 3 decoder blocks and an embedding dimension of 256, compared to the original
    GPT-2XL’s 48 decoder blocks and an embedding dimension of 1,600\. By scaling down
    the GPT model to about 5 million parameters, we can train it on a regular computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated text’s style will depend on the training data. When training
    a model from scratch for text generation, both text length and variation are crucial.
    The training material must be extensive enough for the model to learn and mimic
    a particular writing style effectively. At the same time, if the training material
    lacks variation, the model may simply replicate passages from the training text.
    On the other hand, if the material is too long, training may require excessive
    computational resources. Therefore, we will use three novels by Ernest Hemingway
    as our training material: *The Old Man and the Sea*, *A Farewell to Arms*, and
    *For Whom the Bell Tolls*. This selection ensures that our training data has sufficient
    length and variation for effective learning without being so long that training
    becomes impractical.'
  prefs: []
  type: TYPE_NORMAL
- en: Since GPT models cannot process raw text directly, we will first tokenize the
    text into words. We will then create a dictionary to map each unique token to
    a different index. Using this dictionary, we will convert the text into a long
    sequence of integers, ready for input into a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We will use sequences of 128 indexes as input to train the GPT model. As in
    chapters 8 and 10, we will shift the input sequence by one token to the right
    and use it as the output. This approach forces the model to predict the next word
    in a sentence based on the current token and all previous tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: A key challenge is determining the optimal number of epochs for training the
    model. Our goal is not merely to minimize the cross-entropy loss in the training
    set, as doing so could lead to overfitting, where the model simply replicates
    passages from the training text. To tackle this problem, we plan to train the
    model for 40 epochs. We will save the model at 10-epoch intervals and evaluate
    which version can generate coherent text without merely copying passages from
    the training material. Alternatively, one could potentially use a validation set
    to assess the performance of the model and decide when to stop training, as we
    did in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: Once our GPT model is trained, we will use it to generate text autoregressively,
    as we did in chapter 11\. We’ll test different versions of the trained model.
    The model trained for 40 epochs produces very coherent text, capturing Hemingway’s
    distinctive style. However, it may also generate text partly copied from the training
    material, especially if the prompt is similar to passages in the training text.
    The model trained for 20 epochs also generates coherent text, albeit with occasional
    grammatical errors, but is less likely to directly copy from the training text.
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal of this chapter is not necessarily to generate the most coherent
    text possible, which presents significant challenges. Instead, our objective is
    to teach you how to build a GPT-style model from scratch, tailored to real-world
    applications and your specific needs. More importantly, this chapter outlines
    the steps involved in training a GPT model from scratch. You will learn how to
    select training text based on your objectives, tokenize the text and convert it
    to indexes, and prepare batches of training data. You will also learn how to determine
    the number of epochs for training. Once the model is trained, you will learn how
    to generate text using the model and how to avoid generating text directly copied
    from the training material.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Building and training a GPT from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our objective is to master building and training a GPT model from scratch, tailored
    to specific tasks. This skill is crucial for applying the concepts in this book
    to real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are an avid fan of Ernest Hemingway’s work and wish to train a GPT
    model to generate text in Hemingway’s style. How would you approach this? This
    section discusses the steps involved in this task.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to configure a GPT model suitable for training. You’ll create
    a GPT model with a structure similar to the GPT-2 model you built in chapter 11
    but with significantly fewer parameters to make training feasible in just a few
    hours. As a result, you’ll need to determine key hyperparameters of the model,
    such as sequence length, embedding dimension, number of decoder blocks, and dropout
    rates. These hyperparameters are crucial as they influence both the quality of
    the output from the trained model and the speed of training.
  prefs: []
  type: TYPE_NORMAL
- en: Following that, you will gather the raw text of several Hemingway novels and
    clean it up to ensure it is suitable for training. You will tokenize the text
    and assign a different integer to each unique token so that you can feed it to
    the model. To prepare the training data, you will break down the text into sequences
    of integers of a certain length and use them as inputs. You will then shift the
    inputs one token to the right and use them as outputs. This approach forces the
    model to predict the next token based on the current token and all previous tokens
    in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, you will use it to generate text based on a prompt.
    You will first convert the text in the prompt to a sequence of indexes and feed
    it to the trained model. The model uses the sequence to predict the most likely
    next token iteratively. After that, you will convert the sequence of tokens generated
    by the model back to text.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will first discuss the architecture of the GPT model for
    the task. After that, we will discuss the steps involved in training the model.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 The architecture of a GPT to generate text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although GPT-2 is available in various sizes, they all share a similar architecture.
    The GPT model we construct in this chapter follows the same structural design
    as GPT-2 but is significantly smaller, making it feasible to train without the
    need for supercomputing facilities. Table 12.1 presents a comparison between our
    GPT model and the four versions of the GPT-2 models.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.1 A comparison of our GPT with different versions of GPT-2 models
  prefs: []
  type: TYPE_NORMAL
- en: '|  | GPT-2S | GPT-2M | GPT-2L | GPT-2XL | Our GPT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding dimension | 768 | 1,024 | 1,280 | 1,600 | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of decoder layers | 12 | 24 | 36 | 48 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of heads | 12 | 16 | 20 | 25 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence length | 1,024 | 1,024 | 1,024 | 1,024 | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| Vocabulary size | 50,257 | 50,257 | 50,257 | 50,257 | 10,600 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of parameters | 124 million | 350 million | 774 million | 1,558 million
    | 5.12 million |'
  prefs: []
  type: TYPE_TB
- en: In this chapter, we’ll construct a GPT model with three decoder layers and an
    embedding dimension of 256 (meaning each token is represented by a 256-value vector
    after word embedding). As we mentioned in chapter 11, GPT models use a different
    positional encoding method than the one used in the 2017 paper “Attention Is All
    You Need.” Instead, we use embedding layers to learn the positional encodings
    for different positions in a sequence. As a result, each position in a sequence
    is also represented by a 256-value vector. For calculating causal self-attention,
    we use four parallel attention heads to capture different aspects of the meanings
    of a token in the sequence. Thus, each attention head has a dimension of 256/4
    = 64, similar to that in GPT-2 models. For example, in GPT-2XL, each attention
    head has a dimension of 1,600/25 = 64.
  prefs: []
  type: TYPE_NORMAL
- en: The maximum sequence length in our GPT model is 128, which is much shorter than
    the maximum sequence length of 1,024 in GPT-2 models. This reduction is necessary
    to keep the number of parameters in the model manageable. However, even with 128
    elements in a sequence, the model can learn the relationship between tokens in
    a sequence and generate coherent text.
  prefs: []
  type: TYPE_NORMAL
- en: While GPT-2 models have a vocabulary size of 50,257, our model has a much smaller
    vocabulary size of 10,600\. It’s important to note that the vocabulary size is
    mainly determined by the training data, rather than being a predefined choice.
    If you choose to use more text for training, you may end up with a larger vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 illustrates the architecture of the decoder-only Transformer we
    will create in this chapter. It is similar to the architecture of GPT-2 that you
    have seen in chapter 11, except that it is smaller in size. As a result, the total
    number of parameters in our model is 5.12 million, compared to the 1.558 billion
    in the GPT-2XL model that we built in chapter 11\. Figure 12.1 shows the size
    of the training data at each step of training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH12_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 The architecture of a decoder-only Transformer, designed to generate
    text. The text from three Hemingway novels is tokenized and then converted into
    indexes. We arrange 128 indexes into a sequence, and each batch contains 32 such
    sequences. The input first undergoes word embedding and positional encoding, with
    the input embedding being the sum of these two components. This input embedding
    is then processed through three decoder layers. Following this, the output undergoes
    layer normalization and passes through a linear layer, resulting in an output
    size of 10,600, which corresponds to the number of unique tokens in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the GPT model we create consists of input embeddings, which are
    illustrated at the bottom of figure 12.1\. We will discuss how to calculate these
    embeddings in detail in the next subsection. Briefly, they are the sum of word
    embeddings and positional encodings from the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input embedding is then passed sequentially through three decoder layers.
    Similar to the GPT-2XL model we built in chapter 11, each decoder layer consists
    of two sublayers: a causal self-attention layer and a feed-forward network. Additionally,
    we apply layer normalization and residual connections to each sublayer. After
    this, the output goes through a layer normalization and a linear layer. The number
    of outputs in our GPT model corresponds to the number of unique tokens in the
    vocabulary, which is 10,600\. The output of the model is the logits for the next
    token. Later, we will apply the softmax function to these logits to obtain the
    probability distribution over the vocabulary. The model is designed to predict
    the next token based on the current token and all previous tokens in the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 The training process of the GPT model to generate text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know how to construct the GPT model for text generation, let’s explore
    the steps involved in training the model. We aim to provide an overview of the
    training process before diving into the coding aspect of the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The style of the generated text is influenced by the training text. Since our
    objective is to train the model to generate text in the style of Ernest Hemingway,
    we’ll use the text from three of his novels: *The Old Man and the Sea*, *A Farewell
    to Arms*, and *For Whom the Bell Tolls*. If we were to choose just one novel,
    the training data would lack variety, leading the model to memorize passages from
    the novel and generate text identical to the training data. Conversely, using
    too many novels would increase the number of unique tokens, making it challenging
    to train the model effectively in a short amount of time. Therefore, we strike
    a balance by selecting three novels and combining them as our training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 illustrates the steps involved in training the GPT model to generate
    text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH12_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 The training process for a decoder-only Transformer to generate
    text, Hemingway-style.
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous three chapters, the first step in the training process is
    to convert text into a numerical form so that we can feed the training data to
    the model. Specifically, we first break down the text of the three novels into
    tokens using word-level tokenization, as we did in chapter 8\. In this case, each
    token is a whole word or a punctuation mark (such as a colon, a parenthesis, or
    a comma). Word-level tokenization is easy to implement, and we can control the
    number of unique tokens. After tokenization, we assign a unique index (i.e., an
    integer) to each token, converting the training text into a sequence of integers
    (see step 1 in figure 12.2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we transform the sequence of integers into training data by first dividing
    this sequence into sequences of equal length (step 2 in figure 12.2). We allow
    a maximum length of 128 indexes in each sequence. The choice of 128 allows us
    to capture long-range dependencies among tokens in a sentence while keeping the
    model size manageable. However, the number 128 is not magical: changing the number
    to, say, 100 or 150 will lead to similar results. These sequences form the features
    (the x variable) of our model. As we did in previous chapters, we shift the input
    sequence one token to the right and use it as the output in the training data
    (the y variable; step 3 in figure 12.2).'
  prefs: []
  type: TYPE_NORMAL
- en: The pairs of input and output serve as the training data (x, y). In the example
    of the sentence “the old man and the sea,” we use indexes corresponding to “the
    old man and the” as the input x. We shift the input one token to the right and
    use the indexes for “old man and the sea” as the output y. In the first time step,
    the model uses “the” to predict “old.” In the second time step, the model uses
    “the old” to predict “man,” and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, you will iterate through the training data. In the forward
    passes, you feed the input sequence x through the GPT model (step 4). The GPT
    then makes a prediction based on the current parameters in the model (step 5).
    You compute the cross-entropy loss by comparing the predicted next tokens with
    the output obtained from step 3\. In other words, you compare the model’s prediction
    with the ground truth (step 6). Finally, you will adjust the parameters in the
    GPT model so that in the next iteration, the model’s predictions move closer to
    the actual output, minimizing the cross-entropy loss (step 7). Note that the model
    is essentially performing a multicategory classification problem: it’s predicting
    the next token from all unique tokens in the vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: You will repeat steps 3 to 7 through many iterations. After each iteration,
    the model parameters are adjusted to improve the prediction of the next token.
    We will repeat this process for 40 epochs and save the trained model after every
    10 epochs. As you will see later, if we train the model for too long, it becomes
    overfit, memorizing passages from the training data. The generated text then becomes
    identical to those in the original novels. We will test ex post which version
    of the model generates coherent text and, at the same time, does not simply copy
    from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Tokenizing text of Hemingway novels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you understand the architecture of the GPT model and the training
    process, let’s begin with the first step: tokenizing and indexing the text of
    Hemingway’s novels.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll process the text data to prepare it for training. We’ll break down
    the text into individual tokens, as we did in chapter 8\. Since deep neural networks
    cannot directly process raw text, we’ll create a dictionary that assigns an index
    to each token, effectively mapping them to integers. After that, we’ll organize
    these indexes into batches of training data, which will be crucial for training
    the GPT model in the subsequent steps.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use word-level tokenization for its simplicity in dividing text into words,
    as opposed to the more complex subword tokenization that requires a nuanced understanding
    of linguistic structure. Additionally, word-level tokenization results in a smaller
    number of unique tokens than subword tokenization, reducing the number of parameters
    in the GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Tokenizing the text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train the GPT model, we’ll use the raw text files of three novels by Ernest
    Hemingway*: The Old Man and the Sea*, *A Farewell to Arms*, and *For Whom the
    Bell Tolls*. The text files are downloaded from the Faded Page website: [https://www.fadedpage.com](https://www.fadedpage.com).
    I have cleaned up the text by removing the top and bottom paragraphs that are
    not part of the original book. When preparing your own training text, it’s crucial
    to eliminate all irrelevant information, such as vendor details, formatting, and
    license information. This ensures that the model focuses solely on learning the
    writing style present in the text. I have also removed the text between chapters
    that are not relevant to the main text. You can download the three files OldManAndSea.txt,
    FarewellToArms.txt, and ToWhomTheBellTolls.txt from the book’s GitHub repository:
    [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI). Place them
    in the /files/ folder on your computer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the text file for *The Old Man and the Sea*, both the opening double quote
    (“) and the closing double quote (”) are represented by straight double quotes
    ("). This is not the case in the text files for the other two novels. Therefore,
    we load up the text for *The Old Man and the Sea* and change straight quotes to
    either an opening quote or a closing quote. Doing so allows us to differentiate
    between the opening and closing quotes. This will also aid in formatting the generated
    text later on: we’ll remove the space after the opening quote and the space before
    the closing quote. This step is implemented as shown in the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Changing straight quotes to opening and closing quotes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads up the raw text and breaks it into individual characters
  prefs: []
  type: TYPE_NORMAL
- en: ② If a straight double quote is followed by a space or a line break, changes
    it to a closing quote
  prefs: []
  type: TYPE_NORMAL
- en: ③ Otherwise, changes it to an opening quote
  prefs: []
  type: TYPE_NORMAL
- en: ④ Converts a straight single quote to an apostrophe
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Joins individual characters back to text
  prefs: []
  type: TYPE_NORMAL
- en: If a double quote is followed by a space or a line break, we’ll change it to
    a closing quote; otherwise, we’ll change it to an opening quote. The apostrophe
    was entered as a single straight quote, and we have changed it to an apostrophe
    in the form of a closing single quote in listing 12.1.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we load the text for the other two novels and combine the three novels
    into one single file.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Combining the text from three novels
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Reads the text from the second novel
  prefs: []
  type: TYPE_NORMAL
- en: ② Reads the text from the third novel
  prefs: []
  type: TYPE_NORMAL
- en: ③ Combines the text from the three novels
  prefs: []
  type: TYPE_NORMAL
- en: ④ Saves the combined text in the local folder
  prefs: []
  type: TYPE_NORMAL
- en: We load the text from the other two novels, *A Farewell to Arms* and *For Whom
    the Bell Tolls*. We then combine the text from all three novels to use as our
    training data. Additionally, we save the combined text in a local file named ThreeNovels.txt
    so that we can later verify if the generated text is directly copied from the
    original text.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code listing is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output is the first 250 characters in the combined text.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll tokenize the text by using a space as the delimiter. As seen in the preceding
    output, punctuation marks such as periods (.), hyphens (-), and apostrophes (’)
    are attached to the preceding words without a space. Therefore, we need to insert
    a space around all punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ll convert line breaks (\n) into spaces so that they are not
    included in the vocabulary. Converting all words to lowercase is also beneficial
    in our setting, as it ensures that words like “The” and “the” are recognized as
    the same token. This step helps reduce the number of unique tokens, thereby making
    the training process more efficient. To address these problems, we’ll clean up
    the text as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 Adding spaces around punctuation marks
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Replaces line breaks with spaces
  prefs: []
  type: TYPE_NORMAL
- en: ② Identifies all punctuation marks
  prefs: []
  type: TYPE_NORMAL
- en: ③ Inserts spaces around punctuation marks
  prefs: []
  type: TYPE_NORMAL
- en: ④ Counts the number of unique tokens
  prefs: []
  type: TYPE_NORMAL
- en: We use the `set()` method to obtain all unique characters in the text. We then
    use the `isalpha()` and `isdigit()` methods to identify and remove letters and
    numbers from the set of unique characters, leaving us with only punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you execute the preceding code block, the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This list includes all punctuation marks in the text. We add spaces around them
    and break the text into individual tokens using the `split()` method. The output
    indicates that there are 10,599 unique tokens in the text from the three novels
    by Hemingway, a size that’s much smaller than the 50,257 tokens in GPT-2\. This
    will significantly reduce the model size and training time.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ll add one more token `"UNK"` to represent unknown tokens.
    This is useful in case we encounter a prompt with unknown tokens, allowing us
    to convert them to an index to feed to the model. Otherwise, we can only use a
    prompt with the preceding 10,599 tokens. Suppose you include the word “technology”
    in the prompt. Since “technology” is not one of the tokens in the dictionary `word_to_int`,
    the program will crash. By including the `"UNK"` token, you can prevent the program
    from crashing in such scenarios. When you train your own GPT, you should always
    include the `"UNK"` token since it’s impossible to include all tokens in your
    vocabulary. To that end, we add `"UNK"` to the list of unique tokens and map them
    to indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Mapping tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Adds “UNK” to the list of unique tokens
  prefs: []
  type: TYPE_NORMAL
- en: ② Counts the size of the vocabulary, ntokens, which will be a hyperparamter
    in our model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Maps tokens to indexes
  prefs: []
  type: TYPE_NORMAL
- en: ④ Maps indexes to tokens
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code block is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The text from the three novels contains 698,207 tokens. After including `"UNK"`
    in the vocabulary, the total number of *unique* tokens is now 10,600\. The dictionary
    `word_to_int` assigns a different index to each unique token. For example, the
    most frequent token, the period (.), is assigned an index of 0, and the word “the”
    is assigned an index of 1\. The dictionary `int_to_word` translates an index back
    to a token. For example, index 3 is translated back to the opening quote (“),
    and index 4 is translated back to the closing quote (”).
  prefs: []
  type: TYPE_NORMAL
- en: 'We print out the first 20 tokens in the text and their corresponding indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll break the indexes into sequences of equal length to use as training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Creating batches for training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll use a sequence of 128 tokens as the input to the model. We then shift
    the sequence one token to the right and use it as the output.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we create pairs of (x, y) for training purposes. Each x is a sequence
    with 128 indexes. We choose 128 to strike a balance between training speed and
    the model’s ability to capture long-range dependencies. Setting the number too
    high may slow down training, while setting it too low may prevent the model from
    capturing long-range dependencies effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the sequence x, we slide the sequence window to the right by one
    token and use it as the target y. Shifting the sequence by one token to the right
    and using it as the output during sequence generation is a common technique in
    training language models, including GPTs. We have done this in chapters 8 to 10\.
    The following code block creates the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the sequence length to 128 indexes
  prefs: []
  type: TYPE_NORMAL
- en: ② The input sequence x contains 128 consecutive indexes in the training text.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shifts x one position to the right and uses it as output y
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds the pair (x, y) to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created a list xys to contain pairs of (x, y) as our training data.
    As we did in previous chapters, we organize the training data into batches to
    stabilize training. We choose a batch size of 32:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We print out a pair of x and y as an example. The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Each x and y have a shape of (32, 128). This means that in each batch of training
    data, there are 32 pairs of sequences, with each sequence containing 128 indexes.
    When an index is passed through the `nn.Embedding()` layer, PyTorch looks up the
    corresponding row in the embedding matrix and returns the embedding vector for
    that index, avoiding the need to create potentially very large one-hot vectors.
    Therefore, when x is passed through the word embedding layer, it’s as if x is
    first converted to a one-hot tensor with a dimension of (32, 128, 256). Similarly,
    when x is passed through the positional encoding layer (which is implemented by
    the `nn.Embedding()` layer), it’s as if x is first converted to a one-hot tensor
    with a dimension of (32, 128, 128).
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Building a GPT to generate text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the training data ready, we’ll create a GPT model from scratch
    to generate text. The model we’ll build has a similar architecture as the GPT-2XL
    model we built in chapter 11\. However, instead of having 48 decoder layers, we’ll
    use only 3 decoder layers. The embedding dimensions and the vocabulary size are
    both much smaller, as I have explained earlier in this chapter. As a result, our
    GPT model will have far fewer parameters than GPT-2XL.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll follow the same steps as those in chapter 11\. Along the way, we’ll highlight
    the differences between our GPT model and GPT-2XL and explain the reasons for
    these modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 Model hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The feed-forward network in the decoder block uses the Gaussian error linear
    unit (GELU) activation function. GELU has been shown to enhance model performance
    in deep learning tasks, particularly in natural language processing. This has
    become a standard practice in GPT models. Therefore, we define a GELU class as
    follows, as we did in Chapter 11:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In chapter 11, we didn’t use a GPU even during the text generation stage, as
    the model was simply too large and a regular GPU would run out of memory if we
    loaded the model onto it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, however, our model is significantly smaller. We’ll move the
    model to the GPU for faster training. We’ll also generate text using the model
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a `Config()` class to include all the hyperparameters used in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The attributes in the `Config()` class are used as hyperparameters in our GPT
    model. We set the `n_layer` attribute to 3, indicating our GPT model has three
    decoder layers. The `n_head` attribute is set to 4, meaning we’ll split the query
    Q, key K, and value V vectors into 4 parallel heads when calculating causal self-attention.
    The `n_embd` attribute is set to 256, meaning the embedding dimension is 256:
    each token will be represented by a 256-value vector. The `vocab_size` attribute
    is determined by the number of unique tokens in the vocabulary. As explained in
    the last section, there are 10,600 unique tokens in our training text. The `block_size`
    attribute is set to 128, meaning the input sequence contains a maximum of 128
    tokens. We set the dropout rates to 0.1, as we did in chapter 11\.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 Modeling the causal self-attention mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The causal self-attention is defined in the same way as in chapter 11:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When calculating causal self-attention, the input embedding is passed through
    three neural networks to obtain the query Q, key K, and value V. We then split
    each of them into four parallel heads and calculate masked self-attention within
    each head. After that, we concatenate the four attention vectors back into a single
    attention vector, which is then used as the output of the `CausalSelfAttention()`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3 Building the GPT model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We combine a feed-forward network with the causal self-attention sublayer to
    form a decoder block. The feed-forward network injects nonlinearity into the model.
    Without it, the Transformer would simply be a series of linear operations, constraining
    its capacity to capture complex data relationships. Moreover, the feed-forward
    network processes each position independently and uniformly, enabling the transformation
    of features identified by the self-attention mechanism. This facilitates the capture
    of diverse aspects of the input data, thereby augmenting the model’s ability to
    represent information. A decoder block is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Each decoder block in our GPT model consists of two sublayers: a causal self-attention
    sublayer and a feed-forward network. We apply layer normalization and a residual
    connection to each sublayer for improved stability and performance. We then stack
    three decoder layers on top of each other to form the main body of our GPT model.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.5 Building a GPT model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① Moves the positional encoding to CUDA-enabled GPU, if available
  prefs: []
  type: TYPE_NORMAL
- en: The positional encoding is created within the `Model()` class. Therefore, we
    need to move it to a compute unified device architecture (CUDA)-enabled GPU (if
    available) to ensure that all inputs to the model are on the same device. Failing
    to do this will result in an error message.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the model consists of sequences of indexes corresponding to tokens
    in the vocabulary. We pass the input through word embedding and positional encoding
    and add the two to form the input embedding. The input embedding then goes through
    the three decoder blocks. After that, we apply layer normalization to the output
    and attach a linear head to it so that the number of outputs is 10,600, the size
    of the vocabulary. The outputs are the logits corresponding to the 10,600 tokens
    in the vocabulary. Later, we’ll apply the softmax activation function to the logits
    to obtain the probability distribution over the unique tokens in the vocabulary
    when generating text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create our GPT model by instantiating the `Model()` class we defined
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Our GPT model has 5.12 million parameters. The structure of our model is similar
    to that of GPT-2XL. If you compare the output above with that from chapter 11,
    you’ll see that the only differences are in the hyperparameters, such as the embedding
    dimension, number of decoder layers, vocabulary size, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Training the GPT model to generate text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you’ll train the GPT model you just built using the batches
    of training data we prepared earlier in this chapter. A related question is how
    many epochs we should train the model. While training too few epochs may lead
    to incoherent text, training too many epochs may lead to an overfitted model,
    which may generate text identical to passages in the training text.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will train the model for 40 epochs. We’ll save the model after
    every 10 epochs and assess which version of the trained model can generate coherent
    text without simply copying passages from the training text. Another potential
    approach is to create a validation set and stop training when the model’s performance
    converges in the validation set, as we did in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Training the GPT model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As always, we’ll use the Adam optimizer. Since our GPT model is essentially
    performing a multicategory classification, we’ll use cross-entropy loss as our
    loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will train the model for 40 epochs, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.6 Training the GPT model to generate text
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches of training data
  prefs: []
  type: TYPE_NORMAL
- en: ② Compares model predictions with actual outputs
  prefs: []
  type: TYPE_NORMAL
- en: ③ Clips gradient norm to 1
  prefs: []
  type: TYPE_NORMAL
- en: ④ Tweaks model parameters to minimize loss
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Saves model after every ten epochs
  prefs: []
  type: TYPE_NORMAL
- en: During training, we pass all the input sequences x in a batch through the model
    to obtain predictions. We compare these predictions with the output sequences
    y in the batch and calculate the cross-entropy loss. We then adjust the model
    parameters to minimize this loss. Note that we have clipped the gradient norm
    to 1 to avoid the potential problem of exploding gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient norm clipping
  prefs: []
  type: TYPE_NORMAL
- en: Gradient norm clipping is a technique used in training neural networks to prevent
    the exploding gradient problem. This problem occurs when the gradients of the
    loss function with respect to the model’s parameters become excessively large,
    leading to unstable training and poor model performance. In gradient norm clipping,
    the gradients are scaled down if their norm (magnitude) exceeds a certain threshold.
    This ensures that the gradients do not become too large, maintaining stable training
    and improving convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'This training process takes a couple of hours if you have a CUDA-enabled GPU.
    After training, four files, GPTe10.pth, GPTe20.pth, ..., GPTe40.pth, will be saved
    on your computer. Alternatively, you can download the trained models from my website:
    [https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip](https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip).'
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 A function to generate text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have multiple versions of the trained model, we can proceed to text
    generation and compare the performance of different versions. We can assess which
    version performs the best and use that version to generate text.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the process in GPT-2XL, text generation begins with feeding a sequence
    of indexes (representing tokens) to the model as a prompt. The model predicts
    the index of the next token, which is then appended to the prompt to form a new
    sequence. This new sequence is fed back into the model for further predictions,
    and this process is repeated until a desired number of new tokens is generated.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate this process, we define a `sample()` function. This function takes
    a sequence of indexes as input, representing the current state of the text. It
    then iteratively predicts and appends new indexes to the sequence until the specified
    number of new tokens, `max_new_tokens`, is reached. The following listing shows
    the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.7 A `sample()` function to predict subsequent indexes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads up a version of the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates a fixed number of new indexes
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the model to make predictions
  prefs: []
  type: TYPE_NORMAL
- en: ④ Attaches the new index to the end of the sequence
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Outputs only the new indexes
  prefs: []
  type: TYPE_NORMAL
- en: One of the arguments of the `sample()` function is `weights`, which represents
    the trained weights of one of the models saved on your computer. Unlike the `sample()`
    function we defined in chapter 11, our function here returns only the newly generated
    indexes, not including the original indexes that were fed to the `sample()` function.
    We made this change to accommodate cases where the prompt contains unknown tokens.
    In such cases, our `sample()` function ensures that the final output retains the
    original prompt. Otherwise, all unknown tokens would be replaced with `"UNK"`
    in the final output.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a `generate()` function to generate text based on a prompt.
    The function first converts the prompt to a sequence of indexes. It then uses
    the `sample()` function to generate a new sequence of indexes. After that, the
    `generate()` function concatenates all indexes together and converts them back
    to text. The implementation is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.8 A function to generate text with the trained GPT model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ① Makes sure the prompt is not empty
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts prompt into a sequence of indexes
  prefs: []
  type: TYPE_NORMAL
- en: ③ Uses the sample() function to generate new indexes
  prefs: []
  type: TYPE_NORMAL
- en: ④ Converts the new sequence of indexes back to text
  prefs: []
  type: TYPE_NORMAL
- en: We ensure that the prompt is not empty. If it is, you’ll receive an error message
    saying “prompt must contain at least one token.” The `generate()` function allows
    you to select which version of the model to use by specifying the weights saved
    on your computer. For example, you can choose ‘files/GPTe10.pth’ as the value
    of the weights argument for the function. The function converts the prompt into
    a series of indexes, which are then fed into the model to predict the next index.
    After generating a fixed number of new indexes, the function converts the entire
    index sequence back into textual form.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.3 Text generation with different versions of the trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’ll experiment with different versions of the trained model to generate
    text.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the unknown token `"UNK"` as the prompt for unconditional text generation.
    This is especially beneficial in our context because we want to check if the generated
    text is directly copied from the training text. While a unique prompt that’s very
    different from the training text unlikely leads to passages directly from the
    training text, unconditionally generated text is more likely to be from the training
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first use the model after 20 epochs of training to generate text unconditionally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We set the prompt to `"UNK"` and ask the `generate()` function to unconditionally
    generate 20 new tokens 10 times. We use the `manual_seed()` method to fix the
    random seeds so results are reproducible. As you can see, the 10 short passages
    generated here are all grammatically correct, and they sound like passages from
    Hemingway’s novels. For example, the word “kummel” in the first passage was a
    type of liqueur that was mentioned in *A Farewell to Arms* quite often. At the
    same time, none of the above 10 passages are directly copied from the training
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use the model after 40 epochs of training instead to generate text
    unconditionally and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The 10 short passages generated here are again all grammatically correct, and
    they sound like passages from Hemingway’s novels. However, if you examine them
    closely, a large part of the eighth passage is directly copied from the novel
    *A Farewell to Arms*. The part `they don't marry." i reached for her hand. "don`
    appeared in the novel as well. You can verify by searching in the file ThreeNovels.txt
    that was saved on your computer earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.1
  prefs: []
  type: TYPE_NORMAL
- en: Generate a passage of text with 50 new tokens unconditionally using the model
    trained for 10 epochs. Set the random seed to 42 and keep the `temperature` and
    `top-K` sampling at the default setting. Examine whether the generated passage
    is grammatically correct and if any parts are directly copied from the training
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can use a unique prompt that’s not in the training text
    to generate new text. For example, you might use “the old man saw the shark near
    the” as the prompt and ask the `generate()` function to add 20 new tokens to the
    prompt, repeating this process 10 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The generated text is grammatically correct and coherent, closely resembling
    passages from Hemingway’s novel *The Old Man and the Sea*. Since we used the model
    trained for 40 epochs, there’s a higher likelihood of generating text that directly
    mirrors the training data. However, using a unique prompt can reduce this probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'By setting the `temperature` and using `top-K` sampling, we can further control
    the diversity of the generated text. In this case, with a prompt like “the old
    man saw the shark near the,” and a temperature of 0.9 with top-50 sampling, the
    output remains mostly grammatically correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Since we used the model trained for 20 epochs instead of 40 epochs, the output
    is less coherent, with occasional grammar errors. For example, “with its long
    dip sharply” in the third passage is not grammatically correct. However, the risk
    of generating text directly copied from the training data is also lower.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 12.2
  prefs: []
  type: TYPE_NORMAL
- en: Generate a passage of text with 50 new tokens using the model trained for 40
    epochs. Use “the old man saw the shark near the” as the prompt; set the `random
    seed` to 42, the `temperature` to 0.95, and the `top_k` to 100\. Check if the
    generated passage is grammatically correct and if any part of the text is directly
    copied from the training text.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned how to construct and train a GPT-style Transformer
    model from the ground up. Specifically, you’ve created a simplified version of
    the GPT-2 model with only 5.12 million parameters. Using three novels by Ernest
    Hemingway as training data, you have successfully trained the model. You have
    also generated text that is coherent and stylistically consistent with Hemingway’s
    writing.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The style of the generated text from a GPT model will be heavily influenced
    by the training data. For effective text generation, it’s important to have a
    balance of text length and variation in the training material. The training dataset
    should be sufficiently large for the model to learn and emulate a specific writing
    style accurately. However, if the dataset lacks diversity, the model might end
    up reproducing passages directly from the training text. Conversely, overly long
    training datasets can require excessive computational resources for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right hyperparameters in the GPT model is crucial for successful
    model training and text generation. Setting the hyperparameters too large may
    lead to too many parameters. This results in longer training time and an overfitted
    model. Setting the hyperparameters too small may hinder the model’s ability to
    learn effectively and capture the writing style in the training data. This may
    lead to incoherent generated text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The appropriate number of training epochs is important for text generation.
    While training too few epochs may lead to incoherent text, training for too many
    epochs may lead to an overfitted model that generates text identical to passages
    in the training text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
