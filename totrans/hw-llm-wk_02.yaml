- en: '3 Transformers: How inputs become outputs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converting tokens into vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers, their types, and their roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting vectors back into tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the text generation loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2, we saw how large language models (LLMs) see text as fundamental
    units known as tokens. Now it’s time to talk about what LLMs do with the tokens
    they see. The process that LLMs use to generate their text is markedly different
    from how humans form coherent sentences. When an LLM operates, it is working on
    tokens, yet simultaneously cannot *manipulate* tokens like humans do because the
    LLM does not understand the structure and relationship of the letters each token
    represents.
  prefs: []
  type: TYPE_NORMAL
- en: For example, English speakers know that the words “magic,” “magical,” and “magician”
    are all related. We can understand that sentences containing these words are all
    connected to the same subject matter because these words share a common root.
    However, LLMs that operate on integers representing tokens that make up these
    words cannot understand the relationships between tokens without additional work
    to make those connections.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, LLMs follow a long history in machine learning and deep learning
    of performing a kind of cyclical conversion. First, tokens are converted into
    a numeric form that deep learning algorithms can work on. Then, the LLM converts
    this numeric representation back into a new token. This cycle repeats iteratively,
    which is not comparable to how humans work. You would be incredibly concerned
    if your colleagues had to pull out a calculator to perform several math problems
    between each word they spoke.
  prefs: []
  type: TYPE_NORMAL
- en: Yet this process is, indeed, how LLMs produce outputs. In this chapter, we will
    walk through the process in two stages. First, we will review the entire process
    at a high level to introduce fundamental concepts and construct a mental model
    of how LLMs generate text. Next, this model will serve as a scaffolding for a
    more in-depth discussion of the details and design choices associated with the
    components that LLMs use to capture the relationships between words and language
    and, ultimately, generate the output we are familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The transformer model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many LLMs you encounter today interpret tokens and produce output using a software
    architecture known as a transformer. This architecture consists of a collection
    of algorithms and data structures that store information by representing it as
    numbers in a neural network. At their core, transformers are sequence prediction
    algorithms. While it is common to describe them as “reasoning” or “understanding”
    language, what they actually do is predict tokens. Transformers come with three
    different approaches to token prediction. While we focus on the famous GPT architecture
    (more formally known as decoder-only models), it is also worth introducing encoder-only
    and encoder-decoder models:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder-only models*—These models are designed to create knowledge representations
    that can be used to perform tasks—that is, to encode the input into a numerical
    representation that is more useful to an algorithm. The best way to think of them
    is that they take text and process it into a form that is easier for a machine
    learning algorithm to use. They are widely used in scientific research. Famous
    examples include BERT and RoBERTa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decoder-only models*—These models are designed to generate text. The best
    way to think of them is that they take a partially written document and then produce
    a likely continuation of that document by predicting the next token. Famous examples
    include OpenAI’s GPT and Google’s Gemini.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Encoder-decoder models*—These models are also designed to generate text. Unlike
    decoder-only models, they take an entire passage of text and create a corresponding
    passage rather than continue the existing one. They are less popular than decoder-only
    models because they are more expensive to train, and their use is sometimes more
    challenging. For tasks with a clearly defined input and output sequence, encoder-decoder
    models tend to outperform decoder-only models. For example, they’re much better
    at translation and summarization tasks than decoder-only models. Famous examples
    include T5 and the algorithm that powers Google Translate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regardless of which type of transformer is used, the essential components of
    the model are built from three basic layers, just arranged in different ways internally.
    A reasonable analogy to their interchangeability is that of gasoline car engines:
    they all work similarly and have the same general components. How those components
    (read: layers) are put together within the engine (read: transformer) elicits
    various tradeoffs in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is a neural network layer?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'LLMs are one of many hundreds of algorithms that we now call *neural networks*.
    However, this is a misnomer in several ways. First, what constitutes a neural
    net-work approach today is very broad, to such a degree that referencing a “neural
    network–based approach” does not give the reader too much information about the
    exact approach described. Second, the *neural* part of the name has little or
    nothing to do with neuroscience or how the brain works. Sometimes, there is an
    intuitive “Hey, the brain kinda does something like this; can we mimic that behavior
    and get something useful out of it?” style of inspiration, but not for most current
    methods. Third, a neural network describes more of a standard agreement on assembling
    data structures rather than a particular algorithm. Think about build-ing a house:
    you use two-by-fours, sheetrock, and many options for cabinetry, paints, and design
    choices to assemble everything into a home. Each home looks unique but also familiar:
    they are all assembled in an expected way. The “layer” of a neural network is
    the smallest component, but you can use many types of layers in different ways.
    Transformers are one of many pieces that get assembled into a larger network.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Layers of the transformer model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3.1](#fig__transformer-basic) describes the essential components of
    the transformer model: the *embedding layer*, which generates representations
    of tokens that can hold more meaning; the *transformer layer*, which makes predictions
    based on word relationships; and the *output layer*, which transforms the numeric
    representations used within the transformer into words that humans can read.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F01_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 The basic components of the transformer model, consisting of the
    embedding layer, multiple transformer layers, and the output layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s look at these layers in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Embedding layer*—The embedding layer takes raw tokens as input and maps them
    into representations that capture each token’s meaning. For example, in chapter
    2, we discussed how tokens represent concepts, but individual tokens don’t have
    any relationship with each other. Consider the words “dog” and “wolf.” With our
    understanding of language, we know these terms are related, but we need some way
    of capturing this relationship within a neural network. This is precisely what
    the embedding layer does. It captures information about each token that encodes
    its meaning and allows us to express its conceptual relationship with other tokens.
    Consequently, we can capture the idea that the representations of the tokens `dog`
    and `wolf` are more similar to each other than the representations for the tokens
    `red` and `France`. You can think of the embedding layer as the part of the model
    that processes the words on a page and maps them to abstract conceptual representations
    in your head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transformer layer*—Transformer layers are where most of the computationhappens
    in a language model: they capture the relationships between words created by the
    embedding layer and do the bulk of the actual work to obtain the output. While
    LLMs generally only have one embedding layer and one output layer, they have many
    transformer layers. More powerful models have more transformer layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is tempting to describe the transformer layer as the “thinking” part of the
    model. This definition erroneously implies that transformer layers (or the larger
    model built from them) can think, but thinking as humans do is self-reflecting
    and variable in duration and effort. You can think about something for a half-second
    or months, depending on the effort needed for the task. A transformer always repeats
    the same process with the same effort for every task. There is no introspection
    and no altering a transformer layer’s mental state. Thus, a better way to imagine
    a transformer layer is a set of *fuzzy rules*—*fuzzy* because they do not require
    exact matches (because embeddings might return something similar like “dog” to
    “wolf”) and *rules* because transformers have no flexibility. Once learning is
    complete, a transformer layer will do the same thing every time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Output layer*—After the model has done the computation, additional transformations
    are performed in the output layer to obtain a useful result. Most commonly, the
    output layer operates as the inverse of the embedding layer, transforming the
    result of the computation from the embeddings space, which captures concepts,
    back into token space, which captures actual subwords to build text output. You
    can think of this as the part of the model that takes the answer you’ve decided
    on and then chooses the actual words to express that answer on a page by selecting
    the words most likely to represent the concepts that make up the answer. Finally,
    we end with an unembedding process, which converts the embeddings into tokens.
    Because each token has a one-to-one mapping to a subword, we can use a simple
    dictionary or map to convert the tokens into human-readable text again. This process
    is detailed in figure [3.2](#fig__full_function).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.2 Exploring the transformer architecture in detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To further understand what is happening inside an LLM, it can be helpful to
    reframe what we described as a sequence of steps. So let us do that in figure
    [3.2](#fig__full_function), which describes seven steps. We’ll mark each of these
    with reference to the section where we covered it before or tell you when it is
    a new detail we are about to explain. This chapter provides a lot of information
    at once, so we will break it down piece by piece as we go.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 The process for converting input into output using a large language
    model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The seven steps to an LLM are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Map text to tokens (chapter 2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map tokens into embedding space (*new*, subsection [*Representing tokens with
    vectors*](#sec__chp3_token_convert)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add information to each embedding that captures each token’s position in the
    input text (*new*, subsection [*Adding positional information*](#sec__chp3_pos_embed)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the data through a transformer layer (repeat ![equation image](../Images/eq-chapter-3-32-1.png)
    times) (*new*, subsection [3.2.2](#sec__chp3_trans_layer)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the unembedding layer to get tokens that could make good responses (*new*,
    subsection [3.2.3](#sec__chp3_output)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample from the list of possible tokens to generate a single response (*new*,
    subsection [*Sampling tokens to produce output*](#sec__chp3_sampling)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decode tokens from the response into actual text (chapter 2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.2.1 Embedding layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a lot of nuances to tokenization, embeddings, and how precisely language
    gets translated into things that models can understand. The most important nuance
    is that neural networks still don’t work with tokens directly. On the whole, neural
    networks need numbers that can be manipulated, and a token has a fixed numeric
    identity. We cannot change the identity of a token because the identity allows
    us to convert tokens back to human-readable text. We need a layer that will transform
    tokens in numeric form into the words or subwords they represent.
  prefs: []
  type: TYPE_NORMAL
- en: Representing tokens with vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our transformer needs numbers to work on. By this, we mean *continuous* numbers,
    so any fractional value is available for us to use: 0.3, -5, 3.14, etc. We also
    need more than one number to represent every token to capture nuances of meaning
    and relationships between tokens. If you tried to use just one number to represent
    each word, you would encounter difficulties capturing a word’s multiple meanings,
    synonyms, antonyms, and the relationships that those create. For example, you
    may well want to say that the antonym (opposite) of a word should be achievable
    by multiplying a word by ![equation image](../Images/eq-chapter-3-39-1.png). As
    figure [3.3](#fig__one_number_problem) shows, this quickly leads to silly conclusions
    about word relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F03_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 If you use just one number to represent a token, you quickly encounter
    problems where similar/dissimilar words cannot be made to fit each other. Here
    we see how trying to represent simple synonym/antonym relationships quickly becomes
    nonsensical even with just a handful of words.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For example, say we have a token for `stock` that we have arbitrarily decided
    will be converted to some number (e.g., `5.2`). I want to give related financial
    words, such as `capital`, a similar number (e.g., `5.3`) because they have similar
    meanings. There are also antonyms of `stock`’s other meanings, such as `rare`.
    Let’s say we use a negative value to capture the idea of an antonym and give it
    a value of `-5.2`. But now things get complex because another antonym of `capital`
    is `debt`. But if antonyms are negations, `debt` and `rare` have a similar meaning,
    which is nonsensical. Figure [3.3](#fig__one_number_problem) illustrates the problem:
    when we use a single number to represent a word, we cannot encode their relationships
    without implying weird relationships with other words, and we have not even gotten
    past four words yet!'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F04_Boozallen.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 Adding another dimension to our token representation allows us to
    represent a more diverse arrangement of semantic relationships. Here we see how
    two dimensions can capture relationships for multiple meanings of the same word.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The trick is to use multiple numbers to represent each token, allowing you to
    find better representations that accommodate the different relationships between
    words. An example that uses two numbers is shown in figure [3.4](#fig__vectors).
    We can see things like `bland` being nearly equidistant from `rare` and `well-done`,
    while also having space for `bank` to be far away from all three just mentioned
    words and instead be near `stock`. We were even able to throw in a few extra words.
    The more numbers you use, called *dimensions* in the field’s jargon, the more
    complex relationships you can represent.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If more dimensions are better at capturing subtle meaning, why not use as many
    dimensions as possible to represent our data? When dealing with a large number
    of dimensions, several problems arise. One primary concern is that LLMs deal with
    many embeddings, and adding more dimensions increases the memory and computation
    required to store and process embeddings. Furthermore, as we add more dimensions,
    the size of the semantic space explodes, and the amount of data and time needed
    to train a machine learning model to learn about all locations in the semantic
    space similarly grows exponentially. Mathematician Richard E. Bellman coined the
    term the “curse of dimensionality“ to describe this phenomenon because while we
    want to create a space capable of capturing nuanced meaning, we are limited by
    the fundamental properties of the space we create.
  prefs: []
  type: TYPE_NORMAL
- en: In LLM parlance, the lists of numbers used to represent tokens are referred
    to as *embeddings*. You can think of an embedding as an array or list of floating-point
    values. As a shorthand, we call such arrays *vectors*. Each position in the vector
    is called a dimension. As we show in figure [3.4](#fig__vectors), using multiple
    dimensions allows us to capture subtleties in relationships between words in human
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Since embeddings exist in multiple dimensions, we often state that they live
    in a *semantic space*. In some machine learning applications, this is called a
    *latent space*, especially when not dealing with text. Semantic space is wishy-washy
    jargon that isn’t well defined in the field, but it is most commonly used as a
    shorthand for saying that the vector embeddings that represent each token are
    well behaved in that synonyms/antonyms have nearer/farther distances and that
    we can use those relationships productively. As an example, in figure [3.5](#fig__kingQueenWoman),
    we show a famous case where a “make female” transformation can be built by subtracting
    the embedding for `male` and adding the embedding for `female`. This transformation
    can be applied to many different male-gendered words to find female-gendered words
    of the same concept. The co-location of all the “royal” words in the bottom right
    of figure [3.5](#fig__kingQueenWoman) is also intentional, as many different kinds
    of relationships can be simultaneously maintained in a high-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F05_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 A demonstration of how the relationships between embeddings create
    a semantic space. Words with similar meanings are near each other, and the same
    transformation can be applied to multi-ple words to yield a similar result—in
    this instance, a transformation to find the feminine version of a masculine word.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Shockingly, we cannot guarantee that these semantic relationships will form
    during the training process. It just so happens that they often do, and they were
    discovered to be very useful. By extension, the relationships in a semantic space
    are not foolproof, and biases in your data can seep in. For example, models will
    often determine that `doctor` is more similar to `male` and `nurse` is more similar
    to `female` because, in the generally available text used to build most models,
    it is more common for doctors to be described as male and nurses as female. The
    relationships are thus not a discovered truth of the world but a reflection of
    the data that went into the process.
  prefs: []
  type: TYPE_NORMAL
- en: Adding positional information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One critical problem is that a standard transformer does not understand sequential
    information. If you gave the transformer one sentence and rearranged all the tokens,
    it would view all possible permutations of the tokens as identical! That problem
    is illustrated in figure [3.6](#fig__jumbled_order).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F06_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 Without positional information, transformers do not understand that
    their inputs have a specific order, and all possible reorganizations of the tokens
    look identical to the algorithm. This is problematic because word order can change
    the word’s context or, if done randomly, become gibberish.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For this reason, the embedding layer generates two different kinds of embeddings.
    First, it creates a *word embedding* that captures the meaning of the token, and
    second, it makes a *positional embedding* that captures the token’s location in
    a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is surprisingly simple. Just as we mapped every unique token to a unique
    meaning vector, we will also map every unique token position (first, second, third,
    and so on) to a position vector. So each token will get embedded twice—once for
    its identity and again for its position. These two vectors are then added to create
    one vector representing the word and its location in the sentence. This process
    is outlined in figure [3.7](#fig__embd_token_and_pos).
  prefs: []
  type: TYPE_NORMAL
- en: Note Using multiple dimensions instead of absolute positions makes iteasier
    for the transformer to learn relative positioning, even if it is exces-sively
    redundant for us as humans [1].
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F07_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 Word embeddings do not capture the fact that input tokens appear
    in a specific order. This information is captured by a positional embedding. The
    position embeddings work the same way as word embeddings and are added together.
    The resulting combined embeddings have the information the model needs to understand
    the order of tokens.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Those are all the missing details required to understand how tokens are converted
    into vectors for the transformer layers. This strategy may seem somewhat naive,
    and that is honestly true. People have tried developing more sophisticated methods
    to handle this information, but this simple approach of “Let’s make everything
    a vector and just add them together” works surprisingly well. Importantly, it
    has also demonstrated success in video and images. Having a straightforward strategy
    that functions well enough for many different problems is valuable, which is why
    this naive approach has taken hold.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Transformer layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transformer layer aims to transform the input into a more useful output.
    Most prior neural network layers, such as an embedding layer, are designed to
    incorporate very specific beliefs about how the world works into their operation.
    The idea is that if the encoded belief is accurate to how the world does indeed
    work, your model will reach a better solution using less data. Transformers go
    for the opposite strategy. They encode a general-purpose mechanism that can learn
    many tasks if you get enough data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, transformers operate with three primary components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Query*—Queries are vectors (from an embedding layer) that represent what you
    are looking for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Key*—Key vectors represent the possible answers to pair a query against.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Value*—Every key has a corresponding value vector, the actual value to bereturned
    when a query and key match.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This terminology corresponds to the behavior of a `dict` or dictionary object
    in Python. You look up an item in the dictionary by its key so that you can then
    create some useful output. The difference is that a transformer is fuzzy. It’s
    not that we are looking up a single key, but we are evaluating *all keys*, weighted
    by their degree of similarity to the query. Figure [3.8](#fig__qkv) shows how
    this works with a simple example. While the queries and keys are shown as strings,
    those strings are stand-ins for the vectors that each string will be mapped to
    via the embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F08_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 An example of how queries, keys, and values work inside a transformer
    compared to a Python dictionary. When a Python dictionary matches queries to keys,
    it needs an exact match to find the value, or it will return nothing. A transformer
    always returns something based on the most similar matches between queries and
    keys.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Having every key contribute to one query could be chaotic, especially if there
    is one true match between a query and a specific key. This problem is handled
    by a detail called *attention* or the *attention mechanism*.
  prefs: []
  type: TYPE_NORMAL
- en: Attention inside a transformer can be considered similar to your ability to
    pay attention to what is important. You can tune out irrelevant and distracting
    information (i.e., bad keys) and focus primarily on what is important (the best
    matching keys). The analogy extends further in that attention is adaptive; what
    is important is a function of what other options are available. Your boss giving
    you directions for the week takes up your attention, but the fire alarm going
    off changes your attention away from your boss to the alarm (and a potential fire).
  prefs: []
  type: TYPE_NORMAL
- en: When generating the next token, a transformer takes the *query* for the current
    token and compares it to the *key* for all previous tokens. Comparing the query
    and the key generates a series of values that the attention mechanism uses to
    calculate how much weight it should assign each potential following token when
    deciding which token to generate next. The *value* for each token tells the model
    what each previous token thinks its contribution to the probability should be.
    The attention function then computes the next token, as shown in figure [3.9](#fig__QKV_nextWord).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F09_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 The next token in a sentence is predicted by using the current token
    as the query and calculating matches with the preceding words as the keys. The
    individual values themselves do not need to exist in the semantic space; the output
    of the attention mechanism produces something similar to one of the tokens in
    the vocabulary.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: What is the math of attention?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will not go into every detail of the math behind attention because it would
    take a lot of space to describe it, and it has been covered elsewhere. We did
    so in a previous book: chapter 11 of *Inside Deep Learning* [2] explains transformers
    and attention in much greater technical detail.'
  prefs: []
  type: TYPE_NORMAL
- en: For the curious, the primary equation is
  prefs: []
  type: TYPE_NORMAL
- en: (3.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![equation image](../Images/eq-chapter-3-73-1.png)'
  prefs: []
  type: TYPE_IMG
- en: (3.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![equation image](../Images/eq-chapter-3-74-1.png)'
  prefs: []
  type: TYPE_IMG
- en: The queries, keys, and values are represented by individual matrices ![equation
    image](../Images/eq-chapter-3-75-1.png), ![equation image](../Images/eq-chapter-3-75-2.png),
    and ![equation image](../Images/eq-chapter-3-75-3.png), respectively. Matrix multiplication
    makes attention efficient when implemented on GPUs because they can perform many
    multiplication operations in parallel. The softmax function implements the main
    component of the attention analogy byassigning many values nearly equal to zero,
    which causes the transformer toignore the unimportant items.
  prefs: []
  type: TYPE_NORMAL
- en: The final step of *norm* and *Feedforward* is the application of *layer normalization*
    and a *linear layer* via a *skip connection*. If these terms aren’t familiar to
    you, that is fine; you do not need to know this math to understand the rest of
    the book. If you want to learn what these terms mean, we refer you to *Inside
    Deep Learning* [2] for a technically detailed understanding.
  prefs: []
  type: TYPE_NORMAL
- en: A transformer model is made up of dozens of transformer layers. The intermediate
    transformer layers perform the same mechanical task described in figure [3.9](#fig__QKV_nextWord)
    despite not having to predict a token because the last transformer layer is the
    only one that needs to predict an actual token. The transformer layer is general
    enough that combining many intermediate layers allows the model to learn complex
    tasks such as sorting, stacking, and other sophisticated input transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Unembedding layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last stage of an LLM is the unembedding layer, which transforms the numeric
    vector representation that transformers use into a specific output token so that
    we can ultimately return the text that corresponds to that token. This output
    generation process is also called *decoding* because we decode the transformer
    vector representation to a piece of output text. It is a crucial component for
    using an LLM to generate text. Not only is decoding the current token essential
    for producing output, but the next token will depend on each previous token selected
    for output. This process is shown in figure [3.10](#fig__transformer_decode_loop),
    where we recursively generate tokens one at a time. In statistical parlance, this
    is known as an *autoregressive* process, meaning each element of the output is
    based on the output that came before it.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F10_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 Producing output from LLMs involves converting from documents to
    tokens and then using the model to produce output. We loop through this process
    to both consume text and generate human-readable output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You may be wondering how this process stops. When we build the vocabulary of
    tokens, we include some special tokens that do not occur in the text. One of these
    special tokens is an *end of sequence* (EoS) token. The model trains on texts
    with natural endpoints that are finished with the EoS marker, and when the model
    generates a new token, the EoS token is one of the options it can generate. If
    the EoS is generated, we know it is time to stop the loop and return the full
    text to the user. It is also a good idea to keep a maximum generation limit if
    your model gets into a bad state and fails to generate the EoS token.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling tokens to produce output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'What is missing from this process is how we convert a vector, an array of floating-point
    numbers produced by the transformer layers, into a single token. This process
    is called *sampling* because it uses a statistical method to choose sample tokens
    from the vocabulary based on the LLM’s input and its output so far. The LLM’s
    sampling algorithm evaluates those samples to select which token to produce. There
    are several techniques for doing this sampling, but all follow the same basic
    two-step strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: For each token in the vocabulary, compute the probability that each token will
    be the next selected token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly pick a token according to the probabilities calculated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have used ChatGPT or other LLMs, you may have noticed that they do not
    always provide the same output for the same input. The decoding step is why you
    may get different answers whenever you ask the same question.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem counterintuitive that tokens are selected randomly. However, it
    is a critical component to generating good-quality text. Consider the example
    of text generation in figure [3.11](#fig__token_random_next), where we are trying
    to finish the sentence “I love to eat.” It would be unrealistic if the model always
    picked “sushi” as the next token because it had the highest probability. If someone
    always said “sushi” to you in this context, you would think something was off.
    We need randomness to handle the fact that there are multiple valid choices, and
    not all options are likely to occur.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F11_Boozallen.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 We demonstrate text generation by starting with the phrase "I love
    to eat" and then showing that some possible completions that are foods, such as
    barbeque and sushi, have high proba-bilities, while a car and the number 42 have
    low probabilities. Weighted random selection chooses the word *tacos*. The generation
    loop is stopped when the EoS token appears.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Also note in the example from figure [3.11](#fig__token_random_next) that other
    tokens would be nonsensical, like 42, given tiny probabilities. Again, we need
    to assign every token a probability to know which tokens are likely or unlikely.
  prefs: []
  type: TYPE_NORMAL
- en: How do you get probabilities for tokens?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Each possible next token has a different probability of being selected. Most
    of the tokens have nearly zero chance of being selected. A keen reader may wonder:
    How can we assign a probability to a token before knowing the other tokens? We
    do so by giving every token a score, indicating how good a match that token’s
    embedding is compared to the current vector (i.e., the output from the transformer).
    The score is arbitrary from ![equation image](../Images/eq-chapter-3-91-1.png)
    to ![equation image](../Images/eq-chapter-3-91-2.png) and calculated independently
    for each token. The relative difference in scores is then used to create probabilities.
    For example, if one token had a score of 65.2 and a second token had a score of
    -5.0, the probabilities would be near 100% and 0% for the individual token, respectively.
    If the scores were 65.2 and 65.1, the probabilities would be near 50.5% and 49.5%,
    respectively. Similarly, scores of 0.2 and 0.1 would give the same probabilities
    as the scores 65.2 and 65.1 because we are looking at relative differences in
    scores to assign probabilities, not the individual scores themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: A transformer sometimes gives you unusual or nonsensical generations. It’s not
    common, but the other tokens have a *near-zero* probability, and eventually, one
    weird token will get picked that you would not expect. Once an unexpected token
    has been chosen, all future generated tokens will be produced in a manner that
    tries to make sense of the unusual generation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the LLM produced “I love to eat *chalk*,” you would be pretty
    surprised. But it is not overly unreasonable because chalk-eating is a symptom
    of the medical condition called pica. Once the word *chalk* is selected, the LLM
    may go into a tangent about pica or some other medical diatribe—that is, of course,
    if you are so lucky that your unusual generation is in the sphere of “rare but
    reasonable” and not an utterly errant prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Note Many algorithms can compute the final probabilities used to select words
    for generation. One of these is nucleus sampling, also known as Top-p sampling,
    which involves determining the tokens with the highest probability as potential
    outputs and choosing tokens to output from that list. This method can help us
    avoid unreasonable predictions. If you can, you want to check which sampling algorithm
    your LLM uses so that you can understand its risks of producing rarer to unreasonable
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 The tradeoff between creativity and topical responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on how your users plan to interact with an LLM, generating surprising
    or creative outputs may be desired. Say you are using an LLM to help brainstorm
    new product ideas, and you are using a chatbot as a digital sounding board to
    spark ideas. In this case, you probably want unusual outputs generated because
    the goal is to be creative and think of something new.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, sometimes creativity is wholly undesired. One potential use for
    LLMs is offline search, where you could fit an LLM on a (relatively powerful)
    mobile phone and ask/look up information even when you do not have internet connectivity.
    In this case, you want the outputs of the LLM to be reliable, on topic, and factual.
    A creative reinterpretation is not needed.
  prefs: []
  type: TYPE_NORMAL
- en: A feature in LLMs called *temperature* balances this tradeoff. The temperature
    variable (which is a number between 0 and 1 and often has a default value of 0.7
    or 0.8) is used to exaggerate the probability of low-likelihood tokens (high temperature)
    or depress the probability of low-likelihood tokens (low temperature).
  prefs: []
  type: TYPE_NORMAL
- en: Consider molecules in a glass of water as an analogy. Say we want to know what
    molecule will be at the top of the glass (don’t ask us why; just go with it).
    If the glass was lowered to a temperature of absolute zero, all the molecules
    would be still, and the molecule at the top of the glass would reliably be the
    same each time (i.e., you will always generate the same token). If you raise the
    temperature of the glass so much that it starts to boil, the molecules will bounce
    around, making the molecule at the top of the glass essentially random (i.e.,
    you get a completely random token). As you scale the temperature up and down,
    you change the balance between picking with greater randomness (and, thus, often
    creativity) or focusing on just the most likely next token (thus keeping the generation
    more topical).
  prefs: []
  type: TYPE_NORMAL
- en: In a practical sense, considering our example of “I like to eat,” a higher temperature
    would lead to the generation of different types of foods, not just pizza or sushi
    but possibly less typical or more specific foods like beef wellington or vegetarian
    chili.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Transformers in context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve covered a lot of ground in this chapter. Embedding layers, transformer
    layers, and unembedding layers are the core building blocks that make LLMs work.
    The concepts of how LLMs encode meaning and position and then use stacks of transformer
    layers to uncover the structure in text are all vital to understanding how LLMs
    capture information and produce the quality of output they are capable of. But
    we have more details to cover! How do we create these layers to generate embeddings
    and probabilities by analyzing piles and piles of data in the first place? In
    chapter 4, we will continue exploring how to feed data into this architecture
    and incentivize the LLM to “learn” meaningful relationships in text through the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While LLMs use tokens as their basic unit of semantic meaning, they’remathematically
    represented within the model as *embedding vectors* rather than as strings. These
    embedding vectors can capture relationships about nearness, dissimilarity, antonyms,
    and other linguistic-descriptive properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Position and word order do not come naturally to transformers and are obtained
    via another vector representing the relative position. The model can represent
    word order by adding the position and word embedding vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer layers act as a kind of fuzzy dictionary, returning approximate
    answers to approximate matches. This fuzzy process is called attention and uses
    the terms *query*, *key*, and *value* as analogous to the key and value in a Python
    dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT is an example of a decoder-only transformer, but encoder-only transformers
    and encoder-decoder transformers also exist. Decoder-only transformers are best
    at generating text, but other types of transformers can be better at other tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are autoregressive, meaning they work recursively. All previously generated
    tokens are fed into the model at each step to get the next token. Simply put,
    autoregressive models predict the next thing using the previous things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of any transformer isn’t tokens; instead, the output is a probability
    for how likely every token is. Selecting a specific token is called *unembedding*
    or *sampling* and includes some randomness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strength of randomness can be controlled, resulting in more or less realistic
    output, more creative or unique output, or more consistent output. Most LLMs have
    a default threshold for randomness that is reasonable looking, but you may want
    to change it for different uses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
