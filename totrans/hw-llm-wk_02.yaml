- en: '3 Transformers: How inputs become outputs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3种变换器：输入如何变成输出
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Converting tokens into vectors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记转换为向量
- en: Transformers, their types, and their roles
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器，它们的类型及其作用
- en: Converting vectors back into tokens
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将向量转换回标记
- en: Creating the text generation loop
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建文本生成循环
- en: In chapter 2, we saw how large language models (LLMs) see text as fundamental
    units known as tokens. Now it’s time to talk about what LLMs do with the tokens
    they see. The process that LLMs use to generate their text is markedly different
    from how humans form coherent sentences. When an LLM operates, it is working on
    tokens, yet simultaneously cannot *manipulate* tokens like humans do because the
    LLM does not understand the structure and relationship of the letters each token
    represents.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们看到了大型语言模型（LLMs）如何将文本视为称为标记的基本单元。现在，是时候讨论LLMs对它们看到的标记做了什么。LLMs生成文本的过程与人类形成连贯句子的方式明显不同。当LLM运行时，它正在处理标记，但同时又不能像人类那样操纵标记，因为LLM不理解每个标记所代表的字母的结构和关系。
- en: For example, English speakers know that the words “magic,” “magical,” and “magician”
    are all related. We can understand that sentences containing these words are all
    connected to the same subject matter because these words share a common root.
    However, LLMs that operate on integers representing tokens that make up these
    words cannot understand the relationships between tokens without additional work
    to make those connections.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，英语使用者知道“magic”、“magical”和“magician”这些词都是相关的。我们可以理解，包含这些词的句子都与同一主题相关，因为这些词有一个共同的词根。然而，操作于代表这些词的整数标记的LLMs，如果不能进行额外的工作来建立这些联系，就无法理解标记之间的关系。
- en: For this reason, LLMs follow a long history in machine learning and deep learning
    of performing a kind of cyclical conversion. First, tokens are converted into
    a numeric form that deep learning algorithms can work on. Then, the LLM converts
    this numeric representation back into a new token. This cycle repeats iteratively,
    which is not comparable to how humans work. You would be incredibly concerned
    if your colleagues had to pull out a calculator to perform several math problems
    between each word they spoke.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLMs遵循机器学习和深度学习中的长期历史，执行一种循环转换。首先，标记被转换为深度学习算法可以处理的数字形式。然后，LLM将这种数字表示转换回新的标记。这个周期会迭代重复，这与人类的工作方式不可比。如果你的同事在每说一句话之间都要拿出计算器来解决几个数学问题，你会感到非常担忧。
- en: Yet this process is, indeed, how LLMs produce outputs. In this chapter, we will
    walk through the process in two stages. First, we will review the entire process
    at a high level to introduce fundamental concepts and construct a mental model
    of how LLMs generate text. Next, this model will serve as a scaffolding for a
    more in-depth discussion of the details and design choices associated with the
    components that LLMs use to capture the relationships between words and language
    and, ultimately, generate the output we are familiar with.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个过程确实是LLMs生成输出的方式。在本章中，我们将分两个阶段来介绍这个过程。首先，我们将从高层次上回顾整个过程，以介绍基本概念并构建LLMs生成文本的心理模型。接下来，这个模型将作为深入讨论LLMs使用的组件的细节和设计选择的脚手架，这些组件用于捕捉单词和语言之间的关系，并最终生成我们熟悉的输出。
- en: 3.1 The transformer model
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 变换器模型
- en: 'Many LLMs you encounter today interpret tokens and produce output using a software
    architecture known as a transformer. This architecture consists of a collection
    of algorithms and data structures that store information by representing it as
    numbers in a neural network. At their core, transformers are sequence prediction
    algorithms. While it is common to describe them as “reasoning” or “understanding”
    language, what they actually do is predict tokens. Transformers come with three
    different approaches to token prediction. While we focus on the famous GPT architecture
    (more formally known as decoder-only models), it is also worth introducing encoder-only
    and encoder-decoder models:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你今天遇到的许多大型语言模型（LLMs）使用一种称为变换器的软件架构来解释标记并生成输出。这种架构由一系列算法和数据结构组成，通过将信息表示为神经网络中的数字来存储信息。在核心上，变换器是序列预测算法。虽然通常描述它们为“推理”或“理解”语言，但它们实际上所做的只是预测标记。变换器提供了三种不同的标记预测方法。虽然我们专注于著名的GPT架构（更正式地称为仅解码器模型），但也值得介绍仅编码器和编码器-解码器模型：
- en: '*Encoder-only models*—These models are designed to create knowledge representations
    that can be used to perform tasks—that is, to encode the input into a numerical
    representation that is more useful to an algorithm. The best way to think of them
    is that they take text and process it into a form that is easier for a machine
    learning algorithm to use. They are widely used in scientific research. Famous
    examples include BERT and RoBERTa.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅编码模型*—这些模型被设计用来创建可用于执行任务的认知表示——也就是说，将输入编码成对算法更有用的数值表示。最好的思考方式是，它们将文本处理成机器学习算法更容易使用的形式。它们在科学研究中被广泛使用。著名的例子包括BERT和RoBERTa。'
- en: '*Decoder-only models*—These models are designed to generate text. The best
    way to think of them is that they take a partially written document and then produce
    a likely continuation of that document by predicting the next token. Famous examples
    include OpenAI’s GPT and Google’s Gemini.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅解码器模型*—这些模型被设计用来生成文本。最好的思考方式是，它们接受部分撰写的文档，然后通过预测下一个标记来生成该文档的可能续篇。著名的例子包括OpenAI的GPT和Google的Gemini。'
- en: '*Encoder-decoder models*—These models are also designed to generate text. Unlike
    decoder-only models, they take an entire passage of text and create a corresponding
    passage rather than continue the existing one. They are less popular than decoder-only
    models because they are more expensive to train, and their use is sometimes more
    challenging. For tasks with a clearly defined input and output sequence, encoder-decoder
    models tend to outperform decoder-only models. For example, they’re much better
    at translation and summarization tasks than decoder-only models. Famous examples
    include T5 and the algorithm that powers Google Translate.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码-解码模型*—这些模型也被设计用来生成文本。与仅解码器模型不同，它们接受整段文本并创建相应的段落，而不是继续现有的段落。由于它们训练成本更高，且使用起来有时更具挑战性，因此它们不如仅解码器模型受欢迎。对于具有明确定义的输入和输出序列的任务，编码-解码模型往往优于仅解码器模型。例如，它们在翻译和摘要任务上比仅解码器模型表现得更好。著名的例子包括T5和为Google
    Translate提供动力的算法。'
- en: 'Regardless of which type of transformer is used, the essential components of
    the model are built from three basic layers, just arranged in different ways internally.
    A reasonable analogy to their interchangeability is that of gasoline car engines:
    they all work similarly and have the same general components. How those components
    (read: layers) are put together within the engine (read: transformer) elicits
    various tradeoffs in performance.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不论使用哪种类型的Transformer，模型的基本组件都是由三个基本层构建的，只是内部排列方式不同。一个合理的类比是汽油车引擎：它们都工作得类似，并且具有相同的一般组件。这些组件（即层）如何在引擎（即Transformer）内部组合在一起，会引发各种性能权衡。
- en: What exactly is a neural network layer?
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 神经网络层究竟是什么？
- en: 'LLMs are one of many hundreds of algorithms that we now call *neural networks*.
    However, this is a misnomer in several ways. First, what constitutes a neural
    net-work approach today is very broad, to such a degree that referencing a “neural
    network–based approach” does not give the reader too much information about the
    exact approach described. Second, the *neural* part of the name has little or
    nothing to do with neuroscience or how the brain works. Sometimes, there is an
    intuitive “Hey, the brain kinda does something like this; can we mimic that behavior
    and get something useful out of it?” style of inspiration, but not for most current
    methods. Third, a neural network describes more of a standard agreement on assembling
    data structures rather than a particular algorithm. Think about build-ing a house:
    you use two-by-fours, sheetrock, and many options for cabinetry, paints, and design
    choices to assemble everything into a home. Each home looks unique but also familiar:
    they are all assembled in an expected way. The “layer” of a neural network is
    the smallest component, but you can use many types of layers in different ways.
    Transformers are one of many pieces that get assembled into a larger network.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是我们现在称之为 *神经网络* 的数百种算法之一。然而，这种说法在几个方面都是不准确的。首先，构成当前神经网络方法的内容非常广泛，以至于提到“基于神经网络的方案”并不能给读者提供太多关于具体方法的详细信息。其次，名称中的
    *神经* 部分与神经科学或大脑的工作方式几乎没有关系。有时，可能会有一种直观的“嘿，大脑有点这样做；我们能否模仿这种行为并从中得到一些有用的东西？”这样的灵感，但对于大多数当前的方法来说并不是这样。第三，神经网络更多地描述了一种关于组装数据结构的标准协议，而不是特定的算法。想想建造一栋房子：你使用两英寸乘四英寸的木材、石膏板，以及许多关于橱柜、油漆和设计选择的选项来组装成一座家园。每座房子看起来都是独一无二的，但也很熟悉：它们都是以预期的方式组装的。“层”是神经网络中最小的组件，但你可以用不同的方式使用许多类型的层。变换器是组装成更大网络的多块组件之一。
- en: 3.1.1 Layers of the transformer model
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 变换器模型的层级
- en: 'Figure [3.1](#fig__transformer-basic) describes the essential components of
    the transformer model: the *embedding layer*, which generates representations
    of tokens that can hold more meaning; the *transformer layer*, which makes predictions
    based on word relationships; and the *output layer*, which transforms the numeric
    representations used within the transformer into words that humans can read.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3.1](#fig__transformer-basic) 描述了变换器模型的基本组件：生成能够承载更多意义的标记表示的 *嵌入层*；基于单词关系的
    *变换器层*；以及将变换器内部使用的数值表示转换为人类可读的单词的 *输出层*。
- en: '![figure](../Images/CH03_F01_Boozallen.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F01_Boozallen.png)'
- en: Figure 3.1 The basic components of the transformer model, consisting of the
    embedding layer, multiple transformer layers, and the output layer
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.1 变换器模型的基本组件，包括嵌入层、多个变换器层和输出层
- en: 'Let’s look at these layers in detail:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些层级：
- en: '*Embedding layer*—The embedding layer takes raw tokens as input and maps them
    into representations that capture each token’s meaning. For example, in chapter
    2, we discussed how tokens represent concepts, but individual tokens don’t have
    any relationship with each other. Consider the words “dog” and “wolf.” With our
    understanding of language, we know these terms are related, but we need some way
    of capturing this relationship within a neural network. This is precisely what
    the embedding layer does. It captures information about each token that encodes
    its meaning and allows us to express its conceptual relationship with other tokens.
    Consequently, we can capture the idea that the representations of the tokens `dog`
    and `wolf` are more similar to each other than the representations for the tokens
    `red` and `France`. You can think of the embedding layer as the part of the model
    that processes the words on a page and maps them to abstract conceptual representations
    in your head.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*嵌入层*——嵌入层将原始标记作为输入，并将它们映射到能够捕捉每个标记意义的表示。例如，在第2章中，我们讨论了标记如何表示概念，但单个标记之间没有任何关系。考虑一下“狗”和“狼”这两个词。根据我们对语言的理解，我们知道这些术语是相关的，但我们需要一种方法来捕捉这种关系在神经网络中。这正是嵌入层所做的。它捕捉每个标记的信息，这些信息编码了其意义，并允许我们表达其与其他标记的概念关系。因此，我们可以捕捉到“狗”和“狼”这两个标记的表示比“红色”和“法国”这两个标记的表示更相似的想法。你可以把嵌入层看作是模型中处理页面上的单词并将它们映射到你头脑中的抽象概念表示的部分。'
- en: '*Transformer layer*—Transformer layers are where most of the computationhappens
    in a language model: they capture the relationships between words created by the
    embedding layer and do the bulk of the actual work to obtain the output. While
    LLMs generally only have one embedding layer and one output layer, they have many
    transformer layers. More powerful models have more transformer layers.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Transformer层*—在语言模型中，大部分计算都是在Transformer层中进行的：它们捕捉嵌入层创建的单词之间的关系，并完成大部分实际工作以获得输出。虽然LLMs通常只有一个嵌入层和一个输出层，但它们有许多Transformer层。更强大的模型有更多的Transformer层。'
- en: It is tempting to describe the transformer layer as the “thinking” part of the
    model. This definition erroneously implies that transformer layers (or the larger
    model built from them) can think, but thinking as humans do is self-reflecting
    and variable in duration and effort. You can think about something for a half-second
    or months, depending on the effort needed for the task. A transformer always repeats
    the same process with the same effort for every task. There is no introspection
    and no altering a transformer layer’s mental state. Thus, a better way to imagine
    a transformer layer is a set of *fuzzy rules*—*fuzzy* because they do not require
    exact matches (because embeddings might return something similar like “dog” to
    “wolf”) and *rules* because transformers have no flexibility. Once learning is
    complete, a transformer layer will do the same thing every time.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将Transformer层描述为模型的“思考”部分很有诱惑力。这个定义错误地暗示了Transformer层（或由它们构建的更大模型）可以思考，但像人类一样思考是自我反思的，并且持续时间和努力程度是变化的。你可以思考几秒钟或几个月，这取决于完成任务所需的努力。Transformer总是以相同的努力重复相同的流程。没有自我反思，也没有改变Transformer层的心理状态。因此，想象一个更好的Transformer层的方式是一组*模糊规则*—*模糊*是因为它们不需要精确匹配（因为嵌入可能会返回类似于“狗”到“狼”的东西），*规则*是因为Transformer没有灵活性。一旦学习完成，Transformer层将每次都做同样的事情。
- en: '*Output layer*—After the model has done the computation, additional transformations
    are performed in the output layer to obtain a useful result. Most commonly, the
    output layer operates as the inverse of the embedding layer, transforming the
    result of the computation from the embeddings space, which captures concepts,
    back into token space, which captures actual subwords to build text output. You
    can think of this as the part of the model that takes the answer you’ve decided
    on and then chooses the actual words to express that answer on a page by selecting
    the words most likely to represent the concepts that make up the answer. Finally,
    we end with an unembedding process, which converts the embeddings into tokens.
    Because each token has a one-to-one mapping to a subword, we can use a simple
    dictionary or map to convert the tokens into human-readable text again. This process
    is detailed in figure [3.2](#fig__full_function).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层*—在模型完成计算后，输出层会进行额外的转换以获得有用的结果。最常见的情况是，输出层作为嵌入层的逆过程，将计算结果从捕捉概念的嵌入空间转换回捕捉实际子词的标记空间，以构建文本输出。你可以将这部分模型视为选择实际单词来表达你已决定的答案的部分，通过选择最有可能代表构成答案的概念的单词。最后，我们通过一个反嵌入过程结束，将嵌入转换为标记。因为每个标记都与一个子词有一对一的映射，我们可以使用简单的字典或映射将标记再次转换为人类可读的文本。这个过程在图[3.2](#fig__full_function)中有详细说明。'
- en: 3.2 Exploring the transformer architecture in detail
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 详细探索Transformer架构
- en: To further understand what is happening inside an LLM, it can be helpful to
    reframe what we described as a sequence of steps. So let us do that in figure
    [3.2](#fig__full_function), which describes seven steps. We’ll mark each of these
    with reference to the section where we covered it before or tell you when it is
    a new detail we are about to explain. This chapter provides a lot of information
    at once, so we will break it down piece by piece as we go.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解LLM内部发生的事情，将我们描述的步骤重新框架化可能会有所帮助。因此，让我们在图[3.2](#fig__full_function)中这样做，它描述了七个步骤。我们将用参考标记来标记这些步骤，指出我们之前覆盖的章节，或者告诉你它是一个即将解释的新细节。本章提供的信息很多，因此我们将随着我们的进展逐步分解。
- en: '![figure](../Images/CH03_F02_Boozallen.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F02_Boozallen.png)'
- en: Figure 3.2 The process for converting input into output using a large language
    model
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 使用大型语言模型将输入转换为输出的过程
- en: 'The seven steps to an LLM are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的七个步骤如下：
- en: Map text to tokens (chapter 2).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本映射到标记（第2章）。
- en: Map tokens into embedding space (*new*, subsection [*Representing tokens with
    vectors*](#sec__chp3_token_convert)).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记映射到嵌入空间（*新增*，子节 [*使用向量表示标记*](#sec__chp3_token_convert)）。
- en: Add information to each embedding that captures each token’s position in the
    input text (*new*, subsection [*Adding positional information*](#sec__chp3_pos_embed)).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个嵌入添加信息，以捕捉输入文本中每个标记的位置（*新增*，子节 [添加位置信息](#sec__chp3_pos_embed)）。
- en: Pass the data through a transformer layer (repeat ![equation image](../Images/eq-chapter-3-32-1.png)
    times) (*new*, subsection [3.2.2](#sec__chp3_trans_layer)).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据通过一个转换层（重复 ![方程式图像](../Images/eq-chapter-3-32-1.png) 次）(*新增*，子节 [3.2.2](#sec__chp3_trans_layer)）。
- en: Apply the unembedding layer to get tokens that could make good responses (*new*,
    subsection [3.2.3](#sec__chp3_output)).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用反嵌入层以获取可能构成良好响应的标记（*新增*，子节 [3.2.3](#sec__chp3_output)）。
- en: Sample from the list of possible tokens to generate a single response (*new*,
    subsection [*Sampling tokens to produce output*](#sec__chp3_sampling)).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从可能的标记列表中采样以生成单个响应（*新增*，子节 [*采样标记以生成输出*](#sec__chp3_sampling)）。
- en: Decode tokens from the response into actual text (chapter 2).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将响应中的标记解码为实际文本（第2章）。
- en: 3.2.1 Embedding layers
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 嵌入层
- en: There are a lot of nuances to tokenization, embeddings, and how precisely language
    gets translated into things that models can understand. The most important nuance
    is that neural networks still don’t work with tokens directly. On the whole, neural
    networks need numbers that can be manipulated, and a token has a fixed numeric
    identity. We cannot change the identity of a token because the identity allows
    us to convert tokens back to human-readable text. We need a layer that will transform
    tokens in numeric form into the words or subwords they represent.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化、嵌入以及语言如何精确地被转化为模型能理解的事物有很多细微之处。最重要的细微之处是神经网络仍然不能直接处理标记。总体来说，神经网络需要可以操作的数字，而标记有一个固定的数字标识。我们不能改变标记的标识，因为标识允许我们将标记转换回人类可读的文本。我们需要一个层，将数字形式的标记转换成它们所代表的单词或子词。
- en: Representing tokens with vectors
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用向量表示标记
- en: 'Our transformer needs numbers to work on. By this, we mean *continuous* numbers,
    so any fractional value is available for us to use: 0.3, -5, 3.14, etc. We also
    need more than one number to represent every token to capture nuances of meaning
    and relationships between tokens. If you tried to use just one number to represent
    each word, you would encounter difficulties capturing a word’s multiple meanings,
    synonyms, antonyms, and the relationships that those create. For example, you
    may well want to say that the antonym (opposite) of a word should be achievable
    by multiplying a word by ![equation image](../Images/eq-chapter-3-39-1.png). As
    figure [3.3](#fig__one_number_problem) shows, this quickly leads to silly conclusions
    about word relationships.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的转换器需要数字来工作。这里的“连续”数字意味着任何分数值都可以供我们使用：0.3，-5，3.14等。我们还需要多个数字来表示每个标记，以捕捉意义的细微差别和标记之间的关系。如果你试图只用一个数字来表示每个单词，你将难以捕捉到单词的多种含义、同义词、反义词以及它们所建立的关系。例如，你可能希望说一个单词的反义词（对立面）可以通过将单词乘以
    ![方程式图像](../Images/eq-chapter-3-39-1.png) 来实现。如图 [3.3](#fig__one_number_problem)
    所示，这很快就会导致关于单词关系的荒谬结论。
- en: '![figure](../Images/CH03_F03_Boozallen.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH03_F03_Boozallen.png)'
- en: Figure 3.3 If you use just one number to represent a token, you quickly encounter
    problems where similar/dissimilar words cannot be made to fit each other. Here
    we see how trying to represent simple synonym/antonym relationships quickly becomes
    nonsensical even with just a handful of words.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3 如果你只用一个数字来表示一个标记，你很快就会遇到相似/不同单词无法相互适应的问题。在这里，我们看到尝试表示简单的同义词/反义词关系，即使只有几个单词，也会变得毫无意义。
- en: 'For example, say we have a token for `stock` that we have arbitrarily decided
    will be converted to some number (e.g., `5.2`). I want to give related financial
    words, such as `capital`, a similar number (e.g., `5.3`) because they have similar
    meanings. There are also antonyms of `stock`’s other meanings, such as `rare`.
    Let’s say we use a negative value to capture the idea of an antonym and give it
    a value of `-5.2`. But now things get complex because another antonym of `capital`
    is `debt`. But if antonyms are negations, `debt` and `rare` have a similar meaning,
    which is nonsensical. Figure [3.3](#fig__one_number_problem) illustrates the problem:
    when we use a single number to represent a word, we cannot encode their relationships
    without implying weird relationships with other words, and we have not even gotten
    past four words yet!'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个表示`stock`的标记，我们任意决定将其转换为某个数字（例如，`5.2`）。我想给相关的金融词汇，如`capital`，一个类似的数字（例如，`5.3`），因为它们有相似的意义。`stock`的其他含义也有反义词，例如`rare`。让我们假设我们使用负值来捕捉反义的概念，并给它一个值为`-5.2`。但现在事情变得复杂，因为`capital`的另一个反义词是`debt`。但如果反义词是否定，那么`debt`和`rare`有相似的意义，这是不合逻辑的。图[3.3](#fig__one_number_problem)说明了这个问题：当我们用一个数字来表示一个词时，我们无法编码它们的关系，而不暗示与其他词的奇怪关系，而且我们还没有处理过四个词！
- en: '![figure](../Images/CH03_F04_Boozallen.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F04_Boozallen.jpg)'
- en: Figure 3.4 Adding another dimension to our token representation allows us to
    represent a more diverse arrangement of semantic relationships. Here we see how
    two dimensions can capture relationships for multiple meanings of the same word.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4 在我们的标记表示中添加另一个维度，使我们能够表示更丰富的语义关系排列。在这里，我们看到两个维度如何捕捉同一词的多个含义之间的关系。
- en: The trick is to use multiple numbers to represent each token, allowing you to
    find better representations that accommodate the different relationships between
    words. An example that uses two numbers is shown in figure [3.4](#fig__vectors).
    We can see things like `bland` being nearly equidistant from `rare` and `well-done`,
    while also having space for `bank` to be far away from all three just mentioned
    words and instead be near `stock`. We were even able to throw in a few extra words.
    The more numbers you use, called *dimensions* in the field’s jargon, the more
    complex relationships you can represent.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧是使用多个数字来表示每个标记，这样你可以找到更好的表示，以适应词语之间不同的关系。图[3.4](#fig__vectors)展示了使用两个数字的例子。我们可以看到像`bland`这样的词几乎与`rare`和`well-done`等距，同时还有空间让`bank`远离前面提到的所有三个词，而是靠近`stock`。我们甚至还能加入一些额外的词。你使用的数字越多，在领域的术语中称为*维度*，你就能表示更复杂的关系。
- en: The curse of dimensionality
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: If more dimensions are better at capturing subtle meaning, why not use as many
    dimensions as possible to represent our data? When dealing with a large number
    of dimensions, several problems arise. One primary concern is that LLMs deal with
    many embeddings, and adding more dimensions increases the memory and computation
    required to store and process embeddings. Furthermore, as we add more dimensions,
    the size of the semantic space explodes, and the amount of data and time needed
    to train a machine learning model to learn about all locations in the semantic
    space similarly grows exponentially. Mathematician Richard E. Bellman coined the
    term the “curse of dimensionality“ to describe this phenomenon because while we
    want to create a space capable of capturing nuanced meaning, we are limited by
    the fundamental properties of the space we create.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果更多的维度能更好地捕捉细微的意义，为什么不尽可能多地使用维度来表示我们的数据呢？当处理大量维度时，会出现几个问题。一个主要的问题是LLMs处理许多嵌入，增加维度会增加存储和处理嵌入所需的内存和计算。此外，随着我们增加维度，语义空间的大小急剧增加，训练机器学习模型以了解语义空间中所有位置所需的数据量和时间也会呈指数增长。数学家理查德·E·贝尔曼提出了“维度诅咒”这个术语来描述这一现象，因为虽然我们希望创建一个能够捕捉细微意义的空间，但我们受到我们创建的空间的基本属性的局限。
- en: In LLM parlance, the lists of numbers used to represent tokens are referred
    to as *embeddings*. You can think of an embedding as an array or list of floating-point
    values. As a shorthand, we call such arrays *vectors*. Each position in the vector
    is called a dimension. As we show in figure [3.4](#fig__vectors), using multiple
    dimensions allows us to capture subtleties in relationships between words in human
    language.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM（大型语言模型）的术语中，用于表示标记的数字列表被称为*嵌入*。你可以将嵌入想象成一个浮点值数组或列表。作为简称，我们称这样的数组为*向量*。向量的每个位置称为一个维度。正如我们在图[3.4](#fig__vectors)中所示，使用多个维度使我们能够捕捉到人类语言中词汇之间关系的细微差别。
- en: Since embeddings exist in multiple dimensions, we often state that they live
    in a *semantic space*. In some machine learning applications, this is called a
    *latent space*, especially when not dealing with text. Semantic space is wishy-washy
    jargon that isn’t well defined in the field, but it is most commonly used as a
    shorthand for saying that the vector embeddings that represent each token are
    well behaved in that synonyms/antonyms have nearer/farther distances and that
    we can use those relationships productively. As an example, in figure [3.5](#fig__kingQueenWoman),
    we show a famous case where a “make female” transformation can be built by subtracting
    the embedding for `male` and adding the embedding for `female`. This transformation
    can be applied to many different male-gendered words to find female-gendered words
    of the same concept. The co-location of all the “royal” words in the bottom right
    of figure [3.5](#fig__kingQueenWoman) is also intentional, as many different kinds
    of relationships can be simultaneously maintained in a high-dimensional space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于嵌入存在于多个维度中，我们通常说它们存在于一个*语义空间*中。在一些机器学习应用中，这被称为*潜在空间*，尤其是在不处理文本的情况下。语义空间是一个模糊不清的术语，在领域中并没有很好地定义，但它最常被用作表示每个标记的向量嵌入在该空间中表现良好的简称，即同义词/反义词的距离较近/较远，并且我们可以利用这些关系进行有效利用。例如，在图[3.5](#fig__kingQueenWoman)中，我们展示了一个著名的案例，通过减去`male`的嵌入并添加`female`的嵌入，可以构建一个“使女性化”的转换。这种转换可以应用于许多不同的男性性别词汇，以找到具有相同概念的女性性别词汇。图[3.5](#fig__kingQueenWoman)右下角所有“royal”词汇的共现也是故意的，因为在高维空间中可以同时维持许多不同类型的关系。
- en: '![figure](../Images/CH03_F05_Boozallen.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F05_Boozallen.png)'
- en: Figure 3.5 A demonstration of how the relationships between embeddings create
    a semantic space. Words with similar meanings are near each other, and the same
    transformation can be applied to multi-ple words to yield a similar result—in
    this instance, a transformation to find the feminine version of a masculine word.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5展示了嵌入之间的关系如何创建语义空间。意义相似的词汇彼此靠近，并且可以对多个词汇应用相同的转换以产生相似的结果——在这个例子中，是将男性词汇转换为女性词汇的转换。
- en: Shockingly, we cannot guarantee that these semantic relationships will form
    during the training process. It just so happens that they often do, and they were
    discovered to be very useful. By extension, the relationships in a semantic space
    are not foolproof, and biases in your data can seep in. For example, models will
    often determine that `doctor` is more similar to `male` and `nurse` is more similar
    to `female` because, in the generally available text used to build most models,
    it is more common for doctors to be described as male and nurses as female. The
    relationships are thus not a discovered truth of the world but a reflection of
    the data that went into the process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 令人震惊的是，我们无法保证这些语义关系会在训练过程中形成。它们只是碰巧经常形成，并且被发现非常有用。由此延伸，语义空间中的关系并非万无一失，你的数据中的偏差可能会渗透进来。例如，模型通常会确定`doctor`与`male`更相似，而`nurse`与`female`更相似，因为在构建大多数模型时通常可用的文本中，医生被描述为男性的情况更为常见，而护士被描述为女性的情况更为常见。因此，这些关系不是世界发现的真理，而是反映了进入该过程的数据。
- en: Adding positional information
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加位置信息
- en: One critical problem is that a standard transformer does not understand sequential
    information. If you gave the transformer one sentence and rearranged all the tokens,
    it would view all possible permutations of the tokens as identical! That problem
    is illustrated in figure [3.6](#fig__jumbled_order).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键问题是标准变换器不理解序列信息。如果你给变换器一个句子，然后重新排列所有标记，它会认为所有可能的标记排列都是相同的！这个问题在图[3.6](#fig__jumbled_order)中得到了说明。
- en: '![figure](../Images/CH03_F06_Boozallen.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F06_Boozallen.png)'
- en: Figure 3.6 Without positional information, transformers do not understand that
    their inputs have a specific order, and all possible reorganizations of the tokens
    look identical to the algorithm. This is problematic because word order can change
    the word’s context or, if done randomly, become gibberish.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6 没有位置信息，变换器不理解它们的输入具有特定的顺序，所有可能的标记重新组织对算法来说看起来都是相同的。这是有问题的，因为词序可以改变词的上下文，或者如果随机进行，可能会变成乱码。
- en: For this reason, the embedding layer generates two different kinds of embeddings.
    First, it creates a *word embedding* that captures the meaning of the token, and
    second, it makes a *positional embedding* that captures the token’s location in
    a sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，嵌入层生成两种不同类型的嵌入。首先，它创建一个*词嵌入*，以捕捉标记的意义，其次，它制作一个*位置嵌入*，以捕捉标记在序列中的位置。
- en: The idea is surprisingly simple. Just as we mapped every unique token to a unique
    meaning vector, we will also map every unique token position (first, second, third,
    and so on) to a position vector. So each token will get embedded twice—once for
    its identity and again for its position. These two vectors are then added to create
    one vector representing the word and its location in the sentence. This process
    is outlined in figure [3.7](#fig__embd_token_and_pos).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常简单。正如我们将每个唯一的标记映射到一个唯一的含义向量一样，我们也将每个唯一的标记位置（第一、第二、第三等等）映射到一个位置向量。因此，每个标记将得到两次嵌入——一次用于其身份，再次用于其位置。然后，这两个向量相加，创建一个代表单词及其在句子中位置的向量。这个过程在图[3.7](#fig__embd_token_and_pos)中概述。
- en: Note Using multiple dimensions instead of absolute positions makes iteasier
    for the transformer to learn relative positioning, even if it is exces-sively
    redundant for us as humans [1].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用多个维度而不是绝对位置，对于变换器来说更容易学习相对位置，即使对我们人类来说可能过于冗余[1]。
- en: '![figure](../Images/CH03_F07_Boozallen.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F07_Boozallen.png)'
- en: Figure 3.7 Word embeddings do not capture the fact that input tokens appear
    in a specific order. This information is captured by a positional embedding. The
    position embeddings work the same way as word embeddings and are added together.
    The resulting combined embeddings have the information the model needs to understand
    the order of tokens.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 词嵌入没有捕捉到输入标记出现的特定顺序。这种信息由位置嵌入捕捉。位置嵌入的工作方式与词嵌入相同，并且它们被相加。结果组合嵌入包含了模型需要以理解标记顺序的信息。
- en: Those are all the missing details required to understand how tokens are converted
    into vectors for the transformer layers. This strategy may seem somewhat naive,
    and that is honestly true. People have tried developing more sophisticated methods
    to handle this information, but this simple approach of “Let’s make everything
    a vector and just add them together” works surprisingly well. Importantly, it
    has also demonstrated success in video and images. Having a straightforward strategy
    that functions well enough for many different problems is valuable, which is why
    this naive approach has taken hold.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是理解如何将标记转换为变换层所需的向量的缺失细节。这种策略可能看起来有些天真，这确实是真实的。人们已经尝试开发更复杂的方法来处理这些信息，但“让我们将一切变成向量并简单地相加”这种简单的方法出奇地有效。重要的是，它还在视频和图像中展示了成功。有一个简单且足够有效的策略来处理许多不同的问题是非常有价值的，这就是为什么这种天真方法得以流行。
- en: 3.2.2 Transformer layers
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 变换层
- en: The transformer layer aims to transform the input into a more useful output.
    Most prior neural network layers, such as an embedding layer, are designed to
    incorporate very specific beliefs about how the world works into their operation.
    The idea is that if the encoded belief is accurate to how the world does indeed
    work, your model will reach a better solution using less data. Transformers go
    for the opposite strategy. They encode a general-purpose mechanism that can learn
    many tasks if you get enough data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 变换层旨在将输入转换为更有用的输出。大多数先前的神经网络层，如嵌入层，都是设计用来将关于世界如何运作的非常具体的信念融入其操作中。其理念是，如果编码的信念准确反映了世界的实际运作方式，那么你的模型将使用更少的数据达到更好的解决方案。变换器采取相反的策略。它们编码了一种通用机制，如果你有足够的数据，它可以学习许多任务。
- en: 'To do this, transformers operate with three primary components:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，变换器使用三个主要组件：
- en: '*Query*—Queries are vectors (from an embedding layer) that represent what you
    are looking for.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询*——查询是从嵌入层来的向量，代表你正在寻找的内容。'
- en: '*Key*—Key vectors represent the possible answers to pair a query against.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关键*——键向量代表与查询配对的可能答案。'
- en: '*Value*—Every key has a corresponding value vector, the actual value to bereturned
    when a query and key match.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*值*—每个键都有一个相应的值向量，当查询和键匹配时返回的实际值。'
- en: This terminology corresponds to the behavior of a `dict` or dictionary object
    in Python. You look up an item in the dictionary by its key so that you can then
    create some useful output. The difference is that a transformer is fuzzy. It’s
    not that we are looking up a single key, but we are evaluating *all keys*, weighted
    by their degree of similarity to the query. Figure [3.8](#fig__qkv) shows how
    this works with a simple example. While the queries and keys are shown as strings,
    those strings are stand-ins for the vectors that each string will be mapped to
    via the embedding layer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语对应于Python中`dict`或字典对象的行为。你通过键在字典中查找一个条目，以便然后创建一些有用的输出。不同之处在于转换器是模糊的。我们不是在查找单个键，而是在评估*所有键*，根据它们与查询的相似度进行加权。图[3.8](#fig__qkv)展示了如何通过一个简单的例子来工作。虽然查询和键被显示为字符串，但这些字符串是代表每个字符串将通过嵌入层映射到的向量的替代品。
- en: '![figure](../Images/CH03_F08_Boozallen.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F08_Boozallen.png)'
- en: Figure 3.8 An example of how queries, keys, and values work inside a transformer
    compared to a Python dictionary. When a Python dictionary matches queries to keys,
    it needs an exact match to find the value, or it will return nothing. A transformer
    always returns something based on the most similar matches between queries and
    keys.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 查询、键和值在转换器内部如何工作，与Python字典相比的示例。当Python字典将查询与键匹配时，它需要一个精确匹配来找到值，否则它将返回空值。转换器总是基于查询和键之间最相似的匹配返回一些内容。
- en: Having every key contribute to one query could be chaotic, especially if there
    is one true match between a query and a specific key. This problem is handled
    by a detail called *attention* or the *attention mechanism*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个键都对一个查询做出贡献可能会很混乱，特别是如果查询和特定键之间有一个真正的匹配。这个问题通过一个称为*注意力*或*注意力机制*的细节来处理。
- en: Attention inside a transformer can be considered similar to your ability to
    pay attention to what is important. You can tune out irrelevant and distracting
    information (i.e., bad keys) and focus primarily on what is important (the best
    matching keys). The analogy extends further in that attention is adaptive; what
    is important is a function of what other options are available. Your boss giving
    you directions for the week takes up your attention, but the fire alarm going
    off changes your attention away from your boss to the alarm (and a potential fire).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器内的注意力可以被认为是类似于你关注重要事物的能力。你可以忽略无关的和分散注意力的信息（即，不良的键），并主要关注重要的事情（最佳匹配的键）。这个类比进一步扩展，注意力是自适应的；什么是重要的取决于其他可用的选项。你的老板给你一周的指示会占用你的注意力，但火警响起会将你的注意力从老板转移到警报（以及可能的火灾）。
- en: When generating the next token, a transformer takes the *query* for the current
    token and compares it to the *key* for all previous tokens. Comparing the query
    and the key generates a series of values that the attention mechanism uses to
    calculate how much weight it should assign each potential following token when
    deciding which token to generate next. The *value* for each token tells the model
    what each previous token thinks its contribution to the probability should be.
    The attention function then computes the next token, as shown in figure [3.9](#fig__QKV_nextWord).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成下一个标记时，转换器将当前标记的*查询*与所有先前标记的*键*进行比较。查询和键的比较生成一系列值，注意力机制使用这些值来计算在决定生成下一个标记时应分配给每个潜在后续标记的权重。每个标记的*值*告诉模型每个先前标记认为其对概率的贡献应该是什么。然后，注意力函数计算下一个标记，如图[3.9](#fig__QKV_nextWord)所示。
- en: '![figure](../Images/CH03_F09_Boozallen.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F09_Boozallen.png)'
- en: Figure 3.9 The next token in a sentence is predicted by using the current token
    as the query and calculating matches with the preceding words as the keys. The
    individual values themselves do not need to exist in the semantic space; the output
    of the attention mechanism produces something similar to one of the tokens in
    the vocabulary.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9 通过使用当前标记作为查询并计算与先前单词作为键的匹配来预测句子中的下一个标记。这些个别值本身不需要存在于语义空间中；注意力机制的输出产生的是词汇表中某个标记的类似物。
- en: What is the math of attention?
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意力的数学是什么？
- en: 'We will not go into every detail of the math behind attention because it would
    take a lot of space to describe it, and it has been covered elsewhere. We did
    so in a previous book: chapter 11 of *Inside Deep Learning* [2] explains transformers
    and attention in much greater technical detail.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨注意力背后的数学细节，因为这需要占用很多空间来描述，而且这些内容在其他地方已经有所介绍。我们曾在之前的书中这样做过：*《深度学习内部》*
    [2] 的第11章详细解释了转换器和注意力的技术细节。
- en: For the curious, the primary equation is
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于好奇的人来说，主要方程是
- en: (3.1)
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.1)
- en: '![equation image](../Images/eq-chapter-3-73-1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![equation image](../Images/eq-chapter-3-73-1.png)'
- en: (3.2)
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3.2)
- en: '![equation image](../Images/eq-chapter-3-74-1.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![equation image](../Images/eq-chapter-3-74-1.png)'
- en: The queries, keys, and values are represented by individual matrices ![equation
    image](../Images/eq-chapter-3-75-1.png), ![equation image](../Images/eq-chapter-3-75-2.png),
    and ![equation image](../Images/eq-chapter-3-75-3.png), respectively. Matrix multiplication
    makes attention efficient when implemented on GPUs because they can perform many
    multiplication operations in parallel. The softmax function implements the main
    component of the attention analogy byassigning many values nearly equal to zero,
    which causes the transformer toignore the unimportant items.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 查询、键和值分别由单个矩阵![equation image](../Images/eq-chapter-3-75-1.png)、![equation image](../Images/eq-chapter-3-75-2.png)和![equation
    image](../Images/eq-chapter-3-75-3.png)表示。矩阵乘法在GPU上实现时使注意力变得高效，因为它们可以并行执行许多乘法操作。softmax函数通过分配许多几乎等于零的值来实现注意力类比的主要组件，这导致转换器忽略不重要的项。
- en: The final step of *norm* and *Feedforward* is the application of *layer normalization*
    and a *linear layer* via a *skip connection*. If these terms aren’t familiar to
    you, that is fine; you do not need to know this math to understand the rest of
    the book. If you want to learn what these terms mean, we refer you to *Inside
    Deep Learning* [2] for a technically detailed understanding.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*norm*和*Feedforward*的最后一步是通过跳过连接应用*层归一化*和*线性层*。如果你对这些术语不熟悉，那没关系；你不需要了解这些数学知识就能理解本书的其余部分。如果你想了解这些术语的含义，我们建议你参考*《深度学习内部》*
    [2] 以获得技术上的深入了解。'
- en: A transformer model is made up of dozens of transformer layers. The intermediate
    transformer layers perform the same mechanical task described in figure [3.9](#fig__QKV_nextWord)
    despite not having to predict a token because the last transformer layer is the
    only one that needs to predict an actual token. The transformer layer is general
    enough that combining many intermediate layers allows the model to learn complex
    tasks such as sorting, stacking, and other sophisticated input transformations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器模型由数十个转换器层组成。中间的转换器层执行与图[3.9](#fig__QKV_nextWord)中描述的相同机械任务，尽管它们不需要预测标记，因为最后一个转换器层是唯一需要预测实际标记的层。转换器层足够通用，结合许多中间层可以使模型学习到复杂的任务，如排序、堆叠和其他复杂的输入转换。
- en: 3.2.3 Unembedding layers
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 反嵌入层
- en: The last stage of an LLM is the unembedding layer, which transforms the numeric
    vector representation that transformers use into a specific output token so that
    we can ultimately return the text that corresponds to that token. This output
    generation process is also called *decoding* because we decode the transformer
    vector representation to a piece of output text. It is a crucial component for
    using an LLM to generate text. Not only is decoding the current token essential
    for producing output, but the next token will depend on each previous token selected
    for output. This process is shown in figure [3.10](#fig__transformer_decode_loop),
    where we recursively generate tokens one at a time. In statistical parlance, this
    is known as an *autoregressive* process, meaning each element of the output is
    based on the output that came before it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的最后一个阶段是反嵌入层，它将转换器使用的数值向量表示转换为特定的输出标记，以便我们最终能够返回与该标记对应的文本。这个输出生成过程也被称为*解码*，因为我们解码转换器的向量表示为一段输出文本。这是使用LLM生成文本的关键组件。不仅解码当前标记对于产生输出至关重要，下一个标记将取决于为输出所选的每个先前标记。这个过程如图[3.10](#fig__transformer_decode_loop)所示，我们一次递归生成一个标记。在统计术语中，这个过程被称为*自回归*过程，意味着输出中的每个元素都基于其前面的输出。
- en: '![figure](../Images/CH03_F10_Boozallen.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F10_Boozallen.png)'
- en: Figure 3.10 Producing output from LLMs involves converting from documents to
    tokens and then using the model to produce output. We loop through this process
    to both consume text and generate human-readable output.
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10 从LLM生成输出涉及将文档转换为标记，然后使用模型生成输出。我们循环这个过程来消费文本并生成可读的输出。
- en: You may be wondering how this process stops. When we build the vocabulary of
    tokens, we include some special tokens that do not occur in the text. One of these
    special tokens is an *end of sequence* (EoS) token. The model trains on texts
    with natural endpoints that are finished with the EoS marker, and when the model
    generates a new token, the EoS token is one of the options it can generate. If
    the EoS is generated, we know it is time to stop the loop and return the full
    text to the user. It is also a good idea to keep a maximum generation limit if
    your model gets into a bad state and fails to generate the EoS token.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道这个过程是如何停止的。当我们构建标记词汇表时，我们包括一些在文本中不出现的特殊标记。其中之一是一个*序列结束*（EoS）标记。模型在带有自然端点的文本上训练，这些文本以EoS标记结束，当模型生成一个新标记时，EoS标记是它可以生成的选项之一。如果生成了EoS，我们知道是时候停止循环并将完整文本返回给用户了。如果您的模型陷入不良状态并且无法生成EoS标记，设置一个最大生成限制也是一个好主意。
- en: Sampling tokens to produce output
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 样本标记以生成输出
- en: 'What is missing from this process is how we convert a vector, an array of floating-point
    numbers produced by the transformer layers, into a single token. This process
    is called *sampling* because it uses a statistical method to choose sample tokens
    from the vocabulary based on the LLM’s input and its output so far. The LLM’s
    sampling algorithm evaluates those samples to select which token to produce. There
    are several techniques for doing this sampling, but all follow the same basic
    two-step strategy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程中缺少的是我们如何将一个向量，即由transformer层产生的浮点数数组，转换成一个单独的标记。这个过程被称为*采样*，因为它使用统计方法根据LLM的输入及其迄今为止的输出从词汇表中选择样本标记。LLM的采样算法评估这些样本以选择要生成的标记。有几种进行这种采样的技术，但所有技术都遵循相同的基本两步策略：
- en: For each token in the vocabulary, compute the probability that each token will
    be the next selected token.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于词汇表中的每个标记，计算每个标记将成为下一个选择的标记的概率。
- en: Randomly pick a token according to the probabilities calculated.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据计算出的概率随机选择一个标记。
- en: If you have used ChatGPT or other LLMs, you may have noticed that they do not
    always provide the same output for the same input. The decoding step is why you
    may get different answers whenever you ask the same question.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用过ChatGPT或其他LLM，你可能已经注意到它们并不总是为相同的输入提供相同的输出。解码步骤是为什么每次你问相同的问题时，你可能会得到不同答案的原因。
- en: It may seem counterintuitive that tokens are selected randomly. However, it
    is a critical component to generating good-quality text. Consider the example
    of text generation in figure [3.11](#fig__token_random_next), where we are trying
    to finish the sentence “I love to eat.” It would be unrealistic if the model always
    picked “sushi” as the next token because it had the highest probability. If someone
    always said “sushi” to you in this context, you would think something was off.
    We need randomness to handle the fact that there are multiple valid choices, and
    not all options are likely to occur.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎选择标记是随机的，这似乎有些不合逻辑。然而，这是生成高质量文本的关键组成部分。考虑图[3.11](#fig__token_random_next)中的文本生成示例，我们试图完成句子“I
    love to eat.”如果模型总是选择“寿司”作为下一个标记，因为它具有最高的概率，这将是不现实的。如果有人在这种背景下总是对你说“寿司”，你会认为有什么不对劲。我们需要随机性来处理存在多个有效选择的事实，并且并非所有选项都可能发生。
- en: '![figure](../Images/CH03_F11_Boozallen.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F11_Boozallen.png)'
- en: Figure 3.11 We demonstrate text generation by starting with the phrase "I love
    to eat" and then showing that some possible completions that are foods, such as
    barbeque and sushi, have high proba-bilities, while a car and the number 42 have
    low probabilities. Weighted random selection chooses the word *tacos*. The generation
    loop is stopped when the EoS token appears.
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11 我们通过从短语"I love to eat"开始来展示文本生成，然后显示一些可能的完成项，如烧烤和寿司，具有高概率，而汽车和数字42具有低概率。加权随机选择选择了单词*tacos*。当出现EoS标记时，生成循环停止。
- en: Also note in the example from figure [3.11](#fig__token_random_next) that other
    tokens would be nonsensical, like 42, given tiny probabilities. Again, we need
    to assign every token a probability to know which tokens are likely or unlikely.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在图[3.11](#fig__token_random_next)的例子中，其他标记在给定的极小概率下将是无意义的，比如42。再次强调，我们需要给每个标记分配一个概率，以了解哪些标记可能是或可能不是。
- en: How do you get probabilities for tokens?
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 你如何为标记获取概率？
- en: 'Each possible next token has a different probability of being selected. Most
    of the tokens have nearly zero chance of being selected. A keen reader may wonder:
    How can we assign a probability to a token before knowing the other tokens? We
    do so by giving every token a score, indicating how good a match that token’s
    embedding is compared to the current vector (i.e., the output from the transformer).
    The score is arbitrary from ![equation image](../Images/eq-chapter-3-91-1.png)
    to ![equation image](../Images/eq-chapter-3-91-2.png) and calculated independently
    for each token. The relative difference in scores is then used to create probabilities.
    For example, if one token had a score of 65.2 and a second token had a score of
    -5.0, the probabilities would be near 100% and 0% for the individual token, respectively.
    If the scores were 65.2 and 65.1, the probabilities would be near 50.5% and 49.5%,
    respectively. Similarly, scores of 0.2 and 0.1 would give the same probabilities
    as the scores 65.2 and 65.1 because we are looking at relative differences in
    scores to assign probabilities, not the individual scores themselves.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每个可能的下一个标记都有被选中的不同概率。大多数标记被选中的概率几乎为零。一个敏锐的读者可能会想：在知道其他标记之前，我们如何给一个标记分配一个概率？我们通过给每个标记一个分数来实现，这个分数表示该标记的嵌入与当前向量（即transformer的输出）的匹配程度如何。这个分数是任意的，从![方程式图片](../Images/eq-chapter-3-91-1.png)到![方程式图片](../Images/eq-chapter-3-91-2.png)，并且为每个标记独立计算。然后，使用分数的相对差异来创建概率。例如，如果一个标记的分数是65.2，而另一个标记的分数是-5.0，那么该标记的概率将接近100%，而另一个标记的概率将接近0%。如果分数是65.2和65.1，那么概率将分别接近50.5%和49.5%。同样，分数为0.2和0.1将给出与分数65.2和65.1相同的概率，因为我们关注的是分数的相对差异来分配概率，而不是分数本身。
- en: A transformer sometimes gives you unusual or nonsensical generations. It’s not
    common, but the other tokens have a *near-zero* probability, and eventually, one
    weird token will get picked that you would not expect. Once an unexpected token
    has been chosen, all future generated tokens will be produced in a manner that
    tries to make sense of the unusual generation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer有时会给出不寻常或无意义的生成。这种情况并不常见，但其他标记的概率接近零，最终会选中一个你不会预期的奇怪标记。一旦选定了意外的标记，所有后续生成的标记都将以试图使不寻常的生成有意义的方式进行。
- en: For example, if the LLM produced “I love to eat *chalk*,” you would be pretty
    surprised. But it is not overly unreasonable because chalk-eating is a symptom
    of the medical condition called pica. Once the word *chalk* is selected, the LLM
    may go into a tangent about pica or some other medical diatribe—that is, of course,
    if you are so lucky that your unusual generation is in the sphere of “rare but
    reasonable” and not an utterly errant prediction.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果LLM产生了“我喜欢吃*粉笔*”，你可能会感到非常惊讶。但这并不过分不合理，因为吃粉笔是被称为异食癖的医学状况的症状。一旦选定了单词*粉笔*，LLM可能会进入关于异食癖或其他医学评论的旁白——当然，如果你足够幸运，你的不寻常生成是在“罕见但合理”的领域，而不是一个完全错误的预测。
- en: Note Many algorithms can compute the final probabilities used to select words
    for generation. One of these is nucleus sampling, also known as Top-p sampling,
    which involves determining the tokens with the highest probability as potential
    outputs and choosing tokens to output from that list. This method can help us
    avoid unreasonable predictions. If you can, you want to check which sampling algorithm
    your LLM uses so that you can understand its risks of producing rarer to unreasonable
    outputs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：许多算法可以计算用于选择生成单词的最终概率。其中之一是核采样，也称为Top-p采样，它涉及确定具有最高概率的标记作为潜在输出，并从该列表中选择要输出的标记。这种方法可以帮助我们避免不合理的预测。如果你能的话，你想检查你的LLM使用的是哪种采样算法，这样你就可以了解其产生罕见或不合理输出的风险。
- en: 3.3 The tradeoff between creativity and topical responses
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 创造力和主题响应之间的权衡
- en: Depending on how your users plan to interact with an LLM, generating surprising
    or creative outputs may be desired. Say you are using an LLM to help brainstorm
    new product ideas, and you are using a chatbot as a digital sounding board to
    spark ideas. In this case, you probably want unusual outputs generated because
    the goal is to be creative and think of something new.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户如何计划与 LLM 互动，生成令人惊讶或具有创造性的输出可能是所希望的。比如说你正在使用 LLM 来帮助构思新产品想法，你正在使用聊天机器人作为数字回声板来激发想法。在这种情况下，你可能会希望生成不寻常的输出，因为目标是具有创造性并想出新点子。
- en: Conversely, sometimes creativity is wholly undesired. One potential use for
    LLMs is offline search, where you could fit an LLM on a (relatively powerful)
    mobile phone and ask/look up information even when you do not have internet connectivity.
    In this case, you want the outputs of the LLM to be reliable, on topic, and factual.
    A creative reinterpretation is not needed.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，有时创造性是完全不希望的。LLMs 的一个潜在用途是离线搜索，你可以在（相对强大的）手机上安装 LLM，即使没有互联网连接也能提问/查找信息。在这种情况下，你希望
    LLM 的输出是可靠的、相关的和事实性的。不需要创造性的重新解释。
- en: A feature in LLMs called *temperature* balances this tradeoff. The temperature
    variable (which is a number between 0 and 1 and often has a default value of 0.7
    or 0.8) is used to exaggerate the probability of low-likelihood tokens (high temperature)
    or depress the probability of low-likelihood tokens (low temperature).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 中一个称为 *温度* 的特性平衡了这种权衡。温度变量（这是一个介于 0 和 1 之间的数字，通常默认值为 0.7 或 0.8）用于夸大低概率标记的概率（高温）或降低低概率标记的概率（低温）。
- en: Consider molecules in a glass of water as an analogy. Say we want to know what
    molecule will be at the top of the glass (don’t ask us why; just go with it).
    If the glass was lowered to a temperature of absolute zero, all the molecules
    would be still, and the molecule at the top of the glass would reliably be the
    same each time (i.e., you will always generate the same token). If you raise the
    temperature of the glass so much that it starts to boil, the molecules will bounce
    around, making the molecule at the top of the glass essentially random (i.e.,
    you get a completely random token). As you scale the temperature up and down,
    you change the balance between picking with greater randomness (and, thus, often
    creativity) or focusing on just the most likely next token (thus keeping the generation
    more topical).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以一杯水中的分子作为类比。假设我们想知道哪一种分子会位于杯子的顶部（不要问我们为什么；就按这个思路来）。如果将杯子降低到绝对零度，所有分子都会静止，位于杯子顶部的分子每次都会是相同的（即，你将始终生成相同的标记）。如果你将杯子的温度升高到开始沸腾的程度，分子将会四处弹跳，使得位于杯子顶部的分子基本上是随机的（即，你得到的是一个完全随机的标记）。当你上下调整温度时，你改变了在更大随机性（因此，通常更具创造性）和仅关注最可能的下一个标记（从而保持生成内容的相关性）之间的平衡。
- en: In a practical sense, considering our example of “I like to eat,” a higher temperature
    would lead to the generation of different types of foods, not just pizza or sushi
    but possibly less typical or more specific foods like beef wellington or vegetarian
    chili.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际意义上，以“我喜欢吃”为例，较高的温度会导致生成不同类型的食物，而不仅仅是披萨或寿司，还可能是更不典型或更具体的食物，如牛排 Wellington
    或素食辣椒。
- en: 3.4 Transformers in context
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 上下文中的Transformer
- en: We’ve covered a lot of ground in this chapter. Embedding layers, transformer
    layers, and unembedding layers are the core building blocks that make LLMs work.
    The concepts of how LLMs encode meaning and position and then use stacks of transformer
    layers to uncover the structure in text are all vital to understanding how LLMs
    capture information and produce the quality of output they are capable of. But
    we have more details to cover! How do we create these layers to generate embeddings
    and probabilities by analyzing piles and piles of data in the first place? In
    chapter 4, we will continue exploring how to feed data into this architecture
    and incentivize the LLM to “learn” meaningful relationships in text through the
    training process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经覆盖了很多内容。嵌入层、Transformer 层和反嵌入层是使 LLMs 运作的核心构建块。LLMs 如何编码意义和位置，然后使用堆叠的
    Transformer 层来揭示文本中的结构，这些概念对于理解 LLMs 如何捕捉信息以及它们能够产生的输出质量都是至关重要的。但我们还有更多细节要探讨！我们是如何创建这些层，通过分析堆积如山的数据来生成嵌入和概率的？在第
    4 章中，我们将继续探讨如何将数据输入到这个架构中，并通过训练过程激励 LLM 学习文本中的有意义关系。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: While LLMs use tokens as their basic unit of semantic meaning, they’remathematically
    represented within the model as *embedding vectors* rather than as strings. These
    embedding vectors can capture relationships about nearness, dissimilarity, antonyms,
    and other linguistic-descriptive properties.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）使用标记作为它们基本语义意义的基本单位，但在模型中它们以*嵌入向量*的形式进行数学表示，而不是作为字符串。这些嵌入向量可以捕捉关于邻近性、相似性、反义词以及其他语言描述属性的关系。
- en: Position and word order do not come naturally to transformers and are obtained
    via another vector representing the relative position. The model can represent
    word order by adding the position and word embedding vectors.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置和词序对变换器来说不是自然发生的，它们通过表示相对位置的另一个向量来获得。模型可以通过添加位置和词嵌入向量来表示词序。
- en: Transformer layers act as a kind of fuzzy dictionary, returning approximate
    answers to approximate matches. This fuzzy process is called attention and uses
    the terms *query*, *key*, and *value* as analogous to the key and value in a Python
    dictionary.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器层充当一种模糊字典，对近似匹配返回近似答案。这个过程称为注意力，并使用*查询*、*键*和*值*等术语，类似于Python字典中的键和值。
- en: ChatGPT is an example of a decoder-only transformer, but encoder-only transformers
    and encoder-decoder transformers also exist. Decoder-only transformers are best
    at generating text, but other types of transformers can be better at other tasks.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT是一个仅解码器变换器的例子，但仅编码器变换器和编码器-解码器变换器也存在。仅解码器变换器在生成文本方面表现最佳，但其他类型的变换器可能在其他任务上表现更佳。
- en: LLMs are autoregressive, meaning they work recursively. All previously generated
    tokens are fed into the model at each step to get the next token. Simply put,
    autoregressive models predict the next thing using the previous things.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs是自回归的，这意味着它们是递归工作的。在每一步中，所有先前生成的标记都被输入到模型中，以获取下一个标记。简单来说，自回归模型使用先前的事物来预测下一个事物。
- en: The output of any transformer isn’t tokens; instead, the output is a probability
    for how likely every token is. Selecting a specific token is called *unembedding*
    or *sampling* and includes some randomness.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何变换器的输出都不是标记；相反，输出是每个标记可能性的概率。选择特定的标记被称为*反嵌入*或*采样*，并包含一些随机性。
- en: The strength of randomness can be controlled, resulting in more or less realistic
    output, more creative or unique output, or more consistent output. Most LLMs have
    a default threshold for randomness that is reasonable looking, but you may want
    to change it for different uses.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机性的强度可以控制，从而产生更真实或更不真实、更有创意或更独特、或更一致的输出。大多数LLMs都有一个合理的默认随机性阈值，但您可能希望根据不同的用途进行更改。
