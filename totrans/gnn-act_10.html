<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span></span> <span class="chapter-title-text">Considerations for GNN projects</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Creating a graph data model from nongraph data</li>
<li class="readable-text" id="p3">Extract, transform, load and preprocessing from raw data sources </li>
<li class="readable-text" id="p4">Creating datasets and data loaders with PyTorch Geometric</li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>In this chapter, we describe the practical aspects of working with graph data, as well as how to convert nongraph data into a graph format. We’ll explain some of the considerations involved in taking data from a raw state to a preprocessed format. This includes turning tabular or other nongraph data into graphs and preprocessing them for a graph-based machine learning package. In our mental model, shown in figure 8.1, we are in the left half of the figure.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p6">
<img alt="figure" height="539" src="../Images/8-1.png" width="1009"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.1</span> Mental model for graph training process. We’re at the start of the process, where we prepare our data for training.</h5>
</div>
<div class="readable-text intended-text" id="p7">
<p>We’ll proceed as follows. In section 8.1, we introduce an example problem that might require a graph neural network (GNN) and how to proceed with tackling this project. Section 8.2 goes into more detail on how to use nongraph data in graph models. We then put these ideas into action in section 8.3 by taking a dataset from a raw file to preprocessed data, ready for training. Finally, ideas for finding more graph datasets are given in section 8.4.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>In this chapter, we’ll consider how to apply GNNs to a social graph created by a recruiting firm. In our example, nodes are job candidates, and edges represent relationships between job candidates. We generate graph data from raw data, in the form of edge lists and adjacency lists. We then use that data in a graph processing framework (<code>NetworkX</code>) and a GNN library (PyTorch Geometric [PyG]). The nodes in this data include the candidate’s <em>ID</em>, <em>job type</em> (accountant, engineer, etc.), and <em>industry</em> (banking, retail, tech, etc.). </p>
</div>
<div class="readable-text intended-text" id="p9">
<p>We frame the goals of candidates as a graph-based challenge, detailing the steps to transform their data for graph learning. Our aim here is to map out the data workflow, starting with raw data, converting it into a graph format, and then preparing it for the GNN training we use in the rest of the book. </p>
</div>
<div class="readable-text print-book-callout" id="p10">
<p><span class="print-book-callout-head">Note</span>  Code from this chapter can be found in notebook form at the GitHub repository (<a href="https://mng.bz/Xxn1">https://mng.bz/Xxn1</a>). Colab links and data from this chapter can be accessed in the same locations.</p>
</div>
<div class="readable-text" id="p11">
<h2 class="readable-text-h2"><span class="num-string">8.1</span> Data preparation and project planning</h2>
</div>
<div class="readable-text" id="p12">
<p>Consider the case of a hypothetical recruiting firm called Whole Staffing. Whole Staffing headhunts employees for a variety of industries and maintains a database of their candidate profiles, including their history of engagement with the firm and other candidates. Some candidates get introduced to the firm via referrals from other candidates. </p>
</div>
<div class="readable-text" id="p13">
<h3 class="readable-text-h3"><span class="num-string">8.1.1</span> Project definition</h3>
</div>
<div class="readable-text" id="p14">
<p>Whole Staffing wants to get the most value from its database. They have a few initial questions about their collection of job candidates:</p>
</div>
<ol>
<li class="readable-text" id="p15"> Some profiles have missing data. Is it possible to fill in missing data without bothering the candidate? </li>
<li class="readable-text" id="p16"> History has shown that candidates who have worked on similar projects in the past can work well together in future work. Is it possible to figure out which candidates could work well together? </li>
</ol>
<div class="readable-text" id="p17">
<p>Whole Staffing has tasked you with exploring the data to answer these questions. Among other analytical and machine learning methods, you think there may be an opportunity to represent the data as a graph and use a GNN to answer the client’s questions. </p>
</div>
<div class="readable-text intended-text" id="p18">
<p>Your idea is to take the collection of referrals and convert it into a social network where the job candidates are nodes and the referrals between candidates are edges. To simplify things, you can ignore the direction of the referrals so that the graph can be undirected. You also ignore repeat referrals, so that relationships between candidates remain unweighted. </p>
</div>
<div class="readable-text intended-text" id="p19">
<p>We’ll walk through the steps needed to prepare the data and establish a pipeline to pass the data to a GNN model. First, let’s consider the project planning stage.</p>
</div>
<div class="readable-text" id="p20">
<h3 class="readable-text-h3"><span class="num-string">8.1.2</span> Project objectives and scope</h3>
</div>
<div class="readable-text" id="p21">
<p>Given any problem, having clear objectives, requirements, and scope will serve as a compass that steers all subsequent actions and decisions. Every facet, from planning and schema creation to tool selection should follow the core objectives and scope. Let’s consider each of these for our problem.</p>
</div>
<div class="readable-text" id="p22">
<h4 class="readable-text-h4">Project objectives</h4>
</div>
<div class="readable-text" id="p23">
<p>Whole Staffing wants to optimize the use of its candidate database. First, the project should enhance data quality by filling in missing information in candidate profiles, reducing the need for direct candidate engagement. Second, the work ahead should facilitate informed candidate suggestions, predicting which teams will work well using the historical success of candidates. </p>
</div>
<div class="readable-text" id="p24">
<h4 class="readable-text-h4">Project requirements and scope</h4>
</div>
<div class="readable-text" id="p25">
<p>Several key requirements will directly affect your project. Let’s run through a few and point out their importance to our client’s industry. Then, we’ll draw some conclusions about the project at hand. Requirements include the following:</p>
</div>
<ul>
<li class="readable-text" id="p26"> <em>Data size and velocity</em> —What is the size of the data, in terms of item counts, size in bytes, or number of nodes? How fast is new information added to the data, if at all? Is data expected to be uploaded from a real-time stream, or from a data lake that is updated daily? </li>
</ul>
<div class="readable-text list-body-item" id="p27">
<p>The planned graph might grow with the increase in data, affecting the computational resources needed and the efficiency of algorithms. Accurately assessing data size and velocity ensures that the system can handle the expected load, can offer real-time insights, and is scalable for future growth.</p>
</div>
<ul>
<li class="readable-text" id="p28"> <em>Inference speed</em> —How fast are the application and the underlying machine learning models required to be? Some applications may require sub-second responses, while for others, there is no constraint on time. </li>
</ul>
<div class="readable-text list-body-item" id="p29">
<p>Response time is particularly vital in providing timely recommendations and insights. For a recruitment firm, matching candidates with suitable job openings is time-sensitive, with opportunities quickly becoming unavailable.</p>
</div>
<ul>
<li class="readable-text" id="p30"> <em>Data privacy</em> —What are the policies and regulations regarding personally identifiable information (PII), and how would this involve data transformation and preprocessing? </li>
</ul>
<div class="readable-text list-body-item" id="p31">
<p>Data privacy becomes a huge concern when dealing with sensitive information such as candidate profiles, contact details, and employment histories. In a graph and GNN setting, ensuring that nodes and edges don’t reveal PII is essential. Compliance with regulations such as General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA) is mandatory to avoid legal complications. The graph data should be handled, stored, and processed in a way that respects privacy norms. Anonymization and encryption techniques may be needed to protect individuals’ privacy while still allowing for effective data analysis. Understanding these requirements early in the project planning ensures that the system architecture and data processing pipelines are designed with privacy preservation in mind.</p>
</div>
<ul>
<li class="readable-text" id="p32"> <em>Explainability</em> —How explainable should the responses be? Will direct answers be enough, or should there be additional data that sheds light on why a recommendation or prediction was made? </li>
</ul>
<div class="readable-text list-body-item" id="p33">
<p>In the recruitment sector, explainability and transparency are pivotal. They instill trust among candidates and employers by ensuring fairness and clarity in the talent-selection process. Ethical standards are upheld, and unintended biases should be mitigated. These elements aren’t just ethical imperatives but often legally binding. </p>
</div>
<div class="readable-text" id="p34">
<p>Given the objectives and scope, for Whole Staffing, the deliverables might be a system that does the following:</p>
</div>
<ol>
<li class="readable-text" id="p35"> Fortnightly scan the candidate data for missing items. Missing items can be inferred and suggested or filled in. </li>
<li class="readable-text" id="p36"> Predict candidates that will work well together by using link prediction and/or node classification. Unlike the first deliverable, the response time here should be fast. </li>
</ol>
<div class="readable-text" id="p37">
<p>The following lists some of the specifications for the preceding requirements:</p>
</div>
<ul>
<li class="readable-text" id="p38"> <em>Data size</em> —This is conservatively set at enough capacity for 100,000 candidates and their properties, which is estimated to be 1 GB of data. </li>
<li class="readable-text" id="p39"> <em>Inference speed</em> —Application will run biweekly and can be completed overnight so we don’t have a considerable speed constraint. </li>
<li class="readable-text" id="p40"> <em>Data privacy</em> —No personal data that directly identifies a candidate can be used. However, data known to the recruitment company, such as whether employees have been successfully placed at the same employer, can be used to improve operations of the company, provided this data isn’t shared. </li>
<li class="readable-text" id="p41"> <em>Explainability</em> —There must be some level of explainability for the results. </li>
</ul>
<div class="readable-text" id="p42">
<p>The objectives and requirements will guide the decisions regarding system design, data models, and, often, GNN architecture. The preceding gives an example for the type of considerations needed when beginning or scoping a graph-based project. </p>
</div>
<div class="readable-text" id="p43">
<h2 class="readable-text-h2"><span class="num-string">8.2</span> Designing graph models</h2>
</div>
<div class="readable-text" id="p44">
<p>Given an appropriate scope of work, the next step is in building the graph models. For most machine learning problems, data will be organized in a standard way. For example, when dealing with tabular data, rows are treated as observations, and columns are treated as features. We can join tables of such data by using indexes and keys. This framework is flexible and relatively unambiguous. We may quibble about which observations and features to include, but we know where to place them.</p>
</div>
<div class="readable-text intended-text" id="p45">
<p>When we want to express our data with graphs, in all but the simplest scenarios, we’ll have several options for what structure to use. With graphs, it’s not always intuitive where to place the entities of interest. It’s this ambiguity that drives the need for systemic methods in using graph data, but getting it right early on can serve as a foundation for downstream machine learning tasks [1].</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>In this section, we embark on a journey of transforming Whole Staffing’s recruitment data into graph-based data to support our downstream pipeline. We start by considering the domain and use case, a critical step to understanding the data. Next, we create and refine a schema, pivotal for organizing and interpreting complex datasets. Through rigorous testing of the schema, we could then ensure its robustness and reliability. Any necessary refinements should be made to optimize performance and accuracy. This approach ensures that our future analytic systems, which ingest graph-based data, can answer complex queries about job candidates with precision and reliability. Here’s the process to follow, and figure 8.2 provides a visual:</p>
</div>
<ol>
<li class="readable-text" id="p47"> Understand the data and the use case. </li>
<li class="readable-text" id="p48"> Create a data model, schema, and instance model. </li>
<li class="readable-text" id="p49"> Test your model using the schema and instance model. </li>
<li class="readable-text" id="p50"> Refactor if necessary.<span class="aframe-location"/> </li>
</ol>
<div class="browsable-container figure-container" id="p51">
<img alt="figure" height="1000" src="../Images/8-2.png" width="788"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.2</span> Process of creating a robust graph data model from nongraph data</h5>
</div>
<div class="readable-text" id="p52">
<h3 class="readable-text-h3"><span class="num-string">8.2.1</span> Get familiar with the domain and use case</h3>
</div>
<div class="readable-text" id="p53">
<p>As with most data projects, to be effective, we have to come to grips with the dataset and the context. For our immediate goal of creating a model, understanding our referral data in its raw format and digging into the intricacies of the recruiting industry can provide critical insights. This knowledge also gives us a basis to design tests for the model during deployment. For example, preliminary analysis on the raw data gives us the information in table 8.1. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p54">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.1</span> Features of the dataset</h5>
<table>
<tbody>
<tr>
<td>  Number of candidates <br/></td>
<td>  1,933 <br/></td>
</tr>
<tr>
<td>  Number of referrals <br/></td>
<td>  12,239 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p55">
<p>From the raw data, it’s apparent that there are many relationships, offering potential insights into candidate referrals. The large number of referrals in comparison to the number of candidates suggests an interconnected network. Our models need to be sufficiently large to translate this structure into results within the recruitment problem-space.</p>
</div>
<div class="readable-text intended-text" id="p56">
<p>Turning to domain knowledge, beyond the immediate asks of the client, we should be asking questions that solidify our understanding of the industry. In setting the requirements for our data model, we should consider the <span class="aframe-location"/>key questions and challenges to the industry. For the recruitment problem, we might ask how we can optimize the referral process or what underlying structures and patterns govern candidate referrals. By addressing these types of questions, we can align our model with domain expertise, with a likely boost in both its relevance and validity.</p>
</div>
<div class="readable-text" id="p57">
<h3 class="readable-text-h3"><span class="num-string">8.2.2</span> Constructing the graph dataset and schemas</h3>
</div>
<div class="readable-text" id="p58">
<p>Next, we’ll discuss how to design our database. The term <em>graph dataset </em>denotes a general effort to describe data using the elements and structure of a graph: nodes, edges, and node features and edge features. To achieve this, we need a <em>schema</em> and an <em>instance</em>. These specify the structure and rules of our graph explicitly and allow our graph dataset to be tested and refined. This section is drawn from several references, listed at the end of the book for further reading.</p>
</div>
<div class="readable-text intended-text" id="p59">
<p>By addressing the details of our graph dataset up front, we can avoid technical debt and more easily test the integrity of our data. We can also experiment more systematically with different data structures. In addition, when the structure and rules of our graphs are designed explicitly, it increases the ease with which we can parameterize these rules and experiment with them in our GNN pipeline. </p>
</div>
<div class="readable-text intended-text" id="p60">
<p>Graph datasets can be simple, consisting of one type of node and one type of edge. Or they can be complex, involving many types of nodes and edges, metadata, and, in the case of knowledge graphs, ontologies. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p61">
<h5 class="callout-container-h5 readable-text-h5">Key terms</h5>
</div>
<div class="readable-text" id="p62">
<p>The following are key terms used in this section (for more details on graph data models and types of graphs, see appendix A):</p>
</div>
<ul>
<li class="readable-text" id="p63"> <em>Bi-graph (or bipartite graph)</em>—A graph with two sets of nodes. There are no edges between nodes of the same set. </li>
<li class="readable-text" id="p64"> <em>Entity-relationship diagram (ER diagram)</em>—A figure that shows the entities, relationships, and constraints of a graph. </li>
<li class="readable-text" id="p65"> <em>Graph dataset—</em>A representation of nodes, edges, and their relationships. </li>
<li class="readable-text" id="p66"> <em>Heterogeneous/homogeneous graphs—</em>A homogeneous graph has only one type of node or edge. A heterogeneous graph can have several different types of nodes or edges. </li>
<li class="readable-text" id="p67"> <em>Instance model</em>—A model based on a schema that holds a subset of the actual data. </li>
<li class="readable-text" id="p68"> <em>Ontology—</em>A way of describing the concepts and relationships in a specific domain of knowledge, for example, connections between different entities (writers) in a semantic web (of works of literature). The ontology is the structured framework that defines the roles, attributes, and interrelations of these writers and their literary works. </li>
</ul>
<ul>
<li class="readable-text" id="p69"> <em>Property graph</em>—A model that uses metadata (labels, identifiers, attributes/ properties) to define the graph’s elements. </li>
<li class="readable-text" id="p70"> <em>Resource Description Framework graph (RDF graph, aka Triple Stores)</em>—Model that follows a subject-predicate-object pattern, where nodes are subjects and objects, and edges are predicates. </li>
<li class="readable-text" id="p71"> <em>Schema</em>—A blueprint that defines how the elements of the graph will be organized as well as which specific rules and constraints will be used for these elements. </li>
<li class="readable-text" id="p72"> <em>Conceptual schema</em>—A schema not tied to any particular database or processing system. </li>
<li class="readable-text" id="p73"> <em>System schema</em>—A schema designed with a specific graph database or processing system in mind. </li>
<li class="readable-text" id="p74"> <em>Technical debt</em>—The consequences of prioritizing speedy delivery over quality code, which later has to be refactored. </li>
</ul>
</div>
<div class="readable-text" id="p75">
<p>Graph datasets are good at providing conceptual descriptions of graphs that are quick and easy to grasp by others. For example, for people who understand what a property graph or an RDF graph is, telling them that a graph is a bi-graph implemented on a property graph can reveal much about the design of your data (property graphs and RDF graphs are explained in appendix A). </p>
</div>
<div class="readable-text intended-text" id="p76">
<p>A <em>schema</em> is a blueprint that defines how data is organized in a data storage system, such as a database. A graph schema is a concrete implementation of a graph dataset, explaining in detail how the data in a specific use case is to be represented in a real system. Schemas can consist of diagrams and written documentation. Schemas can be implemented in a graph database using a query language or in a processing system using a programming language. A schema should answer the following questions:</p>
</div>
<ul>
<li class="readable-text" id="p77"> What are the elements (nodes, edges, properties), and what real-world entities and relationships do they represent? </li>
<li class="readable-text" id="p78"> Does the graph include multiple types of nodes and edges? </li>
<li class="readable-text" id="p79"> What are the constraints regarding what can be represented as a node? </li>
<li class="readable-text" id="p80"> What are the constraints for relationships? Do certain nodes have restrictions regarding adjacency and incidence? Are there count restrictions for certain relationships? </li>
<li class="readable-text" id="p81"> How are descriptors and metadata handled? What are the constraints on this data? </li>
</ul>
<div class="readable-text" id="p82">
<p>Depending on the complexity of your data and the systems in use, you may use multiple but consistent schemas. A <em>conceptual schema</em> lays out the elements, <span class="aframe-location"/>rules, and constraints of the graph but isn’t tied to any system. A <em>system schema</em> reflects the conceptual schema’s rules but just for a specific system, such as a database of choice. A system schema could also omit unneeded elements from the conceptual schema. Here are the steps to create a schema: </p>
</div>
<ol>
<li class="readable-text" id="p83"> <em>Identify main entities and relationships</em>. For instance, in our social network example, entities can be candidates, recruiters, referrals, hiring events, and relationships. </li>
<li class="readable-text" id="p84"> <em>Define node and edge labels.</em> These labels serve as identifiers for the types of entities and their interrelationships in the graph. </li>
<li class="readable-text" id="p85"> <em>Specify properties and constraints.</em> Each vertex and edge label is associated with specific properties and constraints that store and restrict information, respectively. </li>
<li class="readable-text" id="p86"> <em>Define indices (optional, for database-oriented schemas)</em>. Indexes, based on properties or combinations thereof, enhance query speeds on graph data. </li>
<li class="readable-text" id="p87"> <em>Apply the graph schema to a database (optional, for database-oriented schemas)</em>. Commands or codes, contingent on the specific graph database, are employed to create the graph schema, with specifications on its static or dynamic nature. </li>
</ol>
<div class="readable-text" id="p88">
<p>Depending on the complexity of the graph dataset and the use cases, one or several schemas could be called for. In the case of more than one schema, compatibility between the schemas via a mapping must also be included.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>For a dataset with few elements, a simple diagram with notes in prose can be sufficient to convey enough information to fellow developers to be able to implement in query language or code. For more complex network designs, ER diagrams and associated grammar are useful in illustrating network schemas in a visual and human readable way. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p90">
<h5 class="callout-container-h5 readable-text-h5">Entity-relationship diagrams (ER diagrams)</h5>
</div>
<div class="readable-text" id="p91">
<p>ER diagrams have the elements to illustrate a graph’s nodes, edges, and attributes and the rules and constraints governing a graph [2, 3]. The following figure (left) shows some connectors notation that can be used to illustrate edges and relationship constraints. The figure (right) shows an example of a schema diagram conveying two node types that might be represented in our recruitment example (Recruiter and Candidate), and two edge types (Knows, and Recruits/Recruited By). The diagram conveys implicit and explicit constraints. </p>
</div>
<div class="readable-text" id="p92">
<p/>
</div>
<div class="browsable-container figure-container" id="p93">
<img alt="sidebar figure" height="531" src="../Images/8-unnumb.png" width="1100"/>
<h5 class="figure-container-h5">At left is the relationship nomenclature for ER diagrams. At right is an example of a conceptual schema using an ER diagram. </h5>
</div>
<div class="readable-text" id="p94">
<p>Some explicit constraints are that one employee can refer many other employees and that one referee can be referred by many employees. Another explicit constraint is that a person can only be employed full-time by one business, but one business might have many employees. An implicit constraint is that, for this graph model, there can be no relationship between a business and a referral.</p>
</div>
</div>
<div class="readable-text" id="p95">
<p>Turning to our example, to design conceptual and system schemas for our example dataset, we should think about the following:</p>
</div>
<ul>
<li class="readable-text" id="p96"> The entities and relationships in our data </li>
<li class="readable-text" id="p97"> Possible rules and constraints </li>
<li class="readable-text" id="p98"> Operational constraints, such as the databases and libraries at our disposal </li>
<li class="readable-text" id="p99"> The output we want from our application </li>
</ul>
<div class="readable-text" id="p100">
<p>Our data will consist of candidates and their profile data (e.g., industry, job type, company, etc.), as well as recruiters. Properties can also be treated as entities; for instance, Medical Industry could be treated as a node. Relations could be Candidate Knows Candidate, Candidate Recommended Candidate, or Recruiter Recruited Candidate. As stated previously, graph data can be extremely flexible in how entities can be represented.</p>
</div>
<div class="readable-text intended-text" id="p101">
<p>Given these choices, we show a few options for the conceptual schema. Option A is shown in figure 8.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p102">
<img alt="figure" height="139" src="../Images/8-3.png" width="692"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.3</span> Schema with one node type and one edge type</h5>
</div>
<div class="readable-text" id="p103">
<p>As you can see, example A consists of one node type (Candidate) connected by one undirected edge type (Knows). Node attributes are the candidate’s Industry and their Job Type. There are no restrictions on the relationships, as any candidate can know 0 to <em>n</em>-1 other candidates, where <em>n</em> is the number of candidates. The second conceptual schema is shown in figure 8.4.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p104">
<img alt="figure" height="392" src="../Images/8-4.png" width="687"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.4</span> Schema with two node types and one edge type</h5>
</div>
<div class="readable-text" id="p105">
<p>Example B consists of two node types (Candidate and Recruiter), linked by one undirected edge type (Knows). Edges between candidates have no restrictions. Edges between candidates and recruiters have a constraint: a candidate can only link to one recruiter, while a recruiter can link to many candidates.</p>
</div>
<div class="readable-text intended-text" id="p106">
<p>The third schema is shown in figure 8.5. It has multiple node and relationship types. In example C, the types are Candidate, Recruiter, and Industry. Relation types include Candidate Knows Candidate, Recruiter Recruits Candidate, Candidate Is a Member of Industry. Note, we’ve made Industry a separate entity, rather than an attribute of a candidate. These types of graphs are known as <em>heterogeneous, </em><span class="aframe-location"/>as they contain many different types of nodes and edges. In a way, we can imagine these as multiple graphs that are layered on top of each other. When we have only one type of nodes and edges, then graphs are known as <em>homogenous.</em> Some of the constraints for example C include the following:<span class="aframe-location"/></p>
</div>
<ul>
<li class="readable-text" id="p108"> Candidates can only have one Recruiter and one Industry. </li>
<li class="readable-text" id="p109"> Recruiters don’t link to Industries. </li>
</ul>
<div class="browsable-container figure-container" id="p107">
<img alt="figure" height="627" src="../Images/8-5.png" width="687"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.5</span> Schema with three node types and three edge types</h5>
</div>
<div class="readable-text" id="p110">
<p>Depending on the queries and the objectives of the machine learning model, we could pick one schema or experiment with all three in the course of developing our application. Let’s stick with the first schema, which can serve as a simple structure for our exploration and experimentation.</p>
</div>
<div class="readable-text" id="p111">
<h3 class="readable-text-h3"><span class="num-string">8.2.3</span> Creating instance models</h3>
</div>
<div class="readable-text" id="p112">
<p>An <em>instance model </em>contrasts the abstract nature of the graph dataset by providing a tangible, specific example of the data, according to the schema. Such an example serves to validate and test the schema. Following are the steps to create an instance model:</p>
</div>
<ol>
<li class="readable-text" id="p113"> <em>Identify the schema</em>. Begin by identifying the general model or schema that your instance will be based upon. Ensure that the class definition, attributes, and methods are well established. </li>
<li class="readable-text" id="p114"> <em>Select a subset of the data</em>. Choose a specific subset of data to represent, adhering to the established graph schema. </li>
<li class="readable-text" id="p115"> <em>Create nodes</em>. Develop nodes for each entity within your data subset, ensuring each has a label, unique identifier, and associated properties. </li>
<li class="readable-text" id="p116"> <em>Create edges</em>. Develop links for each relationship, assigning labels and properties and specifying edge directions and multiplicities. </li>
<li class="readable-text" id="p117"> <em>Adhere to the rules and constraints of your schema</em>. In constructing the instance model, make sure to follow the rules and constraints of the schema. </li>
<li class="readable-text" id="p118"> <em>Visualization</em>. Use visualization tools to represent the instance model graphically. </li>
<li class="readable-text" id="p119"> <em>Instantiation</em>. Realize the instance model using a graph database or graph processing system. This will allow for queries that can test and validate it. </li>
</ol>
<div class="readable-text" id="p120">
<p>Figure 8.6 shows an example of an instance model derived from the schema discussed formerly. The nodes and edges have features filled with the real data of candidates instead of placeholders.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p121">
<img alt="figure" height="494" src="../Images/8-6.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.6</span> Example of an instance model with nodes filled with actual data from the recruiter example. Real instance models may have much more data.</h5>
</div>
<div class="readable-text" id="p122">
<h3 class="readable-text-h3"><span class="num-string">8.2.4</span> Testing and refactoring</h3>
</div>
<div class="readable-text" id="p123">
<p><em>Technical debt </em>can occur when we have to change and evolve our data or code, but we haven’t yet planned for backward- or forward-compatibility in our models. It can also happen when our modeling choices aren’t a good fit for our database and software choices, which may call for expensive (in time or money) workarounds or replacements.</p>
</div>
<div class="readable-text intended-text" id="p124">
<p>Having well-defined rules and constraints on our data and models gives us explicit ways to test our pipeline. For example, if we know that our nodes can at most have two degrees, we can design simple functions or queries to process and test every node against this criterion. </p>
</div>
<div class="readable-text intended-text" id="p125">
<p>Testing and refactoring are iterative processes and crucial in scaling an optimized graph schema and instance model [4, 5]. It will involve executing queries, analyzing results, making necessary adjustments, and validating against metrics. In the context of Whole Staffing’s recruitment data, this practice would ensure the model is tailored to capture real-world relationships and robust new data streams. Following are some examples for tests and refactoring:</p>
</div>
<ol>
<li class="readable-text" id="p126"> <em>Cast your instance model in a system. </em>Store the model in your graph database or processing system of choice.<em> </em> </li>
<li class="readable-text" id="p127"> <em>Create tests and run queries</em>. Based on the specific requirements, draft queries to test the integrity of your model. Use query languages such as Cypher or SPARQL to execute queries on a graph database. Programming languages, for example, Python, can also be used to query graphs within graph processing systems such as NetworkX. </li>
</ol>
<div class="readable-text list-body-item" id="p128">
<p>For our example’s simple schema, here are some possible tests:</p>
</div>
<ul>
<li class="buletless-item" style="list-style-type: none;">
<ul>
<li class="readable-text" id="p129"> <em>Node attributes verification</em> —Each node should be checked to confirm that it possesses the required attributes, specifically the candidate’s industry and job type, and that these attributes have non-null values. </li>
<li class="readable-text" id="p130"> <em>Edge type verification</em> —All connections between candidates should be validated to confirm that they are of the Knows type, ensuring consistency in relationship labeling. </li>
<li class="readable-text" id="p131"> <em>Relationship verification</em> —Check the average number of relationships that exist to ensure it’s consistent with the average number of referrals. </li>
<li class="readable-text" id="p132"> <em>Unique IDs</em> —Every candidate node should be checked for unique identifiers to prevent data duplication and ensure data integrity. </li>
<li class="readable-text" id="p133"> <em>Attribute data type</em> —The data types of <code>industry</code> and <code>jobType</code> attributes should be validated to ensure consistency across all candidate nodes. </li>
<li class="readable-text" id="p134"> <em>Network structure</em> —The structure of the network should be validated to ensure it’s undirected, confirming the bidirectional nature of the Knows relationships between candidate nodes. </li>
<li class="readable-text" id="p135"> <em>Edge cases</em> —Determine edge cases and query for those. In our case, the nodes that are unconnected may present a problem. Using queries to understand the extent of unconnected nodes and their effect on the analytics will drive decisions to refactor. Another edge case could be an isolated group of candidates whose relationships form a cycle. It would be important to ensure the data model and the analytical tools could handle such complex or unusual data patterns and still produce valid answers. </li>
</ul></li>
</ul>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p136"><span class="faux-ol-li-counter">3. </span> <em>Validate and evaluate performance</em> —Based on the results of the tests, determine if there are logical problems with your model and your use case, or problems with the data and attributes. </li>
<li class="readable-text faux-li has-faux-ol-li-counter" id="p137"><span class="faux-ol-li-counter">4. </span> <em>Refactor</em> —Make adjustments to labels, properties, relationships, or constraints as needed to minimize errors. </li>
<li class="readable-text faux-li has-faux-ol-li-counter" id="p138"><span class="faux-ol-li-counter">5. </span> <em>Repeat</em> —Iterate the preceding steps, refining the model based on evaluations and ensuring alignment with the project needs and constraints. </li>
<li class="readable-text faux-li has-faux-ol-li-counter" id="p139"><span class="faux-ol-li-counter">6. </span> <em>Final assessment</em> —Evaluate the final model against criteria and best practices to ensure its readiness for complex queries and machine learning applications. </li>
</ol>
<div class="readable-text" id="p140">
<p>With this iterative process of testing and refactoring, we refine the dataset for Whole Staffing’s recruitment data and use case. Attention to detail guarantees the model is ready to support evaluation of the complex, nuanced relationships hidden within the recruitment data.</p>
</div>
<div class="readable-text intended-text" id="p141">
<p>As we transition into the next section, our focus shifts to the practical implementation of some of these concepts. We’ll look at creating data pipelines in PyG, showing how to convert data from its initial raw form to a preprocessed state, ready for input into other downstream model training and testing routines. </p>
</div>
<div class="readable-text" id="p142">
<h2 class="readable-text-h2"><span class="num-string">8.3</span> Data pipeline example</h2>
</div>
<div class="readable-text" id="p143">
<p>With the schema decided, let’s walk through an example of a data pipeline. In this section, we assume our objective is to create a simple data workflow that takes data from a raw state and ends with a preprocessed dataset that can be passed to a GNN. These steps are summarized in figure 8.7.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>Note that while the overall steps shown can be consistent from one problem to another, the details of implementation for each step can be unique to the problem, its data, and the chosen data storage, processing, and model training options.<span class="aframe-location"/> </p>
</div>
<div class="browsable-container figure-container" id="p145">
<img alt="figure" height="1075" src="../Images/8-7.png" width="929"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.7</span> Summary of steps in the data pipeline process in this section</h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p146">
<h5 class="callout-container-h5 readable-text-h5">Key terms</h5>
</div>
<div class="readable-text" id="p147">
<p>The following are key terms used in this section (for more details on graph data models and types of graphs, see appendix A):</p>
</div>
<ul>
<li class="readable-text" id="p148"> <em>Adjacency list—</em>A basic representation of graph data. In this format, each entry contains a node with a list of its adjacent nodes. </li>
<li class="readable-text" id="p149"> <em>Adjacency matrix</em>—A basic representation of graph data. In a matrix, each row and column correspond to a node. The cells, where these rows and columns intersect, signify the presence of edges between the nodes. A cell with a nonzero value indicates an edge between the nodes, while a zero value signifies no connection. </li>
<li class="readable-text" id="p150"> <em>Degree</em>—The degree of a node is the count of its adjacent nodes. </li>
<li class="readable-text" id="p151"> <em>Edge list—</em>A basic representation of a graph. It’s an array of all the edges in a graph; each entry in the array contains a unique pair of connected nodes. </li>
<li class="readable-text" id="p152"> <em>Mask—</em>A Boolean array (or tensor in the case of PyTorch) that is used to select specific subsets of data. Masks are commonly used for splitting a dataset into different parts, such as training, validation, and testing sets. </li>
</ul>
<ul>
<li class="readable-text" id="p153"> <em>Rank</em>—In our context, rank refers to the position of each node’s degree in a sorted list. So, the node with the highest degree has rank 1, the next highest has rank 2, and so on. </li>
<li class="readable-text" id="p154"> <em>Raw data</em>—Data in its most unprocessed form. </li>
<li class="readable-text" id="p155"> <em>Serialization</em>—Putting data into a format that is easily stored or exported. </li>
<li class="readable-text" id="p156"> <em>Subgraph</em>—A subgraph is a subset of a larger graph’s nodes and edges. </li>
</ul>
</div>
<div class="readable-text" id="p157">
<h3 class="readable-text-h3"><span class="num-string">8.3.1</span> Raw data</h3>
</div>
<div class="readable-text" id="p158">
<p><em>Raw data</em> refers to data in its most unprocessed state; such data is the starting point for our pipeline. This data can be in various databases, serialized in some way, or generated.</p>
</div>
<div class="readable-text intended-text" id="p159">
<p>In the development stage of an application, it’s important to know how closely the raw data used will match the live data used in production. One way to do this is by sampling from data archives.</p>
</div>
<div class="readable-text intended-text" id="p160">
<p>As mentioned in section 8.1, there are at least two sources for our example problem: relational database tables that contain recommendation logs and candidate profiles. To keep our example contained, we assume a helpful engineer has already queried the log data and transformed it into a JSON format, where keys are a recommending candidate, and the values are the recommended candidates. From our profile data, we have two other fields: <em>industry</em> and <em>job type</em>. For both data sources, our engineer has used a hash to protect PII, which we can consider a unique identifier for the candidate. In this section, we’ll use the JSON data, where an example snippet is shown in figure 8.8. The data is displayed in two ways: with a hash and without a hash.</p>
</div>
<div class="readable-text" id="p161">
<h4 class="readable-text-h4">Data encoding and serialization</h4>
</div>
<div class="readable-text" id="p162">
<p>One key consideration when constructing the pipeline is the choice of what data format to use when importing and exporting data from one system to another. For transferring graph data into another system or sending it over the internet, <em>encoding</em> or <em>serialization</em> is typically used. These terms refer to the process of putting data in a form that is easily transferable [6, 7]. Before choosing an encoding format, you must have decided upon the following:</p>
</div>
<ul>
<li class="readable-text" id="p163"> <em>Data model</em> —Simple model, property graph, or other? </li>
<li class="readable-text" id="p164"> <em>Schema</em> —Which entities in your data are nodes, edges, and properties? </li>
<li class="readable-text" id="p165"> <em>Data structure</em> —How is the data stored: in adjacency matrices, adjacency lists, or edge lists? </li>
<li class="readable-text" id="p166"> <em>Receiving systems</em> —How does the receiving system (in our case, GNN libraries and graph-processing systems) accept data? What encodings and data structures are preferred? Is imported data automatically recognized, or is custom programming required to read in data?<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p167">
<img alt="figure" height="986" src="../Images/8-8.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.8</span> View of raw data: JSON file. The figure on the left is in key/value format. The keys are the members, and the values are their known relationships. The figure on the right shows unhashed values, demonstrating example names for these individuals. </h5>
</div>
<div class="readable-text" id="p168">
<p>Here are a few encoding choices you’re likely to encounter:</p>
</div>
<ul>
<li class="readable-text" id="p169"> <em>Language and system-agnostic encodings formats</em> —These are most popular as they are extremely flexible and work across many systems and languages. However, data arrangement can still differ from system to system. Therefore, an edge list in a CSV file, with a specific set of headers, may not be accepted or interpreted in the same way between two different systems. Following are some examples for this format:  
    <ul>
<li> <em>JSON</em> —Has advantages when reading from APIs or feeding into JavaScript applications. <code>Cytoscape.js</code>, a graph visualization library, accepts data in JSON format.  </li>
<li> <em>CSV</em> —Accepted by many processing systems and databases. However, the required arrangement and labeling of the data differs from system to system.  </li>
<li> <em>XML</em> —Graph Exchange XML (GEXF) format is of course an XML format.  </li>
</ul></li>
<li class="readable-text" id="p170"> <em>Language specific</em> <em>—</em>Python, Java, and other languages have built-in encoding formats. </li>
<li class="readable-text" id="p171"> <em>Pickle</em> —Python’s format. Some systems accept Pickle encoded files. Despite this, unless your data pipeline or workflow is governed extensively by Python, pickles should be used lightly. The same applies for other language-specific encodings. </li>
<li class="readable-text" id="p172"> <em>System driven</em> —Specific software, systems, and libraries have their own encoding formats. Though these may be limited in usability between systems, an advantage is that the schema in such formats is consistent. Software and systems that have their own encoding format include Stanford Network Analysis Platform (SNAP), NetworkX, and Gephi. </li>
<li class="readable-text" id="p173"> <em>Big data</em> —Aside from the language-agnostic formats listed previously, there are other encoding formats used for larger sizes of data. </li>
<li class="readable-text" id="p174"> <em>Avro</em> —This encoding is used extensively in Hadoop workflows </li>
<li class="readable-text" id="p175"> <em>Matrix based</em> —Because graphs can be expressed as matrices, there are a few formats that are based on this data structure. For sparse graphs, the following formats provide substantial memory savings and computational advantages (for lookups and matrix/vector multiplication): 
    <ul>
<li> Sparse column matrix (.csc filetype) </li>
<li> Sparse row matrix (.csr filetype) </li>
<li> Matrix market format (.mtx filetype) </li>
</ul></li>
</ul>
<div class="readable-text" id="p176">
<h3 class="readable-text-h3"><span class="num-string">8.3.2</span> The ETL step</h3>
</div>
<div class="readable-text" id="p177">
<p>With the schema chosen and data sources established, the <em>ETL</em> (<em>extract, transform, load</em>) step consists of taking raw data from its sources and then producing data that fits the schema and is ready for preprocessing or training. For our data, this consists of programming a set of actions that begin with pulling the data from the various databases and then joining them as needed. </p>
</div>
<div class="readable-text intended-text" id="p178">
<p>We need data that ends up in a specific format that we can input into a preprocessing step. This could be a JSON format or an edge list. For either the JSON example or edge list example, our schema is fulfilled; we’ll have nodes (the individual persons) and edges (the relationships between these people).</p>
</div>
<div class="readable-text intended-text" id="p179">
<p>For our recruitment example, we want to transform our raw data into a graph data structure, encoded in CSV. This was chosen for ease of manipulation with Python. This file can then be loaded into our graph-processing system, NetworkX, or a GNN package such as PyG. To summarize the next steps, we’ll do the following:</p>
</div>
<ol>
<li class="readable-text" id="p180"> Convert the raw data file to a graph format, following your chosen graph data model. In our case, we convert the raw data into an edge list and an adjacency list. We then saved it as a CSV file. </li>
<li class="readable-text" id="p181"> Load the CSV file into NetworkX for exploratory data analysis (EDA) and visualization. </li>
<li class="readable-text" id="p182"> Load into PyG and preprocess. </li>
</ol>
<div class="readable-text" id="p183">
<h4 class="readable-text-h4">Raw data to adjacency List and edge List</h4>
</div>
<div class="readable-text" id="p184">
<p>Starting with our CSV and JSON files, we next convert the data into two key data models: an edge list and an adjacency list, which we define in appendix A. Both adjacency and edge lists are two basic data representations used with graphs. An edge list is a list where every item in this structure contains a node with a list of its adjacent nodes. These representations are illustrated in figure 8.9.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p185">
<img alt="figure" height="579" src="../Images/8-9.png" width="467"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.9</span> A graph with nodes and edges marked (top). An edge list representation (middle); each entry contains the edge number and the pair of nodes connected. An adjacency list representation in a dictionary (bottom); each key is a node, and the values are its adjacent nodes.</h5>
</div>
<div class="readable-text" id="p186">
<p>First using the <code>json</code> module, we load the data from a JSON file into a Python dictionary. The Python dictionary has the same structure as the JSON, with member hashes as keys and their relationships as values.</p>
</div>
<div class="readable-text" id="p187">
<h4 class="readable-text-h4">Creating an adjacency list</h4>
</div>
<div class="readable-text" id="p188">
<p>Next, we create an adjacency list from this dictionary. This list will be stored as a text file. Each line of the file will contain the member hash, followed by hashes of that member’s relationships. The process for creating an adjacency list is illustrated in figure 8.10.</p>
</div>
<div class="readable-text intended-text" id="p189">
<p>This function transforms our raw data into an adjacency list, which we’ll apply to our recruitment example. We’ll have <em>inputs</em> that consist of the following: </p>
</div>
<ul>
<li class="readable-text" id="p190"> A dictionary of candidate referrals where the keys are members who have referred other candidates, and the values are lists of the people who were referred </li>
<li class="readable-text" id="p191"> A suffix to append to the filename<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p192">
<img alt="figure" height="229" src="../Images/8-10.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.10</span> Flow diagram illustrating the process of transforming a relationship dictionary into a well-structured adjacency list, stored in a text file, while ensuring the symmetry of connections in the undirected graph.</h5>
</div>
<div class="readable-text" id="p193">
<p>We’ll have <em>outputs</em> that consist of the following: </p>
</div>
<ul>
<li class="readable-text" id="p194"> An encoded adjacency list in a txt file </li>
<li class="readable-text" id="p195"> A list of the node IDs found </li>
</ul>
<div class="readable-text" id="p196">
<p>This is shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p197">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.1</span> Create an adjacency list from relationship dictionary</h5>
<div class="code-area-container">
<pre class="code-area">def create_adjacency_list(data_dict, suffix=''):
   list_of_nodes = []

   for source_node in list(data_dict.keys()): <span class="aframe-location"/> #1

       if source_node not in list_of_nodes:
           list_of_nodes.append(source_node)

       for y in data_dict[source_node]:              <span class="aframe-location"/> #2
           if y not in list_of_nodes:                 #2
               list_of_nodes.append(y)                #2
           if y not in data_dict.keys():              #2
               data_dict[y]=[source_node]             #2
           Else:                                      #2
               if source_node not in data_dict[y]:    #2
                   data_dict[y].append(source_node)   #2
               else: continue                         #2

   g= open("adjacency_list_{}.txt".format(suffix),"w+") <span class="aframe-location"/> #3
   for source_node in list(data_dict.keys()): <span class="aframe-location"/> #4
       dt = ' '.join(data_dict[source_node])  <span class="aframe-location"/> #5
       print("{} {}".format(source_node, dt)) <span class="aframe-location"/> #6
       g.write("{} {} \n".format(source_node, dt))  <span class="aframe-location"/> #7

   g.close
   return list_of_nodes</pre>
<div class="code-annotations-overlay-container">
     #1 Runs through every node in the input data dictionary
     <br/>#2 Because this is an undirected graph, there must be a symmetry in the values; that is, every value in a key must contain that key in its own entry. As an example, for entry F, if G is a value, then for entry G, F must be a value. These lines check for that and fix the dictionary if these conditions don’t exist.
     <br/>#3 Creates a text file that will store the adjacency list
     <br/>#4 For every key in the dictionary
     <br/>#5 Creates a string from the list of dictionary values. This value is a string of member IDs separated by empty spaces.
     <br/>#6 Optional print
     <br/>#7 Writes a line to the text file. This line will contain the member hash, and then a string of relationship hashes.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p198">
<h4 class="readable-text-h4">Creating an edge list</h4>
</div>
<div class="readable-text" id="p199">
<p>Next, we show the process to create an edge list. As with the adjacency list, we transform the data to account for node pair symmetry. Note that either format could work for this project. For your own project, another format could also be warranted. Figure 8.11 illustrates the process.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p200">
<img alt="figure" height="434" src="../Images/8-11.png" width="602"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.11</span> Process of creating an edge list file programmed into listing 8.2</h5>
</div>
<div class="readable-text" id="p201">
<p>As with the adjacency list function, the edge list function illustrates the transformation of raw data into an edge list and has the same inputs as the previous function. The outputs consist of the following: </p>
</div>
<ul>
<li class="readable-text" id="p202"> An edge list in a .txt file </li>
<li class="readable-text" id="p203"> Lists of the node IDs found and the edges generated </li>
</ul>
<div class="readable-text" id="p204">
<p>By definition, every entry of an edge list must be unique, so we must ensure that our produced edge list is the same. Here’s the code to create an edge list from a relationship dictionary.</p>
</div>
<div class="browsable-container listing-container" id="p205">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.2</span> Create an edge list from relationship dictionary</h5>
<div class="code-area-container">
<pre class="code-area">def create_edge_list(data_dict, suffix=''):
    edge_list_file = open("edge_list_{}.txt".format(suffix),"w+")
    edges = []    
    nodes_all = []

    for source in list(data_dict.keys()):
        if source not in list_of_nodes_all:
            nodes_all.append(source)
        connections = data_dict[source]

        for destination in connections:  <span class="aframe-location"/> #1
            if destination not in nodes_all:
                nodes_all.append(destination)

           if {source, destination} not in edges:  <span class="aframe-location"/> #2
               print(f"{source} {destination}")
               out_string =  f"{source} {destination}\n”
               edge_list_file.write(out_string)  <span class="aframe-location"/> #3
               edges.append({source, destination })

           else: continue

       edge_list_file.close
       return list_of_edges, list_of_nodes_all</pre>
<div class="code-annotations-overlay-container">
     #1 Each member dictionary value is a list of relationships. For every key, we iterate through every value.
     <br/>#2 Because this graph is undirected, we don’t want to create duplicate edges. For example, because {F,G} is the same as {G,F}, we only need one of these. This line checks if a node pair exists already. We use a set object because the node order doesn’t matter.
     <br/>#3 Writes the line to the text file. This line will consist of the node pair.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p206">
<p>In the next sections, we’ll use the adjacency list to load our graph into NetworkX. One thing to note about the differences between loading a graph using the adjacency list versus the edge list is that edge lists can’t account for single, unlinked nodes. It turns out that quite a few of the candidates at Whole Staffing haven’t recommended anyone, and don’t have edges associated with them. These nodes would be invisible to an edge list representation of the data.</p>
</div>
<div class="readable-text" id="p207">
<h3 class="readable-text-h3"><span class="num-string">8.3.3</span> Data exploration and visualization</h3>
</div>
<div class="readable-text" id="p208">
<p>Next, we want to load our network data into a graph processing framework. We chose NetworkX, but there are many other choices available, depending on your task and language preferences. We chose NetworkX because we have a small graph, and we also want to do some light EDA and visualization.</p>
</div>
<div class="readable-text intended-text" id="p209">
<p>With our newly created adjacency list, we can create a NetworkX graph object by calling the <code>read_edgelist</code> or <code>read_adjlist</code> methods. Next, we can load in the attributes <code>industry</code> and <code>job type</code>. In this example, these attributes are loaded in as a dictionary, where the node IDs serve as keys.</p>
</div>
<div class="readable-text intended-text" id="p210">
<p>With our graph loaded, we can explore and inspect our data to ensure that it aligns with our assumptions. First, the count of nodes and edges should match our member count, and the number of edges created in our edge list, respectively, as shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p211">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.3</span> Create an edge list from the relationship dictionary</h5>
<div class="code-area-container">
<pre class="code-area">social_graph = nx.read_adjlist('adjacency_list_candidates.txt')
nx.set_node_attributes(social_graph, attribute_dict)
print(social_graph.number_of_nodes(), social_graph.number_of_edges())
&gt;&gt; 1933 12239</pre>
</div>
</div>
<div class="readable-text" id="p212">
<p>We want to check how many connected components our graph has:</p>
</div>
<div class="browsable-container listing-container" id="p213">
<div class="code-area-container">
<pre class="code-area">len(list((c for c in nx.connected_components(social_graph))))
&gt;&gt;&gt; 219</pre>
</div>
</div>
<div class="readable-text" id="p214">
<p>The <code>connected_components</code> method generates the connected components of a graph; a visualization is shown in figure 8.12 and generated using NetworkX. There are hundreds of components, but when we inspect this data, we find that there is one large component of 1,698 nodes, and the rest are composed of less than 4 nodes. Most of the disconnected components are singleton nodes (the candidates that never refer anyone). For more information about components of a graph, we give definitions and details in appendix A.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p215">
<img alt="figure" height="712" src="../Images/8-12.png" width="1052"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.12</span> The full graph, with its large connected component in the middle, surrounded by many smaller components. For our example, we’ll use only the nodes in the large connected component.</h5>
</div>
<div class="readable-text" id="p216">
<p>We’re interested in this large connected component and will work with that going forward. The <code>subgraph</code> method can help us to isolate this large component.</p>
</div>
<div class="readable-text intended-text" id="p217">
<p>Finally, we use NetworkX to visualize our graph. For this, we’ll use a standard recipe for analyzing graphs which can also be found in the NetworkX documentation.</p>
</div>
<div class="readable-text intended-text" id="p218">
<p>Let’s go through the different steps (the full code sample for each step is also in the repository, labeled “Function that visualizes the social graph and shows degree statistics”):</p>
</div>
<ol>
<li class="readable-text" id="p219"> <em>Create the graph object</em>. Generate a distinct graph object, selecting the largest connected component from the given graph. In cases where there’s only one connected component, this step might be unnecessary but ensures the selection of the major component. </li>
</ol>
<div class="browsable-container listing-container" id="p220">
<div class="code-area-container">
<pre class="code-area">connected_component = nx.connected_components(social_graph
Gcc = social_graph.subgraph(sorted(connected ), 
                            key=len, 
                            reverse=True)[0]
                            )</pre>
</div>
</div>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p221"><span class="faux-ol-li-counter">2. </span> <em>Determine the layout</em>. Decide the positioning of nodes and edges for visualization. Choose an appropriate layout algorithm; for example, the Spring Layout models the edges as springs and nodes as repelling masses: </li>
</ol>
<div class="browsable-container listing-container" id="p222">
<div class="code-area-container">
<pre class="code-area">pos = nx.spring_layout(Gcc, seed=10396953)</pre>
</div>
</div>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p223"><span class="faux-ol-li-counter">3. </span> <em>Draw nodes and edges</em>. Use the chosen layout to draw nodes on the visualization. Adjust visual parameters such as node size to enhance the clarity of the figure. Based on the selected layout, draw the edges. Modify appearance settings such as transparency to achieve the desired visual effect. </li>
</ol>
<div class="browsable-container listing-container" id="p224">
<div class="code-area-container">
<pre class="code-area">nx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)
nx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)
ax0.set_title("Connected component of Social Graph")
ax0.set_axis_off()</pre>
</div>
</div>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p225"><span class="faux-ol-li-counter">4. </span> <em>Generate and plot node degrees</em>. Employ the degree method on the graph object to create an iterable of nodes with their respective degrees, and sort them from highest to lowest. Visualize the sorted list of node degrees on a plot to analyze the distribution and prominence of various nodes. Use NumPy’s <code>unique</code> method with the <code>return_counts</code> parameter to plot a histogram showing the degrees of nodes and their counts, providing insights into the graph’s structure and complexity:  </li>
</ol>
<div class="browsable-container listing-container" id="p226">
<div class="code-area-container">
<pre class="code-area">degree_sequence = sorted([d for n, d in social_graph.degree()], reverse=True)

ax1 = fig.add_subplot(axgrid[3:, :2])
ax1.plot(degree_sequence, "b-", marker="o")
ax1.set_title("Degree Rank Plot")
ax1.set_ylabel("Degree")
ax1.set_xlabel("Rank")

ax2 = fig.add_subplot(axgrid[3:, 2:])
ax2.bar(*np.unique(degree_sequence, return_counts=True))
ax2.set_title("Degree histogram")
ax2.set_xlabel("Degree")
ax2.set_ylabel("# of Nodes")</pre>
</div>
</div>
<div class="readable-text" id="p227">
<p>These plots are shown in figure 8.13.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p228">
<img alt="figure" height="1009" src="../Images/8-13.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.13</span> Visualization and statistics of the social graph and its large connected component. Network visualization using NetworkX default settings (top). A rank plot of node degree of the entire graph (bottom left). We see that about three-fourths of nodes have less than 20 adjacent nodes. A histogram of degree (bottom right).</h5>
</div>
<div class="readable-text" id="p229">
<p>Lastly, we can visualize an adjacency matrix of our graph, shown in figure 8.14, using the following command:</p>
</div>
<div class="browsable-container listing-container" id="p230">
<div class="code-area-container">
<pre class="code-area">plt.imshow(nx.to_numpy_matrix(social_graph), aspect='equal',cmap='twilight')<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p231">
<img alt="figure" height="969" src="../Images/8-14.png" width="1008"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.14</span> A visualized adjacency matrix of our social graph. Vertical and horizontal values refer to respective nodes.</h5>
</div>
<div class="readable-text" id="p232">
<p>As with the numerical adjacency matrix, for our undirected graph, this visual adjacency matrix has symmetry down the diagonal. All undirected graphs will have symmetric adjacency matrices. For directed graphs, this can happen but isn’t guaranteed. </p>
</div>
<div class="readable-text" id="p233">
<h3 class="readable-text-h3"><span class="num-string">8.3.4</span> Preprocessing and loading data into PyG</h3>
</div>
<div class="readable-text" id="p234">
<p>For this book, <em>preprocessing</em> consists of putting our data, including its properties, labels, or other metadata, in a format suitable for downstream machine learning models. Feature engineering can also be a step in this process. For feature engineering, we’ll often use graph algorithms to calculate the properties of nodes, edges, or subgraphs. </p>
</div>
<div class="readable-text intended-text" id="p235">
<p>An example for node features is betweenness centrality. If our schema allows, we can calculate and attach such properties to the node entities of our data. To perform this, we take the output of the ETL step, say an edge list, and import this into a graph processing framework to calculate betweenness centrality for each node. Once this quantity is obtained, we can store it using a dictionary with the node ID as keys, then use this as a node feature later on.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p236">
<h5 class="callout-container-h5 readable-text-h5">Betweenness centrality</h5>
</div>
<div class="readable-text" id="p237">
<p><em>Betweenness centrality</em> is a critical measure of node importance that quantifies the tendency of a node to lie in the shortest paths from source to destination nodes. Given a graph with <em>n</em> nodes, you could determine the shortest path between every unique pair of nodes in this graph. We could take this set of shortest paths and look for the presence of a particular node. If the node appears in all or most of these paths, it has a high betweenness centrality and would be considered to be highly influential. Conversely, if the node appears a few times (or only once) in the set of shortest paths, it will have a low betweenness centrality, and a low influence.</p>
</div>
</div>
<div class="readable-text" id="p238">
<p>Now that we have our data, we want to make it ready for use in our selected GNN framework. In this book, we use PyG, due to its robust suite of tools and flexibility in handling complex graph data. However, most standard GNN packages have mechanisms to import custom data into their frameworks. For this section, we’ll focus on three modules within PyG:</p>
</div>
<ul>
<li class="readable-text" id="p239"> <code>Data</code><em> module </em>(<code>torch_geometric.data</code>)—Allows inspection, manipulation, and creation of data objects that are used by the PyG environment. </li>
<li class="readable-text" id="p240"> <code>Utils</code> <em>module </em>(<code>torch_geometric.utils</code>)—Many useful methods. Helpful in this section are methods that allow the quick import and export of graph data. </li>
<li class="readable-text" id="p241"> <code>Datasets</code> <em>module </em>(<code>torch_geometric.datasets</code>)—Preloaded datasets, including benchmark datasets, and datasets from influential papers in the field. </li>
</ul>
<div class="readable-text" id="p242">
<p>Let’s begin with the <code>Datasets</code> module. This module contains datasets that have already been preprocessed and can readily be used by PyG’s methods. When starting with PyG, having these datasets allows for easy experimentation without worrying about creating a data pipeline. Similarly, by studying the codebase underlying these datasets, we can also learn how to create our own custom datasets.</p>
</div>
<div class="readable-text intended-text" id="p243">
<p>At the end of the previous section, we converted our raw data into a standard format and loaded our new graphs into a graph-processing framework. Now, we want to load our data into the PyG environment. Preprocessing in PyG has a few objectives:</p>
</div>
<ul>
<li class="readable-text" id="p244"> Creating data objects with multiple attributes from the level of nodes and edges to the subgraph and graph level </li>
<li class="readable-text" id="p245"> Combining different data sources into one object or set of related objects </li>
<li class="readable-text" id="p246"> Converting data into objects that can be processed using GPUs </li>
<li class="readable-text" id="p247"> Allowing splitting of training/testing/validation data </li>
<li class="readable-text" id="p248"> Enabling batching of data for training </li>
</ul>
<div class="readable-text" id="p249">
<p>These objectives are fulfilled by a hierarchy of classes within the <code>Data</code> module:</p>
</div>
<ul>
<li class="readable-text" id="p250"> <code>Data</code><em> class</em>—Creates graph objects. These objects can have optional built-in and custom-made attributes. </li>
<li class="readable-text" id="p251"> <code>Dataset</code><em> and </em><code>InMemoryDataset</code><em> classes</em> —Creates a repeatable data preprocessing pipeline. You can start from raw data files and add custom filters and transformations to achieve your preprocessed <em>data</em> objects. <code>Dataset</code> objects are larger than memory, while <code>InMemoryDataset</code> objects fit in memory. </li>
<li class="readable-text" id="p252"> <code>Dataloader</code><em> class</em> —Batches data objects for model training. </li>
</ul>
<div class="readable-text" id="p253">
<p>This is shown in figure 8.15, including how different data and dataset classes connect to the dataloader.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p254">
<img alt="figure" height="394" src="../Images/8-15.png" width="797"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.15</span> Steps to preprocess data in PyG. From raw files, there are essentially two paths to prep data for ingestion by a PyG algorithm. The first path, shown here, directly creates an iterator of data instances, which is used by the dataloader. The second path mimics the first but performs this process within the dataloader class.</h5>
</div>
<div class="readable-text" id="p255">
<p>There are two paths to preprocess data, one uses a <code>dataset</code> class and the other goes without it. The advantage of using the <code>dataset</code> class is that it allows us to save the generated datasets and also preserve filtering and transformation details. Dataset objects are flexible and can be modified to output variations of a dataset. On the other hand, if your custom dataset is simple or generated on the fly, and you have no use for saving the data or process long term, bypassing dataset objects may serve you well. So, in summary, we have the following different data-related classes:</p>
</div>
<ul>
<li class="readable-text" id="p256"> <code>Datasets</code> <em>objects</em> —Preprocessed datasets for benchmarking or testing an algorithm or architecture (not to be confused with <code>Dataset</code>—no “s” at the end—objects). </li>
<li class="readable-text" id="p257"> <code>Data</code><em> objects into iterator</em> —Graph objects that are generated on the fly or for whom there is no need to save. </li>
<li class="readable-text" id="p258"> <code>Dataset</code><em> object</em> —For graph objects that should be preserved, including the data pipeline, filtering and transformations, input raw data files, and output processed data files. Not to be confused with <code>Datasets</code> (with “s” at the end) objects. </li>
</ul>
<div class="readable-text" id="p259">
<p>With those basics, let’s preprocess our social graph data. We’ll cover the following cases:</p>
</div>
<ul>
<li class="readable-text" id="p260"> <em>Convert into a </em><code>data</code><em> instance using NetworkX.</em> For quick conversion from NetworkX to PyG, ideal for ad hoc processing or when using NetworkX’s functionalities. </li>
<li class="readable-text" id="p261"> <em>Convert into a </em><code>data</code><em> instance using input files.</em> Offers control over the data import process, which is ideal for raw data and custom preprocessing requirements. </li>
<li class="readable-text" id="p262"> <em>Convert to </em><code>dataset</code><em> instance.</em> For systematic, scalable, and reproducible data preprocessing and management, especially for complex or reusable datasets. </li>
<li class="readable-text" id="p263"> <em>Convert </em><code>data</code><em> objects for use in </em><code>dataloader</code><em> without the </em><code>dataset</code><em> class.</em> For scenarios where simplicity and speed are prioritized over systematic data management and preprocessing, or for on-the-fly and synthetic data. </li>
</ul>
<div class="readable-text" id="p264">
<p>First, we’ll import the needed modules from PyG in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p265">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.4</span> Required imports, covering data object creation</h5>
<div class="code-area-container">
<pre class="code-area">import torch
from torch_geometric.data import Data
from torch_geometric.data import InMemoryDataset
from torch_geometric import utils</pre>
</div>
</div>
<div class="readable-text" id="p266">
<h4 class="readable-text-h4">Case A: Create PyG data object using the NetworkX object</h4>
</div>
<div class="readable-text" id="p267">
<p>In the previous sections, we’ve explored a graph expressed as a NetworkX <code>graph</code> object. PyG’s <code>util</code> module has a method that can directly create a PyG <code>data</code> object from a NetworkX <code>graph</code> object:</p>
</div>
<div class="browsable-container listing-container" id="p268">
<div class="code-area-container">
<pre class="code-area">data = utils.from_networkx(social_graph)</pre>
</div>
</div>
<div class="readable-text" id="p269">
<p>The <code>from_networkx</code> method preserves nodes, edges, and their attributes, but it should be checked to ensure the translation from one module to another went smoothly.</p>
</div>
<div class="readable-text" id="p270">
<h4 class="readable-text-h4">Case B: Create PyG data object using raw files</h4>
</div>
<div class="readable-text" id="p271">
<p>For greater control over data import into PyG, we can start with raw files or files from any stage of the ETL process. In our social graph case, we can begin with the edge list file created earlier.</p>
</div>
<div class="readable-text intended-text" id="p272">
<p>Now, let’s review an example where we use code to process and convert our social graph from an edge list text file into a format suitable for training a GNN model in PyG. We prepare node features, labels, edges, and training/testing sets for use in the PyG environment.</p>
</div>
<div class="readable-text" id="p273">
<h4 class="readable-text-h4">Part 1: Import and prepare graph data</h4>
</div>
<div class="readable-text" id="p274">
<p>This part includes reading an edge list from a file to create a NetworkX graph, extracting the list of nodes, creating mappings from node names to indices, and vice versa:</p>
</div>
<div class="browsable-container listing-container" id="p275">
<div class="code-area-container">
<pre class="code-area">social_graph = nx.read_edgelist('edge_list2.txt')  <span class="aframe-location"/> #1

list_of_nodes = list(set(list(social_graph)))  <span class="aframe-location"/> #2
indices_of_nodes = [list_of_nodes.index(x)\
 for x in list_of_nodes]   <span class="aframe-location"/> #3

node_to_index = dict(zip(list_of_nodes, indices_of_nodes))  <span class="aframe-location"/> #4
index_to_node = dict(zip(indices_of_nodes, list_of_nodes))</pre>
<div class="code-annotations-overlay-container">
     #1 An edge list is read from a text file and used to create a NetworkX graph.
     <br/>#2 All unique nodes in the graph are then extracted and listed.
     <br/>#3 Indices for each node are also generated.
     <br/>#4 Two dictionaries are created to allow easy conversion between node names and their respective indices, facilitating the handling and manipulation of graph data.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p276">
<h4 class="readable-text-h4">Part 2: Process edges and node features</h4>
</div>
<div class="readable-text" id="p277">
<p>This part focuses on converting the edges and node attributes into a format that can be easily used with PyTorch for machine learning tasks:</p>
</div>
<div class="browsable-container listing-container" id="p278">
<div class="code-area-container">
<pre class="code-area">list_edges = nx.convert.to_edgelist(social_graph)  <span class="aframe-location"/> #1
list_edges = list(list_edges)
named_edge_list_0 = [x[0] for x in list_edges]  <span class="aframe-location"/> #2
named_edge_list_1 = [x[1] for x in list_edges]

indexed_edge_list_0 = [node_to_index[x]\
 for x in named_edge_list_0]  <span class="aframe-location"/> #3
indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]

x = torch.FloatTensor([[1] for x in\ 
range(len(list_of_nodes))]) <span class="aframe-location"/> #4
y = torch.FloatTensor([1]*974 + [0]*973)  <span class="aframe-location"/> #5
y = y.long()</pre>
<div class="code-annotations-overlay-container">
     #1 A NetworkX edge list object is created.
     <br/>#2 It’s then transformed into two separate lists representing the source and destination nodes of each edge.
     <br/>#3 These lists are then indexed using the previously created node-to-index mapping.
     <br/>#4 The node features and labels are prepared using PyTorch tensor objects, assuming a simple scenario where all nodes have the same single feature. 
     <br/>#5 The node features and labels are prepared using PyTorch tensor objects, assuming a simple scenario where all nodes have the same single feature.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p279">
<h4 class="readable-text-h4">Part 3: Prepare data for training and testing</h4>
</div>
<div class="readable-text" id="p280">
<p>In this part, the dataset is prepared for training and testing by creating masks for data splitting and combining all the processed data into a single PyTorch data object:</p>
</div>
<div class="browsable-container listing-container" id="p281">
<div class="code-area-container">
<pre class="code-area">edge_index = torch.tensor([indexed_edge_list_0,\
 indexed_edge_list_1])   <span class="aframe-location"/> #1

train_mask = torch.zeros(len(list_of_nodes),\
 dtype=torch.uint8)  <span class="aframe-location"/> #2
train_mask[:int(0.8 * len(list_of_nodes))] = 1 #train only on the 80% nodes
test_mask = torch.zeros(len(list_of_nodes),\
 dtype=torch.uint8) #test on 20 % nodes 
test_mask[- int(0.2 * len(list_of_nodes)):] = 1
train_mask = train_mask.bool()
test_mask = test_mask.bool()

data = Data(x=x, y=y, edge_index=edge_index,\
 train_mask=train_mask, test_mask=test_mask)   <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 The edge indices created in part 2 are converted into a PyTorch tensor. 
     <br/>#2 Masks for training and testing datasets are created by splitting the nodes into two separate groups, ensuring that specific portions of the data are used for training and testing.
     <br/>#3 All the processed components, including node features, labels, edge indices, and data masks, are then combined into a single PyTorch Data object, preparing the data for subsequent machine learning tasks.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p282">
<p>We’ve created a <code>data</code> object from an <code>edgelist</code> file. Such an object can be inspected with PyG commands, though the set of commands is limited compared to a graph processing library. Such a <code>data</code> object can also be further prepared so that it can be accessed by a <code>dataloader</code>, which we’ll cover next. </p>
</div>
<div class="readable-text" id="p283">
<h4 class="readable-text-h4">Case C: Create PyG dataset object using custom class and input files</h4>
</div>
<div class="readable-text" id="p284">
<p>If the previous listing is suitable for our purposes, and we want to use it repeatedly, a preferable option is to create a permanent class that we can include for our pipeline. This is what the <code>dataset</code> class does. </p>
</div>
<div class="readable-text intended-text" id="p285">
<p>Let’s next create a <code>dataset</code> object, shown in listing 8.5. In this example, we name our <code>dataset</code> <code>MyOwnDataset</code> and have it inherit from <code>InMemoryDataset</code> because our social graph is small enough to sit in memory. As discussed earlier, for larger graphs, data can be accessed from disk by having the <code>dataset</code> object inherit from <code>Dataset</code> instead of <code>InMemoryDataset</code>.</p>
</div>
<div class="readable-text intended-text" id="p286">
<p>This first part of the code initiates the custom <code>dataset</code> class, inheriting properties from the <code>InMemoryDataset</code> class. The constructor initializes the dataset, loads processed data, and defines the properties for raw and processed filenames. The raw files are kept empty as this example doesn’t require them, and the processed data is fetched from a specified path.</p>
</div>
<div class="browsable-container listing-container" id="p287">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.5</span> Class to create a dataset object (part 1)</h5>
<div class="code-area-container">
<pre class="code-area">class MyOwnDataset(InMemoryDataset):
    def __init__(self, root, \
    transform=None, pre_transform=None):\   <span class="aframe-location"/> #1
        super(MyOwnDataset, self).__init__(root,
    @property transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    def raw_file_names(self): <span class="aframe-location"/> #2
        return []
    @property
    def processed_file_names(self):  <span class="aframe-location"/> #3
        return ['../test.dataset']</pre>
<div class="code-annotations-overlay-container">
     #1 Initializes the dataset class. This class inherits from the InMemoryDataset class. This init method creates data and slices objects to be updated in the process method.
     <br/>#2 An optional method that specifies the location of the raw files required for processing. For our more rudimentary example, we don’t make use of this but have included it for completeness. In later chapters, we’ll make use of this as our dataset becomes a bit more complex.
     <br/>#3 This method saves our generated dataset to disk.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p288">
<p>This segment of the code is for data downloading and processing. It reads an edge list from a text file and converts it into a NetworkX graph. The nodes and edges of the graph are then indexed and converted into tensors suitable for machine learning tasks. The method downloaded is kept as a placeholder in case there’s a need to download raw data in the future.</p>
</div>
<div class="browsable-container listing-container" id="p289">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.6</span> Class to create a dataset object (part 2)</h5>
<div class="code-area-container">
<pre class="code-area">    def download(self):  <span class="aframe-location"/> #1
        # Download to `self.raw_dir`.
        pass

    def process(self):  <span class="aframe-location"/> #2
        # Read data into `Data` list.
        data_list = []

        eg = nx.read_edgelist('edge_list2.txt') 

        list_of_nodes = list(set(list(eg)))
        indices_of_nodes = [list_of_nodes.index(x) for x in list_of_nodes]

        node_to_index = dict(zip(list_of_nodes, indices_of_nodes))
        index_to_node = dict(zip(indices_of_nodes, list_of_nodes))

        list_edges = nx.convert.to_edgelist(eg)
        list_edges = list(list_edges)
        named_edge_list_0 = [x[0] for x in list_edges]
        named_edge_list_1 = [x[1] for x in list_edges]

        indexed_edge_list_0 = [node_to_index[x] for x in named_edge_list_0]
        indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]</pre>
<div class="code-annotations-overlay-container">
     #1 Allows raw data to be downloaded to a local disk.
     <br/>#2 The process method contains the preprocessing steps to create our data object, and then makes additional steps to partition our data for loading.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p290">
<p>This final part of the code is focused on preparing and saving the data for machine learning models. It creates feature and label tensors, prepares the edge index, and generates training and testing masks to split the dataset. The data is then collated and saved in the processed path for easy retrieval during model training.</p>
</div>
<div class="browsable-container listing-container" id="p291">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.7</span> Class to create a dataset object (part 3)</h5>
<div class="code-area-container">
<pre class="code-area">        x = torch.FloatTensor([[1] for x in range(len(list_of_nodes))])#
  [[] for x in xrange(n)]
        y = torch.FloatTensor([1]*974 + [0]*973)
        y = y.long()

        edge_index = torch.tensor([indexed_edge_list_0, indexed_edge_list_1])

        train_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8)
        train_mask[:int(0.8 * len(list_of_nodes))]\
 = 1 #train only on the 80% nodes
        test_mask = torch.zeros(len(list_of_nodes), \
dtype=torch.uint8) #test on 20 % nodes 
        test_mask[- int(0.2 * len(list_of_nodes)):] = 1

        train_mask = train_mask.bool()
        test_mask = test_mask.bool()

        data_example = Data(x=x, y=y, edge_index=edge_index, \
train_mask=train_mask, test_mask=test_mask)

        data_list.append(data_example)          <span class="aframe-location"/> #1

        data, slices = self.collate(data_list)  
        torch.save((data, slices),\
 self.processed_paths[0])   <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 In this first simple use of a dataset class, we use a small dataset. In practice, we’ll process much larger datasets and wouldn’t do this all at once. We’d create examples of our data, then append them to a list. For our purposes (training on this data), pulling from a list object would be slow, so we take this iterable, and use collate to combine the data examples into one data object. The collate method also creates a dictionary named slices that is used to pull single samples from this data object.
     <br/>#2 Saves our preprocessed data to disk
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p292">
<h4 class="readable-text-h4">Case D: Create PyG data objects for use in dataloader without use of a dataset object</h4>
</div>
<div class="readable-text" id="p293">
<p>Lastly, we explain how to bypass <code>dataset</code> object creation and have the <code>dataloader</code> work directly with your <code>data</code> object, as illustrated in figure 8.15. In the PyG documentation, there is a section that outlines how to do this.</p>
</div>
<div class="readable-text intended-text" id="p294">
<p>Just as in regular PyTorch, you don’t have to use datasets, for example, when you want to create synthetic data on the fly without saving them explicitly to disk. In this case, simply pass a regular Python list holding <code>torch_geometric.data.Data</code> objects and pass them to <code>torch_geometric.data.DataLoader</code>:</p>
</div>
<div class="browsable-container listing-container" id="p295">
<div class="code-area-container">
<pre class="code-area">from torch_geometric.data import Data, DataLoader

data_list = [Data(...), ..., Data(...)]
loader = DataLoader(data_list, batch_size=32)</pre>
</div>
</div>
<div class="readable-text" id="p296">
<p>In this chapter, we’ve covered the steps that go from project outline, through to converting raw data into a format ready for GNNs. As we conclude this section, it’s worth noting that every dataset is different. The procedures outlined in this discussion provide a structural framework that serves as a starting point, not a one-size-fits-all solution. In the final section, we turn to the subject of sourcing data to support data projects.</p>
</div>
<div class="readable-text" id="p297">
<h2 class="readable-text-h2"><span class="num-string">8.4</span> Where to find graph data</h2>
</div>
<div class="readable-text" id="p298">
<p>To not start from scratch in developing a graph data model and schema for your problem, there are several sources of published models and schemas. They include industry standard data models, published datasets, published semantic models (including knowledge graphs), and academic papers. A set of example sources is provided in table 8.2. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p299">
<h5 class="callout-container-h5 readable-text-h5">Sourcing graph data</h5>
</div>
<div class="readable-text" id="p300">
<p>Details of different sources for graph-based data that can be used for GNN projects.</p>
</div>
<ul>
<li class="readable-text" id="p301"> <em>From nongraph data</em>—In this chapter, we assumed that the data lies in nongraph sources and must be transformed into a graph format using ETL and preprocessing. Having a schema can help guide such a transformation and keep it ready for further analysis. </li>
<li class="readable-text" id="p302"> <em>Existing graph datasets—</em>The number of freely available graph datasets is growing. Two GNN libraries we use in this book, Deep Graph Library (DGL) and PyG, come with a number of benchmark datasets installed. Many such datasets are from influential academic papers. However, such datasets are small scale, which limits reproducibility of results, and whose performance don’t necessarily scale for large datasets. </li>
<li class="readable-text" id="p303"> A source of data that seeks to mitigate the problems of earlier benchmark datasets in this space is Open Graph Benchmark (OGB). This initiative provides access to a variety of real-world datasets, of varying scales. OGB also publishes performance benchmarks by learning task. Table 8.2 lists a few repositories of graph datasets. </li>
<li class="readable-text" id="p304"> <em>From generation</em>—Many graph processing frameworks and graph databases allow the generation of random graphs using a number of algorithms. Though random, depending on the generating algorithm, the resulting graph will have characteristics that are predictable. </li>
</ul>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p305">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.2</span> Graph datasets and semantic models</h5>
<table>
<thead>
<tr>
<th>
<div>
         Source 
       </div></th>
<th>
<div>
         Type 
       </div></th>
<th>
<div>
         Problem Domains 
       </div></th>
<th>
<div>
         URL 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Open Graph Benchmark (OGB) <br/></td>
<td>  Graph datasets and benchmarks <br/></td>
<td>  Social networks, drug discovery <br/></td>
<td> <a href="https://ogb.stanford.edu/">https://ogb.stanford.edu/</a> <br/></td>
</tr>
<tr>
<td>  GraphChallenge Datasets <br/></td>
<td>  Graph datasets <br/></td>
<td>  Network science, biology <br/></td>
<td> <a href="https://graphchallenge.mit.edu/data-sets">https://graphchallenge.mit.edu/data-sets</a> <br/></td>
</tr>
<tr>
<td>  Network Repository <br/></td>
<td>  Graph datasets <br/></td>
<td>  Network science, bioinformatics, machine learning, data mining, physics, and social science <br/></td>
<td> <a href="http://networkrepository.com/">http://networkrepository.com/</a> <br/></td>
</tr>
<tr>
<td>  SNAP Datasets <br/></td>
<td>  Graph datasets <br/></td>
<td>  Social networks, network science, road networks, commercial networks, finance <br/></td>
<td> <a href="http://snap.stanford.edu/data/">http://snap.stanford.edu/data/</a> <br/></td>
</tr>
<tr>
<td>  Schema.org <br/></td>
<td>  Semantic data model <br/></td>
<td>  Internet web pages <br/></td>
<td> <a href="https://schema.org/">https://schema.org/</a> <br/></td>
</tr>
<tr>
<td>  Wikidata <br/></td>
<td>  Semantic data model <br/></td>
<td>  Wikipedia pages <br/></td>
<td> <a href="http://www.wikidata.org/">www.wikidata.org/</a> <br/></td>
</tr>
<tr>
<td>  Financial Industry Business Ontology <br/></td>
<td>  Semantic data model <br/></td>
<td>  Finance <br/></td>
<td> <a href="https://github.com/edmcouncil/fibo">https://github.com/edmcouncil/fibo</a> <br/></td>
</tr>
<tr>
<td>  Bioportal <br/></td>
<td>  List of medical semantic models <br/></td>
<td>  Medical <br/></td>
<td> <a href="https://bioportal.bioontology.org/ontologies/">https://bioportal.bioontology.org/ontologies/</a> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p306">
<p>Public graph datasets also exist in several places. Published datasets have accessible data, with summary statistics. Often, however, they lack explicit schemas, conceptual or otherwise. To derive the dataset’s entities, relations, rules, and constraints, querying the data becomes necessary.</p>
</div>
<div class="readable-text intended-text" id="p307">
<p>For semantic models based on property, RDF, and other data models, there are some general datasets, and others are targeted to particular industries and verticals. Such references seldom use graph-centric terms (e.g., <em>node</em>, <em>vertex</em>, and <em>edge</em>) but will use terms related to semantics and ontologies (e.g., <em>entity</em>, <em>relationship</em>, <em>links</em>). Unlike the graph datasets, the semantic models offer data frameworks, not the data itself.</p>
</div>
<div class="readable-text intended-text" id="p308">
<p>Reference papers and published schemas can provide ideas and templates that can help in developing your schema. There are a few use cases targeted toward industry verticals that both represent a situation using graphs and use graph algorithms, including GNNs, to solve a relevant problem. Transaction fraud in financial institutions, molecular fingerprinting in chemical engineering, and page rank in social networks are a few examples. Perusing such existing work can provide a boost to development efforts. On the other hand, often such published work is done for academic, not industry goals. A network that is developed to prove an academic point or make empirical observations may not have qualities amenable to an enterprise system that must be maintained and be used on dirty and dynamic data.</p>
</div>
<div class="readable-text" id="p309">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p310"> Planning for a graph learning project involves more steps than in traditional machine learning projects. The objectives and requirements will influence the design of the system, data models, and GNN architecture. The project includes creating robust graph data models, understanding and transforming raw data, and ensuring that the models effectively represent the complex relationships within the recruitment landscape. </li>
<li class="readable-text" id="p311"> One important step is creating the data model and schema for your data. These processes are essential to avoid technical debt. This involves designing the elements, relationships, and constraints; running queries; analyzing results; making adjustments; and validating against criteria to ensure the model’s readiness for complex queries and machine learning applications. A graph data model will be refined through iterative testing and refactoring to ensure it effectively supports the analysis of complex relationships within the recruitment data. </li>
<li class="readable-text" id="p312"> There are many encoding and serialization options for keeping data in memory or in raw files, including language and system-agnostic formats such as JSON, CSV, and XML. Language-specific formats, such as Python’s Pickle, and system-driven formats from specific software and libraries such as SNAP, NetworkX, and Gephi, are also mentioned. For big data, Avro and matrix-based formats (sparse column matrix, sparse row matrix, and matrix market format) are highlighted as efficient options for handling large datasets. </li>
<li class="readable-text" id="p313"> A data pipeline can start with raw data that undergoes exploratory analysis and preprocessing to be usable by GNN libraries such as PyG. The raw data is transformed into standard formats such as edge lists or adjacency matrices, ensuring consistency and usability for different problems. </li>
<li class="readable-text" id="p314"> Graph processing frameworks such as NetworkX are used for light exploratory data analysis (EDA) and visualization. Graph objects, such as adjacency and edge lists, are loaded into NetworkX. The visual representation and statistical analysis, such as the number of nodes, edges, and connected components, are derived to understand the graph’s structure and complexity. </li>
<li class="readable-text" id="p315"> The PyG library is used for preprocessing, involving the conversion of data into formats that can be easily manipulated and trained with. Data objects are created with multiple attributes at various levels, enabling GPU processing and facilitating the splitting of training, testing, and validation data. The choice between using dataset objects or bypassing them depends on the need for saving data and the complexity of the dataset. </li>
<li class="readable-text" id="p316"> There are numerous repositories of ready-to-use graph datasets and semantic models covering various domains, such as social networks and drug discovery. However, while these datasets are useful for learning and benchmarking, they are often small-scale and may not be directly applicable for large, real-world problems. </li>
<li class="readable-text" id="p317"> While public graph datasets and semantic models provide a starting point, they often lack explicit schemas requiring additional work to derive entities, relations, and constraints. Additionally, while academic papers offer templates for developing schemas, they are typically designed for academic purposes and may not be directly transferable to real-world, industry-specific applications with dynamic and dirty data. </li>
</ul>
</div></body></html>