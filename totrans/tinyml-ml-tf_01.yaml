- en: Chapter 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this book is to show how any developer with basic experience using
    a command-line terminal and code editor can get started building their own projects
    running machine learning (ML) on embedded devices.
  prefs: []
  type: TYPE_NORMAL
- en: When I first joined Google in 2014, I discovered a lot of internal projects
    that I had no idea existed, but the most exciting was the work that the OK Google
    team were doing. They were running neural networks that were just 14 kilobytes
    (KB) in size! They needed to be so small because they were running on the digital
    signal processors (DSPs) present in most Android phones, continuously listening
    for the “OK Google” wake words, and these DSPs had only tens of kilobytes of RAM
    and flash memory. The team had to use the DSPs for this job because the main CPU
    was powered off to conserve battery, and these specialized chips use only a few
    milliwatts (mW) of power.
  prefs: []
  type: TYPE_NORMAL
- en: Coming from the image side of deep learning, I’d never seen networks so small,
    and the idea that you could use such low-power chips to run neural models stuck
    with me. As I worked on getting TensorFlow and later TensorFlow Lite running on
    Android and iOS devices, I remained fascinated by the possibilities of working
    with even simple chips. I learned that there were other pioneering projects in
    the audio world (like Pixel’s Music IQ) for predictive maintenance (like PsiKick)
    and even in the vision world (Qualcomm’s Glance camera module).
  prefs: []
  type: TYPE_NORMAL
- en: It became clear to me that there was a whole new class of products emerging,
    with the key characteristics that they used ML to make sense of noisy sensor data,
    could run using a battery or energy harvesting for years, and cost only a dollar
    or two. One term I heard repeatedly was “peel-and-stick sensors,” for devices
    that required no battery changes and could be applied anywhere in an environment
    and forgotten. Making these products real required ways to turn raw sensor data
    into actionable information locally, on the device itself, since the energy costs
    of transmitting streams anywhere have proved to be inherently too high to be practical.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the idea of TinyML comes in. Long conversations with colleagues
    across industry and academia have led to the rough consensus that if you can run
    a neural network model at an energy cost of below 1 mW, it makes a lot of entirely
    new applications possible. This might seem like a somewhat arbitrary number, but
    if you translate it into concrete terms, it means a device running on a coin battery
    has a lifetime of a year. That results in a product that’s small enough to fit
    into any environment and able to run for a useful amount of time without any human
    intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I’m going to be jumping straight into using some technical terms to talk about
    what this book will be covering, but don’t worry if some of them are unfamiliar
    to you; we define their meaning the first time we use them.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering about platforms like the Raspberry Pi,
    or NVIDIA’s Jetson boards. These are fantastic devices, and I use them myself
    frequently, but even the smallest Pi is similar to a mobile phone’s main CPU and
    so draws hundreds of milliwatts. Keeping one running even for a few days requires
    a battery similar to a smartphone’s, making it difficult to build truly untethered
    experiences. NVIDIA’s Jetson is based on a powerful GPU, and we’ve seen it use
    up to 12 watts of power when running at full speed, so it’s even more difficult
    to use without a large external power supply. This is usually not a problem in
    automotive or robotics applications, since the mechanical parts demand a large
    power source themselves, but it does make it tough to use these platforms for
    the kinds of products I’m most interested in, which need to operate without a
    wired power supply. Happily, when using them the lack of resource constraints
    means that frameworks like TensorFlow, TensorFlow Lite, and NVIDIA’s TensorRT
    are available, since they’re usually based on Linux-capable Arm Cortex-A CPUs,
    which have hundreds of megabytes of memory. This book will not be focused on describing
    how to run on those platforms for the reason just mentioned, but if you’re interested,
    there are a lot of resources and documentation available; for example, see [TensorFlow
    Lite’s mobile documentation](https://www.tensorflow.org/lite).
  prefs: []
  type: TYPE_NORMAL
- en: Another characteristic I care about is cost. The cheapest Raspberry Pi Zero
    is $5 for makers, but it is extremely difficult to buy that class of chip in large
    numbers at that price. Purchases of the Zero are usually restricted by quantity,
    and while the prices for industrial purchases aren’t transparent, it’s clear that
    $5 is definitely unusual. By contrast, the cheapest 32-bit microcontrollers cost
    much less than a dollar each. This low price has made it possible for manufacturers
    to replace traditional analog or electromechanical control circuits with software-defined
    alternatives for everything from toys to washing machines. I’m hoping we can use
    the ubiquity of microcontrollers in these devices to introduce artificial intelligence
    as a software update, without requiring a lot of changes to existing designs.
    It should also make it possible to get large numbers of smart sensors deployed
    across environments like buildings or wildlife reserves without the costs outweighing
    the benefits or funds available.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded Devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The definition of TinyML as having an energy cost below 1 mW does mean that
    we need to look to the world of embedded devices for our hardware platforms. Until
    a few years ago, I wasn’t familiar with them myself—they were shrouded in mystery
    for me. Traditionally they had been 8-bit devices and used obscure and proprietary
    toolchains, so it seemed very intimidating to get started with any of them. A
    big step forward came when Arduino introduced a user-friendly integrated development
    environment (IDE) along with standardized hardware. Since then, 32-bit CPUs have
    become the standard, largely thanks to Arm’s Cortex-M series of chips. When I
    started to prototype some ML experiments a couple of years ago, I was pleasantly
    surprised by how relatively straightforward the development process had become.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded devices still come with some tough resource constraints, though. They
    often have only a few hundred kilobytes of RAM, or sometimes much less than that,
    and have similar amounts of flash memory for persistent program and data storage.
    A clock speed of just tens of megahertz is not unusual. They will definitely not
    have full Linux (since that requires a memory controller and at least one megabyte
    of RAM), and if there is an operating system, it may well not provide all or any
    of the POSIX or standard C library functions you expect. Many embedded systems
    avoid using dynamic memory allocation functions like `new` or `malloc()` because
    they’re designed to be reliable and long-running, and it’s extremely difficult
    to ensure that if you have a heap that can be fragmented. You might also find
    it tricky to use a debugger or other familiar tools from desktop development,
    since the interfaces you’ll be using to access the chip are very specialized.
  prefs: []
  type: TYPE_NORMAL
- en: There were some nice surprises as I learned embedded development, though. Having
    a system with no other processes to interrupt your program can make building a
    mental model of what’s happening very simple, and the straightforward nature of
    a processor without branch prediction or instruction pipelining makes manual assembly
    optimization a lot easier than on more complex CPUs. I also find a simple joy
    in seeing LEDs light up on a miniature computer that I can balance on a fingertip,
    knowing that it’s running millions of instructions a second to understand the
    world around it.
  prefs: []
  type: TYPE_NORMAL
- en: Changing Landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s only recently that we’ve been able to run ML on microcontrollers at all,
    and the field is very young, which means hardware, software, and research are
    all changing extremely quickly. This book is a based on a snapshot of the world
    as it existed in 2019, which in this area means some parts were out of date before
    we’d even finished writing the last chapter. We’ve tried to make sure we’re relying
    on hardware platforms that will be available over the long term, but it’s likely
    that devices will continue to improve and evolve. The TensorFlow Lite software
    framework that we use has a stable API, and we’ll continue to support the examples
    we give in the text over time, but we also provide web links to the very latest
    versions of all our sample code and documentation. You can expect to see reference
    applications covering more use cases than we have in this book being added to
    the TensorFlow repository, for example. We also aim to focus on skills like debugging,
    model creation, and developing an understanding of how deep learning works, which
    will remain useful even as the infrastructure you’re using changes.
  prefs: []
  type: TYPE_NORMAL
- en: We want this book to give you the foundation you need to develop embedded ML
    products to solve problems you care about. Hopefully we’ll be able to start you
    along the road of building some of the exciting new applications I’m certain will
    be emerging over the next few years in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Pete Warden
  prefs: []
  type: TYPE_NORMAL
