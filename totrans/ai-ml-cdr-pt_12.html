<html><head></head><body><section data-pdf-bookmark="Chapter 11. Using Convolutional and Recurrent Methods for Sequence Models" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch11_using_convolutional_and_recurrent_methods_for_sequ_1748549734762226">
      <h1><span class="label">Chapter 11. </span>Using Convolutional and Recurrent Methods for Sequence Models</h1>
      <p>The last few chapters introduced you to sequence data. You saw how to predict it, first by using statistical methods and then by using basic ML methods with a deep neural network. You also explored how to tune the model’s hyperparameters for better <span class="keep-together">performance.</span> </p>
      <p>In this chapter, you’ll look at additional techniques that may further enhance your ability to predict sequence data by using convolutional neural networks as well as recurrent neural networks.</p>
      <section data-pdf-bookmark="Convolutions for Sequence Data" data-type="sect1"><div class="sect1" id="ch11_convolutions_for_sequence_data_1748549734762529">
        <h1>Convolutions for Sequence Data</h1>
        <p>In <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a>, you were introduced<a contenteditable="false" data-primary="time series data" data-secondary="convolutions for" data-type="indexterm" id="ch11conv"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="time series data and sequences" data-type="indexterm" id="ch11conv2"/> to convolutions in which a two-dimensional (2D) filter was passed over an image to modify it and potentially extract features. Over time, the neural network learned which filter values were effective at matching the modifications that had been made to the pixels to their labels, thus effectively extracting features from the image. The same technique can be applied to numeric time series data, but with one modification: the convolution will be one dimensional (1D) instead of two dimensional.</p>
        <p>Consider, for example, the series of numbers in <a data-type="xref" href="#ch11_figure_1_1748549734749083">Figure 11-1</a>.</p>
        <figure><div class="figure" id="ch11_figure_1_1748549734749083">
          <img alt="" src="assets/aiml_1101.png"/>
          <h6><span class="label">Figure 11-1. </span>A sequence of numbers</h6>
        </div></figure>
        <p>A 1D convolution could operate on these as follows. Consider the convolution to be a 1 × 3 filter with filter values of –0.5, 1, and –0.5, respectively. In this case, the first value in the sequence will be lost and the second value will be transformed from 8 to –1.5 (see <a data-type="xref" href="#ch11_figure_2_1748549734749133">Figure 11-2</a>).</p>
        <figure><div class="figure" id="ch11_figure_2_1748549734749133">
          <img alt="" src="assets/aiml_1102.png"/>
          <h6><span class="label">Figure 11-2. </span>Using a convolution with the number sequence</h6>
        </div></figure>
        <p>The filter will then stride across the values, calculating new ones as it goes. So, for example, in the next stride, 15 will be transformed into 3 (see <a data-type="xref" href="#ch11_figure_3_1748549734749161">Figure 11-3</a>).</p>
        <figure><div class="figure" id="ch11_figure_3_1748549734749161">
          <img alt="" src="assets/aiml_1103.png"/>
          <h6><span class="label">Figure 11-3. </span>An additional stride in the 1D convolution</h6>
        </div></figure>
        <p>Using this method, it’s possible to extract the patterns between values and learn the filters that extract them successfully, in much the same way that convolutions on the pixels in images can extract features. In this instance, there are no labels, but the convolutions that minimize overall loss can be learned.</p>
        <section data-pdf-bookmark="Coding Convolutions" data-type="sect2"><div class="sect2" id="ch11_coding_convolutions_1748549734762616">
          <h2>Coding Convolutions</h2>
          <p>Before coding convolutions,<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="time series data and sequences" data-tertiary="coding convolutions" data-type="indexterm" id="ch11cod"/><a contenteditable="false" data-primary="time series data" data-secondary="convolutions for" data-tertiary="coding convolutions" data-type="indexterm" id="ch11cod2"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="time series data and sequences" data-tertiary="sliding windows dataset code online" data-type="indexterm" id="id1465"/><a contenteditable="false" data-primary="online resources" data-secondary="code for book" data-tertiary="sliding windows for sequence convolutions" data-type="indexterm" id="id1466"/><a contenteditable="false" data-primary="book code online" data-secondary="sliding windows for sequence convolutions" data-type="indexterm" id="id1467"/><a contenteditable="false" data-primary="GitHub" data-secondary="sliding windows for sequence convolutions" data-type="indexterm" id="id1468"/> you’ll need to use the <em>sliding windows</em> technique to create a dataset, as shown in <a data-type="xref" href="ch10.html#ch10_creating_ml_models_to_predict_sequences_1748549713795870">Chapter 10</a>. The code is available <a href="https://oreil.ly/pytorch_ch11">on this book’s GitHub page</a>. </p>
          <p>Once you have that dataset, you can add a convolutional layer before the dense layers that you had previously. Here’s the code, which we’ll look at line by line:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">CNN1D</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">CNN1D</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv1d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                              <code class="n">out_channels</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
                              <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                              <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
 
        <code class="n">conv_output_size</code> <code class="o">=</code> <code class="n">input_size</code>  <code class="c1"># Same padding maintains input size</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dense1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code> <code class="o">*</code> <code class="n">conv_output_size</code><code class="p">,</code> <code class="mi">28</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dense2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dense3</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="c1"># Transpose input from [batch_size, sequence_length] </code>
        <code class="c1"># to [batch_size, 1, sequence_length]</code>
        <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code> <code class="o">==</code> <code class="mi">2</code><code class="p">:</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="k">elif</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code> <code class="o">==</code> <code class="mi">3</code> <code class="ow">and</code> <code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">!=</code> <code class="mi">1</code><code class="p">:</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
 
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">conv1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense2</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dense3</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code></pre>
          <p>First, notice the new line that defines a 1D convolutional layer:<a contenteditable="false" data-primary="time series data" data-secondary="convolutions for" data-tertiary="1D convolutional layer" data-type="indexterm" id="id1469"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="time series data and sequences" data-tertiary="1D convolutional layer" data-type="indexterm" id="id1470"/></p>
          <pre data-code-language="python" data-type="programlisting">        <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv1d</code><code class="p">(</code><code class="n">in_channels</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                              <code class="n">out_channels</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
                              <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                              <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
          <p>The <code>in_channels</code> parameter defines the dimensionality of the input data. As we have a single sequence of numbers with a single value per data point, this is <code>1</code>. If we were using multiple features per time step, such as perhaps an RGB color value, this would be <code>3</code>. </p>
          <p>The <code>out_channels</code> parameter is the number of filters (aka convolutions) that the network will learn. </p>
          <p>The <code>kernel_size</code> parameter determines the size of the convolution (i.e., the number of data points on the line that a convolution will filter). Refer back to <a data-type="xref" href="#ch11_figure_2_1748549734749133">Figure 11-2</a> and <a data-type="xref" href="#ch11_figure_3_1748549734749161">Figure 11-3</a> and you’ll see a convolution there with a kernel size of 3.</p>
          <p>The <code>padding</code> parameter adds elements to the beginning and end of your list of data. So, for example, the list of numbers in <a data-type="xref" href="#ch11_figure_1_1748549734749083">Figure 11-1</a> is [4 8 15 16 23 42 51 64 99 –1]. When the filter of kernel size 3 looks at this list, it begins with [4 8 15], and <em>4</em> never gets to be the “middle” number. The filter effectively ignores the numbers at the beginning and end of the list. With padding, a 0 will be added at the front and back of the list to make it [0 4 8 15 16 23 42 51 64 99 –1 0], and you can now see that the filter will look first at [0 4 8].</p>
          <p>Next, we see a line that looks like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">conv_output_size</code> <code class="o">=</code> <code class="n">input_size</code>  <code class="c1"># Same padding maintains input size</code></pre>
          <p>This helps us to know the size of the output from the convolutional layer to inform the “next” layer in the sequence. </p>
          <p>Why is it the input size, you might wonder. This is the idea of “same padding” that comes about from setting the <code>padding=1</code> parameter. </p>
          <p>If you consider what would happen if you slid a kernel of size 3 across a list of values as in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch11_figure_2_1748549734749133">11-2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch11_figure_3_1748549734749161">11-3</a>, you’d see an odd effect. Because the kernel starts with its left side aligned with the first value and its center at the second value, and because it slides across to the end of the list where the center of the kernel will be aligned to the second-to-last value, the result of the calculations against the values in the list will give us n – 2 answers, where <em>n</em> is the length of the list. But if we pad the list with <code>padding=1</code>, then the kernel sliding across the list will give us <em>n</em> answers, so the output size from the layer will be the <em>same</em> as the input size.</p>
          <p>So now, after ReLUing and flattening the results, we can see the next line:</p>
          <pre data-code-language="python" data-type="programlisting">        <code class="bp">self</code><code class="o">.</code><code class="n">dense1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code> <code class="o">*</code> <code class="n">conv_output_size</code><code class="p">,</code> <code class="mi">28</code><code class="p">)</code>
 </pre>
          <p>The input to this will be a number of values: the size of the list, multiplied by 128, where 128 is the number of kernels. It will then output 28 values, which will be fed into the next linear layer.</p>
          <p>Now, when you get to the forward function, it begins with this line:</p>
          <pre data-code-language="python" data-type="programlisting">        <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code> <code class="o">==</code> <code class="mi">2</code><code class="p">:</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="k">elif</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code> <code class="o">==</code> <code class="mi">3</code> <code class="ow">and</code> <code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">!=</code> <code class="mi">1</code><code class="p">:</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code></pre>
          <p>This looks quite unusual, but it’s necessary for dealing with the convolutions. First of all, consider what the input to a convolution should look like. There’ll be batches of them being fed in, each batch will have 1 dimension, and each item in the batch will have a number of items in it. If we are learning from sequences of 20 items, for example, and if we batch them 32 at a time, then the dimensionality of data being fed into the neural network will be [32, 1, 20].</p>
          <p>But if our dataset isn’t giving us that—and if, for example, there are only two dimensions [32, 20]—then we want to use unsqueeze to slip in another dimension. When we pass a <code>1</code> into it, it will be put at position <code>1</code>, so we’ll get [32, 1, 20] as desired.</p>
          <p>The other case might be if we haven’t put our dimension in correctly and added it on the end, like in [32, 20, 1], so the <code>x = x.transpose(1, 2)</code> will flip these around and make the dimension [32, 1, 20] again.</p>
          <p>Now, these are two specific cases I hardcoded for. You may encounter others, so watch out for issues with your data when feeding it into the neural network. This is likely a place where you can fix them.</p>
          <p>The rest of the forward pass is pretty straightforward; it’s just passing the data through the different layers.</p>
          <p>The loss function and optimizer are going to be pretty straightforward, too, using a mean-squared-error loss and an Adam optimizer:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">)</code></pre>
          <p>Training with this will give you a model as before, and to get predictions from the model, you can just use the loader in the same way as you did for training the model. So, for example, you can do this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Create DataLoaders</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> 
                          <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">val_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">val_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
          <p>And here’s a helper function that can predict an entire series, batch by batch:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">loader</code><code class="p">):</code>
    <code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"cuda"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s2">"cpu"</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="p">[]</code>
 
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="k">for</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">loader</code><code class="p">:</code>
            <code class="n">inputs</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
            <code class="n">batch_predictions</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
            <code class="n">predictions</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">batch_predictions</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">())</code>
 
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">predictions</code><code class="p">)</code>
 </pre>
          <p>You can then get the full set of predictions like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Make predictions</code>
<code class="n">train_predictions</code> <code class="o">=</code> <code class="n">predict</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">)</code>
<code class="n">val_predictions</code> <code class="o">=</code> <code class="n">predict</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">val_loader</code><code class="p">)</code></pre>
          <p class="pagebreak-before less_space">Similarly, if you want to plot them, you could extend on this a little to pass in a loader and get back arrays of the predictions and the targets as well as an analytic, such as the MAE:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">evaluate_predictions</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">loader</code><code class="p">):</code>
    <code class="sd">"""Generate predictions and calculate metrics"""</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"cuda"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s2">"cpu"</code><code class="p">)</code>
 
    <code class="n">all_predictions</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">all_targets</code> <code class="o">=</code> <code class="p">[]</code>
 
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="k">for</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">targets</code> <code class="ow">in</code> <code class="n">loader</code><code class="p">:</code>
            <code class="n">inputs</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
            <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
            <code class="n">all_predictions</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">outputs</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">())</code>
            <code class="n">all_targets</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">targets</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">())</code>
 
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">all_predictions</code><code class="p">)</code>
    <code class="n">targets</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">all_targets</code><code class="p">)</code>
 
 
    <code class="c1"># Calculate metrics</code>
    <code class="n">mae</code> <code class="o">=</code> <code class="n">mean_absolute_error</code><code class="p">(</code><code class="n">targets</code><code class="p">,</code> <code class="n">predictions</code><code class="p">)</code>
 
    <code class="k">return</code> <code class="n">predictions</code><code class="p">,</code> <code class="n">targets</code><code class="p">,</code> <code class="n">mae</code></pre>
          <p>And then you’d call this to get back the multiple responses like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate predictions</code>
<code class="n">val_predictions</code><code class="p">,</code> <code class="n">val_targets</code><code class="p">,</code> <code class="n">val_mae</code> 
             <code class="o">=</code> <code class="n">evaluate_predictions</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">val_loader</code><code class="p">)</code></pre>
          <p>This is now nice and easy to plot:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">plot_predictions</code><code class="p">(</code><code class="n">val_pred</code><code class="p">,</code> <code class="n">val_true</code><code class="p">):</code>
    <code class="sd">"""Plot the predictions against actual values"""</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
    <code class="c1"># Plot validation data</code>
    <code class="n">offset</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">val_true</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">offset</code><code class="p">,</code> <code class="n">offset</code> <code class="o">+</code> <code class="nb">len</code><code class="p">(</code><code class="n">val_true</code><code class="p">)),</code> 
                   <code class="n">val_true</code><code class="p">,</code> <code class="s1">'b-'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Validation Actual'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">offset</code><code class="p">,</code> <code class="n">offset</code> <code class="o">+</code> <code class="nb">len</code><code class="p">(</code><code class="n">val_pred</code><code class="p">)),</code> 
                   <code class="n">val_pred</code><code class="p">,</code> <code class="s1">'r-'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Validation Predicted'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Time Series Prediction vs Actual'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Time Step'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Value'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">grid</code><code class="p">(</code><code class="kc">True</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
          <p>A plot of the results against the series is in <a data-type="xref" href="#ch11_figure_4_1748549734749177">Figure 11-4</a>.</p>
          <p>The MAE in this case is 5.33, which is slightly worse than for the previous prediction. This could be because we haven’t tuned the convolutional layer appropriately, or it could be that convolutions simply don’t help. This is the type of experimentation you’ll need to do with your data.</p>
          <p>Do note that this data has a random element in it, so values will change across sessions. If you’re using code from <a data-type="xref" href="ch10.html#ch10_creating_ml_models_to_predict_sequences_1748549713795870">Chapter 10</a> and then running this code separately, you will, of course, have random fluctuations affecting your data and thus your MAE.</p>
          <figure><div class="figure" id="ch11_figure_4_1748549734749177">
          
            <img src="assets/aiml_1104.png"/>
            <h6><span class="label">Figure 11-4. </span>Convolutional neural network with time sequence data prediction versus actual</h6>
          </div></figure>
          <p>But when using convolutions, questions always come up. Why choose the parameters that we chose? Why 128 filters? Why size 3 × 1? The good news is that you can experiment with these things easily to explore different results.<a contenteditable="false" data-primary="" data-startref="ch11cod" data-type="indexterm" id="id1471"/><a contenteditable="false" data-primary="" data-startref="ch11cod2" data-type="indexterm" id="id1472"/></p>
        </div></section>
        <section data-pdf-bookmark="Experimenting with the Conv1D Hyperparameters" data-type="sect2"><div class="sect2" id="ch11_experimenting_with_the_conv1d_hyperparameters_1748549734762691">
          <h2>Experimenting with the Conv1D Hyperparameters</h2>
          <p>In the previous section, you saw<a contenteditable="false" data-primary="time series data" data-secondary="convolutions for" data-tertiary="1D convolutional layer" data-type="indexterm" id="ch111d"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="time series data and sequences" data-tertiary="1D convolutional layer" data-type="indexterm" id="ch111d2"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="time series data and sequences" data-tertiary="neural architecture search" data-type="indexterm" id="ch111d3"/><a contenteditable="false" data-primary="time series data" data-secondary="convolutions for" data-tertiary="neural architecture search" data-type="indexterm" id="ch111d4"/><a contenteditable="false" data-primary="neural architecture search" data-type="indexterm" id="ch111d5"/><a contenteditable="false" data-primary="neural networks" data-secondary="neural architecture search" data-type="indexterm" id="ch111d6"/> a 1D convolution that was hardcoded with parameters for things like filter number, kernel size, number of strides, etc. When you were training the neural network with it, it appeared that the MAE went up slightly, so you got no benefit from using the <code>Conv1D</code>. This may not always be the case, depending on your data, but it could be because of suboptimal hyperparameters. So, in this section, you’ll see how you can do a neural architecture search to find the best results.</p>
          <p>One of the nice things about how verbose PyTorch is, in particular for defining the neural network and the forward pass, is that it becomes pretty straightforward to change up the parameters that you use. The idea with a <em>neural architecture search</em> is to come up with sets of different parameters to try and then explore the impact that they have on the results by training for a short time and finding those that give the best results.</p>
          <p>So, for example, here, we used a single <code>Conv1D</code> layer. But what if there were more? Similarly, we hardcoded a number of channels and a kernel size, and we also hardcoded the size of the dense layers and the LR for the optimizer. But what if, instead of hardcoding them, we created a set of options like this?</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define the search space</code>
<code class="n">num_conv_layers_options</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code>  <code class="c1"># Reduced for initial testing</code>
<code class="n">conv_channels_options</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">[</code><code class="mi">32</code><code class="p">],</code>
    <code class="p">[</code><code class="mi">64</code><code class="p">],</code>
    <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">],</code>
    <code class="p">[</code><code class="mi">64</code><code class="p">,</code> <code class="mi">32</code><code class="p">],</code>
<code class="p">]</code>
<code class="n">kernel_sizes</code> <code class="o">=</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">5</code><code class="p">]</code>
<code class="n">dense_sizes_options</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">[</code><code class="mi">16</code><code class="p">],</code>
    <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">],</code>
    <code class="p">[</code><code class="mi">64</code><code class="p">,</code> <code class="mi">32</code><code class="p">],</code>
<code class="p">]</code>
<code class="n">learning_rates</code> <code class="o">=</code> <code class="p">[</code><code class="mf">0.001</code><code class="p">,</code> <code class="mf">0.0001</code><code class="p">]</code></pre>
          <p>With 4 options for the <code>conv</code> layers,<a contenteditable="false" data-primary="neural architecture search" data-secondary="search space" data-type="indexterm" id="id1473"/><a contenteditable="false" data-primary="neural networks" data-secondary="neural architecture search" data-tertiary="search space" data-type="indexterm" id="id1474"/> 2 for the kernel sizes, 3 for the dense dimensions, and 2 for the LR, we have 4 × 2 × 3 × 2 options total, which is 48 combinations. This is called the <em>search space</em>.</p>
          <p>Note that in this case, you might think it would be 96 because there are 2 layers options and 4 channels options. But in the code to define the search space, which you’ll see in a moment, I only allowed <code>conv</code> channel options that match the number of layers, so there will just be 4 options in total for the <code>conv</code> layers.</p>
          <p>These options will be loaded into a configurations array, with name-value pairs set up for the parameters, like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate valid configurations</code>
<code class="n">configurations</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">num_conv_layers</code> <code class="ow">in</code> <code class="n">num_conv_layers_options</code><code class="p">:</code>
    <code class="k">for</code> <code class="n">channels</code> <code class="ow">in</code> <code class="n">conv_channels_options</code><code class="p">:</code>
        <code class="c1"># Only use channel configs that match layer count</code>
        <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">channels</code><code class="p">)</code> <code class="o">==</code> <code class="n">num_conv_layers</code><code class="p">:</code>  
            <code class="k">for</code> <code class="n">kernel_size</code> <code class="ow">in</code> <code class="n">kernel_sizes</code><code class="p">:</code>
                <code class="k">for</code> <code class="n">dense_sizes</code> <code class="ow">in</code> <code class="n">dense_sizes_options</code><code class="p">:</code>
                    <code class="k">for</code> <code class="n">lr</code> <code class="ow">in</code> <code class="n">learning_rates</code><code class="p">:</code>
                        <code class="n">configurations</code><code class="o">.</code><code class="n">append</code><code class="p">({</code>
                            <code class="s1">'num_conv_layers'</code><code class="p">:</code> <code class="n">num_conv_layers</code><code class="p">,</code>
                            <code class="s1">'conv_channels'</code><code class="p">:</code> <code class="n">channels</code><code class="p">,</code>
                            <code class="s1">'kernel_size'</code><code class="p">:</code> <code class="n">kernel_size</code><code class="p">,</code>
                            <code class="s1">'dense_sizes'</code><code class="p">:</code> <code class="n">dense_sizes</code><code class="p">,</code>
                            <code class="s1">'learning_rate'</code><code class="p">:</code> <code class="n">lr</code>
                        <code class="p">})</code></pre>
          <p>So now, we can loop through these configurations and set up our <code>CNN1D</code> model by using them like this. Note the use of the <code>config[]</code> array:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="n">config</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">configurations</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Trying configuration </code><code class="si">{</code><code class="n">idx</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">/</code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">configurations</code><code class="p">)</code><code class="si">}</code><code class="s2">:"</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">config</code><code class="p">)</code>
 
    <code class="k">try</code><code class="p">:</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">CNN1D</code><code class="p">(</code>
            <code class="n">input_size</code><code class="o">=</code><code class="n">input_size</code><code class="p">,</code>
            <code class="n">num_conv_layers</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s1">'num_conv_layers'</code><code class="p">],</code>
            <code class="n">conv_channels</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s1">'conv_channels'</code><code class="p">],</code>
            <code class="n">kernel_size</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s1">'kernel_size'</code><code class="p">],</code>
            <code class="n">dense_sizes</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s1">'dense_sizes'</code><code class="p">]</code>
        <code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
 
        <code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
        <code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> 
                                     <code class="n">lr</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s1">'learning_rate'</code><code class="p">])</code></pre>
          <p>It would be really time-consuming to train<a contenteditable="false" data-primary="training" data-secondary="early stopping" data-type="indexterm" id="id1475"/><a contenteditable="false" data-primary="early stopping from training" data-type="indexterm" id="id1476"/> each of the 48 combinations for the full set of epochs (say, 100), so we’ve introduced the idea of <em>early stopping</em>. First, let’s train the model with the parameters we loaded from the configuration and a new parameter: <code>early stopping patience</code>:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">trained_model</code><code class="p">,</code> <code class="n">val_loss</code> <code class="o">=</code> <code class="n">train_model</code><code class="p">(</code>
    <code class="n">model</code><code class="p">,</code> <code class="n">train_loader</code><code class="p">,</code> <code class="n">val_loader</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code>
    <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">,</code> <code class="n">early_stopping_patience</code><code class="o">=</code><code class="mi">10</code>
<code class="p">)</code></pre>
          <p>Then, within the training loop, we can implement an early stopping like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Early stopping check</code>
<code class="k">if</code> <code class="n">val_loss</code> <code class="o">&lt;</code> <code class="n">best_val_loss</code><code class="p">:</code>
    <code class="n">best_val_loss</code> <code class="o">=</code> <code class="n">val_loss</code>
    <code class="n">best_model</code> <code class="o">=</code> <code class="n">deepcopy</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
    <code class="n">patience_counter</code> <code class="o">=</code> <code class="mi">0</code>
<code class="k">else</code><code class="p">:</code>
    <code class="n">patience_counter</code> <code class="o">+=</code> <code class="mi">1</code>
 
<code class="k">if</code> <code class="n">patience_counter</code> <code class="o">&gt;=</code> <code class="n">early_stopping_patience</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Early stopping triggered after </code><code class="si">{</code><code class="n">epoch</code><code class="si">}</code><code class="s1"> epochs'</code><code class="p">)</code>
    <code class="k">break</code></pre>
          <p>This keeps track of the loss for the best model and compares the current model with the best one. If the current model “loses” more times than our patience parameter (in this case, 10), then we’ll throw it out and move to the next one. If it “wins,” then we’ll keep the current model as the best one. </p>
          <p>Starting from this code, you can try to experiment with the hyperparameters for the number of filters, the size of the kernel, and the size of the stride, keeping the other parameters static.</p>
          <p class="pagebreak-before">After some experimentation, I discovered that 2 convolutional layers, with 64 and 32 filters (respectively), a kernel size of 5, two dense layers of 64 and 32, and an LR of <span class="keep-together">.0001</span> gave the best MAE on the validation set, giving me a final result of 4.4439 MAE. </p>
          <p>After training with this, the model had improved accuracy compared with both the naive CNN created earlier <em>and</em> the original DNN, giving the results shown in <a data-type="xref" href="#ch11_figure_5_1748549734749203">Figure 11-5</a>.</p>
          <figure><div class="figure" id="ch11_figure_5_1748549734749203">
            <img src="assets/aiml_1105.png"/>
            <h6><span class="label">Figure 11-5. </span>Optimized CNN time series predictions versus actual</h6>
          </div></figure>
          <p>Further experimentation with the CNN hyperparameters may improve this further.</p>
          <p>Beyond convolutions, the techniques we explored in the chapters on NLP with RNNs, including LSTMs, may be powerful when working with sequence data. By their very nature, RNNs are designed for maintaining context, so previous values can have an effect on later ones. You’ll explore using RNNs for sequence modeling next. But first, let’s move on from a synthetic dataset and start looking at real data. In this case, we’ll consider weather data.<a contenteditable="false" data-primary="" data-startref="ch11conv" data-type="indexterm" id="id1477"/><a contenteditable="false" data-primary="" data-startref="ch11conv2" data-type="indexterm" id="id1478"/><a contenteditable="false" data-primary="" data-startref="ch111d" data-type="indexterm" id="id1479"/><a contenteditable="false" data-primary="" data-startref="ch111d2" data-type="indexterm" id="id1480"/><a contenteditable="false" data-primary="" data-startref="ch111d3" data-type="indexterm" id="id1481"/><a contenteditable="false" data-primary="" data-startref="ch111d4" data-type="indexterm" id="id1482"/><a contenteditable="false" data-primary="" data-startref="ch111d5" data-type="indexterm" id="id1483"/><a contenteditable="false" data-primary="" data-startref="ch111d6" data-type="indexterm" id="id1484"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Using NASA Weather Data" data-type="sect1"><div class="sect1" id="ch11_using_nasa_weather_data_1748549734762759">
        <h1>Using NASA Weather Data</h1>
        <p>One great resource for time series<a contenteditable="false" data-primary="datasets" data-secondary="NASA weather data" data-type="indexterm" id="ch11nasa"/><a contenteditable="false" data-primary="time series data" data-secondary="NASA weather data" data-type="indexterm" id="ch11nasa2"/><a contenteditable="false" data-primary="NASA weather data" data-type="indexterm" id="ch11nasa3"/><a contenteditable="false" data-primary="online resources" data-secondary="NASA weather data" data-type="indexterm" id="id1485"/><a contenteditable="false" data-primary="Goddard Institute for Space Studies (GISS) weather data" data-type="indexterm" id="id1486"/> weather data is the <a href="https://oreil.ly/6IixP">NASA Goddard Institute for Space Studies (GISS) Surface Temperature Analysis</a>. If you follow the <a href="https://oreil.ly/F9Hmw">Station Data link</a>, on the right side of the page, you can pick a weather station to get data from. For example, I chose the Seattle Tacoma (SeaTac) airport and was taken to the page shown in <a data-type="xref" href="#ch11_figure_6_1748549734749227">Figure 11-6</a>.</p>
        <figure><div class="figure" id="ch11_figure_6_1748549734749227">
          <img alt="" src="assets/aiml_1106.png"/>
          <h6><span class="label">Figure 11-6. </span>Surface temperature data from GISS</h6>
        </div></figure>
        <p>You can also see a link to <a contenteditable="false" data-primary="CSV files as text data sources" data-secondary="NASA weather monthly data download" data-type="indexterm" id="id1487"/><a contenteditable="false" data-primary="NASA weather data" data-secondary="CSV download of monthly data" data-type="indexterm" id="id1488"/><a contenteditable="false" data-primary="Goddard Institute for Space Studies (GISS) weather data" data-secondary="CSV download of monthly data" data-type="indexterm" id="id1489"/><a contenteditable="false" data-primary="time series data" data-secondary="NASA weather data" data-tertiary="CSV download of monthly data" data-type="indexterm" id="id1490"/>download monthly data as CSV at the bottom of this page. If you select this link, a file called <em>station.csv</em> will be downloaded to your device, and if you open it, you’ll see that it’s a grid of data with a year in each row and a month in each column (see <a data-type="xref" href="#ch11_figure_7_1748549734749249">Figure 11-7</a>).</p>
        <figure><div class="figure" id="ch11_figure_7_1748549734749249">
          <img alt="" src="assets/aiml_1107.png"/>
          <h6><span class="label">Figure 11-7. </span>Exploring the data</h6>
        </div></figure>
        <p>As this is CSV data, it’s pretty easy to process in Python, but as with any dataset, do note the format. When reading CSV, you tend to read it line by line, and often, each line has one data point that you’re interested in. In this case, there are at least 12 data points of interest per line, so you’ll have to consider this when reading the data.</p>
        <section data-pdf-bookmark="Reading GISS Data in Python" data-type="sect2"><div class="sect2" id="ch11_reading_giss_data_in_python_1748549734762822">
          <h2>Reading GISS Data in Python</h2>
          <p>The code to read the GISS data is shown here:<a contenteditable="false" data-primary="time series data" data-secondary="NASA weather data" data-tertiary="reading the data in Python" data-type="indexterm" id="ch11read"/><a contenteditable="false" data-primary="datasets" data-secondary="NASA weather data" data-tertiary="reading the data in Python" data-type="indexterm" id="ch11read2"/><a contenteditable="false" data-primary="NASA weather data" data-secondary="reading the data in Python" data-type="indexterm" id="ch11read3"/><a contenteditable="false" data-primary="Goddard Institute for Space Studies (GISS) weather data" data-secondary="reading the data in Python" data-type="indexterm" id="ch11read4"/></p>
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">get_data</code><code class="p">():</code>
    <code class="n">data_file</code> <code class="o">=</code> <code class="s2">"station.csv"</code>
    <code class="n">f</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">data_file</code><code class="p">)</code>
    <code class="n">data</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
    <code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="n">lines</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">)</code>
    <code class="n">header</code> <code class="o">=</code> <code class="n">lines</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">','</code><code class="p">)</code>
    <code class="n">lines</code> <code class="o">=</code> <code class="n">lines</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code>
    <code class="n">temperatures</code><code class="o">=</code><code class="p">[]</code>
    <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">lines</code><code class="p">:</code>
        <code class="k">if</code> <code class="n">line</code><code class="p">:</code>
            <code class="n">linedata</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">','</code><code class="p">)</code>
            <code class="n">linedata</code> <code class="o">=</code> <code class="n">linedata</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="mi">13</code><code class="p">]</code>
            <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">linedata</code><code class="p">:</code>
                <code class="k">if</code> <code class="n">item</code><code class="p">:</code>
                    <code class="n">temperatures</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">float</code><code class="p">(</code><code class="n">item</code><code class="p">))</code>
 
    <code class="n">series</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">temperatures</code><code class="p">)</code>
    <code class="n">time</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">temperatures</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"float32"</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">time</code><code class="p">,</code> <code class="n">series</code></pre>
          <p>This will open the file at the indicated path (yours will, of course, differ) and read in the entire file as a set of lines, where the line split is the new line character (<code>\n</code>). It will then loop through each line, ignoring the first line, and split them on the comma character into a new array called <code>linedata</code>. The items from 1 through 13 in this array will indicate the values for the months January through February as strings, and these values will then be converted into floats and added to the array called <code>temperatures</code>. Once it’s completed, it will be turned into a NumPy array called <code>series</code>, and another NumPy array called <code>time</code> will be created that’s the same size as <code>series</code>. As it is created using <code>np.arange</code>, the first element will be 1, the second will be 2, etc. Thus, this function will return <code>time</code> in steps from 1 to the number of data points and will return <code>series</code> as the data for that time.</p>
          <p>I have noticed that often, there will be “unfilled” data in some of the columns, and these are represented by the value 999.9. This will, of course, skew any predictive results you want to create. But fortunately, 999.9 values are usually at the <em>end</em> of the dataset, so they can easily be cropped. Here’s a helper function to normalize the series while cropping out the 999.9 values:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
 
<code class="k">def</code> <code class="nf">normalize_series</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">missing_value</code><code class="o">=</code><code class="mf">999.9</code><code class="p">):</code>
    <code class="c1"># Convert to numpy array if not already</code>
    <code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">float64</code><code class="p">)</code>
 
    <code class="c1"># Create mask for valid values (not NaN and not missing_value)</code>
    <code class="n">valid_mask</code> <code class="o">=</code> <code class="p">(</code><code class="n">data</code> <code class="o">!=</code> <code class="n">missing_value</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="o">~</code><code class="n">np</code><code class="o">.</code><code class="n">isnan</code><code class="p">(</code><code class="n">data</code><code class="p">))</code>
 
    <code class="c1"># Keep only valid values</code>
    <code class="n">clean_data</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="n">valid_mask</code><code class="p">]</code>
 
    <code class="c1"># Normalize using only valid values</code>
    <code class="n">mean</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">clean_data</code><code class="p">)</code>
    <code class="n">std</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">clean_data</code><code class="p">)</code>
    <code class="n">normalized</code> <code class="o">=</code> <code class="p">(</code><code class="n">clean_data</code> <code class="o">-</code> <code class="n">mean</code><code class="p">)</code> <code class="o">/</code> <code class="n">std</code>
 
    <code class="k">return</code> <code class="n">normalized</code>
 
<code class="n">time</code><code class="p">,</code> <code class="n">series</code> <code class="o">=</code> <code class="n">get_data</code><code class="p">()</code>
<code class="n">series_normalized</code> <code class="o">=</code> <code class="n">normalize_series</code><code class="p">(</code><code class="n">series</code><code class="p">)</code></pre>
          <p>You can now load this into a <code>torch.tensor</code> and turn it into a set of sliding windows with a target value as before. We discussed the helper function in <a data-type="xref" href="ch10.html#ch10_creating_ml_models_to_predict_sequences_1748549713795870">Chapter 10</a>:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">series_tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">series_normalized</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
<code class="n">window_size</code> <code class="o">=</code> <code class="mi">48</code>
<code class="n">features</code><code class="p">,</code> <code class="n">targets</code> <code class="o">=</code> <code class="n">create_sliding_windows_with_target</code><code class="p">(</code><code class="n">series_tensor</code><code class="p">,</code> 
                    <code class="n">window_size</code><code class="o">=</code><code class="n">window_size</code><code class="p">,</code> <code class="n">shift</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
          <p>Once we have that, we can turn it into a <code>TensorDataset</code> and split it into subsets for training and validation:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">split_location</code> <code class="o">=</code> <code class="mi">800</code>
<code class="c1"># Create the full dataset</code>
<code class="n">full_dataset</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">targets</code><code class="p">)</code>
 
<code class="c1"># Calculate split indices</code>
<code class="c1"># Note: Since we're using windows, we need to account for the overlap</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="mi">800</code> <code class="o">-</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code>  <code class="c1"># Adjust for window overlap</code>
<code class="n">total_windows</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">full_dataset</code><code class="p">)</code>
<code class="n">train_indices</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">train_size</code><code class="p">))</code>
<code class="n">val_indices</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">train_size</code><code class="p">,</code> <code class="n">total_windows</code><code class="p">))</code>
 
<code class="c1"># Create training and validation datasets using Subset</code>
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">Subset</code><code class="p">(</code><code class="n">full_dataset</code><code class="p">,</code> <code class="n">train_indices</code><code class="p">)</code>
<code class="n">val_dataset</code> <code class="o">=</code> <code class="n">Subset</code><code class="p">(</code><code class="n">full_dataset</code><code class="p">,</code> <code class="n">val_indices</code><code class="p">)</code></pre>
          <p>Now that we have the splits as datasets, we can create loaders for them that the neural network will use:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> 
                          <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">val_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">val_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
          <p>And now, we’re ready to train with this data. We can inspect the split by charting the data, and we can see this with the training/validation split in <a data-type="xref" href="#ch11_figure_8_1748549734749270">Figure 11-8</a>.</p>
          <figure><div class="figure" id="ch11_figure_8_1748549734749270">
            <img src="assets/aiml_1108.png"/>
            <h6><span class="label">Figure 11-8. </span>The time series train/validation split</h6>
          </div></figure>
          <p>In the next section, we’ll explore creating a simple RNN-based neural network to see if we can predict the next values in the sequence.<a contenteditable="false" data-primary="" data-startref="ch11read" data-type="indexterm" id="id1491"/><a contenteditable="false" data-primary="" data-startref="ch11read2" data-type="indexterm" id="id1492"/><a contenteditable="false" data-primary="" data-startref="ch11read3" data-type="indexterm" id="id1493"/><a contenteditable="false" data-primary="" data-startref="ch11read4" data-type="indexterm" id="id1494"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Using RNNs for Sequence Modeling" data-type="sect1"><div class="sect1" id="ch11_using_rnns_for_sequence_modeling_1748549734762890">
        <h1>Using RNNs for Sequence Modeling</h1>
        <p>Now that you have the data<a contenteditable="false" data-primary="time series data" data-secondary="RNNs for sequence modeling" data-tertiary="NASA weather data" data-type="indexterm" id="ch11rnn"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="sequence modeling" data-tertiary="NASA weather data" data-type="indexterm" id="ch11rnn2"/><a contenteditable="false" data-primary="NASA weather data" data-secondary="RNNs for sequence modeling" data-type="indexterm" id="ch11rnn3"/><a contenteditable="false" data-primary="Goddard Institute for Space Studies (GISS) weather data" data-secondary="RNNs for sequence modeling" data-type="indexterm" id="ch11rnn4"/><a contenteditable="false" data-primary="time series data" data-secondary="NASA weather data" data-tertiary="RNNs for sequence modeling" data-type="indexterm" id="ch11rnn5"/> from the NASA CSV in a windowed dataset, it’s relatively easy to create a model to train a predictor for it. (However, it’s a bit more difficult to train a <em>good</em> one!) Let’s start with a simple, naive model using RNNs. Here’s the code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">SimpleRNNModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> 
                       <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleRNNModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">input_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  <code class="c1"># Add dropout to RNN</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  <code class="c1"># Add dropout to RNN</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code>  <code class="c1"># Additional dropout layer</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">out1</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">rnn1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">out2</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">rnn2</code><code class="p">(</code><code class="n">out1</code><code class="p">)</code>
        <code class="n">last_out</code> <code class="o">=</code> <code class="n">out2</code><code class="p">[:,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="p">:]</code>
        <code class="n">last_out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">last_out</code><code class="p">)</code>  <code class="c1"># Add dropout before final layer</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear</code><code class="p">(</code><code class="n">last_out</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">output</code></pre>
        <p>In this case, as you can see, we use a basic RNN. RNNs are a class of neural networks that are powerful for exploring sequence models, and you first saw them in <a data-type="xref" href="ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648">Chapter 7</a>, when you were looking at NLP. I won’t go into detail on how they work here, but if you’re interested and you skipped that chapter, take a look back at it now. Notably, an RNN has an internal loop that iterates over the time steps of a sequence while maintaining an internal state of the time steps it has seen so far. </p>
        <p>While training, you can use a loss function and optimizer like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">)</code></pre>
        <p>The full code is available<a contenteditable="false" data-primary="GitHub" data-secondary="RNNs for sequence modeling code" data-type="indexterm" id="id1495"/><a contenteditable="false" data-primary="online resources" data-secondary="code for book" data-tertiary="RNNs for sequence modeling code" data-type="indexterm" id="id1496"/> in this book’s <a href="https://oreil.ly/pytorch_ch11">GitHub repository</a>. Even one hundred epochs of training is enough to get an idea of how the model can predict values. <a data-type="xref" href="#ch11_figure_9_1748549734749285">Figure 11-9</a> shows the results.</p>
        <figure><div class="figure" id="ch11_figure_9_1748549734749285">
          <img src="assets/aiml_1109.png"/>
          <h6><span class="label">Figure 11-9. </span>Results of the SimpleRNN time series prediction versus actual</h6>
        </div></figure>
        <p>As you can see, the results were pretty good. It may be a little off in the peaks and when the pattern changes unexpectedly (like at time steps 160–170), but on the whole, it’s not bad. Now, let’s see what happens if we train it for 1,500 epochs (see <a data-type="xref" href="#ch11_figure_10_1748549734749297">Figure 11-10</a>).</p>
        <figure><div class="figure" id="ch11_figure_10_1748549734749297">
          <img src="assets/aiml_1110.png"/>
          <h6><span class="label">Figure 11-10. </span>Time series prediction versus actual for RNN trained over 1,500 epochs</h6>
        </div></figure>
        <p>There’s not much of a difference, except that some of the peaks are smoothed out. If you look at the history of loss on both the validation set and the training set, it looks like <a data-type="xref" href="#ch11_figure_11_1748549734749310">Figure 11-11</a>.</p>
        <figure><div class="figure" id="ch11_figure_11_1748549734749310">
          <img src="assets/aiml_1111.png"/>
          <h6><span class="label">Figure 11-11. </span>Training and validation model loss over time for the SimpleRNN</h6>
        </div></figure>
        <p>As you can see, there’s a healthy match between the training loss and the validation loss, but as the epochs increase, the model begins to overfit on the training set. Perhaps a better number of epochs would be around five hundred.</p>
        <p>One reason for this could be the fact that the data, being monthly weather data, is highly seasonal. Another is that there is a very large training set and a relatively small validation set. </p>
        <p>Next, we’ll explore using a larger climate dataset.<a contenteditable="false" data-primary="" data-startref="ch11nasa" data-type="indexterm" id="id1497"/><a contenteditable="false" data-primary="" data-startref="ch11nasa2" data-type="indexterm" id="id1498"/><a contenteditable="false" data-primary="" data-startref="ch11nasa3" data-type="indexterm" id="id1499"/><a contenteditable="false" data-primary="" data-startref="ch11rnn" data-type="indexterm" id="id1500"/><a contenteditable="false" data-primary="" data-startref="ch11rnn2" data-type="indexterm" id="id1501"/><a contenteditable="false" data-primary="" data-startref="ch11rnn3" data-type="indexterm" id="id1502"/><a contenteditable="false" data-primary="" data-startref="ch11rnn4" data-type="indexterm" id="id1503"/><a contenteditable="false" data-primary="" data-startref="ch11rnn5" data-type="indexterm" id="id1504"/></p>
        <section data-pdf-bookmark="Exploring a Larger Dataset" data-type="sect2"><div class="sect2" id="ch11_exploring_a_larger_dataset_1748549734762953">
          <h2>Exploring a Larger Dataset</h2>
          <p>The <a href="https://oreil.ly/J8CP0">KNMI Climate Explorer</a> allows you to explore<a contenteditable="false" data-primary="datasets" data-secondary="KNMI Climate Explorer" data-type="indexterm" id="ch11knmi"/><a contenteditable="false" data-primary="KNMI Climate Explorer dataset" data-type="indexterm" id="ch11knmi2"/><a contenteditable="false" data-primary="time series data" data-secondary="KNMI Climate Explorer dataset" data-type="indexterm" id="ch11knmi3"/><a contenteditable="false" data-primary="time series data" data-secondary="RNNs for sequence modeling" data-tertiary="KNMI Climate Explorer dataset" data-type="indexterm" id="ch11knmi4"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="sequence modeling" data-tertiary="KNMI Climate Explorer dataset" data-type="indexterm" id="ch11knmi5"/> granular climate data from many locations around the world. <a href="https://oreil.ly/Ci9DI"> I downloaded a dataset</a> consisting of daily temperature readings from the center of England from the years 1772 to 2020. This data is structured differently from the GISS data, with the date as a string, followed by a number of spaces, followed by the reading. Go back to <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a> to check the details on handling and managing large datasets.</p>
          <p class="pagebreak-before less_space">I’ve prepared the data, stripping the headers and removing the extraneous spaces, so that there’s only one space between the date and the reading. That way, it’s easy to read with code like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="k">def</code> <code class="nf">get_data</code><code class="p">():</code>
    <code class="n">data_file</code> <code class="o">=</code> <code class="s2">"tdaily_cet.dat.txt"</code>
    <code class="n">f</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">data_file</code><code class="p">)</code>
    <code class="n">data</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
    <code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="n">lines</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">)</code>
    <code class="n">temperatures</code><code class="o">=</code><code class="p">[]</code>
    <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">lines</code><code class="p">:</code>
        <code class="k">if</code> <code class="n">line</code><code class="p">:</code>
            <code class="n">linedata</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">' '</code><code class="p">)</code>
            <code class="n">temperatures</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">float</code><code class="p">(</code><code class="n">linedata</code><code class="p">[</code><code class="mi">1</code><code class="p">]))</code>
 
    <code class="n">series</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">temperatures</code><code class="p">)</code>
    <code class="n">time</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">temperatures</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"float32"</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">time</code><code class="p">,</code> <code class="n">series</code>
 </pre>
          <p>This dataset has 91,502 data points in it, so before training your model, be sure <span class="keep-together">to split it</span> appropriately. I used a split time of 80,000, leaving 10,663 records for <span class="keep-together">validation:</span></p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">split_location</code> <code class="o">=</code> <code class="mi">80000</code>
 
<code class="n">features</code> <code class="o">=</code> <code class="n">features</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="c1"># Create the full dataset</code>
<code class="n">full_dataset</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">targets</code><code class="p">)</code>
 
<code class="c1"># Calculate split indices</code>
<code class="c1"># Note: Since we're using windows, we need to account for the overlap</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="n">split_location</code> <code class="o">-</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code>  <code class="c1"># Adjust for window overlap</code>
<code class="n">total_windows</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">full_dataset</code><code class="p">)</code>
<code class="n">train_indices</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">train_size</code><code class="p">))</code>
<code class="n">val_indices</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">train_size</code><code class="p">,</code> <code class="n">total_windows</code><code class="p">))</code>
 
<code class="c1"># Create training and validation datasets using Subset</code>
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">Subset</code><code class="p">(</code><code class="n">full_dataset</code><code class="p">,</code> <code class="n">train_indices</code><code class="p">)</code>
<code class="n">val_dataset</code> <code class="o">=</code> <code class="n">Subset</code><code class="p">(</code><code class="n">full_dataset</code><code class="p">,</code> <code class="n">val_indices</code><code class="p">)</code>
 </pre>
          <p>Everything else can remain the same. As you can see in <a data-type="xref" href="#ch11_figure_12_1748549734749322">Figure 11-12</a>, after training for one hundred epochs, the plot of the predictions against the validation set looks pretty good.</p>
          <figure><div class="figure" id="ch11_figure_12_1748549734749322">
            <img src="assets/aiml_1112.png"/>
            <h6><span class="label">Figure 11-12. </span>Plot of predictions against real data</h6>
          </div></figure>
          <p>There’s a lot of data here, so let’s zoom in to the last hundred days’ worth (see <a data-type="xref" href="#ch11_figure_13_1748549734749335">Figure 11-13</a>).</p>
          <figure><div class="figure" id="ch11_figure_13_1748549734749335">
            <img src="assets/aiml_1113.png"/>
            <h6><span class="label">Figure 11-13. </span>Results of time series prediction versus actual for one hundred days’ worth of data</h6>
          </div></figure>
          <p>While the chart generally follows the curve of the data and is getting the trends roughly correct, it is pretty far off, particularly at the extreme ends, so there’s room for improvement.</p>
          <p>It’s also important to remember that we normalized the data, so while our loss and MAE may look low, that’s because they are based on the loss and MAE of normalized values that have a much lower variance than the real ones. As <a data-type="xref" href="#ch11_figure_14_1748549734749347">Figure 11-14</a> shows, a tiny amount of loss might lead you into having a false sense of security.</p>
          <figure><div class="figure" id="ch11_figure_14_1748549734749347">
            <img src="assets/aiml_1114.png"/>
            <h6><span class="label">Figure 11-14. </span>Training and validation model loss over time for large dataset</h6>
          </div></figure>
          <p>To denormalize the data, you can do the inverse of normalization: first, multiply by the standard deviation, and then add back the mean. At that point, if you wish, you can calculate the real MAE for the prediction set as you’ve done previously.<a contenteditable="false" data-primary="" data-startref="ch11knmi" data-type="indexterm" id="id1505"/><a contenteditable="false" data-primary="" data-startref="ch11knmi2" data-type="indexterm" id="id1506"/><a contenteditable="false" data-primary="" data-startref="ch11knmi3" data-type="indexterm" id="id1507"/><a contenteditable="false" data-primary="" data-startref="ch11knmi4" data-type="indexterm" id="id1508"/><a contenteditable="false" data-primary="" data-startref="ch11knmi5" data-type="indexterm" id="id1509"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Using Other Recurrent Methods" data-type="sect1"><div class="sect1" id="ch11_using_other_recurrent_methods_1748549734763014">
        <h1>Using Other Recurrent Methods</h1>
        <p>In addition to the <code>RNN</code>, PyTorch<a contenteditable="false" data-primary="gated recurrent units (GRUs)" data-type="indexterm" id="id1510"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="time series data" data-type="indexterm" id="id1511"/><a contenteditable="false" data-primary="time series data" data-secondary="gated recurrent units" data-type="indexterm" id="id1512"/><a contenteditable="false" data-primary="time series data" data-secondary="long short-term memory layers" data-type="indexterm" id="id1513"/> has other recurrent layer types, such as gated recurrent units (GRUs) and long short-term memory layers (LSTMs), which we discussed in <a data-type="xref" href="ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648">Chapter 7</a>. It is relatively simple to just drop in these RNN types if you want to experiment.</p>
        <p>So, for example, if you consider the simple, naive RNN that you created earlier, replacing it with a GRU becomes as easy as using <code>nn.GRU</code>:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">SimpleRNNModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> 
                       <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleRNNModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">input_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  
 
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  
 
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code>  
        <code class="bp">self</code><code class="o">.</code><code class="n">linear</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>
 </pre>
        <p>With an LSTM, it’s similar:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># LSTM Optional Architecture</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
 
<code class="k">class</code> <code class="nc">SimpleLSTMModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> 
                       <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleLSTMModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">lstm1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">input_size</code><code class="p">,</code>
                            <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                            <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                            <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  <code class="c1"># Add dropout to LSTM</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">lstm2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                            <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                            <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                            <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  <code class="c1"># Add dropout to LSTM</code>
 
        <code class="c1"># Add more layers before final output</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">out1</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>  <code class="c1"># LSTM returns (output, (h_n, c_n))</code>
        <code class="n">out2</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm2</code><code class="p">(</code><code class="n">out1</code><code class="p">)</code> <code class="c1"># We ignore both hidden and cell states with _</code>
        <code class="n">last_out</code> <code class="o">=</code> <code class="n">out2</code><code class="p">[:,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="p">:]</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear</code><code class="p">(</code><code class="n">last_out</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">output</code>
 </pre>
        <p>It’s worth experimenting with these layer types as well as with different hyperparameters, loss functions, and optimizers. There’s no one-size-fits-all solution, so what works best for you in any given situation will depend on your data and your requirements for prediction with that data.</p>
      </div></section>
      <section data-pdf-bookmark="Using Dropout" data-type="sect1"><div class="sect1" id="ch11_using_dropout_1748549734763078">
        <h1>Using Dropout</h1>
        <p>If you encounter overfitting<a contenteditable="false" data-primary="time series data" data-secondary="dropout against overfitting" data-type="indexterm" id="ch11drop"/><a contenteditable="false" data-primary="overfitting" data-secondary="dropout helping overcome" data-tertiary="time series data" data-type="indexterm" id="ch11drop2"/><a contenteditable="false" data-primary="dropout regularization" data-secondary="overfitting in time series" data-type="indexterm" id="ch11drop3"/><a contenteditable="false" data-primary="overfitting" data-secondary="dropout helping overcome" data-tertiary="recurrent dropout for RNNs" data-type="indexterm" id="ch11drop4"/><a contenteditable="false" data-primary="dropout regularization" data-secondary="recurrent dropout for RNNs" data-type="indexterm" id="ch11drop5"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="recurrent dropout for" data-type="indexterm" id="ch11drop6"/><a contenteditable="false" data-primary="recurrent dropout" data-type="indexterm" id="ch11drop7"/> in your models, where the MAE or loss for the training data is much better than with the validation data, you can use dropout. As discussed in earlier chapters, with dropout, neighboring neurons are randomly dropped out (ignored) during training to avoid a familiarity bias. When you’re using RNNs, there’s also a <em>recurrent dropout</em> parameter that you can use.</p>
        <p>What’s the difference? Recall that when using RNNs, you typically have an input value and the neuron calculates an output value and a value that gets passed to the next time step. Dropout will randomly drop out the input values, and recurrent dropout will randomly drop out the recurrent values that get passed to the next step.</p>
        <p>For example, consider the basic RNN architecture shown in <a data-type="xref" href="#ch11_figure_15_1748549734749377">Figure 11-15</a>.</p>
        <figure><div class="figure" id="ch11_figure_15_1748549734749377">
          <img alt="" src="assets/aiml_1115.png"/>
          <h6><span class="label">Figure 11-15. </span>A recurrent neural network</h6>
        </div></figure>
        <p>Here, you can see the inputs into the layers at different time steps (<em>x</em>). The current time is <em>t</em>, and the steps shown are <em>t</em> – 2 through <em>t</em> + 1. The relevant outputs at the same time steps (<em>y</em>) are also shown, and the recurrent values passed between time steps are indicated by the dotted lines and labeled as <em>r</em>.</p>
        <p>Using <em>dropout</em> will randomly drop out the <em>x</em> inputs, while using <em>recurrent dropout</em> will randomly drop out the <em>r</em> recurrent values.</p>
        <p>You can learn more about <a contenteditable="false" data-primary="recurrent dropout" data-secondary="more information online" data-type="indexterm" id="id1514"/><a contenteditable="false" data-primary="online resources" data-secondary="recurrent dropout information" data-type="indexterm" id="id1515"/><a contenteditable="false" data-primary="dropout regularization" data-secondary="recurrent dropout for RNNs" data-tertiary="more information online" data-type="indexterm" id="id1516"/><a contenteditable="false" data-primary="“A Theoretically Grounded Application of Dropout in Recurrent Neural Networks” (Gal and Ghahramani)" data-primary-sortas="Theoretically Grounded Application" data-type="indexterm" id="id1517"/><a contenteditable="false" data-primary="Gal, Yarin" data-type="indexterm" id="id1518"/><a contenteditable="false" data-primary="Ghahramani, Zoubin" data-type="indexterm" id="id1519"/><a contenteditable="false" data-primary="uncertainty in deep learning information online" data-type="indexterm" id="id1520"/><a contenteditable="false" data-primary="deep learning" data-secondary="uncertainty in deep learning information online" data-type="indexterm" id="id1521"/>how recurrent dropout works from a deeper mathematical perspective in the paper <a href="https://oreil.ly/MqqRR">“A Theoretically Grounded Application of Dropout in Recurrent Neural Networks” by Yarin Gal and Zoubin Ghahramani</a>. One other thing to consider when using recurrent dropout is discussed by Gal in his research around <a href="https://oreil.ly/3v8IB">uncertainty in deep learning</a>, in which he demonstrates that the same pattern of dropout units should be applied at every time step and that a similar constant dropout mask should also be applied at every time step. </p>
        <p>To add dropout and recurrent dropout, you use the relevant parameters on your layers. For example, adding them to the basic GRU from earlier was as simple as using a parameter in the recurrent layers and adding another layer between the RNNs and the linears:</p>
        <pre class="pagebreak-before" data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">SimpleRNNModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">)</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> 
                       <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code><code class="p">:</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleRNNModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">(</code><code class="p">)</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">input_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                         <strong> <code class="n">dropout</code></strong><strong><code class="o">=</code></strong><strong><code class="n">dropout_rate</code><code class="p">)</code></strong>  
 
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                         <strong> <code class="n">dropout</code></strong><strong><code class="o">=</code></strong><strong><code class="n">dropout_rate</code><code class="p">)</code></strong>  
 
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code>  
        <code class="bp">self</code><code class="o">.</code><code class="n">linear</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code></pre>
        <p>Each parameter takes a value between 0 and 1 that indicates the proportion of values to drop out. For example, a value of 0.1 will drop out 10% of the requisite values.</p>
        <p>Training a model with dropout like this shows a much steeper learning curve, which is still trending downward at 100 epochs. The validation is quite flat, indicating that a larger validation set may be necessary. It’s also quite noisy, and you’ll often see noise like this in the loss when using dropout. It’s an indication that you may want to tweak the amount of dropout as well as the parameters of the loss function and optimizer, such as the LR. You can see this in <a data-type="xref" href="#ch11_figure_16_1748549734749392">Figure 11-16</a>.</p>
        <figure><div class="figure" id="ch11_figure_16_1748549734749392">
          <img src="assets/aiml_1116.png"/>
          <h6><span class="label">Figure 11-16. </span>Training and validation loss over time using a GRU with dropout</h6>
        </div></figure>
        <p>As you’ve seen in this chapter, predicting time sequence data using neural networks is a difficult proposition, but tweaking their hyperparameters can be a powerful way to improve your model and its subsequent predictions.<a contenteditable="false" data-primary="" data-startref="ch11drop" data-type="indexterm" id="id1522"/><a contenteditable="false" data-primary="" data-startref="ch11drop2" data-type="indexterm" id="id1523"/><a contenteditable="false" data-primary="" data-startref="ch11drop3" data-type="indexterm" id="id1524"/><a contenteditable="false" data-primary="" data-startref="ch11drop4" data-type="indexterm" id="id1525"/><a contenteditable="false" data-primary="" data-startref="ch11drop5" data-type="indexterm" id="id1526"/><a contenteditable="false" data-primary="" data-startref="ch11drop6" data-type="indexterm" id="id1527"/><a contenteditable="false" data-primary="" data-startref="ch11drop7" data-type="indexterm" id="id1528"/></p>
      </div></section>
      <section data-pdf-bookmark="Using Bidirectional RNNs" data-type="sect1"><div class="sect1" id="ch11_using_bidirectional_rnns_1748549734763139">
        <h1>Using Bidirectional RNNs</h1>
        <p>Another technique to consider when<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="sequence modeling" data-tertiary="bidirectional RNNs" data-type="indexterm" id="id1529"/><a contenteditable="false" data-primary="time series data" data-secondary="RNNs for sequence modeling" data-tertiary="bidirectional RNNs" data-type="indexterm" id="id1530"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="bidirectional" data-type="indexterm" id="id1531"/> classifying sequences is to use bidirectional training. This may seem counterintuitive at first, as you might wonder how future values could impact past ones. But recall that time series values can contain seasonality, where values repeat over time, and when using a neural network to make predictions, all we’re doing is sophisticated pattern matching. Given that data repeats, a signal for how data can repeat might be found in future values—and when using bidirectional training, we can train the network to try to spot patterns going from time <em>t</em> to time <em>t</em> + <em>x</em>, as well as going from time <em>t</em> + <em>x</em> to time <em>t</em>.</p>
        <p>Fortunately, coding this is simple. For example, consider the GRU from the previous section. To make this bidirectional, you simply add a <code>bidirectional</code> parameter. This will effectively train twice on each step—once with the sequence data in the original order and once with it in reverse order. The results are then merged before proceeding to the next step.</p>
        <p>Here’s an example:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">BidirectionalGRUModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> 
                       <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">BidirectionalGRUModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">gru1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">input_size</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">,</code>
                          <code class="n">bidirectional</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
 
        <code class="bp">self</code><code class="o">.</code><code class="n">gru2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="n">hidden_size</code> <code class="o">*</code> <code class="mi">2</code><code class="p">,</code>
                          <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_size</code><code class="p">,</code>
                          <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                          <code class="n">dropout</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">,</code>
                          <code class="n">bidirectional</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
 
        <code class="c1"># Additional layers</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code> <code class="o">*</code> <code class="mi">2</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">out1</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">out2</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru2</code><code class="p">(</code><code class="n">out1</code><code class="p">)</code>
        <code class="n">last_out</code> <code class="o">=</code> <code class="n">out2</code><code class="p">[:,</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="p">:]</code>
 
        <code class="c1"># Additional processing</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">last_out</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">output</code>
 </pre>
        <p>A plot of the results of training with a bidirectional GRU with dropout on the time series is shown in <a data-type="xref" href="#ch11_figure_17_1748549734749407">Figure 11-17</a>. While the MAE has improved slightly, the bigger impact is that the predicted curve has lost the “lag” compared with the single direction version.</p>
        <p>Additionally, tweaking the training parameters—particularly <code>window_size</code>, to get multiple seasons—can have a pretty big impact.</p>
        <figure><div class="figure" id="ch11_figure_17_1748549734749407">
          <img src="assets/aiml_1117.png"/>
          <h6><span class="label">Figure 11-17. </span>Time series prediction training with a bidirectional GRU</h6>
        </div></figure>
        <p>As you can see, you can experiment with different network architectures and different hyperparameters to improve your overall predictions. The ideal choices are very much dependent on the data, so the skills you’ve learned in this chapter will help you with your specific datasets!</p>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch11_summary_1748549734763196">
        <h1>Summary</h1>
        <p>In this chapter, you explored different network types for building models to predict time series data. You built on the simple DNN from <a data-type="xref" href="ch10.html#ch10_creating_ml_models_to_predict_sequences_1748549713795870">Chapter 10</a>, adding convolutions, and you experimented with recurrent network types such as simple RNNs, GRUs, and LSTMs. You also learned how to tweak hyperparameters and the network architecture to improve your model’s accuracy, and you practiced working with some real-world datasets, including one massive dataset with hundreds of years’ worth of temperature readings. </p>
        <p>Now, you’re ready to get started building networks for a variety of datasets, and you have a good understanding of what you need to know to optimize them!</p>
      </div></section>
    </div></section></body></html>