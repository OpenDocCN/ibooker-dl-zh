- en: Appendix D. Adding Bells and Whistles to the Training Loop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录D. 为训练循环添加附加功能
- en: In the appendix, we enhance the training function for the pretraining and finetuning
    processes covered in chapters 5-7\. This appendix, in particular, covers *learning
    rate warmup*, *cosine decay*, and *gradient clipping* in the first three sections.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录中，我们增强了第5到第7章中涉及的预训练和微调过程的训练功能。本附录特别涵盖了*学习率预热*、*余弦衰减*和*梯度裁剪*这三部分内容。
- en: The final section then incorporates these techniques into the training function
    developed in chapter 5 and pretrains an LLM.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分将这些技术纳入到第5章开发的训练功能中，并预训练一个LLM。
- en: To make the code in this appendix self-contained, we reinitialize the model
    we trained in chapter 5.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使本附录中的代码自成一体，我们重新初始化了在第5章中训练的模型。
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After initializing the model, we also need to initialize the data loaders we
    used in chapter 5\. First, we load the "The Verdict" short story:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化模型之后，我们还需要初始化第5章中使用的数据加载器。首先，我们加载短篇小说《判决》：
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have re-instantiated the model and data loaders we used in chapter
    5, the next section will introduce the enhancements we make to the training function.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经重新实例化了第5章中使用的模型和数据加载器，下一节将介绍我们对训练功能所做的增强。
- en: D.1 Learning rate warmup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.1 学习率预热
- en: The first technique we introduce is *learning rate warmup*. Implementing a learning
    rate warmup can stabilize the training of complex models such as LLMs. This process
    involves gradually increasing the learning rate from a very low initial value
    (`initial_lr`) to a maximum value specified by the user (`peak_lr`). Starting
    the training with smaller weight updates decreases the risk of the model encountering
    large, destabilizing updates during its training phase.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍的第一个技术是*学习率预热*。实现学习率预热可以稳定复杂模型（如LLM）的训练。这个过程涉及将学习率从一个非常低的初始值（`initial_lr`）逐渐增加到用户指定的最大值（`peak_lr`）。从较小的权重更新开始训练，可以降低模型在训练阶段遇到大幅度、不稳定更新的风险。
- en: 'Suppose we plan to train an LLM for 15 epochs, starting with an initial learning
    rate of 0.0001 and increasing it to a maximum learning rate of 0.01\. Furthermore,
    we define 20 warmup steps to increase the initial learning rate from 0.0001 to
    0.01 in the first 20 training steps:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们计划对一个LLM进行15个周期的训练，初始学习率为0.0001，并增加到最大学习率0.01。此外，我们定义20个预热步骤，以在前20个训练步骤中将初始学习率从0.0001提高到0.01：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we implement a simple training loop template to illustrate this warmup
    process:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个简单的训练循环模板来说明这个预热过程：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After running the preceding code, we visualize how the learning rate was changed
    by the training loop above to verify that the learning rate warmup works as intended:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们可视化训练循环如何改变学习率，以验证学习率预热是否按预期工作：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resulting plot is shown in Figure D.1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表如图D.1所示。
- en: Figure D.1 The learning rate warmup increases the learning rate for the first
    20 training steps. After 20 steps, the learning rate reaches the peak of 0.01
    and remains constant for the rest of the training.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图D.1 学习率预热在前20个训练步骤中提高学习率。经过20个步骤后，学习率达到峰值0.01，并在剩余的训练中保持不变。
- en: '![](images/D__image001.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](images/D__image001.png)'
- en: As shown in Figure D.1, the learning rate starts with a low value and increases
    for 20 steps until it reaches the maximum value after 20 steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如图D.1所示，学习率从一个较低的值开始，并在20个步骤内增加，直到在20个步骤后达到最大值。
- en: In the next section, we will modify the learning rate further so that it decreases
    after reaching the maximum learning rate, which further helps improve the model
    training.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将进一步修改学习率，使其在达到最大学习率后降低，这将进一步有助于改善模型训练。
- en: D.2 Cosine decay
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.2 余弦衰减
- en: Another widely adopted technique for training complex deep neural networks and
    LLMs is *cosine decay*. This method modulates the learning rate throughout the
    training epochs, making it follow a cosine curve after the warmup stage.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛采用的用于训练复杂深度神经网络和LLM的技术是*余弦衰减*。这种方法在训练周期中调节学习率，使其在预热阶段后遵循余弦曲线。
- en: In its popular variant, cosine decay reduces (or decays) the learning rate to
    nearly zero, mimicking the trajectory of a half-cosine cycle. The gradual learning
    decrease in cosine decay aims to decelerate the pace at which the model updates
    its weights. This is particularly important as it helps minimize the risk of overshooting
    the loss minima during the training process, which is essential for ensuring the
    stability of the training during its later phases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在其流行的变体中，余弦衰减将学习率减少（或衰减）至接近零，模拟半个余弦周期的轨迹。余弦衰减中学习率的逐渐降低旨在减缓模型更新权重的速度。这一点尤其重要，因为它有助于最小化训练过程中超越损失最小值的风险，这对于确保训练后期的稳定性至关重要。
- en: 'We can modify the training loop template from the previous section, adding
    cosine decay as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改上一节中的训练循环模板，添加余弦衰减，具体如下：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, to verify that the learning rate has changed as intended, we plot the
    learning rate:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了验证学习率是否按预期发生变化，我们绘制学习率图：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The resulting learning rate plot is shown in Figure D.2.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的学习率图如图 D.2 所示。
- en: Figure D.2 The first 20 steps of linear learning rate warmup are followed by
    a cosine decay, which reduces the learning rate in a half-cosine cycle until it
    reaches its minimum point at the end of training.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 D.2 线性学习率预热的前 20 步之后是余弦衰减，这在一个半余弦周期内降低学习率，直到训练结束时达到最低点。
- en: '![](images/D__image003.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](images/D__image003.png)'
- en: As shown in Figure D.2, the learning rate starts with a linear warmup phase,
    which increases for 20 steps until it reaches the maximum value after 20 steps.
    After the 20 steps of linear warmup, cosine decay kicks in, reducing the learning
    rate gradually until it reaches its minimum.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 D.2 所示，学习率以线性预热阶段开始，持续 20 步直至达到最大值。经过 20 步的线性预热后，余弦衰减开始生效，逐渐降低学习率，直到达到最低点。
- en: D.3 Gradient clipping
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.3 梯度裁剪
- en: In this section, we introduce *gradient clipping*, another important technique
    for enhancing stability during LLM training. This method involves setting a threshold
    above which gradients are downscaled to a predetermined maximum magnitude. This
    process ensures that the updates to the model's parameters during backpropagation
    stay within a manageable range.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍 *梯度裁剪*，这是另一种在 LLM 训练中增强稳定性的重要技术。这种方法涉及设置一个阈值，超出该阈值的梯度将被缩放到预定的最大幅度。这个过程确保了在反向传播期间对模型参数的更新保持在一个可控范围内。
- en: For example, applying the `max_norm=1.0` setting within PyTorch's `clip_grad_norm_`
    function ensures that the norm of the gradients does not surpass 1.0\. Here, the
    term "norm" signifies the measure of the gradient vector's length, or magnitude,
    within the model's parameter space, specifically referring to the L2 norm, also
    known as the Euclidean norm.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 PyTorch 的 `clip_grad_norm_` 函数中应用 `max_norm=1.0` 设置确保梯度的范数不超过 1.0。在这里，“范数”指的是梯度向量在模型参数空间中的长度或幅度，具体是指
    L2 范数，也称为欧几里得范数。
- en: 'In mathematical terms, for a vector ***v*** composed of components ***v***
    = [*v*[1], *v*[2], ..., *v*[n]], the L2 norm is described as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，对于由分量组成的向量 ***v***，其表示为 ***v*** = [*v*[1], *v*[2], ..., *v*[n]]，L2 范数的描述为：
- en: '![](images/D__image005.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](images/D__image005.png)'
- en: This calculation method is also applied to matrices.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算方法也适用于矩阵。
- en: 'For instance, consider a gradient matrix given by:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑给定的梯度矩阵：
- en: '![](images/D__image007.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](images/D__image007.png)'
- en: If we aim to clip these gradients to a max_norm of 1, we first compute the L2
    norm of these gradients, which is
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们旨在将这些梯度裁剪到最大范数为 1，我们首先计算这些梯度的 L2 范数，计算公式为
- en: '![](images/D__image009.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](images/D__image009.png)'
- en: Given that |**G**|[2] = 5 exceeds our `max_norm` of 1, we scale down the gradients
    to ensure their norm equals exactly 1\. This is achieved through a scaling factor,
    calculated as `max_norm`/|**G**|[2] = 1/5\. Consequently, the adjusted gradient
    matrix **G'** becomes
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 |**G**|[2] = 5 超过了我们的 `max_norm` 为 1，我们将梯度缩小以确保其范数恰好为 1。通过计算缩放因子 `max_norm`/|**G**|[2]
    = 1/5 来实现。因此，调整后的梯度矩阵 **G'** 为
- en: '![](images/D__image011.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](images/D__image011.png)'
- en: 'To illustrate this gradient clipping process, we would begin by initializing
    a new model and calculating the loss for a training batch, similar to the procedure
    in a standard training loop:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个梯度裁剪过程，我们将首先初始化一个新模型并计算训练批次的损失，类似于标准训练循环中的步骤：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Upon calling the `.backward()` method in the preceding code snippet, PyTorch
    calculates the loss gradients and stores them in a `.grad` attribute for each
    model weight (parameter) tensor.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用前面代码片段中的`.backward()`方法时，PyTorch计算损失梯度并将其存储在每个模型权重（参数）张量的`.grad`属性中。
- en: 'For illustration purposes, we can define the following `find_highest_gradient`
    utility function to identify the highest gradient value by scanning all the `.grad`
    attributes of the model''s weight tensors after calling `.backward()`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们可以定义以下`find_highest_gradient`工具函数，通过扫描模型权重张量的所有`.grad`属性来识别最高的梯度值，在调用`.backward()`之后：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The largest gradient value identified by the preceding code is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的代码识别到的最大的梯度值如下：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s now apply gradient clipping, which can be implemented with one line
    of code, and see how this affects the largest gradient value:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用梯度裁剪，这可以用一行代码实现，并看看这如何影响最大的梯度值：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The largest gradient value after applying the gradient clipping with the max
    norm of 1 is substantially smaller than before:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 应用最大范数为1的梯度裁剪后，最大的梯度值明显小于之前：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the next section, we will put all the concepts covered in this appendix so
    far into action and modify the LLM training function.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将把到目前为止在本附录中涵盖的所有概念付诸实践，并修改LLM训练函数。
- en: D.4 The modified training function
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D.4 修改后的训练函数
- en: 'In this final section of this appendix, we improve the `train_model_simple`
    training function we used in chapter 5 by adding the three concepts we introduced:
    linear warmup, cosine decay, and gradient clipping. Together, these methods help
    stabilize LLM training.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录的最后部分，我们通过添加我们介绍的三个概念：线性预热、余弦衰减和梯度裁剪，改进了第5章中使用的`train_model_simple`训练函数。这些方法一起有助于稳定LLM训练。
- en: 'The code is as follows, with the changes compared to the `train_model_simple`
    annotated:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下，变化与`train_model_simple`进行了注释：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After defining the `train_model` function, we can use it in a similar fashion
    to train the model compared to the `train_model_simple` method in chapter 5:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了`train_model`函数后，我们可以以类似的方式训练模型，与第5章的`train_model_simple`方法相比：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The training will take about 5 minutes to complete on a MacBook Air or similar
    laptop and print the following outputs:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在MacBook Air或类似笔记本电脑上，训练大约需要5分钟完成，并打印以下输出：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Like chapter 5, the model begins to overfit after a few epochs since it is a
    very small dataset, and we iterate over it multiple times. However, we can see
    that the function is working since it minimizes the training set loss.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 和第5章一样，由于数据集非常小，模型在几个epoch后开始过拟合，我们多次迭代。但是，我们可以看到该函数在工作，因为它最小化了训练集损失。
- en: Readers are encouraged to train the model on a larger text dataset and compare
    the results obtained with this more sophisticated training function to the results
    that can be obtained with the `train_model_simple` function used in chapter 5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者在更大的文本数据集上训练模型，并将使用该更复杂训练函数获得的结果与第5章中使用的`train_model_simple`函数获得的结果进行比较。
