- en: Appendix D. Adding Bells and Whistles to the Training Loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the appendix, we enhance the training function for the pretraining and finetuning
    processes covered in chapters 5-7\. This appendix, in particular, covers *learning
    rate warmup*, *cosine decay*, and *gradient clipping* in the first three sections.
  prefs: []
  type: TYPE_NORMAL
- en: The final section then incorporates these techniques into the training function
    developed in chapter 5 and pretrains an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: To make the code in this appendix self-contained, we reinitialize the model
    we trained in chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After initializing the model, we also need to initialize the data loaders we
    used in chapter 5\. First, we load the "The Verdict" short story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have re-instantiated the model and data loaders we used in chapter
    5, the next section will introduce the enhancements we make to the training function.
  prefs: []
  type: TYPE_NORMAL
- en: D.1 Learning rate warmup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first technique we introduce is *learning rate warmup*. Implementing a learning
    rate warmup can stabilize the training of complex models such as LLMs. This process
    involves gradually increasing the learning rate from a very low initial value
    (`initial_lr`) to a maximum value specified by the user (`peak_lr`). Starting
    the training with smaller weight updates decreases the risk of the model encountering
    large, destabilizing updates during its training phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we plan to train an LLM for 15 epochs, starting with an initial learning
    rate of 0.0001 and increasing it to a maximum learning rate of 0.01\. Furthermore,
    we define 20 warmup steps to increase the initial learning rate from 0.0001 to
    0.01 in the first 20 training steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we implement a simple training loop template to illustrate this warmup
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we visualize how the learning rate was changed
    by the training loop above to verify that the learning rate warmup works as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is shown in Figure D.1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure D.1 The learning rate warmup increases the learning rate for the first
    20 training steps. After 20 steps, the learning rate reaches the peak of 0.01
    and remains constant for the rest of the training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/D__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure D.1, the learning rate starts with a low value and increases
    for 20 steps until it reaches the maximum value after 20 steps.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will modify the learning rate further so that it decreases
    after reaching the maximum learning rate, which further helps improve the model
    training.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Cosine decay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another widely adopted technique for training complex deep neural networks and
    LLMs is *cosine decay*. This method modulates the learning rate throughout the
    training epochs, making it follow a cosine curve after the warmup stage.
  prefs: []
  type: TYPE_NORMAL
- en: In its popular variant, cosine decay reduces (or decays) the learning rate to
    nearly zero, mimicking the trajectory of a half-cosine cycle. The gradual learning
    decrease in cosine decay aims to decelerate the pace at which the model updates
    its weights. This is particularly important as it helps minimize the risk of overshooting
    the loss minima during the training process, which is essential for ensuring the
    stability of the training during its later phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can modify the training loop template from the previous section, adding
    cosine decay as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, to verify that the learning rate has changed as intended, we plot the
    learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The resulting learning rate plot is shown in Figure D.2.
  prefs: []
  type: TYPE_NORMAL
- en: Figure D.2 The first 20 steps of linear learning rate warmup are followed by
    a cosine decay, which reduces the learning rate in a half-cosine cycle until it
    reaches its minimum point at the end of training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/D__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in Figure D.2, the learning rate starts with a linear warmup phase,
    which increases for 20 steps until it reaches the maximum value after 20 steps.
    After the 20 steps of linear warmup, cosine decay kicks in, reducing the learning
    rate gradually until it reaches its minimum.
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Gradient clipping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce *gradient clipping*, another important technique
    for enhancing stability during LLM training. This method involves setting a threshold
    above which gradients are downscaled to a predetermined maximum magnitude. This
    process ensures that the updates to the model's parameters during backpropagation
    stay within a manageable range.
  prefs: []
  type: TYPE_NORMAL
- en: For example, applying the `max_norm=1.0` setting within PyTorch's `clip_grad_norm_`
    function ensures that the norm of the gradients does not surpass 1.0\. Here, the
    term "norm" signifies the measure of the gradient vector's length, or magnitude,
    within the model's parameter space, specifically referring to the L2 norm, also
    known as the Euclidean norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, for a vector ***v*** composed of components ***v***
    = [*v*[1], *v*[2], ..., *v*[n]], the L2 norm is described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/D__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: This calculation method is also applied to matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider a gradient matrix given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/D__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: If we aim to clip these gradients to a max_norm of 1, we first compute the L2
    norm of these gradients, which is
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/D__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: Given that |**G**|[2] = 5 exceeds our `max_norm` of 1, we scale down the gradients
    to ensure their norm equals exactly 1\. This is achieved through a scaling factor,
    calculated as `max_norm`/|**G**|[2] = 1/5\. Consequently, the adjusted gradient
    matrix **G'** becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/D__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To illustrate this gradient clipping process, we would begin by initializing
    a new model and calculating the loss for a training batch, similar to the procedure
    in a standard training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Upon calling the `.backward()` method in the preceding code snippet, PyTorch
    calculates the loss gradients and stores them in a `.grad` attribute for each
    model weight (parameter) tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For illustration purposes, we can define the following `find_highest_gradient`
    utility function to identify the highest gradient value by scanning all the `.grad`
    attributes of the model''s weight tensors after calling `.backward()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The largest gradient value identified by the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now apply gradient clipping, which can be implemented with one line
    of code, and see how this affects the largest gradient value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The largest gradient value after applying the gradient clipping with the max
    norm of 1 is substantially smaller than before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will put all the concepts covered in this appendix so
    far into action and modify the LLM training function.
  prefs: []
  type: TYPE_NORMAL
- en: D.4 The modified training function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this final section of this appendix, we improve the `train_model_simple`
    training function we used in chapter 5 by adding the three concepts we introduced:
    linear warmup, cosine decay, and gradient clipping. Together, these methods help
    stabilize LLM training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is as follows, with the changes compared to the `train_model_simple`
    annotated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the `train_model` function, we can use it in a similar fashion
    to train the model compared to the `train_model_simple` method in chapter 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The training will take about 5 minutes to complete on a MacBook Air or similar
    laptop and print the following outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Like chapter 5, the model begins to overfit after a few epochs since it is a
    very small dataset, and we iterate over it multiple times. However, we can see
    that the function is working since it minimizes the training set loss.
  prefs: []
  type: TYPE_NORMAL
- en: Readers are encouraged to train the model on a larger text dataset and compare
    the results obtained with this more sophisticated training function to the results
    that can be obtained with the `train_model_simple` function used in chapter 5.
  prefs: []
  type: TYPE_NORMAL
