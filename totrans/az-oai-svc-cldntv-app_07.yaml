- en: Chapter 6\. Elaborating Generative AI Business Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first five chapters of the book focused on technical aspects related to
    cloud native architectures for generative AI, advanced capabilities with Azure
    OpenAI and other Azure services, and the operationalization of generative AI in
    the enterprise, including topics such as LLMOps and responsible AI. In [Chapter 3](ch03.html#implementing_cloud_native_generative_ai_with_azure),
    we even explored detailed technical approaches that leverage different Azure resources,
    with recommendations depending on the project scope and type of company data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main motivations for companies to adopt Azure OpenAI, and LLMs in
    general, is to generate significant advantages in the form of savings by automating
    language-based scenarios, or to create differentiation, to offer something better
    than their competitors, with the potential for increased revenue.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the business considerations of building a
    generative AI project with Azure OpenAI Service, including project planning and
    evaluation topics such as cost scenarios and estimations, ROI, roadmapping, etc.
    We will cover the key aspects that will allow any technical implementation to
    become a sustainable and feasible generative AI initiative.
  prefs: []
  type: TYPE_NORMAL
- en: Premortem, or What to Consider Before Implementing a Generative AI Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most interesting managerial techniques is the [premortem](https://oreil.ly/jjw4R).
    A bit less known than the *postmortem* (in which we analyze a project after we
    have finished it), the *premortem* is done before starting the project, by assuming
    that it has already failed, and then trying to identify the factors that caused
    the failure. This is a powerful tool for generative AI and any AI project, given
    the complexity and uncertainty of this type of implementation, because it can
    include any technical or business topic as a way to identify potential risks and
    to create a mitigation plan.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6-1](#table-6-1) compiles a list of typical risks related to a generative
    AI implementation, and the following sections of this chapter will include several
    assets to increase the probability of success of your projects with Azure OpenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Potential risks for generative AI projects
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Risk | What could go wrong |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Contextual** | Regulation and compliance | The use case or potential project
    has to be aligned with upcoming regulations. Even if some of them are “work in
    progress,” examples like social scoring, manipulation, and others are already
    clear examples of forbidden applications.For generative AI deployments, the required
    [transparency obligations](https://oreil.ly/sq8N1) are key and need to be considered
    from the design phase of the project (e.g., by generating traces and changelogs
    directly from the MLOps and LLMOps systems). |'
  prefs: []
  type: TYPE_TB
- en: '| Proper usage | Both [Microsoft](https://oreil.ly/7ojQP) and [OpenAI](https://oreil.ly/eQpg-)
    have clear policies on how to use and not use the models. Failing to comply with
    this (in addition to using the models for use cases that are already forbidden
    by international AI and general regulations) may lead to limitations to service
    access. |'
  prefs: []
  type: TYPE_TB
- en: '| Internal concerns | Due to the new and complex nature of generative AI, multiple
    departments within a company will need to be involved in discussions, and they
    may not all be on the same page. While technical and business departments can
    have a clear idea of how and why to use Azure OpenAI, other departments such as
    legal or compliance may temporarily block implementations to first understand
    questions related to the nature of the service, data privacy, residency, etc.Leveraging
    resources such as the [transparency note](https://oreil.ly/B843c), Microsoft’s
    [EU Data Boundary](https://oreil.ly/Bor0w), the [compliance offering portal](https://oreil.ly/lKBjQ),
    and any [legal and Data Protection Addendum (DPA)-related information](https://oreil.ly/Fqfi9)
    will help to unblock this sort of situation. |'
  prefs: []
  type: TYPE_TB
- en: '| **Business** | Incorrect use case discovery and prioritization | One key
    challenge is to find and prioritize the most feasible and impactful use cases.
    Sometimes prioritization is based on ideas from the executive team, or from technical
    departments.Proper ideation and a clear design thinking process can help to analyze
    all relevant aspects from a list of potential use cases. |'
  prefs: []
  type: TYPE_TB
- en: '| Lack of quantitative usage scenarios | Another challenge for generative AI
    adopters is to imagine how much their new cloud native solution will be used by
    end users.Preliminary scenarios are required to properly size the envisioned solution
    at both the technical and budget levels. This can be achieved by estimating a
    total number of users by the average daily/monthly usage of the platform. |'
  prefs: []
  type: TYPE_TB
- en: '| Unexpected cost | Estimating costs is relatively simple because of the linearity
    of the pricing of generative AI tools, including Azure OpenAI Service.But to accurately
    estimate costs, it is necessary to not only have clear usage scenarios, but also
    to understand the pricing structure and to be able to create estimates based on
    the chosen models, by using the official [Azure calculator](https://oreil.ly/2SQ4C)
    and other tools like [OpenAI’s tokenizer](https://oreil.ly/DDQHG).Also, general
    FinOps best practices such as resource tagging, pricing alerts, and the use of
    Azure Cost Management help monitor costs for your generative AI projects. |'
  prefs: []
  type: TYPE_TB
- en: '| Unclear business case | If the potential use cases are not properly prioritized,
    and there are not clear scenarios and cost estimation, it is difficult to draft
    any business case and ROI expectation.Any generative AI business must include
    an estimation of potential revenue, savings, or differentiation for the adopting
    company. For example, “human hours saved in a call center,” “number of tickets
    being solved automatically,” “item upselling in a recommendation bot,” etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Innovation dilemma | Even if most of the company is willing to adopt generative
    AI, some initiatives may be paused due to the innovation dilemma. This means questions
    such as “Is it too early to implement something like that?” or “Why this and not
    other innovations that require a budget too?” may be raised, and having a solid
    base to justify a new Azure OpenAI project will be key for success. Factors like
    the fast-evolving ecosystem and increasing competition are potential considerations
    for generative AI adoption. |'
  prefs: []
  type: TYPE_TB
- en: '| **Technical** | High complexity | Despite the relatively accessible level
    of technicality for implementations with Azure OpenAI, it is common to see adopting
    companies, integrators, and individuals that are not confident about their level
    of knowledge, especially for complex implementations.The evolution of visual interfaces
    such as Azure OpenAI Studio as a way to test and deploy web apps and bot agents
    equipped with generative AI, as well as for model deployment and other visual
    capabilities such as prompt flow, will help reduce the entry barrier for less
    technical or nonexpert adopters. |'
  prefs: []
  type: TYPE_TB
- en: '| Low performance | Depending on initial expectations, generative AI applications’
    performance can be perceived as imperfect (which is obviously true) because of
    their tendency to produce sporadic errors. If expectations are not realistic,
    the testing phase can lead to disappointment, blocking the way to production.It
    is important to have an initial alignment and clearly defined expectations of
    what “good enough” performance would mean, as well as a proper plan to evaluate
    the solution with subject matter experts (SME) and/or end users. |'
  prefs: []
  type: TYPE_TB
- en: '| Lack of resources | Implementations can be stopped before or during the project
    due to lack of resources such as budget, data for customized models, and available
    people with the right skills.A good way to guarantee feasibility is to properly
    scope the envisioned implementation and plan accordingly based on project priority
    and technical complexity. |'
  prefs: []
  type: TYPE_TB
- en: '| Security concerns | As with any other data or AI systems, generative AI can
    suffer attacks at several levels: perimeter, data sources (those used for grounding),
    and prompts (through prompt injection techniques).Adopting best architecture and
    DevSecOps practices and planning red team activities to simulate scenarios can
    help increase the overall robustness and security of the systems. |'
  prefs: []
  type: TYPE_TB
- en: There will certainly be other relevant topics that you’ll need to consider,
    so any premortem activity has to be prepared and discussed with all relevant stakeholders.
    One of the best ways to understand and reduce risk is to go very granular in terms
    of the envisioned generative AI solution details and the intended implementation.
    For that purpose, defining a detailed roadmap and related resources and activities
    will help a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Implementation Approach, Resources, and Project Roadmap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most challenging activities for complex AI projects is to plan activities
    in a granular way, and to elaborate detailed roadmaps that specify major categories
    of work, duration, and required technical and human resources.
  prefs: []
  type: TYPE_NORMAL
- en: This section includes a nonexhaustive methodology that will enable you as an
    Azure OpenAI adopter to plan your new projects, step by step. To illustrate the
    examples beyond the theory in the following sections, we will refer to [Azure
    DevOps](https://oreil.ly/dGQEr) (the native Azure service for project planning
    and more) and some of its features.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Project Workstreams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before planning project activities, it is key to define the implementation’s
    work scope. For that purpose, creating categories of work (or workstreams) will
    help create buckets to add different activities and user stories. Following is
    a recommended list of workstreams (independent of each other; the bullet points
    don’t represent a sequential approach) for generative AI projects with Azure OpenAI
    Service:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud enablement
  prefs: []
  type: TYPE_NORMAL
- en: All the design, deployment, and optimization activities related to Azure cloud
    landing zones, tenants, resource and resource groups, etc. It also includes security
    and monitoring configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs: []
  type: TYPE_NORMAL
- en: All data preparation, engineering, and storage activities that enable customization
    of Azure OpenAI–enabled systems. This is not the typical data engineering and
    pipelining process from nongenerative AI, in which we need to work towards a consolidated
    data input that will be used to train a model. Instead, the dataset will include
    a diverse set of “pieces” such as documents, JSON/JSONL files, and generated embeddings.
    Topics such as data quality measurement and improvement are still very relevant
    and key for successful implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs: []
  type: TYPE_NORMAL
- en: Everything related to the design, testing, automation, and optimization of system
    and user prompts. This is a specific kind of workstream due to the exploration
    and experimentation required to perform these activities (and therefore the complexity
    to plan and estimate them, from a project planning point of view).
  prefs: []
  type: TYPE_NORMAL
- en: Design and user testing
  prefs: []
  type: TYPE_NORMAL
- en: The end-to-end activities related to the design of the UIs (including previous
    user interviews), as well as the testing of the generative AI solution in terms
    of UI/UX and model performance and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment activities
  prefs: []
  type: TYPE_NORMAL
- en: A transversal workstream that includes all preliminary and production-level
    deployments, including initial pilots, temporary web apps for internal test, etc.
    This workstream will include key milestones and dates for specific steps within
    the full generative AI software lifecycle (i.e., proof of concept, minimum viable
    product), based on Evaluation Driven AI-System Development principles.
  prefs: []
  type: TYPE_NORMAL
- en: These workstreams can be represented as [area paths](https://oreil.ly/82ROJ)
    via Azure DevOps, as these paths help organize any project into groups of work
    items. They will also serve as work categories for visual roadmaps, as we will
    see later in the chapter. Now that we have the workstreams, let’s go into the
    details to define and quantify project resources.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Required Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to understand and plan the different resources we will need
    for an Azure OpenAI implementation. This will go from purely technical aspects
    to “man-hour” costs related to the development of the solution, as well as its
    maintenance. We can define the following resource categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Human resources
  prefs: []
  type: TYPE_NORMAL
- en: 'The different profiles and related skills will obviously depend on the type
    and scope of the implementation. However, there are some key roles you need to
    consider for any generative AI and Azure OpenAI Service–related implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: Business executives
  prefs: []
  type: TYPE_NORMAL
- en: To sponsor your generative AI projects, support business needs, and provide
    the required resources.
  prefs: []
  type: TYPE_NORMAL
- en: Architects/specialists
  prefs: []
  type: TYPE_NORMAL
- en: Technical profiles with highly specialized knowledge of Azure OpenAI and other
    generative AI services. They can evaluate functional and technical requirements,
    and define suitable architectures by leveraging chat, embeddings, text-to-image,
    and other models, as well as analyzing different fine-tuning and grounding options
    depending on the nature and format of the data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Developers
  prefs: []
  type: TYPE_NORMAL
- en: Software developer profiles with previous back- or frontend experience, especially
    for API-enabled integration projects. Some of the main capabilities are related
    to Azure OpenAI APIs, and orchestration blocks such as LangChain and Semantic
    Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineers
  prefs: []
  type: TYPE_NORMAL
- en: Talent with hybrid technical and business experience, with a mix of skills that
    includes prompt design, test, iteration, and optimization, as well as the creation
    of templates for production-level reuse of prompts. Profiles with previous experience
    may include advanced testing abilities, and knowledge of security and prompt injection
    techniques (e.g., quality assurance or QA engineers).
  prefs: []
  type: TYPE_NORMAL
- en: Security professionals
  prefs: []
  type: TYPE_NORMAL
- en: As part of the [AI red team](https://oreil.ly/5Kf3c) activities, or granular
    development activities to secure APIs, data sources, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud engineers/admins
  prefs: []
  type: TYPE_NORMAL
- en: Classic cloud professionals with the skills to administrate, deploy, configure,
    and consume cloud resources from Microsoft Azure. They may have an initial knowledge
    of the Azure portal, studio, and playground interfaces related to Azure OpenAI
    Service.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI and compliance experts
  prefs: []
  type: TYPE_NORMAL
- en: Specific roles with ethics and legal knowledge, related to the data privacy
    and AI regulatory topics mentioned in [Chapter 5](ch05.html#operationalizing_generative_ai_implementations).
    They are usually available on a part-time basis, and they may have specific knowledge
    of tools such as ChatGPT, Bing, Azure OpenAI Service, general cloud, product DPAs,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: Other tactical and technical roles
  prefs: []
  type: TYPE_NORMAL
- en: Project- and product-related roles, such as product managers (PMs) and product
    owners (POs), project managers, scrum masters, etc. They may have low to no generative
    AI experience, but we can expect these roles to continue upskilling and getting
    practical knowledge after a few projects. Also, classic roles such as the data
    trilogy (science, engineering, analysis) may be part of the team, and even take
    some of the core generative AI responsibilities, as an evolution from (or addition
    to) their current roles.
  prefs: []
  type: TYPE_NORMAL
- en: If you are hesitating between classic data and AI roles (e.g., data scientist)
    and the new LLM-oriented roles such as prompt engineer, [Figure 6-1](#fig_1_evolution_of_ai_roles_and_skills_illustrative_exa)
    shows some high-level guidance on roles and skills evolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Evolution of AI roles and skills (illustrative examples)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other technical resources
  prefs: []
  type: TYPE_NORMAL
- en: 'All product and services required to implement Azure OpenAI–enabled and other
    generative AI projects with Microsoft Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud subscription
  prefs: []
  type: TYPE_NORMAL
- en: In this case, all the required Azure services work based on consumption. This
    means there is no fixed price or license required, just the price for each of
    the Azure services being deployed for your specific generative AI implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Other software licenses
  prefs: []
  type: TYPE_NORMAL
- en: Not a requirement for most cases, but some implementations could include specific
    licenses, for example if you use PVAs as an orchestration block and bot interface,
    or if you leverage other non-Azure generative AI services to create end-to-end
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: External costs
  prefs: []
  type: TYPE_NORMAL
- en: This could be related to external technical and consulting services, often leveraged
    by adopter companies to accelerate their generative AI projects. This effort needs
    to be quantified during the ROI exercise we will review in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Other post-implementation costs
  prefs: []
  type: TYPE_NORMAL
- en: This includes future cost scenarios related to maintenance activities, model
    improvement, or evolution of the grounding scope (i.e., adding new files and data
    to the existing knowledge base, generating new embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: All these factors imply a total cost that we will use to plan and evaluate the
    sustainability of the business case related to the generative AI implementation.
    This is a key step to make sure it is really worth it for the adopting companies.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Duration and Effort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A second level of detail from the “human resources” defined in the previous
    section would be to quantify their participation and level of involvement in the
    project. A good way to present it is to list all roles with the specific details
    shown in [Figure 6-2](#fig_2_visual_template_for_project_roles_and_levels_of_ef).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Visual template for project roles and levels of effort
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this figure, we see the distribution of roles, with the number of people
    per role, their scope of activity, and the level of effort compared to a regular
    full-time involvement. This simple visual can be your best ally when trying to
    present the required roles and their level of effort (number of hours per week).
    However, you will need to calculate them depending on the project and your company’s
    context. An illustrative estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: A prompt engineer is a highly specialized role that could be required to work
    on a single project part time. Depending on the scope and level of required effort,
    we could imagine a range of 25% to 50% of their time for one project, which would
    mean working on several generative AI projects at the same time (concretely, between
    two and four).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-demand roles such as compliance and RAI experts could have a maximum number
    of hours assigned to projects, for reactive ad hoc dedication. It depends on the
    project, but the average could be 5% to 10% of their time, which would represent
    4 to 8 hours per week on average. Obviously, their involvement will vary depending
    on the project stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other roles like architects, engineers, admins, etc. will have specific knowledge
    required at different stages of the project. Some of them will even have two or
    more functional roles (e.g., an architect can initially define architecture and
    requirements, then play the engineering role to implement the solution). The same
    applies for generative AI developers, who may work on code-related activities,
    but also may deploy Azure OpenAI models and integrate them to the tools via APIs.
    All of them can go between 20% and 100% dedication, depending on the project scope.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of the type and number of roles, it is important to initially plan
    the resource requirements, to know how many people will work, how many hours per
    week, and during which phases of the end-to-end project. This means the important
    thing is to have a plan, not to look for the perfect one. The ability to plan
    and estimate in an accurate way will obviously depend on increasing project experience
    in relation to generative AI with Azure OpenAI, so it is normal to make incorrect
    assumptions during the first few projects. Even with that, we will have the key
    building blocks to prepare an initial roadmap for a generative AI project. Let’s
    check the details in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a “Living” Roadmap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s analyze what we should have up to now: a clear understanding of the potential
    use cases and implementation approaches, an initial definition of the architecture,
    a preliminary idea of the required resources at both the human and tooling levels,
    and an estimate of the availability of the different team members.'
  prefs: []
  type: TYPE_NORMAL
- en: These are all key elements to create a project’s “living” roadmap. Let’s start
    from the basics. A roadmap is a visual way to communicate a plan for achieving
    a goal or outcome, in this case the project implementation. It includes the major
    steps or milestones needed to reach it, and it includes the workstreams we have
    previously discussed as a way to organize all required tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a “living” asset refers to the ability to evolve it from its
    initial version. This is especially important for AI projects (including generative
    AI), as preliminary roadmaps tend to evolve during the implementation phase due
    to unknowns (e.g., how long we need to try and test different prompts) or unexpected
    events (e.g., limited access to cloud platforms, temporary leaves from team members).
    So you can consider the roadmap a canvas to plan and evolve your implementations
    in a visual, easy-to-read manner, an asset you can use with both technical and
    business/executive stakeholders to regularly discuss plans and progress. [Figure 6-3](#fig_3_visual_roadmap_template_for_ai_and_generative_ai_p)
    is an example of a visual roadmap for a generative AI project, with an illustrative
    case for an imaginary pharma company.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Visual roadmap template for AI and generative AI projects (top)
    and illustrative example (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this kind of one-pager roadmap, the important thing is not to go into hyper-granular
    details, but to define the “what” and “by when” of the workstreams and activities.
    Using sprints to structure blocks of work for a two- to four-week period is the
    way to get more tangible details of the sequence and duration of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: From an Azure point of view, once you have defined a visual roadmap (in some
    regular format such as a PowerPoint slide), you can implement its details via
    the [Boards feature](https://oreil.ly/NKWy8) in Azure DevOps. This functionality
    includes the ability to create [Kanban](https://oreil.ly/kEhyq) and [Scrum](https://oreil.ly/0THjo)
    boards, as well as [delivery plans](https://oreil.ly/v-hwK), which represent the
    scheduled work items by sprint against a calendar view.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a project plan and all its related details, let’s see how
    to create Azure OpenAI usage scenarios to estimate the cost of the cloud-related
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Usage Scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most challenging activities for adopters and related partners (e.g.,
    integrators, consulting firms) is to create realistic scenarios for potential
    usage of generative AI solutions. This is critical for sustainable business cases,
    but also to guarantee there is enough budget for the cloud-related costs (the
    cost structure of Azure OpenAI is well optimized for massive use, but it is still
    linear and depends directly on the number of interactions with the system). The
    challenging part is to imagine how many users will actually leverage the solution,
    and how.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that purpose, the best option is to take a *multilevel scenario drafting
    approach*, in which we will calculate several factors for a chat-based application
    with a potential number of end users, step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of expected (average) users
  prefs: []
  type: TYPE_NORMAL
- en: This is relatively difficult for any business-to-consumer (B2C) scenario in
    which we may have end users arriving dynamically, or even internal employees using
    internal generative AI applications. That said, the idea is to establish a maximum
    average number of users that we know will connect actively to the final solution.
  prefs: []
  type: TYPE_NORMAL
- en: Number of interactions per user, by day/week/month
  prefs: []
  type: TYPE_NORMAL
- en: Again, it is not easy to predict how many times a user will use the solution,
    but we need to define a maximum number. It can be based on the number of past
    interactions with existing solutions, or by allocating a maximum number of sessions
    by day, week, or month. This maximum number can serve as a session limit at the
    application level, to guarantee no one overuses the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Max length of each interaction (in tokens)
  prefs: []
  type: TYPE_NORMAL
- en: This is relatively simple and it applies to both prompts and completions. We
    can limit at both the Azure OpenAI model (via “max length” settings but also by
    defining the length using the system message) and application level (by limiting
    the number of characters and words a user can write). If we handle the maximum
    length for both questions and answers, we can obtain an average length for each
    interaction (e.g., 250 tokens for a 50-token question with a 200-token answer).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The general rule is 1,000 tokens equal 750 words (for English text), but it
    really depends on the language and type of words. For an accurate estimation of
    what an X-token question or answer would look like, check OpenAI’s [tokenizer
    tool](https://oreil.ly/DDQHG).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we get these three elements, we can technically imagine the maximum cost
    allocated to this usage scenario, by creating a very simple formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of expected users × number of interactions per user (for a specific period;
    for this, we’ll say by month)
  prefs: []
  type: TYPE_NORMAL
- en: × max token length of each interaction
  prefs: []
  type: TYPE_NORMAL
- en: = total cost (aka total number of tokens by month)
  prefs: []
  type: TYPE_NORMAL
- en: In this case, if we have the number of interactions by month, the total amount
    of tokens will correspond to the total usage of tokens in one single month. If
    we take that amount, let’s say 2,000 users × 30 interactions × 500 tokens max,
    we obtain a total of 30 million tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you know, the [regular pricing for Azure OpenAI](https://oreil.ly/7Gmq6)
    service is based on “bags” of 1,000 tokens. This means that we pay a fixed amount
    (depending on the pricing of each model, which tends to change and get lower over
    time), in this case 30,000,000 tokens / 1,000 tokens per bag = 30,000 bags. If
    we assume a unit price of $0.002 (illustrative amount for a model “X”), this means
    30,000 × 0.002 = 60 USD of monthly cost for Azure OpenAI usage, which means 720
    USD per year. Obviously, this amount will be higher for bigger scenarios, and
    it does not include:'
  prefs: []
  type: TYPE_NORMAL
- en: Additional Azure OpenAI costs for embedding-based scenarios. This means if we
    leverage embeddings, in addition to the regular chat-type capabilities, we will
    use a different kind of model (e.g., Embeddings Large) with specific pricing by
    1,000-token interaction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other implementation pieces such as Web Apps, other AI services for document
    intelligence, cognitive search, vector storage, speech-to-text and text-to-speech,
    etc. These depend on the type of solution, and can be calculated as any other
    cloud service, by using the official [Azure calculator](https://oreil.ly/2SQ4C).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other related license or external software being used for the final architecture.
    A good example could be using PVA as deployment and orchestration options for
    your Azure OpenAI service. In this case, you will need to add [its monthly cost](https://oreil.ly/KpqfF).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general terms, any “box” or building block from a visual architecture should
    be considered to calculate the total cost of the solution. Check the official
    pricing websites regularly to get updated information on the price per model.
    For company-wide scenarios, you can explore [potential chargeback setups](https://oreil.ly/fpA1N)
    so you can bill the corresponding cost to specific business units or departments.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have elaborated our usage scenario, in addition to all previous human
    and technical resources considerations, we are finally ready to elaborate our
    quantitative business case.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Cost and Potential ROI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can put everything together and focus on both the aggregated cost and
    the total estimated value. These two elements will help us elaborate our business
    case with a clear ROI estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: Total cost
  prefs: []
  type: TYPE_NORMAL
- en: This includes all the building blocks we have previously mentioned (human resources,
    cloud cost, licenses, external expenses, etc.), everything that implies a direct
    or indirect cost for the organization, including the cost of the implementation
    team for upskilling and working on the project. The cost of the cloud must be
    calculated based on the specific usage scenario, keeping in mind the pay-as-you-go
    and PTU pricing modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Estimated value
  prefs: []
  type: TYPE_NORMAL
- en: 'This will include any improvement that the adopting company may get from the
    generative AI project with Azure OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: Hard benefits
  prefs: []
  type: TYPE_NORMAL
- en: This will include any increase in revenue or generated savings. It is possible
    to quantify how much and by when with the financial impact usually starting after
    the initial implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Soft benefits
  prefs: []
  type: TYPE_NORMAL
- en: Any additional advantage related to the creation of new lines of business, strategic
    differentiation for the company, creation of new intellectual property, generative
    AI project experience for the team, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROI formula is simple, and it focuses mainly on tangible financial figures:'
  prefs: []
  type: TYPE_NORMAL
- en: ROI = [(Quantified hard benefits – Total cost) / Total cost] × 100
  prefs: []
  type: TYPE_NORMAL
- en: It can include considerations such as the break-even, which indicates when the
    initial investment will be recovered. For example, you could say that the total
    project implementation cost is X, but it will help generate 2X in two years, so
    the ROI is 200%, and it is likely to happen between the first and second year.
  prefs: []
  type: TYPE_NORMAL
- en: You (and your company) need to consider these financial or company-level aspects
    while evaluating your potential list of generative AI use cases with Azure OpenAI.
    You can use [Table 6-2](#table-6-2) as an example and fill it with your actual
    generative AI projects.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Generative AI use case discovery list
  prefs: []
  type: TYPE_NORMAL
- en: '| Use case | Description | Duration | Cost | ROI | Other benefits | Priority
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| UC1 | Example: Chatbot for pharma company | 7 weeks | X K$ | Between 150%
    and 200% depending on the scenario | Improved employee satisfaction | Top |'
  prefs: []
  type: TYPE_TB
- en: '| … |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| UCn |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: You can obviously add other relevant factors for your internal decision making,
    but this kind of analysis will facilitate internal discussions and prioritization
    of your next generative AI project. As with the other visuals from this chapter,
    these simple tables and slides remove complexity when dealing with all business
    and technical stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes our sixth chapter, which has focused on the key elements (e.g.,
    project roadmaps, required resources, cost estimation) to build sustainable and
    realistic business cases for your generative AI projects with Azure OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, these business-related topics are as important as any other technical
    consideration, and the way to enable successful implementations and guarantee
    user adoption, for both internal and external use cases. If your generative AI
    system is great but numbers don’t add up, then your company won’t be able to adopt
    it and make the most of it. Try to include the recommendations from this chapter
    during your prioritization and design process.
  prefs: []
  type: TYPE_NORMAL
- en: We will now continue with the last chapter of the book, which includes several
    generative AI success stories from experts in the field. We are almost there—let’s
    do it.
  prefs: []
  type: TYPE_NORMAL
