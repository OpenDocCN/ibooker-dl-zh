["```py\nimport torch\nfrom previous_chapters import GPTModel\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,  # Vocabulary size\n    \"ctx_len\": 256,       # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,       # Embedding dimension\n    \"n_heads\": 12,        # Number of attention heads\n    \"n_layers\": 12,       # Number of layers\n    \"drop_rate\": 0.1,     # Dropout rate\n    \"qkv_bias\": False     # Query-key-value bias\n}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\n```", "```py\nimport os\nimport urllib.request\n\nfile_path = \"the-verdict.txt\"\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n\nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text_data = file.read()\nNext, we load the text_data into the data loaders:\nfrom previous_chapters import create_dataloader_v1\n\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntorch.manual_seed(123)\ntrain_loader = create_dataloader_v1(\n    text_data[:split_idx],\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"ctx_len\"],\n    stride=GPT_CONFIG_124M[\"ctx_len\"],\n    drop_last=True,\n    shuffle=True\n)\nval_loader = create_dataloader_v1(\n    text_data[split_idx:],\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"ctx_len\"],\n    stride=GPT_CONFIG_124M[\"ctx_len\"],\n    drop_last=False,\n    shuffle=False\n)\n```", "```py\nn_epochs = 15\ninitial_lr = 0.0001\npeak_lr = 0.01\nwarmup_steps = 20\n```", "```py\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\nlr_increment = (peak_lr - initial_lr) / warmup_steps #A\n\nglobal_step = -1\ntrack_lrs = []\n\nfor epoch in range(n_epochs):  #B\n    for input_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        global_step += 1\n\n        if global_step < warmup_steps: #C\n            lr = initial_lr + global_step * lr_increment\n        else:\n            lr = peak_lr\n\n        for param_group in optimizer.param_groups: #D\n            param_group[\"lr\"] = lr\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n        #E\n```", "```py\nimport matplotlib.pyplot as plt\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Step\")\ntotal_training_steps = len(train_loader) * n_epochs\nplt.plot(range(total_training_steps), track_lrs);\nplt.show()\n```", "```py\nimport math\n\nmin_lr = 0.1 * initial_lr\ntrack_lrs = []\nlr_increment = (peak_lr - initial_lr) / warmup_steps\nglobal_step = -1\n\nfor epoch in range(n_epochs):\n    for input_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        global_step += 1\n\n        if global_step < warmup_steps:\n            lr = initial_lr + global_step * lr_increment  \n        else:# #B\n            progress = ((global_step - warmup_steps) / \n                        (total_training_steps - warmup_steps))\n            lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n```", "```py\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Step\")\nplt.plot(range(total_training_steps), track_lrs)\nplt.show()\n```", "```py\nfrom previous_chapters import calc_loss_batch\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nloss = calc_loss_batch(input_batch, target_batch, model, device)\nloss.backward()\n```", "```py\ndef find_highest_gradient(model):\n    max_grad = None\n    for param in model.parameters():\n        if param.grad is not None:\n            grad_values = param.grad.data.flatten()\n            max_grad_param = grad_values.max()\n            if max_grad is None or max_grad_param > max_grad:\n                max_grad = max_grad_param\n    return max_grad\nprint(find_highest_gradient(model))\n```", "```py\ntensor(0.0373)\n```", "```py\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\nprint(find_highest_gradient(model))\n```", "```py\ntensor(0.0166)\n```", "```py\nfrom previous_chapters import evaluate_model, generate_and_print_sample\n\ndef train_model(model, train_loader, val_loader, optimizer, device, n_epochs,\n                eval_freq, eval_iter, start_context, warmup_steps=10,\n                initial_lr=3e-05, min_lr=1e-6):\n\n    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n    tokens_seen, global_step = 0, -1\n\n    peak_lr = optimizer.param_groups[0][\"lr\"] #A\n    total_training_steps = len(train_loader) * n_epochs #B\n    lr_increment = (peak_lr - initial_lr) / warmup_steps #C\n\n    for epoch in range(n_epochs):\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()\n            global_step += 1\n\n            if global_step < warmup_steps: #D\n                lr = initial_lr + global_step * lr_increment  \n            else:\n                progress = ((global_step - warmup_steps) / \n                            (total_training_steps - warmup_steps))\n                lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n                    1 + math.cos(math.pi * progress))\n\n            for param_group in optimizer.param_groups: #E\n                param_group[\"lr\"] = lr\n            track_lrs.append(lr)\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()\n\n            if global_step > warmup_steps: #F\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            #G\n            optimizer.step() \n            tokens_seen += input_batch.numel()\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader,\n                    device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n        generate_and_print_sample(\n            model, train_loader.dataset.tokenizer,\n            device, start_context\n        )\n\n    return train_losses, val_losses, track_tokens_seen, track_lrs\n```", "```py\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\npeak_lr = 5e-4\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n\nn_epochs = 15\ntrain_losses, val_losses, tokens_seen, lrs = train_model(\n    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n    warmup_steps=10, initial_lr=1e-5, min_lr=1e-5\n)\n```", "```py\nEp 1 (Iter 000000): Train loss 10.934, Val loss 10.939\nEp 1 (Iter 000005): Train loss 8.529, Val loss 8.843\nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nEp 2 (Iter 000010): Train loss 6.400, Val loss 6.825\nEp 2 (Iter 000015): Train loss 6.116, Val loss 6.861\nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n... \nthe irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\nEp 15 (Iter 000130): Train loss 0.101, Val loss 6.707\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n```"]