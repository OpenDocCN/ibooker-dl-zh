- en: Chapter 9\. PyTorch in the Wild
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。PyTorch在实践中
- en: For our final chapter, we’ll look at how PyTorch is used by other people and
    companies. You’ll also learn some new techniques along the way, including resizing
    pictures, generating text, and creating images that can fool neural networks.
    In a slight change from earlier chapters, we’ll be concentrating on how to get
    up and running with existing libraries rather than starting from scratch in PyTorch.
    I’m hoping that this will be a springboard for further exploration.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最后一章中，我们将看看PyTorch如何被其他人和公司使用。您还将学习一些新技术，包括调整图片大小、生成文本和创建可以欺骗神经网络的图像。与之前章节略有不同的是，我们将集中在如何使用现有库快速上手，而不是从头开始使用PyTorch。我希望这将成为进一步探索的跳板。
- en: Let’s start by examining some of the latest approaches for squeezing the most
    out of your data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从检查一些最新的方法开始，以充分利用您的数据。
- en: 'Data Augmentation: Mixed and Smoothed'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强：混合和平滑
- en: Way back in [Chapter 4](ch04.html#transfer-learning-and-other-tricks), we looked
    at various ways of augmenting data to help reduce the model overfitting on the
    training dataset. The ability to do more with less data is naturally an area of
    high activity in deep learning research, and in this section we’ll look at two
    increasingly popular ways to squeeze every last drop of signal from your data.
    Both approaches will also see us changing how we calculate our loss function,
    so it will be a good test of the more flexible training loop that we just created.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第4章](ch04.html#transfer-learning-and-other-tricks)，我们看了各种增强数据的方法，以帮助减少模型在训练数据集上的过拟合。在深度学习研究中，能够用更少的数据做更多事情自然是一个活跃的领域，在本节中，我们将看到两种越来越受欢迎的方法，以从您的数据中挤出最后一滴信号。这两种方法也将使我们改变如何计算我们的损失函数，因此这将是对我们刚刚创建的更灵活的训练循环的一个很好的测试。
- en: mixup
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: mixup
- en: '*mixup* is an intriguing augmentation technique that arises from looking askew
    at what we want our model to do. Our normal understanding of a model is that we
    send it an image like the one in [Figure 9-1](#a-fox) and want the model to return
    a result that the image is a fox.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*mixup*是一种有趣的增强技术，它源于对我们希望模型做什么的侧面看法。我们对模型的正常理解是，我们向其发送一张像[图9-1](#a-fox)中的图像，并希望模型返回一个结果，即该图像是一只狐狸。'
- en: '![A fox](assets/ppdl_0901.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![一只狐狸](assets/ppdl_0901.png)'
- en: Figure 9-1\. A fox
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。一只狐狸
- en: But as you know, we don’t get just that from the model; we get a tensor of all
    the possible classes and, hopefully, the element of that tensor with the highest
    value is the *fox* class. In fact, in the ideal scenario, we’d have a tensor that
    is all 0s except for a 1 in the fox class.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如您所知，我们不仅从模型中得到这些；我们得到一个包含所有可能类别的张量，希望该张量中具有最高值的元素是*狐狸*类。实际上，在理想情况下，我们将有一个张量，除了狐狸类中的1之外，其他都是0。
- en: 'Except that is difficult for a neural network to do! There’s always going to
    be uncertainty, and our activation functions like `softmax` make it difficult
    for the tensors to get to 1 or 0\. mixup takes advantage of this by asking a question:
    what is the class of [Figure 9-2](#a-mixture-of-cat-and-fox)?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了神经网络很难做到这一点！总会有不确定性，我们的激活函数如`softmax`使得张量很难达到1或0。mixup利用这一点提出了一个问题：[图9-2](#a-mixture-of-cat-and-fox)的类是什么？
- en: '![A mixture of cat and fox](assets/ppdl_0902.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![一只猫和一只狐狸的混合](assets/ppdl_0902.png)'
- en: Figure 9-2\. A mixture of cat and fox
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。一只猫和一只狐狸的混合
- en: To our eyes, this may be a bit of a mess, but it is 60% cat and 40% fox. What
    if, instead of trying to make our model make a definitive guess, we could make
    it target two classes? This would mean that our output tensor won’t run into the
    problem of approaching but never reaching 1 in training, and we could alter each
    *mixed* image by a different fraction, improving our model’s ability to generalize.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，这可能有点混乱，但是它是60%的猫和40%的狐狸。如果我们不试图让我们的模型做出明确的猜测，而是让它针对两个类别呢？这意味着我们的输出张量在训练中不会遇到接近但永远无法达到1的问题，我们可以通过不同的分数改变每个*混合*图像，提高我们模型的泛化能力。
- en: 'But how do we calculate the loss function of this mixed-up image? Well, if
    *p* is the percentage of the first image in the mixed image, then we have a simple
    linear combination of the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何计算这个混合图像的损失函数呢？如果*p*是混合图像中第一幅图像的百分比，那么我们有以下简单的线性组合：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It has to predict those images, right? And we need to scale according to how
    much of those images is in the final mixed image, so this new loss function seems
    reasonable. To choose *p*, we could just use random numbers drawn from a normal
    or uniform distribution as we would do in many other cases. However, the writers
    of the mixup paper determined that samples drawn from the *beta* distribution
    work out much better in practice.^([1](ch09.html#idm45762349177880)) Don’t know
    what the beta distribution looks like? Well, neither did I until I saw this paper!
    [Figure 9-3](#beta-distribution) shows how it looks when given the characteristics
    described in the paper.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须预测这些图像，对吧？我们需要根据这些图像在最终混合图像中的比例来缩放，因此这种新的损失函数似乎是合理的。要选择*p*，我们可以像在许多其他情况下那样，使用从正态分布或均匀分布中抽取的随机数。然而，mixup论文的作者确定，从*beta*分布中抽取的样本在实践中效果要好得多。不知道beta分布是什么样子？嗯，我在看到这篇论文之前也不知道！[图9-3](#beta-distribution)展示了在给定论文中描述的特征时它的样子。
- en: '![Beta Distribution where ⍺ = β](assets/ppdl_0903.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Beta分布，其中⍺ = β](assets/ppdl_0903.png)'
- en: Figure 9-3\. Beta distribution, where ⍺ = β
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3。Beta分布，其中⍺ = β
- en: The U-shape is interesting because it tells us that most of the time, our mixed
    image will be mainly one image or another. Again, this makes intuitive sense as
    we can imagine the network is going to have a harder time working out a 50/50
    mixup than a 90/10 one.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: U形状很有趣，因为它告诉我们，大部分时间，我们混合的图像主要是一张图像或另一张图像。再次，这是直观的，因为我们可以想象网络在工作中会更难以解决50/50混合比例而不是90/10的情况。
- en: 'Here’s a modified training loop that takes a new additional data loader, `mix_loader`,
    and mixes the batches together:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个修改后的训练循环，它接受一个新的额外数据加载器`mix_loader`，并将批次混合在一起：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What’s happening here is after we get our two batches, we use `torch.distribution.Beta`
    to generate a series of mix parameters, using the `expand` method to produce a
    tensor of `[1, batch_size]`. We could iterate through the batch and generate the
    parameters one by one, but this is neater, and remember, GPUs love matrix multiplication,
    so it’ll end up being faster to do all the calculations across the batch at once
    (this is shown in [Chapter 7](ch07.html#debugging-pytorch-models) when fixing
    our `BadRandom` transformation, remember!). We multiply the entire batch by this
    tensor, and then the batch to mix in by `1 - mix_factor_tensor` using broadcasting
    (which we covered in [Chapter 1](ch01.html#getting-started-with-pytorch)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的是在获取两个批次后，我们使用`torch.distribution.Beta`生成一系列混合参数，使用`expand`方法生成一个`[1, batch_size]`的张量。我们可以遍历批次并逐个生成参数，但这样更整洁，记住，GPU喜欢矩阵乘法，所以一次跨批次进行所有计算会更快（这在[第7章](ch07.html#debugging-pytorch-models)中展示了，当修复我们的`BadRandom`转换时，记住！）。我们将整个批次乘以这个张量，然后使用广播将要混合的批次乘以`1
    - mix_factor_tensor`。
- en: We then take the losses of the predictions against our targets for both images,
    and our final loss is the mean of the sum of those losses. What’s happening there?
    Well, if you look at the source code for `CrossEntropyLoss`, you’ll see the comment
    `The losses are averaged across observations for each minibatch.` There’s also
    a `reduction` parameter that has a default set to `mean` (we’ve used the default
    so far, so that’s why you haven’t seen it before!). We need to preserve that condition,
    so we take the mean of our combined losses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算两个图像的预测与目标之间的损失，最终的损失是这些损失之和的平均值。发生了什么？如果你查看`CrossEntropyLoss`的源代码，你会看到注释`损失在每个minibatch的观察中进行平均。`还有一个`reduction`参数，默认设置为`mean`（到目前为止我们使用了默认值，所以你之前没有看到它！）。我们需要保持这个条件，所以我们取我们合并的损失的平均值。
- en: 'Now, having two data loaders isn’t too much trouble, but it does make the code
    a little more complicated. If you run this code, you might error out because the
    batches are not balanced as final batches come out of the loaders, meaning that
    you’ll have to write extra code to handle that case. The authors of the mixup
    paper suggest that you could replace the mix data loader with a random shuffle
    of the incoming batch. We can do this with `torch.randperm()`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拥有两个数据加载器并不会带来太多麻烦，但它确实使代码变得更加复杂。如果你运行这段代码，可能会出错，因为最终批次从加载器中出来时不平衡，这意味着你将不得不编写额外的代码来处理这种情况。mixup论文的作者建议你可以用随机洗牌来替换混合数据加载器。我们可以使用`torch.randperm()`来实现这一点：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When using mixup in this way, be aware that you are much more likely to get
    *collisions* where you end up applying the same parameter to the same set of images,
    potentially reducing the accuracy of training. For example, you could have cat1
    mixed with fish1, and draw a beta parameter of 0.3\. Then later in the same batch,
    you pull out fish1 and it gets mixed with cat1 with a parameter of 0.7—making
    it the same mix! Some implementations of mixup—in particular, the fast.ai implementation—resolve
    this issue by replacing our mix parameters with the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方式下使用mixup时，要注意更有可能出现*碰撞*，即最终将相同的参数应用于相同的图像集，可能会降低训练的准确性。例如，你可能有cat1与fish1混合，然后抽取一个beta参数为0.3。然后在同一批次中的后续步骤中，你再次抽取fish1并将其与cat1混合，参数为0.7—这样就得到了相同的混合！一些mixup的实现—特别是fast.ai的实现—通过用以下内容替换我们的混合参数来解决这个问题：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This ensures that the nonshuffled batch will always have the highest component
    when being merged with the mix batch, thus eliminating that potential issue.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了非混洗的批次在与混合批次合并时始终具有最高的分量，从而消除了潜在的问题。
- en: 'Oh, and one more thing: we performed the mixup transformation *after* our image
    transformation pipeline. At this point, our batches are just tensors that we’ve
    added together. This means that there’s no reason mixup training should be restricted
    to images. We could use it on any type of data that’s been transformed into tensors,
    whether text, image, audio, or anything else.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，还有一件事：我们在图像转换流程之后执行了mixup转换。此时，我们的批次只是我们相加在一起的张量。这意味着mixup训练不应该仅限于图像。我们可以对任何转换为张量的数据使用它，无论是文本、图像、音频还是其他任何类型的数据。
- en: 'We can still do a little more to make our labels work harder for us. Enter
    another approach that is now a mainstay of state-of-the-art models: *label smoothing*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以做更多工作让我们的标签更加有效。现在进入另一种方法，这种方法现在是最先进模型的主要特点：*标签平滑*。
- en: Label Smoothing
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签平滑
- en: 'In a similar manner to mixup, *label smoothing* helps to improve model performance
    by making the model less sure of its predictions. Instead of trying to force it
    to predict `1` for the predicted class (which has all the problems we talked about
    in the previous section), we instead alter it to predict 1 minus a small value,
    *epsilon*. We can create a new loss function implementation that wraps up our
    existing `CrossEntropyLoss` function with this functionality. As it turns out,
    writing a custom loss function is just another subclass of `nn.Module`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与mixup类似，*标签平滑*有助于通过使模型对其预测不那么确定来提高模型性能。我们不再试图强迫它预测预测类别为`1`（这在前一节中讨论的所有问题中都有问题），而是将其改为预测1减去一个小值，*epsilon*。我们可以创建一个新的损失函数实现，将我们现有的`CrossEntropyLoss`函数与这个功能包装在一起。事实证明，编写自定义损失函数只是`nn.Module`的另一个子类：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When it comes to computing the loss function, we calculate the cross-entropy
    loss as per the implementation of `CrossEntropyLoss`. Our `final_loss` is constructed
    from negative log-likelihood being multiplied by 1 minus epsilon (our *smoothed*
    label) added to the loss multiplied by epsilon divided by the number of classes.
    This occurs because we are smoothing not only the label for the predicted class
    to be 1 minus epsilon, but also the other labels so that they’re not being forced
    to zero, but instead a value between zero and epsilon.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失函数时，我们按照`CrossEntropyLoss`的实现计算交叉熵损失。我们的`final_loss`由负对数似然乘以1减epsilon（我们的*平滑*标签）加上损失乘以epsilon除以类别数构成。这是因为我们不仅将预测类别的标签平滑为1减epsilon，还将其他标签平滑为不是被迫为零，而是在零和epsilon之间的值。
- en: This new custom loss function can replace `CrossEntropyLoss` in training anywhere
    we’ve used it in the book, and when combined with mixup, it is an incredibly effective
    way of getting that little bit more from your input data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的自定义损失函数可以替代书中任何地方使用的`CrossEntropyLoss`进行训练，并与mixup结合使用，是从输入数据中获得更多的一种非常有效的方法。
- en: 'We’ll now turn away from data augmentation to have a look at another hot topic
    in current deep learning trends: generative adversarial networks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从数据增强转向另一个当前深度学习趋势中的热门话题：生成对抗网络。
- en: Computer, Enhance!
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机，增强！
- en: One odd consequence of the increasing power of deep learning is that for decades,
    we computer people have been mocking television crime shows that have a detective
    click a button to make a blurry camera image suddenly become a sharp, in-focus
    picture. How we laughed and cast derision on shows like CSI for doing this. Except
    we can now actually do this, at least up to a point. Here’s an example of this
    witchcraft, on a smaller 256 × 256 image scaled to 512 × 512, in Figures [9-4](#postbox-at-256)
    and [9-5](#postbox-enhanced).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习能力不断增强的一个奇怪后果是，几十年来，我们计算机人一直在嘲笑那些电视犯罪节目，其中侦探点击按钮，使模糊的摄像头图像突然变得清晰、聚焦。我们曾经嘲笑和嘲弄CSI等节目做这种事情。但现在我们实际上可以做到这一点，至少在一定程度上。这里有一个巫术的例子，将一个较小的256×256图像缩放到512×512，见图[9-4](#postbox-at-256)和[9-5](#postbox-enhanced)。
- en: '![Postbox at 256x256 resolution](assets/ppdl_0904.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![256x256分辨率下的邮箱](assets/ppdl_0904.png)'
- en: Figure 9-4\. Mailbox at 256 × 256 resolution
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4. 256×256分辨率下的邮箱
- en: '![ESRGAN-enhanched postbox at 512x512 resolution](assets/ppdl_0905.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![512x512分辨率下的ESRGAN增强邮箱](assets/ppdl_0905.png)'
- en: Figure 9-5\. ESRGAN-enhanced mailbox at 512 × 512 resolution
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5. 512×512分辨率下的ESRGAN增强邮箱
- en: The neural network learns how to *hallucinate* new details to fill in what’s
    not there, and the effect can be impressive. But how does this work?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学习如何*幻想*新的细节来填补不存在的部分，效果可能令人印象深刻。但这是如何工作的呢？
- en: Introduction to Super-Resolution
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超分辨率简介
- en: 'Here’s the first part of a very simple super-resolution model. To start, it’s
    pretty much exactly the same as any model you’ve seen so far:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的超分辨率模型的第一部分。起初，它几乎与你迄今为止看到的任何模型完全相同：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we pass a random tensor through the network, we end up with a tensor of
    shape `[1, 256, 62, 62]`; the image representation has been compressed into a
    much smaller vector. Let’s now introduce a new layer type, `torch.nn.ConvTranspose2d`.
    You can think of this as a layer that inverts a standard `Conv2d` transform (with
    its own learnable parameters). We’ll add a new `nn.Sequential` layer, `upsample`,
    and put in a simple list of these new layers and `ReLU` activation functions.
    In the `forward()` method, we pass input through that consolidated layer after
    the others:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过网络传递一个随机张量，我们最终得到一个形状为`[1, 256, 62, 62]`的张量；图像表示已经被压缩为一个更小的向量。现在让我们引入一个新的层类型，`torch.nn.ConvTranspose2d`。你可以将其视为一个反转标准`Conv2d`变换的层（具有自己的可学习参数）。我们将添加一个新的`nn.Sequential`层，`upsample`，并放入一系列这些新层和`ReLU`激活函数。在`forward()`方法中，我们将输入通过其他层后通过这个整合层：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you now test the model with a random tensor, you’ll get back a tensor of
    exactly the same size that went in! What we’ve built here is known as an *autoencoder*,
    a type of network that rebuilds its input, usually after compressing it into a
    smaller dimension. That is what we’ve done here; the `features` sequential layer
    is an *encoder* that transforms an image into a tensor of size `[1, 256, 62, 62]`,
    and the `upsample` layer is our *decoder* that turns it back into the original
    shape.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在用一个随机张量测试模型，你将得到一个与输入完全相同大小的张量！我们构建的是一个*自动编码器*，一种网络类型，通常在将其压缩为更小维度后重新构建其输入。这就是我们在这里做的；`features`顺序层是一个*编码器*，将图像转换为大小为`[1,
    256, 62, 62]`的张量，`upsample`层是我们的*解码器*，将其转换回原始形状。
- en: Our labels for training the image would, of course, be our input images, but
    that means we can’t use loss functions like our fairly standard `CrossEntropyLoss`,
    because, well, we don’t have classes! What we want is a loss function that tells
    us how different our output image is from our input image, and for that, taking
    the mean squared loss or mean absolute loss between the pixels of the image is
    a common approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练图像的标签当然是我们的输入图像，但这意味着我们不能使用像我们相当标准的`CrossEntropyLoss`这样的损失函数，因为，嗯，我们没有类别！我们想要的是一个告诉我们输出图像与输入图像有多大不同的损失函数，为此，计算图像像素之间的均方损失或均绝对损失是一种常见方法。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although calculating the loss in terms of pixels makes a lot of sense, it turns
    out that a lot of the most successful super-resolution networks use augmented
    loss functions that try to capture how much a generated image looks like the original,
    tolerating pixel loss for better performance in areas like texture and content
    loss. Some of the papers listed in [“Further Reading”](#chapter-9-further-reading)
    go into deeper detail.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以像素为单位计算损失非常合理，但事实证明，许多最成功的超分辨率网络使用增强损失函数，试图捕捉生成图像与原始图像的相似程度，容忍像素损失以获得更好的纹理和内容损失性能。一些列在[“进一步阅读”](#chapter-9-further-reading)中的论文深入讨论了这一点。
- en: Now that gets us back to the same size input we entered, but what if we add
    another transposed convolution to the mix?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这使我们回到了与输入相同大小的输入，但如果我们在其中添加另一个转置卷积会怎样呢？
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Try it! You should find that the output tensor is twice as big as the input.
    If we have access to a set of ground truth images at that size to act as labels,
    we can train the network to take in images at a size *x* and produce images for
    a size *2x*. In practice, we tend to perform this upsampling by scaling up twice
    as much as we need to and then adding a standard convolutional layer, like so:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 试试吧！您会发现输出张量是输入的两倍大。如果我们有一组与该大小相同的地面真实图像作为标签，我们可以训练网络以接收大小为*x*的图像并为大小为*2x*的图像生成图像。在实践中，我们倾向于通过扩大两倍所需的量，然后添加一个标准的卷积层来执行这种上采样，如下所示：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We do this because the transposed convolution has a tendency to add jaggies
    and moiré patterns as it expands the image. By expanding twice and then scaling
    back down to our required size, we hopefully provide enough information to the
    network to smooth those out and make the output look more realistic.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是因为转置卷积有添加锯齿和moire图案的倾向，因为它扩展图像。通过扩展两次，然后缩小到我们需要的大小，我们希望为网络提供足够的信息来平滑这些图案，并使输出看起来更真实。
- en: Those are the basics behind super-resolution. Most current high-performing super-resolution
    networks are trained with a technique called the generative adversarial network,
    which has stormed the deep learning world in the past few years.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是超分辨率背后的基础。目前大多数性能优越的超分辨率网络都是使用一种称为生成对抗网络的技术进行训练的，这种技术在过去几年中席卷了深度学习世界。
- en: An Introduction to GANs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs简介
- en: One of the universal problems in deep learning (or any machine learning application)
    is the cost of producing labeled data. In this book, we’ve mostly avoided the
    problem by using sample datasets that are all carefully labeled (even some that
    come prepackaged in easy training/validation/test sets!). But in the real world
    producing large quantities of labeled data. Indeed, techniques that you’ve learned
    a lot about so far, like transfer learning, have all been about doing more with
    less. But sometimes you need more, and *generative adversarial networks* (GANs)
    have a way to help.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（或任何机器学习应用）中的一个普遍问题是产生标记数据的成本。在本书中，我们大多数情况下通过使用精心标记的样本数据集来避免这个问题（甚至一些预先打包的易于训练/验证/测试集！）。但在现实世界中，产生大量标记数据。确实，到目前为止，您学到的技术，如迁移学习，都是关于如何用更少的资源做更多的事情。但有时您需要更多，*生成对抗网络*（GANs）有办法帮助。
- en: GANs were introduced by Ian Goodfellow in a 2014 paper and are a novel way of
    providing more data to help train neural networks. And the approach is mainly
    “we know you love neural networks, so we added another.”^([2](ch09.html#idm45762348101832))
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是由Ian Goodfellow在2014年的一篇论文中提出的，是一种提供更多数据以帮助训练神经网络的新颖方法。而这种方法主要是“我们知道你喜欢神经网络，所以我们添加了另一个。”
- en: The Forger and the Critic
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伪造者和评论家
- en: The setup of a GAN is as follows. Two neural networks are trained together.
    The first is the *generator*, which takes random noise from the vector space of
    the input tensors and produces fake data as output. The second network is the
    *discriminator*, which alternates between the generated fake data and real data.
    Its job is to look at the incoming inputs and decide whether they’re real or fake.
    A simple conceptual diagram of a GAN is shown in [Figure 9-6](#a-simple-gan-setup).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的设置如下。两个神经网络一起训练。第一个是*生成器*，它从输入张量的向量空间中获取随机噪声，并产生虚假数据作为输出。第二个网络是*鉴别器*，它在生成的虚假数据和真实数据之间交替。它的工作是查看传入的输入并决定它们是真实的还是虚假的。GAN的简单概念图如[图9-6](#a-simple-gan-setup)所示。
- en: '![A simple GAN setup](assets/ppdl_0906.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的GAN设置](assets/ppdl_0906.png)'
- en: Figure 9-6\. A simple GAN setup
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6。一个简单的GAN设置
- en: 'The great thing about GANs is that although the details end up being somewhat
    complicated, the general idea is easy to convey: the two networks are in opposition
    to each other, and during training they work as hard as they can to defeat the
    other. By the end of the process, the *generator* should be producing data that
    matches the *distribution* of the real input data to flummox the *discriminator*.
    And once you get to that point, you can use the generator to produce more data
    for all your needs, while the discriminator presumably retires to the neural network
    bar to drown its sorrows.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的伟大之处在于，尽管细节最终变得有些复杂，但总体思想很容易传达：这两个网络相互对立，在训练过程中，它们尽力击败对方。在过程结束时，*生成器*应该生成与真实输入数据的*分布*匹配的数据，以迷惑*鉴别器*。一旦达到这一点，您可以使用生成器为所有需求生成更多数据，而鉴别器可能会退休到神经网络酒吧淹没忧愁。
- en: Training a GAN
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练GAN
- en: 'Training a GAN is a little more complicated than training traditional networks.
    During the training loop, we first need to use real data to start training the
    discriminator. We calculate the discriminator’s loss (using BCE, as we have only
    two classes: real or fake), and then do a backward pass to update the parameters
    of the discriminator as usual. But this time, we *don’t* call the optimizer to
    update. Instead, we generate a batch of data from our generator and pass that
    through the model. We calculate the loss and do *another* backward pass, so at
    this point the training loop has calculated the losses of two passes through the
    model. Now, we call the optimizer to update based on these *accumulated* gradients.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN比训练传统网络稍微复杂一些。在训练循环中，我们首先需要使用真实数据开始训练鉴别器。我们计算鉴别器的损失（使用BCE，因为我们只有两类：真实或虚假），然后进行反向传播以更新鉴别器的参数，就像往常一样。但这一次，我们*不*调用优化器来更新。相反，我们从生成器生成一批数据并通过模型传递。我们计算损失并进行*另一次*反向传播，因此此时训练循环已计算了两次通过模型的损失。现在，我们根据这些*累积*梯度调用优化器进行更新。
- en: In the second half of training, we turn to the generator. We give the generator
    access to the discriminator and then generate a new batch of data (which the generator
    insists is all real!) and test it against the discriminator. We form a loss against
    this output data, where each data point that the discriminator says is fake is
    considered a *wrong* answer—because we’re trying to fool it—and then do a standard
    backward/optimize pass.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的后半段，我们转向生成器。我们让生成器访问鉴别器，然后生成一批新数据（生成器坚持说这些都是真实的！）并将其与鉴别器进行测试。我们根据这些输出数据形成一个损失，鉴别器说是假的每个数据点都被视为*错误*答案——因为我们试图欺骗它——然后进行标准的反向/优化传递。
- en: 'Here’s a generalized implementation in PyTorch. Note that the generator and
    discriminator are just standard neural networks, so theoretically they could be
    generating images, text, audio, or whatever type of data, and be constructed of
    any of the types of networks you’ve seen so far:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PyTorch中的一个通用实现。请注意，生成器和鉴别器只是标准的神经网络，因此从理论上讲，它们可以生成图像、文本、音频或任何类型的数据，并且可以由迄今为止看到的任何类型的网络构建：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the flexibility of PyTorch helps a lot here. Without a dedicated training
    loop that is perhaps mainly designed for more standard training, building up a
    new training loop is something we’re used to, and we know all the steps that we
    need to include. In some other frameworks, training GANs is a bit more of a fiddly
    process. And that’s important, because training GANs is a difficult enough task
    without the framework getting in the way.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PyTorch的灵活性在这里非常有帮助。没有专门为更标准的训练而设计的训练循环，构建一个新的训练循环是我们习惯的事情，我们知道需要包含的所有步骤。在其他一些框架中，训练GAN有点更加繁琐。这很重要，因为训练GAN本身就是一个困难的任务，如果框架阻碍了这一过程，那就更加困难了。
- en: The Dangers of Mode Collapse
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式崩溃的危险
- en: In an ideal world, what happens during training is that the discriminator will
    be good at detecting fakes at first, because it’s training on real data, whereas
    the generator is allowed access to only the discriminator and not the real data
    itself. Eventually, the generator will learn how to fool the discriminator, and
    then it will soon improve rapidly to match the data distribution in order to repeatedly
    produce forgeries that slip past the critic.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，训练过程中发生的是，鉴别器一开始会擅长检测假数据，因为它是在真实数据上训练的，而生成器只允许访问鉴别器而不是真实数据本身。最终，生成器将学会如何欺骗鉴别器，然后它将迅速改进以匹配数据分布，以便反复产生能够欺骗评论者的伪造品。
- en: But one thing that plagues many GAN architectures is *mode collapse*. If our
    real data has three types of data, then maybe our generator will start generating
    the first type, and perhaps it starts getting rather good at it. The discriminator
    may then decide that anything that looks like the first type is actually fake,
    even the real example itself, and the generator then starts to generate something
    that looks like the third type. The discriminator starts rejecting all samples
    of the third type, and the generator picks another one of the real examples to
    generate. The cycle continues endlessly; the generator never manages to settle
    into a period where it can generate samples from across the distribution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但是困扰许多GAN架构的一件事是*模式崩溃*。如果我们的真实数据有三种类型的数据，那么也许我们的生成器会开始生成第一种类型，也许它开始变得相当擅长。鉴别器可能会决定任何看起来像第一种类型的东西实际上是假的，甚至是真实的例子本身，然后生成器开始生成看起来像第三种类型的东西。鉴别器开始拒绝所有第三种类型的样本，生成器选择另一个真实例子来生成。这个循环无休止地继续下去；生成器永远无法进入一个可以从整个分布中生成样本的阶段。
- en: Reducing mode collapse is a key performance issue of using GANs and is an on-going
    research area. Some approaches include adding a similarity score to the generated
    data, so that potential collapse can be detected and averted, keeping a replay
    buffer of generated images around so that the discriminator doesn’t overfit onto
    just the most current batch of generated images, allowing actual labels from the
    real dataset to be added to the generator network, and so on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模式崩溃是使用GAN的关键性能问题，也是一个正在进行研究的领域。一些方法包括向生成的数据添加相似性分数，以便可以检测和避免潜在的崩溃，保持一个生成图像的重放缓冲区，以便鉴别器不会过度拟合到最新批次的生成图像，允许从真实数据集中添加实际标签到生成器网络等等。
- en: Next we round off this section by examining a GAN application that performs
    super-resolution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过检查一个执行超分辨率的GAN应用程序来结束本节。
- en: ESRGAN
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ESRGAN
- en: The *Enhanced Super-Resolution Generative Adversarial Network* (ESRGAN) is a
    network developed in 2018 that produces impressive super-resolution results. The
    generator is a series of convolutional network blocks with a combination of residual
    and dense layer connections (so a mixture of both ResNet and DenseNet), with `BatchNorm`
    layers removed as they appear to create artifacts in upsampled images. For the
    discriminator, instead of simply producing a result that says *this is real* or
    *this is fake*, it predicts a probability that a real image is relatively more
    realistic than a fake one, and this helps to make the model produce more natural
    results.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*增强超分辨率生成对抗网络*（ESRGAN）是一种在2018年开发的网络，可以产生令人印象深刻的超分辨率结果。生成器是一系列卷积网络块，其中包含残差和稠密层连接的组合（因此是ResNet和DenseNet的混合），移除了`BatchNorm`层，因为它们似乎会在上采样图像中产生伪影。对于鉴别器，它不是简单地产生一个结果，说*这是真实的*或*这是假的*，而是预测一个真实图像相对更真实的概率比一个假图像更真实，这有助于使模型产生更自然的结果。'
- en: Running ESRGAN
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行ESRGAN
- en: 'To show off ESRGAN, we’re going to download the code from the [GitHub repository](https://github.com/xinntao/ESRGAN).
    Clone that using **`git`**:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示ESRGAN，我们将从[GitHub存储库](https://github.com/xinntao/ESRGAN)下载代码。使用**`git`**克隆：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We then need to download the weights so we can use the model without training.
    Using the Google Drive link in the README, download the *RRDB_ESRGAN_x4.pth* file
    and place it in *./models*. We’re going to upsample a scaled-down version of Helvetica
    in her box, but feel free to place any image into the *./LR* directory. Run the
    supplied *test.py* script and you’ll see upsampled images being generated and
    saved into the *results* directory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要下载权重，这样我们就可以在不训练的情况下使用模型。使用自述文件中的Google Drive链接，下载*RRDB_ESRGAN_x4.pth*文件并将其放在*./models*中。我们将对Helvetica的缩小版本进行上采样，但可以随意将任何图像放入*./LR*目录。运行提供的*test.py*脚本，您将看到生成的上采样图像并保存在*results*目录中。
- en: That wraps it up for super-resolution, but we haven’t quite finished with images
    yet.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是超分辨率的全部内容，但我们还没有完成图像处理。
- en: Further Adventures in Image Detection
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像检测的进一步探索
- en: 'Our image classifications in Chapters [2](ch02.html#image-classification-with-pytorch)–[4](ch04.html#transfer-learning-and-other-tricks)
    all had one thing in common: we determined that the image belonged to a single
    class, cat or fish. And obviously, in real-world applications, that would be extended
    to a much larger set of classes. But we’d also expect images to potentially include
    both a cat and a fish (which might be bad news for the fish), or any of the classes
    we’re looking for. There might be two people in the scene, a car, and a boat,
    and we not only want to determine that they’re present in the image, but also
    *where* they are in the image. There are two main ways to do this: *object detection*
    and *segmentation*. We’ll look at both and then turn to Facebook’s PyTorch implementations
    of Faster R-CNN and Mask R-CNN to look at concrete examples.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[2](ch02.html#image-classification-with-pytorch)章到第[4](ch04.html#transfer-learning-and-other-tricks)章的图像分类都有一个共同点：我们确定图像属于一个类别，猫或鱼。显然，在实际应用中，这将扩展到一个更大的类别集。但我们也希望图像可能包含猫和鱼（这对鱼可能是个坏消息），或者我们正在寻找的任何类别。场景中可能有两个人、一辆车和一艘船，我们不仅希望确定它们是否出现在图像中，还希望确定它们在图像中的*位置*。有两种主要方法可以实现这一点：*目标检测*和*分割*。我们将看看这两种方法，然后转向Facebook的PyTorch实现的Faster
    R-CNN和Mask R-CNN，以查看具体示例。
- en: Object Detection
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测
- en: Let’s take a look at our cat in a box. What we really want is for the network
    to put the cat in a box in another box! In particular, we want a *bounding box*
    that encompasses everything in the image that the model thinks is *cat*, as seen
    in [Figure 9-7](#cat-in-a-box-in-a-bounding-box).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的盒子里的猫。我们真正想要的是让网络将猫放在另一个盒子里！特别是，我们希望有一个*边界框*，包围模型认为是*猫*的图像中的所有内容，如[图9-7](#cat-in-a-box-in-a-bounding-box)所示。
- en: '![Cat In A Box In A Bounding Box](assets/ppdl_0907.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![盒子里的猫在一个边界框中](assets/ppdl_0907.png)'
- en: Figure 9-7\. Cat in a box in a bounding box
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 盒子里的猫在一个边界框中
- en: But how can we get our networks to work this out? Remember that these networks
    can predict anything that you want them to. What if alongside our prediction of
    a class, we also produce four more outputs? In our CATFISH model, we’d have a
    `Linear` layer of output size `6` instead of `2`. The additional four outputs
    will define a rectangle using *x[1], x[2], y[1], y[2]* coordinates. Instead of
    just supplying images as training data, we’ll also have to augment them with bounding
    boxes so that the model has something to train toward, of course. Our loss function
    will now be a combined loss of the cross-entropy loss of our class prediction
    and a mean squared loss for the bounding boxes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何让我们的网络解决这个问题呢？请记住，这些网络可以预测您想要的任何内容。如果在我们的CATFISH模型中，我们除了预测一个类别之外，还产生四个额外的输出怎么样？我们将有一个输出大小为`6`的`Linear`层，而不是`2`。额外的四个输出将使用*x[1]、x[2]、y[1]、y[2]*坐标定义一个矩形。我们不仅要提供图像作为训练数据，还必须用边界框增强它们，以便模型有东西可以训练，当然。我们的损失函数现在将是类别预测的交叉熵损失和边界框的均方损失的组合损失。
- en: There’s no magic here! We just design the model to give us what we need, feed
    in data that has enough information to make and train to those predictions, and
    include a loss function that tells our network how well or badly it’s doing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有魔法！我们只需设计模型以满足我们的需求，提供具有足够信息的数据进行训练，并包含一个告诉网络它的表现如何的损失函数。
- en: An alternative to the proliferation of bounding boxes is *segmentation*. Instead
    of producing boxes, our network outputs an image mask of the same size of the
    input; the pixels in the mask are colored depending on which class they fall into.
    For example, grass could be green, roads could be purple, cars could be red, and
    so on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与边界框的泛滥相比，*分割* 是一种替代方法。我们的网络不是生成框，而是输出与输入相同大小的图像掩模；掩模中的像素根据它们所属的类别着色。例如，草可能是绿色的，道路可能是紫色的，汽车可能是红色的，等等。
- en: As we’re outputting an image, you’d be right in thinking that we’ll probably
    end up using a similar sort of architecture as in the super-resolution section.
    There’s a lot of cross-over between the two topics, and one model type that has
    become popular over the past few years is the *U-Net* architecture, shown in [Figure 9-8](#simplified-u-net-architecture).^([3](ch09.html#idm45762347677112))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在输出图像，您可能会认为我们最终会使用与超分辨率部分类似的架构。这两个主题之间存在很多交叉，近年来变得流行的一种模型类型是*U-Net*架构，如[图9-8](#simplified-u-net-architecture)所示。^([3](ch09.html#idm45762347677112))
- en: '![Simplified U-Net Architecture](assets/ppdl_0908.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![简化的U-Net架构](assets/ppdl_0908.png)'
- en: Figure 9-8\. Simplified U-Net architecture
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 简化的U-Net架构
- en: As you can see, the classic U-Net architecture is a set of convolutional blocks
    that scale down an image and another series of convolutions that scale it back
    up again to the target image. However, the key of U-Net is the lines that go across
    from the left blocks to their counterparts on the righthand side, which are concatenated
    with the output tensors as the image is scaled back up. These connections allow
    information from the higher-level convolutional blocks to transfer across, preserving
    details that might be removed as the convolutional blocks reduce the input image.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，经典的U-Net架构是一组卷积块，将图像缩小，另一系列卷积将其缩放回目标图像。然而，U-Net的关键在于从左侧块到右侧对应块的横跨线，这些线与输出张量连接在一起，当图像被缩放回来时，这些连接允许来自更高级别卷积块的信息传递，保留可能在卷积块减少输入图像时被移除的细节。
- en: You’ll find U-Net-based architectures cropping up all over Kaggle segmentation
    competitions, proving in some ways that this structure is a good one for segmentation.
    Another technique that has been applied to the basic setup is our old friend transfer
    learning. In this approach, the first part of the U is taken from a pretrained
    model such as ResNet or Inception, and the other side of the U, plus skip connections,
    are added on top of the trained network and fine-tuned as usual.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现基于U-Net的架构在Kaggle分割竞赛中随处可见，从某种程度上证明了这种结构对于分割是一个不错的选择。已经应用到基本设置的另一种技术是我们的老朋友迁移学习。在这种方法中，U的第一部分取自预训练模型，如ResNet或Inception，U的另一侧加上跳跃连接，添加到训练网络的顶部，并像往常一样进行微调。
- en: Let’s take a look at some existing pretrained models that can deliver state-of-the-art
    object detection and segmentation, direct from Facebook.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些现有的预训练模型，可以直接从Facebook获得最先进的目标检测和分割。
- en: Faster R-CNN and Mask R-CNN
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Faster R-CNN和Mask R-CNN
- en: 'Facebook Research has produced the *maskrcnn-benchmark* library, which contains
    reference implementations of both object detection and segmentation algorithms.
    We’re going to install the library and add code to generate predictions. At the
    time of this writing, the easiest way to build the models is by using Docker (this
    may change when PyTorch 1.2 is released). Clone the repository from *[*https://github.com/facebookresearch/maskrcnn-benchmark*](https://github.com/facebookresearch/maskrcnn-benchmark)*
    and add this script, *predict.py*, into the *demo* directory to set up a prediction
    pipeline using a ResNet-101 backbone:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook Research开发了*maskrcnn-benchmark*库，其中包含目标检测和分割算法的参考实现。我们将安装该库并添加代码来生成预测。在撰写本文时，构建模型的最简单方法是使用Docker（当PyTorch
    1.2发布时可能会更改）。从[*https://github.com/facebookresearch/maskrcnn-benchmark*](https://github.com/facebookresearch/maskrcnn-benchmark)克隆存储库，并将此脚本*predict.py*添加到*demo*目录中，以设置使用ResNet-101骨干的预测管道：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this short script, we’re first setting up the `COCODemo` predictor, making
    sure that we pass in the configuration that sets up Faster R-CNN instead of Mask
    R-CNN (which will produce segmented output). We then open an image file set on
    the command line, but we have to turn it into `BGR` format instead of `RGB` format
    as the predictor is trained on OpenCV images rather than the PIL images we’ve
    been using so far. Finally, we use `imsave` to write the `predictions` array (the
    original image plus bounding boxes) to a new file, also specified on the command
    line. Copy in a test image file into this *demo* directory and we can then build
    the Docker image:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的脚本中，我们首先设置了`COCODemo`预测器，确保我们传入的配置设置了Faster R-CNN而不是Mask R-CNN（后者会产生分割输出）。然后我们打开一个在命令行上设置的图像文件，但是我们必须将其转换为`BGR`格式而不是`RGB`格式，因为预测器是在OpenCV图像上训练的，而不是我们迄今为止使用的PIL图像。最后，我们使用`imsave`将`predictions`数组（原始图像加上边界框）写入一个新文件，也在命令行上指定。将一个测试图像文件复制到这个*demo*目录中，然后我们可以构建Docker镜像：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We run the script from inside the Docker container and produce output that looks
    like [Figure 9-7](#cat-in-a-box-in-a-bounding-box) (I actually used the library
    to generate that image). Try experimenting with different `confidence_threshold`
    values and different pictures. You can also switch to the `e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml`
    configuration to try out Mask R-CNN and generate segmentation masks as well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Docker容器内运行脚本，并生成类似于[图9-7](#cat-in-a-box-in-a-bounding-box)的输出（我实际上使用了该库来生成该图像）。尝试尝试不同的`confidence_threshold`值和不同的图片。您还可以切换到`e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml`配置，尝试Mask
    R-CNN并生成分割蒙版。
- en: 'To train your own data on the models, you’ll need to supply your own dataset
    that provides bounding box labels for each image. The library provides a helper
    function called `BoxList`. Here’s a skeleton implementation of a dataset that
    you could use as a starting point:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要在这些模型上训练您自己的数据，您需要提供一个数据集，为每个图像提供边界框标签。该库提供了一个名为`BoxList`的辅助函数。以下是一个数据集的骨架实现，您可以将其用作起点：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You’ll then need to add your newly created dataset to *maskrcnn_benchmark/data/datasets/*init*.py*
    and *maskrcnn_benchmark/config/paths_catalog.py*. Training can then be carried
    out using the supplied *train_net.py* script in the repo. Be aware that you may
    have to decrease the batch size to train any of these networks on a single GPU.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要将新创建的数据集添加到*maskrcnn_benchmark/data/datasets/*init*.py*和*maskrcnn_benchmark/config/paths_catalog.py*中。然后可以使用存储库中提供的*train_net.py*脚本进行训练。请注意，您可能需要减少批量大小以在单个GPU上训练这些网络中的任何一个。
- en: That wraps it up for object detection and segmentation, though see [“Further
    Reading”](#chapter-9-further-reading) for more ideas, including the wonderfully
    entitled You Only Look Once (YOLO) architecture. In the meantime, we look at how
    to maliciously break a model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是目标检测和分割的全部内容，但是请参阅[“进一步阅读”](#chapter-9-further-reading)以获取更多想法，包括标题为You Only
    Look Once（YOLO）架构的内容。与此同时，我们将看看如何恶意破坏模型。
- en: Adversarial Samples
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗样本
- en: You have probably seen articles online about images that can somehow prevent
    image recognition from working properly. If a person holds up an image to the
    camera, the neural network thinks it is seeing a panda or something like that.
    These are known as *adversarial samples*, and they’re interesting ways of discovering
    the limitations of your architectures and how best to defend against them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在网上看到过关于图像如何阻止图像识别正常工作的文章。如果一个人将图像举到相机前，神经网络会认为它看到了熊猫或类似的东西。这些被称为*对抗样本*，它们是发现架构限制以及如何最好地防御的有趣方式。
- en: 'Creating an adversarial sample isn’t too difficult, especially if you have
    access to the model. Here’s a simple neural network that classifies images from
    the popular CIFAR-10 dataset. There’s nothing special about this model, so feel
    free to swap it out for AlexNet, ResNet, or any other network presented so far
    in the book:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 创建对抗样本并不太困难，特别是如果你可以访问模型。这里有一个简单的神经网络，用于对来自流行的CIFAR-10数据集的图像进行分类。这个模型没有什么特别之处，所以可以随意将其替换为AlexNet、ResNet或本书中迄今为止介绍的任何其他网络：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once the network has been trained on CIFAR-10, we can get a prediction for the
    image in [Figure 9-9](#our-frog-example). Hopefully the training has gone well
    enough to report that it’s a frog (if not, you might want to train a little more!).
    What we’re going to do is change our picture of a frog just enough that the neural
    network gets confused and thinks it’s something else, even though we can still
    recognize that it’s clearly a frog.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络在CIFAR-10上训练完成，我们可以为[图9-9](#our-frog-example)中的图像获得预测。希望训练已经足够好，可以报告这是一只青蛙（如果没有，可能需要再多训练一会儿！）。我们要做的是稍微改变我们的青蛙图片，让神经网络感到困惑，认为它是其他东西，尽管我们仍然可以清楚地认出它是一只青蛙。
- en: '![Our frog example](assets/ppdl_0909.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![我们的青蛙示例](assets/ppdl_0909.png)'
- en: Figure 9-9\. Our frog example
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9。我们的青蛙示例
- en: To do this, we’ll use a method of attack called the *fast gradient sign method*.^([4](ch09.html#idm45762347036616))
    The idea is to take the image we want to misclassify and run it through the model
    as usual, which gives us an output tensor. Typically for predictions, we’d look
    to see which of the tensor’s values was the highest and use that as the index
    into our classes, using `argmax()`. But this time we’re going to pretend that
    we’re training the network again and backpropagate that result back through the
    model, giving us the gradient changes of the model with respect to the original
    input (in this case, our picture of a frog).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用一种名为*快速梯度符号方法*的攻击方法。^([4](ch09.html#idm45762347036616)) 这个想法是拿我们想要误分类的图像并像往常一样通过模型运行它，这给我们一个输出张量。通常情况下，对于预测，我们会查看张量中哪个值最高，并将其用作我们类的索引，使用`argmax()`。但这一次，我们将假装再次训练网络，并将结果反向传播回模型，给出模型相对于原始输入（在这种情况下，我们的青蛙图片）的梯度变化。
- en: Having done that, we create a new tensor that looks at these gradients and replaces
    an entry with +1 if the gradient is positive and –1 if the gradient is negative.
    That gives us the direction of travel that this image is pushing the model’s decision
    boundaries. We then multiply by a small scalar (called *epsilon* in the paper)
    to produce our malicious mask, which we then add to the original image, creating
    an adversarial example.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们创建一个新的张量，查看这些梯度并用+1替换一个条目，如果梯度为正则用-1。这给我们这个图像推动模型决策边界的方向。然后我们乘以一个小标量（在论文中称为*epsilon*）来生成我们的恶意掩码，然后将其添加到原始图像中，创建一个对抗样本。
- en: 'Here’s a simple PyTorch method that returns the fast gradient sign tensors
    for an input batch when supplied with the batch’s labels, plus the model and the
    loss function used to evaluate the model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的PyTorch方法，当提供批次的标签、模型和用于评估模型的损失函数时，返回输入批次的快速梯度符号张量：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Epsilon is normally found via experimentation. By playing around with various
    images, I discovered that `0.02` works well for this model, but you could also
    use something like a grid or random search to find the value that turns a frog
    into a ship!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验通常可以找到Epsilon。通过尝试各种图像，我发现`0.02`对这个模型效果很好，但你也可以使用类似网格或随机搜索的方法来找到将青蛙变成船的值！
- en: Running this function on our frog and our model, we get a mask, which we can
    then add to our original image to generate our adversarial sample. Have a look
    at [Figure 9-10](#our-adversarial-frog) to see what it looks like!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的青蛙和模型上运行这个函数，我们得到一个掩码，然后我们可以将其添加到我们的原始图像中生成我们的对抗样本。看看[图9-10](#our-adversarial-frog)看看它是什么样子！
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Our adversarial frog](assets/ppdl_0910.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![我们的对抗性青蛙](assets/ppdl_0910.png)'
- en: Figure 9-10\. Our adversarial frog
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10。我们的对抗性青蛙
- en: Clearly, our created image is still a frog to our human eyes. (If it doesn’t
    look like a frog to you, then you may be a neural network. Report yourself for
    a Voight-Kampff test immediately.) But what happens if we get a prediction from
    the model on this new image?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们创建的图像对我们的人眼来说仍然是一只青蛙。（如果对你来说看起来不像青蛙，那么你可能是一个神经网络。立即报告自己进行Voight-Kampff测试。）但如果我们从模型对这个新图像的预测中得到一个结果会发生什么？
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We have defeated the model. But is this as much of a problem as it first appears?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打败了模型。但这是否像最初看起来的那样成为问题呢？
- en: Black-Box Attacks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 黑盒攻击
- en: You may have noticed that to produce an image that fools the classifier, we
    need to know a lot about the model being used. We have the entire structure of
    the model in front of us as well as the loss function that was used in training
    the model, and we need to do forward and backward passes in the model to get our
    gradients. This is a classic example of what’s known in computer security as a
    *white-box attack*, where we can peek into any part of our code to work out what’s
    going on and exploit whatever we can find.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，要生成愚弄分类器的图像，我们需要了解使用的模型的很多信息。我们面前有整个模型的结构以及在训练模型时使用的损失函数，我们需要在模型中进行前向和后向传递以获得我们的梯度。这是计算机安全领域中所知的*白盒攻击*的一个典型例子，我们可以窥视代码的任何部分来弄清楚发生了什么并利用我们能找到的任何东西。
- en: So does this matter? After all, most models that you’ll encounter online won’t
    allow you to peek inside. Is a *black-box attack*, where all you have is the input
    and output, actually possible? Well, sadly, yes. Consider that we have a set of
    inputs, and a set of outputs to match them against. The outputs are *labels*,
    and it is possible to use targeted queries of models to train a new model that
    you can use as a local proxy and carry out attacks in a white-box manner. Just
    as you’ve seen with transfer learning, the attacks on the proxy model can work
    effectively on the actual model. Are we doomed?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这有关系吗？毕竟，大多数你在网上遇到的模型都不会让你窥视内部。*黑盒攻击*，即只有输入和输出的攻击，实际上可能吗？很遗憾，是的。考虑我们有一组输入和一组要与之匹配的输出。输出是*标签*，可以使用模型的有针对性查询来训练一个新模型，你可以将其用作本地代理并以白盒方式进行攻击。就像你在迁移学习中看到的那样，对代理模型的攻击可以有效地作用于实际模型。我们注定要失败吗？
- en: Defending Against Adversarial Attacks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御对抗性攻击
- en: How can we defend against these attacks? For something like classifying an image
    as a cat or a fish, it’s probably not the end of the world, but for self-driving
    systems, cancer-detection applications, and so forth, it could literally mean
    the difference between life and death. Successfully defending against all types
    of adversarial attacks is still an area of research, but highlights so far include
    distilling and validation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何防御这些攻击？对于像将图像分类为猫或鱼这样的任务，这可能不是世界末日，但对于自动驾驶系统、癌症检测应用等，这可能真的意味着生与死的区别。成功地防御各种类型的对抗性攻击仍然是一个研究领域，但迄今为止的重点包括提炼和验证。
- en: '*Distilling* a model by using it to train *another* model seems to help. Using
    label smoothing with the new model, as outlined earlier in this chapter, also
    seems to help. Making the model less sure of its decisions appears to smooth out
    the gradients somewhat, making the gradient-based attack we’ve outlined in this
    chapter less effective.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用模型来训练*另一个*模型来*提炼*似乎有所帮助。使用本章前面概述的新模型的标签平滑也似乎有所帮助。使模型对其决策不那么确定似乎可以在一定程度上平滑梯度，使我们在本章中概述的基于梯度的攻击不那么有效。
- en: A stronger approach is to go back to some parts of the early computer vision
    days. If we perform input validation on the incoming data, we can possibly prevent
    the adversarial image from getting to the model in the first place. In the preceding
    example, the generated attack image has a few pixels that are very out of place
    to what our eyes are expecting when we see a frog. Depending on the domain, we
    could have a filter that allows in only images that pass some filtering tests.
    You could in theory make a neural net to do that too, because then the attackers
    have to try to break two different models with the same image!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 更强大的方法是回到早期计算机视觉时代的一些部分。如果我们对传入数据进行输入验证，可能可以防止对抗性图像首先到达模型。在前面的例子中，生成的攻击图像有一些像素与我们看到青蛙时期望的非常不匹配。根据领域的不同，我们可以设置一个过滤器，只允许通过一些过滤测试的图像。你理论上也可以制作一个神经网络来做这个，因为攻击者必须尝试用相同的图像破坏两个不同的模型！
- en: Now we really are done with images. But let’s look at some developments in text-based
    networks that have occurred the past couple of years.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们真的已经结束了关于图像的讨论。但让我们看看过去几年发生的文本网络方面的一些发展。
- en: 'More Than Meets the Eye: The Transformer Architecture'
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 眼见不一定为实：变压器架构
- en: 'Transfer learning has been a big feature in allowing image-based networks to
    become so effective and prevalent over the past decade, but text has been a more
    difficult nut to crack. In the last couple of years, though, some major steps
    have been taken that are beginning to unlock the potential of using transfer learning
    in text for all sorts of tasks, such as generation, classification, and answering
    questions. We’ve also seen a new type of architecture begin to take center stage:
    the *Transformer network*. These networks don’t come from Cybertron, but the technique
    is behind the most powerful text-based networks we’ve seen, with OpenAI’s GPT-2
    model, released in 2019, showing a scarily impressive quality in its generated
    text, to the extent that OpenAI initially held back the larger version of the
    model to prevent it from being used for nefarious purposes. We look at the general
    theory of Transformer and then dive into how to use Hugging Face’s implementations
    of GPT-2 and BERT.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，迁移学习一直是使基于图像的网络变得如此有效和普遍的一个重要特征，但文本一直是一个更难解决的问题。然而，在过去几年中，已经迈出了一些重要的步骤，开始揭示了在文本中使用迁移学习的潜力，用于各种任务，如生成、分类和回答问题。我们还看到了一种新型架构开始占据主导地位：*变压器网络*。这些网络并不来自赛博特隆，但这种技术是我们看到的最强大的基于文本的网络背后的技术，OpenAI于2019年发布的GPT-2模型展示了其生成文本的惊人质量，以至于OpenAI最初推迟了模型的更大版本，以防止其被用于不良目的。我们将研究变压器的一般理论，然后深入探讨如何使用Hugging
    Face的GPT-2和BERT实现。
- en: Paying Attention
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专注
- en: The initial step along the way to the Transformer architecture was the *attention*
    mechanism, which was initially introduced to RNNs to help in sequence-to-sequence
    applications such as translation.^([5](ch09.html#idm45762346750264))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通往变压器架构的途中的初始步骤是*注意力*机制，最初引入到RNN中，以帮助序列到序列的应用，如翻译。^([5](ch09.html#idm45762346750264))
- en: The issue *attention* was trying to solve was the difficulty in translating
    sentences such as “The cat sat on the mat and she purred.” We know that *she*
    in that sentence refers to the cat, but it’s a hard concept to get a standard
    RNN to understand. It may have the hidden state that we talked about in [Chapter 5](ch05.html#text-classification),
    but by the time we get to *she*, we already have a lot of time steps and hidden
    state for each step!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意力*机制试图解决的问题是翻译句子如“猫坐在垫子上，她发出了咕噜声。”我们知道该句中的*她*指的是猫，但让标准的RNN理解这个概念很困难。它可能有我们在[第5章](ch05.html#text-classification)中讨论过的隐藏状态，但当我们到达*她*时，我们已经有了很多时间步和每个步骤的隐藏状态！'
- en: So what *attention* does is add an extra set of learnable weights attached to
    each time step that focuses the network onto a particular part of the sentence.
    The weights are normally pushed through a `softmax` layer to generate probabilities
    for each step and then the dot product of the attention weights is calculated
    with the previous hidden state. [Figure 9-11](#attention-vector) shows a simplified
    version of this with respect to our sentence.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 那么*attention*的作用是为每个时间步添加一组额外的可学习权重，将网络聚焦在句子的特定部分。这些权重通常通过`softmax`层传递，生成每个步骤的概率，然后将注意力权重的点积与先前的隐藏状态进行计算。[图9-11](#attention-vector)展示了关于我们句子的这个简化版本。
- en: '![An Attention Vector pointing to ''cat''](assets/ppdl_0911.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![指向“cat”的注意向量](assets/ppdl_0911.png)'
- en: Figure 9-11\. An attention vector pointing to *cat*
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11. 指向“cat”的注意向量
- en: The weights ensure that when the hidden state gets combined with the current
    state, *cat* will be a major part of determining the output vector at the time
    step for *she*, which will provide useful context for translating into French,
    for example!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重确保当隐藏状态与当前状态组合时，“cat”将成为决定“she”时间步输出向量的主要部分，这将为将其翻译成法语提供有用的上下文！
- en: We won’t go into all the details about how *attention* can work in a concrete
    implementation, but know the concept was powerful enough that it kickstarted the
    impressive growth and accuracy of Google Translate back in the mid-2010s. But
    more was to come.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细介绍*attention*在具体实现中如何工作，但知道这个概念足够强大，足以在2010年代中期推动了谷歌翻译的显著增长和准确性。但更多的东西即将到来。
- en: Attention Is All You Need
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制就是你需要的一切
- en: In the groundbreaking paper “Attention Is All You Need,”^([6](ch09.html#idm45762346734856))
    Google researchers pointed out that we’d spent all this time bolting attention
    onto an already slow RNN-based network (compared to CNNs or linear units, anyhow).
    What if we didn’t need the RNN after all? The paper showed that with stacked attention-based
    encoders and decoders, you could create a model that didn’t rely on the RNN’s
    hidden state at all, leading the way to the larger and faster Transformer that
    dominates textual deep learning today.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在开创性的论文“注意力就是你需要的一切”中，谷歌研究人员指出，我们花了很多时间将注意力添加到已经相对较慢的基于RNN的网络上（与CNN或线性单元相比）。如果我们根本不需要RNN呢？该论文表明，通过堆叠基于注意力的编码器和解码器，您可以创建一个完全不依赖于RNN隐藏状态的模型，为今天主导文本深度学习的更大更快的Transformer铺平了道路。
- en: The key idea was to use what the authors called *multihead attention*, which
    parallelizes the *attention* step over all the input by using a group of `Linear`
    layers. With these, and borrowing some residual connection tricks from ResNet,
    Transformer quickly began to supplant RNNs for many text-based applications. Two
    important Transformer releases, BERT and GPT-2, represent the current state-of-the-art
    as this book goes to print.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是使用作者称之为*多头注意力*，它通过使用一组`Linear`层在所有输入上并行化*attention*步骤。借助这些技巧，并从ResNet借鉴一些残差连接技巧，Transformer迅速开始取代RNN用于许多基于文本的应用。两个重要的Transformer版本，BERT和GPT-2，代表了当前的最先进技术，本书付印时。
- en: Luckily for us, there’s [a library](https://oreil.ly/xpDzq) from Hugging Face
    that implements both of them in PyTorch. It can be installed using `pip` or `conda`,
    and you should also `git clone` the repo itself, as we’ll be using some of the
    utility scripts later!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Hugging Face有一个库在PyTorch中实现了这两个模型。它可以使用`pip`或`conda`进行安装，您还应该`git clone`该存储库本身，因为我们稍后将使用一些实用脚本！
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: First, we’ll have a look at BERT.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看一下BERT。
- en: BERT
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT
- en: Google’s 2018 *Bidirectional Encoder Representations from Transformers* (BERT)
    model was one of the first successful examples of bringing transfer learning of
    a powerful model to test. BERT itself is a massive Transformer-based model (weighing
    in at 110 million parameters in its smallest version), pretrained on Wikipedia
    and the BookCorpus dataset. The issue that both Transformer and convolutional
    networks traditionally have when working with text is that because they see all
    of the data at once, it’s difficult for those networks to learn the temporal structure
    of language. BERT gets around this in its pretraining stage by masking 15% of
    the text input at random and forcing the model to predict the parts that have
    been masked. Despite being conceptually simple, the combination of the massive
    size of the 340 million parameters in the largest model with the Transformer architecture
    resulted in new state-of-the-art results for a whole series of text-related benchmarks.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌2018年的*双向编码器表示转换器*（BERT）模型是将强大模型的迁移学习成功应用的首批案例之一。BERT本身是一个庞大的基于Transformer的模型（在其最小版本中有1.1亿个参数），在维基百科和BookCorpus数据集上进行了预训练。传统上，Transformer和卷积网络在处理文本时存在的问题是，因为它们一次看到所有数据，这些网络很难学习语言的时间结构。BERT通过在预训练阶段随机屏蔽文本输入的15%，并强制模型预测已被屏蔽的部分来解决这个问题。尽管在概念上很简单，但最大模型中3.4亿个参数的庞大规模与Transformer架构的结合，为一系列与文本相关的基准测试带来了新的最先进结果。
- en: Of course, despite being created by Google with TensorFlow, there are implementations
    of BERT for PyTorch. Let’s take a quick look at one now.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，尽管BERT是由Google与TensorFlow创建的，但也有适用于PyTorch的BERT实现。现在让我们快速看一下其中一个。
- en: FastBERT
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FastBERT
- en: 'An easy way to start using the BERT model in your own classification applications
    is to use the *FastBERT* library that mixes Hugging Face’s repository with the
    fast.ai API (which you’ll see in a bit more detail when we come to ULMFiT shortly).
    It can be installed via `pip` in the usual manner:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在您自己的分类应用程序中开始使用BERT模型的一种简单方法是使用*FastBERT*库，该库将Hugging Face的存储库与fast.ai API混合在一起（稍后我们将在ULMFiT部分更详细地看到）。它可以通过常规方式使用`pip`进行安装：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here’s a script that can be used to fine-tune BERT on our Sentiment140 Twitter
    dataset that we used into [Chapter 5](ch05.html#text-classification):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个可以用来在我们在[第5章](ch05.html#text-classification)中使用的Sentiment140 Twitter数据集上微调BERT的脚本：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After our imports, we set up the `device`, `logger`, and `metrics` objects,
    which are required by the `BertLearner` object. We then create a `BERTTokenizer`
    for tokenizing our input data, and in this base we’re going to use the `bert-base-uncased`
    model (which has 12 layers and 110 million parameters). Next, we need a `BertDataBunch`
    object that contains paths to the training, validation, and test datasets, where
    to find the label column, our batch size, and the maximum length of our input
    data, which in our case is simple because it can be only the length of a tweet,
    at that time 140 characters. Having done that, we will set up a BERT model by
    using the `BertLearner.from_pretrained_model` method. This passes in our input
    data, our BERT model type, the `metric`, `device`, and `logger` objects we set
    up at the start of the script, and finally some flags to turn off training options
    that we don’t need but aren’t given defaults for the method signature.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入之后，我们设置了`device`、`logger`和`metrics`对象，这些对象是`BertLearner`对象所需的。然后我们创建了一个`BERTTokenizer`来对我们的输入数据进行标记化，在这个基础上我们将使用`bert-base-uncased`模型（具有12层和1.1亿参数）。接下来，我们需要一个包含训练、验证和测试数据集路径的`BertDataBunch`对象，以及标签列的位置、批处理大小和我们输入数据的最大长度，对于我们的情况来说很简单，因为它只能是推文的长度，那时是140个字符。做完这些之后，我们将通过使用`BertLearner.from_pretrained_model`方法来设置BERT模型。这个方法传入了我们的输入数据、BERT模型类型、我们在脚本开始时设置的`metric`、`device`和`logger`对象，最后一些标志来关闭我们不需要但方法签名中没有默认值的训练选项。
- en: Finally, the `fit()` method takes care of fine-tuning the BERT model on our
    input data, running on its own internal training loop. In this example, we’re
    training for three epochs with a learning rate of `1e-2`. The trained PyTorch
    model can be accessed afterward using `learner.model`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`fit()`方法负责在我们的输入数据上微调BERT模型，运行自己的内部训练循环。在这个例子中，我们使用学习率为`1e-2`进行三个epochs的训练。训练后的PyTorch模型可以通过`learner.model`进行访问。
- en: And that’s how to get up and running with BERT. Now, onto the competition.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何开始使用BERT。现在，进入比赛。
- en: GPT-2
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: Now, while Google was quietly working on BERT, OpenAI was working on its own
    version of a Transformer-based text model. Instead of using masking to force the
    model to learn language structure, the model constrains the attention mechanism
    within the architecture to simply predict the next word in a sequence, in a similar
    style to the RNNs in [Chapter 5](ch05.html#text-classification). As a result,
    GPT was somewhat left behind by the impressive performance of BERT, but in 2019
    OpenAI struck back with *GPT-2*, a new version of the model that reset the bar
    for text generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当谷歌悄悄地研究BERT时，OpenAI正在研究自己版本的基于Transformer的文本模型。该模型不使用掩码来强制模型学习语言结构，而是将架构内的注意机制限制在简单地预测序列中的下一个单词，类似于[第5章](ch05.html#text-classification)中的RNN的风格。结果，GPT在BERT的出色性能下有些落后，但在2019年，OpenAI推出了*GPT-2*，这是该模型的新版本，重新定义了文本生成的标准。
- en: 'The magic behind GPT-2 is scale: the model is trained on text from over 8 million
    websites, and the largest variant of GPT-2 weighs in at 1.5 billion parameters.
    And while it still doesn’t dislodge BERT on particular benchmarks for things like
    question/answering or other NLP tasks, its ability to create incredibly realistic
    text from a basic prompt led to OpenAI locking the full-size model behind closed
    doors for fear of it being weaponized. They have, however, released smaller versions
    of the model, clocking in at 117 and 340 million parameters.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2背后的魔力在于规模：该模型训练于800万个网站的文本，最大变体的GPT-2拥有15亿个参数。虽然它仍然无法在特定基准上击败BERT，比如问答或其他NLP任务，但它能够从基本提示中创建出极为逼真的文本，这导致OpenAI将全尺寸模型锁在了闭门之后，以防止被武器化。然而，他们发布了模型的较小版本，其中117和340亿个参数。
- en: 'Here’s an example of the output that GPT-2 can generate. Everything in italics
    was written by GPT-2’s 340M model:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是GPT-2可以生成的输出示例。所有斜体部分都是由GPT-2的340M模型编写的：
- en: 'Jack and Jill went up the hill *on a bike ride. The sky was a grey white and
    the wind was blowing, causing a heavy snowfall. It was really difficult to drive
    down the hill, I had to lean forward on a bit of gear to get it on. But then there
    was a moment of freedom that I would never forget: The bike was at a complete
    stop on the mountain side and I was in the middle of it. I didn’t have time to
    say a word, but I did lean forward and touch the brakes and the bike started to
    go.*'
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 杰克和吉尔骑着自行车上山。天空是灰白色的，风在吹，导致大雪纷飞。下山真的很困难，我不得不向前倾斜一点点才能继续前行。但接着有一个我永远不会忘记的自由时刻：自行车完全停在山坡上，我就在其中间。我没有时间说一句话，但我向前倾斜，触碰了刹车，自行车开始前进。
- en: Aside from switching from *Jack and Jill* to *I*, this is an impressive piece
    of text generation. For short pieces of text, it can sometimes be indistinguishable
    from human-created text. It does reveal the machine behind the curtain as the
    generated text continues, but it’s an impressive feat that could be writing tweets
    and Reddit comments right now. Let’s have a look at how to do this with PyTorch.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从*杰克和吉尔*切换到*I*，这是一个令人印象深刻的文本生成。对于短文本，它有时几乎无法与人类创作的文本区分开。随着生成文本的继续，它揭示了幕后的机器，但这是一个令人印象深刻的成就，它现在可以写推文和Reddit评论。让我们看看如何在PyTorch中实现这一点。
- en: Generating Text with GPT-2
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPT-2生成文本
- en: 'Like BERT, the official GPT-2 release from OpenAI is a TensorFlow model. Also
    like BERT, Hugging Face has released a PyTorch version that is contained within
    the same library (`pytorch-transformers`). However, a burgeoning ecosystem has
    been built around the original TensorFlow model that just doesn’t exist currently
    around the PyTorch version. So just this once, we’re going to cheat: we’re going
    to use some of the TensorFlow-based libraries to fine-tune the GPT-2 model, and
    then export the weights and import them into the PyTorch version of the model.
    To save us from too much setup, we also do all the TensorFlow operations in a
    Colab notebook! Let’s get started.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT一样，OpenAI发布的官方GPT-2版本是一个TensorFlow模型。与BERT一样，Hugging Face发布了一个PyTorch版本，该版本包含在同一个库（`pytorch-transformers`）中。然而，围绕原始TensorFlow模型构建了一个蓬勃发展的生态系统，而目前在PyTorch版本周围并不存在。因此，这一次，我们将作弊：我们将使用一些基于TensorFlow的库来微调GPT-2模型，然后导出权重并将其导入模型的PyTorch版本。为了节省我们太多的设置，我们还在Colab笔记本中执行所有TensorFlow操作！让我们开始吧。
- en: 'Open a new Google Colab notebook and install the library that we’re using,
    Max Woolf’s *gpt-2-simple*, which wraps up GPT-2 fine-tuning in a single package.
    Install it by adding this into a cell:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新的Google Colab笔记本，并安装我们正在使用的库，Max Woolf的*gpt-2-simple*，它将GPT-2微调封装在一个单一软件包中。通过将此添加到单元格中进行安装：
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next up, you need some text. In this example, I’m using a public domain text
    of PG Wodehouse’s *My Man Jeeves*. I’m also not going to do any further processing
    on the text after downloading it from the Project Gutenberg website with `wget`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要一些文本。在此示例中，我使用了PG Wodehouse的*My Man Jeeves*的公共领域文本。我还不打算在从Project Gutenberg网站使用`wget`下载文本后对文本进行任何进一步处理：
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we can use the library to train. First, make sure your notebook is connected
    to a GPU (look in Runtime→Change Runtime Type), and then run this code in a cell:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用库进行训练。首先确保您的笔记本连接到GPU（在Runtime→Change Runtime Type中查看），然后在单元格中运行此代码：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Replace the text file with whatever text file you’re using. As the model trains,
    it will spit out a sample every hundred steps. In my case, it was interesting
    to see it turn from spitting out vaguely Shakespearian play scripts to something
    that ended up approaching Wodehouse prose. This will likely take an hour or two
    to train for 1,000 epochs, so go off and do something more interesting instead
    while the cloud’s GPUs are whirring away.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 用您正在使用的文本文件替换文本文件。当模型训练时，它将每一百步输出一个样本。在我的情况下，看到它从模糊的莎士比亚剧本变成接近伍德豪斯散文的东西很有趣。这可能需要一个小时或两个小时来训练1,000个时代，所以在云端的GPU忙碌时，去做一些更有趣的事情吧。
- en: 'Once it has finished, we need to get the weights out of Colab and into your
    Google Drive account so you can download them to wherever you’re running PyTorch
    from:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们需要将权重从Colab中取出并放入您的Google Drive帐户，以便您可以将它们下载到运行PyTorch的任何地方：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: That will point you to open a new web page to copy an authentication code into
    the notebook. Do that, and the weights will be tarred up and saved to your Google
    Drive as *run1.tar.gz*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这将指引您打开一个新的网页，将认证代码复制到笔记本中。完成后，权重将被打包并保存到您的Google Drive中，文件名为*run1.tar.gz*。
- en: 'Now, on the instance or notebook where you’re running PyTorch, download that
    tarfile and extract it. We need to rename a couple of files to make these weights
    compatible with the Hugging Face reimplementation of GPT-2:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在运行PyTorch的实例或笔记本上，下载该tar文件并解压缩。我们需要重命名一些文件，使这些权重与GPT-2的Hugging Face重新实现兼容：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now need to convert the saved TensorFlow weights into ones that are compatible
    with PyTorch. Handily, the `pytorch-transformers` repo comes with a script to
    do that:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将保存的TensorFlow权重转换为与PyTorch兼容的权重。方便的是，`pytorch-transformers`存储库附带了一个脚本来执行此操作：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Creating a new instance of the GPT-2 model can then be performed in code like
    this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以在代码中创建一个新的GPT-2模型实例：
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Or, just to play around with the model, you can use the *run_gpt2.py* script
    to get a prompt where you enter text and get generated samples back from the PyTorch-based
    model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，只是为了玩弄模型，您可以使用*run_gpt2.py*脚本获得一个提示，输入文本并从基于PyTorch的模型获取生成的样本：
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Training GPT-2 is likely to become easier in the coming months as Hugging Face
    incorporates a consistent API for all the models in its repo, but the TensorFlow
    method is the easiest to get started with right now.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Hugging Face在其存储库中整合所有模型的一致API，训练GPT-2可能会变得更加容易，但目前使用TensorFlow方法是最容易入门的。
- en: 'BERT and GPT-2 are the most popular names in text-based learning right now,
    but before we wrap up, we cover the dark horse of the current state-of-the-art
    models: ULMFiT.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和GPT-2目前是文本学习中最流行的名称，但在我们结束之前，我们将介绍当前最先进模型中的黑马：ULMFiT。
- en: ULMFiT
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ULMFiT
- en: In contrast to the behemoths of BERT and GPT-2, *ULMFiT* is based on a good
    old RNN. No Transformer in sight, just the AWD-LSTM, an architecture originally
    created by Stephen Merity. Trained on the WikiText-103 dataset, it has proven
    to be amendable to transfer learning, and despite the *old* type of architecture,
    has proven to be competitive with BERT and GPT-2 in the classification realm.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT和GPT-2这两个庞然大物相比，*ULMFiT*基于一个老式的RNN。看不到Transformer，只有AWD-LSTM，这是由Stephen
    Merity最初创建的架构。在WikiText-103数据集上训练，它已被证明适合迁移学习，尽管是*老式*的架构，但在分类领域已被证明与BERT和GPT-2具有竞争力。
- en: While ULMFiT is, at heart, just another model that can be loaded and used in
    PyTorch like any other, its natural home is within the fast.ai library, which
    sits on top of PyTorch and provides many useful abstractions for getting to grips
    with and being productive with deep learning quickly. To that end, we’ll look
    at how to use ULMFiT with the fast.ai library on the Twitter dataset we used in
    [Chapter 5](ch05.html#text-classification).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ULMFiT本质上只是另一个可以像其他模型一样在PyTorch中加载和使用的模型，但它的自然家园是fast.ai库，该库位于PyTorch之上，并为快速掌握深度学习并快速提高生产力提供了许多有用的抽象。为此，我们将看看如何在Twitter数据集上使用fast.ai库中的ULMFiT，该数据集在[第5章](ch05.html#text-classification)中使用过。
- en: 'We first use fast.ai’s Data Block API to prepare our data for fine-tuning the
    LSTM:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用fast.ai的Data Block API为微调LSTM准备数据：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is fairly similar to the `torchtext` helpers from [Chapter 5](ch05.html#text-classification)
    and just produces what fast.ai calls a `databunch`, from which its models and
    training routines can easily grab data. Next, we create the model, but in fast.ai,
    this happens a little differently. We create a `learner` that we interact with
    to train the model instead of the model itself, though we pass that in as a parameter.
    We also supply a dropout value (we’re using the one suggested in the fast.ai training
    materials):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这与[第5章](ch05.html#text-classification)中的`torchtext`助手非常相似，只是产生了fast.ai称为`databunch`的东西，从中其模型和训练例程可以轻松获取数据。接下来，我们创建模型，但在fast.ai中，这种情况有些不同。我们创建一个`learner`，与之交互以训练模型，而不是模型本身，尽管我们将其作为参数传递。我们还提供了一个dropout值（我们使用了fast.ai培训材料中建议的值）：
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once we have our `learner` object, we can find the optimal learning rate. This
    is just like what we implemented in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    except that it’s built into the library and uses an exponentially moving average
    to smooth out the graph, which in our implementation is pretty spiky:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`learner`对象，我们可以找到最佳学习率。这与我们在[第4章](ch04.html#transfer-learning-and-other-tricks)中实现的类似，只是它内置在库中，并使用指数移动平均值来平滑图表，我们的实现中相当尖锐：
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: From the plot in [Figure 9-12](#ULMFiT-learning-rate-plot), it looks like `1e-2`
    is where we’re starting to hit a steep decline, so we’ll pick that as our learning
    rate. Fast.ai uses a method called `fit_one_cycle`, which uses a 1cycle learning
    scheduler (see [“Further Reading”](#chapter-9-further-reading) for more details
    on 1cycle) and very high learning rates to train a model in an order of magnitude
    fewer epochs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图9-12](#ULMFiT-learning-rate-plot)中的图表来看，`1e-2`是我们开始出现急剧下降的地方，因此我们将选择它作为我们的学习率。Fast.ai使用一种称为`fit_one_cycle`的方法，它使用1cycle学习调度器（有关1cycle的更多详细信息，请参见[“进一步阅读”](#chapter-9-further-reading)），并使用非常高的学习率在数量级更少的时代内训练模型。
- en: '![ULMFiT learning rate plot](assets/ppdl_0912.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![ULMFiT学习率图](assets/ppdl_0912.png)'
- en: Figure 9-12\. ULMFiT learning rate plot
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12. ULMFiT学习率图
- en: 'Here, we’re training for just one cycle and saving the fine-tuned head of the
    network (the *encoder*):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只训练一个周期，并保存网络的微调头部（*编码器*）：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With the fine-tuning of the language model completed (you may want to experiment
    with more cycles in training), we build a new `databunch` for the actual classification
    problem:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型的微调完成（您可能希望在训练中尝试更多周期），我们为实际分类问题构建了一个新的`databunch`：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The only real difference here is that we supply the actual labels by using
    `label_from_df` and we pass in a `vocab` object from the language model training
    that we performed earlier to make sure they’re using the same mapping of words
    to numbers, and then we’re ready to create a new `text_classifier_learner`, where
    the library does all the model creation for you behind the scenes. We load the
    fine-tuned encoder onto this new model and begin the process of training again:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的真正区别是，我们通过使用`label_from_df`提供实际标签，并且从之前执行的语言模型训练中传入一个`vocab`对象，以确保它们使用相同的单词到数字的映射，然后我们准备创建一个新的`text_classifier_learner`，其中库在幕后为您创建所有模型。我们将微调的编码器加载到这个新模型上，并开始再次进行训练过程：
- en: '[PRE34]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: And with a tiny amount of code, we have a classifier that reports an accuracy
    of 76%. We could easily improve that by training the language model for more cycles,
    adding differential learning rates and freezing parts of the model while training,
    all of which fast.ai supports with methods defined on the `learner`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过少量代码，我们得到了一个报告准确率为76%的分类器。我们可以通过训练语言模型更多周期，添加不同的学习率并在训练时冻结部分模型来轻松改进，所有这些都是fast.ai支持的，定义在`learner`上的方法。
- en: What to Use?
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用什么？
- en: 'Given that little whirlwind tour of the current cutting edge of text models
    in deep learning, there’s probably one question on your mind: “That’s all great,
    but which one should I actually *use*?” In general, if you’re working on a classification
    problem, I suggest you start with ULMFiT. BERT is impressive, but ULMFiT is competitive
    with BERT in terms of accuracy, and it has the additional benefit that you don’t
    need to buy a huge number of TPU credits to get the best out of it. A single GPU
    fine-tuning ULMFiT is likely to be enough for most people.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习文本模型的当前前沿进行了一番快速的介绍后，您可能心中有一个问题：“这一切都很棒，但我应该实际*使用*哪一个？”一般来说，如果您正在处理分类问题，我建议您从ULMFiT开始。BERT令人印象深刻，但在准确性方面，ULMFiT与BERT竞争，并且它还有一个额外的好处，即您不需要购买大量的TPU积分来充分利用它。对于大多数人来说，单个GPU微调ULMFiT可能已经足够了。
- en: And as for GPT-2, if you’re after generated text, then yes, it’s a better fit,
    but for classification purposes, it’s going to be harder to approach ULMFiT or
    BERT performance. One thing that I do think might be interesting is to let GPT-2
    loose on data augmentation; if you have a dataset like Sentiment140, which we’ve
    been using throughout this book, why not fine-tune a GPT-2 model on that input
    and use it to generate more data?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 至于GPT-2，如果您想要生成的文本，那么是的，它更适合，但对于分类目的，接近ULMFiT或BERT的性能会更难。我认为可能有趣的一件事是让GPT-2在数据增强上自由发挥；如果您有一个类似Sentiment140的数据集，我们在整本书中一直在使用它，为什么不在该输入上微调一个GPT-2模型并使用它生成更多数据呢？
- en: Conclusion
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This chapter looked at the wider world of PyTorch, including libraries with
    existing models that you can import into your own projects, some cutting-edge
    data augmentation approaches that can be applied to any domain, as well as adversarial
    samples that can ruin your model’s day and how to defend against them. I hope
    that as we come to the end of our journey, you understand how neural networks
    are assembled and how to get images, text, and audio to flow through them as tensors.
    You should be able to train them, augment data, experiment with learning rates,
    and even debug models when they’re not going quite right. And once all that’s
    done, you know how to package them up in Docker and get them serving requests
    from the wider world.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了PyTorch的更广泛世界，包括可以导入到自己项目中的现有模型的库，一些可应用于任何领域的尖端数据增强方法，以及可能破坏模型的对抗样本以及如何防御它们。希望当我们结束这段旅程时，你能理解神经网络是如何组装的，以及如何让图像、文本和音频作为张量流经它们。你应该能够训练它们，增强数据，尝试不同的学习率，并在模型出现问题时进行调试。一旦所有这些都完成了，你就知道如何将它们打包到Docker中，并让它们为更广泛的世界提供服务。
- en: Where do we go from here? Consider having a look at the PyTorch forums and the
    other documentation on the website. I definitely also recommend visiting the fast.ai
    community even if you don’t end up using the library; it’s a hive of activity,
    filled with good ideas and people experimenting with new approaches, while also
    friendly to newcomers!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们去哪里？考虑查看PyTorch论坛和网站上的其他文档。我强烈推荐访问fast.ai社区，即使你最终不使用该库；这是一个充满活力的社区，充满了好主意和尝试新方法的人，同时也对新手友好！
- en: Keeping up with the cutting edge of deep learning is becoming harder and harder.
    Most papers are published on [arXiv](https://arxiv.org), but the rate of papers
    being published seems to be rising at an almost exponential level; as I was typing
    up this conclusion, [XLNet](https://arxiv.org/abs/1906.08237) was released, which
    apparently beats BERT on various tasks. It never ends! To try to help in this,
    I listed a few Twitter accounts here where people often recommend interesting
    papers. I suggest following them to get a taste of current and interesting work,
    and from there you can perhaps use a tool such as [arXiv Sanity Preserver](http://arxiv-sanity.com)
    to drink from the firehose when you feel more comfortable diving in.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 跟上深度学习的前沿变得越来越困难。大多数论文都发表在[arXiv](https://arxiv.org)，但论文的发表速度似乎以近乎指数级增长；当我写这个结论时，[XLNet](https://arxiv.org/abs/1906.08237)刚刚发布，据说在各种任务上击败了BERT。永无止境！为了帮助解决这个问题，我在这里列出了一些Twitter账号，人们经常推荐有趣的论文。我建议关注它们，以了解当前和有趣的工作，然后你可以使用工具如[arXiv
    Sanity Preserver](http://arxiv-sanity.com)来更轻松地深入研究。
- en: 'Finally, I trained a GPT-2 model on the book and it would like to say a few
    words:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我在这本书上训练了一个GPT-2模型，它想说几句话：
- en: Deep learning *is a key driver of how we work on today’s deep learning applications,
    and deep learning is expected to continue to expand into new fields such as image-based
    classification and in 2016, NVIDIA introduced the CUDA LSTM architecture. With
    LSTMs now becoming more popular, LSTMs were also a cheaper and easier to produce
    method of building for research purposes, and CUDA has proven to be a very competitive
    architecture in the deep learning market.*
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度学习*是我们如何处理当今深度学习应用的关键驱动力，预计深度学习将继续扩展到新领域，如基于图像的分类，在2016年，NVIDIA推出了CUDA LSTM架构。随着LSTM变得越来越流行，LSTM也成为了一种更便宜、更易于生产的用于研究目的的构建方法，而CUDA已经证明在深度学习市场上是一种非常有竞争力的架构。*
- en: Thankfully, you can see there’s still a way to go before we authors are out
    of a job. But maybe you can help change that!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你可以看到在我们作者失业之前还有很长的路要走。但也许你可以帮助改变这一点！
- en: Further Reading
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: A survey of [current super-resolution techniques](https://arxiv.org/pdf/1902.06068.pdf)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一份[当前超分辨率技术的调查](https://arxiv.org/pdf/1902.06068.pdf)
- en: Ian Goodfellow’s [lecture on GANs](https://www.youtube.com/watch?v=Z6rxFNMGdn0)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ian Goodfellow的[GAN讲座](https://www.youtube.com/watch?v=Z6rxFNMGdn0)
- en: '[You Only Look Once (YOLO)](https://pjreddie.com/darknet/yolo), a family of
    fast object detection models with highly readable papers'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[You Only Look Once (YOLO)](https://pjreddie.com/darknet/yolo) —— 一系列快速目标检测模型，具有非常易读的论文'
- en: '[CleverHans](https://github.com/tensorflow/cleverhans), a library of adversarial
    generation techniques for TensorFlow and PyTorch'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CleverHans](https://github.com/tensorflow/cleverhans) —— 一个为TensorFlow和PyTorch提供对抗生成技术的库'
- en: '[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer),
    an in-depth voyage through the Transformer architecture'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer)
    —— 通过Transformer架构进行深入探索'
- en: 'Some Twitter accounts to follow:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一些推荐关注的Twitter账号：
- en: '*@jeremyphoward*—Cofounder of fast.ai'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*@jeremyphoward* —— fast.ai的联合创始人'
- en: '*@miles_brundage*—Research scientist (policy) at OpenAI'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*@miles_brundage* —— OpenAI的研究科学家（政策）'
- en: '*@BrundageBot*—Twitter bot that generates a daily summary of interesting papers
    from arXiv (warning: often tweets out 50 papers a day!)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*@BrundageBot* —— 一个每天生成有趣论文摘要的Twitter机器人（警告：通常每天推出50篇论文！）'
- en: '*@pytorch*—Official PyTorch account'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*@pytorch* —— 官方PyTorch账号'
- en: '^([1](ch09.html#idm45762349177880-marker)) See [“mixup: Beyond Empirical Risk
    Minimization”](https://arxiv.org/abs/1710.09412) by Hongyi Zhang et al. (2017).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#idm45762349177880-marker)) 请查看张宏毅等人（2017年）的论文“混合：超越经验风险最小化”。
- en: ^([2](ch09.html#idm45762348101832-marker)) See [“Generative Adversarial Networks”](https://arxiv.org/abs/1406.2661)
    by Ian J. Goodfellow et al. (2014).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.html#idm45762348101832-marker)) 请查看Ian J. Goodfellow等人（2014年）的论文“生成对抗网络”。
- en: '^([3](ch09.html#idm45762347677112-marker)) See [“U-Net: Convolutional Networks
    for Biomedical Image Segmentation”](https://arxiv.org/abs/1505.04597) by Olaf
    Ronneberger et al. (2015).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.html#idm45762347677112-marker)) 请查看Olaf Ronneberger等人（2015年）的论文“U-Net：用于生物医学图像分割的卷积网络”。
- en: ^([4](ch09.html#idm45762347036616-marker)) See [“Explaining and Harnessing Adversarial
    Examples”](https://arxiv.org/abs/1412.6572) by Ian Goodfellow et al. (2014).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.html#idm45762347036616-marker)) 请参阅Ian Goodfellow等人撰写的“解释和利用对抗样本”（2014年）。
- en: ^([5](ch09.html#idm45762346750264-marker)) See [“Neural Machine Translation
    by Jointly Learning to Align and Translate”](https://arxiv.org/abs/1409.0473)
    by Dzmitry Bahdanau et al. (2014).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.html#idm45762346750264-marker)) 请参阅Dzmitry Bahdanau等人撰写的“通过联合学习对齐和翻译进行神经机器翻译”（2014年）。
- en: ^([6](ch09.html#idm45762346734856-marker)) See [“Attention Is All You Need”](https://arxiv.org/abs/1706.03762)
    by Ashish Vaswani et al. (2017).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.html#idm45762346734856-marker)) 请参阅Ashish Vaswani等人撰写的“注意力机制就是一切”（2017年）。
