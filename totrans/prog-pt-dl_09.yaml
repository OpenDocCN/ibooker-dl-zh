- en: Chapter 9\. PyTorch in the Wild
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our final chapter, we’ll look at how PyTorch is used by other people and
    companies. You’ll also learn some new techniques along the way, including resizing
    pictures, generating text, and creating images that can fool neural networks.
    In a slight change from earlier chapters, we’ll be concentrating on how to get
    up and running with existing libraries rather than starting from scratch in PyTorch.
    I’m hoping that this will be a springboard for further exploration.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by examining some of the latest approaches for squeezing the most
    out of your data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Augmentation: Mixed and Smoothed'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Way back in [Chapter 4](ch04.html#transfer-learning-and-other-tricks), we looked
    at various ways of augmenting data to help reduce the model overfitting on the
    training dataset. The ability to do more with less data is naturally an area of
    high activity in deep learning research, and in this section we’ll look at two
    increasingly popular ways to squeeze every last drop of signal from your data.
    Both approaches will also see us changing how we calculate our loss function,
    so it will be a good test of the more flexible training loop that we just created.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: mixup
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*mixup* is an intriguing augmentation technique that arises from looking askew
    at what we want our model to do. Our normal understanding of a model is that we
    send it an image like the one in [Figure 9-1](#a-fox) and want the model to return
    a result that the image is a fox.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![A fox](assets/ppdl_0901.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A fox
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But as you know, we don’t get just that from the model; we get a tensor of all
    the possible classes and, hopefully, the element of that tensor with the highest
    value is the *fox* class. In fact, in the ideal scenario, we’d have a tensor that
    is all 0s except for a 1 in the fox class.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Except that is difficult for a neural network to do! There’s always going to
    be uncertainty, and our activation functions like `softmax` make it difficult
    for the tensors to get to 1 or 0\. mixup takes advantage of this by asking a question:
    what is the class of [Figure 9-2](#a-mixture-of-cat-and-fox)?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![A mixture of cat and fox](assets/ppdl_0902.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. A mixture of cat and fox
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To our eyes, this may be a bit of a mess, but it is 60% cat and 40% fox. What
    if, instead of trying to make our model make a definitive guess, we could make
    it target two classes? This would mean that our output tensor won’t run into the
    problem of approaching but never reaching 1 in training, and we could alter each
    *mixed* image by a different fraction, improving our model’s ability to generalize.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'But how do we calculate the loss function of this mixed-up image? Well, if
    *p* is the percentage of the first image in the mixed image, then we have a simple
    linear combination of the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It has to predict those images, right? And we need to scale according to how
    much of those images is in the final mixed image, so this new loss function seems
    reasonable. To choose *p*, we could just use random numbers drawn from a normal
    or uniform distribution as we would do in many other cases. However, the writers
    of the mixup paper determined that samples drawn from the *beta* distribution
    work out much better in practice.^([1](ch09.html#idm45762349177880)) Don’t know
    what the beta distribution looks like? Well, neither did I until I saw this paper!
    [Figure 9-3](#beta-distribution) shows how it looks when given the characteristics
    described in the paper.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Beta Distribution where ⍺ = β](assets/ppdl_0903.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Beta distribution, where ⍺ = β
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The U-shape is interesting because it tells us that most of the time, our mixed
    image will be mainly one image or another. Again, this makes intuitive sense as
    we can imagine the network is going to have a harder time working out a 50/50
    mixup than a 90/10 one.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a modified training loop that takes a new additional data loader, `mix_loader`,
    and mixes the batches together:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What’s happening here is after we get our two batches, we use `torch.distribution.Beta`
    to generate a series of mix parameters, using the `expand` method to produce a
    tensor of `[1, batch_size]`. We could iterate through the batch and generate the
    parameters one by one, but this is neater, and remember, GPUs love matrix multiplication,
    so it’ll end up being faster to do all the calculations across the batch at once
    (this is shown in [Chapter 7](ch07.html#debugging-pytorch-models) when fixing
    our `BadRandom` transformation, remember!). We multiply the entire batch by this
    tensor, and then the batch to mix in by `1 - mix_factor_tensor` using broadcasting
    (which we covered in [Chapter 1](ch01.html#getting-started-with-pytorch)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: We then take the losses of the predictions against our targets for both images,
    and our final loss is the mean of the sum of those losses. What’s happening there?
    Well, if you look at the source code for `CrossEntropyLoss`, you’ll see the comment
    `The losses are averaged across observations for each minibatch.` There’s also
    a `reduction` parameter that has a default set to `mean` (we’ve used the default
    so far, so that’s why you haven’t seen it before!). We need to preserve that condition,
    so we take the mean of our combined losses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, having two data loaders isn’t too much trouble, but it does make the code
    a little more complicated. If you run this code, you might error out because the
    batches are not balanced as final batches come out of the loaders, meaning that
    you’ll have to write extra code to handle that case. The authors of the mixup
    paper suggest that you could replace the mix data loader with a random shuffle
    of the incoming batch. We can do this with `torch.randperm()`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When using mixup in this way, be aware that you are much more likely to get
    *collisions* where you end up applying the same parameter to the same set of images,
    potentially reducing the accuracy of training. For example, you could have cat1
    mixed with fish1, and draw a beta parameter of 0.3\. Then later in the same batch,
    you pull out fish1 and it gets mixed with cat1 with a parameter of 0.7—making
    it the same mix! Some implementations of mixup—in particular, the fast.ai implementation—resolve
    this issue by replacing our mix parameters with the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This ensures that the nonshuffled batch will always have the highest component
    when being merged with the mix batch, thus eliminating that potential issue.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Oh, and one more thing: we performed the mixup transformation *after* our image
    transformation pipeline. At this point, our batches are just tensors that we’ve
    added together. This means that there’s no reason mixup training should be restricted
    to images. We could use it on any type of data that’s been transformed into tensors,
    whether text, image, audio, or anything else.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'We can still do a little more to make our labels work harder for us. Enter
    another approach that is now a mainstay of state-of-the-art models: *label smoothing*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Label Smoothing
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a similar manner to mixup, *label smoothing* helps to improve model performance
    by making the model less sure of its predictions. Instead of trying to force it
    to predict `1` for the predicted class (which has all the problems we talked about
    in the previous section), we instead alter it to predict 1 minus a small value,
    *epsilon*. We can create a new loss function implementation that wraps up our
    existing `CrossEntropyLoss` function with this functionality. As it turns out,
    writing a custom loss function is just another subclass of `nn.Module`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When it comes to computing the loss function, we calculate the cross-entropy
    loss as per the implementation of `CrossEntropyLoss`. Our `final_loss` is constructed
    from negative log-likelihood being multiplied by 1 minus epsilon (our *smoothed*
    label) added to the loss multiplied by epsilon divided by the number of classes.
    This occurs because we are smoothing not only the label for the predicted class
    to be 1 minus epsilon, but also the other labels so that they’re not being forced
    to zero, but instead a value between zero and epsilon.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失函数时，我们按照`CrossEntropyLoss`的实现计算交叉熵损失。我们的`final_loss`由负对数似然乘以1减epsilon（我们的*平滑*标签）加上损失乘以epsilon除以类别数构成。这是因为我们不仅将预测类别的标签平滑为1减epsilon，还将其他标签平滑为不是被迫为零，而是在零和epsilon之间的值。
- en: This new custom loss function can replace `CrossEntropyLoss` in training anywhere
    we’ve used it in the book, and when combined with mixup, it is an incredibly effective
    way of getting that little bit more from your input data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的自定义损失函数可以替代书中任何地方使用的`CrossEntropyLoss`进行训练，并与mixup结合使用，是从输入数据中获得更多的一种非常有效的方法。
- en: 'We’ll now turn away from data augmentation to have a look at another hot topic
    in current deep learning trends: generative adversarial networks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从数据增强转向另一个当前深度学习趋势中的热门话题：生成对抗网络。
- en: Computer, Enhance!
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机，增强！
- en: One odd consequence of the increasing power of deep learning is that for decades,
    we computer people have been mocking television crime shows that have a detective
    click a button to make a blurry camera image suddenly become a sharp, in-focus
    picture. How we laughed and cast derision on shows like CSI for doing this. Except
    we can now actually do this, at least up to a point. Here’s an example of this
    witchcraft, on a smaller 256 × 256 image scaled to 512 × 512, in Figures [9-4](#postbox-at-256)
    and [9-5](#postbox-enhanced).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习能力不断增强的一个奇怪后果是，几十年来，我们计算机人一直在嘲笑那些电视犯罪节目，其中侦探点击按钮，使模糊的摄像头图像突然变得清晰、聚焦。我们曾经嘲笑和嘲弄CSI等节目做这种事情。但现在我们实际上可以做到这一点，至少在一定程度上。这里有一个巫术的例子，将一个较小的256×256图像缩放到512×512，见图[9-4](#postbox-at-256)和[9-5](#postbox-enhanced)。
- en: '![Postbox at 256x256 resolution](assets/ppdl_0904.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![256x256分辨率下的邮箱](assets/ppdl_0904.png)'
- en: Figure 9-4\. Mailbox at 256 × 256 resolution
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4. 256×256分辨率下的邮箱
- en: '![ESRGAN-enhanched postbox at 512x512 resolution](assets/ppdl_0905.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![512x512分辨率下的ESRGAN增强邮箱](assets/ppdl_0905.png)'
- en: Figure 9-5\. ESRGAN-enhanced mailbox at 512 × 512 resolution
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5. 512×512分辨率下的ESRGAN增强邮箱
- en: The neural network learns how to *hallucinate* new details to fill in what’s
    not there, and the effect can be impressive. But how does this work?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学习如何*幻想*新的细节来填补不存在的部分，效果可能令人印象深刻。但这是如何工作的呢？
- en: Introduction to Super-Resolution
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超分辨率简介
- en: 'Here’s the first part of a very simple super-resolution model. To start, it’s
    pretty much exactly the same as any model you’ve seen so far:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的超分辨率模型的第一部分。起初，它几乎与你迄今为止看到的任何模型完全相同：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we pass a random tensor through the network, we end up with a tensor of
    shape `[1, 256, 62, 62]`; the image representation has been compressed into a
    much smaller vector. Let’s now introduce a new layer type, `torch.nn.ConvTranspose2d`.
    You can think of this as a layer that inverts a standard `Conv2d` transform (with
    its own learnable parameters). We’ll add a new `nn.Sequential` layer, `upsample`,
    and put in a simple list of these new layers and `ReLU` activation functions.
    In the `forward()` method, we pass input through that consolidated layer after
    the others:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过网络传递一个随机张量，我们最终得到一个形状为`[1, 256, 62, 62]`的张量；图像表示已经被压缩为一个更小的向量。现在让我们引入一个新的层类型，`torch.nn.ConvTranspose2d`。你可以将其视为一个反转标准`Conv2d`变换的层（具有自己的可学习参数）。我们将添加一个新的`nn.Sequential`层，`upsample`，并放入一系列这些新层和`ReLU`激活函数。在`forward()`方法中，我们将输入通过其他层后通过这个整合层：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you now test the model with a random tensor, you’ll get back a tensor of
    exactly the same size that went in! What we’ve built here is known as an *autoencoder*,
    a type of network that rebuilds its input, usually after compressing it into a
    smaller dimension. That is what we’ve done here; the `features` sequential layer
    is an *encoder* that transforms an image into a tensor of size `[1, 256, 62, 62]`,
    and the `upsample` layer is our *decoder* that turns it back into the original
    shape.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在用一个随机张量测试模型，你将得到一个与输入完全相同大小的张量！我们构建的是一个*自动编码器*，一种网络类型，通常在将其压缩为更小维度后重新构建其输入。这就是我们在这里做的；`features`顺序层是一个*编码器*，将图像转换为大小为`[1,
    256, 62, 62]`的张量，`upsample`层是我们的*解码器*，将其转换回原始形状。
- en: Our labels for training the image would, of course, be our input images, but
    that means we can’t use loss functions like our fairly standard `CrossEntropyLoss`,
    because, well, we don’t have classes! What we want is a loss function that tells
    us how different our output image is from our input image, and for that, taking
    the mean squared loss or mean absolute loss between the pixels of the image is
    a common approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练图像的标签当然是我们的输入图像，但这意味着我们不能使用像我们相当标准的`CrossEntropyLoss`这样的损失函数，因为，嗯，我们没有类别！我们想要的是一个告诉我们输出图像与输入图像有多大不同的损失函数，为此，计算图像像素之间的均方损失或均绝对损失是一种常见方法。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although calculating the loss in terms of pixels makes a lot of sense, it turns
    out that a lot of the most successful super-resolution networks use augmented
    loss functions that try to capture how much a generated image looks like the original,
    tolerating pixel loss for better performance in areas like texture and content
    loss. Some of the papers listed in [“Further Reading”](#chapter-9-further-reading)
    go into deeper detail.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以像素为单位计算损失非常合理，但事实证明，许多最成功的超分辨率网络使用增强损失函数，试图捕捉生成图像与原始图像的相似程度，容忍像素损失以获得更好的纹理和内容损失性能。一些列在[“进一步阅读”](#chapter-9-further-reading)中的论文深入讨论了这一点。
- en: Now that gets us back to the same size input we entered, but what if we add
    another transposed convolution to the mix?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这使我们回到了与输入相同大小的输入，但如果我们在其中添加另一个转置卷积会怎样呢？
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Try it! You should find that the output tensor is twice as big as the input.
    If we have access to a set of ground truth images at that size to act as labels,
    we can train the network to take in images at a size *x* and produce images for
    a size *2x*. In practice, we tend to perform this upsampling by scaling up twice
    as much as we need to and then adding a standard convolutional layer, like so:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 试试吧！您会发现输出张量是输入的两倍大。如果我们有一组与该大小相同的地面真实图像作为标签，我们可以训练网络以接收大小为*x*的图像并为大小为*2x*的图像生成图像。在实践中，我们倾向于通过扩大两倍所需的量，然后添加一个标准的卷积层来执行这种上采样，如下所示：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We do this because the transposed convolution has a tendency to add jaggies
    and moiré patterns as it expands the image. By expanding twice and then scaling
    back down to our required size, we hopefully provide enough information to the
    network to smooth those out and make the output look more realistic.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是因为转置卷积有添加锯齿和moire图案的倾向，因为它扩展图像。通过扩展两次，然后缩小到我们需要的大小，我们希望为网络提供足够的信息来平滑这些图案，并使输出看起来更真实。
- en: Those are the basics behind super-resolution. Most current high-performing super-resolution
    networks are trained with a technique called the generative adversarial network,
    which has stormed the deep learning world in the past few years.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是超分辨率背后的基础。目前大多数性能优越的超分辨率网络都是使用一种称为生成对抗网络的技术进行训练的，这种技术在过去几年中席卷了深度学习世界。
- en: An Introduction to GANs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs简介
- en: One of the universal problems in deep learning (or any machine learning application)
    is the cost of producing labeled data. In this book, we’ve mostly avoided the
    problem by using sample datasets that are all carefully labeled (even some that
    come prepackaged in easy training/validation/test sets!). But in the real world
    producing large quantities of labeled data. Indeed, techniques that you’ve learned
    a lot about so far, like transfer learning, have all been about doing more with
    less. But sometimes you need more, and *generative adversarial networks* (GANs)
    have a way to help.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（或任何机器学习应用）中的一个普遍问题是产生标记数据的成本。在本书中，我们大多数情况下通过使用精心标记的样本数据集来避免这个问题（甚至一些预先打包的易于训练/验证/测试集！）。但在现实世界中，产生大量标记数据。确实，到目前为止，您学到的技术，如迁移学习，都是关于如何用更少的资源做更多的事情。但有时您需要更多，*生成对抗网络*（GANs）有办法帮助。
- en: GANs were introduced by Ian Goodfellow in a 2014 paper and are a novel way of
    providing more data to help train neural networks. And the approach is mainly
    “we know you love neural networks, so we added another.”^([2](ch09.html#idm45762348101832))
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是由Ian Goodfellow在2014年的一篇论文中提出的，是一种提供更多数据以帮助训练神经网络的新颖方法。而这种方法主要是“我们知道你喜欢神经网络，所以我们添加了另一个。”
- en: The Forger and the Critic
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伪造者和评论家
- en: The setup of a GAN is as follows. Two neural networks are trained together.
    The first is the *generator*, which takes random noise from the vector space of
    the input tensors and produces fake data as output. The second network is the
    *discriminator*, which alternates between the generated fake data and real data.
    Its job is to look at the incoming inputs and decide whether they’re real or fake.
    A simple conceptual diagram of a GAN is shown in [Figure 9-6](#a-simple-gan-setup).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的设置如下。两个神经网络一起训练。第一个是*生成器*，它从输入张量的向量空间中获取随机噪声，并产生虚假数据作为输出。第二个网络是*鉴别器*，它在生成的虚假数据和真实数据之间交替。它的工作是查看传入的输入并决定它们是真实的还是虚假的。GAN的简单概念图如[图9-6](#a-simple-gan-setup)所示。
- en: '![A simple GAN setup](assets/ppdl_0906.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的GAN设置](assets/ppdl_0906.png)'
- en: Figure 9-6\. A simple GAN setup
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6。一个简单的GAN设置
- en: 'The great thing about GANs is that although the details end up being somewhat
    complicated, the general idea is easy to convey: the two networks are in opposition
    to each other, and during training they work as hard as they can to defeat the
    other. By the end of the process, the *generator* should be producing data that
    matches the *distribution* of the real input data to flummox the *discriminator*.
    And once you get to that point, you can use the generator to produce more data
    for all your needs, while the discriminator presumably retires to the neural network
    bar to drown its sorrows.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的伟大之处在于，尽管细节最终变得有些复杂，但总体思想很容易传达：这两个网络相互对立，在训练过程中，它们尽力击败对方。在过程结束时，*生成器*应该生成与真实输入数据的*分布*匹配的数据，以迷惑*鉴别器*。一旦达到这一点，您可以使用生成器为所有需求生成更多数据，而鉴别器可能会退休到神经网络酒吧淹没忧愁。
- en: Training a GAN
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练GAN
- en: 'Training a GAN is a little more complicated than training traditional networks.
    During the training loop, we first need to use real data to start training the
    discriminator. We calculate the discriminator’s loss (using BCE, as we have only
    two classes: real or fake), and then do a backward pass to update the parameters
    of the discriminator as usual. But this time, we *don’t* call the optimizer to
    update. Instead, we generate a batch of data from our generator and pass that
    through the model. We calculate the loss and do *another* backward pass, so at
    this point the training loop has calculated the losses of two passes through the
    model. Now, we call the optimizer to update based on these *accumulated* gradients.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN比训练传统网络稍微复杂一些。在训练循环中，我们首先需要使用真实数据开始训练鉴别器。我们计算鉴别器的损失（使用BCE，因为我们只有两类：真实或虚假），然后进行反向传播以更新鉴别器的参数，就像往常一样。但这一次，我们*不*调用优化器来更新。相反，我们从生成器生成一批数据并通过模型传递。我们计算损失并进行*另一次*反向传播，因此此时训练循环已计算了两次通过模型的损失。现在，我们根据这些*累积*梯度调用优化器进行更新。
- en: In the second half of training, we turn to the generator. We give the generator
    access to the discriminator and then generate a new batch of data (which the generator
    insists is all real!) and test it against the discriminator. We form a loss against
    this output data, where each data point that the discriminator says is fake is
    considered a *wrong* answer—because we’re trying to fool it—and then do a standard
    backward/optimize pass.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的后半段，我们转向生成器。我们让生成器访问鉴别器，然后生成一批新数据（生成器坚持说这些都是真实的！）并将其与鉴别器进行测试。我们根据这些输出数据形成一个损失，鉴别器说是假的每个数据点都被视为*错误*答案——因为我们试图欺骗它——然后进行标准的反向/优化传递。
- en: 'Here’s a generalized implementation in PyTorch. Note that the generator and
    discriminator are just standard neural networks, so theoretically they could be
    generating images, text, audio, or whatever type of data, and be constructed of
    any of the types of networks you’ve seen so far:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PyTorch中的一个通用实现。请注意，生成器和鉴别器只是标准的神经网络，因此从理论上讲，它们可以生成图像、文本、音频或任何类型的数据，并且可以由迄今为止看到的任何类型的网络构建：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the flexibility of PyTorch helps a lot here. Without a dedicated training
    loop that is perhaps mainly designed for more standard training, building up a
    new training loop is something we’re used to, and we know all the steps that we
    need to include. In some other frameworks, training GANs is a bit more of a fiddly
    process. And that’s important, because training GANs is a difficult enough task
    without the framework getting in the way.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PyTorch的灵活性在这里非常有帮助。没有专门为更标准的训练而设计的训练循环，构建一个新的训练循环是我们习惯的事情，我们知道需要包含的所有步骤。在其他一些框架中，训练GAN有点更加繁琐。这很重要，因为训练GAN本身就是一个困难的任务，如果框架阻碍了这一过程，那就更加困难了。
- en: The Dangers of Mode Collapse
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式崩溃的危险
- en: In an ideal world, what happens during training is that the discriminator will
    be good at detecting fakes at first, because it’s training on real data, whereas
    the generator is allowed access to only the discriminator and not the real data
    itself. Eventually, the generator will learn how to fool the discriminator, and
    then it will soon improve rapidly to match the data distribution in order to repeatedly
    produce forgeries that slip past the critic.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，训练过程中发生的是，鉴别器一开始会擅长检测假数据，因为它是在真实数据上训练的，而生成器只允许访问鉴别器而不是真实数据本身。最终，生成器将学会如何欺骗鉴别器，然后它将迅速改进以匹配数据分布，以便反复产生能够欺骗评论者的伪造品。
- en: But one thing that plagues many GAN architectures is *mode collapse*. If our
    real data has three types of data, then maybe our generator will start generating
    the first type, and perhaps it starts getting rather good at it. The discriminator
    may then decide that anything that looks like the first type is actually fake,
    even the real example itself, and the generator then starts to generate something
    that looks like the third type. The discriminator starts rejecting all samples
    of the third type, and the generator picks another one of the real examples to
    generate. The cycle continues endlessly; the generator never manages to settle
    into a period where it can generate samples from across the distribution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但是困扰许多GAN架构的一件事是*模式崩溃*。如果我们的真实数据有三种类型的数据，那么也许我们的生成器会开始生成第一种类型，也许它开始变得相当擅长。鉴别器可能会决定任何看起来像第一种类型的东西实际上是假的，甚至是真实的例子本身，然后生成器开始生成看起来像第三种类型的东西。鉴别器开始拒绝所有第三种类型的样本，生成器选择另一个真实例子来生成。这个循环无休止地继续下去；生成器永远无法进入一个可以从整个分布中生成样本的阶段。
- en: Reducing mode collapse is a key performance issue of using GANs and is an on-going
    research area. Some approaches include adding a similarity score to the generated
    data, so that potential collapse can be detected and averted, keeping a replay
    buffer of generated images around so that the discriminator doesn’t overfit onto
    just the most current batch of generated images, allowing actual labels from the
    real dataset to be added to the generator network, and so on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模式崩溃是使用GAN的关键性能问题，也是一个正在进行研究的领域。一些方法包括向生成的数据添加相似性分数，以便可以检测和避免潜在的崩溃，保持一个生成图像的重放缓冲区，以便鉴别器不会过度拟合到最新批次的生成图像，允许从真实数据集中添加实际标签到生成器网络等等。
- en: Next we round off this section by examining a GAN application that performs
    super-resolution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过检查一个执行超分辨率的GAN应用程序来结束本节。
- en: ESRGAN
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ESRGAN
- en: The *Enhanced Super-Resolution Generative Adversarial Network* (ESRGAN) is a
    network developed in 2018 that produces impressive super-resolution results. The
    generator is a series of convolutional network blocks with a combination of residual
    and dense layer connections (so a mixture of both ResNet and DenseNet), with `BatchNorm`
    layers removed as they appear to create artifacts in upsampled images. For the
    discriminator, instead of simply producing a result that says *this is real* or
    *this is fake*, it predicts a probability that a real image is relatively more
    realistic than a fake one, and this helps to make the model produce more natural
    results.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*增强超分辨率生成对抗网络*（ESRGAN）是一种在2018年开发的网络，可以产生令人印象深刻的超分辨率结果。生成器是一系列卷积网络块，其中包含残差和稠密层连接的组合（因此是ResNet和DenseNet的混合），移除了`BatchNorm`层，因为它们似乎会在上采样图像中产生伪影。对于鉴别器，它不是简单地产生一个结果，说*这是真实的*或*这是假的*，而是预测一个真实图像相对更真实的概率比一个假图像更真实，这有助于使模型产生更自然的结果。'
- en: Running ESRGAN
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行ESRGAN
- en: 'To show off ESRGAN, we’re going to download the code from the [GitHub repository](https://github.com/xinntao/ESRGAN).
    Clone that using **`git`**:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示ESRGAN，我们将从[GitHub存储库](https://github.com/xinntao/ESRGAN)下载代码。使用**`git`**克隆：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We then need to download the weights so we can use the model without training.
    Using the Google Drive link in the README, download the *RRDB_ESRGAN_x4.pth* file
    and place it in *./models*. We’re going to upsample a scaled-down version of Helvetica
    in her box, but feel free to place any image into the *./LR* directory. Run the
    supplied *test.py* script and you’ll see upsampled images being generated and
    saved into the *results* directory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要下载权重，这样我们就可以在不训练的情况下使用模型。使用自述文件中的Google Drive链接，下载*RRDB_ESRGAN_x4.pth*文件并将其放在*./models*中。我们将对Helvetica的缩小版本进行上采样，但可以随意将任何图像放入*./LR*目录。运行提供的*test.py*脚本，您将看到生成的上采样图像并保存在*results*目录中。
- en: That wraps it up for super-resolution, but we haven’t quite finished with images
    yet.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是超分辨率的全部内容，但我们还没有完成图像处理。
- en: Further Adventures in Image Detection
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像检测的进一步探索
- en: 'Our image classifications in Chapters [2](ch02.html#image-classification-with-pytorch)–[4](ch04.html#transfer-learning-and-other-tricks)
    all had one thing in common: we determined that the image belonged to a single
    class, cat or fish. And obviously, in real-world applications, that would be extended
    to a much larger set of classes. But we’d also expect images to potentially include
    both a cat and a fish (which might be bad news for the fish), or any of the classes
    we’re looking for. There might be two people in the scene, a car, and a boat,
    and we not only want to determine that they’re present in the image, but also
    *where* they are in the image. There are two main ways to do this: *object detection*
    and *segmentation*. We’ll look at both and then turn to Facebook’s PyTorch implementations
    of Faster R-CNN and Mask R-CNN to look at concrete examples.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[2](ch02.html#image-classification-with-pytorch)章到第[4](ch04.html#transfer-learning-and-other-tricks)章的图像分类都有一个共同点：我们确定图像属于一个类别，猫或鱼。显然，在实际应用中，这将扩展到一个更大的类别集。但我们也希望图像可能包含猫和鱼（这对鱼可能是个坏消息），或者我们正在寻找的任何类别。场景中可能有两个人、一辆车和一艘船，我们不仅希望确定它们是否出现在图像中，还希望确定它们在图像中的*位置*。有两种主要方法可以实现这一点：*目标检测*和*分割*。我们将看看这两种方法，然后转向Facebook的PyTorch实现的Faster
    R-CNN和Mask R-CNN，以查看具体示例。
- en: Object Detection
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测
- en: Let’s take a look at our cat in a box. What we really want is for the network
    to put the cat in a box in another box! In particular, we want a *bounding box*
    that encompasses everything in the image that the model thinks is *cat*, as seen
    in [Figure 9-7](#cat-in-a-box-in-a-bounding-box).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的盒子里的猫。我们真正想要的是让网络将猫放在另一个盒子里！特别是，我们希望有一个*边界框*，包围模型认为是*猫*的图像中的所有内容，如[图9-7](#cat-in-a-box-in-a-bounding-box)所示。
- en: '![Cat In A Box In A Bounding Box](assets/ppdl_0907.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![盒子里的猫在一个边界框中](assets/ppdl_0907.png)'
- en: Figure 9-7\. Cat in a box in a bounding box
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 盒子里的猫在一个边界框中
- en: But how can we get our networks to work this out? Remember that these networks
    can predict anything that you want them to. What if alongside our prediction of
    a class, we also produce four more outputs? In our CATFISH model, we’d have a
    `Linear` layer of output size `6` instead of `2`. The additional four outputs
    will define a rectangle using *x[1], x[2], y[1], y[2]* coordinates. Instead of
    just supplying images as training data, we’ll also have to augment them with bounding
    boxes so that the model has something to train toward, of course. Our loss function
    will now be a combined loss of the cross-entropy loss of our class prediction
    and a mean squared loss for the bounding boxes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何让我们的网络解决这个问题呢？请记住，这些网络可以预测您想要的任何内容。如果在我们的CATFISH模型中，我们除了预测一个类别之外，还产生四个额外的输出怎么样？我们将有一个输出大小为`6`的`Linear`层，而不是`2`。额外的四个输出将使用*x[1]、x[2]、y[1]、y[2]*坐标定义一个矩形。我们不仅要提供图像作为训练数据，还必须用边界框增强它们，以便模型有东西可以训练，当然。我们的损失函数现在将是类别预测的交叉熵损失和边界框的均方损失的组合损失。
- en: There’s no magic here! We just design the model to give us what we need, feed
    in data that has enough information to make and train to those predictions, and
    include a loss function that tells our network how well or badly it’s doing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有魔法！我们只需设计模型以满足我们的需求，提供具有足够信息的数据进行训练，并包含一个告诉网络它的表现如何的损失函数。
- en: An alternative to the proliferation of bounding boxes is *segmentation*. Instead
    of producing boxes, our network outputs an image mask of the same size of the
    input; the pixels in the mask are colored depending on which class they fall into.
    For example, grass could be green, roads could be purple, cars could be red, and
    so on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与边界框的泛滥相比，*分割* 是一种替代方法。我们的网络不是生成框，而是输出与输入相同大小的图像掩模；掩模中的像素根据它们所属的类别着色。例如，草可能是绿色的，道路可能是紫色的，汽车可能是红色的，等等。
- en: As we’re outputting an image, you’d be right in thinking that we’ll probably
    end up using a similar sort of architecture as in the super-resolution section.
    There’s a lot of cross-over between the two topics, and one model type that has
    become popular over the past few years is the *U-Net* architecture, shown in [Figure 9-8](#simplified-u-net-architecture).^([3](ch09.html#idm45762347677112))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在输出图像，您可能会认为我们最终会使用与超分辨率部分类似的架构。这两个主题之间存在很多交叉，近年来变得流行的一种模型类型是*U-Net*架构，如[图9-8](#simplified-u-net-architecture)所示。^([3](ch09.html#idm45762347677112))
- en: '![Simplified U-Net Architecture](assets/ppdl_0908.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![简化的U-Net架构](assets/ppdl_0908.png)'
- en: Figure 9-8\. Simplified U-Net architecture
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 简化的U-Net架构
- en: As you can see, the classic U-Net architecture is a set of convolutional blocks
    that scale down an image and another series of convolutions that scale it back
    up again to the target image. However, the key of U-Net is the lines that go across
    from the left blocks to their counterparts on the righthand side, which are concatenated
    with the output tensors as the image is scaled back up. These connections allow
    information from the higher-level convolutional blocks to transfer across, preserving
    details that might be removed as the convolutional blocks reduce the input image.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，经典的U-Net架构是一组卷积块，将图像缩小，另一系列卷积将其缩放回目标图像。然而，U-Net的关键在于从左侧块到右侧对应块的横跨线，这些线与输出张量连接在一起，当图像被缩放回来时，这些连接允许来自更高级别卷积块的信息传递，保留可能在卷积块减少输入图像时被移除的细节。
- en: You’ll find U-Net-based architectures cropping up all over Kaggle segmentation
    competitions, proving in some ways that this structure is a good one for segmentation.
    Another technique that has been applied to the basic setup is our old friend transfer
    learning. In this approach, the first part of the U is taken from a pretrained
    model such as ResNet or Inception, and the other side of the U, plus skip connections,
    are added on top of the trained network and fine-tuned as usual.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现基于U-Net的架构在Kaggle分割竞赛中随处可见，从某种程度上证明了这种结构对于分割是一个不错的选择。已经应用到基本设置的另一种技术是我们的老朋友迁移学习。在这种方法中，U的第一部分取自预训练模型，如ResNet或Inception，U的另一侧加上跳跃连接，添加到训练网络的顶部，并像往常一样进行微调。
- en: Let’s take a look at some existing pretrained models that can deliver state-of-the-art
    object detection and segmentation, direct from Facebook.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些现有的预训练模型，可以直接从Facebook获得最先进的目标检测和分割。
- en: Faster R-CNN and Mask R-CNN
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Faster R-CNN和Mask R-CNN
- en: 'Facebook Research has produced the *maskrcnn-benchmark* library, which contains
    reference implementations of both object detection and segmentation algorithms.
    We’re going to install the library and add code to generate predictions. At the
    time of this writing, the easiest way to build the models is by using Docker (this
    may change when PyTorch 1.2 is released). Clone the repository from *[*https://github.com/facebookresearch/maskrcnn-benchmark*](https://github.com/facebookresearch/maskrcnn-benchmark)*
    and add this script, *predict.py*, into the *demo* directory to set up a prediction
    pipeline using a ResNet-101 backbone:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook Research开发了*maskrcnn-benchmark*库，其中包含目标检测和分割算法的参考实现。我们将安装该库并添加代码来生成预测。在撰写本文时，构建模型的最简单方法是使用Docker（当PyTorch
    1.2发布时可能会更改）。从[*https://github.com/facebookresearch/maskrcnn-benchmark*](https://github.com/facebookresearch/maskrcnn-benchmark)克隆存储库，并将此脚本*predict.py*添加到*demo*目录中，以设置使用ResNet-101骨干的预测管道：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this short script, we’re first setting up the `COCODemo` predictor, making
    sure that we pass in the configuration that sets up Faster R-CNN instead of Mask
    R-CNN (which will produce segmented output). We then open an image file set on
    the command line, but we have to turn it into `BGR` format instead of `RGB` format
    as the predictor is trained on OpenCV images rather than the PIL images we’ve
    been using so far. Finally, we use `imsave` to write the `predictions` array (the
    original image plus bounding boxes) to a new file, also specified on the command
    line. Copy in a test image file into this *demo* directory and we can then build
    the Docker image:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的脚本中，我们首先设置了`COCODemo`预测器，确保我们传入的配置设置了Faster R-CNN而不是Mask R-CNN（后者会产生分割输出）。然后我们打开一个在命令行上设置的图像文件，但是我们必须将其转换为`BGR`格式而不是`RGB`格式，因为预测器是在OpenCV图像上训练的，而不是我们迄今为止使用的PIL图像。最后，我们使用`imsave`将`predictions`数组（原始图像加上边界框）写入一个新文件，也在命令行上指定。将一个测试图像文件复制到这个*demo*目录中，然后我们可以构建Docker镜像：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We run the script from inside the Docker container and produce output that looks
    like [Figure 9-7](#cat-in-a-box-in-a-bounding-box) (I actually used the library
    to generate that image). Try experimenting with different `confidence_threshold`
    values and different pictures. You can also switch to the `e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml`
    configuration to try out Mask R-CNN and generate segmentation masks as well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Docker容器内运行脚本，并生成类似于[图9-7](#cat-in-a-box-in-a-bounding-box)的输出（我实际上使用了该库来生成该图像）。尝试尝试不同的`confidence_threshold`值和不同的图片。您还可以切换到`e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml`配置，尝试Mask
    R-CNN并生成分割蒙版。
- en: 'To train your own data on the models, you’ll need to supply your own dataset
    that provides bounding box labels for each image. The library provides a helper
    function called `BoxList`. Here’s a skeleton implementation of a dataset that
    you could use as a starting point:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要在这些模型上训练您自己的数据，您需要提供一个数据集，为每个图像提供边界框标签。该库提供了一个名为`BoxList`的辅助函数。以下是一个数据集的骨架实现，您可以将其用作起点：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You’ll then need to add your newly created dataset to *maskrcnn_benchmark/data/datasets/*init*.py*
    and *maskrcnn_benchmark/config/paths_catalog.py*. Training can then be carried
    out using the supplied *train_net.py* script in the repo. Be aware that you may
    have to decrease the batch size to train any of these networks on a single GPU.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要将新创建的数据集添加到*maskrcnn_benchmark/data/datasets/*init*.py*和*maskrcnn_benchmark/config/paths_catalog.py*中。然后可以使用存储库中提供的*train_net.py*脚本进行训练。请注意，您可能需要减少批量大小以在单个GPU上训练这些网络中的任何一个。
- en: That wraps it up for object detection and segmentation, though see [“Further
    Reading”](#chapter-9-further-reading) for more ideas, including the wonderfully
    entitled You Only Look Once (YOLO) architecture. In the meantime, we look at how
    to maliciously break a model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是目标检测和分割的全部内容，但是请参阅[“进一步阅读”](#chapter-9-further-reading)以获取更多想法，包括标题为You Only
    Look Once（YOLO）架构的内容。与此同时，我们将看看如何恶意破坏模型。
- en: Adversarial Samples
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗样本
- en: You have probably seen articles online about images that can somehow prevent
    image recognition from working properly. If a person holds up an image to the
    camera, the neural network thinks it is seeing a panda or something like that.
    These are known as *adversarial samples*, and they’re interesting ways of discovering
    the limitations of your architectures and how best to defend against them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating an adversarial sample isn’t too difficult, especially if you have
    access to the model. Here’s a simple neural network that classifies images from
    the popular CIFAR-10 dataset. There’s nothing special about this model, so feel
    free to swap it out for AlexNet, ResNet, or any other network presented so far
    in the book:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once the network has been trained on CIFAR-10, we can get a prediction for the
    image in [Figure 9-9](#our-frog-example). Hopefully the training has gone well
    enough to report that it’s a frog (if not, you might want to train a little more!).
    What we’re going to do is change our picture of a frog just enough that the neural
    network gets confused and thinks it’s something else, even though we can still
    recognize that it’s clearly a frog.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Our frog example](assets/ppdl_0909.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. Our frog example
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To do this, we’ll use a method of attack called the *fast gradient sign method*.^([4](ch09.html#idm45762347036616))
    The idea is to take the image we want to misclassify and run it through the model
    as usual, which gives us an output tensor. Typically for predictions, we’d look
    to see which of the tensor’s values was the highest and use that as the index
    into our classes, using `argmax()`. But this time we’re going to pretend that
    we’re training the network again and backpropagate that result back through the
    model, giving us the gradient changes of the model with respect to the original
    input (in this case, our picture of a frog).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Having done that, we create a new tensor that looks at these gradients and replaces
    an entry with +1 if the gradient is positive and –1 if the gradient is negative.
    That gives us the direction of travel that this image is pushing the model’s decision
    boundaries. We then multiply by a small scalar (called *epsilon* in the paper)
    to produce our malicious mask, which we then add to the original image, creating
    an adversarial example.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple PyTorch method that returns the fast gradient sign tensors
    for an input batch when supplied with the batch’s labels, plus the model and the
    loss function used to evaluate the model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Epsilon is normally found via experimentation. By playing around with various
    images, I discovered that `0.02` works well for this model, but you could also
    use something like a grid or random search to find the value that turns a frog
    into a ship!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Running this function on our frog and our model, we get a mask, which we can
    then add to our original image to generate our adversarial sample. Have a look
    at [Figure 9-10](#our-adversarial-frog) to see what it looks like!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Our adversarial frog](assets/ppdl_0910.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 9-10\. Our adversarial frog
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, our created image is still a frog to our human eyes. (If it doesn’t
    look like a frog to you, then you may be a neural network. Report yourself for
    a Voight-Kampff test immediately.) But what happens if we get a prediction from
    the model on this new image?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We have defeated the model. But is this as much of a problem as it first appears?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Black-Box Attacks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that to produce an image that fools the classifier, we
    need to know a lot about the model being used. We have the entire structure of
    the model in front of us as well as the loss function that was used in training
    the model, and we need to do forward and backward passes in the model to get our
    gradients. This is a classic example of what’s known in computer security as a
    *white-box attack*, where we can peek into any part of our code to work out what’s
    going on and exploit whatever we can find.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: So does this matter? After all, most models that you’ll encounter online won’t
    allow you to peek inside. Is a *black-box attack*, where all you have is the input
    and output, actually possible? Well, sadly, yes. Consider that we have a set of
    inputs, and a set of outputs to match them against. The outputs are *labels*,
    and it is possible to use targeted queries of models to train a new model that
    you can use as a local proxy and carry out attacks in a white-box manner. Just
    as you’ve seen with transfer learning, the attacks on the proxy model can work
    effectively on the actual model. Are we doomed?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Defending Against Adversarial Attacks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can we defend against these attacks? For something like classifying an image
    as a cat or a fish, it’s probably not the end of the world, but for self-driving
    systems, cancer-detection applications, and so forth, it could literally mean
    the difference between life and death. Successfully defending against all types
    of adversarial attacks is still an area of research, but highlights so far include
    distilling and validation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '*Distilling* a model by using it to train *another* model seems to help. Using
    label smoothing with the new model, as outlined earlier in this chapter, also
    seems to help. Making the model less sure of its decisions appears to smooth out
    the gradients somewhat, making the gradient-based attack we’ve outlined in this
    chapter less effective.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: A stronger approach is to go back to some parts of the early computer vision
    days. If we perform input validation on the incoming data, we can possibly prevent
    the adversarial image from getting to the model in the first place. In the preceding
    example, the generated attack image has a few pixels that are very out of place
    to what our eyes are expecting when we see a frog. Depending on the domain, we
    could have a filter that allows in only images that pass some filtering tests.
    You could in theory make a neural net to do that too, because then the attackers
    have to try to break two different models with the same image!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Now we really are done with images. But let’s look at some developments in text-based
    networks that have occurred the past couple of years.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'More Than Meets the Eye: The Transformer Architecture'
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transfer learning has been a big feature in allowing image-based networks to
    become so effective and prevalent over the past decade, but text has been a more
    difficult nut to crack. In the last couple of years, though, some major steps
    have been taken that are beginning to unlock the potential of using transfer learning
    in text for all sorts of tasks, such as generation, classification, and answering
    questions. We’ve also seen a new type of architecture begin to take center stage:
    the *Transformer network*. These networks don’t come from Cybertron, but the technique
    is behind the most powerful text-based networks we’ve seen, with OpenAI’s GPT-2
    model, released in 2019, showing a scarily impressive quality in its generated
    text, to the extent that OpenAI initially held back the larger version of the
    model to prevent it from being used for nefarious purposes. We look at the general
    theory of Transformer and then dive into how to use Hugging Face’s implementations
    of GPT-2 and BERT.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Paying Attention
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial step along the way to the Transformer architecture was the *attention*
    mechanism, which was initially introduced to RNNs to help in sequence-to-sequence
    applications such as translation.^([5](ch09.html#idm45762346750264))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The issue *attention* was trying to solve was the difficulty in translating
    sentences such as “The cat sat on the mat and she purred.” We know that *she*
    in that sentence refers to the cat, but it’s a hard concept to get a standard
    RNN to understand. It may have the hidden state that we talked about in [Chapter 5](ch05.html#text-classification),
    but by the time we get to *she*, we already have a lot of time steps and hidden
    state for each step!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: So what *attention* does is add an extra set of learnable weights attached to
    each time step that focuses the network onto a particular part of the sentence.
    The weights are normally pushed through a `softmax` layer to generate probabilities
    for each step and then the dot product of the attention weights is calculated
    with the previous hidden state. [Figure 9-11](#attention-vector) shows a simplified
    version of this with respect to our sentence.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![An Attention Vector pointing to ''cat''](assets/ppdl_0911.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. An attention vector pointing to *cat*
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The weights ensure that when the hidden state gets combined with the current
    state, *cat* will be a major part of determining the output vector at the time
    step for *she*, which will provide useful context for translating into French,
    for example!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into all the details about how *attention* can work in a concrete
    implementation, but know the concept was powerful enough that it kickstarted the
    impressive growth and accuracy of Google Translate back in the mid-2010s. But
    more was to come.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Attention Is All You Need
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the groundbreaking paper “Attention Is All You Need,”^([6](ch09.html#idm45762346734856))
    Google researchers pointed out that we’d spent all this time bolting attention
    onto an already slow RNN-based network (compared to CNNs or linear units, anyhow).
    What if we didn’t need the RNN after all? The paper showed that with stacked attention-based
    encoders and decoders, you could create a model that didn’t rely on the RNN’s
    hidden state at all, leading the way to the larger and faster Transformer that
    dominates textual deep learning today.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The key idea was to use what the authors called *multihead attention*, which
    parallelizes the *attention* step over all the input by using a group of `Linear`
    layers. With these, and borrowing some residual connection tricks from ResNet,
    Transformer quickly began to supplant RNNs for many text-based applications. Two
    important Transformer releases, BERT and GPT-2, represent the current state-of-the-art
    as this book goes to print.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Luckily for us, there’s [a library](https://oreil.ly/xpDzq) from Hugging Face
    that implements both of them in PyTorch. It can be installed using `pip` or `conda`,
    and you should also `git clone` the repo itself, as we’ll be using some of the
    utility scripts later!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: First, we’ll have a look at BERT.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google’s 2018 *Bidirectional Encoder Representations from Transformers* (BERT)
    model was one of the first successful examples of bringing transfer learning of
    a powerful model to test. BERT itself is a massive Transformer-based model (weighing
    in at 110 million parameters in its smallest version), pretrained on Wikipedia
    and the BookCorpus dataset. The issue that both Transformer and convolutional
    networks traditionally have when working with text is that because they see all
    of the data at once, it’s difficult for those networks to learn the temporal structure
    of language. BERT gets around this in its pretraining stage by masking 15% of
    the text input at random and forcing the model to predict the parts that have
    been masked. Despite being conceptually simple, the combination of the massive
    size of the 340 million parameters in the largest model with the Transformer architecture
    resulted in new state-of-the-art results for a whole series of text-related benchmarks.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Of course, despite being created by Google with TensorFlow, there are implementations
    of BERT for PyTorch. Let’s take a quick look at one now.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: FastBERT
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An easy way to start using the BERT model in your own classification applications
    is to use the *FastBERT* library that mixes Hugging Face’s repository with the
    fast.ai API (which you’ll see in a bit more detail when we come to ULMFiT shortly).
    It can be installed via `pip` in the usual manner:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here’s a script that can be used to fine-tune BERT on our Sentiment140 Twitter
    dataset that we used into [Chapter 5](ch05.html#text-classification):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After our imports, we set up the `device`, `logger`, and `metrics` objects,
    which are required by the `BertLearner` object. We then create a `BERTTokenizer`
    for tokenizing our input data, and in this base we’re going to use the `bert-base-uncased`
    model (which has 12 layers and 110 million parameters). Next, we need a `BertDataBunch`
    object that contains paths to the training, validation, and test datasets, where
    to find the label column, our batch size, and the maximum length of our input
    data, which in our case is simple because it can be only the length of a tweet,
    at that time 140 characters. Having done that, we will set up a BERT model by
    using the `BertLearner.from_pretrained_model` method. This passes in our input
    data, our BERT model type, the `metric`, `device`, and `logger` objects we set
    up at the start of the script, and finally some flags to turn off training options
    that we don’t need but aren’t given defaults for the method signature.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `fit()` method takes care of fine-tuning the BERT model on our
    input data, running on its own internal training loop. In this example, we’re
    training for three epochs with a learning rate of `1e-2`. The trained PyTorch
    model can be accessed afterward using `learner.model`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: And that’s how to get up and running with BERT. Now, onto the competition.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, while Google was quietly working on BERT, OpenAI was working on its own
    version of a Transformer-based text model. Instead of using masking to force the
    model to learn language structure, the model constrains the attention mechanism
    within the architecture to simply predict the next word in a sequence, in a similar
    style to the RNNs in [Chapter 5](ch05.html#text-classification). As a result,
    GPT was somewhat left behind by the impressive performance of BERT, but in 2019
    OpenAI struck back with *GPT-2*, a new version of the model that reset the bar
    for text generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The magic behind GPT-2 is scale: the model is trained on text from over 8 million
    websites, and the largest variant of GPT-2 weighs in at 1.5 billion parameters.
    And while it still doesn’t dislodge BERT on particular benchmarks for things like
    question/answering or other NLP tasks, its ability to create incredibly realistic
    text from a basic prompt led to OpenAI locking the full-size model behind closed
    doors for fear of it being weaponized. They have, however, released smaller versions
    of the model, clocking in at 117 and 340 million parameters.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of the output that GPT-2 can generate. Everything in italics
    was written by GPT-2’s 340M model:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Jack and Jill went up the hill *on a bike ride. The sky was a grey white and
    the wind was blowing, causing a heavy snowfall. It was really difficult to drive
    down the hill, I had to lean forward on a bit of gear to get it on. But then there
    was a moment of freedom that I would never forget: The bike was at a complete
    stop on the mountain side and I was in the middle of it. I didn’t have time to
    say a word, but I did lean forward and touch the brakes and the bike started to
    go.*'
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Aside from switching from *Jack and Jill* to *I*, this is an impressive piece
    of text generation. For short pieces of text, it can sometimes be indistinguishable
    from human-created text. It does reveal the machine behind the curtain as the
    generated text continues, but it’s an impressive feat that could be writing tweets
    and Reddit comments right now. Let’s have a look at how to do this with PyTorch.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Generating Text with GPT-2
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like BERT, the official GPT-2 release from OpenAI is a TensorFlow model. Also
    like BERT, Hugging Face has released a PyTorch version that is contained within
    the same library (`pytorch-transformers`). However, a burgeoning ecosystem has
    been built around the original TensorFlow model that just doesn’t exist currently
    around the PyTorch version. So just this once, we’re going to cheat: we’re going
    to use some of the TensorFlow-based libraries to fine-tune the GPT-2 model, and
    then export the weights and import them into the PyTorch version of the model.
    To save us from too much setup, we also do all the TensorFlow operations in a
    Colab notebook! Let’s get started.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Google Colab notebook and install the library that we’re using,
    Max Woolf’s *gpt-2-simple*, which wraps up GPT-2 fine-tuning in a single package.
    Install it by adding this into a cell:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next up, you need some text. In this example, I’m using a public domain text
    of PG Wodehouse’s *My Man Jeeves*. I’m also not going to do any further processing
    on the text after downloading it from the Project Gutenberg website with `wget`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we can use the library to train. First, make sure your notebook is connected
    to a GPU (look in Runtime→Change Runtime Type), and then run this code in a cell:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Replace the text file with whatever text file you’re using. As the model trains,
    it will spit out a sample every hundred steps. In my case, it was interesting
    to see it turn from spitting out vaguely Shakespearian play scripts to something
    that ended up approaching Wodehouse prose. This will likely take an hour or two
    to train for 1,000 epochs, so go off and do something more interesting instead
    while the cloud’s GPUs are whirring away.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it has finished, we need to get the weights out of Colab and into your
    Google Drive account so you can download them to wherever you’re running PyTorch
    from:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: That will point you to open a new web page to copy an authentication code into
    the notebook. Do that, and the weights will be tarred up and saved to your Google
    Drive as *run1.tar.gz*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, on the instance or notebook where you’re running PyTorch, download that
    tarfile and extract it. We need to rename a couple of files to make these weights
    compatible with the Hugging Face reimplementation of GPT-2:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now need to convert the saved TensorFlow weights into ones that are compatible
    with PyTorch. Handily, the `pytorch-transformers` repo comes with a script to
    do that:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Creating a new instance of the GPT-2 model can then be performed in code like
    this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Or, just to play around with the model, you can use the *run_gpt2.py* script
    to get a prompt where you enter text and get generated samples back from the PyTorch-based
    model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Training GPT-2 is likely to become easier in the coming months as Hugging Face
    incorporates a consistent API for all the models in its repo, but the TensorFlow
    method is the easiest to get started with right now.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT and GPT-2 are the most popular names in text-based learning right now,
    but before we wrap up, we cover the dark horse of the current state-of-the-art
    models: ULMFiT.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ULMFiT
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to the behemoths of BERT and GPT-2, *ULMFiT* is based on a good
    old RNN. No Transformer in sight, just the AWD-LSTM, an architecture originally
    created by Stephen Merity. Trained on the WikiText-103 dataset, it has proven
    to be amendable to transfer learning, and despite the *old* type of architecture,
    has proven to be competitive with BERT and GPT-2 in the classification realm.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: While ULMFiT is, at heart, just another model that can be loaded and used in
    PyTorch like any other, its natural home is within the fast.ai library, which
    sits on top of PyTorch and provides many useful abstractions for getting to grips
    with and being productive with deep learning quickly. To that end, we’ll look
    at how to use ULMFiT with the fast.ai library on the Twitter dataset we used in
    [Chapter 5](ch05.html#text-classification).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'We first use fast.ai’s Data Block API to prepare our data for fine-tuning the
    LSTM:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is fairly similar to the `torchtext` helpers from [Chapter 5](ch05.html#text-classification)
    and just produces what fast.ai calls a `databunch`, from which its models and
    training routines can easily grab data. Next, we create the model, but in fast.ai,
    this happens a little differently. We create a `learner` that we interact with
    to train the model instead of the model itself, though we pass that in as a parameter.
    We also supply a dropout value (we’re using the one suggested in the fast.ai training
    materials):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once we have our `learner` object, we can find the optimal learning rate. This
    is just like what we implemented in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    except that it’s built into the library and uses an exponentially moving average
    to smooth out the graph, which in our implementation is pretty spiky:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: From the plot in [Figure 9-12](#ULMFiT-learning-rate-plot), it looks like `1e-2`
    is where we’re starting to hit a steep decline, so we’ll pick that as our learning
    rate. Fast.ai uses a method called `fit_one_cycle`, which uses a 1cycle learning
    scheduler (see [“Further Reading”](#chapter-9-further-reading) for more details
    on 1cycle) and very high learning rates to train a model in an order of magnitude
    fewer epochs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![ULMFiT learning rate plot](assets/ppdl_0912.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 9-12\. ULMFiT learning rate plot
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, we’re training for just one cycle and saving the fine-tuned head of the
    network (the *encoder*):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With the fine-tuning of the language model completed (you may want to experiment
    with more cycles in training), we build a new `databunch` for the actual classification
    problem:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The only real difference here is that we supply the actual labels by using
    `label_from_df` and we pass in a `vocab` object from the language model training
    that we performed earlier to make sure they’re using the same mapping of words
    to numbers, and then we’re ready to create a new `text_classifier_learner`, where
    the library does all the model creation for you behind the scenes. We load the
    fine-tuned encoder onto this new model and begin the process of training again:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: And with a tiny amount of code, we have a classifier that reports an accuracy
    of 76%. We could easily improve that by training the language model for more cycles,
    adding differential learning rates and freezing parts of the model while training,
    all of which fast.ai supports with methods defined on the `learner`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: What to Use?
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given that little whirlwind tour of the current cutting edge of text models
    in deep learning, there’s probably one question on your mind: “That’s all great,
    but which one should I actually *use*?” In general, if you’re working on a classification
    problem, I suggest you start with ULMFiT. BERT is impressive, but ULMFiT is competitive
    with BERT in terms of accuracy, and it has the additional benefit that you don’t
    need to buy a huge number of TPU credits to get the best out of it. A single GPU
    fine-tuning ULMFiT is likely to be enough for most people.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: And as for GPT-2, if you’re after generated text, then yes, it’s a better fit,
    but for classification purposes, it’s going to be harder to approach ULMFiT or
    BERT performance. One thing that I do think might be interesting is to let GPT-2
    loose on data augmentation; if you have a dataset like Sentiment140, which we’ve
    been using throughout this book, why not fine-tune a GPT-2 model on that input
    and use it to generate more data?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter looked at the wider world of PyTorch, including libraries with
    existing models that you can import into your own projects, some cutting-edge
    data augmentation approaches that can be applied to any domain, as well as adversarial
    samples that can ruin your model’s day and how to defend against them. I hope
    that as we come to the end of our journey, you understand how neural networks
    are assembled and how to get images, text, and audio to flow through them as tensors.
    You should be able to train them, augment data, experiment with learning rates,
    and even debug models when they’re not going quite right. And once all that’s
    done, you know how to package them up in Docker and get them serving requests
    from the wider world.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go from here? Consider having a look at the PyTorch forums and the
    other documentation on the website. I definitely also recommend visiting the fast.ai
    community even if you don’t end up using the library; it’s a hive of activity,
    filled with good ideas and people experimenting with new approaches, while also
    friendly to newcomers!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Keeping up with the cutting edge of deep learning is becoming harder and harder.
    Most papers are published on [arXiv](https://arxiv.org), but the rate of papers
    being published seems to be rising at an almost exponential level; as I was typing
    up this conclusion, [XLNet](https://arxiv.org/abs/1906.08237) was released, which
    apparently beats BERT on various tasks. It never ends! To try to help in this,
    I listed a few Twitter accounts here where people often recommend interesting
    papers. I suggest following them to get a taste of current and interesting work,
    and from there you can perhaps use a tool such as [arXiv Sanity Preserver](http://arxiv-sanity.com)
    to drink from the firehose when you feel more comfortable diving in.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I trained a GPT-2 model on the book and it would like to say a few
    words:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning *is a key driver of how we work on today’s deep learning applications,
    and deep learning is expected to continue to expand into new fields such as image-based
    classification and in 2016, NVIDIA introduced the CUDA LSTM architecture. With
    LSTMs now becoming more popular, LSTMs were also a cheaper and easier to produce
    method of building for research purposes, and CUDA has proven to be a very competitive
    architecture in the deep learning market.*
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Thankfully, you can see there’s still a way to go before we authors are out
    of a job. But maybe you can help change that!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A survey of [current super-resolution techniques](https://arxiv.org/pdf/1902.06068.pdf)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ian Goodfellow’s [lecture on GANs](https://www.youtube.com/watch?v=Z6rxFNMGdn0)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[You Only Look Once (YOLO)](https://pjreddie.com/darknet/yolo), a family of
    fast object detection models with highly readable papers'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CleverHans](https://github.com/tensorflow/cleverhans), a library of adversarial
    generation techniques for TensorFlow and PyTorch'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer),
    an in-depth voyage through the Transformer architecture'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some Twitter accounts to follow:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*@jeremyphoward*—Cofounder of fast.ai'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*@miles_brundage*—Research scientist (policy) at OpenAI'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*@BrundageBot*—Twitter bot that generates a daily summary of interesting papers
    from arXiv (warning: often tweets out 50 papers a day!)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*@pytorch*—Official PyTorch account'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](ch09.html#idm45762349177880-marker)) See [“mixup: Beyond Empirical Risk
    Minimization”](https://arxiv.org/abs/1710.09412) by Hongyi Zhang et al. (2017).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.html#idm45762348101832-marker)) See [“Generative Adversarial Networks”](https://arxiv.org/abs/1406.2661)
    by Ian J. Goodfellow et al. (2014).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch09.html#idm45762347677112-marker)) See [“U-Net: Convolutional Networks
    for Biomedical Image Segmentation”](https://arxiv.org/abs/1505.04597) by Olaf
    Ronneberger et al. (2015).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.html#idm45762347036616-marker)) See [“Explaining and Harnessing Adversarial
    Examples”](https://arxiv.org/abs/1412.6572) by Ian Goodfellow et al. (2014).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch09.html#idm45762346750264-marker)) See [“Neural Machine Translation
    by Jointly Learning to Align and Translate”](https://arxiv.org/abs/1409.0473)
    by Dzmitry Bahdanau et al. (2014).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch09.html#idm45762346734856-marker)) See [“Attention Is All You Need”](https://arxiv.org/abs/1706.03762)
    by Ashish Vaswani et al. (2017).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
