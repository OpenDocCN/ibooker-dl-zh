<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__beyond_nlp"> <span class="chapter-title-numbering"><span class="num-string">6</span></span> <span class="title-text"> Beyond natural language processing</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">How transformer layers work on data other than text</li> 
    <li class="readable-text" id="p3">Helping LLMs to write working software</li> 
    <li class="readable-text" id="p4">Tweaking LLMs so they understand mathematical notation </li> 
    <li class="readable-text" id="p5">How transformers replace the input and output steps to work with images</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>While modeling natural language was the transformers’ primary purpose, machine learning researchers quickly discovered they could predict anything involving data sequences. Transformers view a sentence as a sequence of tokens and either produce a related sequence of tokens, such as a translation from one language to another, or predict the following tokens in a sequence, such as when answering questions or acting like a chatbot. While sequence modeling and prediction are potent tools for interpreting and generating natural language, natural language is the only domain where LLMs can be helpful.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Many data types, other than human language, can be represented as a sequence of tokens. Source code used to implement software is one example. Instead of the words and syntax you would expect to see in English, source code is written in a computer programming language like Python. Source code has its own structure that describes the operations a software developer wants a computer to perform. Like human language, the tokens in the source code have meaning according to the language used and the context in which they appear. If anything, source code is more highly structured and specific than human language. A programming language with shades of ambiguity and meaning would be challenging for a computer to interpret and harder for others to modify and maintain.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>Source code, or simply “code” (which is how we’ll refer to it from here on), is just one example of how LLMs and transformers work with data that is not natural language. Almost any data you can recast as a sequence of tokens can use transformers and the many lessons we have learned about how LLMs work. This chapter will review three examples that become progressively less like natural language: code, mathematics, and computer vision.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Each of these three different types of data, known as data <em>modalities</em>, will require a new way of looking at a transformer’s inputs or outputs. However, in all cases, the transformer itself will remain unchanged. We will still stack multiple transformer layers on top of each other to build a model, and we will continue to train the transformer layers using gradient descent. Code, being the most similar to natural language, does not require too many changes. To make a code LLM work well, though, we will change how the outputs of the LLM generate subsequent tokens. Next, we will look at mathematics, where we need to change tokenization to get an LLM to succeed at basic operations such as addition. Finally, for computer vision, which concerns working with images and performing tasks such as object detection and identification, we will modify both the inputs and outputs, showing how you can convert a very different type of data into a sequence by replacing the concept of tokens entirely. We show the parts of LLMs that you must modify to work with each data modality in figure <a href="#fig__codeMathCV">6.1</a>. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p10">  
   <img alt="figure" src="../Images/CH06_F01_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__codeMathCV"><span class="num-string">Figure <span class="browsable-reference-id">6.1</span></span> If we break an LLM into three primary components—input (tokenization), transformation (transformers), and output generation (unembedding)—we can use new data modalities by changing at least one of the input or output components. Meanwhile, the transformer does not require modification for most use cases because it is general-purpose. </h5>
  </div> 
  <div class="readable-text" id="p11"> 
   <p>In working with all three new types of data, we must solve a common problem. How do we give the LLM or transformer the ability to use knowledge related to that specific subject area? We commonly handle this by integrating external software into the LLM. You can think of these external software components as tools. Similarly to how you need a hammer to drive a nail through a piece of wood, LLMs can benefit from using tools to achieve end goals. Tools built for code will help us improve coding LLMs. Knowing how humans do math and the tools we use to automate math will help us make better math LLMs. Understanding how we represent images as pixels (which we ultimately transform into sequences of numbers representing the amount of red, green, and blue in one part of an image) will allow us to convert them into sequences for the LLM. As you think about the specific knowledge related to your work where LLMs have not yet been applied, you will be able to identify the unique characteristics of the data you work with to modify an LLM to better operate with data from that domain of knowledge.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class=" readable-text-h2" id="llms-for-software-development"><span class="num-string browsable-reference-id">6.1</span> LLMs for software development</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>We’ve already briefly discussed that LLMs can write source code for software. In chapter <a href="../Text/chapter-4.html">4</a>, we asked ChatGPT to write some Python code for calculating the mathematical constant <span><img alt="equation image" src="../Images/eq-chapter-6-13-1.png"/></span>. Next, we asked it to convert that code into an obscure language called Modula-3. Software was one of the first things people discovered LLMs could help with as a relatively natural consequence of how programming works. Programming languages are designed to be read and written by humans like text! Consequently, we can generate code without changing the tokenization process. Everything we have discussed about constructing LLMs applies equally to code and human languages.</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>We can see this by looking at ChatGPT’s tokenization of two similar code segments for Python and Java in figure <a href="#fig__pythonJavaTokens">6.2</a>. Here, we use shades of grey to show the OpenAI tokenizer (<a class="uri" href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>), which breaks code into different tokens. While the same token might have a different color in each example, we can focus on how the tokenizer breaks code into tokens and the similarities between both examples. These include things like</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p15">The indentation for each line of code</li> 
   <li class="readable-text" id="p16">The <code> x</code> and <code> i</code> variables (in most cases)</li> 
   <li class="readable-text" id="p17">The function name and return statement</li> 
   <li class="readable-text" id="p18">The operators, such as <code>+=</code></li> 
  </ul> 
  <div class="readable-text" id="p19"> 
   <p>These similarities make it far easier for an LLM to correlate the similarity between each piece of code. The similarities also mean that the LLM shares information between programming languages with common naming, syntax, and coding practices during training.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p20">  
   <img alt="figure" src="../Images/CH06_F02_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__pythonJavaTokens"><span class="num-string">Figure <span class="browsable-reference-id">6.2</span></span> Two similar samples of code written in the programming languages Python (left) and Java (right). These show how byte-pair encoding can identify similar tokens across different languages. The boxes show individual tokens. Standard tokenization methods for human languages do a reasonable job on code since it has many similarities to natural language. </h5>
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Software developers are encouraged to use meaningful variable names that reflect a variable’s role or purpose in the programs they write. Variables named like <code>initValue</code> are broken up into two tokens for <code>init</code> and <code>Value</code>, using the same tokens to represent natural language text where the prefix “init” of the word “Value” occurs. So not only do we share information between programming languages with similar syntax, but we also share information about the context and intention of code via variable names. LLMs also benefit from the code comments that programmers add to describe complex parts of the code for themselves or other programmers. In figure <a href="#fig__javaCommented">6.3</a>, we have the Java version repeated with a change in the variable name and a descriptive (but unnecessary in real life) comment at the top of the function.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p22">  
   <img alt="figure" src="../Images/CH06_F03_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__javaCommented"><span class="num-string">Figure <span class="browsable-reference-id">6.3</span></span> Code written in Java, including a comment describing what the code does. Because (good) code (hopefully) has a lot of comments, there is a natural mix of natural language and code for the LLM to use to obtain information. When variables have descriptive names, it becomes easier for the model to correlate information between the code and the intent described in comments and variable names.</h5>
  </div> 
  <div class="readable-text" id="p23"> 
   <p>In most cases, we get the same tokens between code and comments, linking human and programming languages together since they use the same representation. Whether we are working with a programming language or natural language, we get the same tokens and embeddings. The beauty of this is that an LLM will reuse information about natural languages to capture the meaning of the source code, much like human programmers do.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>In each case, we see that the tokenization is not perfect for the code. There are edge cases where the LLM’s tokenizer does not convert the data types in the code to the same token. For example, you can see that the token for <code>(double</code> in the function argument is handled differently from the token for <code>double</code> in the function body. However, these differences are similar to the problems we already see in LLMs for natural language, where different cases of punctuation around a word like “hello ”, “hello.”, and “hello!” are interpreted as different tokens. Since LLMs can handle these minor differences, it makes sense that they can also handle the same problem for code. The problem is, in many ways, easier for an LLM to handle in code because code is case sensitive, so we do not need to worry about textual situations like “hello” and “Hello” being inappropriately mapped to different tokens. In code, “hello” and “Hello” would be separate and distinct variable or function names. Treating them as separate tokens is correct because the programming language treats them as different elements.</p> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Code generation is particularly interesting from an application perspective because of the various opportunities for self-validation. We can apply all the lessons on supervised fine tuning (SFT) and reinforcement learning with human feedback (RLHF) from chapter <a href="../Text/chapter-5.html">5</a> to make an LLM an effective coding agent.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3" id="improving-llms-to-work-with-code"><span class="num-string browsable-reference-id">6.1.1</span> Improving LLMs to work with code</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>The first step to improving an LLM for code is ensuring that code examples are present within the initial training data. Due to the nature of the internet, most LLM developers have already done this: code examples are frequent online and naturally make their way into everyone’s training datasets.</p> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Improving the results then becomes an opportunity to apply SFT, where we collect additional code examples and fine-tune our LLM on the given code examples. Open source repositories like GitHub, which contain significant volumes of code, make obtaining a large amount of code especially easy. Code collected from sources such as GitHub forms the basis of a fine-tuning dataset for LLMs that interpret and produce code.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>The more interesting case is using RLHF to improve a model’s utility for writing code. Again, there are many tools and datasets available that make it possible to build a decent RLHF dataset for a coding assistant. Sources like Stack Overflow allow users to enter questions, provide a facility for other people to give answers to these questions, and include a system where other users vote on the best answers. Data sources include coding competitions like CodeJam, which provide many example solutions to a specific coding problem. Incorporating information from data sources like these is shown in figure <a href="#fig__RLHF_for_code">6.4</a>.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Like all good machine learning solutions, you get the best results if you create and label your own data specific to your task. It is rumored that OpenAI did this for generating code, hiring contractors to complete coding tasks as part of creating the data for their system [1]. Regardless of how training and fine-tuning data is collected, the overall strategy remains the same: use standard tokenizers and SFT with RLHF to make an LLM tailored to generate code. This recipe has been used successfully to produce LLMs such as Code Llama [2] and StarCoder [3].</p> 
  </div> 
  <div class="browsable-container figure-container" id="p31">  
   <img alt="figure" src="../Images/CH06_F04_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__RLHF_for_code"><span class="num-string">Figure <span class="browsable-reference-id">6.4</span></span> Developing an LLM for code applies multiple rounds of fine-tuning. Standard training procedures, such as those described in chapter <a href="../Text/chapter-4.html">4</a>, produce an initial base LLM. Using a large amount of code, SFT creates an LLM that works well with code. Including RLHF as a second fine-tuning step improves the LLM’s ability to produce code.</h5>
  </div> 
  <div class="readable-text" id="p32"> 
   <h3 class=" readable-text-h3" id="sec__code_validation"><span class="num-string browsable-reference-id">6.1.2</span> Validating code generated by LLMs</h3> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>LLMs are particularly useful for code generation because there is an objective and easy-to-run verification step: attempting to compile the code into an executable program [4]. When generating natural language, it is challenging to check the correctness of the output generated by an LLM because natural language can be subjective. There isn’t an automated way to check the truthfulness or veracity of the output generated by an LLM. However, when generating code, simply checking whether the code compiles successfully into an executable is a good first step and catches a large portion of the incorrect code. Some commercial products take this a step further and integrate tools such as compilers (software that transforms source code into executables) and visualization tools into their backend. For example, ChatGPT can check whether the code it writes compiles before returning it to the user. If the code doesn’t pass this verification step, ChatGPT will try to generate different code for the prompt it received. If the model cannot create valid code to compile, it will warn the user of this fact.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Beyond checking whether code can compile, LLMs are increasingly able to create methods for validating functional correctness. Many code generation tools utilize LLM to generate unit tests, which are tiny programs that provide sample input into generated code and validate that it produces the correct result. In some cases, these capabilities require the developer to describe the test cases that they want the LLM to generate, and the LLM creates an initial implementation as a starting point for further testing.</p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Code is particularly special because multiple ways exist to validate its output beyond just compilation. For example, code compilation can’t happen until the LLM finishes generating its response.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Considering that LLMs are expensive to run, and we don’t want to keep a user waiting too long for output, it would be ideal if the LLM could correct errors before completing a large generation. Again, applying the lessons from chapter 5, we can use a syntax parser to check whether the code is incorrect before completing the entire generation process. If portions of the output code fail a basic syntax check, we can instruct the LLM to regenerate just that faulty portion of code. We show the basic process behind this in figure <a href="#fig__codeSyntaxCheck">6.5</a>, where the LLM performs a check on a per-token basis instead of waiting for the generation to complete before checking the code using compilation. The syntax check is less expensive and can happen faster than compilation, but it does not validate that a compiler can turn the code into a working executable program.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p37">  
   <img alt="figure" src="../Images/CH06_F05_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__codeSyntaxCheck"><span class="num-string">Figure <span class="browsable-reference-id">6.5</span></span> A Python code example where the current tokens <code>if(A &gt; B)</code> have been generated. If the next token produced by the LLM is a newline, a syntax error will occur because an <code>if</code> statement must end in a colon to be valid. Running a syntax checker on each new token allows us to catch this error and force the LLM to pick an alternative token that doesn’t cause a syntax error. </h5>
  </div> 
  <div class="readable-text" id="p38"> 
   <h3 class=" readable-text-h3" id="improving-code-via-formatting"><span class="num-string browsable-reference-id">6.1.3</span> Improving code via formatting</h3> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Using parsers for syntax checking and compilers to produce working executables makes it far easier to adapt LLMs to the new problem domain of generating code. However, one additional trick is helpful. We can use tools known as <em>code formatters</em> (also known by programmers as <em>linters</em>) to change tokenization and improve performance. </p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>The problem is that there can be many ways to write code that performs the same functions yet is tokenized differently. Applying a linter to adjust source code formatting helps remove differences between two functionally equivalent, yet different pieces of code. While reformatting code is not a requirement to make code LLMs function well, it helps to avoid unnecessary redundancy that can occur. For example, consider the Java programming language that uses brackets to begin and end a new scope in a program. Various forms of white space are now nonimportant but would be tokenized differently, especially since the brackets are optional for a scope that only uses a single line of code! Figure <a href="#fig__codeForamtter">6.6</a> shows how these different legal formats exist for the code that performs the same functions and how we could, ideally, convert code to a single canonical representation.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p41">  
   <img alt="figure" src="../Images/CH06_F06_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__codeForamtter"><span class="num-string">Figure <span class="browsable-reference-id">6.6</span></span> A Java code example of how multiple ways to format the same code will lead to different tokenizations, even though each is semantically identical. Linters are a common tool to force code to follow a specific formatting rule. Instead, a linter can be used to create an identical “base” form, thus avoiding representing unnecessary information (like spaces versus tabs).</h5>
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Removing nonfunctional aspects of code is called <em>canonicalization</em>, meaning we convert code with formatting variations into a standard or “canonical” form. Here, we demonstrated a robust method of canonicalization by adding special tokens like <code>&lt;NEW SCOPE&gt;</code> that capture the fact that a new context exists for the <code>if</code> statement, regardless of whether it’s a single-line or multiline statement. Instead of adding special tokens, we can use formatting that is consistent across the code (e.g., always use spaces versus tabs, a newline before <code>{</code> or not). Both special parsing and formatting will improve the performance of a code LLM. The robust method, where we add special tokens, will yield better performance over formatting but has the added cost of writing and maintaining a custom parser for code that adds those special tokens. The problem of altering the tokenizer will be more critical in the next section when we discuss using LLMs for mathematics. </p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h2 class=" readable-text-h2" id="llms-for-formal-mathematics"><span class="num-string browsable-reference-id">6.2</span> LLMs for formal mathematics</h2> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>LLMs can also perform mathematical tasks that are usually quite challenging for humans to do successfully. These tasks are more than just performing operations like addition and subtraction to calculate numbers; they include formal and symbolic mathematics. We give an example of the kinds of formal math we are talking about in figure <a href="#fig__minerva_example">6.7</a>. You can ask these LLMs to calculate derivatives, limits, and integrals and write proofs. They can produce shockingly reasonable results.</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>LLMs for code are practical because we can use parsers and compilers to partially validate their outputs. Proper tokenization is paramount for making a helpful LLM for mathematics. Using LLMs for math is still a particularly active area of research [6], so the best ways to get an LLM to perform math are not yet known. However, researchers have identified some problems that cluster around the tokenization stage of building and running an LLM.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p46">  
   <img alt="figure" src="../Images/CH06_F07_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__minerva_example"><span class="num-string">Figure <span class="browsable-reference-id">6.7</span></span> A symbolic math problem that the Minerva LLM can solve correctly. While this example mixes natural language with mathematical content, the standard tokenization used by many LLMs would not allow this kind of mathematical output and can cause some surprising problems. (Image Creative Commons licensed from [5])</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p47"> 
   <p> <span class="print-book-callout-head">Note</span> In chapter 5, we mentioned that fine-tuning can be applied multiple times, and math LLMs are a great example of this. Researchers often create math LLMs by fine-tuning code LLMs, which are created by fine-tuning general-purpose text LLMs. Between SFT and RLHF at each stage, as many as three to six rounds of fine-tuning are applied to the original downstream LLM for math LLMs. </p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <h3 class=" readable-text-h3" id="sanitized-input"><span class="num-string browsable-reference-id">6.2.1</span> Sanitized input</h3> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Math LLMs often suffer from input preparation that may work well for natural language text but degrade representations of mathematical concepts. In text, formatted mathematics representations often involve symbols like <code>{}&lt;&gt;;^</code>. Special symbols like these are commonly removed from training data when working with regular text. Preserving this information requires rewriting input parsers for tokenization to ensure you do not remove the data you are trying to get your model to learn from.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Multiple representations for equivalent mathematical equations further complicate training LLMs to understand math in a similar way that multiple formatting may cause problems when processing programming languages. Several formats like TeX, asciimath, and MathML allow mathematical notation to be expressed using plain text but provide instructions for a typesetter to render equations correctly. These formats offer many different ways to represent the same equation. We show an example of this problem in figure <a href="#fig__mathRepresentation">6.8</a>. There are problems with the method of typesetting the math (i.e., how to draw the equation by picking TeX versus MathML) and the representation of the math (i.e., two mathematically equivalent ways of expressing the same thing).</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>These are both forms of a problem that has come up a few times in our discussion of LLMs: different ways to represent the same thing. In the case of mathematics, the current preference is to keep math formatted using TeX and very similar but less-frequent alternatives like asciimath and to discard verbose content like MathML. We base this motivation on three factors:</p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/CH06_F08_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__mathRepresentation"><span class="num-string">Figure <span class="browsable-reference-id">6.8</span></span> A mathematical equation in the top-left demonstrates two different representation problems that occur with math. The nicely formatted math requires a typesetting language. TeX and MathML are two different typesetting languages that have vastly different text and, thus, tokenization. Separate from the typesetting language, there are many ways to represent the same mathematical statement.</h5>
  </div> 
  <ul> 
   <li class="readable-text" id="p53">TeX-based formatted math is the most common and available form of math thanks to publicly available sources like arXiv, which consistently uses TeX formatting.</li> 
   <li class="readable-text" id="p54">Keeping all TeX-like representations mitigates the challenge of learning multiple formats and, thus, very different token sets.</li> 
   <li class="readable-text" id="p55">The more verbose MathML uses a larger variety of tokens; thus, more computing resources are required to store the data associated with each unique token.</li> 
  </ul> 
  <div class="readable-text" id="p56"> 
   <p>Choosing TeX as a single preferred representation for math in LLMs doesn’t solve the fact that there are multiple ways to write equivalent equations. Determining which equations are the same is so difficult that researchers have proven that no single algorithm can determine the equivalence of two mathematical expressions. (We are being a little loose with our words here, given that this section is on <em>formal</em> mathematics, so we will point you to the source [7].) So far, the best answer for LLMs appears to be “let the model try to figure that out,” which has been reasonably successful thus far. But we wouldn’t be surprised if the developers of future math LLMs invest heavily in improving preprocessing by creating more consistent canonical representations for mathematical equations that reduce the variety of possible expressions for equivalent expressions.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h3 class=" readable-text-h3" id="helping-llms-understand-numbers"><span class="num-string browsable-reference-id">6.2.2</span> Helping LLMs understand numbers</h3> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>For most people, numbers are the more accessible part of math. You can put them in a calculator and get the result. Although it may be tedious, you can perform calculations by hand if you do not have a calculator. One follows a fixed set of rules to get the result. Somewhat surprisingly, LLMs have a lot of trouble doing that sort of rote calculation, but developers have worked to improve tokenizers’ ability to work better with numbers.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>The first problem is that the standard byte-pair encoding (BPE) algorithm produces tokenizers that create inconsistent tokens for numbers. For example, “1812” will likely be tokenized as a single token because there are references to the War of 1812 in thousands of documents; tokenizers will possibly break up 1811 and 1813 into smaller numbers. To further explore why this happens, consider the initial string <code>3252+3253</code> and how GPT-3 and GPT-4 tokenize this string. GPT-4 will do a better job because it seems to tokenize numbers by starting with the first three digits every time, resulting in a three-digit number followed by a single-digit number. GPT-3 appears inconsistent because it changes the order in which it tokenizes numbers, as shown in figure <a href="#fig__additionTokenProblem">6.9</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/CH06_F09_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__additionTokenProblem"><span class="num-string">Figure <span class="browsable-reference-id">6.9</span></span> LLMs cannot learn to do basic arithmetic unless they tokenize digits consistently. In this figure, underlines denote different tokens. The tokenized digits might represent the tens, hundreds, or thousands place for any given number. GPT-3 (left) is inconsistent in how numbers get tokenized, making adding two numbers needlessly complex. GPT-4 is better (but not perfect) at tokenizing numbers in a consistent way.</h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Now a significant problem has occurred. The “3” token for GPT-3 occurs two times in two different contexts, once in the thousands place (<em>three-thousand</em> two hundred ...) and once in the tens place (three-thousand two hundred and fifty <em>three</em>). For GPT-3 to correctly add these numbers, the tokenizer must properly capture four different digit locations. In contrast, GPT-4 uses the order for digit representations for each number, making it easier to get the correct result.</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>People are still experimenting with different ways of changing the tokenizer to improve LLMs’ ability to work with numbers. If we are going to tokenize digits into subcomponents, the current best approach is to separate each number, like 3252, into individual digits, like “3, 2, 5, 2” [8]. However, other alternatives also exist.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>Another interesting approach for representing numbers is called <em>xVal</em> [9], with the idea of replacing every number with the same token that represents “a number.” We could call this special token <code>NUM</code>, which will get mapped to a vector of numbers by the embedding layer we learned about in chapter <a href="../Text/chapter-3.html">3</a>. </p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>The clever trick is to include a multiplier with each token, a second number multiplied against the embedded vector value. By default, the LLM uses a multiplier of 1 for every token. Multiplying anything by 1 does nothing. But for any <code>NUM</code> token we encounter, it will instead be multiplied by the original number from the text! This way, we can represent every possible number that might appear, even fractional values, including those that did not appear in the training data. Numbers captured in this manner are related in a simple and intuitive way. We show this in more detail in figure <a href="#fig__xVal">6.10</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p65">  
   <img alt="figure" src="../Images/CH06_F10_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__xVal"><span class="num-string">Figure <span class="browsable-reference-id">6.10</span></span> xVal uses a trick to help reduce the number of tokens and make them less ambiguous. By modifying how the LLM converts numbers to vectors, a single vector represents each number, such as the number 1. By always using the 1 token and multiplying it by the number observed, we avoid many edge cases in number token representation, such as numbers that never appeared in the training data. This conversion method also makes fractional numbers like 3.14 easier to support.</h5>
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Both the consistent digits and the xVal strategy share one important realization. We know how to represent math and simple algorithms like grade-school addition and multiplication. If we design the LLM to tokenize mathematics in a way that is more consistent with how we, as humans, do mathematical tasks, our LLMs get better and more consistent mathematical capabilities.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h3 class=" readable-text-h3" id="math-llms-also-use-tools"><span class="num-string browsable-reference-id">6.2.3</span> Math LLMs also use tools</h3> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>The astute reader may have noticed that most of the tokenization problems related to math involve handling digits and not symbolic math. LLMs cannot do essential addition or subtraction without changing the tokenizer and keeping typically “bad” symbols like <code>{}&lt;&gt;;^</code>. Enabling computation by changing the way the tokenizerhandles numbers may seem like a minor problem. Still, it is a significant factor for good symbolic performance and often insufficient for handling other forms of symbolic math. Obtaining the best possible performance on symbolic math relies on external tools and playing clever tricks with LLM output. </p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>If you ever had the TI-89 calculator that could solve derivatives for you, you know that computers can automate calculations without LLMs. Functionally, computer algebra systems (CAS) can provide this functionality. A CAS implements algorithms to perform some (but not all) mathematical steps. Calculating derivatives is one of them, so having an LLM use a CAS, like Sympy, helps ensure the LLM always performs specific steps correctly. However, the ability to integrate a CAS like Sympy into an LLM does not guarantee the entire sequence of steps will be performed correctly.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>To validate correctness, math LLMs have begun to use a programming language called <em>Lean</em>. In Lean, the program is a kind of mathematical proof, and the program will not compile if there is an error in the proof. It effectively makes incorrect proof steps one type of syntax error that can then be detected. Once detected, as we have shown in other examples, the output can be regenerated by the LLM until the proof, output as a Lean program, compiles successfully, just like we show in section <a href="#sec__code_validation">6.1.2</a>.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Using Lean can guarantee that a returned proof from an LLM is 100% correct, but there is no guarantee that the LLM can find the proof. Notably, there may also be cases where the LLM might be able to solve the problem correctly but might not be able to express the solved problem using Lean. We diagram the logic behind this problem in figure <a href="#fig__leanHard">6.11</a>, and it boils down to the fact that the effectiveness of tool use in LLMs depends on the variety of examples of the tool’s use in training data. Since Lean is relatively new and niche, there are few examples of fine-tuning an LLM to use Lean effectively. People like you and me will need to generate those examples to produce suitable training data to teach an LLM how to use Lean. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p72">  
   <img alt="figure" src="../Images/CH06_F11_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__leanHard"><span class="num-string">Figure <span class="browsable-reference-id">6.11</span></span> Given some mathematical goal, getting an LLM to use Lean (right path) might not result in a verifiably correct proof because it may not be effective at using Lean as a tool. Having the LLM produce a normal proof (left path) may yield a correct proof, but not a way for us to verify that it is (in)correct.</h5>
  </div> 
  <div class="readable-text" id="p73"> 
   <p>So what can you do if the LLM cannot provide verifiable proof that its math is correct? A trick used today is to run the LLM multiple times. Because the next token is selected randomly, you can potentially get a different result with a different answer each time you run the LLM. Whichever answer appears most frequently is most likely correct. This process does not guarantee the proof is correct, but it helps.</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h2 class=" readable-text-h2" id="transformers-and-computer-vision"><span class="num-string browsable-reference-id">6.3</span> Transformers and computer vision</h2> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The process of translating code and math to tokens is fairly intuitive. Code is fundamentally text used to tell computers what to do in a highly pedantic way. Math is difficult to convert into tokens, but we have discussed how it is possible. Computer vision is a different story, where the data involved is images or videos represented using pixels. The idea of tokens for images seems confusing. How on earth could we possibly convert an image into tokens? Images typically contain lots of detail, and you cannot just combine a bunch of small images into one coherent image like you do when you string words together to form a sentence. Nevertheless, we can apply transformers to images if we think about tokenization as a process to convert any input into a sequence of numbers.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p76"> 
   <p> <span class="print-book-callout-head">Note</span> <span>There was an approach to representing images as a combination of tiny images called <em>code books</em>. Code books can be useful, but not the same in the spirit of our discussion. Consider this a keyword nugget to explore if you want to learn about some older computer vision techniques.</span> </p> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>While high-quality image recognition algorithms and image generators existed for many years before transformers, transformers have rapidly become one of the premier ways to work with images in machine learning. Both vision transformer (ViT) architectures that strictly use transformers, as well as mixed architecture models such as VQGAN and U-Net transformer that mix transformers with other types of data structures, have seen great success in both interpreting image-based data and producing amazing computer-generated images from text descriptions. It may seem counterintuitive that transformers perform so well in images because images do not look like discrete sequences of symbols like natural language, code, or amino acid sequences do. Still, transformers fulfill a critical role in computer vision by bringing global cohesion to models.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3" id="converting-images-to-patches-and-back"><span class="num-string browsable-reference-id">6.3.1</span> Converting images to patches and back</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Conceptually, we will replace the tokenizer and embedding process with a new process that outputs a sequence of vectors similar to the embedding layers we discussed in section <a href="../Text/chapter-3.html#sec__transformer_layers">3.1.1</a>. The prevailing approach to creating a sequence representing an image is to divide the image into a set of <em>patches</em>. As a result, we will replace our tokenizer with a <em>patch extractor</em> that returns a sequence of vectors. The output of an LLM uses an unembedding layer to convert vectors back into tokens. Since we have no tokens, we need a <em>patch combiner</em> to take the outputs of a transformer and merge them into one coherent image. We show this process in figure <a href="#fig__tokensToPatches">6.12</a>. Please pay special attention to the fact that the central portion of the diagram remains the same as it was for text-based LLMs. We reuse the same transformer layers and learning algorithm (gradient descent) between text and images. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH06_F12_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__tokensToPatches"><span class="num-string">Figure <span class="browsable-reference-id">6.12</span></span> On the left, this simplified diagram shows how text input is tokenized and embedded before going to the transformer. An unembedding layer then converts the transformer output into the desired text representation. The input and output will be images when performing a computer vision task. The transformer stays the same, but then we modify the method for breaking the image into a sequence of vectors to perform patch extraction instead of tokenization. The LLM produces image output using a patch combiner, analogous to the unembedding layer for text LLMs.</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Since everything except the input vector sequence generation and output steps remains the same, we can focus on how the conversion of images to and from vectors works. It will be helpful to focus on the input side first.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>As the name <em>patch</em> implies, the patch extractor breaks up each image into asequence of smaller images. It is common to pick a fixed size for the patch, likea square of <span><img alt="equation image" src="../Images/eq-chapter-6-82-1.png"/></span> pixels. We want a fixed size so that it is easy to feed into a neural network, which always processes data of a fixed size, and small so that they represent just a piece of the entire image. Breaking an image into patches is similar to breaking text into a collection of tokens. Each individual token isn’t informative, but when combined with other tokens, it makes a coherent sentence.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Once an image is broken into patches, each pixel in that patch is converted to three numbers representing the amount of red, green, and blue (RGB) present in each pixel. An initial vector is created by combining each pixel’s RGB values into a single long vector. So for our square of <span><img alt="equation image" src="../Images/eq-chapter-6-83-1.png"/></span> pixels with three color values for each pixel, we will have a vector that is 768 values in length (16 height, 16 width, and an RGB value for each pixel). Then, a small neural network that might have only one or two layers processes each vector separately to make the final outputs. This neural network implements a very light feature-extraction process that does not require significant memory or computation resources. This design is common in computer vision because the first layer usually learns simple patterns like “dark inside, light outside” and does not need a transformer layer’s greater expense or power to learn the basic features of an image patch. This whole process is summarized in figure <a href="#fig__patchExtractor">6.13</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p84">  
   <img alt="figure" src="../Images/CH06_F13_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__patchExtractor"><span class="num-string">Figure <span class="browsable-reference-id">6.13</span></span> Extracting patches is a straightforward process. The patch extractor breaks up an image into square tiles called patches. Images consist of pixel values that are already numbers, so we convert each patch into a vector of numbers. Then, we use a small neural network as a preprocessor before passing the vectors to the full transformer-based neural network. </h5>
  </div> 
  <div class="readable-text" id="p85"> 
   <p>There are many possible ways to design the small neural network used in the patch extractor, but all generally work equally well. One option is to use what is called a <em>convolutional neural network</em> (CNN), which is a type of neural network that understands that pixels near each other are related to each other. Others have used just the same kind of linear layer that is a component of a transformer layer. In this case, the overall model that includes the small neural network and a series of transformers is often called a <em>vision transformer</em>.</p> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>The design of the small network is a minor detail but worth mentioning because its existence is relevant to the patch combiner that produces the final output. It does not matter whether you pick a CNN or a linear layer for the architecture of the small neural network, but it is essential to ensure the output’s shape matches the input’s shape. For example, if you have <span><img alt="equation image" src="../Images/eq-chapter-6-86-1.png"/></span> patches, you can use the small network to force the output to have <span><img alt="equation image" src="../Images/eq-chapter-6-86-2.png"/></span> values, regardless of the size of the transformer layer itself. To produce image output, you reverse the patch extraction process to convert the vectors into patches and then combine the patches into an image, as shown in figure <a href="#fig__patchCombiner">6.14</a>.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>We have thus successfully replaced the input tokenization and the output embedding with new image-centric layers. In many ways, this is much nicer than tokenization. There is no need to build/keep track of a vocabulary, no sampling process, etc. This is a crucial insight into the general applicability of transformers as the general-purpose core of an LLM. If you can find a lot of data and a reasonable method of converting that data into a sequence of vectors, you can use transformers to solve certain classes of input and output problems.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p88">  
   <img alt="figure" src="../Images/CH06_F14_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__patchCombiner"><span class="num-string">Figure <span class="browsable-reference-id">6.14</span></span> Compared to figure <a href="#fig__patchExtractor">6.13</a>, the arrows here go in the opposite direction. The purpose is to emphasize that the patch combiner and extractor do the same thing but operate in different directions. The neural network is more important in this stage as a way to force the transformer’s output to have the same shape as the original patches because we can control the output size of any neural network.</h5>
  </div> 
  <div class="readable-text" id="p89"> 
   <h3 class=" readable-text-h3" id="multimodal-models-using-images-and-text"><span class="num-string browsable-reference-id">6.3.2</span> Multimodal models using images and text</h3> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>The ability to change the input and output of an LLM to arrive at a vision transformer means that we can take an image as input and produce an image as output. It demonstrates how a transformer can produce input of different modalities, but we have only discussed cases where the input and output are the same modality. We either have text as input and text as output or images as input and images as output. However, deep learning is flexible! There is nothing that forces us to use the same modality as both input and output or even restrict input and output to be a single modality. You can combine text as input with image as output, images as input and text as output, text and images as input and audio as output, or any other data modality combinations you might think of. Figure <a href="#fig__mixAndMatch">6.15</a> shows how image and text give us four total ways we might combine them to handle different kinds of data.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>By creating a model that uses images as input and text as output, we create an <em>image captioning model</em>. We can train this model to generate text describing the input image’s content. Models such as these help make images more discoverable and aid visually impaired users.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>By creating a model that uses text as the input and an image as the output, we create an <em>image generation model</em>. You can describe a desired image using words, and the model can create a reasonable image based on your input. Famous products like MidJourney are models of this flavor. Though their implementation involves more than just a vision transformer, the high-level idea is the same: by pairing a text-based input with image-based output and a lot of data, we can create new multimodal capabilities that span different data types.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p93">  
   <img alt="figure" src="../Images/CH06_F15_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__mixAndMatch"><span class="num-string">Figure <span class="browsable-reference-id">6.15</span></span> Four combinations showing different types of model input and output. The example at the furthest right represents a normal text-based LLM we have already learned about. To the left, we show possibilities like an image-generating model that takes text as input (“Draw me a picture of a stop sign in a flood zone”) or an image captioning model that generates text that describes an image input (“This picture shows a stop sign surrounded by murky water”).</h5>
  </div> 
  <div class="readable-text" id="p94"> 
   <h3 class=" readable-text-h3" id="applicability-of-prior-lessons"><span class="num-string browsable-reference-id">6.3.3</span> Applicability of prior lessons</h3> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Other lessons learned throughout this book remain relevant to these vision transformer and multimodal models. Ultimately, they learn to do what they are trained for, and when you try to bend them in ways beyond what is found in the training data, you may get an unusual result. As an example, we might tell an image generation model “Draw anything but an adorable cat,” and you will probably end up with a cat as shown in figure <a href="#fig__adorableCat">6.16</a></p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>These models are (currently) trained with pairs of images and pieces of text describing the image. Thus, they learn a strong correlation to produce visualizations of anything in the input sentence. For example, the model wants to produce a cat since the word <em>cat</em> is in the input sentence. More sophisticated abstract drawing requests like “Draw anything but” do not appear in such datasets, and so the model is not trained to handle such a request.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Similarly, as LLMs like ChatGPT have developed prompting as a strategy for devising inputs that produce desired outputs, prompting has also been developed for image captioning models. It is not uncommon to include unusual information like “Unreal3D,” the name of software used to generate 3D imagery for computer games to produce output with a particular style and quality. Words like <em>high resolution</em> and even the names of artists, alive and dead, are used to try to influence the models into producing particular styles.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p98">  
   <img alt="figure" src="../Images/CH06_F16_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__adorableCat"><span class="num-string">Figure <span class="browsable-reference-id">6.16</span></span> This was generated with an old version of Stable Diffusion, a popular image generation model. Despite telling the model “Do not draw a cat,” the model was trained to generate content. The request is outside what the model was incentivized to learn, so it cannot handle it. This is similar to the problems with LLMs regurgitating close-but-wrong output because the model saw similar data during training.</h5>
  </div> 
  <div class="readable-text" id="p99"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p100">LLMs benefit when they can use external tools. For example, a code LLM can use syntax checkers and compilers to detect erroneous code generation. When the LLM finds an error, the output is regenerated, minimizing the risk of giving the user unhelpful or broken code.</li> 
   <li class="readable-text" id="p101">Tokenizers must be modified to support math by keeping unusual symbols used to express formatted math and changing digit representations. We can improve math LLMs further by giving them tools like computer algebra systems to detect and avoid errors.</li> 
   <li class="readable-text" id="p102">Transformers can be applied to images by breaking up an image into patches, where each patch becomes a vector and makes a sequence of inputs for the transformer to process. Patches are conceptually similar to tokens for text LLMs.</li> 
   <li class="readable-text" id="p103">Transformers can use different data modalities for input and output, allowing the creation of multimodal models like those used in image captioning and image generation.</li> 
  </ul>
 </body></html>