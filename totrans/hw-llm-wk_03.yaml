- en: 4 How LLMs learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training algorithms with loss functions and gradient descent
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How LLMs mimic human text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How training can lead LLMs to produce errors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in scaling LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The words *learning* and *training* are commonly used in the machine learning
    community to describe what algorithms do when they observe data and make predictions
    based on those observations. We use this terminology begrudgingly becausealthough
    it simplifies the discussion of the operations of these algorithms, we feel that
    it is not ideal. Fundamentally, this terminology leads to misconceptions about
    LLMs and artificial intelligence. These words imply that these algorithms have
    human-like qualities; they seduce you into believing that algorithms display emergent
    behavior and are capable of more than they are truly capable of. At a fundamental
    level, this terminology is incorrect. A computer doesn’t learn in any way similar
    to how humans learn. Models do improve based on data and feedback, but it is incredibly
    important to keep this mechanistically distinct from anything like human learning.
    Indeed, you probably do not want an AI to learn like a human: we spend many years
    of our lives focused on education and still make dumb decisions.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms train in a way that is far more formulaic than how
    humans learn. It is formulaic in the literal sense of using a lot of math and
    the figurative meaning of following a simple repetitive procedure billions of
    times until completion. We will spare you the math, but in this chapter, we will
    help you remove the mystery of how LLMs are trained.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Many machine learning algorithms use the training algorithm called *gradient
    descent*. The name of this algorithm implies some details that we’ll review with
    a high-level overview of how gradient descent is used for machine learning. Once
    you understand the general approach used to train many different model types,
    we will explore how gradient descent is applied to LLMs to create a model that
    produces convincing textual output.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these details will help you avoid inaccurate connotations implied
    by words like *learn*. More importantly, it will also prepare you to understand
    better when LLMs succeed and fail in their current design and the often-subtle
    ways such algorithms can produce misleading outputs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Gradient descent
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Gradient descent* is the key to all modern deep-learning algorithms. When
    an industry practitioner mentions gradient descent, they are implicitly referring
    to two critical elements of the training process. The first is known as a *loss
    function*, and the second is calculating *gradients*, which are measurements that
    tell you how to adjust the parameters of the neural network so that the loss function
    produces results in a specific way. You can think of these as two high-level components:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss function*—You need a single numeric score that calculates how poorly
    your algorithm works.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gradient descent*—You need a mechanical process that tweaks the numeric values
    inside an algorithm to make the loss function score as small as possible.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度下降*——你需要一个机械过程来调整算法内部的数值，以使损失函数的得分尽可能小。'
- en: The loss function and gradient descent are components of the training algorithm
    used to produce a machine learning model. Many different training algorithms are
    in use today, but generally, each algorithm sends inputs into a model, observes
    the model’s output, and tweaks the model to improve its performance. A training
    algorithm will repeat this process a tremendous number of times. Given enough
    data, a model will produce the expected outputs repeatedly and reliably when confronted
    with previously unseen input.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和梯度下降是用于生成机器学习模型的训练算法的组成部分。今天正在使用许多不同的训练算法，但通常，每个算法将输入发送到模型中，观察模型的输出，并调整模型以提高其性能。训练算法将重复这个过程无数次。给定足够的数据，当面对之前未见过的输入时，模型将反复可靠地产生预期的输出。
- en: 4.1.1 What is a loss function?
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 什么是损失函数？
- en: 'We will use the example of wanting to make money to help develop a mental picture
    of a suitable loss function. Indeed, an intelligent person can make money, so
    if you have an intelligent computer, it should be able to help you make money.
    To pick a suitable loss function for this or any other task (these lessons generalize
    to any ML problem beyond LLMs), we need to satisfy three criteria: *specificity*,
    *computability*, and *smoothness*. In other words, the loss function needs to
    be'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用想要赚钱的例子来帮助您形成一个合适的损失函数的心理图像。确实，一个聪明的人可以赚钱，所以如果你有一个聪明的计算机，它应该能够帮助你赚钱。为了选择这个或其他任何任务的合适损失函数（这些课程可以推广到任何超出LLMs的ML问题），我们需要满足三个标准：*特异性*、*可计算性*和*平滑性*。换句话说，损失函数需要是
- en: Specific and correlated with the desired behavior of the model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具体且与模型期望的行为相关
- en: Computable in a reasonable amount of time with a reasonable amount of resources
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在合理的时间和资源量内可计算
- en: Smooth, in the sense that the function’s output does not fluctuate wildly when
    given similar inputs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑，即在给相似输入时，函数的输出不会剧烈波动
- en: We will use the following examples and counterexamples to help you develop an
    intuition for each property.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下示例和反例来帮助您对每个属性形成直观理解。
- en: Loss function specificity
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 损失函数的特异性
- en: First, let’s start with a bad example of specificity. If your boss came to you
    and said, “Build an intelligent computer,” that would be a magnificent goal, but
    it is not a specific goal. Remember, in chapter 1, we discussed how difficult
    it is to define intelligence. What exactly does your boss want this computer to
    be intelligent at? Would a street-smart computer that cannot do your calculus
    homework suffice? Instead, you could try to optimize for a specific IQ score,
    but does that correlate with what your boss wants? We have been able to get computers
    to pass IQ tests for over a decade [1], even before the introduction of LLMs.
    However, they could not do anything other than pass an IQ test and perform limited
    tasks. Ultimately, the IQ test does not correlate with what we want computers
    to do. As a result, it is not worth optimizing IQ as a metric for success in machine
    learning or for building the intelligent computer your boss asked you to create.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从一个特异性的坏例子开始。如果你的老板来找你说，“建造一个智能计算机”，那将是一个宏伟的目标，但它不是一个具体的目标。记住，在第一章中，我们讨论了定义智能的难度。你的老板到底希望这台计算机在哪些方面表现出智能？一个只能通过智力测试但不能做你的微积分作业的街头智能计算机就足够了吗？相反，你可以尝试优化一个特定的智商分数，但这与你的老板想要的东西相关吗？我们已经在十多年前让计算机通过智商测试了，甚至在LLMs引入之前。然而，它们除了通过智商测试和执行有限的任务外，什么也不能做。最终，智商测试与我们希望计算机做的事情不相关。因此，将智商作为机器学习成功或构建老板要求你创建的智能计算机的度量标准是没有意义的。
- en: 'Another example involves the challenge of managing money. Consider a scenario
    where you want to minimize the debt you carry. You might even want your debt to
    go negative, meaning others owe you money! We use the example of debt here because
    it is intrinsically a value you want to make smaller. This analogy aligns perfectly
    with the terminology used in practice: you want to minimize your loss just as
    you want to reduce your debt. The volume of debt is also an objective measure,
    making it a good way of ensuring our loss function is relevant under changing
    conditions. Finally, if our overall goal is to maintain a surplus of money, minimizing
    debt correlates well with that goal. Minimizing debt has all of the characteristics
    of a good loss function!'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: A note on terminology
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You may also hear loss functions described as *objective functions*. We recommend
    avoiding this term as a newcomer because it is ambiguous. For example, it is unclear
    whether you want to minimize (debt) or maximize your objective (profit). Both
    approaches technically work; multiply a maximizing objective by ![equation image](../Images/eq-chapter-4-25-1.png),
    and you now have a minimizing objective.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: You may also hear the term *reward function* used in some contexts, such as
    reinforcement learning (RL). This is appropriate because RL algorithms seek to
    maximize reward by performing a desirable behavior.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the terminology, objective functions, reward functions, and loss
    functions all address the same fundamental requirement: they provide a way of
    evaluating the outputs that a machine learning model produces.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Loss function computability
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The loss function must also be something we can compute quickly with a computer.
    The debt example is unsuitable for this aspect because all the inputs and outputs
    you need are not readily available to a computer. Will working harder at your
    job increase your income and thus lower your debt? Maybe, but how will we encode
    your hard work into the computer? Here, we have the problem that the most critical
    factors to minimizing debt are hard to quantify, like job availability, your fit
    for such jobs, likelihood of promotion, etc. So the loss is specific, but the
    inputs that connect to that loss are not computable.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'A better, more computable goal would be to predict the loss on an investment.
    The reasons this goal is better are subtle. The goal is still objective because
    our algorithms learn from historical data. For example, a historic investment
    in bonds X and stocks Y had certain returns. The inputs are also now objective:
    you can quantify the amount of cash you put into each investment. You either put
    money in, or you took it out. There are no hard-to-encode problems like “hard
    work” to deal with. With a copy of historical data, a computer can quickly calculate
    the loss/return on an investment.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Loss function smoothness
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The third thing we need is smoothness. Many people have good intuition for what
    smoothness means by thinking about a smooth versus bumpy texture. Instead of texture,
    we’re talking about the smoothness of a function, which can be depicted by drawing
    that function as a graph. For example, when trying to predict a loss on an investment,
    we run into the problem that investment returns are not usually smooth. They may
    follow a pattern of volatility where price graphs are jagged with sharp, sudden
    changes. This makes learning difficult. A graph showing the unstable values of
    real-world investment returns is shown in figure [4.1](#fig__stock_returns).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F01_Boozallen.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 Investment returns are not easy to predict, partly because they are
    not smooth. (Image modified from [2] under the Creative Commons license )
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Return on investment is an excellent example of a bad (nonsmooth) loss because
    erratic behavior is problematic for any predictive approach. It would be best
    if you were always cautious of anyone or any approach that claims to work well
    in predicting nonsmooth data like this. However, there is a precise technical
    definition of smooth that, if not satisfied by a loss function, is a hard deal-breaker.
    Functions that depend on discontinuities, or breaks in the consistency of their
    values, are the most common functions that are not technically smooth, but we
    would like to be able to use them in practice. Some examples of nonsmooth functions
    are shown in figure [4.2](#fig__nonSmooth) to help you understand. Smoothness
    is usually inhibited due to discontinuities, such as that shown in the center
    graph, or distinct changes in the value of a function, as shown in the graph on
    the right.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F02_Boozallen.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 Examples of a smooth function on the left and two nonsmooth functions
    on the right. The center example is mostly smooth, but one region is not smooth
    because the function has no value. On the right, the function is not smooth anywhere
    due to the hard change in value.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We won’t go deep into the formal mathematical definitions that describe what
    makes something smooth and what value changes are acceptable or unacceptable in
    smooth functions. Still, we’ve given you enough background to understand what
    you need to know. The important thing for you to understand is that your intuition
    of what smooth means, that the value changes continuously, is a good barometer
    for how viable a loss function is. This may seem arbitrary, but it is an ubiquitous
    problem. Say you want to build a model to predict cancer accurately. Accuracy
    is not a smooth function because you count the number of successful predictions
    out of the total predictions. For example, if you had 50 patients and predicted
    48 of them correctly, a smooth function would have an option for 48.2 cases, 47.921351
    cases, or any number you might think of. However, the actual count of cancer cases
    is constrained to the integers 1, 2, 3, ![equation image](../Images/eq-chapter-4-36-1.png),
    48, 49, 50 because there is no such thing as a partial case of cancer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: How do you handle nonsmooth losses?
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It may be shocking that accuracy is one of the most common predictive goals,
    but we cannot use it when training an algorithm. But it is true! So how do we
    handle this strange phenomenon? The answer is to create a *proxy problem*. A proxy
    problem is an alternate way of representing a problem that correlates with what
    we want to solve but is better behaved. In this case, we use a cross-entropy loss
    function instead of accuracy. While we won’t go into the details of cross-entropy
    loss here, its use demonstrates that proxy problems are fundamental tricks used
    in machine learning and artificial intelligence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'This discussion leads us to another critical takeaway about how LLMs learn,
    which is true of most algorithms: the technique we use to train them is not always
    focused on what we want them to do but on what we can make them learn. This focus
    can lead to an incentive mismatch, leading to unexpected results or low performance.
    We will discuss how the nature of an LLM’s loss function creates this incentive
    mismatch after examining the second major training component: gradient descent.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 What is gradient descent?
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having a loss function is a prerequisite for performing a gradient descent.
    The loss function tells you objectively how poorly you are performing the task.
    Gradient descent is the process we use to figure out how to tweak the parameters
    of the neural network to reduce the loss incurred. This is done by comparing the
    input training data and the actual versus expected outputs of the neural network
    using the loss function. In this case, the gradient is the direction and amount
    that you need to change the parameters of a neural network to reduce the amount
    of error measured by the loss function. Gradient descent shows us how to tweak
    all the parameters of a neural network “just a little bit” to improve its performance
    and reduce the difference between the expected and actual outputs. A diagram of
    this process is shown in figure [4.3](#fig__nn_gd_tweaks).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F03_Boozallen.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Inputs and labels (the known correct answers for each input) are
    used to tweak the neural network during gradient descent. A network is made of
    parameters that are altered a small amount each time gradient descent is applied.
    We eventually transform the network into something useful by applying gradient
    descent millions or billions of times.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As figure [4.3](#fig__nn_gd_tweaks) shows, we create a new, slightly different
    network every time we apply gradient descent. Because the changes are small, this
    process has to be performed billions of times. This way, all the small changes
    add up to a more significant, mean-ingful change in the overall network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Note Modern LLMs perform billions of parameter updates because they are trained
    on billions of tokens. The more data you have, the more times you run gradient
    descent. The less data you have, the less often you need to run it. The data used
    to train an LLM is more than you could read in a lifetime.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is a mathematical process that is applied repeatedlywithout
    deviation. There are no guarantees that it will work or find the best or even
    a good solution. Nevertheless, many researchers have been surprised by how practical
    this relatively simple approach is.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: To help you understand how gradient descent works, we will use a simple example
    of rolling a ball down a hill. The ball’s location represents a parameter value
    for a node in the neural network that the training algorithm can alter. The hill’s
    height is the amount of loss and describes how poorly the model performs for the
    training input. We want to roll the ball down the hill into the deepest valley
    because that is the area with the lowest loss, which indicates that the model
    is performing its best. An example of this is shown in figure [4.4](#fig__gd_start)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F04_Boozallen.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 This shows the global big picture of gradient descent applied to
    a single parameter problem. The curve illustrates the value of the loss function
    for a given parameter value. The ball’s location shows the loss for the current
    parameter value. The goal is to find the parameter values corresponding to a global
    minimum representing the ideal solution with the least loss.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you can see, the ball could fall into many valleys. The industry jargon would
    be to call this problem *nonconvex* because multiple paths lead to reduced loss,
    but each path does not necessarily progress toward the best possible solution.
    It is also important to note that this is not an analogy. Gradient descent literally
    looks at the world this way. These examples show how gradient descent works for
    a model with one parameter to optimize. The same procedure is applied to billions
    of parameters when training an LLM.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: So from this position, we greedily look at which direction to move the ball
    downhill. We apply gradient descent two times in figure [4.5](#fig__gd_steps).
    This shows that the greedy option is to the left. When we move to the left by
    adjusting our parameter, we slightly move the ball down the slope. From the graph,
    you can see that a better solution exists by searching to the right, but due to
    the algorithm’s simplicity, it is unlikely that gradient descent will find it.
    Finding the optimal result in this case would require a more intelligent strategy
    involving searching and exploration, which is too costly to do well in practice.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F05_Boozallen.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 The gradient descent algorithm takes steps to adjust parameters to
    find the optimal outcome with the least loss. Unfortunately, the algorithm gets
    stuck in a local minimum, an area of the graph that is not optimal because other
    parameter values correspond to areas with a lower loss.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Also, notice that in the second step in figure [4.5](#fig__gd_steps), the ball
    gets stuck. While it is evident that continuing to move to the left will achieve
    an even lower loss, this result is only obvious because we can see the whole picture.
    Gradient descent cannot see the entire picture or even what is nearby. It only
    knows the exact location due to the current parameters and the loss function.
    Hence, it is a *greedy procedure*. Greedy procedures such as gradient descent
    are simplified approaches with the desired property of computability in that they
    are not prohibitively expensive to run many times to achieve an outcome. Greedy
    procedures are short-sighted because they choose the next optimal step based only
    on the current state, although broader, more optimal solutions may exist. They
    do this because evaluating the current and all possible future states would be
    impossible due to the number of potential outcomes that need to be considered.
    It would simply be too much to compute. The hope is that making many simple optimal
    decisions using limited information will generally lead to the most positive outcome—in
    this case, minimizing the value of the loss function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Important nuances in gradient descent
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this discussion of gradient descent, we have skipped some important nuances
    that need to be considered for real-world use. First, as described here, gradient
    descent would need to use all of the training data simultaneously, which is computationally
    infeasible. Instead, we use a procedure called *stochastic gradient descent* (SGD).
    SGD is precisely the same as we’ve described, except it uses a small random subset
    of the training data instead of the entire dataset. This dramatically reduces
    the memory required to train the model, resulting in faster, better solutions.
    This method works because gradient descent only makes small changes in the current
    greedy direction. It turns out that a little data is almost as good as using all
    the data when figuring out which step to take next. If you have a billion tokens,
    you can take a billion SGD steps in about the same amount of time it takes to
    do one standard gradient descent step using all the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Many training approaches use a particular form of SGD called *Adaptive MomentEstimation*
    (Adam). Adam includes some extra tricks to help minimize the loss function faster
    and avoid getting stuck. Adam’s main trick is that it gives the ball some momentum,
    which builds as updates continually move in one direction. Thismomentum causes
    the ball to roll down the hill faster and means that if a small local minimum
    is hit, there might be enough momentum to plow past that point and continue onward,
    thus reaching the area of the loss function graph with the smallest amount of
    loss.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The downside of Adam is that storing this information about momentum for each
    parameter increases the memory required for training by a factor of three compared
    to plain SGD. Memory is the most critical factor when building LLMs because it
    often determines how many GPUs you need, translating to cash out of your pocket.
    Although Adam won’t make the final model larger because you can throw away the
    data related to Adam’s extra momentum calculations once you are done training,
    you still need a system large enough to perform the training in the first place.
    The increased accuracy that comes with Adam’s ability to minimize loss more effectively
    comes with a distinct price.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 LLMs learn to mimic human text
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand how deep learning algorithms are trained by specifying
    a loss function used with gradient descent, we can discuss how this is applied
    to LLMs. Specifically, we will focus on the data and loss or reward functions
    used to train LLMs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are generally trained on human-authored text. Specifically, they’re explicitly
    trained to mimic texts produced by humans. While this sounds a bit obvious (what
    else would they be trained to do?), this detail is commonly missed or confused
    with other things, even by experts in the field. In particular, language models
    are *not* trained to do any of the following things:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Memorize text
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate new ideas
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build representations of the world
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produce factually accurate text
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is essential to explain this notion further before we go deeper. When one
    trains a model to play chess, the model learns to play well because it gets rewarded
    for winning. A language model, by contrast, only gets rewarded for producing text
    that
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: looks exactly like the training data. Consequently, all text generated by the
    LLM that *looks like text in the training corpus* produces high rewards (or low
    loss), even when those generations are not truthful or factual. This is an example
    of misalignment between the loss function and the designer’s higher-level goal,
    as discussed insection 4.1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained on datasets of hundreds of gigabytes of text scraped from the
    internet. The internet is famous for containing a large amount of incorrect (and
    weird) information. LLMs that are better at most tasks often end up being worse
    at tasks that are commonly misrepresented in their training data (see the Inverse
    Scaling Prize at [https://github.com/inverse-scaling/prize](https://github.com/inverse-scaling/prize)).
    For example, researchers have consistently found that better language models are
    also better at reproducing common knowledge that is false [3], mimicking stereotypes
    and social biases [4]. They tend to fall into a downward spiral that reinforces
    errors. For example, after generating code that contains bugs, they’re more likely
    to generate code that contains additional bugs [5]. These things are commonly
    represented in the training text, so LLMs are positively rewarded for predicting
    them even though it’s wrong. Thus, getting better based on its loss function for
    an LLM also means getting worse at these tasks that require truth and correctness.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 LLM reward functions
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we said that LLMs are rewarded for producing data that “looks like
    its training data.” In this subsection, we will explore what this means more concretely.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained by being shown the first couple of tokens of a sentence and
    having it predict the next token. The loss is based on the accuracy of that prediction
    compared to the training data. For example, it might be shown “This is a” and
    be expected to produce “test.” If the model produces “test,” it gets a point,
    and if it does not, it loses a point. This process is done for all beginning segments
    of the text, as shown in figure [4.6](#fig__autoregression). Here, it is trained
    to predict each of the highlighted words independently. This setup is not unique
    to LLMs. It has been used to train recurrent neural networks (RNNs) for many years.
    However, an essential part of why LLMs have become so popular is that they can
    be trained much more efficiently than an RNN. An RNN must be trained on each generation
    *sequentially* because each newly generated word depends on the prior words chosen.
    An LLM can be trained on all generations *in parallel* due to the transformer
    architecture discussed in chapter 3\. The ability to train a model on related
    generations in parallel represents a massive speed-up, allowing training at a
    large scale, and is a prerequisite for building today’s state-of-the-art LLMs
    using terabytes of data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F06_Boozallen.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 An LLM sees this sentence nine times, each time learning from the
    prediction of a single word at the end of each of the nine sequences.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We discussed how predicting the next token can be problematic because the algorithm
    may be incentivized to produce incorrect or factually errant outputs. We must
    also discuss the intuition behind why, despite this, this approach can produce
    such convincing outputs. It is reasonable to ask: How can an algorithm trained
    to create the next most likely token seemingly perform something we could mistake
    for reasoning?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: To develop this intuition, imagine how you might try to predict the next token
    for a given sentence. A computer has no pressure to respond quickly, so take your
    time. Consider the sentence “I love to eat <blank>,” and try to guess what word
    might go into the <blank>. The earlier parts of the sentence give you valuable
    context. Since we are discussing eating, you can almost immediately narrow the
    scope to a food item. Keeping a list of all possible food items is not difficult
    for a computer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Now if you consider the background of the authors of this book, you will have
    even more context. We are Americans in a common geographical area, which makes
    specific cuisines more likely than others. An LLM will not have this background,
    but if the sentence was longer and had more context, you could start to narrow
    down the choices in the same way as shown in figure [4.7](#fig__contextHelps).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F07_Boozallen.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Context can help you make decent predictions about the next word.
    As you move from left to right, additional text that might occur in a sentence
    is added. The images in the thought bubble for each sentence show how the added
    context eliminates predictions.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you identify keywords or phrases in the preceding text, you can gain insight
    into the best word to predict next. A computer performing these calculations does
    far more processing than a human requires. This kind of brute-force association
    mainly narrows the scope to something very reasonable. Again, the model will be
    updated billions of times to refine these associations and thus acquire a useful
    capability correlated with our goals of an algorithm able to understand and react
    to human text.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: However, correlation is not causation, and the next-word prediction strategy
    can lead to humorous errors. LLMs are susceptible to a “begging the question”
    error, where the premise of the question implies something untrue. Since the LLM
    is not trained for accuracy or contradiction, it attempts to produce a sequence
    of human-like text predictions that might follow your misleading question. An
    example of ChatGPT struggling with this kind of problem is given in figure [4.8](#fig__spaghettiStong),
    where we ask about the exceptional strength of dry spaghetti.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F08_Boozallen.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 While predicting the next token is powerful, it doesn’t imbue the
    network with reasoning or logic abilities. If we ask ChatGPT something absurd
    and untrue, it happily explains how it happens.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The core of why spaghetti can support hundreds of times its own weight is absurd
    and untrue. However, the algorithm has been primed to provide an answer about
    material tensile strength by formatting the question: “Why is it that X is so
    strong?” The model can extract this key context. Previous training data likely
    explains such material properties based on a factual question, which informs the
    model predicting that a similar response is appropriate. The subject of the sentence
    (spaghetti) and object (10 lb. weight) are used to inform minor details of the
    response, which is otherwise generic.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 LLMs and novel tasks
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The nature of the autoregressive, next-word prediction strategy and its use
    as a loss or reward during the training process gives us valuable insight into
    the nature of an LLM’s generated responses and how they can potentially be factually
    inaccurate. However, it also shows us why LLMs can be effective for looking up
    information, as a far more powerful keyword search than a standard search engine.
    There are ways to design around the limitations of nonfactual responses. For example,
    many LLM approaches add citations to the generated output so that it is possible
    to quickly verify that factually accurate content was used to produce the generated
    text. An LLM can also be a valuable sounding board, a pseudo-partner to bounce
    ideas off of as a source of inspiration and creativity. Critically, this also
    helps you understand a key case where you should avoid LLMs because they will
    be more likely to produce errors—novel problems and tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are generally not good at performing novel tasks. Figuring out if your
    task is novel can be pretty challenging, as the internet is weird. Tons of random
    things exist on the internet, including competitions on how to programmatically
    draw ducks and unicorns [6]. If the task is sufficiently similar to one already
    seen before or structurally similar to other things in the training data, you
    may end up with something that appears reasonable. This result can be extremely
    useful, but it can degrade as your task becomes more unique compared to what exists
    in the training data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: For example, we asked ChatGPT to write code that calculates the mathematical
    constant ![equation image](../Images/eq-chapter-4-81-1.png) (pi) in Python. This
    task is not novel; tons of code like this exists online, and ChatGPT faithfully
    returns the correct code for us.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 ChatGPT calculating pi in Python
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Tests the function; the more terms, the more accurate the approximation'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Now let us force ChatGPT to do some not terribly challenging extrapolation.
    We asked ChatGPT to translate this function to the programming language Modula-3\.
    This task is not too big of an extrapolation; Modula-3 is a programming language
    with a similar style and a historically significant programming language that
    influenced the eventual design of almost all the most popular languages today!
    However, it is excessively esoteric. You can find very few examples of this programming
    language today, mainly in the context of university compiler classes. The next
    listing shows Chat-GPT’s reasonable attempt. As you may have been able to predict
    from the context of this chapter thus far, ChatGPT made some errors, marked in
    the listing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 ChatGPT calculates pi in Modula-3
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Missing EXPORTS Main;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '#2 ** isn''t an operator.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '#3 PutReal can take only one optional second argument, and it''s not an integer.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: This short program has three errors that would prevent it from working. It is
    more interesting that ChatGPT gets these wrong because it confidently extrapolates
    standard coding practices from other languages. (In this case, *confidently* means
    that ChatGPT does not warn us of its potential errors. One of the authors likes
    to say that ChatGPT sounds like their most overconfident and often incorrect friend.)
    In this case, `**` is a commonly used exponentiation function, so ChatGPT decides
    that Modula-3 supports this operation. As far as we can tell from scouring the
    internet, Modula-3 has no documented example of how to exponentiate a variable.
    Because most programming languages support this action with a `^`, `**`, or `pow()`
    option, Chat-GPT just extrapolates one into existence. The correct answer would
    be that it must first implement a `pow` function and then use it to compute pi.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The arguments provided to the `PutReal` function are another mystery. Our best
    guess is that the `15` corresponds to an extrapolation of printing out 15 digits
    of a floating-point value, a typical default when calculating pi. Regardless,
    it is not how that function works.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The more significant point is that ChatGPT gets some of the nuanced details
    right but only for the parts that can be found on the internet and are already
    explained (e.g., `FLOAT(i)` is required, as is doing `4.0 * pi` instead of `4
    * pi`). The tasks without examples on the internet are the ones where ChatGPT
    makes errors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: This example also highlights the limits of perceived versus actualized “reasoning”
    within LLMs today. The complete language specification for Modula-3 is available
    online and has documented all of these details or their lack of existence. ChatGPT
    has almost surely seen many other coding language specifications, parser specifications,
    and millions of lines of code in common programming languages. If a person had
    this background knowledge and resources, performing the logical induction required
    to avoid all three errors should not be too challenging. However, the LLM does
    not perform any induction process and, thus, makes errors despite the breadth
    of available information.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: This is not to say that the result is not massively impressive, and it can be
    a valuable tool to accelerate your own code development or use of unfamiliar APIs
    and languages. But it also informs you that such tools will work far better for
    widely used and documented languages and APIs, especially if they conform to expected
    standards. For example, most databases use the language SQL, which makes accurate
    extrapolation of how to use a novel database that also uses SQL more likely.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Failing to identify the correct task
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another notable case in which LLM’s fail is when they cannot correctly identify
    the task they are supposed to perform and instead will answer a question different
    from what the user intended. Failure to correctly identify the task used to be
    a substantial problem for models like the original GPT-3, but subsequent work
    aimed at increasing the number of task-structured examples in the training data
    has substantially increased the ability of later ChatGPT models to follow instructions.
    However, ChatGPT will still fail to identify the correct task in some cases. For
    example, this behavior can be elicited reliably by asking about an unusual task
    subtly different from a common task or by modifying a problem it has seen many
    times in an unfamiliar way.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: One example is a famous logic puzzle about bringing a cabbage, a goat, and a
    wolf across a river in a boat. The puzzle stipulates that the goat can’t be left
    alone with the cabbage (as the goat will eat it) or with the wolf (which will
    devour the goat). ChatGPT can quickly solve this puzzle, but if we change the
    logical structure of the puzzle slightly, the model continues to use the old reasoning
    as shown in figure [4.9](#fig__cabbage1).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: While it is often hard to trace errors made by LLMs back to specific causes,
    in this case, the model happily tells us to “ensure that none of the items (cabbage,
    goat, wolf) are left together unsupervised.” While this instruction is correct
    in the original version of the cabbage/goat/wolf problem (and was likely based
    on the specification of the constraints in the logic problem), the model is unaware
    that the given version has no problem with the goat and wolf being alone together.
    Not only is there no need to swap the animals as suggested, but ChatGPT’s advice
    will fail because it places the wolf and cabbage together, which we explicitly
    disallowed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Another curious example of this phenomenon happens when you remove the need
    to leave anything behind. Any logical understanding of the puzzle makes it clear
    that you only need to load everything into the boat and cross. Yet again, the
    model is too accustomed to answering the version of the problem that it has seen
    many times before and does so.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F09_Boozallen.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 ChatGPT fails to solve two modified versions of a classic logic puzzle
    due to how LLMs are trained. Content frequently occurring in the same general
    form (e.g., a famous logic puzzle) leads the model to regurgitate the frequent
    answer. This can happen even when the content is modified in important ways that
    are obvious to a person.
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To understand why this happens, it is important to recall the autoregressive
    nature of LLM training discussed in chapter 3\. The model is explicitly incentivized
    to generate content based on prior content. The content generated to solve the
    reframed logic puzzle appears almost exactly like the content that solves the
    original logic puzzle in terms of words and order. As a result, it is a good fuzzy
    match in the transformer layer’s query and key pairing that produces the values
    that make up the original puzzle’s solution. The fuzzy match is made, and the
    previous solution is faithfully returned via the attention mechanism used by the
    transformers. While this strategy is excellent for the model to correctly predict
    the tokens for the famous puzzle, it does not involve reasoning through the puzzle’s
    logic.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 LLMs cannot plan
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another subtle limitation of the autoregressive nature of LLMs is that they
    can only work with the information they see in context. LLMs are trained to take
    an input and produce a plausible continuation. However, they cannot plan, make
    commitments, or track internal states. A great example occurs when you attempt
    to play the game 20 questions with ChatGPT. When a human plays 20 questions, they
    precommit to a piece of hidden information, the object they’ve chosen to use the
    answers to identify. When ChatGPT plays this game, it answers questions individually
    and then, after the fact, finds an output consistent with the provided answers.
    This example is illustrated in figure [4.10](#fig__20qs), which shows possible
    dialog trees for playing 20 questions. When someone plays a game with an LLM,
    one of these dialog trees is chosen randomly instead of coming up with a target
    object that stays consistent throughout the game.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F10_Boozallen.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 The dialogue agent doesn’t commit to a specific object at the start
    of the game.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4 If LLMs cannot extrapolate well, can I use them?
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most work that needs to be done is not novel or new. At least, it’s not novel
    or new enough to a degree that would make an LLM fail. However, understanding
    that an LLM’s abilities degrade quickly as more logic or nuance is required can
    help you narrow the scope of how you use it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: When we design production-grade computer systems, an essential factor to consider
    is the scope of when and how the tool will be used. When you make an LLM product
    like ChatGPT available to a general audience without a specific scope, people
    will ask it to do all sorts of random, crazy things you do not expect. While this
    might be great for research, it is often not practical for production applications.
    Although your users and customers will try to do unpredictable things with your
    LLM application, suppose you limit who has access to the system and design around
    your users having a specific goal, limited use cases, or even restrict how their
    inputs get to your LLM. In that case, you can build something with a much more
    reliable user experience.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: How can I use an LLM without user input?
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs are excellent at providing low-effort coding or data processing, especially
    when you are doing everyday tasks on data that is not so cleanly formatted or
    curated. However, you can get utility without as much risk by giving users a finite
    set of choices. Having a limited set of prompts as code that a user can choose
    from or letting a user decide what data source (e.g., some internal database)
    a prompt is run over allows you to keep (most) people from giving an LLM arbitrary
    text.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you may ask, “Can we detect novel requests and give the user some error
    instead?” Hypothetically, yes, you could try to do this. First, we discourage
    it because it is not great from a user experience perspective. Second, it becomes
    a task known as *novelty detection* or *outlier detection*. This problem is challenging
    and is likely impossible to solve in a way that is guaranteed to be error-free.
    As a result, we encourage prevention over detection by choosing use cases that
    do not require highly accurate prediction of failures through the analysis of
    LLM input or output.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Applications for prompting
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prompting is the art of crafting an input to a large language model that induces
    desirable behavior. Language models can be very sensitive to the exact framing
    of their inputs, making the ability to design inputs that are responded to appropriately
    highly valuable. A recurring theme in using LLMs is that people typically don’t
    think about how to interact with them correctly. The best way to prompt an LLM
    is to think about how the kind of output you’re interested in would look like
    in the training data and then write the first quarter of it. Instead, people often
    describe the task they want a language model to perform, assuming that this clarification
    will keep an LLM focused on the problem. Unfortunately, the approach yields inconsistent
    results and has inspired research in tuning LLMs by feeding them a large number
    of instructions and responses as training data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Is bigger better?
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2019, Rich Sutton coined the term “the bitter lesson” to describe his experience
    with machine learning. “The biggest lesson that can be read from 70 years of AI
    research is that general methods that leverage computation are ultimately the
    most effective, and by a large margin” [7].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: There is a genuine sense that transformers are the ultimate example of this
    principle. You can keep making them bigger, training them with more parallelism,
    and adding more GPUs. This differs notably from RNNs, which cannot be parallelized
    nearly as efficiently as a transformer. We also see this in the image domain with
    Generative Adversarial Network (GAN) methods, which struggle to reach the billion-parameters
    scale. The transformer-based methods used in LLMs easily scale to the tens of
    billions, allowing the construction of bigger and better models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: From a solutions design perspective, your prototype today may encounter significant
    constraints due to model size. Larger models require more resources and take longer
    to make predictions. What is the maximum response time your users will accept?
    How expensive is the hardware needed to run your model at this speed? The growth
    rate in model size exceeds the growth rate of consumer hardware. As a result,
    you may not be able to deploy your model to embedded devices, or you may require
    internet connectivity to offload the costs. Consequently, you need to consider
    networking infrastructure in your design to handle the need for continuous connection.
    This requirement increases battery usage, which is a consideration when continually
    running a Wi-Fi radio instead of local computing. So although larger models are
    more accurate, design constraints may prevent their deployment in a practical
    manner. Combining these constraints with the facts about how LLMs make their predictions
    and the use cases of when and where LLMs fail that you learned in this chapter
    positions you well for understanding how to use LLMs to solve the problems you
    care about most effectively.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning needs a loss/reward function that specifically quantifies how
    badly an algorithm is at making predictions
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This loss/reward function should be designed to correlate with the overarching
    goal of what we want the algorithm to achieve in real life.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent involves incrementally using a loss/reward function to alter
    the network’s parameters.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are trained to mimic human text by predicting the next token. This task
    is sufficiently specific to train a model to perform it, but it does not perfectly
    correlate with high-level objectives like reasoning.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs will perform best on tasks similar to common and repetitive tasks observed
    in its training data but will fail when the task is sufficiently novel.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
