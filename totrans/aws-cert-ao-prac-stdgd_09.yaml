- en: Chapter 8\. A Framework for Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2014, Microsoft launched Xiaoice, an AI-powered chatbot in China that successfully
    attracted over 40 million users. A year later, Microsoft released a version on
    Twitter called Tay. The launch was a disaster. Tay quickly began spouting racist
    and hostile comments, forcing Microsoft to shut it down within 24 hours.
  prefs: []
  type: TYPE_NORMAL
- en: The incident occurred because Microsoft had a false sense of security from its
    experience in China, where stricter content limitations were in place, and underestimated
    the freewheeling nature of Twitter, where users actively tried to manipulate the
    bot. In a blog post, a Microsoft executive acknowledged the company had learned
    valuable lessons, stating that the challenges of AI are “just as much social as
    they are technical” and that caution is required when iterating in public forums.
  prefs: []
  type: TYPE_NORMAL
- en: This event spurred Microsoft to create its own principles for responsible AI—a
    framework for developing and deploying AI systems in a safe, trustworthy, and
    ethical way. As AI becomes more integrated into critical areas like healthcare
    and criminal justice, this focus on responsibility has never been more important
    and is a key part of the AIF-C01 exam.
  prefs: []
  type: TYPE_NORMAL
- en: Risks of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As powerful as generative AI can be, its capabilities also introduce a wide
    range of risks—some subtle, others deeply disruptive. From producing toxic content
    to infringing on intellectual property and contributing to job displacement, these
    risks can have serious societal, ethical, and legal consequences. This is why
    the concept of responsible AI is so important. It serves as a foundational approach
    to identifying, mitigating, and managing these challenges from the outset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than treating these issues as isolated technical flaws, responsible
    AI emphasizes a holistic strategy: ensuring fairness, transparency, safety, and
    human oversight in how AI systems are built and used. In the following sections,
    we’ll explore several key risk areas tied to generative AI—each illustrating why
    a responsible framework isn’t optional, but essential. We’ll then follow this
    up by understanding the core elements of responsible AI and how they serve as
    practical tools for addressing these risks.'
  prefs: []
  type: TYPE_NORMAL
- en: Toxicity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Managing toxicity is a central concern of responsible AI. If generative AI systems
    produce content that is offensive, harmful, or inappropriate, it can erode trust,
    damage brand reputation, and even cause real-world harm. Ensuring responsible
    AI means putting safeguards in place to minimize these risks—through thoughtful
    design, filtering mechanisms, and ongoing oversight.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there is a major problem with toxicity: it is highly subjective. What may
    be offensive to one person may be perfectly fine for another. There are also age-related
    considerations and the differences among cultures.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for an AI developer, it can be incredibly difficult to develop the right
    filters. Inevitably, it seems impossible not to offend someone.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is that it can be challenging to identify toxicity. Because generative
    AI systems are based on complex probability systems, the content may have shades
    or nuances of offensiveness—that may not be picked up in a filter.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, one approach is to have human curation of data for generative AI models.
    But this can be time-consuming—and is far from perfect either.
  prefs: []
  type: TYPE_NORMAL
- en: Intellectual Property
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Respecting intellectual property (IP) rights is a cornerstone of responsible
    AI. Generative AI systems that fail to properly attribute, license, or protect
    creative works can undermine industries, violate legal protections, and erode
    public trust. A responsible approach to AI development means being proactive about
    copyright, ownership, and fair compensation.
  prefs: []
  type: TYPE_NORMAL
- en: The 2023 Hollywood writers’ strike underscored these concerns, bringing to light
    the growing tensions between creative professionals and the rapid rise of AI-generated
    content. Central to the dispute was the concern that generative AI could replicate
    writers’ work without sufficient compensation or credit.^([1](ch08.html#ch01fn26))
    The Writers Guild of America (WGA) successfully negotiated provisions ensuring
    that AI-generated content cannot replace human writers and that any AI assistance
    used in writing processes would still require full credit and compensation for
    the human writers involved. It was certainly historic—and yet another example
    of the influence of AI on society.
  prefs: []
  type: TYPE_NORMAL
- en: But when it comes to generative AI, the issue of IP rights has been critical
    since the early days of the launch of ChatGPT. Within a few months, there were
    already various lawsuits. For example, the *New York Time*s filed a complaint
    against OpenAI and Microsoft, alleging that they used newspaper articles without
    permission.
  prefs: []
  type: TYPE_NORMAL
- en: The legal issues for generative AI and IP are complex and will likely take years
    to sort out. Ultimately, they may be decided by the Supreme Court.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, AI developers are finding ways to address the concerns. One
    approach has been to strike licensing deals with content providers.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to deal with IP issues is to provide indemnification protection.
    This is where the AI developer will defend and cover legal costs for litigation.
    Some of the companies that provide this protection include OpenAI, Microsoft,
    and Adobe.
  prefs: []
  type: TYPE_NORMAL
- en: Plagiarism and Cheating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Promoting academic integrity is an essential component of responsible AI. While
    generative AI can serve as a powerful educational tool, it must be deployed with
    guardrails that discourage misuse—such as plagiarism. Responsible AI means fostering
    transparency, accountability, and ethical behavior, especially in learning environments.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI tools like ChatGPT offer students the ability to explore academic
    subjects, ask complex questions, and receive personalized, on-demand help. This
    has the potential to enhance learning in significant ways.
  prefs: []
  type: TYPE_NORMAL
- en: But on the flip side, the same technology can be exploited to bypass genuine
    effort—writing essays, completing assignments, or answering exam questions. This
    raises serious concerns about fairness and learning outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: The response from educational institutions has been divided. Some have implemented
    bans or put restrictions on its use. In other cases, the approach has been to
    promote the use of generative AI and include it in the curriculum, such as to
    learn how to better leverage the technology.
  prefs: []
  type: TYPE_NORMAL
- en: There have been attempts to detect AI-generated content. But this has proven
    extremely difficult. It’s not uncommon for these tools to give false positives.
    Besides, students can be creative in evading detection. For example, they may
    rewrite some of the content. This can even be done using an AI tool!
  prefs: []
  type: TYPE_NORMAL
- en: Disruption of the Nature of Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most pressing challenges for responsible AI is its potential to disrupt
    the global workforce. As AI systems become capable of performing complex tasks
    once reserved for highly skilled professionals, responsible AI must account not
    only for safety and fairness but also for long-term economic and social impacts.
    That includes planning for workforce transition, supporting job augmentation,
    and fostering inclusive innovation.
  prefs: []
  type: TYPE_NORMAL
- en: In 1930, legendary economist John Maynard Keynes wrote a paper about how technology
    would displace a huge number of jobs. He called this “technological unemployment.”
  prefs: []
  type: TYPE_NORMAL
- en: For many decades, his prediction was off the mark. Often, new technology led
    to even more jobs. But today, the fears of Keynes seem much more realistic. The
    fact is that generative AI can already engage effectively in complex, knowledge-based
    fields like software development, financial services, and law.
  prefs: []
  type: TYPE_NORMAL
- en: For example, research from Goldman Sachs predicts that generative AI could [automate
    about 300 million full-time jobs globally](https://oreil.ly/M1Pxu) and that about
    two-thirds of jobs in the US are vulnerable to AI automation.
  prefs: []
  type: TYPE_NORMAL
- en: Then there was a report from the McKinsey Global Institute. It forecasted that
    by 2030 up to [30% of hours currently worked in the US and Europe could be automated](https://oreil.ly/e6ZQh).
  prefs: []
  type: TYPE_NORMAL
- en: No doubt, this is far from encouraging. If these predictions wind up being on
    target, there is likely to be significant disruption—economically and socially.
  prefs: []
  type: TYPE_NORMAL
- en: This certainly underscores the importance of developing and implementing responsible
    AI practices. It could mean thinking about retraining and reskilling the workforce,
    as well as seeing how AI can better augment work—not replace jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the core of responsible AI is the accuracy of the results of a model. This
    is essential for reliability, safety, and trustworthiness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#chapter_three_ai_and_machine_learning), we learned
    about some techniques to measure the accuracy of an AI model, which include:'
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs: []
  type: TYPE_NORMAL
- en: This is the difference between the average predicted values and actual values.
    High bias often results in underfitting, where the model performs poorly on both
    training and unseen data because it cannot represent the complexity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs: []
  type: TYPE_NORMAL
- en: This is where the model is sensitive to changes or noise in the training data.
    High variance leads to overfitting, where the model captures noise in the training
    data as if it were a true pattern. While adding more data can sometimes reduce
    overfitting, this is not guaranteed. If the model is too complex relative to the
    amount of data or if the data is noisy, simply increasing the dataset size may
    not improve accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ultimately, the goal is to find a balance where both bias and variance are
    minimized. These are some techniques to help with the trade-off:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating an AI model by training several others on subsets of the data available
    helps to detect overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Increase data
  prefs: []
  type: TYPE_NORMAL
- en: Add more data samples, especially those that are more diverse.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs: []
  type: TYPE_NORMAL
- en: This penalizes extreme values, which can mitigate overfitting and the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Simpler models
  prefs: []
  type: TYPE_NORMAL
- en: Simple models can help with overfitting because they are less likely to capture
    noise in the training data. But if the model is too simple, this can lead to bias.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs: []
  type: TYPE_NORMAL
- en: Simplification by reducing the number of features in a dataset while trying
    to retain as much information as possible can reduce variance.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting model parameters can help balance bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs: []
  type: TYPE_NORMAL
- en: This can simplify the model and reduce variance.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides Microsoft’s principles of responsible AI, there are other companies
    that have their own frameworks, like Google. This is also true for organizations
    like UNESCO and the United Nations. Even the Vatican has its own guidelines. Regardless,
    they generally share many of the same concepts.
  prefs: []
  type: TYPE_NORMAL
- en: As for the AIF-C01 exam, there are some principles you should keep in mind.
    But they should not be considered in isolation. Implementing one often involves
    considering others. For example, achieving transparency in AI systems typically
    requires explainability, fairness, and robust governance structures. Similarly,
    ensuring safety and controllability involves robust design and clear governance.
    In other words, there should be a holistic approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some of the principles you should know for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fairness means that AI systems should make decisions that are impartial. There
    should not be discrimination against individuals or groups, such as based on race,
    gender, or socioeconomic status. By incorporating fairness in an AI system, you
    help bolster inclusion and trust.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly enough, Apple and Goldman Sachs did not use gender as a factor
    in their AI models and a New York state investigation did not find that there
    was inherent bias.^([2](ch08.html#ch01fn28)) Nevertheless, the algorithms were
    changed, and the results turned out to be fairer.
  prefs: []
  type: TYPE_NORMAL
- en: What this points out is that—even if you do not use certain data—a model can
    still be unfair. The reason is that related data may lead to the same results.
    For example, a credit scoring system may give a lower credit limit to teachers,
    which may have a higher representation of women.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explainable AI (XAI) is where an AI system is developed to make the decision-making
    processes transparent and understandable. This can help improve trust and accountability.
  prefs: []
  type: TYPE_NORMAL
- en: In regulated sectors, XAI is critical. A system may not be able to pass regulatory
    muster if it does not meet certain requirements and standards. For instance, if
    an AI system is used to diagnose a disease and recommends treatments, it must
    have clear explanations for the underlying process and reasoning. Otherwise, patients
    could potentially be in danger.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, XAI has many challenges. Current techniques are generally done
    with post hoc interpretations that may not accurately reflect the mode’s actual
    decision-making process. Moreover, there’s a lack of standardized metrics to evaluate
    the quality and effectiveness of explanations, and efforts to make models more
    interpretable can sometimes compromise their performance.
  prefs: []
  type: TYPE_NORMAL
- en: But there has been considerable research in XAI, and there continues to be ongoing
    progress.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that there are various explainability frameworks like SHapley Additive
    exPlanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), and
    counterfactual explanations. These frameworks will summarize and interpret the
    decision making of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: As for AWS, there are some helpful tools like SageMaker Clarify, which we will
    cover later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Privacy and security ensure that individuals’ data is protected and that they
    maintain control over how their information is used. This involves both safeguarding
    data from unauthorized access and providing users with clear choices regarding
    their data’s usage.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing strong privacy and security measures not only complies with legal
    requirements but also builds trust with users. When individuals are confident
    that their data is handled responsibly, they are more likely to engage with AI
    technologies. This helps to foster innovation and broader adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transparency is sharing information about how AI systems are developed, the
    data they use, and their decision-making processes. This openness enables stakeholders—like
    users, regulators, and developers—to understand the system’s capabilities and
    limitations. For instance, transparency can involve disclosing the sources of
    training data, the objectives of the AI model, and any inherent risks or biases.
  prefs: []
  type: TYPE_NORMAL
- en: While transparency and explainability are related concepts in AI, they serve
    distinct purposes. Transparency pertains to the overall openness about an AI system’s
    design, data sources, and functioning. Explainability, on the other hand, focuses
    on the specific reasoning for individual decisions made by the AI.
  prefs: []
  type: TYPE_NORMAL
- en: Veracity and Robustness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Veracity and robustness help to ensure that AI systems operate reliably and
    accurately. This is the case even when there are unexpected inputs or challenging
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Veracity pertains to the truthfulness and accuracy of the AI’s outputs. Robustness
    is about an AI system’s ability to maintain consistent performance despite variations
    in input data, adversarial attacks, or unforeseen circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of these attributes cannot be overstated, especially in critical
    applications such as healthcare, finance, and autonomous systems. For instance,
    in healthcare, an AI diagnostic tool must provide accurate assessments even when
    patient data is incomplete or contains anomalies. A robust AI system can handle
    such irregularities without compromising the quality of its output. Similarly,
    in finance, AI models must remain reliable amidst fluctuating market conditions
    and data inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI governance refers to the policies and procedures that companies set up to
    guide the ethical and compliant development for AI systems. This includes defining
    clear roles and responsibilities, implementing oversight structures, and establishing
    protocols for risk assessment and mitigation. Effective AI governance helps organizations
    manage potential risks, such as bias, discrimination, and privacy violations,
    while promoting transparency and accountability in AI operations.
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic nature of AI technologies requires ongoing monitoring and adaptation
    of governance strategies. Establishing cross-functional teams that include ethicists,
    legal experts, technologists, and other stakeholders can help organizations proactively
    identify and address emerging ethical dilemmas and compliance challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring the safety of AI involves developing and operating AI systems to perform
    their intended functions without causing harm to humans or the environment. This
    involves addressing potential risks such as unintended behaviors, algorithmic
    bias, and misuse.
  prefs: []
  type: TYPE_NORMAL
- en: A critical aspect of AI safety is rigorous testing and validation. This includes
    stress testing AI systems under extreme conditions and using diverse datasets
    to ensure consistent performance across various scenarios. Such practices help
    in identifying and mitigating risks before deployment. Additionally, implementing
    robust safeguards and oversight mechanisms can prevent malfunctions and misuse.
  prefs: []
  type: TYPE_NORMAL
- en: There’s an important trade-off between the safety of a model and transparency.
    Model safety is all about protecting sensitive data, while model transparency
    is about making it easier to see how and why a model makes decisions. Striking
    the right balance between the two is often challenging. This is especially the
    case in environments where both privacy and accountability are critical.
  prefs: []
  type: TYPE_NORMAL
- en: For example, highly complex models like deep neural networks typically offer
    stronger performance and accuracy but are often difficult to interpret. Simpler
    models, such as linear regressions, are easier to explain but may not perform
    as well on complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: There are also techniques designed to protect data privacy, such as differential
    privacy, which helps prevent the exposure of individual data points. However,
    this can make it more difficult to understand how a model arrives at its conclusions—improving
    security at the cost of transparency. Similarly, models trained in isolated environments—known
    as air-gapped systems, which are physically or logically disconnected from external
    networks—further enhance security by preventing outside access. But this isolation
    can make it harder for external parties to audit or evaluate the model’s behavior.
    To ensure performance and resilience, AWS Bedrock allows for stress testing of
    models under various loads and scenarios, helping validate how well they operate
    in demanding environments.
  prefs: []
  type: TYPE_NORMAL
- en: Controllability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Controllability is the capacity to guide and regulate AI systems so that their
    actions remain aligned with human intentions and ethical standards. This involves
    designing AI architectures that allow for human oversight. This allows developers
    and users to monitor, intervene, and adjust the system’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The “AI control problem” addresses the difficulty of ensuring that advanced
    AI systems act in accordance with human values and objectives. As AI systems become
    more autonomous, there’s an increased risk of them pursuing goals in unintended
    ways. This can potentially lead to harmful outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: The controllability of a model also plays a key role in transparency and debugging.
    If a model reacts logically to adjustments in the training data, it becomes easier
    to understand how it’s functioning and to trace issues when something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The degree of controllability is influenced by the type of model. Simpler models
    like linear regressions typically allow for more direct control, while more complex
    models can behave in unpredictable ways. To assess a model’s controllability,
    you can run tests where you intentionally modify or augment data to see if the
    model’s outputs shift in expected ways.
  prefs: []
  type: TYPE_NORMAL
- en: The Benefits of Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Responsible AI isn’t just about ethics—it’s also good business. While doing
    the right thing should always be a priority, integrating responsible AI practices
    can significantly boost a company’s performance and long-term success.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at other reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Building trust and enhancing brand image
  prefs: []
  type: TYPE_NORMAL
- en: When users believe an AI system is transparent, fair, and secure, they’re more
    inclined to engage with it. That confidence builds loyalty and strengthens a company’s
    reputation. It also means that an AI application will be more effective and useful.
  prefs: []
  type: TYPE_NORMAL
- en: Staying ahead of regulation
  prefs: []
  type: TYPE_NORMAL
- en: As governments and industry bodies develop new rules around AI, organizations
    with ethical frameworks already in place will find it easier to adapt.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing risk exposure
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI helps companies proactively identify and mitigate dangers, such
    as algorithmic bias, data misuse, and security lapses. This lowers the chances
    of legal trouble, reputational harm, or financial losses from unintended consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Standing out in the market
  prefs: []
  type: TYPE_NORMAL
- en: Ethical AI can set a company apart from its rivals. As more consumers pay attention
    to how companies use AI, those that demonstrate responsibility and integrity can
    earn a stronger competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Smarter outcomes
  prefs: []
  type: TYPE_NORMAL
- en: When fairness and transparency are core design principles, AI systems tend to
    produce more dependable insights. This leads to sounder strategies and better-informed
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Driving innovation
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI brings more perspectives into the conversation. This diversity
    can lead to more original thinking, helping teams create products and services
    that are both impactful and forward-thinking.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Tools for Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For AWS AI platforms, there are extensive capabilities and tools for helping
    create responsible AI. This has been a major priority, which has involved much
    investment over the years.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at these features for Amazon Bedrock, SageMaker Clarify, Amazon A2I,
    and SageMaker Model Monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Bedrock, you can easily evaluate and compare various FMs. Some of the automatic
    metrics include accuracy, robustness, and toxicity. But there are also human evaluations,
    which focus on more subjective categories like style and alignment of brand voice.
    This can be done with your own employees or those managed by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another powerful feature for responsible AI is Bedrock’s guardrails, which
    we briefly covered in [Chapter 6](ch06.html#chapter_six_building_with_amazon_bedroc).
    This system allows for controlling how users interact with FMs. You can restrict
    interactions by:'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering content
  prefs: []
  type: TYPE_NORMAL
- en: You can create filters or use built-in versions that detect hateful, insulting,
    sexual, or violent content. For these, you can set the thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Redacting PII
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails can detect sensitive data, like names, addresses, Social Security
    numbers, and so on. This information will be blocked from inputs and outputs in
    FMs.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing content safety and privacy policies
  prefs: []
  type: TYPE_NORMAL
- en: You do not have to use a scripting language for this; you can use natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails also apply to AI agents. This is particularly important since these
    systems can act autonomously. Thus, there is often a need to allow for human approval
    or feedback.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Clarify and Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Clarify allows you to detect biases in datasets and AI models, without
    the need for advanced coding. You will specify factors like gender or age, and
    the system will conduct an analysis and produce a report.
  prefs: []
  type: TYPE_NORMAL
- en: Clarify has other features. For example, it can provide details about the decision
    making of an AI system, saying which features have the most influence on the responses
    of a model.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also offers SageMaker Experiments. This helps manage the interactive nature
    of AI development. You can organize, track, and compare different training runs.
    For these, you will capture the inputs, parameters, and results. This helps to
    better evaluate FMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker also has various governance tools:'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Role Manager
  prefs: []
  type: TYPE_NORMAL
- en: This allows administrators to define user permissions efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Model Cards
  prefs: []
  type: TYPE_NORMAL
- en: This provides the documentation of essential model information. This includes
    intended use cases, risk assessments, and training details.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Model Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: This provides a unified interface to monitor model performance. This integrates
    data from a myriad of sources to track metrics like data quality, model accuracy,
    and bias over time.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Augmented AI (Amazon A21)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Augmented AI (Amazon A2I) plays a key role in responsible AI by allowing
    human oversight in automated decision-making processes. It helps to reduce the
    risk of harmful errors, improve fairness, and build trust in AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: You can define conditions under which human reviews are triggered, such as low-confidence
    predictions or random sampling for auditing purposes. This flexibility allows
    for the incorporation of human judgment in various scenarios, including content
    moderation, text extraction, and translation tasks. For instance, in content moderation,
    images flagged with confidence scores below a certain threshold can be routed
    to human reviewers for further assessment.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon A2I supports multiple workforce options. You can use your private team
    of reviewers, engage third-party vendors through the AWS Marketplace, or access
    a global workforce of over 500,000 independent contractors via Amazon Mechanical
    Turk.
  prefs: []
  type: TYPE_NORMAL
- en: You can use Amazon A2I with Amazon SageMaker, Amazon Textract, Amazon Rekognition,
    Amazon Comprehend, Amazon Transcribe, and Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Model Monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#chapter_three_ai_and_machine_learning), we briefly
    covered SageMaker Model Monitor. It is a fully managed service that allows for
    continuous review of AI models that are in production. It will detect different
    types of drift that can impact the performance of a model, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality drift
  prefs: []
  type: TYPE_NORMAL
- en: Identifies changes in the statistical properties of input data, such as shifts
    in mean or variance
  prefs: []
  type: TYPE_NORMAL
- en: Model quality drift
  prefs: []
  type: TYPE_NORMAL
- en: Monitors the performance metrics like accuracy and precision by comparing model
    predictions against actual outcomes
  prefs: []
  type: TYPE_NORMAL
- en: Bias drift
  prefs: []
  type: TYPE_NORMAL
- en: Detects unintended biases in model predictions over time
  prefs: []
  type: TYPE_NORMAL
- en: Feature attribution drift
  prefs: []
  type: TYPE_NORMAL
- en: Observes changes in the importance of input features in influencing model predictions
  prefs: []
  type: TYPE_NORMAL
- en: With the Model Monitor, you can establish baselines using training data to define
    acceptable performance thresholds. Monitoring jobs can be scheduled at regular
    intervals or executed on-demand.
  prefs: []
  type: TYPE_NORMAL
- en: Going Further with Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a look at some additional considerations when it comes to creating
    responsible AI in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Sustainability and Environmental Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sustainability and environmental considerations refer to the development of
    AI technologies that are viable over the long term—socially, economically, and
    environmentally—while actively minimizing ecological harm. This involves creating
    systems that not only deliver performance and innovation but also support societal
    well-being and reduce negative impacts on the planet. It includes managing the
    full lifecycle of AI systems, from the energy required to train and run models
    to the materials used in hardware, with the goal of lowering the environmental
    footprint and promoting responsible, resource-efficient practices.
  prefs: []
  type: TYPE_NORMAL
- en: These principles are central to responsible AI, which emphasizes the ethical,
    transparent, and accountable development of artificial intelligence. As AI continues
    to scale, its environmental impact can no longer be treated as an afterthought.
    Responsible AI initiatives must ensure that sustainability is built into the design,
    deployment, and governance of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: One major concern is the energy consumption associated with training and running
    large AI models. These processes can demand significant computational resources,
    which increases electricity use and contributes to greenhouse gas emissions. A
    responsible approach involves improving energy efficiency through better model
    architectures, using power-saving hardware, and sourcing electricity from renewable
    energy. For instance, optimizing training schedules to coincide with periods of
    low-carbon energy availability can reduce environmental impact without sacrificing
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is the resource intensity of AI infrastructure. Manufacturing
    and deploying specialized hardware such as GPUs and TPUs often involves environmentally
    damaging materials and processes. Sustainable AI development promotes the reuse
    of existing hardware, prioritizes recyclable or longer-lasting components, and
    limits the production of electronic waste.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, environmental impact assessments should be an integral part of
    the AI development lifecycle. These assessments evaluate both the direct effects
    (like energy use) and indirect effects (such as enabling high-emission industries)
    of deploying an AI system. Where risks are identified, mitigation strategies—such
    as reducing model size, leveraging cloud-based green computing, or introducing
    policy safeguards—should be put in place.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating responsible AI systems requires the thoughtful preparation of datasets
    to ensure fairness and accuracy. A key factor is balancing datasets so that AI
    models do not inadvertently favor certain groups or outcomes. For instance, in
    applications like hiring or lending, an unbalanced dataset could lead to biased
    decisions that unfairly disadvantage specific demographics.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve balanced datasets, it’s important to collect data that is both inclusive
    and diverse. This means ensuring that the dataset accurately reflects the variety
    of perspectives and experiences relevant to the AI system’s intended use. For
    example, if developing a healthcare AI model focused on diagnosing conditions
    across all age groups, the training data should include a representative sample
    of patients from different age brackets. Neglecting to do so could result in a
    model that performs well for one age group but poorly for others.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond collection, data curation plays an essential role in balancing datasets.
    This involves preprocessing steps like cleaning the data to remove inaccuracies,
    normalizing data to ensure consistency, and selecting relevant features that contribute
    meaningfully to the model’s predictions. Data augmentation techniques, such as
    generating synthetic examples for underrepresented groups, can also help in achieving
    balance. Regular auditing of datasets is necessary to identify and correct any
    emerging biases over time.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like Amazon SageMaker Clarify and SageMaker Data Wrangler can assist in
    this process. SageMaker Clarify helps identify potential biases in datasets by
    analyzing the distribution of different features and outcomes. If imbalances are
    detected, SageMaker Data Wrangler offers methods like random oversampling, random
    undersampling, and the Synthetic Minority Oversampling Technique (SMOTE) to rebalance
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability Versus Explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the context of responsible AI, the priority between interpretability (covered
    in [Chapter 4](ch04.html#chapter_four_understanding_generative_a)) and explainability
    depends on the risk, regulatory environment, and stakeholders involved:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs: []
  type: TYPE_NORMAL
- en: This is generally favored when transparency and accountability are paramount,
    such as in regulated industries.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs: []
  type: TYPE_NORMAL
- en: Explainability is essential when using complex models that can’t easily be interpreted,
    but human oversight is still required—for example, in predictive diagnostics or
    automated hiring.
  prefs: []
  type: TYPE_NORMAL
- en: Both are important pillars of responsible AI, but interpretability is often
    seen as the gold standard when decisions must be clearly understood. [Table 8-1](#table_eight_onedot_interpretability_ver)
    shows some scenarios for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8-1\. Interpretability versus explainability: when to use each'
  prefs: []
  type: TYPE_NORMAL
- en: '| Use case | Goal | Preferred approach | Rationale |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Loan approval in a bank | Regulatory compliance, fairness | Interpretability
    | Clear rules needed for auditability and legal compliance |'
  prefs: []
  type: TYPE_TB
- en: '| Diagnosing rare diseases with AI | High accuracy with human oversight | Explainability
    | Complex models like deep learning used, but need explanations for decisions
    |'
  prefs: []
  type: TYPE_TB
- en: '| Resume screening with ML | Bias prevention, HR transparency | Explainability
    | Must explain why a candidate was filtered out; internal logic may be opaque
    |'
  prefs: []
  type: TYPE_TB
- en: '| Credit score predictions for consumers | Public trust, clarity | Interpretability
    | Consumers and regulators must understand how scores are computed |'
  prefs: []
  type: TYPE_TB
- en: Human-Centered Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human-centered design (HCD) is when technology is created with the end user
    in mind. It’s about prioritizing clarity, usability, and fairness. By using HCD,
    you can provide for amplified decision making. These are key principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Clarity
  prefs: []
  type: TYPE_NORMAL
- en: Information must be presented plainly—no jargon, no ambiguity. For example,
    a doctor reviewing an AI-recommended treatment plan needs a straightforward explanation
    of why it was suggested.
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity
  prefs: []
  type: TYPE_NORMAL
- en: Less is more. Remove unnecessary data points and highlight what matters. A logistics
    manager doesn’t need the model’s internal math—just a clear route recommendation
    and a confidence level.
  prefs: []
  type: TYPE_NORMAL
- en: Usability
  prefs: []
  type: TYPE_NORMAL
- en: Interfaces should be intuitive for both tech-savvy and nontechnical users. A
    loan officer, for instance, should be able to navigate the AI tool without special
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Reflexivity
  prefs: []
  type: TYPE_NORMAL
- en: Tools should prompt users to think critically about the decision. A pop-up asking
    “Is there additional context this system may have missed?” can trigger thoughtful
    review.
  prefs: []
  type: TYPE_NORMAL
- en: Accountability
  prefs: []
  type: TYPE_NORMAL
- en: There must be clear ownership over AI-assisted decisions. If a hiring tool recommends
    a candidate, the HR professional remains responsible for the final choice.
  prefs: []
  type: TYPE_NORMAL
- en: Personalization
  prefs: []
  type: TYPE_NORMAL
- en: Tailor the experience to the user. For example, a customer service AI can adapt
    its tone and suggestions based on an agent’s interaction style.
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive apprenticeship
  prefs: []
  type: TYPE_NORMAL
- en: Just as junior employees learn by shadowing experts, AI systems should learn
    from experienced users through examples and corrections.
  prefs: []
  type: TYPE_NORMAL
- en: User-centered tools
  prefs: []
  type: TYPE_NORMAL
- en: Make systems inclusive and accessible. A training platform should work equally
    well for an entry-level employee with a visual impairment and a senior manager
    with limited AI knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#chapter_four_understanding_generative_a), we briefly
    covered RLHF. This is where models learn to make better decisions by incorporating
    human preferences. RLHF plays an important role in responsible AI by aligning
    model behavior with human values, ethics, and expectations, helping to reduce
    harmful or biased outputs. It supports the creation of AI systems that are not
    only more accurate but also more transparent, fair, and aligned with societal
    norms.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine developing a virtual assistant designed to help users manage their daily
    tasks. Initially, the assistant might suggest reminders or schedule meetings based
    on general patterns. However, users might prefer certain suggestions over others.
    By observing which suggestions users accept or reject and gathering feedback on
    their preferences, the assistant can learn to tailor its recommendations more
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some of the advantages of RLHF:'
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced model performance
  prefs: []
  type: TYPE_NORMAL
- en: Models can refine their outputs to better meet user expectations. This can lead
    to improved accuracy and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Handling complex scenarios
  prefs: []
  type: TYPE_NORMAL
- en: In situations where it’s challenging to define explicit rules, human feedback
    provides nuanced guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Improved user satisfaction
  prefs: []
  type: TYPE_NORMAL
- en: Models that adapt based on user preferences tend to provide more personalized
    and satisfactory experiences. This helps to foster greater user trust and engagement.
  prefs: []
  type: TYPE_NORMAL
- en: Platforms like Amazon SageMaker Ground Truth provide capabilities to incorporate
    RLHF into the ML lifecycle. For instance, data annotators can review model outputs,
    ranking or classifying them based on quality. This feedback serves as a valuable
    input for training models. This allows them to align more closely with human judgments
    and expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Responsible AI involves developing AI systems ethically, safely, and transparently.
    It is about managing risks like toxicity, intellectual property disputes, job
    displacement, and accuracy issues.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned about the core principles of responsible AI and
    their use cases. We also saw the various tools from AWS that can help with the
    process, like Amazon Bedrock and SageMaker Clarify.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at security, compliance, and governance for
    AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 8 Answer Key”](app02.html#answers_ch_8).
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following methods can help reduce overfitting in an AI model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increasing the complexity of the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding more noisy data
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Stopping training early
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoiding hyperparameters
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is one technique companies use to address intellectual property concerns
    in generative AI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing all training data from public sources
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Restricting access to AI tools
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Limiting AI to internal company use only
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating licensing agreements with content providers
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is accuracy in AI models considered key to responsible AI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It improves reliability, trust, and safety.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Accurate models require fewer updates and patches.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Accuracy makes models less costly.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Accuracy only matters for visual models.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the main purpose of fairness in AI systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increasing the sophistication of models
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing latency in decision making
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Enhancing personalization features
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoiding discrimination against individuals or groups
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How does explainability compare to transparency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explainability focuses on user interface design.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Explainability explains model decisions; transparency shares system details.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: They mean the same thing.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transparency is only required in open source models.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What do privacy and security in AI primarily aim to protect?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model weights and parameters
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithm transparency
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Individual data and usage control
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Developer intellectual property
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([1](ch08.html#ch01fn26-marker)) Jake Coyle, [“In Hollywood Writers’ Battle
    Against AI, Humans Win (for Now)”](https://oreil.ly/KrpWD), Associated Press,
    September 27, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.html#ch01fn28-marker)) Sanya Mansoor, [“A Viral Tweet Accused Apple’s
    New Credit Card of Being ‘Sexist’”](https://oreil.ly/lE4hb), *Time*, November
    12, 2019.
  prefs: []
  type: TYPE_NORMAL
