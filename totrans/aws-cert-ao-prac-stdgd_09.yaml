- en: Chapter 8\. A Framework for Responsible AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 负责任的人工智能框架
- en: In 2014, Microsoft launched Xiaoice, an AI-powered chatbot in China that successfully
    attracted over 40 million users. A year later, Microsoft released a version on
    Twitter called Tay. The launch was a disaster. Tay quickly began spouting racist
    and hostile comments, forcing Microsoft to shut it down within 24 hours.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，微软在中国推出了Xiaoice，这是一个由人工智能驱动的聊天机器人，成功吸引了超过4000万用户。一年后，微软在Twitter上发布了一个名为Tay的版本。这次发布是一场灾难。Tay很快就开始发表种族主义和敌对言论，迫使微软在24小时内将其关闭。
- en: The incident occurred because Microsoft had a false sense of security from its
    experience in China, where stricter content limitations were in place, and underestimated
    the freewheeling nature of Twitter, where users actively tried to manipulate the
    bot. In a blog post, a Microsoft executive acknowledged the company had learned
    valuable lessons, stating that the challenges of AI are “just as much social as
    they are technical” and that caution is required when iterating in public forums.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这起事件发生是因为微软对其在中国市场的经验有一种错误的安全感，那里有更严格的内容限制，并且低估了Twitter的自由放任性质，用户们积极尝试操纵这个机器人。在一篇博客文章中，一位微软高管承认公司学到了宝贵的教训，表示人工智能的挑战“同样多的是社会性的，也是技术性的”，并且在公共论坛上迭代时需要谨慎。
- en: This event spurred Microsoft to create its own principles for responsible AI—a
    framework for developing and deploying AI systems in a safe, trustworthy, and
    ethical way. As AI becomes more integrated into critical areas like healthcare
    and criminal justice, this focus on responsibility has never been more important
    and is a key part of the AIF-C01 exam.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事件促使微软为其负责任人工智能制定了自己的原则——一个在安全、值得信赖和道德的方式下开发和部署人工智能系统的框架。随着人工智能越来越多地融入关键领域，如医疗保健和刑事司法，这种对责任的关注从未如此重要，并且是AIF-C01考试的关键部分。
- en: Risks of Generative AI
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能的风险
- en: As powerful as generative AI can be, its capabilities also introduce a wide
    range of risks—some subtle, others deeply disruptive. From producing toxic content
    to infringing on intellectual property and contributing to job displacement, these
    risks can have serious societal, ethical, and legal consequences. This is why
    the concept of responsible AI is so important. It serves as a foundational approach
    to identifying, mitigating, and managing these challenges from the outset.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成式人工智能可能非常强大，但其能力也引入了广泛的风险——有些微妙，有些破坏性极强。从产生有毒内容到侵犯知识产权和导致就业岗位流失，这些风险可能对社会、伦理和法律产生严重后果。这就是为什么负责任人工智能的概念如此重要的原因。它作为识别、减轻和管理这些挑战的基础方法，从一开始就是必不可少的。我们将探讨与生成式人工智能相关的几个关键风险领域——每个领域都说明了为什么负责任框架不是可选择的，而是必需的。然后，我们将继续了解负责任人工智能的核心要素以及它们如何作为解决这些风险的实用工具。
- en: 'Rather than treating these issues as isolated technical flaws, responsible
    AI emphasizes a holistic strategy: ensuring fairness, transparency, safety, and
    human oversight in how AI systems are built and used. In the following sections,
    we’ll explore several key risk areas tied to generative AI—each illustrating why
    a responsible framework isn’t optional, but essential. We’ll then follow this
    up by understanding the core elements of responsible AI and how they serve as
    practical tools for addressing these risks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是将这些问题视为孤立的技术缺陷，负责任的人工智能强调一种整体策略：确保人工智能系统在构建和使用过程中的公平性、透明度、安全性和人为监督。在接下来的几节中，我们将探讨与生成式人工智能相关的几个关键风险领域——每个领域都说明了为什么负责任框架不是可选择的，而是必需的。然后，我们将继续了解负责任人工智能的核心要素以及它们如何作为解决这些风险的实用工具。
- en: Toxicity
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 毒性
- en: Managing toxicity is a central concern of responsible AI. If generative AI systems
    produce content that is offensive, harmful, or inappropriate, it can erode trust,
    damage brand reputation, and even cause real-world harm. Ensuring responsible
    AI means putting safeguards in place to minimize these risks—through thoughtful
    design, filtering mechanisms, and ongoing oversight.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管理毒性是负责任人工智能的一个核心关注点。如果生成式人工智能系统产生冒犯性、有害或不适当的内容，它可能会损害信任、损害品牌声誉，甚至造成现实世界的伤害。确保负责任的人工智能意味着通过深思熟虑的设计、过滤机制和持续的监督来实施保障措施，以最大限度地减少这些风险。
- en: 'But there is a major problem with toxicity: it is highly subjective. What may
    be offensive to one person may be perfectly fine for another. There are also age-related
    considerations and the differences among cultures.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，毒性问题有一个主要问题：它非常主观。对某个人可能冒犯的内容，对另一个人来说可能完全没问题。还有与年龄相关的考虑以及文化之间的差异。
- en: Thus, for an AI developer, it can be incredibly difficult to develop the right
    filters. Inevitably, it seems impossible not to offend someone.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于一个AI开发者来说，开发正确的过滤器可能极其困难。不可避免地，似乎不可能不冒犯到某个人。
- en: Another issue is that it can be challenging to identify toxicity. Because generative
    AI systems are based on complex probability systems, the content may have shades
    or nuances of offensiveness—that may not be picked up in a filter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题在于识别毒性可能具有挑战性。因为生成式AI系统基于复杂的概率系统，内容可能包含一些攻击性的细微差别或微妙之处——这些可能不会被过滤器捕捉到。
- en: In fact, one approach is to have human curation of data for generative AI models.
    But this can be time-consuming—and is far from perfect either.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一种方法是为生成式AI模型进行人工数据管理。但这可能耗时且远非完美。
- en: Intellectual Property
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识产权
- en: Respecting intellectual property (IP) rights is a cornerstone of responsible
    AI. Generative AI systems that fail to properly attribute, license, or protect
    creative works can undermine industries, violate legal protections, and erode
    public trust. A responsible approach to AI development means being proactive about
    copyright, ownership, and fair compensation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尊重知识产权（IP）是负责任AI的基石。未能适当归属、许可或保护创意作品的生成式AI系统可能会破坏行业、违反法律保护，并侵蚀公众信任。对AI开发的负责任方法意味着在版权、所有权和公平补偿方面采取主动。
- en: The 2023 Hollywood writers’ strike underscored these concerns, bringing to light
    the growing tensions between creative professionals and the rapid rise of AI-generated
    content. Central to the dispute was the concern that generative AI could replicate
    writers’ work without sufficient compensation or credit.^([1](ch08.html#ch01fn26))
    The Writers Guild of America (WGA) successfully negotiated provisions ensuring
    that AI-generated content cannot replace human writers and that any AI assistance
    used in writing processes would still require full credit and compensation for
    the human writers involved. It was certainly historic—and yet another example
    of the influence of AI on society.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年好莱坞编剧罢工凸显了这些担忧，揭示了创意专业人士与AI生成内容快速崛起之间的日益紧张关系。争议的核心是担心生成式AI可能在不充分补偿或认可的情况下复制作家的作品。[^([1](ch08.html#ch01fn26))]
    美国编剧工会（WGA）成功谈判了确保AI生成的内容不能取代人类作家，以及任何在写作过程中使用的AI辅助仍需为涉及的人类作家提供全额信用和补偿的条款。这无疑是历史性的——AI对社会影响的又一例证。
- en: But when it comes to generative AI, the issue of IP rights has been critical
    since the early days of the launch of ChatGPT. Within a few months, there were
    already various lawsuits. For example, the *New York Time*s filed a complaint
    against OpenAI and Microsoft, alleging that they used newspaper articles without
    permission.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但在生成式AI领域，自ChatGPT发布之初，知识产权问题就一直是关键。在几个月内，就已经出现了各种诉讼。例如，《纽约时报》对OpenAI和微软提起了诉讼，指控他们未经许可使用了报纸文章。
- en: The legal issues for generative AI and IP are complex and will likely take years
    to sort out. Ultimately, they may be decided by the Supreme Court.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI和知识产权的法律问题复杂，可能需要数年才能解决。最终，它们可能由最高法院裁决。
- en: In the meantime, AI developers are finding ways to address the concerns. One
    approach has been to strike licensing deals with content providers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，AI开发者正在寻找解决这些问题的方法。一种方法是与内容提供商签订许可协议。
- en: Another approach to deal with IP issues is to provide indemnification protection.
    This is where the AI developer will defend and cover legal costs for litigation.
    Some of the companies that provide this protection include OpenAI, Microsoft,
    and Adobe.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理知识产权问题的方法是提供赔偿保护。这是指AI开发者将捍卫并承担诉讼的法律费用。提供这种保护的一些公司包括OpenAI、微软和Adobe。
- en: Plagiarism and Cheating
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抄袭和作弊
- en: Promoting academic integrity is an essential component of responsible AI. While
    generative AI can serve as a powerful educational tool, it must be deployed with
    guardrails that discourage misuse—such as plagiarism. Responsible AI means fostering
    transparency, accountability, and ethical behavior, especially in learning environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 促进学术诚信是负责任AI的一个基本组成部分。虽然生成式AI可以作为强大的教育工具，但必须部署有约束性的措施来阻止滥用——如抄袭。负责任AI意味着在学习和工作环境中培养透明度、责任感和道德行为。
- en: Generative AI tools like ChatGPT offer students the ability to explore academic
    subjects, ask complex questions, and receive personalized, on-demand help. This
    has the potential to enhance learning in significant ways.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似ChatGPT的生成式AI工具为学生提供了探索学术主题、提出复杂问题以及获得个性化、按需帮助的能力。这有可能在重大方面提升学习效果。
- en: But on the flip side, the same technology can be exploited to bypass genuine
    effort—writing essays, completing assignments, or answering exam questions. This
    raises serious concerns about fairness and learning outcomes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，同样的技术也可以被用来规避真正的努力——撰写论文、完成作业或回答考试问题。这引发了关于公平和学习成果的严重担忧。
- en: The response from educational institutions has been divided. Some have implemented
    bans or put restrictions on its use. In other cases, the approach has been to
    promote the use of generative AI and include it in the curriculum, such as to
    learn how to better leverage the technology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 教育机构的反应各不相同。一些已经实施了禁令或对其使用施加了限制。在其他情况下，方法是通过推广生成AI的使用并将其纳入课程，例如学习如何更好地利用这项技术。
- en: There have been attempts to detect AI-generated content. But this has proven
    extremely difficult. It’s not uncommon for these tools to give false positives.
    Besides, students can be creative in evading detection. For example, they may
    rewrite some of the content. This can even be done using an AI tool!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 已有尝试检测AI生成的内容。但这证明极其困难。这些工具给出假阳性结果并不罕见。此外，学生可以创造性地规避检测。例如，他们可能会重写部分内容。这甚至可以使用AI工具来完成！
- en: Disruption of the Nature of Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作性质的改变
- en: One of the most pressing challenges for responsible AI is its potential to disrupt
    the global workforce. As AI systems become capable of performing complex tasks
    once reserved for highly skilled professionals, responsible AI must account not
    only for safety and fairness but also for long-term economic and social impacts.
    That includes planning for workforce transition, supporting job augmentation,
    and fostering inclusive innovation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于负责任的AI来说，最紧迫的挑战之一是其可能对全球劳动力市场造成破坏。随着AI系统能够执行以前只有高技能专业人士才能完成的复杂任务，负责任的AI不仅必须考虑安全和公平，还要考虑长期的经济和社会影响。这包括规划劳动力转型、支持工作增强和培养包容性创新。
- en: In 1930, legendary economist John Maynard Keynes wrote a paper about how technology
    would displace a huge number of jobs. He called this “technological unemployment.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 1930年，传奇经济学家约翰·梅纳德·凯恩斯撰写了一篇关于技术将取代大量工作的论文。他称之为“技术性失业”。
- en: For many decades, his prediction was off the mark. Often, new technology led
    to even more jobs. But today, the fears of Keynes seem much more realistic. The
    fact is that generative AI can already engage effectively in complex, knowledge-based
    fields like software development, financial services, and law.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年中，他的预测往往不准确。通常，新技术会导致更多的就业机会。但今天，凯恩斯的担忧似乎更加现实。事实上，生成式AI已经在软件开发、金融服务和法律等复杂、知识密集型领域有效地参与其中。
- en: For example, research from Goldman Sachs predicts that generative AI could [automate
    about 300 million full-time jobs globally](https://oreil.ly/M1Pxu) and that about
    two-thirds of jobs in the US are vulnerable to AI automation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，高盛的研究预测，生成式AI可能[在全球自动化约3亿个全职工作](https://oreil.ly/M1Pxu)，并且大约三分之二的美国工作可能受到AI自动化的威胁。
- en: Then there was a report from the McKinsey Global Institute. It forecasted that
    by 2030 up to [30% of hours currently worked in the US and Europe could be automated](https://oreil.ly/e6ZQh).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 麦肯锡全球研究院发布了一份报告。它预测到2030年，美国和欧洲目前30%的工作时间可能被自动化([30% of hours currently worked
    in the US and Europe could be automated](https://oreil.ly/e6ZQh))。
- en: No doubt, this is far from encouraging. If these predictions wind up being on
    target, there is likely to be significant disruption—economically and socially.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，这远非鼓舞人心。如果这些预测最终成为现实，可能会造成重大的经济和社会动荡。
- en: This certainly underscores the importance of developing and implementing responsible
    AI practices. It could mean thinking about retraining and reskilling the workforce,
    as well as seeing how AI can better augment work—not replace jobs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实强调了发展和实施负责任的AI实践的重要性。这可能意味着考虑重新培训和再培训劳动力，以及看看AI如何更好地增强工作——而不是取代工作。
- en: Accuracy
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确性
- en: At the core of responsible AI is the accuracy of the results of a model. This
    is essential for reliability, safety, and trustworthiness.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI的核心是模型的准确性。这对于可靠性、安全性和可信度至关重要。
- en: 'In [Chapter 3](ch03.html#chapter_three_ai_and_machine_learning), we learned
    about some techniques to measure the accuracy of an AI model, which include:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#chapter_three_ai_and_machine_learning)中，我们了解了一些测量AI模型准确性的技术，包括：
- en: Bias
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差
- en: This is the difference between the average predicted values and actual values.
    High bias often results in underfitting, where the model performs poorly on both
    training and unseen data because it cannot represent the complexity of the data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是平均预测值与实际值之间的差异。高偏差通常会导致欠拟合，其中模型在训练和未见过的数据上表现不佳，因为它无法表示数据的复杂性。
- en: Variance
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 方差
- en: This is where the model is sensitive to changes or noise in the training data.
    High variance leads to overfitting, where the model captures noise in the training
    data as if it were a true pattern. While adding more data can sometimes reduce
    overfitting, this is not guaranteed. If the model is too complex relative to the
    amount of data or if the data is noisy, simply increasing the dataset size may
    not improve accuracy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是指模型对训练数据中的变化或噪声敏感的地方。高方差会导致过拟合，其中模型将训练数据中的噪声捕获为真实模式。虽然增加更多数据有时可以减少过拟合，但这并不保证。如果模型相对于数据的复杂度过于复杂，或者数据是噪声的，仅仅增加数据集的大小可能不会提高准确性。
- en: 'Ultimately, the goal is to find a balance where both bias and variance are
    minimized. These are some techniques to help with the trade-off:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标是找到偏差和方差都最小化的平衡点。以下是一些帮助权衡的技术：
- en: Cross validation
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Evaluating an AI model by training several others on subsets of the data available
    helps to detect overfitting.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在可用数据的子集上训练多个其他模型来评估人工智能模型，有助于检测过拟合。
- en: Increase data
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 增加数据
- en: Add more data samples, especially those that are more diverse.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多数据样本，特别是那些更具多样性的样本。
- en: Regularization
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化
- en: This penalizes extreme values, which can mitigate overfitting and the variance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这会惩罚极端值，从而减轻过拟合和方差。
- en: Simpler models
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的模型
- en: Simple models can help with overfitting because they are less likely to capture
    noise in the training data. But if the model is too simple, this can lead to bias.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的模型可以帮助避免过拟合，因为它们不太可能捕捉到训练数据中的噪声。但若模型过于简单，这可能会导致偏差。
- en: Dimensionality reduction
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 维度约简
- en: Simplification by reducing the number of features in a dataset while trying
    to retain as much information as possible can reduce variance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少数据集中特征的数量，同时尽可能保留信息，可以减少方差。
- en: Hyperparameter tuning
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Adjusting model parameters can help balance bias and variance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 调整模型参数可以帮助平衡偏差和方差。
- en: Feature selection
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择
- en: This can simplify the model and reduce variance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以简化模型并减少方差。
- en: Elements of Responsible AI
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任人工智能的要素
- en: Besides Microsoft’s principles of responsible AI, there are other companies
    that have their own frameworks, like Google. This is also true for organizations
    like UNESCO and the United Nations. Even the Vatican has its own guidelines. Regardless,
    they generally share many of the same concepts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了微软的负责任人工智能原则之外，还有其他公司拥有自己的框架，例如谷歌。这也适用于像联合国教科文组织和联合国这样的组织。甚至梵蒂冈也有自己的指导方针。无论如何，它们通常共享许多相同的概念。
- en: As for the AIF-C01 exam, there are some principles you should keep in mind.
    But they should not be considered in isolation. Implementing one often involves
    considering others. For example, achieving transparency in AI systems typically
    requires explainability, fairness, and robust governance structures. Similarly,
    ensuring safety and controllability involves robust design and clear governance.
    In other words, there should be a holistic approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于AIF-C01考试，有一些原则你应该记住。但它们不应孤立考虑。实施一个原则通常需要考虑其他原则。例如，实现人工智能系统的透明度通常需要可解释性、公平性和稳健的治理结构。同样，确保安全和可控性涉及稳健的设计和清晰的治理。换句话说，应该采取整体的方法。
- en: Let’s take a look at some of the principles you should know for the exam.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看你应该知道的考试原则。
- en: Fairness
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性
- en: Fairness means that AI systems should make decisions that are impartial. There
    should not be discrimination against individuals or groups, such as based on race,
    gender, or socioeconomic status. By incorporating fairness in an AI system, you
    help bolster inclusion and trust.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性意味着人工智能系统应做出不偏不倚的决定。不应基于种族、性别或社会经济地位等对个人或群体进行歧视。通过在人工智能系统中融入公平性，你可以帮助加强包容性和信任。
- en: Interestingly enough, Apple and Goldman Sachs did not use gender as a factor
    in their AI models and a New York state investigation did not find that there
    was inherent bias.^([2](ch08.html#ch01fn28)) Nevertheless, the algorithms were
    changed, and the results turned out to be fairer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 趣味的是，苹果和高盛在他们的AI模型中没有使用性别作为一个因素，纽约州的一项调查也没有发现固有的偏见。[2](ch08.html#ch01fn28) 尽管如此，算法被更改了，结果变得更为公平。
- en: What this points out is that—even if you do not use certain data—a model can
    still be unfair. The reason is that related data may lead to the same results.
    For example, a credit scoring system may give a lower credit limit to teachers,
    which may have a higher representation of women.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点指出的是——即使你未使用某些数据，模型仍然可能是不公平的。原因是相关数据可能导致相同的结果。例如，信用评分系统可能给教师较低的信用额度，而女性可能在这个群体中有更高的代表性。
- en: Explainability
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Explainable AI (XAI) is where an AI system is developed to make the decision-making
    processes transparent and understandable. This can help improve trust and accountability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能（XAI）是指开发AI系统以使决策过程透明和可理解。这有助于提高信任和问责制。
- en: In regulated sectors, XAI is critical. A system may not be able to pass regulatory
    muster if it does not meet certain requirements and standards. For instance, if
    an AI system is used to diagnose a disease and recommends treatments, it must
    have clear explanations for the underlying process and reasoning. Otherwise, patients
    could potentially be in danger.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在监管行业，可解释人工智能（XAI）至关重要。如果一个系统不能满足某些要求和标准，它可能无法通过监管审查。例如，如果一个AI系统用于诊断疾病并推荐治疗方案，它必须对底层过程和推理有明确的解释。否则，患者可能会处于危险之中。
- en: Unfortunately, XAI has many challenges. Current techniques are generally done
    with post hoc interpretations that may not accurately reflect the mode’s actual
    decision-making process. Moreover, there’s a lack of standardized metrics to evaluate
    the quality and effectiveness of explanations, and efforts to make models more
    interpretable can sometimes compromise their performance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，XAI面临着许多挑战。当前的技术通常是通过事后解释来完成的，这些解释可能无法准确反映模型的实际决策过程。此外，缺乏标准化的指标来评估解释的质量和有效性，而使模型更具可解释性的努力有时可能会损害其性能。
- en: But there has been considerable research in XAI, and there continues to be ongoing
    progress.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但在XAI领域已经进行了大量研究，并且仍在持续进步。
- en: Keep in mind that there are various explainability frameworks like SHapley Additive
    exPlanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), and
    counterfactual explanations. These frameworks will summarize and interpret the
    decision making of AI systems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，存在各种可解释性框架，如SHapley Additive exPlanations (SHAP)、Local Interpretable Model-Agnostic
    Explanations (LIME)和反事实解释。这些框架将总结和解释AI系统的决策过程。
- en: As for AWS, there are some helpful tools like SageMaker Clarify, which we will
    cover later in this chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AWS来说，有一些有用的工具，如SageMaker Clarify，我们将在本章后面介绍。
- en: Privacy and Security
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私和安全
- en: Privacy and security ensure that individuals’ data is protected and that they
    maintain control over how their information is used. This involves both safeguarding
    data from unauthorized access and providing users with clear choices regarding
    their data’s usage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私和安全确保个人数据得到保护，并且他们能够控制自己的信息如何被使用。这包括保护数据免受未经授权的访问，并为用户提供关于其数据使用的明确选择。
- en: Implementing strong privacy and security measures not only complies with legal
    requirements but also builds trust with users. When individuals are confident
    that their data is handled responsibly, they are more likely to engage with AI
    technologies. This helps to foster innovation and broader adoption.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实施强大的隐私和安全措施不仅符合法律要求，而且与用户建立信任。当个人对其数据得到负责任的处理有信心时，他们更有可能参与AI技术。这有助于促进创新和更广泛的应用。
- en: Transparency
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度
- en: Transparency is sharing information about how AI systems are developed, the
    data they use, and their decision-making processes. This openness enables stakeholders—like
    users, regulators, and developers—to understand the system’s capabilities and
    limitations. For instance, transparency can involve disclosing the sources of
    training data, the objectives of the AI model, and any inherent risks or biases.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度是分享关于AI系统如何开发、使用的数据以及其决策过程的信息。这种开放性使利益相关者——如用户、监管者和开发者——能够理解系统的能力和局限性。例如，透明度可能包括披露训练数据来源、AI模型的目标以及任何固有的风险或偏见。
- en: While transparency and explainability are related concepts in AI, they serve
    distinct purposes. Transparency pertains to the overall openness about an AI system’s
    design, data sources, and functioning. Explainability, on the other hand, focuses
    on the specific reasoning for individual decisions made by the AI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然透明度和可解释性是人工智能中的相关概念，但它们服务于不同的目的。透明度涉及对人工智能系统设计、数据来源和功能的整体开放性。另一方面，可解释性专注于人工智能就个别决策所提供的具体推理。
- en: Veracity and Robustness
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实性和鲁棒性
- en: Veracity and robustness help to ensure that AI systems operate reliably and
    accurately. This is the case even when there are unexpected inputs or challenging
    environments.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 真实性和鲁棒性有助于确保人工智能系统可靠且准确地运行。即使在意外输入或具有挑战性的环境中也是如此。
- en: Veracity pertains to the truthfulness and accuracy of the AI’s outputs. Robustness
    is about an AI system’s ability to maintain consistent performance despite variations
    in input data, adversarial attacks, or unforeseen circumstances.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 真实性涉及人工智能输出的真实性和准确性。鲁棒性是关于人工智能系统在输入数据变化、对抗性攻击或不可预见的情况下维持一致性能的能力。
- en: The importance of these attributes cannot be overstated, especially in critical
    applications such as healthcare, finance, and autonomous systems. For instance,
    in healthcare, an AI diagnostic tool must provide accurate assessments even when
    patient data is incomplete or contains anomalies. A robust AI system can handle
    such irregularities without compromising the quality of its output. Similarly,
    in finance, AI models must remain reliable amidst fluctuating market conditions
    and data inconsistencies.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性的重要性不容小觑，尤其是在医疗保健、金融和自主系统等关键应用中。例如，在医疗保健领域，即使患者数据不完整或包含异常，人工智能诊断工具也必须提供准确的评估。一个强大的AI系统可以处理这些不规则性，而不会降低其输出的质量。同样，在金融领域，AI模型必须在市场波动和数据不一致的情况下保持可靠性。
- en: Governance
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 治理
- en: AI governance refers to the policies and procedures that companies set up to
    guide the ethical and compliant development for AI systems. This includes defining
    clear roles and responsibilities, implementing oversight structures, and establishing
    protocols for risk assessment and mitigation. Effective AI governance helps organizations
    manage potential risks, such as bias, discrimination, and privacy violations,
    while promoting transparency and accountability in AI operations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能治理是指公司为引导人工智能系统的道德和合规发展而设定的政策和程序。这包括定义明确的角色和责任，实施监督结构，并建立风险评估和缓解的协议。有效的AI治理有助于组织管理潜在风险，如偏见、歧视和隐私侵犯，同时促进人工智能操作的透明度和问责制。
- en: The dynamic nature of AI technologies requires ongoing monitoring and adaptation
    of governance strategies. Establishing cross-functional teams that include ethicists,
    legal experts, technologists, and other stakeholders can help organizations proactively
    identify and address emerging ethical dilemmas and compliance challenges.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能技术的动态性要求持续监控和调整治理策略。建立包括伦理学家、法律专家、技术专家和其他利益相关者的跨职能团队，可以帮助组织主动识别和解决新兴的伦理困境和合规挑战。
- en: Safety
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: Ensuring the safety of AI involves developing and operating AI systems to perform
    their intended functions without causing harm to humans or the environment. This
    involves addressing potential risks such as unintended behaviors, algorithmic
    bias, and misuse.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 确保人工智能的安全性涉及开发和运营人工智能系统以执行其预期功能，而不会对人类或环境造成伤害。这包括解决潜在风险，如意外行为、算法偏见和滥用。
- en: A critical aspect of AI safety is rigorous testing and validation. This includes
    stress testing AI systems under extreme conditions and using diverse datasets
    to ensure consistent performance across various scenarios. Such practices help
    in identifying and mitigating risks before deployment. Additionally, implementing
    robust safeguards and oversight mechanisms can prevent malfunctions and misuse.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能安全的一个关键方面是严格的测试和验证。这包括在极端条件下对人工智能系统进行压力测试，并使用多样化的数据集以确保在各种场景中的一致性能。这些做法有助于在部署前识别和缓解风险。此外，实施强大的安全措施和监督机制可以防止故障和滥用。
- en: There’s an important trade-off between the safety of a model and transparency.
    Model safety is all about protecting sensitive data, while model transparency
    is about making it easier to see how and why a model makes decisions. Striking
    the right balance between the two is often challenging. This is especially the
    case in environments where both privacy and accountability are critical.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的安全性和透明度之间存在重要的权衡。模型安全主要关于保护敏感数据，而模型透明度则是关于使人们更容易看到模型是如何和为什么做出决策的。在这两者之间找到正确的平衡往往具有挑战性。这在隐私和问责制都至关重要的环境中尤其如此。
- en: For example, highly complex models like deep neural networks typically offer
    stronger performance and accuracy but are often difficult to interpret. Simpler
    models, such as linear regressions, are easier to explain but may not perform
    as well on complex tasks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像深度神经网络这样高度复杂的模型通常提供更强的性能和准确性，但通常难以解释。像线性回归这样的简单模型更容易解释，但在复杂任务上可能表现不佳。
- en: There are also techniques designed to protect data privacy, such as differential
    privacy, which helps prevent the exposure of individual data points. However,
    this can make it more difficult to understand how a model arrives at its conclusions—improving
    security at the cost of transparency. Similarly, models trained in isolated environments—known
    as air-gapped systems, which are physically or logically disconnected from external
    networks—further enhance security by preventing outside access. But this isolation
    can make it harder for external parties to audit or evaluate the model’s behavior.
    To ensure performance and resilience, AWS Bedrock allows for stress testing of
    models under various loads and scenarios, helping validate how well they operate
    in demanding environments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些技术旨在保护数据隐私，例如差分隐私，它有助于防止个人数据点的泄露。然而，这可能会使人们更难理解模型是如何得出结论的——以牺牲透明度为代价提高安全性。同样，在隔离环境中训练的模型——称为断网系统，它们在物理上或逻辑上与外部网络断开连接——通过防止外部访问进一步增强了安全性。但这种隔离可能会使外部各方更难审计或评估模型的行为。为了确保性能和弹性，AWS
    Bedrock允许在多种负载和场景下对模型进行压力测试，帮助验证它们在苛刻环境中的运行情况。
- en: Controllability
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可控性
- en: Controllability is the capacity to guide and regulate AI systems so that their
    actions remain aligned with human intentions and ethical standards. This involves
    designing AI architectures that allow for human oversight. This allows developers
    and users to monitor, intervene, and adjust the system’s behavior.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可控性是指引导和调节AI系统，使其行为始终与人类意图和道德标准保持一致的能力。这涉及到设计允许人类监督的AI架构。这允许开发人员和用户监控、干预和调整系统的行为。
- en: The “AI control problem” addresses the difficulty of ensuring that advanced
    AI systems act in accordance with human values and objectives. As AI systems become
    more autonomous, there’s an increased risk of them pursuing goals in unintended
    ways. This can potentially lead to harmful outcomes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: “AI控制问题”解决的是确保高级AI系统按照人类价值观和目标行动的困难。随着AI系统变得更加自主，它们以意想不到的方式追求目标的风险增加。这可能导致有害的结果。
- en: The controllability of a model also plays a key role in transparency and debugging.
    If a model reacts logically to adjustments in the training data, it becomes easier
    to understand how it’s functioning and to trace issues when something goes wrong.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的可控性在透明度和调试中也发挥着关键作用。如果模型对训练数据的调整做出合理的反应，那么理解其功能就变得更容易，当出现问题时也能更容易追踪问题。
- en: The degree of controllability is influenced by the type of model. Simpler models
    like linear regressions typically allow for more direct control, while more complex
    models can behave in unpredictable ways. To assess a model’s controllability,
    you can run tests where you intentionally modify or augment data to see if the
    model’s outputs shift in expected ways.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可控性的程度受模型类型的影响。像线性回归这样的简单模型通常允许更直接的控制，而更复杂的模型可能会以不可预测的方式表现。为了评估模型的可控性，你可以运行测试，故意修改或增强数据，以查看模型输出是否以预期的方式发生变化。
- en: The Benefits of Responsible AI
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任AI的好处
- en: Responsible AI isn’t just about ethics—it’s also good business. While doing
    the right thing should always be a priority, integrating responsible AI practices
    can significantly boost a company’s performance and long-term success.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI不仅仅是关于伦理——它也是一项好生意。虽然做正确的事情始终应该是优先事项，但整合负责任的AI实践可以显著提升公司的业绩和长期成功。
- en: 'Let’s look at other reasons:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他原因：
- en: Building trust and enhancing brand image
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 建立信任和提升品牌形象
- en: When users believe an AI system is transparent, fair, and secure, they’re more
    inclined to engage with it. That confidence builds loyalty and strengthens a company’s
    reputation. It also means that an AI application will be more effective and useful.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户认为AI系统是透明的、公平的和安全的，他们更愿意与之互动。这种信心会建立忠诚度并加强公司的声誉。这也意味着AI应用将更有效和有用。
- en: Staying ahead of regulation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 保持在法规之前
- en: As governments and industry bodies develop new rules around AI, organizations
    with ethical frameworks already in place will find it easier to adapt.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随着政府和行业机构围绕AI制定新规则，已经建立道德框架的组织将发现适应起来更容易。
- en: Reducing risk exposure
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 降低风险暴露
- en: Responsible AI helps companies proactively identify and mitigate dangers, such
    as algorithmic bias, data misuse, and security lapses. This lowers the chances
    of legal trouble, reputational harm, or financial losses from unintended consequences.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI帮助公司主动识别和减轻危险，如算法偏差、数据滥用和安全漏洞。这降低了法律麻烦、声誉损害或因意外后果造成的财务损失的可能性。
- en: Standing out in the market
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场上脱颖而出
- en: Ethical AI can set a company apart from its rivals. As more consumers pay attention
    to how companies use AI, those that demonstrate responsibility and integrity can
    earn a stronger competitive advantage.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 道德AI可以使公司区别于其竞争对手。随着越来越多的消费者关注公司如何使用AI，那些表现出责任和诚信的公司可以赢得更强的竞争优势。
- en: Smarter outcomes
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 更智能的结果
- en: When fairness and transparency are core design principles, AI systems tend to
    produce more dependable insights. This leads to sounder strategies and better-informed
    decisions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当公平和透明是核心设计原则时，AI系统往往会产生更可靠的见解。这导致更稳健的策略和更明智的决策。
- en: Driving innovation
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 推动创新
- en: Responsible AI brings more perspectives into the conversation. This diversity
    can lead to more original thinking, helping teams create products and services
    that are both impactful and forward-thinking.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI将更多视角带入对话。这种多样性可以导致更多原创思维，帮助团队创建既有影响力又具有前瞻性的产品和服务。
- en: Amazon Tools for Responsible AI
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon Tools for Responsible AI
- en: For AWS AI platforms, there are extensive capabilities and tools for helping
    create responsible AI. This has been a major priority, which has involved much
    investment over the years.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AWS AI平台，有广泛的能力和工具来帮助创建负责任的AI。这已经成为一个主要优先事项，多年来投入了大量投资。
- en: Let’s look at these features for Amazon Bedrock, SageMaker Clarify, Amazon A2I,
    and SageMaker Model Monitor.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Amazon Bedrock、SageMaker Clarify、Amazon A2I和SageMaker Model Monitor的这些功能。
- en: Amazon Bedrock
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Bedrock
- en: With Bedrock, you can easily evaluate and compare various FMs. Some of the automatic
    metrics include accuracy, robustness, and toxicity. But there are also human evaluations,
    which focus on more subjective categories like style and alignment of brand voice.
    This can be done with your own employees or those managed by AWS.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Bedrock，您可以轻松评估和比较各种FM。一些自动指标包括准确性、鲁棒性和毒性。但还有人类评估，这些评估侧重于更主观的类别，如品牌声音的风格和对齐。这可以通过您的员工或由AWS管理的员工来完成。
- en: 'Another powerful feature for responsible AI is Bedrock’s guardrails, which
    we briefly covered in [Chapter 6](ch06.html#chapter_six_building_with_amazon_bedroc).
    This system allows for controlling how users interact with FMs. You can restrict
    interactions by:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个强大的负责任AI功能是Bedrock的指南，我们在[第6章](ch06.html#chapter_six_building_with_amazon_bedroc)中简要介绍了。这个系统允许控制用户如何与FM互动。您可以通过以下方式限制互动：
- en: Filtering content
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤内容
- en: You can create filters or use built-in versions that detect hateful, insulting,
    sexual, or violent content. For these, you can set the thresholds.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建过滤器或使用内置版本来检测仇恨、侮辱、性或暴力内容。对于这些内容，您可以设置阈值。
- en: Redacting PII
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 修改个人识别信息（PII）
- en: Guardrails can detect sensitive data, like names, addresses, Social Security
    numbers, and so on. This information will be blocked from inputs and outputs in
    FMs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 指南可以检测敏感数据，如姓名、地址、社会保障号码等。这些信息将在FM的输入和输出中被阻止。
- en: Implementing content safety and privacy policies
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 实施内容安全和隐私政策
- en: You do not have to use a scripting language for this; you can use natural language.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要使用脚本语言来做这件事；您可以使用自然语言。
- en: Guardrails also apply to AI agents. This is particularly important since these
    systems can act autonomously. Thus, there is often a need to allow for human approval
    or feedback.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 指南也适用于AI代理。这一点尤为重要，因为这些系统可以自主行动。因此，通常需要允许人类进行审批或反馈。
- en: SageMaker Clarify and Experiments
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker Clarify和实验
- en: SageMaker Clarify allows you to detect biases in datasets and AI models, without
    the need for advanced coding. You will specify factors like gender or age, and
    the system will conduct an analysis and produce a report.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify允许您在无需高级编码的情况下检测数据集和AI模型中的偏差。您将指定如性别或年龄等因素，系统将进行分析并生成报告。
- en: Clarify has other features. For example, it can provide details about the decision
    making of an AI system, saying which features have the most influence on the responses
    of a model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Clarify具有其他功能。例如，它可以提供有关AI系统决策的详细信息，说明哪些特征对模型的响应影响最大。
- en: AWS also offers SageMaker Experiments. This helps manage the interactive nature
    of AI development. You can organize, track, and compare different training runs.
    For these, you will capture the inputs, parameters, and results. This helps to
    better evaluate FMs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: AWS还提供SageMaker Experiments。这有助于管理AI开发的交互性。您可以组织、跟踪和比较不同的训练运行。为此，您将捕获输入、参数和结果。这有助于更好地评估FM。
- en: 'SageMaker also has various governance tools:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker还拥有各种治理工具：
- en: SageMaker Role Manager
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker角色管理器
- en: This allows administrators to define user permissions efficiently.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许管理员高效地定义用户权限。
- en: SageMaker Model Cards
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker模型卡片
- en: This provides the documentation of essential model information. This includes
    intended use cases, risk assessments, and training details.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了必要模型信息的文档。这包括预期用例、风险评估和训练细节。
- en: SageMaker Model Dashboard
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker模型仪表板
- en: This provides a unified interface to monitor model performance. This integrates
    data from a myriad of sources to track metrics like data quality, model accuracy,
    and bias over time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一个统一的界面来监控模型性能。它集成了来自众多来源的数据，以跟踪数据质量、模型准确性和偏差随时间的变化。
- en: Amazon Augmented AI (Amazon A21)
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊增强人工智能（Amazon A21）
- en: Amazon Augmented AI (Amazon A2I) plays a key role in responsible AI by allowing
    human oversight in automated decision-making processes. It helps to reduce the
    risk of harmful errors, improve fairness, and build trust in AI systems.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊增强人工智能（Amazon A2I）在负责任的AI中扮演着关键角色，它允许在自动化决策过程中进行人工监督。这有助于降低有害错误的概率，提高公平性，并在AI系统中建立信任。
- en: You can define conditions under which human reviews are triggered, such as low-confidence
    predictions or random sampling for auditing purposes. This flexibility allows
    for the incorporation of human judgment in various scenarios, including content
    moderation, text extraction, and translation tasks. For instance, in content moderation,
    images flagged with confidence scores below a certain threshold can be routed
    to human reviewers for further assessment.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以定义触发人工审查的条件，例如低置信度预测或为审计目的的随机抽样。这种灵活性允许在包括内容审核、文本提取和翻译任务在内的各种场景中融入人类判断。例如，在内容审核中，置信度得分低于特定阈值的图像可以路由到人工审查员进行进一步评估。
- en: Amazon A2I supports multiple workforce options. You can use your private team
    of reviewers, engage third-party vendors through the AWS Marketplace, or access
    a global workforce of over 500,000 independent contractors via Amazon Mechanical
    Turk.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon A2I支持多种劳动力选项。您可以使用自己的私人审查团队，通过AWS Marketplace与第三方供应商合作，或通过Amazon Mechanical
    Turk访问超过500,000名独立承包商的全球劳动力。
- en: You can use Amazon A2I with Amazon SageMaker, Amazon Textract, Amazon Rekognition,
    Amazon Comprehend, Amazon Transcribe, and Amazon Translate.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Amazon A2I与Amazon SageMaker、Amazon Textract、Amazon Rekognition、Amazon Comprehend、Amazon
    Transcribe和Amazon Translate一起使用。
- en: SageMaker Model Monitor
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker模型监控器
- en: 'In [Chapter 3](ch03.html#chapter_three_ai_and_machine_learning), we briefly
    covered SageMaker Model Monitor. It is a fully managed service that allows for
    continuous review of AI models that are in production. It will detect different
    types of drift that can impact the performance of a model, including the following:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#chapter_three_ai_and_machine_learning)中，我们简要介绍了SageMaker模型监控器。它是一项完全托管的服务，允许持续审查生产中的AI模型。它将检测可能影响模型性能的不同类型的漂移，包括以下内容：
- en: Data quality drift
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量漂移
- en: Identifies changes in the statistical properties of input data, such as shifts
    in mean or variance
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 识别输入数据统计特性的变化，例如均值或方差的偏移
- en: Model quality drift
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 模型质量漂移
- en: Monitors the performance metrics like accuracy and precision by comparing model
    predictions against actual outcomes
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较模型预测与实际结果来监控性能指标，如准确性和精确度
- en: Bias drift
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差漂移
- en: Detects unintended biases in model predictions over time
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 检测模型预测中的无意偏差随时间的变化
- en: Feature attribution drift
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归因漂移
- en: Observes changes in the importance of input features in influencing model predictions
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 观察输入特征在影响模型预测中的重要性变化
- en: With the Model Monitor, you can establish baselines using training data to define
    acceptable performance thresholds. Monitoring jobs can be scheduled at regular
    intervals or executed on-demand.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型监控器，您可以使用训练数据建立基线，以定义可接受的性能阈值。监控作业可以定期安排或按需执行。
- en: Going Further with Responsible AI
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步推进负责任AI
- en: Let’s take a look at some additional considerations when it comes to creating
    responsible AI in the following sections.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在以下部分中创建负责任AI时需要考虑的一些额外因素。
- en: Sustainability and Environmental Considerations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可持续性和环境考量
- en: Sustainability and environmental considerations refer to the development of
    AI technologies that are viable over the long term—socially, economically, and
    environmentally—while actively minimizing ecological harm. This involves creating
    systems that not only deliver performance and innovation but also support societal
    well-being and reduce negative impacts on the planet. It includes managing the
    full lifecycle of AI systems, from the energy required to train and run models
    to the materials used in hardware, with the goal of lowering the environmental
    footprint and promoting responsible, resource-efficient practices.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续性和环境考量指的是开发那些在长期内（在社会、经济和环境方面）可行的AI技术，同时积极最小化生态损害。这包括创建系统，不仅提供性能和创新，而且支持社会福祉并减少对地球的负面影响。它包括管理AI系统的整个生命周期，从训练和运行模型所需的能源到硬件中使用的材料，目标是降低环境影响并促进负责任、资源高效的实践。
- en: These principles are central to responsible AI, which emphasizes the ethical,
    transparent, and accountable development of artificial intelligence. As AI continues
    to scale, its environmental impact can no longer be treated as an afterthought.
    Responsible AI initiatives must ensure that sustainability is built into the design,
    deployment, and governance of AI systems.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则是负责任AI的核心，强调人工智能的道德、透明和可问责的发展。随着AI的不断扩展，其环境影响不能再被视为事后考虑。负责任AI的倡议必须确保可持续性被融入AI系统的设计、部署和管理中。
- en: One major concern is the energy consumption associated with training and running
    large AI models. These processes can demand significant computational resources,
    which increases electricity use and contributes to greenhouse gas emissions. A
    responsible approach involves improving energy efficiency through better model
    architectures, using power-saving hardware, and sourcing electricity from renewable
    energy. For instance, optimizing training schedules to coincide with periods of
    low-carbon energy availability can reduce environmental impact without sacrificing
    performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要问题是与训练和运行大型AI模型相关的能源消耗。这些过程可能需要大量的计算资源，这增加了电力使用并导致温室气体排放。负责任的方法涉及通过更好的模型架构、使用节能硬件和从可再生能源获取电力来提高能源效率。例如，优化训练计划以与低碳能源可用性较低的时期相一致，可以在不牺牲性能的情况下减少环境影响。
- en: Another issue is the resource intensity of AI infrastructure. Manufacturing
    and deploying specialized hardware such as GPUs and TPUs often involves environmentally
    damaging materials and processes. Sustainable AI development promotes the reuse
    of existing hardware, prioritizes recyclable or longer-lasting components, and
    limits the production of electronic waste.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是与AI基础设施的资源强度相关。制造和部署专门的硬件，如GPU和TPU，通常涉及对环境有害的材料和过程。可持续AI开发促进现有硬件的再利用，优先考虑可回收或更持久的组件，并限制电子废物的生产。
- en: In addition, environmental impact assessments should be an integral part of
    the AI development lifecycle. These assessments evaluate both the direct effects
    (like energy use) and indirect effects (such as enabling high-emission industries)
    of deploying an AI system. Where risks are identified, mitigation strategies—such
    as reducing model size, leveraging cloud-based green computing, or introducing
    policy safeguards—should be put in place.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，环境影响评估应该是AI开发生命周期的一个组成部分。这些评估评估了部署AI系统的直接影响（如能源使用）和间接影响（如促进高排放行业）。在识别到风险时，应实施缓解策略——例如减少模型大小、利用基于云的绿色计算或引入政策保障措施。
- en: Data Preparation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: Creating responsible AI systems requires the thoughtful preparation of datasets
    to ensure fairness and accuracy. A key factor is balancing datasets so that AI
    models do not inadvertently favor certain groups or outcomes. For instance, in
    applications like hiring or lending, an unbalanced dataset could lead to biased
    decisions that unfairly disadvantage specific demographics.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: To achieve balanced datasets, it’s important to collect data that is both inclusive
    and diverse. This means ensuring that the dataset accurately reflects the variety
    of perspectives and experiences relevant to the AI system’s intended use. For
    example, if developing a healthcare AI model focused on diagnosing conditions
    across all age groups, the training data should include a representative sample
    of patients from different age brackets. Neglecting to do so could result in a
    model that performs well for one age group but poorly for others.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Beyond collection, data curation plays an essential role in balancing datasets.
    This involves preprocessing steps like cleaning the data to remove inaccuracies,
    normalizing data to ensure consistency, and selecting relevant features that contribute
    meaningfully to the model’s predictions. Data augmentation techniques, such as
    generating synthetic examples for underrepresented groups, can also help in achieving
    balance. Regular auditing of datasets is necessary to identify and correct any
    emerging biases over time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Tools like Amazon SageMaker Clarify and SageMaker Data Wrangler can assist in
    this process. SageMaker Clarify helps identify potential biases in datasets by
    analyzing the distribution of different features and outcomes. If imbalances are
    detected, SageMaker Data Wrangler offers methods like random oversampling, random
    undersampling, and the Synthetic Minority Oversampling Technique (SMOTE) to rebalance
    the data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability Versus Explainability
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the context of responsible AI, the priority between interpretability (covered
    in [Chapter 4](ch04.html#chapter_four_understanding_generative_a)) and explainability
    depends on the risk, regulatory environment, and stakeholders involved:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: This is generally favored when transparency and accountability are paramount,
    such as in regulated industries.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Explainability is essential when using complex models that can’t easily be interpreted,
    but human oversight is still required—for example, in predictive diagnostics or
    automated hiring.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Both are important pillars of responsible AI, but interpretability is often
    seen as the gold standard when decisions must be clearly understood. [Table 8-1](#table_eight_onedot_interpretability_ver)
    shows some scenarios for this.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8-1\. Interpretability versus explainability: when to use each'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '| Use case | Goal | Preferred approach | Rationale |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Loan approval in a bank | Regulatory compliance, fairness | Interpretability
    | Clear rules needed for auditability and legal compliance |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Diagnosing rare diseases with AI | High accuracy with human oversight | Explainability
    | Complex models like deep learning used, but need explanations for decisions
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 使用人工智能诊断罕见疾病 | 在人工监督下实现高精度 | 可解释性 | 使用了复杂的模型如深度学习，但需要对决策进行解释 |'
- en: '| Resume screening with ML | Bias prevention, HR transparency | Explainability
    | Must explain why a candidate was filtered out; internal logic may be opaque
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 使用机器学习进行简历筛选 | 防止偏见，人力资源透明度 | 可解释性 | 必须解释为什么候选人被筛选出去；内部逻辑可能不透明 |'
- en: '| Credit score predictions for consumers | Public trust, clarity | Interpretability
    | Consumers and regulators must understand how scores are computed |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 消费者信用评分预测 | 公众信任，清晰 | 可解释性 | 消费者和监管机构必须了解分数是如何计算的 |'
- en: Human-Centered Design
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以人为中心的设计
- en: 'Human-centered design (HCD) is when technology is created with the end user
    in mind. It’s about prioritizing clarity, usability, and fairness. By using HCD,
    you can provide for amplified decision making. These are key principles:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以人为中心的设计（HCD）是指技术是以最终用户为中心创建的。它关乎优先考虑清晰度、可用性和公平性。通过使用HCD，你可以提供增强的决策能力。这些是关键原则：
- en: Clarity
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 清晰性
- en: Information must be presented plainly—no jargon, no ambiguity. For example,
    a doctor reviewing an AI-recommended treatment plan needs a straightforward explanation
    of why it was suggested.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 信息必须以清晰的方式呈现——没有术语，没有歧义。例如，一位审查人工智能推荐的治疗方案的医生需要对其建议的原因有一个简单明了的解释。
- en: Simplicity
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性
- en: Less is more. Remove unnecessary data points and highlight what matters. A logistics
    manager doesn’t need the model’s internal math—just a clear route recommendation
    and a confidence level.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 少即是多。删除不必要的数据点，突出重点。物流经理不需要模型内部的数学——只需要一个清晰的路线推荐和一个置信水平。
- en: Usability
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性
- en: Interfaces should be intuitive for both tech-savvy and nontechnical users. A
    loan officer, for instance, should be able to navigate the AI tool without special
    training.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 界面应直观，既适用于技术熟练的用户，也适用于非技术用户。例如，一位贷款官员应该能够在没有特殊培训的情况下导航AI工具。
- en: Reflexivity
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 反思性
- en: Tools should prompt users to think critically about the decision. A pop-up asking
    “Is there additional context this system may have missed?” can trigger thoughtful
    review.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 工具应提示用户对决策进行批判性思考。一个弹出窗口询问“这个系统可能遗漏了额外的背景信息吗？”可以引发深思熟虑的审查。
- en: Accountability
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 责任
- en: There must be clear ownership over AI-assisted decisions. If a hiring tool recommends
    a candidate, the HR professional remains responsible for the final choice.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能辅助决策必须有明确的归属权。如果招聘工具推荐了候选人，人力资源专业人士仍然对最终选择负责。
- en: Personalization
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化
- en: Tailor the experience to the user. For example, a customer service AI can adapt
    its tone and suggestions based on an agent’s interaction style.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户定制体验。例如，客户服务人工智能可以根据代理人的互动风格调整其语气和建议。
- en: Cognitive apprenticeship
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 认知学徒制
- en: Just as junior employees learn by shadowing experts, AI systems should learn
    from experienced users through examples and corrections.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 就像初级员工通过跟随专家学习一样，人工智能系统应该通过示例和纠正从经验丰富的用户那里学习。
- en: User-centered tools
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以用户为中心的工具
- en: Make systems inclusive and accessible. A training platform should work equally
    well for an entry-level employee with a visual impairment and a senior manager
    with limited AI knowledge.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使系统包容并易于访问。例如，一个有视觉障碍的初级员工和一个对人工智能知识有限的资深经理都应该能够同样好地使用培训平台。
- en: RLHF
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF
- en: In [Chapter 4](ch04.html#chapter_four_understanding_generative_a), we briefly
    covered RLHF. This is where models learn to make better decisions by incorporating
    human preferences. RLHF plays an important role in responsible AI by aligning
    model behavior with human values, ethics, and expectations, helping to reduce
    harmful or biased outputs. It supports the creation of AI systems that are not
    only more accurate but also more transparent, fair, and aligned with societal
    norms.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#chapter_four_understanding_generative_a)中，我们简要介绍了RLHF。这是模型通过结合人类偏好来学习做出更好决策的地方。RLHF在负责任的AI中发挥着重要作用，通过使模型行为与人类价值观、伦理和期望保持一致，有助于减少有害或偏见的结果。它支持创建不仅更准确，而且更透明、公平，并与社会规范保持一致的AI系统。
- en: Imagine developing a virtual assistant designed to help users manage their daily
    tasks. Initially, the assistant might suggest reminders or schedule meetings based
    on general patterns. However, users might prefer certain suggestions over others.
    By observing which suggestions users accept or reject and gathering feedback on
    their preferences, the assistant can learn to tailor its recommendations more
    effectively.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下开发一个旨在帮助用户管理日常任务的虚拟助手。最初，助手可能会根据一般模式建议提醒或安排会议。然而，用户可能更喜欢某些建议而不是其他建议。通过观察用户接受或拒绝哪些建议以及收集他们对偏好的反馈，助手可以学会更有效地定制其推荐。
- en: 'These are some of the advantages of RLHF:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是强化学习与人类反馈（RLHF）的一些优势：
- en: Enhanced model performance
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 提高模型性能
- en: Models can refine their outputs to better meet user expectations. This can lead
    to improved accuracy and relevance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以优化其输出以更好地满足用户期望。这可能导致准确性和相关性的提高。
- en: Handling complex scenarios
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 处理复杂场景
- en: In situations where it’s challenging to define explicit rules, human feedback
    provides nuanced guidance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在难以定义明确规则的情况下，人类反馈提供了细微的指导。
- en: Improved user satisfaction
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 提高用户满意度
- en: Models that adapt based on user preferences tend to provide more personalized
    and satisfactory experiences. This helps to foster greater user trust and engagement.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户偏好进行适应的模型往往能提供更个性化和令人满意的体验。这有助于培养更大的用户信任和参与度。
- en: Platforms like Amazon SageMaker Ground Truth provide capabilities to incorporate
    RLHF into the ML lifecycle. For instance, data annotators can review model outputs,
    ranking or classifying them based on quality. This feedback serves as a valuable
    input for training models. This allows them to align more closely with human judgments
    and expectations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Amazon SageMaker Ground Truth这样的平台提供了将强化学习与人类反馈（RLHF）集成到机器学习生命周期中的能力。例如，数据标注员可以审查模型输出，根据质量对其进行排名或分类。这种反馈作为训练模型的有价值输入。这使它们能够更紧密地与人类判断和期望保持一致。
- en: Conclusion
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Responsible AI involves developing AI systems ethically, safely, and transparently.
    It is about managing risks like toxicity, intellectual property disputes, job
    displacement, and accuracy issues.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能涉及以道德、安全和透明的方式开发人工智能系统。它关乎管理如毒性、知识产权争议、就业替代和准确性问题等风险。
- en: In this chapter, we learned about the core principles of responsible AI and
    their use cases. We also saw the various tools from AWS that can help with the
    process, like Amazon Bedrock and SageMaker Clarify.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了负责任人工智能的核心原则及其用例。我们还看到了AWS提供的各种工具，如Amazon Bedrock和SageMaker Clarify，可以帮助这个过程。
- en: In the next chapter, we’ll look at security, compliance, and governance for
    AI solutions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨人工智能解决方案的安全、合规和治理。
- en: Quiz
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验
- en: To check your answers, please refer to the [“Chapter 8 Answer Key”](app02.html#answers_ch_8).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查你的答案，请参考[“第8章答案键”](app02.html#answers_ch_8)。
- en: Which of the following methods can help reduce overfitting in an AI model?
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪种方法可以帮助减少人工智能模型中的过拟合？
- en: Increasing the complexity of the model
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加模型的复杂性
- en: Adding more noisy data
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加更多噪声数据
- en: Stopping training early
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提前停止训练
- en: Avoiding hyperparameters
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免超参数
- en: What is one technique companies use to address intellectual property concerns
    in generative AI?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 公司用来解决生成人工智能中知识产权关注的技术是什么？
- en: Removing all training data from public sources
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从公共来源移除所有训练数据
- en: Restricting access to AI tools
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 限制对人工智能工具的访问
- en: Limiting AI to internal company use only
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将人工智能限制为仅限公司内部使用
- en: Creating licensing agreements with content providers
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与内容提供商创建许可协议
- en: Why is accuracy in AI models considered key to responsible AI?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么人工智能模型中的准确性被认为是负责任人工智能的关键？
- en: It improves reliability, trust, and safety.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提高了可靠性、信任和安全。
- en: Accurate models require fewer updates and patches.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确的模型需要更少的更新和补丁。
- en: Accuracy makes models less costly.
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确性使模型成本更低。
- en: Accuracy only matters for visual models.
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准确性仅对视觉模型重要。
- en: What is the main purpose of fairness in AI systems?
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工智能系统中公平性的主要目的是什么？
- en: Increasing the sophistication of models
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提高模型的复杂性
- en: Reducing latency in decision making
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少决策中的延迟
- en: Enhancing personalization features
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增强个性化功能
- en: Avoiding discrimination against individuals or groups
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免对个人或群体进行歧视
- en: How does explainability compare to transparency?
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可解释性与透明度如何比较？
- en: Explainability focuses on user interface design.
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可解释性关注用户界面设计。
- en: Explainability explains model decisions; transparency shares system details.
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可解释性解释模型决策；透明度分享系统细节。
- en: They mean the same thing.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们意味着相同的意思。
- en: Transparency is only required in open source models.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 透明度仅在开源模型中是必需的。
- en: What do privacy and security in AI primarily aim to protect?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工智能中的隐私和安全主要旨在保护什么？
- en: Model weights and parameters
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型权重和参数
- en: Algorithm transparency
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法透明度
- en: Individual data and usage control
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 个人数据和用法控制
- en: Developer intellectual property
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发者知识产权
- en: ^([1](ch08.html#ch01fn26-marker)) Jake Coyle, [“In Hollywood Writers’ Battle
    Against AI, Humans Win (for Now)”](https://oreil.ly/KrpWD), Associated Press,
    September 27, 2023.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#ch01fn26-marker)) Jake Coyle, [“在好莱坞编剧与人工智能的斗争中，人类暂时获胜”](https://oreil.ly/KrpWD),
    联合社，2023年9月27日。
- en: ^([2](ch08.html#ch01fn28-marker)) Sanya Mansoor, [“A Viral Tweet Accused Apple’s
    New Credit Card of Being ‘Sexist’”](https://oreil.ly/lE4hb), *Time*, November
    12, 2019.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.html#ch01fn28-marker)) Sanya Mansoor, [“一条病毒式推文指责苹果的新信用卡具有‘性别歧视’”](https://oreil.ly/lE4hb),
    *《时代》杂志*, 2019年11月12日。
