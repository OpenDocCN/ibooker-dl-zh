# 第十九章。将模型从 TensorFlow 迁移到 TensorFlow Lite

如果你已经走到这一步，你会明白我们倡导在新任务中尽可能重用现有模型。从头开始训练一个全新的模型可能需要大量时间和实验，即使是专家也经常无法在尝试许多不同的原型之前预测最佳方法。这意味着创建新架构的完整指南超出了本书的范围，我们建议查看第二十一章以获取更多相关信息。然而，有一些方面（如使用受限操作集或预处理需求）是独特于资源受限、设备端机器学习的，因此本章提供了关于这些方面的建议。

# 了解需要哪些操作

本书侧重于在 TensorFlow 中创建的模型，因为作者在 Google 团队工作，但即使在一个框架内，创建模型的方式有很多不同。如果你查看[语音命令训练脚本](https://oreil.ly/ZTYu7)，你会看到它直接使用核心 TensorFlow 操作构建模型，并手动运行训练循环。这在当今是一种相当老式的工作方式（该脚本最初是在 2017 年编写的），而使用 TensorFlow 2.0 的现代示例可能会使用 Keras 作为一个高级 API，它会处理很多细节。

这样做的缺点是，从检查代码中不再明显地了解模型使用的底层操作。相反，它们将作为层的一部分被创建，这些层代表图中的较大块在一个调用中。这是一个问题，因为了解模型使用了哪些 TensorFlow 操作对于理解模型是否能在 TensorFlow Lite 中运行以及资源需求是非常重要的。幸运的是，即使从 Keras 中，只要可以使用[`tf.keras.backend.get_session()`](https://oreil.ly/4zurk)检索底层的`Session`对象，你仍然可以访问底层的低级操作。如果你直接在 TensorFlow 中编码，很可能已经将会话存储在一个变量中，所以下面的代码仍然有效：

```py
for op in sess.graph.get_operations():
  print(op.type)
```

如果你将会话分配给了`sess`变量，这将打印出模型中所有操作的类型。你也可以访问其他属性，比如`name`，以获取更多信息。了解 TensorFlow 操作的存在将有助于在转换过程中到 TensorFlow Lite 时；否则，你看到的任何错误将更难理解。

# 查看 Tensorflow Lite 中现有操作的覆盖范围

TensorFlow Lite 仅支持 TensorFlow 的一部分操作，并且有一些限制。你可以在[操作兼容性指南](https://oreil.ly/Pix9U)中查看最新列表。这意味着如果你计划创建一个新模型，你应该确保一开始就不依赖于不受支持的功能或操作。特别是，LSTMs、GRUs 和其他递归神经网络目前还不能使用。目前在完整的移动版本 TensorFlow Lite 和微控制器分支之间存在差距。了解当前 TensorFlow Lite for Microcontrollers 支持哪些操作的最简单方法是查看[*all_ops_resolver.cc*](https://oreil.ly/HNpmM)，因为操作不断被添加。

在 TensorFlow 训练会话中显示的操作与 TensorFlow Lite 支持的操作进行比较可能会有点混淆，因为在导出过程中会发生几个转换步骤。例如，这些步骤将存储为变量的权重转换为常量，并可能将浮点操作量化为其整数等效项以进行优化。还有一些仅作为训练循环的一部分存在的操作，比如参与反向传播的操作，这些操作将被完全剥离。找出可能遇到的问题的最佳方法是在创建模型后立即尝试导出潜在模型，而不是在训练之前，这样您就可以在花费大量时间进行训练之前调整其结构。

# 将预处理和后处理移入应用代码

深度学习模型通常有三个阶段。通常有一个预处理步骤，可能只是从磁盘加载图像和标签并解码 JPEG，或者像将音频数据转换为频谱图这样复杂的语音示例。然后是一个核心神经网络，它接收值数组并以类似形式输出结果。最后，您需要在后处理步骤中理解这些值。对于许多分类问题，这只是将向量中的分数与相应的标签进行匹配，但是如果看一下像 [MobileSSD](https://oreil.ly/QT_dS) 这样的模型，网络输出是一堆重叠的边界框，需要经过一个称为“非最大抑制”的复杂过程才能作为结果有用。

核心神经网络模型通常是计算量最大的部分，通常由相对较少的操作组成，如卷积和激活。预处理和后处理阶段通常需要更多的操作，包括控制流，尽管它们的计算负载要低得多。这意味着通常更合理的做法是将非核心步骤作为应用中的常规代码实现，而不是将它们嵌入到 TensorFlow Lite 模型中。例如，机器视觉模型的神经网络部分将接收特定尺寸的图像，如高 224 像素，宽 224 像素。在训练环境中，我们将使用 `DecodeJpeg` 操作，然后是 `ResizeImages` 操作将结果转换为正确的尺寸。然而，在设备上运行时，我们几乎肯定是从固定大小的源中获取输入图像，无需解压缩，因此编写自定义代码来创建神经网络输入比依赖库中的通用操作更有意义。我们可能还需要处理异步捕获，并可能从线程化所涉及的工作中获得一些好处。在语音命令的情况下，我们会做很多工作来缓存 FFT 的中间结果，以便在流式输入运行时尽可能重用尽可能多的计算。

并非每个模型在训练环境中都有显著的后处理阶段，但是在设备上运行时，通常希望利用随时间的连贯性来改善向用户显示的结果。即使模型只是一个分类器，唤醒词检测代码每秒运行多次并且 [使用平均值](https://oreil.ly/E68Q4) 来提高结果的准确性是非常常见的。这种代码最好在应用级别实现，因为将其表达为 TensorFlow Lite 操作很困难，并且并不提供太多好处。虽然可能会看到在 [*detection_postprocess.cc*](https://oreil.ly/IMlsT) 中，但是这需要在导出过程中从底层 TensorFlow 图中进行大量工作的连接，因为通常表达为 TensorFlow 中的小操作并不是在设备上实现它的有效方式。

这意味着您应该尝试排除图中的非核心部分，这将需要一些工作来确定哪些部分是哪些。我们发现[Netron](https://oreil.ly/qoQNY)是一个很好的工具，可以用来探索 TensorFlow Lite 图，了解存在哪些操作，并了解它们是神经网络的核心部分还是仅仅是处理步骤。一旦了解内部发生的情况，您应该能够隔离核心网络，仅导出这些操作，并将其余部分实现为应用程序代码。

# 必要时实现所需操作

如果您发现有一些您绝对需要的 TensorFlow 操作在 TensorFlow Lite 中不受支持，那么可以将它们保存为 TensorFlow Lite 文件格式中的 *自定义* 操作，然后在框架内自行实现。完整的过程超出了本书的范围，但以下是关键步骤：

+   使用启用 `allow_custom_ops` 的 `toco` 运行，以便将不受支持的操作存储为序列化模型文件中的自定义操作。

+   编写实现操作的内核，并在您的应用程序中使用的 op 解析器中使用 `AddCustom()` 进行注册。

+   在调用 `Init()` 方法时，解压存储在 FlexBuffer 格式中的参数。

# 优化操作

即使您在新模型中使用了受支持的操作，您可能以尚未优化的方式使用它们。TensorFlow Lite 团队的优先事项受特定用例驱动，因此如果您正在运行一个新模型，可能会遇到尚未优化的代码路径。我们在第十五章中讨论了这一点，但正如我们建议您尽快检查导出兼容性一样——甚至在训练模型之前——确保在计划开发时间表之前获得所需的性能是值得的，因为您可能需要预留一些时间来处理操作延迟。

# 总结

训练一个新颖的神经网络以成功完成任务本身就具有挑战性，但要想构建一个能够产生良好结果并在嵌入式硬件上高效运行的网络更加困难！本章讨论了您将面临的一些挑战，并提供了克服这些挑战的方法建议，但这是一个庞大且不断增长的研究领域，因此我们建议查看第二十一章中的一些资源，看看是否有新的灵感来源可以用于您的模型架构。特别是，在这个领域，跟踪 arXiv 上最新的研究论文可能非常有用。

克服所有这些挑战后，您应该拥有一个小巧、快速、节能的产品，可以随时部署到现实世界中。在发布之前，值得考虑一下它可能对用户造成的潜在有害影响，因此第二十章涵盖了围绕隐私和安全的问题。
