["```py\nfrom sklearn.datasets import load_sample_images\nimport tensorflow as tf\n\nimages = load_sample_images()[\"images\"]\nimages = tf.keras.layers.CenterCrop(height=70, width=120)(images)\nimages = tf.keras.layers.Rescaling(scale=1 / 255)(images)\n```", "```py\n>>> images.shape\nTensorShape([2, 70, 120, 3])\n```", "```py\nconv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\nfmaps = conv_layer(images)\n```", "```py\n>>> fmaps.shape\nTensorShape([2, 64, 114, 32])\n```", "```py\n>>> conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n...                                     padding=\"same\")\n...\n>>> fmaps = conv_layer(images)\n>>> fmaps.shape\nTensorShape([2, 70, 120, 32])\n```", "```py\n>>> kernels, biases = conv_layer.get_weights()\n>>> kernels.shape\n(7, 7, 3, 32)\n>>> biases.shape\n(32,)\n```", "```py\nmax_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n```", "```py\nclass DepthPool(tf.keras.layers.Layer):\n    def __init__(self, pool_size=2, **kwargs):\n        super().__init__(**kwargs)\n        self.pool_size = pool_size\n\n    def call(self, inputs):\n        shape = tf.shape(inputs)  # shape[-1] is the number of channels\n        groups = shape[-1] // self.pool_size  # number of channel groups\n        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)\n```", "```py\nglobal_avg_pool = tf.keras.layers.GlobalAvgPool2D()\n```", "```py\nglobal_avg_pool = tf.keras.layers.Lambda(\n    lambda X: tf.reduce_mean(X, axis=[1, 2]))\n```", "```py\n>>> global_avg_pool(images)\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.64338624, 0.5971759 , 0.5824972 ],\n [0.76306933, 0.26011038, 0.10849128]], dtype=float32)>\n```", "```py\nfrom functools import partial\n\nDefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n                        activation=\"relu\", kernel_initializer=\"he_normal\")\nmodel = tf.keras.Sequential([\n    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n    tf.keras.layers.MaxPool2D(),\n    DefaultConv2D(filters=128),\n    DefaultConv2D(filters=128),\n    tf.keras.layers.MaxPool2D(),\n    DefaultConv2D(filters=256),\n    DefaultConv2D(filters=256),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=128, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(units=64, activation=\"relu\",\n                          kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n])\n```", "```py\nDefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n                        padding=\"same\", kernel_initializer=\"he_normal\",\n                        use_bias=False)\n\nclass ResidualUnit(tf.keras.layers.Layer):\n    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n        super().__init__(**kwargs)\n        self.activation = tf.keras.activations.get(activation)\n        self.main_layers = [\n            DefaultConv2D(filters, strides=strides),\n            tf.keras.layers.BatchNormalization(),\n            self.activation,\n            DefaultConv2D(filters),\n            tf.keras.layers.BatchNormalization()\n        ]\n        self.skip_layers = []\n        if strides > 1:\n            self.skip_layers = [\n                DefaultConv2D(filters, kernel_size=1, strides=strides),\n                tf.keras.layers.BatchNormalization()\n            ]\n\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.main_layers:\n            Z = layer(Z)\n        skip_Z = inputs\n        for layer in self.skip_layers:\n            skip_Z = layer(skip_Z)\n        return self.activation(Z + skip_Z)\n```", "```py\nmodel = tf.keras.Sequential([\n    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation(\"relu\"),\n    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n])\nprev_filters = 64\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n    strides = 1 if filters == prev_filters else 2\n    model.add(ResidualUnit(filters, strides=strides))\n    prev_filters = filters\n\nmodel.add(tf.keras.layers.GlobalAvgPool2D())\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n```", "```py\nmodel = tf.keras.applications.ResNet50(weights=\"imagenet\")\n```", "```py\nimages = load_sample_images()[\"images\"]\nimages_resized = tf.keras.layers.Resizing(height=224, width=224,\n                                          crop_to_aspect_ratio=True)(images)\n```", "```py\ninputs = tf.keras.applications.resnet50.preprocess_input(images_resized)\n```", "```py\n>>> Y_proba = model.predict(inputs)\n>>> Y_proba.shape\n(2, 1000)\n```", "```py\ntop_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\nfor image_index in range(len(images)):\n    print(f\"Image #{image_index}\")\n    for class_id, name, y_proba in top_K[image_index]:\n        print(f\" {class_id} - {name:12s}{y_proba:.2%}\")\n```", "```py\nImage #0\n  n03877845 - palace       54.69%\n  n03781244 - monastery    24.72%\n  n02825657 - bell_cote    18.55%\nImage #1\n  n04522168 - vase         32.66%\n  n11939491 - daisy        17.81%\n  n03530642 - honeycomb    12.06%\n```", "```py\nimport tensorflow_datasets as tfds\n\ndataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\ndataset_size = info.splits[\"train\"].num_examples  # 3670\nclass_names = info.features[\"label\"].names  # [\"dandelion\", \"daisy\", ...]\nn_classes = info.features[\"label\"].num_classes  # 5\n```", "```py\ntest_set_raw, valid_set_raw, train_set_raw = tfds.load(\n    \"tf_flowers\",\n    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n    as_supervised=True)\n```", "```py\nbatch_size = 32\npreprocess = tf.keras.Sequential([\n    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n])\ntrain_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\ntrain_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\nvalid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\ntest_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n```", "```py\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n])\n```", "```py\nbase_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n                                                     include_top=False)\navg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\nmodel = tf.keras.Model(inputs=base_model.input, outputs=output)\n```", "```py\nfor layer in base_model.layers:\n    layer.trainable = False\n```", "```py\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set, validation_data=valid_set, epochs=3)\n```", "```py\nfor layer in base_model.layers[56:]:\n    layer.trainable = True\n```", "```py\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set, validation_data=valid_set, epochs=10)\n```", "```py\nbase_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n                                                     include_top=False)\navg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\nclass_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\nloc_output = tf.keras.layers.Dense(4)(avg)\nmodel = tf.keras.Model(inputs=base_model.input,\n                       outputs=[class_output, loc_output])\nmodel.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n              loss_weights=[0.8, 0.2],  # depends on what you care most about\n              optimizer=optimizer, metrics=[\"accuracy\"])\n```"]