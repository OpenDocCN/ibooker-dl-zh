- en: Appendix D. TensorFlow Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this appendix, we will explore the graphs generated by TF functions (see
    [Chapter 12](ch12.html#tensorflow_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: TF Functions and Concrete Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TF functions are polymorphic, meaning they support inputs of different types
    (and shapes). For example, consider the following `tf_cube()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Every time you call a TF function with a new combination of input types or
    shapes, it generates a new *concrete function*, with its own graph specialized
    for this particular combination. Such a combination of argument types and shapes
    is called an *input signature*. If you call the TF function with an input signature
    it has already seen before, it will reuse the concrete function it generated earlier.
    For example, if you call `tf_cube(tf.constant(3.0))`, the TF function will reuse
    the same concrete function it used for `tf_cube(tf.constant(2.0))` (for float32
    scalar tensors). But it will generate a new concrete function if you call `tf_cube(tf.constant([2.0]))`
    or `tf_cube(tf.constant([3.0]))` (for float32 tensors of shape [1]), and yet another
    for `tf_cube(tf.constant([[1.0, 2.0], [3.0, 4.0]]))` (for float32 tensors of shape
    [2, 2]). You can get the concrete function for a particular combination of inputs
    by calling the TF function’s `get_concrete_function()` method. It can then be
    called like a regular function, but it will only support one input signature (in
    this example, float32 scalar tensors):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure D-1](#tf_function_diagram) shows the `tf_cube()` TF function, after
    we called `tf_cube(2)` and `tf_cube(tf.constant(2.0))`: two concrete functions
    were generated, one for each signature, each with its own optimized *function
    graph* (`FuncGraph`) and its own *function definition* (`FunctionDef`). A function
    definition points to the parts of the graph that correspond to the function’s
    inputs and outputs. In each `FuncGraph`, the nodes (ovals) represent operations
    (e.g., power, constants, or placeholders for arguments like `x`), while the edges
    (the solid arrows between the operations) represent the tensors that will flow
    through the graph. The concrete function on the left is specialized for `x=2`,
    so TensorFlow managed to simplify it to just output 8 all the time (note that
    the function definition does not even have an input). The concrete function on
    the right is specialized for float32 scalar tensors, and it could not be simplified.
    If we call `tf_cube(tf.constant(5.0))`, the second concrete function will be called,
    the placeholder operation for `x` will output 5.0, then the power operation will
    compute `5.0 ** 3`, so the output will be 125.0.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 ad01](assets/mls3_ad01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure D-1\. The `tf_cube()` TF function, with its `ConcreteFunction`s and their
    `FuncGraph`s
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The tensors in these graphs are *symbolic tensors*, meaning they don’t have
    an actual value, just a data type, a shape, and a name. They represent the future
    tensors that will flow through the graph once an actual value is fed to the placeholder
    `x` and the graph is executed. Symbolic tensors make it possible to specify ahead
    of time how to connect operations, and they also allow TensorFlow to recursively
    infer the data types and shapes of all tensors, given the data types and shapes
    of their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s continue to peek under the hood, and see how to access function definitions
    and function graphs and how to explore a graph’s operations and tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Function Definitions and Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can access a concrete function’s computation graph using the `graph` attribute,
    and get the list of its operations by calling the graph’s `get_operations()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the first operation represents the input argument `x` (it
    is called a *placeholder*), the second “operation” represents the constant `3`,
    the third operation represents the power operation (`**`), and the final operation
    represents the output of this function (it is an identity operation, meaning it
    will do nothing more than copy the output of the power operation⁠^([1](app04.html#idm45720155973312))).
    Each operation has a list of input and output tensors that you can easily access
    using the operation’s `inputs` and `outputs` attributes. For example, let’s get
    the list of inputs and outputs of the power operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This computation graph is represented in [Figure D-2](#computation_graph_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 ad02](assets/mls3_ad02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure D-2\. Example of a computation graph
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note that each operation has a name. It defaults to the name of the operation
    (e.g., `"pow"`), but you can define it manually when calling the operation (e.g.,
    `tf.pow(x, 3, name="other_name")`). If a name already exists, TensorFlow automatically
    adds a unique index (e.g., `"pow_1"`, `"pow_2"`, etc.). Each tensor also has a
    unique name: it is always the name of the operation that outputs this tensor,
    plus `:0` if it is the operation’s first output, or `:1` if it is the second output,
    and so on. You can fetch an operation or a tensor by name using the graph’s `get_operation_by_name()`
    or `get_tensor_by_name()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The concrete function also contains the function definition (represented as
    a protocol buffer⁠^([2](app04.html#idm45720155848416))), which includes the function’s
    signature. This signature allows the concrete function to know which placeholders
    to feed with the input values, and which tensors to return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s look more closely at tracing.
  prefs: []
  type: TYPE_NORMAL
- en: A Closer Look at Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s tweak the `tf_cube()` function to print its input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s call it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `result` looks good, but look at what was printed: `x` is a symbolic tensor!
    It has a shape and a data type, but no value. Plus it has a name (`"x:0"`). This
    is because the `print()` function is not a TensorFlow operation, so it will only
    run when the Python function is traced, which happens in graph mode, with arguments
    replaced with symbolic tensors (same type and shape, but no value). Since the
    `print()` function was not captured into the graph, the next times we call `tf_cube()`
    with float32 scalar tensors, nothing is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'But if we call `tf_cube()` with a tensor of a different type or shape, or with
    a new Python value, the function will be traced again, so the `print()` function
    will be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your function has Python side effects (e.g., it saves some logs to disk),
    be aware that this code will only run when the function is traced (i.e., every
    time the TF function is called with a new input signature). It’s best to assume
    that the function may be traced (or not) any time the TF function is called.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, you may want to restrict a TF function to a specific input signature.
    For example, suppose you know that you will only ever call a TF function with
    batches of 28 × 28–pixel images, but the batches will have very different sizes.
    You may not want TensorFlow to generate a different concrete function for each
    batch size, or count on it to figure out on its own when to use `None`. In this
    case, you can specify the input signature like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This TF function will accept any float32 tensor of shape [*, 28, 28], and it
    will reuse the same concrete function every time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you try to call this TF function with a Python value, or a tensor
    of an unexpected data type or shape, you will get an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using AutoGraph to Capture Control Flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If your function contains a simple `for` loop, what do you expect will happen?
    For example, let’s write a function that will add 10 to its input, by just adding
    1 10 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It works fine, but when we look at its graph, we find that it does not contain
    a loop: it just contains 10 addition operations!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This actually makes sense: when the function got traced, the loop ran 10 times,
    so the `x += 1` operation was run 10 times, and since it was in graph mode, it
    recorded this operation 10 times in the graph. You can think of this `for` loop
    as a “static” loop that gets unrolled when the graph is created.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want the graph to contain a “dynamic” loop instead (i.e., one that runs
    when the graph is executed), you can create one manually using the `tf.while_loop()`
    operation, but it is not very intuitive (see the “Using AutoGraph to Capture Control
    Flow” section of the Chapter 12 notebook for an example). Instead, it is much
    simpler to use TensorFlow’s *AutoGraph* feature, discussed in [Chapter 12](ch12.html#tensorflow_chapter).
    AutoGraph is actually activated by default (if you ever need to turn it off, you
    can pass `autograph=False` to `tf.function()`). So if it is on, why didn’t it
    capture the `for` loop in the `add_10()` function? It only captures `for` loops
    that iterate over tensors of `tf.data.Dataset` objects, so you should use `tf.range()`,
    not `range()`. This is to give you the choice:'
  prefs: []
  type: TYPE_NORMAL
- en: If you use `range()`, the `for` loop will be static, meaning it will only be
    executed when the function is traced. The loop will be “unrolled” into a set of
    operations for each iteration, as we saw.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use `tf.range()`, the loop will be dynamic, meaning that it will be included
    in the graph itself (but it will not run during tracing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the graph that gets generated if we just replace `range()` with
    `tf.range()` in the `add_10()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the graph now contains a `While` loop operation, as if we had
    called the `tf.while_loop()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Variables and Other Resources in TF Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In TensorFlow, variables and other stateful objects, such as queues or datasets,
    are called *resources*. TF functions treat them with special care: any operation
    that reads or updates a resource is considered stateful, and TF functions ensure
    that stateful operations are executed in the order they appear (as opposed to
    stateless operations, which may be run in parallel, so their order of execution
    is not guaranteed). Moreover, when you pass a resource as an argument to a TF
    function, it gets passed by reference, so the function may modify it. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If you peek at the function definition, the first argument is marked as a resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to use a `tf.Variable` defined outside of the function,
    without explicitly passing it as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The TF function will treat this as an implicit first argument, so it will actually
    end up with the same signature (except for the name of the argument). However,
    using global variables can quickly become messy, so you should generally wrap
    variables (and other resources) inside classes. The good news is `@tf.function`
    works fine with methods too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do not use `=`, `+=`, `-=`, or any other Python assignment operator with TF
    variables. Instead, you must use the `assign()`, `assign_add()`, or `assign_sub()`
    methods. If you try to use a Python assignment operator, you will get an exception
    when you call the method.
  prefs: []
  type: TYPE_NORMAL
- en: A good example of this object-oriented approach is, of course, Keras. Let’s
    see how to use TF functions with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Using TF Functions with Keras (or Not)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, any custom function, layer, or model you use with Keras will automatically
    be converted to a TF function; you do not need to do anything at all! However,
    in some cases you may want to deactivate this automatic conversion—for example,
    if your custom code cannot be turned into a TF function, or if you just want to
    debug your code (which is much easier in eager mode). To do this, you can simply
    pass `dynamic=True` when creating the model or any of its layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If your custom model or layer will always be dynamic, you can instead call
    the base class’s constructor with `dynamic=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can pass `run_eagerly=True` when calling the `compile()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now you know how TF functions handle polymorphism (with multiple concrete functions),
    how graphs are automatically generated using AutoGraph and tracing, what graphs
    look like, how to explore their symbolic operations and tensors, how to handle
    variables and resources, and how to use TF functions with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](app04.html#idm45720155973312-marker)) You can safely ignore it—it is only
    here for technical reasons, to ensure that TF functions don’t leak internal structures.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](app04.html#idm45720155848416-marker)) A popular binary format discussed
    in [Chapter 13](ch13.html#data_chapter).
  prefs: []
  type: TYPE_NORMAL
