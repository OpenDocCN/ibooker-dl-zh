- en: Chapter 7\. Agents II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) introduced
    the *agent* architecture, the most powerful of the LLM architectures we have seen
    up until now. It is hard to overstate the potential of this combination of chain-of-thought
    prompting, tool use, and looping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter discusses two extensions to the agent architecture that improve
    performance for some use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs: []
  type: TYPE_NORMAL
- en: Taking another page out of the repertoire of human thought patterns, this is
    about giving your LLM app the opportunity to analyze its past output and choices,
    together with the ability to remember reflections from past iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-agent
  prefs: []
  type: TYPE_NORMAL
- en: Much the same way as a team can accomplish more than a single person, there
    are problems that can be best tackled by teams of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with reflection.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One prompting technique we haven’t covered yet is *reflection* (also known as
    *self-critique*). *Reflection* is the creation of a loop between a creator prompt
    and a reviser prompt. This mirrors the creation process for many human-created
    artifacts, such as this chapter you’re reading now, which is the result of a back
    and forth between the authors, reviewers, and editor until all are happy with
    the final product.
  prefs: []
  type: TYPE_NORMAL
- en: As with many of the prompting techniques we have seen so far, reflection can
    be combined with other techniques, such as chain-of-thought and tool calling.
    In this section, we’ll look at reflection in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: A parallel can be drawn to the modes of human thinking known as *System 1* (reactive
    or instinctive) and *System 2* (methodical and reflective), first introduced by
    Daniel Kahneman in the book *Thinking, Fast and Slow* (Farrar, Straus and Giroux,
    2011). When applied correctly, self-critique can help LLM applications get closer
    to something that resembles System 2 behavior ([Figure 7-1](#ch07_figure_1_1736545673018473)).
  prefs: []
  type: TYPE_NORMAL
- en: '![System 1 and System 2 thinking](assets/lelc_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. System 1 and System 2 thinking
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’ll implement reflection as a graph with two nodes: `generate` and `reflect`.
    This graph will be tasked with writing three-paragraph essays, with the `generate`
    node writing or revising drafts of the essay, and `reflect` writing a critique
    to inform the next revision. We’ll run the loop a fixed number of times, but a
    variation on this technique would be to have the `reflect` node decide when to
    finish. Let’s see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The visual representation of the graph is shown in [Figure 7-2](#ch07_figure_2_1736545673018506).
  prefs: []
  type: TYPE_NORMAL
- en: '![The Reflection architecture](assets/lelc_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. The reflection architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how the `reflect` node tricks the LLM into thinking it is critiquing
    essays written by the user. And in tandem, the `generate` node is made to think
    that the critique comes from the user. This subterfuge is required because dialogue-tuned
    LLMs are trained on pairs of human-AI messages, so a sequence of many messages
    from the same participant would result in poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing to note: you might, at first glance, expect the end to come
    after a revise step, but in this architecture we have a fixed number of iterations
    of the `generate-reflect` loop; therefore we terminate after `generate` (so that
    the last set of revisions requested are dealt with). A variation on this architecture
    would instead have the `reflect` step make the decision to end the process (once
    it had no more comments).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what one of the critiques looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And the final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This simple type of reflection can sometimes improve performance by giving the
    LLM multiple attempts at refining its output and by letting the reflection node
    adopt a different persona while critiquing the output.
  prefs: []
  type: TYPE_NORMAL
- en: There are several possible variations of this architecture. For one, we could
    combine the reflection step with the agent architecture of [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341),
    adding it as the last node right before sending output to the user. This would
    make the critique appear to come from the user, and give the application a chance
    to improve its final output without direct user intervention. Obviously this approach
    would come at the expense of higher latency.
  prefs: []
  type: TYPE_NORMAL
- en: In certain use cases, it could be helpful to ground the critique with external
    information. For instance, if you were writing a code-generation agent, you could
    have a step before `reflect` that would run the code through a linter or compiler
    and report any errors as input to `reflect`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Whenever this approach is possible, we strongly recommend giving it a try, as
    it’s likely to increase the quality of the final output.
  prefs: []
  type: TYPE_NORMAL
- en: Subgraphs in LangGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into multi-agent architectures, let’s look at an important technical
    concept in LangGraph that enables it. *Subgraphs* are graphs that are used as
    part of another graph. Here are some use cases for subgraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: Building multi-agent systems (discussed in the next section).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want to reuse a set of nodes in multiple graphs, you can define them
    once in a subgraph and then use them in multiple parent graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want different teams to work on different parts of the graph independently,
    you can define each part as a subgraph, and as long as the subgraph interface
    (the input and output schemas) is respected, the parent graph can be built without
    knowing any details of the subgraph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways to add subgraph nodes to a parent graph:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a node that calls the subgraph directly
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when the parent graph and the subgraph share state keys, and
    you don’t need to transform state on the way in or out.
  prefs: []
  type: TYPE_NORMAL
- en: Add a node with a function that invokes the subgraph
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when the parent graph and the subgraph have different state schemas,
    and you need to transform state before or after calling the subgraph.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at each in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Calling a Subgraph Directly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest way to create subgraph nodes is to attach a subgraph directly as
    a node. When doing so, it is important that the parent graph and the subgraph
    share state keys, because those shared keys will be used to communicate. (If your
    graph and subgraph do not share any keys, see the next section.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you pass extra keys to the subgraph node (that is, in addition to the shared
    keys), they will be ignored by the subgraph node. Similarly, if you return extra
    keys from the subgraph, they will be ignored by the parent graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what it looks like in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Calling a Subgraph with a Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might want to define a subgraph with a completely different schema. In that
    case, you can create a node with a function that invokes the subgraph. This function
    will need to transform the input (parent) state to the subgraph state before invoking
    the subgraph and transform the results back to the parent state before returning
    the state update from the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know how to use subgraphs, let’s take a look at one of the big
    use cases for them: multi-agent architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Agent Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As LLM agents grow in size, scope, or complexity, several issues can show up
    and impact their performance, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent is given too many tools to choose from and makes poor decisions about
    which tool to call next ([Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341)
    discussed some approaches to this problem).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context grows too complex for a single agent to keep track of; that is,
    the size of the prompts and the number of things they mention grows beyond the
    capability of the model you’re using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to use a specialized subsystem for a particular area, for instance,
    planning, research, solving math problems, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To tackle these problems, you might consider breaking your application into
    multiple smaller, independent agents and composing them into a multi-agent system.
    These independent agents can be as simple as a prompt and an LLM call or as complex
    as a ReAct agent (introduced in [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341)).
    [Figure 7-3](#ch07_figure_3_1736545673018556) illustrates several ways to connect
    agents in a multi-agent system.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a network  Description automatically generated](assets/lelc_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Multiple strategies for coordinating multiple agents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at [Figure 7-3](#ch07_figure_3_1736545673018556) in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs: []
  type: TYPE_NORMAL
- en: Each agent can communicate with every other agent. Any agent can decide which
    other agent is to be executed next.
  prefs: []
  type: TYPE_NORMAL
- en: Supervisor
  prefs: []
  type: TYPE_NORMAL
- en: Each agent communicates with a single agent, called the *supervisor*. The supervisor
    agent makes decisions on which agent (or agents) should be called next. A special
    case of this architecture implements the supervisor agent as an LLM call with
    tools, as covered in [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341).
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical
  prefs: []
  type: TYPE_NORMAL
- en: You can define a multi-agent system with a supervisor of supervisors. This is
    a generalization of the supervisor architecture and allows for more complex control
    flows.
  prefs: []
  type: TYPE_NORMAL
- en: Custom multi-agent workflow
  prefs: []
  type: TYPE_NORMAL
- en: Each agent communicates with only a subset of agents. Parts of the flow are
    deterministic, and only select agents can decide which other agents to call next.
  prefs: []
  type: TYPE_NORMAL
- en: The next section dives deeper into the supervisor architecture, which we think
    has a good balance of capability and ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: Supervisor Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this architecture, we add each agent to the graph as a node and also add
    a supervisor node, which decides which agents should be called next. We use conditional
    edges to route execution to the appropriate agent node based on the supervisor’s
    decision. Refer back to [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    for an introduction to LangGraph, which goes over the concepts of nodes, edges,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first see what the supervisor node looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code in the prompt requires the names of your subagents to be self-explanatory
    and distinct. For instance, if they were simply called `agent_1` and `agent_2`,
    the LLM would have no information to decide which one is appropriate for each
    task. If needed, you could modify the prompt to add a description of each agent,
    which could help the LLM in picking an agent for each query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see how to integrate this supervisor node into a larger graph that
    includes two other subagents, which we will call researcher and coder. Our overall
    goal with this graph is to handle queries that can be answered either by the researcher
    by itself or the coder by itself, or even both of them in succession. This example
    doesn’t include implementations for either the researcher or coder—the key idea
    is they could be any other LangGraph graph or node:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things to notice: In this example, both subagents (researcher and coder)
    can see each other’s work, as all progress is recorded in the messages list. This
    isn’t the only way to organize this. Each of the subagents could be more complex.
    For instance, a subagent could be its own graph that maintains internal state
    and only outputs a summary of the work it did.'
  prefs: []
  type: TYPE_NORMAL
- en: After each agent executes, we route back to the supervisor node, which decides
    if there is more work to be done and which agent to delegate that to if so. This
    routing isn’t a hard requirement for this architecture; we could have each subagent
    make a decision as to whether its output should be returned directly to the user.
    To do that, we’d replace the hard edge between, say, researcher and supervisor,
    with a conditional edge (which would read some state key updated by researcher).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covered two important extensions to the agent architecture: reflection
    and multi-agent architectures. The chapter also looked at how to work with subgraphs
    in LangGraph, which are a key building block for multi-agent systems.'
  prefs: []
  type: TYPE_NORMAL
- en: These extensions add more power to the LLM agent architecture, but they shouldn’t
    be the first thing you reach for when creating a new agent. The best place to
    start is usually the straightforward architecture we discussed in [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341).
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)
    returns to the trade-off between reliability and agency, which is the key design
    decision when building LLM apps today. This is especially important when using
    the agent or multi-agent architectures, as their power comes at the expense of
    reliability if left unchecked. After diving deeper into why this trade-off exists,
    [Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)
    will cover the most important techniques at your disposal to navigate that decision,
    and ultimately improve your LLM applications and agents.'
  prefs: []
  type: TYPE_NORMAL
