- en: Chapter 1\. The Machine Learning Landscape
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. 机器学习景观
- en: 'Not so long ago, if you had picked up your phone and asked it to tell you the
    way home, it would have ignored you—and people would have questioned your sanity.
    But machine learning is no longer science fiction: billions of people use it every
    day. And the truth is it has actually been around for decades in some specialized
    applications, such as optical character recognition (OCR). The first ML application
    that really became mainstream, improving the lives of hundreds of millions of
    people, discretely took over the world back in the 1990s: the *spam filter*. It’s
    not exactly a self-aware robot, but it does technically qualify as machine learning:
    it has actually learned so well that you seldom need to flag an email as spam
    anymore. Then thanks to big data, hardware improvements, and a few algorithmic
    innovations, hundreds of ML applications followed and now quietly power hundreds
    of products and features that you use regularly: voice prompts, automatic translation,
    image search, product recommendations, and many more. And finally came ChatGPT,
    Gemini (formerly Bard), Claude, Perplexity, and many other chatbots: AI is no
    longer just powering services in the background, it *is* the service itself.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 不久以前，如果你拿起手机让它告诉你回家的路，它可能会忽略你——人们可能会质疑你的理智。但机器学习不再是科幻小说：数十亿人每天都在使用它。而且事实是，它在一些专业应用中已经存在了几十年，例如光学字符识别（OCR）。第一个真正成为主流的ML应用，改善了数亿人的生活，悄然在20世纪90年代接管了世界：*垃圾邮件过滤器*。它并不完全是一个有自我意识的机器人，但从技术上讲，它确实符合机器学习的定义：它已经学得很好，以至于你很少需要标记电子邮件为垃圾邮件。然后，得益于大数据、硬件改进和几个算法创新，数百个ML应用紧随其后，现在默默地支持着你日常使用的数百个产品和功能：语音提示、自动翻译、图像搜索、产品推荐等等。最后，ChatGPT、Gemini（原名Bard）、Claude、Perplexity以及许多其他聊天机器人出现：AI不再只是后台支持服务，它*本身就是服务本身*。
- en: Where does machine learning start and where does it end? What exactly does it
    mean for a machine to *learn* something? If I download a copy of all Wikipedia
    articles, has my computer really learned something? Is it suddenly smarter? In
    this chapter I will start by clarifying what machine learning is and why you may
    want to use it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从何开始，到何为止？机器学习到某事究竟意味着什么？如果我下载了所有维基百科文章的副本，我的电脑真的学到了什么？它突然变得更聪明了吗？在本章中，我将首先阐明机器学习是什么，以及为什么你可能想使用它。
- en: 'Then, before we set out to explore the machine learning continent, we will
    take a look at the map and learn about the main regions and the most notable landmarks:
    supervised versus unsupervised learning and their variants, online versus batch
    learning, instance-based versus model-based learning. Then we will look at the
    workflow of a typical ML project, discuss the main challenges you may face, and
    cover how to evaluate and fine-tune a machine learning system.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们开始探索机器学习大陆之前，我们将先看看地图，了解主要区域和最著名的地标：监督学习与无监督学习及其变体，在线学习与批量学习，基于实例的学习与基于模型的学习。然后我们将查看典型ML项目的流程，讨论你可能会面临的主要挑战，并介绍如何评估和微调机器学习系统。
- en: This chapter introduces a lot of fundamental concepts (and jargon) that every
    data scientist should know by heart. It will be a high-level overview (it’s the
    only chapter without much code), all rather simple, but my goal is to ensure everything
    is crystal clear to you before we continue on to the rest of the book. So grab
    a coffee and let’s get started!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了大量基础概念（以及术语）每个数据科学家都应该牢记于心。这将是一个高级概述（这是唯一一个没有太多代码的章节），所有内容都相当简单，但我的目标是确保在我们继续阅读本书的其余部分之前，一切对你来说都清晰明了。所以，拿杯咖啡，我们开始吧！
- en: Tip
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you are already familiar with machine learning basics, you may want to skip
    directly to [Chapter 2](ch02.html#project_chapter). If you are not sure, try to
    answer all the questions listed at the end of the chapter before moving on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉了机器学习的基础知识，你可能想直接跳到[第二章](ch02.html#project_chapter)。如果你不确定，在继续之前，尝试回答本章末尾列出的所有问题。
- en: What Is Machine Learning?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Machine learning is the science (and art) of programming computers so they can
    *learn from data*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是科学（以及艺术）的编程，使计算机能够*从数据中学习*。
- en: 'Here is a slightly more general definition:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个稍微更普遍的定义：
- en: '[Machine learning is the] field of study that gives computers the ability to
    learn without being explicitly programmed.'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[机器学习是]研究领域，赋予计算机在没有明确编程的情况下学习的能力。'
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Arthur Samuel, 1959
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亚瑟·萨缪尔，1959年
- en: 'And a more engineering-oriented one:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一个更偏向工程的角度：
- en: A computer program is said to learn from experience *E* with respect to some
    task *T* and some performance measure *P*, if its performance on *T*, as measured
    by *P*, improves with experience *E*.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果计算机程序在任务*T*上通过性能指标*P*从经验*E*中学习，那么它的性能会随着经验*E*的提高而提高。
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tom Mitchell, 1997
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 托马斯·米切尔，1997
- en: Your spam filter is a machine learning program that, given examples of spam
    emails (flagged by users) and examples of regular emails (nonspam, also called
    “ham”), can learn to flag spam. The examples that the system uses to learn are
    called the *training set*. Each training example is called a *training instance*
    (or *sample*). The part of a machine learning system that learns and makes predictions
    is called a *model*. Neural networks and random forests are examples of models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你的垃圾邮件过滤器是一个机器学习程序，它通过提供垃圾邮件示例（由用户标记）和常规邮件示例（非垃圾邮件，也称为“ham”），可以学习标记垃圾邮件。系统用来学习的示例称为*训练集*。每个训练示例称为*训练实例*（或*样本*）。机器学习系统中学习和做出预测的部分称为*模型*。神经网络和随机森林是模型的例子。
- en: In this case, the task *T* is to flag spam for new emails, the experience *E*
    is the *training data*, and the performance measure *P* needs to be defined; for
    example, you can use the ratio of correctly classified emails. This particular
    performance measure is called *accuracy*, and it is often used in classification
    tasks (we will discuss several others in [Chapter 3](ch03.html#classification_chapter)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，任务*T*是对新电子邮件标记垃圾邮件，经验*E*是*训练数据*，性能指标*P*需要定义；例如，你可以使用正确分类电子邮件的比率。这个特定的性能指标称为*准确率*，它通常用于分类任务（我们将在[第3章](ch03.html#classification_chapter)中讨论几个其他指标）。
- en: If you just download a copy of all Wikipedia articles, your computer has a lot
    more data, but it is not suddenly better at any task. This is not machine learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是下载所有维基百科文章的副本，你的电脑将有更多的数据，但它在任何任务上都不会突然变得更好。这并不是机器学习。
- en: Why Use Machine Learning?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用机器学习？
- en: 'Consider how you would write a spam filter using traditional programming techniques
    ([Figure 1-1](#traditional_approach_diagram)):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下你将如何使用传统编程技术编写垃圾邮件过滤器（[图1-1](#traditional_approach_diagram)）：
- en: First you would examine what spam typically looks like. You might notice that
    some words or phrases (such as “4U”, “credit card”, “free”, and “amazing”) tend
    to come up a lot in the subject line. Perhaps you would also notice a few other
    patterns in the sender’s name, the email’s body, and other parts of the email.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你会检查垃圾邮件通常看起来是什么样子。你可能会注意到一些单词或短语（如“4U”、“信用卡”、“免费”和“惊人”）在主题行中经常出现。也许你还会注意到发件人姓名、电子邮件正文和其他电子邮件部分的几个其他模式。
- en: You would write a detection algorithm for each of the patterns that you noticed,
    and your program would flag emails as spam if a number of these patterns were
    detected.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会为注意到的每个模式编写一个检测算法，如果你的程序检测到这些模式中的许多，就会将电子邮件标记为垃圾邮件。
- en: You would test your program and repeat steps 1 and 2 until it was good enough
    to launch.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会测试你的程序，并重复步骤1和2，直到它足够好可以发布。
- en: '![Diagram illustrating the traditional approach to programming a spam filter,
    highlighting steps: study the problem, write rules, evaluate, and analyze errors,
    with feedback loops and launch point.](assets/hmls_0101.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![说明传统编程方法编写垃圾邮件过滤器的图解，突出显示步骤：研究问题、编写规则、评估、分析错误，以及反馈循环和发布点。](assets/hmls_0101.png)'
- en: Figure 1-1\. The traditional approach
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 传统方法
- en: Since the problem is difficult, your program will likely become a long list
    of complex rules—pretty hard to maintain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题困难，你的程序可能变成一个复杂的规则长列表——很难维护。
- en: In contrast, a spam filter based on machine learning techniques automatically
    learns which words and phrases are good predictors of spam by detecting unusually
    frequent patterns of words in the spam examples compared to the ham examples ([Figure 1-2](#ml_approach_diagram)).
    The program is much shorter, easier to maintain, and most likely more accurate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，基于机器学习技术的垃圾邮件过滤器通过检测与正常邮件相比垃圾邮件示例中异常频繁的单词模式，自动学习哪些单词和短语是垃圾邮件的良好预测指标（[图1-2](#ml_approach_diagram)）。该程序更短，更容易维护，并且可能更准确。
- en: '![Diagram illustrating the machine learning approach to spam filtering, showing
    steps: study the problem, train ML model, evaluate, analyze errors, and launch
    if successful.](assets/hmls_0102.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![说明机器学习方法的垃圾邮件过滤图解，显示步骤：研究问题、训练ML模型、评估、分析错误，如果成功则发布。](assets/hmls_0102.png)'
- en: Figure 1-2\. The machine learning approach
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 机器学习方法
- en: What if spammers notice that all their emails containing “4U” are blocked? They
    might start writing “For U” instead. A spam filter using traditional programming
    techniques would need to be updated to flag “For U” emails. If spammers keep working
    around your spam filter, you will need to keep writing new rules forever.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果垃圾邮件发送者注意到所有包含“4U”的邮件都被拦截了？他们可能会开始写“For U”。使用传统编程技术的垃圾邮件过滤器需要更新以标记“For U”邮件。如果垃圾邮件发送者继续绕过你的垃圾邮件过滤器，你可能需要永远不断地编写新的规则。
- en: In contrast, a spam filter based on machine learning techniques automatically
    notices that “For U” has become unusually frequent in spam flagged by users, and
    it starts flagging them without your intervention ([Figure 1-3](#adapting_to_change_diagram)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于机器学习技术的垃圾邮件过滤器会自动注意到“For U”在用户标记的垃圾邮件中变得异常频繁，并开始标记它们，而无需你的干预（[图1-3](#adapting_to_change_diagram)）。
- en: '![Diagram showing a machine learning model''s iterative process: training,
    evaluating, launching, and updating data, illustrating its ability to adapt automatically.](assets/hmls_0103.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![展示机器学习模型迭代过程的图解：训练、评估、启动和更新数据，说明其自动适应的能力](assets/hmls_0103.png)'
- en: Figure 1-3\. Automatically adapting to change
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 自动适应变化
- en: Another area where machine learning shines is for problems that either are too
    complex for traditional approaches or have no known algorithm. For example, consider
    speech recognition. Say you want to start simple and write a program capable of
    distinguishing the words “one” and “two”. You might notice that the word “two”
    starts with a high-pitch sound (“T”), so you could hardcode an algorithm that
    measures high-pitch sound intensity and use that to distinguish ones and twos⁠—but
    obviously this technique will not scale to thousands of words spoken by millions
    of very different people in noisy environments and in dozens of languages. The
    best solution (at least today) is to write an algorithm that learns by itself,
    given many example recordings for each word.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个机器学习大放异彩的领域是那些对于传统方法来说过于复杂或者没有已知算法的问题。例如，考虑语音识别。假设你想从简单开始，编写一个能够区分“一”和“二”这两个词的程序。你可能注意到“二”这个词以高音调声音（“T”）开头，因此你可以硬编码一个算法来测量高音调声音强度，并使用这个强度来区分“一”和“二”——但显然这种技术无法扩展到成千上万由数百万不同的人在嘈杂环境中用数十种语言说出的单词。目前最好的解决方案（至少现在是这样）是编写一个能够通过大量每个单词的示例录音来自我学习的算法。
- en: Finally, machine learning can help humans learn ([Figure 1-4](#data_mining_diagram)).
    ML models can be inspected to see what they have learned (although for some models
    this can be tricky). For instance, once a spam filter has been trained on enough
    spam, it can easily be inspected to reveal the list of words and combinations
    of words that it believes are the best predictors of spam. Sometimes this will
    reveal unsuspected correlations or new trends, and thereby lead to a better understanding
    of the problem. Digging into large amounts of data to discover hidden patterns
    is called *data mining*, and machine learning excels at it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，机器学习可以帮助人类学习（[图1-4](#data_mining_diagram)）。可以通过检查机器学习模型来了解它们学到了什么（尽管对于某些模型来说这可能很棘手）。例如，一旦垃圾邮件过滤器在足够的垃圾邮件上进行了训练，就可以轻松检查以揭示它认为最好的垃圾邮件预测因素的单词列表和单词组合。有时这会揭示意想不到的相关性或新趋势，从而更好地理解问题。挖掘大量数据以发现隐藏模式被称为*数据挖掘*，而机器学习在这方面表现卓越。
- en: '![Diagram illustrating the process of using machine learning to solve problems,
    highlighting steps such as studying the problem, training models, inspecting solutions,
    and gaining better understanding through data mining.](assets/hmls_0104.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用机器学习解决问题过程的图解，突出研究问题、训练模型、检查解决方案和通过数据挖掘获得更好理解等步骤](assets/hmls_0104.png)'
- en: Figure 1-4\. Machine learning can help humans learn
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 机器学习可以帮助人类学习
- en: 'To summarize, machine learning is great for:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，机器学习非常适合：
- en: Problems for which existing solutions require a lot of work and maintenance,
    such as long lists of rules (a machine learning model can often simplify code
    and perform better than the traditional approach)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于现有解决方案需要大量工作和维护的问题，例如长列表的规则（机器学习模型通常可以简化代码并比传统方法表现更好）
- en: Complex problems for which using a traditional approach yields no good solution
    (the best machine learning techniques can perhaps find a solution)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于使用传统方法无法得到良好解决方案的复杂问题（最好的机器学习技术可能找到解决方案）
- en: Fluctuating environments (a machine learning system can easily be retrained
    on new data, always keeping it up to date)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波动环境（机器学习系统可以很容易地在新的数据上重新训练，始终保持最新）
- en: Getting insights about complex problems and large amounts of data
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取关于复杂问题和大量数据的见解
- en: Examples of Applications
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用示例
- en: 'Let’s look at some concrete examples of machine learning tasks, along with
    the techniques that can tackle them:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些具体的机器学习任务示例，以及可以解决它们的技巧：
- en: Analyzing images of products on a production line to automatically classify
    them
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分析生产线上的产品图像以自动对其进行分类
- en: This is image classification, typically performed using convolutional neural
    networks (CNNs; see [Chapter 12](ch12.html#cnn_chapter)) or vision transformers
    (see [Chapter 16](ch16.html#vit_chapter)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图像分类，通常使用卷积神经网络（CNNs；见第12章[cnn_chapter](ch12.html#cnn_chapter)）或视觉转换器（见第16章[vit_chapter](ch16.html#vit_chapter)）执行。
- en: Detecting tumors in brain scans
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在脑部扫描中检测肿瘤
- en: This is semantic image segmentation, where each pixel in the image is classified
    (as we want to determine the exact location and shape of tumors), typically using
    CNNs or vision transformers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语义图像分割，其中图像中的每个像素都被分类（正如我们想要确定肿瘤的确切位置和形状），通常使用CNN或视觉转换器。
- en: Automatically classifying news articles
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自动分类新闻文章
- en: This is natural language processing (NLP), and more specifically text classification,
    which can be tackled using recurrent neural networks (RNNs) and CNNs, but transformers
    work even better (see [Chapter 15](ch15.html#transformer_chapter)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言处理（NLP），更具体地说，是文本分类，可以使用循环神经网络（RNNs）和CNNs来解决，但转换器效果更好（见第15章[transformer_chapter](ch15.html#transformer_chapter)）。
- en: Automatically flagging offensive comments on discussion forums
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 自动标记讨论论坛上的冒犯性评论
- en: This is also text classification, using the same NLP tools.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是文本分类，使用相同的NLP工具。
- en: Summarizing long documents automatically
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自动总结长文档
- en: This is a branch of NLP called text summarization, again using the same tools.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种名为文本摘要的NLP分支，再次使用相同的工具。
- en: Estimating a person’s genetic risk for a given disease by analyzing a very long
    DNA sequence
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析非常长的DNA序列来估计一个人对特定疾病的遗传风险
- en: Such a task requires discovering spread out patterns across very long sequences,
    which is where state space models (SSMs) particularly shine (see “State-Space
    Models (SSMs)” at [*https://homl.info*](https://homl.info)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的任务需要发现非常长序列中的分散模式，这正是状态空间模型（SSMs）特别擅长的领域（见[*https://homl.info*](https://homl.info)上的“状态空间模型（SSMs）”）。
- en: Creating a chatbot or a personal assistant
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 创建聊天机器人或个人助理
- en: This involves many NLP components, including natural language understanding
    (NLU) and question-answering modules.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到许多NLP组件，包括自然语言理解（NLU）和问答模块。
- en: Forecasting your company’s revenue next year, based on many performance metrics
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 根据许多性能指标预测您公司明年的收入
- en: This is a regression task (i.e., predicting values) that may be tackled using
    any regression model, such as a linear regression or polynomial regression model
    (see [Chapter 4](ch04.html#linear_models_chapter)), a regression support vector
    machine (see the online appendix on SVMs at [*https://homl.info*](https://homl.info)),
    a regression random forest (see [Chapter 6](ch06.html#ensembles_chapter)), or
    an artificial neural network (see [Chapter 9](ch09.html#ann_chapter)). If you
    want to take into account sequences of past performance metrics, you may want
    to use RNNs, CNNs, or transformers (see Chapters [13](ch13.html#rnn_chapter) to
    [15](ch15.html#transformer_chapter)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个回归任务（即预测值），可以使用任何回归模型来解决，例如线性回归或多项式回归模型（见第4章[linear_models_chapter](ch04.html#linear_models_chapter)），回归支持向量机（见[*https://homl.info*](https://homl.info)上的在线附录SVMs），回归随机森林（见第6章[ensembles_chapter](ch06.html#ensembles_chapter)），或人工神经网络（见第9章[ann_chapter](ch09.html#ann_chapter)）。如果您想考虑过去性能指标的时间序列，您可能希望使用RNNs，CNNs或转换器（见第13章[ch13.html#rnn_chapter]到第15章[ch15.html#transformer_chapter]）。
- en: Making your app react to voice commands
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使您的应用能够响应语音命令
- en: This is speech recognition, which requires processing audio samples. Since they
    are long and complex sequences, they are typically processed using RNNs, CNNs,
    or transformers (see Chapters [13](ch13.html#rnn_chapter) to [15](ch15.html#transformer_chapter)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语音识别，需要处理音频样本。由于它们是长而复杂的序列，通常使用RNNs，CNNs或转换器（见第13章[ch13.html#rnn_chapter]到第15章[ch15.html#transformer_chapter]）进行处理。
- en: Detecting credit card fraud
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 检测信用卡欺诈
- en: This is anomaly detection, which can be tackled using isolation forests, Gaussian
    mixture models (see [Chapter 8](ch08.html#unsupervised_learning_chapter)), or
    autoencoders (see [Chapter 18](ch18.html#autoencoders_chapter)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是异常检测，可以使用隔离森林、高斯混合模型（参见[第8章](ch08.html#unsupervised_learning_chapter)）或自编码器（参见[第18章](ch18.html#autoencoders_chapter)）来解决。
- en: Segmenting clients based on their purchases so that you can design a different
    marketing strategy for each segment
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据客户的购买行为进行客户细分，以便你可以为每个细分设计不同的营销策略
- en: This is clustering, which can be achieved using *k*-means, DBSCAN, and more
    (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是聚类，可以使用*k*-means、DBSCAN等方法实现（参见[第8章](ch08.html#unsupervised_learning_chapter)）。
- en: Representing a complex, high-dimensional dataset in a clear and insightful diagram
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以清晰和有洞察力的图表来表示复杂、高维的数据集
- en: This is data visualization, often involving dimensionality reduction techniques
    (see [Chapter 7](ch07.html#dimensionality_chapter)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据可视化，通常涉及降维技术（参见[第7章](ch07.html#dimensionality_chapter)）。
- en: Recommending a product that a client may be interested in, based on past purchases
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据客户的过去购买推荐可能感兴趣的产品
- en: This is a recommender system. One approach is to feed past purchases (and other
    information about the client) to an artificial neural network (see [Chapter 9](ch09.html#ann_chapter)),
    and get it to output the most likely next purchase. This neural net would typically
    be trained on past sequences of purchases across all clients.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个推荐系统。一种方法是将过去的购买记录（以及关于客户的其它信息）输入到一个人工神经网络（参见[第9章](ch09.html#ann_chapter)），并让它输出最可能的下一个购买。这个神经网络通常会在所有客户的过去购买序列上进行训练。
- en: Building an intelligent bot for a game
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为游戏构建一个智能机器人
- en: This is often tackled using reinforcement learning (RL; see [Chapter 19](ch19.html#rl_chapter)),
    which is a branch of machine learning that trains agents (such as bots) to pick
    the actions that will maximize their rewards over time (e.g., a bot may get a
    reward every time the player loses some life points), within a given environment
    (such as the game). The famous AlphaGo program that beat the world champion at
    the game of Go was built using RL.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过强化学习（RL；参见[第19章](ch19.html#rl_chapter)）来解决的，强化学习是机器学习的一个分支，它训练代理（如机器人）选择那些能够在给定环境中（如游戏）最大化其奖励的行动（例如，机器人每次玩家失去一些生命值时都会获得奖励），这就像科学家在训练数据中检测模式并构建预测模型时所做的那样。著名的AlphaGo程序就是使用强化学习来击败围棋世界冠军的。
- en: This list could go on and on, but hopefully it gives you a sense of the incredible
    breadth and complexity of the tasks that machine learning can tackle, and the
    types of techniques that you would use for each task.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表可以一直继续下去，但希望这能给你一个关于机器学习可以解决的任务的广阔范围和复杂性的感觉，以及你将用于每个任务的技巧类型。
- en: Types of Machine Learning Systems
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统的类型
- en: 'There are so many different types of machine learning systems that it is useful
    to classify them in broad categories, based on the following criteria:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习系统类型繁多，根据以下标准对它们进行广泛分类是有用的：
- en: How they are guided during training (supervised, unsupervised, semi-supervised,
    self-supervised, and others)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在训练过程中的指导方式（监督学习、无监督学习、半监督学习、自监督学习以及其他）
- en: Whether or not they can learn incrementally on the fly (online versus batch
    learning)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是否能够即时增量学习（在线学习与批量学习）
- en: Whether they work by simply comparing new data points to known data points,
    or instead by detecting patterns in the training data and building a predictive
    model, much like scientists do (instance-based versus model-based learning)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是否通过简单地比较新数据点与已知数据点来工作，或者通过在训练数据中检测模式并构建预测模型来工作，就像科学家所做的那样（基于实例的学习与基于模型的学习）
- en: These criteria are not exclusive; you can combine them in any way you like.
    For example, a state-of-the-art spam filter may learn on the fly using a deep
    neural network model trained using human-provided examples of spam and ham; this
    makes it an online, model-based, supervised learning system.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标准不是互斥的；你可以以任何你喜欢的方式将它们结合起来。例如，一个最先进的垃圾邮件过滤器可能会使用深度神经网络模型即时学习，该模型使用人类提供的垃圾邮件和正常邮件的示例进行训练；这使得它成为一个在线的、基于模型的、监督学习系统。
- en: Let’s look at each of these criteria a bit more closely.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些标准。
- en: Training Supervision
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练监督
- en: 'ML systems can be classified according to the amount and type of supervision
    they get during training. There are many categories, but we’ll discuss the main
    ones: supervised learning, unsupervised learning, self-supervised learning, semi-supervised
    learning, and reinforcement learning.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据在训练期间获得的监督量及其类型，可以将机器学习系统进行分类。有许多类别，但我们将讨论主要类别：监督学习、无监督学习、自监督学习、半监督学习和强化学习。
- en: Supervised learning
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: In *supervised learning*, the training set you feed to the algorithm includes
    the desired solutions, called *labels* ([Figure 1-5](#supervised_learning_diagram)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *监督学习* 中，你提供给算法的训练集包括期望的解决方案，称为 *标签* ([图 1-5](#supervised_learning_diagram))。
- en: '![Diagram illustrating a labeled training set for spam classification, where
    emails are marked as spam or not spam, and this classification is applied to new
    emails.](assets/hmls_0105.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![说明用于垃圾邮件分类的标记训练集的图，其中电子邮件被标记为垃圾邮件或非垃圾邮件，并将此分类应用于新电子邮件。](assets/hmls_0105.png)'
- en: Figure 1-5\. A labeled training set for spam classification (an example of supervised
    learning)
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 用于垃圾邮件分类的标记训练集（监督学习的一个示例）
- en: 'A typical supervised learning task is *classification*. The spam filter is
    a good example of this: it is trained with many example emails along with their
    *class* (spam or ham), and it must learn how to classify new emails.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的监督学习任务是 *分类*。垃圾邮件过滤器是此类任务的一个好例子：它使用许多带有其 *类别*（垃圾邮件或正常邮件）的示例电子邮件进行训练，并且必须学习如何对新电子邮件进行分类。
- en: Another typical task is to predict a *target* numeric value, such as the price
    of a car, given a set of *features* (mileage, age, brand, etc.). This sort of
    task is called *regression* ([Figure 1-6](#regression_diagram)).⁠^([1](ch01.html#id789))
    To train the system, you need to give it many examples of cars, including both
    their features and their targets (i.e., their prices).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个典型的任务是根据一组 *特征*（如里程、年龄、品牌等）预测一个 *目标* 数值，例如汽车的价格。这类任务被称为 *回归* ([图 1-6](#regression_diagram))。⁠^([1](ch01.html#id789))
    为了训练系统，你需要提供许多汽车的例子，包括它们的特征和目标（即价格）。
- en: Note that some regression models can be used for classification as well, and
    vice versa. For example, *logistic regression* is commonly used for classification,
    as it can output a value that corresponds to the probability of belonging to a
    given class (e.g., 20% chance of being spam).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一些回归模型也可以用于分类，反之亦然。例如，*逻辑回归* 常用于分类，因为它可以输出一个与属于给定类别的概率相对应的值（例如，有 20% 的可能性是垃圾邮件）。
- en: '![Scatter plot illustrating a regression problem, showing a new instance on
    the x-axis labeled "Feature 1" with a question mark on determining its corresponding
    value on the y-axis.](assets/hmls_0106.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![散点图说明回归问题，显示 x 轴上的一个新实例，标记为 "特征 1"，并在 y 轴上有一个问号以确定其对应的值。](assets/hmls_0106.png)'
- en: 'Figure 1-6\. A regression problem: predict a value, given an input feature
    (there are usually multiple input features, and sometimes multiple output values)'
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. 回归问题：给定输入特征（通常有多个输入特征，有时还有多个输出值）预测一个值
- en: Note
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: The words *target* and *label* are generally treated as synonyms in supervised
    learning, but *target* is more common in regression tasks and *label* is more
    common in classification tasks. Moreover, *features* are sometimes called *predictors*
    or *attributes*. These terms may refer to individual samples (e.g., “this car’s
    mileage feature is equal to 15,000”) or to all samples (e.g., “the mileage feature
    is strongly correlated with price”).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，*目标* 和 *标签* 通常被视为同义词，但 *目标* 在回归任务中更常见，而 *标签* 在分类任务中更常见。此外，*特征* 有时也被称为
    *预测器* 或 *属性*。这些术语可能指单个样本（例如，“这辆车的里程特征等于 15,000”）或所有样本（例如，“里程特征与价格高度相关”）。
- en: Unsupervised learning
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In *unsupervised learning*, as you might guess, the training data is unlabeled.
    The system tries to learn without a teacher.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *无监督学习* 中，正如你可能猜到的，训练数据是无标签的。系统试图在没有教师的情况下学习。
- en: 'For example, say you have a lot of data about your blog’s visitors. You may
    want to run a *clustering* algorithm to try to detect groups of similar visitors
    ([Figure 1-7](#clustering_diagram)). The features may include the user’s age group,
    their region, their interests, the duration of their sessions, and so on. At no
    point do you tell the algorithm which group a visitor belongs to: it finds those
    connections without your help. For example, it might notice that 40% of your visitors
    are teenagers who love comic books and generally read your blog after school,
    while 20% are adults who enjoy sci-fi and who visit during the weekends. If you
    use a *hierarchical clustering* algorithm, it may also subdivide each group into
    smaller groups. This may help you target your posts for each group.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有很多关于你的博客访客的数据。你可能想运行一个*聚类*算法来尝试检测相似访客的组（[图1-7](#clustering_diagram)）。特征可能包括用户的年龄组、他们的地区、他们的兴趣、会话的持续时间等等。在任何时候，你都没有告诉算法访客属于哪个组：它在不依赖你的帮助的情况下找到这些联系。例如，它可能会注意到40%的访客是喜欢漫画书的青少年，他们通常在放学后阅读你的博客，而20%是喜欢科幻的成年人，他们在周末访问。如果你使用*层次聚类*算法，它也可能将每个组进一步细分为更小的组。这可能有助于你针对每个组的目标帖子。
- en: '![A diagram illustrating clustering, showing groups of similar data points
    represented by icons of people, separated by dashed lines along two features.](assets/hmls_0107.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![展示聚类过程的图解，用人物图标表示相似数据点，沿两个特征用虚线分隔](assets/hmls_0107.png)'
- en: Figure 1-7\. Clustering
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7\. 聚类
- en: '*Visualization* algorithms are also good examples of unsupervised learning:
    you feed them a lot of complex and unlabeled data, and they output a 2D or 3D
    representation of your data that can easily be plotted ([Figure 1-8](#socher_ganjoo_manning_ng_2013_paper)).
    These algorithms try to preserve as much structure as they can (e.g., trying to
    keep separate clusters in the input space from overlapping in the visualization)
    so that you can understand how the data is organized and perhaps identify unsuspected
    patterns.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*可视化*算法也是无监督学习的良好例子：你向它们提供大量复杂且未标记的数据，它们会输出数据的2D或3D表示，这些表示可以轻松绘制（[图1-8](#socher_ganjoo_manning_ng_2013_paper)）。这些算法试图尽可能多地保留结构（例如，尝试在输入空间中保持不同的簇在可视化中不重叠），这样你就可以理解数据的组织方式，也许可以识别出未预料到的模式。'
- en: A related task is *dimensionality reduction*, in which the goal is to simplify
    the data without losing too much information. One way to do this is to merge several
    correlated features into one. For example, a car’s mileage may be strongly correlated
    with its age, so the dimensionality reduction algorithm will merge them into one
    feature that represents the car’s wear and tear. This is called *feature extraction*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的任务是*降维*，其目标是简化数据而不丢失太多信息。完成此任务的一种方法是将几个相关特征合并为一个。例如，一辆车的里程数可能与它的年龄高度相关，因此降维算法将它们合并为一个特征，该特征代表车辆的磨损情况。这被称为*特征提取*。
- en: Tip
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is often a good idea to try to reduce the number of dimensions in your training
    data using a dimensionality reduction algorithm before you feed it to another
    machine learning algorithm (such as a supervised learning algorithm). It will
    run much faster, the data will take up less disk and memory space, and in some
    cases it may also perform better.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在将训练数据输入另一个机器学习算法（如监督学习算法）之前，尝试使用降维算法减少训练数据中的维度通常是一个好主意。这将运行得更快，数据将占用更少的磁盘和内存空间，在某些情况下，它也可能表现得更好。
- en: '![A t-SNE visualization showing semantic clusters of various categories such
    as animals and vehicles, highlighting how different groups are generally well-separated.](assets/hmls_0108.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![t-SNE可视化展示了各种类别（如动物和车辆）的语义簇，突出了不同组通常很好地分隔开来的情况](assets/hmls_0108.png)'
- en: Figure 1-8\. Example of a t-SNE visualization highlighting semantic clusters⁠^([2](ch01.html#id802))
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. t-SNE可视化示例，突出显示语义簇⁠^([2](ch01.html#id802))
- en: 'Yet another important unsupervised task is *anomaly detection*—for example,
    detecting unusual credit card transactions to prevent fraud, catching manufacturing
    defects, or automatically removing outliers from a dataset before feeding it to
    another learning algorithm. The system is shown mostly normal instances during
    training, so it learns to recognize them; then, when it sees a new instance, it
    can tell whether it looks like a normal one or whether it is likely an anomaly
    (see [Figure 1-9](#anomaly_detection_diagram)). The features may include distance
    from home, time of day, day of the week, amount withdrawn, merchant category,
    transaction frequency, etc. A very similar task is *novelty detection*: it aims
    to detect new instances that look different from all instances in the training
    set. This requires having a very “clean” training set, devoid of any instance
    that you would like the algorithm to detect. For example, if you have thousands
    of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a novelty
    detection algorithm should not treat new pictures of Chihuahuas as novelties.
    On the other hand, anomaly detection algorithms may consider these dogs as so
    rare and so different from other dogs that they would likely classify them as
    anomalies (no offense to Chihuahuas).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的无监督任务是*异常检测*——例如，检测异常的信用卡交易以防止欺诈，捕捉制造缺陷，或在将数据输入另一个学习算法之前自动从数据集中移除异常值。系统在训练期间主要显示正常实例，因此它学会了识别它们；然后，当它看到一个新的实例时，它可以判断它看起来是否像正常的一个，或者它是否可能是一个异常（参见[图1-9](#anomaly_detection_diagram)）。一个非常类似的任务是*新颖性检测*：它的目标是检测出与训练集中所有实例都不同的新实例。这要求训练集非常“干净”，没有任何你希望算法检测的实例。例如，如果你有成千上万只狗的图片，其中1%的图片代表吉娃娃，那么新颖性检测算法不应将新的吉娃娃图片视为新颖性。另一方面，异常检测算法可能会将这些狗视为非常罕见且与其他狗不同，因此它们可能会将它们分类为异常（对吉娃娃没有冒犯之意）。
- en: '![Diagram illustrating anomaly detection with normal and anomaly instances
    plotted in a feature space, showing how new instances are classified.](assets/hmls_0109.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![一个在特征空间中绘制正常和异常实例的异常检测示意图，展示了如何对新的实例进行分类。](assets/hmls_0109.png)'
- en: Figure 1-9\. Anomaly detection
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 异常检测
- en: Finally, another common unsupervised task is *association rule learning*, in
    which the goal is to dig into large amounts of data and discover interesting relations
    between attributes. For example, suppose you own a supermarket. Running an association
    rule on your sales logs may reveal that people who purchase barbecue sauce and
    potato chips also tend to buy steak. Thus, you may want to place these items close
    to one another.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个常见的无监督任务是*关联规则学习*，其目标是深入大量数据，发现属性之间的有趣关系。例如，假设你拥有一家超市。对你的销售日志运行关联规则可能会揭示出购买烧烤酱和薯片的人也倾向于购买牛排。因此，你可能想要将这些商品放置在彼此附近。
- en: Semi-supervised learning
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Since labeling data is usually time-consuming and costly, you will often have
    plenty of unlabeled instances, and few labeled instances. Some algorithms can
    deal with data that’s partially labeled. This is called *semi-supervised learning*
    ([Figure 1-10](#semi_supervised_learning_diagram)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标注数据通常耗时且成本高昂，你通常会拥有大量的未标注实例，而标注实例却很少。一些算法可以处理部分标注的数据。这被称为*半监督学习*([图1-10](#semi_supervised_learning_diagram))。
- en: '![A diagram illustrating semi-supervised learning with triangles and squares
    as labeled classes, and circles as unlabeled examples, showing how a new instance
    (cross) is more likely classified as a triangle through the influence of nearby
    unlabeled data.](assets/hmls_0110.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![一个用三角形和正方形作为标注类，圆圈作为未标注示例的半监督学习示意图，展示了如何通过附近未标注数据的影响，将新的实例（交叉）更有可能分类为三角形。](assets/hmls_0110.png)'
- en: 'Figure 1-10\. Semi-supervised learning with two classes (triangles and squares):
    the unlabeled examples (circles) help classify a new instance (the cross) into
    the triangle class rather than the square class, even though it is closer to the
    labeled squares'
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10\. 具有两个类别（三角形和正方形）的半监督学习：未标注的示例（圆圈）帮助将新的实例（交叉）分类为三角形类别，而不是正方形类别，尽管它更接近标注的正方形。
- en: Some photo-hosting services, such as Google Photos, are good examples of this.
    Once you upload all your family photos to the service, it automatically recognizes
    that the same person A shows up in photos 1, 5, and 11, while another person B
    shows up in photos 2, 5, and 7\. This is the unsupervised part of the algorithm
    (clustering). Now all the system needs is for you to tell it who these people
    are. Just add one label per person⁠^([3](ch01.html#id812)) and it is able to name
    everyone in every photo, which is useful for searching photos.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些照片托管服务，如Google Photos，是这方面的好例子。一旦你将所有家庭照片上传到该服务，它就会自动识别出同一个人A出现在照片1、5和11中，而另一个人B出现在照片2、5和7中。这是算法的无监督部分（聚类）。现在系统需要的只是你告诉它这些人的身份。只需为每个人添加一个标签⁠^([3](ch01.html#id812))，它就能命名每张照片中的每个人，这对于搜索照片非常有用。
- en: Most semi-supervised learning algorithms are combinations of unsupervised and
    supervised algorithms. For example, a clustering algorithm may be used to group
    similar instances together, and then every unlabeled instance can be labeled with
    the most common label in its cluster. Once the whole dataset is labeled, it is
    possible to use any supervised learning algorithm.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数半监督学习算法是无监督和监督算法的组合。例如，可以使用聚类算法将相似实例分组在一起，然后每个未标记的实例都可以用其簇中最常见的标签进行标记。一旦整个数据集被标记，就可以使用任何监督学习算法。
- en: Self-supervised learning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自监督学习
- en: Another approach to machine learning involves actually generating a fully labeled
    dataset from a fully unlabeled one. Again, once the whole dataset is labeled,
    any supervised learning algorithm can be used. This approach is called *self-supervised
    learning*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种机器学习方法涉及从完全未标记的数据集中生成一个完全标记的数据集。同样，一旦整个数据集被标记，任何监督学习算法都可以使用。这种方法被称为*自监督学习*。
- en: For example, if you have a large dataset of unlabeled images, you can randomly
    mask a small part of each image and then train a model to recover the original
    image ([Figure 1-11](#self_supervised_learning_diagram)). During training, the
    masked images are used as the inputs to the model, and the original images are
    used as the labels.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个大量未标记图像的数据集，你可以随机遮挡每张图像的一小部分，然后训练一个模型来恢复原始图像（[图1-11](#self_supervised_learning_diagram)）。在训练过程中，遮挡图像被用作模型的输入，原始图像被用作标签。
- en: '![Diagram illustrating self-supervised learning with a partially masked kitten
    image on the left as input and the complete image as the target on the right.](assets/hmls_0111.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用部分遮挡的小猫图像作为输入，完整图像作为目标的自监督学习图的示意图](assets/hmls_0111.png)'
- en: 'Figure 1-11\. Self-supervised learning example: input (left) and target (right)'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-11\. 自监督学习示例：输入（左）和目标（右）
- en: The resulting model may be quite useful in itself—for example, to repair damaged
    images or to erase unwanted objects from pictures. But more often than not, a
    model trained using self-supervised learning is not the final goal. You’ll usually
    want to tweak and fine-tune the model for a slightly different task—one that you
    actually care about.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型本身可能非常有用——例如，修复损坏的图像或从图片中擦除不需要的对象。但通常情况下，使用自监督学习训练的模型并不是最终目标。你通常会想要调整和微调模型以适应不同的任务——一个你真正关心的任务。
- en: 'For example, suppose that what you really want is to have a pet classification
    model: given a picture of any pet, it will tell you what species it belongs to.
    If you have a large dataset of unlabeled photos of pets, you can start by training
    an image-repairing model using self-supervised learning. Once it’s performing
    well, it should be able to distinguish different pet species: when it repairs
    an image of a cat whose face is masked, it must know not to add a dog’s face.
    Assuming your model’s architecture allows it (and most neural network architectures
    do), it is then possible to tweak the model so that it predicts pet species instead
    of repairing images. The final step consists of fine-tuning the model on a labeled
    dataset: the model already knows what cats, dogs, and other pet species look like,
    so this step is only needed so the model can learn the mapping between the species
    it already knows and the labels we expect from it.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你真正想要的是一个宠物分类模型：给定任何宠物的图片，它将告诉你它属于哪个物种。如果你有一大批未标记的宠物照片数据集，你可以先通过自监督学习训练一个图像修复模型。一旦它表现良好，它应该能够区分不同的宠物物种：当它修复一张被遮蔽脸部的猫的图像时，它必须知道不要添加狗的脸。假设你的模型架构允许这样做（大多数神经网络架构都是这样），那么你可以调整模型，使其预测宠物物种而不是修复图像。最后一步是在标记的数据集上微调模型：模型已经知道猫、狗和其他宠物物种看起来是什么样子，所以这一步只需要让模型学习它已知的物种和我们所期望的标签之间的映射。
- en: Note
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Transferring knowledge from one task to another is called *transfer learning*,
    and it’s one of the most important techniques in machine learning today, especially
    when using *deep neural networks* (i.e., neural networks composed of many layers
    of neurons). We will discuss this in detail in [Part II](part02.html#neural_nets_part).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将知识从一个任务转移到另一个任务称为*迁移学习*，它是今天机器学习中最重要的一项技术，尤其是在使用*深度神经网络*（即由许多神经元层组成的神经网络）时。我们将在[第二部分](part02.html#neural_nets_part)中详细讨论这一点。
- en: As we will see in [Chapter 15](ch15.html#transformer_chapter), large language
    models (LLMs) are trained in a very similar way, by masking random words in a
    huge text corpus and training the model to predict the missing words. This large
    pretrained model can then be fine-tuned for various applications, from sentiment
    analysis to chatbots.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第15章](ch15.html#transformer_chapter)中将会看到的，大型语言模型（LLMs）是以非常相似的方式进行训练的，即在巨大的文本语料库中随机遮蔽一些单词，并训练模型预测缺失的单词。这个大型预训练模型随后可以被微调以适应各种应用，从情感分析到聊天机器人。
- en: 'Some people consider self-supervised learning to be a part of unsupervised
    learning, since it deals with fully unlabeled datasets. But self-supervised learning
    uses (generated) labels during training, so in that regard it’s closer to supervised
    learning. And the term “unsupervised learning” is generally used when dealing
    with tasks like clustering, dimensionality reduction, or anomaly detection, whereas
    self-supervised learning focuses on the same tasks as supervised learning: mainly
    classification and regression. In short, it’s best to treat self-supervised learning
    as its own category.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为自监督学习是未监督学习的一部分，因为它处理的是完全未标记的数据集。但自监督学习在训练过程中使用了（生成的）标签，所以在某种程度上它更接近监督学习。而“未监督学习”这个术语通常用于处理诸如聚类、降维或异常检测等任务，而自监督学习则专注于与监督学习相同的目标：主要是分类和回归。简而言之，最好将自监督学习视为一个独立的类别。
- en: Reinforcement learning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习
- en: '*Reinforcement learning* is a very different beast. The learning system, called
    an *agent* in this context, can observe the environment, select and perform actions,
    and get *rewards* in return (or *penalties* in the form of negative rewards, as
    shown in [Figure 1-12](#reinforcement_learning_diagram)). It must then learn by
    itself what is the best strategy, called a *policy*, to get the most reward over
    time. A policy defines what action the agent should choose when it is in a given
    situation.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是一个完全不同的概念。在这个背景下，学习系统被称为*智能体*，它可以观察环境，选择并执行动作，并获得*奖励*（或者以负奖励形式的*惩罚*，如图1-12所示）。然后它必须自己学习最佳策略，即*策略*，以在一段时间内获得最多的奖励。策略定义了智能体在给定情况下应该选择什么动作。'
- en: '![Diagram illustrating reinforcement learning, where an agent interacts with
    its environment, selects actions, and receives rewards or penalties, leading to
    policy updates and learning.](assets/hmls_0112.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![说明强化学习的图，其中智能体与其环境交互，选择动作，并接收奖励或惩罚，从而更新策略并学习。](assets/hmls_0112.png)'
- en: Figure 1-12\. Reinforcement learning
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-12\. 强化学习
- en: 'For example, many robots implement reinforcement learning algorithms to learn
    how to walk. DeepMind’s AlphaGo program is also a good example of reinforcement
    learning: it made the headlines in May 2017 when it beat Ke Jie, the number one
    ranked player in the world at the time, at the game of Go. It learned its winning
    policy by analyzing millions of games, and then playing many games against itself.
    Note that learning was turned off during the games against the champion; AlphaGo
    was just applying the policy it had learned. As you will see in the next section,
    this is called *offline learning*.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，许多机器人实现强化学习算法来学习如何行走。DeepMind的AlphaGo程序也是强化学习的良好例子：它在2017年5月成为头条新闻，当时它击败了当时世界排名第一的棋手柯洁。它通过分析数百万场比赛并与之进行多轮比赛来学习其获胜策略。请注意，在对冠军的比赛期间，学习被关闭；AlphaGo只是在应用它所学的策略。正如你将在下一节中看到的，这被称为*离线学习*。
- en: Batch Versus Online Learning
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量学习与在线学习
- en: Another criterion used to classify machine learning systems is whether the system
    can learn incrementally from a stream of incoming data. For example, random forests
    (see [Chapter 6](ch06.html#ensembles_chapter)) can only be trained from scratch
    on the full dataset—this is called batch learning—while other models can be trained
    one batch of data at a time, for example, using *gradient descent* (see [Chapter 4](ch04.html#linear_models_chapter))—this
    is called online learning.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对机器学习系统进行分类的另一个标准是系统是否可以从传入的数据流中增量学习。例如，随机森林（见[第6章](ch06.html#ensembles_chapter)）只能从头开始使用完整的数据集进行训练——这被称为批量学习——而其他模型可以一次训练一批数据，例如，使用*梯度下降*（见[第4章](ch04.html#linear_models_chapter)）——这被称为在线学习。
- en: Batch learning
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量学习
- en: In *batch learning*, the system must be trained using all the available data.
    This will generally take a lot of time and computing resources, so it is typically
    done offline. First the system is trained, and then it is launched into production
    and runs without learning anymore; it just applies what it has learned. This is
    called *offline learning*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在*批量学习*中，系统必须使用所有可用数据进行训练。这通常需要大量的时间和计算资源，因此通常是在离线状态下进行的。首先训练系统，然后将其投入生产并运行，不再进行学习；它只是应用它所学的。这被称为*离线学习*。
- en: 'Unfortunately, a model’s performance tends to decay slowly over time, simply
    because the world continues to evolve while the model remains unchanged. This
    phenomenon is often called *data drift* (or *model rot*). The solution is to regularly
    retrain the model on up-to-date data. How often you need to do that depends on
    the use case: if the model classifies pictures of cats and dogs, its performance
    will decay very slowly, but if the model deals with fast-evolving systems, for
    example making predictions on the financial market, then it is likely to decay
    quite fast.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，模型的表现往往会随着时间的推移而缓慢下降，仅仅是因为世界在继续演变，而模型保持不变。这种现象通常被称为*数据漂移*（或*模型退化*）。解决方案是定期使用最新的数据重新训练模型。你需要多久做一次取决于用例：如果模型分类猫和狗的图片，其性能会非常缓慢地下降，但如果模型处理快速演变的系统，例如对金融市场进行预测，那么它很可能会迅速下降。
- en: Warning
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Even a model trained to classify pictures of cats and dogs may need to be retrained
    regularly, not because cats and dogs will mutate overnight, but because cameras
    keep changing, along with image formats, sharpness, brightness, and size ratios.
    Moreover, people may love different breeds next year, or they may decide to dress
    their pets with tiny hats—who knows?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是训练用于分类猫和狗图片的模型，也可能需要定期重新训练，这并不是因为猫和狗会一夜之间发生变异，而是因为相机不断变化，包括图像格式、清晰度、亮度和尺寸比。此外，人们可能明年会喜欢不同的品种，或者他们可能会决定给他们的宠物戴上小小的帽子——谁知道呢？
- en: If you want a batch learning system to know about new data (such as a new type
    of spam), you need to train a new version of the system from scratch on the full
    dataset (not just the new data, but also the old data), then replace the old model
    with the new one. Fortunately, the whole process of training, evaluating, and
    launching a machine learning system can be automated (as we saw in [Figure 1-3](#adapting_to_change_diagram)),
    so even a batch learning system can adapt to change. Simply update the data and
    train a new version of the system from scratch as often as needed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望批量学习系统了解新的数据（例如新的垃圾邮件类型），你需要从头开始使用完整的数据集（不仅仅是新数据，还包括旧数据）训练系统的新版本，然后替换旧模型。幸运的是，训练、评估和部署机器学习系统的整个过程可以自动化（如我们在[图1-3](#adapting_to_change_diagram)中看到），因此即使是批量学习系统也可以适应变化。只需根据需要更新数据并从头开始训练系统的新版本即可。
- en: This solution is simple and often works fine, but training using the full set
    of data can take many hours, so you would typically train a new system only every
    24 hours or even just weekly. If your system needs to adapt to rapidly changing
    data (e.g., to predict stock prices), then you need a more reactive solution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案很简单，通常效果很好，但使用全部数据集进行训练可能需要很多小时，所以你通常会每隔 24 小时或甚至每周只训练一个新的系统。如果你的系统需要适应快速变化的数据（例如，预测股价），那么你需要一个更反应灵敏的解决方案。
- en: Also, training on the full set of data requires a lot of computing resources
    (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot
    of data and you automate your system to train from scratch every day, it will
    end up costing you a lot of money. If the amount of data is huge and your system
    must always be up to date, it may even be impossible to use batch learning.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在全部数据集上进行训练需要大量的计算资源（CPU、内存空间、磁盘空间、磁盘 I/O、网络 I/O 等）。如果你有大量数据，并且自动化你的系统每天从头开始训练，这最终会花费你很多钱。如果数据量巨大，并且你的系统必须始终是最新的，那么甚至可能无法使用批量学习。
- en: Finally, if your system needs to be able to learn autonomously and it has limited
    resources (e.g., a smartphone application or a rover on Mars), then carrying around
    large amounts of training data and taking up a lot of resources to train for hours
    every day is a showstopper.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你的系统需要能够自主学习并且资源有限（例如，智能手机应用程序或火星漫游车），那么携带大量训练数据并且每天花费大量资源进行数小时训练将是一个障碍。
- en: A better option in all these scenarios is to use algorithms that are capable
    of learning incrementally.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，更好的选择是使用能够增量学习的算法。
- en: Online learning
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线学习
- en: In *online learning*, you train the system incrementally by feeding it data
    instances sequentially, either individually or in small groups called *mini-batches*.
    Each learning step is fast and cheap, so the system can learn about new data on
    the fly, as it arrives (see [Figure 1-13](#online_learning_diagram)). The most
    common online algorithm by far is gradient descent, but there are a few others.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在*在线学习*中，通过按顺序向系统提供数据实例（单独或以称为*小批量*的小组形式）来增量训练系统。每个学习步骤都很快且成本低，因此系统可以实时学习新数据，随着数据的到来（参见[图
    1-13](#online_learning_diagram)）。最常用的在线算法是梯度下降，但也有其他几种。
- en: '![Diagram illustrating the online learning process, showing model training,
    evaluation, launching, and continuous learning with new data.](assets/hmls_0113.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![在线学习过程的示意图，展示了模型训练、评估、启动和用新数据进行持续学习。](assets/hmls_0113.png)'
- en: Figure 1-13\. In online learning, a model is trained and launched into production,
    and then it keeps learning as new data comes in
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-13\. 在在线学习中，模型被训练并投入生产，然后随着新数据的到来持续学习
- en: Online learning is useful for systems that need to adapt to change extremely
    rapidly (e.g., to detect new patterns in the stock market). It is also a good
    option if you have limited computing resources; for example, if the model is trained
    on a mobile device.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习对于需要快速适应变化的系统非常有用（例如，检测股市中的新模式）。如果你拥有有限的计算资源，这也是一个好的选择；例如，如果模型是在移动设备上训练的。
- en: Most importantly, online learning algorithms can be used to train models on
    huge datasets that cannot fit in one machine’s memory (this is called *out-of-core*
    learning). The algorithm loads part of the data, runs a training step on that
    data, and repeats the process until it has run on all of the data (see [Figure 1-14](#ol_for_huge_datasets_diagram)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，在线学习算法可以用来训练那些无法装入一台机器内存的大型数据集（这被称为*离核学习*）。算法加载部分数据，在该数据上运行一个训练步骤，然后重复这个过程，直到运行完所有数据（参见[图
    1-14](#ol_for_huge_datasets_diagram)）。
- en: '![Diagram showing the process of online learning for large datasets, involving
    data partitioning, training, evaluating, analyzing errors, and launching the model.](assets/hmls_0114.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![展示大型数据集在线学习过程的示意图，包括数据分区、训练、评估、分析错误和启动模型。](assets/hmls_0114.png)'
- en: Figure 1-14\. Using online learning to handle huge datasets
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-14\. 使用在线学习处理大量数据集
- en: 'One important parameter of online learning systems is how fast they should
    adapt to changing data: this is called the *learning rate*. If you set a high
    learning rate, then your system will rapidly adapt to new data, but it will also
    tend to quickly forget the old data: this is called *catastrophic forgetting*
    (or *catastrophic interference*). You don’t want a spam filter to flag only the
    latest kinds of spam it was shown! Conversely, if you set a low learning rate,
    the system will have more inertia; that is, it will learn more slowly, but it
    will also be less sensitive to noise in the new data or to sequences of nonrepresentative
    data points (outliers).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习系统的一个重要参数是它们应该多快适应变化的数据：这被称为**学习率**。如果你设置一个高的学习率，那么你的系统将迅速适应新的数据，但它也会倾向于快速忘记旧数据：这被称为**灾难性遗忘**（或**灾难性干扰**）。你不想一个垃圾邮件过滤器只标记它所展示的最新类型的垃圾邮件！相反，如果你设置一个低的学习率，系统将具有更多的惯性；也就是说，它将学习得更慢，但它对新的数据中的噪声或非代表性数据点的序列（异常值）的敏感性也会降低。
- en: Warning
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Out-of-core learning is usually done offline (i.e., not on the live system),
    so *online learning* can be a confusing name. Think of it as *incremental learning*.
    Moreover, mini-batches are often just called “batches”, so *batch learning* is
    also a confusing name. Think of it as learning from scratch on the full dataset.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 核外学习通常是在线下的（即不在实时系统上），所以**在线学习**这个名字可能会让人困惑。把它想作**增量学习**。此外，小批量通常只被称为“批量”，所以**批量学习**也是一个让人困惑的名字。把它想作从头开始在完整数据集上学习。
- en: A big challenge with online learning is that if bad data is fed to the system,
    the system’s performance will decline, possibly quickly (depending on the data
    quality and learning rate). If it’s a live system, your clients will notice. For
    example, bad data could come from a bug (e.g., a malfunctioning sensor on a robot),
    or it could come from someone trying to game the system (e.g., spamming a search
    engine to try to rank high in search results). To reduce this risk, you need to
    monitor your system closely and promptly switch learning off (and possibly revert
    to a previously working state) if you detect a drop in performance. You may also
    want to monitor the input data and react to abnormal data; for example, using
    an anomaly detection algorithm (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的一个大挑战是，如果向系统输入了坏数据，系统的性能将下降，可能很快（取决于数据质量和学习率）。如果是一个实时系统，你的客户会注意到。例如，坏数据可能来自一个错误（例如，机器人上的一个故障传感器），或者可能来自试图操纵系统的人（例如，向搜索引擎发送垃圾邮件以试图在搜索结果中排名靠前）。为了降低这种风险，你需要密切监控你的系统，并在检测到性能下降时及时关闭学习（并可能恢复到之前的工作状态）。你可能还想要监控输入数据，并对异常数据做出反应；例如，使用异常检测算法（见第8章[第8章](ch08.html#unsupervised_learning_chapter)）。
- en: Instance-Based Versus Model-Based Learning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于实例学习与基于模型学习
- en: One more way to categorize machine learning systems is by how they *generalize*.
    Most machine learning tasks are about making predictions. This means that given
    a number of training examples, the system needs to be able to make good predictions
    for (generalize to) examples it has never seen before. Having a good performance
    measure on the training data is good, but insufficient; the true goal is to perform
    well on new instances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种对机器学习系统进行分类的方法是它们如何**泛化**。大多数机器学习任务都是关于做出预测。这意味着给定一些训练示例，系统需要能够对它以前从未见过的示例做出良好的预测（泛化）。在训练数据上有一个好的性能指标是好的，但不足以；真正的目标是能够在新的实例上表现良好。
- en: 'There are two main approaches to generalization: instance-based learning and
    model-based learning.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的两种主要方法是基于实例的学习和基于模型的学习。
- en: Instance-based learning
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于实例的学习
- en: Possibly the most trivial form of learning is simply to learn by heart. If you
    were to create a spam filter this way, it would just flag all emails that are
    identical to emails that have already been flagged by users—not the worst solution,
    but certainly not the best.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最简单的一种学习方法就是死记硬背。如果你用这种方法创建一个垃圾邮件过滤器，它只会标记所有与用户已经标记的邮件相同的邮件——这不是最差的方法，但绝对不是最好的。
- en: Instead of just flagging emails that are identical to known spam emails, your
    spam filter could be programmed to also flag emails that are very similar to known
    spam emails. This requires a *measure of similarity* between two emails. A (very
    basic) similarity measure between two emails could be to count the number of words
    they have in common. The system would flag an email as spam if it has many words
    in common with a known spam email.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标记与已知垃圾邮件完全相同的电子邮件外，你的垃圾邮件过滤器还可以编程为标记与已知垃圾邮件非常相似的电子邮件。这需要两个电子邮件之间的**相似度度量**。两个电子邮件之间的（非常基础的）相似度度量可以是计算它们共有的单词数量。如果一封电子邮件与已知垃圾邮件有大量共同单词，系统就会将其标记为垃圾邮件。
- en: 'This is called *instance-based learning*: the system learns the examples by
    heart, then generalizes to new cases by using a similarity measure to compare
    them to the learned examples (or a subset of them). For example, in [Figure 1-15](#instance_based_learning_diagram)
    the new instance would be classified as a triangle because the majority of the
    most similar instances belong to that class.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**基于实例的学习**：系统通过记忆学习示例，然后通过使用相似度度量将它们与学习到的示例（或它们的子集）进行比较来推广到新的案例。例如，在[图1-15](#instance_based_learning_diagram)中，新实例将被分类为三角形，因为大多数最相似的实例属于该类别。
- en: 'Instance-based learning often shines with small datasets, especially if the
    data keeps changing, but it does not scale very well: it requires deploying a
    whole copy of the training set to production; making predictions requires searching
    for similar instances, which can be quite slow; and it doesn’t work well with
    high-dimensional data such as images.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例的学习在小型数据集上通常表现良好，特别是如果数据不断变化，但它扩展性不佳：它需要在生产中部署整个训练集的副本；进行预测需要搜索相似的实例，这可能相当慢；并且它不适用于高维数据，如图像。
- en: '![Diagram illustrating instance-based learning, showing a new instance and
    its three nearest neighbors among training instances with two features.](assets/hmls_0115.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![基于实例的学习示意图，展示了在具有两个特征的训练实例中，一个新实例及其三个最近邻。](assets/hmls_0115.png)'
- en: 'Figure 1-15\. Instance-based learning: in this example we consider the class
    of the three nearest neighbors in the training set'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-15。基于实例的学习：在这个例子中，我们考虑训练集中三个最近邻的类别
- en: Model-based learning and a typical machine learning workflow
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于模型的学习和典型的机器学习工作流程
- en: Another way to generalize from a set of examples is to build a model of these
    examples and then use that model to make *predictions*. This is called *model-based
    learning* ([Figure 1-16](#model_based_learning_diagram)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从一组示例中推广的另一种方法是构建这些示例的模型，然后使用该模型进行预测。这被称为**基于模型的学习**（[图1-16](#model_based_learning_diagram)）。
- en: '![Diagram illustrating model-based learning with a decision boundary separating
    two classes of data points represented as triangles and squares based on two features.](assets/hmls_0116.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![基于模型的学习示意图，展示了根据两个特征将数据点分为三角形和正方形的两个类别的决策边界。](assets/hmls_0116.png)'
- en: Figure 1-16\. Model-based learning
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-16。基于模型的学习
- en: For example, suppose you want to know if money makes people happy, so you download
    the Better Life Index data from the [OECD’s website](https://www.oecdbetterlifeindex.org),
    and [World Bank stats](https://ourworldindata.org) about gross domestic product
    (GDP) per capita. Then you join the tables and sort by GDP per capita. [Table 1-1](#life_satisfaction_table_excerpt)
    shows an excerpt of what you get.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你想知道金钱是否能让人快乐，那么你可以从[经合组织网站](https://www.oecdbetterlifeindex.org)下载更好的生活指数数据，以及[世界银行统计数据](https://ourworldindata.org)关于人均国内生产总值（GDP）。然后你将这两个表连接起来，并按人均GDP排序。[表1-1](#life_satisfaction_table_excerpt)显示了你可以得到的部分内容。
- en: Table 1-1\. Does money make people happier?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1。金钱能让人更快乐吗？
- en: '| Country | GDP per capita (USD) | Life satisfaction |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 人均GDP（美元） | 生活满意度 |'
- en: '| --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Turkey | 28,384 | 5.5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 土耳其 | 28,384 | 5.5 |'
- en: '| Hungary | 31,008 | 5.6 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 匈牙利 | 31,008 | 5.6 |'
- en: '| France | 42,026 | 6.5 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 法国 | 42,026 | 6.5 |'
- en: '| United States | 60,236 | 6.9 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 美国 | 60,236 | 6.9 |'
- en: '| New Zealand | 42,404 | 7.3 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 新西兰 | 42,404 | 7.3 |'
- en: '| Australia | 48,698 | 7.3 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 48,698 | 7.3 |'
- en: '| Denmark | 55,938 | 7.6 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 丹麦 | 55,938 | 7.6 |'
- en: Let’s plot the data for these countries ([Figure 1-17](#money_happy_scatterplot)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些国家的数据图（[图1-17](#money_happy_scatterplot)）。
- en: '![Scatterplot showing the relationship between GDP per capita and life satisfaction
    for various countries, indicating a positive trend.](assets/hmls_0117.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示了人均GDP和生活满意度之间的关系，表明存在正相关趋势。](assets/hmls_0117.png)'
- en: Figure 1-17\. Do you see a trend here?
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-17。你在这里看到了趋势吗？
- en: 'There does seem to be a trend here! Although the data is *noisy* (i.e., partly
    random), it looks like life satisfaction goes up more or less linearly as the
    country’s GDP per capita increases. So you decide to model life satisfaction as
    a linear function of GDP per capita (you assume that any deviation from that line
    is just random noise). This step is called *model selection*: you selected a *linear
    model* of life satisfaction with just one attribute, GDP per capita ([Equation
    1-1](#a_simple_linear_model)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里似乎确实存在一种趋势！尽管数据是*噪声的*（即，部分是随机的），但看起来随着人均国民生产总值的增加，生活满意度或多或少呈线性增长。因此，你决定将生活满意度建模为人均国民生产总值的线性函数（你假设任何偏离该线的偏差只是随机噪声）。这一步被称为*模型选择*：你选择了一个只包含一个属性，即人均国民生产总值的*线性模型*（[方程式
    1-1](#a_simple_linear_model)）。
- en: Equation 1-1\. A simple linear model
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 1-1\. 一个简单的线性模型
- en: <mrow><mtext>life_satisfaction</mtext> <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>×</mo> <mtext>GDP_per_capita</mtext></mrow>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mtext>生活满意度</mtext> <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <mo>×</mo> <mtext>人均国民生产总值</mtext></mrow>
- en: This model has two *model parameters*, *θ*[0] and *θ*[1].⁠^([4](ch01.html#id844))
    By tweaking these parameters, you can make your model represent any linear function,
    as shown in [Figure 1-18](#tweaking_model_params_plot).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型有两个*模型参数*，*θ*[0] 和 *θ*[1]。⁠^([4](ch01.html#id844)) 通过调整这些参数，你可以使你的模型表示任何线性函数，如图[图
    1-18](#tweaking_model_params_plot)所示。
- en: '![Diagram showing possible linear models with different parameter values predicting
    life satisfaction based on GDP per capita.](assets/hmls_0118.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![展示基于人均国民生产总值预测生活满意度的不同参数值可能线性模型的图表](assets/hmls_0118.png)'
- en: Figure 1-18\. A few possible linear models
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-18\. 几个可能的线性模型
- en: Before you can use your model, you need to define the parameter values *θ*[0]
    and *θ*[1]. How can you know which values will make your model perform best? To
    answer this question, you need to specify a performance measure. You can either
    define a *utility function* (or *fitness function*) that measures how *good* your
    model is, or you can define a *cost function* (a.k.a., *loss function*) that measures
    how *bad* it is. For linear regression problems, people typically use a cost function
    that measures the distance between the linear model’s predictions and the training
    examples; the objective is to minimize this distance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够使用你的模型之前，你需要定义参数值 *θ*[0] 和 *θ*[1]。你如何知道哪些值会使你的模型表现最佳？为了回答这个问题，你需要指定一个性能指标。你可以定义一个*效用函数*（或*适应函数*），它衡量你的模型有多好，或者你可以定义一个*成本函数*（也称为*损失函数*），它衡量它有多坏。对于线性回归问题，人们通常使用一个衡量线性模型预测与训练示例之间距离的成本函数；目标是使这个距离最小化。
- en: 'This is where the linear regression algorithm comes in: you feed it your training
    examples, and it finds the parameters that make the linear model fit best to your
    data. This is called *training* the model. In our case, the algorithm finds that
    the optimal parameter values are *θ*[0] = 3.75 and *θ*[1] = 6.78 × 10^(–5).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是线性回归算法发挥作用的地方：你给它提供你的训练示例，然后它找到使线性模型最佳拟合你的数据的参数。这被称为*训练*模型。在我们的情况下，算法发现最佳参数值是*θ*[0]
    = 3.75 和 *θ*[1] = 6.78 × 10^(–5)。
- en: Warning
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Confusingly, the word “model” can refer to a *type of model* (e.g., linear regression),
    to a *fully specified model architecture* (e.g., linear regression with one input
    and one output), or to the *final trained model* ready to be used for predictions
    (e.g., linear regression with one input and one output, using *θ*[0] = 3.75 and
    *θ*[1] = 6.78 × 10^(–5)). Model selection consists in choosing the type of model
    and fully specifying its architecture. Training a model means running an algorithm
    to find the model parameters that will make it best fit the training data, and
    hopefully make good predictions on new data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 令人困惑的是，“模型”一词可以指代*模型类型*（例如，线性回归），一个*完全指定的模型架构*（例如，一个输入和一个输出的线性回归），或者一个*最终训练好的模型*，准备用于预测（例如，一个输入和一个输出的线性回归，使用
    *θ*[0] = 3.75 和 *θ*[1] = 6.78 × 10^(–5)）。模型选择包括选择模型类型并完全指定其架构。训练一个模型意味着运行一个算法来找到将使模型最佳拟合训练数据的模型参数，并希望在新数据上做出良好的预测。
- en: Now the model fits the training data as closely as possible (for a linear model),
    as you can see in [Figure 1-19](#best_fit_model_plot).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个模型尽可能紧密地拟合了训练数据（对于线性模型），正如你在[图 1-19](#best_fit_model_plot)中可以看到的那样。
- en: '![Scatter plot with a line of best fit showing the relationship between GDP
    per capita (USD) and life satisfaction, with model parameters θ₀ = 3.75 and θ₁
    = 6.78 × 10⁻⁵.](assets/hmls_0119.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![散点图显示人均GDP（美元）与生活满意度之间的关系，最佳拟合线参数θ₀ = 3.75和θ₁ = 6.78 × 10⁻⁵。](assets/hmls_0119.png)'
- en: Figure 1-19\. The linear model that fits the training data best
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-19。最佳拟合训练数据的线性模型
- en: 'You are finally ready to run the model to make predictions. For example, say
    you want to know how happy Puerto Ricans are, and the OECD data does not have
    the answer. Fortunately, you can use your model to make a good prediction: you
    look up Puerto Rico’s GDP per capita, find $33,442, and then apply your model
    and find that life satisfaction is likely to be somewhere around 3.75 + 33,442
    × 6.78 × 10^(–5) = 6.02.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您终于准备好运行模型进行预测了。例如，假设您想知道波多黎各人的幸福感如何，而经合组织的数据没有给出答案。幸运的是，您可以使用您的模型进行良好的预测：您查找波多黎各的人均GDP，找到$33,442，然后应用您的模型，发现生活满意度可能大约在3.75
    + 33,442 × 6.78 × 10^(–5) = 6.02。
- en: To whet your appetite, [Example 1-1](#example_scikit_code) shows the Python
    code that loads the data, separates the inputs `X` from the labels `y`, creates
    a scatterplot for visualization, and then trains a linear model and makes a prediction.⁠^([5](ch01.html#id850))
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激发您的兴趣，[示例1-1](#example_scikit_code)展示了加载数据、将输入`X`与标签`y`分开、创建散点图进行可视化和训练线性模型并做出预测的Python代码。⁠^([5](ch01.html#id850))
- en: Example 1-1\. Training and running a linear model using Scikit-Learn
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例1-1。使用Scikit-Learn训练和运行线性模型
- en: '[PRE0]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you had used an instance-based learning algorithm instead, you would have
    found that Poland has the closest GDP per capita to that of Puerto Rico ($32,238),
    and since the OECD data tells us that Poles’ life satisfaction is 6.1, you would
    have predicted a life satisfaction of 6.1 as well for Puerto Rico. If you zoom
    out a bit and look at the next two closest countries, you will find Portugal with
    a life satisfaction of 5.4, and Estonia with a life satisfaction of 5.7\. Averaging
    these three values, you get 5.73, which is a bit below your model-based prediction.
    This simple algorithm is called *k-nearest neighbors* regression (in this example,
    *k* = 3).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是基于实例的学习算法，您会发现波兰的人均GDP与波多黎各最接近（$32,238），而经合组织的数据告诉我们波兰人的生活满意度是6.1，因此您也会预测波多黎各的生活满意度为6.1。如果您稍微放大视角，看看下一个最接近的两个国家，您会发现葡萄牙的生活满意度为5.4，爱沙尼亚的生活满意度为5.7。这三个值的平均值为5.73，略低于您基于模型的预测。这个简单的算法被称为*k-最近邻回归*（在这个例子中，*k*
    = 3）。
- en: 'Replacing the linear regression model with *k*-nearest neighbors regression
    in the previous code is as easy as replacing these lines:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中将线性回归模型替换为*k-最近邻回归*与替换以下这些行一样简单：
- en: '[PRE1]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'with these two:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与这两个：
- en: '[PRE2]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If all went well, your model will make good predictions. If not, you may need
    to use more attributes (employment rate, health, air pollution, etc.), get more
    or better-quality training data, or perhaps select a more powerful model (e.g.,
    a polynomial regression model).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您的模型将做出良好的预测。如果不顺利，您可能需要使用更多的属性（就业率、健康、空气污染等）、获取更多或更好的训练数据，或者可能需要选择一个更强大的模型（例如，多项式回归模型）。
- en: 'In summary:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 总结如下：
- en: You studied the data.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经研究了数据。
- en: You selected a model.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已选择了一个模型。
- en: You trained it on the training data (i.e., the learning algorithm searched for
    the model parameter values that minimize a cost function).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在训练数据上训练了它（即，学习算法搜索最小化成本函数的模型参数值）。
- en: Finally, you applied the model to make predictions on new cases (this is called
    *inference*), hoping that this model will generalize well.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，您将模型应用于对新案例进行预测（这被称为*推理*），希望这个模型能够很好地泛化。
- en: This is what a typical machine learning project looks like. In [Chapter 2](ch02.html#project_chapter)
    you will experience this firsthand by going through a project end to end.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是典型的机器学习项目看起来像什么。在[第2章](ch02.html#project_chapter)中，您将通过从头到尾完成一个项目来亲身体验这一点。
- en: We discussed quite a few categories of ML systems, but this field has more!
    For example, *ensemble learning* involves training multiple models and combining
    their individual predictions into improved predictions (see [Chapter 6](ch06.html#ensembles_chapter));
    *federated learning* is a decentralized approach where models are trained across
    multiple devices (e.g., smartphones) and adapted to each user without exchanging
    raw data, thereby protecting the user’s privacy; *meta-learning* is a learning-to-learn
    approach where models learn how to learn new tasks quickly with minimal data.
    And the list goes on! [Figure 1-20](#ml_categories_diagram) summarizes the various
    classifications of ML systems we have discussed so far.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了许多机器学习系统的类别，但这个领域还有更多！例如，*集成学习*涉及训练多个模型并将它们的个别预测组合成改进的预测（见[第6章](ch06.html#ensembles_chapter)）；*联邦学习*是一种去中心化的方法，模型在多个设备（例如，智能手机）上训练，并针对每个用户进行调整，而不交换原始数据，从而保护用户的隐私；*元学习*是一种学习如何快速学习新任务的方法，模型用最少的数据学习。而且还有更多！[图1-20](#ml_categories_diagram)总结了我们迄今为止讨论的机器学习系统的各种分类。
- en: '![Diagram showing categories of machine learning, including learning paradigms,
    tasks, training methods, modeling approaches, and other types like ensemble, federated,
    and meta-learning.](assets/hmls_0120.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![展示机器学习类别图，包括学习范式、任务、训练方法、建模方法以及其他类型如集成、联邦和元学习等。](assets/hmls_0120.png)'
- en: Figure 1-20\. Overview of ML categories
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-20. 机器学习类别概述
- en: 'We have covered a lot of ground so far: you now know what machine learning
    is really about, why it is useful, what some of the most common categories of
    ML systems are, and what a typical project workflow looks like. Now let’s look
    at what can go wrong in learning and prevent you from making accurate predictions.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经覆盖了很多内容：你现在知道机器学习的真正含义，为什么它有用，一些最常见的机器学习系统类别是什么，以及一个典型项目的工作流程是什么样的。现在让我们看看在学习过程中可能出错的地方，以及这些错误如何阻止你做出准确的预测。
- en: Main Challenges of Machine Learning
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的主要挑战
- en: In short, since your main task is to select a model and train it on some data,
    the two things that can go wrong are “bad model” and “bad data”. Let’s start with
    examples of bad data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，由于你的主要任务是选择一个模型并在某些数据上对其进行训练，可能出现的问题有两种：“模型不佳”和“数据不佳”。让我们先从数据不佳的例子开始。
- en: Insufficient Quantity of Training Data
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据量不足
- en: For a toddler to learn what an apple is, all it takes is for you to point to
    an apple and say “apple” (possibly repeating this procedure a few times). Now
    the child is able to recognize apples in all sorts of colors and shapes. Genius.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个学步儿童来说，要学习苹果是什么，你只需要指着苹果说“苹果”（可能需要重复这个程序几次）。现在孩子能够识别各种颜色和形状的苹果。天才。
- en: Machine learning is not quite there yet; it takes a lot of data for most machine
    learning algorithms to work properly. Even for very simple problems you typically
    need thousands of examples, and for complex problems such as image or speech recognition
    you may need millions of examples (unless you can reuse parts of an existing model,
    i.e., transfer learning).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习还没有完全达到这个水平；对于大多数机器学习算法来说，需要大量的数据才能正常工作。即使是非常简单的问题，通常也需要成千上万的例子，而对于复杂问题，如图像或语音识别，你可能需要数百万个例子（除非你可以重用现有模型的部分，即迁移学习）。
- en: Nonrepresentative Training Data
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非代表性训练数据
- en: In order to generalize well, it is crucial that your training data be representative
    of the new cases you want to generalize to. This is true whether you use instance-based
    learning or model-based learning.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够很好地泛化，你的训练数据必须能够代表你想要泛化的新案例。这无论你使用基于实例的学习还是基于模型的学习都是正确的。
- en: For example, the set of countries you used earlier for training the linear model
    was not perfectly representative; it did not contain any country with a GDP per
    capita lower than $23,500 or higher than $62,500\. [Figure 1-22](#representative_training_data_scatterplot)
    shows what the data looks like when you add such countries.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你之前用于训练线性模型的国家的集合并不完全具有代表性；它不包含人均GDP低于23,500美元或高于62,500美元的任何国家。[图1-22](#representative_training_data_scatterplot)显示了添加此类国家后的数据看起来是什么样子。
- en: If you train a linear model on this data, you get the solid line, while the
    old model is represented by the dotted line. As you can see, not only does adding
    a few missing countries significantly alter the model, but it makes it clear that
    such a simple linear model is probably never going to work well. It seems that
    very rich countries are not happier than moderately rich countries (in fact, they
    seem slightly unhappier!), and conversely some poor countries seem happier than
    many rich countries.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个数据上训练一个线性模型，你会得到实线，而旧模型由虚线表示。正如你所见，添加几个缺失的国家不仅会显著改变模型，而且清楚地表明这样一个简单的线性模型可能永远不会很好地工作。似乎非常富裕的国家并不比中等富裕的国家更快乐（事实上，它们似乎稍微不快乐！），反之，一些贫穷的国家似乎比许多富裕的国家更快乐。
- en: By using a nonrepresentative training set, you trained a model that is unlikely
    to make accurate predictions, especially for very poor and very rich countries.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用一个不具有代表性的训练集，你训练了一个不太可能做出准确预测的模型，尤其是对于非常贫穷和非常富裕的国家。
- en: 'It is crucial to use a training set that is representative of the cases you
    want to generalize to. This is often harder than it sounds: if the sample is too
    small, you will have *sampling noise* (i.e., nonrepresentative data as a result
    of chance), but even very large samples can be nonrepresentative if the sampling
    method is flawed. This is called *sampling bias*.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个能代表你想要推广的案例的训练集至关重要。这通常比听起来更难：如果样本太小，你将会有*抽样噪声*（即由于偶然性而产生的非代表性数据），但即使是非常大的样本也可能不具有代表性，如果抽样方法有缺陷。这被称为*抽样偏差*。
- en: '![A scatter plot showing life satisfaction versus GDP per capita, highlighting
    countries like Colombia, Brazil, Norway, and Luxembourg, to illustrate sampling
    bias and the importance of a representative training set.](assets/hmls_0122.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![展示人均GDP与生活满意度关系的散点图，突出哥伦比亚、巴西、挪威和卢森堡等国家，以说明抽样偏差和代表性训练集的重要性。](assets/hmls_0122.png)'
- en: Figure 1-22\. A more representative training sample
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-22\. 更具代表性的训练样本
- en: Poor-Quality Data
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低质量数据
- en: 'Obviously, if your training data is full of errors, outliers, and noise (e.g.,
    due to poor-quality measurements), it will make it harder for the system to detect
    the underlying patterns, so your system is less likely to perform well. It is
    often well worth the effort to spend time cleaning up your training data. The
    truth is, most data scientists spend a significant part of their time doing just
    that. The following are a couple examples of when you’d want to clean up training
    data:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果你的训练数据充满了错误、异常值和噪声（例如，由于测量质量差），这将使系统更难检测到潜在的模式，因此你的系统不太可能表现良好。花时间清理你的训练数据通常是值得的。事实上，大多数数据科学家都会花大量时间做这件事。以下是一些你想要清理训练数据的例子：
- en: If some instances are clearly outliers, it may help to simply discard them or
    try to fix the errors manually.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些实例明显是异常值，简单地丢弃它们或尝试手动修复错误可能会有所帮助。
- en: If some instances are missing a few features (e.g., 5% of your customers did
    not specify their age), you must decide whether you want to ignore this attribute
    altogether, ignore these instances, fill in the missing values (e.g., with the
    median age), or train one model with the feature and one model without it.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些实例缺少一些特征（例如，5%的客户没有指定他们的年龄），你必须决定你是否想完全忽略这个属性，忽略这些实例，填充缺失值（例如，用中位数年龄），或者训练一个带有该特征的模型和一个不带该特征的模型。
- en: Irrelevant Features
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不相关特征
- en: 'As the saying goes: garbage in, garbage out. Your system will only be capable
    of learning if the training data contains enough relevant features and not too
    many irrelevant ones. A critical part of the success of a machine learning project
    is coming up with a good set of features to train on. This process, called *feature
    engineering*, involves the following steps:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正如俗话所说：垃圾进，垃圾出。如果你的训练数据包含足够的相关特征而不是太多不相关特征，你的系统才能具备学习能力。机器学习项目成功的关键部分是制定一套好的特征进行训练。这个过程称为*特征工程*，包括以下步骤：
- en: '*Feature selection* (selecting the most useful features to train on among existing
    features)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征选择*（在现有特征中选择最有用的特征进行训练）'
- en: '*Feature extraction* (combining existing features to produce a more useful
    one⁠—as we saw earlier, dimensionality reduction algorithms can help)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征提取*（将现有特征结合以产生更有用的一个——如我们之前所见，降维算法可以帮助）'
- en: Creating new features by gathering new data
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过收集新数据创建新特征
- en: Now that we have looked at many examples of bad data, let’s look at a couple
    examples of bad algorithms.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看了许多不良数据的例子，让我们看看几个不良算法的例子。
- en: Overfitting the Training Data
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过度拟合训练数据
- en: 'Say you are visiting a foreign country and the taxi driver rips you off. You
    might be tempted to say that *all* taxi drivers in that country are thieves. Overgeneralizing
    is something that we humans do all too often, and unfortunately machines can fall
    into the same trap if we are not careful. In machine learning this is called *overfitting*:
    it means that the model performs well on the training data, but it does not generalize
    well.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在访问一个外国国家，出租车司机骗了你。你可能会想当然地说，那个国家的*所有*出租车司机都是小偷。过度概括是我们人类经常做的事情，而且不幸的是，如果我们不小心，机器也可能陷入同样的陷阱。在机器学习中，这被称为*过度拟合*：这意味着模型在训练数据上表现良好，但推广能力不强。
- en: '[Figure 1-23](#overfitting_model_plot) shows an example of a high-degree polynomial
    life satisfaction model that strongly overfits the training data. Even though
    it performs much better on the training data than the simple linear model, would
    you really trust its predictions?'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-23](#overfitting_model_plot) 展示了一个高度多项式生活满意度模型过度拟合训练数据的示例。尽管它在训练数据上的表现比简单的线性模型要好得多，但你真的会相信它的预测吗？'
- en: '![A graph depicting a high-degree polynomial model that overfits life satisfaction
    data based on GDP per capita, illustrating poor generalization beyond the training
    set.](assets/hmls_0123.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表，描述了一个高度多项式模型，它基于人均GDP过度拟合生活满意度数据，展示了超出训练集的糟糕泛化。](assets/hmls_0123.png)'
- en: Figure 1-23\. Overfitting the training data
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-23\. 过度拟合训练数据
- en: 'Complex models such as deep neural networks can detect subtle patterns in the
    data, but if the training set is noisy, or if it is too small, which introduces
    sampling noise, then the model is likely to detect patterns in the noise itself
    (as in the taxi driver example). Obviously these patterns will not generalize
    to new instances. For example, say you feed your life satisfaction model many
    more attributes, including uninformative ones such as the country’s name. In that
    case, a complex model may detect patterns like the fact that all countries in
    the training data with a *w* in their name have a life satisfaction greater than
    7: New Zealand (7.3), Norway (7.6), Sweden (7.3), and Switzerland (7.5). How confident
    are you that the *w*-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously
    this pattern occurred in the training data by pure chance, but the model has no
    way to tell whether a pattern is real or simply the result of noise in the data.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 比如深度神经网络这样的复杂模型可以在数据中检测到微妙的模式，但如果训练集噪声较大，或者如果它太小，这会引入采样噪声，那么模型很可能会检测到噪声本身中的模式（如在出租车司机示例中）。显然，这些模式不会推广到新的实例。例如，假设你给你的生活满意度模型提供了许多更多的属性，包括诸如国家名称这样的非信息性属性。在这种情况下，一个复杂的模型可能会检测到像所有在训练数据中名字中带有*w*的国家生活满意度都大于7这样的模式：新西兰（7.3）、挪威（7.6）、瑞典（7.3）和瑞士（7.5）。你有多自信这个*w*-满意度规则可以推广到卢旺达或津巴布韦？显然，这种模式是由于训练数据中的纯偶然性而发生的，但模型无法判断一个模式是真实的还是仅仅是数据噪声的结果。
- en: Warning
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Overfitting happens when the model is too complex relative to the amount and
    noisiness of the training data, so it starts to learn random patterns in the training
    data. Here are possible solutions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 过度拟合发生在模型相对于训练数据的数量和噪声程度过于复杂的情况下，因此它开始学习训练数据中的随机模式。以下是一些可能的解决方案：
- en: Simplify the model by selecting one with fewer parameters (e.g., a linear model
    rather than a high-degree polynomial model), by reducing the number of attributes
    in the training data, or by constraining the model.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择具有较少参数的模型（例如，线性模型而不是高次多项式模型）、减少训练数据中的属性数量或约束模型来简化模型。
- en: Gather more training data.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多的训练数据。
- en: Reduce the noise in the training data (e.g., fix data errors and remove outliers).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少训练数据中的噪声（例如，修复数据错误和移除异常值）。
- en: 'Constraining a model to make it simpler and reduce the risk of overfitting
    is called *regularization*. For example, the linear model we defined earlier has
    two parameters, *θ*[0] and *θ*[1]. This gives the learning algorithm two *degrees
    of freedom* to adapt the model to the training data: it can tweak both the height
    (*θ*[0]) and the slope (*θ*[1]) of the line. If we forced *θ*[1] = 0, the algorithm
    would have only one degree of freedom and would have a much harder time fitting
    the data properly: all it could do is move the line up or down to get as close
    as possible to the training instances, so it would end up around the mean. A very
    simple model indeed! If we allow the algorithm to modify *θ*[1] but we force it
    to keep it small, then the learning algorithm will effectively have somewhere
    in between one and two degrees of freedom. It will produce a model that’s simpler
    than one with two degrees of freedom, but more complex than one with just one.
    You want to find the right balance between fitting the training data perfectly
    and keeping the model simple enough to ensure that it will generalize well.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通过约束模型使其更简单并降低过拟合的风险称为*正则化*。例如，我们之前定义的线性模型有两个参数，*θ*[0]和*θ*[1]。这给学习算法提供了两个*自由度*来调整模型以适应训练数据：它可以调整线的**高度**（*θ*[0]）和**斜率**（*θ*[1]）。如果我们强制*θ*[1]
    = 0，算法将只有一个自由度，并且将很难正确拟合数据：它所能做的就是移动线以尽可能接近训练实例，因此它最终会位于平均值附近。确实是一个非常简单的模型！如果我们允许算法修改*θ*[1]，但强制它保持较小，那么学习算法实际上将具有介于一个和两个自由度之间。它将产生一个比两个自由度更简单但比一个自由度更复杂的模型。你希望找到在完美拟合训练数据和保持模型足够简单以确保它能够很好地泛化之间的正确平衡。
- en: '[Figure 1-24](#ridge_model_plot) shows three models. The dotted line represents
    the original model that was trained on the countries represented as circles (without
    the countries represented as squares), the solid line is our second model trained
    with all countries (circles and squares), and the dashed line is a model trained
    with the same data as the first model but with a regularization constraint. You
    can see that regularization forced the model to have a smaller slope: this model
    does not fit the training data (circles) as well as the first model, but it actually
    generalizes better to new examples that it did not see during training (squares).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-24](#ridge_model_plot) 展示了三个模型。虚线代表原始模型，该模型是在代表圆圈（不包括代表方形的各国）的国家上训练的，实线是我们用所有国家（圆圈和方形）训练的第二模型，而虚线是使用与第一个模型相同数据但带有正则化约束的模型。你可以看到正则化迫使模型具有更小的斜率：这个模型不如第一个模型拟合训练数据（圆圈），但它实际上对新例子（方形）的泛化能力更好。'
- en: '![A plot showing three linear regression models on life satisfaction versus
    GDP per capita, illustrating how regularization affects model fitting and generalization
    to unseen data.](assets/hmls_0124.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![展示三个线性回归模型在人均GDP与生活满意度之间的图表，说明正则化如何影响模型拟合和推广到未见数据的能力。](assets/hmls_0124.png)'
- en: Figure 1-24\. Regularization reduces the risk of overfitting
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-24\. 正则化降低了过拟合的风险
- en: The amount of regularization to apply during learning can be controlled by a
    *hyperparameter*. A hyperparameter is a parameter of a learning algorithm (not
    of the model). As such, it is not affected by the learning algorithm itself; it
    must be set prior to training and remains constant during training. If you set
    the regularization hyperparameter to a very large value, you will get an almost
    flat model (a slope close to zero); the learning algorithm will almost certainly
    not overfit the training data, but it will be less likely to find a good solution.
    Tuning hyperparameters is an important part of building a machine learning system
    (you will see a detailed example in the next chapter).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中应用正则化的程度可以通过一个*超参数*来控制。超参数是学习算法的参数（而不是模型的参数）。因此，它不受学习算法本身的影响；它必须在训练之前设置，并在训练过程中保持不变。如果你将正则化超参数设置得非常大，你将得到一个非常平缓的模型（斜率接近零）；学习算法几乎肯定不会过拟合训练数据，但它不太可能找到好的解决方案。调整超参数是构建机器学习系统的重要部分（你将在下一章看到一个详细的例子）。
- en: Underfitting the Training Data
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对训练数据进行欠拟合
- en: 'As you might guess, *underfitting* is the opposite of overfitting: it occurs
    when your model is too simple to learn the underlying structure of the data. For
    example, a linear model of life satisfaction is prone to underfit; reality is
    just more complex than the model, so its predictions are bound to be inaccurate,
    even on the training examples.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，**欠拟合**是过拟合的反面：它发生在你的模型太简单，无法学习数据的潜在结构时。例如，生活满意度的线性模型容易欠拟合；现实世界比模型更复杂，所以其预测必然是不准确的，即使在训练示例上也是如此。
- en: 'Here are the main options for fixing this problem:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是解决这个问题的主要选项：
- en: Select a more powerful model, with more parameters.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个更强大的模型，具有更多参数。
- en: Feed better features to the learning algorithm (feature engineering).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向学习算法提供更好的特征（特征工程）。
- en: Reduce the constraints on the model (for example by reducing the regularization
    hyperparameter).
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少对模型的限制（例如通过减少正则化超参数）。
- en: Deployment Issues
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署问题
- en: 'Even if you have a large and clean dataset and you manage to train a beautiful
    model that neither underfits nor overfits the data, you may still run into issues
    during deployment: for example, the model may be too complex to maintain, or too
    large to fit in memory, or too slow, or it may not scale properly, it may have
    security vulnerabilities, it may become outdated if you don’t update it often
    enough, etc.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你有一个大而干净的数据集，并且你设法训练了一个既不过拟合也不欠拟合数据的美丽模型，你仍然可能在部署期间遇到问题：例如，模型可能太复杂而难以维护，或者太大而无法适应内存，或者太慢，或者它可能没有正确扩展，它可能有安全漏洞，如果你不经常更新它，它可能会过时等。
- en: In short, there’s more to an ML project than just data and models. However,
    the skillset required to handle these operational problems are fairly different
    from those required for data modeling, which is why companies often have a dedicated
    MLOps team (ML operations) to handle this.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，机器学习项目不仅仅是数据和模型。然而，处理这些操作问题所需的技能集与数据建模所需的技能集相当不同，这就是为什么公司通常有一个专门的MLOps团队（机器学习运营）来处理这些问题。
- en: Stepping Back
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 退一步
- en: 'By now you know a lot about machine learning. However, we went through so many
    concepts that you may be feeling a little lost, so let’s step back and look at
    the big picture:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你对机器学习已经了解很多。然而，我们经历了这么多概念，你可能感到有些迷茫，所以让我们退一步，看看大局：
- en: Machine learning is about making machines get better at some task by learning
    from data, instead of having to explicitly code rules.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是关于通过从数据中学习来使机器在某个任务上变得更好，而不是必须明确编码规则。
- en: 'There are many different types of ML systems: supervised or not, batch or online,
    instance-based or model-based.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多不同类型的机器学习系统：监督的或非监督的，批量的或在线的，基于实例的或基于模型的。
- en: In an ML project you gather data in a training set, and you feed the training
    set to a learning algorithm. If the algorithm is model-based, it tunes some parameters
    to fit the model to the training set (i.e., to make good predictions on the training
    set itself), and then hopefully it will be able to make good predictions on new
    cases as well. If the algorithm is instance-based, it just learns the examples
    by heart and generalizes to new instances by using a similarity measure to compare
    them to the learned instances.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习项目中，你在训练集中收集数据，并将训练集输入到学习算法中。如果算法是基于模型的，它会调整一些参数以使模型适应训练集（即，在训练集本身上做出良好的预测），然后希望它也能在新案例上做出良好的预测。如果算法是基于实例的，它只是通过记忆学习示例，并通过使用相似度度量来比较它们与学习到的实例来对新实例进行泛化。
- en: The system will not perform well if your training set is too small, or if the
    data is not representative, is noisy, or is polluted with irrelevant features
    (garbage in, garbage out). Your model needs to be neither too simple (in which
    case it will underfit) nor too complex (in which case it will overfit). Lastly,
    you must think carefully about deployment constraints.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的训练集太小，或者数据不具有代表性，是嘈杂的，或者被无关特征（垃圾输入，垃圾输出）污染，系统将无法良好运行。你的模型既不能太简单（在这种情况下会欠拟合）也不能太复杂（在这种情况下会过拟合）。最后，你必须仔细考虑部署限制。
- en: 'There’s just one last important topic to cover: once you have trained a model,
    you don’t want to just “hope” it generalizes to new cases. You want to evaluate
    it and fine-tune it if necessary. Let’s see how to do that.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后还有一个重要的话题要讨论：一旦你训练了一个模型，你不想只是“希望”它泛化到新案例。你想要评估它，并在必要时进行微调。让我们看看如何做到这一点。
- en: Testing and Validating
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和验证
- en: The only way to know how well a model will generalize to new cases is to actually
    try it out on new cases. One way to do that is to put your model in production
    and monitor how well it performs. This works well, but if your model is horribly
    bad, your users will complain—not the best idea.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 了解一个模型在新案例上的泛化能力如何的唯一方法是在新案例上实际尝试它。这样做的一种方法是将你的模型投入生产并监控其表现。这很有效，但如果你的模型表现极差，用户会抱怨——这不是一个好主意。
- en: 'A better option is to split your data into two sets: the *training set* and
    the *test set*. As these names imply, you train your model using the training
    set, and you test it using the test set. The error rate on new cases is called
    the *generalization error* (or *out-of-sample error*), and by evaluating your
    model on the test set, you get an estimate of this error. This value tells you
    how well your model will perform on instances it has never seen before.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的选择是将你的数据分成两个集合：*训练集*和*测试集*。正如这些名称所暗示的，你使用训练集来训练你的模型，并使用测试集来测试它。新案例上的错误率称为*泛化误差*（或*样本外误差*），通过在测试集上评估你的模型，你可以得到这个误差的估计。这个值告诉你你的模型在它从未见过的实例上的表现如何。
- en: If the training error is low (i.e., your model makes few mistakes on the training
    set) but the generalization error is high, it means that your model is overfitting
    the training data.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练误差很低（即你的模型在训练集上犯的错误很少）但泛化误差很高，这意味着你的模型过度拟合了训练数据。
- en: Tip
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'It is common to use 80% of the data for training and *hold out* 20% for testing.
    However, this depends on the size of the dataset: if it contains 10 million instances,
    then holding out 1% means your test set will contain 100,000 instances, probably
    more than enough to get a good estimate of the generalization error.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用80%的数据进行训练，*保留*20%用于测试。然而，这取决于数据集的大小：如果它包含1000万个实例，那么保留1%意味着你的测试集将包含10万个实例，这可能是获得泛化误差良好估计的足够数量。
- en: Hyperparameter Tuning and Model Selection
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整和模型选择
- en: 'Evaluating a model is simple enough: just use a test set. But suppose you are
    hesitating between two types of models (say, a linear model and a polynomial model):
    how can you decide between them? One option is to train both and compare how well
    they generalize using the test set.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个模型很简单：只需使用测试集。但假设你在两种类型的模型之间犹豫（比如说，线性模型和多项式模型）：你如何决定它们之间的区别？一个选择是训练它们并使用测试集比较它们的泛化能力。
- en: Now suppose that the linear model generalizes better, but you want to apply
    some regularization to avoid overfitting. The question is, how do you choose the
    value of the regularization hyperparameter? One option is to train 100 different
    models using 100 different values for this hyperparameter. Suppose you find the
    best hyperparameter value that produces a model with the lowest generalization
    error⁠—say, just 5% error. You launch this model into production, but unfortunately
    it does not perform as well as expected and produces 15% errors. What just happened?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设线性模型泛化得更好，但你想要应用一些正则化来避免过拟合。问题是，你如何选择正则化超参数的值？一个选择是使用100个不同的值来训练100个不同的模型。假设你找到了产生最低泛化误差的最佳超参数值——比如说，只有5%的误差。你将这个模型投入生产，但不幸的是，它的表现并没有达到预期，产生了15%的误差。发生了什么？
- en: The problem is that you measured the generalization error multiple times on
    the test set, and you adapted the model and hyperparameters to produce the best
    model *for that particular set*. This means the model is unlikely to perform as
    well on new data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于你在测试集上多次测量了泛化误差，并调整了模型和超参数以产生针对该特定集的最佳模型。这意味着该模型在新数据上可能表现不佳。
- en: 'A common solution to this problem is called *holdout validation* ([Figure 1-25](#hyperparameter_tuning_diagram)):
    you simply hold out part of the training set to evaluate several candidate models
    and select the best one. The new held-out set is called the *validation set* (or
    the *development set*, or *dev set*). More specifically, you train multiple models
    with various hyperparameters on the reduced training set (i.e., the full training
    set minus the validation set), and you select the model that performs best on
    the validation set. After this holdout validation process, you train the best
    model on the full training set (including the validation set), and this gives
    you the final model. Lastly, you evaluate this final model on the test set to
    get an estimate of the generalization error.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的常见方法被称为**保留验证法**([图1-25](#hyperparameter_tuning_diagram))：你只需保留一部分训练集来评估几个候选模型，并选择最佳的一个。新的保留集被称为**验证集**（或**开发集**，或**dev集**）。更具体地说，你使用各种超参数在减少的训练集（即减去验证集的完整训练集）上训练多个模型，并选择在验证集上表现最佳的模型。在保留验证过程之后，你在包括验证集的完整训练集上训练最佳模型，这为你提供了最终模型。最后，你评估这个最终模型在测试集上的表现，以获得泛化错误的估计。
- en: '![Diagram illustrating holdout validation for model selection, showing steps
    from training multiple models on a training set, evaluating them on a dev set,
    retraining the best model, and evaluating the final model on a test set.](assets/hmls_0125.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![说明模型选择保留验证法的图解，展示在训练集上训练多个模型、在开发集上评估它们、重新训练最佳模型以及在测试集上评估最终模型的过程。](assets/hmls_0125.png)'
- en: Figure 1-25\. Model selection using holdout validation
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-25. 使用保留验证法进行模型选择
- en: 'This solution usually works quite well. However, if the validation set is too
    small, then the model evaluations will be imprecise: you may end up selecting
    a suboptimal model by mistake. Conversely, if the validation set is too large,
    then the remaining training set will be much smaller than the full training set.
    Why is this bad? Well, since the final model will be trained on the full training
    set, it is not ideal to compare candidate models trained on a much smaller training
    set. It would be like selecting the fastest sprinter to participate in a marathon.
    One way to solve this problem is to perform repeated *cross-validation*, using
    many small validation sets. Each model is evaluated once per validation set after
    it is trained on the rest of the data. By averaging out all the evaluations of
    a model, you get a much more accurate measure of its performance. There is a drawback,
    however: the training time is multiplied by the number of validation sets.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案通常效果很好。然而，如果验证集太小，那么模型评估将不够精确：你可能会错误地选择一个次优模型。相反，如果验证集太大，那么剩余的训练集将比完整训练集小得多。为什么这不好？嗯，因为最终模型将在完整训练集上训练，所以比较在大大较小的训练集上训练的候选模型并不理想。这就像选择最快的短跑运动员参加马拉松一样。解决这个问题的方法之一是进行重复的**交叉验证**，使用许多小的验证集。每个模型在训练其余数据后，在每个验证集上评估一次。通过平均一个模型的全部评估，你可以得到一个对其性能的更准确衡量。然而，有一个缺点：训练时间会乘以验证集的数量。
- en: Data Mismatch
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据不匹配
- en: In some cases, it’s easy to get a large amount of data for training, but this
    data probably won’t be perfectly representative of the data that will be used
    in production. For example, suppose you want to create a mobile app to take pictures
    of flowers and automatically determine their species. You can easily download
    millions of pictures of flowers on the web, but they won’t be perfectly representative
    of the pictures that will actually be taken using the app on a mobile device.
    Perhaps you only have 1,000 representative pictures (i.e., actually taken with
    the app).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，获取大量训练数据很容易，但这些数据可能不会完美地代表生产中使用的那些数据。例如，假设你想创建一个移动应用程序，用于拍摄花朵照片并自动确定它们的种类。你可以在网上轻松下载数百万张花朵照片，但它们可能不会完美地代表使用移动设备上的应用程序实际拍摄的照片。也许你只有1,000张代表性照片（即实际上用应用程序拍摄的）。
- en: 'In this case, the most important rule to remember is that both the validation
    set and the test set must be as representative as possible of the data you expect
    to use in production, so they should be composed exclusively of representative
    pictures: you can shuffle them and put half in the validation set and half in
    the test set (making sure that no duplicates or near-duplicates end up in both
    sets). After training your model on the web pictures, if you observe that the
    performance of the model on the validation set is disappointing, you will not
    know whether this is because your model has overfit the training set, or whether
    this is just due to the mismatch between the web pictures and the mobile app pictures.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，最重要的规则要记住的是，验证集和测试集必须尽可能代表你预期在生产中使用的那些数据，因此它们应该仅由代表性图片组成：你可以将它们打乱，将一半放入验证集，另一半放入测试集（确保没有重复或几乎重复的图片出现在两个集合中）。在用网络图片训练你的模型后，如果你观察到模型在验证集上的表现令人失望，你将不知道这是否是因为你的模型对训练集过拟合，或者这仅仅是由于网络图片和移动应用图片之间的不匹配。
- en: One solution is to hold out some of the training pictures (from the web) in
    yet another set that Andrew Ng dubbed the *train-dev set* ([Figure 1-26](#train_dev_diagram)).
    After the model is trained (on the training set, *not* on the train-dev set),
    you can evaluate it on the train-dev set. If the model performs poorly, then it
    must have overfit the training set, so you should try to simplify or regularize
    the model, get more training data, and clean up the training data. But if it performs
    well on the train-dev set, then you can evaluate the model on the dev set. If
    it performs poorly, then the problem must be coming from the data mismatch. You
    can try to tackle this problem by preprocessing the web images to make them look
    more like the pictures that will be taken by the mobile app, and then retraining
    the model. Once you have a model that performs well on both the train-dev set
    and the dev set, you can evaluate it one last time on the test set to know how
    well it is likely to perform in production.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是将一些训练图片（来自网络）保留在另一个由Andrew Ng命名为*train-dev set*的集合中（[图1-26](#train_dev_diagram)）。在模型训练（在训练集上，*不是*在train-dev集上）之后，你可以在train-dev集上评估它。如果模型表现不佳，那么它一定是对训练集过拟合了，所以你应该尝试简化或正则化模型，获取更多训练数据，并清理训练数据。但如果它在train-dev集上表现良好，那么你可以在dev集上评估模型。如果它表现不佳，那么问题肯定来自数据不匹配。你可以尝试通过预处理网络图片，使它们看起来更像移动应用将拍摄的图片，然后重新训练模型。一旦你有一个在train-dev集和dev集上都表现良好的模型，你可以在test集上最后一次评估它，以了解它在生产中可能的表现有多好。
- en: '![Diagram showing abundant training data on the left divided into "Train" and
    "Train-dev" sets, with scarcer real data on the right used for "Dev" and "Test"
    sets to address overfitting and data mismatch.](assets/hmls_0126.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![展示左侧丰富的训练数据分为“Train”和“Train-dev”集，右侧稀缺的真实数据用于“Dev”和“Test”集以解决过拟合和数据不匹配的图表](assets/hmls_0126.png)'
- en: Figure 1-26\. When real data is scarce (right), you may use similar abundant
    data (left) for training and hold out some of it in a train-dev set to evaluate
    overfitting; the real data is then used to evaluate data mismatch (dev set) and
    to evaluate the final model’s performance (test set)
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-26。当真实数据稀缺（右侧）时，你可以使用类似丰富的数据（左侧）进行训练，并保留其中一部分作为train-dev集以评估过拟合；然后使用真实数据评估数据不匹配（dev集）和评估最终模型的表现（test集）
- en: Exercises
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'In this chapter we have covered some of the most important concepts in machine
    learning. In the next chapters we will dive deeper and write more code, but before
    we do, make sure you can answer the following questions:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些机器学习中最重要概念。在接下来的章节中，我们将深入探讨并编写更多代码，但在我们这样做之前，请确保你能回答以下问题：
- en: How would you define machine learning?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何定义机器学习？
- en: Can you name four types of applications where it shines?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出四种它闪耀的应用类型吗？
- en: What is a labeled training set?
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是标记的训练集？
- en: What are the two most common supervised tasks?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两种最常见的监督任务是什么？
- en: Can you name four common unsupervised tasks?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出四种常见的无监督任务吗？
- en: What type of algorithm would you use to allow a robot to walk in various unknown
    terrains?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会用哪种类型的算法让机器人能够在各种未知地形上行走？
- en: What type of algorithm would you use to segment your customers into multiple
    groups?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会用哪种类型的算法将你的客户分成多个组？
- en: Would you frame the problem of spam detection as a supervised learning problem
    or an unsupervised learning problem?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会将垃圾邮件检测问题视为监督学习问题还是无监督学习问题？
- en: What is an online learning system?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是在线学习系统？
- en: What is out-of-core learning?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是离线学习？
- en: What type of algorithm relies on a similarity measure to make predictions?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种类型的算法依赖于相似度度量来进行预测？
- en: What is the difference between a model parameter and a model hyperparameter?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数和模型超参数之间的区别是什么？
- en: What do model-based algorithms search for? What is the most common strategy
    they use to succeed? How do they make predictions?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于模型的算法在寻找什么？它们最常用的成功策略是什么？他们是如何进行预测的？
- en: Can you name four of the main challenges in machine learning?
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出机器学习中的四个主要挑战吗？
- en: If your model performs great on the training data but generalizes poorly to
    new instances, what is happening? Can you name three possible solutions?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的模型在训练数据上表现良好，但泛化到新实例上表现不佳，发生了什么？你能说出三种可能的解决方案吗？
- en: What is a test set, and why would you want to use it?
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试集是什么，为什么你想使用它？
- en: What is the purpose of a validation set?
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证集的目的是什么？
- en: What is the train-dev set, when do you need it, and how do you use it?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练-开发集是什么，何时需要它，以及如何使用它？
- en: What can go wrong if you tune hyperparameters using the test set?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用测试集调整超参数，可能会出什么问题？
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch01.html#id789-marker)) Fun fact: this odd-sounding name is a statistics
    term introduced by Francis Galton while he was studying the fact that the children
    of tall people tend to be shorter than their parents. Since the children were
    shorter, he called this *regression to the mean*. This name was then applied to
    the methods he used to analyze correlations between variables.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#id789-marker)) 有趣的事实：这个听起来古怪的名字是 Francis Galton 在研究高个子的人的孩子往往比他们的父母矮这一事实时提出的统计学术语。由于孩子们比较矮，他称之为
    *回归到平均值*。这个名字后来被应用到分析变量之间相关性的方法上。
- en: '^([2](ch01.html#id802-marker)) Notice how animals are rather well separated
    from vehicles and how horses are close to deer but far from birds. Figure reproduced
    with permission from Richard Socher et al., “Zero-Shot Learning Through Cross-Modal
    Transfer”, *Proceedings of the 26th International Conference on Neural Information
    Processing Systems* 1 (2013): 935–943.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch01.html#id802-marker)) 注意动物与车辆之间的分隔情况，以及马与鹿接近但与鸟远距离的情况。图示经 Richard
    Socher 等人许可复制，来自“Zero-Shot Learning Through Cross-Modal Transfer”，*Proceedings
    of the 26th International Conference on Neural Information Processing Systems*
    1 (2013): 935–943。'
- en: ^([3](ch01.html#id812-marker)) That’s when the system works perfectly. In practice
    it often creates a few clusters per person, and sometimes mixes up two people
    who look alike, so you may need to provide a few labels per person and manually
    clean up some clusters.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.html#id812-marker)) 那就是系统工作得完美的时候。在实践中，它通常为每个人创建几个簇，有时还会把看起来相似的两个人的数据混在一起，所以你可能需要为每个人提供几个标签，并手动清理一些簇。
- en: ^([4](ch01.html#id844-marker)) By convention, the Greek letter *θ* (theta) is
    frequently used to represent model parameters.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.html#id844-marker)) 按照惯例，希腊字母 *θ*（theta）常用来表示模型参数。
- en: ^([5](ch01.html#id850-marker)) It’s OK if you don’t understand all the code
    yet; I will present Scikit-Learn in the following chapters.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.html#id850-marker)) 如果你现在还不完全理解所有的代码，没关系；我将在接下来的章节中介绍 Scikit-Learn。
- en: ^([6](ch01.html#id866-marker)) For example, knowing whether to write “to”, “two”,
    or “too”, depending on the context.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.html#id866-marker)) 例如，根据上下文决定写“to”，“two”还是“too”。
- en: '^([7](ch01.html#id867-marker)) Figure reproduced with permission from Michele
    Banko and Eric Brill, “Scaling to Very Very Large Corpora for Natural Language
    Disambiguation”, *Proceedings of the 39th Annual Meeting of the Association for
    Computational Linguistics* (2001): 26–33.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '^([7](ch01.html#id867-marker)) 图示经 Michele Banko 和 Eric Brill 许可复制，来自“Scaling
    to Very Very Large Corpora for Natural Language Disambiguation”，*Proceedings of
    the 39th Annual Meeting of the Association for Computational Linguistics* (2001):
    26–33。'
- en: '^([8](ch01.html#id868-marker)) Peter Norvig et al., “The Unreasonable Effectiveness
    of Data”, *IEEE Intelligent Systems* 24, no. 2 (2009): 8–12.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch01.html#id868-marker)) Peter Norvig 等人，“数据的不合理有效性”，*IEEE Intelligent
    Systems* 24, no. 2 (2009): 8–12。'
- en: '^([9](ch01.html#id987-marker)) David Wolpert, “The Lack of A Priori Distinctions
    Between Learning Algorithms”, *Neural Computation* 8, no. 7 (1996): 1341–1390.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch01.html#id987-marker)) David Wolpert，“在学习算法之间缺乏先验区分”，*Neural Computation*
    8, no. 7 (1996): 1341–1390。'
