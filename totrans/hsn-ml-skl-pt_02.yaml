- en: Chapter 1\. The Machine Learning Landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Not so long ago, if you had picked up your phone and asked it to tell you the
    way home, it would have ignored you—and people would have questioned your sanity.
    But machine learning is no longer science fiction: billions of people use it every
    day. And the truth is it has actually been around for decades in some specialized
    applications, such as optical character recognition (OCR). The first ML application
    that really became mainstream, improving the lives of hundreds of millions of
    people, discretely took over the world back in the 1990s: the *spam filter*. It’s
    not exactly a self-aware robot, but it does technically qualify as machine learning:
    it has actually learned so well that you seldom need to flag an email as spam
    anymore. Then thanks to big data, hardware improvements, and a few algorithmic
    innovations, hundreds of ML applications followed and now quietly power hundreds
    of products and features that you use regularly: voice prompts, automatic translation,
    image search, product recommendations, and many more. And finally came ChatGPT,
    Gemini (formerly Bard), Claude, Perplexity, and many other chatbots: AI is no
    longer just powering services in the background, it *is* the service itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Where does machine learning start and where does it end? What exactly does it
    mean for a machine to *learn* something? If I download a copy of all Wikipedia
    articles, has my computer really learned something? Is it suddenly smarter? In
    this chapter I will start by clarifying what machine learning is and why you may
    want to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, before we set out to explore the machine learning continent, we will
    take a look at the map and learn about the main regions and the most notable landmarks:
    supervised versus unsupervised learning and their variants, online versus batch
    learning, instance-based versus model-based learning. Then we will look at the
    workflow of a typical ML project, discuss the main challenges you may face, and
    cover how to evaluate and fine-tune a machine learning system.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces a lot of fundamental concepts (and jargon) that every
    data scientist should know by heart. It will be a high-level overview (it’s the
    only chapter without much code), all rather simple, but my goal is to ensure everything
    is crystal clear to you before we continue on to the rest of the book. So grab
    a coffee and let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are already familiar with machine learning basics, you may want to skip
    directly to [Chapter 2](ch02.html#project_chapter). If you are not sure, try to
    answer all the questions listed at the end of the chapter before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is the science (and art) of programming computers so they can
    *learn from data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a slightly more general definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine learning is the] field of study that gives computers the ability to
    learn without being explicitly programmed.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Arthur Samuel, 1959
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'And a more engineering-oriented one:'
  prefs: []
  type: TYPE_NORMAL
- en: A computer program is said to learn from experience *E* with respect to some
    task *T* and some performance measure *P*, if its performance on *T*, as measured
    by *P*, improves with experience *E*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tom Mitchell, 1997
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Your spam filter is a machine learning program that, given examples of spam
    emails (flagged by users) and examples of regular emails (nonspam, also called
    “ham”), can learn to flag spam. The examples that the system uses to learn are
    called the *training set*. Each training example is called a *training instance*
    (or *sample*). The part of a machine learning system that learns and makes predictions
    is called a *model*. Neural networks and random forests are examples of models.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the task *T* is to flag spam for new emails, the experience *E*
    is the *training data*, and the performance measure *P* needs to be defined; for
    example, you can use the ratio of correctly classified emails. This particular
    performance measure is called *accuracy*, and it is often used in classification
    tasks (we will discuss several others in [Chapter 3](ch03.html#classification_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: If you just download a copy of all Wikipedia articles, your computer has a lot
    more data, but it is not suddenly better at any task. This is not machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider how you would write a spam filter using traditional programming techniques
    ([Figure 1-1](#traditional_approach_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: First you would examine what spam typically looks like. You might notice that
    some words or phrases (such as “4U”, “credit card”, “free”, and “amazing”) tend
    to come up a lot in the subject line. Perhaps you would also notice a few other
    patterns in the sender’s name, the email’s body, and other parts of the email.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You would write a detection algorithm for each of the patterns that you noticed,
    and your program would flag emails as spam if a number of these patterns were
    detected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You would test your program and repeat steps 1 and 2 until it was good enough
    to launch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Diagram illustrating the traditional approach to programming a spam filter,
    highlighting steps: study the problem, write rules, evaluate, and analyze errors,
    with feedback loops and launch point.](assets/hmls_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The traditional approach
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the problem is difficult, your program will likely become a long list
    of complex rules—pretty hard to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a spam filter based on machine learning techniques automatically
    learns which words and phrases are good predictors of spam by detecting unusually
    frequent patterns of words in the spam examples compared to the ham examples ([Figure 1-2](#ml_approach_diagram)).
    The program is much shorter, easier to maintain, and most likely more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the machine learning approach to spam filtering, showing
    steps: study the problem, train ML model, evaluate, analyze errors, and launch
    if successful.](assets/hmls_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. The machine learning approach
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What if spammers notice that all their emails containing “4U” are blocked? They
    might start writing “For U” instead. A spam filter using traditional programming
    techniques would need to be updated to flag “For U” emails. If spammers keep working
    around your spam filter, you will need to keep writing new rules forever.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a spam filter based on machine learning techniques automatically
    notices that “For U” has become unusually frequent in spam flagged by users, and
    it starts flagging them without your intervention ([Figure 1-3](#adapting_to_change_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing a machine learning model''s iterative process: training,
    evaluating, launching, and updating data, illustrating its ability to adapt automatically.](assets/hmls_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Automatically adapting to change
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another area where machine learning shines is for problems that either are too
    complex for traditional approaches or have no known algorithm. For example, consider
    speech recognition. Say you want to start simple and write a program capable of
    distinguishing the words “one” and “two”. You might notice that the word “two”
    starts with a high-pitch sound (“T”), so you could hardcode an algorithm that
    measures high-pitch sound intensity and use that to distinguish ones and twos⁠—but
    obviously this technique will not scale to thousands of words spoken by millions
    of very different people in noisy environments and in dozens of languages. The
    best solution (at least today) is to write an algorithm that learns by itself,
    given many example recordings for each word.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, machine learning can help humans learn ([Figure 1-4](#data_mining_diagram)).
    ML models can be inspected to see what they have learned (although for some models
    this can be tricky). For instance, once a spam filter has been trained on enough
    spam, it can easily be inspected to reveal the list of words and combinations
    of words that it believes are the best predictors of spam. Sometimes this will
    reveal unsuspected correlations or new trends, and thereby lead to a better understanding
    of the problem. Digging into large amounts of data to discover hidden patterns
    is called *data mining*, and machine learning excels at it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the process of using machine learning to solve problems,
    highlighting steps such as studying the problem, training models, inspecting solutions,
    and gaining better understanding through data mining.](assets/hmls_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Machine learning can help humans learn
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To summarize, machine learning is great for:'
  prefs: []
  type: TYPE_NORMAL
- en: Problems for which existing solutions require a lot of work and maintenance,
    such as long lists of rules (a machine learning model can often simplify code
    and perform better than the traditional approach)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex problems for which using a traditional approach yields no good solution
    (the best machine learning techniques can perhaps find a solution)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluctuating environments (a machine learning system can easily be retrained
    on new data, always keeping it up to date)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting insights about complex problems and large amounts of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s look at some concrete examples of machine learning tasks, along with
    the techniques that can tackle them:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing images of products on a production line to automatically classify
    them
  prefs: []
  type: TYPE_NORMAL
- en: This is image classification, typically performed using convolutional neural
    networks (CNNs; see [Chapter 12](ch12.html#cnn_chapter)) or vision transformers
    (see [Chapter 16](ch16.html#vit_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Detecting tumors in brain scans
  prefs: []
  type: TYPE_NORMAL
- en: This is semantic image segmentation, where each pixel in the image is classified
    (as we want to determine the exact location and shape of tumors), typically using
    CNNs or vision transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically classifying news articles
  prefs: []
  type: TYPE_NORMAL
- en: This is natural language processing (NLP), and more specifically text classification,
    which can be tackled using recurrent neural networks (RNNs) and CNNs, but transformers
    work even better (see [Chapter 15](ch15.html#transformer_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Automatically flagging offensive comments on discussion forums
  prefs: []
  type: TYPE_NORMAL
- en: This is also text classification, using the same NLP tools.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing long documents automatically
  prefs: []
  type: TYPE_NORMAL
- en: This is a branch of NLP called text summarization, again using the same tools.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating a person’s genetic risk for a given disease by analyzing a very long
    DNA sequence
  prefs: []
  type: TYPE_NORMAL
- en: Such a task requires discovering spread out patterns across very long sequences,
    which is where state space models (SSMs) particularly shine (see “State-Space
    Models (SSMs)” at [*https://homl.info*](https://homl.info)).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a chatbot or a personal assistant
  prefs: []
  type: TYPE_NORMAL
- en: This involves many NLP components, including natural language understanding
    (NLU) and question-answering modules.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting your company’s revenue next year, based on many performance metrics
  prefs: []
  type: TYPE_NORMAL
- en: This is a regression task (i.e., predicting values) that may be tackled using
    any regression model, such as a linear regression or polynomial regression model
    (see [Chapter 4](ch04.html#linear_models_chapter)), a regression support vector
    machine (see the online appendix on SVMs at [*https://homl.info*](https://homl.info)),
    a regression random forest (see [Chapter 6](ch06.html#ensembles_chapter)), or
    an artificial neural network (see [Chapter 9](ch09.html#ann_chapter)). If you
    want to take into account sequences of past performance metrics, you may want
    to use RNNs, CNNs, or transformers (see Chapters [13](ch13.html#rnn_chapter) to
    [15](ch15.html#transformer_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Making your app react to voice commands
  prefs: []
  type: TYPE_NORMAL
- en: This is speech recognition, which requires processing audio samples. Since they
    are long and complex sequences, they are typically processed using RNNs, CNNs,
    or transformers (see Chapters [13](ch13.html#rnn_chapter) to [15](ch15.html#transformer_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Detecting credit card fraud
  prefs: []
  type: TYPE_NORMAL
- en: This is anomaly detection, which can be tackled using isolation forests, Gaussian
    mixture models (see [Chapter 8](ch08.html#unsupervised_learning_chapter)), or
    autoencoders (see [Chapter 18](ch18.html#autoencoders_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting clients based on their purchases so that you can design a different
    marketing strategy for each segment
  prefs: []
  type: TYPE_NORMAL
- en: This is clustering, which can be achieved using *k*-means, DBSCAN, and more
    (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Representing a complex, high-dimensional dataset in a clear and insightful diagram
  prefs: []
  type: TYPE_NORMAL
- en: This is data visualization, often involving dimensionality reduction techniques
    (see [Chapter 7](ch07.html#dimensionality_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Recommending a product that a client may be interested in, based on past purchases
  prefs: []
  type: TYPE_NORMAL
- en: This is a recommender system. One approach is to feed past purchases (and other
    information about the client) to an artificial neural network (see [Chapter 9](ch09.html#ann_chapter)),
    and get it to output the most likely next purchase. This neural net would typically
    be trained on past sequences of purchases across all clients.
  prefs: []
  type: TYPE_NORMAL
- en: Building an intelligent bot for a game
  prefs: []
  type: TYPE_NORMAL
- en: This is often tackled using reinforcement learning (RL; see [Chapter 19](ch19.html#rl_chapter)),
    which is a branch of machine learning that trains agents (such as bots) to pick
    the actions that will maximize their rewards over time (e.g., a bot may get a
    reward every time the player loses some life points), within a given environment
    (such as the game). The famous AlphaGo program that beat the world champion at
    the game of Go was built using RL.
  prefs: []
  type: TYPE_NORMAL
- en: This list could go on and on, but hopefully it gives you a sense of the incredible
    breadth and complexity of the tasks that machine learning can tackle, and the
    types of techniques that you would use for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Machine Learning Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are so many different types of machine learning systems that it is useful
    to classify them in broad categories, based on the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: How they are guided during training (supervised, unsupervised, semi-supervised,
    self-supervised, and others)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not they can learn incrementally on the fly (online versus batch
    learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether they work by simply comparing new data points to known data points,
    or instead by detecting patterns in the training data and building a predictive
    model, much like scientists do (instance-based versus model-based learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These criteria are not exclusive; you can combine them in any way you like.
    For example, a state-of-the-art spam filter may learn on the fly using a deep
    neural network model trained using human-provided examples of spam and ham; this
    makes it an online, model-based, supervised learning system.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at each of these criteria a bit more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Training Supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML systems can be classified according to the amount and type of supervision
    they get during training. There are many categories, but we’ll discuss the main
    ones: supervised learning, unsupervised learning, self-supervised learning, semi-supervised
    learning, and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *supervised learning*, the training set you feed to the algorithm includes
    the desired solutions, called *labels* ([Figure 1-5](#supervised_learning_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating a labeled training set for spam classification, where
    emails are marked as spam or not spam, and this classification is applied to new
    emails.](assets/hmls_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. A labeled training set for spam classification (an example of supervised
    learning)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A typical supervised learning task is *classification*. The spam filter is
    a good example of this: it is trained with many example emails along with their
    *class* (spam or ham), and it must learn how to classify new emails.'
  prefs: []
  type: TYPE_NORMAL
- en: Another typical task is to predict a *target* numeric value, such as the price
    of a car, given a set of *features* (mileage, age, brand, etc.). This sort of
    task is called *regression* ([Figure 1-6](#regression_diagram)).⁠^([1](ch01.html#id789))
    To train the system, you need to give it many examples of cars, including both
    their features and their targets (i.e., their prices).
  prefs: []
  type: TYPE_NORMAL
- en: Note that some regression models can be used for classification as well, and
    vice versa. For example, *logistic regression* is commonly used for classification,
    as it can output a value that corresponds to the probability of belonging to a
    given class (e.g., 20% chance of being spam).
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plot illustrating a regression problem, showing a new instance on
    the x-axis labeled "Feature 1" with a question mark on determining its corresponding
    value on the y-axis.](assets/hmls_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-6\. A regression problem: predict a value, given an input feature
    (there are usually multiple input features, and sometimes multiple output values)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The words *target* and *label* are generally treated as synonyms in supervised
    learning, but *target* is more common in regression tasks and *label* is more
    common in classification tasks. Moreover, *features* are sometimes called *predictors*
    or *attributes*. These terms may refer to individual samples (e.g., “this car’s
    mileage feature is equal to 15,000”) or to all samples (e.g., “the mileage feature
    is strongly correlated with price”).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *unsupervised learning*, as you might guess, the training data is unlabeled.
    The system tries to learn without a teacher.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say you have a lot of data about your blog’s visitors. You may
    want to run a *clustering* algorithm to try to detect groups of similar visitors
    ([Figure 1-7](#clustering_diagram)). The features may include the user’s age group,
    their region, their interests, the duration of their sessions, and so on. At no
    point do you tell the algorithm which group a visitor belongs to: it finds those
    connections without your help. For example, it might notice that 40% of your visitors
    are teenagers who love comic books and generally read your blog after school,
    while 20% are adults who enjoy sci-fi and who visit during the weekends. If you
    use a *hierarchical clustering* algorithm, it may also subdivide each group into
    smaller groups. This may help you target your posts for each group.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram illustrating clustering, showing groups of similar data points
    represented by icons of people, separated by dashed lines along two features.](assets/hmls_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. Clustering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Visualization* algorithms are also good examples of unsupervised learning:
    you feed them a lot of complex and unlabeled data, and they output a 2D or 3D
    representation of your data that can easily be plotted ([Figure 1-8](#socher_ganjoo_manning_ng_2013_paper)).
    These algorithms try to preserve as much structure as they can (e.g., trying to
    keep separate clusters in the input space from overlapping in the visualization)
    so that you can understand how the data is organized and perhaps identify unsuspected
    patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: A related task is *dimensionality reduction*, in which the goal is to simplify
    the data without losing too much information. One way to do this is to merge several
    correlated features into one. For example, a car’s mileage may be strongly correlated
    with its age, so the dimensionality reduction algorithm will merge them into one
    feature that represents the car’s wear and tear. This is called *feature extraction*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is often a good idea to try to reduce the number of dimensions in your training
    data using a dimensionality reduction algorithm before you feed it to another
    machine learning algorithm (such as a supervised learning algorithm). It will
    run much faster, the data will take up less disk and memory space, and in some
    cases it may also perform better.
  prefs: []
  type: TYPE_NORMAL
- en: '![A t-SNE visualization showing semantic clusters of various categories such
    as animals and vehicles, highlighting how different groups are generally well-separated.](assets/hmls_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. Example of a t-SNE visualization highlighting semantic clusters⁠^([2](ch01.html#id802))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Yet another important unsupervised task is *anomaly detection*—for example,
    detecting unusual credit card transactions to prevent fraud, catching manufacturing
    defects, or automatically removing outliers from a dataset before feeding it to
    another learning algorithm. The system is shown mostly normal instances during
    training, so it learns to recognize them; then, when it sees a new instance, it
    can tell whether it looks like a normal one or whether it is likely an anomaly
    (see [Figure 1-9](#anomaly_detection_diagram)). The features may include distance
    from home, time of day, day of the week, amount withdrawn, merchant category,
    transaction frequency, etc. A very similar task is *novelty detection*: it aims
    to detect new instances that look different from all instances in the training
    set. This requires having a very “clean” training set, devoid of any instance
    that you would like the algorithm to detect. For example, if you have thousands
    of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a novelty
    detection algorithm should not treat new pictures of Chihuahuas as novelties.
    On the other hand, anomaly detection algorithms may consider these dogs as so
    rare and so different from other dogs that they would likely classify them as
    anomalies (no offense to Chihuahuas).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating anomaly detection with normal and anomaly instances
    plotted in a feature space, showing how new instances are classified.](assets/hmls_0109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. Anomaly detection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, another common unsupervised task is *association rule learning*, in
    which the goal is to dig into large amounts of data and discover interesting relations
    between attributes. For example, suppose you own a supermarket. Running an association
    rule on your sales logs may reveal that people who purchase barbecue sauce and
    potato chips also tend to buy steak. Thus, you may want to place these items close
    to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since labeling data is usually time-consuming and costly, you will often have
    plenty of unlabeled instances, and few labeled instances. Some algorithms can
    deal with data that’s partially labeled. This is called *semi-supervised learning*
    ([Figure 1-10](#semi_supervised_learning_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram illustrating semi-supervised learning with triangles and squares
    as labeled classes, and circles as unlabeled examples, showing how a new instance
    (cross) is more likely classified as a triangle through the influence of nearby
    unlabeled data.](assets/hmls_0110.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-10\. Semi-supervised learning with two classes (triangles and squares):
    the unlabeled examples (circles) help classify a new instance (the cross) into
    the triangle class rather than the square class, even though it is closer to the
    labeled squares'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some photo-hosting services, such as Google Photos, are good examples of this.
    Once you upload all your family photos to the service, it automatically recognizes
    that the same person A shows up in photos 1, 5, and 11, while another person B
    shows up in photos 2, 5, and 7\. This is the unsupervised part of the algorithm
    (clustering). Now all the system needs is for you to tell it who these people
    are. Just add one label per person⁠^([3](ch01.html#id812)) and it is able to name
    everyone in every photo, which is useful for searching photos.
  prefs: []
  type: TYPE_NORMAL
- en: Most semi-supervised learning algorithms are combinations of unsupervised and
    supervised algorithms. For example, a clustering algorithm may be used to group
    similar instances together, and then every unlabeled instance can be labeled with
    the most common label in its cluster. Once the whole dataset is labeled, it is
    possible to use any supervised learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach to machine learning involves actually generating a fully labeled
    dataset from a fully unlabeled one. Again, once the whole dataset is labeled,
    any supervised learning algorithm can be used. This approach is called *self-supervised
    learning*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have a large dataset of unlabeled images, you can randomly
    mask a small part of each image and then train a model to recover the original
    image ([Figure 1-11](#self_supervised_learning_diagram)). During training, the
    masked images are used as the inputs to the model, and the original images are
    used as the labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating self-supervised learning with a partially masked kitten
    image on the left as input and the complete image as the target on the right.](assets/hmls_0111.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-11\. Self-supervised learning example: input (left) and target (right)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The resulting model may be quite useful in itself—for example, to repair damaged
    images or to erase unwanted objects from pictures. But more often than not, a
    model trained using self-supervised learning is not the final goal. You’ll usually
    want to tweak and fine-tune the model for a slightly different task—one that you
    actually care about.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose that what you really want is to have a pet classification
    model: given a picture of any pet, it will tell you what species it belongs to.
    If you have a large dataset of unlabeled photos of pets, you can start by training
    an image-repairing model using self-supervised learning. Once it’s performing
    well, it should be able to distinguish different pet species: when it repairs
    an image of a cat whose face is masked, it must know not to add a dog’s face.
    Assuming your model’s architecture allows it (and most neural network architectures
    do), it is then possible to tweak the model so that it predicts pet species instead
    of repairing images. The final step consists of fine-tuning the model on a labeled
    dataset: the model already knows what cats, dogs, and other pet species look like,
    so this step is only needed so the model can learn the mapping between the species
    it already knows and the labels we expect from it.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transferring knowledge from one task to another is called *transfer learning*,
    and it’s one of the most important techniques in machine learning today, especially
    when using *deep neural networks* (i.e., neural networks composed of many layers
    of neurons). We will discuss this in detail in [Part II](part02.html#neural_nets_part).
  prefs: []
  type: TYPE_NORMAL
- en: As we will see in [Chapter 15](ch15.html#transformer_chapter), large language
    models (LLMs) are trained in a very similar way, by masking random words in a
    huge text corpus and training the model to predict the missing words. This large
    pretrained model can then be fine-tuned for various applications, from sentiment
    analysis to chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some people consider self-supervised learning to be a part of unsupervised
    learning, since it deals with fully unlabeled datasets. But self-supervised learning
    uses (generated) labels during training, so in that regard it’s closer to supervised
    learning. And the term “unsupervised learning” is generally used when dealing
    with tasks like clustering, dimensionality reduction, or anomaly detection, whereas
    self-supervised learning focuses on the same tasks as supervised learning: mainly
    classification and regression. In short, it’s best to treat self-supervised learning
    as its own category.'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Reinforcement learning* is a very different beast. The learning system, called
    an *agent* in this context, can observe the environment, select and perform actions,
    and get *rewards* in return (or *penalties* in the form of negative rewards, as
    shown in [Figure 1-12](#reinforcement_learning_diagram)). It must then learn by
    itself what is the best strategy, called a *policy*, to get the most reward over
    time. A policy defines what action the agent should choose when it is in a given
    situation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating reinforcement learning, where an agent interacts with
    its environment, selects actions, and receives rewards or penalties, leading to
    policy updates and learning.](assets/hmls_0112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-12\. Reinforcement learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For example, many robots implement reinforcement learning algorithms to learn
    how to walk. DeepMind’s AlphaGo program is also a good example of reinforcement
    learning: it made the headlines in May 2017 when it beat Ke Jie, the number one
    ranked player in the world at the time, at the game of Go. It learned its winning
    policy by analyzing millions of games, and then playing many games against itself.
    Note that learning was turned off during the games against the champion; AlphaGo
    was just applying the policy it had learned. As you will see in the next section,
    this is called *offline learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch Versus Online Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another criterion used to classify machine learning systems is whether the system
    can learn incrementally from a stream of incoming data. For example, random forests
    (see [Chapter 6](ch06.html#ensembles_chapter)) can only be trained from scratch
    on the full dataset—this is called batch learning—while other models can be trained
    one batch of data at a time, for example, using *gradient descent* (see [Chapter 4](ch04.html#linear_models_chapter))—this
    is called online learning.
  prefs: []
  type: TYPE_NORMAL
- en: Batch learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *batch learning*, the system must be trained using all the available data.
    This will generally take a lot of time and computing resources, so it is typically
    done offline. First the system is trained, and then it is launched into production
    and runs without learning anymore; it just applies what it has learned. This is
    called *offline learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, a model’s performance tends to decay slowly over time, simply
    because the world continues to evolve while the model remains unchanged. This
    phenomenon is often called *data drift* (or *model rot*). The solution is to regularly
    retrain the model on up-to-date data. How often you need to do that depends on
    the use case: if the model classifies pictures of cats and dogs, its performance
    will decay very slowly, but if the model deals with fast-evolving systems, for
    example making predictions on the financial market, then it is likely to decay
    quite fast.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even a model trained to classify pictures of cats and dogs may need to be retrained
    regularly, not because cats and dogs will mutate overnight, but because cameras
    keep changing, along with image formats, sharpness, brightness, and size ratios.
    Moreover, people may love different breeds next year, or they may decide to dress
    their pets with tiny hats—who knows?
  prefs: []
  type: TYPE_NORMAL
- en: If you want a batch learning system to know about new data (such as a new type
    of spam), you need to train a new version of the system from scratch on the full
    dataset (not just the new data, but also the old data), then replace the old model
    with the new one. Fortunately, the whole process of training, evaluating, and
    launching a machine learning system can be automated (as we saw in [Figure 1-3](#adapting_to_change_diagram)),
    so even a batch learning system can adapt to change. Simply update the data and
    train a new version of the system from scratch as often as needed.
  prefs: []
  type: TYPE_NORMAL
- en: This solution is simple and often works fine, but training using the full set
    of data can take many hours, so you would typically train a new system only every
    24 hours or even just weekly. If your system needs to adapt to rapidly changing
    data (e.g., to predict stock prices), then you need a more reactive solution.
  prefs: []
  type: TYPE_NORMAL
- en: Also, training on the full set of data requires a lot of computing resources
    (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot
    of data and you automate your system to train from scratch every day, it will
    end up costing you a lot of money. If the amount of data is huge and your system
    must always be up to date, it may even be impossible to use batch learning.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if your system needs to be able to learn autonomously and it has limited
    resources (e.g., a smartphone application or a rover on Mars), then carrying around
    large amounts of training data and taking up a lot of resources to train for hours
    every day is a showstopper.
  prefs: []
  type: TYPE_NORMAL
- en: A better option in all these scenarios is to use algorithms that are capable
    of learning incrementally.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *online learning*, you train the system incrementally by feeding it data
    instances sequentially, either individually or in small groups called *mini-batches*.
    Each learning step is fast and cheap, so the system can learn about new data on
    the fly, as it arrives (see [Figure 1-13](#online_learning_diagram)). The most
    common online algorithm by far is gradient descent, but there are a few others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the online learning process, showing model training,
    evaluation, launching, and continuous learning with new data.](assets/hmls_0113.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-13\. In online learning, a model is trained and launched into production,
    and then it keeps learning as new data comes in
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Online learning is useful for systems that need to adapt to change extremely
    rapidly (e.g., to detect new patterns in the stock market). It is also a good
    option if you have limited computing resources; for example, if the model is trained
    on a mobile device.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, online learning algorithms can be used to train models on
    huge datasets that cannot fit in one machine’s memory (this is called *out-of-core*
    learning). The algorithm loads part of the data, runs a training step on that
    data, and repeats the process until it has run on all of the data (see [Figure 1-14](#ol_for_huge_datasets_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing the process of online learning for large datasets, involving
    data partitioning, training, evaluating, analyzing errors, and launching the model.](assets/hmls_0114.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-14\. Using online learning to handle huge datasets
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One important parameter of online learning systems is how fast they should
    adapt to changing data: this is called the *learning rate*. If you set a high
    learning rate, then your system will rapidly adapt to new data, but it will also
    tend to quickly forget the old data: this is called *catastrophic forgetting*
    (or *catastrophic interference*). You don’t want a spam filter to flag only the
    latest kinds of spam it was shown! Conversely, if you set a low learning rate,
    the system will have more inertia; that is, it will learn more slowly, but it
    will also be less sensitive to noise in the new data or to sequences of nonrepresentative
    data points (outliers).'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Out-of-core learning is usually done offline (i.e., not on the live system),
    so *online learning* can be a confusing name. Think of it as *incremental learning*.
    Moreover, mini-batches are often just called “batches”, so *batch learning* is
    also a confusing name. Think of it as learning from scratch on the full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A big challenge with online learning is that if bad data is fed to the system,
    the system’s performance will decline, possibly quickly (depending on the data
    quality and learning rate). If it’s a live system, your clients will notice. For
    example, bad data could come from a bug (e.g., a malfunctioning sensor on a robot),
    or it could come from someone trying to game the system (e.g., spamming a search
    engine to try to rank high in search results). To reduce this risk, you need to
    monitor your system closely and promptly switch learning off (and possibly revert
    to a previously working state) if you detect a drop in performance. You may also
    want to monitor the input data and react to abnormal data; for example, using
    an anomaly detection algorithm (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Instance-Based Versus Model-Based Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One more way to categorize machine learning systems is by how they *generalize*.
    Most machine learning tasks are about making predictions. This means that given
    a number of training examples, the system needs to be able to make good predictions
    for (generalize to) examples it has never seen before. Having a good performance
    measure on the training data is good, but insufficient; the true goal is to perform
    well on new instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main approaches to generalization: instance-based learning and
    model-based learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Instance-based learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Possibly the most trivial form of learning is simply to learn by heart. If you
    were to create a spam filter this way, it would just flag all emails that are
    identical to emails that have already been flagged by users—not the worst solution,
    but certainly not the best.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of just flagging emails that are identical to known spam emails, your
    spam filter could be programmed to also flag emails that are very similar to known
    spam emails. This requires a *measure of similarity* between two emails. A (very
    basic) similarity measure between two emails could be to count the number of words
    they have in common. The system would flag an email as spam if it has many words
    in common with a known spam email.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is called *instance-based learning*: the system learns the examples by
    heart, then generalizes to new cases by using a similarity measure to compare
    them to the learned examples (or a subset of them). For example, in [Figure 1-15](#instance_based_learning_diagram)
    the new instance would be classified as a triangle because the majority of the
    most similar instances belong to that class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance-based learning often shines with small datasets, especially if the
    data keeps changing, but it does not scale very well: it requires deploying a
    whole copy of the training set to production; making predictions requires searching
    for similar instances, which can be quite slow; and it doesn’t work well with
    high-dimensional data such as images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating instance-based learning, showing a new instance and
    its three nearest neighbors among training instances with two features.](assets/hmls_0115.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-15\. Instance-based learning: in this example we consider the class
    of the three nearest neighbors in the training set'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model-based learning and a typical machine learning workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to generalize from a set of examples is to build a model of these
    examples and then use that model to make *predictions*. This is called *model-based
    learning* ([Figure 1-16](#model_based_learning_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating model-based learning with a decision boundary separating
    two classes of data points represented as triangles and squares based on two features.](assets/hmls_0116.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-16\. Model-based learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, suppose you want to know if money makes people happy, so you download
    the Better Life Index data from the [OECD’s website](https://www.oecdbetterlifeindex.org),
    and [World Bank stats](https://ourworldindata.org) about gross domestic product
    (GDP) per capita. Then you join the tables and sort by GDP per capita. [Table 1-1](#life_satisfaction_table_excerpt)
    shows an excerpt of what you get.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. Does money make people happier?
  prefs: []
  type: TYPE_NORMAL
- en: '| Country | GDP per capita (USD) | Life satisfaction |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Turkey | 28,384 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Hungary | 31,008 | 5.6 |'
  prefs: []
  type: TYPE_TB
- en: '| France | 42,026 | 6.5 |'
  prefs: []
  type: TYPE_TB
- en: '| United States | 60,236 | 6.9 |'
  prefs: []
  type: TYPE_TB
- en: '| New Zealand | 42,404 | 7.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Australia | 48,698 | 7.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Denmark | 55,938 | 7.6 |'
  prefs: []
  type: TYPE_TB
- en: Let’s plot the data for these countries ([Figure 1-17](#money_happy_scatterplot)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatterplot showing the relationship between GDP per capita and life satisfaction
    for various countries, indicating a positive trend.](assets/hmls_0117.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-17\. Do you see a trend here?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There does seem to be a trend here! Although the data is *noisy* (i.e., partly
    random), it looks like life satisfaction goes up more or less linearly as the
    country’s GDP per capita increases. So you decide to model life satisfaction as
    a linear function of GDP per capita (you assume that any deviation from that line
    is just random noise). This step is called *model selection*: you selected a *linear
    model* of life satisfaction with just one attribute, GDP per capita ([Equation
    1-1](#a_simple_linear_model)).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 1-1\. A simple linear model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><mtext>life_satisfaction</mtext> <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>×</mo> <mtext>GDP_per_capita</mtext></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: This model has two *model parameters*, *θ*[0] and *θ*[1].⁠^([4](ch01.html#id844))
    By tweaking these parameters, you can make your model represent any linear function,
    as shown in [Figure 1-18](#tweaking_model_params_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing possible linear models with different parameter values predicting
    life satisfaction based on GDP per capita.](assets/hmls_0118.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-18\. A few possible linear models
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before you can use your model, you need to define the parameter values *θ*[0]
    and *θ*[1]. How can you know which values will make your model perform best? To
    answer this question, you need to specify a performance measure. You can either
    define a *utility function* (or *fitness function*) that measures how *good* your
    model is, or you can define a *cost function* (a.k.a., *loss function*) that measures
    how *bad* it is. For linear regression problems, people typically use a cost function
    that measures the distance between the linear model’s predictions and the training
    examples; the objective is to minimize this distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the linear regression algorithm comes in: you feed it your training
    examples, and it finds the parameters that make the linear model fit best to your
    data. This is called *training* the model. In our case, the algorithm finds that
    the optimal parameter values are *θ*[0] = 3.75 and *θ*[1] = 6.78 × 10^(–5).'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Confusingly, the word “model” can refer to a *type of model* (e.g., linear regression),
    to a *fully specified model architecture* (e.g., linear regression with one input
    and one output), or to the *final trained model* ready to be used for predictions
    (e.g., linear regression with one input and one output, using *θ*[0] = 3.75 and
    *θ*[1] = 6.78 × 10^(–5)). Model selection consists in choosing the type of model
    and fully specifying its architecture. Training a model means running an algorithm
    to find the model parameters that will make it best fit the training data, and
    hopefully make good predictions on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Now the model fits the training data as closely as possible (for a linear model),
    as you can see in [Figure 1-19](#best_fit_model_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plot with a line of best fit showing the relationship between GDP
    per capita (USD) and life satisfaction, with model parameters θ₀ = 3.75 and θ₁
    = 6.78 × 10⁻⁵.](assets/hmls_0119.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-19\. The linear model that fits the training data best
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You are finally ready to run the model to make predictions. For example, say
    you want to know how happy Puerto Ricans are, and the OECD data does not have
    the answer. Fortunately, you can use your model to make a good prediction: you
    look up Puerto Rico’s GDP per capita, find $33,442, and then apply your model
    and find that life satisfaction is likely to be somewhere around 3.75 + 33,442
    × 6.78 × 10^(–5) = 6.02.'
  prefs: []
  type: TYPE_NORMAL
- en: To whet your appetite, [Example 1-1](#example_scikit_code) shows the Python
    code that loads the data, separates the inputs `X` from the labels `y`, creates
    a scatterplot for visualization, and then trains a linear model and makes a prediction.⁠^([5](ch01.html#id850))
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-1\. Training and running a linear model using Scikit-Learn
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you had used an instance-based learning algorithm instead, you would have
    found that Poland has the closest GDP per capita to that of Puerto Rico ($32,238),
    and since the OECD data tells us that Poles’ life satisfaction is 6.1, you would
    have predicted a life satisfaction of 6.1 as well for Puerto Rico. If you zoom
    out a bit and look at the next two closest countries, you will find Portugal with
    a life satisfaction of 5.4, and Estonia with a life satisfaction of 5.7\. Averaging
    these three values, you get 5.73, which is a bit below your model-based prediction.
    This simple algorithm is called *k-nearest neighbors* regression (in this example,
    *k* = 3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Replacing the linear regression model with *k*-nearest neighbors regression
    in the previous code is as easy as replacing these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'with these two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If all went well, your model will make good predictions. If not, you may need
    to use more attributes (employment rate, health, air pollution, etc.), get more
    or better-quality training data, or perhaps select a more powerful model (e.g.,
    a polynomial regression model).
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary:'
  prefs: []
  type: TYPE_NORMAL
- en: You studied the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You selected a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You trained it on the training data (i.e., the learning algorithm searched for
    the model parameter values that minimize a cost function).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you applied the model to make predictions on new cases (this is called
    *inference*), hoping that this model will generalize well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is what a typical machine learning project looks like. In [Chapter 2](ch02.html#project_chapter)
    you will experience this firsthand by going through a project end to end.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed quite a few categories of ML systems, but this field has more!
    For example, *ensemble learning* involves training multiple models and combining
    their individual predictions into improved predictions (see [Chapter 6](ch06.html#ensembles_chapter));
    *federated learning* is a decentralized approach where models are trained across
    multiple devices (e.g., smartphones) and adapted to each user without exchanging
    raw data, thereby protecting the user’s privacy; *meta-learning* is a learning-to-learn
    approach where models learn how to learn new tasks quickly with minimal data.
    And the list goes on! [Figure 1-20](#ml_categories_diagram) summarizes the various
    classifications of ML systems we have discussed so far.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing categories of machine learning, including learning paradigms,
    tasks, training methods, modeling approaches, and other types like ensemble, federated,
    and meta-learning.](assets/hmls_0120.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-20\. Overview of ML categories
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have covered a lot of ground so far: you now know what machine learning
    is really about, why it is useful, what some of the most common categories of
    ML systems are, and what a typical project workflow looks like. Now let’s look
    at what can go wrong in learning and prevent you from making accurate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Main Challenges of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In short, since your main task is to select a model and train it on some data,
    the two things that can go wrong are “bad model” and “bad data”. Let’s start with
    examples of bad data.
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient Quantity of Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a toddler to learn what an apple is, all it takes is for you to point to
    an apple and say “apple” (possibly repeating this procedure a few times). Now
    the child is able to recognize apples in all sorts of colors and shapes. Genius.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is not quite there yet; it takes a lot of data for most machine
    learning algorithms to work properly. Even for very simple problems you typically
    need thousands of examples, and for complex problems such as image or speech recognition
    you may need millions of examples (unless you can reuse parts of an existing model,
    i.e., transfer learning).
  prefs: []
  type: TYPE_NORMAL
- en: Nonrepresentative Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to generalize well, it is crucial that your training data be representative
    of the new cases you want to generalize to. This is true whether you use instance-based
    learning or model-based learning.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the set of countries you used earlier for training the linear model
    was not perfectly representative; it did not contain any country with a GDP per
    capita lower than $23,500 or higher than $62,500\. [Figure 1-22](#representative_training_data_scatterplot)
    shows what the data looks like when you add such countries.
  prefs: []
  type: TYPE_NORMAL
- en: If you train a linear model on this data, you get the solid line, while the
    old model is represented by the dotted line. As you can see, not only does adding
    a few missing countries significantly alter the model, but it makes it clear that
    such a simple linear model is probably never going to work well. It seems that
    very rich countries are not happier than moderately rich countries (in fact, they
    seem slightly unhappier!), and conversely some poor countries seem happier than
    many rich countries.
  prefs: []
  type: TYPE_NORMAL
- en: By using a nonrepresentative training set, you trained a model that is unlikely
    to make accurate predictions, especially for very poor and very rich countries.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is crucial to use a training set that is representative of the cases you
    want to generalize to. This is often harder than it sounds: if the sample is too
    small, you will have *sampling noise* (i.e., nonrepresentative data as a result
    of chance), but even very large samples can be nonrepresentative if the sampling
    method is flawed. This is called *sampling bias*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A scatter plot showing life satisfaction versus GDP per capita, highlighting
    countries like Colombia, Brazil, Norway, and Luxembourg, to illustrate sampling
    bias and the importance of a representative training set.](assets/hmls_0122.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-22\. A more representative training sample
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Poor-Quality Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Obviously, if your training data is full of errors, outliers, and noise (e.g.,
    due to poor-quality measurements), it will make it harder for the system to detect
    the underlying patterns, so your system is less likely to perform well. It is
    often well worth the effort to spend time cleaning up your training data. The
    truth is, most data scientists spend a significant part of their time doing just
    that. The following are a couple examples of when you’d want to clean up training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: If some instances are clearly outliers, it may help to simply discard them or
    try to fix the errors manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some instances are missing a few features (e.g., 5% of your customers did
    not specify their age), you must decide whether you want to ignore this attribute
    altogether, ignore these instances, fill in the missing values (e.g., with the
    median age), or train one model with the feature and one model without it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irrelevant Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the saying goes: garbage in, garbage out. Your system will only be capable
    of learning if the training data contains enough relevant features and not too
    many irrelevant ones. A critical part of the success of a machine learning project
    is coming up with a good set of features to train on. This process, called *feature
    engineering*, involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature selection* (selecting the most useful features to train on among existing
    features)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feature extraction* (combining existing features to produce a more useful
    one⁠—as we saw earlier, dimensionality reduction algorithms can help)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new features by gathering new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have looked at many examples of bad data, let’s look at a couple
    examples of bad algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting the Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Say you are visiting a foreign country and the taxi driver rips you off. You
    might be tempted to say that *all* taxi drivers in that country are thieves. Overgeneralizing
    is something that we humans do all too often, and unfortunately machines can fall
    into the same trap if we are not careful. In machine learning this is called *overfitting*:
    it means that the model performs well on the training data, but it does not generalize
    well.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-23](#overfitting_model_plot) shows an example of a high-degree polynomial
    life satisfaction model that strongly overfits the training data. Even though
    it performs much better on the training data than the simple linear model, would
    you really trust its predictions?'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph depicting a high-degree polynomial model that overfits life satisfaction
    data based on GDP per capita, illustrating poor generalization beyond the training
    set.](assets/hmls_0123.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-23\. Overfitting the training data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Complex models such as deep neural networks can detect subtle patterns in the
    data, but if the training set is noisy, or if it is too small, which introduces
    sampling noise, then the model is likely to detect patterns in the noise itself
    (as in the taxi driver example). Obviously these patterns will not generalize
    to new instances. For example, say you feed your life satisfaction model many
    more attributes, including uninformative ones such as the country’s name. In that
    case, a complex model may detect patterns like the fact that all countries in
    the training data with a *w* in their name have a life satisfaction greater than
    7: New Zealand (7.3), Norway (7.6), Sweden (7.3), and Switzerland (7.5). How confident
    are you that the *w*-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously
    this pattern occurred in the training data by pure chance, but the model has no
    way to tell whether a pattern is real or simply the result of noise in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Overfitting happens when the model is too complex relative to the amount and
    noisiness of the training data, so it starts to learn random patterns in the training
    data. Here are possible solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplify the model by selecting one with fewer parameters (e.g., a linear model
    rather than a high-degree polynomial model), by reducing the number of attributes
    in the training data, or by constraining the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gather more training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the noise in the training data (e.g., fix data errors and remove outliers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Constraining a model to make it simpler and reduce the risk of overfitting
    is called *regularization*. For example, the linear model we defined earlier has
    two parameters, *θ*[0] and *θ*[1]. This gives the learning algorithm two *degrees
    of freedom* to adapt the model to the training data: it can tweak both the height
    (*θ*[0]) and the slope (*θ*[1]) of the line. If we forced *θ*[1] = 0, the algorithm
    would have only one degree of freedom and would have a much harder time fitting
    the data properly: all it could do is move the line up or down to get as close
    as possible to the training instances, so it would end up around the mean. A very
    simple model indeed! If we allow the algorithm to modify *θ*[1] but we force it
    to keep it small, then the learning algorithm will effectively have somewhere
    in between one and two degrees of freedom. It will produce a model that’s simpler
    than one with two degrees of freedom, but more complex than one with just one.
    You want to find the right balance between fitting the training data perfectly
    and keeping the model simple enough to ensure that it will generalize well.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-24](#ridge_model_plot) shows three models. The dotted line represents
    the original model that was trained on the countries represented as circles (without
    the countries represented as squares), the solid line is our second model trained
    with all countries (circles and squares), and the dashed line is a model trained
    with the same data as the first model but with a regularization constraint. You
    can see that regularization forced the model to have a smaller slope: this model
    does not fit the training data (circles) as well as the first model, but it actually
    generalizes better to new examples that it did not see during training (squares).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A plot showing three linear regression models on life satisfaction versus
    GDP per capita, illustrating how regularization affects model fitting and generalization
    to unseen data.](assets/hmls_0124.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-24\. Regularization reduces the risk of overfitting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The amount of regularization to apply during learning can be controlled by a
    *hyperparameter*. A hyperparameter is a parameter of a learning algorithm (not
    of the model). As such, it is not affected by the learning algorithm itself; it
    must be set prior to training and remains constant during training. If you set
    the regularization hyperparameter to a very large value, you will get an almost
    flat model (a slope close to zero); the learning algorithm will almost certainly
    not overfit the training data, but it will be less likely to find a good solution.
    Tuning hyperparameters is an important part of building a machine learning system
    (you will see a detailed example in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting the Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you might guess, *underfitting* is the opposite of overfitting: it occurs
    when your model is too simple to learn the underlying structure of the data. For
    example, a linear model of life satisfaction is prone to underfit; reality is
    just more complex than the model, so its predictions are bound to be inaccurate,
    even on the training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main options for fixing this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a more powerful model, with more parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed better features to the learning algorithm (feature engineering).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the constraints on the model (for example by reducing the regularization
    hyperparameter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even if you have a large and clean dataset and you manage to train a beautiful
    model that neither underfits nor overfits the data, you may still run into issues
    during deployment: for example, the model may be too complex to maintain, or too
    large to fit in memory, or too slow, or it may not scale properly, it may have
    security vulnerabilities, it may become outdated if you don’t update it often
    enough, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, there’s more to an ML project than just data and models. However,
    the skillset required to handle these operational problems are fairly different
    from those required for data modeling, which is why companies often have a dedicated
    MLOps team (ML operations) to handle this.
  prefs: []
  type: TYPE_NORMAL
- en: Stepping Back
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By now you know a lot about machine learning. However, we went through so many
    concepts that you may be feeling a little lost, so let’s step back and look at
    the big picture:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is about making machines get better at some task by learning
    from data, instead of having to explicitly code rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many different types of ML systems: supervised or not, batch or online,
    instance-based or model-based.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an ML project you gather data in a training set, and you feed the training
    set to a learning algorithm. If the algorithm is model-based, it tunes some parameters
    to fit the model to the training set (i.e., to make good predictions on the training
    set itself), and then hopefully it will be able to make good predictions on new
    cases as well. If the algorithm is instance-based, it just learns the examples
    by heart and generalizes to new instances by using a similarity measure to compare
    them to the learned instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system will not perform well if your training set is too small, or if the
    data is not representative, is noisy, or is polluted with irrelevant features
    (garbage in, garbage out). Your model needs to be neither too simple (in which
    case it will underfit) nor too complex (in which case it will overfit). Lastly,
    you must think carefully about deployment constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There’s just one last important topic to cover: once you have trained a model,
    you don’t want to just “hope” it generalizes to new cases. You want to evaluate
    it and fine-tune it if necessary. Let’s see how to do that.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing and Validating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The only way to know how well a model will generalize to new cases is to actually
    try it out on new cases. One way to do that is to put your model in production
    and monitor how well it performs. This works well, but if your model is horribly
    bad, your users will complain—not the best idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better option is to split your data into two sets: the *training set* and
    the *test set*. As these names imply, you train your model using the training
    set, and you test it using the test set. The error rate on new cases is called
    the *generalization error* (or *out-of-sample error*), and by evaluating your
    model on the test set, you get an estimate of this error. This value tells you
    how well your model will perform on instances it has never seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: If the training error is low (i.e., your model makes few mistakes on the training
    set) but the generalization error is high, it means that your model is overfitting
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is common to use 80% of the data for training and *hold out* 20% for testing.
    However, this depends on the size of the dataset: if it contains 10 million instances,
    then holding out 1% means your test set will contain 100,000 instances, probably
    more than enough to get a good estimate of the generalization error.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning and Model Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating a model is simple enough: just use a test set. But suppose you are
    hesitating between two types of models (say, a linear model and a polynomial model):
    how can you decide between them? One option is to train both and compare how well
    they generalize using the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that the linear model generalizes better, but you want to apply
    some regularization to avoid overfitting. The question is, how do you choose the
    value of the regularization hyperparameter? One option is to train 100 different
    models using 100 different values for this hyperparameter. Suppose you find the
    best hyperparameter value that produces a model with the lowest generalization
    error⁠—say, just 5% error. You launch this model into production, but unfortunately
    it does not perform as well as expected and produces 15% errors. What just happened?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that you measured the generalization error multiple times on
    the test set, and you adapted the model and hyperparameters to produce the best
    model *for that particular set*. This means the model is unlikely to perform as
    well on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common solution to this problem is called *holdout validation* ([Figure 1-25](#hyperparameter_tuning_diagram)):
    you simply hold out part of the training set to evaluate several candidate models
    and select the best one. The new held-out set is called the *validation set* (or
    the *development set*, or *dev set*). More specifically, you train multiple models
    with various hyperparameters on the reduced training set (i.e., the full training
    set minus the validation set), and you select the model that performs best on
    the validation set. After this holdout validation process, you train the best
    model on the full training set (including the validation set), and this gives
    you the final model. Lastly, you evaluate this final model on the test set to
    get an estimate of the generalization error.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating holdout validation for model selection, showing steps
    from training multiple models on a training set, evaluating them on a dev set,
    retraining the best model, and evaluating the final model on a test set.](assets/hmls_0125.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-25\. Model selection using holdout validation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This solution usually works quite well. However, if the validation set is too
    small, then the model evaluations will be imprecise: you may end up selecting
    a suboptimal model by mistake. Conversely, if the validation set is too large,
    then the remaining training set will be much smaller than the full training set.
    Why is this bad? Well, since the final model will be trained on the full training
    set, it is not ideal to compare candidate models trained on a much smaller training
    set. It would be like selecting the fastest sprinter to participate in a marathon.
    One way to solve this problem is to perform repeated *cross-validation*, using
    many small validation sets. Each model is evaluated once per validation set after
    it is trained on the rest of the data. By averaging out all the evaluations of
    a model, you get a much more accurate measure of its performance. There is a drawback,
    however: the training time is multiplied by the number of validation sets.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Mismatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, it’s easy to get a large amount of data for training, but this
    data probably won’t be perfectly representative of the data that will be used
    in production. For example, suppose you want to create a mobile app to take pictures
    of flowers and automatically determine their species. You can easily download
    millions of pictures of flowers on the web, but they won’t be perfectly representative
    of the pictures that will actually be taken using the app on a mobile device.
    Perhaps you only have 1,000 representative pictures (i.e., actually taken with
    the app).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the most important rule to remember is that both the validation
    set and the test set must be as representative as possible of the data you expect
    to use in production, so they should be composed exclusively of representative
    pictures: you can shuffle them and put half in the validation set and half in
    the test set (making sure that no duplicates or near-duplicates end up in both
    sets). After training your model on the web pictures, if you observe that the
    performance of the model on the validation set is disappointing, you will not
    know whether this is because your model has overfit the training set, or whether
    this is just due to the mismatch between the web pictures and the mobile app pictures.'
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to hold out some of the training pictures (from the web) in
    yet another set that Andrew Ng dubbed the *train-dev set* ([Figure 1-26](#train_dev_diagram)).
    After the model is trained (on the training set, *not* on the train-dev set),
    you can evaluate it on the train-dev set. If the model performs poorly, then it
    must have overfit the training set, so you should try to simplify or regularize
    the model, get more training data, and clean up the training data. But if it performs
    well on the train-dev set, then you can evaluate the model on the dev set. If
    it performs poorly, then the problem must be coming from the data mismatch. You
    can try to tackle this problem by preprocessing the web images to make them look
    more like the pictures that will be taken by the mobile app, and then retraining
    the model. Once you have a model that performs well on both the train-dev set
    and the dev set, you can evaluate it one last time on the test set to know how
    well it is likely to perform in production.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing abundant training data on the left divided into "Train" and
    "Train-dev" sets, with scarcer real data on the right used for "Dev" and "Test"
    sets to address overfitting and data mismatch.](assets/hmls_0126.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-26\. When real data is scarce (right), you may use similar abundant
    data (left) for training and hold out some of it in a train-dev set to evaluate
    overfitting; the real data is then used to evaluate data mismatch (dev set) and
    to evaluate the final model’s performance (test set)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we have covered some of the most important concepts in machine
    learning. In the next chapters we will dive deeper and write more code, but before
    we do, make sure you can answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How would you define machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name four types of applications where it shines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a labeled training set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the two most common supervised tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name four common unsupervised tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of algorithm would you use to allow a robot to walk in various unknown
    terrains?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of algorithm would you use to segment your customers into multiple
    groups?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Would you frame the problem of spam detection as a supervised learning problem
    or an unsupervised learning problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an online learning system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is out-of-core learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of algorithm relies on a similarity measure to make predictions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a model parameter and a model hyperparameter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do model-based algorithms search for? What is the most common strategy
    they use to succeed? How do they make predictions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name four of the main challenges in machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your model performs great on the training data but generalizes poorly to
    new instances, what is happening? Can you name three possible solutions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a test set, and why would you want to use it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of a validation set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the train-dev set, when do you need it, and how do you use it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can go wrong if you tune hyperparameters using the test set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch01.html#id789-marker)) Fun fact: this odd-sounding name is a statistics
    term introduced by Francis Galton while he was studying the fact that the children
    of tall people tend to be shorter than their parents. Since the children were
    shorter, he called this *regression to the mean*. This name was then applied to
    the methods he used to analyze correlations between variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch01.html#id802-marker)) Notice how animals are rather well separated
    from vehicles and how horses are close to deer but far from birds. Figure reproduced
    with permission from Richard Socher et al., “Zero-Shot Learning Through Cross-Modal
    Transfer”, *Proceedings of the 26th International Conference on Neural Information
    Processing Systems* 1 (2013): 935–943.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch01.html#id812-marker)) That’s when the system works perfectly. In practice
    it often creates a few clusters per person, and sometimes mixes up two people
    who look alike, so you may need to provide a few labels per person and manually
    clean up some clusters.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch01.html#id844-marker)) By convention, the Greek letter *θ* (theta) is
    frequently used to represent model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch01.html#id850-marker)) It’s OK if you don’t understand all the code
    yet; I will present Scikit-Learn in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch01.html#id866-marker)) For example, knowing whether to write “to”, “two”,
    or “too”, depending on the context.
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch01.html#id867-marker)) Figure reproduced with permission from Michele
    Banko and Eric Brill, “Scaling to Very Very Large Corpora for Natural Language
    Disambiguation”, *Proceedings of the 39th Annual Meeting of the Association for
    Computational Linguistics* (2001): 26–33.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch01.html#id868-marker)) Peter Norvig et al., “The Unreasonable Effectiveness
    of Data”, *IEEE Intelligent Systems* 24, no. 2 (2009): 8–12.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch01.html#id987-marker)) David Wolpert, “The Lack of A Priori Distinctions
    Between Learning Algorithms”, *Neural Computation* 8, no. 7 (1996): 1341–1390.'
  prefs: []
  type: TYPE_NORMAL
