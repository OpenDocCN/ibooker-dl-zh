- en: Chapter 7\. Suggesting Emojis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ll build a model to suggest emojis given a small piece of
    text. We’ll start by developing a simple sentiment classifier based on a public
    set of tweets labeled with various sentiments, like happiness, love, surprise,
    etc. We’ll first try a Bayesian classifier to get an idea of the baseline performance
    and take a look at what this classifier can learn. We’ll then switch to a convolutional
    network and look at various ways to tune this classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll look at how we can harvest tweets using the Twitter API ourselves,
    and then we’ll apply the convolutional model from [Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis)
    before moving on to a word-level model. We’ll then construct and apply a recurrent
    word-level network, and compare the three different models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll combine all three models into an ensemble model that outperforms
    any of the three.
  prefs: []
  type: TYPE_NORMAL
- en: The final model does a very decent job and just needs to be rolled into a mobile
    app!
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found in these notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 7.1 Building a Simple Sentiment Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you determine the sentiment expressed in a piece of text?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Find a dataset consisting of sentences where the sentiment is labeled and run
    a simple classifier over them.
  prefs: []
  type: TYPE_NORMAL
- en: Before trying something complicated, it is a good idea to first try the simplest
    thing we can think of on a dataset that is readily available. In this case we’ll
    try to build a simple sentiment classifier based on a published dataset. In the
    following recipes we’ll try to do something more involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick Google search leads us to a decent dataset from CrowdFlower containing
    tweets and sentiment labels. Since sentiment labels are similar to emojis on some
    level, this is a good start. Let’s download the file and take a peek:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '| tweet_id | sentiment | author | content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1956967341 | empty | xoshayzers @tiffanylue i know i was listenin to
    bad habi… |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1956967666 | sadness | wannamama Layin n bed with a headache ughhhh…waitin
    o… |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1956967696 | sadness | coolfunky Funeral ceremony…gloomy friday… |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1956967789 | enthusiasm | czareaquino wants to hang out with friends
    SOON! |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1956968416 | neutral | xkilljoyx @dannycastillo We want to trade with
    someone w… |'
  prefs: []
  type: TYPE_TB
- en: 'We can also check how frequently the various emotions occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the simplest models that often give surprisingly good results are from
    the naive Bayes family. We’ll start by encoding the data using the methods that
    `sklearn` provides. `TfidfVectorizer` assigns weights to words according to their
    inverse document frequency; words that occur often get a lower weight since they
    tend to be less informative. `LabelEncoder` assigns unique integers to the different
    labels it sees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With this data in hand, we can now construct the Bayesian model and evaluate
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get 28% right. If we always predicted the most likely category we would
    get a bit over 20%, so we’re off to a good start. There are some other simple
    classifiers to try that might do a little better, but tend to be slower:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trying out “the simplest thing that could possibly work” helps us get started
    quickly and gives us an idea of whether the data has enough signal in it to do
    the job that we want to do.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian classifiers proved very effective in the early days of email spam fighting.
    However, they assume the contributions of each factor are independent from each
    other—so in this case, each word in a tweet has a certain effect on the predicted
    label, independent from the other words—which is clearly not always the case.
    A simple example is that inserting the word *not* into a sentence can negate the
    sentiment expressed. Still the model is easy to construct, and gets us results
    very quickly, and the results are understandable. As a rule, if a Bayesian model
    does not produce any good results on your data, using something more complex will
    probably not help much.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bayesian models often seem to work even better than we’d naively expect. There
    has been some interesting research on why this is. Before machine learning they
    helped break the Enigma code, and they helped power the first email spam detectors.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Inspecting a Simple Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you see what a simple classifier has learned?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look at the contributing factors that make the classifier output a result.
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of using a Bayesian approach is that we get a model that
    we can understand. As we discussed in the previous recipe, Bayesian models assume
    that the contribution of each word is independent of the other words, so to get
    an idea of what our model has learned, we can just ask the model’s opinion on
    the individual words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now remember, the model expects a series of documents, each encoded as a vector
    whose length is equal to the size of the vocabulary, with each element encoding
    the relative frequency of the corresponding word in this document versus all the
    documents. So, a collection of documents that each contained just one word would
    be a square matrix with ones on the diagonal; the *n*th document would have zeros
    for all words in the vocabulary, except for word *n*. Now we can for each word
    predict the likelihoods for the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can go through all the predictions and find the word scores for each
    class. We store this in a `Counter` object so we can easily access the top contributing
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspecting what a simple model learns before diving into something more complex
    is a useful exercise. As powerful as deep learning models are, the fact is that
    it is hard to really tell what they are doing. We can get a general idea of how
    they work, but truly understanding the millions of weights that result from training
    is almost impossible.
  prefs: []
  type: TYPE_NORMAL
- en: The results from our Bayesian model here are in line with what we would expect.
    The word “sad” is an indication for the class “sadness” and “wow” is an indication
    for surprise. Touchingly, the word “mothers” is a strong indication for love.
  prefs: []
  type: TYPE_NORMAL
- en: We do see a bunch of odd words, like “kimbermuffin” and “makinitrite.” On inspection
    it turns out that these are Twitter handles. “foolproofdiva” is just a very enthusiastic
    person. Depending on the goal, we might consider filtering these out.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Using a Convolutional Network for Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to try using a deep network to determine the sentiment expressed
    in a piece of text using a deep network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs are more commonly associated with image recognition (see [Chapter 9](ch09.html#transfer_learning)),
    but they do also work well with certain text classification tasks. The idea is
    to slide a window over the text and that way convert a sequence of items into
    a (shorter) sequence of features. The items in this case would be characters.
    The same weights are used for each step, so we don’t have to learn the same thing
    multiple times—the word “cat” means “cat” wherever it occurs in a tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For the model to run, we first have to vectorize our data. We’ll use the same
    one-hot encoding we saw in the previous recipe, encoding each character as a vector
    filled with all zeros, except for the *n*th entry, where *n* corresponds to the
    character we’re encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s split our data into a training and a test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model and evaluate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After 20 epochs, the training accuracy reaches 0.39, but the test accuracy is
    only 0.31\. The difference is explained by overfitting; the model doesn’t just
    learn general aspects of the data that are also applicable to the test set, but
    starts to memorize part of the training data. This is similar to a student learning
    which answers match which questions, without understanding why.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolutional networks work well in situations where we want our network to
    learn things independently of where they occur. For image recognition, we don’t
    want the network to learn separately for each pixel; we want it to learn to recognize
    features independently of where they occur in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for text, we want the model to learn that if the word “love” appears
    anywhere in the tweet, “love” would be a good label. We don’t want the model to
    learn this for each position separately. A CNN accomplishes this by running a
    sliding window over the text. In this case we use a window of size 6, so we take
    6 characters at a time; for a tweet containing 125 characters, we would apply
    this 120 times.
  prefs: []
  type: TYPE_NORMAL
- en: The crucial thing is that each of those 120 neurons uses the same weights, so
    they all learn the same thing. After the convolution, we apply a `max_pooling`
    layer. This layer will take groups of six neurons and output the maximum value
    of their activations. We can think of this as forwarding the strongest theory
    that any of the neurons have to the next layer. It also reduces the size by a
    factor of six.
  prefs: []
  type: TYPE_NORMAL
- en: In our model we have two convolutional/max-pooling layers, which changes the
    size from an input of 167×100 to 3×256\. We can think of these as steps that increase
    the level of abstraction. At the input level, we only know for each of the 167
    positions which of any of the 100 different characters occurs. After the last
    convolution, we have 3 vectors of 256 each, which encode what is happening at
    the beginning, the middle, and the end of the tweet.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Collecting Twitter Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you collect a large amount of Twitter data for training purposes automatically?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Twitter API.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to do is to head over to [*https://apps.twitter.com*](https://apps.twitter.com)
    to register a new app. Click the Create New App button and fill in the form. We’re
    not going to do anything on behalf of users, so you can leave the Callback URL
    field empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completion, you should have two keys and two secrets that allow access
    to the API. Let’s store them in their corresponding variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now construct an authentication object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The Twitter API has two parts. The REST API makes it possible to call various
    functions to search for tweets, get the status for a user, and even post to Twitter.
    In this recipe we’ll use the streaming API, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you pay Twitter, you’ll get a stream that contains all tweets as they are
    happening. If you don’t pay, you get a sample of all tweets. That’s good enough
    for our purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `stream` object has an iterator, `sample`, which will yield tweets. Let’s
    take a look at some of these using `itertools.islice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case we only want tweets that are in English and contain at least one
    emoji:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now get a hundred tweets containing at least one emoji with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We get two to three tweets a second, which is not bad, but it will take a while
    until we have a sizeable training set. We only care about the tweets that have
    only one type of emoji, and we only want to keep that emoji and the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Twitter can be a very useful source of training data. Each tweet has a wealth
    of metadata associated with it, from the account that posted the tweet to the
    images and hash tags. In this chapter we only use the language metainformation,
    but it is a rich area for exploring.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 A Simple Emoji Predictor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you predict the emoji that best matches a piece of text?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Repurpose the sentiment classifier from [Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you collected a sizeable amount of tweets in the previous step, you can
    use those. If not, you can find a good sample in *data/emojis.txt*. Let’s read
    those into a Pandas `DataFrame`. We’re going to filter out any emoji that occurs
    less than 1,000 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset is too large to keep in memory in vectorized form, so we’ll train
    using a generator. Pandas comes conveniently with a `sample` method, which allows
    us to have the following `data_generator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the model from [Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis)
    without modifications using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The model trains to about 40% precision. This sounds pretty good, even if we
    take into account that the top emojis occur a lot more often than the bottom ones.
    If we run the model over the evaluation set the precision score drops from 40%
    to a little over 35%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With no changes to the model itself, we are able to suggest emojis for a tweet
    instead of running sentiment classification. This is not too surprising; in a
    way emojis are sentiment labels applied by the author. That the performance is
    about the same for both tasks is maybe less expected, since we have so many more
    labels and since we would expect the labels to be more noisy.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Dropout and Multiple Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you increase the performance of your network?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Increase the number of trainable variables while introducing dropout, a technique
    that makes it harder for a bigger network to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: The easy way to increase the expressive power of a neural network is to make
    it bigger, either by making the individual layers bigger or by adding more layers
    to the network. A network with more variables has a higher capacity for learning
    and can generalize better. This doesn’t come for free, though; at some point the
    network starts to *overfit*. ([Recipe 1.3](ch01.html#preprocessing_data) describes
    this problem in more detail.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by expanding our current network. In the previous recipe we used
    a step size of 6 for our convolutions. Six characters seems like a reasonable
    amount to capture local information, but it is also slightly arbitrary. Why not
    four or five? We can in fact do all three and then join the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Precision goes up to 47% during training using this network with its extra layers.
    But unfortunately the precision on the test set reaches only 37%. That is still
    slightly better than what we had before, but the overfitting gap has increased
    by quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of techniques to stop overfitting, and they all have in common
    that they restrict what the model can learn. One of the most popular is adding
    a `Dropout` layer. During training, `Dropout` randomly sets the weights of a fraction
    of all neurons to zero. This forces the network to learn more robustly since it
    can’t rely on a specific neuron to be present. During prediction, all neurons
    work, which averages the results and makes outliers less likely. This slows the
    overfitting down.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras we add `Dropout` just like any other layer. Our model then becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Picking the dropout value is a bit of an art. A higher value means a more robust
    model, but one that also trains more slowly. Running with 0.2 brings the training
    precision to 0.43 and the test precision to 0.39, suggesting that we could still
    go higher.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe gives an idea of some of the techniques we can use to improve the
    performance of our networks. By adding more layers, trying different windows,
    and introducing `Dropout` layers at various places, we have a lot of knobs to
    turn to optimize our network. The process of finding the best values is called
    *hyperparameter tuning*.
  prefs: []
  type: TYPE_NORMAL
- en: There are frameworks that can automatically find the best parameters by trying
    various combinations. Since they do need to train the model many times, you need
    to either be patient or have access to multiple instances to train your models
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Building a Word-Level Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tweets are words, not just random characters. How can you take advantage of
    this fact?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a model that takes as input sequences of word embeddings, rather than
    sequences of characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to tokenize our tweets. We’ll construct a tokenizer
    that keeps the top 50,000 words, apply it to our training and test sets, and then
    pad both so they have a uniform length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can have our model get started quickly by using pretrained embeddings (see
    [Chapter 3](ch03.html#word_embeddings)). We’ll load the weights with a utility
    function, `load_wv2`, which will load the Word2vec embeddings and match them to
    the words in our corpus. This will construct a matrix with a row for each of our
    tokens containing the weights from the Word2vec model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a model very similar to our character model, mostly just
    changing how we process the input. Our input takes a sequence of tokens and the
    embedding layer looks each of those tokens up in the matrix we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This model works, but not as well as the character model. We can fiddle with
    the various hyperparameters, but the gap is quite big (38% precision for the character-level
    model versus 30% for the word-level model). There is one thing we can change that
    does make a difference—setting the embedding layer’s `trainable` property to `True`.
    This helps to get the precision for the word-level model up to 36%, but it also
    means that we’re using the wrong embeddings. We’ll take a look at fixing that
    in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A word-level model has a bigger view of the input data than a character-level
    model because it looks at clusters of words rather than clusters of characters.
    Rather than using the one-hot encoding we used for characters, we use word embeddings
    to get started quickly. Here, we represent each word by a vector representing
    the semantic value of that word as an input to the model. (See [Chapter 3](ch03.html#word_embeddings)
    for more information on word embeddings.)
  prefs: []
  type: TYPE_NORMAL
- en: The model presented in this recipe doesn’t outperform our character-level model
    and doesn’t do much better than the Bayesian model we saw in [Recipe 7.1](#building-a-simple-sentiment-classifier).
    This indicates that the weights from our pretrained word embeddings are a bad
    match for our problem. Things work a lot better if we set the embedding layer
    to trainable; the model improves if we allow it to change those embeddings. We’ll
    look at this in more detail in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: That the weights aren’t a good match is not all that surprising. The Word2vec
    model was trained on Google News, which has a rather different use of language
    than what we find on average on social media. Popular hashtags, for example, won’t
    occur in the Google News corpus, while they seem rather important for classifying
    tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Constructing Your Own Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can you acquire word embeddings that match your corpus?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train your own word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gensim` package not only lets us use a pretrained embedding model, it
    also makes it possible to train new embeddings. The only thing it needs to do
    so is a generator that produces sequences of tokens. It will use this to build
    up a vocabulary and then go on to train a model by going through the generator
    multiple times. The following object will go through a stream of tweets, clean
    them up, and tokenize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can now train the model. The sensible way to do it is to collect a week or
    so of tweets, save them in a set of files (one JSON document per line is a popular
    format), and then pass a generator that goes through the files into the `TokensYielder`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we set off to do this and wait a week for our tweets to dribble in,
    we can test if this works at all by just getting 100,000 filtered tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And then construct the model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the closest neighbors of the word “love” shows us that we have indeed
    our own domain-specific embeddings—only on Twitter is “453” related to “love,”
    since online it is short for “cool story, bro”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: “Melanin” is slightly less expected.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using existing word embeddings is a great way to get started quickly but is
    only suitable to the extent that the text we’re processing is similar to the text
    that the embeddings were trained on. In situations where this is not the case
    and where we have access to a large body of text similar to what we are training
    on, we can easily train our own word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous recipe, an alternative to training fresh embeddings
    is to take existing embeddings but set the `trainable` property of the layer to
    `True`. This will make the network adjust the weights of the words in the embedding
    layer and find new ones where they are missing.
  prefs: []
  type: TYPE_NORMAL
- en: 7.9 Using a Recurrent Neural Network for Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Surely there’s a way to take advantage of the fact that a tweet is a sequence
    of words. How can you do this?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a word-level recurrent network to do the classification.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional networks are good for spotting local patterns in an input stream.
    For sentiment analysis this often works quite well; certain phrases influence
    the sentiment of a sentence independently of where they appear. The task of suggesting
    emojis has a time element in it, though, that we don’t take advantage of using
    a CNN. The emoji associated with a tweet is often the conclusion of the tweet.
    In this sort of situation, an RNN can be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw how we can teach RNNs to generate texts in [Chapter 5](ch05.html#text_generation).
    We can use a similar approach for suggesting emojis. Just like with the word-level
    CNN, we’ll feed in words converted to their embeddings. A one-layer LSTM does
    quite well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: After 10 epochs we reach a precision of 50% on training and 40% on the test
    set, outperforming the CNN model by quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LSTM model we used here strongly outperforms our word-level CNN. We can
    attribute this superior performance to the fact that tweets are sequences, where
    what happens at the end of a tweet has a different impact from what happens at
    the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Since our character-level CNN tended to do better than our word-level CNN and
    our word-level LSTM does better than our character-level CNN, we might wonder
    if a character-level LSTM wouldn’t be even better. It turns out it isn’t.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that if we feed an LSTM one character at a time, it will
    mostly have forgotten what happened at the beginning of the tweet by the time
    it gets to the end. If we feed the LSTM one word at a time, it’s able to overcome
    this. Note also that our character-level CNN doesn’t actually handle the input
    one character at a time. We use sequences of four, five, or six characters at
    a time and have multiple convolutions stacked on top of each other, such that
    the average tweet has at the highest level only three feature vectors left.
  prefs: []
  type: TYPE_NORMAL
- en: We could try to combine the two, though, by creating a CNN that compresses the
    tweet into fragments with a higher level of abstraction and then feeding those
    vectors into an LSTM to draw the final conclusion. This is of course close to
    how our word-level LSTM works. Instead of using a CNN to classify fragments of
    text, we use the pretrained word embeddings to do the same on a per-word level.
  prefs: []
  type: TYPE_NORMAL
- en: 7.10 Visualizing (Dis)Agreement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to visualize how the different models you’ve built compare in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use Pandas to show where they agree and disagree.
  prefs: []
  type: TYPE_NORMAL
- en: Precision gives us an idea of how well our models are doing. Suggesting emojis
    is a rather noisy task though, so it can be very useful to take a look at how
    our various models are doing side-by-side. Pandas is a great tool for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by getting the test data for our character model in as a vector,
    rather than a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s run predictions on the first 100 items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can construct and display a Pandas `DataFrame` with the first 25 predictions
    for each model next to the tweet text and the original emoji:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '| # | content | true | char_cnn | cnn | lstm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | @Gurmeetramrahim @RedFMIndia @rjraunac #8DaysToLionHeart Great | ![](assets/clapping-hands.png)
    | ![](assets/thumbs-up.png) | ![](assets/clapping-hands.png) | ![](assets/face-throwing-a-kiss.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | @suchsmallgods I can’t wait to show him these tweets | ![](assets/smiling-face-with-horns.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/red-heart.png) | ![](assets/loudly-crying-face.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | @Captain_RedWolf I have like 20 set lol WAYYYYYY ahead of you | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | @OtherkinOK were just at @EPfestival, what a set! Next stop is @whelanslive
    on Friday 11th November 2016. | ![](assets/ok-hand-sign.png) | ![](assets/flexed-biceps.png)
    | ![](assets/red-heart.png) | ![](assets/smiling-face-with-smiling-eyes.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | @jochendria: KathNiel with GForce Jorge. #PushAwardsKathNiels | ![](assets/blue-heart.png)
    | ![](assets/blue-heart.png) | ![](assets/blue-heart.png) | ![](assets/blue-heart.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Okay good | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/red-heart.png) | ![](assets/face-with-tears-of-joy.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | “Distraught means to be upset” “So that means confused right?” -@ReevesDakota
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/red-heart.png) | ![](assets/loudly-crying-face.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | @JennLiri babe wtf call bck I’m tryna listen to this ring tone | ![](assets/face-with-rolling-eyes.png)
    | ![](assets/face-with-rolling-eyes.png) | ![](assets/copyright-symbol.png) |
    ![](assets/weary-face.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | does Jen want to be friends? we can so be friends. love you, girl. #BachelorInParadise
    | ![](assets/red-heart.png) | ![](assets/crying-face.png) | ![](assets/red-heart.png)
    | ![](assets/red-heart.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | @amwalker38: Go Follow these hot accounts @the1stMe420 @DanaDeelish @So_deelish
    @aka_teemoney38 @CamPromoXXX @SexyLThings @l... | ![](assets/downpointing-backhand.png)
    | ![](assets/crown.png) | ![](assets/fire.png) | ![](assets/sparkles.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | @gspisak: I always made fun of the parents that show up 30+ mins early
    to pick up their kids today thats me At least I got a... | ![](assets/see-no-evil-monkey.png)
    | ![](assets/upside-down-face.png) | ![](assets/face-with-tears-of-joy.png) |
    ![](assets/face-with-tears-of-joy.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | @ShawnMendes: Toronto Billboard. So cool! @spotify #ShawnXSpotify go
    find them in your city | ![](assets/smiling-face-with-smiling-eyes.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | @kayleeburt77 can I have your number? I seem to have lost mine. | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/thinking-face.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | @KentMurphy: Tim Tebow hits a dinger on his first pitch seen in professional
    ball | ![](assets/flushed-face.png) | ![](assets/flushed-face.png) | ![](assets/flushed-face.png)
    | ![](assets/flushed-face.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | @HailKingSoup... | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/loudly-crying-face.png) | ![](assets/face-with-tears-of-joy.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | @RoxeteraRibbons Same and I have to figure to prove it | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | @theseoulstory: September comebacks: 2PM, SHINee, INFINITE, BTS, Red
    Velvet, Gain, Song Jieun, Kanto... | ![](assets/fire.png) | ![](assets/fire.png)
    | ![](assets/fire.png) | ![](assets/fire.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | @VixenMusicLabel - Peace & Love | ![](assets/victory-hand.png) | ![](assets/red-heart.png)
    | ![](assets/face-throwing-a-kiss.png) | ![](assets/red-heart.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | @iDrinkGallons sorry | ![](assets/slightly-frowning-face.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | @StarYouFollow: 19- Frisson | ![](assets/face-with-rolling-eyes.png)
    | ![](assets/ok-hand-sign.png) | ![](assets/smiling-face-with-heart-shaped-eyes.png)
    | ![](assets/sparkles.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | @RapsDaiIy: Don’t sleep on Ugly God | ![](assets/fire.png) | ![](assets/fire.png)
    | ![](assets/fire.png) | ![](assets/fire.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | How tf do all my shifts get picked up so quickly?! Wtf | ![](assets/loudly-crying-face.png)
    | ![](assets/weary-face.png) | ![](assets/face-with-tears-of-joy.png) | ![](assets/weary-face.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | @ShadowhuntersTV: #Shadowhunters fans, how many s would YOU give this
    father-daughter #FlashbackFriday bonding moment betwee... | ![](assets/red-heart.png)
    | ![](assets/red-heart.png) | ![](assets/red-heart.png) | ![](assets/red-heart.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | @mbaylisxo: thank god I have a uniform and don’t have to worry about
    what to wear everyday | ![](assets/smiling-face-with-open-mouth-and-cold-sweat.png)
    | ![](assets/face-with-rolling-eyes.png) | ![](assets/red-heart.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | Mood swings like... | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-rolling-eyes.png)
    |'
  prefs: []
  type: TYPE_TB
- en: Browsing these results, we can see that often when the models get it wrong,
    they land on an emoji that is very similar to the one in the original tweet. Sometimes
    the predictions seem to make more sense than what was actually used, and sometimes
    none of the models do very well.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at the actual data can help us see where our models go wrong. In this
    case a simple thing to improve performance would be to treat all the emojis that
    are similar as the same. The different hearts and different smiley faces express
    more or less the same things.
  prefs: []
  type: TYPE_NORMAL
- en: One alternative would be to learn embeddings for the emojis. This would give
    us a notion of how related emojis are. We could then have a loss function that
    takes this similarity into account, rather than a hard correct/incorrect measure.
  prefs: []
  type: TYPE_NORMAL
- en: 7.11 Combining Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to harness the combined prediction power of your models to get a
    better answer.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combine the models into an ensemble model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of the wisdom of crowds—that the average of the opinions of a group
    is often more accurate than any specific opinion—also goes for machine learning
    models. We can combine all three models into one by using three inputs and combining
    the outputs of our models using the `Average` layer from Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a different data generator to train this model; rather than specifying
    one input, we now have three. Since they have different names, we can have our
    data generator yield a dictionary to feed the three inputs. We also need to do
    some wrangling to get the character-level data to line up with the word-level
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then train the model using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combined models or ensemble models are a great way to combine various approaches
    to a problem in one model. It is not a coincidence that in popular machine learning
    competitions like Kaggle the winners almost always are based on this technique.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of keeping the models almost completely separate and then joining them
    up at the very end using the `Average` layer, we could also join them earlier,
    for example by concatenating the first dense layer of each of the models. Indeed,
    this is to some extent what we did with the more complex CNN, where we used various
    window sizes for small subnets that then were concatenated for a final conclusion.
  prefs: []
  type: TYPE_NORMAL
