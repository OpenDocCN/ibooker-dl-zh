- en: Chapter 7\. Suggesting Emojis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。建议表情符号
- en: In this chapter we’ll build a model to suggest emojis given a small piece of
    text. We’ll start by developing a simple sentiment classifier based on a public
    set of tweets labeled with various sentiments, like happiness, love, surprise,
    etc. We’ll first try a Bayesian classifier to get an idea of the baseline performance
    and take a look at what this classifier can learn. We’ll then switch to a convolutional
    network and look at various ways to tune this classifier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个模型，根据一小段文本建议表情符号。我们将首先基于一组带有各种情感标签的推文开发一个简单的情感分类器，如快乐、爱、惊讶等。我们首先尝试一个贝叶斯分类器，以了解基线性能，并查看这个分类器可以学到什么。然后我们将切换到卷积网络，并查看各种调整这个分类器的方法。
- en: Next we’ll look at how we can harvest tweets using the Twitter API ourselves,
    and then we’ll apply the convolutional model from [Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis)
    before moving on to a word-level model. We’ll then construct and apply a recurrent
    word-level network, and compare the three different models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看看如何使用Twitter API收集推文，然后我们将应用[配方7.3](#using-a-convolutional-network-for-sentiment-analysis)中的卷积模型，然后转向一个单词级模型。然后我们将构建并应用一个递归单词级网络，并比较这三种不同的模型。
- en: Finally, we’ll combine all three models into an ensemble model that outperforms
    any of the three.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把这三个模型组合成一个胜过任何一个的集成模型。
- en: The final model does a very decent job and just needs to be rolled into a mobile
    app!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型表现得非常不错，只需要整合到一个移动应用程序中！
- en: 'The code for this chapter can be found in these notebooks:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在这些笔记本中找到：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 7.1 Building a Simple Sentiment Classifier
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.1 构建一个简单的情感分类器
- en: Problem
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you determine the sentiment expressed in a piece of text?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确定文本中表达的情感？
- en: Solution
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Find a dataset consisting of sentences where the sentiment is labeled and run
    a simple classifier over them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个由标记了情感的句子组成的数据集，并对其运行一个简单的分类器。
- en: Before trying something complicated, it is a good idea to first try the simplest
    thing we can think of on a dataset that is readily available. In this case we’ll
    try to build a simple sentiment classifier based on a published dataset. In the
    following recipes we’ll try to do something more involved.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试复杂的东西之前，首先尝试在一个readily可用的数据集上尝试我们能想到的最简单的事情是一个好主意。在这种情况下，我们将尝试基于一个已发布数据集构建一个简单的情感分类器。在接下来的配方中，我们将尝试做一些更复杂的事情。
- en: 'A quick Google search leads us to a decent dataset from CrowdFlower containing
    tweets and sentiment labels. Since sentiment labels are similar to emojis on some
    level, this is a good start. Let’s download the file and take a peek:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 快速的谷歌搜索让我们找到了一个来自CrowdFlower的不错的数据集，其中包含推文和情感标签。由于情感标签在某种程度上类似于表情符号，这是一个很好的开始。让我们下载文件并看一眼：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This results in:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致：
- en: '| tweet_id | sentiment | author | content |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 推文ID | 情感 | 作者 | 内容 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 1956967341 | empty | xoshayzers @tiffanylue i know i was listenin to
    bad habi… |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1956967341 | 空 | xoshayzers @tiffanylue 我知道我在听坏习惯... |'
- en: '| 1 | 1956967666 | sadness | wannamama Layin n bed with a headache ughhhh…waitin
    o… |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1956967666 | 伤心 | wannamama 躺在床上头疼，等待... |'
- en: '| 2 | 1956967696 | sadness | coolfunky Funeral ceremony…gloomy friday… |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1956967696 | 伤心 | coolfunky 葬礼仪式...阴郁的星期五... |'
- en: '| 3 | 1956967789 | enthusiasm | czareaquino wants to hang out with friends
    SOON! |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1956967789 | 热情 | czareaquino 想很快和朋友们一起出去！ |'
- en: '| 4 | 1956968416 | neutral | xkilljoyx @dannycastillo We want to trade with
    someone w… |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1956968416 | 中性 | xkilljoyx @dannycastillo 我们想和某人交易... |'
- en: 'We can also check how frequently the various emotions occur:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查各种情绪发生的频率：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Some of the simplest models that often give surprisingly good results are from
    the naive Bayes family. We’ll start by encoding the data using the methods that
    `sklearn` provides. `TfidfVectorizer` assigns weights to words according to their
    inverse document frequency; words that occur often get a lower weight since they
    tend to be less informative. `LabelEncoder` assigns unique integers to the different
    labels it sees:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最简单的模型通常会产生令人惊讶的好结果，来自朴素贝叶斯家族。我们将从使用`sklearn`提供的方法对数据进行编码。`TfidfVectorizer`根据其逆文档频率为单词分配权重；经常出现的单词获得较低的权重，因为它们往往不太具有信息性。`LabelEncoder`为它看到的不同标签分配唯一的整数：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With this data in hand, we can now construct the Bayesian model and evaluate
    it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些数据，我们现在可以构建贝叶斯模型并评估它：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We get 28% right. If we always predicted the most likely category we would
    get a bit over 20%, so we’re off to a good start. There are some other simple
    classifiers to try that might do a little better, but tend to be slower:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有28%的正确率。如果我们总是预测最可能的类别，我们会得到略高于20%，所以我们有了一个良好的开端。还有一些其他简单的分类器可以尝试，可能会做得更好，但速度较慢：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Discussion
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Trying out “the simplest thing that could possibly work” helps us get started
    quickly and gives us an idea of whether the data has enough signal in it to do
    the job that we want to do.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试“可能起作用的最简单的事情”有助于我们快速入门，并让我们了解数据是否具有足够的信号来完成我们想要做的工作。
- en: Bayesian classifiers proved very effective in the early days of email spam fighting.
    However, they assume the contributions of each factor are independent from each
    other—so in this case, each word in a tweet has a certain effect on the predicted
    label, independent from the other words—which is clearly not always the case.
    A simple example is that inserting the word *not* into a sentence can negate the
    sentiment expressed. Still the model is easy to construct, and gets us results
    very quickly, and the results are understandable. As a rule, if a Bayesian model
    does not produce any good results on your data, using something more complex will
    probably not help much.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器在早期的电子邮件垃圾邮件对抗中表现非常有效。然而，它们假设每个因素的贡献是彼此独立的——因此在这种情况下，推文中的每个单词对预测标签都有一定影响，独立于其他单词——这显然并非总是如此。一个简单的例子是，在句子中插入单词“not”可以否定表达的情感。尽管如此，该模型很容易构建，并且可以很快为我们带来结果，而且结果是可以理解的。一般来说，如果贝叶斯模型在您的数据上没有产生好的结果，使用更复杂的东西可能不会有太大帮助。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Bayesian models often seem to work even better than we’d naively expect. There
    has been some interesting research on why this is. Before machine learning they
    helped break the Enigma code, and they helped power the first email spam detectors.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯模型通常比我们天真地期望的要好。关于这一点已经有一些有趣的研究。在机器学习之前，它们帮助破译了恩尼格玛密码，也帮助驱动了第一个电子邮件垃圾邮件检测器。
- en: 7.2 Inspecting a Simple Classifier
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.2 检查一个简单分类器
- en: Problem
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you see what a simple classifier has learned?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如何查看一个简单分类器学到了什么？
- en: Solution
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Look at the contributing factors that make the classifier output a result.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 查看使分类器输出结果的贡献因素。
- en: One of the advantages of using a Bayesian approach is that we get a model that
    we can understand. As we discussed in the previous recipe, Bayesian models assume
    that the contribution of each word is independent of the other words, so to get
    an idea of what our model has learned, we can just ask the model’s opinion on
    the individual words.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯方法的一个优点是我们可以理解模型。正如我们在前面的配方中讨论的那样，贝叶斯模型假设每个单词的贡献与其他单词无关，因此为了了解我们的模型学到了什么，我们可以询问模型对个别单词的看法。
- en: 'Now remember, the model expects a series of documents, each encoded as a vector
    whose length is equal to the size of the vocabulary, with each element encoding
    the relative frequency of the corresponding word in this document versus all the
    documents. So, a collection of documents that each contained just one word would
    be a square matrix with ones on the diagonal; the *n*th document would have zeros
    for all words in the vocabulary, except for word *n*. Now we can for each word
    predict the likelihoods for the labels:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在记住，模型期望一系列文档，每个文档都编码为一个向量，其长度等于词汇表的大小，每个元素编码为该文档中对应单词相对频率与所有文档的比率。因此，每个只包含一个单词的文档集合将是一个对角线上有1的方阵；第n个文档将对词汇表中的所有单词都有零，除了单词n。现在我们可以为每个单词预测标签的可能性：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we can go through all the predictions and find the word scores for each
    class. We store this in a `Counter` object so we can easily access the top contributing
    words:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以查看所有预测，并找到每个类别的单词分数。我们将这些存储在一个`Counter`对象中，以便我们可以轻松访问贡献最大的单词：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s print the results:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印结果：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Discussion
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Inspecting what a simple model learns before diving into something more complex
    is a useful exercise. As powerful as deep learning models are, the fact is that
    it is hard to really tell what they are doing. We can get a general idea of how
    they work, but truly understanding the millions of weights that result from training
    is almost impossible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究更复杂的内容之前检查一个简单模型学到了什么是一个有用的练习。尽管深度学习模型非常强大，但事实是很难真正了解它们在做什么。我们可以大致了解它们的工作原理，但要真正理解训练结果中的数百万个权重几乎是不可能的。
- en: The results from our Bayesian model here are in line with what we would expect.
    The word “sad” is an indication for the class “sadness” and “wow” is an indication
    for surprise. Touchingly, the word “mothers” is a strong indication for love.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贝叶斯模型的结果符合我们的预期。单词“sad”是“sadness”类的指示，“wow”是惊讶的指示。令人感动的是，单词“mothers”是爱的强烈指示。
- en: We do see a bunch of odd words, like “kimbermuffin” and “makinitrite.” On inspection
    it turns out that these are Twitter handles. “foolproofdiva” is just a very enthusiastic
    person. Depending on the goal, we might consider filtering these out.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一堆奇怪的单词，比如“kimbermuffin”和“makinitrite”。检查后发现这些是Twitter用户名。 “foolproofdiva”只是一个非常热情的人。根据目标，我们可能会考虑将这些过滤掉。
- en: 7.3 Using a Convolutional Network for Sentiment Analysis
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.3 使用卷积网络进行情感分析
- en: Problem
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to try using a deep network to determine the sentiment expressed
    in a piece of text using a deep network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您想尝试使用深度网络来确定文本中表达的情感。
- en: Solution
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a convolutional network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积网络。
- en: 'CNNs are more commonly associated with image recognition (see [Chapter 9](ch09.html#transfer_learning)),
    but they do also work well with certain text classification tasks. The idea is
    to slide a window over the text and that way convert a sequence of items into
    a (shorter) sequence of features. The items in this case would be characters.
    The same weights are used for each step, so we don’t have to learn the same thing
    multiple times—the word “cat” means “cat” wherever it occurs in a tweet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs更常用于图像识别（参见[第9章](ch09.html#transfer_learning)），但它们在某些文本分类任务中也表现良好。其思想是在文本上滑动一个窗口，从而将一系列项目转换为（更短的）特征序列。在这种情况下，项目将是字符。每一步都使用相同的权重，因此我们不必多次学习相同的内容——单词“cat”在推文中的任何位置都表示“cat”：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For the model to run, we first have to vectorize our data. We’ll use the same
    one-hot encoding we saw in the previous recipe, encoding each character as a vector
    filled with all zeros, except for the *n*th entry, where *n* corresponds to the
    character we’re encoding:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型运行，我们首先必须对数据进行向量化。我们将使用在前面的配方中看到的相同的一热编码，将每个字符编码为一个填满所有零的向量，除了第n个条目，其中n对应于我们要编码的字符：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s split our data into a training and a test set:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据分成训练集和测试集：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now train the model and evaluate it:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练模型并评估它：
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After 20 epochs, the training accuracy reaches 0.39, but the test accuracy is
    only 0.31\. The difference is explained by overfitting; the model doesn’t just
    learn general aspects of the data that are also applicable to the test set, but
    starts to memorize part of the training data. This is similar to a student learning
    which answers match which questions, without understanding why.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 经过20个时代，训练准确率达到0.39，但测试准确率只有0.31。这种差异可以通过过拟合来解释；模型不仅学习了数据的一般方面，这些方面也适用于测试集，而且开始记忆部分训练数据。这类似于学生学习哪些答案与哪些问题匹配，而不理解为什么。
- en: Discussion
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Convolutional networks work well in situations where we want our network to
    learn things independently of where they occur. For image recognition, we don’t
    want the network to learn separately for each pixel; we want it to learn to recognize
    features independently of where they occur in the image.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络在我们希望网络学习独立于发生位置的情况下效果很好。对于图像识别，我们不希望网络为每个像素单独学习；我们希望它学会独立于图像中发生位置的特征。
- en: Similarly, for text, we want the model to learn that if the word “love” appears
    anywhere in the tweet, “love” would be a good label. We don’t want the model to
    learn this for each position separately. A CNN accomplishes this by running a
    sliding window over the text. In this case we use a window of size 6, so we take
    6 characters at a time; for a tweet containing 125 characters, we would apply
    this 120 times.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于文本，我们希望模型学会，如果推文中出现“爱”这个词，那么“爱”将是一个好的标签。我们不希望模型为每个位置单独学习这一点。CNN通过在文本上运行一个滑动窗口来实现这一点。在这种情况下，我们使用大小为6的窗口，因此我们每次取6个字符；对于包含125个字符的推文，我们会应用这个过程120次。
- en: The crucial thing is that each of those 120 neurons uses the same weights, so
    they all learn the same thing. After the convolution, we apply a `max_pooling`
    layer. This layer will take groups of six neurons and output the maximum value
    of their activations. We can think of this as forwarding the strongest theory
    that any of the neurons have to the next layer. It also reduces the size by a
    factor of six.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，这120个神经元中的每一个都使用相同的权重，因此它们都学习相同的东西。在卷积之后，我们应用一个`max_pooling`层。这一层将取六个神经元的组并输出它们激活的最大值。我们可以将其视为将任何神经元中最强的理论传递给下一层。它还将大小减小了六分之一。
- en: In our model we have two convolutional/max-pooling layers, which changes the
    size from an input of 167×100 to 3×256\. We can think of these as steps that increase
    the level of abstraction. At the input level, we only know for each of the 167
    positions which of any of the 100 different characters occurs. After the last
    convolution, we have 3 vectors of 256 each, which encode what is happening at
    the beginning, the middle, and the end of the tweet.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，我们有两个卷积/最大池化层，它将输入从167×100的大小更改为3×256。我们可以将这些看作是增加抽象级别的步骤。在输入级别，我们只知道在167个位置中的每一个位置上出现了100个不同字符中的哪一个。在最后一个卷积之后，我们有3个256个向量，它们分别编码了推文开头、中间和结尾发生的情况。
- en: 7.4 Collecting Twitter Data
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.4收集Twitter数据
- en: Problem
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you collect a large amount of Twitter data for training purposes automatically?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如何自动收集大量用于训练目的的Twitter数据？
- en: Solution
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the Twitter API.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Twitter API。
- en: The first thing to do is to head over to [*https://apps.twitter.com*](https://apps.twitter.com)
    to register a new app. Click the Create New App button and fill in the form. We’re
    not going to do anything on behalf of users, so you can leave the Callback URL
    field empty.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是前往[*https://apps.twitter.com*](https://apps.twitter.com)注册一个新应用。点击“创建新应用”按钮并填写表格。我们不会代表用户做任何事情，所以可以将回调URL字段留空。
- en: 'After completion, you should have two keys and two secrets that allow access
    to the API. Let’s store them in their corresponding variables:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您应该有两个密钥和两个允许访问API的密钥。让我们将它们存储在相应的变量中：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now construct an authentication object:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建一个认证对象：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The Twitter API has two parts. The REST API makes it possible to call various
    functions to search for tweets, get the status for a user, and even post to Twitter.
    In this recipe we’ll use the streaming API, though.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter API有两部分。REST API使得可以调用各种函数来搜索推文、获取用户的状态，甚至发布到Twitter。在这个示例中，我们将使用流API。
- en: 'If you pay Twitter, you’ll get a stream that contains all tweets as they are
    happening. If you don’t pay, you get a sample of all tweets. That’s good enough
    for our purpose:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你付费给Twitter，你将获得一个包含所有推文的流。如果你不付费，你会得到所有推文的一个样本。这对我们的目的已经足够了：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `stream` object has an iterator, `sample`, which will yield tweets. Let’s
    take a look at some of these using `itertools.islice`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream`对象有一个迭代器`sample`，它将产生推文。让我们使用`itertools.islice`来查看其中一些：'
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this case we only want tweets that are in English and contain at least one
    emoji:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只想要英文推文，并且至少包含一个表情符号：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can now get a hundred tweets containing at least one emoji with:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以获取包含至少一个表情符号的一百条推文：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We get two to three tweets a second, which is not bad, but it will take a while
    until we have a sizeable training set. We only care about the tweets that have
    only one type of emoji, and we only want to keep that emoji and the text:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每秒获得两到三条推文，这还不错，但要花一段时间才能拥有一个规模可观的训练集。我们只关心那些只有一种类型的表情符号的推文，我们只想保留该表情符号和文本：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Discussion
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Twitter can be a very useful source of training data. Each tweet has a wealth
    of metadata associated with it, from the account that posted the tweet to the
    images and hash tags. In this chapter we only use the language metainformation,
    but it is a rich area for exploring.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter可以是一个非常有用的训练数据来源。每条推文都有大量与之相关的元数据，从发布推文的账户到图片和哈希标签。在本章中，我们只使用语言元信息，但这是一个值得探索的丰富领域。
- en: 7.5 A Simple Emoji Predictor
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.5一个简单的表情符号预测器
- en: Problem
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you predict the emoji that best matches a piece of text?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如何预测最匹配一段文本的表情符号？
- en: Solution
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Repurpose the sentiment classifier from [Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 重新利用来自[Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis)的情感分类器。
- en: 'If you collected a sizeable amount of tweets in the previous step, you can
    use those. If not, you can find a good sample in *data/emojis.txt*. Let’s read
    those into a Pandas `DataFrame`. We’re going to filter out any emoji that occurs
    less than 1,000 times:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在上一步中收集了大量推文，可以使用这些。如果没有，可以在*data/emojis.txt*中找到一个好的样本。让我们将这些读入Pandas的`DataFrame`。我们将过滤掉出现次数少于1000次的任何表情符号：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This dataset is too large to keep in memory in vectorized form, so we’ll train
    using a generator. Pandas comes conveniently with a `sample` method, which allows
    us to have the following `data_generator`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集太大了，无法以向量化形式保存在内存中，所以我们将使用生成器进行训练。Pandas方便地提供了一个`sample`方法，允许我们使用以下`data_generator`：
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now train the model from [Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis)
    without modifications using:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在不进行修改的情况下从[Recipe 7.3](#using-a-convolutional-network-for-sentiment-analysis)训练模型：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The model trains to about 40% precision. This sounds pretty good, even if we
    take into account that the top emojis occur a lot more often than the bottom ones.
    If we run the model over the evaluation set the precision score drops from 40%
    to a little over 35%:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练到大约40%的精度。即使考虑到顶部表情符号比底部表情符号更频繁出现，这听起来还是相当不错的。如果我们在评估集上运行模型，精度得分会从40%下降到略高于35%：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Discussion
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: With no changes to the model itself, we are able to suggest emojis for a tweet
    instead of running sentiment classification. This is not too surprising; in a
    way emojis are sentiment labels applied by the author. That the performance is
    about the same for both tasks is maybe less expected, since we have so many more
    labels and since we would expect the labels to be more noisy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在不对模型本身进行任何更改的情况下，我们能够为推文建议表情符号，而不是运行情感分类。这并不太令人惊讶；在某种程度上，表情符号是作者应用的情感标签。对于这两个任务性能大致相同可能不太出乎意料，因为我们有更多的标签，而且我们预计标签会更加嘈杂。
- en: 7.6 Dropout and Multiple Windows
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.6 Dropout和多窗口
- en: Problem
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you increase the performance of your network?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何提高网络的性能？
- en: Solution
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Increase the number of trainable variables while introducing dropout, a technique
    that makes it harder for a bigger network to overfit.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 增加可训练变量的数量，同时引入了一种使更大的网络难以过拟合的技术——dropout。
- en: The easy way to increase the expressive power of a neural network is to make
    it bigger, either by making the individual layers bigger or by adding more layers
    to the network. A network with more variables has a higher capacity for learning
    and can generalize better. This doesn’t come for free, though; at some point the
    network starts to *overfit*. ([Recipe 1.3](ch01.html#preprocessing_data) describes
    this problem in more detail.)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 增加神经网络的表达能力的简单方法是使其更大，可以通过使单个层更大或向网络添加更多层来实现。具有更多变量的网络具有更高的学习能力，并且可以更好地泛化。然而，这并非是免费的；在某个时候，网络开始*过拟合*。([Recipe
    1.3](ch01.html#preprocessing_data)更详细地描述了这个问题。)
- en: 'Let’s start by expanding our current network. In the previous recipe we used
    a step size of 6 for our convolutions. Six characters seems like a reasonable
    amount to capture local information, but it is also slightly arbitrary. Why not
    four or five? We can in fact do all three and then join the results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从扩展当前网络开始。在上一个配方中，我们为卷积使用了步长6。六个字符似乎是一个合理的数量来捕捉局部信息，但也稍微随意。为什么不是四或五呢？实际上我们可以做这三种然后将结果合并：
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Precision goes up to 47% during training using this network with its extra layers.
    But unfortunately the precision on the test set reaches only 37%. That is still
    slightly better than what we had before, but the overfitting gap has increased
    by quite a bit.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个网络的额外层，训练过程中精度提高到47%。但不幸的是，测试集上的精度仅达到37%。这仍然比之前稍微好一点，但过拟合差距已经增加了很多。
- en: There are a number of techniques to stop overfitting, and they all have in common
    that they restrict what the model can learn. One of the most popular is adding
    a `Dropout` layer. During training, `Dropout` randomly sets the weights of a fraction
    of all neurons to zero. This forces the network to learn more robustly since it
    can’t rely on a specific neuron to be present. During prediction, all neurons
    work, which averages the results and makes outliers less likely. This slows the
    overfitting down.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多防止过拟合的技术，它们的共同点是限制模型可以学习的内容。其中最流行的之一是添加`Dropout`层。在训练期间，`Dropout`会随机将所有神经元的权重设置为零的一部分。这迫使网络更加稳健地学习，因为它不能依赖于特定的神经元存在。在预测期间，所有神经元都在工作，这会平均结果并减少异常值。这减缓了过拟合的速度。
- en: 'In Keras we add `Dropout` just like any other layer. Our model then becomes:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，我们像添加任何其他层一样添加`Dropout`。我们的模型随后变为：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Picking the dropout value is a bit of an art. A higher value means a more robust
    model, but one that also trains more slowly. Running with 0.2 brings the training
    precision to 0.43 and the test precision to 0.39, suggesting that we could still
    go higher.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '选择dropout值有点艺术性。较高的值意味着更稳健的模型，但训练速度也更慢。使用0.2进行训练将训练精度提高到0.43，测试精度提高到0.39，这表明我们仍然可以更高。 '
- en: Discussion
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: This recipe gives an idea of some of the techniques we can use to improve the
    performance of our networks. By adding more layers, trying different windows,
    and introducing `Dropout` layers at various places, we have a lot of knobs to
    turn to optimize our network. The process of finding the best values is called
    *hyperparameter tuning*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方提供了一些我们可以使用的技术来改善网络性能的想法。通过添加更多层，尝试不同的窗口，并在不同位置引入`Dropout`层，我们有很多旋钮可以调整来优化我们的网络。找到最佳值的过程称为*超参数调整*。
- en: There are frameworks that can automatically find the best parameters by trying
    various combinations. Since they do need to train the model many times, you need
    to either be patient or have access to multiple instances to train your models
    in parallel.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些框架可以通过尝试各种组合来自动找到最佳参数。由于它们需要多次训练模型，您需要耐心等待或者可以同时训练多个实例来并行训练您的模型。
- en: 7.7 Building a Word-Level Model
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.7 构建一个单词级模型
- en: Problem
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Tweets are words, not just random characters. How can you take advantage of
    this fact?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 推文是单词，而不仅仅是随机字符。您如何利用这一事实？
- en: Solution
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train a model that takes as input sequences of word embeddings, rather than
    sequences of characters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个以单词嵌入序列作为输入而不是字符序列的模型。
- en: 'The first thing to do is to tokenize our tweets. We’ll construct a tokenizer
    that keeps the top 50,000 words, apply it to our training and test sets, and then
    pad both so they have a uniform length:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是对我们的推文进行标记化。我们将构建一个保留前50000个单词的标记器，将其应用于我们的训练和测试集，然后填充两者，使它们具有统一的长度：
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can have our model get started quickly by using pretrained embeddings (see
    [Chapter 3](ch03.html#word_embeddings)). We’ll load the weights with a utility
    function, `load_wv2`, which will load the Word2vec embeddings and match them to
    the words in our corpus. This will construct a matrix with a row for each of our
    tokens containing the weights from the Word2vec model:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用预训练的嵌入来快速启动我们的模型（请参见[第3章](ch03.html#word_embeddings)）。我们将使用一个实用函数`load_wv2`加载权重，它将加载Word2vec嵌入并将其与我们语料库中的单词匹配。这将构建一个矩阵，每个令牌包含来自Word2vec模型的权重的一行：
- en: '[PRE32]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can now create a model very similar to our character model, mostly just
    changing how we process the input. Our input takes a sequence of tokens and the
    embedding layer looks each of those tokens up in the matrix we just created:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个与我们的字符模型非常相似的模型，主要是改变如何处理输入。我们的输入接受一系列令牌，嵌入层在我们刚刚创建的矩阵中查找每个令牌：
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This model works, but not as well as the character model. We can fiddle with
    the various hyperparameters, but the gap is quite big (38% precision for the character-level
    model versus 30% for the word-level model). There is one thing we can change that
    does make a difference—setting the embedding layer’s `trainable` property to `True`.
    This helps to get the precision for the word-level model up to 36%, but it also
    means that we’re using the wrong embeddings. We’ll take a look at fixing that
    in the next recipe.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以工作，但效果不如字符模型好。我们可以调整各种超参数，但差距相当大（字符级模型的精度为38%，而单词级模型的精度为30%）。有一件事可以改变这种情况——将嵌入层的`trainable`属性设置为`True`。这有助于将单词级模型的精度提高到36%，但这也意味着我们使用了错误的嵌入。我们将在下一个配方中看看如何解决这个问题。
- en: Discussion
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: A word-level model has a bigger view of the input data than a character-level
    model because it looks at clusters of words rather than clusters of characters.
    Rather than using the one-hot encoding we used for characters, we use word embeddings
    to get started quickly. Here, we represent each word by a vector representing
    the semantic value of that word as an input to the model. (See [Chapter 3](ch03.html#word_embeddings)
    for more information on word embeddings.)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单词级模型比一个字符级模型对输入数据有更广泛的视角，因为它查看的是单词的簇，而不是字符的簇。我们使用单词嵌入来快速开始，而不是使用字符的独热编码。在这里，我们通过表示每个单词的向量来表示该单词的语义值作为模型的输入。（有关单词嵌入的更多信息，请参见[第3章](ch03.html#word_embeddings)。）
- en: The model presented in this recipe doesn’t outperform our character-level model
    and doesn’t do much better than the Bayesian model we saw in [Recipe 7.1](#building-a-simple-sentiment-classifier).
    This indicates that the weights from our pretrained word embeddings are a bad
    match for our problem. Things work a lot better if we set the embedding layer
    to trainable; the model improves if we allow it to change those embeddings. We’ll
    look at this in more detail in the next recipe.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方中介绍的模型并没有超越我们的字符级模型，也没有比我们在[配方7.1](#building-a-simple-sentiment-classifier)中看到的贝叶斯模型做得更好。这表明我们预训练的单词嵌入的权重与我们的问题不匹配。如果我们将嵌入层设置为可训练，事情会好得多；如果允许它更改这些嵌入，模型会有所改进。我们将在下一个配方中更详细地讨论这个问题。
- en: That the weights aren’t a good match is not all that surprising. The Word2vec
    model was trained on Google News, which has a rather different use of language
    than what we find on average on social media. Popular hashtags, for example, won’t
    occur in the Google News corpus, while they seem rather important for classifying
    tweets.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 权重不匹配并不令人惊讶。Word2vec模型是在Google新闻上训练的，它的语言使用方式与社交媒体上的平均使用方式有很大不同。例如，流行的标签在Google新闻语料库中不会出现，而它们似乎对于分类推文非常重要。
- en: 7.8 Constructing Your Own Embeddings
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.8 构建您自己的嵌入
- en: Problem
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you acquire word embeddings that match your corpus?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如何获取与您的语料库匹配的单词嵌入？
- en: Solution
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train your own word embeddings.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 训练您自己的单词嵌入。
- en: 'The `gensim` package not only lets us use a pretrained embedding model, it
    also makes it possible to train new embeddings. The only thing it needs to do
    so is a generator that produces sequences of tokens. It will use this to build
    up a vocabulary and then go on to train a model by going through the generator
    multiple times. The following object will go through a stream of tweets, clean
    them up, and tokenize them:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`包不仅让我们可以使用预训练的嵌入模型，还可以训练新的嵌入。它需要的唯一东西是一个生成器，产生令牌序列。它将使用这个生成器来建立词汇表，然后通过多次遍历生成器来训练模型。以下对象将遍历一系列推文，清理它们，并对其进行标记化：'
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can now train the model. The sensible way to do it is to collect a week or
    so of tweets, save them in a set of files (one JSON document per line is a popular
    format), and then pass a generator that goes through the files into the `TokensYielder`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练模型了。明智的做法是收集一周左右的推文，将它们保存在一组文件中（每行一个JSON文档是一种流行的格式），然后将一个遍历文件的生成器传递到`TokensYielder`中。
- en: 'Before we set off to do this and wait a week for our tweets to dribble in,
    we can test if this works at all by just getting 100,000 filtered tweets:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始这项工作并等待一周让我们的推文涓涓细流进来之前，我们可以通过获取100,000条经过筛选的推文来测试这是否有效：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And then construct the model with:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后构建模型：
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Looking at the closest neighbors of the word “love” shows us that we have indeed
    our own domain-specific embeddings—only on Twitter is “453” related to “love,”
    since online it is short for “cool story, bro”:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 查看单词“爱”的最近邻居，我们发现我们确实有自己的领域特定的嵌入——只有在Twitter上，“453”与“爱”相关，因为在线上它是“酷故事，兄弟”的缩写：
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: “Melanin” is slightly less expected.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: “黑色素”稍微出乎意料。
- en: Discussion
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Using existing word embeddings is a great way to get started quickly but is
    only suitable to the extent that the text we’re processing is similar to the text
    that the embeddings were trained on. In situations where this is not the case
    and where we have access to a large body of text similar to what we are training
    on, we can easily train our own word embeddings.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现有的词嵌入是一个快速入门的好方法，但只适用于我们处理的文本与嵌入训练的文本相似的情况。在这种情况不成立且我们可以访问大量与我们正在训练的文本相似的文本的情况下，我们可以轻松地训练自己的词嵌入。
- en: As we saw in the previous recipe, an alternative to training fresh embeddings
    is to take existing embeddings but set the `trainable` property of the layer to
    `True`. This will make the network adjust the weights of the words in the embedding
    layer and find new ones where they are missing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一篇文章中看到的，一个训练新嵌入的替代方法是使用现有的嵌入，但将层的“trainable”属性设置为“True”。这将使网络调整嵌入层中单词的权重，并在缺失的地方找到新的单词。
- en: 7.9 Using a Recurrent Neural Network for Classification
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.9 使用递归神经网络进行分类
- en: Problem
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Surely there’s a way to take advantage of the fact that a tweet is a sequence
    of words. How can you do this?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当然有一种方法可以利用推文是一系列单词的事实。你可以怎么做呢？
- en: Solution
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a word-level recurrent network to do the classification.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单词级递归网络进行分类。
- en: Convolutional networks are good for spotting local patterns in an input stream.
    For sentiment analysis this often works quite well; certain phrases influence
    the sentiment of a sentence independently of where they appear. The task of suggesting
    emojis has a time element in it, though, that we don’t take advantage of using
    a CNN. The emoji associated with a tweet is often the conclusion of the tweet.
    In this sort of situation, an RNN can be a better fit.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络适用于在输入流中发现局部模式。对于情感分析，这通常效果很好；某些短语会独立于它们出现的位置影响句子的情感。然而，建议表情符号的任务中有一个时间元素，我们没有利用CNN。与推文相关联的表情符号通常是推文的结论。在这种情况下，RNN可能更合适。
- en: 'We saw how we can teach RNNs to generate texts in [Chapter 5](ch05.html#text_generation).
    We can use a similar approach for suggesting emojis. Just like with the word-level
    CNN, we’ll feed in words converted to their embeddings. A one-layer LSTM does
    quite well:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何教RNN生成文本在[第5章](ch05.html#text_generation)。我们可以使用类似的方法来建议表情符号。就像单词级CNN一样，我们将输入转换为它们的嵌入的单词。一个单层LSTM表现得相当不错：
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: After 10 epochs we reach a precision of 50% on training and 40% on the test
    set, outperforming the CNN model by quite a bit.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10个时期，我们在训练集上达到了50%的精度，在测试集上达到了40%，远远超过了CNN模型。
- en: Discussion
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The LSTM model we used here strongly outperforms our word-level CNN. We can
    attribute this superior performance to the fact that tweets are sequences, where
    what happens at the end of a tweet has a different impact from what happens at
    the beginning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的LSTM模型明显优于我们的单词级CNN。我们可以将这种卓越的性能归因于推文是序列，推文末尾发生的事情与开头发生的事情有不同的影响。
- en: Since our character-level CNN tended to do better than our word-level CNN and
    our word-level LSTM does better than our character-level CNN, we might wonder
    if a character-level LSTM wouldn’t be even better. It turns out it isn’t.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的字符级CNN往往比我们的单词级CNN做得更好，而我们的单词级LSTM比字符级CNN做得更好，我们可能会想知道字符级LSTM是否会更好。结果表明并不是。
- en: The reason for this is that if we feed an LSTM one character at a time, it will
    mostly have forgotten what happened at the beginning of the tweet by the time
    it gets to the end. If we feed the LSTM one word at a time, it’s able to overcome
    this. Note also that our character-level CNN doesn’t actually handle the input
    one character at a time. We use sequences of four, five, or six characters at
    a time and have multiple convolutions stacked on top of each other, such that
    the average tweet has at the highest level only three feature vectors left.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是，如果我们一次向LSTM输入一个字符，到达末尾时，它大部分时间都会忘记推文开头发生的事情。如果我们一次向LSTM输入一个单词，它就能克服这个问题。还要注意，我们的字符级CNN实际上并不是一次处理一个字符。我们一次使用四、五或六个字符的序列，并且将多个卷积层堆叠在一起，这样平均推文在最高级别只剩下三个特征向量。
- en: We could try to combine the two, though, by creating a CNN that compresses the
    tweet into fragments with a higher level of abstraction and then feeding those
    vectors into an LSTM to draw the final conclusion. This is of course close to
    how our word-level LSTM works. Instead of using a CNN to classify fragments of
    text, we use the pretrained word embeddings to do the same on a per-word level.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试将两者结合起来，通过创建一个CNN将推文压缩成更高抽象级别的片段，然后将这些向量输入到LSTM中得出最终结论。当然，这与我们的单词级LSTM的工作方式非常接近。我们不是使用CNN对文本片段进行分类，而是使用预训练的词嵌入在每个单词级别上执行相同的操作。
- en: 7.10 Visualizing (Dis)Agreement
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.10 可视化（不）一致性
- en: Problem
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to visualize how the different models you’ve built compare in practice.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望可视化您构建的不同模型在实践中的比较。
- en: Solution
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Pandas to show where they agree and disagree.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas显示它们的一致性和不一致性。
- en: Precision gives us an idea of how well our models are doing. Suggesting emojis
    is a rather noisy task though, so it can be very useful to take a look at how
    our various models are doing side-by-side. Pandas is a great tool for this.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 精度给我们一个关于我们的模型表现如何的概念。虽然建议表情符号是一个相当嘈杂的任务，但是将我们的各种模型的表现并排进行比较是非常有用的。Pandas是一个很好的工具。
- en: 'Let’s start by getting the test data for our character model in as a vector,
    rather than a generator:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将字符模型的测试数据作为向量而不是生成器导入：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now let’s run predictions on the first 100 items:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对前100个项目进行预测：
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we can construct and display a Pandas `DataFrame` with the first 25 predictions
    for each model next to the tweet text and the original emoji:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建并显示一个Pandas `DataFrame`，其中包含每个模型的前25个预测，以及推文文本和原始表情符号：
- en: '[PRE42]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This results in:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就得到了：
- en: '| # | content | true | char_cnn | cnn | lstm |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| # | 内容 | 真实 | char_cnn | cnn | lstm |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | @Gurmeetramrahim @RedFMIndia @rjraunac #8DaysToLionHeart Great | ![](assets/clapping-hands.png)
    | ![](assets/thumbs-up.png) | ![](assets/clapping-hands.png) | ![](assets/face-throwing-a-kiss.png)
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 0 | @Gurmeetramrahim @RedFMIndia @rjraunac #8DaysToLionHeart 太棒了 | ![](assets/clapping-hands.png)
    | ![](assets/thumbs-up.png) | ![](assets/clapping-hands.png) | ![](assets/face-throwing-a-kiss.png)
    |'
- en: '| 1 | @suchsmallgods I can’t wait to show him these tweets | ![](assets/smiling-face-with-horns.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/red-heart.png) | ![](assets/loudly-crying-face.png)
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 1 | @suchsmallgods 我迫不及待想向他展示这些推文 | ![](assets/smiling-face-with-horns.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/red-heart.png) | ![](assets/loudly-crying-face.png)
    |'
- en: '| 2 | @Captain_RedWolf I have like 20 set lol WAYYYYYY ahead of you | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2 | @Captain_RedWolf 我有大约20套lol 比你领先太多了 | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) |'
- en: '| 3 | @OtherkinOK were just at @EPfestival, what a set! Next stop is @whelanslive
    on Friday 11th November 2016. | ![](assets/ok-hand-sign.png) | ![](assets/flexed-biceps.png)
    | ![](assets/red-heart.png) | ![](assets/smiling-face-with-smiling-eyes.png) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 3 | @OtherkinOK 刚刚在@EPfestival，太棒了！下一站是@whelanslive，2016年11月11日星期五。 | ![](assets/ok-hand-sign.png)
    | ![](assets/flexed-biceps.png) | ![](assets/red-heart.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    |'
- en: '| 4 | @jochendria: KathNiel with GForce Jorge. #PushAwardsKathNiels | ![](assets/blue-heart.png)
    | ![](assets/blue-heart.png) | ![](assets/blue-heart.png) | ![](assets/blue-heart.png)
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 4 | @jochendria: KathNiel与GForce Jorge。#PushAwardsKathNiels | ![](assets/blue-heart.png)
    | ![](assets/blue-heart.png) | ![](assets/blue-heart.png) | ![](assets/blue-heart.png)
    |'
- en: '| 5 | Okay good | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/red-heart.png) | ![](assets/face-with-tears-of-joy.png) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 好的 | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/red-heart.png) | ![](assets/face-with-tears-of-joy.png) |'
- en: '| 6 | “Distraught means to be upset” “So that means confused right?” -@ReevesDakota
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/red-heart.png) | ![](assets/loudly-crying-face.png) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 6 | “Distraught意味着心烦意乱” “那意味着困惑对吧？” -@ReevesDakota | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/red-heart.png) | ![](assets/loudly-crying-face.png)
    |'
- en: '| 7 | @JennLiri babe wtf call bck I’m tryna listen to this ring tone | ![](assets/face-with-rolling-eyes.png)
    | ![](assets/face-with-rolling-eyes.png) | ![](assets/copyright-symbol.png) |
    ![](assets/weary-face.png) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 7 | @JennLiri 宝贝，怎么了，打电话回来，我想听这首铃声 | ![](assets/face-with-rolling-eyes.png)
    | ![](assets/face-with-rolling-eyes.png) | ![](assets/copyright-symbol.png) |
    ![](assets/weary-face.png) |'
- en: '| 8 | does Jen want to be friends? we can so be friends. love you, girl. #BachelorInParadise
    | ![](assets/red-heart.png) | ![](assets/crying-face.png) | ![](assets/red-heart.png)
    | ![](assets/red-heart.png) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 珍想要做朋友吗？我们可以成为朋友。爱你，女孩。#BachelorInParadise | ![](assets/red-heart.png)
    | ![](assets/crying-face.png) | ![](assets/red-heart.png) | ![](assets/red-heart.png)
    |'
- en: '| 9 | @amwalker38: Go Follow these hot accounts @the1stMe420 @DanaDeelish @So_deelish
    @aka_teemoney38 @CamPromoXXX @SexyLThings @l... | ![](assets/downpointing-backhand.png)
    | ![](assets/crown.png) | ![](assets/fire.png) | ![](assets/sparkles.png) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 9 | @amwalker38: 去关注这些热门账号 @the1stMe420 @DanaDeelish @So_deelish @aka_teemoney38
    @CamPromoXXX @SexyLThings @l... | ![](assets/downpointing-backhand.png) | ![](assets/crown.png)
    | ![](assets/fire.png) | ![](assets/sparkles.png) |'
- en: '| 10 | @gspisak: I always made fun of the parents that show up 30+ mins early
    to pick up their kids today thats me At least I got a... | ![](assets/see-no-evil-monkey.png)
    | ![](assets/upside-down-face.png) | ![](assets/face-with-tears-of-joy.png) |
    ![](assets/face-with-tears-of-joy.png) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 10 | @gspisak: 我总是取笑那些提前30分钟以上来接孩子的父母，今天轮到我了，至少我得到了... | ![](assets/see-no-evil-monkey.png)
    | ![](assets/upside-down-face.png) | ![](assets/face-with-tears-of-joy.png) |
    ![](assets/face-with-tears-of-joy.png) |'
- en: '| 11 | @ShawnMendes: Toronto Billboard. So cool! @spotify #ShawnXSpotify go
    find them in your city | ![](assets/smiling-face-with-smiling-eyes.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 11 | @ShawnMendes: 多伦多广告牌。太酷了！@spotify #ShawnXSpotify 去找到你所在城市的广告牌 | ![](assets/smiling-face-with-smiling-eyes.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) |'
- en: '| 12 | @kayleeburt77 can I have your number? I seem to have lost mine. | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/thinking-face.png) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 12 | @kayleeburt77 我可以要你的号码吗？我好像把我的弄丢了。 | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/thinking-face.png) |'
- en: '| 13 | @KentMurphy: Tim Tebow hits a dinger on his first pitch seen in professional
    ball | ![](assets/flushed-face.png) | ![](assets/flushed-face.png) | ![](assets/flushed-face.png)
    | ![](assets/flushed-face.png) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 13 | @KentMurphy: 蒂姆·提博在职业棒球比赛中第一球就击出了一支全垒打 | ![](assets/flushed-face.png)
    | ![](assets/flushed-face.png) | ![](assets/flushed-face.png) | ![](assets/flushed-face.png)
    |'
- en: '| 14 | @HailKingSoup... | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/loudly-crying-face.png) | ![](assets/face-with-tears-of-joy.png)
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 14 | @HailKingSoup... | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/loudly-crying-face.png) | ![](assets/face-with-tears-of-joy.png)
    |'
- en: '| 15 | @RoxeteraRibbons Same and I have to figure to prove it | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 15 | @RoxeteraRibbons 同样，我必须找出证明 | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/smiling-face-with-smiling-eyes.png) |'
- en: '| 16 | @theseoulstory: September comebacks: 2PM, SHINee, INFINITE, BTS, Red
    Velvet, Gain, Song Jieun, Kanto... | ![](assets/fire.png) | ![](assets/fire.png)
    | ![](assets/fire.png) | ![](assets/fire.png) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 16 | @theseoulstory: 九月回归：2PM，SHINee，INFINITE，BTS，Red Velvet，Gain，Song Jieun，Kanto...
    | ![](assets/fire.png) | ![](assets/fire.png) | ![](assets/fire.png) | ![](assets/fire.png)
    |'
- en: '| 17 | @VixenMusicLabel - Peace & Love | ![](assets/victory-hand.png) | ![](assets/red-heart.png)
    | ![](assets/face-throwing-a-kiss.png) | ![](assets/red-heart.png) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 17 | @VixenMusicLabel - 和平与爱 | ![](assets/victory-hand.png) | ![](assets/red-heart.png)
    | ![](assets/face-throwing-a-kiss.png) | ![](assets/red-heart.png) |'
- en: '| 18 | @iDrinkGallons sorry | ![](assets/slightly-frowning-face.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 18 | @iDrinkGallons 抱歉 | ![](assets/slightly-frowning-face.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    |'
- en: '| 19 | @StarYouFollow: 19- Frisson | ![](assets/face-with-rolling-eyes.png)
    | ![](assets/ok-hand-sign.png) | ![](assets/smiling-face-with-heart-shaped-eyes.png)
    | ![](assets/sparkles.png) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 19 | @StarYouFollow: 19- Frisson | ![](assets/face-with-rolling-eyes.png)
    | ![](assets/ok-hand-sign.png) | ![](assets/smiling-face-with-heart-shaped-eyes.png)
    | ![](assets/sparkles.png) |'
- en: '| 20 | @RapsDaiIy: Don’t sleep on Ugly God | ![](assets/fire.png) | ![](assets/fire.png)
    | ![](assets/fire.png) | ![](assets/fire.png) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 20 | @RapsDaiIy: 别错过Ugly God | ![](assets/fire.png) | ![](assets/fire.png)
    | ![](assets/fire.png) | ![](assets/fire.png) |'
- en: '| 21 | How tf do all my shifts get picked up so quickly?! Wtf | ![](assets/loudly-crying-face.png)
    | ![](assets/weary-face.png) | ![](assets/face-with-tears-of-joy.png) | ![](assets/weary-face.png)
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 怎么我的所有班次都这么快被接走了？！什么鬼 | ![](assets/loudly-crying-face.png) | ![](assets/weary-face.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/weary-face.png) |'
- en: '| 22 | @ShadowhuntersTV: #Shadowhunters fans, how many s would YOU give this
    father-daughter #FlashbackFriday bonding moment betwee... | ![](assets/red-heart.png)
    | ![](assets/red-heart.png) | ![](assets/red-heart.png) | ![](assets/red-heart.png)
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 22 | @ShadowhuntersTV: #Shadowhunters 粉丝，你们会给这个父女#FlashbackFriday亲密时刻打几分？
    | ![](assets/red-heart.png) | ![](assets/red-heart.png) | ![](assets/red-heart.png)
    | ![](assets/red-heart.png) |'
- en: '| 23 | @mbaylisxo: thank god I have a uniform and don’t have to worry about
    what to wear everyday | ![](assets/smiling-face-with-open-mouth-and-cold-sweat.png)
    | ![](assets/face-with-rolling-eyes.png) | ![](assets/red-heart.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 23 | @mbaylisxo: 感谢上帝，我有一套制服，不用每天担心穿什么 | ![](assets/smiling-face-with-open-mouth-and-cold-sweat.png)
    | ![](assets/face-with-rolling-eyes.png) | ![](assets/red-heart.png) | ![](assets/smiling-face-with-smiling-eyes.png)
    |'
- en: '| 24 | Mood swings like... | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-rolling-eyes.png)
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 心情波动如... | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-tears-of-joy.png)
    | ![](assets/face-with-tears-of-joy.png) | ![](assets/face-with-rolling-eyes.png)
    |'
- en: Browsing these results, we can see that often when the models get it wrong,
    they land on an emoji that is very similar to the one in the original tweet. Sometimes
    the predictions seem to make more sense than what was actually used, and sometimes
    none of the models do very well.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览这些结果，我们可以看到通常当模型出错时，它们会落在与原始推文中非常相似的表情符号上。有时，预测似乎比实际使用的更有意义，有时候没有一个模型表现得很好。
- en: Discussion
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Looking at the actual data can help us see where our models go wrong. In this
    case a simple thing to improve performance would be to treat all the emojis that
    are similar as the same. The different hearts and different smiley faces express
    more or less the same things.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 查看实际数据可以帮助我们看到我们的模型出错的地方。在这种情况下，提高性能的一个简单方法是将所有相似的表情符号视为相同的。不同的心形和不同的笑脸表达的基本上是相同的。
- en: One alternative would be to learn embeddings for the emojis. This would give
    us a notion of how related emojis are. We could then have a loss function that
    takes this similarity into account, rather than a hard correct/incorrect measure.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个替代方案是为表情符号学习嵌入。这将给我们一个关于表情符号相关性的概念。然后，我们可以有一个损失函数，考虑到这种相似性，而不是一个硬性的正确/错误度量。
- en: 7.11 Combining Models
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.11 结合模型
- en: Problem
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to harness the combined prediction power of your models to get a
    better answer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望利用模型的联合预测能力获得更好的答案。
- en: Solution
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Combine the models into an ensemble model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型组合成一个集成模型。
- en: 'The idea of the wisdom of crowds—that the average of the opinions of a group
    is often more accurate than any specific opinion—also goes for machine learning
    models. We can combine all three models into one by using three inputs and combining
    the outputs of our models using the `Average` layer from Keras:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 群体智慧的概念——即群体意见的平均值通常比任何具体意见更准确——也适用于机器学习模型。我们可以通过使用三个输入将所有三个模型合并为一个模型，并使用Keras的`Average`层来组合我们模型的输出：
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We need a different data generator to train this model; rather than specifying
    one input, we now have three. Since they have different names, we can have our
    data generator yield a dictionary to feed the three inputs. We also need to do
    some wrangling to get the character-level data to line up with the word-level
    data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个不同的数据生成器来训练这个模型；而不是指定一个输入，我们现在有三个输入。由于它们有不同的名称，我们可以让我们的数据生成器产生一个字典来提供这三个输入。我们还需要做一些整理工作，使字符级数据与单词级数据对齐：
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can then train the model using:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用以下方式训练模型：
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Discussion
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Combined models or ensemble models are a great way to combine various approaches
    to a problem in one model. It is not a coincidence that in popular machine learning
    competitions like Kaggle the winners almost always are based on this technique.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 组合模型或集成模型是将各种方法结合到一个模型中解决问题的好方法。在像Kaggle这样的流行机器学习竞赛中，获胜者几乎总是基于这种技术，这并非巧合。
- en: Instead of keeping the models almost completely separate and then joining them
    up at the very end using the `Average` layer, we could also join them earlier,
    for example by concatenating the first dense layer of each of the models. Indeed,
    this is to some extent what we did with the more complex CNN, where we used various
    window sizes for small subnets that then were concatenated for a final conclusion.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是将模型几乎完全分开，然后在最后使用`Average`层将它们连接起来，我们也可以在更早的阶段将它们连接起来，例如通过连接每个模型的第一个密集层。实际上，这在更复杂的CNN中是我们所做的一部分，我们使用了各种窗口大小的小子网，然后将它们连接起来得出最终结论。
