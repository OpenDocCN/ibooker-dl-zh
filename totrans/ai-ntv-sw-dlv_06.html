<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Chaos Engineering and Service Reliability"><div class="chapter" id="chapter_6_chaos_engineering_and_service_reliability_1749354010916149">
      <h1><span class="label">Chapter 6. </span>Chaos Engineering and Service Reliability</h1>
      <p class="fix_tracking">Complex modern systems<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" id="xi_chaosengineering6332"/> are inherently vulnerable. Even seemingly minor disruptions, or a single weak link, can cause issues that spiral to have catastrophic consequences.</p>
      <p>Consider this scenario: a prominent e-commerce platform suffers a significant outage during a peak sales event, comparable to Black Friday. As traffic builds, the platform’s checkout service grinds to a crawl, eventually culminating in complete failure. Thousands of customers are left unable to finalize purchases, resulting in not only immediate lost revenue but also damaged reputation and eroded trust and brand loyalty. Postincident analysis reveals the root cause to be network latency between the checkout service and a critical pricing data cache. As the cache response slowed under the strain of high traffic, the system’s retry mechanism became overwhelmed, leading to a cascade of failed requests that ultimately overloaded the database. </p>
      <p>Scenarios like these and the rising cost of failures have led to the emergence of service reliability<a contenteditable="false" data-type="indexterm" data-primary="reliability" data-secondary="and chaos engineering" data-secondary-sortas="chaos engineering" id="id819"/> as a discipline and the practice of chaos engineering (sometimes called failure or fault injection testing). The goal of chaos engineering is to provide an understanding of how systems behave when stressed in an unusual (chaotic) way. The increased popularity of these practices has been fueled by the development of new tools, technologies, and practices. </p>
      <p>The term chaos engineering can be traced back to Netflix<a contenteditable="false" data-type="indexterm" data-primary="Netflix" id="id820"/> in 2010. The company was transitioning its infrastructure to the cloud, which introduced new complexity, with hundreds of microservices interacting in unpredictable ways. To test the resilience of their systems, Netflix engineers developed Chaos Monkey<a contenteditable="false" data-type="indexterm" data-primary="Chaos Monkey" id="id821"/>, a tool designed to randomly terminate VM instances in their production environment. This simulated real-world failures, forcing engineers to build systems that could gracefully handle unexpected disruptions. </p>
      <p class="fix_tracking">The use of the word chaos and the image of a monkey set loose to randomly terminate software in a production environment does conjure mayhem. Given these preconceptions, introducing chaos engineering into an organization may be met with resistance. More than one boss has wondered, “Don’t we have enough chaos around here?” </p>
      <p>In this chapter, we’ll counter those notions by understanding modern chaos engineering as a rigorous approach to implementing experiments. As a methodology, we use this <em>controlled</em> disruption to test the resilience<a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="chaos engineering and SLOs" id="id822"/> of our systems. In addition to testing our current state, chaos engineering gives us a powerful methodology to systematically improve resilience.</p>
      
      <p>The experiments we conduct give us a deeper understanding of our software’s behavior under stress. This knowledge enables us to design targeted improvements. We then test to validate their effectiveness in meeting our targets. </p>
      <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_6_service_reliability_and_service_resiliency_1749354010916279">
        <h1>Service Reliability and Service Resiliency</h1>
        <p>Service reliability and service resiliency<a contenteditable="false" data-type="indexterm" data-primary="service reliability and service resiliency" data-seealso="reliability; resiliency" id="id823"/> are related concepts. The former is the probability that a service will perform its intended function without failure for a specified period under defined conditions. The latter is the ability of a service to withstand and recover from disruptions, such as hardware failures, software bugs, network outages, or even cyberattacks. It’s about how well a service can bounce back from adversity.</p>
        <p>While distinct, they are interconnected. A highly reliable service is less likely to experience failures, but even the most reliable systems can encounter unexpected problems. That’s where resiliency comes in. It ensures that even when failures occur, the service can recover quickly and minimize disruptions to users.</p>
      </div></aside>
      <p>We’ll also cover how to use service-level objectives (SLOs)<a contenteditable="false" data-type="indexterm" data-primary="service level objectives (SLOs)" id="id824"/><a contenteditable="false" data-type="indexterm" data-primary="SLOs (service level objectives)" id="id825"/> to set our resiliency targets. We’ll look at using error budgets to allow for an acceptable level of failure within that target. We’ll see how chaos engineering works with these mechanisms by helping us validate whether our system can operate within its error budget and still meet our targets even when facing unexpected disruptions.</p>
      <p>In this chapter we’ll also move beyond static chaos experiments to understand a more modern and dynamic approach that involves integrating chaos engineering into our CI/CD pipelines, allowing us to continuously assess and improve system resilience as part of our regular development workflows.</p>
      <p>Throughout this chapter, we will explore how advanced chaos engineering tools leverage AI/ML-driven insights<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="resiliency testing insights" id="id826"/><a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="AI testing insights" id="id827"/> to recommend and guide the execution of these experiments, leading to more efficient and effective resilience testing while reducing risk. We will also see how agentic AI, GenAI, and MCP address critical scalability and precision challenges in chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="precision challenges, chaos engineering" id="id828"/> by automating experiment design, enabling dynamic risk detection<a contenteditable="false" data-type="indexterm" data-primary="dynamic risk detection" id="id829"/>, and providing intelligent remediation. These technologies transform chaos engineering from a reactive practice into a proactive, self-optimizing system resilience strategy.</p>
      <section data-type="sect1" data-pdf-bookmark="Getting Started with Chaos Engineering"><div class="sect1" id="chapter_6_getting_started_with_chaos_engineering_1749354010916345">
        <h1>Getting Started with Chaos Engineering</h1>
        <p>While many chaos engineering experiments employ randomness (e.g., selecting a random server or service to take down), the practice of chaos engineering is as methodical as lab science. In this section, we’ll dive into the core tenets of chaos engineering and look at best practices to reduce the risk of causing service disruptions when conducting experiments.</p>
        <section data-type="sect2" data-pdf-bookmark="Principles of Chaos Engineering"><div class="sect2" id="chapter_6_principles_of_chaos_engineering_1749354010916404">
          <h2>Principles of Chaos Engineering</h2>
          <p>Netflix<a contenteditable="false" data-type="indexterm" data-primary="Netflix" id="id830"/> has defined a set of core principles<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="principles of" id="id831"/> that provide a useful framework for exploring how your systems behave under stress. A structured approach ensures that your chaos experiments are not just disruptive events but structured investigations that generate valuable data that you can use to drive improvements to your system’s resilience. These principles are:</p>
          <dl>
            <dt>Defining a “steady state” that characterizes normal system behavior</dt>
            <dd>
              <p>Observability is key here. You must have the metrics you need to understand the normal range of values that indicate your system is healthy and performing as expected. This could include request latency, error rates, throughput, or application-specific metrics. Be sure to account for external factors that might influence your metrics, such as time of day, day of week, or the presence of a marketing campaign that could spike traffic.</p>
            </dd>
            <dt>Turning expectation into a hypothesis</dt>
            <dd>
              <p>Based on your understanding of the system’s architecture and dependencies, formulate a hypothesis about how it <em>should</em> behave when a specific failure is introduced. Frame your hypothesis in a way that can be objectively tested using your chosen metrics. For example, “If we simulate a 20% increase in traffic, the average response time should remain below three seconds, and the error rate should not exceed 0.5%.”</p>
            </dd>
            <dt>Executing the experiment by simulating real-world events</dt>
            <dd>
              <p>Use chaos engineering tools to automate the injection of failures. Simulate a server crashing or becoming unavailable, an outage of a critical third-party service, or a sudden surge in user requests.</p>
            </dd>
            <dt class="pagebreak-before less_space">Evaluating the results against the hypothesis</dt>
            <dd>
              <p>Compare the system’s behavior during the experiment to your established baseline and your hypothesized outcome. Did the metrics stay within acceptable ranges? Did the system recover as expected? Were any unexpected side effects observed?<em> </em>If the system deviates from the expected behavior, investigate the root cause. Based on the experiment’s outcome, refine your hypotheses and adjust your system design or operational procedures to enhance resilience.</p>
            </dd>
          </dl>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Starting Small and Scaling"><div class="sect2" id="chapter_6_starting_small_and_scaling_1749354010916455">
          <h2>Starting Small and Scaling</h2>
          <p>Simulating failures<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="starting small and scaling" id="xi_chaosengineeringstartingsmallandscaling64433"/><a contenteditable="false" data-type="indexterm" data-primary="scalability" data-secondary="chaos engineering practices" id="xi_scalabilitychaosengineeringpractices64433"/><a contenteditable="false" data-type="indexterm" data-primary="risk mitigation" id="xi_riskmitigation64433"/> to intentionally take down systems does, of course, incur risk. We knowingly incur risk in this controlled way to validate the hypothesis we’ve defined. An important strategy for reducing risk is to start with small experiments. </p>
          <p>To illustrate starting small and scaling experiments, let’s walk through an example focused on testing a checkout service in an e-commerce system. This service is a critical microservice that processes user purchases. The expected outcome is simple: a customer adds items to their cart, proceeds to checkout, and completes the payment. The customer expects a smooth, fast, and reliable experience. </p>
          <p>Behind the scenes, this straightforward operation relies on a series of complex processes. The checkout service depends on multiple APIs and external services to function properly, including inventory systems, payment gateways, and caching layers (like Redis) to quickly retrieve important data such as product prices, discounts, and availability.<strong> </strong>The checkout service fetches pricing data from a cache for quick access. If the cache is slow or fails, the checkout service should still provide the right information by failing over to another cache instance or even to a database as a backup, though it may be slower. </p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>GenAI<a contenteditable="false" data-type="indexterm" data-primary="generative AI (GenAI)" data-secondary="in chaos engineering" data-secondary-sortas="chaos engineering" id="id832"/> can transform chaos engineering from manual hypothesis testing into an adaptive, self-optimizing resilience validation system. This approach proves particularly valuable in critical <span class="keep-together">e-commerce</span> workflows like checkout services, where balancing risk mitigation with realistic failure simulation is paramount.</p>
          </div>
          <p>Developers typically configure retry logic, timeouts, and circuit breakers to handle network issues or failures. Let’s look at each:</p>
          <dl>
            <dt>Retry logic</dt>
            <dd>
              <p>This ensures that if a request to the cache fails or experiences network issues<a contenteditable="false" data-type="indexterm" data-primary="retry logic" id="id833"/>, the system will automatically try again a few times before giving up. This helps handle transient failures. The system might, for example, retry up to three times with a delay of 100 ms between each retry.</p>
            </dd>
            <dt>Timeouts</dt>
            <dd>
              <p>Timeout settings<a contenteditable="false" data-type="indexterm" data-primary="timeouts" id="id834"/> define how long the service should wait for a response before deciding that the attempt has failed. This prevents the service from hanging indefinitely if the cache is slow or unresponsive. A system might be configured to time out after 200 ms for each request to the cache.</p>
            </dd>
            <dt>Circuit breakers</dt>
            <dd>
              <p>A circuit breaker<a contenteditable="false" data-type="indexterm" data-primary="circuit breakers" id="id835"/> prevents further attempts to call a failing service after a certain number of failed attempts. If the cache continues to fail or is too slow, the circuit breaker “trips” and routes traffic to a fallback system (e.g., another cache or a database). The circuit breaker can automatically reset after a set period to test if the original service has recovered. For example, a circuit breaker might be configured to trip after five consecutive retries fail.</p>
            </dd>
          </dl>
          <p>We’ll start testing the checkout service by introducing small latencies to ensure the retry logic and timeout settings are functioning before scaling up to introduce more severe issues that will ultimately trigger the circuit breaker. If all goes well, we expect the system to fail over to an alternative data source. These are our steps.</p>
          <section data-type="sect3" data-pdf-bookmark="Step 1: Conduct a simple latency experiment"><div class="sect3" id="chapter_6_step_1_conduct_a_simple_latency_experiment_1749354010916505">
            <h3>Step 1: Conduct a simple latency experiment</h3>
            <p>We start with a test of our retry logic<a contenteditable="false" data-type="indexterm" data-primary="retry logic" id="id836"/>. We want to ensure the system is resilient if network issues arise<a contenteditable="false" data-type="indexterm" data-primary="latency experiment, chaos engineering" id="id837"/>, such as high latency or a temporary loss of connectivity. Our steady state is a responsive service that responds within an acceptable time limit.</p>
            <p>Our hypothesis is that if the network experiences significant latency when trying to reach the cache, the system should use its retry logic and timeout settings to handle the issue gracefully, eventually tripping the circuit breaker to prevent further degradation of service.</p>
            <p>We start small by injecting a small amount of network latency (e.g., 200 ms) between the checkout service and the cache.</p>
            <p>We observe whether the retry logic kicks in and whether the service handles the delay within the acceptable time limit without user impact. We continue to monitor whether the system continues functioning as expected, pulling from the cache after the latency delay.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Step 2: Test resilience against a more significant network issue"><div class="sect3" id="chapter_6_step_2_test_resilience_against_a_more_significant_1749354010916563">
            <h3>Step 2: Test resilience against a more significant network issue</h3>
            <p>Once we’ve tested our retry logic with a small latency, we can increase the scope and intensity of the experiment to simulate a more significant network issue. This tests our timeouts. We increase the network latency (e.g., from 500 ms to 1 second) to see how the service behaves under heavier load or network congestion. We test how the retry logic handles the extended delay. Does the service retry the call to the cache, and does it respect the timeout setting? If so, we increase the severity of the issue by causing the cache API to completely fail after a set number of retries.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Step 3: Validate that the circuit breaker fails over to an alternative"><div class="sect3" id="chapter_6_step_3_validate_that_the_circuit_breaker_fails_ov_1749354010916613">
            <h3>Step 3: Validate that the circuit breaker fails over to an alternative</h3>
            <p>We next set experiment conditions to render the cache inaccessible. After retrying, the circuit breaker mechanism should be triggered. When the circuit breaker is tripped, the checkout service fails over to an alternative data source, such as another cache instance in a different data center (in this case, our Postgres database). While the Postgres database might be slower than the cache, the goal is to keep the service operational, albeit with slightly degraded performance.</p>
            <p>AI agents<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="chaos engineering tests" id="id838"/> can make this process even simpler by dynamically adjusting failure injection parameters using reinforcement learning. For example:</p>
            <ol>
              <li>
                <p>Start with 200 ms delays, then autonomously scale to 500 ms to 1 second based on real-time performance telemetry.</p>
              </li>
              <li>
                <p>Limit experiment impact to 0.5% of transactions initially, expanding only after validating safety mechanisms.</p>
              </li>
              <li>
                <p>Optimize trip thresholds (e.g., five failures to four) through historical success pattern analysis.</p>
              </li>
            </ol>
            <p>You can further scale the experiment to test the resilience of the failover by introducing similar network issues between the checkout service and the Postgres database to see how the system continues to adapt under increased failure conditions. By following this process, we gradually increase the complexity of the experiment to validate the system’s resilience mechanisms, without jumping into major disruptions immediately.</p>
            <p>It’s important to note that the initial settings for resilience mechanisms are often based on educated guesses rather than precise data. This is another reason that testing through chaos engineering experiments is so crucial.</p>
            <p>Injecting network latency is just one condition we can scale in a chaos experiment. We’ll discuss other conditions later in this section<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_chaosengineeringstartingsmallandscaling64433" id="id839"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_scalabilitychaosengineeringpractices64433" id="id840"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_riskmitigation64433" id="id841"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Starting in Production-Like Environments"><div class="sect2" id="chapter_6_starting_in_production_like_environments_1749354010916661">
          <h2>Starting in Production-Like Environments</h2>
          <p>Another important best practice to minimize risk in chaos engineering is to test experiments in pre-production environments<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="pre-production environments" id="id842"/><a contenteditable="false" data-type="indexterm" data-primary="pre-production environments" data-secondary="chaos engineering in" id="id843"/><a contenteditable="false" data-type="indexterm" data-primary="pre-production environments" data-secondary="starting chaos engineering tests in" id="id844"/> before moving to production. This allows us to experiment safely without impacting real users. We can rapidly iterate, adjust parameters, and observe results free from production constraints. Once we confirm system resilience in these settings, we promote our experiment to the next environment, eventually reaching production. Each promotion carries risk, so we proceed with caution. Configuration drift<a contenteditable="false" data-type="indexterm" data-primary="configuration drift" id="id845"/> between environments can lead to <span class="keep-together">discrepancies</span> in experiment results. Maintaining the “start small and scale” approach when moving between environments is crucial in case we encounter issues. Thoroughly vetting our experiments in pre-production ensures that our experiments are well-designed and insightful without unintended consequences.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Leveraging Modern Tools"><div class="sect2" id="chapter_6_leveraging_modern_tools_1749354010916712">
          <h2>Leveraging Modern Tools</h2>
          <p>We looked extensively at an example of testing network latency in a chaos experiment<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="tools for" id="xi_chaosengineeringtoolsfor610398"/>. There are many other types of conditions that are important to test. Modern tools (such as Harness Chaos Engineering<a contenteditable="false" data-type="indexterm" data-primary="Harness" data-secondary="Chaos Engineering" id="id846"/>, Chaos Monkey<a contenteditable="false" data-type="indexterm" data-primary="Chaos Monkey" id="id847"/>, and LitmusChaos<a contenteditable="false" data-type="indexterm" data-primary="LitmusChaos" id="id848"/>) can help here by offering extensive catalogs of predefined experiments. Modern tools will typically offer chaos engineering experiments across categories and common failure patterns, including:</p>
          <dl>
            <dt>Resource exhaustion</dt>
            <dd>
            <dl>
              <dt>CPU exhaustion</dt>
              <dd><p>Force high CPU utilization to simulate a process consuming excessive processing power<a contenteditable="false" data-type="indexterm" data-primary="resource exhaustion, chaos engineering experiments in" id="id849"/>.</p></dd>
              <dt>Memory exhaustion</dt>
              <dd><p>Consume all available memory to test how your application handles memory pressure and potential out-of-memory errors.</p></dd>
              <dt>Disk I/O exhaustion</dt>
              <dd><p>Generate heavy disk read/write operations to simulate storage bottlenecks.</p></dd>
              <dt>Network bandwidth exhaustion</dt>
              <dd><p>Saturate network bandwidth to test how your application performs under network congestion.</p></dd>
            </dl>
         </dd>

            <dt>Network disruption</dt>
            <dd>
            <dl>
              <dt>Network latency</dt>
              <dd><p>Introduce delays in network<a contenteditable="false" data-type="indexterm" data-primary="network disruption, chaos engineering experiments in" id="id850"/> communication between services or with external dependencies.</p></dd>
              <dt>Packet loss</dt><dd><p>Simulate the loss of network packets to test how your application handles unreliable connections.</p></dd>
              <dt>Network partition</dt>
              <dd><p>Isolate parts of your network to simulate connectivity issues between services or availability zone outages.</p></dd>
              <dt>DNS failures</dt>
              <dd><p>Simulate DNS resolution problems to test how your application handles DNS outages or incorrect responses.</p></dd>
            </dl>
          </dd>

            <dt>Infrastructure failure</dt>
            <dd>
            <dl>
              <dt>Node failure</dt>
              <dd><p>Terminate or shut down VMs or containers<a contenteditable="false" data-type="indexterm" data-primary="infrastructure failure, chaos engineering experiments in" id="id851"/> to simulate hardware failures.</p></dd>
              <dt>Pod failure (Kubernetes)</dt>
              <dd><p>Kill or evict pods to test the self-healing capabilities of your Kubernetes deployments.</p></dd>
              <dt>Availability zone outage</dt>
              <dd><p>Simulate the failure of an entire availability zone to test your disaster recovery plan and multiregion deployments.</p></dd>
              <dt>Inference layer attacks</dt>
              <dd><p>Simulate GPU memory exhaustion during ML model serving.</p></dd>
            </dl>
          </dd>

            <dt>Application-level faults</dt>

            <dd>
            <dl>
              <dt>Service failure</dt>
              <dd><p>Stop or crash specific services within your application to test fault<a contenteditable="false" data-type="indexterm" data-primary="application-level faults, chaos engineering experiments in" id="id852"/> tolerance and service degradation.</p></dd>
              <dt>Function failure</dt>
              <dd><p>Introduce errors or exceptions within specific functions or methods to test error handling and recovery mechanisms.</p></dd>
              <dt>Data corruption</dt>
              <dd><p>Corrupt data in a database or storage system to test your data integrity and recovery processes.</p></dd>
            </dl>
          </dd>

            <dt>State management</dt>
            <dd>
            <dl>
              <dt>Time travel</dt>
              <dd><p>Manipulate the system clock to simulate time shifts<a contenteditable="false" data-type="indexterm" data-primary="state management, chaos engineering experiments in" id="id853"/>, testing how your application handles time-sensitive operations or scheduled tasks.</p></dd>
              <dt>State injection</dt>
              <dd><p>Inject specific data or states into your application to test its behavior under unusual conditions. Use GenAI to create plausible corrupt data entries matching schema constraints.</p></dd>
            </dl>
         </dd>

            <dt>Dynamic scenario generation using AI</dt>
            <dd>
            <dl>
              <dt>Architecture modeling</dt>
              <dd><p>Use AI to analyze service dependencies<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="dynamic scenario generation for chaos engineering experiments" id="id854"/> (e.g., Redis cache → payment gateway → database) to create failure chains mirroring production <span class="keep-together">environments</span>.</p></dd>
              <dt>Generative adversarial networks</dt>
              <dd><p>Create novel failure modes by pitting AI models against each other to discover unexplored vulnerability combinations.</p></dd>
            </dl>
          </dd>
          </dl>

          <p>The more types of experiments we try, the more we can learn about weaknesses in our system and how we can strengthen our resiliency. </p>
          <p>Newer tools can go beyond offering catalogs to analyze your system architecture to suggest targeted experiments that expose potential weaknesses specific to your setup. For example, for software built with a microservices architecture, a chaos engineering tool might analyze network traffic and dependencies to identify critical services and suggest experiments that target these specifically. A modern tool might also recommend injecting latency or errors into API calls between services to test resilience to communication disruptions. </p>
          <p>For applications deployed with Kubernetes<a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="chaos engineering testing tools" id="id855"/>, the tool could analyze your Kubernetes deployments and suggest experiments that target specific pods, deployments, or namespaces to test replica scaling, resource limits, and health checks. Tools like Red Hat’s Krkn use AI to profile Kubernetes pods to prioritize network-intensive services for partition tests. In the case of multiregion deployments, a modern tool might analyze your multiregion setup and suggest experiments that simulate regional failures or network partitions to test your disaster recovery plan and the ability of your application to failover to another region<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_chaosengineeringtoolsfor610398" id="id856"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Learning from Others"><div class="sect2" id="chapter_6_learning_from_others_1749354010916761">
          <h2>Learning from Others</h2>
          <p>Paying close attention to industry-wide incidents<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="learning from others’ failures" id="id857"/>, particularly those affecting companies with similar tech stacks, is crucial for proactive risk mitigation. For instance, an OpenAI outage on December 11, 2024, serves as a stark reminder of how seemingly minor deployments can have cascading consequences.</p>
          <p class="fix_tracking">In this case, a new telemetry service overwhelmed the company’s Kubernetes control plane, triggering DNS failures that brought down its API, ChatGPT, and Sora platforms for hours. The impact was widespread and long-lasting: for several hours, developers and users couldn’t access the services they rely on. Engineers identified the root cause within minutes but faced a major hurdle—without access to the <span class="keep-together">Kubernetes</span> control plane, rolling back or fixing the deployment was extremely challenging. </p>
          <p>Let’s look at a few targeted chaos engineering experiments to see how these cascading failures might have been prevented.</p>
          <section data-type="sect3" data-pdf-bookmark="Experiment 1: Control plane overload simulation"><div class="sect3" id="chapter_6_experiment_1_control_plane_overload_simulation_1749354010916808">
            <h3>Experiment 1: Control plane overload simulation</h3>
            <p>First, we design an experiment<a contenteditable="false" data-type="indexterm" data-primary="control plane overload simulation" id="id858"/> to test our Kubernetes API server resilience. In this experiment, we would intentionally flood the Kubernetes API server with a high volume of read/write operations to mimic what the new telemetry service did in production. By running this test on a staging environment with a production-like scale, we could have spotted the exact threshold where the API server starts to fail. This early detection would inform better load limiting, improved alerting, and possibly a safer phased rollout strategy.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Experiment 2: DNS failure testing"><div class="sect3" id="chapter_6_experiment_2_dns_failure_testing_1749354010916854">
            <h3>Experiment 2: DNS failure testing</h3>
            <p>This experiment would involve introducing latency or failures in the DNS<a contenteditable="false" data-type="indexterm" data-primary="DNS failure testing, chaos engineering example" id="id859"/> resolution process—specifically targeting the components responsible for service discovery. Running this experiment helps confirm that essential services can continue functioning even if DNS is disrupted. We will discover if our caches, fallback mechanisms, or alternative routing strategies are sufficient. If not, we would know to invest in those areas before a real outage hits.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Example 3: Break-glass access drills"><div class="sect3" id="chapter_6_example_3_break_glass_access_drills_1749354010916901">
            <h3>Example 3: Break-glass access drills</h3>
            <p>This last experiment (or drill) involves simulating a situation<a contenteditable="false" data-type="indexterm" data-primary="break-glass access drills, chaos engineering" id="id860"/> where engineers are locked out of the Kubernetes API under heavy load. By practicing emergency access methods—like having dedicated out-of-band channels or specialized tooling—teams can quickly revert or disable problematic deployments when the standard control plane is inaccessible. If this drill had been done beforehand, teams would have known exactly how to remove the faulty telemetry service within minutes, minimizing downtime.</p>
          </div></section>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Service-Level Objectives and Service Resiliency"><div class="sect1" id="chapter_6_service_level_objectives_and_service_resiliency_1749354010916951">
        <h1>Service-Level Objectives and Service Resiliency</h1>
        <p>We see how chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="SLOs and service resiliency" id="xi_chaosengineeringSLOsandserviceresiliency620540"/><a contenteditable="false" data-type="indexterm" data-primary="SLOs (service level objectives)" id="xi_SLOsservicelevelobjectives620540"/><a contenteditable="false" data-type="indexterm" data-primary="reliability" data-secondary="targeting for and shared ownership of" id="xi_reliabilitytargetingforandsharedownershipof620540"/><a contenteditable="false" data-type="indexterm" data-primary="service level objectives (SLOs)" id="xi_servicelevelobjectivesSLOs620540"/> helps us uncover weaknesses and build more resilient systems. But how do we define “resilient”? How do we measure and track whether our systems are meeting our reliability goals? This is where SLOs and service-level indicators (SLIs)<a contenteditable="false" data-type="indexterm" data-primary="service level indicators (SLIs)" id="id861"/><a contenteditable="false" data-type="indexterm" data-primary="SLIs (service level indicators)" id="id862"/> come in. Together, these provide the framework for defining and measuring the reliability of our services, giving us a clear target to aim for and a way to track our progress.</p>
        <p>SLOs are the targets we set for the reliability of our services. SLIs are the specific metrics we use to measure whether we’re meeting those targets. SLOs are typically expressed as a percentage of time or number of requests that must meet the defined SLI criteria. For example, <em>99.9% of requests should have a latency of under 200 <span class="keep-together">milliseconds</span></em>. SLIs are the specific, measurable metrics that reflect the performance of your service from a user’s perspective. They quantify aspects like availability, latency, error rate, throughput, and other relevant factors. </p>
        <p>In essence, SLIs are <em>what</em> you measure, and SLOs are the <em>targets</em> you set for those measurements.</p>
        <section data-type="sect2" data-pdf-bookmark="Establishing Reliability Targets"><div class="sect2" id="chapter_6_establishing_reliability_targets_1749354010916999">
          <h2>Establishing Reliability Targets</h2>
          <p>When establishing reliability targets, it’s essential to align them with the overarching business needs. Monitoring and observability solutions provide many SLI metrics, but it is important to prioritize those that accurately reflect how your customers experience your applications. The goal is not to track every individual service, but to focus on those services that are critical to the customer experience.</p>
          <p>Common SLIs include “the four golden signals”: </p>
          <dl>
            <dt>Request latency</dt>
            <dd>
              <p>The time<a contenteditable="false" data-type="indexterm" data-primary="request latency SLI" id="id863"/> taken to process a user request</p>
            </dd>
            <dt>Throughput</dt>
            <dd>
              <p>The number of requests<a contenteditable="false" data-type="indexterm" data-primary="throughput SLI" id="id864"/> processed per second</p>
            </dd>
            <dt>Error rate</dt>
            <dd>
              <p>The percentage of failed<a contenteditable="false" data-type="indexterm" data-primary="error rate SLI" id="id865"/> requests</p>
            </dd>
            <dt>Saturation</dt>
            <dd>
              <p>The utilization percentage<a contenteditable="false" data-type="indexterm" data-primary="saturation SLI" id="id866"/> of the system</p>
            </dd>
          </dl>
          <p>Consider carefully how to implement each of these within your system. For instance, when measuring latency (response time), you can choose to track all transactions or focus on a subset of the most crucial ones, such as login, payment submission, or adding items to a shopping cart. Again, select a metric that provides a meaningful representation of your customers’ experience.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Shared Ownership of System Reliability"><div class="sect2" id="chapter_6_shared_ownership_of_system_reliability_1749354010917045">
          <h2>Shared Ownership of System Reliability</h2>
          <p>In <a data-type="xref" href="ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299">Chapter 1</a>, we introduced DevOps as practices that combine software development (Dev) and IT operations (Ops) concerns. Nowhere are shared ownership<a contenteditable="false" data-type="indexterm" data-primary="shared ownership of system reliability" id="id867"/> and collaboration more important than in ensuring system reliability. SLOs are a great example of this shared responsibility. Development, operations, and reliability teams should work together to define SLOs. The collaboration establishes an understanding of acceptable system performance and creates a common goal for everyone to work toward. SLOs then act as a guide for making decisions that balance the need for rapid development (velocity) with the need for stable and reliable systems.</p>
          <p>With this shared understanding, developers can prioritize features that maintain reliability, knowing how their work impacts overall system performance. At the same time, operations teams gain the context they need to effectively support the application. If an SLO is breached, it triggers activities that encourage engineering teams to stabilize the service before releasing new features. This helps prevent a cycle of instability and ensures that reliability remains a top priority.</p>
          <p>A collaborative approach to designing, prioritizing, and conducting the chaos engineering experiments themselves brings teams together. All teams benefit from the insights gained from these experiments and from working together to address when failures are found.</p>
          <p>Modern tools facilitate this collaborative approach to system reliability. Monitoring platforms, incident management systems, and communication tools give a shared visibility into system performance and potential issues. Real-time data and automated alerts empower both Dev and Ops teams to respond quickly to incidents. More importantly, these tools foster a culture of proactive problem-solving (such as data-driven prioritization, real-time collaboration triage, etc.), where teams can identify and address potential issues before they impact users<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_chaosengineeringSLOsandserviceresiliency620540" id="id868"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_SLOsservicelevelobjectives620540" id="id869"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_reliabilitytargetingforandsharedownershipof620540" id="id870"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_servicelevelobjectivesSLOs620540" id="id871"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Error Budgets and Their Role in Reliability and Innovation"><div class="sect1" id="chapter_6_error_budgets_and_their_role_in_reliability_and_in_1749354010917096">
        <h1>Error Budgets and Their Role in Reliability and Innovation</h1>
        <p>We’ve learned how chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="error budgets" id="xi_chaosengineeringerrorbudgets624247"/><a contenteditable="false" data-type="indexterm" data-primary="error budgets, chaos engineering" id="xi_errorbudgetschaosengineering624247"/><a contenteditable="false" data-type="indexterm" data-primary="reliability" data-secondary="error budget’s role in" id="xi_reliabilityerrorbudgetsrolein624247"/> helps us proactively find system weaknesses, and how SLOs and SLIs provide a clear framework for defining our reliability goals and measuring whether our systems are meeting those targets. <a href="https://oreil.ly/K0JqJ">Error budgets</a> enter to provide a safety net. </p>
        <p>Error budgets represent the maximum amount of unreliability or downtime that a service can have while still meeting its SLOs. By tolerating minor hiccups in the pursuit of rapid innovation, error budgets acknowledge that perfection is unattainable, and instead help us achieve an acceptable level of reliability that balances these two competing priorities.</p>
        <p>Let’s look at how this works by returning to our e-commerce example. Imagine we’ve set an SLO of 99.9% for website logins taking less than 300 ms. Over a one-week period, this translates to a maximum allowable SLO violation time of 10.08 minutes. This is our error budget. How does that impact us? In the event that the error budget burns down to zero, we will stop or slow down deployments of new software and focus on stabilizing the system while our error budget replenishes. Not only does the state of our error budget impact our deployment priorities, but it also factors into chaos testing priorities.</p>
        <section data-type="sect2" data-pdf-bookmark="Monitoring to Inform Chaos Testing Experiments"><div class="sect2" id="chapter_6_monitoring_to_inform_chaos_testing_experiments_1749354010917144">
          <h2>Monitoring to Inform Chaos Testing Experiments</h2>
          <p>Keeping a close eye on your SLIs<a contenteditable="false" data-type="indexterm" data-primary="service level indicators (SLIs)" id="id872"/><a contenteditable="false" data-type="indexterm" data-primary="SLIs (service level indicators)" id="id873"/> does more than just alert you to immediate problems—it reveals potential weaknesses in your system. For example, if you notice your system constantly pushing against latency limits, draining your error budget, your system might be struggling to keep up in high-traffic scenarios. This suggests a good area to focus your chaos experiments on. By simulating those high-traffic, high-latency situations, you can see how your system holds up under pressure and make sure it can still meet its SLOs during peak usage. </p>
          <p>With modern tools, you can automate this by automatically triggering chaos tests based on these patterns, so you can continuously test and improve your system’s resilience without lifting a finger. Modern platforms can correlate SLI trends with chaos test recommendations using AI, thus increasing test coverage significantly.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Strategic Use of Error Budgets for Chaos Testing Experiments"><div class="sect2" id="chapter_6_strategic_use_of_error_budgets_for_chaos_testing_e_1749354010917192">
          <h2>Strategic Use of Error Budgets for Chaos Testing Experiments</h2>
          <p>Error budgets are not merely a safety net for occasional failures; they are a tool for managing risk<a contenteditable="false" data-type="indexterm" data-primary="risk mitigation" id="id874"/>. Using our e-commerce website example, we think of the 10.08-minute error budget as a resource to be spent wisely. In this section we’ll look at how to proactively use this budget to conduct chaos experiments.</p>
          <section data-type="sect3" data-pdf-bookmark="Prioritizing chaos experiments in alignment with your available error budget"><div class="sect3" id="chapter_6_prioritizing_chaos_experiments_in_alignment_with_y_1749354010917241">
            <h3>Prioritizing chaos experiments in alignment with your available error budget</h3>
            <p>Effective chaos engineering requires consideration of your available error budget. When your error budget is healthy, your runway is long. You have more freedom to conduct aggressive experiments, simulating large-scale failures or pushing critical system components to their limits. This might involve testing failover mechanisms, injecting network latency, or even simulating the complete outage of a core service.</p>
            <p>As your error budget dwindles, it’s essential to shift focus toward smaller-scale experiments that carry less risk of significant disruption. These might involve testing individual components in isolation, simulating minor network issues, or validating the resilience of recent changes. Prioritizing experiments in this way ensures that you can continue to learn and improve without jeopardizing overall system stability.</p>
            <p>Modern automation tools can help. By analyzing your error budget in real time, these tools can recommend appropriate experiments based on your available “room for failure.” This allows you to maintain a balance between proactive testing and service reliability, ensuring that your chaos engineering efforts are both insightful and responsible.</p>
          </div></section>
          <section data-type="sect3" class="pagebreak-before" data-pdf-bookmark="Protect your error budget by utilizing AI-augmented dry runs and simulations"><div class="sect3" id="chapter_6_protect_your_error_budget_by_utilizing_ai_augmente_1749354010917289">
            <h3 class="less_space">Protect your error budget by utilizing AI-augmented dry runs and simulations</h3>
            <p>Simulating first is another strategy to minimize the risk of unintended consequences during chaos experiments<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="remediation to roll back experiments" id="id875"/>. This is especially important when up against a dwindling error budget. The practice of AI-augmented “dry running” chaos experiments involves simulating experiments in a controlled environment, using system models or replicas, to assess their potential impact before executing them in production, and using AI remediation agents to roll back experiments if anomaly detection thresholds breach predefined limits. By identifying potential issues and refining experiment parameters beforehand, teams can reduce the likelihood of causing significant disruptions that could drain your error budget and cause significant disruptions<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_chaosengineeringerrorbudgets624247" id="id876"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_errorbudgetschaosengineering624247" id="id877"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_reliabilityerrorbudgetsrolein624247" id="id878"/>.</p>
          </div></section>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Integrating Chaos Engineering and SLOs into CI/CD Pipelines"><div class="sect1" id="chapter_6_integrating_chaos_engineering_and_slos_into_ci_cd_1749354010917338">
        <h1>Integrating Chaos Engineering and SLOs <span class="keep-together">into CI/CD Pipelines</span></h1>
        <p>Reliability<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="CI/CD pipeline integration" id="xi_chaosengineeringCICDpipelineintegration626723"/><a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="chaos engineering integration" id="xi_CICDpipelinechaosengineeringintegration626723"/> issues are primarily driven by change, changes to our applications, or changes to your infrastructure. Google DevOps Research and Assessment (DORA)<a contenteditable="false" data-type="indexterm" data-primary="DevOps Research and Assessment (DORA)" id="id879"/><a contenteditable="false" data-type="indexterm" data-primary="DORA (DevOps Research and Assessment)" id="id880"/> defines the change failure rate (CFR) metric<a contenteditable="false" data-type="indexterm" data-primary="change failure rate (CFR) metric" id="id881"/><a contenteditable="false" data-type="indexterm" data-primary="metrics" data-secondary="CFR" id="id882"/>, which gives us another view of the challenge. The CFR describes how often our changes, such as new code deployments or infrastructure updates, introduce problems in production that require hotfixes or rollbacks. The DORA 2024 State of DevOps report indicates that 80% of surveyed teams have average CFRs of 20% of their releases. In fact, 25% of teams have CFRs averaging an alarming 40% of releases. </p>
        <p>In addition, we must consider the time and cost to remediate each change failure. The failed deployment recovery time metric<a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="recovery time metric" id="id883"/><a contenteditable="false" data-type="indexterm" data-primary="metrics" data-secondary="recovery time" id="id884"/> (replacing the similar mean time to recovery [MTTR] metric) focuses on how quickly an organization can recover from failures. This gives us a sense of the challenges teams face on this front. While many teams are able to remediate in less than a day, 25% require a week to a month to replace defective software. </p>
        <p>Throughout earlier chapters, we’ve looked at strategies to prevent defects from getting to production. We test at every stage in our delivery pipelines, executing tests of every type. We take care in managing our environments. We guard against configuration drift<a contenteditable="false" data-type="indexterm" data-primary="configuration drift" id="id885"/> with practices like GitOps, combined with IaC. And we conduct chaos engineering testing in pre-production and production environments to help us find weaknesses in our systems. Yet, despite our best efforts, occasional defects that require fast remediation are inevitable. This is where continuous resilience comes in. </p>
        <p>Just as continuous integration and continuous delivery are about using automation to build, test, and deploy our code, continuous resilience<a contenteditable="false" data-type="indexterm" data-primary="continuous resilience, CI/CD pipeline" id="xi_continuousresilienceCICDpipeline6270152"/><a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="continuous" id="xi_resiliencycontinuous6270152"/> is about automating our resiliency practices by adding chaos engineering experiments to our CI/CD pipelines. Doing so means we are not just testing functionality, but actively and constantly evaluating how changes stand to impact the stability of our systems. Using AI agents for DevOps, chaos experiments can be intelligently integrated into CI/CD pipelines.</p>
        <p>In <a data-type="xref" data-xrefstyle="select:nopage" href="#chapter_6_scaling_your_chaos_engineering_practices_1749354010917388">“Scaling Your Chaos Engineering Practices”</a> we’ll explore how to scale a chaos engineering practice by incorporating it into our delivery pipeline with the help of modern tools. We’ll look at how to prioritize the experiments to add to your pipeline, and best practices for securing and governing chaos experiments in your pipeline.</p>
        <section data-type="sect2" data-pdf-bookmark="Scaling Your Chaos Engineering Practices"><div class="sect2" id="chapter_6_scaling_your_chaos_engineering_practices_1749354010917388">
          <h2>Scaling Your Chaos Engineering Practices</h2>
          <p>Organizations start their chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="scalability" data-secondary="chaos engineering practices" id="xi_scalabilitychaosengineeringpractices627457"/> journey in different ways. Often a single team or two will adopt an open source tool and introduce experiments in a small pocket of an organization. An organization may host periodic chaos engineering “game days.” There are all-hands-on-deck, planned events where teams deliberately inject a series of failures into systems to practice incident response and identify weaknesses in a controlled environment. These are typically infrequent and responses are reactive to issues that are discovered. </p>
          <p>The trick to implementing continuous resiliency at scale, across an organization, can be a matter of choosing the right tooling. While both open source and proprietary solutions offer valuable capabilities, organizations should carefully evaluate their requirements. Some enterprise environments may need specific features like advanced security controls, comprehensive audit trails, and  RBAC—features that may vary in availability and maturity across open source solutions.</p>
          <p>This challenge was acutely felt by a leading fintech company processing over a billion daily payment transactions. Faced with increasing transaction failures during peak demand, it sought a solution to improve the reliability of its complex platform supporting 20+ financial products.</p>
          <p>The company’s choice of a modern chaos engineering tool was instrumental in overcoming the obstacles of scaling its chaos engineering practices. The tool it chose (in this case, Harness Chaos Engineering) included an extensive library of prebuilt experiments that simplified the work of automating and orchestrating numerous chaos experiments. In addition, comprehensive analysis and reporting capabilities gave the company quick insights into the resiliency of its systems.</p>
          <p>The company began by focusing on a single critical service that handled nine million daily payment requests. It pinpointed fault-tolerant targets within the intricate infrastructure, laying the groundwork for a controlled rollout of resilience testing. By prioritizing the automation of chaos experiments within delivery pipelines and production environments, it addressed the root causes of transaction failures and built a foundation for continuous resilience.</p>
          <p class="pagebreak-before">Through its automated resilience testing platform, the company was able to expand the breadth of its testing to uncover vulnerabilities in service recovery, optimize application design patterns, and fine-tune configurations. The results were significant: a 16× reduction in failed transactions, MTTR reduced to 10 minutes, and a 10× improvement in customer satisfaction. Without modern tooling that offers security, templates and automation, and orchestration, it would have been impossible to roll out chaos engineering across the organization and achieve these results in the short time it took<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_scalabilitychaosengineeringpractices627457" id="id886"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Adding Chaos Engineering Experiments and SLOs to Your CI/CD Pipeline"><div class="sect2" id="chapter_6_adding_chaos_engineering_experiments_and_slos_to_y_1749354010917449">
          <h2>Adding Chaos Engineering Experiments <span class="keep-together">and SLOs to Your CI/CD Pipeline</span></h2>
          <p>To solidify your resilience<a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="chaos engineering and SLOs" id="xi_resiliencychaosengineeringandSLOs628341"/> strategy, integrate SLOs as reliability guardrails within your CI/CD pipeline. Think of SLOs as the brakes on a race car—essential for maintaining control while pushing for maximum speed. Development teams, much like race car drivers, strive for rapid deployments, but without robust SLOs in place, they risk crashing their systems with unchecked changes. By monitoring key metrics, you can automatically block deployments that breach these thresholds or exhaust their error budget. This approach can accelerate your development velocity without sacrificing stability.</p>
          <p>When adding chaos engineering experiments to your CI/CD pipeline, keep in mind two measures to guide your progress: resilience scores and resilience coverage. Resilience scores are simply how well your services perform against the experiments you apply in QA and production. Resilience coverage, similar to code coverage, assesses how many more experiments are needed to comprehensively evaluate system resilience, guiding the creation of a practical number of tests. Together, these metrics provide a holistic view of resilience, enabling all teams to contribute to and measure progress toward continuous resilience goals.</p>
          <p class="fix_tracking">Start by adding experiments that test against known resilience conditions, ensuring your resilience score remains stable with each new deployment. Slowly increase your resilience coverage by adding experiments to test new conditions. If increasing the resilience coverage means that the resilience scores are reduced, determine if the failed chaos experiment warrants stopping the pipeline or if action can be taken in parallel.</p>
          <p>Next, add experiments that address changes to the platform on which the target deployments run. For example, when upgrading underlying platforms like <span class="keep-together">Kubernetes</span>, incorporate chaos experiments into your CI/CD pipeline to proactively identify potential weaknesses and compatibility issues. This helps prevent latent issues from impacting applications in the future and ensures a smooth transition during platform updates. By catching these issues early, you can avoid costly incidents and maintain continuous resilience.</p>
          <p>Add experiments to the pipeline that validate the deployments against previous production incidents and alerts as incidents occur. Lastly, add experiments that validate the deployments against configuration changes to the target infrastructure. This is another scenario where the resilience tests that passed earlier will start failing because the target environment changed through a higher or lower configuration.</p>
          <p>As you invest in creating and fine-tuning your experiments, treat them like any other piece of software: version them, test them, and manage their lifecycle in a repository. This ensures your chaos engineering practice remains effective and adapts to changes in your systems. Centralized repositories facilitate collaboration and the sharing of these experiments, promoting consistency and best practices across teams<a contenteditable="false" data-type="indexterm" data-primary="service level objectives (SLOs)" id="id887"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_resiliencychaosengineeringandSLOs628341" id="id888"/><a contenteditable="false" data-type="indexterm" data-primary="SLOs (service level objectives)" id="id889"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Security and Governance for Chaos Engineering"><div class="sect2" id="chapter_6_security_and_governance_for_chaos_engineering_1749354010917500">
          <h2>Security and Governance for Chaos Engineering</h2>
          <p>Clearly, chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="chaos engineering" id="id890"/><a contenteditable="false" data-type="indexterm" data-primary="governance" data-secondary="chaos engineering" id="id891"/> is a powerful approach, but careless experimentation has the potential to cause serious harm to both system resilience and trust in your chaos engineering program. By integrating it with your security and governance frameworks, you can define the guardrails you need to ensure experiments are conducted responsibly.</p>
          <p>Just like tech debt, resilience debt can accumulate in your production services. Every alert, incident, hotfix, or workaround—like simply throwing more resources at a problem—contributes to this debt. Instead of addressing the root cause, these quick fixes often mask underlying issues and create a false sense of stability. </p>
          <p>Modern chaos engineering tools can help you establish and enforce policies to combat this. For example, we could set a policy that mandates a corresponding chaos experiment for every production incident related to component misbehavior, network issues, API failures, or unexpected load. This experiment, integrated into your CI/CD pipeline, would need to be validated within a specific timeframe, say, within 60 days of the incident. Such a policy would not only enforce a discipline of addressing resilience debt but also encourage developers and QA teams to prioritize fixing production code over pushing new features that might further exacerbate the problem.</p>
          <p>In addition to using policies to manage resilience debt, you can use security governance policies to prevent unauthorized experiments, restrict access to critical systems, and limit testing by environment, time window, personnel, or even fault type. By automating oversight and integrating these policies into your CI/CD pipelines, you can increase resilience coverage while reducing risk<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_continuousresilienceCICDpipeline6270152" id="id892"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_resiliencycontinuous6270152" id="id893"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="The Future of AI-Native Chaos Engineering in Service Reliability"><div class="sect1" id="chapter_6_the_future_of_ai_native_chaos_engineering_in_servi_1749354010917554">
        <h1>The Future of AI-Native Chaos Engineering <span class="keep-together">in Service Reliability</span></h1>
        <p>The future of chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="AI-native software delivery" data-secondary="chaos engineering" id="id894"/> promises even greater sophistication and integration within service reliability practices. Tools such as Harness Chaos Engineering<a contenteditable="false" data-type="indexterm" data-primary="Harness" data-secondary="Chaos Engineering" id="id895"/> and Chaos Monkey<a contenteditable="false" data-type="indexterm" data-primary="Chaos Monkey" id="id896"/> will not only automate experiments but also leverage AI/ML to predict their impact, analyze system behavior under stress, and recommend optimal mitigation strategies. This intelligent automation<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="chaos engineering" id="id897"/> will minimize risk, allowing teams to conduct more complex experiments with greater confidence and efficiency. Advancements in observability and tracing will provide deeper insights into system dynamics, enabling more precise identification of vulnerabilities and faster recovery from disruptions.</p>
        <p>As systems grow increasingly complex, with distributed architectures and microservices becoming ever-present, chaos engineering will play a crucial role in ensuring their resilience. Even large language model–based multiagentic systems can be <a href="https://oreil.ly/g7tjd">enhanced using chaos engineering</a>. By combining chaos testing with AI-powered analysis and automated<a contenteditable="false" data-type="indexterm" data-primary="ChaosEater" id="id898"/> remediation (for example, <a href="https://oreil.ly/IKlJW"><span class="keep-together">ChaosEater</span></a>), we will be able to address potential failures faster and with greater precision, minimizing downtime and maintaining high levels of service <span class="keep-together">reliability</span><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_chaosengineeringCICDpipelineintegration626723" id="id899"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipelinechaosengineeringintegration626723" id="id900"/>. </p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="chapter_6_summary_1749354010917599">
        <h1>Summary</h1>
        <p>In this chapter, we explored chaos engineering as a methodical approach to building and validating system resilience. We learned to design and execute experiments responsibly, using SLOs and error budgets to balance innovation and stability. By integrating chaos engineering into CI/CD pipelines and leveraging modern tooling, organizations can proactively identify weaknesses, learn from failures, and continuously improve resilience. Ultimately, chaos engineering empowers us to create more robust systems that meet the demands of today’s complex digital world<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_chaosengineering6332" id="id901"/>. With these principles in place, the next step is to apply them seamlessly as part of your deployment process. Let’s explore how to ensure stability during production rollouts.</p>
      </div></section>
    </div></section></div></div></body></html>