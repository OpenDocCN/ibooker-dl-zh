- en: 7 An end-to-end example using XGBoost
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用XGBoost的端到端示例
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Gathering and preparing data from the internet, using generative AI to help
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从互联网收集和准备数据，使用生成式AI来帮助
- en: Drafting a baseline and first tentative model to be optimized
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起草一个基线模型和第一个尝试优化的模型
- en: Figuring out how the model works and inspecting it
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型的工作原理并检查它
- en: This chapter concludes our overview of classical machine learning for tabular
    data. To wrap things up, we’ll work through a complete example from the field
    of data journalism. Along the way, we’ll summarize all the concepts and techniques
    we’ve used so far. We will also use a generative AI tool, ChatGPT, to help you
    get the job done and demonstrate a few use cases where having a large language
    model (LLM) can improve your work with tabular data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了我们对表格数据的经典机器学习的概述。为了总结，我们将通过数据新闻领域的完整示例进行操作。在这个过程中，我们将总结到目前为止我们使用过的所有概念和技术。我们还将使用生成式AI工具ChatGPT来帮助你完成任务，并展示一些大型语言模型（LLM）如何改善你处理表格数据的工作的用例。
- en: We will finally build a model to predict prices, this time using a regression-based
    approach. Doing this will help us understand how the model works and why it performs
    in a particular manner to gain further insights into the pricing dynamics for
    Airbnb listings and challenge our initial hypothesis regarding how pricing happens
    for short-term rentals.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终将构建一个预测价格的模型，这次使用基于回归的方法。这样做将帮助我们理解模型的工作原理以及为什么它以特定的方式表现，从而进一步深入了解Airbnb列表的定价动态，并挑战我们对短期租赁定价发生的初始假设。
- en: 7.1 Preparing and exploring your data
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 准备和探索你的数据
- en: To get started, we’ll focus on a different dataset as we continue our analysis
    of short-term and long-term Airbnb rental listings in New York City. This dataset
    comes directly from the Inside Airbnb Network initiative ([http://insideairbnb.com/](http://insideairbnb.com/)),
    “a mission-driven project that provides data and advocacy about Airbnb’s effect
    on residential communities.” We will also be using public data from other online
    services, such as Foursquare ([https://foursquare.com](https://foursquare.com)),
    the social network and geolocation technology company.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们将专注于不同的数据集，因为我们继续分析纽约市的短期和长期Airbnb租赁列表。这个数据集直接来自Inside Airbnb Network倡议（[http://insideairbnb.com/](http://insideairbnb.com/))，“一个以使命为导向的项目，提供关于Airbnb对住宅社区影响的数据和倡导。”我们还将使用来自其他在线服务的公共数据，例如Foursquare（[https://foursquare.com](https://foursquare.com)），社交网络和地理位置技术公司。
- en: Following the data acquisition phase, we will organize and conduct comprehensive
    feature engineering based on relevant business hypotheses to extract valuable
    insights for our modeling stage. During this process, we will also perform basic
    exploratory analysis on our predictors and target variables, making necessary
    adjustments or exclusions of examples and features to ensure we get the optimal
    data for our project.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集阶段之后，我们将根据相关的业务假设进行组织和执行综合的特征工程，以从我们的建模阶段提取有价值的见解。在这个过程中，我们还将对我们的预测变量和目标变量进行基本的探索性分析，对示例和特征进行必要的调整或排除，以确保我们获得项目最优的数据。
- en: 7.1.1 Using generative AI to help prepare data
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 使用生成式AI来帮助准备数据
- en: ChatGPT is an advanced language model developed by OpenAI. To create and train
    a generative pretrained transformer (GPT) model like ChatGPT, OpenAI applied vast
    amounts of diverse internet text to help the model learn to understand and generate
    human-like text by predicting the next word in a sequence of words based on its
    contextual understanding. This pretraining allows ChatGPT to capture grammar,
    context, and even nuanced information, but it is not enough to make it a helpful
    assistant in every circumstance. In fact, these models have the potential to produce
    outputs that are inaccurate or harmful or contain toxic content. The reason for
    this is that the training dataset—that is, the internet—contains text that is
    varied and, at times, unreliable. To enhance the safety, utility, and alignment
    of ChatGPT models, a method known as reinforcement learning from human feedback
    is employed. In the reinforcement learning from human feedback process, human
    labelers provide feedback illustrating the preferred model behavior, and they
    evaluate multiple outputs produced by the model through ranking. This data is
    subsequently utilized to fine-tune GPT-3.5 further, refining its responses based
    on the human feedback.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是由OpenAI开发的高级语言模型。为了创建和训练一个像ChatGPT这样的生成预训练转换器（GPT）模型，OpenAI使用了大量的多样化互联网文本，帮助模型通过预测一系列词语中的下一个词语来学习理解和生成类似人类的文本，这是基于其对上下文的理解。这种预训练使得ChatGPT能够捕捉语法、上下文，甚至细微的信息，但这并不足以使其在所有情况下都成为一个有用的助手。实际上，这些模型有可能产生不准确或有害的输出，或者包含有毒内容。这是因为训练数据集——即互联网——包含的文本是多样化的，有时是不可靠的。为了提高ChatGPT模型的安全性、实用性和一致性，采用了名为基于人类反馈的强化学习的方法。在基于人类反馈的强化学习过程中，人类标注者提供反馈，说明首选的模型行为，并通过排名评估模型产生的多个输出。随后，这些数据被用来进一步微调GPT-3.5，根据人类反馈来优化其响应。
- en: To use the free version of ChatGPT (at the moment, that is ChatGPT 3.5, which
    is updated with information up to January 2022), you must first create an account
    at [https://chat.openai.com](https://chat.openai.com). Once you have an account,
    you can start using ChatGPT by simply entering a prompt. A prompt, in the context
    of an LLM like ChatGPT, is a written instruction or input provided to the model
    to generate a specific output. It serves as a query or request that guides the
    model in producing a relevant response. Prompts can vary in complexity, ranging
    from simple commands to more detailed descriptions or inquiries, and they play
    a crucial role in shaping the nature of the language model’s output. The prompt’s
    quality and clarity significantly influence the generated content’s accuracy and
    relevance, but selecting the right prompt is not always straightforward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用ChatGPT的免费版本（目前是ChatGPT 3.5，信息更新至2022年1月），您必须首先在[https://chat.openai.com](https://chat.openai.com)创建一个账户。一旦您有了账户，您只需输入一个提示就可以开始使用ChatGPT。在一个像ChatGPT这样的LLM（大型语言模型）的上下文中，提示是一个提供给模型的书面指令或输入，以生成特定的输出。它作为一个查询或请求，引导模型产生相关的响应。提示的复杂度各不相同，从简单的命令到更详细的描述或询问，它们在塑造语言模型输出性质方面发挥着关键作用。提示的质量和清晰度在很大程度上影响着生成内容的准确性和相关性，但选择正确的提示并不总是那么简单。
- en: The effectiveness of different prompts can vary depending on the specific language
    model they are designed for. Each language model has its own strengths, weaknesses,
    and nuances, making it essential to tailor prompts accordingly. When working with
    ChatGPT, for instance, we obtained the best results starting with simple prompts,
    evaluating their results, and then proceeding by adding more specifications to
    the prompt to refine the results toward our expectations. ChatGPT tend to work
    better when you tell it to “write,” “create,” “show me how to,” or “summarize.”
    Sometimes showing examples and how you expect ChatGPT to elaborate them is quite
    helpful. Also, let ChatGPT know your expectations in terms of the answer, like
    how long the response should be, what information it should include, if you want
    just code as a result or just text, and how the answer should be structured in
    return; for instance, you could ask using the JSON format or a Python style list.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同提示语的有效性可能因它们针对的具体语言模型而异。每种语言模型都有其自身的优势、劣势和细微差别，因此根据需要调整提示语至关重要。例如，在与ChatGPT合作时，我们从简单的提示语开始，评估其结果，然后通过添加更多具体说明到提示语中，以细化结果，使其更符合我们的预期。ChatGPT在被告知“写”、“创建”、“展示如何”或“总结”时往往表现更好。有时展示示例以及你期望ChatGPT如何展开这些示例是非常有帮助的。此外，让ChatGPT了解你对答案的期望，例如响应的长度、应包含的信息、是否只希望得到代码或文本结果，以及答案应该如何结构化返回；例如，你可以使用JSON格式或Python风格列表来提问。
- en: LLMs such as ChatGPT, and the related Copilot feature in GitHub, have proven
    to be useful assistants for a variety of programming tasks. This usefulness applies
    to tabular data applications. You can ask these models various questions or request
    assistance in coding tasks, and they can assist by providing code snippets, explanations
    about how code works, or guidance in using specific commands or algorithms. However,
    although LLMs such as ChatGPT can assist users by generating code snippets for
    data manipulation, cleaning, and transformation tasks, as well as provide explanations
    and guidance on various statistical and machine learning techniques applicable
    to tabular datasets, our intention in this chapter is to show a few select and
    less obvious LLM capabilities that you can use for your tabular data analysis
    and modeling.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如ChatGPT等大型语言模型，以及GitHub中的相关Copilot功能，已被证明是各种编程任务的实用助手。这种实用性适用于表格数据应用。你可以向这些模型提出各种问题或请求在编码任务中的帮助，他们可以通过提供代码片段、解释代码的工作原理或指导使用特定命令或算法来协助你。然而，尽管如ChatGPT等大型语言模型可以通过生成代码片段来协助用户进行数据处理、清洗和转换任务，并提供适用于表格数据集的各种统计和机器学习技术的解释和指导，但本章的目的是展示一些精选且不太明显的语言模型能力，这些能力可用于你的表格数据分析建模。
- en: 7.1.2 Getting and preparing your data
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 获取和准备你的数据
- en: 'As a starting point, we will navigate the Inside Airbnb Network website ([http://insideairbnb.com/](http://insideairbnb.com/))
    and find the data we need. Our goal is to explore the situation in a completely
    different city: Tokyo. First, you have to manually download the data and store
    it in a working directory on your computer or cloud instance. To do so, on the
    home page of the Inside Airbnb Network initiative, as shown in figure 7.1, in
    the Data menu, choose Get the Data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，我们将导航到Inside Airbnb Network网站([http://insideairbnb.com/](http://insideairbnb.com/))，并找到所需的数据。我们的目标是探索一个完全不同的城市：东京。首先，你必须手动下载数据并将其存储在你的计算机或云实例的工作目录中。为此，在Inside
    Airbnb Network倡议的首页上，如图7.1所示，在数据菜单中选择“获取数据”。
- en: '![](../Images/CH07_F01_Ryan2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F01_Ryan2.png)'
- en: Figure 7.1 Choosing from the Data menu
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 从数据菜单中选择
- en: Once you choose the menu item, you will be moved to a new page containing Data
    Downloads, presenting various cities and their data file to be downloaded. Scroll
    the page until you find the city of Tokyo.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了菜单项，你将被带到一个新的页面，包含数据下载部分，展示了各种城市及其要下载的数据文件。滚动页面，直到找到东京市。
- en: Figure 7.2 shows the portion of the page containing the data files for Tokyo
    as they were at the time of writing this book.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2显示了在撰写本书时包含东京数据文件的页面部分。
- en: '![](../Images/CH07_F02_Ryan2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F02_Ryan2.png)'
- en: Figure 7.2 The Tokyo Data Downloads section
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 东京数据下载部分
- en: 'For our analysis, we need two files from the page: `listings.csv`, which contains
    the summary listings and other information about the Airbnb accommodations in
    Tokyo, and `calendar.csv.gz`, a zipped file containing `calendar.csv`, a dataset
    containing occupancy and price information for a given year for each listing.
    Hover over the links, right-click, and select to save them to disk in your working
    directory. For example, in Google Chrome, you need to select “Save link as,” and
    in Mozilla Firefox, you have to select “Save target as.” At this point, you will
    just need to extract the files into your working directory. Once the files we
    need are unzipped in our local directory, we can ingest them into a pandas DataFrame
    using the `read_csv` command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，我们需要从页面获取两个文件：`listings.csv`，其中包含东京Airbnb住宿的摘要列表和其他信息，以及`calendar.csv.gz`，这是一个包含`calendar.csv`的压缩文件，其中包含每个列表给定年份的占用和价格信息。将鼠标悬停在链接上，右键单击，并选择将它们保存到工作目录中。例如，在Google
    Chrome中，您需要选择“另存为”，而在Mozilla Firefox中，您必须选择“另存目标为”。在此阶段，您只需将文件提取到工作目录中。一旦我们需要的文件在我们的本地目录中解压缩，我们可以使用`read_csv`命令将它们导入到pandas
    DataFrame中：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With the list and type of columns, we can get an idea of the kind of data we
    will be dealing with:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过列的列表和类型，我们可以了解我们将要处理的数据类型：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The list comprises 18 columns, largely the same as the Airbnb NYC dataset introduced
    in chapter 3\. Here we describe each of them:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该列表包含18列，与第3章中介绍的Airbnb纽约市数据集大致相同。这里我们描述每个字段：
- en: '`id`—A unique identifier for each listing on Airbnb. It is an `int64` data
    type, meaning it is a numerical ID representation. In other tables, it can be
    referred to as `listing_id`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`—Airbnb上每个列表的唯一标识符。它是一个`int64`数据类型，意味着它是一个数值ID表示。在其他表中，它可以被称为`listing_id`。'
- en: '`name`—The description of the Airbnb listing. It is of the `object d`ata type,
    which typically represents a string or text.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`—Airbnb列表的描述。它属于`object`数据类型，通常表示字符串或文本。'
- en: '`host_id`—A unique identifier for each host on Airbnb. It is an `int64` data
    type.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_id`—Airbnb上每个房东的唯一标识符。它是一个`int64`数据类型。'
- en: '`host_name`—The name of the host who owns the listing. It is of the `object`
    data type.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_name`—拥有列表的房东的姓名。它属于`object`数据类型。'
- en: '`neighbourhood_group`—Represents the broader area or region the neighborhood
    belongs to. It is stored as a `float64` data type, but it is important to note
    that using a float data type to represent groups or categories is uncommon. In
    this case, the presence of float values indicates that the data for this field
    is entirely made up of missing values.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbourhood_group`—表示该社区所属的更广泛的区域或地区。它存储为`float64`数据类型，但需要注意的是，使用浮点数据类型来表示组或类别是不常见的。在这种情况下，浮点值的存在表明该字段的全部数据都是缺失值。'
- en: '`neighbourhood`—The specific neighborhood where the listing is located. It
    is of the `object` data type.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbourhood`—列表所在的特定社区。它属于`object`数据类型。'
- en: '`latitude`—The latitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latitude`—列表位置的纬度坐标。它属于`float64`数据类型。'
- en: '`longitude`—The longitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`longitude`—列表位置的经度坐标。它属于`float64`数据类型。'
- en: '`room_type`—The type of room or accommodation offered in the listing (e.g.,
    entire home/apartment, private room, shared room). It is of the `object` data
    type.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`room_type`—列表中提供的房间或住宿类型（例如，整个房子/公寓，私人房间，共享房间）。它属于`object`数据类型。'
- en: '`price`—The price per night to rent the listing. It is of the `int64` data
    type, representing an integer price value.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`price`—每晚出租该列表的价格。它属于`int64`数据类型，表示一个整数值的价格。'
- en: '`minimum_nights`—The minimum number of nights that is required for booking
    the listing. It is of the `int64` data type.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minimum_nights`—预订列表所需的最少夜晚数。它属于`int64`数据类型。'
- en: '`number_of_reviews`—The total number of reviews received by the listing. It
    is of the `int64` data type.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_reviews`—列表收到的总评论数。它属于`int64`数据类型。'
- en: '`last_review`—The date of the last review received by the listing. It is of
    the `object` data type, which could represent date and time information, but it
    might require further parsing to be used effectively.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_review`—列表收到的最后一条评论的日期。它属于`object`数据类型，可能代表日期和时间信息，但可能需要进一步解析才能有效使用。'
- en: '`reviews_per_month`—The average number of reviews per month for the listing.
    It is of the `float64` data type.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reviews_per_month`—列表每月的平均评论数。它属于`float64`数据类型。'
- en: '`calculated_host_listings_count`—The total number of listings the host has
    on Airbnb. It is of the `int64` data type.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculated_host_listings_count`—房东在Airbnb上的列表总数。它是`int64`数据类型。'
- en: '`availability_365`—The number of days the listing is available for booking
    in a year (out of 365 days). It is of the `int64` data type.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`availability_365`—一年中可供预订的天数（365天中的天数）。它是`int64`数据类型。'
- en: '`number_of_reviews_ltm`—The number of reviews received in the last 12 months.
    It is of the `int64` data type.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_reviews_ltm`—过去12个月内收到的评论数量。它是`int64`数据类型。'
- en: '`license`—The license number or information related to the listing. It is of
    the `object` data type, which typically represents a string or text.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`license`—列表的许可证号码或相关信息。它是`object`数据类型，通常表示字符串或文本。'
- en: We can safely ignore features such as `host_id`, `host_name`, `neighbourhood_group`
    (because they are completely missing), or `license` (which is a kind of identifier
    based on the host’s license).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以安全地忽略诸如`host_id`、`host_name`、`neighbourhood_group`（因为它们完全缺失）或`license`（基于房东许可证的一种标识符）之类的特征。
- en: 'As for the other features, whereas most of them are numeric, the `name` feature
    is a string containing various pieces of information to be extracted based on
    how the data has been organized. By visualizing a single example from it, we can
    have an idea of its organization:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 至于其他特征，虽然大多数都是数值型的，但`name`特征是一个字符串，包含根据数据组织方式提取的各种信息。通过可视化其中的一个示例，我们可以了解其组织结构：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The string is arranged into five distinct parts separated by conventional signs
    and with some kind of partially structured and repeated content:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串被安排成五个不同的部分，由传统的符号分隔，并包含某种部分结构化和重复的内容：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first portion of the string is a description of the unit type and location.
    The second portion is the average score received from guests. The third portion
    is the number of bedrooms, the fourth portion is the number of beds, and the last
    is the number of bathrooms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串的前一部分是单元类型和位置的描述。第二部分是客人给出的平均评分。第三部分是卧室数量，第四部分是床的数量，最后一部分是浴室数量。
- en: Apart from the numeric values, we can also extract some specific information
    related to the kind of accommodation or services offered—for instance, if the
    apartment is a studio, if the bath is shared, and if it is a half-bath (a room
    with a toilet and washbasin but no bath or shower). We can deal with such information
    by creating simple string correspondence checks and obtaining a binary feature
    pointing out the presence or absence of the characteristic or using regex commands.
    Regex (short for regular expressions) commands are a sequence of characters constituting
    a search pattern. They are used for pattern matching within strings. Table 7.1
    shows the transformations we apply to the description field and highlights what
    strings we strive to match, what regex command we use, and what resulting feature
    we obtain.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数值之外，我们还可以提取一些与提供的住宿或服务类型相关的特定信息——例如，如果公寓是工作室，如果浴室是共享的，以及如果它是半浴室（一个有厕所和洗脸盆但没有浴缸或淋浴的房间）。我们可以通过创建简单的字符串对应检查来处理此类信息，并获取一个二进制特征，指出特征的存在或不存在，或者使用正则表达式命令。正则表达式（缩写为regex）命令是一系列字符，构成搜索模式。它们用于字符串内的模式匹配。表7.1显示了我们对描述字段应用的转换，并突出显示了我们试图匹配的字符串，使用的正则表达式命令以及获得的结果特征。
- en: Table 7.1 Regex commands for feature engineering
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 特征工程的正则表达式命令
- en: '| Matched description | Regex | Resulting feature |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 匹配描述 | 正则表达式 | 结果特征 |'
- en: '| --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Text that starts with “in,” followed by a space, then captures any characters
    until another space, and ends with a dot | `r''in\s(.*?)\s·''`  | Area of Tokyo
    of the listing |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 以“in”开头，后跟一个空格，然后捕获任何字符直到另一个空格，并以点结束的文本 | `r''in\s(.*?)\s·''`  | 列表的东京区域
    |'
- en: '| Text that starts with the character “★” (a star), followed by one or more
    digits, a dot, and one or more additional digits (e.g., ★4.5) | `r''`★`(\d+\.\d+)''` 
    | Star ratings |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 以字符“★”（星号）开头，后跟一个或多个数字，一个点，以及一个或多个额外的数字（例如，★4.5）的文本 | `r''`★`(\d+\.\d+)''` 
    | 星级评分 |'
- en: '| Text containing a numerical value followed by zero or more whitespace characters
    and the words “bedroom” or “bedrooms” (with or without the “s” at the end) | `r''(\d+)\s*(?:bedroom&#124;bedrooms)''` 
    | Number of bedrooms |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 包含数值值后跟零个或多个空白字符和单词“bedroom”或“bedrooms”（带或不带末尾的“s”）的文本 | `r''(\d+)\s*(?:bedroom&#124;bedrooms)''` 
    | 卧室数量 |'
- en: '| Text containing a numerical value followed by one or more whitespace characters
    and the word “bed” or “beds” as a whole word (with or without the “s” at the end)
    | `r''(\d+)\s+(?:beds?\b)''`  | Number of beds |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 包含一个数值后跟一个或多个空格字符和单词“bed”或“beds”（带或不带末尾的“s”）的文本 | `r''(\d+)\s+(?:beds?\b)''` 
    | 床的数量 |'
- en: '| Text containing a numerical value representing the number of baths | `r''(?P<baths>\d+)\s*(shared\s+)?(?:half-)?baths?\b''` 
    | Number of baths |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 包含表示浴次数的数值文本 | `r''(?P<baths>\d+)\s*(shared\s+)?(?:half-)?baths?\b''`  |
    浴次数 |'
- en: 'Working with regex commands is a bit complicated. Hence this is the first application
    where generative AI could help. Most LLMs, such as ChatGPT, have a good knowledge
    of different programming languages (in particular, most of them are quite strong
    in Python) because they have been trained on text and information extracted from
    the internet, where there is plenty of information about how to code even very
    specific problems. In our case, showing an example of the strings in the prompt
    and asking for the desired information to be extracted should do the trick:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式命令有点复杂。因此，这是生成式AI可以提供帮助的第一个应用。大多数LLMs，如ChatGPT，对不同的编程语言有很好的了解（特别是，它们中的大多数在Python方面都很强），因为它们是在从互联网上提取的文本和信息上训练的，那里有大量关于如何编码甚至非常具体问题的信息。在我们的情况下，展示提示中的字符串示例并请求提取所需的信息应该可以解决问题：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output should be something already suitable for usage, arranged in blocks
    of code snippets with some explanation about the extraction rule, such as shown
    in figure 7.3.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是已经适合使用的，以代码片段块的形式排列，并附带一些关于提取规则的说明，如图7.3所示。
- en: '![](../Images/CH07_F03_Ryan2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F03_Ryan2.png)'
- en: Figure 7.3 Results from a prompt on regex processing on ChatGPT 3.5
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 ChatGPT 3.5上正则表达式处理的提示结果
- en: Without being asked, the language model should simply decide to propose a Python-based
    solution, and you just click on the “Copy code” icon on top of the code listing
    to copy the snippet on the clipboard and then paste it into your notebook or IDE
    editor.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有被询问的情况下，语言模型应该简单地决定提出一个基于Python的解决方案，然后你只需点击代码列表顶部的“复制代码”图标，将代码片段复制到剪贴板，然后将其粘贴到你的笔记本或IDE编辑器中。
- en: Usually, the solutions provided may vary from query to query and can differ
    from the solution we provided in the table. This is because LLMs are, in the end,
    probabilistic machines. Temperature is the parameter usually set to influence
    the randomness of the model’s output. It is used during text generation to control
    the generated content’s creativity. In simple terms, temperature affects the likelihood
    of the model choosing the next word in a sequence. Low-temperature values result
    in more deterministic and expected output. In contrast, high-temperature values
    introduce more randomness and creativity in the generated output because the model
    tends to choose less probable words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，提供的解决方案可能因查询而异，与我们表格中提供的解决方案不同。这是因为LLMs最终是概率机器。温度是通常设置以影响模型输出随机性的参数。在文本生成过程中，它用于控制生成内容的创造力。简单来说，温度影响模型选择序列中下一个词的可能性。低温值导致更确定性和预期的输出。相反，高温值在生成的输出中引入更多的随机性和创造力，因为模型倾向于选择不太可能的词。
- en: After getting an apparently useful answer, one important step prior to using
    the solutions proposed by an LLM is to test them on more examples than the one
    or two shown in the prompt. Such a step may reveal that the code is not working
    well, and you may also need to tell the model that the example doesn’t work or
    signal the problem you are experiencing by more detailed instructions to the model.
    For instance, we found that sometimes the commands didn’t work properly if there
    were upper-case letters in some parts of the input string. Hence, we had to find
    a supplemental solution. All these regex commands operate lowercase, thanks to
    the `re.IGNORECASE` flag, which makes the match operating case insensitive. In
    the following listing, we proceed to extract information from the text descriptions
    using the regex commands we found using ChatGPT.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用LLM提出的解决方案之前，一个重要的步骤是在比提示中显示的一个或两个示例更多的示例上测试它们。这一步骤可能会揭示代码运行不正常，你也可能需要告诉模型示例不起作用，或者通过更详细的指令向模型发出你遇到的问题的信号。例如，我们发现有时如果输入字符串的某些部分有大小写字母，命令可能无法正常工作。因此，我们必须找到补充解决方案。所有这些正则表达式命令都操作小写，多亏了`re.IGNORECASE`标志，这使得匹配操作不区分大小写。在下面的列表中，我们继续使用我们在ChatGPT上找到的正则表达式命令从文本描述中提取信息。
- en: Listing 7.1 Extracting information from text descriptions
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 从文本描述中提取信息
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Extracts the type of accommodation from a list of options
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从选项列表中提取住宿类型
- en: ② Extracts the area of Tokyo mentioned in the listing name
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从列表名称中提取提到的东京地区
- en: ③ Extracts the rating score from a star symbol followed by a numerical value
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从带有数值的星号符号中提取评分
- en: ④ Extracts the number of bedrooms from the listing name
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 从列表名称中提取卧室数量
- en: ⑤ Extracts the number of beds from the listing name
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从列表名称中提取床的数量
- en: ⑥ Extracts the number of baths from the listing name
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从列表名称中提取浴室数量
- en: Listing 7.2 completes the feature extraction work by creating additional Boolean
    columns based on specific keywords in the name column. It computes two calculated
    features based on a difference telling us the number of days between today’s date
    and the `last_review` date and a ratio expressing how the number of reviews in
    the last year relates to the total number of reviews. Such a ratio can reveal
    if the bulk of reviews is recent or if the listing has been successful mainly
    in the past.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 通过在名称列中基于特定关键词创建额外的布尔列来完成特征提取工作。它根据告诉我们今天日期和 `last_review` 日期之间天数差异的差异以及表示过去一年中评论数量与总评论数量之间关系的比率来计算两个计算特征。这样的比率可以揭示大部分评论是否是最近的，或者列表是否主要在过去取得成功。
- en: Listing 7.2 Extracting binary flags and time information
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 提取二进制标志和时间信息
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Checks if the word “new” is present in the name (case-insensitive)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ① 检查名称中是否包含单词“new”（不区分大小写）
- en: ② Checks if the word “studio” is present in the name (case-insensitive)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ② 检查名称中是否包含单词“studio”（不区分大小写）
- en: ③ Checks if the word “shared” is present in the name (case-insensitive)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 检查名称中是否包含单词“shared”（不区分大小写）
- en: ④ Checks if the word “half” is present in the name (case-insensitive)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 检查名称中是否包含单词“half”（不区分大小写）
- en: ⑤ Calculates the number of days between today’s date and the last_review date
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算今天日期和 last_review 日期之间的天数
- en: ⑥ Calculates the ratio of number_of_reviews_ltm to number_of_reviews for each
    listing
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 计算每条列表的 number_of_reviews_ltm 与 number_of_reviews 之间的比率
- en: The `summary_listings` data also has a price feature that we could use as a
    target. Still, we prefer to create it by aggregating the `calendar.csv` data to
    decide whether to pick an average of all the prices, the minimum, or the maximum.
    The `calendar.csv` contains information for each day about the availability of
    the accommodation, its price (also adjusted for discounts), and the minimum and
    maximum number of nights allowed for booking at that time. We are interested in
    processing the adjusted price as a target, representing the effective market price
    of the accommodation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary_listings` 数据也具有一个价格特征，我们可以将其用作目标。然而，我们更倾向于通过聚合 `calendar.csv` 数据来创建它，以决定是选择所有价格的平均值、最小值还是最大值。`calendar.csv`
    包含有关每天住宿可用性、其价格（也考虑了折扣）以及当时允许预订的最小和最大夜数的详细信息。我们感兴趣的是将调整后的价格作为目标，代表住宿的有效市场价格。'
- en: Listing 7.3 Creating the target from daily listings
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 从每日列表创建目标
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Removes the dollar sign ($) and commas (,) from the values and then converts
    them to float
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从值中删除美元符号（$）和逗号（,），然后将其转换为浮点数
- en: ② A group by operation on the calendar DataFrame based on the listing_id column
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ② 基于 listing_id 列在日历 DataFrame 上执行分组操作
- en: '③ Calculates three statistics for the adjusted_price column: the mean, minimum,
    and maximum values'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算调整后价格列的三个统计数据：平均值、最小值和最大值
- en: 'After completing the aggregation, we can check the result by requiring the
    first five rows of this newly created dataset:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 完成聚合后，我们可以通过要求查看新创建数据集的前五行来检查结果：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Figure 7.4 verifies how we now have both a mean price per listing available
    together with the maximum and minimum prices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 验证我们现在既有每条列表的平均价格，也有最大和最小价格。
- en: '![](../Images/CH07_F04_Ryan2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F04_Ryan2.png)'
- en: Figure 7.4 Price statistics for Airbnb listings
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 Airbnb 列表的定价统计
- en: We will save this `price_stats` DataFrame and concentrate in the next section
    on improving the number and effectiveness of our features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保存这个 `price_stats` DataFrame，并在下一节集中讨论提高特征的数量和有效性。
- en: 7.1.3 Engineering more complex features
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 工程更复杂的功能
- en: Real estate assets have a pretty peculiar behavior, distinguishable from other
    products or services you find on the market. An adage in the real estate business
    mentions that all that matters when dealing with buildings and facilities is “location,
    location, location.” The position of an apartment in a city or a road can make
    a difference to the value of a property. We will adopt this adage for Airbnb listings
    and develop some feature engineering based on location.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 房地产资产有一种相当奇特的行为，与其他市场上找到的产品或服务不同。房地产业的一个谚语提到，在处理建筑和设施时，最重要的是“位置，位置，位置。”一个公寓在城市或道路上的位置可能会影响房产的价值。我们将采用这个谚语来处理Airbnb列表，并基于位置开发一些特征工程。
- en: As a first step, we will reprise the example from the previous chapter, where
    we created small geographic subdivisions that we later target encoded. By this
    approach, you should be able to capture the specific characteristics of an area,
    though it will be difficult to explain why renting in one particular location
    costs more than in others. We should prepare more specific features to provide
    some explainability to listings in Tokyo. Here is another point where generative
    AI can come to the aid of the tabular data practitioner by providing help in the
    form of suggestions and idea generation. LLMs have sifted through more data than
    you can imagine, and if queried with enough details (and some role-playing, i.e.,
    asking them to personify an expert proficient in a specific field), they can return
    hints and reflections that could have cost you multiple hours of research and
    readings on the web.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将重述前一章中的例子，其中我们创建了小的地理子区域，后来对这些区域进行了目标编码。通过这种方法，你应该能够捕捉到特定地区的特定特征，尽管解释为什么在某个特定地点租房比其他地方贵将会有困难。我们应该准备更具体的特征，为东京的列表提供一些可解释性。这里又是生成式AI可以帮助表格数据从业者的地方，通过提供建议和创意生成帮助。大型语言模型已经处理了比你想象的更多的数据，如果你提供足够的细节（以及一些角色扮演，即要求它们扮演一个特定领域的专家），它们可以提供可能需要你花费数小时在网络上研究和阅读的提示和反思。
- en: Our prompt for ChatGPT is
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为ChatGPT的提示是
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/logo-MR.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/logo-MR.png)'
- en: '| You are an expert data scientist and you have downloaded a dataset containing
    summarized Airbnb listings. This is the structure of the dataset: id (int64),
    name (object), neighbourhood (object), latitude (float64), longitude (float64),
    room_type (object), price (int64), minimum_nights (int64), number_of_reviews (int64),
    last_review (object), reviews_per_month (float64), calculated_host_listings_count
    (int64), availability_365 (int64), number_of_reviews_ltm (int64). You are training
    a machine learning model to predict the price of listings. Which features should
    you engineer to improve the performance of the model? |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位数据科学专家，你已经下载了一个包含总结的Airbnb列表数据的集。这是数据集的结构：id（int64），名称（对象），社区（对象），纬度（float64），经度（float64），房间类型（对象），价格（int64），最低入住夜数（int64），评论数（int64），最后评论（对象），每月评论数（float64），计算出的房东列表数量（int64），365天可用性（int64），过去12个月的评论数（int64）。你正在训练一个机器学习模型来预测列表的价格。你应该设计哪些特征来提高模型的性能？
    |'
- en: Our strategy is to set a persona (“you are an expert data scientist”) and to
    provide some further information about the features available (removing the features
    that we actually already decided not to use) and the target variable. Here we
    also used the fact that we expected ChatGPT to already know something about the
    dataset we were using (“a dataset containing summarized Airbnb listings”), but
    you can also propose less known problems to the LLM by briefly describing the
    dataset, including the types of features and their relationships to each other.
    In addition, if you have any domain expertise relevant to the dataset, for instance,
    any hypotheses about how the features in the dataset might be related to the target
    variable, share them with the LLM. This information can help it identify features
    that may be important for the task. Moreover, providing a list of the existing
    features in the dataset, as we did, may not be enough for more challenging tasks.
    Providing an explanation or description of each variable and clearly stating the
    objective of your machine learning model, such as predicting customer churn, estimating
    real estate values, or forecasting sales, will help the LLM identify gaps or opportunities
    for creating new well-engineered features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略是设定一个角色（“你是一位数据科学家专家”）并提供一些有关可用特征（移除我们实际上已经决定不使用的特征）和目标变量的更多信息。在这里，我们也使用了这样一个事实，即我们预计ChatGPT已经对我们使用的数据集有所了解（“包含总结的Airbnb列表的数据集”），但你也可以通过简要描述数据集（包括特征类型及其相互关系）向LLM提出不太为人所知的问题。此外，如果您对数据集有任何相关的领域专业知识，例如关于数据集中特征可能与目标变量相关联的任何假设，请与LLM分享。这些信息可以帮助它识别对任务可能重要的特征。此外，提供数据集中现有特征的列表，就像我们这样做，可能对于更具挑战性的任务还不够。提供每个变量的解释或描述，并清楚地说明您的机器学习模型的目标，例如预测客户流失、估计房地产价值或预测销售额，将帮助LLM识别创建新精心设计的特征的差距或机会。
- en: 'The following are the suggested features, or feature types, to strive to generate
    to obtain the best results in our dataset and type of problem, as stated by ChatGPT:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们根据ChatGPT的说明，为了在我们的数据集和问题类型中获得最佳结果而应努力生成的建议功能或功能类型：
- en: '*Distance from key locations*—Calculate the distance of each listing from key
    locations or landmarks in the city. This can be done using the latitude and longitude
    information.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*距离关键地点*—计算每个列表与城市中关键地点或地标之间的距离。这可以通过使用经纬度信息来完成。'
- en: '*Neighborhood statistics*—Aggregate statistics for each neighborhood, such
    as the average price, minimum and maximum prices, and the number of listings in
    each neighborhood.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社区统计数据*—对每个社区进行汇总统计，例如平均价格、最低和最高价格以及每个社区中的列表数量。'
- en: '*Room type encoding*—Convert the categorical variable `room_type` into numerical
    values using one-hot encoding or label encoding.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*房间类型编码*—使用独热编码或标签编码将分类变量`room_type`转换为数值。'
- en: '*Temporal features*—Extract relevant features from the `last_review` column,
    such as the month or season of the last review or the time since the last review.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间特征*—从`last_review`列中提取相关特征，例如最后评论的月份或季节或自最后评论以来经过的时间。'
- en: '*Host experience*—Combine information from `calculated_host_listings_count`
    and `number_of_reviews_ltm` to create a feature representing the host’s experience
    or popularity.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*房东经验*—结合`calculated_host_listings_count`和`number_of_reviews_ltm`的信息，创建一个表示房东经验或受欢迎程度的特征。'
- en: '*Booking density*—Calculate the ratio of `number_of_reviews` to `availability_365`
    to represent how frequently a listing is booked relative to its availability.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预订密度*—计算`number_of_reviews`与`availability_365`的比率，以表示列表相对于其可用性的预订频率。'
- en: '*Review sentiment*—If you have access to the actual review texts, you can use
    natural language processing techniques to analyze sentiment and extract sentiment-based
    features.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论情感*—如果您可以访问实际的评论文本，您可以使用自然语言处理技术来分析情感并提取基于情感的特征。'
- en: '*Interaction terms*—Create interaction terms between relevant features, such
    as the product of `minimum_nights` and `availability_365` to capture the overall
    availability for longer stays.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交互项*—创建相关特征之间的交互项，例如`minimum_nights`和`availability_365`的乘积，以捕捉较长住宿的整体可用性。'
- en: '*Price binning*—Convert the continuous `price` variable into bins or categories,
    creating a new categorical feature representing different price ranges.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格分箱*—将连续的`价格`变量转换为箱或类别，创建一个表示不同价格范围的新分类特征。'
- en: '*Geographical clustering*—Use clustering algorithms to group listings based
    on geographical proximity. This can capture spatial patterns in pricing.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*地理聚类*——使用聚类算法根据地理位置将列表分组。这可以捕捉到价格的空间模式。'
- en: Of the proposed points, a few do resonate with our experience, such as computing
    distance from key locations and neighborhood statistics or creating geographical
    clustering. Also, we already provided room-type encoding with our previous work
    on text fields and regex commands. Leaving aside review sentiment and price binning
    (price is actually our target), what is left is a set of ideas that can guide
    or inspire you to improve your work. Use the outputs of LLMs to your advantage,
    but don’t forget that your previous knowledge, experience, sensibility, and critical
    thinking can play an even more important role in the task of a tabular data analysis
    and that LLMs are just facilitators in your work.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出的要点中，有一些与我们的经验相呼应，例如从关键位置计算距离和邻里统计数据，或创建地理聚类。此外，我们已经在之前的关于文本字段和正则表达式命令的工作中提供了房间类型编码。除了评论情感和价格分箱（价格实际上是我们的目标）之外，剩下的是一套可以指导或启发你改进工作的想法。利用LLM的输出，但不要忘记，你以前的知识、经验、敏感性和批判性思维在表格数据分析的任务中可以发挥更重要的作用，而LLM只是你工作中的辅助工具。
- en: Keeping in mind the proposed features suggested by ChatGPT, we proceed to create
    some of them. Regarding geographical clustering, you can find all you need in
    listing 7.4 to create a high cardinality geographical feature from coordinates.
    Later, in the data pipeline, we will target encode the high cardinality categorical
    feature we are going to generate with the code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到ChatGPT提出的建议，我们继续创建其中的一些。关于地理聚类，你可以在列表7.4中找到所有你需要的内容，从坐标创建高基数地理特征。稍后，在数据管道中，我们将使用代码对即将生成的具有高基数分类特征进行目标编码。
- en: Listing 7.4 Creating a high cardinality geographical feature
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 创建高基数地理特征
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Discretizes the latitude and longitude by bin size
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ① 通过网格大小对纬度和经度进行离散化
- en: ② Composes the new coordinates feature by summing the discretized latitude and
    longitude
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过对离散化的纬度和经度求和来组成新的坐标特征
- en: This code generates a feature with 317 unique values. We covered all the Tokyo
    municipalities in a 32 × 32 grid, which means potentially 1,024 values. Only 317
    of these coordinates contain a listing, meaning that in the future, our model
    can effectively predict based on this feature only if a new listing falls in one
    of the 317 slots previously defined. In case a new area appears, thanks to the
    target encoder novelty dealing capabilities (see the description of the `handle_unknown`
    parameter at [https://mng.bz/oK1p](https://mng.bz/oK1p)), we can simply impute
    unknown values to the target means using the setting `handle_unknown`=“value”.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成一个具有317个唯一值的特征。我们在一个32 × 32的网格中覆盖了所有东京自治市，这意味着可能有1,024个值。只有317个这些坐标包含一个列表，这意味着如果未来有一个新的列表落在之前定义的317个槽位之一，我们的模型可以仅基于这个特征有效地进行预测。如果出现新的区域，多亏了目标编码器的创新处理能力（请参阅[https://mng.bz/oK1p](https://mng.bz/oK1p)中`handle_unknown`参数的描述），我们可以简单地使用设置`handle_unknown`="value"将未知值插补到目标均值。
- en: 'A critical aspect of the Tokyo real estate market is that there is an important
    geographical center for cultural and historical reasons, which is the Imperial
    Palace. Locations near this site tend to have higher real estate valuations, and
    some of Japan’s most expensive flats are located close to the Imperial Palace.
    We try to reflect this reality by creating a feature comparing the location of
    our Airbnb accommodation with the area of the Imperial Palace (which can be taken
    from sites such as latlong.net: [https://mng.bz/nRW2](https://mng.bz/nRW2)). For
    distances, we convert the value into meters using a formula that involves the
    cosine of the radiants multiplied by a conversion factor for the measure to be
    intelligible to a human examination. We also adopt the Manhattan distance for
    better-representing distances in a city, which is the summed difference in absolute
    values between the latitudes and longitudes.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 东京房地产市场的关键方面是，由于文化和历史原因，存在一个重要的地理中心，即皇宫。靠近这个地点的位置往往有更高的房地产估值，日本一些最昂贵的公寓就位于皇宫附近。我们试图通过创建一个将我们的Airbnb住宿位置与皇宫区域（可以从latlong.net等网站获取：[https://mng.bz/nRW2](https://mng.bz/nRW2)）进行比较的特征来反映这一现实。对于距离，我们使用涉及辐射角的余弦乘以一个转换系数的公式将值转换为米，以便人类检查时可以理解。我们还采用曼哈顿距离来更好地表示城市中的距离，这是纬度和经度的绝对值之差的和。
- en: Listing 7.5 Computing a distance metric from the city center
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 从市中心计算距离度量
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Conversion factor representing the approximate number of meters per degree
    of latitude
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ① 表示每度纬度大约有多少米的转换系数
- en: ② Calculates the distance in meters by multiplying the degree-based distance
    by the conversion factor and adjusting for the latitude’s cosine
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过将基于度的距离乘以转换系数并调整纬度的余弦值来计算距离（以米为单位）
- en: ③ Calculates the absolute distance in degrees by subtracting the Imperial Palace’s
    latitude and longitude from the values in the dataset
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过从数据集中的值中减去故宫的纬度和经度来计算绝对距离（以度为单位）
- en: 'When dealing with coordinates inside a city, the choice between Euclidean distance
    and Manhattan distance as a feature for machine learning depends on the specific
    context and the problem you are trying to solve:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理城市内部的坐标时，选择欧几里得距离和曼哈顿距离作为机器学习特征取决于具体的上下文和你要解决的问题：
- en: '*Euclidean distance* is based on the straight-line distance between two points
    in an Euclidean space. It assumes a direct path between the points and can be
    more suitable when considering physical distances. You may also hear it referred
    to as the *L2 norm*, which instead is a mathematical concept referring to the
    distance between the vector and the origin of the vector space. Since the L2 norm
    is based on the Euclidean distance formula, it is used interchangeably because
    it is a closely related mathematical concept.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧几里得距离*基于欧几里得空间中两点之间的直线距离。它假设两点之间存在直接路径，在考虑物理距离时可能更合适。你也可能听到它被称为 *L2范数*，这是一个数学概念，指的是向量与向量空间原点之间的距离。由于L2范数基于欧几里得距离公式，因此它可以互换使用，因为它是一个紧密相关的数学概念。'
- en: '*Manhattan distance*, also known as city block distance or taxicab distance,
    measures the distance between two points by adding up the absolute differences
    between their coordinates. It considers only horizontal and vertical movements
    and disregards diagonal paths. Similarly to the Euclidean distance, you may hear
    the Manhattan distance referred to as the *L1 norm* when operating with vectors
    and vector spaces.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*曼哈顿距离*，也称为街区距离或出租车距离，通过计算两点坐标的绝对差值之和来衡量两点之间的距离。它只考虑水平和垂直移动，忽略对角线路径。与欧几里得距离类似，当操作向量和向量空间时，你可能会听到曼哈顿距离被称为
    *L1范数*。'
- en: Manhattan distance can be more appropriate when considering the actual movement
    or navigation within a city, where travel often occurs along streets and road
    networks. Considering that coordinates are inside a city, where the road network
    structure and navigation along streets matter, Manhattan distance might be more
    suitable for capturing the movement and accessibility between locations. It aligns
    with the concept of following the roads and making right-angle turns.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑城市中的实际移动或导航时，曼哈顿距离可能更合适，因为旅行通常沿着街道和道路网络进行。考虑到坐标位于城市内部，道路网络结构和街道导航很重要，曼哈顿距离可能更适合捕捉地点之间的移动和可达性。它与遵循道路和进行直角转弯的概念相符。
- en: 'After calculating the `imperial_palace_distance` feature, we can examine its
    average value, expressed in meters, using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完 `imperial_palace_distance` 特征之后，我们可以使用以下代码来检查其平均值，以米为单位：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The result shows that the average distance to the Imperial Palace is around
    7.9 kilometers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，到故宫的平均距离大约为7.9公里。
- en: 'Next, we can identify the listing that is located nearest to the Imperial Palace.
    To achieve this, we can use the `idxmin()` function to find the index of the listing
    with the minimum distance and then access its corresponding details:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以识别出距离故宫最近的列表。为了实现这一点，我们可以使用 `idxmin()` 函数找到距离最小的列表的索引，然后访问其相应的详细信息：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result is as follows, which is a bit surprising:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下，这有点令人惊讶：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Indeed, the listing is not situated close to the Imperial Palace, which emphasizes
    the presence of potentially misleading errors in the geolocation of listings.
    It is not uncommon to encounter similar problems in datasets, no matter how carefully
    they are curated. As discussed in chapter 2, from a general point of view, after
    some data quality checks where you look for consistency among features and marking
    likely inexact values, you have a few viable options, listed here in descending
    order of effort from more demanding to less demanding:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，该列表并未位于皇宫附近，这强调了列表地理位置可能存在的误导性错误。在精心整理的数据集中遇到类似问题并不罕见。正如第2章所讨论的，从一般的角度来看，在进行一些数据质量检查后，你将找到一些可行的选项，以下按从要求高到要求低列出：
- en: '*Geocoding* (from address to coordinate) and *reverse geocoding* (from coordinates
    to address)—To figure out if location information matches with the provided latitude
    and longitude coordinates and decide whether to trust the provided address or
    coordinates'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*地理编码*（从地址到坐标）和*反向地理编码*（从坐标到地址）——为了确定位置信息是否与提供的经纬度坐标匹配，并决定是否信任提供的地址或坐标'
- en: '*Data imputation*—Imputing the dubious values as they were missing, using the
    coordinates of a default location'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据插补*——将可疑值作为缺失值处理，使用默认位置的坐标'
- en: '*Listwise deletion*—Removing all rows that have some dubious values'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*逐行删除*——删除所有包含可疑值的行'
- en: '*Leave it to XGBoost*—Tree-based methods tend to be less affected by erroneous
    and dubious values being robust to outliers and noise in the data'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交给XGBoost处理*——基于树的算法通常受错误和可疑值的影响较小，对数据中的异常值和噪声具有鲁棒性'
- en: In our example, we decided to leave the situation to XGBoost because our model
    is not so critical as to require a thorough data quality check. It could be different
    with your own project, and you may evaluate a solution requiring more data-cleaning
    efforts.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们决定将情况交给XGBoost处理，因为我们的模型并不那么关键，不需要彻底的数据质量检查。对于你自己的项目来说，情况可能不同，你可能需要评估一个需要更多数据清理工作的解决方案。
- en: 'Distances from landmarks and services work well as features in real estate
    modeling. Hence we do not limit ourselves to computing the distance from the Imperial
    Palace, the center of Tokyo. In the paper “Modeling User Activity Preference by
    Leveraging User Spatial Temporal Characteristics in LBSNs” by Dingqi Yang, Daqing
    Zhang, Vincent W. Zheng, Zhiyong Yu (*IEEE Transactions. on Systems, Man, and
    Cybernetics: Systems*, 45(1), 129-142, 2015), the authors collected datasets from
    Foursquare check-ins in New York City and Tokyo with their geographical coordinates
    and the type of location they refer to; we can access the data from Kaggle Datasets
    ([https://mng.bz/4aDj](https://mng.bz/4aDj)).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '从地标和服务点的距离在房地产建模中作为特征效果良好。因此，我们不仅计算从东京中心——皇宫的距离，在Dingqi Yang、Daqing Zhang、Vincent
    W. Zheng和Zhiyong Yu的论文“通过利用LBSN中的用户时空特征建模用户活动偏好”（IEEE Transactions on Systems,
    Man, and Cybernetics: Systems，45(1)，129-142，2015）中，作者们收集了来自纽约市和东京的Foursquare签到数据集，包括它们的地理坐标和所引用的位置类型；我们可以从Kaggle数据集（[https://mng.bz/4aDj](https://mng.bz/4aDj)）中获取数据。'
- en: Foursquare is a social network based on geopositioning. Thanks to its mobile
    app, it allows users to discover nearby venues to visit, such as restaurants and
    shops, and means of transportation and to share information about the places they
    visit. Another characteristic of the app is check-ins, which happen when using
    the platform at a venue. Check-ins refer to the presence of a user at a specific
    location. When a user checks into a place, they share their positioning with their
    Foursquare friends, and they may also have the option to post about their visit
    to other social media platforms like Facebook and X. To map the value of a listing
    in terms of convenience, we have available airports, bus, train, and subway stations
    among commonly checked-in venues. Together with convenience stores, a type of
    retail store that mainly sells a wide selection of everyday items and products
    to customers for their convenience, the proximity of such venues can add value
    to an accommodation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Foursquare是一个基于地理位置的社交网络。得益于其移动应用，它允许用户发现附近的场所，如餐厅和商店、交通方式，并分享他们访问地点的信息。该应用的特点之一是签到，当在场所使用该平台时发生。签到指的是用户在特定地点的存在。当用户签到时，他们将与他们的Foursquare朋友分享他们的定位，他们还可能有在Facebook和X等社交媒体平台上发布他们访问信息的选项。为了从便利性的角度映射列表的价值，我们拥有机场、公交、火车和地铁站等常见的签到场所。与便利商店一起，这是一种主要向客户销售广泛日常物品和产品的零售店，这些场所的邻近性可以为住宿增加价值。
- en: Hence, to enrich our dataset, we first extracted the GPS coordinates of these
    Tokyo locations directly from Kaggle. The code and the extracted dataset are available
    at [https://mng.bz/QDPv](https://mng.bz/QDPv), and you can download the processed
    file into your working directory from the page [https://mng.bz/XxNa](https://mng.bz/XxNa)
    where you can get the file `relevant_spots_Tokyo.csv`. The file contains information
    about 3,560 convenience store locations, 1,878 bus stations and stops, 439 subway
    stations, and 264 locations associated with airports. Then, using listing 7.6,
    we can compare the location of our Airbnb Tokyo listings with each of these venues
    and report the nearest distance of each one. Our idea is that the closer a listing
    is to a convenience store and means of transportation, the higher the expected
    price.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了丰富我们的数据集，我们首先直接从Kaggle提取了这些东京地点的GPS坐标。代码和提取的数据集可在[https://mng.bz/QDPv](https://mng.bz/QDPv)找到，您可以从[https://mng.bz/XxNa](https://mng.bz/XxNa)页面下载处理后的文件到您的工作目录，在那里您可以获取文件`relevant_spots_Tokyo.csv`。该文件包含关于3,560家便利店位置、1,878个公交站和停靠点、439个地铁站以及264个与机场相关的地点的信息。然后，使用列表7.6，我们可以将我们的Airbnb东京列表的位置与每个这些场所进行比较，并报告每个场所的最近距离。我们的想法是，列表与便利店和交通方式的距离越近，预期的价格就越高。
- en: In listing 7.6 we do not compare each accommodation with all the possible venues
    we have gathered because that would take too much time and computation. Instead,
    we utilize the k-dimensional tree (KDTree) data structure from Scikit-learn, an
    optimized algorithm designed to efficiently find the nearest point among many
    points for a given location. Scikit-learn is used for algorithms such as K-nearest
    neighbors in situations when you have to find the most similar examples to a test
    sample in the training set. In our case, the training set is the set of venues,
    and the algorithm is trained to find the nearest venue to a given location based
    on Manhattan distance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表7.6中，我们没有将每个住宿与我们所收集的所有可能的场所进行比较，因为这会花费太多时间和计算。相反，我们利用Scikit-learn中的k维树（KDTree）数据结构，这是一个优化算法，旨在高效地找到给定位置附近的最接近点。Scikit-learn用于K最近邻等算法，在这些情况下，您必须找到训练集中与测试样本最相似的示例。在我们的案例中，训练集是场所集合，该算法被训练基于曼哈顿距离找到给定位置最近的场所。
- en: Listing 7.6 Finding the nearest facilities and transportation
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 寻找最近的设施和交通
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Stores the minimum distances in a dictionary
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在字典中存储最小距离
- en: ② Filters the relevant venue locations
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ② 过滤相关场所位置
- en: ③ Creates a KDTree using the filtered venue locations with the Manhattan metric
    for fast nearest-neighbor searches
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用曼哈顿距离创建KDTree，以快速进行最近邻搜索
- en: ④ Queries the KDTree to find the nearest point and its distance to each Airbnb
    listing (k=1 returns the nearest one)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 查询KDTree以找到最近点及其与每个Airbnb列表的距离（k=1返回最近的点）
- en: ⑤ The dictionary of the minimum distances for each type of venue is converted
    into a DataFrame.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将每种类型场所的最小距离字典转换为DataFrame。
- en: 'The code will run quite quickly by iterating over each type of venue, training
    on the locations of selected venues, and finding, using the KDTree, the nearest
    location to each accommodation and automatically calculating the distance. Ultimately,
    we just have to wrap up the results into a pandas DataFrame after converting the
    distances (Manhattan distances in degrees) to meters by the previously seen function
    `degrees_to_meters`. We can verify the results by inspecting the first five rows
    of the resulting dataset:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遍历每种类型的场所，在选定场所的位置上进行训练，并使用KDTree找到每个住宿地点最近的地点，以及自动计算距离，代码将运行得相当快。最终，我们只需将距离（曼哈顿距离，以度为单位）通过之前看到的函数`degrees_to_meters`转换为米，然后将结果汇总到一个pandas
    DataFrame中。我们可以通过检查结果数据集的前五行来验证结果：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Figure 7.5 shows the results, representing the contents from the created `min_distances`
    DataFrame.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5显示了结果，表示从创建的`min_distances` DataFrame中的内容。
- en: '![](../Images/CH07_F05_Ryan2.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F05_Ryan2.png)'
- en: Figure 7.5 The first five rows of the `min_distances` DataFrame
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 `min_distances` DataFrame的前五行
- en: Having the minimum distance to our selection of venues for each listing of our
    dataset, we now proceed with putting together all these new features into a final
    dataset of predictors and extracting a target series or vector to be used for
    modeling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个数据集的列表中有了我们选择场所的最小距离后，我们现在继续将这些新特征组合成一个最终的预测变量数据集，并提取一个用于建模的目标序列或向量。
- en: 7.1.4 Finalizing your data
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 最终化你的数据
- en: 'After creating some additional features, we can finalize our predictive features
    and their target. In listing 7.7, we join the `summary_listing` dataset to the
    minimum distance to our selected landmarks (airports, subway, train, bus stations,
    convenience stores). Then, we rearrange the joined data with respect to our target:
    the mean price we computed in the `price_stats_ordered` dataset.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了一些额外的特征之后，我们可以最终确定我们的预测特征及其目标。在列表7.7中，我们将`summary_listing`数据集与到我们选定地标（机场、地铁、火车、公交车站、便利店）的最小距离连接起来。然后，我们根据目标重新排列连接的数据：在`price_stats_ordered`数据集中计算的平均价格。
- en: Listing 7.7 Assembling data
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 组装数据
- en: '[PRE16]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Reindexes X to match the index of price_stats
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将X重新索引以匹配price_stats的索引
- en: ② Reindexes price_stats to match the index of X, ensuring the reindexed price
    statistics align with the listings in X
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ② 重新索引price_stats以匹配X的索引，确保重新索引的价格统计数据与X中的列表相匹配
- en: ③ Copies the “mean” price column as the target variable
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将“平均”价格列作为目标变量
- en: 'Once we have completed the script, we can visualize our dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了脚本，我们可以可视化我们的数据集：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As index, we have our `listing_id`, and among the columns, there are all the
    features we have prepared for the problem, as shown in figure 7.6.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 作为索引，我们有`listing_id`，在列中，有我们为问题准备的所有特征，如图7.6所示。
- en: '![](../Images/CH07_F06_Ryan2.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F06_Ryan2.png)'
- en: Figure 7.6 Top rows of the predictors’ dataset
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 预测变量数据集的前几行
- en: At this point, we can start examining the data in detail and figure out if there
    are any additional problems to be fixed or other insights to be discovered that
    could play an essential role in how we develop our XGBoost model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以开始详细检查数据，并找出是否有需要修复的额外问题或可以发现的见解，这些见解可能在如何开发我们的XGBoost模型中发挥关键作用。
- en: 7.1.5 Exploring and fixing your data
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.5 探索和修复你的数据
- en: 'The next step after having assembled all the predictors into a single dataset
    is to explore it to detect any problems, such as missing data or extreme values,
    that may affect the performance of a machine learning algorithm. Therefore, our
    first action is to check for any missing data with the following command:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在将所有预测变量组装到一个单一数据集之后，下一步是探索它以检测可能影响机器学习算法性能的问题，如缺失数据或极端值。因此，我们的第一个行动是使用以下命令检查任何缺失数据：
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resulting list points out that there are three features with some missing
    data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 结果列表指出有三个特征存在一些缺失数据：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we discussed in chapter 2 and then again in the previous chapter, it is
    crucial in the presence of missing data to investigate why there are missing values
    and if that could be deemed a missing completely at random, missing at random,
    or missing not at random situation. In this specific case, missing values are
    not at all at random, but they depend on the fact that there are no reviews or
    there are not enough reviews to compute a score. In fact, by inspecting how many
    accommodations are there without reviews, you will notice how the figure matches
    the number of missing cases on two features with missing values:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第二章和上一章中讨论的那样，在存在缺失数据的情况下，调查为什么存在缺失值以及这些缺失值是否可以被视为完全随机缺失、随机缺失或非随机缺失的情况至关重要。在这个特定案例中，缺失值根本不是随机分布的，而是取决于没有评论或评论不足以致计算分数的事实。实际上，通过检查有多少住宿没有评论，你会注意到图形如何与两个具有缺失值的特征的缺失案例数量相匹配：
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As anticipated, the result is 1,252, matching the number of missing values.
    In this case, it is better to refrain from using the capabilities of XGBoost and
    other GBDT implementations to deal with missing data because its behavior will
    mimic an average situation. Missing reviews is an extreme yet legitimate case
    when an accommodation has just entered the market or is seldom chosen. Here, you
    need to directly input a number that may help any machine learning algorithm figure
    out that there are no reviews, hence the missing values. A common strategy is
    to use a value at the boundaries of the existing distribution, usually a negative
    number if we are representing counts or positive values. A quick check can assure
    us if there are the prerequisites for such a missing values strategy:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，结果是1,252，与缺失值的数量相匹配。在这种情况下，最好避免使用XGBoost和其他GBDT实现处理缺失数据的特性，因为其行为将模拟平均情况。当住宿刚刚进入市场或很少被选择时，缺失评论是一个极端但合法的情况。在这里，你需要直接输入一个数字，这可能有助于任何机器学习算法确定没有评论，因此存在缺失值。一种常见的策略是使用现有分布边界上的值，通常如果我们表示计数，则通常是一个负数；如果我们表示正值，则通常是一个正数。快速检查可以确保我们是否有实施这种缺失值策略的先决条件：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As a result, we confirmed that the minimum value is always greater than zero
    for all three considered features. It means we can simply replace the missing
    values using the –1 value (we cannot use zero because the minimum value is zero
    for `days_since_last_review`), which will work as a solution both for a linear
    model (it is at the lower extreme of the existing distributions) and for tree-based
    ensembles (they will just split on that negative figure):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们确认了所有三个考虑的特征的最小值始终大于零。这意味着我们可以简单地使用-1值来替换缺失值（我们不能使用零，因为`days_since_last_review`的最小值是零），这将作为线性模型（它位于现有分布的较低极端）和基于树的集成（它们将仅在该负数上分割）的解决方案：
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As a next step, we will be on the lookout for extreme values among our numeric
    features. As discussed in chapter 2, a straightforward approach to looking for
    outliers and extreme values is to chart a boxplot for each numeric feature arranged
    in a panel of subplots or a single plot if they have comparable scales. In our
    case, in the following listing, we have prepared a panel of boxplots to be inspected
    for extreme values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们将密切关注我们的数值特征中的极端值。正如第二章中讨论的那样，寻找异常值和极端值的一个直接方法是为每个数值特征绘制箱线图，这些特征排列在子图面板中，或者如果它们具有可比的尺度，则在一个单独的图形中。在我们的案例中，在下面的列表中，我们已经准备了一个箱线图面板，以检查极端值。
- en: Listing 7.8 Plotting boxplots for numeric features
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 为数值特征绘制箱线图
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Estimates the number of rows needed to arrange subplots
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ① 估计排列子图所需的行数
- en: ② Calculates the number of columns needed to arrange subplots
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算排列子图所需的列数
- en: ③ Creates a figure with subplots
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建包含子图的图形
- en: ④ Flattens the axes array to a 1D array so that it can be iterated through
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将轴数组展平为1D数组，以便可以迭代
- en: Figure 7.7 shows the charted results. By inspecting the values outside the whiskers
    of the plots, represented as empty points, we immediately notice that almost all
    the distributions have heavy tails on the right, with values decisively much larger
    than the mean value. If, for distance-based features, such extreme values may
    sound reasonable because of the extension of Tokyo’s metropolitan area, as for
    features such as `minimum_night` and `number_of_reviews`, such extreme values
    may represent outliers quite far from the core of the distribution. We could use
    winsorizing to solve such a problem, using a solution we proposed in chapter 2\.
    This data transformation technique replaces extreme values in a dataset with less
    extreme values to reduce the influence of outliers on statistical analyses and
    modeling.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7展示了图表化的结果。通过检查图表外的值，即表示为空点的值，我们立即注意到几乎所有分布都有右侧的重尾，值明显大于平均值。对于基于距离的特征，由于东京都市区的扩展，这样的极端值可能听起来是合理的，但对于如`minimum_night`和`number_of_reviews`这样的特征，这样的极端值可能代表远离分布核心的异常值。我们可以使用winsorizing来解决这个问题，使用我们在第2章中提出的解决方案。这种数据转换技术将数据集中的极端值替换为不太极端的值，以减少异常值对统计分析建模的影响。
- en: In listing 7.9, using the `winsorize` function from the Scipy package, we winsorize
    the 0.1% of the upper part of the distribution of the `minimum_nights` feature.
    All values above the 0.999 percentile will be changed to the value of the 0.999
    percentile, thus removing any extreme values.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表7.9中，使用Scipy包中的`winsorize`函数，我们对`minimum_nights`特征的分布的0.1%的上部分进行winsorize处理。所有高于0.999百分位的值将被改变为0.999百分位的值，从而消除任何极端值。
- en: '![](../Images/CH07_F07_Ryan2.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F07_Ryan2.png)'
- en: Figure 7.7 Panel of boxplots illustrating the skewed distributions of most numeric
    features
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7展示了大多数数值特征的偏斜分布的箱线图面板
- en: Listing 7.9 Winsorizing extreme values
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9 Winsorizing极端值
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① Indicates the lower percentile below which values will not be changed during
    winsorization
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ① 表示winsorization过程中不会改变的值的下限百分位数
- en: ② Indicates the upper percentile above which values will not be changed during
    winsorization
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ② 表示winsorization过程中不会改变的值的上限百分位数
- en: Figure 7.8 shows the highest value is now 120, not over 1,000 as before.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8显示最高值现在是120，而不是之前的超过1000。
- en: '![](../Images/CH07_F08_Ryan2.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F08_Ryan2.png)'
- en: Figure 7.8 Boxplot of winsorized `minimum_nights` feature
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8展示了winsorized `minimum_nights`特征的箱线图
- en: 'We replicate the same also for the `number_of_reviews` feature:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对`number_of_reviews`特征也进行了相同的处理：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Figure 7.9 shows that now the feature still has a heavy right tail. However,
    the extreme values have been compressed to below the 500 value.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9显示，现在该特征仍然有重右尾。然而，极端值已被压缩到500以下。
- en: '![](../Images/CH07_F09_Ryan2.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F09_Ryan2.png)'
- en: Figure 7.9 Boxplot of winsorized `number_of_reviews` feature
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9展示了`number_of_reviews`特征的winsorized箱线图
- en: Having completed checking and remediating for missing values and extreme values
    in our dataset of predictors, we can proceed in the next subsection to look at
    the target itself.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成检查和修复预测数据集中的缺失值和极端值后，我们可以在下一小节中继续查看目标本身。
- en: 7.1.6 Exploring your target
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.6 探索目标
- en: 'When dealing with exploratory data analysis (EDA), it is critical to examine
    the predictors and the target, and sometimes both predictors and target together
    and how they relate to each other. For our example, being a regression problem,
    we simply start by figuring out the mean and the range of the target:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理探索性数据分析（EDA）时，检查预测值和目标值，有时甚至同时检查预测值和目标值以及它们之间的关系至关重要。对于我们的例子，作为一个回归问题，我们首先简单地确定目标值的平均值和范围：
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'These commands will print the minimum, the average, and the maximum values
    in the target variable y:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将打印目标变量y中的最小值、平均值和最大值：
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We immediately notice that the maximum is on a quite different scale than the
    average and the minimum. Estimating percentiles can be helpful to understand better
    if there is a problem with extreme values or skewed distribution in the target.
    The presence of extreme values can be better understood by requiring a range of
    percentiles focusing on the extremities of the distribution:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即注意到最大值与平均值和最小值的尺度完全不同。在目标中存在极端值或偏斜分布问题时，估计百分位数可以帮助更好地理解。通过要求一系列百分位数，重点关注分布的极端部分，可以更好地理解极端值的存在：
- en: '[PRE28]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following are the output percentiles, and we have confirmation of the presence
    of extreme values at the right of the distribution since even the 99th percentile
    is quite far from the maximum we had previously reported:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出百分位数表明，分布右侧存在极端值，因为即使是第99百分位数也相当远离我们之前报告的最大值：
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Not only is the maximum value quite far from the 99th percentile, but there
    also appears to be a significant gap between the 95th and 99th percentiles. Our
    decision is to focus on the core of the distribution by dropping 10% of the distribution:
    5% on the lower and 5% on the upper part. We manage this by selection using a
    Boolean selection variable:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅最大值与第99百分位数相当遥远，而且第95百分位数和第99百分位数之间似乎也存在显著的差距。我们的决定是通过删除10%的分布来关注分布的核心：下部分5%，上部分5%。我们通过布尔选择变量进行选择来管理这一点：
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Before definitely applying the selection, we plot the resulting distribution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定应用选择之前，我们绘制了结果分布。
- en: Listing 7.10 Plotting the target distribution
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10 绘制目标分布图
- en: '[PRE31]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ① Selects only the part of the target distribution we consider to model
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ① 仅选择我们认为需要建模的目标分布的一部分
- en: ② Represents the median value of the distribution
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ② 表示分布的中位数
- en: The previous code snippet will output a density plot, revealing the distribution
    and the median value for the selection of the target based on the selection variable
    we have just defined. Figure 7.10 shows the resulting plot.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码片段将输出一个密度图，揭示基于我们刚刚定义的选择变量的目标分布和中间值。图7.10显示了结果图。
- en: '![](../Images/CH07_F10_Ryan2.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F10_Ryan2.png)'
- en: Figure 7.10 Density distribution of the target variable
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 目标变量的密度分布
- en: 'The resulting distribution shown in figure 7.10 is definitely skewed to the
    right, a condition also called a positive skew. In particular, you can observe
    a lump of data at the start and a long decreasing tail following to the right,
    although, by the end, we can notice another small lump closing the distribution,
    probably a separate cluster of high-end accommodations. However, the range and
    distribution of the target are fine now. Therefore, we will select both the target
    and data using the previously defined Boolean selection variable:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10中显示的结果分布明显向右偏斜，这种情况也称为正偏斜。特别是，你可以观察到数据在开始处的一个块状分布，以及随后向右延伸的较长递减尾端，尽管到结尾时，我们还可以注意到另一个小块状分布，可能是一个高端住宿的独立集群。然而，现在目标变量的范围和分布都很合适。因此，我们将使用之前定义的布尔选择变量来选择目标和数据：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the next section, we will proceed to define both aspects of the validation
    process and the data pipeline necessary for modeling. Afterward, we will try a
    baseline model using classical machine learning linear models and a first tentative
    XGBoost model and optimize it before training our definitive model for the Tokyo
    Airbnb dataset problem.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续定义验证过程的两方面以及建模所需的数据管道。之后，我们将尝试使用经典的机器学习线性模型和一个初步的XGBoost模型作为基线模型，并在训练我们针对东京Airbnb数据集问题的最终模型之前对其进行优化。
- en: 7.2 Building and optimizing your model
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 构建和优化你的模型
- en: In this section, we will use the data we have prepared to build a model. Before
    getting a complete final model, we will address various challenges related to
    defining the cross-validation strategy, preparing the data pipeline, and building
    first a baseline model and then a tentative first XGBoost model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用我们准备好的数据来构建一个模型。在得到完整的最终模型之前，我们将解决与定义交叉验证策略、准备数据管道以及首先构建基线模型然后构建初步的XGBoost模型相关的各种挑战。
- en: 7.2.1 Preparing a cross-validation strategy
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 准备交叉验证策略
- en: 'Generally, a K-fold cross-validation strategy works quite well in most cases,
    but in our specific situation, we are dealing with real estate units whose value
    is strongly influenced by their location. In our case, Stratified K-fold cross-validation
    is more appropriate, controlling for the effect of location. Although similar
    to K-fold cross-validation, there is a key difference in stratified K-fold cross-validation:
    the class distribution of a feature of our choice in the dataset is preserved
    in each fold. Such stratified sampling will allow folds to have a similar mix
    of territories as the complete dataset. However, it is important to check beforehand
    if some territories are difficult to split among folds because of their low numerosity.
    If we count the different neighborhoods represented in the data, we get a long
    list with many locations, some showcasing a considerable number of listings and
    others only a limited few:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，K 折交叉验证策略在大多数情况下都工作得相当好，但在我们特定的情境中，我们处理的是房地产单元，其价值受到其位置的影响很大。在我们的案例中，分层 K
    折交叉验证更为合适，可以控制位置的影响。尽管分层 K 折交叉验证与 K 折交叉验证类似，但在分层 K 折交叉验证中有一个关键的区别：数据集中我们选择的一个特征的类别分布在每个折叠中都被保留。这种分层抽样将允许折叠具有与完整数据集相似的领土混合。然而，在事先检查某些领土是否因为数量少而难以在折叠之间分割是很重要的。如果我们计算数据中表示的不同邻里，我们会得到一个包含许多位置的冗长列表，其中一些展示了相当数量的列表，而其他则只有少数几个：
- en: '[PRE33]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Clearly you cannot accept considering all the neighborhoods having less than
    a certain number of examples because, if you are going to split the data into
    folds, you will hardly have them well represented. Since areas are spatially distributed,
    just gathering them into an extra class won’t do because you will mix very different
    situations of areas quite far from each other. In listing 7.11, we solve this
    problem by aggregating areas with less than 30 examples (implying about 6 examples
    for each fold if we use a five-fold validation split) with their nearest larger
    neighborhood. To achieve that, we use a KDTree data structure again. Thus we can
    match each area with less than 30 accommodations with its nearest area with more
    than 30.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你不能接受考虑所有少于一定数量的示例的邻里，因为如果你打算将数据分割成折叠，你几乎不可能很好地代表它们。由于区域是空间分布的，仅仅将它们聚集到一个额外的类别中是不够的，因为你会混合非常不同的区域情况，这些区域相距很远。在列表
    7.11 中，我们通过聚合少于 30 个示例的区域（如果我们使用五折验证分割，则意味着每个折叠大约有 6 个示例）及其最近的较大邻里来解决这个问题。为了实现这一点，我们再次使用
    KDTree 数据结构。因此，我们可以将每个少于 30 间住宿的区域与其最近的超过 30 间住宿的区域相匹配。
- en: Listing 7.11 Aggregating nearby neighborhood areas
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 聚合附近的邻里区域
- en: '[PRE34]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Calculates the mean latitude, mean longitude, and the count of listings in
    each neighborhood
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ① 计算每个邻里的平均纬度、平均经度和列表数量
- en: ② Separates the neighborhoods into two groups based on the number of listings
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ② 根据列表数量将邻里分为两组
- en: ③ Creates a KDTree using the mean latitude and longitude values of neighborhoods
    with counts greater than 30
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用计数大于 30 的邻里的平均纬度和经度值创建一个 KDTree
- en: ④ Initializes an empty dictionary to store the mappings of the neighborhoods
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化一个空字典以存储邻里的映射
- en: ⑤ Iterates through each neighborhood with counts less than 30 and queries the
    KDTree to find the nearest neighborhood with a count greater than 30
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 遍历计数少于 30 的每个邻里，并查询 KDTree 以找到计数大于 30 的最近邻里
- en: ⑥ Replaces the original neighborhood values with the new neighborhood values
    based on the mapping in change_list
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 根据 change_list 中的映射替换原始邻里值
- en: 'After having run the code, you can check how the mapping has been performed
    and if the resulting aggregation has neighborhood areas with less than 30 listings
    by issuing the following commands:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码后，你可以通过发出以下命令来检查映射是如何执行的，以及是否产生了具有少于 30 个列表的邻里区域聚合：
- en: '[PRE35]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Having elaborated a suitable area subdivision, we can now proceed to define
    our stratified cross-validation strategy in the following listing.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细阐述了合适的区域细分之后，我们现在可以继续定义以下列表中的分层交叉验证策略。
- en: Listing 7.12 Defining a stratified strategy
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 定义分层策略
- en: '[PRE36]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① Defines a five-fold stratified random splitting
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义五折分层随机分割
- en: ② Generates the cross-validation splits while maintaining the same distribution
    of neighborhoods with more than 30 listings in each fold
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在保持每个折叠中超过 30 个列表的邻里分布相同的同时生成交叉验证分割
- en: 'The resulting `cv_splits` is a generator, and you can examine it using the
    following command:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`cv_splits`是一个生成器，你可以使用以下命令来检查它：
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is the type of object:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是对象的类型：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Since `cv_splits` is a generator, it can be used a single time, but you can
    reinstantiate an identical one by simply re-executing the commands in listing
    7.12\. In the next subsection, we will deal with the data pipeline and determine
    which transformations to apply to our data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`cv_splits`是一个生成器，它只能使用一次，但你可以通过简单地重新执行列表7.12中的命令来重新实例化一个相同的生成器。在下一小节中，我们将处理数据管道，并确定对数据应用哪些转换。
- en: 7.2.2 Preparing your pipeline
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 准备您的管道
- en: A second preparatory step is to define a pipeline for transforming our predictors
    in the most appropriate way for running generally with all classical machine learning
    algorithms, not just gradient boosting. Ideally, it would be better to have multiple
    pipelines based on how each model deals with the different features. For instance,
    in our pipeline, we are going to ordinally encode a couple of categorical features,
    and such encoding, though fit for tree-based models, doesn’t always work properly
    with linear models. However, while having unique pipelines can lead to better
    performance, it can also become a maintenance nightmare. The human effort required
    to create and manage multiple pipelines may outweigh the marginal performance
    gains achieved by customizing each pipeline for specific models. Therefore it
    is better to decide on multiple pipelines if you have evidence that it is worth
    it.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个准备步骤是定义一个管道，以最合适的方式转换我们的预测器，以便通常与所有经典机器学习算法一起运行，而不仅仅是梯度提升。理想情况下，最好根据每个模型如何处理不同特征来拥有多个管道。例如，在我们的管道中，我们将对几个分类特征进行顺序编码，尽管这种编码适合基于树的模型，但并不总是与线性模型正确工作。然而，尽管拥有独特的管道可以带来更好的性能，但它也可能成为一个维护噩梦。创建和管理多个管道所需的人力可能超过了为特定模型定制每个管道所获得的边际性能提升。因此，如果你有证据表明这样做值得，那么决定使用多个管道会更好。
- en: 'Let’s start by classifying the different kinds of features that we will be
    using into categorical features, numeric, and binary features:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先对我们将要使用的不同类型的特征进行分类，分为分类特征、数值特征和二进制特征：
- en: '[PRE39]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A further inspection of the categorical features is necessary because we need
    to understand whether to treat them as high cardinality features or not. We can
    understand better what to do after having counted how many unique values each
    categorical feature has:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步检查分类特征是必要的，因为我们需要了解是否将它们视为高基数特征。在计算每个分类特征的唯一值数量后，我们可以更好地理解应该怎么做：
- en: '[PRE40]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'From the results, we can determine that probably the only feature that could
    be considered as a high cardinality categorical is the `coordinates` feature,
    which has almost 300 unique values. As for `neighbourhood_more_than_30` and `type_of_accomodation`,
    we could apply ordinal encoding to them for tree-based modeling, whereas for linear
    models, it would be better to apply one-hot-encoding to these features (thus producing
    about 50 new binary features) or target encoded:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以确定可能唯一可以考虑为高基数分类的特征是`coordinates`特征，它有近300个唯一值。至于`neighbourhood_more_than_30`和`type_of_accomodation`，我们可以对它们应用顺序编码以进行基于树的建模，而对于线性模型，最好对这些特征应用独热编码（从而产生大约50个新的二进制特征）或目标编码：
- en: '[PRE41]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Since our example revolves around XGBoost and demonstrating how it can work
    with similar problems, we decide to one-hot encode only the `room_type`, ordinal
    encoding `neighbourhood_more_than_30` and `type_of_accomodation`, and target encoding
    `coordinates`:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的示例围绕XGBoost，并展示它如何处理类似问题，我们决定仅对`room_type`进行独热编码，对`neighbourhood_more_than_30`和`type_of_accomodation`进行顺序编码，对`coordinates`进行目标编码：
- en: '[PRE42]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Our choice of working with XGBoost, a tree-based model, also justifies leaving
    all the numeric features as-is. Using linear models, statistical standardization,
    for better convergence when using regularization or generalized linear models,
    and feature transformation, for better fitting nonlinearities, are usually the
    standard.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用基于树的模型XGBoost，这也解释了为什么我们保留所有数值特征不变。使用线性模型、统计标准化，以在使用正则化或广义线性模型时获得更好的收敛性，以及特征转换，以更好地拟合非线性，通常是标准做法。
- en: In listing 7.13, we define all the necessary feature transformations and ensemble
    them in a Scikit-learn’s Column Transformer, which will be part of the pipeline
    that also contains the machine learning model of our choice. It is also important
    to note that we take steps in defining the column transformers to handle unknown
    categories and missing values that may unexpectedly appear at test time. Our strategy
    for one-hot encoding is to ignore new unknown categories. For ordinal encoding,
    the value assigned to the parameter `unknown_value`, which is by default `np.nan,`
    will be used to encode unknown categories. This means an XGBoost model will deal
    with such situations as missing cases using the most frequent split. Other machine
    learning algorithms may instead break into similar occurrences, which is an advantage
    of XGBoost. As for the target encoder, the target mean is substituted for unknown
    categories. Don’t forget to install the `category_encoders` package. If it is
    unavailable on your system, use the `pip install` `category_encoders` command.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表7.13中，我们定义了所有必要的特征转换，并将它们集成到Scikit-learn的列转换器中，这将作为管道的一部分，该管道还将包含我们选择的机器学习模型。还应注意，我们在定义列转换器时采取步骤来处理测试时可能意外出现的未知类别和缺失值。我们单热编码的策略是忽略新的未知类别。对于有序编码，分配给参数`unknown_value`的值，默认为`np.nan`，将用于编码未知类别。这意味着XGBoost模型将使用最频繁的分割来处理此类情况。其他机器学习算法可能会在类似情况下中断，这是XGBoost的优势。至于目标编码器，未知类别用目标均值替换。不要忘记安装`category_encoders`包。如果您的系统上不可用，请使用`pip
    install category_encoders`命令。
- en: Listing 7.13 Defining column transformations
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.13 定义列转换
- en: '[PRE43]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ① Creates a One-Hot Encoder object with the option to handle unknown categories
    by ignoring them during encoding
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个单热编码器对象，具有在编码过程中忽略未知类别的选项
- en: ② Creates an Ordinal Encoder object with handling of unknown categories and
    the unknown value replaced by np.nan
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个具有处理未知类别和将未知值替换为np.nan的有序编码器对象
- en: ③ Creates a TargetEncoder object, handling unknown values by encoding them using
    the mean target value and applying smoothing with a parameter of 0.5
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个目标编码器对象，通过使用平均目标值进行编码并应用平滑参数为0.5来处理未知值
- en: ④ A Column Transformer object applying the specified encoders to the respective
    columns
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建一个列转换器对象，将指定的编码器应用于相应的列
- en: ⑤ Drops remaining columns that are not specified in the transformer
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 删除转换器中未指定的剩余列
- en: ⑥ Keeps verbose feature names for transformed columns
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 保留转换列的详细特征名称
- en: ⑦ Ensures that the transformed data is kept as dense arrays
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 确保转换后的数据保持为密集数组
- en: 'After having run the code listing, we can immediately test transforming the
    data we have and checking the transformed column names:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码列表后，我们可以立即测试转换我们拥有的数据并检查转换后的列名：
- en: '[PRE44]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output shows that now the features are preceded by a prefix pointing out
    what transformation they underwent. Binary features created by one-hot encoding
    are also followed by the category they represent:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，现在特征前面有一个前缀，指出它们经历了什么转换。由单热编码创建的二进制特征也跟随着它们所代表的类别：
- en: '[PRE45]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As a final step, we just store away into a single CSV file both the processed
    features and the target. We will use such data again later, in chapter 12, when
    we test a deep learning solution and compare its performance with the XGBoost
    model we train in this chapter.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们将处理后的特征和目标值存储到一个单独的CSV文件中。我们将在第12章再次使用这些数据，那时我们将测试一个深度学习解决方案，并将其性能与本章训练的XGBoost模型进行比较。
- en: '[PRE46]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now that we have the data processing part of our pipeline, we can proceed to
    define a baseline model and then, finally, an XGBoost regressor.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了管道的数据处理部分，我们可以继续定义基线模型，然后最终定义XGBoost回归器。
- en: 7.2.3 Building a baseline model
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 构建基线模型
- en: 'Having a baseline model in machine learning is important for several reasons:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中拥有基线模型有几个重要原因：
- en: '*Comparing performance*—The baseline model serves as a benchmark to compare
    the performance of more complex models, helping you to understand whether more
    complexity really adds value.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*比较性能*——基线模型作为基准，用于比较更复杂模型的性能，帮助你理解更多的复杂性是否真的增加了价值。'
- en: '*Detecting overfitting*—By comparing the performance of your advanced model
    against the baseline on unseen data, you can identify if the advanced model is
    overfitting because the baseline model will perform much better.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检测过拟合*——通过比较你的高级模型与基线在未见数据上的性能，你可以确定高级模型是否过拟合，因为基线模型的表现会好得多。'
- en: '*Understanding the problem*—Creating a simple baseline model, especially if
    it is a linear model, forces you to understand the data and the problem better.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解问题*——创建一个简单的基线模型，尤其是如果它是一个线性模型，迫使你更好地理解数据和问题。'
- en: '*Debugging and validation*—A baseline model can help you validate that your
    data preprocessing pipeline is correct because the effects of the variables on
    the model won’t be hidden by its complexity.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调试和验证*——基线模型可以帮助你验证你的数据预处理流程是否正确，因为变量的影响不会被模型的复杂性所掩盖。'
- en: '*Providing a minimum viable model*—A baseline model provides a minimum viable
    solution to the problem at hand.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提供最小可行模型*——基线模型为当前问题提供了一个最小可行解决方案。'
- en: For all these reasons, we won’t immediately jump into training our model using
    a gradient boosting model, which we expect will perform well on the problem, but
    we take a step back and test a simple linear model. In addition, at this time,
    we will try to get predictions from a type of model we can easily evaluate and
    compare. Instead of just evaluating metrics through cross-validation, we’ll employ
    cross-validation prediction. This method provides unbiased predictions for all
    training cases by making predictions on validation folds within cross-validation.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些原因，我们不会立即跳入使用梯度提升模型训练我们的模型，因为我们期望它在问题上表现良好，但我们退一步，测试一个简单的线性模型。此外，在此期间，我们将尝试从一种我们可以轻松评估和比较的模型类型中获得预测。我们不仅通过交叉验证评估指标，还会采用交叉验证预测。这种方法通过在交叉验证的验证折叠中进行预测，为所有训练案例提供无偏预测。
- en: During cross-validation, evaluation metrics are calculated separately for each
    fold. These metrics represent the performance of the model on each fold individually.
    The final evaluation metric reported from cross-validation is usually an average
    (mean or median) of the individual fold metrics. This aggregated metric provides
    an estimate of the model’s generalization performance on unseen data. However,
    if we are using cross-validation predictions, we concentrate on the ability of
    the model to perform on the data at hand. In fact, the primary use of cross-validation
    predictions is to analyze the predictions made by the model on different parts
    of the data used as validation. Using such predictions helps us understand how
    well the model performs across different subsets of the data and identifies if
    the model is overfitting or underfitting because we can compare the predictions
    with the expected target values.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证过程中，评估指标会为每个折叠单独计算。这些指标代表模型在每个折叠上的性能。从交叉验证中报告的最终评估指标通常是单个折叠指标的均值（平均值或中位数）。这个聚合指标提供了模型在未见数据上的泛化性能估计。然而，如果我们使用交叉验证预测，我们则专注于模型在现有数据上的表现能力。实际上，交叉验证预测的主要用途是分析模型在不同数据部分上的预测。使用这些预测有助于我们了解模型在不同数据子集上的表现如何，并确定模型是否过拟合或欠拟合，因为我们可以将预测与预期的目标值进行比较。
- en: Listing 7.14 Linear regression baseline model with diagnostic plots
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.14 带诊断图的线性回归基线模型
- en: '[PRE47]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ① Initializes a LinearRegression model without intercept
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化一个没有截距的线性回归模型
- en: ② Fits the LinearRegression model to the transformed training data
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将线性回归模型拟合到转换后的训练数据
- en: ③ Creates stratified cross-validation splits based on the neighborhoods with
    more than 30 counts
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 根据超过30个计数的邻域创建分层交叉验证分割
- en: ④ Performs cross-validated predictions
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 执行交叉验证预测
- en: ⑤ Prints the range of cross-validated predictions
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证预测的范围
- en: ⑥ Calculates R-squared, root mean squared error, and mean absolute error evaluation
    metrics to assess the model’s performance
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 计算R平方、均方根误差和平均绝对误差评估指标以评估模型性能
- en: ⑦ Creates a scatter plot of actual vs. predicted values
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 创建实际值与预测值之间的散点图
- en: ⑧ Plots a dashed orange zero line to the plot as a reference for the ideal fit
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 在图表上绘制一条虚线橙色零线作为理想拟合的参考
- en: 'After running the code, we obtain the evaluation results, and we can immediately
    notice how some predictions are negative. Since a linear regression model is not
    bounded in its predictions, the mean absolute error (MAE) is quite high (over
    12,000 Yen), and the R squared, a typical measure of fit measuring how much of
    the variance of the target is captured by the model, is just a modest 0.32:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们获得了评估结果，我们可以立即注意到一些预测是负数。由于线性回归模型的预测没有界限，平均绝对误差（MAE）相当高（超过12,000日元），而R平方，一个典型的拟合度度量，衡量模型捕获目标方差的比例，仅为0.32：
- en: '[PRE48]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Clearly, the fitting of the model is not particularly impressive, and we can
    get a confirmation as shown in figure 7.11, where we represent the scatterplot
    of the cross-validation predictions on the y-axis against the expected target
    values on the x-axis. Apart from a few negative predictions at the start of the
    target distribution, we can also notice how the predictions depart from the ideal
    fit dashed line, showing a flat trend, a clear sign of underfitting, and how there
    are a few outlying predictions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型的拟合并不特别令人印象深刻，我们可以在图7.11中得到证实，其中我们在y轴上表示交叉验证预测的散点图，与x轴上的预期目标值相对应。除了目标分布开始处的一些负预测外，我们还可以注意到预测如何偏离理想的拟合虚线，显示出平坦的趋势，这是欠拟合的明显迹象，以及如何有一些异常的预测。
- en: '![](../Images/CH07_F11_Ryan2.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F11_Ryan2.png)'
- en: Figure 7.11 Plot of the fitted results from the baseline linear regression against
    their ideal value
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 基线线性回归拟合结果与其理想值的对比图
- en: 'As a first step in examining the results, we ask for the percentage of predictions
    that are below or equal to zero, an unfeasible prediction because listings should
    be positive:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查结果的第一步中，我们要求预测值低于或等于零的百分比，这是一个不可行的预测，因为列表应该是正数：
- en: '[PRE49]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result is a minimal percentage, about 0.5%:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是极低的百分比，大约0.5%：
- en: '[PRE50]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Ideally, our predictions should be greater than zero, and in a linear model
    that could be achieved by a target transformation—for instance, a logarithmic
    transformation. However, the role of a baseline model is not to be a perfect model
    but just a model to highlight challenges in the data and be a helpful comparison
    for more sophisticated models.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的预测应该大于零，在一个线性模型中，这可以通过目标转换来实现——例如，对数转换。然而，基线模型的作用并不是成为一个完美的模型，而只是一个用来突出数据中的挑战，并为更复杂的模型提供有益比较的模型。
- en: 'Now we proceed to locate the rows that are positive outliers:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续定位正异常值的行：
- en: '[PRE51]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We received two cases: 5509 and 8307:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了两个案例：5509和8307：
- en: '[PRE52]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We also inquiry about the negative outliers:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还调查了负异常值：
- en: '[PRE53]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here we get a single case, 182:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们得到一个单一案例，182：
- en: '[PRE54]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In listing 7.15, we define a function that can help us check for outliers. For
    each predictor feature, this function prints the coefficient and the resulting
    multiplication of the coefficient with the value of the feature for that case,
    thus making explicit the contributions of each feature to the prediction.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表7.15中，我们定义了一个函数，可以帮助我们检查异常值。对于每个预测特征，这个函数打印出系数以及该案例中系数与特征值的乘积，从而明确每个特征对预测的贡献。
- en: Listing 7.15 Inspection of the coefficients
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.15 系数检查
- en: '[PRE55]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ① Extracts the feature values for the specified case number from the data array
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从数据数组中提取指定案例编号的特征值
- en: ② Calculates the coefficient values for each feature by multiplying its value
    with the corresponding coefficient from the model
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过将每个特征的值与其模型中的相应系数相乘来计算每个特征的系数值
- en: ③ Loops and prints through the feature names, their values, and corresponding
    coefficient values
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 遍历并打印特征名称、它们的值以及相应的系数值
- en: ④ Prints the sum of the calculated coefficient values for the case
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打印出计算出的系数值的总和
- en: 'Having our inspection function ready, we can start examining case 8307, which
    represents a case of too positively large outlier in the predictions:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的检查函数准备就绪后，我们可以开始检查案例8307，它代表了一个预测中过于正大的异常值：
- en: '[PRE56]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following are the results for case 8307, where it is evident that the extra
    contribution that made the prediction an outlier is because of the number of bedrooms
    (hinting that this property is probably a hostel). This high value pushed the
    final predicted listing upwards:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对案例8307的结果，很明显，使预测成为异常值的额外贡献是由于卧室数量（暗示这可能是宿舍）。这个高值将最终预测列表推高：
- en: '[PRE57]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Similar problems are due to the fact that each feature is modeled in a linear
    way. Thus the prediction contribution of a feature is unbounded, having no maximum
    or minimum but decreasing and increasing in accordance with the feature value.
    Typically, introducing nonlinearities and interactions into the model nonlinearities
    and interactions can mitigate such problems. Let’s now check the only negative
    outlier:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的问题是由于每个特征都是线性建模的。因此，特征的预测贡献是无界的，没有最大值或最小值，而是根据特征值增加或减少。通常，将非线性性和交互作用引入模型中的非线性性和交互作用可以缓解这些问题。现在让我们检查唯一的负异常值：
- en: '[PRE58]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here the problem is represented by the value of the minimum of nights, which
    is again too high and drags down the estimated value. In fact, some listings act
    as seasonal accommodation, typically for workers or students, not just for short
    stays. The model is indeed too simple to catch such nuances, and again, having
    introduced nonlinearities and interactions could have helped:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里问题表现为夜数的最小值，这又太高，拉低了估计值。事实上，一些列表作为季节性住宿，通常为工人或学生，而不仅仅是短期住宿。模型确实过于简单，无法捕捉到这样的细微差别，而且，引入非线性性和交互作用可能会有所帮助：
- en: '[PRE59]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In conclusion, our baseline model has signaled us that successfully solving
    the problem presented by the Tokyo Airbnb dataset requires a better fitting mode
    that can handle positive predictions (they should be necessarily positive) and
    can represent nonlinear relationships and interactions between a particular characteristic
    of the accommodation (a large number of bedrooms indicate a hostel, a high number
    of minimum stay nights indicates an accommodation for seasonal tenants). In the
    next subsection, we will solve all these problems at once by using an XGBoost
    model, which should be able to deal with this data in a more sophisticated and
    smart way.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的基线模型已经向我们表明，成功解决东京Airbnb数据集提出的问题需要一个更好的拟合模式，该模式可以处理正预测（它们应该是必要的正数）并且可以表示住宿的特定特征（如大量卧室表示旅舍，高最低住宿夜数表示季节性租户）之间的非线性关系和交互作用。在下一小节中，我们将通过使用XGBoost模型一次性解决所有这些问题，该模型应该能够以更复杂和智能的方式处理这些数据。
- en: 7.2.4 Building a first tentative model
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 构建第一个尝试性模型
- en: First, we chose an XGBoost regressor, trying to incorporate some of the insights
    we gained from our previous EDA and baseline model inspections. We decided to
    use a gamma objective function, commonly used in regression problems, for modeling
    positive continuous variables that are positive and right-skewed. Gamma is particularly
    useful when the target variable is always positive, and it includes many small
    values and a few larger values, as it handles such distribution characteristics
    quite well.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们选择了一个XGBoost回归器，试图结合我们从之前的EDA和基线模型检查中获得的一些见解。我们决定使用gamma目标函数，这在回归问题中常用，用于建模正的连续变量，这些变量是正偏斜的。当目标变量始终为正，并且包含许多小值和一些较大值时，gamma特别有用，因为它很好地处理了这种分布特征。
- en: In addition, since our baseline model has shown signs of underfitting and not
    handling interactions or linearities properly, we decide on a max depth of at
    most six for the decision trees composing the boosted ensemble, thus allowing
    for an adequate number of splits to handle most common similar data characteristics.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们的基线模型已经显示出欠拟合的迹象，并且没有正确处理交互作用或线性关系，我们决定对于组成增强集的决策树的最大深度不超过六，从而允许有足够数量的分割来处理大多数常见的数据特征。
- en: In the following listing, arranged similarly as the previous listing training
    the linear regression baseline, we train an XGBoost regressor, and we test its
    out-of-fold cross-validation predictions.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，与之前的列表训练线性回归基线类似，我们训练了一个XGBoost回归器，并测试了其折叠交叉验证的预测结果。
- en: Listing 7.16 First XGBoost model
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.16 第一个XGBoost模型
- en: '[PRE60]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: ① Sets up an XGBoost regressor with specific hyperparameters
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用特定超参数设置XGBoost回归器
- en: ② Defines ‘reg:gamma’ as the objective function
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将“reg:gamma”定义为目标函数
- en: ③ Generates cross-validation splits based on the neighbourhood_more_than_30
    feature
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 根据neighbourhood_more_than_30特征生成交叉验证分割
- en: ④ Performs cross-validated predictions
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 执行交叉验证预测
- en: ⑤ Prints the range of predicted values
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印预测值的范围
- en: ⑥ Calculates R-squared, root mean squared error, and MAE evaluation metrics
    to assess the model’s performance
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 计算R平方、均方根误差和MAE评估指标以评估模型性能
- en: ⑦ Creates a scatter plot of actual vs. predicted values
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 创建实际值与预测值之间的散点图
- en: ⑧ Adds reference lines to the plot for an ideal fit and a zero line
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 在图中添加理想拟合和零线的参考线
- en: 'This time, the prediction range is strictly in the positive range, as we expected.
    The MAE is almost half of those of the baseline linear model and the R-squared
    scores almost 0.7, which is a reasonably good result showing how the model is
    now able to intercept most of the variance present in the target:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，预测范围严格在正数范围内，正如我们所预期的那样。平均绝对误差（MAE）几乎是基线线性模型的一半，R-squared得分接近0.7，这是一个相当好的结果，显示了模型现在能够拦截目标中大部分的方差：
- en: '[PRE61]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'A further inspection of the fitted results, represented in figure 7.12 as a
    scatterplot between the cross-validation predictions (on the y-axis) and the expected
    target values (on the x-axis), shows that the points are now slightly more in
    line with our ideal fit. In addition, it is important to notice how the XGBoost
    model tends to extrapolate the predictions: the column of predictions at the rightmost
    part of the chart indicates that our model predicted values sometimes higher than
    the observed maximum in the target, whereas, on the leftmost part of the chart,
    there are no nonpositive estimations.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步检查拟合结果，如图7.12所示的散点图，横轴为交叉验证预测（y轴），纵轴为预期目标值，显示点现在与我们的理想拟合稍微更一致。此外，重要的是要注意XGBoost模型如何倾向于外推预测：图表最右侧的预测列表明，我们的模型有时预测的值高于目标中观察到的最大值，而在图表最左侧，没有非正估计。
- en: '![](../Images/CH07_F12_Ryan2.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F12_Ryan2.png)'
- en: Figure 7.12 Plot of the fitted results for the XGBoost model against their ideal
    value
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 XGBoost模型拟合结果与其理想值的对比图
- en: Typically, you shouldn’t limit yourself to a single model in a data science
    project, as in our example. Because of space constraints, we just focus on an
    XGBoost model. Still, it is advisable in a working project to try even more diverse
    classical machine learning algorithms, such as other gradient boosting implementations,
    as well as more tree ensembles, generalized linear models, and even more unusual,
    nowadays, classical machine learning models (such as k-nearest neighbors or support
    vector machines). There is no free lunch in machine learning, and you may find
    reasonable solutions to this problem even with different algorithms that may better
    suit your necessities in terms of performance, speed of inference, memory occupancy,
    and portability onto other systems.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在数据科学项目中，你不应该只限于使用单一模型，就像我们的例子那样。由于空间限制，我们只关注XGBoost模型。然而，在一个实际项目中尝试更多样化的经典机器学习算法是明智的，例如其他梯度提升实现、更多的树集成、广义线性模型，甚至是现在越来越不常见的经典机器学习模型（如k-最近邻或支持向量机）。在机器学习中没有免费的午餐，你可能会发现即使使用不同的算法，也可能找到适合你需求的合理解决方案，这些需求包括性能、推理速度、内存占用以及移植到其他系统。
- en: In the following subsection, we optimize our XGBoost solution using Bayesian
    optimization to strive to perform as best as possible on our problem.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们使用贝叶斯优化来优化我们的XGBoost解决方案，力求在我们的问题上表现最佳。
- en: 7.2.5 Optimizing your model
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 优化你的模型
- en: Since XGBoost works quite well for the problem, we’ll take some time to refine
    its parameters and test different boosting approaches and objectives. We are going
    to use Optuna, a Bayesian optimizer presented in the previous chapter, because
    it can efficiently explore a GBDT hyperparameter search space, adaptively choosing
    in a short set of rounds, based on the outcomes of the previous experiments, the
    next set of hyperparameters to be evaluated.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 由于XGBoost对于这个问题表现相当好，我们将花一些时间来细化其参数，并测试不同的提升方法和目标。我们将使用Optuna，这是在前一章中介绍的一种贝叶斯优化器，因为它可以有效地探索GBDT超参数搜索空间，根据先前实验的结果，在短时间内自适应地选择下一组要评估的超参数。
- en: If you don’t have Optuna available on your system, you can install it by issuing
    the command `pip` `install` `optuna` in a shell or a notebook cell.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你系统上没有Optuna，你可以在shell或笔记本单元中运行`pip install optuna`命令来安装它。
- en: Listing 7.17 performs hyperparameter optimization using Optuna for our previously
    tested XGBoost Regressor model to find the best hyperparameters that minimize
    the MAE of the model on the Tokyo Airbnb dataset. The core of the listing is the
    objective function that suggests to Optuna different hyperparameter values using
    t`rial.suggest_...` methods. In particular, it tests the classic `gbtree` booster
    (gradient boosting) and also the `gblinear`. This booster utilizes a linear model
    as its base learner, incorporating both L1 and L2 regularization instead of employing
    a decision tree. Regarding the objective function, it tests the classical squared
    error, the gamma objective, and the Tweedie, blending aspects of the gamma and
    Poisson distributions. When selecting the gblinear boosting or the Tweedie objective,
    the code overrides the chosen parameters. It makes modifications and additions
    to them to fit the requirements of the gblinear booster or the Tweedie objective.
    Finally, it then creates an XGBoost Regressor with the suggested hyperparameters
    at each test and performs cross-validation to evaluate the MAE. The process is
    repeated for a specified number of trials (60 in this case). After optimization,
    the best achieved MAE and corresponding best hyperparameters are printed.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.17 使用 Optuna 对之前测试过的 XGBoost 回归器模型进行超参数优化，以找到最佳超参数，最小化东京 Airbnb 数据集上模型的
    MAE。列表的核心是目标函数，它使用 `trial.suggest_...` 方法向 Optuna 建议不同的超参数值。特别是，它测试了经典的 `gbtree`
    增强器（梯度提升）和 `gblinear`。这个增强器使用线性模型作为其基学习器，结合 L1 和 L2 正则化，而不是使用决策树。关于目标函数，它测试了经典平方误差、gamma
    目标和 Tweedie，结合了 gamma 和泊松分布的方面。在选择 gblinear 增强器或 Tweedie 目标时，代码覆盖了所选参数。它对它们进行修改和添加，以适应
    gblinear 增强器或 Tweedie 目标的要求。最后，它在每个测试中创建具有建议超参数的 XGBoost 回归器，并执行交叉验证以评估 MAE。这个过程重复进行指定次数的试验（本例中为
    60 次）。优化完成后，打印出最佳 MAE 和相应的最佳超参数。
- en: Listing 7.17 Optimizing the XGBoost regressor
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.17 优化 XGBoost 回归器
- en: '[PRE62]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: ① Defines an optimization objective function using the Optuna library
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 Optuna 库定义一个优化目标函数
- en: ② Dictionary containing hyperparameters for optimization, including booster
    type, objectives, and others
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ② 包含优化超参数的字典，包括增强器类型、目标和其他内容
- en: ③ Adjusts hyperparameters based on the chosen booster type
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 根据所选增强器类型调整超参数
- en: ④ Suggests the additional parameter 'tweedie_variance_power' for a tweedie objective
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为 tweedie 目标建议额外的参数 'tweedie_variance_power'
- en: ⑤ Initializes an XGBoost Regressor with the suggested hyperparameters
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用建议的超参数初始化 XGBoost 回归器
- en: ⑥ Performs cross-validation using the defined pipeline and optimizing for MAE
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用定义的管道执行交叉验证，并优化 MAE
- en: ⑦ Calculates the MAE from the negative MAE scores
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 从负 MAE 分数计算 MAE
- en: ⑧ Returns the calculated evaluation metric value to be minimized
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 返回要最小化的计算评估指标值
- en: ⑨ Creates an Optuna study with storage in a SQLite database for optimization
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 创建一个 Optuna 研究，存储在 SQLite 数据库中以进行优化
- en: ⑩ Performs optimization for a specified number of trials
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 对指定次数的试验进行优化
- en: ⑪ Prints the best evaluation metric value achieved during optimization
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 打印优化过程中达到的最佳评估指标值
- en: ⑫ Prints the best hyperparameters found during optimization
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 打印优化过程中找到的最佳超参数
- en: 'After having the optimization run for a while, we obtain a reduced MAE in respect
    of our first attempt and a set of suitable hyperparameters, showing a max depth
    of seven levels, about 900 estimators, and a Tweedie objective function with a
    variance power of 1.5, indicating a mixed distribution between Poisson and gamma:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间的优化运行后，我们获得了相对于第一次尝试的降低 MAE 和一组合适的超参数，显示最大深度为七级，大约 900 个估计量，以及具有 1.5 的方差功率的
    Tweedie 目标函数，表明 Poisson 和 gamma 之间的混合分布：
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'At this point, we can also plot some diagnostic charts to understand better
    how the optimization went. For instance, we can first plot how the optimization
    proceeds across the 60 trials that we initially set:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们还可以绘制一些诊断图表，以更好地了解优化过程。例如，我们可以首先绘制优化如何在我们最初设置的 60 次试验中进行的：
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Figure 7.13 shows how the best value was achieved quite early, before 20 trials,
    and it didn’t improve after that. This is important information because if the
    best optimization could have been achieved later, you may have suspected further
    improvements lying ahead in some more rounds of hyperparameter exploration by
    Optuna. Actually, you may have achieved that anytime by rerunning the command
    `study.optimize(objective,` `n_trials=100)`, setting your intended number of extra
    trials instead of the initial 100\. Since we set to store trials on an SQLite
    database, you may reprise the optimization any time from the point it stopped
    (which is a strong point for using Optuna instead of other optimization options).
    Another important fact to gather from the chart is that there is quite a crowd
    of hyperparameter sets that are optimal or almost optimal. That means that there
    is no single optimization possible for this problem. That allows you to explore
    the different settings and decide on the solution that suits your needs. For instance,
    you may decide on a near-optimal solution that samples more features or requires
    fewer estimators since they are faster at inference time.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13显示了最佳值在20次试验之前就已经实现，之后并没有改善。这是重要信息，因为如果最佳优化可以在之后实现，你可能会怀疑在更多轮次的超参数探索中还有进一步的改进空间。实际上，你可以通过重新运行命令`study.optimize(objective,
    n_trials=100)`来实现这一点，设置你想要的额外试验次数而不是最初的100次。由于我们设置为将试验存储在SQLite数据库中，你可以在停止的点重新开始优化（这是使用Optuna而不是其他优化选项的优势之一）。从图表中还可以得出另一个重要的事实，那就是有相当多的超参数集是最佳或几乎最佳的。这意味着对于这个问题没有单一的优化方法。这允许你探索不同的设置，并选择适合你需求的解决方案。例如，你可能会选择一个近似最佳解决方案，它采样更多特征或需要更少的估计量，因为它们在推理时间上更快。
- en: '![](../Images/CH07_F13_Ryan2.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F13_Ryan2.png)'
- en: Figure 7.13 How tests by Optuna progressively performed during the optimization
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 Optuna在优化过程中逐步进行的测试
- en: 'After observing how the optimization proceeded, another important piece of
    information is provided by charting the importance of the hyperparameters because
    it could hint at expanding the search space for such hyperparameters if they proved
    so important for the optimization process:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到优化过程后，图表还提供了关于超参数重要性的另一条重要信息，因为如果它们在优化过程中证明非常重要，这可能会提示扩大这些超参数的搜索空间：
- en: '[PRE65]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In our case, the most critical factors proved to be `colsample_bytree` and `min_child_weight`,
    hyperparameters that caused the most variance in results, as seen in figure 7.14.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，最关键的因素证明是`colsample_bytree`和`min_child_weight`，这些超参数在结果中造成了最大的变化，如图7.14所示。
- en: '![](../Images/CH07_F14_Ryan2.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F14_Ryan2.png)'
- en: Figure 7.14 Importance of hyperparameters in the tuning process
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 调优过程中超参数的重要性
- en: Now we have a good set of hyperparameters. In the next subsection, we will complete
    our training phase by testing the model with cross-validation, an evaluation for
    generalization purposes, and training the final model using all the available
    data.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一组很好的超参数。在下一小节中，我们将通过使用交叉验证来测试模型，这是一种用于泛化目的的评估，并使用所有可用数据训练最终模型，来完成我们的训练阶段。
- en: 7.2.6 Training the final model
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 训练最终模型
- en: Having completed the optimization, we can conclude our work by testing the results
    directly by cross-validation and then training the model on all available data.
    The code presented in listing 7.18 doesn’t change much from the code we previously
    used. Notice that now, for our estimation, we are using a cross-validation procedure,
    not cross-validation predictions, because we are more interested in understanding
    the generalization capabilities of our model and not how it fits precisely the
    data at hand.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 完成优化后，我们可以通过直接进行交叉验证来测试结果，然后在所有可用数据上训练模型。列表7.18中展示的代码与我们之前使用的代码变化不大。请注意，现在，为了我们的估计，我们使用的是交叉验证过程，而不是交叉验证预测，因为我们更感兴趣的是了解我们模型的泛化能力，而不是它如何精确地拟合手头的数据。
- en: Listing 7.18 Training the model with full data
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.18 使用全部数据训练模型
- en: '[PRE66]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: ① Initializes an XGBoost Regressor using the best hyperparameters obtained from
    the Optuna study
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用从Optuna研究中获得的最佳超参数初始化XGBoost回归器
- en: ② Splits the data using the specified StratifiedKFold strategy
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用指定的StratifiedKFold策略分割数据
- en: ③ Iterates through the cross-validation folds to test the model
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 迭代交叉验证折来测试模型
- en: ④ Trains the final model on the entire dataset
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在整个数据集上训练最终模型
- en: 'The following is the output you receive when running the code, containing the
    used parameters and the evaluation metrics, all based on our cross-validation
    strategy:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在运行代码时收到的输出，包含使用的参数和评估指标，所有这些均基于我们的交叉验证策略：
- en: '[PRE67]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We can also visualize, as shown in figure 7.15, the complete pipeline comprising
    the column transformer, accepting the different features for its distinct transformation
    operations, and the XGBoost model receiving all the assembled data from the column
    transformer.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以可视化，如图7.15所示，完整的管道，包括列转换器，接受不同的特征进行其独特的转换操作，以及接收来自列转换器的所有组装数据的XGBoost模型。
- en: '![](../Images/CH07_F15_Ryan2.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F15_Ryan2.png)'
- en: Figure 7.15 Pipeline comprising column transformations and XGBoost model
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 由列转换和XGBoost模型组成的管道
- en: Having thoroughly trained our model, we could say that we are done. Actually,
    this could be just the first cycle of multiple iterations because models have
    to be retrained often to escape what is called concept drift, as we explained
    in chapter 2, where the relationships between the predictors and the target change
    over time, rendering past models ineffective after a while.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在彻底训练我们的模型后，我们可以说我们已经完成了。实际上，这可能是多个迭代周期的第一个循环，因为模型需要经常重新训练以避免所谓的概念漂移，正如我们在第2章中解释的那样，预测变量与目标变量之间的关系会随时间变化，使得过去的模型在一段时间后变得无效。
- en: In addition, often, the work of a machine learning engineer and of a data scientist
    doesn’t end with a working model because it is crucial to be able to figure out
    how it works and how the predictors actually relate with the target, providing
    insights into how a model arrives at its predictions. Explaining how a model works
    helps build trust, facilitates debugging, aids regulatory compliance, and enables
    humans to understand, validate, and improve the decision-making process of AI
    systems, which is the topic of the concluding section of this chapter.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通常，机器学习工程师和数据科学家的工作并不随着一个工作模型的完成而结束，因为能够弄清楚它是如何工作的以及预测变量实际上是如何与目标变量相关联的至关重要，这有助于了解模型是如何得出其预测的。解释模型的工作原理有助于建立信任，便于调试，有助于合规性，并使人类能够理解、验证和改进人工智能系统的决策过程，这是本章最后一节的主题。
- en: 7.3 Explaining your model with SHAP
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 使用SHAP解释你的模型
- en: To conclude, we spend some time trying to understand how our XGBoost model works
    because, as EDA helps you understand how your model can use data, explainability
    techniques such as SHAP (SHapley Additive exPlanations) or partial dependence
    plots (described in the previous chapter) can help you know how your model uses
    the data to come to its predictions. Explainability can provide valuable insights
    that help you better prepare your data, revise previous assumptions, and discard
    unuseful or detrimental features.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们花了一些时间来尝试理解我们的XGBoost模型是如何工作的，因为，正如EDA帮助你理解模型如何使用数据一样，可解释性技术，如SHAP（SHapley
    Additive exPlanations）或部分依赖图（在上一章中描述），可以帮助你了解模型是如何使用数据来做出预测的。可解释性可以提供有价值的见解，帮助你更好地准备数据，修正先前的假设，并丢弃无用或有害的特征。
- en: 'In addition, explainability plays other softer roles in a data science project
    than providing insights into how the model uses its features and generates predictions:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可解释性在数据科学项目中还扮演着其他更软性的角色，除了提供模型如何使用其特征和生成预测的见解之外：
- en: '*Human-AI collaboration*—When working with tabular data, data scientists collaborate
    with domain experts or business stakeholders who may not be well-versed in complex
    models. Explainability allows data scientists to communicate model insights effectively
    to nontechnical audiences.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人机协作*——当处理表格数据时，数据科学家会与领域专家或业务利益相关者合作，他们可能对复杂模型不太熟悉。可解释性允许数据科学家有效地与非技术受众沟通模型见解。'
- en: '*Building trust*—In certain domains, such as healthcare or finance, model explainability
    is essential to build trust with stakeholders and regulatory bodies.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*建立信任*——在医疗保健或金融等特定领域，模型的可解释性对于与利益相关者和监管机构建立信任至关重要。'
- en: '*Compliance and regulations*—In some geographical areas and industries, there
    are regulatory requirements for model transparency and explainability, such as
    in the European Union, where the General Data Protection Regulation emphasizes
    the “right to explanation” for automated decision-making systems.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合规性和法规*——在某些地理区域和行业中，对模型透明度和可解释性有监管要求，例如在欧洲联盟，通用数据保护条例强调了自动化决策系统的“解释权”。'
- en: '*Bias detection and mitigation*—Explainability can help identify biases in
    the data and the model’s decision-making process, highlighting if the model’s
    decision-making process could disadvantage any sensible group.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏差检测和缓解*——可解释性可以帮助识别数据和模型决策过程中的偏差，突出模型决策过程是否可能对任何合理的群体造成不利。'
- en: Given all these reasons, we decided to produce SHAP values, which can be generated
    by the SHAP package ([https://github.com/shap/shap](https://github.com/shap/shap);
    install with `pip` `install` `shap`) and its TreeSHAP algorithm for tree-based
    models but also natively and more efficiently by XGBoost, as well as LightGBM,
    using a simple procedure.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些原因，我们决定生成SHAP值，这些值可以通过SHAP包（[https://github.com/shap/shap](https://github.com/shap/shap)；使用`pip
    install shap`安装）及其TreeSHAP算法为基于树的模型生成，也可以通过XGBoost、LightGBM等原生且更高效的方式通过一个简单的程序生成。
- en: SHAP values are a method that can explain the way predictions of machine learning
    models are built. They are based on Shapley values, a cooperative game theory
    concept that fairly distributes each feature’s “credit” or “importance” in a model’s
    prediction for a specific data instance. In other words, SHAP values allocate
    the contribution of each feature to the model’s output using a simple additive
    formula.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP值是一种可以解释机器学习模型预测构建方式的方法。它们基于Shapley值，这是一个合作博弈论概念，在模型对特定数据实例的预测中公平地分配每个特征的“信用”或“重要性”。换句话说，SHAP值通过一个简单的加法公式将每个特征的贡献分配给模型输出。
- en: Shapley values consider the contribution of a feature across all possible combinations
    of features, which can be thought of as “games” in the model. These “games” involve
    training the model on different feature subsets. SHAP values approximate Shapley
    values using a resampling strategy to avoid computing all possible games for the
    model and feature sets. By using SHAP values, we gain insights into how each feature
    influences the model’s predictions on specific instances. This information is
    valuable for model debugging, feature engineering, and enhancing machine learning
    models’ overall interpretability and trustworthiness.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值考虑了特征在所有可能的特征组合中的贡献，这可以被视为模型中的“游戏”。这些“游戏”涉及在不同的特征子集上训练模型。SHAP值通过重采样策略来近似Shapley值，以避免为模型和特征集计算所有可能的“游戏”。通过使用SHAP值，我们可以深入了解每个特征如何影响模型对特定实例的预测。这些信息对于模型调试、特征工程以及增强机器学习模型的整体可解释性和可信度非常有价值。
- en: 'We implemented SHAP values in listing 7.19 to gain insights into our previously
    built XGBoost model. In the code, we first retrieve the trained XGBoost model
    from a pipeline. In particular, we get its booster, the core component of the
    XGBoost model responsible for implementing the gradient boosting algorithm. Then
    we transformed the training data two times: first because we could not use the
    pipeline to feed the data into the booster directly. Hence, we preprocess it by
    hand and extract its feature names for reference. Second, we transform the data
    into a DMatrix data structure (see the XGBoost documentation at [https://mng.bz/yWQd](https://mng.bz/yWQd)),
    a specific XGBoost data structure for efficient processing that is required for
    feeding the booster directly. At this point, we compute the SHAP values by a predict
    command with the parameter `pred_contribs` set to true. Another simple predict
    command just provides us with the predictions from the model to be used for comparison.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在列表7.19中实现了SHAP值，以深入了解我们之前构建的XGBoost模型。在代码中，我们首先从管道中检索训练好的XGBoost模型。特别是，我们获取其booster，这是XGBoost模型的核心组件，负责实现梯度提升算法。然后我们两次转换训练数据：首先是因为我们不能直接使用管道将数据喂给booster。因此，我们手动预处理并提取其特征名称以供参考。其次，我们将数据转换为一个DMatrix数据结构（参见XGBoost文档[https://mng.bz/yWQd](https://mng.bz/yWQd))，这是XGBoost的一个特定数据结构，用于高效处理，这对于直接喂给booster是必需的。在此阶段，我们通过设置参数`pred_contribs`为true的predict命令来计算SHAP值。另一个简单的predict命令只提供了模型用于比较的预测。
- en: Listing 7.19 SHAP values as an XGBoost output
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.19 SHAP值作为XGBoost输出
- en: '[PRE68]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: ① Retrieves the trained XGBoost booster object from the pipeline’s trained XGBoost
    model
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从管道的训练XGBoost模型中检索训练好的XGBoost booster对象
- en: ② Transforms the input data X using the processing pipeline
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用处理管道转换输入数据X
- en: ③ Gets the names of the transformed features after the processing pipeline’s
    transformations
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 获取处理管道转换后的转换后特征名称
- en: ④ Creates a DMatrix from the transformed input data
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 从转换后的输入数据创建一个DMatrix
- en: ⑤ Calculates SHAP values using the booster’s predict function with the pred_contribs=True
    argument
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用增强器的predict函数计算SHAP值，带有pred_contribs=True参数
- en: ⑥ Gets the raw predicted values for the input data
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 获取输入数据的原始预测值
- en: Just for comparison, we have to note that LightGBM is also capable of the same,
    using the same prediction method with the `pred_contribs` parameter set to true.
    The only difference is that you do not need to extract any booster from the trained
    LightGBM model. You just use the model itself directly.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 仅为了比较，我们必须指出，LightGBM也能够做到同样的事情，使用相同的预测方法，将`pred_contribs`参数设置为true。唯一的区别是您不需要从训练好的LightGBM模型中提取任何增强器。您只需直接使用该模型即可。
- en: 'Note that whether you are doing a classification or a regression, the resulting
    SHAP values obtained by this method are log transformations of a multiplicative
    model. It means that if you want to recreate the original prediction, you first
    have to exponentiate the values and then multiply them by themselves, as demonstrated
    by the following code snippet, reconstructing for the first example the prediction
    from the SHAP values and comparing it to the effective prediction:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，无论您是在进行分类还是回归，通过此方法获得的SHAP值都是乘法模型的对数变换。这意味着如果您想重新创建原始预测，您首先必须对值进行指数化，然后将它们相乘，如下面的代码片段所示，重建第一个示例的预测与有效预测进行比较：
- en: '[PRE69]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As you can see, there are slight discrepancies in the reconstruction, which
    can be attributed to approximations and small errors. However, in general, the
    SHAP values provide a good approximation of the predictions themselves. When applying
    the same approach to the entire training set and assessing its adherence to the
    original predictions using Pearson’s correlation, it demonstrates a strong fit
    of the SHAP values to the predictions:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在重建过程中存在细微的差异，这些差异可以归因于近似和小误差。然而，总的来说，SHAP值提供了对预测本身的良好近似。当将相同的方法应用于整个训练集，并使用皮尔逊相关系数评估其对原始预测的遵循程度时，它显示了SHAP值与预测之间有很强的拟合度：
- en: '[PRE70]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'As an alternative to directly outputting the SHAP values as an XGBoost prediction,
    you can use the `TreeExplainer` function from the SHAP package ([https://mng.bz/pKXR](https://mng.bz/pKXR)).
    The function, though being declared built with fast C++ implementations, is way
    slower than the direct predictions from XGBoost. However, using the `TreeExplainer`,
    you can specify more output options, particularly the output type and the calculation
    method, which can allow you to reconstruct the original prediction as seen previously
    (using the parameter `feature_perturbation="tree_path_dependent"`) or using a
    method that “breaks the dependencies between features according to the rules dictated
    by casual inference,” thus providing more reliable insights when there is strong
    collinearity among the features (using the parameter `feature_perturbation="interventional"`).
    You can obtain the interventional SHAP values using the following code snippet:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 作为直接输出SHAP值作为XGBoost预测的替代方案，您可以使用SHAP包中的`TreeExplainer`函数([https://mng.bz/pKXR](https://mng.bz/pKXR))。该函数虽然声明是用快速的C++实现构建的，但比XGBoost的直接预测要慢得多。然而，使用`TreeExplainer`，您可以指定更多的输出选项，特别是输出类型和计算方法，这可以使您重建如前所述的原始预测（使用参数`feature_perturbation="tree_path_dependent"`）或使用一种“根据因果推理规则打破特征之间依赖关系”的方法，从而在特征之间存在强共线性时提供更可靠的见解（使用参数`feature_perturbation="interventional"`）。您可以使用以下代码片段获取干预SHAP值：
- en: '[PRE71]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The resulting SHAP values matrix is less truthful to the original data and
    cannot reconstruct the predictions as seen before. Still, such an approach could
    provide more reliable contribution estimates “true to the model” as explained
    in technical terms in the following GitHub problem: [https://github.com/shap/shap/issues/1098](https://github.com/shap/shap/issues/1098).
    Based on our experience, we suggest using `TreeExplainer` and the interventional
    approach, although it may require longer computation times when dealing with data
    presenting highly multicollinear features.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的SHAP值矩阵对原始数据不太忠实，无法重建如前所述的预测。尽管如此，这种方法可能提供更可靠的“忠实于模型”的贡献估计，如以下GitHub问题中用技术术语解释：[https://github.com/shap/shap/issues/1098](https://github.com/shap/shap/issues/1098)。根据我们的经验，我们建议使用`TreeExplainer`和干预方法，尽管在处理具有高度多重共线性特征的数据时，这可能会需要更长的计算时间。
- en: Up to now, we have used the SHAP values as a method for explaining individual
    samples. We investigated, by the inspection of feature contributions, the reasons
    why a certain prediction is made. However, we can consider all the SHAP values
    together and reason about them to figure out a general explanation for the entire
    model. In this case, as for other methods, we can plot some summary and diagnostic
    charts to figure this out better. The first listing we propose quantifies the
    relative importance of the features by an average of the SHAP values. Here, we
    use plotting facilities from the `shap` package. You install the package by running
    the command `pip` `install` `shap` in a shell or a cell of your notebook.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已将 SHAP 值用作解释单个样本的方法。通过检查特征贡献，我们研究了为何做出某种预测的原因。然而，我们可以将所有 SHAP 值综合考虑，并对其进行分析，以找出整个模型的一般解释。在这种情况下，与其他方法一样，我们可以绘制一些总结和诊断图表来更好地理解这一点。我们提出的第一个列表通过
    SHAP 值的平均值来量化特征的相对重要性。在这里，我们使用来自 `shap` 包的绘图功能。您可以通过在 shell 或笔记本的单元格中运行命令 `pip
    install shap` 来安装此包。
- en: Listing 7.20 SHAP importance plot
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.20 SHAP 重要性图
- en: '[PRE72]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: ① Generates a summary plot of SHAP feature importance for the top 10 most important
    features
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成 SHAP 特征重要性的前 10 个最重要特征的总结图
- en: 'Figure 7.16 shows the resulting plot, and you can immediately determine that
    four features tend to dominate the predictions, which are the availability, which
    is also a proxy for the market offer-demand dynamic for a certain accommodation
    (less availability may imply a shared use or less demand for that accommodation);
    target encoded coordinates (i.e., the position of the accommodation in the city);
    the number of bedrooms, a proxy of how large the accommodation is; and the number
    of beds, which helps together with the previous figure to distinguish the hostel-like
    listings, which are usually less pricey. All the other features play a lesser
    role, which can be seen from the scale of the plot: the importance of the last
    of the 10 most important features is a fifth of the top important features.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 显示了生成的图表，您可以立即确定有四个特征倾向于主导预测，这些特征是可用性，这同时也是某种住宿市场供需动态的代理（可用性较低可能意味着共享使用或对该住宿的需求较低）；目标编码坐标（即住宿在城市中的位置）；卧室数量，这是住宿大小的代理；以及床位数，这有助于与前面的图一起区分类似旅舍的列表，这些列表通常价格较低。所有其他特征的作用较小，这可以从图表的规模中看出：前
    10 个最重要特征中最后一个的重要性是顶级重要特征的五分之一。
- en: '![](../Images/CH07_F16_Ryan2.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F16_Ryan2.png)'
- en: Figure 7.16 SHAP importance
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 SHAP 重要性
- en: 'Importance, however, tells just a part of the story. We also need directionality.
    Hence, the violin chart can provide even more information on the model’s behavior.
    In a violin plot produced by the `shap` package, you can get hints from these
    details:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要性只是故事的一部分。我们还需要方向性。因此，小提琴图可以提供关于模型行为的更多信息。在由 `shap` 包生成的小提琴图中，您可以从以下细节中获得提示：
- en: '*Feature importance*—The width of the violin plot indicates the density of
    SHAP values. Wider sections represent more instances with similar SHAP values
    for that feature. Thus, features with broader violin plots are generally more
    important in the model’s predictions.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征重要性*——小提琴图的宽度表示 SHAP 值的密度。较宽的部分代表具有相似 SHAP 值的实例更多。因此，具有更宽小提琴图的特征在模型的预测中通常更重要。'
- en: '*Shape of the violin*—The violin’s shape indicates the distribution of SHAP
    values for the corresponding feature. If the violin is symmetric, it suggests
    that SHAP values are evenly distributed around the median, signifying a balanced
    effect on predictions. Asymmetry indicates skewness and suggests that certain
    feature values have more significant effects than others.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*小提琴形状*——小提琴的形状表示对应特征的 SHAP 值分布。如果小提琴是对称的，则表明 SHAP 值均匀分布在均值周围，这意味着对预测的影响是平衡的。不对称表示偏斜，并表明某些特征值对预测的影响比其他特征值更大。'
- en: '*Positive and negative contributions*—The violin plot’s center line (median)
    is usually zero. The left and right halves of the violin represent their respective
    contributions for features with positive and negative SHAP values. Positive SHAP
    values push predictions higher, while negative SHAP values push them lower.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正负贡献*——小提琴图的中心线（均值）通常是零。小提琴的左右两侧分别代表具有正 SHAP 值和负 SHAP 值的特征的贡献。正 SHAP 值将预测推向更高，而负
    SHAP 值将预测推向更低。'
- en: '*Association with the feature value*—The color of the violin plot can help
    you associate blue areas, where the feature has lower values, and red areas, where
    the feature has higher values, with specific SHAP contributions. This helps in
    understanding how the feature is generally related to the outcome.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与特征值的相关性*—小提琴图的颜色可以帮助您将蓝色区域（特征值较低）和红色区域（特征值较高）与特定的SHAP贡献关联起来。这有助于理解特征通常如何与结果相关。'
- en: '*Outliers*—Outliers or extreme SHAP values outside the range of the violin
    plot suggest instances where the corresponding feature has an unusually strong
    effect on the prediction.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常值*—异常值或超出小提琴图范围的外部SHAP值表明，相应的特征对预测有异常强烈的影响。'
- en: In the following listing, the violin plot provides useful insights into the
    distribution and the role of each feature on the model’s predictions.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，小提琴图提供了关于分布和每个特征在模型预测中作用的宝贵见解。
- en: Listing 7.21 SHAP violin plot
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.21 SHAP小提琴图
- en: '[PRE73]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: ① Creates a SHAP summary plot using violin plots to visualize the distribution
    of SHAP values for each feature
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用小提琴图创建SHAP摘要图，以可视化每个特征的SHAP值分布
- en: 'Figure 7.17 shows the resulting violin plot. As for our top important features,
    we can figure out the following:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17显示了结果的小提琴图。至于我们最重要的特征，我们可以得出以下结论：
- en: '`Numeric__availability_365`—Higher availability corresponds to a positive effect
    on price. Listings with lower availability are usually penalized.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Numeric__availability_365`—更高的可用性对应着对价格的正影响。可用性较低的列表通常会受到惩罚。'
- en: '`Target_encoding__coordinates`—It is difficult to interpret since its values
    are unrelated to a specific directionality. We can observe that there are long
    tails on both sides with a prevalence of a negative contribution to the pricing
    of the accommodation.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Target_encoding__coordinates`—由于其值与特定方向性无关，难以解释。我们可以观察到两侧都有长尾，且对住宿定价的负贡献更为普遍。'
- en: '`Numeric__number_of_bedrooms`—A higher number of bedrooms implies a higher
    price, with a long skewed tail to the right.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Numeric__number_of_bedrooms`—卧室数量越多，价格越高，右侧有一个长的偏斜尾。'
- en: '`Numeric__number_of_beds`—Similarly, a higher number of beds implies a higher
    price, with a long skewed tail to the right.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Numeric__number_of_beds`—同样，床位数越多，价格越高，右侧有一个长的偏斜尾。'
- en: '![](../Images/CH07_F17_Ryan2.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F17_Ryan2.png)'
- en: Figure 7.17 SHAP violin plot
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 SHAP小提琴图
- en: A glance at other features provides an idea of how the model behaves intuitively.
    For instance, the nearer the accommodation is to the Imperial Palace or the airports,
    the higher the price.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 简单看一下其他特征，可以了解模型直观上的行为。例如，住宿地点离故宫或机场越近，价格就越高。
- en: This concludes our end-to-end example using gradient boosting. In the next chapter,
    we are going to get back to the Airbnb NYC problem and will review a set of deep
    learning stacks (low-level framework, high-level API, and deep learning for tabular
    data library) and use three of these stacks (fastai, PyTorch with TabNet, and
    Lightning Flash) to solve it and compare the different solutions.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们使用梯度提升的端到端示例。在下一章中，我们将回到Airbnb纽约问题，并回顾一系列深度学习堆栈（低级框架、高级API和表格数据深度学习库），并使用这三个堆栈（fastai、PyTorch与TabNet、Lightning
    Flash）来解决问题，并比较不同的解决方案。
- en: 'In this phase, a generative AI tool such as ChatGPT could be useful for creating
    narratives explaining the SHAP values assigned to each example. Being able to
    create an easy explanation for each prediction could prove to be a strong point
    when demonstrating the potentialities of your model or trying to persuade clients
    and stakeholders. In addition, the need for a narration explaining the model’s
    predictions of the dataset is crucial under regulations such as those in the European
    Union. Transparency and interpretability are essential components of regulations
    striving at data protection and preserving privacy, such as the General Data Protection
    Regulation in the EU. According to these regulations, individuals have the right
    to make sense of the logic behind automated decision-making processes that significantly
    affect their lives. Providing a clear and comprehensible explanation for why a
    specific prediction has been made ensures transparency and accountability and
    promotes fairness: it gives individuals the power to seek clarification, challenge
    unfair decisions, and ultimately safeguard their rights.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，一个生成式AI工具，如ChatGPT，可以用来创建解释每个示例分配的SHAP值的叙述。能够为每个预测创建简单的解释，在展示模型潜力或试图说服客户和利益相关者时可能是一个优势。此外，在欧盟等法规下，解释模型对数据集预测的需求至关重要。透明度和可解释性是旨在数据保护和保护隐私的法规的必要组成部分，例如欧盟的通用数据保护条例。根据这些法规，个人有权理解对其生活产生重大影响的自动化决策过程背后的逻辑。为特定预测提供清晰易懂的解释确保了透明度和问责制，并促进了公平：它赋予个人寻求澄清、挑战不公平决定并最终保护其权利的能力。
- en: 'You can actually generate each of these narratives by single prompts to create
    explanations on the fly or by using the ChatGPT API and have the model process
    batches of explanations that you can later recall when questioned about the reason
    for a specific prediction. The recipe is, however, the same in both on-the-fly
    or batch processing approaches: you have to tell the LLM to explain by providing
    the list of the features (detailed with their description or meaning, if necessary),
    the original value in the dataset, and the SHAP value relative to the feature.
    Of course, it is necessary to mention the resulting prediction. Gluing together
    all such information for the LLM to process using JSON (a dictionary of dictionaries)
    could be ideal. In listing 7.22, we offer a solution for preparing a JSON structure
    to facilitate the prompt request to ChatGPT to explain a specific example in the
    dataset, identified by its row index. The code generates a data structure that
    encompasses all the information required to construct a coherent narrative explanation.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您可以通过单个提示生成这些叙述，以实时创建解释，或者使用ChatGPT API，让模型处理您稍后可以回忆的关于特定预测原因的解释批次。然而，无论是实时处理还是批量处理方法，配方都是相同的：您必须告诉LLM通过提供特征列表（如有必要，详细说明其描述或含义）以及数据集中的原始值和相对于特征的SHAP值来解释。当然，有必要提及最终的预测。将所有这些信息粘合在一起，以便LLM使用JSON（字典的字典）进行处理可能是理想的。在列表7.22中，我们提供了一个准备JSON结构的解决方案，以方便向ChatGPT提出提示请求，解释数据集中的特定示例，该示例由其行索引标识。该代码生成一个包含构建连贯叙述解释所需所有信息的数据结构。
- en: Listing 7.22 Building a JSON of SHAP explanations as part of a prompt
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.22 将SHAP解释作为提示的一部分构建JSON
- en: '[PRE74]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: ① Instantiates the JSON data structure as a Python dictionary
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将JSON数据结构实例化为Python字典
- en: ② Includes the predicted value to explain in the JSON
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: ② 包含要解释的预测值到JSON中
- en: ③ Iterates over the features, original value, and SHAP values of the examined
    row
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 遍历检查行的特征、原始值和SHAP值
- en: ④ Index of the prediction to explain
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 要解释的预测的索引
- en: In our example, we require a description of why the model predicted a particular
    value for row 5 of the dataset. The printed JSON can then be enclosed in a prompt
    such as
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们需要描述为什么模型预测了数据集第5行的特定值。然后可以将打印的JSON包含在如下提示中
- en: '[PRE75]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'When you submit this prompt to ChatGPT, you will receive a text organized in
    bullet points categorized by types of variables. This text describes the influence
    of each individual variable or group of variables on the outcome. The following
    is an excerpt of the insights we derived for the specific instance represented
    by row 5:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将此提示提交给ChatGPT时，您将收到一个按变量类型分类的以项目符号组织的文本。此文本描述了每个单独的变量或变量组对结果的影响。以下是针对第5行表示的具体实例所获得的见解摘录：
- en: '*Room type*—The “Entire home/apt” room type has a positive effect on the predicted
    price, contributing a SHAP value of 0.034\. This suggests that listings with the
    entire home/apartment as the room type tend to have higher prices.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*房间类型*—“整个家庭/公寓”的房间类型对预测价格有积极影响，贡献了0.034的SHAP值。这表明，将整个家庭/公寓作为房间类型的房源往往价格更高。'
- en: '*The other room types* (“Hotel room,” “Private room,” and “Shared room”)—These
    have smaller positive or negligible contributions, indicating that their effect
    on the price is not as significant.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*其他房间类型*（“酒店房间”、“私人房间”和“共享房间”）—它们有较小的正贡献或可忽略不计的贡献，表明它们对价格的影响并不那么显著。'
- en: '*Neighborhood*—The feature `neighbourhood_more_than_30` has a positive SHAP
    value of 0.083, suggesting that being in a neighborhood with more than 30 listings
    positively influences the price.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社区*—特征`neighbourhood_more_than_30`具有0.083的正SHAP值，表明位于拥有超过30个房源的社区对价格有积极影响。'
- en: '*Type of accommodation*—The `type_of_accommodation` feature has a small negative
    effect with a SHAP value of –0.008\. This implies that certain types of accommodation
    might have a slightly lower price.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*住宿类型*—`type_of_accommodation`特征具有-0.008的小负面影响，这表明某些类型的住宿可能价格略低。'
- en: The complete text actually touches on all the features, and you can prompt the
    LLM to reduce the results to only the top 5 or 10 impactful features, if you prefer.
    Certainly, using a language model for this job makes a difficult task simple and
    automates it in a breeze.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的文本实际上涉及了所有特征，如果你愿意，你可以提示LLM将结果缩减到只有前5个或10个最具影响力的特征。当然，使用语言模型来完成这项工作可以使一个困难的任务变得简单，并且轻松自动化。
- en: Summary
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Getting and preparing your data requires downloading, restructuring, and assembling
    it all together. It is often a long and laborious part of the work in an end-to-end
    project, but it is an indispensable one, building the foundations for the success
    of your following work.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取和准备数据需要下载、重构并将所有内容组装在一起。这通常是端到端项目中漫长而繁重的工作部分，但它是不可或缺的，为后续工作的成功奠定基础。
- en: Feature engineering is not just magic or randomly combining features; most often,
    it is embedding prior knowledge about a problem and how to solve it in the features
    you will be using to train your model. Exploring the set of domain knowledge related
    to a problem is the first step to model your data effectively.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程不仅仅是魔法或随机组合特征；大多数情况下，它是在特征中嵌入关于问题及其解决方法的知识。探索与问题相关的领域知识集合是有效地建模数据的第一步。
- en: Exploring your predictions and target in the EDA phase is an essential part
    of your schedule for modeling a tabular problem. Look for outliers and extreme
    values, missing data, and any other peculiarity from the data. Feel free to drop
    examples if you are unsure they can provide real value to your model.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据探索性分析（EDA）阶段探索你的预测和目标是你建模表格问题的日程安排中的一个重要部分。寻找异常值和极端值、缺失数据以及数据中的任何其他特殊性。如果你不确定它们是否能为你的模型提供真正的价值，请随意删除示例。
- en: Before delving into modeling, check for your validation strategy, which may
    require extra work, EDA, and your data pipeline. Both can make a difference in
    the modeling phase. Ideally, prepare a pipeline for each type of model you want
    to test because each model has different ways of dealing with the various types
    of data you find in tabular datasets. In our example, as a simplification, we
    tried a one-size-fits-all approach. Please remember that such examples work well
    in books, but there are better ways to do it in real-world projects.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深入建模之前，检查你的验证策略，这可能需要额外的工作，数据探索性分析（EDA）以及你的数据处理流程。这两者都可能对建模阶段产生影响。理想情况下，为你要测试的每种类型的模型准备一个管道，因为每种模型处理表格数据集中的各种类型数据的方式都不同。在我们的例子中，作为一种简化，我们尝试了一种一刀切的方法。请记住，这样的例子在书中效果很好，但在现实世界的项目中还有更好的方法。
- en: Building a baseline model is an often-neglected phase in modeling tabular data
    problems. Still, it can provide valuable insights by inspecting how the model
    underfits or overfits the data and its internal coefficients. A baseline model
    should necessarily be simple, meaning that linear and logistic regression are
    the best candidates for regression and classification problems.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表格数据问题建模中，建立基线模型是一个经常被忽视的阶段。然而，通过检查模型如何欠拟合或过拟合数据及其内部系数，它可以提供有价值的见解。基线模型必然是简单的，这意味着线性回归和逻辑回归是回归和分类问题的最佳候选者。
- en: After you get insights from your baseline model, you can proceed to more complex
    models such as XGBoost. Cues about underfitting, nonlinearities, interactions,
    targets, and the predictors’ characteristics should be considered when setting
    up the first tentative values for key hyperparameters.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在从基线模型中获得洞察之后，您可以继续使用更复杂的模型，例如XGBoost。在设置关键超参数的第一个尝试值时，应考虑欠拟合、非线性、交互作用、目标以及预测者的特征等提示。
- en: Optimizing your model using Optuna can save you a lot of time if you set your
    search space to incorporate insights and hypotheses you have developed so far
    regarding how your model should handle the data and the problem. Once the optimization
    has been completed, further insights can be gained from observing hyperparameter
    importance and optimization path charts.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您将搜索空间设置为包含您迄今为止关于模型如何处理数据和问题的洞察和假设，那么使用Optuna优化您的模型可以为您节省大量时间。一旦优化完成，您可以通过观察超参数重要性和优化路径图表来获得更多洞察。
- en: Explaining your trained model can be easily done with XGBoost and LightGBM using
    the predict method with the parameter `pred_contribs` set to true. Once the SHAP
    values, which are effectively multipliers with respect to the prediction, are
    obtained, you can use standard charts from the `shap` package, such as the importance
    plot or the violin plot.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost和LightGBM，通过将`pred_contribs`参数设置为true的predict方法，可以轻松解释您的训练模型。一旦获得了与预测有效相关的乘数SHAP值，您可以使用`shap`包中的标准图表，例如重要性图或小提琴图。
