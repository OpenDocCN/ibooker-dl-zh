- en: 7 An end-to-end example using XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Gathering and preparing data from the internet, using generative AI to help
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drafting a baseline and first tentative model to be optimized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figuring out how the model works and inspecting it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter concludes our overview of classical machine learning for tabular
    data. To wrap things up, we’ll work through a complete example from the field
    of data journalism. Along the way, we’ll summarize all the concepts and techniques
    we’ve used so far. We will also use a generative AI tool, ChatGPT, to help you
    get the job done and demonstrate a few use cases where having a large language
    model (LLM) can improve your work with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: We will finally build a model to predict prices, this time using a regression-based
    approach. Doing this will help us understand how the model works and why it performs
    in a particular manner to gain further insights into the pricing dynamics for
    Airbnb listings and challenge our initial hypothesis regarding how pricing happens
    for short-term rentals.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Preparing and exploring your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, we’ll focus on a different dataset as we continue our analysis
    of short-term and long-term Airbnb rental listings in New York City. This dataset
    comes directly from the Inside Airbnb Network initiative ([http://insideairbnb.com/](http://insideairbnb.com/)),
    “a mission-driven project that provides data and advocacy about Airbnb’s effect
    on residential communities.” We will also be using public data from other online
    services, such as Foursquare ([https://foursquare.com](https://foursquare.com)),
    the social network and geolocation technology company.
  prefs: []
  type: TYPE_NORMAL
- en: Following the data acquisition phase, we will organize and conduct comprehensive
    feature engineering based on relevant business hypotheses to extract valuable
    insights for our modeling stage. During this process, we will also perform basic
    exploratory analysis on our predictors and target variables, making necessary
    adjustments or exclusions of examples and features to ensure we get the optimal
    data for our project.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Using generative AI to help prepare data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ChatGPT is an advanced language model developed by OpenAI. To create and train
    a generative pretrained transformer (GPT) model like ChatGPT, OpenAI applied vast
    amounts of diverse internet text to help the model learn to understand and generate
    human-like text by predicting the next word in a sequence of words based on its
    contextual understanding. This pretraining allows ChatGPT to capture grammar,
    context, and even nuanced information, but it is not enough to make it a helpful
    assistant in every circumstance. In fact, these models have the potential to produce
    outputs that are inaccurate or harmful or contain toxic content. The reason for
    this is that the training dataset—that is, the internet—contains text that is
    varied and, at times, unreliable. To enhance the safety, utility, and alignment
    of ChatGPT models, a method known as reinforcement learning from human feedback
    is employed. In the reinforcement learning from human feedback process, human
    labelers provide feedback illustrating the preferred model behavior, and they
    evaluate multiple outputs produced by the model through ranking. This data is
    subsequently utilized to fine-tune GPT-3.5 further, refining its responses based
    on the human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: To use the free version of ChatGPT (at the moment, that is ChatGPT 3.5, which
    is updated with information up to January 2022), you must first create an account
    at [https://chat.openai.com](https://chat.openai.com). Once you have an account,
    you can start using ChatGPT by simply entering a prompt. A prompt, in the context
    of an LLM like ChatGPT, is a written instruction or input provided to the model
    to generate a specific output. It serves as a query or request that guides the
    model in producing a relevant response. Prompts can vary in complexity, ranging
    from simple commands to more detailed descriptions or inquiries, and they play
    a crucial role in shaping the nature of the language model’s output. The prompt’s
    quality and clarity significantly influence the generated content’s accuracy and
    relevance, but selecting the right prompt is not always straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of different prompts can vary depending on the specific language
    model they are designed for. Each language model has its own strengths, weaknesses,
    and nuances, making it essential to tailor prompts accordingly. When working with
    ChatGPT, for instance, we obtained the best results starting with simple prompts,
    evaluating their results, and then proceeding by adding more specifications to
    the prompt to refine the results toward our expectations. ChatGPT tend to work
    better when you tell it to “write,” “create,” “show me how to,” or “summarize.”
    Sometimes showing examples and how you expect ChatGPT to elaborate them is quite
    helpful. Also, let ChatGPT know your expectations in terms of the answer, like
    how long the response should be, what information it should include, if you want
    just code as a result or just text, and how the answer should be structured in
    return; for instance, you could ask using the JSON format or a Python style list.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs such as ChatGPT, and the related Copilot feature in GitHub, have proven
    to be useful assistants for a variety of programming tasks. This usefulness applies
    to tabular data applications. You can ask these models various questions or request
    assistance in coding tasks, and they can assist by providing code snippets, explanations
    about how code works, or guidance in using specific commands or algorithms. However,
    although LLMs such as ChatGPT can assist users by generating code snippets for
    data manipulation, cleaning, and transformation tasks, as well as provide explanations
    and guidance on various statistical and machine learning techniques applicable
    to tabular datasets, our intention in this chapter is to show a few select and
    less obvious LLM capabilities that you can use for your tabular data analysis
    and modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Getting and preparing your data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a starting point, we will navigate the Inside Airbnb Network website ([http://insideairbnb.com/](http://insideairbnb.com/))
    and find the data we need. Our goal is to explore the situation in a completely
    different city: Tokyo. First, you have to manually download the data and store
    it in a working directory on your computer or cloud instance. To do so, on the
    home page of the Inside Airbnb Network initiative, as shown in figure 7.1, in
    the Data menu, choose Get the Data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Choosing from the Data menu
  prefs: []
  type: TYPE_NORMAL
- en: Once you choose the menu item, you will be moved to a new page containing Data
    Downloads, presenting various cities and their data file to be downloaded. Scroll
    the page until you find the city of Tokyo.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 shows the portion of the page containing the data files for Tokyo
    as they were at the time of writing this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 The Tokyo Data Downloads section
  prefs: []
  type: TYPE_NORMAL
- en: 'For our analysis, we need two files from the page: `listings.csv`, which contains
    the summary listings and other information about the Airbnb accommodations in
    Tokyo, and `calendar.csv.gz`, a zipped file containing `calendar.csv`, a dataset
    containing occupancy and price information for a given year for each listing.
    Hover over the links, right-click, and select to save them to disk in your working
    directory. For example, in Google Chrome, you need to select “Save link as,” and
    in Mozilla Firefox, you have to select “Save target as.” At this point, you will
    just need to extract the files into your working directory. Once the files we
    need are unzipped in our local directory, we can ingest them into a pandas DataFrame
    using the `read_csv` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With the list and type of columns, we can get an idea of the kind of data we
    will be dealing with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The list comprises 18 columns, largely the same as the Airbnb NYC dataset introduced
    in chapter 3\. Here we describe each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`—A unique identifier for each listing on Airbnb. It is an `int64` data
    type, meaning it is a numerical ID representation. In other tables, it can be
    referred to as `listing_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`—The description of the Airbnb listing. It is of the `object d`ata type,
    which typically represents a string or text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host_id`—A unique identifier for each host on Airbnb. It is an `int64` data
    type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host_name`—The name of the host who owns the listing. It is of the `object`
    data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighbourhood_group`—Represents the broader area or region the neighborhood
    belongs to. It is stored as a `float64` data type, but it is important to note
    that using a float data type to represent groups or categories is uncommon. In
    this case, the presence of float values indicates that the data for this field
    is entirely made up of missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighbourhood`—The specific neighborhood where the listing is located. It
    is of the `object` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latitude`—The latitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`longitude`—The longitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`room_type`—The type of room or accommodation offered in the listing (e.g.,
    entire home/apartment, private room, shared room). It is of the `object` data
    type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`price`—The price per night to rent the listing. It is of the `int64` data
    type, representing an integer price value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minimum_nights`—The minimum number of nights that is required for booking
    the listing. It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_reviews`—The total number of reviews received by the listing. It
    is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_review`—The date of the last review received by the listing. It is of
    the `object` data type, which could represent date and time information, but it
    might require further parsing to be used effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reviews_per_month`—The average number of reviews per month for the listing.
    It is of the `float64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`calculated_host_listings_count`—The total number of listings the host has
    on Airbnb. It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`availability_365`—The number of days the listing is available for booking
    in a year (out of 365 days). It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_reviews_ltm`—The number of reviews received in the last 12 months.
    It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`license`—The license number or information related to the listing. It is of
    the `object` data type, which typically represents a string or text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can safely ignore features such as `host_id`, `host_name`, `neighbourhood_group`
    (because they are completely missing), or `license` (which is a kind of identifier
    based on the host’s license).
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the other features, whereas most of them are numeric, the `name` feature
    is a string containing various pieces of information to be extracted based on
    how the data has been organized. By visualizing a single example from it, we can
    have an idea of its organization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The string is arranged into five distinct parts separated by conventional signs
    and with some kind of partially structured and repeated content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The first portion of the string is a description of the unit type and location.
    The second portion is the average score received from guests. The third portion
    is the number of bedrooms, the fourth portion is the number of beds, and the last
    is the number of bathrooms.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the numeric values, we can also extract some specific information
    related to the kind of accommodation or services offered—for instance, if the
    apartment is a studio, if the bath is shared, and if it is a half-bath (a room
    with a toilet and washbasin but no bath or shower). We can deal with such information
    by creating simple string correspondence checks and obtaining a binary feature
    pointing out the presence or absence of the characteristic or using regex commands.
    Regex (short for regular expressions) commands are a sequence of characters constituting
    a search pattern. They are used for pattern matching within strings. Table 7.1
    shows the transformations we apply to the description field and highlights what
    strings we strive to match, what regex command we use, and what resulting feature
    we obtain.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1 Regex commands for feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: '| Matched description | Regex | Resulting feature |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Text that starts with “in,” followed by a space, then captures any characters
    until another space, and ends with a dot | `r''in\s(.*?)\s·''`  | Area of Tokyo
    of the listing |'
  prefs: []
  type: TYPE_TB
- en: '| Text that starts with the character “★” (a star), followed by one or more
    digits, a dot, and one or more additional digits (e.g., ★4.5) | `r''`★`(\d+\.\d+)''` 
    | Star ratings |'
  prefs: []
  type: TYPE_TB
- en: '| Text containing a numerical value followed by zero or more whitespace characters
    and the words “bedroom” or “bedrooms” (with or without the “s” at the end) | `r''(\d+)\s*(?:bedroom&#124;bedrooms)''` 
    | Number of bedrooms |'
  prefs: []
  type: TYPE_TB
- en: '| Text containing a numerical value followed by one or more whitespace characters
    and the word “bed” or “beds” as a whole word (with or without the “s” at the end)
    | `r''(\d+)\s+(?:beds?\b)''`  | Number of beds |'
  prefs: []
  type: TYPE_TB
- en: '| Text containing a numerical value representing the number of baths | `r''(?P<baths>\d+)\s*(shared\s+)?(?:half-)?baths?\b''` 
    | Number of baths |'
  prefs: []
  type: TYPE_TB
- en: 'Working with regex commands is a bit complicated. Hence this is the first application
    where generative AI could help. Most LLMs, such as ChatGPT, have a good knowledge
    of different programming languages (in particular, most of them are quite strong
    in Python) because they have been trained on text and information extracted from
    the internet, where there is plenty of information about how to code even very
    specific problems. In our case, showing an example of the strings in the prompt
    and asking for the desired information to be extracted should do the trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output should be something already suitable for usage, arranged in blocks
    of code snippets with some explanation about the extraction rule, such as shown
    in figure 7.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 Results from a prompt on regex processing on ChatGPT 3.5
  prefs: []
  type: TYPE_NORMAL
- en: Without being asked, the language model should simply decide to propose a Python-based
    solution, and you just click on the “Copy code” icon on top of the code listing
    to copy the snippet on the clipboard and then paste it into your notebook or IDE
    editor.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the solutions provided may vary from query to query and can differ
    from the solution we provided in the table. This is because LLMs are, in the end,
    probabilistic machines. Temperature is the parameter usually set to influence
    the randomness of the model’s output. It is used during text generation to control
    the generated content’s creativity. In simple terms, temperature affects the likelihood
    of the model choosing the next word in a sequence. Low-temperature values result
    in more deterministic and expected output. In contrast, high-temperature values
    introduce more randomness and creativity in the generated output because the model
    tends to choose less probable words.
  prefs: []
  type: TYPE_NORMAL
- en: After getting an apparently useful answer, one important step prior to using
    the solutions proposed by an LLM is to test them on more examples than the one
    or two shown in the prompt. Such a step may reveal that the code is not working
    well, and you may also need to tell the model that the example doesn’t work or
    signal the problem you are experiencing by more detailed instructions to the model.
    For instance, we found that sometimes the commands didn’t work properly if there
    were upper-case letters in some parts of the input string. Hence, we had to find
    a supplemental solution. All these regex commands operate lowercase, thanks to
    the `re.IGNORECASE` flag, which makes the match operating case insensitive. In
    the following listing, we proceed to extract information from the text descriptions
    using the regex commands we found using ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 Extracting information from text descriptions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Extracts the type of accommodation from a list of options
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts the area of Tokyo mentioned in the listing name
  prefs: []
  type: TYPE_NORMAL
- en: ③ Extracts the rating score from a star symbol followed by a numerical value
  prefs: []
  type: TYPE_NORMAL
- en: ④ Extracts the number of bedrooms from the listing name
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Extracts the number of beds from the listing name
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Extracts the number of baths from the listing name
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 completes the feature extraction work by creating additional Boolean
    columns based on specific keywords in the name column. It computes two calculated
    features based on a difference telling us the number of days between today’s date
    and the `last_review` date and a ratio expressing how the number of reviews in
    the last year relates to the total number of reviews. Such a ratio can reveal
    if the bulk of reviews is recent or if the listing has been successful mainly
    in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 Extracting binary flags and time information
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Checks if the word “new” is present in the name (case-insensitive)
  prefs: []
  type: TYPE_NORMAL
- en: ② Checks if the word “studio” is present in the name (case-insensitive)
  prefs: []
  type: TYPE_NORMAL
- en: ③ Checks if the word “shared” is present in the name (case-insensitive)
  prefs: []
  type: TYPE_NORMAL
- en: ④ Checks if the word “half” is present in the name (case-insensitive)
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculates the number of days between today’s date and the last_review date
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Calculates the ratio of number_of_reviews_ltm to number_of_reviews for each
    listing
  prefs: []
  type: TYPE_NORMAL
- en: The `summary_listings` data also has a price feature that we could use as a
    target. Still, we prefer to create it by aggregating the `calendar.csv` data to
    decide whether to pick an average of all the prices, the minimum, or the maximum.
    The `calendar.csv` contains information for each day about the availability of
    the accommodation, its price (also adjusted for discounts), and the minimum and
    maximum number of nights allowed for booking at that time. We are interested in
    processing the adjusted price as a target, representing the effective market price
    of the accommodation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Creating the target from daily listings
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Removes the dollar sign ($) and commas (,) from the values and then converts
    them to float
  prefs: []
  type: TYPE_NORMAL
- en: ② A group by operation on the calendar DataFrame based on the listing_id column
  prefs: []
  type: TYPE_NORMAL
- en: '③ Calculates three statistics for the adjusted_price column: the mean, minimum,
    and maximum values'
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing the aggregation, we can check the result by requiring the
    first five rows of this newly created dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Figure 7.4 verifies how we now have both a mean price per listing available
    together with the maximum and minimum prices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Price statistics for Airbnb listings
  prefs: []
  type: TYPE_NORMAL
- en: We will save this `price_stats` DataFrame and concentrate in the next section
    on improving the number and effectiveness of our features.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Engineering more complex features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Real estate assets have a pretty peculiar behavior, distinguishable from other
    products or services you find on the market. An adage in the real estate business
    mentions that all that matters when dealing with buildings and facilities is “location,
    location, location.” The position of an apartment in a city or a road can make
    a difference to the value of a property. We will adopt this adage for Airbnb listings
    and develop some feature engineering based on location.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we will reprise the example from the previous chapter, where
    we created small geographic subdivisions that we later target encoded. By this
    approach, you should be able to capture the specific characteristics of an area,
    though it will be difficult to explain why renting in one particular location
    costs more than in others. We should prepare more specific features to provide
    some explainability to listings in Tokyo. Here is another point where generative
    AI can come to the aid of the tabular data practitioner by providing help in the
    form of suggestions and idea generation. LLMs have sifted through more data than
    you can imagine, and if queried with enough details (and some role-playing, i.e.,
    asking them to personify an expert proficient in a specific field), they can return
    hints and reflections that could have cost you multiple hours of research and
    readings on the web.
  prefs: []
  type: TYPE_NORMAL
- en: Our prompt for ChatGPT is
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-MR.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an expert data scientist and you have downloaded a dataset containing
    summarized Airbnb listings. This is the structure of the dataset: id (int64),
    name (object), neighbourhood (object), latitude (float64), longitude (float64),
    room_type (object), price (int64), minimum_nights (int64), number_of_reviews (int64),
    last_review (object), reviews_per_month (float64), calculated_host_listings_count
    (int64), availability_365 (int64), number_of_reviews_ltm (int64). You are training
    a machine learning model to predict the price of listings. Which features should
    you engineer to improve the performance of the model? |'
  prefs: []
  type: TYPE_TB
- en: Our strategy is to set a persona (“you are an expert data scientist”) and to
    provide some further information about the features available (removing the features
    that we actually already decided not to use) and the target variable. Here we
    also used the fact that we expected ChatGPT to already know something about the
    dataset we were using (“a dataset containing summarized Airbnb listings”), but
    you can also propose less known problems to the LLM by briefly describing the
    dataset, including the types of features and their relationships to each other.
    In addition, if you have any domain expertise relevant to the dataset, for instance,
    any hypotheses about how the features in the dataset might be related to the target
    variable, share them with the LLM. This information can help it identify features
    that may be important for the task. Moreover, providing a list of the existing
    features in the dataset, as we did, may not be enough for more challenging tasks.
    Providing an explanation or description of each variable and clearly stating the
    objective of your machine learning model, such as predicting customer churn, estimating
    real estate values, or forecasting sales, will help the LLM identify gaps or opportunities
    for creating new well-engineered features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the suggested features, or feature types, to strive to generate
    to obtain the best results in our dataset and type of problem, as stated by ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Distance from key locations*—Calculate the distance of each listing from key
    locations or landmarks in the city. This can be done using the latitude and longitude
    information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neighborhood statistics*—Aggregate statistics for each neighborhood, such
    as the average price, minimum and maximum prices, and the number of listings in
    each neighborhood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Room type encoding*—Convert the categorical variable `room_type` into numerical
    values using one-hot encoding or label encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Temporal features*—Extract relevant features from the `last_review` column,
    such as the month or season of the last review or the time since the last review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Host experience*—Combine information from `calculated_host_listings_count`
    and `number_of_reviews_ltm` to create a feature representing the host’s experience
    or popularity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Booking density*—Calculate the ratio of `number_of_reviews` to `availability_365`
    to represent how frequently a listing is booked relative to its availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Review sentiment*—If you have access to the actual review texts, you can use
    natural language processing techniques to analyze sentiment and extract sentiment-based
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interaction terms*—Create interaction terms between relevant features, such
    as the product of `minimum_nights` and `availability_365` to capture the overall
    availability for longer stays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Price binning*—Convert the continuous `price` variable into bins or categories,
    creating a new categorical feature representing different price ranges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Geographical clustering*—Use clustering algorithms to group listings based
    on geographical proximity. This can capture spatial patterns in pricing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of the proposed points, a few do resonate with our experience, such as computing
    distance from key locations and neighborhood statistics or creating geographical
    clustering. Also, we already provided room-type encoding with our previous work
    on text fields and regex commands. Leaving aside review sentiment and price binning
    (price is actually our target), what is left is a set of ideas that can guide
    or inspire you to improve your work. Use the outputs of LLMs to your advantage,
    but don’t forget that your previous knowledge, experience, sensibility, and critical
    thinking can play an even more important role in the task of a tabular data analysis
    and that LLMs are just facilitators in your work.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping in mind the proposed features suggested by ChatGPT, we proceed to create
    some of them. Regarding geographical clustering, you can find all you need in
    listing 7.4 to create a high cardinality geographical feature from coordinates.
    Later, in the data pipeline, we will target encode the high cardinality categorical
    feature we are going to generate with the code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.4 Creating a high cardinality geographical feature
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Discretizes the latitude and longitude by bin size
  prefs: []
  type: TYPE_NORMAL
- en: ② Composes the new coordinates feature by summing the discretized latitude and
    longitude
  prefs: []
  type: TYPE_NORMAL
- en: This code generates a feature with 317 unique values. We covered all the Tokyo
    municipalities in a 32 × 32 grid, which means potentially 1,024 values. Only 317
    of these coordinates contain a listing, meaning that in the future, our model
    can effectively predict based on this feature only if a new listing falls in one
    of the 317 slots previously defined. In case a new area appears, thanks to the
    target encoder novelty dealing capabilities (see the description of the `handle_unknown`
    parameter at [https://mng.bz/oK1p](https://mng.bz/oK1p)), we can simply impute
    unknown values to the target means using the setting `handle_unknown`=“value”.
  prefs: []
  type: TYPE_NORMAL
- en: 'A critical aspect of the Tokyo real estate market is that there is an important
    geographical center for cultural and historical reasons, which is the Imperial
    Palace. Locations near this site tend to have higher real estate valuations, and
    some of Japan’s most expensive flats are located close to the Imperial Palace.
    We try to reflect this reality by creating a feature comparing the location of
    our Airbnb accommodation with the area of the Imperial Palace (which can be taken
    from sites such as latlong.net: [https://mng.bz/nRW2](https://mng.bz/nRW2)). For
    distances, we convert the value into meters using a formula that involves the
    cosine of the radiants multiplied by a conversion factor for the measure to be
    intelligible to a human examination. We also adopt the Manhattan distance for
    better-representing distances in a city, which is the summed difference in absolute
    values between the latitudes and longitudes.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 Computing a distance metric from the city center
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Conversion factor representing the approximate number of meters per degree
    of latitude
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates the distance in meters by multiplying the degree-based distance
    by the conversion factor and adjusting for the latitude’s cosine
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the absolute distance in degrees by subtracting the Imperial Palace’s
    latitude and longitude from the values in the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with coordinates inside a city, the choice between Euclidean distance
    and Manhattan distance as a feature for machine learning depends on the specific
    context and the problem you are trying to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Euclidean distance* is based on the straight-line distance between two points
    in an Euclidean space. It assumes a direct path between the points and can be
    more suitable when considering physical distances. You may also hear it referred
    to as the *L2 norm*, which instead is a mathematical concept referring to the
    distance between the vector and the origin of the vector space. Since the L2 norm
    is based on the Euclidean distance formula, it is used interchangeably because
    it is a closely related mathematical concept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Manhattan distance*, also known as city block distance or taxicab distance,
    measures the distance between two points by adding up the absolute differences
    between their coordinates. It considers only horizontal and vertical movements
    and disregards diagonal paths. Similarly to the Euclidean distance, you may hear
    the Manhattan distance referred to as the *L1 norm* when operating with vectors
    and vector spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manhattan distance can be more appropriate when considering the actual movement
    or navigation within a city, where travel often occurs along streets and road
    networks. Considering that coordinates are inside a city, where the road network
    structure and navigation along streets matter, Manhattan distance might be more
    suitable for capturing the movement and accessibility between locations. It aligns
    with the concept of following the roads and making right-angle turns.
  prefs: []
  type: TYPE_NORMAL
- en: 'After calculating the `imperial_palace_distance` feature, we can examine its
    average value, expressed in meters, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the average distance to the Imperial Palace is around
    7.9 kilometers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can identify the listing that is located nearest to the Imperial Palace.
    To achieve this, we can use the `idxmin()` function to find the index of the listing
    with the minimum distance and then access its corresponding details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows, which is a bit surprising:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, the listing is not situated close to the Imperial Palace, which emphasizes
    the presence of potentially misleading errors in the geolocation of listings.
    It is not uncommon to encounter similar problems in datasets, no matter how carefully
    they are curated. As discussed in chapter 2, from a general point of view, after
    some data quality checks where you look for consistency among features and marking
    likely inexact values, you have a few viable options, listed here in descending
    order of effort from more demanding to less demanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Geocoding* (from address to coordinate) and *reverse geocoding* (from coordinates
    to address)—To figure out if location information matches with the provided latitude
    and longitude coordinates and decide whether to trust the provided address or
    coordinates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data imputation*—Imputing the dubious values as they were missing, using the
    coordinates of a default location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Listwise deletion*—Removing all rows that have some dubious values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Leave it to XGBoost*—Tree-based methods tend to be less affected by erroneous
    and dubious values being robust to outliers and noise in the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, we decided to leave the situation to XGBoost because our model
    is not so critical as to require a thorough data quality check. It could be different
    with your own project, and you may evaluate a solution requiring more data-cleaning
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distances from landmarks and services work well as features in real estate
    modeling. Hence we do not limit ourselves to computing the distance from the Imperial
    Palace, the center of Tokyo. In the paper “Modeling User Activity Preference by
    Leveraging User Spatial Temporal Characteristics in LBSNs” by Dingqi Yang, Daqing
    Zhang, Vincent W. Zheng, Zhiyong Yu (*IEEE Transactions. on Systems, Man, and
    Cybernetics: Systems*, 45(1), 129-142, 2015), the authors collected datasets from
    Foursquare check-ins in New York City and Tokyo with their geographical coordinates
    and the type of location they refer to; we can access the data from Kaggle Datasets
    ([https://mng.bz/4aDj](https://mng.bz/4aDj)).'
  prefs: []
  type: TYPE_NORMAL
- en: Foursquare is a social network based on geopositioning. Thanks to its mobile
    app, it allows users to discover nearby venues to visit, such as restaurants and
    shops, and means of transportation and to share information about the places they
    visit. Another characteristic of the app is check-ins, which happen when using
    the platform at a venue. Check-ins refer to the presence of a user at a specific
    location. When a user checks into a place, they share their positioning with their
    Foursquare friends, and they may also have the option to post about their visit
    to other social media platforms like Facebook and X. To map the value of a listing
    in terms of convenience, we have available airports, bus, train, and subway stations
    among commonly checked-in venues. Together with convenience stores, a type of
    retail store that mainly sells a wide selection of everyday items and products
    to customers for their convenience, the proximity of such venues can add value
    to an accommodation.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, to enrich our dataset, we first extracted the GPS coordinates of these
    Tokyo locations directly from Kaggle. The code and the extracted dataset are available
    at [https://mng.bz/QDPv](https://mng.bz/QDPv), and you can download the processed
    file into your working directory from the page [https://mng.bz/XxNa](https://mng.bz/XxNa)
    where you can get the file `relevant_spots_Tokyo.csv`. The file contains information
    about 3,560 convenience store locations, 1,878 bus stations and stops, 439 subway
    stations, and 264 locations associated with airports. Then, using listing 7.6,
    we can compare the location of our Airbnb Tokyo listings with each of these venues
    and report the nearest distance of each one. Our idea is that the closer a listing
    is to a convenience store and means of transportation, the higher the expected
    price.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 7.6 we do not compare each accommodation with all the possible venues
    we have gathered because that would take too much time and computation. Instead,
    we utilize the k-dimensional tree (KDTree) data structure from Scikit-learn, an
    optimized algorithm designed to efficiently find the nearest point among many
    points for a given location. Scikit-learn is used for algorithms such as K-nearest
    neighbors in situations when you have to find the most similar examples to a test
    sample in the training set. In our case, the training set is the set of venues,
    and the algorithm is trained to find the nearest venue to a given location based
    on Manhattan distance.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Finding the nearest facilities and transportation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Stores the minimum distances in a dictionary
  prefs: []
  type: TYPE_NORMAL
- en: ② Filters the relevant venue locations
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a KDTree using the filtered venue locations with the Manhattan metric
    for fast nearest-neighbor searches
  prefs: []
  type: TYPE_NORMAL
- en: ④ Queries the KDTree to find the nearest point and its distance to each Airbnb
    listing (k=1 returns the nearest one)
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The dictionary of the minimum distances for each type of venue is converted
    into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will run quite quickly by iterating over each type of venue, training
    on the locations of selected venues, and finding, using the KDTree, the nearest
    location to each accommodation and automatically calculating the distance. Ultimately,
    we just have to wrap up the results into a pandas DataFrame after converting the
    distances (Manhattan distances in degrees) to meters by the previously seen function
    `degrees_to_meters`. We can verify the results by inspecting the first five rows
    of the resulting dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Figure 7.5 shows the results, representing the contents from the created `min_distances`
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 The first five rows of the `min_distances` DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Having the minimum distance to our selection of venues for each listing of our
    dataset, we now proceed with putting together all these new features into a final
    dataset of predictors and extracting a target series or vector to be used for
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.4 Finalizing your data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After creating some additional features, we can finalize our predictive features
    and their target. In listing 7.7, we join the `summary_listing` dataset to the
    minimum distance to our selected landmarks (airports, subway, train, bus stations,
    convenience stores). Then, we rearrange the joined data with respect to our target:
    the mean price we computed in the `price_stats_ordered` dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Assembling data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① Reindexes X to match the index of price_stats
  prefs: []
  type: TYPE_NORMAL
- en: ② Reindexes price_stats to match the index of X, ensuring the reindexed price
    statistics align with the listings in X
  prefs: []
  type: TYPE_NORMAL
- en: ③ Copies the “mean” price column as the target variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have completed the script, we can visualize our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As index, we have our `listing_id`, and among the columns, there are all the
    features we have prepared for the problem, as shown in figure 7.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 Top rows of the predictors’ dataset
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can start examining the data in detail and figure out if there
    are any additional problems to be fixed or other insights to be discovered that
    could play an essential role in how we develop our XGBoost model.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.5 Exploring and fixing your data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step after having assembled all the predictors into a single dataset
    is to explore it to detect any problems, such as missing data or extreme values,
    that may affect the performance of a machine learning algorithm. Therefore, our
    first action is to check for any missing data with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting list points out that there are three features with some missing
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed in chapter 2 and then again in the previous chapter, it is
    crucial in the presence of missing data to investigate why there are missing values
    and if that could be deemed a missing completely at random, missing at random,
    or missing not at random situation. In this specific case, missing values are
    not at all at random, but they depend on the fact that there are no reviews or
    there are not enough reviews to compute a score. In fact, by inspecting how many
    accommodations are there without reviews, you will notice how the figure matches
    the number of missing cases on two features with missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As anticipated, the result is 1,252, matching the number of missing values.
    In this case, it is better to refrain from using the capabilities of XGBoost and
    other GBDT implementations to deal with missing data because its behavior will
    mimic an average situation. Missing reviews is an extreme yet legitimate case
    when an accommodation has just entered the market or is seldom chosen. Here, you
    need to directly input a number that may help any machine learning algorithm figure
    out that there are no reviews, hence the missing values. A common strategy is
    to use a value at the boundaries of the existing distribution, usually a negative
    number if we are representing counts or positive values. A quick check can assure
    us if there are the prerequisites for such a missing values strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we confirmed that the minimum value is always greater than zero
    for all three considered features. It means we can simply replace the missing
    values using the –1 value (we cannot use zero because the minimum value is zero
    for `days_since_last_review`), which will work as a solution both for a linear
    model (it is at the lower extreme of the existing distributions) and for tree-based
    ensembles (they will just split on that negative figure):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As a next step, we will be on the lookout for extreme values among our numeric
    features. As discussed in chapter 2, a straightforward approach to looking for
    outliers and extreme values is to chart a boxplot for each numeric feature arranged
    in a panel of subplots or a single plot if they have comparable scales. In our
    case, in the following listing, we have prepared a panel of boxplots to be inspected
    for extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 Plotting boxplots for numeric features
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ① Estimates the number of rows needed to arrange subplots
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates the number of columns needed to arrange subplots
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a figure with subplots
  prefs: []
  type: TYPE_NORMAL
- en: ④ Flattens the axes array to a 1D array so that it can be iterated through
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 shows the charted results. By inspecting the values outside the whiskers
    of the plots, represented as empty points, we immediately notice that almost all
    the distributions have heavy tails on the right, with values decisively much larger
    than the mean value. If, for distance-based features, such extreme values may
    sound reasonable because of the extension of Tokyo’s metropolitan area, as for
    features such as `minimum_night` and `number_of_reviews`, such extreme values
    may represent outliers quite far from the core of the distribution. We could use
    winsorizing to solve such a problem, using a solution we proposed in chapter 2\.
    This data transformation technique replaces extreme values in a dataset with less
    extreme values to reduce the influence of outliers on statistical analyses and
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 7.9, using the `winsorize` function from the Scipy package, we winsorize
    the 0.1% of the upper part of the distribution of the `minimum_nights` feature.
    All values above the 0.999 percentile will be changed to the value of the 0.999
    percentile, thus removing any extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 Panel of boxplots illustrating the skewed distributions of most numeric
    features
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 Winsorizing extreme values
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: ① Indicates the lower percentile below which values will not be changed during
    winsorization
  prefs: []
  type: TYPE_NORMAL
- en: ② Indicates the upper percentile above which values will not be changed during
    winsorization
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 shows the highest value is now 120, not over 1,000 as before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 Boxplot of winsorized `minimum_nights` feature
  prefs: []
  type: TYPE_NORMAL
- en: 'We replicate the same also for the `number_of_reviews` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Figure 7.9 shows that now the feature still has a heavy right tail. However,
    the extreme values have been compressed to below the 500 value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 Boxplot of winsorized `number_of_reviews` feature
  prefs: []
  type: TYPE_NORMAL
- en: Having completed checking and remediating for missing values and extreme values
    in our dataset of predictors, we can proceed in the next subsection to look at
    the target itself.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.6 Exploring your target
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When dealing with exploratory data analysis (EDA), it is critical to examine
    the predictors and the target, and sometimes both predictors and target together
    and how they relate to each other. For our example, being a regression problem,
    we simply start by figuring out the mean and the range of the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands will print the minimum, the average, and the maximum values
    in the target variable y:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We immediately notice that the maximum is on a quite different scale than the
    average and the minimum. Estimating percentiles can be helpful to understand better
    if there is a problem with extreme values or skewed distribution in the target.
    The presence of extreme values can be better understood by requiring a range of
    percentiles focusing on the extremities of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the output percentiles, and we have confirmation of the presence
    of extreme values at the right of the distribution since even the 99th percentile
    is quite far from the maximum we had previously reported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Not only is the maximum value quite far from the 99th percentile, but there
    also appears to be a significant gap between the 95th and 99th percentiles. Our
    decision is to focus on the core of the distribution by dropping 10% of the distribution:
    5% on the lower and 5% on the upper part. We manage this by selection using a
    Boolean selection variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Before definitely applying the selection, we plot the resulting distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 Plotting the target distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: ① Selects only the part of the target distribution we consider to model
  prefs: []
  type: TYPE_NORMAL
- en: ② Represents the median value of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: The previous code snippet will output a density plot, revealing the distribution
    and the median value for the selection of the target based on the selection variable
    we have just defined. Figure 7.10 shows the resulting plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 Density distribution of the target variable
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting distribution shown in figure 7.10 is definitely skewed to the
    right, a condition also called a positive skew. In particular, you can observe
    a lump of data at the start and a long decreasing tail following to the right,
    although, by the end, we can notice another small lump closing the distribution,
    probably a separate cluster of high-end accommodations. However, the range and
    distribution of the target are fine now. Therefore, we will select both the target
    and data using the previously defined Boolean selection variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will proceed to define both aspects of the validation
    process and the data pipeline necessary for modeling. Afterward, we will try a
    baseline model using classical machine learning linear models and a first tentative
    XGBoost model and optimize it before training our definitive model for the Tokyo
    Airbnb dataset problem.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Building and optimizing your model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use the data we have prepared to build a model. Before
    getting a complete final model, we will address various challenges related to
    defining the cross-validation strategy, preparing the data pipeline, and building
    first a baseline model and then a tentative first XGBoost model.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Preparing a cross-validation strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally, a K-fold cross-validation strategy works quite well in most cases,
    but in our specific situation, we are dealing with real estate units whose value
    is strongly influenced by their location. In our case, Stratified K-fold cross-validation
    is more appropriate, controlling for the effect of location. Although similar
    to K-fold cross-validation, there is a key difference in stratified K-fold cross-validation:
    the class distribution of a feature of our choice in the dataset is preserved
    in each fold. Such stratified sampling will allow folds to have a similar mix
    of territories as the complete dataset. However, it is important to check beforehand
    if some territories are difficult to split among folds because of their low numerosity.
    If we count the different neighborhoods represented in the data, we get a long
    list with many locations, some showcasing a considerable number of listings and
    others only a limited few:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Clearly you cannot accept considering all the neighborhoods having less than
    a certain number of examples because, if you are going to split the data into
    folds, you will hardly have them well represented. Since areas are spatially distributed,
    just gathering them into an extra class won’t do because you will mix very different
    situations of areas quite far from each other. In listing 7.11, we solve this
    problem by aggregating areas with less than 30 examples (implying about 6 examples
    for each fold if we use a five-fold validation split) with their nearest larger
    neighborhood. To achieve that, we use a KDTree data structure again. Thus we can
    match each area with less than 30 accommodations with its nearest area with more
    than 30.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.11 Aggregating nearby neighborhood areas
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: ① Calculates the mean latitude, mean longitude, and the count of listings in
    each neighborhood
  prefs: []
  type: TYPE_NORMAL
- en: ② Separates the neighborhoods into two groups based on the number of listings
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a KDTree using the mean latitude and longitude values of neighborhoods
    with counts greater than 30
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initializes an empty dictionary to store the mappings of the neighborhoods
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Iterates through each neighborhood with counts less than 30 and queries the
    KDTree to find the nearest neighborhood with a count greater than 30
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Replaces the original neighborhood values with the new neighborhood values
    based on the mapping in change_list
  prefs: []
  type: TYPE_NORMAL
- en: 'After having run the code, you can check how the mapping has been performed
    and if the resulting aggregation has neighborhood areas with less than 30 listings
    by issuing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Having elaborated a suitable area subdivision, we can now proceed to define
    our stratified cross-validation strategy in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.12 Defining a stratified strategy
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a five-fold stratified random splitting
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates the cross-validation splits while maintaining the same distribution
    of neighborhoods with more than 30 listings in each fold
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting `cv_splits` is a generator, and you can examine it using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the type of object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Since `cv_splits` is a generator, it can be used a single time, but you can
    reinstantiate an identical one by simply re-executing the commands in listing
    7.12\. In the next subsection, we will deal with the data pipeline and determine
    which transformations to apply to our data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Preparing your pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A second preparatory step is to define a pipeline for transforming our predictors
    in the most appropriate way for running generally with all classical machine learning
    algorithms, not just gradient boosting. Ideally, it would be better to have multiple
    pipelines based on how each model deals with the different features. For instance,
    in our pipeline, we are going to ordinally encode a couple of categorical features,
    and such encoding, though fit for tree-based models, doesn’t always work properly
    with linear models. However, while having unique pipelines can lead to better
    performance, it can also become a maintenance nightmare. The human effort required
    to create and manage multiple pipelines may outweigh the marginal performance
    gains achieved by customizing each pipeline for specific models. Therefore it
    is better to decide on multiple pipelines if you have evidence that it is worth
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by classifying the different kinds of features that we will be
    using into categorical features, numeric, and binary features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'A further inspection of the categorical features is necessary because we need
    to understand whether to treat them as high cardinality features or not. We can
    understand better what to do after having counted how many unique values each
    categorical feature has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'From the results, we can determine that probably the only feature that could
    be considered as a high cardinality categorical is the `coordinates` feature,
    which has almost 300 unique values. As for `neighbourhood_more_than_30` and `type_of_accomodation`,
    we could apply ordinal encoding to them for tree-based modeling, whereas for linear
    models, it would be better to apply one-hot-encoding to these features (thus producing
    about 50 new binary features) or target encoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our example revolves around XGBoost and demonstrating how it can work
    with similar problems, we decide to one-hot encode only the `room_type`, ordinal
    encoding `neighbourhood_more_than_30` and `type_of_accomodation`, and target encoding
    `coordinates`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Our choice of working with XGBoost, a tree-based model, also justifies leaving
    all the numeric features as-is. Using linear models, statistical standardization,
    for better convergence when using regularization or generalized linear models,
    and feature transformation, for better fitting nonlinearities, are usually the
    standard.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 7.13, we define all the necessary feature transformations and ensemble
    them in a Scikit-learn’s Column Transformer, which will be part of the pipeline
    that also contains the machine learning model of our choice. It is also important
    to note that we take steps in defining the column transformers to handle unknown
    categories and missing values that may unexpectedly appear at test time. Our strategy
    for one-hot encoding is to ignore new unknown categories. For ordinal encoding,
    the value assigned to the parameter `unknown_value`, which is by default `np.nan,`
    will be used to encode unknown categories. This means an XGBoost model will deal
    with such situations as missing cases using the most frequent split. Other machine
    learning algorithms may instead break into similar occurrences, which is an advantage
    of XGBoost. As for the target encoder, the target mean is substituted for unknown
    categories. Don’t forget to install the `category_encoders` package. If it is
    unavailable on your system, use the `pip install` `category_encoders` command.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.13 Defining column transformations
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a One-Hot Encoder object with the option to handle unknown categories
    by ignoring them during encoding
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates an Ordinal Encoder object with handling of unknown categories and
    the unknown value replaced by np.nan
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a TargetEncoder object, handling unknown values by encoding them using
    the mean target value and applying smoothing with a parameter of 0.5
  prefs: []
  type: TYPE_NORMAL
- en: ④ A Column Transformer object applying the specified encoders to the respective
    columns
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Drops remaining columns that are not specified in the transformer
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Keeps verbose feature names for transformed columns
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Ensures that the transformed data is kept as dense arrays
  prefs: []
  type: TYPE_NORMAL
- en: 'After having run the code listing, we can immediately test transforming the
    data we have and checking the transformed column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that now the features are preceded by a prefix pointing out
    what transformation they underwent. Binary features created by one-hot encoding
    are also followed by the category they represent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: As a final step, we just store away into a single CSV file both the processed
    features and the target. We will use such data again later, in chapter 12, when
    we test a deep learning solution and compare its performance with the XGBoost
    model we train in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the data processing part of our pipeline, we can proceed to
    define a baseline model and then, finally, an XGBoost regressor.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3 Building a baseline model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having a baseline model in machine learning is important for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Comparing performance*—The baseline model serves as a benchmark to compare
    the performance of more complex models, helping you to understand whether more
    complexity really adds value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Detecting overfitting*—By comparing the performance of your advanced model
    against the baseline on unseen data, you can identify if the advanced model is
    overfitting because the baseline model will perform much better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understanding the problem*—Creating a simple baseline model, especially if
    it is a linear model, forces you to understand the data and the problem better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Debugging and validation*—A baseline model can help you validate that your
    data preprocessing pipeline is correct because the effects of the variables on
    the model won’t be hidden by its complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Providing a minimum viable model*—A baseline model provides a minimum viable
    solution to the problem at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all these reasons, we won’t immediately jump into training our model using
    a gradient boosting model, which we expect will perform well on the problem, but
    we take a step back and test a simple linear model. In addition, at this time,
    we will try to get predictions from a type of model we can easily evaluate and
    compare. Instead of just evaluating metrics through cross-validation, we’ll employ
    cross-validation prediction. This method provides unbiased predictions for all
    training cases by making predictions on validation folds within cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: During cross-validation, evaluation metrics are calculated separately for each
    fold. These metrics represent the performance of the model on each fold individually.
    The final evaluation metric reported from cross-validation is usually an average
    (mean or median) of the individual fold metrics. This aggregated metric provides
    an estimate of the model’s generalization performance on unseen data. However,
    if we are using cross-validation predictions, we concentrate on the ability of
    the model to perform on the data at hand. In fact, the primary use of cross-validation
    predictions is to analyze the predictions made by the model on different parts
    of the data used as validation. Using such predictions helps us understand how
    well the model performs across different subsets of the data and identifies if
    the model is overfitting or underfitting because we can compare the predictions
    with the expected target values.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.14 Linear regression baseline model with diagnostic plots
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: ① Initializes a LinearRegression model without intercept
  prefs: []
  type: TYPE_NORMAL
- en: ② Fits the LinearRegression model to the transformed training data
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates stratified cross-validation splits based on the neighborhoods with
    more than 30 counts
  prefs: []
  type: TYPE_NORMAL
- en: ④ Performs cross-validated predictions
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the range of cross-validated predictions
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Calculates R-squared, root mean squared error, and mean absolute error evaluation
    metrics to assess the model’s performance
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Creates a scatter plot of actual vs. predicted values
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Plots a dashed orange zero line to the plot as a reference for the ideal fit
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the code, we obtain the evaluation results, and we can immediately
    notice how some predictions are negative. Since a linear regression model is not
    bounded in its predictions, the mean absolute error (MAE) is quite high (over
    12,000 Yen), and the R squared, a typical measure of fit measuring how much of
    the variance of the target is captured by the model, is just a modest 0.32:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Clearly, the fitting of the model is not particularly impressive, and we can
    get a confirmation as shown in figure 7.11, where we represent the scatterplot
    of the cross-validation predictions on the y-axis against the expected target
    values on the x-axis. Apart from a few negative predictions at the start of the
    target distribution, we can also notice how the predictions depart from the ideal
    fit dashed line, showing a flat trend, a clear sign of underfitting, and how there
    are a few outlying predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 Plot of the fitted results from the baseline linear regression against
    their ideal value
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step in examining the results, we ask for the percentage of predictions
    that are below or equal to zero, an unfeasible prediction because listings should
    be positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a minimal percentage, about 0.5%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Ideally, our predictions should be greater than zero, and in a linear model
    that could be achieved by a target transformation—for instance, a logarithmic
    transformation. However, the role of a baseline model is not to be a perfect model
    but just a model to highlight challenges in the data and be a helpful comparison
    for more sophisticated models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we proceed to locate the rows that are positive outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We received two cases: 5509 and 8307:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We also inquiry about the negative outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we get a single case, 182:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In listing 7.15, we define a function that can help us check for outliers. For
    each predictor feature, this function prints the coefficient and the resulting
    multiplication of the coefficient with the value of the feature for that case,
    thus making explicit the contributions of each feature to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.15 Inspection of the coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: ① Extracts the feature values for the specified case number from the data array
  prefs: []
  type: TYPE_NORMAL
- en: ② Calculates the coefficient values for each feature by multiplying its value
    with the corresponding coefficient from the model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Loops and prints through the feature names, their values, and corresponding
    coefficient values
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the sum of the calculated coefficient values for the case
  prefs: []
  type: TYPE_NORMAL
- en: 'Having our inspection function ready, we can start examining case 8307, which
    represents a case of too positively large outlier in the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the results for case 8307, where it is evident that the extra
    contribution that made the prediction an outlier is because of the number of bedrooms
    (hinting that this property is probably a hostel). This high value pushed the
    final predicted listing upwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar problems are due to the fact that each feature is modeled in a linear
    way. Thus the prediction contribution of a feature is unbounded, having no maximum
    or minimum but decreasing and increasing in accordance with the feature value.
    Typically, introducing nonlinearities and interactions into the model nonlinearities
    and interactions can mitigate such problems. Let’s now check the only negative
    outlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the problem is represented by the value of the minimum of nights, which
    is again too high and drags down the estimated value. In fact, some listings act
    as seasonal accommodation, typically for workers or students, not just for short
    stays. The model is indeed too simple to catch such nuances, and again, having
    introduced nonlinearities and interactions could have helped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, our baseline model has signaled us that successfully solving
    the problem presented by the Tokyo Airbnb dataset requires a better fitting mode
    that can handle positive predictions (they should be necessarily positive) and
    can represent nonlinear relationships and interactions between a particular characteristic
    of the accommodation (a large number of bedrooms indicate a hostel, a high number
    of minimum stay nights indicates an accommodation for seasonal tenants). In the
    next subsection, we will solve all these problems at once by using an XGBoost
    model, which should be able to deal with this data in a more sophisticated and
    smart way.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4 Building a first tentative model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we chose an XGBoost regressor, trying to incorporate some of the insights
    we gained from our previous EDA and baseline model inspections. We decided to
    use a gamma objective function, commonly used in regression problems, for modeling
    positive continuous variables that are positive and right-skewed. Gamma is particularly
    useful when the target variable is always positive, and it includes many small
    values and a few larger values, as it handles such distribution characteristics
    quite well.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, since our baseline model has shown signs of underfitting and not
    handling interactions or linearities properly, we decide on a max depth of at
    most six for the decision trees composing the boosted ensemble, thus allowing
    for an adequate number of splits to handle most common similar data characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: In the following listing, arranged similarly as the previous listing training
    the linear regression baseline, we train an XGBoost regressor, and we test its
    out-of-fold cross-validation predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.16 First XGBoost model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets up an XGBoost regressor with specific hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines ‘reg:gamma’ as the objective function
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generates cross-validation splits based on the neighbourhood_more_than_30
    feature
  prefs: []
  type: TYPE_NORMAL
- en: ④ Performs cross-validated predictions
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the range of predicted values
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Calculates R-squared, root mean squared error, and MAE evaluation metrics
    to assess the model’s performance
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Creates a scatter plot of actual vs. predicted values
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Adds reference lines to the plot for an ideal fit and a zero line
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, the prediction range is strictly in the positive range, as we expected.
    The MAE is almost half of those of the baseline linear model and the R-squared
    scores almost 0.7, which is a reasonably good result showing how the model is
    now able to intercept most of the variance present in the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'A further inspection of the fitted results, represented in figure 7.12 as a
    scatterplot between the cross-validation predictions (on the y-axis) and the expected
    target values (on the x-axis), shows that the points are now slightly more in
    line with our ideal fit. In addition, it is important to notice how the XGBoost
    model tends to extrapolate the predictions: the column of predictions at the rightmost
    part of the chart indicates that our model predicted values sometimes higher than
    the observed maximum in the target, whereas, on the leftmost part of the chart,
    there are no nonpositive estimations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 Plot of the fitted results for the XGBoost model against their ideal
    value
  prefs: []
  type: TYPE_NORMAL
- en: Typically, you shouldn’t limit yourself to a single model in a data science
    project, as in our example. Because of space constraints, we just focus on an
    XGBoost model. Still, it is advisable in a working project to try even more diverse
    classical machine learning algorithms, such as other gradient boosting implementations,
    as well as more tree ensembles, generalized linear models, and even more unusual,
    nowadays, classical machine learning models (such as k-nearest neighbors or support
    vector machines). There is no free lunch in machine learning, and you may find
    reasonable solutions to this problem even with different algorithms that may better
    suit your necessities in terms of performance, speed of inference, memory occupancy,
    and portability onto other systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsection, we optimize our XGBoost solution using Bayesian
    optimization to strive to perform as best as possible on our problem.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.5 Optimizing your model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since XGBoost works quite well for the problem, we’ll take some time to refine
    its parameters and test different boosting approaches and objectives. We are going
    to use Optuna, a Bayesian optimizer presented in the previous chapter, because
    it can efficiently explore a GBDT hyperparameter search space, adaptively choosing
    in a short set of rounds, based on the outcomes of the previous experiments, the
    next set of hyperparameters to be evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have Optuna available on your system, you can install it by issuing
    the command `pip` `install` `optuna` in a shell or a notebook cell.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.17 performs hyperparameter optimization using Optuna for our previously
    tested XGBoost Regressor model to find the best hyperparameters that minimize
    the MAE of the model on the Tokyo Airbnb dataset. The core of the listing is the
    objective function that suggests to Optuna different hyperparameter values using
    t`rial.suggest_...` methods. In particular, it tests the classic `gbtree` booster
    (gradient boosting) and also the `gblinear`. This booster utilizes a linear model
    as its base learner, incorporating both L1 and L2 regularization instead of employing
    a decision tree. Regarding the objective function, it tests the classical squared
    error, the gamma objective, and the Tweedie, blending aspects of the gamma and
    Poisson distributions. When selecting the gblinear boosting or the Tweedie objective,
    the code overrides the chosen parameters. It makes modifications and additions
    to them to fit the requirements of the gblinear booster or the Tweedie objective.
    Finally, it then creates an XGBoost Regressor with the suggested hyperparameters
    at each test and performs cross-validation to evaluate the MAE. The process is
    repeated for a specified number of trials (60 in this case). After optimization,
    the best achieved MAE and corresponding best hyperparameters are printed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.17 Optimizing the XGBoost regressor
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines an optimization objective function using the Optuna library
  prefs: []
  type: TYPE_NORMAL
- en: ② Dictionary containing hyperparameters for optimization, including booster
    type, objectives, and others
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adjusts hyperparameters based on the chosen booster type
  prefs: []
  type: TYPE_NORMAL
- en: ④ Suggests the additional parameter 'tweedie_variance_power' for a tweedie objective
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Initializes an XGBoost Regressor with the suggested hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Performs cross-validation using the defined pipeline and optimizing for MAE
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Calculates the MAE from the negative MAE scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Returns the calculated evaluation metric value to be minimized
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Creates an Optuna study with storage in a SQLite database for optimization
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Performs optimization for a specified number of trials
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Prints the best evaluation metric value achieved during optimization
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ Prints the best hyperparameters found during optimization
  prefs: []
  type: TYPE_NORMAL
- en: 'After having the optimization run for a while, we obtain a reduced MAE in respect
    of our first attempt and a set of suitable hyperparameters, showing a max depth
    of seven levels, about 900 estimators, and a Tweedie objective function with a
    variance power of 1.5, indicating a mixed distribution between Poisson and gamma:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can also plot some diagnostic charts to understand better
    how the optimization went. For instance, we can first plot how the optimization
    proceeds across the 60 trials that we initially set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Figure 7.13 shows how the best value was achieved quite early, before 20 trials,
    and it didn’t improve after that. This is important information because if the
    best optimization could have been achieved later, you may have suspected further
    improvements lying ahead in some more rounds of hyperparameter exploration by
    Optuna. Actually, you may have achieved that anytime by rerunning the command
    `study.optimize(objective,` `n_trials=100)`, setting your intended number of extra
    trials instead of the initial 100\. Since we set to store trials on an SQLite
    database, you may reprise the optimization any time from the point it stopped
    (which is a strong point for using Optuna instead of other optimization options).
    Another important fact to gather from the chart is that there is quite a crowd
    of hyperparameter sets that are optimal or almost optimal. That means that there
    is no single optimization possible for this problem. That allows you to explore
    the different settings and decide on the solution that suits your needs. For instance,
    you may decide on a near-optimal solution that samples more features or requires
    fewer estimators since they are faster at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 How tests by Optuna progressively performed during the optimization
  prefs: []
  type: TYPE_NORMAL
- en: 'After observing how the optimization proceeded, another important piece of
    information is provided by charting the importance of the hyperparameters because
    it could hint at expanding the search space for such hyperparameters if they proved
    so important for the optimization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the most critical factors proved to be `colsample_bytree` and `min_child_weight`,
    hyperparameters that caused the most variance in results, as seen in figure 7.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F14_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 Importance of hyperparameters in the tuning process
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a good set of hyperparameters. In the next subsection, we will complete
    our training phase by testing the model with cross-validation, an evaluation for
    generalization purposes, and training the final model using all the available
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.6 Training the final model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having completed the optimization, we can conclude our work by testing the results
    directly by cross-validation and then training the model on all available data.
    The code presented in listing 7.18 doesn’t change much from the code we previously
    used. Notice that now, for our estimation, we are using a cross-validation procedure,
    not cross-validation predictions, because we are more interested in understanding
    the generalization capabilities of our model and not how it fits precisely the
    data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.18 Training the model with full data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: ① Initializes an XGBoost Regressor using the best hyperparameters obtained from
    the Optuna study
  prefs: []
  type: TYPE_NORMAL
- en: ② Splits the data using the specified StratifiedKFold strategy
  prefs: []
  type: TYPE_NORMAL
- en: ③ Iterates through the cross-validation folds to test the model
  prefs: []
  type: TYPE_NORMAL
- en: ④ Trains the final model on the entire dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output you receive when running the code, containing the
    used parameters and the evaluation metrics, all based on our cross-validation
    strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We can also visualize, as shown in figure 7.15, the complete pipeline comprising
    the column transformer, accepting the different features for its distinct transformation
    operations, and the XGBoost model receiving all the assembled data from the column
    transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F15_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 Pipeline comprising column transformations and XGBoost model
  prefs: []
  type: TYPE_NORMAL
- en: Having thoroughly trained our model, we could say that we are done. Actually,
    this could be just the first cycle of multiple iterations because models have
    to be retrained often to escape what is called concept drift, as we explained
    in chapter 2, where the relationships between the predictors and the target change
    over time, rendering past models ineffective after a while.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, often, the work of a machine learning engineer and of a data scientist
    doesn’t end with a working model because it is crucial to be able to figure out
    how it works and how the predictors actually relate with the target, providing
    insights into how a model arrives at its predictions. Explaining how a model works
    helps build trust, facilitates debugging, aids regulatory compliance, and enables
    humans to understand, validate, and improve the decision-making process of AI
    systems, which is the topic of the concluding section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Explaining your model with SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To conclude, we spend some time trying to understand how our XGBoost model works
    because, as EDA helps you understand how your model can use data, explainability
    techniques such as SHAP (SHapley Additive exPlanations) or partial dependence
    plots (described in the previous chapter) can help you know how your model uses
    the data to come to its predictions. Explainability can provide valuable insights
    that help you better prepare your data, revise previous assumptions, and discard
    unuseful or detrimental features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, explainability plays other softer roles in a data science project
    than providing insights into how the model uses its features and generates predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Human-AI collaboration*—When working with tabular data, data scientists collaborate
    with domain experts or business stakeholders who may not be well-versed in complex
    models. Explainability allows data scientists to communicate model insights effectively
    to nontechnical audiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Building trust*—In certain domains, such as healthcare or finance, model explainability
    is essential to build trust with stakeholders and regulatory bodies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compliance and regulations*—In some geographical areas and industries, there
    are regulatory requirements for model transparency and explainability, such as
    in the European Union, where the General Data Protection Regulation emphasizes
    the “right to explanation” for automated decision-making systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bias detection and mitigation*—Explainability can help identify biases in
    the data and the model’s decision-making process, highlighting if the model’s
    decision-making process could disadvantage any sensible group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given all these reasons, we decided to produce SHAP values, which can be generated
    by the SHAP package ([https://github.com/shap/shap](https://github.com/shap/shap);
    install with `pip` `install` `shap`) and its TreeSHAP algorithm for tree-based
    models but also natively and more efficiently by XGBoost, as well as LightGBM,
    using a simple procedure.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP values are a method that can explain the way predictions of machine learning
    models are built. They are based on Shapley values, a cooperative game theory
    concept that fairly distributes each feature’s “credit” or “importance” in a model’s
    prediction for a specific data instance. In other words, SHAP values allocate
    the contribution of each feature to the model’s output using a simple additive
    formula.
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values consider the contribution of a feature across all possible combinations
    of features, which can be thought of as “games” in the model. These “games” involve
    training the model on different feature subsets. SHAP values approximate Shapley
    values using a resampling strategy to avoid computing all possible games for the
    model and feature sets. By using SHAP values, we gain insights into how each feature
    influences the model’s predictions on specific instances. This information is
    valuable for model debugging, feature engineering, and enhancing machine learning
    models’ overall interpretability and trustworthiness.
  prefs: []
  type: TYPE_NORMAL
- en: 'We implemented SHAP values in listing 7.19 to gain insights into our previously
    built XGBoost model. In the code, we first retrieve the trained XGBoost model
    from a pipeline. In particular, we get its booster, the core component of the
    XGBoost model responsible for implementing the gradient boosting algorithm. Then
    we transformed the training data two times: first because we could not use the
    pipeline to feed the data into the booster directly. Hence, we preprocess it by
    hand and extract its feature names for reference. Second, we transform the data
    into a DMatrix data structure (see the XGBoost documentation at [https://mng.bz/yWQd](https://mng.bz/yWQd)),
    a specific XGBoost data structure for efficient processing that is required for
    feeding the booster directly. At this point, we compute the SHAP values by a predict
    command with the parameter `pred_contribs` set to true. Another simple predict
    command just provides us with the predictions from the model to be used for comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.19 SHAP values as an XGBoost output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: ① Retrieves the trained XGBoost booster object from the pipeline’s trained XGBoost
    model
  prefs: []
  type: TYPE_NORMAL
- en: ② Transforms the input data X using the processing pipeline
  prefs: []
  type: TYPE_NORMAL
- en: ③ Gets the names of the transformed features after the processing pipeline’s
    transformations
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a DMatrix from the transformed input data
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculates SHAP values using the booster’s predict function with the pred_contribs=True
    argument
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Gets the raw predicted values for the input data
  prefs: []
  type: TYPE_NORMAL
- en: Just for comparison, we have to note that LightGBM is also capable of the same,
    using the same prediction method with the `pred_contribs` parameter set to true.
    The only difference is that you do not need to extract any booster from the trained
    LightGBM model. You just use the model itself directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that whether you are doing a classification or a regression, the resulting
    SHAP values obtained by this method are log transformations of a multiplicative
    model. It means that if you want to recreate the original prediction, you first
    have to exponentiate the values and then multiply them by themselves, as demonstrated
    by the following code snippet, reconstructing for the first example the prediction
    from the SHAP values and comparing it to the effective prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, there are slight discrepancies in the reconstruction, which
    can be attributed to approximations and small errors. However, in general, the
    SHAP values provide a good approximation of the predictions themselves. When applying
    the same approach to the entire training set and assessing its adherence to the
    original predictions using Pearson’s correlation, it demonstrates a strong fit
    of the SHAP values to the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'As an alternative to directly outputting the SHAP values as an XGBoost prediction,
    you can use the `TreeExplainer` function from the SHAP package ([https://mng.bz/pKXR](https://mng.bz/pKXR)).
    The function, though being declared built with fast C++ implementations, is way
    slower than the direct predictions from XGBoost. However, using the `TreeExplainer`,
    you can specify more output options, particularly the output type and the calculation
    method, which can allow you to reconstruct the original prediction as seen previously
    (using the parameter `feature_perturbation="tree_path_dependent"`) or using a
    method that “breaks the dependencies between features according to the rules dictated
    by casual inference,” thus providing more reliable insights when there is strong
    collinearity among the features (using the parameter `feature_perturbation="interventional"`).
    You can obtain the interventional SHAP values using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting SHAP values matrix is less truthful to the original data and
    cannot reconstruct the predictions as seen before. Still, such an approach could
    provide more reliable contribution estimates “true to the model” as explained
    in technical terms in the following GitHub problem: [https://github.com/shap/shap/issues/1098](https://github.com/shap/shap/issues/1098).
    Based on our experience, we suggest using `TreeExplainer` and the interventional
    approach, although it may require longer computation times when dealing with data
    presenting highly multicollinear features.'
  prefs: []
  type: TYPE_NORMAL
- en: Up to now, we have used the SHAP values as a method for explaining individual
    samples. We investigated, by the inspection of feature contributions, the reasons
    why a certain prediction is made. However, we can consider all the SHAP values
    together and reason about them to figure out a general explanation for the entire
    model. In this case, as for other methods, we can plot some summary and diagnostic
    charts to figure this out better. The first listing we propose quantifies the
    relative importance of the features by an average of the SHAP values. Here, we
    use plotting facilities from the `shap` package. You install the package by running
    the command `pip` `install` `shap` in a shell or a cell of your notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.20 SHAP importance plot
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates a summary plot of SHAP feature importance for the top 10 most important
    features
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.16 shows the resulting plot, and you can immediately determine that
    four features tend to dominate the predictions, which are the availability, which
    is also a proxy for the market offer-demand dynamic for a certain accommodation
    (less availability may imply a shared use or less demand for that accommodation);
    target encoded coordinates (i.e., the position of the accommodation in the city);
    the number of bedrooms, a proxy of how large the accommodation is; and the number
    of beds, which helps together with the previous figure to distinguish the hostel-like
    listings, which are usually less pricey. All the other features play a lesser
    role, which can be seen from the scale of the plot: the importance of the last
    of the 10 most important features is a fifth of the top important features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F16_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 SHAP importance
  prefs: []
  type: TYPE_NORMAL
- en: 'Importance, however, tells just a part of the story. We also need directionality.
    Hence, the violin chart can provide even more information on the model’s behavior.
    In a violin plot produced by the `shap` package, you can get hints from these
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature importance*—The width of the violin plot indicates the density of
    SHAP values. Wider sections represent more instances with similar SHAP values
    for that feature. Thus, features with broader violin plots are generally more
    important in the model’s predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shape of the violin*—The violin’s shape indicates the distribution of SHAP
    values for the corresponding feature. If the violin is symmetric, it suggests
    that SHAP values are evenly distributed around the median, signifying a balanced
    effect on predictions. Asymmetry indicates skewness and suggests that certain
    feature values have more significant effects than others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Positive and negative contributions*—The violin plot’s center line (median)
    is usually zero. The left and right halves of the violin represent their respective
    contributions for features with positive and negative SHAP values. Positive SHAP
    values push predictions higher, while negative SHAP values push them lower.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Association with the feature value*—The color of the violin plot can help
    you associate blue areas, where the feature has lower values, and red areas, where
    the feature has higher values, with specific SHAP contributions. This helps in
    understanding how the feature is generally related to the outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Outliers*—Outliers or extreme SHAP values outside the range of the violin
    plot suggest instances where the corresponding feature has an unusually strong
    effect on the prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following listing, the violin plot provides useful insights into the
    distribution and the role of each feature on the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.21 SHAP violin plot
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a SHAP summary plot using violin plots to visualize the distribution
    of SHAP values for each feature
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.17 shows the resulting violin plot. As for our top important features,
    we can figure out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Numeric__availability_365`—Higher availability corresponds to a positive effect
    on price. Listings with lower availability are usually penalized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Target_encoding__coordinates`—It is difficult to interpret since its values
    are unrelated to a specific directionality. We can observe that there are long
    tails on both sides with a prevalence of a negative contribution to the pricing
    of the accommodation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Numeric__number_of_bedrooms`—A higher number of bedrooms implies a higher
    price, with a long skewed tail to the right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Numeric__number_of_beds`—Similarly, a higher number of beds implies a higher
    price, with a long skewed tail to the right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH07_F17_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 SHAP violin plot
  prefs: []
  type: TYPE_NORMAL
- en: A glance at other features provides an idea of how the model behaves intuitively.
    For instance, the nearer the accommodation is to the Imperial Palace or the airports,
    the higher the price.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our end-to-end example using gradient boosting. In the next chapter,
    we are going to get back to the Airbnb NYC problem and will review a set of deep
    learning stacks (low-level framework, high-level API, and deep learning for tabular
    data library) and use three of these stacks (fastai, PyTorch with TabNet, and
    Lightning Flash) to solve it and compare the different solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this phase, a generative AI tool such as ChatGPT could be useful for creating
    narratives explaining the SHAP values assigned to each example. Being able to
    create an easy explanation for each prediction could prove to be a strong point
    when demonstrating the potentialities of your model or trying to persuade clients
    and stakeholders. In addition, the need for a narration explaining the model’s
    predictions of the dataset is crucial under regulations such as those in the European
    Union. Transparency and interpretability are essential components of regulations
    striving at data protection and preserving privacy, such as the General Data Protection
    Regulation in the EU. According to these regulations, individuals have the right
    to make sense of the logic behind automated decision-making processes that significantly
    affect their lives. Providing a clear and comprehensible explanation for why a
    specific prediction has been made ensures transparency and accountability and
    promotes fairness: it gives individuals the power to seek clarification, challenge
    unfair decisions, and ultimately safeguard their rights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can actually generate each of these narratives by single prompts to create
    explanations on the fly or by using the ChatGPT API and have the model process
    batches of explanations that you can later recall when questioned about the reason
    for a specific prediction. The recipe is, however, the same in both on-the-fly
    or batch processing approaches: you have to tell the LLM to explain by providing
    the list of the features (detailed with their description or meaning, if necessary),
    the original value in the dataset, and the SHAP value relative to the feature.
    Of course, it is necessary to mention the resulting prediction. Gluing together
    all such information for the LLM to process using JSON (a dictionary of dictionaries)
    could be ideal. In listing 7.22, we offer a solution for preparing a JSON structure
    to facilitate the prompt request to ChatGPT to explain a specific example in the
    dataset, identified by its row index. The code generates a data structure that
    encompasses all the information required to construct a coherent narrative explanation.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.22 Building a JSON of SHAP explanations as part of a prompt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: ① Instantiates the JSON data structure as a Python dictionary
  prefs: []
  type: TYPE_NORMAL
- en: ② Includes the predicted value to explain in the JSON
  prefs: []
  type: TYPE_NORMAL
- en: ③ Iterates over the features, original value, and SHAP values of the examined
    row
  prefs: []
  type: TYPE_NORMAL
- en: ④ Index of the prediction to explain
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we require a description of why the model predicted a particular
    value for row 5 of the dataset. The printed JSON can then be enclosed in a prompt
    such as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'When you submit this prompt to ChatGPT, you will receive a text organized in
    bullet points categorized by types of variables. This text describes the influence
    of each individual variable or group of variables on the outcome. The following
    is an excerpt of the insights we derived for the specific instance represented
    by row 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Room type*—The “Entire home/apt” room type has a positive effect on the predicted
    price, contributing a SHAP value of 0.034\. This suggests that listings with the
    entire home/apartment as the room type tend to have higher prices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The other room types* (“Hotel room,” “Private room,” and “Shared room”)—These
    have smaller positive or negligible contributions, indicating that their effect
    on the price is not as significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neighborhood*—The feature `neighbourhood_more_than_30` has a positive SHAP
    value of 0.083, suggesting that being in a neighborhood with more than 30 listings
    positively influences the price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Type of accommodation*—The `type_of_accommodation` feature has a small negative
    effect with a SHAP value of –0.008\. This implies that certain types of accommodation
    might have a slightly lower price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete text actually touches on all the features, and you can prompt the
    LLM to reduce the results to only the top 5 or 10 impactful features, if you prefer.
    Certainly, using a language model for this job makes a difficult task simple and
    automates it in a breeze.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting and preparing your data requires downloading, restructuring, and assembling
    it all together. It is often a long and laborious part of the work in an end-to-end
    project, but it is an indispensable one, building the foundations for the success
    of your following work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering is not just magic or randomly combining features; most often,
    it is embedding prior knowledge about a problem and how to solve it in the features
    you will be using to train your model. Exploring the set of domain knowledge related
    to a problem is the first step to model your data effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring your predictions and target in the EDA phase is an essential part
    of your schedule for modeling a tabular problem. Look for outliers and extreme
    values, missing data, and any other peculiarity from the data. Feel free to drop
    examples if you are unsure they can provide real value to your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before delving into modeling, check for your validation strategy, which may
    require extra work, EDA, and your data pipeline. Both can make a difference in
    the modeling phase. Ideally, prepare a pipeline for each type of model you want
    to test because each model has different ways of dealing with the various types
    of data you find in tabular datasets. In our example, as a simplification, we
    tried a one-size-fits-all approach. Please remember that such examples work well
    in books, but there are better ways to do it in real-world projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a baseline model is an often-neglected phase in modeling tabular data
    problems. Still, it can provide valuable insights by inspecting how the model
    underfits or overfits the data and its internal coefficients. A baseline model
    should necessarily be simple, meaning that linear and logistic regression are
    the best candidates for regression and classification problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you get insights from your baseline model, you can proceed to more complex
    models such as XGBoost. Cues about underfitting, nonlinearities, interactions,
    targets, and the predictors’ characteristics should be considered when setting
    up the first tentative values for key hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing your model using Optuna can save you a lot of time if you set your
    search space to incorporate insights and hypotheses you have developed so far
    regarding how your model should handle the data and the problem. Once the optimization
    has been completed, further insights can be gained from observing hyperparameter
    importance and optimization path charts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining your trained model can be easily done with XGBoost and LightGBM using
    the predict method with the parameter `pred_contribs` set to true. Once the SHAP
    values, which are effectively multipliers with respect to the prediction, are
    obtained, you can use standard charts from the `shap` package, such as the importance
    plot or the violin plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
