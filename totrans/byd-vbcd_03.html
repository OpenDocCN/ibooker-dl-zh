<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. The Art of the Prompt: Communicating Effectively with AI"><div class="chapter" id="ch02_the_art_of_the_prompt_communicating_effectively_w_1752630042971067">
<h1><span class="label">Chapter 2. </span>The Art of the Prompt: <span class="keep-together">Communicating Effectively with AI</span></h1>

<p>In vibe coding, prompts are the new source code.</p>

<p>The way you communicate your intent to the AI has a direct impact on the quality of the code it generates. <a contenteditable="false" data-primary="prompts" data-secondary="effective communication with AI" data-type="indexterm" id="id407"/>Writing a good prompt is both an art and a science, often called <em>prompt engineering</em>. This chapter will equip you with techniques to get the most out of your AI coding assistant. <a contenteditable="false" data-primary="prompt engineering" data-type="indexterm" id="id408"/>We’ll start with some fundamentals about why prompts matter and then delve into a toolbox of prompting techniques, from simple to advanced. By learning how to craft effective prompts and how to iteratively refine them (<a data-type="xref" href="#ch02_figure_1_1752630042957579">Figure 2-1</a>), you’ll be able to cocreate with AI more efficiently and accurately.</p>

<figure><div id="ch02_figure_1_1752630042957579" class="figure"><img src="assets/bevc_0201.png" width="881" height="1248"/>
<h6><span class="label">Figure 2-1. </span>An illustration of a chatbot assisting with coding. The developer and AI engage in a dialogue: the developer provides instructions or questions (prompts), and the AI responds with code or answers. Communicating effectively with the AI through well-crafted prompts is key to getting accurate and useful code generation.</h6>
</div></figure>

<section data-type="sect1" data-pdf-bookmark="Prompt Engineering Fundamentals"><div class="sect1" id="ch02_prompt_engineering_fundamentals_1752630042971247">
<h1>Prompt Engineering Fundamentals</h1>

<p>If vibe coding is a conversation between you and an AI model, <em>prompt engineering</em> is the skill of speaking the AI’s language to get the best results.<a contenteditable="false" data-primary="prompt engineering" data-secondary="fundamentals of" data-type="indexterm" id="id409"/> A well-crafted prompt can be the difference between an irrelevant or buggy code suggestion and a perfect solution. Mastering prompt engineering means understanding how to guide the AI effectively, how to provide context, and how to iterate with the AI when the first answer isn’t quite right.</p>

<p>When you program with an AI, you are essentially programming <em>through</em> the AI using natural language. The prompt you provide is like a high-level programming language that the AI interpreter then translates into actual code. Just as a compiler’s output is only as good as the source code fed to it, an AI’s output is only as good as the prompt.</p>

<p>Why are prompts so important? LLMs, despite their sophistication, are not mind readers. They respond only to the input they’re given. <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="ambiguous prompts, problems with" data-type="indexterm" id="id410"/>Ambiguous or poorly worded prompts can lead to irrelevant or incorrect code, while a clear and specific prompt can yield a spot-on solution on the first try. In traditional coding, you spend time thinking about algorithms and writing code; in vibe coding, you spend time thinking about how to convey your requirements to the AI.<a contenteditable="false" data-primary="requirements" data-secondary="thinking about how to convey to AI" data-type="indexterm" id="id411"/> It’s a shift in what “writing code” means: you might write a paragraph instead of a function, but you still need to be precise and logical.</p>

<p>Think of writing prompts as being like writing documentation or user stories for a very literal and pedantic junior developer, one who will do exactly (and only) what the documentation says, and who has a lot of knowledge but no common sense beyond patterns they have seen. If your instructions (prompts) leave room for interpretation, the AI might fill the gaps in ways you didn’t intend. Thus, learning to <em>communicate</em> with the AI is as crucial as learning a programming language’s syntax used to be.</p>

<p>Another reason prompts are crucial is reproducibility and future-proofing. If you discover a prompt that reliably generates good code for a certain pattern or task, that prompt becomes a valuable piece of knowledge (almost like a snippet or template). You might save it or reuse it in similar contexts. In teams, developers might share effective prompt patterns with each other, similar to how they share coding best <span class="keep-together">practices</span>.</p>

<p>Finally, as models get better and more integrated, they may allow more complex interactions. Being good at prompting will let you harness new capabilities quickly. For instance, some advanced systems allow you to attach extensive instructions or provide entire reference documents as part of the context for the model. Knowing how to structure that input is key to leveraging such power.</p>

<p>So treat prompt writing as a new essential skill. In many ways, prompting <em>is</em> programming. The main difference is you’re writing in a language (like English) that the AI then converts into code. But you still have to be clear, logical, and anticipate edge cases in your description.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Specificity and Clarity: Writing Prompts That Deliver"><div class="sect1" id="ch02_specificity_and_clarity_writing_prompts_that_deli_1752630042971305">
<h1>Specificity and Clarity: Writing Prompts That Deliver</h1>

<p>One of the golden rules of prompting (which I’ll lay out more fully in <a data-type="xref" href="ch03.html#ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933">Chapter 3</a>) is to be specific and clear about what you want. <a contenteditable="false" data-primary="prompts" data-secondary="effective communication with AI" data-tertiary="specificity and clarity in prompts" data-type="indexterm" id="id412"/>Unlike a human collaborator, an AI doesn’t truly understand your goal beyond the words you provide. A common mistake is giving the AI a very high-level prompt like “Make a website” and expecting magic. The AI works better with concrete details.</p>

<p>Always assume it knows nothing about your project beyond what you provide. Include relevant details such as the programming language, framework, and libraries, as well as the specific function or snippet in question. If there’s an error, provide the exact error message and describe what the code is supposed to do. Any vagueness or room for interpretation can lead to unintended outputs.</p>

<p>For example, instead of “Write a sorting function,” you could say:</p>

<blockquote>
<p>Write a Python function sort_by_lastname(customers) that takes a list of customer records (each with a first_name and last_name field) and returns a list sorted by last_name alphabetically. Include a brief docstring and handle the case of missing last names by treating them as empty strings.</p>
</blockquote>

<p>This prompt sets clear expectations about the language (Python), the function name and purpose, the input structure, the sort key, additional requirements (docstring), and an edge case. It’s likely to produce exactly what you need or very close to it. Essentially, think like a spec writer: the more precisely you specify the task, the less guesswork the AI has to do and the fewer revisions you’ll need.</p>

<p>Strategies <a contenteditable="false" data-primary="specificity in prompts" data-secondary="strategies for" data-type="indexterm" id="id413"/>for specificity include:</p>

<dl>
	<dt>Mention the language or environment</dt>
	<dd>
	<p>If you want a<a contenteditable="false" data-primary="programming languages" data-secondary="specifying in prompts" data-type="indexterm" id="id414"/> solution <a contenteditable="false" data-primary="environment, specifying in prompts" data-type="indexterm" id="id415"/>in JavaScript, say so: “Write a JavaScript function...” versus just “Write a function...” If you want it for a specific framework or version, include that (“Using React Hooks...” or “in Python 3...”).</p>
	</dd>
	<dt>Define the scope of the output</dt>
	<dd>
	<p>Do you want just a single function? A full file or module? Tests included? For example, “Provide only the function implementation” and “Provide a complete runnable script” can yield different responses.</p>
	</dd>
	<dt>Include requirements and constraints</dt>
	<dd>
	<p>In the login example, we specified password length and attempt limit.<a contenteditable="false" data-primary="requirements" data-secondary="including in prompts" data-type="indexterm" id="id416"/><a contenteditable="false" data-primary="constraints" data-secondary="including in prompts" data-type="indexterm" id="id417"/> Think of edge cases or constraints and put them in the prompt. If you need the code to be optimized for performance or use a certain algorithm, say so: “using O(n) time and O(1) space” or “using a binary search approach.”</p>
	</dd>
	<dt>Avoid ambiguous references</dt>
	<dd>
	<p>Don’t use words like <em>it</em> without a clear antecedent. <a contenteditable="false" data-primary="ambiguous references in prompts" data-type="indexterm" id="id418"/>Instead of “Process it and return the result,” say, “Process the array and return the resulting array.”</p>
	</dd>
	<dt>Name your desired output format</dt>
	<dd>
	<p>If you want the <a contenteditable="false" data-primary="output format, specifying in prompts" data-type="indexterm" id="id419"/>AI to output just code or code with comments or an explanation, you can instruct that: “Give only the code, no explanation” or “Provide code and a brief comment for each step.”</p>
	</dd>
</dl>

<p>A clear prompt sets the AI up for success. If you find the AI’s answers often need a lot of correction, examine whether your prompts might be underspecified.</p>

<p>Here’s what not to do:</p>

<dl>
	<dt>Don’t write a whole novel</dt>
	<dd>
	<p>Long-winded prompts that include irrelevant info can confuse the model or cause it to focus on the wrong thing. Be concise but complete in your description. For instance, you usually don’t need to preface with “You are a world-class programmer...” in a coding context (some people do that in general ChatGPT usage, but for coding tasks, it’s often unnecessary and could add noise).</p>
	</dd>
	<dt>Don’t assume the AI will fill in details by itself correctly</dt>
	<dd>
	<p>If something is important (like thread safety, handling of special characters, etc.), mention it. If it’s not mentioned, assume the AI might not handle it.</p>
	</dd>
	<dt>Avoid open-ended “creative” prompts when you need deterministic outputs</dt>
	<dd>
	<p>For example, saying, “Write some code to analyze data” might cause the AI to guess what analysis you want. Instead, specify:</p>
	</dd>
</dl>

<blockquote>
<p>Calculate the average and standard deviation of a list of numbers.</p>
</blockquote>

<p>In summary, <em>say exactly what you mean</em>. The more the AI “knows” about what you truly want, the better it can deliver. If you find yourself having to correct the AI multiple times, ask: could my initial prompt have been clearer?</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Iterative Refinement: The Feedback Loop with the AI"><div class="sect1" id="ch02_iterative_refinement_the_feedback_loop_with_the_a_1752630042971361">
<h1>Iterative Refinement: The Feedback Loop with the AI</h1>

<p>Even with clear prompts, you won’t always <a contenteditable="false" data-primary="prompts" data-secondary="effective communication with AI" data-tertiary="iterative refinement and feedback loop with AI" data-type="indexterm" id="id420"/>get the perfect answer on the first try. Think of interacting with the AI as a conversation or an iterative development process.<a contenteditable="false" data-primary="iterative refinement of prompts" data-type="indexterm" id="id421"/> This is the feedback loop I touched on in <a data-type="xref" href="ch01.html#ch01_introduction_what_is_vibe_coding_1752630042333281">Chapter 1</a>.</p>

<p>When the AI gives you code, review it critically, just as you would code written by a human. Does it meet the requirements? If not, identify what’s missing or wrong. Then provide feedback or a refined prompt. This can be done in a conversational AI by simply continuing the dialogue, or in an editor by writing another comment for the AI to respond to.</p>

<p>By providing feedback to the AI, you steer it closer to your desired outcome.<a contenteditable="false" data-primary="feedback" data-secondary="feedback loop with AI in prompts" data-type="indexterm" id="id422"/> In a sense, you are <em>training</em> it on the fly for your specific problem. Advanced prompt engineering is like the loop in <a data-type="xref" href="#ch02_figure_2_1752630042957608">Figure 2-2</a>: Prompt → AI output → Review → Refine prompt → AI output →...until satisfied. Keeping each iteration’s changes small is useful; if you overhaul the prompt too much, you may lose some good parts of the previous output.</p>

<figure><div id="ch02_figure_2_1752630042957608" class="figure"><img src="assets/bevc_0202.png" width="489" height="475"/>
<h6><span class="label">Figure 2-2. </span>Advanced prompt engineering loop.</h6>
</div></figure>

<p>For example, you might prompt:</p>

<blockquote>
<p>Write a function that takes a list of integers and returns their sum.</p>
</blockquote>

<p>The AI then returns a function, but its code assumes a non-empty list and doesn’t handle an empty list well. You could then reply:</p>

<blockquote>
<p>That looks good. However, please modify it to return 0 if the list is empty.</p>
</blockquote>

<p>The AI would then update the function accordingly. In this way, you didn’t have to prompt from scratch; you just told the AI to make an adjustment. The AI already had the context of the previous code it gave.</p>

<p>If you’re using an inline assistant, refinement might look like editing the code and perhaps writing a comment like <code># TODO: handle empty list</code> and then seeing if the AI suggests a fix for that.</p>

<p>Another refinement approach is reprompting with more info if the first output wasn’t right. Suppose you said, “Sort a list of names,” and it gave code sorting case-sensitively but you wanted case-insensitive. You could rephrase:</p>

<blockquote>
<p>Sort a list of names case-insensitively.</p>
</blockquote>

<p>Or even:</p>

<blockquote>
<p>The previous code sorts case-sensitively. Modify it to be case-insensitive.</p>
</blockquote>

<p>In debugging, for more<a contenteditable="false" data-primary="debugging" data-secondary="effective prompts to AI for" data-type="indexterm" id="id423"/> complex logic bugs (where no obvious error message is thrown but the output is wrong), you can prompt the AI to walk through the code’s execution. For instance:</p>

<blockquote>
<p>Walk through this function line by line and track the value of total at each step. It’s not accumulating correctly—where does the logic go wrong?</p>
</blockquote>

<p>This is an example of a “rubber duck” debugging prompt: you’re <a contenteditable="false" data-primary="rubber duck debugging prompts" data-type="indexterm" id="id424"/>essentially asking the AI to simulate the debugging process a human might do with prints or a debugger. Such prompts often reveal subtle issues like variables not resetting or incorrect conditional logic, because the AI will spell out the state at each step. If you suspect a certain part of the code, you can zoom in:</p>

<blockquote>
<p>Explain what the filter call is doing here and if it might be excluding more items than it should.</p>
</blockquote>

<p class="fix_tracking">Engaging the AI in an explanatory role can surface the bug in the process of <span class="keep-together">explanation</span>.</p>

<p>After the explanation, it’s often effective to directly ask for what you need:</p>

<blockquote>
<p>What might be causing this issue, and how can I fix it?</p>
</blockquote>

<p>This invites the AI to both diagnose and propose a solution. If the AI’s first answer is unclear or partially helpful, don’t hesitate to ask a follow-up question:</p>

<blockquote>
<p>That explanation makes sense. Can you show me how to fix the code? Please provide the corrected code.</p>
</blockquote>

<p>In a chat setting, the AI has the conversation history, so it can directly output the modified code. If you’re using an inline tool like Copilot in VSCode or Cursor without a chat, you might instead write a comment above the code:</p>

<pre data-type="programlisting">
// BUG: returns NaN, fix this function and see how it autocompletes </pre>

<p>In general, though, the interactive chat yields more thorough explanations.</p>

<p>Another follow-up pattern: if the AI gives a fix but you don’t understand why, ask:</p>

<blockquote>
<p>Can you explain why that change solves the problem?</p>
</blockquote>

<p>This way, you learn for next time, and you double-check that the AI’s reasoning is sound.</p>

<p>LLMs thrive on examples and corrections. <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="thriving on examples and correlations" data-type="indexterm" id="id425"/>If you point out what’s wrong or give a quick example, the AI can incorporate it:</p>

<blockquote>
<p>If input is [], it should return 0, but now it errors.</p>
</blockquote>

<p class="fix_tracking">This iterative process is normal. In fact, trying to cram every detail into one prompt might be less effective than a couple of back-and-forth turns. Use that to your <span class="keep-together">advantage</span>.</p>

<p>Be patient and specific in your feedback. Instead of saying, “No, that’s wrong,” say what’s wrong or what’s needed:</p>

<blockquote>
<p>This code doesn’t handle negative numbers correctly. It should treat them as 0 in the sum.</p>
</blockquote>

<p>Also, if the AI goes off track, you can steer it back: sometimes resetting or rephrasing is easier than trying to salvage a very incorrect attempt. Use your judgment. If the AI output shows that it is completely misunderstanding you, clarify your prompt from scratch.</p>

<p>As you refine, you’ll also learn how the AI interpreted your prompt. This can inform how you write future prompts. You might realize, “Oh, it took ‘login system’ to mean an entire UI. Next time I’ll specify backend only.”</p>

<p>Think of it like debugging code: if the AI output is wrong, the “bug” might be in your prompt, not in the AI’s processing. Just as you’d examine and fix your code when it produces incorrect results, you should refine your prompts when the AI generates unexpected or incorrect output. The conversation between you and the AI is like a debugging session where each exchange helps you pinpoint and fix the issue.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Comparing Two Prompts"><div class="sect1" id="ch02_comparing_two_prompts_1752630042971413">
<h1>Comparing Two Prompts</h1>

<p>Imagine<a contenteditable="false" data-primary="prompts" data-secondary="effective communication with AI" data-tertiary="comparison of two prompts" data-type="indexterm" id="ix_prmptcmp"/> you have a simple Node.js function meant to convert a list of user objects to a lookup map by user ID. However, it’s throwing an error. Here’s the buggy code:</p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="c1">// Buggy function: converts array of users to a map by ID</code>
<code class="kd">function</code> <code class="nx">mapUsersById</code><code class="p">(</code><code class="nx">users</code><code class="p">)</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="nx">userMap</code> <code class="o">=</code> <code class="p">{};</code>
  <code class="k">for</code> <code class="p">(</code><code class="kd">let</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;=</code> <code class="nx">users</code><code class="p">.</code><code class="nx">length</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>  
    <code class="c1">// Note: using &lt;= will go out of bounds on the last iteration</code>
    <code class="kr">const</code> <code class="nx">user</code> <code class="o">=</code> <code class="nx">users</code><code class="p">[</code><code class="nx">i</code><code class="p">];</code>
    <code class="nx">userMap</code><code class="p">[</code><code class="nx">user</code><code class="p">.</code><code class="nx">id</code><code class="p">]</code> <code class="o">=</code> <code class="nx">user</code><code class="p">;</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="nx">userMap</code><code class="p">;</code>
<code class="p">}</code>

<code class="c1">// Example usage:</code>
<code class="kr">const</code> <code class="nx">result</code> <code class="o">=</code> <code class="nx">mapUsersById</code><code class="p">([{</code> <code class="nx">id</code><code class="o">:</code> <code class="mi">1</code><code class="p">,</code> <code class="nx">name</code><code class="o">:</code> <code class="s2">"Alice"</code> <code class="p">}]);</code></pre>

<p>This code is supposed to iterate through the user’s array and build an object where keys are user IDs. However, there is a bug: the loop uses <code>i &lt;= users.length</code> instead of <code>i &lt; users.length</code>. This will cause <code>users[i]</code> to be undefined on the last iteration, likely leading to a runtime error (like attempting to access the property ID of <span class="keep-together"><code>undefined</code></span>). Let’s see how an AI assistant might respond to two different prompts about this issue (see <a data-type="xref" href="#ch02_figure_3_1752630042957630">Figure 2-3</a>).</p>

<figure><div id="ch02_figure_3_1752630042957630" class="figure"><img src="assets/bevc_0203.png" width="588" height="790"/>
<h6><span class="label">Figure 2-3. </span>Prompt quality impact comparison: vague prompts produce generic or incorrect code, while specific, well-structured prompts generate accurate, contextually appropriate solutions.</h6>
</div></figure>

<section data-type="sect2" data-pdf-bookmark="Poor prompt"><div class="sect2" id="ch02_poor_prompt_1752630042971462">
<h2>Poor prompt</h2>

<p>Here’s an example<a contenteditable="false" data-primary="poor prompts, example of" data-type="indexterm" id="id426"/> of a poor prompt:</p>

<blockquote>
<p>Why isn’t my mapUsersById function working?</p>
</blockquote>

<p>AI’s response to the poor prompt (simulated):</p>

<blockquote>
<p>It’s hard to say without more information. Are you sure the function is being called with a proper array of users? Perhaps the issue is that users is empty or not an array, causing the loop not to execute properly. It could also be that there’s an error when setting the property on userMap. Make sure that each user has an id property. If users is undefined or null, that would also cause it to not work.</p>
</blockquote>

<p>This answer is quite vague. The prompt gave no details except the function name, so the AI is guessing at generic causes (“Maybe the array is empty or not an array,” “Make sure each user has an ID”). It did not identify the actual bug.</p>

<p>This is the kind of response you get when the question is underspecified—the model can only regurgitate common debugging tips. If you’re a developer, this isn’t very helpful because it doesn’t address the real mistake. The lack of context and specificity in the prompt yields a generic answer: essentially, the AI equivalent of “Have you tried turning it off and on again?” This aligns with the community advice that vague prompts lead to irrelevant or generic answers.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Improved prompt"><div class="sect2" id="ch02_improved_prompt_1752630042971508">
<h2>Improved prompt</h2>

<p>Let’s try this <a contenteditable="false" data-primary="poor prompts, example of" data-secondary="improving the prompt" data-type="indexterm" id="id427"/>again:</p>

<blockquote>
<p>I have a JavaScript function <code>mapUsersById</code> that should convert an array of user objects into a <code>map (object)</code> keyed by user ID. However, it throws an error when I run it. For example, when I pass <code>[ {id: 1, name: "Alice"} ]</code>, I get <code>TypeError: Cannot read property 'id' of undefined</code>. Here is the function code:</p>

<pre data-type="programlisting">
function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}</pre>
<p>It should return <code>{ "1": {id: 1, name: "Alice"} }</code>. What is the bug and how can I fix it?</p>
</blockquote>

<p>The AI’s response to the improved prompt (simulated):</p>

<blockquote>
<p>The error Cannot read property <code>'id'</code> of undefined suggests that at some point, user is undefined, and then you try to access user.id. Looking at your loop, the issue is with the loop bounds. You are using <code>i &lt;= users.length</code>. If <code>users.length</code> is <code>1</code> (for one user in the array), the loop runs with <code>i = 0</code> and <code>i = 1</code>. When <code>i</code> is <code>1</code>, <code>users[1]</code> is undefined (since the only index in the array is 0). Thus user becomes undefined on that iteration, and <code>user.id</code> throws a <code>TypeError</code>. The fix is to change the loop condition to <code>i &lt; users.length</code> so that <code>i</code> goes from <code>0</code> to <code>users.length-1</code>. For example:</p>

<pre data-type="programlisting">
for (let i = 0; i &lt; users.length; i++) {
  const user = users[i];
  userMap[user.id] = user;
}</pre>
<p>This way, <code>i</code> will be <code>0</code> for the first (and only) element, and then the loop stops. With that change, your function will no longer hit an undefined user and should work as expected, returning <code>{"1": {id: 1, name: "Alice"}}</code> for the given input.</p>
</blockquote>

<p>This response directly pinpoints the bug (using <code>&lt;=</code> in the loop) and explains why it causes the user to be undefined. It then provides the correct code solution (<code>i &lt; users.length</code>) and confirms the expected outcome. The improved prompt succeeded because it followed our principles: it specified the language (JavaScript), described the function’s purpose, included the exact error message and a sample input, and even provided the code snippet in question.</p>

<p>Notice how the AI used the error message as a clue to focus on the loop bounds—a targeted prompt enabled the AI to engage in true problem solving, effectively simulating how a human debugger would think: “Where could undefined come from? Likely from the loop indexing.” This is a concrete demonstration of the benefit of detailed prompts.<a contenteditable="false" data-primary="prompts" data-secondary="effective communication with AI" data-startref="ix_prmptcmp" data-tertiary="comparison of two prompts" data-type="indexterm" id="id428"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Prompting Techniques: A Toolbox for Effective Communication"><div class="sect1" id="ch02_prompting_techniques_a_toolbox_for_effective_comm_1752630042971568">
<h1>Prompting Techniques: A Toolbox for <span class="keep-together">Effective Communication</span></h1>

<p>Now let’s get into specific techniques<a contenteditable="false" data-primary="prompting techniques" data-type="indexterm" id="ix_prmpttch"/> that can supercharge your prompting skills. These are like patterns or recipes you can use when a straightforward instruction isn’t enough or when you want to guide the AI in a certain way.</p>

<p>By mastering these techniques, you can handle an array of situations: instructing the AI in plain English, giving it examples, making it explain or structure its output, or setting it into different mindsets or roles. All of these help you guide the AI to produce exactly what you need.</p>

<p>Prompting techniques are not mutually exclusive; you will often use several together for best results, especially on complex tasks.</p>

<div data-type="tip">
<h1>A Note on Style</h1>

<p>When you use these techniques, adapt your tone to the model. <a contenteditable="false" data-primary="models (AI)" data-secondary="adapting your tone in prompts to" data-type="indexterm" id="id429"/>Many models respond well to polite or neutral instructions. You don’t need to use archaic or overly formal language. Direct but polite often works: “Please do X” or “Let’s do Y.” For example, with chain-of-thought (CoT) prompting, a popular phrase is “Let’s think step-by-step.” Models like GPT-4 recognize this as a cue to show reasoning.</p>
</div>

<section data-type="sect2" data-pdf-bookmark="Zero-Shot Prompting"><div class="sect2" id="ch02_zero_shot_prompting_1752630042971628">
<h2>Zero-Shot Prompting</h2>

<p><em>Zero-shot</em> prompting is <a contenteditable="false" data-primary="prompting techniques" data-secondary="zero-shot prompting" data-type="indexterm" id="id430"/>simply asking <a contenteditable="false" data-primary="zero-shot prompting" data-type="indexterm" id="id431"/>the model to do something without providing any examples or additional guidance beyond the instruction. Essentially, the model is solving the task from “zero” examples.</p>

<p><em>When to use:</em> This is the most common scenario: you just ask for what you want in plain language. If the task is standard and the prompt is clear, this is often sufficient.</p>

<p><em>Example:</em></p>

<blockquote>
<p>Write a Python function that checks if a number is prime.</p>
</blockquote>

<p>This is zero shot. The AI will likely produce a prime-checking function using a loop or trial division.</p>

<p><em>Pros:</em> It’s quick and relies on the model’s learned knowledge. Modern models are surprisingly good at zero-shot responses for many programming tasks, especially if they’re common (like prime checking, sorting, or string manipulation).</p>

<p><em>Cons:</em> If the task is unusual or output format is specific, zero shot might yield a result that doesn’t quite match what you need on the first try, because the model might have multiple ways to interpret it.</p>

<p>Usually, it’s a good idea to try zero shot first for simple things. If the result is off, you may then shift to refining or other techniques.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="One-Shot and Few-Shot Prompting"><div class="sect2" id="ch02_one_shot_and_few_shot_prompting_1752630042971678">
<h2>One-Shot and Few-Shot Prompting</h2>

<p><em>One-shot</em> prompting means you<a contenteditable="false" data-primary="prompting techniques" data-secondary="one-shot and two-shot prompting" data-type="indexterm" id="id432"/> provide <a contenteditable="false" data-primary="one-shot prompting" data-type="indexterm" id="id433"/>exactly one example of what you want (input and desired output) as part of the prompt; <em>few-shot</em> prompting means providing a few examples (typically two to five) before asking the model to perform the task on a new input.<a contenteditable="false" data-primary="few-shot prompting" data-type="indexterm" id="id434"/></p>

<p>This is like showing the model, “Here’s how I solve one instance. Now you do the next one similarly.”</p>

<p><em>When to use:</em> This type of prompting is useful when the model might not know exactly the format or style you need or when the task is a bit unusual. By giving examples, you reduce ambiguity.</p>

<p><em>Example (one shot):</em> Suppose you’re using a language or a certain style that the model might not have seen as much. Let’s say you want pseudocode in a specific format. Your prompt might be:</p>

<blockquote>
<p>Convert the following English instructions to Python-like pseudocode.</p>
</blockquote>

<p>Example instruction: “Calculate the factorial of n”:</p>

<pre data-type="programlisting">
Example pseudocode:

function factorial(n):

    if n &lt;= 1:

        return 1

    else:

        return n * factorial(n-1)

Instruction: "Find the largest number in a list"

Pseudocode:</pre>

<p>You’ve provided one example (factorial) and the format you want. Now the model is more likely to produce pseudocode for the “largest number” instruction in a similar format (with a function, with if/else or loop logic as needed).</p>

<p><em>Example (few shot):</em> Let’s say you want the AI to use a specific algorithm. You might give it a smaller example of that algorithm in action as a hint. Or if the task has multiple correct answers but you prefer a certain one, an example can push it toward that.</p>

<p>Few-shot prompting is powerful for formatting; for instance:</p>
<blockquote>
	<p>Convert The Following English Statements To SQL Queries.\N1."Get All Employees Hired After 2020” → Select * From Employees Where Hire_Date &gt; ’2020-01-01’;\N2. “List Customer Names Who Made A Purchase In The Last Month” → Select Name From Customers Join Purchases On ... Where Purchase_Date &gt; ...;\N3. “Count Of Products That Are Out Of Stock” →</p>
</blockquote>

<p>Here, once you give two examples of English-to-SQL, the AI is likely to answer the third query correctly by following the pattern. Few-shot examples can be applied to coding too: show the AI the style you want in a small sample and then ask for more. It’s like giving it a minitraining dataset within your prompt.</p>

<p><em>Pros:</em> You can achieve outputs in very specific styles. This technique also helps the model handle tasks that involve following a pattern or applying a concept repetitively.</p>

<div data-type="tip">
<h1>Context Window</h1>

<p>The term <em>context window</em> refers to the maximum <a contenteditable="false" data-primary="context window" data-type="indexterm" id="id435"/>amount of text (measured in tokens) that a language model can process in a single interaction, including both the input prompt and the generated response. This represents a limitation of current AI models—once you reach this threshold, the model cannot process additional information. When you’re crafting prompts, everything you include (instructions, examples, data, and the space needed for the response) must fit within this fixed capacity.</p>
</div>

<p><em>Cons:</em> Few-shot prompting in particular makes the prompt longer (which uses up context window). For very large/complex examples, it might “eat” a lot of the model’s capacity. But usually a small example or two is fine.</p>

<p><em>Tip:</em> If you want the model to strictly adhere to a certain output structure, giving an example can nearly guarantee it matches that structure rather than offering a free-form reply that you have to parse.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Chain-of-Thought Prompting"><div class="sect2" id="ch02_chain_of_thought_cot_prompting_1752630042971730">
<h2>Chain-of-Thought Prompting</h2>

<p><em>Chain-of-thought</em> (CoT) prompting involves asking the model to think step-by-step or show its reasoning before giving the final answer.<a contenteditable="false" data-primary="prompting techniques" data-secondary="chain-of-thought prompting" data-type="indexterm" id="ix_prmpttchCoT"/><a contenteditable="false" data-primary="chain-of-thought (CoT) prompting" data-type="indexterm" id="ix_chnoth"/> In other words, you encourage the model to break down the problem.</p>

<p><em>When to use:</em> This is useful for complex problems that involve reasoning and multistep computations, or when you suspect the model might make a mistake if it jumps straight to the answer. It’s also useful if you want an explanation in the output.</p>

<div data-type="tip">
<h1>Combinatorial Math</h1>

<p><em>Combinatorial math</em> deals with counting, arranging, and selecting objects according to specific rules or constraints. <a contenteditable="false" data-primary="combinatorial math" data-type="indexterm" id="id436"/>Common problems include calculating permutations (arrangements where order matters), combinations (selections where order does not matter), and other counting principles. Key notation includes “n choose k” (written as C(n,k) or nCk), which represents the number of ways to choose k items from n total items, calculated using the formula n!/(k!(n-k)!). These calculations frequently appear in probability, statistics, and discrete mathematics applications.</p>
</div>

<p><em>Example:</em> Instead of just asking a <a href="https://oreil.ly/vI3V5">combinatorial math problem</a> like “What is 12 choose 4?” and getting an answer, you might say:</p>

<blockquote>
<p>Solve 12 choose 4 step-by-step.</p>
</blockquote>

<p>The model might then outline:</p>

<pre data-type="programlisting">
12 choose 4 = 12!/(4!*8!) = ... = 495</pre>

<p>In coding, CoT can be useful for tricky algorithmic tasks. You might try the following prompt:</p>

<blockquote>
<p>Explain step-by-step how to merge two sorted lists, then provide the Python code.</p>
</blockquote>

<p>The model would first outline something like:</p>

<blockquote>
<p>We will use two pointers starting at the heads of each list, compare the elements, append the smaller to a result list, and move that pointer, and so on…</p>
</blockquote>

<p>Then it might give the code. This ensures it has structured the solution correctly before coding.</p>

<p>Another use is debugging or understanding output:</p>

<blockquote>
<p>Walk through the logic to determine if the number 19 is prime, then give the result.</p>
</blockquote>

<p>The model might list divisions by primes, then conclude:</p>

<blockquote>
<p>19 is prime.</p>
</blockquote>

<p><em>Pros:</em> Improves correctness on tasks requiring reasoning. There’s <a href="https://oreil.ly/t7flF">research evidence</a> that prompting the model to “think out loud” can lead to better results on math and logic tasks.<a contenteditable="false" data-primary="thinking out loud, prompting AI models to" data-type="indexterm" id="id437"/> It also gives you insight into the model’s process, which can be instructive or help you trust the answer more.</p>

<p><em>Cons:</em> The output is longer (which might not be what you want in final code). Also, some interfaces (like typical code completions) aren’t set up to show reasoning separate from code. This technique is more common in Q&amp;A or chat scenarios. However, you can instruct the model to include the reasoning as comments in the code, which is a neat way to get thoroughly commented code.<a contenteditable="false" data-primary="prompting techniques" data-secondary="chain-of-thought prompting" data-type="indexterm" id="id438"/><a contenteditable="false" data-primary="chain-of-thought (CoT) prompting" data-startref="ix_chnoth" data-type="indexterm" id="id439"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Role Prompting"><div class="sect2" id="ch02_role_prompting_1752630042971778">
<h2>Role Prompting</h2>

<p><em>Role prompting</em> means you ask the AI to assume a certain identity or role that might influence how it responds.<a contenteditable="false" data-primary="prompting techniques" data-secondary="role prompting" data-type="indexterm" id="id440"/></p>

<p><em>When to use:</em> This is useful when you want to influence the style or detail of the answer or get a certain perspective. For instance, an AI taking on an “expert” role might give a more advanced solution or more explanation, while a “beginner” role might make it explain more basic concepts.</p>

<p><em>Examples:</em></p>

<ul>
	<li>
	<p>You are a Python instructor.<a contenteditable="false" data-primary="instructor role, assigning to AI" data-type="indexterm" id="id441"/> Explain the following code and then modify it to be more Pythonic.</p>
	</li>
	<li>
	<p>Act as a security analyst. Here’s some code. Identify any security vulnerabilities.<a contenteditable="false" data-primary="security analyst role, assigning to AI" data-type="indexterm" id="id442"/></p>
	</li>
	<li>
	<p>Pretend you are a linter that checks code for style issues.</p>
	</li>
</ul>

<p>This can significantly affect the response. Assigning the AI a security analyst role might make it focus on things it otherwise wouldn’t mention (like data validation, secure coding practices, or potential vulnerabilities). An instructor role might make it provide clearer explanations and perhaps not assume prior knowledge.</p>

<p>In coding, you might say before asking for code:</p>

<blockquote>
<p>You are an expert C++ programmer well-versed in optimization, instructing a junior developer.</p>
</blockquote>

<p>The result will likely use more advanced C++ features and explain why certain choices were made, balancing technical sophistication with educational clarity.</p>

<p><em>Pros:</em> This technique steers the tone and depth of the answer. This can tailor the solution to a certain level of complexity or thoroughness. It’s useful if you want either a very simple solution (tell it to act as a novice and maybe it’ll avoid complex tricks) or a very optimized one (tell it to act as a performance guru).</p>

<p><em>Cons:</em> Sometimes the model might focus more on the persona than needed (an “instructor” might start explaining things you already know). <a contenteditable="false" data-primary="safety systems (AI), sensitivity to role descriptions" data-type="indexterm" id="id443"/>Also, some AI safety systems are more sensitive to certain role descriptions—particularly those that might suggest deception, authority impersonation, or potentially harmful activities—though straightforward technical and professional roles like “data analyst” or “software engineer” typically work without issues.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Contextual Prompting"><div class="sect2" id="ch02_contextual_prompting_1752630042971826">
<h2>Contextual Prompting</h2>

<p><em>Contextual prompting</em> means giving the AI additional context or information beyond the immediate task description.<a contenteditable="false" data-primary="contextual prompting" data-type="indexterm" id="id444"/><a contenteditable="false" data-primary="prompting techniques" data-secondary="contextual prompting" data-type="indexterm" id="id445"/> AI models don’t have persistent memory of your entire project unless you provide it in the prompt (or through some integrated context window in advanced IDE integrations). So if you want the AI to write code that fits into your existing codebase, give it that context. Basically, you supply relevant data or background as part of the prompt.</p>

<p><em>When to use:</em> Use when solving a problem requires knowing certain data or definitions that the model might not know or might not recall correctly from training. Or use when you want to ensure consistency with some external info (like an API spec or previous part of conversation).</p>

<p><em>Examples:</em></p>

<p>If you have a data structure and you want code that works with it, you might paste its definition:</p>

<pre data-type="programlisting">
Given the class below, implement the function X.

class Node:

    def __init__(self, value, next=None):

        self.value = value

        self.next = next

# Now write a function to count the nodes in a linked list starting at head.</pre>

<p>By including the class definition, you make the AI much more likely to use <code>Node.value</code> and <code>Node.next</code> properly in its code.</p>

<p>If you want to use a specific API, include a snippet of the documentation in the prompt:</p>

<blockquote>
<p>Using the requests library, fetch the data from the API. (The API returns JSON with format: {...})</p>
</blockquote>

<p>If you include even a short example of API usage from docs, the AI can mimic it.</p>

<p>For disambiguation:</p>

<blockquote>
<p>Using the term <code><em>student</em></code> to refer to high school students, write a function that…</p>
</blockquote>

<p>If <em>student</em> could be ambiguous in context, you’ve clarified it.</p>

<p><em>Pros:</em> You’re grounding the AI in the context you care about. It’s less likely to make wrong assumptions if you supply the facts. This is extremely helpful if the AI otherwise might not remember or know your specific use-case details.</p>

<p><em>Cons:</em> This technique makes prompts longer. Also, the model might occasionally regurgitate the provided context into the answer (like copying lines from a documentation snippet into the code if not careful). But usually it uses it appropriately.</p>

<p><em>Tip:</em> If you have a large context (like a big schema or many lines of code), sometimes it’s better to summarize the key elements for the model rather than including everything verbatim. This approach helps you stay within context limits while ensuring the model receives the most relevant information. However, if the content is small enough, just include it raw.</p>

<p>Constraints are also<a contenteditable="false" data-primary="constraints" data-secondary="mentioning in contextual prompts" data-type="indexterm" id="id446"/> useful to <a contenteditable="false" data-primary="performance constraints, mentioning in prompts" data-type="indexterm" id="id447"/>mention: performance constraints (“Optimize for O(n log n) or better”), compatibility <a contenteditable="false" data-primary="compatibility constraints, mentioning in prompts" data-type="indexterm" id="id448"/>constraints (“Must run on Python 3.8”), or library choices (“Use standard library only, no external dependencies”). These act like guardrails and ensure the AI doesn’t suggest something outside acceptable bounds.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Metaprompting"><div class="sect2" id="ch02_metaprompting_1752630042971875">
<h2>Metaprompting</h2>

<p><em>Metaprompting</em> is giving instructions about the output itself, not just what the solution should do.<a contenteditable="false" data-primary="prompting techniques" data-secondary="metaprogramming" data-type="indexterm" id="id449"/><a contenteditable="false" data-primary="metaprogramming" data-type="indexterm" id="id450"/> It’s like telling the AI how to format or approach the solution.<a contenteditable="false" data-primary="output" data-secondary="giving instructions about in metaprogramming prompts" data-type="indexterm" id="id451"/></p>

<p><em>When to use:</em> Useful when you need the answer in a specific format or style or when you want to control how the AI works through the problem.</p>

<p><em>Examples:</em></p>

<blockquote>
<p>First, explain the approach in two sentences, then provide the code.</p>
</blockquote>

<p>This ensures the AI doesn’t launch straight into code:</p>

<blockquote>
<p>Do not use any libraries in the solution.</p>
</blockquote>

<p>This places a constraint on the solution:</p>

<blockquote>
<p>Format the output as JSON.</p>
</blockquote>

<p>This is useful if you’re using the AI to produce data, not code:</p>

<blockquote>
<p>Only provide the function body, without the definition line.</p>
</blockquote>

<p>This is handy if you want to insert the function into existing code:</p>

<blockquote>
<p>If the input is invalid, instead of error, return None.</p>
</blockquote>

<p>This is not exactly the output format, but it’s instructing the AI how to behave for certain cases.</p>

<p><em>Pros:</em> You get exactly what you need, how you need it, without extra editing. This is crucial for some scenarios. If you plan to automatically use the AI’s output in a pipeline, then you really want consistent formatting.</p>

<p><em>Cons:</em> If the instructions conflict with the model’s default style, sometimes it might partially follow them or you have to emphasize them. For instance, even if you say “only code, no explanation,” occasionally the model might include a tiny comment or so. Usually, phrasing it as a direct imperative helps:</p>

<blockquote>
<p>Do not include any explanation; output only code inside a single code block.</p>
</blockquote>

<p>Models like GPT follow that quite well.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Self-Consistency (Multiple Outputs and Majority Voting)"><div class="sect2" id="ch02_self_consistency_multiple_outputs_and_majority_vo_1752630042971935">
<h2>Self-Consistency (Multiple Outputs and Majority Voting)</h2>

<p><em>Self-consistency</em> is more of a strategy than a prompt style.<a contenteditable="false" data-primary="self-consistency" data-type="indexterm" id="id452"/><a contenteditable="false" data-primary="output" data-secondary="getting multiple outputs from same prompt and choosing best one" data-type="indexterm" id="id453"/><a contenteditable="false" data-primary="prompting techniques" data-secondary="self-consistency" data-type="indexterm" id="id454"/> The idea is to get multiple outputs for the same prompt and then decide on the best or most common one. As <a href="https://oreil.ly/fHABW">Sander Schulhoff of Learn Prompting notes</a>, self-consistency leverages the notion that if you ask the model multiple times (with slight randomness) and many of its answers agree, that consensus is likely correct.</p>

<p><em>When to use:</em> This is useful for complex problems where you’re unsure the model’s first answer is correct, especially if you can’t verify it easily yourself, or if you want a confidence check from the AI by seeing whether it gives the same answer repeatedly.</p>

<p><em>How to use manually:</em> On some platforms (like ChatGPT), you can click “Regenerate answer.” Or you can copy the prompt into a new session and see if it gives the same result. If you get three answers and two are the same and one is different, you might trust the two (assuming the problem has a single correct answer).</p>

<p>In programming context, if it’s generating code for something deterministic, usually it will give very similar code each time (with small variations in variable names or style). But if it’s an algorithmic question (like “What’s the output of this code?”), you could check multiple runs.</p>

<p>This technique is more powerful in noncoding tasks (like logic puzzles) but worth noting.</p>

<p><em>Another angle—ensemble prompting:</em> You can actually ask the model within one prompt to consider multiple possibilities:</p>

<blockquote>
<p>Give two different solutions to this problem.</p>
</blockquote>

<p>Then perhaps you can see which one you like or test both. This is like self-consistency in one shot because you get multiple answers.</p>

<p><em>Pros:</em> This technique can increase confidence in the solution if multiple attempts converge. Also, you might get variety (which is good if you want to choose the most elegant solution among many).</p>

<p><em>Cons:</em> It’s time-consuming to do multiple calls and compare outputs.</p>

<p>In practice, if I’m unsure about an answer, I’ll often repose the question differently to see if I get the same answer. If I do, I’m more confident it’s correct.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="ReAct (Reason + Act) Prompting"><div class="sect2" id="ch02_react_reason_act_prompting_1752630042971986">
<h2>ReAct (Reason + Act) Prompting</h2>

<p><em>ReAct</em> is a more advanced prompting technique that <a href="https://arxiv.org/abs/2210.03629">combines <em>reasoning</em> and <em>acting</em></a>. It gets the <a contenteditable="false" data-primary="ReAct (Reason + Act) prompting" data-type="indexterm" id="id455"/>model <a contenteditable="false" data-primary="prompting techniques" data-secondary="ReAct" data-type="indexterm" id="id456"/>not <a contenteditable="false" data-primary="actions and reasoning in ReAct prompts" data-type="indexterm" id="id457"/>only to think, like CoT does, but also to take actions like making a calculation, calling an API, or using a tool. (See the <a href="https://oreil.ly/P_KIV">ReAct Prompt Engineering Guide</a> for more). In current practice, this is often used with frameworks like LangChain, where the AI can output a special format that a program interprets as an action (like a command to execute or a query to run), then feed the result back.</p>

<p>For our scope (without such an execution environment in the loop), you can still do a form of ReAct by instructing the AI to first outline a plan, then output the result. It’s similar to CoT but specifically oriented to using tools or performing subtasks.</p>

<p><em>Example:</em></p>

<blockquote>
<p>Using Python, determine the current weather in Paris and print it.</p>
</blockquote>

<p>Unless the AI has browsing capabilities, it cannot truly get the current weather. A ReAct approach would have the AI first reason through the problem by stating:</p>

<blockquote>
<p>I need to access current weather data for Paris, which requires calling a weather API.</p>
</blockquote>

<p>The AI would then attempt to use an available tool to make this API call. If successful, it would receive actual weather data; if no such tool is available, it might acknowledge the limitation or work with hypothetical data. Finally, the AI would write the Python code to display the weather information, incorporating whatever data it was able to obtain through this reasoning and action process.</p>

<p>Without external tool access, ReAct might not be particularly relevant for simple prompting tasks. However, when evaluating AI tools for your organization, determining whether they can access current information from the internet represents a critical capability assessment. Many AI models operate with knowledge cutoffs, meaning their training data only extends to a specific date, which can result in outdated information for rapidly changing topics.</p>

<p class="pagebreak-before less_space">If you are using an environment where the AI can execute code (such as Jupyter integrations or similar platforms), you could implement ReAct by instructing the system:</p>

<blockquote>
<p>First write a test for this function, run it, then adjust the code accordingly.</p>
</blockquote>

<p>This demonstrates the ReAct pattern through a reasoning step (writing the test), followed by an action (executing the test), and then code adjustment based on the results. However, orchestrating such workflows through pure prompts requires advanced prompting techniques and appropriate technical infrastructure.</p>

<p><em>Simpler use:</em> You can simulate a Q&amp;A where the AI has intermediate steps that mimic actions:</p>

<blockquote>
<p>Think step-by-step and if you need to, do calculations.</p>
</blockquote>

<p>It’s effectively CoT but with a more imperative tone.<a contenteditable="false" data-primary="chain-of-thought (CoT) prompting" data-secondary="ReAct prompting and" data-type="indexterm" id="id458"/></p>

<p><em>Pros:</em> When available, it can solve problems that require external info or iterative trial (like the AI can correct itself by actually running code). In debugging contexts, an AI that can execute code to test it is fantastic.</p>

<p><em>Cons:</em> This technique is not widely accessible without specific tooling. And if you just prompt that way in plain ChatGPT, it will either imagine the actions or just do CoT.</p>

<p>For our purposes in prompt writing, keep in mind that some systems (like OpenAI’s tool-using agents or others) exist, but in vanilla prompting we mostly do CoT, and we ourselves handle actions like running the code or tests.<a contenteditable="false" data-primary="prompting techniques" data-startref="ix_prmpttch" data-type="indexterm" id="id459"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Advanced Prompting: Combining Techniques and Handling Complexity"><div class="sect1" id="ch02_advanced_prompting_combining_techniques_and_handl_1752630042972043">
<h1>Advanced Prompting: Combining Techniques <span class="keep-together">and Handling Complexity</span></h1>

<p>Prompting techniques can be combined.<a contenteditable="false" data-primary="prompting techniques" data-secondary="advanced prompting, combining techniques and handling complexity" data-type="indexterm" id="ix_prmpttchcmb"/> For instance, you might do a few-shot prompt that also demonstrates CoT in the examples. Or you might combine a role with CoT:</p>

<blockquote>
<p>As a senior engineer, think step-by-step through the problem, then give the code.</p>
</blockquote>

<p>Now that we’ve explored various prompting techniques, let’s see them in action with a scenario or two, then discuss how to review and refine the AI’s output (which leads into the next chapter about understanding and owning the generated code).</p>

<p>Imagine you have a function that isn’t working. You might use a combination of role and CoT prompting:</p>

<blockquote>
<p>You are a Python debugger. Let’s think step-by-step to find the bug in the following code.</p>
</blockquote>

<p>This would be followed by the code. The AI might respond with an analysis of each line and pinpoint the bug.</p>

<p>Or let’s say you want to generate code for a somewhat complex algorithm, ensure it’s well commented, and also get test cases for it. A combined prompt might look like this:</p>

<blockquote>
<p>You are an expert Python developer. Let’s solve this step-by-step. We need a function <code>merge_sorted_lists(list1, list2)</code> that merges two sorted lists into one sorted list. First, explain the approach, then provide the Python code with comments. After that, give <span class="keep-together">2–3</span> example tests in code to demonstrate it works.</p>
</blockquote>

<p>This single prompt is quite comprehensive. The first sentence sets a role. The second requests step-by-step reasoning. The third gives the main task. The fourth sentence asks for code with explanatory comments, and the fifth even asks for tests.</p>

<p>The AI might then output an explanation, then the code with inline comments, then some test cases at the end. This is an advanced use, but it shows how you can direct the AI through a multifaceted response.</p>

<section data-type="sect2" data-pdf-bookmark="Know the Model’s Limits"><div class="sect2" id="ch02_know_the_model_s_limits_1752630042972101">
<h2>Know the Model’s Limits</h2>

<p>Prompt engineering also involves knowing what <em>not</em> to ask and how to avoid pitfalls.<a contenteditable="false" data-primary="models (AI)" data-secondary="knowing limits of" data-type="indexterm" id="id460"/><a contenteditable="false" data-primary="prompting techniques" data-secondary="advanced prompting, combining techniques and handling complexity" data-tertiary="knowing model's limits" data-type="indexterm" id="id461"/> If a prompt is getting too large or includes too many instructions, the model might get confused or truncate some output. If you find it starts ignoring parts of your prompt, you might need to simplify or do it in parts. If an AI model sometimes produces incorrect facts or code (it “hallucinates”), you learn to double-check and not use it as a factual oracle. If you find it tends to give overly verbose code, you can preempt that with “Make the solution as concise as possible.” If it sometimes uses functions that don’t exist, you might instruct, “Use only the API functions listed below” and list them.<a contenteditable="false" data-primary="complexity" data-secondary="breaking up complex tasks in prompts" data-type="indexterm" id="id462"/> The better you understand the AI’s behavior, the more you can mold your prompts to get around any weaknesses.</p>

<p>If a task is very complex, you can also break it into subtasks for the AI. For example, you might first prompt:</p>

<blockquote>
<p>List the steps to implement a basic compiler for a simple arithmetic expression <span class="keep-together">language</span>.</p>
</blockquote>

<p>Once the AI gives the steps, you tackle each step with separate prompts, maybe even in separate files or sessions:</p>

<blockquote>
<p>Now implement step 1: tokenization.</p>
</blockquote>

<p>This is like doing system design with the AI: you can outline then refine each piece. It leverages the AI’s ability to assist in planning (not just coding).</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Stateful Conversation Versus One-Shot Prompting"><div class="sect2" id="ch02_stateful_conversation_versus_one_shot_prompting_1752630042972152">
<h2>Stateful Conversation Versus One-Shot Prompting</h2>

<p>In a chat setting, you have a conversation history, known as <em>state</em>. <a contenteditable="false" data-primary="state" data-type="indexterm" id="id463"/><a contenteditable="false" data-primary="stateful conversation versus one-shot prompting" data-type="indexterm" id="id464"/>You can build up context by discussing with the AI.<a contenteditable="false" data-primary="conversation, stateful, versus one-shot prompting" data-type="indexterm" id="id465"/><a contenteditable="false" data-primary="one-shot prompting" data-secondary="stateful conversation versus" data-type="indexterm" id="id466"/> In an IDE completion setting, the context is mostly your file content and comments. Both allow cumulative context in different ways. Use conversation if you need the AI to remember what was said (like refining an answer). Use fresh prompts or file context if you want to ensure it’s focusing only on what’s relevant now. Sometimes wiping away the context prevents the model from sticking to a potentially wrong earlier assumption.</p>

<p>By practicing with these techniques on various examples, you’ll become adept at knowing which approach to use and when:</p>

<ul>
	<li>
	<p class="fix_tracking">If output format is important, give examples (few shot) or explicit formatting <span class="keep-together">instructions</span>.</p>
	</li>
	<li>
	<p>If logic is tricky, use CoT or step-by-step.</p>
	</li>
	<li>
	<p>If the solution can vary in quality, set a role (like “seasoned engineer”) to get a better style.</p>
	</li>
	<li>
	<p>If the model isn’t complying, maybe break your prompts into pieces, simplify them, or use stronger wording for constraints.</p>
	</li>
</ul>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Common Prompt Antipatterns and How to Avoid Them"><div class="sect2" id="ch02_common_prompt_antipatterns_and_how_to_avoid_them_1752630042972205">
<h2>Common Prompt Antipatterns and How to Avoid Them</h2>

<p>Not all prompts are created equal. <a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-type="indexterm" id="ix_prmptanti"/>By now, we’ve seen numerous examples of effective prompts, but it’s equally instructive to recognize antipatterns—common mistakes that lead to poor AI responses. This section covers some frequent prompt failures and how to fix them.</p>

<section data-type="sect3" data-pdf-bookmark="The vague prompt"><div class="sect3" id="ch02_the_vague_prompt_1752630042972254">
<h3>The vague prompt</h3>

<p>This is the<a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="vague prompts" data-type="indexterm" id="id467"/><a contenteditable="false" data-primary="vague prompts" data-type="indexterm" id="id468"/> classic “It doesn’t work, please fix it” or “Write something that does X” without enough detail.<a contenteditable="false" data-primary="specificity in prompts" data-type="indexterm" id="id469"/> The question “Why isn’t my function working?” will generally get a useless answer. Vague prompts force the AI to guess the context and often result in generic advice or irrelevant code.</p>

<p>The fix is straightforward: add context and specifics. If you find yourself asking a question and the answer feels like a Magic 8–Ball response (“Have you tried checking X?”), stop and reframe your query with more details (error messages, code excerpt, expected versus actual outcome, etc.). A good practice is to read your prompt and ask, “Could this question apply to dozens of different scenarios?” If the answer is yes, it’s too vague. Make it so specific that it could only apply to your scenario.</p>
</div></section>

<section class="pagebreak-before" data-type="sect3" data-pdf-bookmark="The overloaded prompt"><div class="sect3" id="ch02_the_overloaded_prompt_1752630042972299">
<h3 class="less_space">The overloaded prompt</h3>

<p>This is the opposite issue: asking the AI to do too many things at once.<a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="overloaded prompts" data-type="indexterm" id="id470"/><a contenteditable="false" data-primary="overloaded prompts" data-type="indexterm" id="id471"/> For instance:</p>

<blockquote>
<p>Generate a complete Node.js app with authentication, a frontend in React, and deployment scripts.</p>
</blockquote>

<p>Or even, on a smaller scale:</p>

<blockquote>
<p>Fix these 5 bugs and also add these 3 features in one go.</p>
</blockquote>

<p>The AI might attempt it, but you’ll likely get a jumbled or incomplete result, or it might ignore some parts of the request. Even if it addresses everything, the response will be long and harder to verify.</p>

<p>The remedy is to split the tasks. Prioritize: do one thing at a time, as we emphasized earlier. This makes it easier to catch mistakes and ensures the model stays focused. If you catch yourself writing a paragraph that uses “and” multiple times in the instructions, consider breaking it into separate prompts or sequential steps.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Missing the question"><div class="sect3" id="ch02_missing_the_question_1752630042972344">
<h3>Missing the question</h3>

<p>Sometimes users will present a <a contenteditable="false" data-primary="question, missing in prompts" data-type="indexterm" id="id472"/>lot of <a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="missing the question" data-type="indexterm" id="id473"/>information but never clearly ask a question or specify what they need, like dumping a large code snippet and just saying, “Here’s my code.” This can confuse the AI—it doesn’t know what you want.</p>

<p>Always include a clear ask:</p>

<ul>
	<li>
	<p>Identify any bugs in the above code.</p>
	</li>
	<li>
	<p>Explain what this code does.</p>
	</li>
	<li>
	<p>Complete the to-dos in the code.</p>
	</li>
</ul>

<p>A prompt should have a purpose. If you just provide text without a question or instruction, the AI might make incorrect assumptions (like summarizing the code instead of fixing it, etc.). Make sure the AI knows why you showed it some code. Even a simple addition like “What’s wrong with this code?” or “Please continue implementing this function” gives it direction.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Vague success criteria"><div class="sect3" id="ch02_vague_success_criteria_1752630042972389">
<h3>Vague success criteria</h3>

<p>This is a subtle one. <a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="vague success criteria" data-type="indexterm" id="id474"/><a contenteditable="false" data-primary="success criteria (vague), in prompts" data-type="indexterm" id="id475"/>Sometimes you might ask for an optimization or improvement, but you don’t define what success looks like—for example, “Make this function faster.” Faster by what metric? If the AI doesn’t know your performance constraints, it might micro-optimize something that doesn’t matter or use an approach that’s theoretically faster but practically negligible. Or “Make this code cleaner”: “cleaner” is subjective. We dealt with this by explicitly stating goals like “reduce duplication” or “improve variable names,” etc.</p>

<p>The fix: quantify or qualify the improvement:</p>

<ul>
	<li>
	<p>Optimize this function to run in linear time (current version is quadratic).</p>
	</li>
	<li>
	<p>Refactor this to remove global variables and use a class instead.</p>
	</li>
</ul>

<p>Basically, be explicit about what problem you’re solving with the refactor or feature. If you leave it too open, the AI might solve a different problem than the one you care about.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Ignoring AI’s clarification or output"><div class="sect3" id="ch02_ignoring_ai_s_clarification_or_output_1752630042972437">
<h3>Ignoring AI’s clarification or output</h3>

<p>Sometimes the AI might<a contenteditable="false" data-primary="ignoring AI's clarification or output (prompt antipattern)" data-type="indexterm" id="id476"/> respond with a <a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="ignoring AI's clarification or output" data-type="indexterm" id="id477"/>clarifying question or an assumption:</p>

<ul>
	<li>
	<p>Are you using React class components or functional components?</p>
	</li>
	<li>
	<p>I assume the input is a string—please confirm.</p>
	</li>
</ul>

<p>If you ignore these and just reiterate your request, you’re missing an opportunity to improve the prompt. The AI is signaling that it needs more info. Always answer its questions or refine your prompt to include those details.</p>

<p>Additionally, if the AI’s output is clearly off (like it misunderstood the question), don’t just retry the same prompt verbatim.<a contenteditable="false" data-primary="output" data-secondary="clearly incorrect output from AI, handling" data-type="indexterm" id="id478"/> Take a moment to adjust your wording. Maybe your prompt had an ambiguous phrase or omitted something essential. Treat it like a conversation: if a human misunderstood, you’d explain differently; do the same for the AI.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Inconsistency"><div class="sect3" id="ch02_inconsistency_1752630042972483">
<h3>Inconsistency</h3>

<p>If you keep changing how you ask or mixing different formats in one go, the model can get confused. <a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="inconsistency in prompts" data-type="indexterm" id="id479"/><a contenteditable="false" data-primary="inconsistency in prompts" data-type="indexterm" id="id480"/>Two examples include switching between first person and third person in instructions or mixing pseudocode with actual code in a confusing way.</p>

<p>Try to maintain a consistent style within a single prompt. If you provide examples, ensure they are clearly delineated (use Markdown triple backticks for code, quotes for input/output examples, etc.). Consistency helps the model parse your intent correctly. Also, if you have a preferred style (say, ES6 versus ES5 syntax), consistently mention it; otherwise, the model might suggest one way in one prompt and another way later.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Vague references like “the above code”"><div class="sect3" id="ch02_vague_references_like_the_above_code_1752630042972530">
<h3>Vague references like “the above code”</h3>

<p>When using chat, if you say “the above function” or “the previous output,” be sure the reference is clear. <a contenteditable="false" data-primary="references (vague), in prompts" data-type="indexterm" id="id481"/><a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-secondary="vague references" data-type="indexterm" id="id482"/>If the conversation is long and you say, “Refactor the above code,” the AI might lose track or pick the wrong code snippet to refactor.</p>

<p>It’s safer to either quote the code again or specifically name the function you want refactored. Models have a limited attention window, and although many LLMs can refer to prior parts of the conversation, giving it explicit context again can help avoid confusion. This is especially true if some time (or several messages) passed since the code was shown.<a contenteditable="false" data-primary="prompt antipatterns, avoiding" data-startref="ix_prmptanti" data-type="indexterm" id="id483"/><a contenteditable="false" data-primary="prompting techniques" data-secondary="advanced prompting, combining techniques and handling complexity" data-startref="ix_prmpttchcmb" data-type="indexterm" id="id484"/></p>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary and Next Steps"><div class="sect1" id="ch02_summary_and_next_steps_1752630042972578">
<h1>Summary and Next Steps</h1>

<p>The art of prompting is iterative and creative. As models evolve, prompt best practices might change (for instance, future models might better understand intent with less wording). But the underlying principle remains: communicate effectively, and the AI will serve you better.</p>

<p>In essence, mastering prompt engineering is like mastering a new programming language—the language of instructions for AI. It’s a blend of technical writing, foresight, and interactive debugging of the <em>prompt</em> itself. But once you get good at it, the AI truly starts to feel like an extension of your own mind, because you can reliably extract the solutions you envision (or even those you don’t fully envision yet but can guide the AI to discover) with minimal friction. This skill will likely become as fundamental as knowing how to google things or how to use a debugger—it’s part of the modern developer’s skill set in the age of vibe coding.<a contenteditable="false" data-primary="prompts" data-secondary="effective communication with AI" data-startref="ix_prmpt" data-type="indexterm" id="id485"/></p>

<p>If AI can solve about 70% of a problem, how do you approach it as a partner in coding? <a data-type="xref" href="ch03.html#ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933">Chapter 3</a> looks at how developers <em>really</em> use AI and sets out some “golden rules” for vibe coding.</p>
</div></section>
</div></section></div></div></body></html>