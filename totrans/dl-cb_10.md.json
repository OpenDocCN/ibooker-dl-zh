["```py\n10.1 Building an inverse image search service\n```", "```py\nquery = \"\"\"SELECT DISTINCT ?pic\nWHERE\n{\n ?item wdt:P31 ?class .\n ?class wdt:P18 ?pic\n}\n\"\"\"\n```", "```py\nurl = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\ndata = requests.get(url, params={'query': query, 'format': 'json'}).json()\nimages = [x['pic']['value'] for x in data['results']['bindings']]\n```", "```py\ndef center_crop_resize(img, new_size):\n    w, h = img.size\n    s = min(w, h)\n    y = (h - s) // 2\n    x = (w - s) // 2\n    img = img.crop((x, y, s, s))\n    return img.resize((new_size, new_size))\n\ndef fetch_image(image_cache, image_url):\n    image_name = image_url.rsplit('/', 1)[-1]\n    local_name = image_name.rsplit('.', 1)[0] + '.jpg'\n    local_path = os.path.join(image_cache, local_name)\n    if os.path.isfile(local_path):\n        img = Image.open(local_path)\n        img.load()\n        return center_crop_resize(img, 299)\n    image_name = unquote(image_name).replace(' ', '_')\n    m = md5()\n    m.update(image_name.encode('utf8'))\n    c = m.hexdigest()\n    for prefix in ('http://upload.wikimedia.org/wikipedia/en',\n                   'http://upload.wikimedia.org/wikipedia/commons'):\n        url = '/'.join((prefix, c[0], c[0:2], image_name))\n        r = requests.get(url)\n        if r.status_code != 404:\n            try:\n                img = Image.open(BytesIO(r.content))\n                if img.mode != 'RGB':\n                    img = img.convert('RGB')\n                img.save(local_path)\n                return center_crop_resize(img, 299)\n            except IOError:\n                pass\n    return None\n```", "```py\nvalid_images = []\nfor image_name in tqdm(images):\n    img = fetch_image(IMAGE_DIR, image_name)\n    if img:\n        valid_images.append(img)\n```", "```py\nbase_model = InceptionV3(weights='imagenet', include_top=True)\nbase_model.summary()\n```", "```py\nmodel = Model(inputs=base_model.input,\n              outputs=base_model.get_layer('avg_pool').output)\n```", "```py\ndef get_vector(img):\n    if not type(img) == list:\n        images = [img]\n    else:\n        images = img\n    target_size = int(max(model.input.shape[1:]))\n    images = [img.resize((target_size, target_size), Image.ANTIALIAS)\n              for img in images]\n    np_imgs = [image.img_to_array(img) for img in images]\n    pre_processed = preprocess_input(np.asarray(np_imgs))\n    return model.predict(pre_processed)\n```", "```py\nchunks = [get_vector(valid_images[i:i+256])\n          for i in range(0, len(valid_images), 256)]\nvectors = np.concatenate(chunks)\n```", "```py\nnbrs = NearestNeighbors(n_neighbors=10,\n                        balgorithm='ball_tree').fit(vectors)\n```", "```py\ncat = get_vector(Image.open('data/cat.jpg'))\ndistances, indices = nbrs.kneighbors(cat)\n```", "```py\nhtml = []\nfor idx, dist in zip(indices[0], distances[0]):\n    b = BytesIO()\n    valid_images[idx].save(b, format='jpeg')\n    b64_img = base64.b64encode(b.getvalue()).decode('utf-8'))\n    html.append(\"<img src='data:image/jpg;base64,{0}'/>\".format(b64_img)\nHTML(''.join(html))\n```", "```py\nnbrs64 = NearestNeighbors(n_neighbors=64, algorithm='ball_tree').fit(vectors)\ndistances64, indices64 = nbrs64.kneighbors(cat)\n```", "```py\nvectors64 = np.asarray([vectors[idx] for idx in indices64[0]])\nsvd = TruncatedSVD(n_components=2)\nvectors64_transformed = svd.fit_transform(vectors64)\n```", "```py\nmins = np.min(vectors64_transformed, axis=0)\nmaxs = np.max(vectors64_transformed, axis=0)\nxys = (vectors64_transformed - mins) / (maxs - mins)\n```", "```py\nimg64 = Image.new('RGB', (8 * 75, 8 * 75), (180, 180, 180))\n\nfor idx, (x, y) in zip(indices64[0], xys):\n    x = int(x * 7) * 75\n    y = int(y * 7) * 75\n    img64.paste(valid_images[idx].resize((75, 75)), (x, y))\n\nimg64\n```"]