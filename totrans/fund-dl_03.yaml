- en: Chapter 3\. The Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Intelligent Machines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The brain is the most incredible organ in the human body. It dictates the way
    we perceive every sight, sound, smell, taste, and touch. It enables us to store
    memories, experience emotions, and even dream. Without it, we would be primitive
    organisms, incapable of anything other than the simplest of reflexes. The brain
    is, inherently, what makes us intelligent.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The infant brain weighs only a single pound, but somehow it solves problems
    that even our biggest, most powerful supercomputers find impossible. Within a
    matter of months after birth, infants can recognize the faces of their parents,
    discern discrete objects from their backgrounds, and even tell voices apart. Within
    a year, theyâ€™ve already developed an intuition for natural physics, can track
    objects even when they become partially or completely blocked, and can associate
    sounds with specific meanings. And by early childhood, they have a sophisticated
    understanding of grammar and thousands of words in their vocabularies.^([1](ch03.xhtml#idm45934167495712))
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: For decades, weâ€™ve dreamed of building intelligent machines with brains like
    oursâ€”robotic assistants to clean our homes, cars that drive themselves, microscopes
    that automatically detect diseases. But building these artificially intelligent
    machines requires us to solve some of the most complex computational problems
    we have ever grappled with; problems that our brains can already solve in a manner
    of microseconds. To tackle these problems, weâ€™ll have to develop a radically different
    way of programming a computer using techniques largely developed over the past
    decade. This is an extremely active field of artificial computer intelligence
    often referred to as *deep learning*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The Limits of Traditional Computer Programs
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Why exactly are certain problems so difficult for computers to solve? Well,
    it turns out that traditional computer programs are designed to be very good at
    two things: (1) performing arithmetic really fast and (2) explicitly following
    a list of instructions. So if you want to do some heavy financial number crunching,
    youâ€™re in luck. Traditional computer programs can do the trick. But letâ€™s say
    we want to do something slightly more interesting, like write a program to automatically
    read someoneâ€™s handwriting. [FigureÂ 3-1](#handwritten_digit_dataset) will serve
    as a starting point.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](Images/fdl2_0301.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Image from MNIST handwritten digit dataset^([2](ch03.xhtml#idm45934169246960))
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although every digit in [FigureÂ 3-1](#handwritten_digit_dataset) is written
    in a slightly different way, we can easily recognize every digit in the first
    row as a zero, every digit in the second row as a one, etc. Letâ€™s try to write
    a computer program to crack this task. What rules could we use to tell one digit
    from another?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Well, we can start simple! For example, we might state that we have a zero if
    our image has only a single, closed loop. All the examples inÂ [FigureÂ 3-1](#handwritten_digit_dataset)
    seem to fit this bill, but this isnâ€™t really a sufficient condition. What if someone
    doesnâ€™t perfectly close the loop on their zero? And, as inÂ [FigureÂ 3-2](#difficult_to_distinguish),
    how do you distinguish a messy zero from a six?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](Images/fdl2_0302.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A zero thatâ€™s algorithmically difficult to distinguish from a six
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You could potentially establish some sort of cutoff for the distance between
    the starting point of the loop and the ending point, but itâ€™s not exactly clear
    where we should be drawing the line. But this dilemma is only the beginning of
    our worries. How do we distinguish between threes and fives? Or between fours
    and nines? We can add more and more rules, or *features*,Â through careful observation
    and months of trial and error, but itâ€™s quite clear that this isnâ€™t going to be
    an easy process.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Many other classes of problems fall into this same category: object recognition,
    speech comprehension, automated translation, etc. We donâ€™t know what program to
    write because we donâ€™t know how itâ€™s done by our brains. Â And even if we did know
    how to do it, the program might be horrendously complicated.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå…¶ä»–ç±»åˆ«çš„é—®é¢˜ä¹Ÿå±äºè¿™ä¸€ç±»ï¼šç‰©ä½“è¯†åˆ«ï¼Œè¯­éŸ³ç†è§£ï¼Œè‡ªåŠ¨ç¿»è¯‘ç­‰ã€‚æˆ‘ä»¬ä¸çŸ¥é“è¦å†™ä»€ä¹ˆç¨‹åºï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“æˆ‘ä»¬çš„å¤§è„‘æ˜¯å¦‚ä½•åšçš„ã€‚å³ä½¿æˆ‘ä»¬çŸ¥é“å¦‚ä½•åšï¼Œç¨‹åºå¯èƒ½ä¼šéå¸¸å¤æ‚ã€‚
- en: The Mechanics of Machine Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ çš„æœºåˆ¶
- en: To tackle these classes of problems, weâ€™ll have to use a different kind of approach.
    A lot of the things we learn in school growing up have much in common with traditional
    computer programs. We learn how to multiply numbers, solve equations, and take
    derivatives by internalizing a set of instructions. But the things we learn at
    an extremely early age, the things we find most natural, are learned by example,
    not by formula.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ç±»åˆ«ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸é‡‡ç”¨ä¸€ç§ä¸åŒçš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å­¦æ ¡å­¦åˆ°çš„å¾ˆå¤šä¸œè¥¿ä¸ä¼ ç»Ÿçš„è®¡ç®—æœºç¨‹åºæœ‰å¾ˆå¤šå…±åŒä¹‹å¤„ã€‚æˆ‘ä»¬å­¦ä¼šå¦‚ä½•ç›¸ä¹˜ï¼Œè§£æ–¹ç¨‹ï¼Œé€šè¿‡å†…åŒ–ä¸€ç»„æŒ‡ä»¤æ¥æ±‚å¯¼æ•°ã€‚ä½†æˆ‘ä»¬åœ¨ææ—©æœŸå­¦åˆ°çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ€è‡ªç„¶çš„ä¸œè¥¿ï¼Œæ˜¯é€šè¿‡ç¤ºä¾‹å­¦ä¹ çš„ï¼Œè€Œä¸æ˜¯é€šè¿‡å…¬å¼ã€‚
- en: For instance, when we were two years old, our parents didnâ€™t teach us how to
    recognize a dog by measuring the shape of its nose or the contours of its body.
    We learned to recognize a dog by being shown multiple examples and being corrected
    when we made the wrong guess. When we were born, our brains provided us with a
    model that described how we would be able to see the world. As we grew up, that
    model would take in our sensory inputs and make a guess about what we were experiencing.
    If that guess was confirmed by our parents, our model would be reinforced. If
    our parents said we were wrong, weâ€™d modify our model to incorporate this new
    information. Over our lifetime, our model becomes more and more accurate as we
    assimilate more and more examples. Obviously all of this happens subconsciously,
    but we can use this to our advantage.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬ä¸¤å²æ—¶ï¼Œæˆ‘ä»¬çš„çˆ¶æ¯æ²¡æœ‰æ•™æˆ‘ä»¬é€šè¿‡æµ‹é‡ç‹—çš„é¼»å­å½¢çŠ¶æˆ–èº«ä½“è½®å»“æ¥è®¤è¯†ç‹—ã€‚æˆ‘ä»¬é€šè¿‡å±•ç¤ºå¤šä¸ªç¤ºä¾‹å¹¶åœ¨çŒœé”™æ—¶å¾—åˆ°çº æ­£æ¥å­¦ä¼šè®¤è¯†ç‹—ã€‚å½“æˆ‘ä»¬å‡ºç”Ÿæ—¶ï¼Œæˆ‘ä»¬çš„å¤§è„‘ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæè¿°æˆ‘ä»¬å¦‚ä½•çœ‹ä¸–ç•Œçš„æ¨¡å‹ã€‚éšç€æˆ‘ä»¬é•¿å¤§ï¼Œè¯¥æ¨¡å‹ä¼šæ¥æ”¶æˆ‘ä»¬çš„æ„Ÿå®˜è¾“å…¥å¹¶çŒœæµ‹æˆ‘ä»¬æ­£åœ¨ç»å†ä»€ä¹ˆã€‚å¦‚æœçˆ¶æ¯ç¡®è®¤äº†è¿™ä¸ªçŒœæµ‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°±ä¼šå¾—åˆ°å¼ºåŒ–ã€‚å¦‚æœçˆ¶æ¯è¯´æˆ‘ä»¬é”™äº†ï¼Œæˆ‘ä»¬ä¼šä¿®æ”¹æˆ‘ä»¬çš„æ¨¡å‹ä»¥çº³å…¥è¿™ä¸ªæ–°ä¿¡æ¯ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œéšç€æˆ‘ä»¬å¸æ”¶æ›´å¤šçš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå‡†ç¡®ã€‚æ˜¾ç„¶ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨æ½œæ„è¯†ä¸­å‘ç”Ÿçš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹ã€‚
- en: Deep learning is a subset of a more general field of AI called *machine learning*,
    which is predicated on this idea of learning from example. In machine learning,
    instead of teaching a computer a massive list of rules to solve the problem, we
    give it a *model* with which it can evaluate examples, and a small set of instructions
    to modify the model when it makes a mistake. We expect that, over time, a well-suited
    model would be able to solve the problem extremely accurately.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½æ›´ä¸€èˆ¬é¢†åŸŸæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå…¶åŸºäºä»ç¤ºä¾‹ä¸­å­¦ä¹ çš„è¿™ä¸€æƒ³æ³•ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯æ•™è®¡ç®—æœºè§£å†³é—®é¢˜çš„å¤§é‡è§„åˆ™ï¼Œè€Œæ˜¯ç»™å®ƒä¸€ä¸ªæ¨¡å‹ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å‹å®ƒå¯ä»¥è¯„ä¼°ç¤ºä¾‹ï¼Œå¹¶ç»™å‡ºä¸€å°ç»„æŒ‡ä»¤æ¥åœ¨çŠ¯é”™è¯¯æ—¶ä¿®æ”¹æ¨¡å‹ã€‚æˆ‘ä»¬æœŸæœ›éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸€ä¸ªåˆé€‚çš„æ¨¡å‹å°†èƒ½å¤Ÿæå…¶å‡†ç¡®åœ°è§£å†³é—®é¢˜ã€‚
- en: Letâ€™s be a little bit more rigorous about what this means so we can formulate
    this idea mathematically. Letâ€™s define our model to be a functionÂ  <math alttext="h
    left-parenthesis bold x comma theta right-parenthesis"><mrow><mi>h</mi> <mo>(</mo>
    <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow></math> . The input **x** is
    an example expressed in vector form. For example, if **x** were a grayscale image,
    the vectorâ€™s components would be pixel intensities at each position, as shown
    in [FigureÂ 3-3](#process_of_vectorizing).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¹è¿™æ„æ€æ›´åŠ ä¸¥è°¨ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç”¨æ•°å­¦çš„æ–¹å¼æ¥è¡¨è¾¾è¿™ä¸ªæƒ³æ³•ã€‚è®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ä¸ºä¸€ä¸ªå‡½æ•°h(ğ±,Î¸)ã€‚è¾“å…¥xæ˜¯ä»¥å‘é‡å½¢å¼è¡¨ç¤ºçš„ç¤ºä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœxæ˜¯ä¸€ä¸ªç°åº¦å›¾åƒï¼Œå‘é‡çš„åˆ†é‡å°†æ˜¯æ¯ä¸ªä½ç½®çš„åƒç´ å¼ºåº¦ï¼Œå¦‚[å›¾3-3](#process_of_vectorizing)æ‰€ç¤ºã€‚
- en: '![ ](Images/fdl2_0303.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0303.png)'
- en: Figure 3-3\. The process of vectorizing an image for a machine learning algorithm
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-3ã€‚å°†å›¾åƒå‘é‡åŒ–ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•çš„è¿‡ç¨‹
- en: The input <math alttext="theta"><mi>Î¸</mi></math> is a vector of the parameters
    that our model uses. Our machine learning program tries to perfect the values
    of these parameters as it is exposed to more and more examples. Weâ€™ll see this
    in action and in more detail in [ChapterÂ 4](ch04.xhtml#training_feed_forward).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥Î¸æ˜¯æˆ‘ä»¬æ¨¡å‹ä½¿ç”¨çš„å‚æ•°çš„å‘é‡ã€‚éšç€æš´éœ²äºè¶Šæ¥è¶Šå¤šçš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ ç¨‹åºè¯•å›¾å®Œå–„è¿™äº›å‚æ•°çš„å€¼ã€‚æˆ‘ä»¬å°†åœ¨[ç¬¬4ç« ](ch04.xhtml#training_feed_forward)ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶æ›´è¯¦ç»†åœ°äº†è§£ã€‚
- en: 'To develop a more intuitive understanding for machine learning models, letâ€™s
    walk through a quick example. Letâ€™s say we wanted to determine how to predict
    exam performance based on the number of hours of sleep we get and the number of
    hours we study the previous day. We collect a lot of data, and for each data point
    <math alttext="bold x equals Start 1 By 2 Matrix 1st Row 1st Column x 1 2nd Column
    x 2 EndMatrix Superscript upper T"><mrow><mi>ğ±</mi> <mo>=</mo> <msup><mfenced
    open="[" close="]"><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced> <mi>T</mi></msup></mrow></math>
    , we record the number of hours of sleep we got ( <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> ), the number of hours we spent studying ( <math alttext="x
    2"><msub><mi>x</mi> <mn>2</mn></msub></math> ), and whether we performed above
    or below the class average. Our goal, then, might be to learn a modelÂ  <math alttext="h
    left-parenthesis bold x comma theta right-parenthesis"><mrow><mi>h</mi> <mo>(</mo>
    <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow></math> Â with parameter vector
    <math alttext="theta equals Start 1 By 3 Matrix 1st Row 1st Column theta 0 2nd
    Column theta 1 3rd Column theta 2 EndMatrix Superscript upper T"><mrow><mi>Î¸</mi>
    <mo>=</mo> <msup><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>Î¸</mi>
    <mn>0</mn></msub></mtd> <mtd><msub><mi>Î¸</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>Î¸</mi>
    <mn>2</mn></msub></mtd></mtr></mtable></mfenced> <mi>T</mi></msup></mrow></math>
    Â such that:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´ç›´è§‚åœ°ç†è§£æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¿«é€Ÿç¤ºä¾‹æ¥èµ°ä¸€éã€‚å‡è®¾æˆ‘ä»¬æƒ³ç¡®å®šå¦‚ä½•æ ¹æ®æˆ‘ä»¬ç¡çœ æ—¶é—´å’Œå‰ä¸€å¤©å­¦ä¹ æ—¶é—´æ¥é¢„æµ‹è€ƒè¯•è¡¨ç°ã€‚æˆ‘ä»¬æ”¶é›†äº†å¤§é‡æ•°æ®ï¼Œå¯¹äºæ¯ä¸ªæ•°æ®ç‚¹
    <math alttext="bold x equals Start 1 By 2 Matrix 1st Row 1st Column x 1 2nd Column
    x 2 EndMatrix Superscript upper T"><mrow><mi>ğ±</mi> <mo>=</mo> <msup><mfenced
    open="[" close="]"><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd>
    <mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced> <mi>T</mi></msup></mrow></math>ï¼Œæˆ‘ä»¬è®°å½•äº†æˆ‘ä»¬çš„ç¡çœ æ—¶é—´ï¼ˆ
    <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> ï¼‰ï¼Œæˆ‘ä»¬å­¦ä¹ çš„æ—¶é—´ï¼ˆ <math
    alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> ï¼‰ï¼Œä»¥åŠæˆ‘ä»¬æ˜¯å¦è¡¨ç°é«˜äºæˆ–ä½äºç­çº§å¹³å‡æ°´å¹³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å¯èƒ½æ˜¯å­¦ä¹ ä¸€ä¸ªå¸¦æœ‰å‚æ•°å‘é‡
    <math alttext="theta equals Start 1 By 3 Matrix 1st Row 1st Column theta 0 2nd
    Column theta 1 3rd Column theta 2 EndMatrix Superscript upper T"><mrow><mi>Î¸</mi>
    <mo>=</mo> <msup><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>Î¸</mi>
    <mn>0</mn></msub></mtd> <mtd><msub><mi>Î¸</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>Î¸</mi>
    <mn>2</mn></msub></mtd></mtr></mtable></mfenced> <mi>T</mi></msup></mrow></math>
    çš„æ¨¡å‹ <math alttext="h left-parenthesis bold x comma theta right-parenthesis"><mrow><mi>h</mi>
    <mo>(</mo> <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow></math>ï¼Œä½¿å¾—ï¼š
- en: <math alttext="h left-parenthesis bold x comma theta right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column negative 1 2nd Column if bold
    x Superscript upper T Baseline dot StartBinomialOrMatrix theta 1 Choose theta
    2 EndBinomialOrMatrix plus theta 0 less-than 0 2nd Row 1st Column 1 2nd Column
    if bold x Superscript upper T Baseline dot StartBinomialOrMatrix theta 1 Choose
    theta 2 EndBinomialOrMatrix plus theta 0 greater-than-or-equal-to 0 EndLayout"><mrow><mi>h</mi>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><msup><mi>ğ±</mi>
    <mi>T</mi></msup> <mo>Â·</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>Î¸</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>Î¸</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <msub><mi>Î¸</mi> <mn>0</mn></msub> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mrow><msup><mi>ğ±</mi> <mi>T</mi></msup> <mo>Â·</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>Î¸</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>Î¸</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <msub><mi>Î¸</mi> <mn>0</mn></msub> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h left-parenthesis bold x comma theta right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column negative 1 2nd Column if bold
    x Superscript upper T Baseline dot StartBinomialOrMatrix theta 1 Choose theta
    2 EndBinomialOrMatrix plus theta 0 less-than 0 2nd Row 1st Column 1 2nd Column
    if bold x Superscript upper T Baseline dot StartBinomialOrMatrix theta 1 Choose
    theta 2 EndBinomialOrMatrix plus theta 0 greater-than-or-equal-to 0 EndLayout"><mrow><mi>h</mi>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><msup><mi>ğ±</mi>
    <mi>T</mi></msup> <mo>Â·</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>Î¸</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>Î¸</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <msub><mi>Î¸</mi> <mn>0</mn></msub> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mrow><msup><mi>ğ±</mi> <mi>T</mi></msup> <mo>Â·</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>Î¸</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>Î¸</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <msub><mi>Î¸</mi> <mn>0</mn></msub> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: So we guess that the blueprint for our modelÂ  <math alttext="h left-parenthesis
    bold x comma theta right-parenthesis"><mrow><mi>h</mi> <mo>(</mo> <mi>ğ±</mi> <mo>,</mo>
    <mi>Î¸</mi> <mo>)</mo></mrow></math> Â is as described (geometrically, this particular
    blueprint describes a linear classifier that divides the coordinate plane into
    two halves). Then, we want to learn a parameter vectorÂ  <math alttext="theta"><mi>Î¸</mi></math>
    Â such that our model makes the right predictions (âˆ’1 if we perform below average,
    and 1 otherwise) given an input example **x**. This model is called a linear *perceptron*,
    and itâ€™s a model thatâ€™s been used since the 1950s.^([3](ch03.xhtml#idm45934169203808))Â Letâ€™s
    assume our data is as shown inÂ [FigureÂ 3-4](#sample_data_for_our_exam).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çŒœæƒ³æˆ‘ä»¬æ¨¡å‹çš„è“å›¾ <math alttext="h left-parenthesis bold x comma theta right-parenthesis"><mrow><mi>h</mi>
    <mo>(</mo> <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow></math> å¦‚ä¸Šæ‰€è¿°ï¼ˆä»å‡ ä½•ä¸Šè®²ï¼Œè¿™ä¸ªç‰¹å®šçš„è“å›¾æè¿°äº†ä¸€ä¸ªå°†åæ ‡å¹³é¢åˆ†æˆä¸¤åŠçš„çº¿æ€§åˆ†ç±»å™¨ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬æƒ³å­¦ä¹ ä¸€ä¸ªå‚æ•°å‘é‡
    <math alttext="theta"><mi>Î¸</mi></math>ï¼Œä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç»™å®šè¾“å…¥ç¤ºä¾‹ **x** æ—¶åšå‡ºæ­£ç¡®çš„é¢„æµ‹ï¼ˆå¦‚æœæˆ‘ä»¬è¡¨ç°ä½äºå¹³å‡æ°´å¹³ï¼Œåˆ™ä¸º-1ï¼Œå¦åˆ™ä¸º1ï¼‰ã€‚è¿™ä¸ªæ¨¡å‹è¢«ç§°ä¸ºçº¿æ€§*æ„ŸçŸ¥å™¨*ï¼Œè‡ªä¸Šä¸–çºª50å¹´ä»£ä»¥æ¥ä¸€ç›´è¢«ä½¿ç”¨ã€‚è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„æ•°æ®å¦‚å›¾3-4æ‰€ç¤ºã€‚
- en: '![ ](Images/fdl2_0304.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0304.png)'
- en: Figure 3-4\. Sample data for our exam predictor algorithm and a potential classifier
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-4ã€‚æˆ‘ä»¬è€ƒè¯•é¢„æµ‹ç®—æ³•å’Œæ½œåœ¨åˆ†ç±»å™¨çš„æ ·æœ¬æ•°æ®
- en: 'Then it turns out that by selectingÂ  <math alttext="theta equals Start 1 By
    3 Matrix 1st Row 1st Column negative 24 2nd Column 3 3rd Column 4 EndMatrix Superscript
    upper T"><mrow><mi>Î¸</mi> <mo>=</mo> <msup><mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mo>-</mo><mn>24</mn></mrow></mtd><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mi>T</mi></msup></mrow></math> , our machine learning model makes the correct
    prediction on every data point:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå‘ç°ï¼Œé€šè¿‡é€‰æ‹© <math alttext="theta equals Start 1 By 3 Matrix 1st Row 1st Column
    negative 24 2nd Column 3 3rd Column 4 EndMatrix Superscript upper T"><mrow><mi>Î¸</mi>
    <mo>=</mo> <msup><mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mo>-</mo><mn>24</mn></mrow></mtd><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mi>T</mi></msup></mrow></math>ï¼Œæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ¯ä¸ªæ•°æ®ç‚¹éƒ½åšå‡ºäº†æ­£ç¡®çš„é¢„æµ‹ï¼š
- en: <math alttext="h left-parenthesis bold x comma theta right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column negative 1 2nd Column if 3
    x 1 plus 4 x 2 minus 24 less-than 0 2nd Row 1st Column 1 2nd Column if 3 x 1 plus
    4 x 2 minus 24 greater-than-or-equal-to 0 EndLayout"><mrow><mi>h</mi> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h left-parenthesis bold x comma theta right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column negative 1 2nd Column if 3
    x 1 plus 4 x 2 minus 24 less-than 0 2nd Row 1st Column 1 2nd Column if 3 x 1 plus
    4 x 2 minus 24 greater-than-or-equal-to 0 EndLayout"><mrow><mi>h</mi> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: An optimal parameter vectorÂ  <math alttext="theta"><mi>Î¸</mi></math> Â positions
    the classifier so that we make as many correct predictions as possible. In most
    cases, there are many (or even infinitely many)Â possible choices forÂ  <math alttext="theta"><mi>Î¸</mi></math>
    Â that are optimal. Fortunately for us, most of the time these alternatives are
    so close to one another that the difference is negligible. If this is not the
    case, we may want to collect more data to narrow our choice ofÂ  <math alttext="theta"><mi>Î¸</mi></math>
    .
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœ€ä½³å‚æ•°å‘é‡ <math alttext="theta"><mi>Î¸</mi></math> ä½¿åˆ†ç±»å™¨å®šä½ï¼Œä»¥ä¾¿æˆ‘ä»¬å°½å¯èƒ½å¤šåœ°åšå‡ºæ­£ç¡®é¢„æµ‹ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæœ‰è®¸å¤šï¼ˆç”šè‡³æ— é™å¤šï¼‰å¯èƒ½çš„æœ€ä½³é€‰æ‹©
    <math alttext="theta"><mi>Î¸</mi></math>ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™äº›æ›¿ä»£æ–¹æ¡ˆä¹‹é—´çš„å·®å¼‚æ˜¯å¾®ä¸è¶³é“çš„ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ”¶é›†æ›´å¤šæ•°æ®æ¥ç¼©å°æˆ‘ä»¬å¯¹
    <math alttext="theta"><mi>Î¸</mi></math> çš„é€‰æ‹©ã€‚
- en: While the setup seems reasonable, there are still some pretty significant questions
    that remain. First off, how do we even come up with an optimal value for the parameter
    vectorÂ  <math alttext="theta"><mi>Î¸</mi></math> Â in the first place? Solving this
    problem requires a technique commonly known as *optimization*. An optimizer aims
    to maximize the performance of a machine learning model by iteratively tweaking
    its parameters until the error is minimized.Â Weâ€™ll begin to tackle this question
    of learning parameter vectors in more detail in [ChapterÂ 4](ch04.xhtml#training_feed_forward),
    when we describe the process of *gradient descent*.^([4](ch03.xhtml#idm45934168242992))
    In later chapters, weâ€™ll try to find ways to make this process even more efficient.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è®¾ç½®çœ‹èµ·æ¥åˆç†ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›ç›¸å½“é‡è¦çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°å‚æ•°å‘é‡<math alttext="theta"><mi>Î¸</mi></math>çš„æœ€ä¼˜å€¼ï¼Ÿè§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦ä¸€ç§å¸¸ç”¨çš„æŠ€æœ¯ï¼Œç§°ä¸º*ä¼˜åŒ–*ã€‚ä¼˜åŒ–å™¨æ—¨åœ¨é€šè¿‡è¿­ä»£è°ƒæ•´å…¶å‚æ•°ï¼Œç›´åˆ°è¯¯å·®æœ€å°åŒ–ï¼Œä»è€Œæœ€å¤§åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†åœ¨[ç¬¬4ç« ](ch04.xhtml#training_feed_forward)ä¸­æ›´è¯¦ç»†åœ°è®¨è®ºå­¦ä¹ å‚æ•°å‘é‡çš„é—®é¢˜ï¼Œå½“æˆ‘ä»¬æè¿°*æ¢¯åº¦ä¸‹é™*è¿‡ç¨‹æ—¶ã€‚åœ¨åé¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•æ‰¾åˆ°ä½¿è¿™ä¸ªè¿‡ç¨‹æ›´åŠ é«˜æ•ˆçš„æ–¹æ³•ã€‚
- en: Second, itâ€™s quite clear that this particular model (the linear perceptron model)
    is quite limited in the relationships it can learn. For example, the distributions
    of data shown inÂ [FigureÂ 3-5](#data_takes_on_more_complex_forms) cannot be described
    well by a linear perceptron.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œå¾ˆæ˜æ˜¾ï¼Œè¿™ä¸ªç‰¹å®šæ¨¡å‹ï¼ˆçº¿æ€§æ„ŸçŸ¥å™¨æ¨¡å‹ï¼‰åœ¨å­¦ä¹ å…³ç³»æ–¹é¢æ˜¯ç›¸å½“æœ‰é™çš„ã€‚ä¾‹å¦‚ï¼Œ[å›¾3-5](#data_takes_on_more_complex_forms)ä¸­æ˜¾ç¤ºçš„æ•°æ®åˆ†å¸ƒæ— æ³•å¾ˆå¥½åœ°è¢«çº¿æ€§æ„ŸçŸ¥å™¨æè¿°ã€‚
- en: '![ ](Images/fdl2_0305.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0305.png)'
- en: Figure 3-5\. As our data takes on more complex forms, we need more complex models
    to describe them
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-5ã€‚éšç€æˆ‘ä»¬çš„æ•°æ®å‘ˆç°æ›´å¤æ‚çš„å½¢å¼ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤æ‚çš„æ¨¡å‹æ¥æè¿°å®ƒä»¬
- en: But these situations are only the tip of the iceberg. As we move on to much
    more complex problems, such as object recognition and text analysis, our data
    becomes extremely high dimensional, and the relationships we want to capture become
    highly nonlinear. To accommodate this complexity, recent research in machine learning
    has attempted to build models that resemble the structures utilized by our brains.
    Itâ€™s essentially this body of research, commonly referred to asÂ *deep learning*,
    that has had spectacular success in tackling problems in computer vision and natural
    language processing. These algorithms not only far surpass other kinds of machine
    learning algorithms, but also rival (or even exceed) the accuracies achieved by
    humans.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™äº›æƒ…å†µåªæ˜¯å†°å±±ä¸€è§’ã€‚éšç€æˆ‘ä»¬è½¬å‘æ›´å¤æ‚çš„é—®é¢˜ï¼Œå¦‚ç‰©ä½“è¯†åˆ«å’Œæ–‡æœ¬åˆ†æï¼Œæˆ‘ä»¬çš„æ•°æ®å˜å¾—æé«˜ç»´ï¼Œæˆ‘ä»¬æƒ³è¦æ•æ‰çš„å…³ç³»å˜å¾—é«˜åº¦éçº¿æ€§ã€‚ä¸ºäº†é€‚åº”è¿™ç§å¤æ‚æ€§ï¼Œæœ€è¿‘æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶å°è¯•æ„å»ºç±»ä¼¼äºæˆ‘ä»¬å¤§è„‘æ‰€åˆ©ç”¨çš„ç»“æ„çš„æ¨¡å‹ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ç³»åˆ—ç ”ç©¶ï¼Œé€šå¸¸è¢«ç§°ä¸º*æ·±åº¦å­¦ä¹ *ï¼Œåœ¨è§£å†³è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜æ–¹é¢å–å¾—äº†æƒŠäººçš„æˆåŠŸã€‚è¿™äº›ç®—æ³•ä¸ä»…è¿œè¿œè¶…è¿‡å…¶ä»–ç±»å‹çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œè€Œä¸”ä¸äººç±»è¾¾åˆ°çš„å‡†ç¡®åº¦ç›¸åª²ç¾ï¼ˆç”šè‡³è¶…è¿‡ï¼‰ã€‚
- en: The Neuron
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¥ç»å…ƒ
- en: The foundational unit of the human brain is the neuron. A tiny piece of the
    brain, about the size of grain of rice, contains over 10,000 neurons, each of
    which forms an average of 6,000 connections with other neurons.^([5](ch03.xhtml#idm45934168225968))
    Itâ€™s this massive biological network that enables us to experience the Â world
    around us. Our goal in this section is to use this natural structure to build
    machine learning models that solve problems in an analogous way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»å¤§è„‘çš„åŸºæœ¬å•ä½æ˜¯ç¥ç»å…ƒã€‚å¤§è„‘ä¸­ä¸€ä¸ªå¾®å°çš„éƒ¨åˆ†ï¼Œå¤§çº¦æ˜¯ä¸€ç²’ç±³çš„å¤§å°ï¼ŒåŒ…å«è¶…è¿‡10,000ä¸ªç¥ç»å…ƒï¼Œæ¯ä¸ªç¥ç»å…ƒå¹³å‡ä¸å…¶ä»–ç¥ç»å…ƒå½¢æˆ6,000ä¸ªè¿æ¥ã€‚æ­£æ˜¯è¿™ä¸ªåºå¤§çš„ç”Ÿç‰©ç½‘ç»œä½¿æˆ‘ä»¬èƒ½å¤Ÿä½“éªŒå‘¨å›´çš„ä¸–ç•Œã€‚æˆ‘ä»¬åœ¨æœ¬èŠ‚çš„ç›®æ ‡æ˜¯åˆ©ç”¨è¿™ç§è‡ªç„¶ç»“æ„æ¥æ„å»ºè§£å†³ç±»ä¼¼é—®é¢˜çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
- en: At its core, the neuron is optimized to receive information from other neurons,
    process this information in a unique way, and send its result to other cells.
    This process is summarized inÂ [FigureÂ 3-6](#function_description_of_biological_neuron_structure).
    The neuron receives its inputs along antennae-like structures calledÂ *dendrites*.
    Each of these incoming connections is dynamically strengthened or weakened based
    on how often it is used (this is how we learn new concepts), and itâ€™s the strength
    of each connection that determines the contribution of the input to the neuronâ€™s
    output. After being weighted by the strength of their respective connections,
    the inputs are summed together in theÂ *cell body*. This sum is then transformed
    into a new signal thatâ€™s propagated along the cellâ€™sÂ *axon*Â and sent off to other
    neurons.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¶æ ¸å¿ƒï¼Œç¥ç»å…ƒè¢«ä¼˜åŒ–ä¸ºæ¥æ”¶æ¥è‡ªå…¶ä»–ç¥ç»å…ƒçš„ä¿¡æ¯ï¼Œä»¥ç‹¬ç‰¹çš„æ–¹å¼å¤„ç†è¿™äº›ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç»“æœå‘é€ç»™å…¶ä»–ç»†èƒã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨[å›¾3-6](#function_description_of_biological_neuron_structure)ä¸­è¿›è¡Œäº†æ€»ç»“ã€‚ç¥ç»å…ƒæ²¿ç€ç±»ä¼¼è§¦è§’çš„ç»“æ„ç§°ä¸º*æ ‘çª*æ¥æ”¶å…¶è¾“å…¥ã€‚æ¯ä¸ªä¼ å…¥è¿æ¥æ ¹æ®ä½¿ç”¨é¢‘ç‡åŠ¨æ€åŠ å¼ºæˆ–å‡å¼±ï¼ˆè¿™å°±æ˜¯æˆ‘ä»¬å­¦ä¹ æ–°æ¦‚å¿µçš„æ–¹å¼ï¼‰ï¼Œè€Œæ¯ä¸ªè¿æ¥çš„å¼ºåº¦å†³å®šäº†è¾“å…¥å¯¹ç¥ç»å…ƒè¾“å‡ºçš„è´¡çŒ®ã€‚åœ¨æ ¹æ®å„è‡ªè¿æ¥çš„å¼ºåº¦åŠ æƒåï¼Œè¾“å…¥åœ¨*ç»†èƒä½“*ä¸­ç›¸åŠ ã€‚ç„¶åï¼Œè¿™ä¸ªæ€»å’Œè¢«è½¬åŒ–ä¸ºä¸€ä¸ªæ–°ä¿¡å·ï¼Œæ²¿ç€ç»†èƒçš„*è½´çª*ä¼ æ’­ï¼Œå¹¶å‘é€ç»™å…¶ä»–ç¥ç»å…ƒã€‚
- en: '![ ](Images/fdl2_0306.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0306.png)'
- en: Figure 3-6\. A functional description of a biological neuronâ€™s structure
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-6ã€‚ç”Ÿç‰©ç¥ç»å…ƒç»“æ„çš„åŠŸèƒ½æè¿°
- en: We can translate this functional understanding of the neurons in our brain into
    an artificial model that we can represent on our computer. Such a model is described
    inÂ [FigureÂ 3-7](#schematic_for_a_neuron), leveraging the approach first pioneered
    in 1943 by Warren S. McCulloch and Walter H. Pitts.^([6](ch03.xhtml#idm45934168212000))
    Just as in biological neurons, our artificial neuron takes in some number of inputs,Â 
    <math alttext="x 1 comma x 2 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math> , each of which is
    multiplied by a specific weight, <math alttext="w 1 comma w 2 comma ellipsis comma
    w Subscript n Baseline"><mrow><msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>w</mi> <mi>n</mi></msub></mrow></math>
    .Â These weighted inputs are, as before, summed to produce theÂ *logit*Â of the neuron,Â 
    <math alttext="z equals sigma-summation Underscript i equals 0 Overscript n Endscripts
    w Subscript i Baseline x Subscript i"><mrow><mi>z</mi> <mo>=</mo> <msubsup><mo>âˆ‘</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></msubsup> <msub><mi>w</mi>
    <mi>i</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></math> . In many cases,
    the logit also includes aÂ *bias*, which is a constant (not shown in the figure).Â The
    logit is then passed through a functionÂ  <math alttext="f"><mi>f</mi></math> Â to
    produce the outputÂ  <math alttext="y equals f left-parenthesis z right-parenthesis"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math> . This output
    can be transmitted to other neurons.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](Images/fdl2_0307.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Schematic for a neuron in an artificial neural net
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weâ€™ll conclude our mathematical discussion of the artificial neuron by re-expressing
    its functionality in vector form. Letâ€™s reformulate the inputs as a vector **x**
    = [x[1] x[2] â€¦ x[n]] and the weights of the neuron as **w** = [w[1] w[2] â€¦ w[n]].
    Then we can re-express the output of the neuron as <math alttext="y equals f left-parenthesis
    bold x dot bold w plus b right-parenthesis"><mrow><mi>y</mi> <mo>=</mo> <mi>f</mi>
    <mfenced separators="" open="(" close=")"><mi>ğ±</mi> <mo>Â·</mo> <mi>ğ°</mi> <mo>+</mo>
    <mi>b</mi></mfenced></mrow></math> , where <math alttext="b"><mi>b</mi></math>
    is the bias term. We can compute the output by performing the dot product of the
    input and weight vectors, adding in the bias term to produce the logit, and then
    applying the transformation function. While this seems like a trivial reformulation,
    thinking about neurons as a series of vector manipulations will be crucial to
    how we implement them in software later in this book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Expressing Linear Perceptrons as Neurons
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [â€œThe Mechanics of Machine Learningâ€](#mech_machine_learn), we talked about
    using machine learning models to capture the relationship between success on exams
    and time spent studying and sleeping. To tackle this problem, we constructed a
    linear perceptron classifier that divided the Cartesian coordinate plane into
    two halves:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="h left-parenthesis bold x comma theta right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column negative 1 2nd Column if 3
    x 1 plus 4 x 2 minus 24 less-than 0 2nd Row 1st Column 1 2nd Column if 3 x 1 plus
    4 x 2 minus 24 greater-than-or-equal-to 0 EndLayout"><mrow><mi>h</mi> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h left-parenthesis bold x comma theta right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column negative 1 2nd Column if 3
    x 1 plus 4 x 2 minus 24 less-than 0 2nd Row 1st Column 1 2nd Column if 3 x 1 plus
    4 x 2 minus 24 greater-than-or-equal-to 0 EndLayout"><mrow><mi>h</mi> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>,</mo> <mi>Î¸</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mn>3</mn> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>24</mn> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'As shown inÂ [FigureÂ 3-4](#sample_data_for_our_exam), this is an optimal choice
    forÂ  <math alttext="theta"><mi>Î¸</mi></math> Â because it correctly classifies
    every sample in our dataset. Here, we show that our modelÂ *h*Â is easily using
    a neuron. Consider the neuron depicted inÂ [FigureÂ 3-8](#expressing_our_exam_performance).
    The neuron has two inputs, a bias, and uses the function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis z right-parenthesis equals StartLayout Enlarged
    left-brace 1st Row 1st Column negative 1 2nd Column if z less-than 0 2nd Row 1st
    Column 1 2nd Column if z greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mrow><mi>z</mi> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis z right-parenthesis equals StartLayout Enlarged
    left-brace 1st Row 1st Column negative 1 2nd Column if z less-than 0 2nd Row 1st
    Column 1 2nd Column if z greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mrow><mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mrow><mi>z</mi> <mo>â‰¥</mo> <mn>0</mn></mrow></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Itâ€™s easy to show that our linear perceptron and the neuronal model are perfectly
    equivalent. And in general, itâ€™s quite simple to show that singular neurons are
    strictly more expressive than linear perceptrons. Every linear perceptron can
    be expressed as a single neuron, but single neurons can also express models that
    cannot be expressed by any linear perceptron.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](Images/fdl2_0308.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Expressing our exam performance perceptron as a neuron
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Feed-Forward Neural Networks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although single neurons are more powerful than linear perceptrons, theyâ€™re not
    nearly expressive enough to solve complicated learning problems. Thereâ€™s a reason
    our brain is made of more than one neuron. For example, it is impossible for a
    single neuron to differentiate handwritten digits. So to tackle much more complicated
    tasks, weâ€™ll have to take our machine learning model even further.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The neurons in the human brain are organized in layers. In fact, the human cerebral
    cortex (the structure responsible for most of human intelligence) is made up of
    six layers.^([7](ch03.xhtml#idm45934167717376)) Information flows from one layer
    to another until sensory input is converted into conceptual understanding. For
    example, the bottommost layer of the visual cortex receives raw visual data from
    the eyes. This information is processed by each layer and passed on to the next
    until, in the sixth layer, we conclude whether we are looking at a cat, or a soda
    can, or an airplane.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Borrowing from these concepts, we can construct an *artificial neural network*.
    A neural network comes about when we start hooking up neurons to each other, the
    input data, and to the output nodes, which correspond to the networkâ€™s answer
    to a learning problem.Â [FigureÂ 3-9](#simple_example_of_a_feed_forward_neural_network)
    demonstrates a simple example of an artificial neural network, similar to the
    architecture described in McCulloch and Pittâ€™s work in 1943.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![ ](Images/fdl2_0309.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. A feed-forward neural network with three layers (input, one hidden,
    and output) and three neurons per layer
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The bottom layer of the network pulls in the input data. The top layer of neurons
    (output nodes) computes our final answer. The middle layer(s) of neurons are called
    the *hidden layers*, and we letÂ  <math alttext="w Subscript i comma j Superscript
    left-parenthesis k right-parenthesis"><msubsup><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></math> Â be the weight of
    the connection between theÂ  <math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> Â neuron in theÂ  <math alttext="k
    Superscript t h"><msup><mi>k</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    Â layer with theÂ  **<math alttext="j Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>**neuron
    in the**Â  <math alttext="k plus 1 Superscript s t"><mrow><mi>k</mi> <mo>+</mo>
    <msup><mn>1</mn> <mrow><mi>s</mi><mi>t</mi></mrow></msup></mrow></math> Â **layer.
    These weights constitute our parameter vector,Â  <math alttext="theta"><mi>Î¸</mi></math>
    , and just as before, our ability to solve problems with neural networks depends
    on finding the optimal values to plug intoÂ  <math alttext="theta"><mi>Î¸</mi></math>
    .
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: We note that in this example, connections traverse only from a lower layer to
    a higher layer. There are no connections between neurons in the same layer, and
    there are no connections that transmit data from a higher layer to a lower layer.
    These neural networks are called *feed-forward* networks, and we start by discussing
    these networks because they are the simplest to analyze. We present this analysis
    (specifically, the process of selecting the optimal values for the weights) in
    [ChapterÂ 4](ch04.xhtml#training_feed_forward). More complicated connectivities
    will be addressed in later chapters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Weâ€™ll discuss the major types of layers that are utilized in feed-forward neural
    networks, but before we proceed, hereâ€™s a couple of important notes to keep in
    mind:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, the layers of neurons that lie sandwiched between the first
    layer of neurons (input layer) and the last layer of neurons (output layer) are
    called the hidden layers. This is where most of the magic is happening when the
    neural net tries to solve problems. Whereas (as in the handwritten digit example)
    we would previously have to spend a lot of time identifying useful features, the
    hidden layers automate this process for us. Oftentimes, taking a look at the activities
    of hidden layers can tell you a lot about the features the network has automatically
    learned to extract from the data.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æåˆ°çš„ï¼Œä½äºç¬¬ä¸€å±‚ç¥ç»å…ƒï¼ˆè¾“å…¥å±‚ï¼‰å’Œæœ€åä¸€å±‚ç¥ç»å…ƒï¼ˆè¾“å‡ºå±‚ï¼‰ä¹‹é—´çš„ç¥ç»å…ƒå±‚è¢«ç§°ä¸ºéšè—å±‚ã€‚è¿™æ˜¯ç¥ç»ç½‘ç»œå°è¯•è§£å†³é—®é¢˜æ—¶å‘ç”Ÿå¤§éƒ¨åˆ†é­”æ³•çš„åœ°æ–¹ã€‚ä¸ä»¥å‰éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´è¯†åˆ«æœ‰ç”¨ç‰¹å¾ï¼ˆå¦‚æ‰‹å†™æ•°å­—ç¤ºä¾‹ï¼‰ä¸åŒï¼Œéšè—å±‚ä¸ºæˆ‘ä»¬è‡ªåŠ¨åŒ–äº†è¿™ä¸ªè¿‡ç¨‹ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼ŒæŸ¥çœ‹éšè—å±‚çš„æ´»åŠ¨å¯ä»¥å‘Šè¯‰æ‚¨å¾ˆå¤šå…³äºç½‘ç»œè‡ªåŠ¨å­¦ä¹ ä»æ•°æ®ä¸­æå–çš„ç‰¹å¾ã€‚
- en: Although in [FigureÂ 3-9](#simple_example_of_a_feed_forward_neural_network) every
    layer has the same number of neurons, this is neither necessary nor recommended.
    More often than not, hidden layers have fewer neurons than the input layer to
    force the network to learn compressed representations of the original input. For
    example, while our eyes obtain raw pixel values from our surroundings, our brain
    thinks in terms of edges and contours. This is because the hidden layers of biological
    neurons in our brain, force us to come up with better representations for everything
    we perceive.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°½ç®¡åœ¨[å›¾3-9](#simple_example_of_a_feed_forward_neural_network)ä¸­æ¯ä¸€å±‚éƒ½æœ‰ç›¸åŒæ•°é‡çš„ç¥ç»å…ƒï¼Œä½†è¿™æ—¢ä¸æ˜¯å¿…è¦çš„ä¹Ÿä¸æ¨èã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œéšè—å±‚çš„ç¥ç»å…ƒæ•°é‡æ¯”è¾“å…¥å±‚å°‘ï¼Œä»¥å¼ºåˆ¶ç½‘ç»œå­¦ä¹ åŸå§‹è¾“å…¥çš„å‹ç¼©è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œè™½ç„¶æˆ‘ä»¬çš„çœ¼ç›ä»å‘¨å›´è·å–åŸå§‹åƒç´ å€¼ï¼Œä½†æˆ‘ä»¬çš„å¤§è„‘å´ä»¥è¾¹ç¼˜å’Œè½®å»“çš„å½¢å¼æ€è€ƒã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬å¤§è„‘ä¸­çš„éšè—å±‚ç”Ÿç‰©ç¥ç»å…ƒè¿«ä½¿æˆ‘ä»¬ä¸ºæˆ‘ä»¬æ‰€æ„ŸçŸ¥çš„ä¸€åˆ‡æå‡ºæ›´å¥½çš„è¡¨ç¤ºã€‚
- en: It is not required that every neuron has its output connected to the inputs
    of all neurons in the next layer. In fact, selecting which neurons to connect
    to which other neurons in the next layer is an art that comes from experience.
    Weâ€™ll discuss this issue in more depth as we work through various examples of
    neural networks.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¹¶ä¸è¦æ±‚æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºéƒ½è¿æ¥åˆ°ä¸‹ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å…¥ã€‚äº‹å®ä¸Šï¼Œé€‰æ‹©è¿æ¥åˆ°ä¸‹ä¸€å±‚å“ªäº›ç¥ç»å…ƒçš„å“ªäº›å…¶ä»–ç¥ç»å…ƒæ˜¯ä¸€é—¨æ¥è‡ªç»éªŒçš„è‰ºæœ¯ã€‚éšç€æˆ‘ä»¬é€æ­¥è®¨è®ºå„ç§ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚
- en: The inputs and outputs are *vectorized* representations. For example, you might
    imagine a neural network where the inputs are the individual pixel RGB values
    in an image represented as a vector (refer to [FigureÂ 3-3](#process_of_vectorizing)).
    The last layer might have two neurons that correspond to the answer to our problem:Â 
    <math alttext="left-bracket 1 comma 0 right-bracket"><mrow><mo>[</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>]</mo></mrow></math> Â if the image contains a dog,Â 
    <math alttext="left-bracket 0 comma 1 right-bracket"><mrow><mo>[</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow></math> Â if the image contains a cat,Â 
    <math alttext="left-bracket 1 comma 1 right-bracket"><mrow><mo>[</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow></math> Â if it contains both, andÂ  <math
    alttext="left-bracket 0 comma 0 right-bracket"><mrow><mo>[</mo> <mn>0</mn> <mo>,</mo>
    <mn>0</mn> <mo>]</mo></mrow></math> Â if it contains neither.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾“å…¥å’Œè¾“å‡ºæ˜¯*å‘é‡åŒ–*è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æƒ³è±¡ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå…¶ä¸­è¾“å…¥æ˜¯å›¾åƒä¸­è¡¨ç¤ºä¸ºå‘é‡çš„å•ä¸ªåƒç´ RGBå€¼ï¼ˆå‚è§[å›¾3-3](#process_of_vectorizing)ï¼‰ã€‚æœ€åä¸€å±‚å¯èƒ½æœ‰ä¸¤ä¸ªç¥ç»å…ƒï¼Œå¯¹åº”äºæˆ‘ä»¬é—®é¢˜çš„ç­”æ¡ˆï¼š<math
    alttext="left-bracket 1 comma 0 right-bracket"><mrow><mo>[</mo> <mn>1</mn> <mo>,</mo>
    <mn>0</mn> <mo>]</mo></mrow></math> å¦‚æœå›¾åƒåŒ…å«ç‹—ï¼Œ<math alttext="left-bracket 0 comma
    1 right-bracket"><mrow><mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow></math>
    å¦‚æœå›¾åƒåŒ…å«çŒ«ï¼Œ<math alttext="left-bracket 1 comma 1 right-bracket"><mrow><mo>[</mo>
    <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow></math> å¦‚æœä¸¤è€…éƒ½åŒ…å«ï¼Œ<math alttext="left-bracket
    0 comma 0 right-bracket"><mrow><mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo>]</mo></mrow></math>
    å¦‚æœä¸¤è€…éƒ½ä¸åŒ…å«ã€‚
- en: Weâ€™ll also observe that, similarly to our reformulation for the neuron, we can
    also mathematically express a neural network as a series of vector and matrix
    operations. Letâ€™s consider the input to the <math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> layer of the network to be a vector
    **x** = [x[1] x[2] â€¦ x[n]]. Weâ€™d like to find the vector **y** = [y[1] y[2] â€¦
    y[m]] produced by propagating the input through the neurons. We can express this
    as a simple matrix multiply if we construct a weight matrix <math alttext="bold
    upper W"><mi>ğ–</mi></math> of size <math alttext="n times m"><mrow><mi>n</mi>
    <mo>Ã—</mo> <mi>m</mi></mrow></math> and a bias vector of size <math alttext="m"><mi>m</mi></math>
    . In this matrix, each column corresponds to a neuron, where the <math alttext="j
    Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    element of the column corresponds to the weight of the connection pulling in the
    <math alttext="j Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    element of the input. In other words, **y** = Æ’(**W**^(**T**)**x** + **b**), where
    the transformation function is applied to the vector element-wise. This reformulation
    will become all the more critical as we begin to implement these networks in software.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†è§‚å¯Ÿåˆ°ï¼Œç±»ä¼¼äºæˆ‘ä»¬å¯¹ç¥ç»å…ƒçš„é‡æ–°è¡¨è¿°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†ç¥ç»ç½‘ç»œæ•°å­¦è¡¨è¾¾ä¸ºä¸€ç³»åˆ—å‘é‡å’ŒçŸ©é˜µè¿ç®—ã€‚è®©æˆ‘ä»¬å°†ç½‘ç»œçš„ç¬¬<math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>å±‚çš„è¾“å…¥è§†ä¸ºå‘é‡**x**
    = [x[1] x[2] â€¦ x[n]]ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡ç¥ç»å…ƒä¼ æ’­è¾“å…¥æ¥æ‰¾åˆ°å‘é‡**y** = [y[1] y[2] â€¦ y[m]]ã€‚å¦‚æœæˆ‘ä»¬æ„å»ºä¸€ä¸ªå¤§å°ä¸º<math
    alttext="n times m"><mrow><mi>n</mi> <mo>Ã—</mo> <mi>m</mi></mrow></math>çš„æƒé‡çŸ©é˜µ<math
    alttext="bold upper W"><mi>ğ–</mi></math>å’Œå¤§å°ä¸º<math alttext="m"><mi>m</mi></math>çš„åç½®å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªè¿‡ç¨‹è¡¨è¾¾ä¸ºç®€å•çš„çŸ©é˜µä¹˜æ³•ã€‚åœ¨è¿™ä¸ªçŸ©é˜µä¸­ï¼Œæ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ªç¥ç»å…ƒï¼Œå…¶ä¸­åˆ—çš„ç¬¬<math
    alttext="j Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>å…ƒç´ å¯¹åº”äºè¿æ¥æ‹‰å…¥è¾“å…¥çš„ç¬¬<math
    alttext="j Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>å…ƒç´ çš„æƒé‡ã€‚æ¢å¥è¯è¯´ï¼Œ**y**
    = Æ’(**W**^(**T**)**x** + **b**)ï¼Œå…¶ä¸­å˜æ¢å‡½æ•°é€å…ƒç´ åº”ç”¨äºå‘é‡ã€‚éšç€æˆ‘ä»¬å¼€å§‹åœ¨è½¯ä»¶ä¸­å®ç°è¿™äº›ç½‘ç»œï¼Œè¿™ç§é‡æ–°è¡¨è¿°å°†å˜å¾—æ›´åŠ å…³é”®ã€‚
- en: Linear Neurons and Their Limitations
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿æ€§ç¥ç»å…ƒåŠå…¶å±€é™æ€§
- en: Most neuron types are defined by the functionÂ  <math alttext="f"><mi>f</mi></math>
    Â they apply to their logit <math alttext="z"><mi>z</mi></math> . Letâ€™s first consider
    layers of neurons that use a linear function in the form of <math alttext="f left-parenthesis
    z right-parenthesis equals a z plus b"><mrow><mi>f</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>z</mi> <mo>+</mo> <mi>b</mi></mrow></math>
    . For example, a neuron that attempts to estimate a cost of a meal in a fast-food
    restaurant would use a linear neuron where <math alttext="a equals 1"><mrow><mi>a</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> and <math alttext="b equals 0"><mrow><mi>b</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> . Using <math alttext="f left-parenthesis
    z right-parenthesis equals z"><mrow><mi>f</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo>
    <mo>=</mo> <mi>z</mi></mrow></math> and weights equal to the price of each item,
    the linear neuron in [FigureÂ 3-10](#example_of_a_linear_neuron) would take in
    some ordered triple of servings of burgers, fries, and sodas, and output the price
    of the combination.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç¥ç»å…ƒç±»å‹ç”±å®ƒä»¬åº”ç”¨äºlogit <math alttext="z"><mi>z</mi></math> çš„å‡½æ•° <math alttext="f"><mi>f</mi></math>
    å®šä¹‰ã€‚è®©æˆ‘ä»¬é¦–å…ˆè€ƒè™‘ä½¿ç”¨çº¿æ€§å‡½æ•°çš„ç¥ç»å…ƒå±‚ï¼Œå½¢å¼ä¸º <math alttext="f left-parenthesis z right-parenthesis
    equals a z plus b"><mrow><mi>f</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo>
    <mi>a</mi> <mi>z</mi> <mo>+</mo> <mi>b</mi></mrow></math>ã€‚ä¾‹å¦‚ï¼Œè¯•å›¾ä¼°ç®—å¿«é¤åº—é¤ç‚¹æˆæœ¬çš„ç¥ç»å…ƒå°†ä½¿ç”¨çº¿æ€§ç¥ç»å…ƒï¼Œå…¶ä¸­
    <math alttext="a equals 1"><mrow><mi>a</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    å’Œ <math alttext="b equals 0"><mrow><mi>b</mi> <mo>=</mo> <mn>0</mn></mrow></math>ã€‚ä½¿ç”¨
    <math alttext="f left-parenthesis z right-parenthesis equals z"><mrow><mi>f</mi>
    <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo> <mi>z</mi></mrow></math> å’Œæƒé‡ç­‰äºæ¯ä¸ªé¡¹ç›®çš„ä»·æ ¼ï¼Œ[å›¾3-10](#example_of_a_linear_neuron)ä¸­çš„çº¿æ€§ç¥ç»å…ƒå°†æ¥æ”¶ä¸€äº›æœ‰åºçš„ä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬æ±‰å ¡åŒ…ã€è–¯æ¡å’Œè‹æ‰“æ°´çš„ä»½æ•°ï¼Œå¹¶è¾“å‡ºç»„åˆçš„ä»·æ ¼ã€‚
- en: '![ ](Images/fdl2_0310.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0310.png)'
- en: Figure 3-10\. An example of a linear neuron
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-10. çº¿æ€§ç¥ç»å…ƒçš„ç¤ºä¾‹
- en: Linear neurons are easy to compute with, but they run into serious limitations.
    In fact, it can be shown that any feed-forward neural network consisting of only
    linear neurons can be expressed as a network with no hidden layers. This is problematic
    because, as we discussed, hidden layers are what enable us to learn important
    features from the input data. In other words, to learn complex relationships,
    we need to use neurons that employ some sort of nonlinearity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§ç¥ç»å…ƒæ˜“äºè®¡ç®—ï¼Œä½†å­˜åœ¨ä¸¥é‡é™åˆ¶ã€‚äº‹å®ä¸Šï¼Œå¯ä»¥è¯æ˜ä»…ç”±çº¿æ€§ç¥ç»å…ƒç»„æˆçš„å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥è¡¨ç¤ºä¸ºæ²¡æœ‰éšè—å±‚çš„ç½‘ç»œã€‚è¿™æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› ä¸ºæ­£å¦‚æˆ‘ä»¬è®¨è®ºçš„é‚£æ ·ï¼Œéšè—å±‚ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»è¾“å…¥æ•°æ®ä¸­å­¦ä¹ é‡è¦ç‰¹å¾ã€‚æ¢å¥è¯è¯´ï¼Œä¸ºäº†å­¦ä¹ å¤æ‚å…³ç³»ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨é‡‡ç”¨æŸç§éçº¿æ€§çš„ç¥ç»å…ƒã€‚
- en: Sigmoid, Tanh, and ReLU Neurons
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoidï¼ŒTanhå’ŒReLUç¥ç»å…ƒ
- en: 'Three major types of neurons are used in practice that introduce nonlinearities
    in their computations. The first of these is the *sigmoid neuron*, which uses
    the function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å®è·µä¸­ä½¿ç”¨ä¸‰ç§ä¸»è¦ç±»å‹çš„ç¥ç»å…ƒï¼Œåœ¨è®¡ç®—ä¸­å¼•å…¥éçº¿æ€§ã€‚å…¶ä¸­ä¹‹ä¸€æ˜¯*sigmoidç¥ç»å…ƒ*ï¼Œå®ƒä½¿ç”¨å‡½æ•°ï¼š
- en: <math alttext="f left-parenthesis z right-parenthesis equals StartFraction 1
    Over 1 plus e Superscript negative z Baseline EndFraction"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis z right-parenthesis equals StartFraction 1
    Over 1 plus e Superscript negative z Baseline EndFraction"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math>
- en: Intuitively, this means that when the logit is very small, the output of a logistic
    neuron is close to 0\. When the logit is very large, the output of the logistic
    neuron is close to 1\. In-between these two extremes, the neuron assumes an S-shape,
    as shown inÂ [FigureÂ 3-11](#otuput_of_a_sigmoid_neuron).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°è¯´ï¼Œå½“logitéå¸¸å°æ—¶ï¼Œé€»è¾‘ç¥ç»å…ƒçš„è¾“å‡ºæ¥è¿‘0ã€‚å½“logitéå¸¸å¤§æ—¶ï¼Œé€»è¾‘ç¥ç»å…ƒçš„è¾“å‡ºæ¥è¿‘1ã€‚åœ¨è¿™ä¸¤ä¸ªæç«¯ä¹‹é—´ï¼Œç¥ç»å…ƒå‘ˆSå½¢çŠ¶ï¼Œå¦‚[å›¾3-11](#otuput_of_a_sigmoid_neuron)æ‰€ç¤ºã€‚
- en: '![ ](Images/fdl2_0311.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0311.png)'
- en: Figure 3-11\. The output of a sigmoid neuron as z varies
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-11. éšç€zå˜åŒ–çš„sigmoidç¥ç»å…ƒçš„è¾“å‡º
- en: '*Tanh neurons*Â use a similar kind of S-shaped nonlinearity, but instead of
    ranging from 0 to 1, the output of tanh neurons ranges from âˆ’1 to 1\. As you would
    expect, they useÂ  <math alttext="f left-parenthesis z right-parenthesis equals
    hyperbolic tangent left-parenthesis z right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>z</mi> <mo>)</mo> <mo>=</mo> <mo form="prefix">tanh</mo> <mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow></math> . The resulting relationship between the outputÂ  <math
    alttext="y"><mi>y</mi></math> Â and the logitÂ  <math alttext="z"><mi>z</mi></math>
    Â is depicted inÂ [FigureÂ 3-12](#output_of_a_tanh_neuron). When S-shaped nonlinearities
    are used, the tanh neuron is often preferred over the sigmoid neuron because it
    is zero-centered.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*Tanhç¥ç»å…ƒ*ä½¿ç”¨ç±»ä¼¼çš„Så½¢éçº¿æ€§ï¼Œä½†è¾“å‡ºèŒƒå›´ä»-1åˆ°1ï¼Œè€Œä¸æ˜¯ä»0åˆ°1ã€‚æ­£å¦‚æ‚¨æ‰€æœŸæœ›çš„é‚£æ ·ï¼Œå®ƒä»¬ä½¿ç”¨ <math alttext="f left-parenthesis
    z right-parenthesis equals hyperbolic tangent left-parenthesis z right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo> <mo form="prefix">tanh</mo> <mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow></math>ã€‚è¾“å‡º <math alttext="y"><mi>y</mi></math> ä¸logit
    <math alttext="z"><mi>z</mi></math> ä¹‹é—´çš„å…³ç³»å¦‚[å›¾3-12](#output_of_a_tanh_neuron)æ‰€ç¤ºã€‚å½“ä½¿ç”¨Så½¢éçº¿æ€§æ—¶ï¼Œé€šå¸¸ä¼˜å…ˆé€‰æ‹©tanhç¥ç»å…ƒè€Œä¸æ˜¯sigmoidç¥ç»å…ƒï¼Œå› ä¸ºå®ƒæ˜¯ä»¥é›¶ä¸ºä¸­å¿ƒçš„ã€‚'
- en: A different kind of nonlinearity is used by the *Rectified Linear Unit (ReLU)
    neuron*. It uses the functionÂ  <math alttext="f left-parenthesis z right-parenthesis
    equals max left-parenthesis 0 comma z right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>z</mi> <mo>)</mo> <mo>=</mo> <mo movablelimits="true" form="prefix">max</mo>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></math> , resulting
    in a characteristic hockey-stick-shaped response, as shown inÂ [FigureÂ 3-13](#output_of_a_relu_neuron).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*Rectified Linear Unit (ReLU)ç¥ç»å…ƒ*ä½¿ç”¨ä¸åŒç±»å‹çš„éçº¿æ€§ã€‚å®ƒä½¿ç”¨å‡½æ•° <math alttext="f left-parenthesis
    z right-parenthesis equals max left-parenthesis 0 comma z right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo> <mo movablelimits="true" form="prefix">max</mo>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></math>ï¼Œå¯¼è‡´å…·æœ‰ç‰¹å¾æ›²æ£çƒå½¢çŠ¶å“åº”ï¼Œå¦‚[å›¾3-13](#output_of_a_relu_neuron)æ‰€ç¤ºã€‚'
- en: The ReLU has recently become the neuron of choice for many tasks (especially
    in computer vision) for a number of reasons, despite some drawbacks.^([8](ch03.xhtml#idm45934165761792))
    Weâ€™ll discuss these reasons in [ChapterÂ 7](ch07.xhtml#convolutional_neural_networks),
    as well as strategies to combat the potential pitfalls.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒReLUå·²æˆä¸ºè®¸å¤šä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼‰çš„é¦–é€‰ç¥ç»å…ƒï¼Œå°½ç®¡å­˜åœ¨ä¸€äº›ç¼ºç‚¹ã€‚æˆ‘ä»¬å°†åœ¨[ç¬¬7ç« ](ch07.xhtml#convolutional_neural_networks)ä¸­è®¨è®ºè¿™äº›åŸå› ï¼Œä»¥åŠåº”å¯¹æ½œåœ¨é—®é¢˜çš„ç­–ç•¥ã€‚
- en: '![ ](Images/fdl2_0312.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0312.png)'
- en: Figure 3-12\. The output of a tanh neuron as z varies
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-12. éšç€zå˜åŒ–çš„tanhç¥ç»å…ƒçš„è¾“å‡º
- en: '![ ](Images/fdl2_0313.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![ ](Images/fdl2_0313.png)'
- en: Figure 3-13\. The output of a ReLU neuron as z varies
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-13. éšç€zå˜åŒ–çš„ReLUç¥ç»å…ƒçš„è¾“å‡º
- en: Softmax Output Layers
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmaxè¾“å‡ºå±‚
- en: 'Oftentimes, we want our output vector to be a probability distribution over
    a set of mutually exclusive labels. For example, letâ€™s say we want to build a
    neural network to recognize handwritten digits from the MNIST dataset. Each label
    (0 through 9) is mutually exclusive, but itâ€™s unlikely that we will be able to
    recognize digits with 100% confidence. Using a probability distribution gives
    us a better idea of how confident we are in our predictions. As a result, the
    desired output vector is of the following form, where <math alttext="sigma-summation
    Underscript i equals 0 Overscript 9 Endscripts p Subscript i Baseline equals 1"><mrow><msubsup><mo>âˆ‘</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mn>9</mn></msubsup> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math> :'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæˆ‘ä»¬å¸Œæœ›è¾“å‡ºå‘é‡æ˜¯ä¸€ç»„ç›¸äº’æ’æ–¥æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«MNISTæ•°æ®é›†ä¸­çš„æ‰‹å†™æ•°å­—ã€‚æ¯ä¸ªæ ‡ç­¾ï¼ˆ0åˆ°9ï¼‰æ˜¯ç›¸äº’æ’æ–¥çš„ï¼Œä½†æˆ‘ä»¬ä¸å¤ªå¯èƒ½èƒ½å¤Ÿä»¥100%çš„ä¿¡å¿ƒè¯†åˆ«æ•°å­—ã€‚ä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒå¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°äº†è§£æˆ‘ä»¬å¯¹é¢„æµ‹çš„ä¿¡å¿ƒã€‚å› æ­¤ï¼ŒæœŸæœ›çš„è¾“å‡ºå‘é‡å…·æœ‰ä»¥ä¸‹å½¢å¼ï¼Œå…¶ä¸­<math
    alttext="sigma-summation Underscript i equals 0 Overscript 9 Endscripts p Subscript
    i Baseline equals 1"><mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></row>
    <mn>9</mn></msubsup> <msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>ï¼š
- en: <math alttext="Start 1 By 6 Matrix 1st Row 1st Column p 0 2nd Column p 1 3rd
    Column p 2 4th Column p 3 5th Column  ellipsis 6th Column p 9 EndMatrix"><mrow><mo>[</mo>
    <mtable><mtr><mtd><msub><mi>p</mi> <mn>0</mn></msub></mtd> <mtd><msub><mi>p</mi>
    <mn>1</mn></msub></mtd> <mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>p</mi> <mn>9</mn></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 1 By 6 Matrix 1st Row 1st Column p 0 2nd Column p 1 3rd
    Column p 2 4th Column p 3 5th Column  ellipsis 6th Column p 9 EndMatrix"><mrow><mo>[</mo>
    <mtable><mtr><mtd><msub><mi>p</mi> <mn>0</mn></msub></mtd> <mtd><msub><mi>p</mi>
    <mn>1</mn></msub></mtd> <mtd><msub><mi>p</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>p</mi>
    <mn>3</mn></msub></mtd> <mtd><mo>...</mo></mtd> <mtd><msub><mi>p</mi> <mn>9</mn></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></math>
- en: 'This is achieved by using a special output layer called a *softmax layer*.
    Unlike in other kinds of layers, the output of a neuron in a softmax layer depends
    on the outputs of all the other neurons in its layer. This is because we require
    the sum of all the outputs to be equal to 1\. Letting <math alttext="z Subscript
    i"><msub><mi>z</mi> <mi>i</mi></msub></math> be the logit of theÂ  <math alttext="i
    Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    softmax neuron, we can achieve this normalization by setting its output to:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é€šè¿‡ä½¿ç”¨ç§°ä¸º*softmaxå±‚*çš„ç‰¹æ®Šè¾“å‡ºå±‚æ¥å®ç°çš„ã€‚ä¸å…¶ä»–ç±»å‹çš„å±‚ä¸åŒï¼Œsoftmaxå±‚ä¸­ç¥ç»å…ƒçš„è¾“å‡ºå–å†³äºå…¶å±‚ä¸­æ‰€æœ‰å…¶ä»–ç¥ç»å…ƒçš„è¾“å‡ºã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¦æ±‚æ‰€æœ‰è¾“å‡ºçš„æ€»å’Œç­‰äº1ã€‚è®©<math
    alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>æ˜¯ç¬¬iä¸ªsoftmaxç¥ç»å…ƒçš„é€»è¾‘ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å…¶è¾“å‡ºè®¾ç½®ä¸ºæ¥å®ç°è¿™ç§å½’ä¸€åŒ–ï¼š
- en: <math alttext="y Subscript i Baseline equals StartFraction e Superscript z Super
    Subscript i Superscript Baseline Over sigma-summation Underscript j Endscripts
    e Superscript z Super Subscript j Superscript Baseline EndFraction"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><msup><mi>e</mi> <msub><mi>z</mi> <mi>i</mi></msub></msup>
    <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>z</mi> <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript i Baseline equals StartFraction e Superscript z Super
    Subscript i Superscript Baseline Over sigma-summation Underscript j Endscripts
    e Superscript z Super Subscript j Superscript Baseline EndFraction"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><msup><mi>e</mi> <msub><mi>z</mi> <mi>i</mi></msub></msup>
    <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>z</mi> <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
- en: A strong prediction would have a single entry in the vector close to 1, while
    the remaining entries would be close to 0\. A weak prediction would have multiple
    possible labels that are more or less equally likely.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºé¢„æµ‹å°†åœ¨å‘é‡ä¸­æœ‰ä¸€ä¸ªæ¥è¿‘1çš„å•ä¸ªæ¡ç›®ï¼Œè€Œå…¶ä½™æ¡ç›®å°†æ¥è¿‘0ã€‚å¼±é¢„æµ‹å°†æœ‰å¤šä¸ªå¯èƒ½çš„æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾æ›´æˆ–å¤šæˆ–å°‘æ˜¯ç­‰å¯èƒ½çš„ã€‚
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, weâ€™ve built a basic intuition for machine learning and neural
    networks. Weâ€™ve talked about the basic structure of a neuron, how feed-forward
    neural networks work, and the importance of nonlinearity in tackling complex learning
    problems. In the next chapter, we will begin to build the mathematical background
    necessary to train a neural network to solve problems. Specifically, we will talk
    about finding optimal parameter vectors, best practices while training neural
    networks, and major challenges. In later chapters, we will take these foundational
    ideas to build more specialized neural architectures.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¯¹æœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„åŸºæœ¬ç›´è§‰ã€‚æˆ‘ä»¬è®¨è®ºäº†ç¥ç»å…ƒçš„åŸºæœ¬ç»“æ„ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ï¼Œä»¥åŠåœ¨è§£å†³å¤æ‚å­¦ä¹ é—®é¢˜æ—¶éçº¿æ€§çš„é‡è¦æ€§ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹å»ºç«‹è®­ç»ƒç¥ç»ç½‘ç»œè§£å†³é—®é¢˜æ‰€éœ€çš„æ•°å­¦èƒŒæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•æ‰¾åˆ°æœ€ä½³å‚æ•°å‘é‡ï¼Œåœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶çš„æœ€ä½³å®è·µä»¥åŠä¸»è¦æŒ‘æˆ˜ã€‚åœ¨åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨è¿™äº›åŸºç¡€æ€§æ€æƒ³æ„å»ºæ›´ä¸“ä¸šåŒ–çš„ç¥ç»æ¶æ„ã€‚
- en: ^([1](ch03.xhtml#idm45934167495712-marker)) Kuhn, Deanna, et al. *Handbook of
    Child Psychology. Vol. 2, Cognition, Perception, and Language*. Wiley, 1998.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 1. Kuhn, Deannaç­‰äººã€‚ã€Šå„¿ç«¥å¿ƒç†å­¦æ‰‹å†Œã€‹ã€‚ç¬¬2å·ï¼Œè®¤çŸ¥ã€æ„ŸçŸ¥å’Œè¯­è¨€ã€‚Wileyï¼Œ1998å¹´ã€‚
- en: ^([2](ch03.xhtml#idm45934169246960-marker)) Y. LeCun, L. Bottou, Y. Bengio,
    and P. Haffner. â€œGradient-Based Learning Applied to Document Recognition.â€ *Proceedings
    of the IEEE*, 86(11):2278-2324, November 1998.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 2. Y. LeCun, L. Bottou, Y. Bengioå’ŒP. Haffnerã€‚"åŸºäºæ¢¯åº¦çš„å­¦ä¹ åº”ç”¨äºæ–‡æ¡£è¯†åˆ«ã€‚"ã€ŠIEEEä¼šè®®å½•ã€‹ï¼Œ86(11)ï¼š2278-2324ï¼Œ1998å¹´11æœˆã€‚
- en: '^([3](ch03.xhtml#idm45934169203808-marker)) Rosenblatt, Frank. â€œThe perceptron:
    A Probabilistic Model for Information Storage and Organization in the Brain.â€
    *Psychological Review* 65.6 (1958): 386.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 3. Rosenblatt, Frankã€‚"æ„ŸçŸ¥å™¨ï¼šå¤§è„‘ä¸­ä¿¡æ¯å­˜å‚¨å’Œç»„ç»‡çš„æ¦‚ç‡æ¨¡å‹ã€‚"ã€Šå¿ƒç†è¯„è®ºã€‹65.6ï¼ˆ1958ï¼‰ï¼š386ã€‚
- en: '^([4](ch03.xhtml#idm45934168242992-marker)) Bubeck, SÃ©bastien. â€œConvex Optimization:
    Algorithms and Complexity.â€ *Foundations and TrendsÂ® in Machine Learning*. 8.3-4
    (2015): 231-357.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 4. Bubeck, SÃ©bastienã€‚"å‡¸ä¼˜åŒ–ï¼šç®—æ³•å’Œå¤æ‚æ€§ã€‚"ã€Šæœºå™¨å­¦ä¹ åŸºç¡€ä¸è¶‹åŠ¿ã€‹ã€‚8.3-4ï¼ˆ2015ï¼‰ï¼š231-357ã€‚
- en: ^([5](ch03.xhtml#idm45934168225968-marker)) Restak, Richard M. and David Grubin.
    *The Secret Life of the Brain*. Joseph Henry Press, 2001.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 5. Restak, Richard M.å’ŒDavid Grubinã€‚ã€Šå¤§è„‘çš„ç§˜å¯†ç”Ÿæ´»ã€‹ã€‚Joseph Henry Pressï¼Œ2001å¹´ã€‚
- en: '^([6](ch03.xhtml#idm45934168212000-marker)) McCulloch, Warren S., and Walter
    Pitts. â€œA Logical Calculus of the Ideas Immanent in Nervous Activity.â€ *The Bulletin
    of Mathematical Biophysics*. 5.4 (1943): 115-133.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 6. McCulloch, Warren S.å’ŒWalter Pittsã€‚"ç¥ç»æ´»åŠ¨ä¸­å†…åœ¨æ€æƒ³çš„é€»è¾‘æ¼”ç®—ã€‚"ã€Šæ•°å­¦ç”Ÿç‰©ç‰©ç†å­¦å…¬æŠ¥ã€‹5.4ï¼ˆ1943ï¼‰ï¼š115-133ã€‚
- en: '^([7](ch03.xhtml#idm45934167717376-marker)) Mountcastle, Vernon B. â€œModality
    and Topographic Properties of Single Neurons of Catâ€™s Somatic Sensory Cortex.â€
    *Journal of Neurophysiology* 20.4 (1957): 408-434.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 7. Mountcastle, Vernon B.ã€‚â€œçŒ«ä½“æ„Ÿè§‰çš®å±‚å•ä¸ªç¥ç»å…ƒçš„æ¨¡æ€æ€§å’Œæ‹“æ‰‘ç‰¹æ€§ã€‚â€ã€Šç¥ç»ç”Ÿç†å­¦æ‚å¿—ã€‹20.4ï¼ˆ1957ï¼‰ï¼š408-434ã€‚
- en: ^([8](ch03.xhtml#idm45934165761792-marker)) Nair, Vinod, and Geoffrey E. Hinton.
    â€œRectified Linear Units Improve Restricted Boltzmann Machines.â€ *Proceedings of
    the 27th International Conference on Machine Learning* (ICML-10), 2010.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 8. Nair, Vinodå’ŒGeoffrey E. Hintonã€‚"ä¿®æ­£çº¿æ€§å•å…ƒæ”¹è¿›äº†å—é™ç»å°”å…¹æ›¼æœºã€‚"ã€Šç¬¬27å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†ã€‹ï¼ˆICML-10ï¼‰ï¼Œ2010å¹´ã€‚
