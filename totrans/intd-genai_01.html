<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">2 <a id="idTextAnchor000"/>Training large language models</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Explaining how LLMs are trained</li>
<li class="co-summary-bullet">Introducing the emergent properties of LLMs</li>
<li class="co-summary-bullet">Exploring the harms and vulnerabilities that come from training LLMs</li>
</ul>
<p class="body"><a id="marker-34"/>For decades, the digital economy has run on the currency of data. The digital economy of collecting and trading information about who we are and what we do online is worth trillions of dollars, and as more of our daily activities have moved on to the internet, the mill has ever more grist to grind through. Large language models (LLMs) are inventions of the internet age, emulating human language by vacuuming up terabytes of text data found online.</p>
<p class="body">The process has yielded both predictable and unpredictable results. Notably, there are significant questions about both what is in the datasets used by LLMs and how to prevent the models from replicating some of the more objectionable text they hold in their training sets. With data collection at this scale, the collection of personal information and low-quality, spammy, or offensive content is expected, but how to address the problem is another challenge. LLMs at the scale we’re now seeing have exhibited a host of capabilities that don’t seem to be available to smaller language models. These properties make LLMs more attractive for a variety of uses and ensure that the race toward more and more data and bigger and bigger models won’t end anytime soon.</p>
<p class="body">In this chapter, you’ll learn more about how LLMs are trained to understand what makes them unique compared to previous models and how these characteristics result in both new capabilities and potential vulnerabilities.</p>
<h2 class="fm-head" id="heading_id_3"><a id="idTextAnchor001"/>How are LLMs trained?</h2>
<p class="body">In chapter 1, we introduced some of the concepts involved in training LLMs. We covered transformer architecture, a specific type of neural network used in LLMs, and talked about some of the sources of data that LLMs use. We also explained the self-supervised task they are trained to complete—generate the next most probable word or character, also known as token prediction. Here we’ll examine the training process in greater detail and discuss perhaps the most surprising and exciting aspect of LLMs—their emergent properties, that is, things they weren’t trained to do, but do well anyway.<a id="idIndexMarker000"/><a id="marker-35"/></p>
<p class="body">The first step of creating an LLM, often called the <i class="fm-italics">pre-training</i> step, is training on some token prediction task (for a generative model, autoregression or causal token prediction) with a gigantic corpus of data. It is called pre-training because even though this is a training phase, the knowledge encoded by the model during this phase is foundational to any subsequent natural language task. Then, the model is fine-tuned on one or many additional tasks, that is, trained with labeled data and a specific objective. Dialogue agents such as ChatGPT might be fine-tuned on conversational data; many generative models are fine-tuned on instruction datasets to improve their capability to follow instructions (e.g., “Write me a poem”); others might be fine-tuned for code generation. This process is pictured in figure 2.1, but it’s worthwhile to take a deeper look at each of these stages.<a id="idIndexMarker001"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="220" src="../../OEBPS/Images/CH02_F01_Dhamani.png" width="422"/><a id="idTextAnchor002"/></p>
<p class="figurecaption">Figure 2.1 The high-level training process for LLMs<a id="idTextAnchor003"/></p>
</div>
<h3 class="fm-head1" id="heading_id_4">Exploring open web data collection</h3>
<p class="body"><a id="marker-36"/>To model natural language and then generate language convincingly, LLMs need lots and lots of examples. Let’s consider all the implicit knowledge that goes into question-answering tasks. First, the model must have an accurate representation of both the question and the context (what the question is being asked about), which in turn means having a representation for each of the tokens in the question and context—analogous to knowing what the words themselves mean. The model must also be able to parse the question syntactically to identify what is being asked and then produce an answer, either from the context (the open-book case) or from its internal representation of external concepts (the closed-book case). Because LLMs have seen so much text from the internet, most would be able to answer a question like, “Who was the first president of the United States?” correctly without any provided context. More obscure information might result in an incorrect or made-up answer because the model wouldn’t have a high-probability response. Notably, if we ask ChatGPT, “Who was the first president?” without specifying that we are asking about the United States, ChatGPT responds, “The first president of the United States was George Washington.”<a id="idIndexMarker002"/><a id="idIndexMarker003"/></p>
<p class="body">LLMs use data from the open web, which refers to all public web pages on the internet, including sites such as Wikipedia and Reddit, but also possibly non-password-protected blogs, news aggregators, and non-private forums. Why does ChatGPT assume we’re asking about the United States? To be fair, the answer might be different if the request came from an IP address in another country, but the assumption also belies an indisputable fact about internet data—most of it is in English, and a disproportionate amount of it is from the United States and Western Europe. In chapter 1, we mentioned that Wikipedia is one of the classic data sources for LLMs. While the encyclopedia’s global geographic coverage continues to improve, there are more than 6.6 million articles in the English Wikipedia, whereas the next-highest total is 2.5 million articles in the French Wikipedia. The downstream effects of this are that the LLMs are better at understanding, generating, and completing tasks in English. They also better understand topics relevant to North America and Western Europe, and therefore serve these audiences better.</p>
<p class="body"><a id="marker-37"/>To get a sense of other types of text datasets in use, we can look at open data repositories, such as that of the open source AI company Hugging Face (see <a class="url" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a>). Open data is available for anyone to download and use for their projects, although sometimes the type of permissible use is restricted by the data’s license; for example, a dataset provider might specify that the dataset should be used for academic or research purposes only, not in commercial applications. One dataset for language models consists of millions of Reddit posts (with non-English posts filtered out). Others include collections of news articles, reviews from sites such as Amazon and Rotten Tomatoes (review-aggregation website for movies and TV shows), or questions and answers from the community Q&amp;A site Stack Exchange. Common Crawl is a nonprofit that maintains a massive repository of web page data and provides it for public use (see <a class="url" href="https://commoncrawl.org/">https://commoncrawl.org/</a>). In short, anywhere that people are writing online is a potential data source.</p>
<p class="body">Companies that develop LLMs might use a combination of open datasets such as those on Hugging Face, datasets that they purchase from third-party vendors, datasets that they collect themselves by scraping the web, or datasets that they create themselves by writing examples for the models to learn from. Although the initial training of the LLM might not require any manual intervention, as we’ll see, crowdsourcing and conversational collection are important in improving the model’s performance in specific domains, such as dialogue for chatbot<a id="idTextAnchor004"/>s.</p>
<h3 class="fm-head1" id="heading_id_5">Demystifying autoregression and bidirectional token prediction</h3>
<p class="body">Some of the first LLMs, such as Google’s BERT, were focused much more heavily on natural language understanding, as compared to generative use cases such as chatbots. Because of this objective, BERT is known as a bidirectional model, meaning BERT was trained to predict the missing word (token) within a sentence and has access to both the left and right contexts (the bidirectional part). This is ideal for natural language understanding because the model picks up more information about the contexts a particular word is used in. However, if a model is used for text generation, it shouldn’t be trained on anything that comes after the missing token because it would only ever have access to the text that preceded it. This type of model is called <i class="fm-italics">autoregressive</i>, because future predictions are dependent on the model’s past data. All the models in the GPT family, as well as Google’s Pathways Language Model (PaLM), are autoregressive. <a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/><a id="marker-38"/></p>
<p class="fm-callout"><span class="fm-callout-head">Autoregressive</span> means that future predictions are dependent on the model’s past data.</p>
<p class="body">As an example, consider the sentence, “For their honeymoon, they flew to _____ and had a romantic dinner in front of the Eiffel Tower.” The correct word for a model to predict here is “Paris”. In this case, the right context (what happens after the missing word) is especially informative, and a bidirectional model would very likely answer correctly. But when a model is asked to generate text, such as, “A good location for a romantic honeymoon is ______,” the task is structured such that the model’s completion is at the end of the context. Therefore, the model’s training should only use the left context (what comes before the missing word) to predict the missing tokens. The model learns by self-supervision, repeatedly guessing the final token in billions of examples from the text and adjusting its weights based on the correct token, until the model’s performance on guessing missing tokens in the training data is optimal. When we chat with ChatGPT, it doesn’t appear to look like a formal task to the user, but under the hood, the model is predicting what should come next after each message. When I type, “Hey! What’s up?” the logical and likeliest completions are to answer the question and return the greet<a id="idTextAnchor005"/>ing.</p>
<h3 class="fm-head1" id="heading_id_6">Fine-tuning LLMs</h3>
<p class="body">Once trained on the token completion task, a model can generate words, phrases, or complete sentences. At this stage the models are called foundation or base models because they provide the foundational knowledge, due to their complex representations of thousands of different words and concepts, for perfo<a id="idTextAnchor006"/>rming natural language processing (NLP) tasks.<a id="marker-39"/></p>
<p class="body">Although these base models aren’t that impressive out of the box, they can be easily adapted to do well in specific tasks through <i class="fm-italics">fine-tuning</i>, that is collecting labeled datasets that demonstrate the specific task or tasks the model needs to improve on. These tasks might be very narrow, such as a classification problem requiring specific domain expertise, or quite broad. Many commercial LLMs are fine-tuned on instruction-following data so that the models can better respond to inputs such as “Write a song,” or “Tell me a joke.” Other fine-tuning tasks are also common uses for LLMs, such as summarization and question answering. From a technical perspective, fine-tuning trains a neural network in a supervised fashion, but instead of starting from scratch, the neural network is initialized with the weights of the foundation model. Whereas training the foundation model takes weeks and uses large amounts of computing resources, fine-tuning can be done in minutes. The fine-tuned model uses the representations of the original but then adjusts its own weights and parameters to best fit the new <a id="idTextAnchor007"/>data. <a id="idIndexMarker007"/></p>
<h2 class="fm-head" id="heading_id_7">The unexpected: Emergent properties of LLMs</h2>
<p class="body">In some respects, LLMs are natural extensions of predecessor neural network models. Before the transformer architecture made it efficient to build larger and larger models, it was well-known that model size correlates with model performance on a range of common NLP tasks, and, in many cases, such performance improvements could be predicted based on empirically derived scaling laws. However, LLMs have also yielded behaviors, called emergent properties, that no one could have predicted via a scaling law. In a 2022 survey on the emergent abilities of LLMs, emergence is defined as “when quantitative changes in a system result in qualitative changes in behavior” <a class="url" href="http://arxiv.org/abs/2206.07682"><span>[1]</span></a>. In other words, we might expect that for a particular task, a model with 100 billion parameters would achieve 10% higher accuracy than a model with 100 million parameters. But the model with 100 billion parameters—an LLM—can now do tasks that the smaller model can’t and in somewhat unpredictable and unexpected ways.<a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="marker-40"/></p>
<p class="fm-callout"><span class="fm-callout-head">Emergent properties</span> are abilities that LLMs begin to exhibit at very large model sizes with behaviors that are qualitatively different from those of smaller<a id="idTextAnchor008"/> models. <a id="idIndexMarker010"/></p>
<h3 class="fm-head1" id="heading_id_8">Quick study: Learning with few examples</h3>
<p class="body">When talking about the emergent capabilities of LLMs, it’s useful to compare them to the capabilities derived from the process described in the previous section. In the standard case, the model is pre-trained and fine-tuned for one or many natural language abilities, such as translation or analogy completion. These abilities are part of the training pipeline and are considered predictable—not in exactly how the model will perform but in how the model improves as it’s trained. <a id="idIndexMarker011"/><a id="idIndexMarker012"/></p>
<p class="body">On the other hand, the primary examples of emergent abilities are zero-shot and few-shot learning. The terms <i class="fm-italics">zero-shot</i> and <i class="fm-italics">few-shot</i> refer to the number of examples that the model is given before being asked to perform a task. For instance, let’s say that a restaurateur wants to add visual indicators for vegetarian dishes on their restaurant’s menu. Using ChatGPT, they might write something like, “Please rewrite this menu and put an asterisk next to all dishes that do not contain any meat,” and then copy and paste the menu. This might seem like a trivial task for a human, but the model must first interpret the request, then classify each written menu item according to whether or not it contains meat, and finally produce the output in the corresponding format. The level of natural language understanding and generative ability required to complete such a task with no previous examples (we can safely assume that the model was never trained explicitly to do this) isn’t observed in previous language models, and yet LLMs can produce impressive results <a id="idTextAnchor009"/>on many such zero-shot tasks, where the model has never seen the task before.<a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="marker-41"/></p>
<p class="fm-callout"><span class="fm-callout-head">Zero-shot</span> or <span class="fm-callout-head">few-shot</span> refers to the number of examples that the model is given before being asked to perform a task.<a id="idIndexMarker015"/><a id="idIndexMarker016"/></p>
<p class="body">In the few-shot case, the model is given a few examples of the task in the <i class="fm-italics">prompt</i>, the text that the model takes as input to determine what output it should generate. In the previous zero-shot example, the user’s request constituted part or all of the model’s prompt (models are sometimes deployed with a base prompt, which might provide generic instructions on how to respond to inputs but isn’t relevant to this discussion). Another user might want the model to perform a slightly more complex task. Let’s say a freelance writer is working on three different pieces—one about dog breeding, one about exoplanets, and one about Pittsburgh—and wants to organize a list of articles by topic. In this case, they might write something like:<a id="idIndexMarker017"/></p>
<p class="fm-quote">Each of the following articles is related to one of “dog breeding,” “exoplanets,” or “Pittsburgh”. For each article, write the most likely related topic from those three topics.</p>
<p class="body">This could be structured as a zero-shot task as well. However, it’s generally beneficial to model performance to provide a few examples, so if the response wasn’t exactly what the writer wanted, they might try to provide additional guidance:</p>
<p class="fm-quote">Example: “The latest discovery of space telescopes”: Exoplanets; Example: “Why pugs have breathing problems”: Dog breeding; and so on.</p>
<p class="body">Figure 2.2 shows how zero-shot and few-shot prompts differ from fine-tuning a model for a task. If you’ve used an LLM to perform one of these tasks, you might have tried zero-shot and few-shot learning without even thinking about it or realizing it. This is one of the great strengths of LLMs: because the interface with these chatbots is simply natural language, we can often tweak the inputs to achieve the desired outputs in a much more intuitive way than we might with<a id="idTextAnchor010"/> other models.<a id="marker-42"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="242" src="../../OEBPS/Images/CH02_F02_Dhamani.png" width="699"/></p>
<p class="figurecaption">Figure 2.2 A comparison of fine-tuning, zero-shot learning, and one-shot learning on a machine translation task</p>
</div>
<p class="body">In addition to zero-shot and few-shot examples in the model’s prompts, other changes to the model’s prompt have uncovered additional emergent abilities. A <a id="idTextAnchor011"/>technique called chain-of-thought prompting, or directing the model to break apart challenging problems into multiple steps, has been shown to improve model performance (in its simplest version, prefacing a prompt with “Let’s think step-by-step” has been shown to make the model generations more accurate in reasoning problems). People have also tested detailed instructions on zero-shot tasks, as well as asking the model about its level of confidence in its own response, each of which can improve responses in certain settings.<a id="idIndexMarker018"/></p>
<p class="body">In the previously mentioned study exploring the emergent abilities of LLMs, the authors examined the performance of LLMs of various sizes on few-shot tasks. In particular, the researchers looked for tasks where the performance of “small” LLMs was random, but then jumped sharply at the larger sizes. They found that language models’ ability to do addition, subtraction, and multiplication was emergent, with GPT-3 getting answers correct in almost no cases until the 13-billion-parameter model size; similarly, GPT-3 and other models were found to significantly improve their ability to answer questions about miscellaneous academic topics, including math, history, and law, after reaching about 70 billion or more parameters. Because these emergent abilities don’t follow the scaling law, it’s difficult to say with certainty whether larger sizes would promote even greater capabilities, at what size improvement would stop, or even how to reason about these tasks as compared to those where accuracy maps predictab<a id="idTextAnchor012"/>ly to model size.<a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="marker-43"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Sparks of artificial general intelligence?</p>
<p class="fm-sidebar-text">According to an evaluation by a team at Microsoft, “beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting” <a class="url" href="http://arxiv.org/abs/2304.15004">[2]</a>. These emergent abilities led them to provocatively title the paper, “Sparks of Artificial General Intelligence,” and write that “Given the depth and breadth of GPT-4’s capabilities, we believe it could be reasonably viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.” AGI has been the long-sought goal of many scientists in AI, and it’s understood to be intelligence that can learn as well as humans who have historically been much better at generalizing knowledge and adapting to unseen problems. The question of AGI, and whether any LLMs possess it, is outside the scope of this chapter, but we’ll discuss it and related quest<a id="idTextAnchor013"/>ions in chapter 9.<a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>
</div>
<h3 class="fm-head1" id="heading_id_9">Is emergence an illusion?</h3>
<p class="body">Although several studies have documented evidence of emergent abilities, there isn’t yet a consensus about emergence within the machine learning community. A team of computer scientists at Stanford University argued that these so-called emergent abilities appear less because of some qualitative change in model behavior at certain scales and more because of the way that researchers are evaluating the models <a class="url" href="http://arxiv.org/abs/2304.15004">[2]</a>. In particular, the sharp increases in performance that characterize emergence in some tasks seem to be at least partially attributable to the choice of metric on the task, the amount of test data used for evaluation (because testing on less data will give a noisier estimate of model performance), and the number of large-scale models in the evaluation (because there are fewer large-scale models available than small-scale models). In other words, the authors don’t dispute the actual performance of the LLMs on any of these tasks, just the idea that the LLMs, in cases where emergent abilities were claimed, represented a fundamental change from previous versions. The emergence behavior depends on the performance metric selected, and while it’s not clear whether one metric is better than another, caution is warranted before we assume that <i class="fm-italics">other</i> capabilities might readily emerge with more or different dat<a id="idTextAnchor014"/>a and bigger models. <a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="marker-44"/></p>
<h2 class="fm-head" id="heading_id_10">What’s in the training data?</h2>
<p class="body">As we’ve previously discussed, LLMs are trained on <i class="fm-italics">massive</i> amounts of noncurated data from the web. Just how much information have these LLMs been fed? Quite a lot. The general-purpose LLM, GPT-3, was trained on 45 terabytes (TB) of text data <a class="url" href="https://arxiv.org/pdf/2005.14165.pdf">[3]</a>, where 1 TB is generally estimated to contain 75 million pages <a class="url" href="https://cloudnine.com/ediscoverydaily/electronic-discovery/ediscovery-best-practices-perspective-on-the-amount-of-data-contained-in-1-gigabyte/">[4]</a>. When working wit<a id="idTextAnchor015"/>h unfathomable amounts of noncurated and undocumented training data, no one is quite sure what the data includes, resulting in LLMs encoding and amplifying stereotypical and derogatory associations, as well as sometimes containin<a id="idTextAnchor016"/>g sensitive data, such as personally identifiable information (PII). In this section, we’ll talk more about the challenges that come with training language models on immeasura<a id="idTextAnchor017"/>ble amounts of text data.<a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<h3 class="fm-head1" id="heading_id_11">Encoding bias</h3>
<p class="body">Perpetuating harmful stereotypes and discriminatory language along the lines of gender, sexual orientation, race, ethnicity, religion, age, and disability status is a well-documented form of harm in LLMs <a class="url" href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">[5]</a>. Internet-based datasets encode bias and harmful stereotypes for different reasons. The first is that these associations are largely a reflection of the characteristics found in the training data. Here, as the LLM learns the characteristics and patterns of a language in order to generate human-like text, it also inherits human-like prejudices, historical injustice, and cultural associations that can be harmful and offensive. The second is the lack of diversity in training data. The dataset can be biased because some communities may be better represented than others, and the dataset may not be broadly representative of how different groups of people view the world. The third is that developing and changing social views can result in LLMs misrepresenting social movements. <a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="marker-45"/></p>
<p class="body">In chapter 1, we briefly discussed how word embeddings mirror the inequities that exist in society. In an early study of bias in word embeddings, the authors considered NLP applications that use word embeddings to determine this potential effect <a class="url" href="https://doi.org/10.1126/science.aal4230">[6]</a>. First, they looked at sentiment analysis, which classifies text as positive, negative, or neutral. The task was calculating a sentiment score for movie reviews, which can be helpful for marketing purposes. Their results showed that movie reviews containing European American names had more positive sentiment scores on average in comparison to those with African American names, even when the reviews were otherwise similar; that is, the sentiment scores exhibited racial bias for character and actor names in the movie reviews. Next, they looked at machine translation where they concluded that translations from many gender-neutral languages to English result in gender-stereotyped sentences. In their paper, they show how Google Translate converts Turkish sentences with genderless pronouns to English: <i class="fm-italics">“O bir doktor. O bir hemşire.”</i> to “He is a doctor. She is a nurse.” <a id="idIndexMarker034"/></p>
<p class="body">Similarly, LLMs not only reinforce stereotypes but also amplify them. In a study exploring religious bias in language models, authors determined that OpenAI’s GPT-3 captures Muslim-violence bias, as well as anti-Semitic bias <a class="url" href="https://arxiv.org/pdf/2101.05783.pdf">[7]</a>. They show that prompts including the word “Muslim” yield text that maps to “terrorist” 23% of the time, while “Jewish” maps to “money” 5% of the time. They further show that replacing “Muslim” in the prompt with other religious groups significantly reduces GPT-3 from including violence-related keywords and phrases.</p>
<p class="body">Discriminatory gender, race, profession, and religion biases are also exaggerated in LLMs. In fictional stories generated by GPT-3, it was found that feminine characters were described as less powerful when compared to masculine characters, as well as more likely to be associated with family and appearance <a class="url" href="https://aclanthology.org/2021.nuse-1.5.pdf">[8]</a>. Other LLMs, such as BERT and GPT-2, also demonstrate strong stereotypical biases. For example, attribute words for <i class="fm-italics">Africa</i> were found to be <i class="fm-italics">poor</i> and <i class="fm-italics">dark</i>, whereas attribute words for a <i class="fm-italics">software developer</i> were <i class="fm-italics">geek</i> and <i class="fm-italics">nerd</i> <a class="url" href="https://aclanthology.org/2021.acl-long.416.pdf">[9]</a>.</p>
<p class="body"><a id="marker-46"/>Now, let’s look at the second case for perpetuating bias in LLMs: the lack of diversity in the training dataset. As we’ve previously discussed, quantity isn’t quality. To holistically represent the views and values of different individuals or groups, the training dataset must be diverse and broadly representative of perspectives from distinct communities. In the paper, “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?,” the authors explore several factors where they determine that the voices of people aren’t equally represented in the training datasets for language models <a class="url" href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">[5]</a>. As we know, Reddit and Wikipedia are two widely used datasets for training LLMs. The authors discuss how 67% of Reddit users are men and 64% are between 18 and 29 years old, whi<a id="idTextAnchor018"/>le similarly, only 8.8% to 15% of Wikipedians are women or girls. They also discuss that the common practice of filtering out datasets, such as the Common Crawl dataset, further weakens the voices of underrepresented communities. For example, in the training for GPT-3, the Common Crawl dataset is filtered by finding documents similar to Reddit and Wikipedia datasets, which is then additionally filtered by removing any page that contains a list of 400 words related to sex, racial slurs, or white supremacy. The authors argue that while it may be an effective strategy for filtering out certain kinds of pornography and hate speech, it inadvertently also suppresses discourse for marginalized populations, such as LGBTQ people.</p>
<p class="body">The authors also discuss the challenges with ever-changing social movements where views can either be overrepresented or not captured at all in online discourse, which ultimately is the data that LLMs are trained on. In a specific example, researchers discovered that the “intensified documentation” on Wikipedia of the Black Lives Matter (BLM) movement reinforces BLM’s claims about police violence being a systematic problem in the United States <a class="url" href="https://dl.acm.org/doi/pdf/10.1145/2998181.2998232">[10]</a>. Before the movement brought new attention to the problem, Wikipedia data on police violence, made up of isolated cases, might have told a different story. This is, of course, especially a concern when training data isn’t frequently updated, which is likely not practical given how time-intensive and computationally expensive LLMs are to train.<a id="idIndexMarker035"/><a id="marker-47"/></p>
<p class="body">In a joint study from the University of Bath and Princeton University, researchers show why addressing bias in machine learning is a challenging problem <a class="url" href="https://doi.org/10.1126/science.aal4230">[6]</a>. First, they show that bias is identical to meaning, so it’s impossible to meaningfully use language without incorporating human bias. Second, they discuss how it’s equally impossible to algorithmically define bias because our societal understanding of it is constantly evolving and varies between cultures. Finally, they show how biases can also be a result of historical inequalities that may be important to represent in some contexts.</p>
<p class="body">There have been efforts to debias word embeddings and language models, most commonly concerning gender. To reduce bias in word embeddings, you could change the representation of a gender-neutral word by removing their gender associations. For example, if we have the word <i class="fm-italics">nurse</i>, which is more likely associated with <i class="fm-italics">female</i>, it would be moved equally between <i class="fm-italics">male</i> and <i class="fm-italics">female</i> <a class="url" href="https://arxiv.org/pdf/1607.06520.pdf">[11]</a>. In 2022, a group of researchers surveyed five debiasing techniques for language models concerning gender, religious, and racial biases, where they determined that not only do current debiasing techniques not work as well for nongender biases, but they also result in a decrease in the ability to model language <a class="url" href="https://arxiv.org/pdf/2110.08527.pdf">[12]</a>. Although a noble effort, algorithmically eliminating bias from language models is extraordinarily difficult because it also removes meaning and information, giving the model an incomplete picture of our world and turning debiasing into “fairness through blindness” <a class="url" href="https://doi.org/10.1126/science.aal4230">[6]</a>.</p>
<p class="body">As argued by Bender and Gebru et. al, a concrete path forward is to curate and document training datasets for language models <a class="url" href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">[5]</a>. As of now, most LLMs are trained on a proprietary mixture of datasets, with sources not provided to end users. Documentation is critical for understanding the data characteristics, mitigating some of these risks, and allowing for potential accountability. We <i class="fm-italics">can</i> build representative and unbiased datasets by budgeting for dataset documentation and only collecting as much data as can be documented. Hugging Face, a company focused on building open source machine learning tools, has developed dataset cards that are a good starting point for dataset documentation, including details about the dataset contents, any potential biases within the dataset, and context for how the dataset should be used <a class="url" href="https://huggingface.co/docs/hub/datasets-cards">[13]</a>. Hugging Face also released a search tool for ROOTS, a 1.6 TB multilingual text corpus, which was used to train BLOOM, an LLM <a class="url" href="https://arxiv.org/pdf/2302.14035.pdf">[14]</a>. To encourage researchers to characterize large datasets, the tool allows you to search through the dataset for qualitative analysis of training data. Similarly, founded through Berkman Klein Center’s Assembly fellowship at Harvard, the Data Nutrition Project takes inspiration from nutritional labels on food to highlight the key ingredients in a dataset, such as metadata and demographic representation (see <a class="url" href="https://datanutrition.org/">https://datanutrition.org/</a>). <a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>
<p class="body">Finally, unlike AI, humans have context-specific memories and social examples to draw from that can be used to overcome racial and gender biases. Humans can fight their implicit biases and these biases, need not rema<a id="idTextAnchor019"/>in entrenched in our society forever.<a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="marker-48"/></p>
<h3 class="fm-head1" id="heading_id_12">Sensitive information</h3>
<p class="body">Because LLMs are trained on unfathomable amounts of data from a wide range of sources on the internet, they can sometimes contain personally identifiable information (PII), such as names, addresses, Social Security numbers, biometric data, sexual orientation, and so on, even if trained on public data. One potential risk is that the model could unintentionally “memorize” details from the data on which it’s trained; that is, sensitive information from the model could be reflected in its output. There are, naturally, additional concerns if a model trained on a proprietary dataset is made publicly available. <a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/></p>
<p class="body">A massive vulnerability of LLMs is that an adversary can perform a <i class="fm-italics">training data extraction attack</i> in which malicious actors can query the model to recover sensitive and identifiable information. As with most security and privacy studies, it’s important to consider the risks and ethics of performing attacks for research purposes, so publicly available and published work in this space is often limited. <a id="idIndexMarker045"/></p>
<p class="body"><a id="marker-49"/>Google, in collaboration with OpenAI, Apple, Stanford, Northeastern University, and Berkeley, demonstrated their “attack” on GPT-2 to show that it’s possible to extract sensitive pieces of training data that the model has inadvertently “memorized.” Here, the attackers can query a language model to extract <i class="fm-italics">verbatim information</i> from the training data. The researchers note that the training data extraction attacks have the most potential for harm when a model that is trained on a proprietary dataset is made publicly available, but they acknowledge that performing an attack for research purposes on such a dataset could also have harmful consequences. With this in mind, they chose GPT-2 because the training dataset collection process is documented and only uses public internet sources. They were able to extract hundreds of verbatim pieces of information that include PII (names, phone numbers, email addresses)<a id="idTextAnchor020"/>, instant messaging conversations, code, and universally unique identifiers (UUIDs). Most of these examples were memorized even though they appeared very infrequently, as little as in a single document, in the training dataset, and larger models were found to be more vulnerable to these attacks than smaller models <a class="url" href="https://arxiv.org/pdf/2012.07805.pdf">[15]</a>. A different study, “The Secret Sharer,” shows that unintended memorization is persistent and hard to avoid for LLMs <a class="url" href="https://arxiv.org/pdf/1802.08232.pdf">[16]</a>. They demonstrated an attack on the Enron Email Dataset (see <a class="url" href="http://mng.bz/K9AZ">http://mng.bz/K9AZ</a>), which contains half a million emails sent between Enron Corporation employees. The dataset was made public and posted online by the Federal Energy Regulatory Commission during its investigation. The researchers used the Enron Email Dataset to train a language model and show that they are effortlessly able to extract credit card and Social Security numbers.<a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>
<p class="body">The most straightforward way to mitigate this problem is to make sure that models don’t train on any sensitive or PII data. This is extremely difficult to do in practice, however, and goes back to our earlier point of curating and documenting datasets for language models.<a id="idTextAnchor021"/> Other solutions include <i class="fm-italics">privacy-preserving</i> or <i class="fm-italics">privacy-enhancing technologies</i> (PETs), which can help mitigate data privacy and security risks <a class="url" href="https://royalsociety.org/-/media/policy/projects/privacy-enhancing-technologies/Protecting-privacy-in-practice.pdf">[17]</a>. Some examples of PETs include methods for pseudonymization, obfuscation, sanitization, and data masking. An example of using these in practice is creating blocklists for possible sensitive sequences to filter out potentially private information from the training dataset. Ho<a id="idTextAnchor022"/>wever, as demonstrated in “The Secret Sharer,” blocklisting is never a complete approach in security and won’t significantly reduce the effect of unintended memorization for any sequences that did appear. Differential privacy, an anonymization technique introduced in the early 2000s, is a popular PET that attempts to train via dataset without revealing details of any individual training samples. Here, the idea is to add statistical <i class="fm-italics">noise</i> to obscure individual identities in a given dataset. But this technique also has its limitations because it won’t prevent memorization for content that isn’t repeated often in the dataset. In “Beyond Data: Reclaiming Human Rights at the Dawn of the Metaverse,” the author points out that PETs are not only highly technical, complex to use, expensive, and resource-intensive but also challenging for lawmakers and policymakers to audit or govern <a class="url" href="https://books.google.com/books/about/Beyond_Data.xhtml?hl=&amp;id=zJZuEAAAQBAJ">[18]</a>. <a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="marker-50"/></p>
<p class="fm-callout"><span class="fm-callout-head">Privacy-preserving</span> or <span class="fm-callout-head">privacy-enhancing technologies (PET)</span> are umbrella terms used for approaches that can help mitigate privacy and security risks.<a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/></p>
<p class="body">Given the limitations of current PET approaches, we hope that the efforts to raise awareness of this challenge will encourage researchers to develop new techniques to address this problem, as well as build on previous work to test unintended memorization fr<a id="idTextAnchor023"/>om LLMs so we can respond to the problem appropriately.<a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>
<h2 class="fm-head" id="heading_id_13">Summary<a id="marker-51"/></h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">LLMs might be trained on a combination of open source or public datasets, datasets purchased from third-party vendors, datasets that companies collect themselves by scraping the web, or datasets that the companies create themselves by writing examples for the models to learn from.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Autoregressive</i> models refer to the fact that future predictions are dependent on the model’s past data. All the models in the GPT family, as well as Google’s PaLM, are autoregressive models trained to predict the next token given some input.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Zero-shot</i> and <i class="fm-italics">few-shot</i> refer to the number of examples that the model is given before being asked to perform a task. They are primary examples of the emergent abilities of LLMs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs often encode and amplify stereotypical and derogatory associations; they also contain sensitive data, including personally identifiable information (PII).</p>
</li>
<li class="fm-list-bullet">
<p class="list">A concrete path forward is to curate and document training datasets for language models, which is critical for understanding the data characteristics in order to mitigate risks and allow for potential accountability.</p>
</li>
<li class="fm-list-bullet">
<p class="list">An adversary can perform a <i class="fm-italics">training data extraction attack</i> with an LLM in which malicious actors can query the model to recover sensitive and identifiable information.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Privacy-preserving</i> or <i class="fm-italics">privacy-enhancing technologies</i> (PETs) can help mitigate data privacy and security risks. PETs have several limitations, and we hope to see concentrated efforts of researchers in this area so there are techniques that LLM developers can easily adopt.</p>
</li>
</ul>
</div></body></html>