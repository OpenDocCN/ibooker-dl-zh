- en: Chapter 6\. Assembling the Prompt
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. 组装提示
- en: In the previous chapters, you gathered a wealth of content that will serve as
    the building blocks for your prompt. Now, it’s time to put these pieces together
    and craft a prompt that effectively communicates your needs. This chapter will
    guide you through the process of shaping your prompt by first exploring the different
    structures and options available to you. How you choose to organize these individual
    snippets will play a crucial role in the effectiveness of your final prompt.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你收集了大量将作为你提示构建块的内容。现在，是时候将这些碎片组合起来，制作一个能够有效传达你需求的提示了。本章将指导你通过首先探索你可用到的不同结构和选项来塑造你的提示。你如何选择组织这些单个片段将在你最终提示的有效性中扮演关键角色。
- en: The next step involves triaging your content—deciding what to keep and what
    to discard so that it will fit within any size constraints you might have. This
    process is key to refining your prompt and ensuring it remains focused and relevant.
    With your content finalized, you’ll then move on to assembling your prompt, which
    will be your tool for eliciting relevant, coherent, and contextually accurate
    responses from the model. Let’s dive in.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及对内容进行分类——决定保留什么和丢弃什么，以便它能够适应你可能有的任何大小限制。这个过程对于完善提示并确保其保持专注和相关性至关重要。内容确定后，你将进入组装提示的阶段，这将是你从模型中获取相关、连贯和上下文准确响应的工具。让我们深入探讨。
- en: Anatomy of the Ideal Prompt
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理想提示的结构
- en: Before we go into the details of how to get there, let’s visualize where we
    want to go. Take a look at [Figure 6-1](#ch06a_figure_1_1728442733835135), which
    gives a bird’s eye view of how your prompt *should* look. We’ll go through its
    elements one at a time. Concise and crisp prompts are generally more effective—plus,
    they use less computational power and are processed more quickly. Additionally,
    you have a hard cut-off with the context window size.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨如何达到那里的细节之前，让我们先可视化我们想要达到的地方。看看[图6-1](#ch06a_figure_1_1728442733835135)，它提供了一个俯瞰你的提示应该如何看起来。我们将逐个分析其元素。简洁明了的提示通常更有效——此外，它们使用的计算能力更少，处理速度更快。此外，你还有一个与上下文窗口大小相关的硬截止点。
- en: As discussed in [Chapter 5](ch05.html#ch05_prompt_content_1728435524680844),
    a prompt consists of elements drawn from dynamic context and static instructions
    that clarify your question. There are no hard rules for the size or number of
    these elements. In fact, as applications evolve, a large prompt element might
    be broken into several smaller ones for more precise constructions. We’ve worked
    on projects with prompts ranging from just three lengthy elements to hundreds
    of one-line elements.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第5章[讨论的](ch05.html#ch05_prompt_content_1728435524680844)中，提示由来自动态上下文和阐明你问题的静态指令的元素组成。这些元素的大小或数量没有硬性规定。实际上，随着应用的演变，一个大的提示元素可能会被拆分成几个更小的元素，以实现更精确的构建。我们曾参与过从仅有三个长元素到数百个单行元素的项目。
- en: '![](assets/pefl_0601.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/pefl_0601.png)'
- en: Figure 6-1\. Anatomy of a well-constructed prompt
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 构建良好提示的结构
- en: There’s no theoretical rule that each prompt element must end with a newline
    character. However, in practice, enforcing the rule that all elements end with
    newlines can simplify your string manipulation code. It can also assist with token
    length computation, depending on the tokenizer used (more on that next). If your
    prompt elements don’t easily fit this format, don’t feel compelled to force it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何理论规则规定每个提示元素必须以换行符结束。然而，在实践中，强制执行所有元素都以换行符结束的规则可以简化你的字符串操作代码。这也可以帮助进行标记长度计算，具体取决于所使用的标记器（更多内容将在下一节中介绍）。如果你的提示元素不容易适应这种格式，不必强迫它们这样做。
- en: Most prompts include a handful of certain elements. First is the *introduction,*
    which helps you clarify the type of document you’re writing and sets up the model
    to approach the rest of the content correctly. The introduction sets the context
    for everything that follows. For example, if the model states, “This is about
    recommending a book,” it will focus on relevant aspects for book recommendations
    and interpret the context accordingly. The introduction also lets the model begin
    thinking about the problem from the start. Since the model has a fixed “thought
    budget” per token and can’t pause for deeper reflection, guiding its focus early
    on can improve its output.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数提示语都包含一些特定的元素。首先是*介绍*，它有助于你明确你正在撰写的文档类型，并使模型正确地处理其余内容。介绍为随后的一切设定了背景。例如，如果模型声明“这是关于推荐一本书”，它将专注于与书籍推荐相关的方面，并相应地解释上下文。介绍还让模型从开始就思考问题。由于模型每个标记都有一个固定的“思考预算”，并且不能暂停进行更深入的反思，因此早期引导其关注点可以提高其输出质量。
- en: 'Most prompts have only one introduction to set up the main question. But the
    principle also applies to subsections of the prompt: if there are some pieces
    of context where the model needs to focus on a certain aspect, it helps if you
    set up that aspect in the beginning.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数提示语只有一个介绍来设定主要问题。但这个原则也适用于提示语的子部分：如果有一些上下文，模型需要关注某个特定方面，那么在开头设定这个方面是有帮助的。
- en: 'After the introduction, you’ll see a long parade of different prompt elements.
    The model will try to make good use of all of them, but not equally. All LLMs
    are subject to two effects:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍之后，你会看到一系列不同的提示语元素。模型将尝试充分利用它们，但并不平等。所有大型语言模型都受到两种效应的影响：
- en: '[*In-context learning*](https://browse.arxiv.org/pdf/2302.11042)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*情境学习*](https://browse.arxiv.org/pdf/2302.11042)'
- en: The closer a piece of information is to the end of the prompt, the more impact
    it has on the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 信息离提示语结尾越近，对模型的影响就越大。
- en: '*The* [*lost middle phenomenon*](https://browse.arxiv.org/pdf/2307.03172.pdf)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*丢失中间现象*[*](https://browse.arxiv.org/pdf/2307.03172.pdf)'
- en: While the model can easily recall the beginning and end of the prompt, it struggles
    with the information stuffed in the middle.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型可以轻松回忆起提示语的开始和结束，但它对中间填充的信息感到困难。
- en: These two dynamics create the *Valley of Meh*, as we like to call it. The valley
    lies around the early middle of the prompt, and the context that is placed there
    is not used as effectively as the context in the beginning or the second half
    of the document. How deep the Valley of Meh is and its exact location depends
    on the model, but all models have it—as do humans!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种动态共同构成了我们所说的*平淡谷*。这个谷位于提示语的早期中间部分，那里放置的上下文并没有像开头或文档后半部分那样有效地被使用。平淡谷的深度及其确切位置取决于模型，但所有模型都有这个谷——人类也不例外！
- en: The Valley of Meh is most problematic with large prompts, and there’s no perfect
    solution. You can reduce its impact by placing key, high-quality prompt elements
    outside the Valley of Meh and by filtering context to keep the prompt as concise
    as possible.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 平淡谷在大型提示语中最为问题化，而且没有完美的解决方案。你可以通过将关键的高质量提示语元素放置在平淡谷之外，并通过过滤上下文来尽可能使提示语简洁，从而减少其影响。
- en: When you’ve included all the context, it’s time to remind the model of the main
    question. We call this the *refocus,* which is necessary for longer prompts, where
    you’ve spent a long time adding context and you need to focus the model’s attention
    back to the question. Most prompt engineers use the *sandwich technique*, in which
    they start and end the prompt by clearly stating what they want the model to do
    (see [Table 6-1](#ch06a_table_1_1728442733843102)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当你包含了所有上下文后，就是时候提醒模型主要问题了。我们称之为*重新聚焦*，这对于较长的提示语是必要的，因为你已经花费了很长时间添加上下文，你需要将模型的注意力重新集中在问题上。大多数提示工程师使用*三明治技术*，即在提示语的开头和结尾明确说明他们希望模型做什么（见[表
    6-1](#ch06a_table_1_1728442733843102)）。
- en: Table 6-1\. Sandwiching the context between two versions of the same question
    for a model with the ChatML API
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. 使用 ChatML API 在同一问题的两个版本之间三明治上下文
- en: '| Prompt part | Sandwich | Prompt |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 提示语部分 | 三明治 | 提示语 |'
- en: '| --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Introduction |   |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 介绍 |   |'
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sandwich part 1 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 三明治部分 1 |'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|   |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Context |   |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 |   |'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `[…]` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `[…]` |'
- en: '| `[…]` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `[…]` |'
- en: '| `[…]` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `[…]` |'
- en: '| Refocus + Transition |   |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 重新聚焦 + 过渡 |   |'
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sandwich part 2 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 三明治部分 2 |'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The refocus can be as short as half a line, but it’s common to include key clarifications
    here. The introduction sets the stage (“I’m thinking about book suggestions for
    X.”), while the refocus gives clear details (“What’s the best book to recommend
    next, focusing on narrative prose currently available?”). If the clarification
    gets lengthy, you might need a brief refocus at the end, especially when discussing
    the output format.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重新聚焦可以短至半行，但通常在这里包含关键澄清。介绍设定了舞台（“我在考虑为X推荐书籍。”），而重新聚焦提供了清晰的细节（“接下来推荐哪本书最好，专注于目前可用的叙事散文？”）。如果澄清变得很长，你可能需要在结束时进行简短的重新聚焦，尤其是在讨论输出格式时。
- en: The very last part of your prompt should firmly transition from explaining the
    problem to solving the problem—that’s the bit you want the LLM to help with, after
    all. It’s not helpful if it just keeps adding more (likely made-up) context to
    your main question.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你的提示的最后部分应该明确地从解释问题过渡到解决问题——毕竟，这是你希望LLM帮助你的部分。如果它只是不断地添加更多（可能是虚构的）上下文到你的主要问题中，那就没有帮助。
- en: When using a chatlike interface, this part is usually as simple as including
    a question mark at the end. RLHF has drilled those models to respond by solving
    the last question stated—or sometimes even just implied—in their input. Some commercial
    platforms, like OpenAI’s ChatGPT, automatically signal when the assistant should
    begin its response after receiving a prompt through their API. However, traditional
    completion models require more explicit guidance to achieve the same effect.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用类似聊天的界面时，这部分通常只需要在结尾加上一个问号。RLHF已经训练这些模型通过解决输入中最后提出的（有时甚至只是暗示的）最后一个问题来回答。一些商业平台，如OpenAI的ChatGPT，在通过API接收到提示后，会自动向助手发出开始响应的信号。然而，传统的完成模型需要更多的明确指导才能达到同样的效果。
- en: The most common way to transition—especially when using a completion API—is
    to change your perspective from problem poser to problem solver and begin writing
    the answer for the model. This way, the model has no choice but to present its
    solution. [Figure 6-2](#ch06a_figure_2_1728442733835180) demonstrates the difference
    a good transition can make in getting an answer out of the model. Note that the
    opening quotation mark ending the transition in column three is still part of
    the prompt.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的过渡方式——尤其是在使用完成API时——是从问题提出者转变为问题解决者，并开始为模型撰写答案。这样，模型别无选择，只能展示其解决方案。[图6-2](#ch06a_figure_2_1728442733835180)展示了良好的过渡如何帮助从模型中获得答案。请注意，在第三列结束过渡的引号仍然是提示的一部分。
- en: '![](assets/pefl_0602.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0602.png)'
- en: 'Figure 6-2\. Three variations of transition: missing, on the left; naive, in
    the middle; and refined, on the right (all completions [shaded backgrounds] obtained
    using OpenAI’s text-davinci-002, which is a completion model, rather than a chat
    model)'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 过渡的三种变体：缺失的，在左边；天真的，在中间；和精炼的，在右边（所有完成[阴影背景]都是使用OpenAI的text-davinci-002完成的，这是一个完成模型，而不是聊天模型）
- en: As shown in [Figure 6-2](#ch06a_figure_2_1728442733835180), you can often merge
    the refocus and the transition. In those cases, you write the beginning of the
    answer that is just restating or summarizing the problem statement. The actual
    answer is then supplied by the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6-2](#ch06a_figure_2_1728442733835180)所示，你通常可以将重新聚焦和过渡合并。在这些情况下，你写下答案的开始部分，这部分只是重申或总结问题陈述。实际的答案随后由模型提供。
- en: What Kind of Document?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么类型的文档？
- en: A prompt and completion together form a document, and as the Little Red Riding
    Hood principle from [Chapter 4](ch04.html#ch04_designing_llm_applications_1728407230643376)
    suggests, it’s best to use documents similar to those in the training data so
    that the format of the completion will be easy to anticipate. But what type of
    document should you aim for? There are several useful types, each with room for
    personalization. Let’s explore the most common ones and when to use each.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提示和完成共同构成一个文档，正如第四章中提到的《小红帽原则》[ch04.html#ch04_designing_llm_applications_1728407230643376]所建议的，最好使用与训练数据中相似的文档，这样完成的格式就更容易预测。但你应该追求哪种类型的文档呢？有几种有用的类型，每种类型都有个性化的空间。让我们探索最常见的几种，以及何时使用每种。
- en: The Advice Conversation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建议对话
- en: In the most common archetype, your document represents a conversation between
    two people. One asks for some kind of help, and the other provides it. The one
    who asks for help represents either your application or its user, while the model
    will take on the role of help provider.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在最常见的范例中，你的文档代表两个人之间的对话。一个人寻求某种帮助，另一个人提供帮助。寻求帮助的人代表你的应用程序或其用户，而模型将扮演帮助提供者的角色。
- en: 'This approach is ideal for chat models, but even completion models can benefit
    from it. In fact, OpenAI developed ChatML to focus on advice conversations because
    they believed they were the most universally useful and easiest to implement.
    Advice conversations have many advantages, including the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常适合聊天模型，但即使是完成模型也能从中受益。实际上，OpenAI开发了ChatML来专注于建议对话，因为他们认为它们是最普遍有用且最容易实现的。建议对话有许多优点，包括以下内容：
- en: Natural interaction
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 自然交互
- en: It’s easy for people to think in terms of conversations. You can pose a question
    directly to the model and take its continuation as the answer to simplify interactions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 人们很容易用对话的方式来思考。你可以直接向模型提出问题，并将它的延续作为答案来简化交互。
- en: Multiround interactions
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 多轮交互
- en: For complex interactions, you can continue the prompt with new questions and
    answers, making it easier to manage and break down the conversation. This approach
    allows you to add your logic between questions and helps the model handle each
    query directly.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的交互，你可以通过新的问题和答案来继续提示，这使得管理对话和分解对话变得更加容易。这种方法允许你在问题之间添加你的逻辑，并帮助模型直接处理每个查询。
- en: Real-world integration
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界集成
- en: Conversations work well for multiround processes and integrating with real-world
    tools and techniques, whether you’re using a chat model or a completion model
    with conversational documents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对话对于多轮过程和与真实世界工具和技术集成都很有用，无论你是在使用聊天模型还是具有对话文档的完成模型。
- en: If you use this structure with a chat model, you’ll get the additional advantages
    of RLHF related to compliance with your instructions. But if you use it with a
    completion model instead, you can avoid any RLHF trait that’s unhelpful for your
    scenario (e.g., stylistic habits, content policing).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用这种结构与聊天模型一起，你将获得与RLHF相关的额外优势，这些优势与你的指令的合规性有关。但如果你用完成模型代替，你可以避免任何对你场景无用的RLHF特性（例如，风格习惯、内容监控）。
- en: On the other hand, if you happen to be using a completion model, you can use
    a trick called *inception*, in which you dictate the beginning of the answer.
    Do you remember the 2010 movie *Inception*? Same idea here—start the answer for
    the model and the model will think that it’s the one that came up with it, and
    it will generate the rest of the completion accordingly. This approach can improve
    model compliance and make answers easier to parse, as well as help you avoid uncertainty
    about whether the response will start with a general statement or get straight
    to the point.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你恰好在使用完成模型，你可以使用一种称为“起始”的技巧，其中你规定答案的开头。你还记得2010年的电影《盗梦空间》（Inception）吗？这里也是同样的想法——为模型开始答案，模型会认为它是想出来的，并据此生成剩余的完成内容。这种方法可以提高模型的合规性，使答案更容易解析，同时帮助你避免对响应是否以一般性陈述开始或直接切入主题的不确定性。
- en: When writing a prompt for a completion model, you’ll have to decide the format
    for your transcript. The good news is that LLMs are used to many different formats,
    as shown in [Table 6-2](#ch06a_table_2_1728442733843133), in which the same conversation
    is cast into different formats. Note that, typically, an application would provide
    more context for this question.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当为完成模型编写提示时，你必须决定你的转录本的格式。好消息是，LLM已经习惯了多种不同的格式，如[表6-2](#ch06a_table_2_1728442733843133)所示，其中相同的对话被转换成不同的格式。请注意，通常，应用程序会为此问题提供更多上下文。
- en: Table 6-2\. Asking an LLM for ideas to plan your day
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-2\. 向大型语言模型请求规划你一天的想法
- en: '| Format | Freeform text | Script format | Markerless | Structured |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 自由文本 | 脚本格式 | 无标记 | 结构化 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Introduction | I asked my husband, “What shall we do tomorrow?” | Me: What
    shall we do tomorrow? | What shall we do tomorrow? | <me>What shall we do tomorrow?</me>
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 简介 | 我问我的丈夫，“我们明天做什么？” | 我：我们明天做什么？ | 我们明天做什么？ | <我>我们明天做什么？</我> |'
- en: '| Context | and he replied, “Well, what’s the weather like?”I reported that
    “We expect a balmy 75 degrees with sunshine in the whole Boston area.” | Husband:
    Well, what’s the weather like?Me: We expect a balmy 75 degrees with sunshine in
    the whole Boston area. | Well, what’s the weather like?We expect a balmy 75 degrees
    with sunshine in the whole Boston area. | <husband>Well, what’s the weather like?</husband><me>We
    expect a balmy 75 degrees with sunshine in the whole Boston area.</me> |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 | 他回答说，“嗯，天气怎么样？”我报告说，“我们预计整个波士顿地区的天气将温暖宜人，阳光明媚。” | 丈夫：嗯，天气怎么样？我：我们预计整个波士顿地区的天气将温暖宜人，阳光明媚。
    | 嗯，天气怎么样？我们预计整个波士顿地区的天气将温暖宜人，阳光明媚。 | <husband>嗯，天气怎么样？</husband><me>我们预计整个波士顿地区的天气将温暖宜人，阳光明媚。</me>
    |'
- en: '| Refocus | He reflected a bit about what would be a good Sunday activity for
    us, | Husband (after reflection about good Sunday activities) | (reflects about
    good Sunday activities) | <direction> Husband reflects about good Sunday activities
    </direction> |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 重新聚焦 | 他稍微反思了一下，关于我们周日的好活动是什么， | 丈夫（在反思周日的好活动之后） | （反思周日的好活动） | <direction>丈夫反思周日的好活动</direction>
    |'
- en: '| Transition | and he suggested, | : | I suggest we should | <husband> |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 过渡 | 他建议说， | : | 我建议我们应该 | <husband> |'
- en: '| Completion (text-davinci-003) | “Let’s go on an outing to the beach, then.
    We could grab some coffee and sandwiches and have a picnic on the sand. We could
    sit on a blanket and watch the waves. Sounds nice.” | I think it would be fun
    to take a bike ride, maybe in one of the nearby parks. Maybe we can bring a picnic
    lunch and make a day of it outside in the sunshine. | take a picnic and a book
    down to the Charles River and spend the day lounging, reading, and enjoying the
    sunshine. | If it’s going to be so nice, how about a hike in the Blue Hills? We
    can check out the reservoir, have a picnic, and watch the sunset. </husband> |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 完成内容（text-davinci-003） | “那我们去看海滩吧。我们可以买些咖啡和三明治，在沙滩上野餐。我们可以坐在毯子上，看着波浪。听起来不错。”
    | 我觉得骑自行车会很有趣，也许可以去附近的公园。也许我们可以带个野餐午餐，在阳光下度过一天。 | 带上野餐和一本书去查尔斯河，整天悠闲地阅读，享受阳光。
    | 如果天气这么好，去蓝山徒步旅行怎么样？我们可以看看水库，野餐，并观赏日落。 </husband> |'
- en: 'While all formats are effective, each of them has unique strengths, and we’ve
    arranged them so that each format addresses the weaknesses of the one before it:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所有格式都很有效，但每种格式都有其独特的优势，我们已将它们排列得如此，以便每种格式都弥补了前一种格式的不足：
- en: Freeform text
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 自由文本
- en: This allows you to insert various types of information between quotes, but it
    is challenging to assemble on the fly. It can be difficult to create a reliable
    system for dynamically generating prompts with many elements.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许你在引号之间插入各种类型的信息，但现场组装起来可能具有挑战性。创建一个能够动态生成包含许多元素的提示的可靠系统可能很困难。
- en: Transcript format
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 录音格式
- en: This is easy to assemble but less effective for long or formatted elements (like
    source code with important indentation).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易组装，但对于长文本或格式化元素（如带有重要缩进的源代码）来说效果较差。
- en: Markerless format
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 无标记格式
- en: This works well with formatted text and longer pieces (such as pasted emails),
    but it can be difficult for the model to track speakers and for the application
    to determine when the model’s response ends and the next input begins.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这与格式化文本和较长的内容（如粘贴的电子邮件）配合得很好，但对于模型跟踪说话者以及应用程序确定模型响应何时结束以及下一个输入何时开始可能很困难。
- en: Structured format
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化格式
- en: This clearly indicates who is speaking and when they finish. Various structures
    are available, and they are detailed in [“The Structured Document”](#ch06a_the_structured_document_1728442733858464).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明了谁在说话以及他们何时结束。有多种结构可供选择，它们在[“结构化文档”](#ch06a_the_structured_document_1728442733858464)中有详细说明。
- en: In [Chapter 3](ch03.html#ch03a_moving_toward_chat_1728432131625250), we introduced
    the notion that writing a conversational prompt is like playwriting. Except for
    stage directions, all parts of the text belong to one of the “roles” in the play.
    In conversations between an advice seeker and an assistant, you typically let
    the user write the speaking parts for the advice seeker and let the LLM write
    the speaking parts for the assistant. This needn’t be the case—there’s nothing
    stopping you, as a prompt engineer, from writing for the role of the assistant.
    This is another form of the inception approach—you speak for the assistant, and
    in all subsequent turns of the conversation, the assistant will act as if it actually
    said what you said.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#ch03a_moving_toward_chat_1728432131625250)中，我们介绍了写作对话提示的概念，就像戏剧创作。除了舞台指示外，文本的各个部分都属于戏剧中的一个“角色”。在寻求建议者和助手之间的对话中，你通常让用户为寻求建议者编写说话部分，让LLM为助手编写说话部分。这并不一定非得如此——作为提示工程师，你为助手角色写作没有任何阻碍。这是起始方法的一种形式——你代表助手说话，在所有随后的对话回合中，助手将表现得好像它真的说了你所说的话。
- en: Tip
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Writing a prompt from the assistant’s perspective helps frame context as if
    it’s responding to a question they asked. This approach ensures that the completion
    starts with the answer rather than another clarifying question.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从助手的视角编写提示有助于将上下文框架化为似乎是在回答他们提出的问题。这种方法确保了完成部分从答案开始，而不是另一个澄清问题。
- en: The Analytic Report
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析报告
- en: 'Each year, millions of students are trained in report writing. They learn the
    art of crafting introductions, expositions, analyses, and conclusions, and once
    they graduate and enter the workforce, they churn out reports that analyze markets,
    weigh costs and benefits, and propose actionable conclusions. All of this is hard
    work, and fortunately, it serves a purpose: it provides excellent training material
    for LLMs. And these models are trained on vast datasets filled with reports of
    every kind and size.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，数百万学生接受报告写作的训练。他们学习撰写引言、阐述、分析和结论的艺术，一旦他们毕业并进入职场，他们就会撰写分析市场、权衡成本和收益并提出可操作结论的报告。所有这些都是艰苦的工作，幸运的是，它有其目的：它为LLM提供了优秀的训练材料。而且，这些模型是在包含各种类型和规模的报告的大量数据集上训练的。
- en: Leveraging this abundance of reports is straightforward, especially if your
    task falls within domains where analytical reports are common, such as business,
    literature, science, or law (though it’s probably best to leave legal defense
    to human professionals). Reports are easy to structure because they follow a familiar
    format that usually starts with an introduction, leads to a conclusion, and often
    includes a recap. The information you’ve already gathered can be easily inserted
    into the discussion or background sections.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些丰富的报告资源很简单，特别是如果你的任务属于分析报告常见的领域，如商业、文学、科学或法律（尽管最好将法律辩护留给人类专业人士）。报告很容易结构化，因为它们遵循一个熟悉的格式，通常从引言开始，导致结论，并经常包括总结。你已经收集的信息可以轻松地插入讨论或背景部分。
- en: However, crafting static prompt elements like instructions requires some thought,
    especially if you’re making sure to keep things clear and to the point. A helpful
    strategy is to include a *S**cope* section that clearly defines the boundaries
    of the report. Instead of going back and forth in a dialogue to clarify exclusions
    (e.g., “Please suggest only novels, not self-help books.”), you can state up front,
    “This report focuses solely on novels, excluding self-help books.” LLMs tend to
    respect such clear boundaries more consistently in reports than in dialogues.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，制作静态提示元素，如指令，需要一些思考，尤其是当你确保内容清晰、简洁时。一个有用的策略是包括一个*范围*部分，明确界定报告的边界。而不是在对话中来回确认排除的内容（例如，“请只建议小说，不要自助书籍。”），你可以一开始就声明，“本报告仅关注小说，不包括自助书籍。”LLM在报告中比在对话中更一致地尊重这样的明确边界。
- en: Reports also favor objective analysis, which lightens the cognitive load for
    the LLM by avoiding the need for it to simulate social interaction. That said,
    because analysis typically precedes the conclusion, you must ensure a clear transition
    when you want the model to shift into a decision-making mode. Otherwise, you might
    end up with a meandering response that requires extra parsing. On the flip side,
    this format lends itself well to chain-of-thought prompting, which is covered
    in more detail in [Chapter 8](ch08.html#ch08_01_conversational_agency_1728429579285372).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 报告还倾向于客观分析，这通过避免需要模拟社交互动来减轻 LLM 的认知负担。话虽如此，因为分析通常在结论之前，所以当你想让模型进入决策模式时，你必须确保有一个清晰的过渡。否则，你可能会得到一个需要额外解析的冗长回答。另一方面，这种格式非常适合思维链提示，这在第
    8 章中有更详细的介绍 [Chapter 8](ch08.html#ch08_01_conversational_agency_1728429579285372)。
- en: 'Dialogues can take many forms, depending on the context (see [Table 6-2](#ch06a_table_2_1728442733843133)).
    However, for reports, we recommend that you consistently stick to one format:
    writing your prompts in Markdown. Here’s why:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对话可以采取多种形式，具体取决于上下文（见 [Table 6-2](#ch06a_table_2_1728442733843133)）。然而，对于报告，我们建议您始终坚持一种格式：用
    Markdown 编写您的提示。以下是原因：
- en: It’s pretty universal, and the internet is full of Markdown files, so LLMs know
    it well.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它相当通用，互联网上充满了 Markdown 文件，所以 LLM 对它很熟悉。
- en: Markdown is a simple, lightweight language with only a few key features. This
    makes it easy to write and straightforward for models to interpret the output.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markdown 是一种简单、轻量级的语言，只有几个关键特性。这使得它易于编写，并且对模型来说，解释输出是直接的。
- en: Markdown’s headings help define hierarchy, which allows you to organize prompt
    elements into clear sections that can be easily rearranged or omitted while maintaining
    structure.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markdown 的标题有助于定义层次结构，这使得您可以将提示元素组织成清晰的章节，可以轻松地重新排列或省略，同时保持结构。
- en: Another useful feature is that indentation doesn’t usually matter, but for technical
    content (e.g., source code), you can use blocks opened and closed with triple
    backticks ([PRE7]
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个有用的特性是缩进通常不重要，但对于技术内容（例如，源代码），您可以使用三重反引号打开和关闭的块（[PRE7]
- en: <examples>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <examples>
- en: <example>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <example>
- en: <user_query>Can you help me create a Python script to calculate the
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <user_query>你能帮我创建一个计算阶乘的 Python 脚本吗？
- en: factorial of a number?</user_query>
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: factorial of a number?</user_query>
- en: <assistant_response>
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <assistant_response>
- en: Sure! Here's a Python script that calculates the factorial of a
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然！这里有一个计算阶乘的 Python 脚本
- en: 'number:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数字：
- en: <antThinking>Creating a Python script to calculate factorials meets
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <antThinking>创建一个计算阶乘的 Python 脚本符合
- en: the criteria for a good Artifact. Therefore, I'm creating a new
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 良好文件的标准。因此，我正在创建一个新的
- en: Artifact.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文件。
- en: </antThinking>
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: </antThinking>
- en: <antArtifact identifier="factorial-script"
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <antArtifact identifier="factorial-script"
- en: type="application/vnd.ant.code" language="python"
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: type="application/vnd.ant.code" language="python"
- en: title="Simple Python factorial script">
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标题="简单的 Python 阶乘脚本">
- en: 'def factorial(n):'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def factorial(n):'
- en: 'if n == 0:'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'if n == 0:'
- en: return 1
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: return 1
- en: 'else:'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'else:'
- en: return n * factorial(n - 1)
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: return n * factorial(n - 1)
- en: '...'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: </assistant_response>
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: </assistant_response>
- en: </example>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: </example>
- en: '[...several examples omitted...]'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[...省略几个示例...]'
- en: </examples>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: </examples>
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: factors = []
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: factors = []
- en: d = 2
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d = 2
- en: 'while n > 1:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'while n > 1:'
- en: 'while n % d == 0:'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'while n % d == 0:'
- en: factors.append(d)
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: factors.append(d)
- en: n /= d
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: n /= d
- en: d += 1
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: d += 1
- en: return factors
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return factors
- en: 'print(prime_factors(24)) # Output: [2, 2, 2, 3]'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(prime_factors(24)) # 输出: [2, 2, 2, 3]'
- en: </assistant_response>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: </assistant_response>
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: weather = {
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: weather = {
- en: '"description": "sunny",'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '"description": "sunny",'
- en: '"temperature": 75'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '"temperature": 75'
- en: '}'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'User: What''s the weather like?'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：天气怎么样？
- en: 'Assistant: It''s going to be {{ weather["description"] }} with a temperature
    of'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：天气将 {{ weather["description"] }} ，温度为
- en: '{{ weather["temperature"] }} degrees.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '{{ weather["temperature"] }} 摄氏度。'
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Weather Forecast
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 天气预报
- en: '{{ weather["description"] }} with a temperature of {{ weather["temperature"]
    }}'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '{{ weather["description"] }} ，温度为 {{ weather["temperature"] }}'
- en: degrees
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 摄氏度
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <weather>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <weather>
- en: <description>sunny</description>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <description>sunny</description>
- en: <temperature>75</temperature>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <temperature>75</temperature>
- en: </weather>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: </weather>
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: // <consider this snippet from ../skill.go>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: // <consider this snippet from ../skill.go>
- en: // type Skill interface {
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: // type Skill interface {
- en: "//\tExecute(data []byte) (refs, error)"
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: "//\tExecute(data []byte) (refs, error)"
- en: // }
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: // }
- en: // </end snippet>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: // </end snippet>
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the following, when I encounter a question like "Who was the first President
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下情况下，当我遇到像 "谁是第一任总统
- en: of the United States?" I will give an answer like "George Washington."
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 美国的第一任总统是谁？" 我会给出一个像 "乔治·华盛顿" 这样的答案。
- en: '```'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: Alternatively, you can integrate examples directly into the document as solutions
    to previous tasks. This approach requires careful formulation but can be highly
    effective. It allows the model to leverage the few-shot examples more naturally
    and creates a smoother prompt. This method is especially useful in ChatML or similar
    conversation transcript settings, where you can make the model believe it has
    successfully solved previous tasks in the style of the examples, thus encouraging
    it to continue using that successful approach.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以直接将示例集成到文档中，作为先前任务的解决方案。这种方法需要仔细的表述，但可能非常有效。它允许模型更自然地利用少量示例，并创建更平滑的提示。这种方法在ChatML或类似的对话记录设置中特别有用，您可以让模型相信它已经以示例风格成功解决了先前任务，从而鼓励它继续使用那种成功的方法。
- en: Elastic Snippets
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性片段
- en: When you’re converting content into snippets, each piece of information usually
    corresponds to a single snippet. However, sometimes, one piece of content can
    be split into multiple snippets or be represented in various forms.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将内容转换为片段时，每条信息通常对应一个单独的片段。然而，有时，一条内容可以被分割成多个片段，或者以各种形式表示。
- en: For example, consider a literary analysis task asking about the significance
    of a specific scene in Alex Garland’s novel *The Beach*. If you ask ChatGPT about
    this scene, chances are that it won’t be familiar with that particular one, and
    any [answer](https://oreil.ly/2FPat) it gives will be vague, wrong, or both. To
    improve the response, you need to include relevant context from the book in your
    prompt. You remember how to retrieve relevant book passages from [Chapter 5](ch05.html#ch05_prompt_content_1728435524680844),
    and let’s say you identify two key moments.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个文学分析任务，询问亚历克斯·加兰德小说《海滩》中一个特定场景的重要性。如果您向ChatGPT询问这个场景，它可能不熟悉这个特定的场景，它给出的任何[答案](https://oreil.ly/2FPat)都可能是不明确的、错误的，或者两者兼而有之。为了改进响应，您需要在提示中包含来自书籍的相关上下文。您记得如何从[第5章](ch05.html#ch05_prompt_content_1728435524680844)检索相关的书籍段落，并且假设您确定了两个关键时刻。
- en: 'You can snippetize these passages in different ways, as shown in [Figure 6-5](#ch06a_figure_5_1728442733835249).
    Ideally, you’d include the entire chapter for full context. That’s one way, but
    with limited prompt space and limited model attention, you probably must tighten
    your belt regarding context a bit. But that leaves you with different possibilities:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用不同的方式将这些段落片段化，如图[图6-5](#ch06a_figure_5_1728442733835249)所示。理想情况下，您应该包括整个章节以获得完整的上下文。这是其中一种方法，但考虑到有限的提示空间和模型注意力有限，您可能必须稍微收紧上下文。但这给您留下了不同的可能性：
- en: Add two snippets with no context around them.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加两个没有上下文的片段。
- en: Add two snippets with some context around each.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加两个带有一些上下文的片段。
- en: Add one combined snippet with context linking the parts.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个带有将部分链接起来的上下文的组合片段。
- en: 'All three options have things to recommend them. The first option is short,
    the last option conveys most of the information (including how the snippets relate
    to each other), and the middle option is somewhere in between. But of course,
    there are even more options: you can choose a tiny bit of context, a lot of context,
    etc. How do you deal with such a situation?'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种选项都有其可取之处。第一种选项较短，最后一种选项传达了大部分信息（包括片段之间如何相互关联），而中间选项则介于两者之间。当然，还有更多选项：您可以选择一点上下文，很多上下文，等等。您如何处理这种情况？
- en: '![](assets/pefl_0605.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0605.png)'
- en: Figure 6-5\. Snippetizing context into flexible snippets
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 将上下文片段化为灵活的片段
- en: 'There are two general approaches to such a situation where there’s a variable
    amount of context you could include. We have used both, depending on our exact
    requirements:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有两种一般方法可以处理包含可变数量上下文的情况。我们根据具体需求使用了这两种方法：
- en: You could use what we call *elastic* prompt elements, which are prompt elements
    that have different versions, ranging from short to long. In this case, the longest
    version would be the whole chapter, a slightly shorter version would be one where
    one paragraph is replaced by “...,” and an even shorter version would be one where
    two paragraphs are replaced by “...” This can go all the way down to the shortest
    version, where it’s just the two snippets you want to quote with no extra context
    and a “...” between them. Then, when you’re assembling the prompt, you don’t ask,
    “Do we have space to include this snippet?” Rather, you ask, “What’s the biggest
    version of this snippet we have space for?”
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用我们所说的*弹性*提示元素，这些提示元素有不同的版本，从短到长不等。在这种情况下，最长的版本将是整个章节，稍微短一点的版本可能是一个段落被替换为“...”，而更短的版本则可能是两个段落被替换为“...”。这可以一直缩减到最短的版本，其中只包含你想要引用的两个片段，它们之间没有额外的上下文，并且用“...”隔开。然后，当你组装提示时，你不会问，“我们有没有空间包含这个片段？”而是问，“我们能为这个片段留出多大空间的最大版本？”
- en: Alternatively, you could create multiple prompt elements from the retrieved
    information. For example, you might have one snippet that is the first relevant
    text passage, another that is that passage plus some context, and another with
    even more context. You’ll have to remember to only actually include one of these,
    as they overlap. So, this approach requires a prompt assembly method that allows
    you to declare prompt elements as incompatible (see the next section).
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，你可以从检索到的信息中创建多个提示元素。例如，你可能有一个片段是第一段相关的文本，另一个是那段文本加上一些上下文，还有一个包含更多上下文。你必须记住只实际包含其中一个，因为它们有重叠。因此，这种方法需要一个允许你声明提示元素为不兼容的提示组装方法（见下一节）。
- en: Relationships Among Prompt Elements
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示元素之间的关系
- en: 'Prompt elements don’t exist in a vacuum: a prompt is an amalgam of several
    of them. Any algorithm that combines prompt elements has to account for three
    ways in which the elements relate to each other: position and ordering, importance,
    and dependency. You’ll need to keep these three dimensions in mind while you’re
    constructing the prompt elements. Let’s go through each of them.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 提示元素不是孤立存在的：提示是由几个元素混合而成的。任何结合提示元素的算法都必须考虑到元素之间相互关联的三种方式：位置和顺序、重要性以及依赖性。在构建提示元素时，你需要牢记这三个维度。让我们逐一探讨它们。
- en: Position
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置
- en: '*Position* determines where each element should appear in the prompt. Prompt
    elements usually need to follow a specific order—while you might skip some, *rearranging*
    them can make the document confusing. For example, if you’re quoting from reference
    documents, you should maintain the original order; don’t place the second snippet
    before the first. In chats or narratives, stick to the chronological order. In
    other situations, ensure elements are in the correct sections; for instance, a
    description of a book the user likes shouldn’t go in the “Books I really hate”
    section.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*位置*决定了每个元素在提示中应该出现的位置。提示元素通常需要遵循特定的顺序——虽然你可能可以跳过一些，但*重新排列*它们可能会使文档变得混乱。例如，如果你正在引用参考文档，你应该保持原始顺序；不要将第二个片段放在第一个之前。在聊天或叙述中，坚持按时间顺序进行。在其他情况下，确保元素位于正确的部分；例如，用户喜欢的书籍描述不应该放在“我真的很讨厌的书籍”部分。'
- en: To manage these relationships, you might use an array or linked list of prompt
    elements, an index covering all elements, or a unique position value for each
    element. Often, the order reflects how you gather information (e.g., scanning
    a document or retrieving context section by section). In such cases, you generally
    only need to append new elements to the end.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理这些关系，你可能可以使用提示元素的数组或链表、覆盖所有元素的索引，或者为每个元素提供一个独特的位置值。通常，顺序反映了你收集信息的方式（例如，扫描文档或逐节检索上下文）。在这种情况下，你通常只需要将新元素追加到末尾。
- en: Importance
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要性
- en: '*Importance* determines how crucial it is to include a prompt element to convey
    relevant information to the model. Beginners often confuse position with importance,
    as they are frequently correlated—recent information is often more important.
    But there are many exceptions—for example, your introduction is often more important
    than most of the details in the middle (which are rightfully consigned to the
    Valley of Meh from [Figure 6-1](#ch06a_figure_1_1728442733835135)).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要性** 决定了包含提示元素以向模型传达相关信息的必要性。初学者经常混淆位置和重要性，因为它们通常是相关的——最近的信息通常更重要。但有许多例外——例如，你的介绍通常比中间的大部分细节（这些细节理应归入[图6-1](#ch06a_figure_1_1728442733835135)中的“梅尔谷”）更重要。'
- en: When evaluating the importance of each element, consider the tradeoff between
    including large chunks of relevant information and including many smaller, less
    critical elements. Decide whether to measure importance based on snippet length
    or an absolute scale, but choose one method and apply it consistently. Short,
    efficient prompt elements are often preferable to longer ones that convey the
    same amount of information. If you don’t account for length initially, ensure
    the prompt assembly engine can adjust importance based on token length later.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估每个元素的重要性时，考虑包含大量相关信息的块与包含许多较小、不那么关键的元素之间的权衡。决定是否根据片段长度或绝对尺度来衡量重要性，但选择一种方法并始终如一地应用。简短、高效的提示元素通常比传达相同信息量的长提示元素更可取。如果你最初没有考虑到长度，确保提示组装引擎可以在以后根据标记长度调整重要性。
- en: To assess importance, use either a numerical score or discrete priority tiers.
    *Tiers* are a small number of levels you can quickly sort your sources into, with
    lower tiers being cut first, if necessary. Some elements —such as the central
    instructions and the description of the output format—are so vital that they must
    be included at all costs. These need to occupy the highest tier. Next typically
    come explanations in the second-highest tier and context in the third. But as
    you delve deeper into the subtleties and compare different context sources or
    different degrees of relevance, consider adding numbers for finer prioritization.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估重要性，可以使用数值分数或离散优先级层级。*层级* 是你可以快速将来源分类到其中的少数几个级别，如果需要，较低层级首先被裁剪。一些元素——例如中心指令和输出格式的描述——如此关键，以至于它们必须不惜一切代价包含在内。这些元素需要占据最高层级。接下来通常是第二高层的解释，然后是第三层的环境。但随着你深入探讨细节并比较不同的环境来源或不同程度的相关性，考虑添加数字以进行更精细的优先级排序。
- en: Assigning importance involves judgment and is crucial for effective prompt engineering.
    You also need to test and refine these importance parameters with the methods
    we’ll explore further in [Chapter 10](ch10.html#ch10_evaluating_llm_applications_1728407085475721).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 分配重要性涉及判断，对于有效的提示工程至关重要。你还需要使用我们在[第10章](ch10.html#ch10_evaluating_llm_applications_1728407085475721)中将进一步探讨的方法来测试和细化这些重要性参数。
- en: Dependency
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**依赖**'
- en: '*Dependency* is the final type of relationship among prompt elements, and it
    focuses on how including one element affects the inclusion of others. Dependencies
    can be complex, but in practice, they usually fall into two categories—requirements
    and incompatibilities:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*依赖* 是提示元素之间关系的最终类型，它关注的是包含一个元素如何影响其他元素的包含。依赖关系可能很复杂，但在实践中，它们通常分为两类——需求和互斥性：'
- en: Requirements
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**需求**'
- en: These occur when one prompt element depends on another. For example, you need
    to establish that “Richard is the protagonist of *The Beach*” before stating “He
    grew up in England.”
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个提示元素依赖于另一个元素时，就会发生这种情况。例如，在说“他在英国长大”之前，你需要确定“理查德是《海滩》的主角”。
- en: Incompatibilities
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**互斥性**'
- en: These occur when one prompt element excludes another. This often happens when
    the same information can be presented in different ways, such as in a summary
    versus a detailed explanation. If your prompt assembly engine can handle incompatibilities,
    you can include both versions with an exclusion note, giving the longer version
    when space allows and using the shorter version as a fallback.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个提示元素排除另一个元素时，就会发生这种情况。这通常发生在相同的信息可以用不同的方式呈现时，例如在摘要与详细解释之间。如果你的提示组装引擎可以处理互斥性，你可以包含两个版本，并附带排除说明，在空间允许时提供较长的版本，并使用较短的版本作为后备。
- en: 'At this point in the text, you should have transformed all your pieces of content:
    the static ones that you prepared beforehand and the dynamic ones that you gathered
    as context into proper prompt elements like the ones in [Figure 6-6](#ch06a_figure_6_1728442733835267).
    That means you’re finally ready to assemble your prompt.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本的这个位置，您应该已经将所有内容片段转换成适当的形式：您事先准备的静态内容以及您作为上下文收集的动态内容，就像[图6-6](#ch06a_figure_6_1728442733835267)中的那些。这意味着您最终准备好组装您的提示。
- en: '![](assets/pefl_0606.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0606.png)'
- en: Figure 6-6\. Prompt elements and their properties, including all the information
    you need to assemble your prompt
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. 提示元素及其属性，包括您组装提示所需的所有信息
- en: Putting It All Together
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: 'To create the final prompt, you need to solve an optimization problem: deciding
    which elements to include in the prompt to maximize its overall value.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建最终的提示，您需要解决一个优化问题：决定在提示中包含哪些元素以最大化其整体价值。
- en: 'You have two main constraints:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您有两个主要约束：
- en: Dependency structure
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖结构
- en: Ensure that any requirements and incompatibilities between elements are respected.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 确保尊重元素之间的任何要求和兼容性问题。
- en: Prompt length
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 提示长度
- en: Keep the total prompt length within a set limit, typically your context window
    size minus the tokens needed for the model’s response. If your context window
    is very large, you might use a softer token budget based on available compute
    and to avoid including too much irrelevant context.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示的总长度保持在一定的限制内，通常是您的上下文窗口大小减去模型响应所需的标记数。如果您的上下文窗口非常大，您可能使用基于可用计算和避免包含过多无关上下文的较软的标记预算。
- en: Once you decide which elements to include, arrange them according to their positions
    to form the final prompt.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您决定包含哪些元素，就根据它们的顺序排列它们，以形成最终的提示。
- en: This problem is similar to linear programming and 0-1 knapsack problems, where
    you decide whether to include an element (though knapsack problems often don’t
    account for dependencies). However, there isn’t a standard tool that automatically
    solves this for you, so you’ll need to create your own solution. It can be a rewarding
    process, allowing you to customize it to your specific needs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题类似于线性规划和0-1背包问题，其中您决定是否包含一个元素（尽管背包问题通常不考虑依赖关系）。然而，没有标准的工具可以自动为您解决这个问题，因此您需要创建自己的解决方案。这可能是一个有成就感的过程，让您能够根据具体需求进行定制。
- en: Consider what you need from your prompt assembly; for example, if you need a
    fast assembly for interactive apps or if you have specific dependency patterns
    to handle. In Copilot code completions, code snippets often require a specific
    postfix, so we handle these with custom functions that manage dependencies between
    lines of code.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑您从提示汇编中需要什么；例如，如果您需要一个快速的汇编用于交互式应用程序，或者您有特定的依赖模式需要处理。在Copilot代码补全中，代码片段通常需要一个特定的后缀，因此我们使用自定义函数来管理代码行之间的依赖关系。
- en: When developing your app iteratively—starting with a basic version and then
    expanding—it’s useful to begin with a minimal prompt crafter like the one shown
    in [Figure 6-7](#ch06a_figure_7_1728442733835286). This simple tool helps you
    test whether your app idea has potential. With this approach, you don’t need to
    evaluate or prioritize snippets because the prompt crafter uses only the end portion
    of your content. This method works well because LLMs are trained to handle document
    suffixes effectively. It’s also suitable for applications where you build on a
    main text or for chatlike applications where recent exchanges are most relevant.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当您迭代开发您的应用程序时——从一个基本版本开始，然后扩展——使用[图6-7](#ch06a_figure_7_1728442733835286)中展示的最小提示构建器开始是有用的。这个简单的工具可以帮助您测试您的应用程序想法是否有潜力。采用这种方法，您不需要评估或优先考虑代码片段，因为提示构建器只使用您内容的最末端部分。这种方法效果很好，因为LLMs被训练来有效地处理文档后缀。它也适用于您基于主要文本构建的应用程序，或者对于最近交换最相关的聊天式应用程序。
- en: '![](assets/pefl_0607.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0607.png)'
- en: Figure 6-7\. The minimal prompt crafter, which orders prompt elements and keeps
    as many at the end as it can fit into the token budget
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7\. 最小提示构建器，它按顺序排列提示元素，并尽可能在标记预算内保留尽可能多的元素
- en: 'As your app develops, you’ll need a more advanced prompt-crafting engine. For
    speed, consider using a greedy algorithm as shown in [Figure 6-8](#ch06a_figure_8_1728442733835307)
    (possibly combined with some limited exploration of alternatives). There are two
    main types of greedy algorithms you can use, depending on how your prompt elements
    interact: an additive approach and a subtractive approach.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的应用发展，你需要一个更高级的提示构建引擎。为了速度，考虑使用如图[图6-8](#ch06a_figure_8_1728442733835307)所示的贪婪算法（可能结合一些对替代方案的有限探索）。你可以使用两种主要的贪婪算法类型，这取决于你的提示元素如何交互：一种加法方法，一种减法方法。
- en: In the *additive greedy approach*, you start with an empty prompt and add elements
    one by one. Each step involves adding the highest-value element that meets all
    requirements, doesn’t conflict with existing elements, and fits within the prompt
    length. This method is even effective if you have many more elements than fit
    into your prompt and you need to eliminate a lot. However, it requires few cyclical
    requirements and few cases of high-value elements dependent on low-value ones.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在*加法贪婪方法*中，你从一个空的提示开始，逐个添加元素。每一步都涉及添加满足所有要求、不与现有元素冲突且适合提示长度的最高价值元素。即使你有比提示中能容纳的更多元素，并且需要消除很多元素，这种方法也是有效的。然而，它需要很少的循环要求，以及很少的高价值元素依赖于低价值元素的情况。
- en: When using the additive greedy approach, you can simplify the process of finding
    the best element to add by sorting your elements based on their requirements and
    values. This way, you consider elements only once all their dependencies are satisfied.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用加法贪婪方法时，你可以通过根据它们的要求和值对元素进行排序来简化找到最佳添加元素的过程。这样，你只考虑所有依赖关系都得到满足的元素。
- en: '![](assets/pefl_0608.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/pefl_0608.png)'
- en: Figure 6-8\. Additive greedy approach, in which the prompt crafter iteratively
    adds high-value elements to the prompt until the token budget is filled up, then
    re-sorts the elements according to position
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8。加法贪婪方法，其中提示构建者迭代地向提示中添加高价值元素，直到填满令牌预算，然后根据位置重新排序元素
- en: With the *subtractive* *greedy approach* shown in [Figure 6-9](#ch06a_figure_9_1728442733835326),
    you start by including all prompt elements and then gradually removing those that
    are less valuable or whose dependencies are no longer met. This method works well
    if you have a manageable number of elements and few incompatibilities. Otherwise,
    the process can become cumbersome. High-value elements dependent on low-value
    elements can also lead to suboptimal results, unless you use advanced techniques
    to prioritize retaining high-value dependencies. Elastic snippets are normally
    easier to handle in a subtractive approach than in an additive approach.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6-9](#ch06a_figure_9_1728442733835326)中所示的*减法* *贪婪方法*，你首先包括所有提示元素，然后逐渐移除那些价值较低或其依赖关系不再满足的元素。如果你有可管理的元素数量和很少的不兼容性，这种方法效果很好。否则，这个过程可能会变得繁琐。依赖于低价值元素的高价值元素也可能导致次优结果，除非你使用高级技术来优先保留高价值依赖关系。弹性片段在减法方法中通常比在加法方法中更容易处理。
- en: '![A collage of different colored squares  Description automatically generated](assets/pefl_0609.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色方块的拼贴  自动生成描述](assets/pefl_0609.png)'
- en: Figure 6-9\. The subtractive greedy approach, in which the prompt-crafting engine
    successively eliminates prompt elements of low value, pruning missing requirements
    in between
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9。减法贪婪方法，其中提示构建引擎依次消除低价值的提示元素，并在其中修剪缺失的要求
- en: Please note, however, that all of the sketches for prompt-crafting engines presented
    in this chapter are meant as basic prototypes. Maybe you’ll find them sufficient
    for your application, but you should be willing to move beyond them in a way dictated
    by your specific requirements as these requirements become clear to you when you
    refine your application.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，本章中展示的所有提示构建引擎草图都旨在作为基本原型。也许你会发现它们对你的应用足够用，但你应该愿意根据你具体的要求超越它们，因为这些要求在你细化应用时变得清晰。
- en: Conclusion
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we’ve covered the art of crafting an effective prompt from
    your gathered information. We explored how to choose the right document format
    and examined various prototype documents that LLMs excel at completing.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了从收集到的信息中构建有效提示的艺术。我们探讨了如何选择正确的文档格式，并检查了各种原型文档，这些文档在LLMs（大型语言模型）擅长完成方面表现出色。
- en: You’ve also learned how to convert your information into prompt elements—snippets
    of text that fit seamlessly into your document while aligning in relevance, order,
    and dependency. Now, you’ll be able to refine these elements to create a concise
    and effective prompt using a custom prompt-crafting engine, inspired by the strategies
    covered here, with ease.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学会了如何将你的信息转化为提示元素——这些文本片段能够无缝地融入你的文档中，同时保持相关性、顺序和依赖性。现在，你将能够通过一个定制的提示构建引擎，轻松地精炼这些元素，创建一个简洁而有效的提示。
- en: Congratulations on completing the feedforward pass from [Chapter 3](ch03.html#ch03a_moving_toward_chat_1728432131625250)—you’ve
    successfully created a coherent prompt for the model. In the next chapter, we’ll
    focus on how to ensure that you receive meaningful and accurate responses.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了从[第3章](ch03.html#ch03a_moving_toward_chat_1728432131625250)的反馈传递——你已经成功为模型创建了一个连贯的提示。在下一章中，我们将关注如何确保你获得有意义的和准确的响应。
