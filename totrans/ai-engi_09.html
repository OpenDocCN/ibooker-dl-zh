<html><head></head><body><section data-pdf-bookmark="Chapter 9. Inference Optimization" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch09_inference_optimization_1730130963006301">&#13;
      <h1><span class="label">Chapter 9. </span>Inference Optimization</h1>&#13;
<p><a contenteditable="false" data-primary="inference optimization" data-type="indexterm" id="ch09.html0"/>New models come and go, but one thing will always remain relevant: making them better, cheaper, and faster. Up until now, the book has discussed various techniques for making models better. This chapter focuses on making them faster and cheaper.</p>&#13;
<p>No matter how good your model is, if it’s too slow, your users might lose patience, or worse, its predictions might become useless—imagine a next-day stock price prediction model that takes two days to compute each outcome. If your model is too expensive, its return on investment won’t be worth it.</p>&#13;
<p>Inference optimization can be done at the model, hardware, and service levels. At the model level, you can reduce a trained model’s size or develop more efficient architectures, such as one without the computation bottlenecks in the attention mechanism often used in transformer models. At the hardware level, you can design more powerful hardware.</p>&#13;
<p>The inference service runs the model on the given hardware to accommodate user requests. It can incorporate techniques that optimize models for specific hardware. It also needs to consider usage and traffic patterns to efficiently allocate resources to reduce latency and cost.</p>&#13;
<p>Because of this, inference optimization is an interdisciplinary field that often sees collaboration among model researchers, application developers, system engineers, compiler designers, hardware architects, and even data center operators.</p>&#13;
<p>This chapter discusses bottlenecks for AI inference and techniques to overcome them. It’ll focus mostly on optimization at the model and service levels, with an overview of AI accelerators.</p>&#13;
<p class="pagebreak-before">This chapter also covers performance metrics and trade-offs. Sometimes, a technique that speeds up a model can also reduce its cost. For example, reducing a model’s precision makes it smaller and faster. But often, optimization requires trade-offs. For example, the best hardware might make your model run faster but at a higher cost.</p>&#13;
<p>Given the growing availability of open source models, more teams are building their own inference services. However, even if you don’t implement these inference optimization techniques, understanding these techniques will help you evaluate inference services and frameworks. If your application’s latency and cost are hurting you, read on. This chapter might help you diagnose the causes and potential solutions.</p>&#13;
      <section data-pdf-bookmark="Understanding Inference Optimization" data-type="sect1"><div class="sect1" id="ch09_understanding_inference_optimization_1730130963006725">&#13;
        <h1>Understanding Inference Optimization</h1>&#13;
<p><a contenteditable="false" data-primary="inference optimization" data-secondary="understanding" data-type="indexterm" id="ch09.html1"/>There are two distinct phases in an AI model’s lifecycle: training and inference. Training refers to the process of building a model. Inference refers to the process of using a model to compute an output for a given input.<sup><a data-type="noteref" href="ch09.html#id1597" id="id1597-marker">1</a></sup> Unless you train or finetune a model, you’ll mostly need to care about inference.<sup><a data-type="noteref" href="ch09.html#id1598" id="id1598-marker">2</a></sup></p>&#13;
<p>This section starts with an overview of inference that introduces a shared vocabulary to discuss the rest of the chapter. If you’re already familiar with these concepts, feel free to skip to the section of interest.</p>&#13;
        <section data-pdf-bookmark="Inference Overview" data-type="sect2"><div class="sect2" id="ch09_inference_overview_1730130963006834">&#13;
          <h2>Inference Overview</h2>&#13;
  <p><a contenteditable="false" data-primary="inference optimization" data-secondary="understanding" data-tertiary="inference overview" data-type="indexterm" id="ch09.html2"/>In production, the component that runs model inference is called an inference server. It hosts the available models and has access to the necessary hardware. Based on requests from applications (e.g., user prompts), it allocates resources to execute the appropriate models and returns the responses to users. <a contenteditable="false" data-primary="inference service" data-secondary="and inference optimization" data-secondary-sortas="inference optimization" data-type="indexterm" id="id1599"/>An inference server is part of a broader inference service, which is also responsible for receiving, routing, and possibly preprocessing requests before they reach the inference server. A visualization of a simple inference service is shown in <a data-type="xref" href="#ch09_figure_1_1730130962952524">Figure 9-1</a>.</p>&#13;
          <figure><div class="figure" id="ch09_figure_1_1730130962952524">&#13;
    <p>. </p>&#13;
            <img alt="A diagram of a computer hardware system&#10;&#10;Description automatically generated" src="assets/aien_0901.png"/>&#13;
            <h6><span class="label">Figure 9-1. </span>A simple inference service.</h6>&#13;
          </div></figure>&#13;
  <p>Model APIs like those provided by OpenAI and Google are inference services. If you use one of these services, you won’t be implementing most of the techniques discussed in this chapter. However, if you host a model yourself, you’ll be responsible for building, optimizing, and maintaining its inference service.</p>&#13;
          <section data-pdf-bookmark="Computational bottlenecks" data-type="sect3"><div class="sect3" id="ch09_computational_bottlenecks_1730130963006904">&#13;
            <h3>Computational bottlenecks</h3>&#13;
    <p><a contenteditable="false" data-primary="bottlenecks" data-secondary="computational" data-type="indexterm" id="ch09.html3"/><a contenteditable="false" data-primary="computational bottlenecks" data-type="indexterm" id="ch09.html4"/><a contenteditable="false" data-primary="inference optimization" data-secondary="inference overview" data-tertiary="computational bottlenecks" data-type="indexterm" id="ch09.html5"/>Optimization is about identifying bottlenecks and addressing them. For example, to optimize traffic, city planners might identify congestion points and take measures to alleviate congestion. Similarly, an inference server should be designed to address the computational bottlenecks of the inference workloads it serves. There are two main computational bottlenecks, <em>compute-bound</em> and <em>memory bandwidth-bound</em>:</p>&#13;
            <dl>&#13;
              <dt>Compute-bound </dt>&#13;
<dd><p><a contenteditable="false" data-primary="bottlenecks" data-secondary="compute-bound" data-type="indexterm" id="id1600"/><a contenteditable="false" data-primary="compute-bound bottlenecks" data-type="indexterm" id="id1601"/>This refers to tasks whose time-to-complete is determined by the computation needed for the tasks. For example, password decryption is typically compute-bound due to the intensive mathematical calculations required to break encryption algorithms.</p></dd>&#13;
              <dt>Memory bandwidth-bound </dt>&#13;
<dd><p><a contenteditable="false" data-primary="bottlenecks" data-secondary="memory" data-type="indexterm" id="id1602"/><a contenteditable="false" data-primary="memory bottlenecks" data-secondary="bandwidth-bound" data-type="indexterm" id="id1603"/>These tasks are constrained by the data transfer rate within the system, such as the speed of data movement between memory and processors. For example, if you store your data in the CPU memory and train a model on GPUs, you have to move data from the CPU to the GPU, which can take a long time. This can be shortened as bandwidth-bound. In literature, memory bandwidth-bound is often referred to as memory-bound.</p></dd>&#13;
            </dl>&#13;
            <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch09_terminology_ambiguity_memory_bound_versus_bandwid_1730130963006967">&#13;
              <h1>Terminology Ambiguity: Memory-Bound Versus Bandwidth-Bound</h1>&#13;
      <p><a contenteditable="false" data-primary="inference optimization" data-secondary="memory-bound versus bandwidth-bound interference" data-type="indexterm" id="id1604"/><em>Memory-bound</em> is also used by some people to refer to tasks whose time-to-complete is constrained by memory capacity instead of memory bandwidth. This occurs when your hardware doesn’t have sufficient memory to handle the task, for example, if your machine doesn’t have enough memory to store the entire internet. This memory is often manifested in the error recognizable by engineers everywhere: OOM, out-of-memory.<sup><a data-type="noteref" href="ch09.html#id1605" id="id1605-marker">3</a></sup></p>&#13;
      <p>However, this situation can often be mitigated by splitting your task into smaller pieces. For example, if you’re constrained by GPU memory and cannot fit an entire model into the GPU, you can split the model across GPU memory and CPU memory. This splitting will slow down your computation because of the time it takes to transfer data between the CPU and GPU. However, if data transfer is fast enough, this becomes less of an issue. Therefore, the memory capacity limitation is actually more about memory bandwidth.</p>&#13;
            </div></aside>&#13;
    <p>The concepts of compute-bound or memory bandwidth-bound were introduced in the paper “Roofline” (<a href="https://oreil.ly/M_aGR">Williams et al., 2009</a>).<sup><a data-type="noteref" href="ch09.html#id1606" id="id1606-marker">4</a></sup> Mathematically, an operation can be classified as compute-bound or memory bandwidth-bound based on its <a href="https://oreil.ly/K3j6t"><em>arithmetic intensity</em></a>, which is the number of arithmetic operations per byte of memory access. Profiling tools like NVIDIA Nsight will show you a roofline chart to tell you whether your workload is compute-bound or memory bandwidth-bound, as shown in <a data-type="xref" href="#ch09_figure_2_1730130962952613">Figure 9-2</a>. This chart is a <em>roofline</em> chart because it resembles a roof. Roofline charts are common in hardware performance analyses.</p>&#13;
           &#13;
 <p>Different optimization techniques aim to mitigate different bottlenecks. For example, a compute-bound workload might be sped up by spreading it out to more chips or by leveraging chips with more computational power (e.g., a higher FLOP/s number). A memory bandwidth-bound workload might be sped up by leveraging chips with higher bandwidth.</p>&#13;
&#13;
            <figure><div class="figure" id="ch09_figure_2_1730130962952613">&#13;
              <img alt="A graph with a line and a point&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0902.png"/>&#13;
              <h6><span class="label">Figure 9-2. </span>The roofline chart can help you visualize whether an operation is compute-bound or memory bandwidth-bound. This graph is on a log scale.</h6>&#13;
            </div></figure>&#13;
   &#13;
&#13;
    <p>Different model architectures and workloads result in different computational bottlenecks. For example, inference for image generators like Stable Diffusion is typically compute-bound, whereas inference for autoregression language models is typically memory bandwidth-bound.</p>&#13;
    <p>As an illustration, let’s look into language model inference. Recall from <a data-type="xref" href="ch02.html#ch02_understanding_foundation_models_1730147895571359">Chapter 2</a> that inference for a transformer-based language model consists of two steps, prefilling and decoding:</p>&#13;
            <dl>&#13;
              <dt>Prefill</dt>&#13;
<dd><p>The model processes the input tokens in parallel.<sup><a data-type="noteref" href="ch09.html#id1607" id="id1607-marker">5</a></sup> How many tokens can be processed at once is limited by the number of operations your hardware can execute in a given time. Therefore, prefilling is <em>compute-bound</em>.</p></dd>&#13;
              <dt>Decode</dt>&#13;
<dd><p>The model generates one output token at a time. At a high level, this step typically involves loading large matrices (e.g., model weights) into GPUs, which is limited by how quickly your hardware can load data into memory. Decoding is, therefore, <em>memory bandwidth-bound</em>.</p></dd>&#13;
            </dl>&#13;
&#13;
<p><a data-type="xref" href="#ch09_figure_3_1730130962952638">Figure 9-3</a> visualizes prefilling and decoding.</p>&#13;
            <figure><div class="figure" id="ch09_figure_3_1730130962952638">&#13;
              <img alt="A diagram of a computer&#10;&#10;Description automatically generated" src="assets/aien_0903.png"/>&#13;
              <h6><span class="label">Figure 9-3. </span>Autoregressive language models follow two steps for inference: prefill and decode. <code>&lt;eos&gt;</code> denotes the end of the sequence token.</h6>&#13;
            </div></figure>&#13;
    <p>Because prefill and decode have different computational profiles, they are often decoupled in production with separate machines. This technique will be discussed <a data-type="xref" href="#ch09_inference_service_optimization_1730130963008735">“Inference Service Optimization”</a>.</p>&#13;
    <p>The factors that affect the amount of prefilling and decoding computation in an LLM inference server, and therefore its bottlenecks, include context length, output length, and request batching strategies. Long context typically results in a memory bandwidth-bound workload, but clever optimization techniques, such as those discussed later in this chapter, can remove this bottleneck.</p>&#13;
    <p>As of this writing, due to the prevalence of the transformer architecture and the limitations of the existing accelerator technologies, many AI and data workloads are memory bandwidth-bound. However, future software and hardware advancements will be able to make AI and data workloads compute-bound.<a contenteditable="false" data-primary="" data-startref="ch09.html5" data-type="indexterm" id="id1608"/><a contenteditable="false" data-primary="" data-startref="ch09.html4" data-type="indexterm" id="id1609"/><a contenteditable="false" data-primary="" data-startref="ch09.html3" data-type="indexterm" id="id1610"/></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Online and batch inference APIs" data-type="sect3"><div class="sect3" id="ch09_online_and_batch_inference_apis_1730130963007027">&#13;
            <h3>Online and batch inference APIs</h3>&#13;
    <p><a contenteditable="false" data-primary="inference APIs" data-type="indexterm" id="ch09.html6b"/><a contenteditable="false" data-primary="batching" data-secondary="batch inference APIs" data-type="indexterm" id="ch09.html6a"/><a contenteditable="false" data-primary="batch inference APIs" data-type="indexterm" id="ch09.html6"/><a contenteditable="false" data-primary="inference optimization" data-secondary="inference overview" data-tertiary="online and batch inference APIs" data-type="indexterm" id="ch09.html7"/><a contenteditable="false" data-primary="online inference APIs" data-type="indexterm" id="ch09.html8"/>Many providers offer two types of inference APIs, online and batch:</p>&#13;
            <ul>&#13;
<li><p>Online APIs optimize for latency. Requests are processed as soon as they arrive.</p></li>&#13;
<li><p>Batch APIs optimize for cost. If your application doesn’t have strict latency requirements, you can send them to batch APIs for more efficient processing. Higher latency allows a broader range of optimization techniques, including batching requests together and using cheaper hardware. For example, as of this writing, both <a contenteditable="false" data-primary="OpenAI" data-secondary="batch APIs" data-type="indexterm" id="id1611"/>Google Gemini and OpenAI offer batch APIs at a 50% cost <span class="keep-together">reduction</span> and significantly higher turnaround time, i.e., in the order of hours instead of seconds or minutes.<sup><a data-type="noteref" href="ch09.html#id1612" id="id1612-marker">6</a></sup></p></li>&#13;
            </ul>&#13;
    <p>Online APIs might still batch requests together as long as it doesn’t significantly impact latency, as discussed in <a data-type="xref" href="#ch09_batching_1730130963008799">“Batching”</a>. The only real difference is that an online API focuses on lower latency, whereas a batch API focuses on higher throughput.</p>&#13;
    <p>Customer-facing use cases, such as chatbots and code generation, typically require lower latency, and, therefore, tend to use online APIs. Use cases with less stringent latency requirements, which are ideal for batch APIs, include the following:</p>&#13;
            <ul>&#13;
<li><p>Synthetic data generation</p></li>&#13;
<li><p>Periodic reporting, such as summarizing Slack messages, sentiment analysis of brand mentions on social media, and analyzing customer support tickets</p></li>&#13;
<li><p>Onboarding new customers who require processing of all their uploaded <span class="keep-together">documents</span></p></li>&#13;
<li><p>Migrating to a new model that requires reprocessing of all the data</p></li>&#13;
<li><p>Generating personalized recommendations or newsletters for a large customer base</p></li>&#13;
<li><p>Knowledge base updates by reindexing an organization’s data</p></li>&#13;
            </ul>&#13;
    <p>APIs usually return complete responses by default. However, with autoregressive decoding, it can take a long time for a model to complete a response, and users are impatient. Many online APIs offer <em>streaming mode</em>, which returns each token as it’s generated. This reduces the time the users have to wait until the first token. The downside of this approach is that you can’t score a response before showing it to users, increasing the risk of users seeing bad responses. However, you can still retrospectively update or remove a response as soon as the risk is detected.</p>&#13;
            <div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
      <p>A batch API for foundation models differs from batch inference for traditional ML. In traditional ML:</p>&#13;
              <ul>&#13;
                <li>&#13;
          <p>Online inference means that predictions are computed <em>after</em> requests have arrived.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>Batch inference means that predictions are precomputed <em>before</em> requests have arrived. </p>&#13;
                </li>&#13;
              </ul>&#13;
      <p>Precompution is possible for use cases with finite and predictable inputs like recommendation systems, where recommendations can be generated for all users in advance. These precomputed predictions are fetched when requests arrive, e.g., when a user visits the website. However, with foundation model use cases where the inputs are open-ended, it’s hard to predict all user prompts<a contenteditable="false" data-primary="" data-startref="ch09.html8" data-type="indexterm" id="id1613"/><a contenteditable="false" data-primary="" data-startref="ch09.html7" data-type="indexterm" id="id1614"/><a contenteditable="false" data-primary="" data-startref="ch09.html6b" data-type="indexterm" id="id1615"/><a contenteditable="false" data-primary="" data-startref="ch09.html6a" data-type="indexterm" id="id1616"/><a contenteditable="false" data-primary="" data-startref="ch09.html6" data-type="indexterm" id="id1617"/>.<a contenteditable="false" data-primary="" data-startref="ch09.html2" data-type="indexterm" id="id1618"/><sup><a data-type="noteref" href="ch09.html#id1619" id="id1619-marker">7</a></sup> </p>&#13;
            </div>&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Inference Performance Metrics" data-type="sect2"><div class="sect2" id="ch09_inference_performance_metrics_1730130963007094">&#13;
          <h2>Inference Performance Metrics</h2>&#13;
  <p><a contenteditable="false" data-primary="metrics" data-secondary="inference performance metrics" data-type="indexterm" id="ch09.html9a"/><a contenteditable="false" data-primary="inference optimization" data-secondary="inference performance metrics" data-type="indexterm" id="ch09.html9"/><a contenteditable="false" data-primary="inference optimization" data-secondary="understanding" data-tertiary="inference performance metrics" data-type="indexterm" id="ch09.html10"/><a contenteditable="false" data-primary="inference performance metrics" data-type="indexterm" id="ch09.html11"/>Before jumping into optimization, it’s important to understand what metrics to optimize for. From the user perspective, the central axis is latency (response quality is a property of the model itself, not of the inference service). However, application developers must also consider throughput and utilization as they determine the cost of their applications.</p>&#13;
          <section data-pdf-bookmark="Latency, TTFT, and TPOT" data-type="sect3"><div class="sect3" id="ch09_latency_ttft_and_tpot_1730130963007162">&#13;
            <h3>Latency, TTFT, and TPOT</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="inference performance metrics" data-tertiary="latency, TTFT, and TPOT" data-type="indexterm" id="ch09.html12"/><a contenteditable="false" data-primary="inference performance metrics" data-secondary="latency, TTFT, and TPOT" data-type="indexterm" id="ch09.html13"/><a contenteditable="false" data-primary="latency" data-secondary="inference performance and" data-type="indexterm" id="ch09.html14"/><a contenteditable="false" data-primary="time per output token (TPOT)" data-type="indexterm" id="ch09.html15"/><a contenteditable="false" data-primary="time to first token (TTFT)" data-type="indexterm" id="ch09.html16"/><a contenteditable="false" data-primary="TPOT (time per output token)" data-type="indexterm" id="ch09.html17"/><a contenteditable="false" data-primary="TTFT (time to first token)" data-type="indexterm" id="ch09.html18"/>Latency measures the time from when users send a query until they receive the complete response. For autoregressive generation, especially in the streaming mode, the overall latency can be broken into several metrics:</p>&#13;
            <dl>&#13;
              <dt>Time to first token </dt>&#13;
<dd><p>TTFT measures how quickly the first token is generated after users send a query. It corresponds to the duration of the prefill step and depends on the input’s length. Users might have different expectations for TTFT for different applications. For example, for conversational chatbots, the TTFT should be instantaneous.<sup><a data-type="noteref" href="ch09.html#id1620" id="id1620-marker">8</a></sup> However, users might be willing to wait longer to summarize long documents.</p></dd>&#13;
              <dt>Time per output token </dt>&#13;
<dd><p>TPOT measures how quickly each output token is generated after the first token. If each token takes 100 ms, a response of 1,000 tokens will take 100 s.</p></dd>&#13;
<dd><p>In the streaming mode, where users read each token as it’s generated, TPOT should be faster than human reading speed but doesn’t have to be much faster. A very fast reader can read 120 ms/token, so a TPOT of around 120 ms, or 6–8 tokens/second, is sufficient for most use cases.</p></dd>&#13;
              <dt>Time between tokens and inter-token latency</dt>&#13;
<dd><p><a contenteditable="false" data-primary="time between tokens (TBT)" data-type="indexterm" id="id1621"/><a contenteditable="false" data-primary="inter-token latency (ITL)" data-type="indexterm" id="id1622"/>Variations of this metric include <em>time between tokens (TBT)</em> and i<em>nter-token latency (ITL)</em>.<sup><a data-type="noteref" href="ch09.html#id1623" id="id1623-marker">9</a></sup> Both measure the time between output tokens.</p></dd>&#13;
            </dl>&#13;
    <p>The total latency will equal <code>TTFT + TPOT </code>×<code> (number of output tokens).</code></p>&#13;
    <p>Two applications with the same total latency can offer different user experiences with different TTFT and TPOT. Would your users prefer instant first tokens with a longer wait between tokens, or would they rather wait slightly longer for the first tokens but enjoy faster token generation afterward? User studies will be necessary to determine the optimal user experience. Reducing TTFT at the cost of higher TPOT is possible by shifting more compute instances from decoding to prefilling and vice versa.<sup><a data-type="noteref" href="ch09.html#id1624" id="id1624-marker">10</a></sup></p>&#13;
    <p>It’s important to note that the TTFT and TPOT values observed by users might differ from those observed by models, especially in scenarios involving CoT (chain-of-thought) or agentic queries where models generate intermediate steps not shown to users. Some teams use the metric <em>time to publish</em> to make it explicit that it measures time to the first token users see.</p>&#13;
    <p>Consider the scenario where, after a user sends a query, the model performs the following steps:</p>&#13;
            <ol>&#13;
<li><p>Generate a plan, which consists of a sequence of actions. This plan isn’t shown to the user.</p></li>&#13;
<li><p>Take actions and log their outputs. These outputs aren’t shown to the user.</p></li>&#13;
<li><p>Based on these outputs, generate a final response to show the user.</p></li>&#13;
            </ol>&#13;
    <p class="pagebreak-before">From the model’s perspective, the first token is generated in step 1. This is when the model internally begins its token generation process. The user, however, only sees the first token of the final output generated in step 3. Thus, from their perspective, TTFT is much longer.</p>&#13;
    <p>Because latency is a distribution, the average can be misleading. Imagine you have 10 requests whose TTFT values are 100 ms, 102 ms, 100 ms, 100 ms, 99 ms, 104 ms, 110 ms, 90 ms, 3,000 ms, 95 ms. The average TTFT value is 390 ms, which makes your inference service seem slower than it is. There might have been a network error that slowed down one request or a particularly long prompt that took a much longer time to prefill. Either way, you should investigate. With a large volume of requests, outliers that skew the average latency are almost inevitable.</p>&#13;
    <p>It’s more helpful to look at latency in percentiles, as they tell you something about a certain percentage of your requests. The most common percentile is the 50th percentile, abbreviated as p50 (median). If the median is 100 ms, half of the requests take longer than 100 ms to generate the first token, and half take less than 100 ms. Percentiles also help you discover outliers, which might be symptoms of something wrong. Typically, the percentiles you’ll want to look at are p90, p95, and p99. It’s also helpful to plot TTFT values against inputs’ lengths.<a contenteditable="false" data-primary="" data-startref="ch09.html18" data-type="indexterm" id="id1625"/><a contenteditable="false" data-primary="" data-startref="ch09.html17" data-type="indexterm" id="id1626"/><a contenteditable="false" data-primary="" data-startref="ch09.html16" data-type="indexterm" id="id1627"/><a contenteditable="false" data-primary="" data-startref="ch09.html15" data-type="indexterm" id="id1628"/><a contenteditable="false" data-primary="" data-startref="ch09.html14" data-type="indexterm" id="id1629"/><a contenteditable="false" data-primary="" data-startref="ch09.html13" data-type="indexterm" id="id1630"/><a contenteditable="false" data-primary="" data-startref="ch09.html12" data-type="indexterm" id="id1631"/></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Throughput and goodput" data-type="sect3"><div class="sect3" id="ch09_throughput_and_goodput_1730130963007220">&#13;
            <h3>Throughput and goodput</h3>&#13;
    <p><a contenteditable="false" data-primary="inference service" data-secondary="throughput/goodput" data-type="indexterm" id="ch09.html19a"/><a contenteditable="false" data-primary="goodput" data-type="indexterm" id="ch09.html19"/><a contenteditable="false" data-primary="inference optimization" data-secondary="inference performance metrics" data-tertiary="throughput/goodput" data-type="indexterm" id="ch09.html20"/><a contenteditable="false" data-primary="inference performance metrics" data-secondary="throughput/goodput" data-type="indexterm" id="ch09.html21"/><a contenteditable="false" data-primary="throughput" data-type="indexterm" id="ch09.html22"/>Throughput measures the number of output tokens per second an inference service can generate across all users and requests.</p>&#13;
    <p>Some teams count both input and output tokens in throughput calculation. However, since processing input tokens (prefilling) and generating output tokens (decoding) have different computational bottlenecks and are often decoupled in modern inference servers, input and output throughput should be counted separately. When throughput is used without any modifier, it usually refers to output tokens.</p>&#13;
    <p>Throughput is typically measured as tokens/s (TPS). If you serve multiple users, tokens/s/user is also used to evaluate how the system scales with more users.</p>&#13;
    <p>Throughput can also be measured as the number of <em>completed</em> requests during a given time. Many applications use requests per second (RPS). However, for applications built on top of foundation models, a request might take seconds to complete, so many people use completed requests per minute (RPM) instead. Tracking this metric is useful for understanding how an inference service handles concurrent requests. Some providers might throttle your service if you send too many concurrent requests at the same time.</p>&#13;
    <p class="pagebreak-before">Throughput is directly linked to compute cost. A higher throughput typically means lower cost. If your system costs $2/h in compute and its throughput is 100 tokens/s, it costs around $5.556 per 1M output tokens. If each request generates 200 output tokens on average, the cost for decoding 1K requests would be $1.11.</p>&#13;
    <p>The prefill cost can be similarly calculated. If your hardware costs $2 per hour and it can prefill 100 requests per minute, the cost for prefilling 1K requests would be $0.33.</p>&#13;
    <p>The total cost per request is the sum of the prefilling and decoding costs. In this example, the total cost for 1K requests would be $1.11 + $0.33 = $1.44.</p>&#13;
    <p>What’s considered good throughput depends on the model, the hardware, and the workload. Smaller models and higher-end chips typically result in higher throughput. Workloads with consistent input and output lengths are easier to optimize than workloads with variable lengths.</p>&#13;
    <p>Even for similarly sized models, hardware, and workloads, direct throughput comparisons might be only approximate because token count depends on what constitutes a token, and different models have different tokenizers. It’s better to compare the efficiency of inference servers using metrics such as cost per request. </p>&#13;
    <p>Just like most other software applications, AI applications have the latency/throughput trade-off. Techniques like batching can improve throughput but reduce latency. According to the LinkedIn AI team in their reflection after a year of deploying generative AI products (<a href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US">LinkedIn, 2024</a>), it’s not uncommon to double or triple the throughput if you’re willing to sacrifice TTFT and TPOT.</p>&#13;
    <p>Due to this trade-off, focusing on an inference service based solely on its throughput and cost can lead to a bad user experience. Instead, some teams focus on <a href="https://en.wikipedia.org/wiki/Goodput"><em>goodput</em></a>, a metric adapted from networking for LLM applications. Goodput measures the number of requests per second that satisfies the SLO, software-level objective.</p>&#13;
    <p>Imagine that your application has the following objectives: TTFT of at most 200 ms and TPOT of at most 100 ms. Let’s say that your inference service can complete 100 requests per minute. However, out of these 100 requests, only 30 satisfy the SLO. Then, the goodput of this service is 30 requests per minute.<a contenteditable="false" data-primary="" data-startref="ch09.html22" data-type="indexterm" id="id1632"/><a contenteditable="false" data-primary="" data-startref="ch09.html21" data-type="indexterm" id="id1633"/><a contenteditable="false" data-primary="" data-startref="ch09.html20" data-type="indexterm" id="id1634"/><a contenteditable="false" data-primary="" data-startref="ch09.html19a" data-type="indexterm" id="id1635"/><a contenteditable="false" data-primary="" data-startref="ch09.html19" data-type="indexterm" id="id1636"/> A visualization of this is shown in <a data-type="xref" href="#ch09_figure_4_1730130962952660">Figure 9-4</a>.</p>&#13;
            <figure><div class="figure" id="ch09_figure_4_1730130962952660">&#13;
              <img alt="A graph showing different colored bars&#10;&#10;Description automatically generated" src="assets/aien_0904.png"/>&#13;
              <h6><span class="label">Figure 9-4. </span>If an inference service can complete 10 RPS but only 3 satisfy the SLO, then its goodput is 3 RPS.</h6>&#13;
            </div></figure>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Utilization, MFU, and MBU" data-type="sect3"><div class="sect3" id="ch09_utilization_mfu_and_mbu_1730130963007279">&#13;
            <h3>Utilization, MFU, and MBU</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="inference performance metrics" data-tertiary="utilization, MFU, and MBU" data-type="indexterm" id="ch09.html23"/><a contenteditable="false" data-primary="inference performance metrics" data-secondary="utilization, MFU, and MBU" data-type="indexterm" id="ch09.html24"/><a contenteditable="false" data-primary="MBU (model bandwidth utilization)" data-type="indexterm" id="ch09.html25"/><a contenteditable="false" data-primary="MFU (model FLOPs utilization)" data-type="indexterm" id="ch09.html26"/><a contenteditable="false" data-primary="model bandwidth utilization (MBU)" data-type="indexterm" id="ch09.html27"/><a contenteditable="false" data-primary="model FLOPs utilization (MFU)" data-type="indexterm" id="ch09.html28"/>Utilization metrics measure how efficiently a resource is being used. It typically quantifies the proportion of the resource actively being used compared to its total available capacity.</p>&#13;
    <p>A common but often misunderstood metric is <em>GPU utilization</em>, and NVIDIA is partially to blame for this misunderstanding. The official NVIDIA tool for monitoring GPU usage is <a href="https://oreil.ly/ludJ2"><code>nvidia-smi</code></a>—SMI stands for System Management Interface. One metric this tool shows is GPU utilization, which represents the percentage of time during which the GPU is actively processing tasks. For example, if you run inference on a GPU cluster for 10 hours, and the GPUs are actively processing tasks for 5 of those hours, your GPU utilization would be 50%.</p>&#13;
    <p>However, actively processing tasks doesn’t mean doing so efficiently. For simplicity, consider a tiny GPU capable of doing 100 operations per second. In <code>nvidia-smi</code>’s definition of utilization, this GPU can report 100% utilization even if it’s only doing one operation per second.</p>&#13;
    <p>If you pay for a machine that can do 100 operations and use it for only 1 operation, you’re wasting money. <code>nvidia-smi</code>’s GPU optimization metric is, therefore, not very useful. A utilization metric you might care about, out of all the operations a machine is capable of computing, is how many it’s doing in a given time. This metric is called <em>MFU (Model FLOP/s Utilization)</em>, which distinguishes it from the NVIDIA GPU utilization metric.</p>&#13;
    <p>MFU is the ratio of the observed throughput (tokens/s) relative to the theoretical maximum throughput of a system operating at peak FLOP/s. If at the peak FLOP/s advertised by the chip maker, the chip can generate 100 tokens/s, but when used for your inference service, it can generate only 20 tokens/s, your MFU is 20%.<sup><a data-type="noteref" href="ch09.html#id1637" id="id1637-marker">11</a></sup></p>&#13;
    <p>Similarly, because memory bandwidth is expensive, you might also want to know how efficiently your hardware’s bandwidth is utilized. <em>MBU (Model Bandwidth Utilization)</em> measures the percentage of achievable memory bandwidth used. If the chip’s peak bandwidth is 1 TB/s and your inference uses only 500 GB/s, your MBU is 50%.</p>&#13;
    <p>Computing the memory bandwidth being used for LLM inference is straightforward:</p>&#13;
            <pre data-type="programlisting">parameter count × bytes/param × tokens/s</pre>&#13;
    <p>MBU is computed as follows:</p>&#13;
            <pre data-type="programlisting">(parameter count × bytes/param × tokens/s) / (theoretical bandwidth)</pre>&#13;
    <p>For example, if you use a 7B-parameter model in FP16 (two bytes per parameter) and achieve 100 tokens/s, the bandwidth used is:</p>&#13;
            <pre data-type="programlisting">7B × 2 × 100 = 700 GB/s</pre>&#13;
    <p>This underscores the importance of quantization (discussed in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>). Fewer bytes per parameter mean your model consumes less valuable bandwidth.</p>&#13;
    <p>If this is done on an A100-80GB GPU with a theoretical 2 TB/s of memory bandwidth, the MBU is:</p>&#13;
            <pre data-type="programlisting">(700 GB/s) / (2 TB/s) = 70%</pre>&#13;
    <p>The relationships between throughput (tokens/s) and MBU and between throughput and MFU are linear, so some people might use throughput to refer to MBU and MFU.</p>&#13;
    <p>What’s considered a good MFU and MBU depends on the model, hardware, and workload. Compute-bound workloads typically have higher MFU and lower MBU, while bandwidth-bound workloads often show lower MFU and higher MBU.</p>&#13;
    <p>Because training can benefit from more efficient optimization (e.g., better batching), thanks to having more predictable workloads, MFU for training is typically higher than MFU for inference. For inference, since prefill is compute-bound and decode is memory bandwidth-bound, MFU during prefilling is typically higher than MFU during decoding. For model training, as of this writing, an MFU above 50% is generally considered good, but it can be hard to achieve on specific hardware.<sup><a data-type="noteref" href="ch09.html#id1638" id="id1638-marker">12</a></sup> <a data-type="xref" href="#ch09_table_1_1730130962971021">Table 9-1</a> shows MFU for several models and accelerators. </p>&#13;
            <table id="ch09_table_1_1730130962971021">&#13;
              <caption><span class="label">Table 9-1. </span>MFU examples from “PaLM: Scaling Language Modeling with Pathways” (Chowdhery et al., 2022).</caption>&#13;
              <thead>&#13;
                <tr>&#13;
                  <th>Model</th>&#13;
                  <th>Number of parameters (in billions)</th>&#13;
                  <th>Accelerator chips</th>&#13;
                  <th>Model FLOP/s utilization</th>&#13;
                </tr>&#13;
              </thead>&#13;
              <tr>&#13;
                <td>GPT-3</td>&#13;
                <td>175B</td>&#13;
                <td>V100</td>&#13;
                <td>21.3%</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>Gopher</td>&#13;
                <td>280B</td>&#13;
                <td>4096 TPU v3</td>&#13;
                <td>32.5%</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>Megatron-Turing NLG</td>&#13;
                <td>530B</td>&#13;
                <td>2240 A100</td>&#13;
                <td>30.2%</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>PaLM</td>&#13;
                <td>540B</td>&#13;
                <td>6144 TPU v4</td>&#13;
                <td>46.2%</td>&#13;
              </tr>&#13;
            </table>&#13;
    <p><a data-type="xref" href="#ch09_figure_5_1730130962952692">Figure 9-5</a> shows the MBU for the inference process using Llama 2-70B in FP16 on different hardware. The decline is likely due to the higher computational load per second with more users, shifting the workload from being bandwidth-bound to compute-bound.</p>&#13;
            <figure><div class="figure" id="ch09_figure_5_1730130962952692">&#13;
              <img alt="A graph of a number of users&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0905.png"/>&#13;
              <h6><span class="label">Figure 9-5. </span>Bandwidth utilization for Llama 2-70B in FP16 across three different chips shows a decrease in MBU as the number of concurrent users increases. Image from “LLM Training and Inference with Intel Gaudi 2 AI Accelerators” (<a href="https://oreil.ly/tOOOD">Databricks, 2024</a>).</h6>&#13;
            </div></figure>&#13;
    <p>Utilization metrics are helpful to track your system’s efficiency. Higher utilization rates for similar workloads on the same hardware generally mean that your services are becoming more efficient. However, <em>the goal isn’t to get the chips with the highest utilization</em>. What you really care about is how to get your jobs done faster and cheaper. A higher utilization rate means nothing if the cost and latency both increase<a contenteditable="false" data-primary="" data-startref="ch09.html28" data-type="indexterm" id="id1639"/><a contenteditable="false" data-primary="" data-startref="ch09.html27" data-type="indexterm" id="id1640"/><a contenteditable="false" data-primary="" data-startref="ch09.html26" data-type="indexterm" id="id1641"/><a contenteditable="false" data-primary="" data-startref="ch09.html25" data-type="indexterm" id="id1642"/><a contenteditable="false" data-primary="" data-startref="ch09.html24" data-type="indexterm" id="id1643"/><a contenteditable="false" data-primary="" data-startref="ch09.html23" data-type="indexterm" id="id1644"/>.<a contenteditable="false" data-primary="" data-startref="ch09.html11" data-type="indexterm" id="id1645"/><a contenteditable="false" data-primary="" data-startref="ch09.html10" data-type="indexterm" id="id1646"/><a contenteditable="false" data-primary="" data-startref="ch09.html9" data-type="indexterm" id="id1647"/><a contenteditable="false" data-primary="" data-startref="ch09.html9a" data-type="indexterm" id="id1648"/></p>&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="AI Accelerators" data-type="sect2"><div class="sect2" id="ch09_ai_accelerators_1730130963007351">&#13;
          <h2>AI Accelerators</h2>&#13;
  <p><a contenteditable="false" data-primary="accelerators" data-type="indexterm" id="ch09.html29"/><a contenteditable="false" data-primary="inference optimization" data-secondary="understanding" data-tertiary="AI accelerators" data-type="indexterm" id="ch09.html30"/>How fast and cheap software can run depends on the hardware it runs on. While there are optimization techniques that work across hardware, understanding hardware allows for deeper optimization. This section looks at hardware from an inference perspective, but it can be applied to training as well.</p>&#13;
  <p>The development of AI models and hardware has always been intertwined. The lack of sufficiently powerful computers was one of the contributing factors to the first AI winter in the 1970s.<sup><a data-type="noteref" href="ch09.html#id1649" id="id1649-marker">13</a></sup></p>&#13;
  <p>The revival of interest in deep learning in 2012 was also closely tied to compute. One commonly acknowledged reason for the popularity of AlexNet (<a href="https://oreil.ly/Yv4V7">Krizhevsky et al., 2012</a>) is that it was the first paper to successfully use <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>, graphics processing units, to train neural networks.<sup><a data-type="noteref" href="ch09.html#id1650" id="id1650-marker">14</a></sup> Before GPUs, if you wanted to train a model at AlexNet’s scale, you’d have to use thousands of CPUs, like the one <a href="https://oreil.ly/Xpwco">Google released just a few months before AlexNet</a>. Compared to thousands of CPUs, a couple of GPUs were a lot more accessible to PhD students and researchers, setting off the deep learning research boom.</p>&#13;
          <section class="pagebreak-before" data-pdf-bookmark="What’s an accelerator?" data-type="sect3"><div class="sect3" id="ch09_what_s_an_accelerator_1730130963007426">&#13;
            <h3 class="less_space">What’s an accelerator?</h3>&#13;
    <p><a contenteditable="false" data-primary="accelerators" data-secondary="defined" data-type="indexterm" id="ch09.html31"/><a contenteditable="false" data-primary="inference optimization" data-secondary="AI accelerators" data-tertiary="defined" data-type="indexterm" id="ch09.html32"/>An accelerator is a chip designed to accelerate a specific type of computational workload. An AI accelerator is designed for AI workloads. The dominant type of AI accelerator is GPUs, and the biggest economic driver during the AI boom in the early 2020s is undoubtedly NVIDIA.</p>&#13;
    <p>The main difference between CPUs and GPUs is that CPUs are designed for general-purpose usage, whereas GPUs are designed for parallel processing:</p>&#13;
            <ul>&#13;
<li><p>CPUs have a few powerful cores, typically up to 64 cores for high-end consumer machines. While many CPU cores can handle multi-threaded workloads effectively, they excel at tasks requiring high single-thread performance, such as running an operating system, managing I/O (input/output) operations, or handling complex, sequential processes.</p></li>&#13;
              <li><p>GPUs have thousands of smaller, less powerful cores optimized for tasks that can be broken down into many smaller, independent calculations, such as graphics rendering and machine learning. The operation that constitutes most ML workloads is matrix multiplication, which is highly parallelizable.<sup><a data-type="noteref" href="ch09.html#id1651" id="id1651-marker">15</a></sup></p>&#13;
              </li>&#13;
            </ul>&#13;
    <p>While the pursuit of efficient parallel processing increases computational capabilities, it imposes challenges on memory design and power consumption.</p>&#13;
    <p>The success of NVIDIA GPUs has inspired many accelerators designed to speed up AI workloads, including <a href="https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units">Advanced Micro Devices (AMD)’s newer generations of GPUs</a>, Google’s TPU (<a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit">Tensor Processing Unit</a>), <a href="https://oreil.ly/oDQOk">Intel’s Habana Gaudi</a>, <a href="https://oreil.ly/6ySTY">Graphcore’s Intelligent Processing Unit</a> (IPU), <a href="https://oreil.ly/R7gXn">Groq’s Language Processing Unit</a> (LPU), <a href="https://oreil.ly/ACIty">Cerebras’ Wafer-Scale</a> <a href="https://en.wikipedia.org/wiki/List_of_quantum_processors">Quant Processing Unit</a> (QPU), and many more being introduced.</p>&#13;
    <p>While many chips can handle both training and inference, one big theme emerging is specialized chips for inference. A survey by <a href="https://oreil.ly/qSpMK">Desislavov et al. (2023)</a> shares that inference can exceed the cost of training in commonly used systems, and that inference accounts for up to 90% of the machine learning costs for deployed AI systems.</p>&#13;
    <p class="pagebreak-before">As discussed in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>, training demands much more memory due to backpropagation and is generally more difficult to perform in lower precision. Furthermore, training usually emphasizes throughput, whereas inference aims to minimize latency.</p>&#13;
    <p>Consequently, chips designed for inference are often optimized for lower precision and faster memory access, rather than large memory capacity. Examples of such chips include the Apple <a href="https://en.wikipedia.org/wiki/Neural_Engine">Neural Engine</a>, <a href="https://oreil.ly/42LSB">AWS Inferentia</a>, and <a href="https://oreil.ly/XH2bh">MTIA</a> (Meta Training and Inference Accelerator). Chips designed for edge computing, like <a href="https://oreil.ly/m8daG">Google’s Edge TPU</a> and the <a href="https://oreil.ly/PRZSQ">NVIDIA Jetson Xavier</a>, are also typically geared toward inference.</p>&#13;
    <p>There are also chips specialized for different model architectures, such as chips specialized for the transformer.<sup><a data-type="noteref" href="ch09.html#id1652" id="id1652-marker">16</a></sup> Many chips are designed for data centers, with more and more being designed for consumer devices (such as phones and laptops).</p>&#13;
    <p>Different hardware architectures have different memory layouts and specialized compute units that evolve over time. These units are optimized for specific data types, such as scalars, vectors, or tensors, as shown in <a data-type="xref" href="#ch09_figure_6_1730130962952710">Figure 9-6</a>.</p>&#13;
            <figure><div class="figure" id="ch09_figure_6_1730130962952710">&#13;
              <img alt="A diagram of a computer&#10;&#10;Description automatically generated" src="assets/aien_0906.png"/>&#13;
              <h6><span class="label">Figure 9-6. </span>Different compute primitives. Image inspired by <a href="https://arxiv.org/abs/1802.04799">Chen et al. (2018)</a>.</h6>&#13;
            </div></figure>&#13;
    <p>A chip might have a mixture of different compute units optimized for various data types. For example, GPUs traditionally supported vector operations, but many modern GPUs now include tensor cores optimized for matrix and tensor computations. TPUs, on the other hand, are designed with tensor operations as their primary compute primitive. To efficiently operate a model on a hardware architecture, its memory layout and compute primitives need to be taken into account.</p>&#13;
    <p>A chip’s specifications contain many details that can be useful when evaluating this chip for each specific use case. However, the main characteristics that matter across use cases are computational capabilities, memory size and bandwidth, and power consumption. I’ll use GPUs as examples to illustrate these characteristics.<a contenteditable="false" data-primary="" data-startref="ch09.html32" data-type="indexterm" id="id1653"/><a contenteditable="false" data-primary="" data-startref="ch09.html31" data-type="indexterm" id="id1654"/></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Computational capabilities" data-type="sect3"><div class="sect3" id="ch09_computational_capabilities_1730130963007489">&#13;
            <h3>Computational capabilities</h3>&#13;
    <p><a contenteditable="false" data-primary="accelerators" data-secondary="computational capabilities" data-type="indexterm" id="id1655"/><a contenteditable="false" data-primary="computational capabilities, of AI accelerators" data-type="indexterm" id="id1656"/><a contenteditable="false" data-primary="inference optimization" data-secondary="AI accelerators" data-tertiary="computational capabilities" data-type="indexterm" id="id1657"/>Computational capabilities are typically measured by the number of operations a chip can perform in a given time. The most common metric is <em>FLOP/s</em>, often written as FLOPS, which measures the <em>peak</em> number of floating-point operations per second. In reality, however, it’s very unlikely that an application can achieve this peak FLOP/s. The ratio between the actual FLOP/s and the theoretical FLOP/s is one <em>utilization</em> metric.</p>&#13;
    <p>The number of operations a chip can perform in a second depends on the numerical precision—the higher the precision, the fewer operations the chip can execute. Think about how adding two 32-bit numbers generally requires twice the computation of adding two 16-bit numbers. The number of 32-bit operations a chip can perform in a given time is not exactly half that of 16-bit operations because of different chips’ optimization. For an overview of numerical precision, revisit <a data-type="xref" href="ch07.html#ch07b_numerical_representations_1730159634259493">“Numerical Representations”</a>.</p>&#13;
    <p><a data-type="xref" href="#ch09_table_2_1730130962971057">Table 9-2</a> shows the FLOP/s specs for different precision formats for <a href="https://oreil.ly/bNAOG">NVIDIA H100 SXM chips</a>.</p>&#13;
            <table id="ch09_table_2_1730130962971057">&#13;
              <caption><span class="label">Table 9-2. </span>FLOP/s specs for NVIDIA H100 SXM chips.</caption>&#13;
              <thead>&#13;
                <tr>&#13;
                  <th>Numerical precision</th>&#13;
                  <th>teraFLOP/s (trillion FLOP/s) with sparsity</th>&#13;
                </tr>&#13;
              </thead>&#13;
              <tbody>&#13;
              <tr>&#13;
                <td>TF32 Tensor Core<sup><a data-type="noteref" href="ch09.html#id1658" id="id1658-marker">a</a></sup></td>&#13;
                <td>989</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>BFLOAT16 Tensor Core</td>&#13;
                <td>1,979</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>FP16 Tensor Core</td>&#13;
                <td>1,979</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>FP8 Tensor Core</td>&#13;
                <td>3,958</td>&#13;
              </tr>&#13;
            </tbody>&#13;
            <tbody><tr class="footnotes"><td colspan="2"><p data-type="footnote" id="id1658"><sup><a href="ch09.html#id1658-marker">a</a></sup> Recall from <a data-type="xref" href="ch07.html#ch07">Chapter 7</a> that TF32 is a 19-bit, not 32-bit, format.</p></td></tr></tbody></table>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Memory size and bandwidth" data-type="sect3"><div class="sect3" id="ch09_memory_size_and_bandwidth_1730130963007548">&#13;
            <h3>Memory size and bandwidth</h3>&#13;
    <p><a contenteditable="false" data-primary="accelerators" data-secondary="memory size and bandwidth" data-type="indexterm" id="ch09.html33"/><a contenteditable="false" data-primary="inference optimization" data-secondary="AI accelerators" data-tertiary="memory size and bandwidth" data-type="indexterm" id="ch09.html34"/><a contenteditable="false" data-primary="memory bottlenecks" data-secondary="size and bandwidth" data-type="indexterm" id="ch09.html35"/>Because a GPU has many cores working in parallel, data often needs to be moved from the memory to these cores, and, therefore, data transfer speed is important. Data transfer is crucial when working with AI models that involve large weight matrices and training data. These large amounts of data need to be moved quickly to keep the cores efficiently occupied. Therefore, GPU memory needs to have higher bandwidth and lower latency than CPU memory, and thus, GPU memory requires more advanced memory technologies. This is one of the factors that makes GPU memory more expensive than CPU memory.</p>&#13;
    <p class="pagebreak-before"><a contenteditable="false" data-primary="DDR SDRAM (doubled data rate synchronous dynamic random-access memory)" data-type="indexterm" id="id1659"/><a contenteditable="false" data-primary="doubled data rate synchronous dynamic random-access memory (DDR SDRAM)" data-type="indexterm" id="id1660"/>To be more specific, CPUs typically use <a href="https://en.wikipedia.org/wiki/DDR_SDRAM">DDR SDRAM</a> (Double Data Rate Synchronous Dynamic Random-Access Memory), which has a 2D structure. GPUs, particularly high-end ones, often use <a href="https://en.wikipedia.org/wiki/High_Bandwidth_Memory">HBM</a> (high-bandwidth memory), which has a 3D stacked structure.<sup><a data-type="noteref" href="ch09.html#id1661" id="id1661-marker">17</a></sup></p>&#13;
    <p>An accelerator’s memory is measured by its <em>size and bandwidth</em>. These numbers need to be evaluated within the system an accelerator is part of. An accelerator, such as a GPU, typically interacts with three levels of memory, as visualized in <a data-type="xref" href="#ch09_figure_7_1730130962952731">Figure 9-7</a>:</p>&#13;
            &#13;
            <dl>&#13;
              <dt>CPU memory (DRAM)</dt>&#13;
<dd><p><a contenteditable="false" data-primary="CPU memory (DRAM)" data-type="indexterm" id="id1662"/><a contenteditable="false" data-primary="DRAM (CPU memory)" data-type="indexterm" id="id1663"/>Accelerators are usually deployed alongside CPUs, giving them access to the CPU memory (also known as system memory, host memory, or just CPU DRAM).</p></dd>&#13;
<dd><p>CPU memory usually has the lowest bandwidth among these memory types, with data transfer speeds ranging from 25 GB/s to 50 GB/s. CPU memory size varies. Average laptops might have around 16–64 GB, whereas high-end workstations can have one TB or more.</p></dd>&#13;
              <dt>GPU high-bandwidth memory (HBM)</dt>&#13;
<dd><p><a contenteditable="false" data-primary="high-bandwidth memory (HBM)" data-type="indexterm" id="id1664"/>This is the memory dedicated to the GPU, located close to the GPU for faster access than CPU memory.</p></dd>&#13;
<dd><p>HBM provides significantly higher bandwidth, with data transfer speeds typically ranging from 256 GB/s to over 1.5 TB/s. This speed is essential for efficiently handling large data transfers and high-throughput tasks. A consumer GPU has around 24–80 GB of HBM.</p></dd>&#13;
              <dt>GPU on-chip SRAM</dt>&#13;
<dd><p><a contenteditable="false" data-primary="GPU on-chip SRAM" data-type="indexterm" id="id1665"/>Integrated directly into the chip, this memory is used to store frequently accessed data and instructions for nearly instant access. It includes L1 and L2 caches made of SRAM, and, in some architectures, L3 caches as well. These caches are part of the broader on-chip memory, which also includes other components like register files and shared memory.</p></dd>&#13;
<dd><p>RAM has extremely high data transfer speeds, often exceeding 10 TB/s. The size of GPU SRAM is small, typically 40 MB or under.</p></dd>&#13;
            </dl>&#13;
&#13;
            <figure><div class="figure" id="ch09_figure_7_1730130962952731">&#13;
              <img alt="A colorful pyramid with multiple layers&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0907.png"/>&#13;
              <h6><span class="label">Figure 9-7. </span>The memory hierarchy of an AI accelerator. The numbers are for reference only. The actual numbers vary for each chip.</h6>&#13;
            </div></figure>&#13;
            &#13;
    <p>A lot of GPU optimization is about how to make the most out of this memory hierarchy. However, as of this writing, popular frameworks such as PyTorch and TensorFlow don’t yet allow fine-grained control of memory access. This has led many AI researchers and engineers to become interested in GPU programming languages such as <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> (originally Compute Unified Device Architecture), <a href="https://github.com/triton-lang/triton">OpenAI’s Triton</a>, and <a href="https://github.com/ROCm/ROCm">ROCm</a> (Radeon Open Compute). The latter is AMD’s open source alternative to NVIDIA’s proprietary CUDA.<a contenteditable="false" data-primary="" data-startref="ch09.html35" data-type="indexterm" id="id1666"/><a contenteditable="false" data-primary="" data-startref="ch09.html34" data-type="indexterm" id="id1667"/><a contenteditable="false" data-primary="" data-startref="ch09.html33" data-type="indexterm" id="id1668"/></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Power consumption" data-type="sect3"><div class="sect3" id="ch09_power_consumption_1730130963007605">&#13;
            <h3>Power consumption</h3>&#13;
    <p><a contenteditable="false" data-primary="accelerators" data-secondary="power consumption" data-type="indexterm" id="ch09.html36"/><a contenteditable="false" data-primary="inference optimization" data-secondary="AI accelerators" data-tertiary="power consumption" data-type="indexterm" id="ch09.html37"/><a contenteditable="false" data-primary="power consumption" data-type="indexterm" id="ch09.html38"/>Chips rely on transistors to perform computation. Each computation is done by transistors switching on and off, which requires energy. A GPU can have billions of transistors—an NVIDIA A100 has <a href="https://oreil.ly/5vRsP">54 billion</a> transistors, while an NVIDIA H100 has <a href="https://en.wikipedia.org/wiki/Hopper_(microarchitecture)">80 billion</a>. When an accelerator is used efficiently, billions of transistors rapidly switch states, consuming a substantial amount of energy and generating a nontrivial amount of heat. This heat requires cooling systems, which also consume electricity, adding to data centers’ overall energy consumption.</p>&#13;
    <p>Chip energy consumption threatens to have a staggering impact on the <a href="https://oreil.ly/RqY-3">environment</a>, increasing the pressure on companies to invest in technologies for <a href="https://en.wikipedia.org/wiki/Green_data_center">green data centers</a>. An NVIDIA H100 running at its peak for a year consumes approximately 7,000 kWh. For comparison, the average US household’s annual electricity consumption is 10,000 kWh. That’s why electricity is a bottleneck to scaling up compute.<sup><a data-type="noteref" href="ch09.html#id1669" id="id1669-marker">18</a></sup></p>&#13;
    <p>Accelerators typically specify their power consumption under <em>maximum power draw</em> or a proxy metric <em>TDP (thermal design power):</em></p>&#13;
            <ul>&#13;
<li><p>Maximum power draw indicates the peak power that the chip could draw under full load. </p></li>&#13;
<li><p><em>TDP</em> represents the maximum heat a cooling system needs to dissipate when the chip operates under typical workloads. While it’s not an exact measure of power consumption, it’s an indication of the expected power draw. For CPUs and GPUs, the maximum power draw can be roughly 1.1 to 1.5 times the TDP, though the exact relationship varies depending on the specific architecture and workload.</p></li>&#13;
            </ul>&#13;
    <p>If you opt for cloud providers, you won’t need to worry about cooling or electricity. However, these numbers can still be of interest to understand the impact of accelerators on the environment and the overall electricity demand. </p>&#13;
            <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1670">&#13;
              <h1>Selecting Accelerators</h1>&#13;
      <p>What accelerators to use depends on your workload. If your workloads are compute-bound, you might want to look for chips with more FLOP/s. If your workloads are memory-bound, shelling out money for chips with higher bandwidth and more memory will make your life easier.</p>&#13;
      <p>When evaluating which chips to buy, there are three main questions:</p>&#13;
              <ul>&#13;
                <li><p>Can the hardware run your workloads?</p></li>&#13;
                <li><p>How long does it take to do so?</p></li>&#13;
                <li><p>How much does it cost?</p></li>&#13;
              </ul>&#13;
      <p>FLOP/s, memory size, and memory bandwidth are the three big numbers that help you answer the first two questions. The last question is straightforward. Cloud providers’ pricing is typically usage-based and fairly similar across providers. If you buy your hardware, the cost can be calculated based on the initial price and ongoing power <a contenteditable="false" data-primary="" data-startref="ch09.html38" data-type="indexterm" id="id1671"/><a contenteditable="false" data-primary="" data-startref="ch09.html37" data-type="indexterm" id="id1672"/><a contenteditable="false" data-primary="" data-startref="ch09.html36" data-type="indexterm" id="id1673"/>consumption<a contenteditable="false" data-primary="" data-startref="ch09.html30" data-type="indexterm" id="id1674"/><a contenteditable="false" data-primary="" data-startref="ch09.html29" data-type="indexterm" id="id1675"/>.<a contenteditable="false" data-primary="" data-startref="ch09.html1" data-type="indexterm" id="id1676"/></p>&#13;
            </div></aside>&#13;
          </div></section>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section class="pagebreak-before" data-pdf-bookmark="Inference Optimization " data-type="sect1"><div class="sect1" id="ch09_inference_optimization_1730130963007697">&#13;
        <h1 class="less_space">Inference Optimization </h1>&#13;
<p><a contenteditable="false" data-primary="inference optimization" data-secondary="at model/hardware/service levels" data-secondary-sortas="model" data-type="indexterm" id="id1677"/>Inference optimization can be done at the model, hardware, or service level. To illustrate their differences, consider archery. Model-level optimization is like crafting better arrows. Hardware-level optimization is like training a stronger and better archer. Service-level optimization is like refining the entire shooting process, including the bow and aiming conditions.</p>&#13;
<p>Ideally, optimizing a model for speed and cost shouldn’t change the model’s quality. However, many techniques might cause model degradation. <a data-type="xref" href="#ch09_figure_8_1730130962952759">Figure 9-8</a> shows the same Llama models’ performance on different benchmarks, served by different inference service providers.</p>&#13;
        <figure><div class="figure" id="ch09_figure_8_1730130962952759">&#13;
          <img alt="A graph of different types of numbers&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0908.png"/>&#13;
          <h6><span class="label">Figure 9-8. </span>An inference service provider might use optimization techniques that can alter a model’s behavior, causing different providers to have slight model quality variations. The experiment was conducted by <a href="https://oreil.ly/5hFSF">Cerebras (2024)</a>.</h6>&#13;
        </div></figure>&#13;
<p>Since hardware design is outside the scope of this book, I’ll discuss techniques at the model and service levels. While the techniques are discussed separately, keep in mind that, in production, optimization typically involves techniques at more than one level.</p>&#13;
        <section data-pdf-bookmark="Model Optimization" data-type="sect2"><div class="sect2" id="ch09_model_optimization_1730130963007804">&#13;
          <h2>Model Optimization</h2>&#13;
  <p><a contenteditable="false" data-primary="inference optimization" data-secondary="model optimization" data-type="indexterm" id="ch09.html40"/><a contenteditable="false" data-primary="model optimization" data-type="indexterm" id="ch09.html41"/>Model-level optimization aims to make the model more efficient, often by modifying the model itself, which can alter its behavior. As of this writing, many foundation models follow the transformer architecture and include an autoregressive language model component. These models have three characteristics that make inference resource-intensive: model size, autoregressive decoding, and the attention mechanism. Let’s discuss approaches to address these challenges.</p>&#13;
          <section data-pdf-bookmark="Model compression" data-type="sect3"><div class="sect3" id="ch09_model_compression_1730130963007882">&#13;
            <h3>Model compression</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="model optimization" data-tertiary="model compression" data-type="indexterm" id="id1678"/><a contenteditable="false" data-primary="model compression" data-type="indexterm" id="id1679"/><a contenteditable="false" data-primary="model optimization" data-secondary="model compression" data-type="indexterm" id="id1680"/>Model compression involves techniques that reduce a model’s size. Making a model smaller can also make it faster. This book has already discussed two model compression techniques: quantization and distillation. Quantization, reducing the precision of a model to reduce its memory footprint and increase its throughput, is discussed in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>. Model distillation, training a small model to mimic the behavior of the large model, is discussed in <a data-type="xref" href="ch08.html#ch08_dataset_engineering_1730130932019888">Chapter 8</a>.</p>&#13;
    <p><a contenteditable="false" data-primary="distillation" data-secondary="model distillation" data-type="indexterm" id="id1681"/>Model distillation suggests that it’s possible to capture a large model’s behaviors using fewer parameters. Could it be that within the large model, there exists a subset of parameters capable of capturing the entire model’s behavior? This is the core concept behind pruning.</p>&#13;
    <p>Pruning, in the context of neural networks, has two meanings. One is to remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters. Another is to find parameters least useful to predictions and set them to zero. <a contenteditable="false" data-primary="sparse models" data-type="indexterm" id="id1682"/>In this case, pruning doesn’t reduce the total number of parameters, only the number of non-zero parameters. This makes the model more sparse, which both reduces the model’s storage space and speeds up computation.</p>&#13;
    <p>Pruned models can be used as-is or be further finetuned to adjust the remaining parameters and restore any performance degradation caused by the pruning process. Pruning can help discover promising model architectures (<a href="https://arxiv.org/abs/1810.05270">Liu et al., 2018</a>). These pruned architectures, smaller than the pre-pruned architectures, can also be trained from scratch (<a href="https://arxiv.org/abs/1710.01878">Zhu et al., 2017</a>).</p>&#13;
    <p>In the literature, there have been many encouraging pruning results. For example, <a href="https://oreil.ly/qwlHE">Frankle and Carbin (2019)</a> showed that pruning techniques can reduce the non-zero parameter counts of certain trained networks by over 90%, decreasing memory footprints and improving speed without compromising accuracy. However, in practice, as of this writing, pruning is less common. It’s harder to do, as it requires an understanding of the original model’s architecture, and the performance boost it can bring is often much less than that of other approaches. Pruning also results in sparse models, and not all hardware architectures are designed to take advantage of the resulting sparsity.</p>&#13;
    <p><em>Weight-only quantization is by far the most popular approach since it’s easy to use, works out of the box for many models, and is extremely effective.</em> Reducing a model’s precision from 32 bits to 16 bits reduces its memory footprint by half. However, we’re close to the limit of quantization—we can’t go lower than 1 bit per value. Distillation is also common because it can result in a smaller model whose behavior is comparative to that of a much larger one for your needs.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Overcoming the autoregressive decoding bottleneck" data-type="sect3"><div class="sect3" id="ch09_overcoming_the_autoregressive_decoding_bottleneck_1730130963007948">&#13;
            <h3>Overcoming the autoregressive decoding bottleneck</h3>&#13;
    <p><a contenteditable="false" data-primary="decoding" data-secondary="autoregressive decoding bottleneck" data-type="indexterm" id="ch09.html42a"/><a contenteditable="false" data-primary="autoregressive decoding bottleneck" data-type="indexterm" id="ch09.html42"/><a contenteditable="false" data-primary="bottlenecks" data-secondary="autoregressive decoding" data-type="indexterm" id="ch09.html43"/><a contenteditable="false" data-primary="inference optimization" data-secondary="model optimization" data-tertiary="autoregressive decoding bottleneck" data-type="indexterm" id="ch09.html44"/><a contenteditable="false" data-primary="model optimization" data-secondary="autoregressive decoding bottleneck" data-type="indexterm" id="ch09.html45"/>As discussed in <a data-type="xref" href="ch02.html#ch02_understanding_foundation_models_1730147895571359">Chapter 2</a>, autoregressive language models generate one token after another. If it takes 100 ms to generate one token, a response of 100 tokens will take <span class="keep-together">10 s</span>.<sup><a data-type="noteref" href="ch09.html#id1683" id="id1683-marker">19</a></sup> This process is not just slow, it’s also expensive. Across model API providers, an output token costs approximately two to four times an input token. In an experiment, Anyscale found that a single output token can have the same impact on latency as 100 input tokens (<a href="https://oreil.ly/QYdG8">Kadous et al., 2023</a>). Improving the autoregressive generation process by a small percentage can significantly improve user experience.</p>&#13;
    <p>As the space is rapidly evolving, new techniques are being developed to overcome this seemingly impossible bottleneck. Perhaps one day, there will be architectures that don’t have this bottleneck. The techniques covered here are to illustrate what the solution might look like, but the techniques are still evolving.</p>&#13;
            <section data-pdf-bookmark="Speculative decoding" data-type="sect4"><div class="sect4" id="ch09_speculative_decoding_1730130963008014">&#13;
              <h4>Speculative decoding</h4>&#13;
      <p><a contenteditable="false" data-primary="autoregressive decoding bottleneck" data-secondary="speculative decoding" data-type="indexterm" id="ch09.html46"/><a contenteditable="false" data-primary="model optimization" data-secondary="autoregressive decoding bottleneck" data-tertiary="speculative decoding" data-type="indexterm" id="ch09.html47"/><a contenteditable="false" data-primary="speculative decoding" data-type="indexterm" id="ch09.html48"/>Speculative decoding (also called speculative sampling) uses a faster but less powerful model to generate a sequence of tokens, which are then verified by the target model. The target model is the model you want to use. The faster model is called the draft or proposal model because it proposes the draft output.</p>&#13;
      <p>Imagine the input tokens are <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em></sub>:</p>&#13;
              <ol>&#13;
                <li>&#13;
          <p>The draft model generates a sequence of <em>K</em> tokens: <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>K</em></sub>.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>The target model verifies these <em>K</em> generated tokens in parallel.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>The target model <em>accepts</em> the longest subsequence of draft tokens, from left to right, which the target model agrees to use.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>Let’s say the target model accepts <em>j</em> draft tokens, <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>j</em></sub>. The target model then generates one extra token, <em>x</em><sub><em>t</em> + <em>j</em> + 1</sub>.</p>&#13;
                </li>&#13;
              </ol>&#13;
      <p> The process returns to step 1, with the draft model generating <em>K</em> tokens conditioned on <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>j</em></sub>. The process is visualized in <a data-type="xref" href="#ch09_figure_9_1730130962952786">Figure 9-9</a>.</p>&#13;
&#13;
      <p>If no draft token is accepted, this loop produces only one token generated by the target model. If all draft tokens are accepted, this loop produces <em>K</em> + 1 tokens, with <em>K</em> generated by the draft model and one by the target model. </p>&#13;
&#13;
&#13;
              <figure><div class="figure" id="ch09_figure_9_1730130962952786">&#13;
                <img alt="A diagram of words&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0909.png"/>&#13;
                <h6><span class="label">Figure 9-9. </span>A draft model generates a sequence of K tokens, and the main model accepts the longest subsequence that it agrees with. The image is from “Blockwise Parallel Decoding for Deep Autoregressive Models” (<a href="https://arxiv.org/abs/1811.03115">Stern et al., 2018</a>).</h6>&#13;
              </div></figure>&#13;
      &#13;
      <p>If all draft sequences are rejected, the target model must generate the entire response in addition to verifying it, potentially leading to increased latency. However, this can be avoided because of these three insights:</p>&#13;
              <ol>&#13;
                <li>&#13;
          <p>The time it takes for the target model to verify a sequence of tokens is less than the time it takes to generate it, because verification is parallelizable, while generation is sequential. Speculative decoding effectively turns the computation profile of decoding into that of prefilling.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>In an output token sequence, some tokens are easier to predict than others. It’s possible to find a weaker draft model capable of getting these easier-to-predict tokens right, leading to a high acceptance rate of the draft tokens.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>Decoding is memory bandwidth-bound, which means that during the coding process, there are typically idle FLOPs that can be used for free verification.<sup><a data-type="noteref" href="ch09.html#id1684" id="id1684-marker">20</a></sup></p>&#13;
                </li>&#13;
              </ol>&#13;
      <p>Acceptance rates are domain-dependent. For texts that follow specific structures like code, the acceptance rate is typically higher. Larger values of <em>K</em> mean fewer verifying calls for the target model but a low acceptance rate of the draft tokens. The draft model can be of any architecture, though ideally it should share the same vocabulary and tokenizer as the target model. You can train a custom draft model or use an existing weaker model.</p>&#13;
      <p>For example, to speed up the decoding process of Chinchilla-70B, DeepMind trained a 4B-parameter draft model of the same architecture (<a href="https://arxiv.org/abs/2302.01318">Chen et al., 2023</a>). The draft model can generate a token eight times faster than the target model (1.8 ms/token compared to 14.1 ms/token). This reduces the overall response latency by more than half without compromising response quality. A similar speed-up was achieved for T5-XXL (<a href="https://arxiv.org/abs/2211.17192">Laviathan et al., 2022</a>).</p>&#13;
      <p>This approach has gained traction because it’s relatively easy to implement and doesn’t change a model’s quality. For example, it’s possible to do so in <a href="https://oreil.ly/IaPOB">50 lines of code in PyTorch</a>. It’s been incorporated into popular inference frameworks such as <a href="https://oreil.ly/uzg1s">vLLM</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>, and <a href="https://github.com/ggerganov/llama.cpp/pull/2926">llama.cpp</a>.<a contenteditable="false" data-primary="" data-startref="ch09.html48" data-type="indexterm" id="id1685"/><a contenteditable="false" data-primary="" data-startref="ch09.html47" data-type="indexterm" id="id1686"/><a contenteditable="false" data-primary="" data-startref="ch09.html46" data-type="indexterm" id="id1687"/></p>&#13;
            </div></section>&#13;
            <section data-pdf-bookmark="Inference with reference" data-type="sect4"><div class="sect4" id="ch09_inference_with_reference_1730130963008072">&#13;
              <h4>Inference with reference</h4>&#13;
      <p><a contenteditable="false" data-primary="autoregressive decoding bottleneck" data-secondary="inference with reference" data-type="indexterm" id="id1688"/><a contenteditable="false" data-primary="inference with reference" data-type="indexterm" id="id1689"/><a contenteditable="false" data-primary="model optimization" data-secondary="autoregressive decoding bottleneck" data-tertiary="inference with reference" data-type="indexterm" id="id1690"/>Often, a response needs to reference tokens from the input. For example, if you ask your model a question about an attached document, the model might repeat a chunk of text verbatim from the document. Another example is if you ask the model to fix bugs in a piece of code, the model might reuse the majority of the original code with minor changes. Instead of making the model generate these repeated tokens, what if we copy these tokens from the input to speed up the generation? This is the core idea behind inference with reference.</p>&#13;
      <p>Inference with reference is similar to speculative decoding, but instead of using a model to generate draft tokens, it selects draft tokens from the input. The key challenge is to develop an algorithm to identify the most relevant text span from the context at each decoding step. The simplest option is to find a text span that matches the current tokens.</p>&#13;
      <p>Unlike speculative decoding, inference with reference doesn’t require an extra model. However, it’s useful only in generation scenarios where there’s a significant overlap between contexts and outputs, such as in retrieval systems, coding, or multi-turn conversations. In “Inference with Reference: Lossless Acceleration of Large Language Models” (<a href="https://arxiv.org/abs/2304.04487">Yang et al., 2023</a>), this technique helps achieve two times generation speedup in such use cases.</p>&#13;
      <p>Examples of how inference with reference works are shown in <a data-type="xref" href="#ch09_figure_10_1730130962952808">Figure 9-10</a>.</p>&#13;
              <figure class="width-60"><div class="figure" id="ch09_figure_10_1730130962952808">&#13;
                <img alt="A screenshot of a diagram&#10;&#10;Description automatically generated" src="assets/aien_0910.png"/>&#13;
                <h6><span class="label">Figure 9-10. </span>Two examples of inference with reference. The text spans that are successfully copied from the input are in red and green. Image from Yang et al. (2023). The image is licensed under CC BY 4.0.</h6>&#13;
              </div></figure>&#13;
            </div></section>&#13;
            <section data-pdf-bookmark="Parallel decoding" data-type="sect4"><div class="sect4" id="ch09_parallel_decoding_1730130963008129">&#13;
              <h4>Parallel decoding</h4>&#13;
      <p><a contenteditable="false" data-primary="autoregressive decoding bottleneck" data-secondary="parallel decoding" data-type="indexterm" id="id1691"/><a contenteditable="false" data-primary="model optimization" data-secondary="autoregressive decoding bottleneck" data-tertiary="parallel decoding" data-type="indexterm" id="id1692"/><a contenteditable="false" data-primary="parallel decoding" data-type="indexterm" id="id1693"/>Instead of making autoregressive generation faster with draft tokens, some techniques aim to break the sequential dependency. Given an existing sequence of tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>,…,<em>x</em><sub><em>t</em></sub>, these techniques attempt to generate <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>,…,<em>x</em><sub><em>t</em> + <em>k</em></sub> simultaneously. This means that the model generates <em>x</em><sub><em>t</em> + 2</sub> before it knows that the token before it is <em>x</em><sub><em>t</em> + 1</sub>.</p>&#13;
      <p>This can work because the knowledge of the existing sequence often is sufficient to predict the next few tokens. For example, given “the cat sits”, without knowing that the next token is “on”, “under”, or “behind”, you might still predict that the word after it is “the”.</p>&#13;
      <p>The parallel tokens can be generated by the same decoder, as in Lookahead decoding (<a href="https://arxiv.org/abs/2402.02057">Fu et al., 2024</a>), or by different decoding heads, as in Medusa (<a href="https://arxiv.org/abs/2401.10774">Cai et al., 2024</a>). In Medusa, the original model is extended with multiple decoding heads, and each head is a small neural network layer that is then trained to predict a future token at a specific position. If the original model is trained to predict the next token <em>x</em><sub><em>t</em> + 1</sub>, the <em>k</em><sup><em>th</em></sup> head will predict the token <em>x</em><sub><em>t</em> + <em>k</em> + 1</sub>. These heads are trained together with the original model, but the original model is frozen. NVIDIA claimed Medusa helped boost Llama 3.1 token generation by up to 1.9× on their HGX H200 GPUs (<a href="https://oreil.ly/FWYf5">Eassa et al., 2024</a>).</p>&#13;
      <p>However, because these tokens aren’t generated sequentially, they need to be verified to make sure that they fit together. An essential part of parallel decoding is verification and integration. Lookahead decoding uses the <a href="https://en.wikipedia.org/wiki/Jacobi_method">Jacobi method</a><sup><a data-type="noteref" href="ch09.html#id1694" id="id1694-marker">21</a></sup> to verify the generated tokens, which works as follows:</p>&#13;
              <ol>&#13;
                <li>&#13;
          <p>K future tokens are generated in parallel.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>These <em>K</em> tokens are verified for coherence and consistency with the context.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>If one or more tokens fail verification, instead of aggregating all <em>K</em> future tokens, the model regenerates or adjusts only these failed tokens.</p>&#13;
                </li>&#13;
              </ol>&#13;
      <p>The model keeps refining the generated tokens until they all pass verification and are integrated into the final output. This family of parallel decoding algorithms is also called Jacobi decoding.</p>&#13;
      <p>On the other hand, Medusa uses a tree-based attention mechanism to verify and integrate tokens. Each Medusa head produces several options for each position. These options are then organized into a tree-like structure to select the most promising combination. The process is visualized in <a data-type="xref" href="#ch09_figure_11_1730130962952823">Figure 9-11</a>.</p>&#13;
              <figure><div class="figure" id="ch09_figure_11_1730130962952823">&#13;
                <img alt="A diagram of a model&#10;&#10;Description automatically generated" src="assets/aien_0911.png"/>&#13;
                <h6><span class="label">Figure 9-11. </span>In Medusa (Cai et al., 2024), each head predicts several options for a token position. The most promising sequence from these options is selected. Image adapted from the paper, which is licensed under CC BY 4.0.</h6>&#13;
              </div></figure>&#13;
      <p>While the perspective of being able to circumvent sequential dependency is appealing, parallel decoding is not intuitive, and some techniques, like Medusa, can be challenging to implement. <a contenteditable="false" data-primary="" data-startref="ch09.html45" data-type="indexterm" id="id1695"/><a contenteditable="false" data-primary="" data-startref="ch09.html44" data-type="indexterm" id="id1696"/><a contenteditable="false" data-primary="" data-startref="ch09.html43" data-type="indexterm" id="id1697"/><a contenteditable="false" data-primary="" data-startref="ch09.html42a" data-type="indexterm" id="id1698"/><a contenteditable="false" data-primary="" data-startref="ch09.html42" data-type="indexterm" id="id1699"/></p>&#13;
            </div></section>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Attention mechanism optimization" data-type="sect3"><div class="sect3" id="ch09_attention_mechanism_optimization_1730130963008191">&#13;
            <h3>Attention mechanism optimization</h3>&#13;
    <p><a contenteditable="false" data-primary="attention mechanisms" data-secondary="optimization" data-type="indexterm" id="ch09.html49"/><a contenteditable="false" data-primary="inference optimization" data-secondary="model optimization" data-tertiary="attention mechanism optimization" data-type="indexterm" id="ch09.html50"/><a contenteditable="false" data-primary="model optimization" data-secondary="attention mechanism optimization" data-type="indexterm" id="ch09.html51"/>Recall from <a data-type="xref" href="ch02.html#ch02_understanding_foundation_models_1730147895571359">Chapter 2</a> that generating the next token requires the key and value vectors for all previous tokens. This means that the following applies:</p>&#13;
            <ul>&#13;
<li><p>Generating token <em>x</em><sub><em>t</em></sub> requires the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em> – 1</sub>.</p></li>&#13;
<li><p>Generating token <em>x</em><sub><em>t</em> + 1</sub> requires the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …,<em>x</em><sub><em>t</em> – 1</sub>, <em>x</em><sub><em>t</em></sub>.</p></li>&#13;
            </ul>&#13;
    <p>When generating token <em>x</em><sub><em>t</em> + 1</sub>, instead of computing the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em> – 1</sub> again, you reuse these vectors from the previous step. This means that you’ll need to compute the key and value vectors for only the most recent token, <em>x</em><sub><em>t</em></sub>. The cache that stores key and value vectors for reuse is called the KV cache. <a contenteditable="false" data-primary="key-value (KV) cache" data-type="indexterm" id="ch09.html51a"/>The newly computed key and value vectors are then added to the KV cache, which is visualized in <a data-type="xref" href="#ch09_figure_12_1730130962952844">Figure 9-12</a>.</p>&#13;
            <figure><div class="figure" id="ch09_figure_12_1730130962952844">&#13;
              <img alt="A diagram of a graph&#10;&#10;Description automatically generated" src="assets/aien_0912.png"/>&#13;
              <h6><span class="label">Figure 9-12. </span>To avoid recomputing the key and value vectors at each decoding step, use a KV cache to store these vectors to reuse.</h6>&#13;
            </div></figure>&#13;
            <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
      <p>A KV cache is used only during inference, not training. During training, because all tokens in a sequence are known in advance, next token generation can be computed all at once instead of sequentially, as during inference. Therefore, there’s no need for a KV cache.</p>&#13;
            </div>&#13;
    <p>Because generating a token requires computing the attention scores with all previous tokens, the number of attention computations grows exponentially with sequence length.<sup><a data-type="noteref" href="ch09.html#id1700" id="id1700-marker">22</a></sup> The KV cache size, on the other hand, grows linearly with sequence length.</p>&#13;
    <p>The KV cache size also grows with larger batch sizes. A Google paper calculated that for a 500B+ model with multi-head attention, batch size 512, and context length 2048, the KV cache totals 3TB <a href="https://arxiv.org/abs/2211.05102">(Pope et al., 2022)</a>. This is three times the size of that model’s weights.</p>&#13;
    <p>The KV cache size is ultimately limited by the available hardware storage, creating a bottleneck for running applications with long context. A large cache size also takes time to load into memory, which can be an issue for applications with strict latency. </p>&#13;
&#13;
    <p>The computation and memory requirements of the attention mechanism are one of the reasons why it’s so hard to have longer context.</p>&#13;
    <p>Many techniques have been developed to make the attention mechanism more efficient. In general, they fall into three buckets: redesigning the attention mechanism, optimizing the KV cache, and writing kernels for attention computation.</p>&#13;
&#13;
&#13;
            <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch09_calculating_the_kv_cache_size_1730130963008252">&#13;
              <h1>Calculating the KV Cache Size</h1>&#13;
      <p><a contenteditable="false" data-primary="inference optimization" data-secondary="KV cache size calculation" data-type="indexterm" id="id1701"/>The memory needed for the KV cache, without any optimization, is calculated as <span class="keep-together">follows:</span></p>&#13;
      <ul class="simplelist">&#13;
      <li><p>2 × <em>B</em> × <em>S</em> × <em>L</em> × <em>H</em> × <em>M</em></p></li>&#13;
              </ul>&#13;
              <ul>&#13;
                <li>&#13;
          <p><em>B</em>: batch size</p>&#13;
                </li>&#13;
                <li>&#13;
          <p><em>S</em>: sequence length</p>&#13;
                </li>&#13;
                <li>&#13;
          <p><em>L</em>: number of transformer layers</p>&#13;
                </li>&#13;
                <li>&#13;
          <p><em>H</em>: model dimension</p>&#13;
                </li>&#13;
                <li>&#13;
          <p><em>M</em>: memory needed for the cache’s numerical representation (e.g., FP16 or FP32).</p>&#13;
                </li>&#13;
              </ul>&#13;
      <p>This value can become substantial as the context length increases. For example, LLama 2 13B has 40 layers and a model dimension of 5,120. With a batch size of 32, sequence length of 2,048, and 2 bytes per value, the memory needed for its KV cache, without any optimization, is 2 × 32 × 2,048 × 40 × 5,120 × 2 = 54 GB.</p>&#13;
 &#13;
            </div></aside>&#13;
    &#13;
            <section data-pdf-bookmark="Redesigning the attention mechanism" data-type="sect4"><div class="sect4" id="ch09_redesigning_the_attention_mechanism_1730130963008309">&#13;
              <h4>Redesigning the attention mechanism</h4>&#13;
      <p><a contenteditable="false" data-primary="attention mechanisms" data-secondary="optimization" data-tertiary="attention mechanism redesign" data-type="indexterm" id="id1702"/><a contenteditable="false" data-primary="attention mechanisms" data-secondary="redesign" data-type="indexterm" id="id1703"/><a contenteditable="false" data-primary="model optimization" data-secondary="attention mechanism optimization" data-tertiary="attention mechanism redesign" data-type="indexterm" id="id1704"/>These techniques involve altering how the attention mechanism works. Even though these techniques help optimize inference, because they change a model’s architecture directly, they can be applied only during training or finetuning. </p>&#13;
      <p>For example, when generating a new token, instead of attending to all previous tokens, <em>local windowed attention</em> attends only to a fixed size window of nearby tokens (<a href="https://arxiv.org/abs/2004.05150v2">Beltagy et al., 2020</a>). This reduces the effective sequence length to a fixed size window, reducing both the KV cache and the attention computation. If the average sequence length is 10,000 tokens, attending to a window size of 1,000 tokens reduces the KV cache size by 10 times.</p>&#13;
      <p>Local windowed attention can be interleaved with global attention, with local attention capturing nearby context; the global attention captures task-specific information across the document.</p>&#13;
      <p><a contenteditable="false" data-primary="cross-layer attention" data-type="indexterm" id="id1705"/><a contenteditable="false" data-primary="multi-query attention" data-type="indexterm" id="id1706"/>Both <em>cross-layer attention</em> (<a href="https://arxiv.org/abs/2405.12981?ref=research.character.ai">Brandon et al., 2024</a>) and <em>multi-query attention</em> (<a href="https://arxiv.org/abs/1911.02150?ref=research.character.ai">Shazeer, 2019</a>) reduce the memory footprint of the KV cache by reducing the number of key-value pairs. Cross-layer attention shares key and value vectors across adjacent layers. Having three layers sharing the same key-value vectors means reducing the KV cache three times. On the other hand, multi-query attention shares key-value vectors across query heads.</p>&#13;
      <p class="pagebreak-before"><a contenteditable="false" data-primary="grouped-query attention" data-type="indexterm" id="id1707"/><em>Grouped-query attention</em> (<a href="https://arxiv.org/abs/2305.13245">Ainslie et al., 2023</a>) is a generalization of multi-query attention. Instead of using only one set of key-value pairs for all query heads, its grouped-query attention puts query heads into smaller groups and shares key-value pairs only among query heads in the same group. This allows for a more flexible balance between the number of query heads and the number of key-value pairs.</p>&#13;
      <p>Character.AI, an AI chatbot application, shares that their average conversation has a dialogue history of <a href="https://oreil.ly/nLt6A">180 messages</a> (2024). Given the typically long sequences, the primary bottleneck for inference throughput is the KV cache size. Three attention mechanism designs—multi-query attention, interleaving local attention and global attention, and cross-layer attention—help them <em>reduce KV cache by over 20 times</em>. More importantly, this significant KV cache reduction means that memory is no longer a bottleneck for them for serving large batch sizes.</p>&#13;
            </div></section>&#13;
            <section data-pdf-bookmark="Optimizing the KV cache size" data-type="sect4"><div class="sect4" id="ch09_reduce_the_kv_cache_size_1730130963008365">&#13;
              <h4>Optimizing the KV cache size</h4>&#13;
      <p><a contenteditable="false" data-primary="model optimization" data-secondary="attention mechanism optimization" data-tertiary="KV cache size optimization" data-type="indexterm" id="id1708"/>The way the KV cache is managed is critical in mitigating the memory bottleneck during inference and enabling a larger batch size, especially for applications with long context. Many techniques are actively being developed to reduce and manage the KV cache. </p>&#13;
      <p>One of the fastest growing inference frameworks, <a href="https://github.com/vllm-project/vllm">vLLM</a>, gained popularity for introducing PagedAttention, which optimizes memory management by dividing the KV cache into non-contiguous blocks, reducing fragmentation, and enabling flexible memory sharing to improve LLM serving efficiency (<a href="https://arxiv.org/abs/2309.06180">Kwon et al., 2023</a>).</p>&#13;
      <p>Other techniques include KV cache quantization (<a href="https://arxiv.org/abs/2401.18079">Hooper et al., 2024</a>; <a href="https://arxiv.org/abs/2403.05527">Kang et al., 2024</a>), adaptive KV cache compression (<a href="https://arxiv.org/abs/2310.01801">Ge et al., 2023</a>), and selective KV cache (<a href="https://oreil.ly/ixtBl">Liu et al., 2024</a>).<a contenteditable="false" data-primary="" data-startref="ch09.html51a" data-type="indexterm" id="id1709"/></p>&#13;
            </div></section>&#13;
            <section data-pdf-bookmark="Writing kernels for attention computation" data-type="sect4"><div class="sect4" id="ch09_writing_kernels_for_attention_computation_1730130963008422">&#13;
              <h4>Writing kernels for attention computation</h4>&#13;
      <p><a contenteditable="false" data-primary="attention mechanisms" data-secondary="optimization" data-tertiary="wiring kernels for attention computation" data-type="indexterm" id="id1710"/><a contenteditable="false" data-primary="kernels" data-type="indexterm" id="id1711"/><a contenteditable="false" data-primary="model optimization" data-secondary="attention mechanism optimization" data-tertiary="write kernels for attention computation" data-type="indexterm" id="id1712"/>Instead of changing the mechanism design or optimizing the storage, this approach looks into how attention scores are computed and finds ways to make this computation more efficient. This approach is the most effective when it takes into account the hardware executing the computation. The code optimized for a specific chip is called a kernel. Kernel writing will be discussed further in the next section.</p>&#13;
      <p>One of the most well-known kernels optimized for attention computation is <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> (Dao et al., 2022). This kernel fused together many operations commonly used in a transformer-based model to make them run faster, as shown in <a contenteditable="false" data-primary="" data-startref="ch09.html51" data-type="indexterm" id="id1713"/><a contenteditable="false" data-primary="" data-startref="ch09.html50" data-type="indexterm" id="id1714"/><a contenteditable="false" data-primary="" data-startref="ch09.html49" data-type="indexterm" id="id1715"/><a data-type="xref" href="#ch09_figure_13_1730130962952862">Figure 9-13</a>. </p>&#13;
              <figure><div class="figure" id="ch09_figure_13_1730130962952862">&#13;
                <img alt="A graph of a graph with text&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0913.png"/>&#13;
                <h6><span class="label">Figure 9-13. </span>FlashAttention is a kernel that fuses together several common operators. Adapted from an original image licensed under BSD 3-Clause.</h6>&#13;
              </div></figure>&#13;
            </div></section>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Kernels and compilers" data-type="sect3"><div class="sect3" id="ch09_kernels_and_compilers_1730130963008481">&#13;
            <h3>Kernels and compilers</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="model optimization" data-tertiary="kernels and compilers" data-type="indexterm" id="ch09.html52"/><a contenteditable="false" data-primary="kernels" data-type="indexterm" id="ch09.html53"/><a contenteditable="false" data-primary="model optimization" data-secondary="kernels and compilers" data-type="indexterm" id="ch09.html54"/>Kernels are specialized pieces of code optimized for specific hardware accelerators, such as GPUs or TPUs. They are typically written to perform computationally intensive routines that need to be executed repeatedly, often in parallel, to maximize the performance of these accelerators.</p>&#13;
    <p>Common AI operations, including matrix multiplication, attention computation, and convolution operation, all have specialized kernels to make their computation more efficient on different hardware.<sup><a data-type="noteref" href="ch09.html#id1716" id="id1716-marker">23</a></sup></p>&#13;
    <p>Writing kernels requires a deep understanding of the underlying hardware architecture. This includes knowledge about how the memory hierarchy is structured (such as caches, global memory, shared memory, and registers) and how data is accessed and moved between these different levels.</p>&#13;
    <p>Moreover, kernels are typically written in lower-level programming languages like CUDA (for NVIDIA GPUs), Triton (a language developed by OpenAI for writing custom kernels), and ROCm (for AMD GPUs). These languages allow fine-grained control over thread management and memory access but are also harder to learn than the languages that most AI engineers are familiar with, like Python.</p>&#13;
    <p>Due to this entry barrier, writing kernels used to be a dark art practiced by a few. Chip makers like NVIDIA and AMD employ optimization engineers to write kernels to make their hardware efficient for AI workloads, whereas AI frameworks like PyTorch and TensorFlow employ kernel engineers to optimize their frameworks on different accelerators.</p>&#13;
    <p>However, with the rising demand for inference optimization and the ubiquity of accelerators, more AI engineers have taken an interest in writing kernels. There are many great online tutorials for kernel writing. Here, I’ll cover four common techniques often used to speed up computation:</p>&#13;
            <dl>&#13;
              <dt>Vectorization</dt>&#13;
<dd><p><a contenteditable="false" data-primary="vectorization" data-type="indexterm" id="id1717"/>Given a loop or a nested loop, instead of processing one data element at a time, simultaneously execute multiple data elements that are contiguous in memory. This reduces latency by minimizing data I/O operations.</p></dd>&#13;
              <dt>Parallelization</dt>&#13;
<dd><p><a contenteditable="false" data-primary="parallelization" data-type="indexterm" id="id1718"/>Divide an input array (or n-dimensional array) into independent chunks that can be processed simultaneously on different cores or threads, speeding up the computation.</p></dd>&#13;
              <dt>Loop tiling</dt>&#13;
<dd><p><a contenteditable="false" data-primary="loop tiling" data-type="indexterm" id="id1719"/>Optimize the data accessing order in a loop for the hardware’s memory layout and cache. This optimization is hardware-dependent. An efficient CPU tiling pattern may not work well on GPUs.</p></dd>&#13;
              <dt>Operator fusion</dt>&#13;
<dd><p><a contenteditable="false" data-primary="operator fusion" data-type="indexterm" id="id1720"/>Combine multiple operators into a single pass to avoid redundant memory access. For example, if two loops operate over the same array, they can be fused into one, reducing the number of times data is read and written.</p></dd>&#13;
<dd><p>While vectorization, parallelization, and loop tiling can be applied broadly across different models, operator fusion requires a deeper understanding of a model’s specific operators and architecture. As a result, operator fusion demands more attention from optimization engineers.</p></dd>&#13;
            </dl>&#13;
    <p>Kernels are optimized for a hardware architecture. This means that whenever a new hardware architecture is introduced, new kernels need to be developed. For example, <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> (Dao et al., 2022) was originally developed primarily for NVIDIA A100 GPUs. Later on, FlashAttention-3 was introduced for H100 GPUs (<a href="https://arxiv.org/abs/2407.08608">Shah et al., 2024</a>).</p>&#13;
    <p>A model script specifies a series of operations that need to be performed to execute that model. To run this code on a piece of hardware, such as a GPU, it has to be converted into a language compatible with that hardware. This process is called <em>lowering</em>. <a contenteditable="false" data-primary="compilers" data-type="indexterm" id="id1721"/>A tool that <em>lowers</em> code to run a specific hardware is called a compiler. Compilers bridge ML models and the hardware they run on. During the lowering process, whenever possible, these operations are converted into specialized kernels to run faster on the target hardware.</p>&#13;
    &#13;
            <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ch09_inference_optimization_case_study_from_pytorch_1730130963008570">&#13;
              <h1>Inference Optimization Case Study from PyTorch</h1>&#13;
      <p><a contenteditable="false" data-primary="Llama" data-secondary="inference optimization" data-type="indexterm" id="id1722"/><a contenteditable="false" data-primary="inference optimization" data-secondary="case study from PyTorch" data-type="indexterm" id="id1723"/><a data-type="xref" href="#ch09_figure_14_1730130962952879">Figure 9-14</a> shows how much throughput improvement the PyTorch team could give to Llama-7B through the following optimization steps (<a href="https://oreil.ly/_5Nqa">PyTorch, 2023</a>):</p>&#13;
              <ol>&#13;
                <li>&#13;
          <p>Call torch.compile to compile the model into more efficient kernels.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>Quantize the model weights to INT8.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>Further quantize the model weights to INT4.</p>&#13;
                </li>&#13;
                <li>&#13;
          <p>Add speculative decoding.</p>&#13;
                </li>&#13;
              </ol>&#13;
            &#13;
            <figure><div class="figure" id="ch09_figure_14_1730130962952879">&#13;
              <img alt="A graph with numbers and a bar&#10;&#10;Description automatically generated" src="assets/aien_0914.png"/>&#13;
              <h6><span class="label">Figure 9-14. </span>Throughput improvement by different optimization techniques in PyTorch. Image from PyTorch (2023).</h6>&#13;
            </div></figure>&#13;
           &#13;
      <p>The experiment was run on an A100 GPU with 80 GB of memory. It was unclear how these optimization steps impact the model’s output quality.<a contenteditable="false" data-primary="" data-startref="ch09.html41" data-type="indexterm" id="id1724"/><a contenteditable="false" data-primary="" data-startref="ch09.html40" data-type="indexterm" id="id1725"/></p>&#13;
            </div></aside>&#13;
&#13;
            <p class="pagebreak-before">Compilers can be standalone tools, such as <a href="https://github.com/apache/tvm">Apache TVM</a> and <a href="https://mlir.llvm.org">MLIR</a> (Multi-Level Intermediate Representation) or integrated into ML and inference frameworks, like <a href="https://oreil.ly/6bjVM"><code>torch.compile</code></a> (a feature in PyTorch), <a href="https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra">XLA</a> (Accelerated Linear Algebra, originally developed by TensorFlow, with an open source version called <a href="https://github.com/openxla/xla">OpenXLA</a>), and the compiler built into the <a href="https://github.com/NVIDIA/TensorRT">TensorRT</a>, which is optimized for NVIDIA GPUs. AI companies might have their own compilers, with their proprietary kernels designed to speed up their own workloads<a contenteditable="false" data-primary="" data-startref="ch09.html54" data-type="indexterm" id="id1726"/><a contenteditable="false" data-primary="" data-startref="ch09.html53" data-type="indexterm" id="id1727"/><a contenteditable="false" data-primary="" data-startref="ch09.html52" data-type="indexterm" id="id1728"/>.<sup><a data-type="noteref" href="ch09.html#id1729" id="id1729-marker">24</a></sup></p>&#13;
&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Inference Service Optimization" data-type="sect2"><div class="sect2" id="ch09_inference_service_optimization_1730130963008735">&#13;
          <h2>Inference Service Optimization</h2>&#13;
  <p><a contenteditable="false" data-primary="inference optimization" data-secondary="inference service optimization" data-type="indexterm" id="ch09.html55"/><a contenteditable="false" data-primary="inference service optimization" data-type="indexterm" id="ch09.html56"/>Most service-level optimization techniques focus on resource management. Given a fixed amount of resources (compute and memory) and dynamic workloads (inference requests from users that may involve different models), the goal is to efficiently allocate resources to these workloads to optimize for latency and cost. Unlike many model-level techniques, service-level techniques don’t modify models and shouldn’t change the output quality.</p>&#13;
          <section data-pdf-bookmark="Batching" data-type="sect3"><div class="sect3" id="ch09_batching_1730130963008799">&#13;
            <h3>Batching</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="inference service optimization" data-tertiary="batching" data-type="indexterm" id="id1730"/>One of the easiest ways to reduce your cost is batching. In production, your inference service might receive multiple requests simultaneously. Instead of processing each request separately, batching the requests that arrive around the same time together can significantly reduce the service’s throughput. If processing each request separately is like everyone driving their own car, batching is like putting them together on a bus. A bus can move more people, but it can also make each person’s journey longer. However, if you do it intelligently, the impact on latency can be minimal.</p>&#13;
    <p>The three main techniques for batching are: static batching, dynamic batching, and continuous batching.</p>&#13;
    <p><a contenteditable="false" data-primary="batching" data-secondary="static" data-type="indexterm" id="id1731"/><a contenteditable="false" data-primary="static batching" data-type="indexterm" id="id1732"/>The simplest batching technique is <em>static batching</em>. The service groups a fixed number of inputs together in a batch. It’s like a bus that waits until every seat is filled before departing. The drawback of static batching is that all requests have to wait until the batch is full to be executed. Thus the first request in a batch is delayed until the batch’s last request arrives, no matter how late the last request is.</p>&#13;
    <p class="pagebreak-before"><a contenteditable="false" data-primary="batching" data-secondary="dynamic" data-type="indexterm" id="id1733"/><a contenteditable="false" data-primary="dynamic batching" data-type="indexterm" id="id1734"/><em>Dynamic batching</em>, on the other hand, sets a maximum time window for each batch. If the batch size is four and the window is 100 ms, the server processes the batch either when it has four requests or when 100 ms has passed, whichever happens first. It’s like a bus that leaves on a fixed schedule or when it’s full. This approach keeps latency under control, so earlier requests aren’t held up by later ones. The downside is that batches may not always be full when processed, possibly leading to wasted compute. Static batching and dynamic batching are visualized in <a data-type="xref" href="#ch09_figure_15_1730130962952896">Figure 9-15</a>.</p>&#13;
            <figure class="width-95"><div class="figure" id="ch09_figure_15_1730130962952896">&#13;
              <img alt="A screenshot of a computer&#10;&#10;Description automatically generated" src="assets/aien_0915.png"/>&#13;
              <h6><span class="label">Figure 9-15. </span>Dynamic batching keeps the latency manageable but might be less compute-efficient.</h6>&#13;
            </div></figure>&#13;
    <p>In naive batching implementations, all batch requests have to be completed before their responses are returned. For LLMs, some requests might take much longer than others. If one request in a batch generates only 10 response tokens and another request generates 1,000 response tokens, the short response has to wait until the long response is completed before being returned to the user. This results in unnecessary latency for short requests.</p>&#13;
    <p><a contenteditable="false" data-primary="continuous batching" data-type="indexterm" id="id1735"/><a contenteditable="false" data-primary="batching" data-secondary="continuous" data-type="indexterm" id="id1736"/><em>Continuous batching</em> allows responses in a batch to be returned to users as soon as they are completed. It works by selectively batching operations that don’t cause the generation of one response to hold up another, as introduced in the paper Orca (<a href="https://oreil.ly/SJ7Mb">Yu et al., 2022</a>). After a request in a batch is completed and its response returned, the service can add another request into the batch in its place, making the batching continuous. It’s like a bus that, after dropping off one passenger, can immediately pick up another passenger to maximize its occupancy rate. Continuous batching, also called <a href="https://oreil.ly/DlIPs"><em>in-flight batching</em></a>, is visualized in <a data-type="xref" href="#ch09_figure_16_1730130962952915">Figure 9-16</a>.</p>&#13;
            <figure><div class="figure" id="ch09_figure_16_1730130962952915">&#13;
              <img alt="A screenshot of a diagram&#10;&#10;Description automatically generated" src="assets/aien_0916.png"/>&#13;
              <h6><span class="label">Figure 9-16. </span>With continuous batching, completed responses can be returned immediately to users, and new requests can be processed in their place.</h6>&#13;
            </div></figure>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Decoupling prefill and decode" data-type="sect3"><div class="sect3" id="ch09_decoupling_prefill_and_decode_1730130963008857">&#13;
            <h3>Decoupling prefill and decode</h3>&#13;
    <p><a contenteditable="false" data-primary="decoding" data-secondary="decoupling from prefilling" data-type="indexterm" id="id1737"/><a contenteditable="false" data-primary="inference optimization" data-secondary="inference service optimization" data-tertiary="decoupling prefill and decode" data-type="indexterm" id="id1738"/><a contenteditable="false" data-primary="inference service optimization" data-secondary="decoupling prefill and decode" data-type="indexterm" id="id1739"/><a contenteditable="false" data-primary="prefilling, decoupling from decoding" data-type="indexterm" id="id1740"/>LLM inference consists of two steps: prefill and decode. Because prefill is compute-bound and decode is memory bandwidth-bound, using the same machine to perform both can cause them to inefficiently compete for resources and significantly slow down both TTFT and TPOT. Imagine a GPU that is already handling prefilling and decoding near its peak computational capacity. It might be able to handle another low computational job like decoding. However, adding a new query to this GPU means introducing a prefilling job along with a decoding job. This one prefilling job can drain computational resources from existing decoding jobs, slowing down TPOT for these requests.</p>&#13;
    <p>One common optimization technique for inference servers is to disaggregate prefill and decode. “DistServe” (<a href="https://arxiv.org/html/2401.09670v1">Zhong et al., 2024</a>) and “Inference Without Interference” (<a href="https://arxiv.org/abs/2401.11181">Hu et al., 2024</a>) show that for various popular LLMs and applications, assigning prefill and decode operations to different instances (e.g., different GPUs) can significantly improve the volume of processed requests while adhering to latency requirements. Even though decoupling requires transferring intermediate states from prefill instances to decode instances, the paper shows communication overhead is not substantial in modern GPU clusters with high-bandwidth connections such as <a href="https://en.wikipedia.org/wiki/NVLink">NVLink</a> within a node.</p>&#13;
    <p class="pagebreak-before">The ratio of prefill instances to decode instances depends on many factors, such as the workload characteristics (e.g., longer input lengths require more prefill compute) and latency requirements (e.g., whether you want lower TTFT or TPOT). For example, if input sequences are usually long and you want to prioritize TTFT, this ratio can be between 2:1 and 4:1. If input sequences are short and you want to prioritize TPOT, this ratio can be 1:2 to 1:1.<sup><a data-type="noteref" href="ch09.html#id1741" id="id1741-marker">25</a></sup></p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Prompt caching" data-type="sect3"><div class="sect3" id="ch09_prompt_caching_1730130963008914">&#13;
            <h3>Prompt caching</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="inference service optimization" data-tertiary="prompt caching" data-type="indexterm" id="ch09.html57"/><a contenteditable="false" data-primary="inference service optimization" data-secondary="prompt caching" data-type="indexterm" id="ch09.html58"/><a contenteditable="false" data-primary="prompt caching" data-type="indexterm" id="ch09.html59"/>Many prompts in an application have overlapping text segments. A prompt cache stores these overlapping segments for reuse, so you only need to process them once. A common overlapping text segment in different prompts is the system prompt. Without a prompt cache, your model needs to process the system prompt with every query. With a prompt cache, the system prompt needs to be processed just once for the first query.</p>&#13;
    <p>Prompt caching is useful for queries that involve long documents. For example, if many of your user queries are related to the same long document (such as a book or a codebase), this long document can be cached for reuse across queries. It’s also useful for long conversations when the processing of earlier messages can be cached and reused when predicting future messages.</p>&#13;
    <p>A prompt cache is visualized in <a data-type="xref" href="#ch09_figure_17_1730130962952933">Figure 9-17</a>. It’s also called a context cache or prefix cache.</p>&#13;
            <figure><div class="figure" id="ch09_figure_17_1730130962952933">&#13;
              <img alt="A screenshot of a computer&#10;&#10;Description automatically generated" src="assets/aien_0917.png"/>&#13;
              <h6><span class="label">Figure 9-17. </span>With a prompt cache, overlapping segments in different prompts can be cached and reused.</h6>&#13;
            </div></figure>&#13;
    <p class="pagebreak-before">For applications with long system prompts, prompt caching can significantly reduce both latency and cost. If your system prompt is 1,000 tokens, and your application generates one million model API calls daily, a prompt cache will save you from processing approximately one billion repetitive input tokens a day! However, this isn’t entirely free. Like the KV cache, prompt cache size can be quite large and take up memory space. Unless you use a model API with this functionality, implementing prompt caching can require significant engineering effort.</p>&#13;
    <p>Since its introduction in November 2023 by <a href="https://oreil.ly/Pd6Pk">Gim et al.</a>, the prompt cache has been rapidly incorporated into model APIs. As of this writing, <a contenteditable="false" data-primary="Gemini" data-type="indexterm" id="id1742"/>Google Gemini offers this <a href="https://oreil.ly/pIHkL">functionality</a>, with cached input tokens given a 75% discount compared to regular input tokens, but you’ll have to pay extra for cache storage (as of writing, $1.00/one million tokens per hour). <a contenteditable="false" data-primary="Anthropic" data-secondary="prompt caching" data-type="indexterm" id="id1743"/>Anthropic offers <a href="https://oreil.ly/8rtsF">prompt caching</a> that promises up to 90% cost savings (the longer the cached context, the higher the savings) and up to 75% latency reduction. The impact of prompt caching on the cost and latency of different scenarios is shown <a contenteditable="false" data-primary="" data-startref="ch09.html59" data-type="indexterm" id="id1744"/><a contenteditable="false" data-primary="" data-startref="ch09.html58" data-type="indexterm" id="id1745"/><a contenteditable="false" data-primary="" data-startref="ch09.html57" data-type="indexterm" id="id1746"/>in <a data-type="xref" href="#ch09_table_3_1730130962971081">Table 9-3</a>.<sup><a data-type="noteref" href="ch09.html#id1747" id="id1747-marker">26</a></sup></p>&#13;
            <table id="ch09_table_3_1730130962971081">&#13;
              <caption><span class="label">Table 9-3. </span>Cost and latency reduced by prompt caching. Information from Anthropic (2024).</caption>&#13;
              <thead>&#13;
                <tr>&#13;
                  <th>Use case</th>&#13;
                  <th>Latency w/o caching <span class="keep-together">(time to first token)</span></th>&#13;
                  <th>Latency with caching <span class="keep-together">(time to first token)</span></th>&#13;
                  <th>Cost reduction</th>&#13;
                </tr>&#13;
              </thead>&#13;
              <tr>&#13;
                <td>Chat with a book (100,000-token cached prompt)</td>&#13;
                <td>11.5 s</td>&#13;
                <td>2.4 s (–79%)</td>&#13;
                <td>–90%</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>Many-shot prompting (10,000-token prompt)</td>&#13;
                <td>1.6 s</td>&#13;
                <td>1.1 s (–31%)</td>&#13;
                <td>–86%</td>&#13;
              </tr>&#13;
              <tr>&#13;
                <td>Multi-turn conversation (10-turn convo with a long system prompt)</td>&#13;
                <td>~10 s</td>&#13;
                <td>~2.5 s (–75%) </td>&#13;
                <td>–53%</td>&#13;
              </tr>&#13;
            </table>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Parallelism" data-type="sect3"><div class="sect3" id="ch09_parallelism_1730130963008972">&#13;
            <h3>Parallelism</h3>&#13;
    <p><a contenteditable="false" data-primary="inference optimization" data-secondary="inference service optimization" data-tertiary="parallelism" data-type="indexterm" id="ch09.html60"/><a contenteditable="false" data-primary="inference service optimization" data-secondary="parallelism" data-type="indexterm" id="ch09.html61"/><a contenteditable="false" data-primary="parallelism" data-type="indexterm" id="ch09.html62"/>Accelerators are designed for parallel processing, and parallelism strategies are the backbone of high-performance computing. Many new parallelization strategies are being developed. This section covers only a few of them for reference. Two families of parallelization strategies that can be applied across all models are data parallelism and model parallelism. A family of strategies applied specifically for LLMs is context and sequence parallelism. An optimization technique might involve multiple parallelism strategies.</p>&#13;
    <p><a contenteditable="false" data-primary="replica parallelism" data-type="indexterm" id="id1748"/><em>Replica parallelism</em> is the most straightforward strategy to implement. It simply creates multiple replicas of the model you want to serve.<sup><a data-type="noteref" href="ch09.html#id1749" id="id1749-marker">27</a></sup> More replicas allow you to handle more requests at the same time, potentially at the cost of using more chips. Trying to fit models of different sizes onto different chips is a bin-packing problem, which can get complicated with more models, more replicas, and more chips.</p>&#13;
    <p>Let’s say you have a mixture of models of different sizes (e.g., 8B, 13B, 34B, and 70B parameters) and access to GPUs of different memory capabilities (e.g., 24 GB, 40 GB, 48 GB, and 80 GB). For simplicity, assume that all models are in the same precision, 8 bits:</p>&#13;
            <ul>&#13;
<li><p>If you have a fixed number of chips, you need to decide how many replicas to create for each model and what GPUs to use for each replica to maximize your metrics. For example, should you place three 13B models on a 40 GB GPU, or should you reserve this GPU for one 34B model?</p></li>&#13;
<li><p>If you have a fixed number of model replicas, you need to decide what chips to acquire to minimize the cost. This situation, however, rarely occurs.</p></li>&#13;
            </ul>&#13;
    <p>Often, your model is so big that it can’t fit into one machine. <em>Model parallelism</em> refers to the practice of splitting the same model across multiple machines. Fitting models onto chips can become an even more complicated problem with model parallelism.</p>&#13;
    <p>There are several ways to split a model. The most common approach for inference is <em>tensor parallelism</em>, also known as <em>intra-operator parallelism</em>. Inference involves a sequence of operators on multidimensional tensors, such as matrix multiplication. In this approach, tensors involved in an operator are partitioned across multiple devices, effectively breaking up this operator into smaller pieces to be executed in parallel, thus speeding up the computation. For example, when multiplying two matrices, you can split one of the matrices columnwise, as shown in <a data-type="xref" href="#ch09_figure_18_1730130962952949">Figure 9-18</a>.</p>&#13;
&#13;
    <p>Tensor parallelism provides two benefits. First, it makes it possible to serve large models that don’t fit on single machines. Second, it reduces latency. The latency benefit, however, might be reduced due to extra communication overhead.</p>&#13;
            <figure><div class="figure" id="ch09_figure_18_1730130962952949">&#13;
              <img alt="A diagram of a grid with squares and a few squares&#10;&#10;Description automatically generated with medium confidence" src="assets/aien_0918.png"/>&#13;
              <h6><span class="label">Figure 9-18. </span>Tensor parallelism for matrix multiplication.</h6>&#13;
            </div></figure>&#13;
    &#13;
    <p>Another way to split a model is <em>pipeline parallelism</em>, which involves dividing a model’s computation into distinct stages and assigning each stage to a different device. As data flows through the model, each stage processes one part while others process subsequent parts, enabling overlapping computations. <a data-type="xref" href="#ch09_figure_19_1730130962952966">Figure 9-19</a> shows what pipeline parallelism looks like on four machines.</p>&#13;
            <figure><div class="figure" id="ch09_figure_19_1730130962952966">&#13;
              <img alt="A diagram of a layer&#10;&#10;Description automatically generated" src="assets/aien_0919.png"/>&#13;
              <h6><span class="label">Figure 9-19. </span>Pipeline parallelism enables model splits to be executed in parallel. </h6>&#13;
            </div></figure>&#13;
    <p><a data-type="xref" href="#ch09_figure_19_1730130962952966">Figure 9-19</a> shows a batch can be split into smaller micro-batches. After a micro-batch is processed on one machine, its output is passed onto the next part of the model on the next machine.</p>&#13;
    <p>While pipeline parallelism enables serving large models on multiple machines, it increases the total latency for each request due to extra communication between pipeline stages. Therefore, for applications with strict latency requirements, pipeline parallelism is typically avoided in favor of replica parallelism. However, pipeline parallelism is commonly used in training since it can help increase throughput.</p>&#13;
    <p>Two techniques that are less common but might warrant a quick mention to illustrate the diversity of techniques are <em>context parallelism</em> and <em>sequence parallelism</em>. They were both developed to make long input sequence processing more efficient, including context parallelism and sequence parallelism.</p>&#13;
    <p><a contenteditable="false" data-primary="context parallelism" data-type="indexterm" id="id1750"/>In <a href="https://oreil.ly/On2-B"><em>context parallelism</em></a>, the input sequence itself is split across different devices to be processed separately. For example, the first half of the input is processed on machine 1 and the second half on machine 2.</p>&#13;
    <p><a contenteditable="false" data-primary="sequence parallelism" data-type="indexterm" id="id1751"/>In <em>sequence parallelism</em>, operators needed for the entire input are split across machines. <a contenteditable="false" data-primary="feedforward computation" data-type="indexterm" id="id1752"/>For example, if the input requires both attention and feedforward computation, attention might be processed on machine 1 while feedforward is processed on machine <a contenteditable="false" data-primary="" data-startref="ch09.html62" data-type="indexterm" id="id1753"/><a contenteditable="false" data-primary="" data-startref="ch09.html61" data-type="indexterm" id="id1754"/><a contenteditable="false" data-primary="" data-startref="ch09.html60" data-type="indexterm" id="id1755"/>2<a contenteditable="false" data-primary="" data-startref="ch09.html56" data-type="indexterm" id="id1756"/><a contenteditable="false" data-primary="" data-startref="ch09.html55" data-type="indexterm" id="id1757"/>.</p>&#13;
          </div></section>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch09_summary_1730130963009027">&#13;
        <h1>Summary</h1>&#13;
<p>A model’s usability depends heavily on its inference cost and latency. Cheaper inference makes AI-powered decisions more affordable, while faster inference enables the integration of AI into more applications. Given the massive potential impact of inference optimization, it has attracted many talented individuals who continually come up with innovative approaches.</p>&#13;
<p>Before we start making things more efficient, we need to understand how efficiency is measured. This chapter started with common efficiency metrics for latency, throughput, and utilization. For language model-based inference, latency can be broken into time to first token (TTFT), which is influenced by the prefilling phase, and time per output token (TPOT), which is influenced by the decoding phase. Throughput metrics are directly related to cost. There’s a trade-off between latency and throughput. You can potentially reduce cost if you’re okay with increased latency, and reducing latency often involves increasing cost.</p>&#13;
<p>How efficiently a model can run depends on the hardware it is run on. For this reason, this chapter also provided a quick overview of AI hardware and what it takes to optimize models on different accelerators.</p>&#13;
<p>The chapter then continued with different techniques for inference optimization. Given the availability of model APIs, most application developers will use these APIs with their built-in optimization instead of implementing these techniques themselves. While these techniques might not be relevant to all application developers, I believe that understanding what techniques are possible can be helpful for evaluating the efficiency of model APIs.</p>&#13;
<p>This chapter also focused on optimization at the model level and the inference service level. Model-level optimization often requires changing the model itself, which can lead to changes in the model behaviors. Inference service-level optimization, on the other hand, typically keeps the model intact and only changes how it’s served.</p>&#13;
<p>Model-level techniques include model-agnostic techniques like quantization and distillation. Different model architectures require their own optimization. For example, because a key bottleneck of transformer models is in the attention mechanism, many optimization techniques involve making attention more efficient, including KV cache management and writing attention kernels. A big bottleneck for an autoregressive language model is in its autoregressive decoding process, and consequently, many techniques have been developed to address it, too.</p>&#13;
<p>Inference service-level techniques include various batching and parallelism strategies. There are also techniques developed especially for autoregressive language models, including prefilling/decoding decoupling and prompt caching.</p>&#13;
<p>The choice of optimization techniques depends on your workloads. For example, KV caching is significantly more important for workloads with long contexts than those with short contexts. Prompt caching, on the other hand, is crucial for workloads involving long, overlapping prompt segments or multi-turn conversations. The choice also depends on your performance requirements. For instance, if low latency is a higher priority than cost, you might want to scale up replica parallelism. While more replicas require additional machines, each machine handles fewer requests, allowing it to allocate more resources per request and, thus, improve response time.</p>&#13;
<p>However, across various use cases, the most impactful techniques are typically quantization (which generally works well across models), tensor parallelism (which both reduces latency and enables serving larger models), replica parallelism (which is relatively straightforward to implement), and attention mechanism optimization (which can significantly accelerate transformer models).</p>&#13;
<p>Inference optimization concludes the list of model adaptation techniques covered in this book. The next chapter will explore how to integrate these techniques into a cohesive system.<a contenteditable="false" data-primary="" data-startref="ch09.html0" data-type="indexterm" id="id1758"/></p>&#13;
      </div></section>&#13;
    <div data-type="footnotes"><p data-type="footnote" id="id1597"><sup><a href="ch09.html#id1597-marker">1</a></sup> As discussed in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>, inference involves the forward pass while training involves both the forward and backward passes.</p><p data-type="footnote" id="id1598"><sup><a href="ch09.html#id1598-marker">2</a></sup> A friend, Mark Saroufim, pointed me to an interesting relationship between a model’s training cost and inference cost. Imagine you’re a model provider. Let <em>T</em> be the total training cost, <em>p</em> be the cost you’re charging per inference, and <em>N</em> be the number of inference calls you can sell. Developing a model only makes sense if the money you can recover from inference for a model is more than its training cost, i.e., <em>T</em> &lt;= <em>p</em> × <em>N</em>. The more a model is used in production, the more model providers can reduce inference cost. However, this doesn’t apply for third-party API providers who sell inference calls on top of open source models.</p><p data-type="footnote" id="id1605"><sup><a href="ch09.html#id1605-marker">3</a></sup> Anecdotally, I find that people coming from a system background (e.g., optimization engineers and GPU engineers) use <em>memory-bound</em> to refer to <em>bandwidth-bound</em>, and people coming from an AI background (e.g., ML and AI engineers) use to memory-bound to refer to memory capacity-bound.</p><p data-type="footnote" id="id1606"><sup><a href="ch09.html#id1606-marker">4</a></sup> The Roofline paper uses the term memory-bound to refer to memory-bandwidth bound.</p><p data-type="footnote" id="id1607"><sup><a href="ch09.html#id1607-marker">5</a></sup> Prefilling effectively populates the initial KV cache for the transformer model.</p><p data-type="footnote" id="id1612"><sup><a href="ch09.html#id1612-marker">6</a></sup> If you run an inference service, separating your inference APIs into online and batch can help you prioritize latency for requests where latency matters the most. Let’s say that your inference server can serve only a maximum of X requests/second without latency degradation, you have to serve Y requests/second, and Y is larger than X. In an ideal world, users with less-urgent requests can send their requests to the batch API, so that your service can focus on processing the online API requests first.</p><p data-type="footnote" id="id1619"><sup><a href="ch09.html#id1619-marker">7</a></sup> As discussed in <a data-type="xref" href="#ch09_prompt_caching_1730130963008914">“Prompt caching”</a>, it’s common to know in advance the system prompt of an application. It’s just the exact user queries that are hard to predict. </p><p data-type="footnote" id="id1620"><sup><a href="ch09.html#id1620-marker">8</a></sup> In the early days of chatbots, some people complained about chatbots responding too fast, which seemed unnatural. See <a href="https://oreil.ly/jD5Pj">“Lufthansa Delays Chatbot’s Responses to Make It More ‘Human’”</a> (Ry Crozier, iTnews, May 2017). However, as people become more familiar with chatbots, this is no longer the case.</p><p data-type="footnote" id="id1623"><sup><a href="ch09.html#id1623-marker">9</a></sup> Time between tokens (TBT) is used by <a href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US">LinkedIn</a> and inter-token latency (ITL) is used by <a href="https://oreil.ly/zHsb8">NVIDIA</a>.</p><p data-type="footnote" id="id1624"><sup><a href="ch09.html#id1624-marker">10</a></sup> An experiment by Anyscale shows that 100 input tokens have approximately the same impact on the overall latency as a single output token.</p><p data-type="footnote" id="id1637"><sup><a href="ch09.html#id1637-marker">11</a></sup> People have cared about FLOP/s utilization for a long time, but the term MFU was introduced in the PaLM paper (<a href="https://arxiv.org/abs/2204.02311">Chowdhery et al., 2022</a>).</p><p data-type="footnote" id="id1638"><sup><a href="ch09.html#id1638-marker">12</a></sup> Chip makers might also be doing what I call <em>peak FLOP/s hacking</em>. This might run experiments in certain conditions, such as using sparse matrices with specific shapes, to increase their peak FLOP/s. Higher peak FLOP/s numbers make their chips more attractive, but it can be harder for users to achieve high MFU.</p><p data-type="footnote" id="id1649"><sup><a href="ch09.html#id1649-marker">13</a></sup> In the 1960s, computers could run only one-layer neural networks, which had very limited capabilities. In their famous 1969 book <a href="https://en.wikipedia.org/wiki/Perceptrons_(book)"><em>Perceptrons: An Introduction to Computational Geometry</em></a> (MIT Press), two AI pioneers, Marvin Minsky and Seymour Papert, argued that neural networks with hidden layers would still be able to do little. Their exact quote was: “Virtually nothing is known about the computational capabilities of this latter kind of machine. We believe that it can do little more than can a low order perceptron<em>.</em>” There wasn’t sufficient compute power to dispute their argument, which was then cited by many people as a key reason for the drying up of AI funding in the 1970s.</p><p data-type="footnote" id="id1650"><sup><a href="ch09.html#id1650-marker">14</a></sup> There have been discussions on whether to <a href="https://oreil.ly/mRNCP">rename the GPU</a> since it’s used for a lot more than graphics (Jon Peddie, “Chasing Pixels,” July 2018). Jensen Huang, NVIDIA’s CEO, said in an <a href="https://oreil.ly/iK0tN">interview</a> (<em>Stratechery</em>, March 2022) that once the GPU took off and they added more capabilities to it, they considered renaming it to something more general like GPGPU (general-purpose GPU) or XGU. They decided against renaming because they assumed that people who buy GPUs will be smart enough to know what a GPU is good for beyond its name.</p><p data-type="footnote" id="id1651"><sup><a href="ch09.html#id1651-marker">15</a></sup> Matrix multiplication, affectionately known as matmul, is estimated to account for more than 90% of all floating point operations in a neural network, according to <a href="https://arxiv.org/abs/2007.00072">“Data Movement Is All You Need: A Case Study on Optimizing Transformers”</a> (Ivanov et al., <em>arXiv</em>, v3, November 2021) and <a href="https://arxiv.org/abs/1802.04799">“Scalable MatMul-free Language Modeling”</a> (Zhu et al., <em>arXiv</em>, June 2024).</p><p data-type="footnote" id="id1652"><sup><a href="ch09.html#id1652-marker">16</a></sup> While a chip can be developed to run one model architecture, a model architecture can be developed to make the most out of a chip, too. For example, the transformer was originally designed by Google to <a href="https://oreil.ly/y45q6">run fast on TPUs</a> and only later optimized on GPUs.</p><p data-type="footnote" id="id1661"><sup><a href="ch09.html#id1661-marker">17</a></sup> Lower-end to mid-range GPUs might use <a href="https://en.wikipedia.org/wiki/GDDR_SDRAM">GDDR</a> (Graphics Double Data Rate) memory.</p><p data-type="footnote" id="id1669"><sup><a href="ch09.html#id1669-marker">18</a></sup> A main challenge in building data centers with tens of thousands of GPUs is finding a location that can guarantee the necessary electricity. Building large-scale data centers requires navigating electricity supply, speed, and geopolitical constraints. For example, remote regions might provide cheaper electricity but can increase network latency, making the data centers less appealing for use cases with stringent latency requirements like inference.</p><p data-type="footnote" id="id1683"><sup><a href="ch09.html#id1683-marker">19</a></sup> Each token generation step necessitates the transfer of the entire model’s parameters from the accelerator’s high-bandwidth memory to its compute units. This makes this operation bandwidth-heavy. Because the model can produce only one token at a time, the process consumes only a small number of FLOP/s, resulting in computational inefficiency.</p><p data-type="footnote" id="id1684"><sup><a href="ch09.html#id1684-marker">20</a></sup> This also means that if your MFU is already maxed out, speculative decoding makes less sense.</p><p data-type="footnote" id="id1694"><sup><a href="ch09.html#id1694-marker">21</a></sup> The Jacobi method is an iterative algorithm where multiple parts of a solution can be updated simultaneously and independently.</p><p data-type="footnote" id="id1700"><sup><a href="ch09.html#id1700-marker">22</a></sup> The number of attention computations for an autoregressive model is <em>O</em>(<em>n</em><sup>2</sup>).</p><p data-type="footnote" id="id1716"><sup><a href="ch09.html#id1716-marker">23</a></sup> Convolution operations are often used in image generation models like Stable Diffusion.</p><p data-type="footnote" id="id1729"><sup><a href="ch09.html#id1729-marker">24</a></sup> Many companies consider their kernels their trade secrets. Having kernels that allow them to run models faster and cheaper than their competitors is a competitive advantage.</p><p data-type="footnote" id="id1741"><sup><a href="ch09.html#id1741-marker">25</a></sup> Talks mentioning the prefill to decode instance ratio include <a href="https://oreil.ly/eMQ_P">“Llama Inference at Meta”</a> (Meta, 2024).</p><p data-type="footnote" id="id1747"><sup><a href="ch09.html#id1747-marker">26</a></sup> While llama.cpp also has <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#prompt-caching">prompt caching</a>, it seems to cache only whole prompts and work for queries in the same chat session, as of this writing. Its documentation is limited, but my guess from reading the code is that in a long conversation, it caches the previous messages and processes only the newest message.</p><p data-type="footnote" id="id1749"><sup><a href="ch09.html#id1749-marker">27</a></sup> During training, the same technique is called data parallelism.</p></div></div></section></body></html>