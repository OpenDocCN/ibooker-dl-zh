["```py\na1 = torch.randn(3,3)\nw1 = torch.randn(3,3)\n\na2 = torch.randn(3,3)\nw2 = torch.randn(3,3)\n\nw3 = torch.randn(3,3)\n\n# operations\nwm1 = WeightMultiply(w1)\nwm2 = WeightMultiply(w2)\nadd2 = Add(2, 1)\nmult3 = Multiply(2, 1)\n\nb1 = wm1.forward(a1)\nb2 = wm2.forward(a2)\nc1 = add2.forward((b1, b2))\nL = mult3.forward((c1, b2))\n```", "```py\nc1_grad, b2_grad_1 = mult3.backward(L_grad)\n\nb1_grad, b2_grad_2 = add2.backward(c1_grad)\n\n# combine these gradients to reflect the fact that b2 is used twice on the\n# forward pass\nb2_grad = b2_grad_1 + b2_grad_2\n\na2_grad = wm2.backward(b2_grad)\n\na1_grad = wm1.backward(b1_grad)\n```", "```py\na = array([3,3])\nprint(\"Addition using '__add__':\", a.__add__(4))\nprint(\"Addition using '+':\", a + 4)\n```", "```py\nAddition using '__add__': [7 7]\nAddition using '+': [7 7]\n```", "```py\nNumberable = Union[float, int]\n\ndef ensure_number(num: Numberable) -> NumberWithGrad:\n    if isinstance(num, NumberWithGrad):\n        return num\n    else:\n        return NumberWithGrad(num)\n\nclass NumberWithGrad(object):\n\n    def __init__(self,\n                 num: Numberable,\n                 depends_on: List[Numberable] = None,\n                 creation_op: str = ''):\n        self.num = num\n        self.grad = None\n        self.depends_on = depends_on or []\n        self.creation_op = creation_op\n\n    def __add__(self,\n                other: Numberable) -> NumberWithGrad:\n        return NumberWithGrad(self.num + ensure_number(other).num,\n                              depends_on = [self, ensure_number(other)],\n                              creation_op = 'add')\n\n    def __mul__(self,\n                other: Numberable = None) -> NumberWithGrad:\n\n        return NumberWithGrad(self.num * ensure_number(other).num,\n                              depends_on = [self, ensure_number(other)],\n                              creation_op = 'mul')\n\n    def backward(self, backward_grad: Numberable = None) -> None:\n        if backward_grad is None: # first time calling backward\n            self.grad = 1\n        else:\n            # These lines allow gradients to accumulate.\n            # If the gradient doesn't exist yet, simply set it equal\n            # to backward_grad\n            if self.grad is None:\n                self.grad = backward_grad\n            # Otherwise, simply add backward_grad to the existing gradient\n            else:\n                self.grad += backward_grad\n\n        if self.creation_op == \"add\":\n            # Simply send backward self.grad, since increasing either of these\n            # elements will increase the output by that same amount\n            self.depends_on[0].backward(self.grad)\n            self.depends_on[1].backward(self.grad)\n\n        if self.creation_op == \"mul\":\n\n            # Calculate the derivative with respect to the first element\n            new = self.depends_on[1] * self.grad\n            # Send backward the derivative with respect to that element\n            self.depends_on[0].backward(new.num)\n\n            # Calculate the derivative with respect to the second element\n            new = self.depends_on[0] * self.grad\n            # Send backward the derivative with respect to that element\n            self.depends_on[1].backward(new.num)\n```", "```py\na = NumberWithGrad(3)\n\nb = a * 4\nc = b + 5\n```", "```py\nc.backward()\n```", "```py\nprint(a.grad)\n```", "```py\n4\n```", "```py\nprint(b.grad)\n```", "```py\n1\n```", "```py\na = NumberWithGrad(3)\n\nb = a * 4\nc = b + 3\nd = c * (a + 2)\n```", "```py\ndef forward(num: int):\n    b = num * 4\n    c = b + 3\n    return c * (num + 2)\n\nprint(round(forward(3.01) - forward(2.99)) / 0.02), 3)\n```", "```py\n35.0\n```", "```py\na = NumberWithGrad(3)\n\nb = a * 4\nc = b + 3\nd = (a + 2)\ne = c * d\ne.backward()\n\nprint(a.grad)\n```", "```py\n35\n```", "```py\n    if self.grad is None:\n        self.grad = backward_grad\n    else:\n        self.grad += backward_grad\n    ```", "```py\nweight_grad += grad_from_time_step\n```", "```py\n    def forward(self, x_batch: ndarray) -> ndarray:\n\n        assert_dim(ndarray, 3)\n\n        x_out = x_batch\n        for layer in self.layers:\n            x_out = layer.forward(x_out)\n\n        return x_out\n    ```", "```py\nsequence_length = x_seq_in.shape[1]\n\nx_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n\nfor t in range(sequence_length):\n\n    x_in = x_seq_in[:, t, :]\n\n    y_out, H_in = self.nodes[t].forward(x_in, H_in, self.params)\n\n    x_seq_out[:, t, :] = y_out\n```", "```py\nbatch_size = x_seq_in.shape[0]\n\nH_in = np.copy(self.start_H)\n\nH_in = np.repeat(H_in, batch_size, axis=0)\n```", "```py\nself.start_H = H_in.mean(axis=0, keepdims=True)\n```", "```py\nh_in_grad = np.zeros((batch_size, self.hidden_size))\n\nsequence_length = x_seq_out_grad.shape[1]\n\nx_seq_in_grad = np.zeros((batch_size, sequence_length, self.feature_size))\n\nfor t in reversed(range(sequence_length)):\n\n    x_out_grad = x_seq_out_grad[:, t, :]\n\n    grad_out, h_in_grad = \\\n        self.nodes[t].backward(x_out_grad, h_in_grad, self.params)\n\n    x_seq_in_grad[:, t, :] = grad_out\n```", "```py\nsequence_length = x_seq_in.shape[1]\n\nx_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n\nfor t in range(sequence_length):\n\n    x_in = x_seq_in[:, t, :]\n\n    y_out, H_in = self.nodes[t].forward(x_in, H_in, self.params)\n\n    x_seq_out[:, t, :] = y_out\n```", "```py\ny_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in self.params)\n```", "```py\ngrad_out, h_in_grad, c_in_grad = \\\n    self.nodes[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n```", "```py\ndef forward(self,\n            x_in: ndarray,\n            H_in: ndarray,\n            params_dict: Dict[str, Dict[str, ndarray]]\n            ) -> Tuple[ndarray]:\n    '''\n param x: numpy array of shape (batch_size, vocab_size)\n param H_prev: numpy array of shape (batch_size, hidden_size)\n return self.x_out: numpy array of shape (batch_size, vocab_size)\n return self.H: numpy array of shape (batch_size, hidden_size)\n '''\n    self.X_in = x_in\n    self.H_in = H_in\n\n    self.Z = np.column_stack((x_in, H_in))\n\n    self.H_int = np.dot(self.Z, params_dict['W_f']['value']) \\\n                                + params_dict['B_f']['value']\n\n    self.H_out = tanh(self.H_int)\n\n    self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) \\\n                                    + params_dict['B_v']['value']\n\n    return self.X_out, self.H_out\n```", "```py\ndef forward(self,\n            x_in: ndarray,\n            H_in: ndarray,\n            params_dict: Dict[str, Dict[str, ndarray]]\n            ) -> Tuple[ndarray]:\n    '''\n param x: numpy array of shape (batch_size, vocab_size)\n param H_prev: numpy array of shape (batch_size, hidden_size)\n return self.x_out: numpy array of shape (batch_size, vocab_size)\n return self.H: numpy array of shape (batch_size, hidden_size)\n '''\n    self.X_in = x_in\n    self.H_in = H_in\n\n    self.Z = np.column_stack((x_in, H_in))\n\n    self.H_int = np.dot(self.Z, params_dict['W_f']['value']) \\\n                                + params_dict['B_f']['value']\n\n    self.H_out = tanh(self.H_int)\n\n    self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) \\\n                                    + params_dict['B_v']['value']\n\n    return self.X_out, self.H_out\n```", "```py\ndef forward(self,\n            X_in: ndarray,\n            H_in: ndarray,\n            params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n    '''\n param X_in: numpy array of shape (batch_size, vocab_size)\n param H_in: numpy array of shape (batch_size, hidden_size)\n return self.X_out: numpy array of shape (batch_size, vocab_size)\n return self.H_out: numpy array of shape (batch_size, hidden_size)\n '''\n    self.X_in = X_in\n    self.H_in = H_in\n\n    # reset gate\n    self.X_r = np.dot(X_in, params_dict['W_xr']['value'])\n    self.H_r = np.dot(H_in, params_dict['W_hr']['value'])\n\n    # update gate\n    self.X_u = np.dot(X_in, params_dict['W_xu']['value'])\n    self.H_u = np.dot(H_in, params_dict['W_hu']['value'])\n\n    # gates\n    self.r_int = self.X_r + self.H_r + params_dict['B_r']['value']\n    self.r = sigmoid(self.r_int)\n\n    self.u_int = self.X_r + self.H_r + params_dict['B_u']['value']\n    self.u = sigmoid(self.u_int)\n\n    # new state\n    self.h_reset = self.r * H_in\n    self.X_h = np.dot(X_in, params_dict['W_xh']['value'])\n    self.H_h = np.dot(self.h_reset, params_dict['W_hh']['value'])\n    self.h_bar_int = self.X_h + self.H_h + params_dict['B_h']['value']\n    self.h_bar = np.tanh(self.h_bar_int)\n\n    self.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar\n\n    self.X_out = (\n  np.dot(self.H_out, params_dict['W_v']['value']) \\\n  + params_dict['B_v']['value']\n  )\n\n    return self.X_out, self.H_out\n```", "```py\nself.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar\n```", "```py\ndef forward(self,\n  X_in: ndarray,\n  H_in: ndarray,\n  C_in: ndarray,\n  params_dict: Dict[str, Dict[str, ndarray]]):\n  '''\n param X_in: numpy array of shape (batch_size, vocab_size)\n param H_in: numpy array of shape (batch_size, hidden_size)\n param C_in: numpy array of shape (batch_size, hidden_size)\n return self.X_out: numpy array of shape (batch_size, output_size)\n return self.H: numpy array of shape (batch_size, hidden_size)\n return self.C: numpy array of shape (batch_size, hidden_size)\n '''\n\n  self.X_in = X_in\n  self.C_in = C_in\n\n  self.Z = np.column_stack((X_in, H_in))\n  self.f_int = (\n    np.dot(self.Z, params_dict['W_f']['value']) \\\n    + params_dict['B_f']['value']\n    )\n  self.f = sigmoid(self.f_int)\n\n  self.i_int = (\n    np.dot(self.Z, params_dict['W_i']['value']) \\\n    + params_dict['B_i']['value']\n    )\n  self.i = sigmoid(self.i_int)\n\n  self.C_bar_int = (\n    np.dot(self.Z, params_dict['W_c']['value']) \\\n    + params_dict['B_c']['value']\n    )\n  self.C_bar = tanh(self.C_bar_int)\n  self.C_out = self.f * C_in + self.i * self.C_bar\n\n  self.o_int = (\n    np.dot(self.Z, params_dict['W_o']['value']) \\\n    + params_dict['B_o']['value']\n    )\n  self.o = sigmoid(self.o_int)\n  self.H_out = self.o * tanh(self.C_out)\n\n  self.X_out = (\n    np.dot(self.H_out, params_dict['W_v']['value']) \\\n    + params_dict['B_v']['value']\n    )\n\n  return self.X_out, self.H_out, self.C_out\n```", "```py\n[RNNLayer(hidden_size=256, output_size=128),\nRNNLayer(hidden_size=256, output_size=62)]\n```", "```py\n[GRULayer(hidden_size=256, output_size=128),\nLSTMLayer(hidden_size=256, output_size=62)]\n```"]