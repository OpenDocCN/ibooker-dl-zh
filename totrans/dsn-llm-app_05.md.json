["```py\nfrom nltk.corpus import gutenberg\nfrom nltk.text import Text\ncorpus = gutenberg.words('austen-emma.txt')\ntext = Text(corpus)\ntext.concordance(\"nervous\")\n```", "```py\nDisplaying 11 of 11 matches:\n...spirits required support . He was a nervous man , easily depressed...\n...sitting for his picture made him so nervous , that I could only take...\n...assure you , excepting those little nervous headaches and palpitations...\n...My visit was of use to the nervous part of her complaint , I hope...\n...much at ease on the subject as his nervous constitution allowed...\n...Her father was growing nervous , and could not understand her....\n...\n```", "```py\nNumber of tokens in the vocabulary * The vector dimension size\n```", "```py\nimport torch\nfrom transformers import LlamaTokenizer, LlamaModel\n\ntokenizer = LlamaTokenizer.from_pretrained('llama3-base')\nmodel = LlamaModel.from_pretrained('llama3-base')\n\nsentence = \"He ate it all\"\n\ninputs = tokenizer(sentence, return_tensors=\"pt\")\ninput_ids = inputs['input_ids']\ntokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n\nwith torch.no_grad():\n    embeddings = model.embeddings(input_ids)\n\nfor token, embedding in zip(tokens, embeddings[0]):\n    print(f\"Token: {token}\\n `print``(``f``\"Embedding:` `{``embedding``}``\\n``\"``)`\n```", "```py` The embedding vectors are the inputs that are then propagated through the rest of the network.    Next, let’s go through each of the components in a Transformer block in detail and explore their role in the modeling process.    ## Self-Attention    The self-attention mechanism draws on the same principle as the distributional hypothesis introduced in [“Representing Meaning”](#representing-meaning), emphasizing the role of context in shaping the meaning of a token. This operation generates representations for each token in a text sequence, capturing various aspects of language like syntax, semantics, and even pragmatics.    In the standard self-attention implementation, the representation of each token is a function of the representation of all other tokens in the sequence. Given a token for which we are calculating its representation, tokens in the sequence that contribute more to the meaning of the token are given more weight.    For example, consider the sequence:    ```", "```py    [Figure 4-3](#Attention-map) depicts how the representation for the token *he* is heavily weighted by the representation of the token *Mark*. In this case, the token *he* is a pronoun used to describe Mark in shorthand. In NLP, mapping a pronoun to its referent is called *co-reference resolution*.  ![Attention-map](assets/dllm_0403.png)  ###### Figure 4-3\\. Attention map    In practice, self-attention in the Transformer is calculated using three sets of weight matrices called queries, keys, and values. Let’s go through them in detail. [Figure 4-4](#kqv) shows how the query, key, and value matrices are used in the self-attention calculation.    Each token is represented by its embedding vector. This vector is multiplied with the query, key, and value weight matrices to generate three input vectors. Self-attention for each token is then calculated like this:    1.  For each token, the dot products of its query vector with the key vectors of all the tokens (including itself) are taken. The resulting values are called attention scores.           2.  The scores are scaled down by dividing them by the square root of the dimension of the key vectors.           3.  The scores are then passed through a [*softmax function*](https://oreil.ly/b6gHV) to turn them into a probability distribution that sums to 1\\. The softmax activation function tends to amplify larger values, hence the reason for scaling down the attention scores in the previous step.           4.  The normalized attention scores are then multiplied by the value vector for the corresponding token. The normalized attention score can be interpreted as the proportion that each token contributes to the representation of a given token.           5.  In practice, there are multiple sets of query, key, and value vectors, calculating parallel representations. This is called multi-headed attention. The idea behind using multiple heads is that the model gets sufficient capacity to model various aspects of the input. The more the number of heads, the more chances that the *right* aspects of the input are being represented.            ![kqv](assets/dllm_0404.png)  ###### Figure 4-4\\. Self-attention calculation    This is how we implement self-attention in code:    ```", "```py    ###### Note    In some Transformer variants, self-attention is calculated only on a subset of tokens in the sequence; thus the vector representation of a token is a function of the representations of only some and not all the tokens in the sequence.    ## Positional Encoding    As discussed earlier, pre-Transformer architectures like LSTM were sequence models, with tokens being processed one after the other. Thus the positional information about the tokens, i.e., the relative positions of the tokens in a sequence, was implicitly baked into the model. However, for Transformers all calculations are done in parallel, and positional information should be fed to the model explicitly. Several methods have been proposed to add positional information, and this is still a very active field of research. Some of the common methods used in LLMs today include:    Absolute positional embeddings      These were used in the original Transformer implementation by [Vaswani et al.](https://oreil.ly/CDq60); examples of models using absolute positional embeddings include earlier models like BERT and RoBERTa.      Attention with Linear Biases (ALiBi)      In this technique, the attention scores are [penalized](https://arxiv.org/abs/2108.12409) with a bias term proportional to the distance between the query token and the key token. This technique also enables modeling sequences of longer length during inference than what was encountered in the training process.      Rotary Position Embedding (RoPE)      Just like ALiBi, this [technique](https://arxiv.org/abs/2104.09864) has the property of relative decay; there is a decay in the attention scores as the distance between the query token and the key token increases.      No Positional Encoding (NoPE)      A contrarian [technique](https://oreil.ly/QM9dW) argues that positional embeddings in fact are not required and that Transformers implicitly capture positional information.      Models these days are mostly using ALiBi or RoPE, although this is one aspect of the Transformer architecture that is still actively improving.    ## Feedforward Networks    The output from a self-attention block is fed through a [*feedforward network*](https://oreil.ly/Bdphg). Each token representation is independently fed through the network. The feedforward network incorporates a nonlinear activation function like [Rectified Linear Unit (ReLU)](https://oreil.ly/KUqtP) or [Gaussian Error Linear Units (GELU)](https://oreil.ly/MSDKE), thus enabling the model to learn more complex features from the data. For more details on these activation functions, refer to this [blog post from v7](https://oreil.ly/NfOb0).    The feedforward layers are implemented in code in this way:    ```", "```py    ## Layer Normalization    Layer normalization is performed to ensure training stability and faster training convergence. While the original Transformer architecture performed normalization at the beginning of the block, modern implementations do it at the end of the block. The normalization is performed as follows:    1.  Given an input of batch size `b`, sequence length `n`, and vector dimension `d`, calculate the mean and variance across each vector dimension.           2.  Normalize the input by subtracting the mean and dividing it by the square root of the variance. A small epsilon value is added to the denominator for numerical stability.           3.  Multiply by a scale parameter and add a shift parameter to the resulting values. These parameters are learned during the training process.              This is how it is represented in code:    ```", "```py ```", "```py```", "```py Cross-Entropy= −∑(gold truth probability)×log(output probability) ```", "```py 'His pizza tasted ______' ```", "```py −(0×log(0.65)+0×log(0.12)+1×log(0.11)+...)= −log(0.11) ```", "```py Cross-Entropy = -log(output probability of gold truth token) ```", "```py Perplexity = 2^Cross-Entropy ```", "```py 'Tammy jumped over the' ```", "```py 'Language models are ubiquitous' ```", "```py import torch from transformers import AutoTokenizer, GPTNeoForCausalLM   tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\") model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")   input_ids = tokenizer(\"Language models are\", return_tensors=\"pt\") gen_tokens = model.generate(**input_ids, max_new_tokens =1,  output_scores=True, return_dict_in_generate=True) output_scores = gen_tokens[\"scores\"] scores_tensor = output_scores[0] sorted_indices = torch.argsort(scores_tensor[0], descending=True)[:20]   for index in sorted_indices:     token_id = index     token_name = tokenizer.decode([token_id.item()])     token_score = scores_tensor[0][index].item()     print(f\"Token: {token_name}, Score: {token_score}\") ```", "```py Output: Token:  a, Score: -1.102203369140625 Token:  used, Score: -1.4315788745880127 Token:  the, Score: -1.7675716876983643 Token:  often, Score: -1.8415470123291016 Token:  an, Score: -2.4652323722839355 Token:  widely, Score: -2.657834053039551 Token:  not, Score: -2.6726579666137695 Token:  increasingly, Score: -2.7568516731262207 Token:  ubiquitous, Score: -2.8688106536865234 Token:  important, Score: -2.902832508087158 Token:  one, Score: -2.9083480834960938 Token:  defined, Score: -3.0815649032592773 Token:  being, Score: -3.2117576599121094 Token:  commonly, Score: -3.3110013008117676 Token:  very, Score: -3.317342758178711 Token:  typically, Score: -3.4478530883789062 Token:  complex, Score: -3.521362781524658 Token:  powerful, Score: -3.5338563919067383 Token:  language, Score: -3.550961971282959 Token:  pervasive, Score: -3.563507080078125 ```", "```py 'I had 25 eggs. I gave away 12\\. I now have 13' ```", "```py input_ids = tokenizer(\"'I had 25 eggs. I gave away 12\\. I now have\",   return_tensors=\"pt\") ```", "```py Token:  12, Score: -2.3242850303649902 Token:  25, Score: -2.5023117065429688 Token:  only, Score: -2.5456185340881348 Token:  a, Score: -2.5726099014282227 Token:  2, Score: -2.6731367111206055 Token:  15, Score: -2.6967623233795166 Token:  4, Score: -2.8040688037872314 Token:  3, Score: -2.839219570159912 Token:  14, Score: -2.847306728363037 Token:  11, Score: -2.8585362434387207 Token:  1, Score: -2.877161979675293 Token:  10, Score: -2.9321107864379883 Token:  6, Score: -2.982785224914551 Token:  18, Score: -3.0570476055145264 Token:  20, Score: -3.079172134399414 Token:  5, Score: -3.111320972442627 Token:  13, Score: -3.117424726486206 Token:  9, Score: -3.125835657119751 Token:  16, Score: -3.1476120948791504 Token:  7, Score: -3.1622045040130615 ```", "```py import openai openai.api_key = <Insert your OpenAI key>   openai.Completion.create(   model=\"gpt-4o\",   prompt=\"I had 25 eggs. I gave away 12\\. I now have \",   max_tokens=1,   temperature=0,   logprobs = 10 ) ```", "```py \"top_logprobs\": [           {             \"\\n\": -0.08367541,             \" 13\": -2.8566456,             \"____\": -4.579212,             \"_____\": -4.978668,             \"________\": -6.220278             …           } ```", "```py from transformers import T5Tokenizer, T5ForConditionalGeneration  tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\") model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")  input_ids = tokenizer(\"Tempura <extra_id_0>  been a source <extra_id_1> in the `family` `due` `to` `unexplained` `reasons``\", return_tensors=\"``pt``\").input_ids` ```", "```py` `<``extra_id_2``>``\", return_tensors=\"``pt``\").input_ids` ```", "```py ```", "```py`` ```", "```py `` `The targets can be prepared using a simple templating function.    More generally, MLM can be interpreted as a *denoising autoencoder*. You corrupt your input by adding noise (masking, dropping tokens), and then you train a model to regenerate the original input. BART takes this to the next level by using five different types of span corruptions:    Random token masking      [Figure 4-9](#bart-enoiser-objectives1) depicts the corruption and denoising steps.  ![BART Denoiser Objectives1](assets/dllm_0409.png)  ###### Figure 4-9\\. Random token masking in BART      Random token deletion      The model needs to predict the positions in the text where tokens have been deleted. [Figure 4-10](#bart-enoiser-objectives2) depicts the corruption and denoising steps.  ![BART Denoiser Objectives2](assets/dllm_0410.png)  ###### Figure 4-10\\. Random token deletion in BART      Span masking      Text spans are sampled from text, with span lengths coming from a Poisson distribution. This means zero-length spans are possible. The spans are deleted from the text and replaced with a single mask token. Therefore, the model now has to also predict the number of tokens deleted. [Figure 4-11](#bart-enoiser-objectives3) depicts the corruption and denoising steps.  ![BART Denoiser Objectives3](assets/dllm_0411.png)  ###### Figure 4-11\\. Span masking in BART      Document shuffling      Sentences in the input document are shuffled. The model is taught to arrange them in the right order. [Figure 4-12](#bart-enoiser-objectives4) depicts the corruption and denoising steps.  ![BART Denoiser Objectives4](assets/dllm_0412.png)  ###### Figure 4-12\\. Document shuffling objective in BART      Document rotation      The document is rotated so that it starts from an arbitrary token. The model is trained to detect the correct start of the document. [Figure 4-13](#bart-enoiser-objectives5) depicts the corruption and denoising steps.  ![BART Denoiser Objectives5](assets/dllm_0413.png)  ###### Figure 4-13\\. Document rotation objective in BART` `` ```", "```py `` `## Which Learning Objectives Are Better?    It has been shown that models trained with FLM are better at generation, and models trained with MLM are better at classification tasks. However, it is inefficient to use different language models for different use cases. The consolidation effect continues to take hold, with the introduction of [UL2](https://oreil.ly/xJc3U), a paradigm that combines the best of different learning objective types in a single model.    UL2 mimics the effect of PLMs, MLMs, and PrefixLMs in a single paradigm called *Mixture of Denoisers*.    The denoisers used are as follows:    R-Denoiser      This is similar to the T5 span corruption task. Spans between length 2–5 tokens are replaced by a single mask token. [Figure 4-14](#ul2-mixture-denoisers1) depicts the workings of the R-denoiser.  ![UL2's Mixture of Denoisers1](assets/dllm_0414.png)  ###### Figure 4-14\\. UL2’s R-Denoiser      S-Denoiser      Similar to prefix LM, the text is divided into a prefix and a suffix. The suffix is masked, while the prefix has access to bidirectional context. [Figure 4-15](#ul2-mixture-denoisers2) depicts the workings of the S-Denoiser.  ![UL2's Mixture of Denoisers2](assets/dllm_0415.png)  ###### Figure 4-15\\. UL2’s S-Denoiser      X-Denoiser      This stands for extreme denoising, where a large proportion of text is masked (often over 50%). [Figure 4-16](#ul2-mixture-denoisers3) depicts the workings of the X-Denoiser.  ![UL2's Mixture of Denoisers3](assets/dllm_0416.png)  ###### Figure 4-16\\. UL2’s X-Denoiser` `` ```", "```py```", "````` ```py`# Pre-Training Models    Now that we have learned about the ingredients that go into a language model in detail, let’s learn how to pre-train one from scratch.    The language models of today are learning to model two types of concepts with one model:    *   Language, the vehicle used to communicate facts, opinions, and feelings.           *   The underlying phenomena that led to the construction of text in the language.              For many application areas, we are far more interested in learning to model the latter than the former. While a language model that is fluent in the language is welcome, we would prefer to see it get better at domains like science or law and skills like reasoning and arithmetic.    These concepts and skills are expressed in languages like English, which primarily serve a social function. Human languages are inherently ambiguous, contain lots of redundancies, and in general are inefficient vehicles to transmit underlying concepts.    This brings us to the question: are human languages even the best vehicle for language models to learn underlying skills and concepts? Can we separate the process of modeling the language from modeling the underlying concepts expressed through language?    Let’s put this theory to the test using an example. Consider training an LLM from scratch to learn to play the game of chess.    Recall the ingredients of a language model from [Chapter 2](ch02.html#ch02). We need:    *   A pre-training dataset           *   A vocabulary and tokenization scheme           *   A model architecture           *   A learning objective              For training the chess language model, we can choose the Transformer architecture with the next-token prediction learning objective, which is the de facto paradigm used today.    For the pre-training dataset, we can use the chess games dataset from [Lichess](https://oreil.ly/XmWvv), containing billions of games. We select a subset of 20 million chess games for our training.    This dataset is in the Portable Game Notation (PGN) format, which is used to represent the sequence of chess moves in a concise notation.    Finally, we have to choose the vocabulary of the model. Since the only purpose of this model is to learn chess, we don’t need to support an extensive English vocabulary. In fact, we can take advantage of the PGN notation to assign tokens to specific chess concepts.    Here is an example of a chess game in PGN format, taken from [pgnmentor.com](https://oreil.ly/H3yOs):    ``` 1\\. e4 c5 2\\. Nf3 a6 3\\. d3 g6 4\\. g3 Bg7 5\\. Bg2 b5 6\\. O-O Bb7 7\\. c3 e5 8\\. a3 Ne7 9\\. b4 d6 10\\. Nbd2 O-O 11\\. Nb3 Nd7 12\\. Be3 Rc8 13\\. Rc1 h6 14\\. Nfd2 f5 15\\. f4 Kh7 16\\. Qe2 cxb4 17\\. axb4 exf4 18\\. Bxf4 Rxc3 19\\. Rxc3 Bxc3 20\\. Bxd6 Qb6+ 21. Bc5 Nxc5 22\\. bxc5 Qe6 23\\. d4 Rd8 24\\. Qd3 Bxd2 25\\. Nxd2 fxe4 26\\. Nxe4 Nf5 27. d5 Qe5 28\\. g4 Ne7 29\\. Rf7+ Kg8 30\\. Qf1 Nxd5 31\\. Rxb7 Qd4+ 32\\. Kh1 Rf8 33\\. Qg1 Ne3 34\\. Re7 a5 35\\. c6 a4 36\\. Qxe3 Qxe3 37\\. Nf6+ Rxf6 38\\. Rxe3 Rd6 39\\. h4 Rd1+ 40\\. Kh2 b4 41\\. c7 1-0 ```py    The rows of the board are assigned letters a–h and the columns are assigned numbers 1–8\\. Except for pawns, each piece type is assigned a capital letter, with N for knight, R for rook, B for bishop, Q for queen, and K for king. A + appended to a move indicates a check, a % appended to the move indicates a checkmate, and 0-0 is used to indicate castling. If you are unfamiliar with the rules of chess, refer to [this piece for a primer](https://oreil.ly/EbcfQ).    Based on this notation, the vocabulary can consist of:    *   A separate token for each square on the board, with 64 total (a1, a2, a3…​h6, h7, h8)           *   A separate token for each piece type (N, B, R, K, Q)           *   Tokens for move numbers (1., 2., 3., etc.)           *   Tokens for special moves (+ for check, x for capture, etc.)              Now, let’s train a language model from scratch on this chess dataset using our special domain-specific vocabulary. The model is directly learning from the PGN notation with no human language text present in the dataset. The book’s [GitHub repo](https://oreil.ly/llm-playbooks) contains the code and setup for training this model.    After training the model for three epochs, let’s test the model’s ability to play chess. We can see that the model seems to have learned the rules of the game without having to be provided the rules explicitly in natural language. In fact, the model can even beat human players some of the time and can execute moves like castling.    Note that this model was able to learn the concepts (chess) using a domain-specific language (PGN). How will we fare if the concepts were taught in natural language?    Let’s explore this in another experiment. Take the same dataset used to pre-train the chess language model and run it through an LLM to convert each move in PGN to a sentence in English. An example game would look like:    *White moves pawn to e4*    *Black moves bishop to g7*    and so on. Train a new language model on the same number of games as the previous one, but this time with the English-language dataset. Let the vocabulary of this model be the standard English vocabulary generated by training the tokenizer over the training set.    How does this compare to the chess LM trained on the PGN dataset? The model trained on English descriptions of chess moves performs worse and doesn’t seem to have understood the rules of the game yet, despite being trained on the same number of games as the other model.    This shows that natural language is not necessarily the most efficient vehicle for a model to learn skills and concepts, and domain-specific languages and notations perform better.    Thus, language design is an important skill to acquire, enabling you to create domain-specific languages for learning concepts and skills. For your application areas, you could use existing domain-specific languages or create a new one yourself.    # Summary    In this chapter, we discussed the various components of the Transformer architecture in detail, including self-attention, feedforward networks, position encodings, and layer normalization. We also discussed several variants and configurations such as encoder-only, encoder-decoder, decoder-only, and MoE models. Finally, we learned how to put our knowledge of language models together to train our own model from scratch and how to design domain-specific languages for more efficient learning.```` ```py`` `````", "```````"]