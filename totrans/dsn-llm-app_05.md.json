["```py\nfrom nltk.corpus import gutenberg\nfrom nltk.text import Text\ncorpus = gutenberg.words('austen-emma.txt')\ntext = Text(corpus)\ntext.concordance(\"nervous\")\n```", "```py\nDisplaying 11 of 11 matches:\n...spirits required support . He was a nervous man , easily depressed...\n...sitting for his picture made him so nervous , that I could only take...\n...assure you , excepting those little nervous headaches and palpitations...\n...My visit was of use to the nervous part of her complaint , I hope...\n...much at ease on the subject as his nervous constitution allowed...\n...Her father was growing nervous , and could not understand her....\n...\n```", "```py\nNumber of tokens in the vocabulary * The vector dimension size\n```", "```py\nimport torch\nfrom transformers import LlamaTokenizer, LlamaModel\n\ntokenizer = LlamaTokenizer.from_pretrained('llama3-base')\nmodel = LlamaModel.from_pretrained('llama3-base')\n\nsentence = \"He ate it all\"\n\ninputs = tokenizer(sentence, return_tensors=\"pt\")\ninput_ids = inputs['input_ids']\ntokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n\nwith torch.no_grad():\n    embeddings = model.embeddings(input_ids)\n\nfor token, embedding in zip(tokens, embeddings[0]):\n    print(f\"Token: {token}\\n\n    print(f\"Embedding: {embedding}\\n\")\n```", "```py\n'Mark told Sam that he was planning to resign.'\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nq = wQ(input_embeddings)\nk = WK(input_embeddings)\nv = WV(input_embeddings)\ndim_k = k.size(-1)\n\nattn_scores = torch.matmul(q, k.transpose(-2, -1))\nscaled_attn_scores = attn_scores/torch.sqrt(torch.tensor(dim_k,\n  dtype=torch.float32))\n\nnormalized_attn_scores = F.softmax(scaled_attn_scores, dim=-1)\n\noutput = torch.matmul(normalized_attn_scores, v)\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass FeedForward(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(FeedForward, self).__init__()\n        self.l1 = nn.Linear(input_dim, hidden_dim)\n        self.l2 = nn.Linear(hidden_dim, input_dim)\n        self.selu = nn.SeLU()\n\n    def forward(self, x):\n        x = self.selu(self.l1(x))\n        x = self.l2(x)\n        return x\n\nfeed_forward = FeedForward(input_dim, hidden_dim)\noutputs = feed_forward(inputs)\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dimension, gamma=None, beta=None, epsilon=1e-5):\n        super(LayerNorm, self).__init__()\n        self.epsilon = epsilon\n        self.gamma = gamma if gamma is not None else\n        nn.Parameter(torch.ones(dimension))\n        self.beta = beta if beta is not None else\n        nn.Parameter(torch.zeros(dimension))\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        variance = x.var(-1, keepdim=True, unbiased=False)\n        x_normalized = (x - mean) / torch.sqrt(variance + self.epsilon)\n        return self.gamma * x_normalized + self.beta\n\nlayer_norm = LayerNorm(embedding_dim)\noutputs = layer_norm(inputs)\n```", "```py\nCross-Entropy= −∑(gold truth probability)×log(output probability)\n```", "```py\n'His pizza tasted ______'\n```", "```py\n−(0×log(0.65)+0×log(0.12)+1×log(0.11)+...)= −log(0.11)\n```", "```py\nCross-Entropy = -log(output probability of gold truth token)\n```", "```py\nPerplexity = 2^Cross-Entropy\n```", "```py\n'Tammy jumped over the'\n```", "```py\n'Language models are ubiquitous'\n```", "```py\nimport torch\nfrom transformers import AutoTokenizer, GPTNeoForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\nmodel = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n\ninput_ids = tokenizer(\"Language models are\", return_tensors=\"pt\")\ngen_tokens = model.generate(**input_ids, max_new_tokens =1,\n\noutput_scores=True, return_dict_in_generate=True)\noutput_scores = gen_tokens[\"scores\"]\nscores_tensor = output_scores[0]\nsorted_indices = torch.argsort(scores_tensor[0], descending=True)[:20]\n\nfor index in sorted_indices:\n    token_id = index\n    token_name = tokenizer.decode([token_id.item()])\n    token_score = scores_tensor[0][index].item()\n    print(f\"Token: {token_name}, Score: {token_score}\")\n```", "```py\nOutput: Token:  a, Score: -1.102203369140625\nToken:  used, Score: -1.4315788745880127\nToken:  the, Score: -1.7675716876983643\nToken:  often, Score: -1.8415470123291016\nToken:  an, Score: -2.4652323722839355\nToken:  widely, Score: -2.657834053039551\nToken:  not, Score: -2.6726579666137695\nToken:  increasingly, Score: -2.7568516731262207\nToken:  ubiquitous, Score: -2.8688106536865234\nToken:  important, Score: -2.902832508087158\nToken:  one, Score: -2.9083480834960938\nToken:  defined, Score: -3.0815649032592773\nToken:  being, Score: -3.2117576599121094\nToken:  commonly, Score: -3.3110013008117676\nToken:  very, Score: -3.317342758178711\nToken:  typically, Score: -3.4478530883789062\nToken:  complex, Score: -3.521362781524658\nToken:  powerful, Score: -3.5338563919067383\nToken:  language, Score: -3.550961971282959\nToken:  pervasive, Score: -3.563507080078125\n```", "```py\n'I had 25 eggs. I gave away 12\\. I now have 13'\n```", "```py\ninput_ids = tokenizer(\"'I had 25 eggs. I gave away 12\\. I now have\",\n  return_tensors=\"pt\")\n```", "```py\nToken:  12, Score: -2.3242850303649902\nToken:  25, Score: -2.5023117065429688\nToken:  only, Score: -2.5456185340881348\nToken:  a, Score: -2.5726099014282227\nToken:  2, Score: -2.6731367111206055\nToken:  15, Score: -2.6967623233795166\nToken:  4, Score: -2.8040688037872314\nToken:  3, Score: -2.839219570159912\nToken:  14, Score: -2.847306728363037\nToken:  11, Score: -2.8585362434387207\nToken:  1, Score: -2.877161979675293\nToken:  10, Score: -2.9321107864379883\nToken:  6, Score: -2.982785224914551\nToken:  18, Score: -3.0570476055145264\nToken:  20, Score: -3.079172134399414\nToken:  5, Score: -3.111320972442627\nToken:  13, Score: -3.117424726486206\nToken:  9, Score: -3.125835657119751\nToken:  16, Score: -3.1476120948791504\nToken:  7, Score: -3.1622045040130615\n```", "```py\nimport openai\nopenai.api_key = <Insert your OpenAI key>\n\nopenai.Completion.create(\n  model=\"gpt-4o\",\n  prompt=\"I had 25 eggs. I gave away 12\\. I now have \",\n  max_tokens=1,\n  temperature=0,\n  logprobs = 10\n)\n```", "```py\n\"top_logprobs\": [\n          {\n            \"\\n\": -0.08367541,\n            \" 13\": -2.8566456,\n            \"____\": -4.579212,\n            \"_____\": -4.978668,\n            \"________\": -6.220278\n            …\n          }\n```", "```py\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n\ninput_ids = tokenizer(\"Tempura <extra_id_0>  been a source <extra_id_1> in the\nfamily due to unexplained reasons\", return_tensors=\"pt\").input_ids\ntargets = tokenizer(\"<extra_id_0> has always <extra_id_1> of conflict\n\n<extra_id_2>\", return_tensors=\"pt\").input_ids\nloss = model(input_ids=input_ids, labels=labels).loss\n```", "```py\n1\\. e4 c5 2\\. Nf3 a6 3\\. d3 g6 4\\. g3 Bg7 5\\. Bg2 b5 6\\. O-O Bb7 7\\. c3 e5 8\\. a3 Ne7\n9\\. b4 d6 10\\. Nbd2 O-O 11\\. Nb3 Nd7 12\\. Be3 Rc8 13\\. Rc1 h6 14\\. Nfd2 f5 15\\. f4\nKh7 16\\. Qe2 cxb4 17\\. axb4 exf4 18\\. Bxf4 Rxc3 19\\. Rxc3 Bxc3 20\\. Bxd6 Qb6+ 21.\nBc5 Nxc5 22\\. bxc5 Qe6 23\\. d4 Rd8 24\\. Qd3 Bxd2 25\\. Nxd2 fxe4 26\\. Nxe4 Nf5 27.\nd5 Qe5 28\\. g4 Ne7 29\\. Rf7+ Kg8 30\\. Qf1 Nxd5 31\\. Rxb7 Qd4+ 32\\. Kh1 Rf8 33\\. Qg1\nNe3 34\\. Re7 a5 35\\. c6 a4 36\\. Qxe3 Qxe3 37\\. Nf6+ Rxf6 38\\. Rxe3 Rd6 39\\. h4 Rd1+\n40\\. Kh2 b4 41\\. c7 1-0\n```"]