- en: Chapter 9\. Distributed TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。分布式TensorFlow
- en: In this chapter we discuss the use of TensorFlow for distributed computing.
    We start by briefly surveying the different approaches to distributing model training
    in machine learning in general, and specifically for deep learning. We then introduce
    the elements of TensorFlow designed to support distributed computing, and finally
    put everything together with an end-to-end example.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了使用TensorFlow进行分布式计算。我们首先简要调查了在机器学习中分布模型训练的不同方法，特别是深度学习。然后介绍了为支持分布式计算而设计的TensorFlow元素，最后通过一个端到端的示例将所有内容整合在一起。
- en: Distributed Computing
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算
- en: '*Distributed* *computing*, in the most general terms, entails the utilization
    of more than one component in order to perform the desired computation or achieve
    a goal. In our case, this means using multiple machines in order to speed up the
    training of a deep learning model.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式* *计算*，在最一般的术语中，意味着利用多个组件来执行所需的计算或实现目标。在我们的情况下，这意味着使用多台机器来加快深度学习模型的训练。'
- en: The basic idea behind this is that by using more computing power, we should
    be able to train the same model faster. This is indeed often the case, although
    just how much faster depends on many factors (i.e., if you expect to use 10× resources
    and get a 10× speedup, you are most likely going to be disappointed!).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的基本思想是通过使用更多的计算能力，我们应该能够更快地训练相同的模型。尽管通常情况下确实如此，但实际上更快多少取决于许多因素（即，如果您期望使用10倍资源并获得10倍加速，您很可能会感到失望！）。
- en: There are many ways to distribute computations in a machine learning setting.
    You may want to utilize multiple devices, either on the same machine or across
    a cluster. When training a single model, you may want to compute gradients across
    a cluster to speed up training, either synchronously or asynchronously. A cluster
    may also be used to train multiple models at the same time, or in order to search
    for the optimal parameters for a single model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习环境中有许多分布计算的方式。您可能希望利用多个设备，无论是在同一台机器上还是跨集群。在训练单个模型时，您可能希望在集群上计算梯度以加快训练速度，无论是同步还是异步。集群也可以用于同时训练多个模型，或者为单个模型搜索最佳参数。
- en: In the following subsections we map out these many aspects of parallelism.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将详细介绍并行化的许多方面。
- en: Where Does the Parallelization Take Place?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化发生在哪里？
- en: The first split in the classification of types of parallelization is the location.
    Are we using multiple computing devices on a single machine or across a cluster?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行化类型的分类中，第一个分割是位置。我们是在单台机器上使用多个计算设备还是跨集群？
- en: It is becoming increasingly common to have powerful hardware with multiple devices
    on a single machine. Cloud providers (such as Amazon Web Services) now offer this
    sort of platform set up and ready to go.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在一台机器上拥有强大的硬件与多个设备变得越来越普遍。云服务提供商（如亚马逊网络服务）现在提供这种类型的平台设置并准备就绪。
- en: Whether in the cloud or on premises, a cluster configuration affords more flexibility
    in design and evolution, and the setup can grow way beyond what is currently feasible
    with multiple devices on the same board (essentially, you can use a cluster of
    arbitrary size).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是在云端还是本地，集群配置在设计和演进方面提供了更多的灵活性，设置可以扩展到目前在同一板上使用多个设备所不可行的程度（基本上，您可以使用任意大小的集群）。
- en: On the other hand, while several devices on the same board can use shared memory,
    the cluster approach introduces the time cost of communication between nodes.
    This can become a limiting factor, when the amount of information that has to
    be shared is large and communication is relatively slow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，虽然同一板上的几个设备可以使用共享内存，但集群方法引入了节点之间通信的时间成本。当需要共享的信息量很大且通信相对缓慢时，这可能成为一个限制因素。
- en: What Is the Goal of Parallelization?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化的目标是什么？
- en: The second split is the actual goal. Do we want to use more hardware to make
    the same process faster, or in order to parallelize the training of multiple models?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个分割是实际目标。我们是想使用更多硬件使相同的过程更快，还是为了并行化多个模型的训练？
- en: The need to train multiple models often arises in development stages where a
    choice needs to be made regarding either the models or the hyperparameters to
    use. In this case it is common to run several options and choose the best-performing
    one. It is natural to do so in parallel.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发阶段经常需要训练多个模型，需要在模型或超参数之间做出选择。在这种情况下，通常会运行几个选项并选择表现最佳的一个。这样做是很自然的。
- en: Alternatively, when training a single (often large) model, a cluster may be
    used in order to speed up training. In the most common approach, known as *data
    parallelism*, the same model structure exists on each computation device separately,
    and the data running through each copy is what is parallelized.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当训练单个（通常是大型）模型时，可以使用集群来加快训练速度。在最常见的方法中，称为*数据并行*，每个计算设备上都存在相同的模型结构，每个副本上运行的数据是并行化的。
- en: 'For example, when training a deep learning model with gradient descent, the
    process is composed of the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用梯度下降训练深度学习模型时，该过程由以下步骤组成：
- en: Compute the gradients for a batch of training examples.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一批训练样本的梯度。
- en: Sum the gradients.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对梯度求和。
- en: Apply an update to the model parameters accordingly.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相应地对模型参数应用更新。
- en: Clearly, step 1 of this schema lends itself to parallelization. Simply use multiple
    devices to compute the gradients (with respect to different training examples),
    and then aggregate the results and sum them up in step 2, just as in the regular
    case.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这个模式的第1步适合并行化。简单地使用多个设备计算梯度（针对不同的训练样本），然后在第2步中聚合结果并求和，就像常规情况下一样。
- en: '**Synchronous versus asynchronous data parallelism**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**同步与异步数据并行**'
- en: In the process just described, gradients from different training examples are
    aggregated together, in order to make a single update to the model parameters.
    This is what is known as *synchronous* training, since the summation step defines
    a point where the flow has to wait for all of the nodes to complete the gradient
    computation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚才描述的过程中，来自不同训练示例的梯度被聚合在一起，以对模型参数进行单次更新。这就是所谓的*同步*训练，因为求和步骤定义了一个流必须等待所有节点完成梯度计算的点。
- en: One case where it might be better to avoid this is when there are heterogeneous
    computing resources being used together, since the synchronous option entails
    waiting for the slowest of the nodes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种情况可能更好地避免这种情况，即当异构计算资源一起使用时，因为同步选项意味着等待节点中最慢的节点。
- en: The alternative, *asynchronous* option is to apply the update step independently
    after each node finishes computing the gradients for the training examples it
    was assigned.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*异步*选项是在每个节点完成为其分配的训练示例的梯度计算后独立应用更新步骤。'
- en: TensorFlow Elements
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow元素
- en: In this section we go over the TensorFlow elements and concepts that are used
    in parallel computations.  This is not a complete overview, and primarily serves
    as an introduction to the parallel example that concludes this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍在并行计算中使用的TensorFlow元素和概念。这不是完整的概述，主要作为本章结束的并行示例的介绍。
- en: tf.app.flags
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tf.app.flags
- en: We start with a mechanism that is completely unrelated to parallel computing,
    but is essential for our example at the end of the chapter. Indeed, the `flags`
    mechanism is heavily used in TensorFlow examples and deserves to be discussed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个与并行计算完全无关但对本章末尾的示例至关重要的机制开始。实际上，在TensorFlow示例中广泛使用`flags`机制，值得讨论。
- en: Essentially, `tf.app.flags` is a wrapper for the Python `argparse` module, which
    is commonly used to process command-line arguments, with some extra and specific
    functionality.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，`tf.app.flags`是Python `argparse`模块的包装器，通常用于处理命令行参数，具有一些额外和特定的功能。
- en: 'Consider, for instance, a Python command-line program with typical command-line
    arguments:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个具有典型命令行参数的Python命令行程序：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The program *distribute.py* is passed the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 程序*distribute.py*传递以下内容：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This information is then extracted within the Python script, by using:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在Python脚本中提取这些信息，使用：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The arguments (both string and integer) are defined by the name in the command
    line, a default value, and a description of the argument.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参数（字符串和整数）由命令行中的名称、默认值和参数描述定义。
- en: 'The `flags` mechanism allows the following types of arguments:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`flags`机制允许以下类型的参数：'
- en: '`tf.app.flags.DEFINE_string` defines a string value.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.app.flags.DEFINE_string`定义一个字符串值。'
- en: '`tf.app.flags.DEFINE_boolean` defines a Boolean value.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.app.flags.DEFINE_boolean`定义一个布尔值。'
- en: '`tf.app.flags.DEFINE_float` defines a floating-point value.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.app.flags.DEFINE_float`定义一个浮点值。'
- en: '`tf.app.flags.DEFINE_integer` defines an integer value.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.app.flags.DEFINE_integer`定义一个整数值。'
- en: Finally, `tf.app.flags.FLAGS` is a structure containing the values of all the
    arguments parsed from the command-line input. The arguments are accessed as `FLAGS.arg`,
    or via the dictionary `FLAGS.__flags` if necessary (it is, however, highly recommended
    to use the first option—the way it was designed to be used).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`tf.app.flags.FLAGS`是一个结构，包含从命令行输入解析的所有参数的值。参数可以通过`FLAGS.arg`访问，或者在必要时通过字典`FLAGS.__flags`访问（然而，强烈建议使用第一种选项——它设计的方式）。
- en: Clusters and Servers
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群和服务器
- en: 'A TensorFlow cluster is simply a set of nodes (a.k.a. tasks) that participate
    in parallel processing of a computation graph. Each task is defined by the network
    address at which it may be accessed. For example:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TensorFlow集群只是参与计算图并行处理的节点（也称为任务）的集合。每个任务由其可以访问的网络地址定义。例如：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here we defined four local tasks (note that `localhost:*XXXX*` points to port
    *XXXX* on the current machine, and in a multiple-computer setting the `localhost`
    would be replaced by an IP address). The tasks are divided into a single *parameter
    server* and three *workers*. The parameter server/worker assignments are referred
    to as *jobs*. We further describe what each of these does during training later
    on in the chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了四个本地任务（请注意，`localhost:*XXXX*`指向当前机器上端口*XXXX*，在多台计算机设置中，`localhost`将被IP地址替换）。任务分为一个*参数服务器*和三个*工作节点*。参数服务器/工作节点分配被称为*作业*。我们稍后在本章中进一步描述这些在训练期间的作用。
- en: Each of the tasks must run a TensorFlow server, in order to both use local resources
    for the actual computations and communicate with other tasks in the cluster to
    facilitate parallelization.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个任务必须运行一个TensorFlow服务器，以便既使用本地资源进行实际计算，又与集群中的其他任务通信，以促进并行化。
- en: 'Building on the cluster definition, a server on the first worker node (i.e.,
    `localhost:2223`) would be started by:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于集群定义，第一个工作节点上的服务器（即`localhost:2223`）将通过以下方式启动：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The arguments received by `Server()` let it know its identity, as well as the
    identities and addresses of the other members in the cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由`Server()`接收的参数让它知道自己的身份，以及集群中其他成员的身份和地址。
- en: Once we have the clusters and servers in place, we build the computation graph
    that will allow us to go forward with the parallel computation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了集群和服务器，我们就构建计算图，这将使我们能够继续进行并行计算。
- en: Replicating a Computational Graph Across Devices
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在设备之间复制计算图
- en: As mentioned previously, there is more than one way to perform parallel training. In
    [“Device Placement”](#device_Placement), we briefly discuss how to directly place
    operations on a specific task in a cluster. In the rest of this section we go
    over what is necessary for between-graph replication.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有多种方法可以进行并行训练。在[“设备放置”](#device_Placement)中，我们简要讨论如何直接将操作放置在集群中特定任务上。在本节的其余部分，我们将介绍对于图间复制所必需的内容。
- en: '*Between-graph* *replication* refers to the common parallelization mode where
    a separate but identical computation graph is built on each of the worker tasks.
    During training, gradients are computed by each of the workers and combined by
    the parameter server, which also keeps track of the current versions of the parameters,
    and possibly other global elements of training (such as a global step counter,
    etc.).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*图间* *复制* 指的是常见的并行化模式，其中在每个worker任务上构建一个单独但相同的计算图。在训练期间，每个worker计算梯度，并由参数服务器组合，参数服务器还跟踪参数的当前版本，以及可能是训练的其他全局元素（如全局步骤计数器等）。'
- en: 'We use `tf.train.replica_device_setter()` in order to replicate the model (computation
    graph) on each of the tasks. The `worker_device` argument should point to the
    current task within the cluster. For instance, on the first worker we run this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`tf.train.replica_device_setter()`来在每个任务上复制模型（计算图）。`worker_device`参数应该指向集群中当前任务。例如，在第一个worker上我们运行这个：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The exception is the parameter server, on which we don’t build a computation
    graph. In order for the process not to terminate, we use:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例外是参数服务器，我们不在其上构建计算图。为了使进程不终止，我们使用：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: which will keep the parameter server alive for the duration of the parallel
    computation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在并行计算的过程中保持参数服务器的运行。
- en: Managed Sessions
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理的会话
- en: 'In this section we cover the mechanism that we will later use for parallel
    training of our model. First, we define a `Supervisor`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍我们将在模型的并行训练中使用的机制。首先，我们定义一个`Supervisor`：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As the name suggests, the `Supervisor` is used to supervise training, providing
    some utilities necessary for the parallel setting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，`Supervisor`用于监督训练，在并行设置中提供一些必要的实用程序。
- en: 'There are four arguments passed:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 传递了四个参数：
- en: '`is_chief` (Boolean)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_chief`（布尔值）'
- en: There must be a single *chief*, which is the task responsible for initialization,
    etc.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 必须有一个单一的*chief*，负责初始化等任务。
- en: '`logdir` (string)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`logdir`（字符串）'
- en: Where to store logs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 存储日志的位置。
- en: '`global_step`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`global_step`'
- en: A TensorFlow Variable that will hold the current global step during training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TensorFlow变量，将在训练期间保存当前的全局步骤。
- en: '`init_op`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`init_op`'
- en: A TensorFlow op for initializing the model, such as `tf.global_variables_initializer()`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于初始化模型的TensorFlow操作，比如`tf.global_variables_initializer()`。
- en: 'The actual session is then launched:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后启动实际会话：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point the chief will initialize variables, while all other tasks wait
    for this to be completed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，chief将初始化变量，而所有其他任务等待这个过程完成。
- en: Device Placement
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设备放置
- en: The final TensorFlow mechanism we discuss in this section is *device placement*.
    While the full extent of this topic is outside the scope of this chapter, the
    overview would not be complete without a mention of this ability, which is mostly
    useful when engineering advanced systems.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中我们讨论的最终TensorFlow机制是*设备放置*。虽然这个主题的全部内容超出了本章的范围，但概述中没有提到这种能力是不完整的，这在工程高级系统时非常有用。
- en: When operating in an environment with multiple computational devices (CPUs,
    GPUs, or any combination of these), it may be useful to control where each operation
    in the computational graph is going to take place. This may be done to better
    utilize parallelism, exploit the different capabilities of different devices,
    and overcome limitations such as memory limits on some devices.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多个计算设备（CPU、GPU或这些组合）的环境中操作时，控制计算图中每个操作将发生的位置可能是有用的。这可能是为了更好地利用并行性，利用不同设备的不同能力，并克服某些设备的内存限制等限制。
- en: 'Even when you do not explicitly choose device placement, TensorFlow will output
    the placement used if required to. This is enabled while constructing the session:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您没有明确选择设备放置，TensorFlow也会在需要时输出所使用的放置。这是在构建会话时启用的：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In order to explicitly choose a device, we use:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确选择一个设备，我们使用：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `'/gpu:0'` points TensorFlow to the first GPU on the system; likewise, we
    could have used `'/cpu:0'` to place the op on the CPUs, or `'/gpu:X'` on a system
    with multiple GPU devices, where `X` is the index of the GPU we would like to
    use.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`''/gpu:0''`指向系统上的第一个GPU；同样，我们可以使用`''/cpu:0''`将操作放置在CPU上，或者在具有多个GPU设备的系统上使用`''/gpu:X''`，其中`X`是我们想要使用的GPU的索引。'
- en: 'Finally, placement across a cluster is done by pointing to the specific task.
    For instance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，跨集群的放置是通过指向特定任务来完成的。例如：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will assign to the second `worker` task, as defined in the cluster specification.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将分配给集群规范中定义的第二个`worker`任务。
- en: Placement across CPUs
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨CPU的放置
- en: By default, TensorFlow uses all the CPUs available on the system and handles
    the threading internally. For this reason, the device placement `'/cpu:0'` is
    the full CPU power, and `'/cpu:1'` doesn’t exist by default, even in a multiple-CPU
    environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorFlow使用系统上所有可用的CPU，并在内部处理线程。因此，设备放置`'/cpu:0'`是完整的CPU功率，`'/cpu:1'`默认情况下不存在，即使在多CPU环境中也是如此。
- en: 'In order to manually assign to specific CPUs (which you would need a very good
    reason to do—otherwise, let TensorFlow handle it), a session has to be defined
    with the directive to separate the CPUs:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了手动分配到特定的CPU（除非您有非常充分的理由这样做，否则让TensorFlow处理），必须使用指令定义一个会话来分离CPU：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, we define two parameters:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了两个参数：
- en: '`inter_op_parallelism_threads=8`, meaning we allow eight threads for different
    ops'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inter_op_parallelism_threads=8`，意味着我们允许八个线程用于不同的操作'
- en: '`intra_op_parallelism_threads=1`, indicating that each op gets a single thread'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intra_op_parallelism_threads=1`，表示每个操作都有一个线程'
- en: These settings would make sense for an 8-CPU system.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置对于一个8-CPU系统是有意义的。
- en: Distributed Example
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式示例
- en: 'In this section we put it all together with an end-to-end example of distributed
    training of the MNIST CNN model we saw in [Chapter 4](ch04.html#convolutional_neural_networks).
    We will use one parameter server and three worker tasks. In order to make it easily
    reproducible, we will assume all the tasks are running locally on a single machine
    (this is easily adapted to a multiple-machine setting by replacing `localhost`
    with the IP address, as described earlier). As usual, we first present the full
    code, and then break it down into elements and explain it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将所有内容整合在一起，以端到端的方式展示了我们在[第4章](ch04.html#convolutional_neural_networks)中看到的MNIST
    CNN模型的分布式训练示例。我们将使用一个参数服务器和三个工作任务。为了使其易于重现，我们将假设所有任务都在单台机器上本地运行（通过将`localhost`替换为IP地址，如前所述，可以轻松适应多机设置）。像往常一样，我们首先呈现完整的代码，然后将其分解为元素并加以解释：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In order to run this distributed example, from four different terminals we
    execute the four commands for dispatching each of the tasks (we will shortly explain
    how exactly this happens):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这个分布式示例，我们从四个不同的终端执行四个命令来分派每个任务（我们将很快解释这是如何发生的）：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Alternatively, the following will dispatch the four tasks automatically (depending
    on the system you are using, the output may all go to a single terminal or to
    four separate ones):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，以下将自动分派四个任务（取决于您使用的系统，输出可能全部发送到单个终端或四个单独的终端）：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, we go over the code in the preceding example and highlight where this
    is different from the examples we have seen thus far in the book.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查前面示例中的代码，并突出显示这与我们迄今在书中看到的示例有何不同。
- en: 'The first block deals with imports and constants:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个块处理导入和常量：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here we define:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们定义：
- en: '`BATCH_SIZE`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`BATCH_SIZE`'
- en: The number of examples to use during training in each mini-batch.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个小批次训练中要使用的示例数。
- en: '`TRAINING_STEPS`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`TRAINING_STEPS`'
- en: The total number of mini-batches we will use during training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在训练中使用的小批次总数。
- en: '`PRINT_EVERY`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`PRINT_EVERY`'
- en: How often to print diagnostic information. Since in the distributed training
    we use there is a single counter of the current step for all of the tasks, the
    `print` at a certain step will happen only from a single task.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 打印诊断信息的频率。由于在我们使用的分布式训练中，所有任务都有一个当前步骤的计数器，因此在某个步骤上的`print`只会从一个任务中发生。
- en: '`LOG_DIR`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`LOG_DIR`'
- en: The training supervisor will save logs and temporary information to this location.
    Should be emptied between runs of the program, since old info could cause the
    next session to crash.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练监督员将把日志和临时信息保存到此位置。在程序运行之间应该清空，因为旧信息可能导致下一个会话崩溃。
- en: 'Next, we define the cluster, as discussed earlier in this chapter:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义集群，如本章前面讨论的：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We run all tasks locally. In order to use multiple computers, replace `localhost`
    with the correct IP address. The ports 2222–2225 are also arbitrary, of course
    (but naturally have to be distinct when using a single machine): you might as
    well use the same port on all machines in a distributed setting.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本地运行所有任务。为了使用多台计算机，将`localhost`替换为正确的IP地址。端口2222-2225也是任意的，当然（但在使用单台机器时必须是不同的）：在分布式设置中，您可能会在所有机器上使用相同的端口。
- en: 'In the following, we use the `tf.app.flags` mechanism to define two parameters
    that we will provide through the command line when we call the program on each
    task:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们使用`tf.app.flags`机制来定义两个参数，我们将通过命令行在每个任务调用程序时提供：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The parameters are as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '`job_name`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`job_name`'
- en: This will be either `'ps'` for the single-parameter server, or `'worker'` for
    each of the worker tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是`'ps'`表示单参数服务器，或者对于每个工作任务将是`'worker'`。
- en: '`task_index`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`task_index`'
- en: The index of the task in each of the types of jobs. The parameter server will
    therefore use `task_index = 0`, and for the workers we will have `0`, `1`, and
    `2`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型工作中任务的索引。因此，参数服务器将使用`task_index = 0`，而对于工作任务，我们将有`0`，`1`和`2`。
- en: 'Now we are ready to use the identity of the current task in the cluster we
    defined in order to define the server for this current task. Note that this happens
    on each of the four tasks that we run. Each one of the four tasks knows its identity
    (`job_name`, `task_index`), as well as that of everybody else in the cluster (which
    is provided by the first argument):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用我们在本章中定义的集群中当前任务的身份来定义此当前任务的服务器。请注意，这将在我们运行的四个任务中的每一个上发生。这四个任务中的每一个都知道自己的身份（`job_name`，`task_index`），以及集群中其他每个人的身份（由第一个参数提供）：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before we start the actual training, we define our network and load the data
    to be used. This is similar to what we have done in previous examples, so we will
    not go into the details again here. We use TF-Slim for the sake of brevity:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实际训练之前，我们定义我们的网络并加载要使用的数据。这类似于我们在以前的示例中所做的，所以我们不会在这里再次详细说明。为了简洁起见，我们使用TF-Slim：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The actual processing to do during training depends of the type of task. For
    the parameter server, we want the mechanism to, well, serve parameters, for the
    most part. This entails waiting for requests and processing them. This is all
    it takes to achieve this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间要执行的实际处理取决于任务的类型。对于参数服务器，我们希望机制主要是为参数提供服务。这包括等待请求并处理它们。要实现这一点，只需要这样做：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `.join()` method of the server will not terminate even when all other tasks
    do, so this process will have to be killed externally once it is no longer needed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的`.join()`方法即使在所有其他任务终止时也不会终止，因此一旦不再需要，必须在外部终止此进程。
- en: 'In each of the worker tasks, we define the same computation graph:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个工作任务中，我们定义相同的计算图：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We use `tf.train.replica_device_setter()` in order to specify this, meaning
    that the TensorFlow Variables will be synchronized through the parameter server
    (which is the mechanism that allows us to do the distributed computations).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`tf.train.replica_device_setter()`来指定这一点，这意味着TensorFlow变量将通过参数服务器进行同步（这是允许我们进行分布式计算的机制）。
- en: The `global_step` Variable will hold the total number of steps during training
    across the tasks (each step index will occur only on a single task). This creates
    a timeline so that we can always know where we are in the grand scheme, from each
    of the tasks separately.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`global_step`变量将保存跨任务训练期间的总步数（每个步骤索引只会出现在一个任务上）。这样可以创建一个时间线，以便我们始终知道我们在整个计划中的位置，从每个任务分开。'
- en: The rest of the code is the standard setup we have seen before in numerous examples
    throughout the book.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码是我们在整本书中已经看过的许多示例中看到的标准设置。
- en: 'Next, we set up a `Supervisor` and a `managed_session`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置一个`Supervisor`和一个`managed_session`：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is similar to the regular session we use throughout, except it is able
    to handle some aspects of the distribution. The initialization of the Variables
    will be done only in a single task (the chief designated via the `is_chief` argument;
    in our case, this will be the first worker task). All other tasks will wait for
    this to happen, then continue.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于我们在整个过程中使用的常规会话，只是它能够处理分布式的一些方面。变量的初始化将仅在一个任务中完成（通过`is_chief`参数指定的首席任务；在我们的情况下，这将是第一个工作任务）。所有其他任务将等待这个任务完成，然后继续。
- en: 'With the session live, we run training:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 会话开启后，我们开始训练：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Every `PRINT_EVERY` steps, we print the current accuracy on the current mini-batch.
    This will go to 100% pretty fast. For instance, the first two rows might be:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每隔`PRINT_EVERY`步，我们打印当前小批量的当前准确率。这将很快达到100%。例如，前两行可能是：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we run the test accuracy:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行测试准确率：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that this will execute on each of the worker tasks, and thus the same exact
    output will appear three times. In order to save on computation, we could have
    run this in only a single task (for instance, in the first worker only).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这将在每个工作任务上执行，因此相同的输出将出现三次。为了节省计算资源，我们可以只在一个任务中运行这个（例如，只在第一个工作任务中）。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we covered the main concepts pertaining to parallelization in
    deep learning and machine learning in general, and concluded with an end-to-end
    example of distributed training on a cluster with data parallelization.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了关于深度学习和机器学习中并行化的主要概念，并以一个关于数据并行化集群上分布式训练的端到端示例结束。
- en: Distributed training is a very important tool that is utilized both in order
    to speed up training, and to train models that would otherwise be infeasible.
    In the next chapter we introduce the serving capabilities of TensorFlow, allowing
    trained models to be utilized in production environments.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练是一个非常重要的工具，既可以加快训练速度，也可以训练那些否则不可行的模型。在下一章中，我们将介绍TensorFlow的Serving功能，允许训练好的模型在生产环境中被利用。
