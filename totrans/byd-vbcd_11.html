<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Security, Maintainability, and Reliability"><div class="chapter" id="ch08_security_maintainability_and_reliability_1752630044621528">
<h1><span class="label">Chapter 8. </span>Security, Maintainability, and Reliability</h1>

<p>This chapter confronts a critical aspect of vibe coding and AI-assisted engineering—ensuring that the code you produce with AI assistance is secure, reliable, and maintainable.<a contenteditable="false" data-primary="security" data-type="indexterm" id="ix_secu"/> Speed and productivity mean little if the resulting software is riddled with vulnerabilities or prone to crashing.</p>

<p>First, I’ll examine common security pitfalls that arise in AI-generated code, from injection vulnerabilities to secrets leakage. You’ll learn techniques for auditing and reviewing AI-written code for such issues, effectively acting as the security safety net for your AI pair programmer.</p>

<p>Next, I’ll discuss building effective testing and QA frameworks around AI-generated code to catch bugs and reliability issues early. Performance considerations will also be covered. AI might write correct code, but it’s not always the most efficient code, so I’ll outline how to identify and optimize performance bottlenecks. I’ll also explore strategies to ensure maintainability, such as enforcing consistent styles or refactoring AI code, since AI suggestions can sometimes be inconsistent or overly verbose.</p>

<p>I’ll show you how to adapt your code-review practices to an AI-assisted workflow, highlighting what human reviewers should focus on when reviewing code that was partially or wholly machine-generated. Finally, I’ll round up best practices for deploying AI-assisted projects with confidence, from continuous integration pipelines to monitoring in production. By the end of this chapter, you’ll have a toolkit of approaches to keep your AI-accelerated development safe and robust.</p>

<section data-type="sect1" data-pdf-bookmark="Common Security Vulnerabilities in AI-Generated Code"><div class="sect1" id="ch08_common_security_vulnerabilities_in_ai_generated_co_1752630044621750">
<h1>Common Security Vulnerabilities in AI-Generated Code</h1>

<p>AI coding assistants, while powerful, can inadvertently introduce security issues if not guided properly.<a contenteditable="false" data-primary="security" data-secondary="common vulnerabilities in AI-generated code" data-type="indexterm" id="ix_secuAIvul"/> They learn from lots of public code—which includes both good and bad practices—and may regurgitate insecure patterns if the prompt or context doesn’t steer them away. It’s vital for you to know these common pitfalls so you can spot and fix them. This can include using both manual and automated means to detect potential security issues (see <a data-type="xref" href="#ch08_figure_1_1752630044605613">Figure 8-1</a>).</p>

<figure><div id="ch08_figure_1_1752630044605613" class="figure"><img src="assets/bevc_0801.png" width="753" height="463"/>
<h6><span class="label">Figure 8-1. </span>AI-introduced security vulnerabilities: AI-generated code may contain subtle security flaws that require careful review and automated security scanning to identify and remediate.</h6>
</div></figure>

<p>Some typical security issues observed in AI-generated code include:</p>

<dl>
	<dt>Hard-coded secrets or credentials</dt>
	<dd>
	<p>Sometimes <a contenteditable="false" data-primary="credentials" data-secondary="hard-coded, in AI-generated code" data-type="indexterm" id="id853"/><a contenteditable="false" data-primary="secrets or credentials, hard-coded, in AI-generated code" data-type="indexterm" id="id854"/>AI outputs <a contenteditable="false" data-primary="credentials, hard-coded, in AI-generated code" data-type="indexterm" id="id855"/>API keys, passwords, or tokens in code, especially if similar examples were in its training data. For instance, if you ask it to integrate with AWS, it might put a dummy AWS secret key directly in the code. This is dangerous if left in—it could leak sensitive info if the code is shared. Always ensure that secrets are properly managed via environment variables or config files. If an AI suggests something like <code>api_key = "ABC123SECRET"</code>, treat it as a flag—real keys should not be in source code.</p>
	</dd>
	<dt>SQL injection vulnerabilities</dt>
	<dd>
	<p>If you have your <a contenteditable="false" data-primary="SQL injection vulnerabilities in AI-generated code" data-type="indexterm" id="id856"/>AI model generate SQL queries or ORM usage, check that it’s not constructing queries by concatenating user input directly. For example, an insecure pattern would be:</p>

	<pre data-type="programlisting">sql = "SELECT * FROM users WHERE name = '" + username + "'";</pre>

	<p>This is susceptible to injection attacks. An AI might produce this if you don’t specifically tell it to parameterize queries. Always use prepared statements or parameter binding. Many AI assistants will do so if they recall best practices (like using <code>?</code> or placeholders for user inputs in SQL), but it’s not guaranteed. It’s on you to verify and ask the AI to fix it if needed:</p>

	<pre data-type="programlisting">Modify this query to use parameters to prevent SQL injection.</pre>
	</dd>
	<dt>Cross-site scripting (XSS) in web apps</dt>
	<dd>
	<p>When generating web code, AI tools don’t always automatically escape user input in outputs.<a contenteditable="false" data-primary="XSS (cross-site scripting) in web apps" data-type="indexterm" id="id857"/><a contenteditable="false" data-primary="cross-site scripting (XSS) in web apps" data-type="indexterm" id="id858"/> For example, your AI might produce a templating snippet that directly inserts <code>{{comment.text}}</code> into HTML without escaping, which could allow a malicious script placed in a comment to run. If using frameworks, AIs often escape by default, but if they’re handling raw HTML construction, be careful. Implement output encoding or sanitization routines. You can prompt the AI:</p>

	<pre data-type="programlisting">Add sanitization for user inputs to prevent XSS.</pre>

	<p>Many modern frameworks have built-in mechanisms, so ensure that the AI uses them, like <code>innerText</code> versus <code>innerHTML</code> in <a href="https://oreil.ly/5o_2x">Document Object Model (DOM) manipulation</a>.</p>
	</dd>
	<dt>Improper authentication and authorization</dt>
	<dd>
	<p>AIs can write <a contenteditable="false" data-primary="authentication" data-secondary="improper, in AI-generated code" data-type="indexterm" id="id859"/>authentication flows, but subtle mistakes might creep in: for instance, generating a <a href="https://oreil.ly/rf7JL">JSON Web Token (JWT)</a> without a sufficiently strong secret or not checking a password hash correctly.</p>

	<p>The same is true for authorization: an <a contenteditable="false" data-primary="authorization, improper, in AI-generated code" data-type="indexterm" id="id860"/>AI might not automatically enforce that an action (like deleting a resource) is limited to the user who owns that resource. These logic issues are hard to catch automatically—they require thinking through the security model. When writing such code, specify clearly:</p>

	<pre data-type="programlisting">Ensure that only the owner of the resource can delete it. Add checks for user ID.</pre>

	<p>Then test those conditions. It’s easy for an AI to omit a check because it doesn’t truly “understand” the context unless told.</p>
	</dd>
	<dt>Insecure defaults or configurations</dt>
	<dd>
	<p>AI might choose <a contenteditable="false" data-primary="configurations, insecure, in AI-generated code" data-type="indexterm" id="id861"/>convenience <a contenteditable="false" data-primary="defaults, insecure, in AI-generated code" data-type="indexterm" id="id862"/>over security unless prompted to do otherwise. Examples include:</p>

	<ul>
		<li>
		<p>Using HTTP instead of HTTPS for API calls (if TLS is not specified)</p>
		</li>
		<li>
		<p>Not validating SSL certificates (some code examples on the internet use <span class="keep-together"><code>verify=false</code></span> in requests, which AI might copy)</p>
		</li>
		<li>
		<p>Widely enabling CORS for all origins and methods without restriction (potentially opening the app to any cross-origin requests)</p>
		</li>
		<li>
		<p>Choosing outdated cryptography (like MD5 or SHA1 for hashes, which are weak, instead of SHA-256/Bcrypt/Argon2 for passwords)</p>
		</li>
	</ul>

	<p>These issues are often subtle, which is one reason it’s good to audit your configuration files and initialization code. If the AI sets up something like <code>app.UseCors(allowAll)</code> or chooses an old cipher, you should spot that and correct it.</p>
	</dd>
	<dt>Error handling revealing sensitive info</dt>
	<dd>
	<p>AI-generated <a contenteditable="false" data-primary="error handling" data-secondary="revealing sensitive information in AI-generated code" data-type="indexterm" id="id863"/>error handling might print or return stack traces. For example, a Node.js API might catch an error and do <code>res.send(err.toString())</code>, which could leak internal details. Ensure that error messages to users are sanitized and logs are properly handled. Adjust as needed to avoid giving attackers clues like full error messages or file paths.</p>
	</dd>
	<dt>Dependency management and updates</dt>
	<dd>
	<p>If the AI adds<a contenteditable="false" data-primary="dependency management and updates, security vulnerability in AI-generated code" data-type="indexterm" id="id864"/> dependencies (such as libraries) to your project, ensure that they’re up to date and from reputable sources.<a contenteditable="false" data-primary="updates to dependencies added in AI-generated code" data-type="indexterm" id="id865"/> An AI might pick a library that was popular in its training data, but that is no longer maintained or has known vulnerabilities. For instance, if it suggests using an older version of a package, you should bump it to the latest stable. Running <code>npm audit</code> or equivalent after generation is wise too. Or ask the AI:</p>
<blockquote>
		<p>Is this library still maintained and secure?</p>
</blockquote>

	<p>It might not fully know, but it could tell you if there’s a known deprecation.</p>
	</dd>
</dl>

<p>A 2023 large-scale analysis of GitHub Copilot in real-world projects revealed that as much as 25%–33% of generated code—depending on language—contained potential security weaknesses, including high-severity CWEs such as command injection, code injection, and <a href="https://arxiv.org/abs/2310.02059">cross-site scripting</a>. These findings underscore that Copilot reflects insecure patterns present in its training data, as opposed to intentionally producing flawed code. The consistent recommendation? Developers must stay alert: manually review AI-generated code, use security-aware tooling, and maintain strict code hygiene. Especially during “vibe coding,” the speed and scope of AI-generated content demand even more vigilance. More code in less time means more surface area to audit.</p>

<p>Let’s look at a short example.</p>

<section data-type="sect2" data-pdf-bookmark="Improper Authentication and Authorization"><div class="sect2" id="ch08_improper_authentication_and_authorization_1752630044621817">
<h2>Improper Authentication and Authorization</h2>

<p>Imagine you ask an AI to create a login <a contenteditable="false" data-primary="security" data-secondary="common vulnerabilities in AI-generated code" data-tertiary="improper authentication and authorization" data-type="indexterm" id="id866"/>route in an Express app.<a contenteditable="false" data-primary="authentication" data-secondary="improper, in AI-generated code" data-type="indexterm" id="id867"/> It might produce something like this:</p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="c1">// Insecure example</code>
<code class="nx">app</code><code class="p">.</code><code class="nx">post</code><code class="p">(</code><code class="s1">'/login'</code><code class="p">,</code> <code class="nx">async</code> <code class="p">(</code><code class="nx">req</code><code class="p">,</code> <code class="nx">res</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="p">{</code> <code class="nx">username</code><code class="p">,</code> <code class="nx">password</code> <code class="p">}</code> <code class="o">=</code> <code class="nx">req</code><code class="p">.</code><code class="nx">body</code><code class="p">;</code>
  <code class="kr">const</code> <code class="nx">user</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">Users</code><code class="p">.</code><code class="nx">findOne</code><code class="p">({</code> <code class="nx">username</code><code class="o">:</code> <code class="nx">username</code> <code class="p">});</code>
  <code class="k">if</code> <code class="p">(</code><code class="o">!</code><code class="nx">user</code><code class="p">)</code> <code class="k">return</code> <code class="nx">res</code><code class="p">.</code><code class="nx">status</code><code class="p">(</code><code class="mi">401</code><code class="p">).</code><code class="nx">send</code><code class="p">(</code><code class="s2">"No such user"</code><code class="p">);</code>
  <code class="k">if</code> <code class="p">(</code><code class="nx">user</code><code class="p">.</code><code class="nx">password</code> <code class="o">===</code> <code class="nx">password</code><code class="p">)</code> <code class="p">{</code> <code class="c1">// plain text password comparison</code>
    <code class="nx">res</code><code class="p">.</code><code class="nx">send</code><code class="p">(</code><code class="s2">"Login successful!"</code><code class="p">);</code>
  <code class="p">}</code> <code class="k">else</code> <code class="p">{</code>
    <code class="nx">res</code><code class="p">.</code><code class="nx">status</code><code class="p">(</code><code class="mi">401</code><code class="p">).</code><code class="nx">send</code><code class="p">(</code><code class="s2">"Incorrect password"</code><code class="p">);</code>
  <code class="p">}</code>
<code class="p">});</code></pre>

<p>What are the issues here?</p>

<ul>
	<li>
	<p>It compares passwords directly, implying that the password is stored in plain text in the database—a big no-no.</p>
	</li>
	<li>
	<p>It sends very generic responses, which may be appropriate for security but could also inadvertently expose sensitive information.</p>
	</li>
</ul>

<p>Consider authentication  error messages as a critical example.<a contenteditable="false" data-primary="error handling" data-secondary="revealing sensitive information in AI-generated code" data-type="indexterm" id="id868"/> A properly secure system should return a generic message like “Invalid credentials” when login fails, regardless of whether the username or password was incorrect. However, AI-generated code might produce more specific errors such as “Username not found” or “Incorrect password for this user.”</p>

<p>These specific messages create a security <a contenteditable="false" data-primary="passwords" data-secondary="security vulnerabilities in AI-generated code" data-type="indexterm" id="id869"/>vulnerability by confirming to potential attackers which piece of information they have correct. If an attacker receives “Incorrect password” as an error, they now know they have discovered a valid username in your system. This enables them to build a list of legitimate usernames through repeated attempts, then focus their efforts on cracking passwords for those confirmed accounts.<a contenteditable="false" data-primary="user enumeration" data-type="indexterm" id="id870"/> This technique, known as <em>user enumeration</em>, transforms a guessing game into a more targeted attack. The AI’s tendency toward helpful, specific error messages inadvertently aids malicious actors unless you explicitly instruct it to maintain appropriately vague responses for security-sensitive operations.</p>

<p>While it isn’t shown in the code, no rate limiting<a contenteditable="false" data-primary="rate limiting or throttling mechanisms to prevent brute force attacks" data-type="indexterm" id="id871"/> or other protections are included here.</p>

<p>A secure implementation would incorporate several critical safeguards:</p>

<ul>
	<li>
	<p>First and foremost, passwords should never be stored in plain text.<a contenteditable="false" data-primary="passwords" data-secondary="cryptographically hashed" data-type="indexterm" id="id872"/> Instead, the system should store cryptographically hashed passwords and use a secure comparison function to verify login attempts.</p>
	</li>
	<li>
	<p>This comparison process must employ constant-time algorithms to prevent timing attacks, where attackers analyze response times to deduce information about password correctness.<a contenteditable="false" data-primary="timing attacks" data-type="indexterm" id="id873"/> Many security-focused libraries provide these constant-time comparison functions specifically to address this vulnerability.</p>
	</li>
	<li>
	<p>Additionally, the authentication system should implement rate limiting or throttling mechanisms to prevent brute-force attacks. Failed login attempts should be logged for security monitoring, allowing administrators to detect and respond to suspicious patterns. These measures work together to create a defense-in-depth approach that protects user credentials even if one security layer is compromised.<a contenteditable="false" data-primary="passwords" data-secondary="using bcrypt to hash and compare in AI-generated code" data-type="indexterm" id="id874"/><a contenteditable="false" data-primary="bcrypt, using to hash and compare passwords" data-type="indexterm" id="id875"/></p>
	</li>
</ul>

<p class="pagebreak-before less_space">You can ask AI to help fix this:</p>

<blockquote>
<p>Improve the login route to use bcrypt to hash and compare passwords, and ensure the password in the database is hashed.</p>
</blockquote>

<p>It might then output:</p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">bcrypt</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s1">'bcrypt'</code><code class="p">);</code>

<code class="nx">app</code><code class="p">.</code><code class="nx">post</code><code class="p">(</code><code class="s1">'/login'</code><code class="p">,</code> <code class="nx">async</code> <code class="p">(</code><code class="nx">req</code><code class="p">,</code> <code class="nx">res</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="p">{</code> <code class="nx">username</code><code class="p">,</code> <code class="nx">password</code> <code class="p">}</code> <code class="o">=</code> <code class="nx">req</code><code class="p">.</code><code class="nx">body</code><code class="p">;</code>
  <code class="kr">const</code> <code class="nx">user</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">Users</code><code class="p">.</code><code class="nx">findOne</code><code class="p">({</code> <code class="nx">username</code> <code class="p">});</code>
  <code class="k">if</code> <code class="p">(</code><code class="o">!</code><code class="nx">user</code><code class="p">)</code> <code class="p">{</code>
    <code class="k">return</code> <code class="nx">res</code><code class="p">.</code><code class="nx">status</code><code class="p">(</code><code class="mi">401</code><code class="p">).</code><code class="nx">send</code><code class="p">(</code><code class="s2">"Invalid credentials"</code><code class="p">);</code>
  <code class="p">}</code>
  <code class="kr">const</code> <code class="nx">match</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">bcrypt</code><code class="p">.</code><code class="nx">compare</code><code class="p">(</code><code class="nx">password</code><code class="p">,</code> <code class="nx">user</code><code class="p">.</code><code class="nx">passwordHash</code><code class="p">);</code>
  <code class="k">if</code> <code class="p">(</code><code class="o">!</code><code class="nx">match</code><code class="p">)</code> <code class="p">{</code>
    <code class="k">return</code> <code class="nx">res</code><code class="p">.</code><code class="nx">status</code><code class="p">(</code><code class="mi">401</code><code class="p">).</code><code class="nx">send</code><code class="p">(</code><code class="s2">"Invalid credentials"</code><code class="p">);</code>
  <code class="p">}</code>
  <code class="nx">res</code><code class="p">.</code><code class="nx">send</code><code class="p">(</code><code class="s2">"Login successful!"</code><code class="p">);</code>
<code class="p">});</code></pre>

<p>This is better: it uses <code>bcrypt</code> to compare against a hashed password (assuming the variable <code>user.passwordHash</code> stores that). When creating users, you’d also want to make sure to use <code>bcrypt.hash</code> to hash their passwords.</p>

<p>With a bit of guidance, the AI can do the right thing, but its initial naive output might well be insecure. This underscores the pattern: <em>review and refine</em>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Package Management Issues"><div class="sect2" id="ch08_package_management_issues_1752630044621879">
<h2>Package Management Issues</h2>

<p>Another common vulnerability category is package management.<a contenteditable="false" data-primary="package management issues in AI-generated code" data-type="indexterm" id="id876"/><a contenteditable="false" data-primary="security" data-secondary="common vulnerabilities in AI-generated code" data-tertiary="package management issues" data-type="indexterm" id="id877"/> AI sometimes invents a library <a contenteditable="false" data-primary="package hallucination" data-type="indexterm" id="id878"/>or misremembers a name, a problem known as <em>package hallucination</em>. Such a package might not exist, but an attacker could, theoretically, publish packages under commonly hallucinated names that contain malicious code. If you install such a package without confirming that it both exists and is the correct package, you could be introducing serious risk. If you’re not sure about a particular package, try a quick web search or check npm/PyPI directly.</p>

<p>Additionally, the AI might inadvertently produce code that is identical to a licensed snippet from training data.<a contenteditable="false" data-primary="licensing considerations in AI code" data-type="indexterm" id="id879"/> This is more an intellectual property concern than a security issue, but it warrants careful attention.<a contenteditable="false" data-primary="intellectual property, considerations with AI-generated code" data-type="indexterm" id="id880"/> GitHub Copilot, for instance, includes a duplicate detection feature that can flag when generated code closely matches public repositories, helping developers avoid potential licensing conflicts. Similar tools are emerging to address this specific challenge of AI-generated code provenance. <a data-type="xref" href="ch09.html#ch09_the_ethical_implications_of_vibe_coding_1752630044848930">Chapter 9</a> will delve into licensing and intellectual property considerations in more detail, providing comprehensive guidance on navigating these complex issues.</p>

<p>In summary, the main message remains—and yes, I realize I’ve emphasized this point throughout the book to the point where you could probably recite it in your sleep—that <em>AI output requires the same careful review you would apply to a junior developer’s code</em>. The repetition is intentional, because this principle underpins virtually every aspect of safe and effective AI-assisted development. Whether you’re prototyping, building backends, or implementing security features, this mental model provides the right balance of trust and verification to make AI a powerful ally rather than a risky shortcut. It can write a lot of code fast, but you need to instill security best practices into it and double-check for vulnerabilities. Novelist Frank Herbert put it this way in an <a href="https://oreil.ly/yr2B_">often-quoted</a> line from <em>God Emperor of Dune</em> (Putnam, 1981): “They increase the number of things we can do without thinking. Things we do without thinking—there’s the real danger.”</p>

<p>Using AI can lull you into doing less thinking about routine code, and you should be consciously thinking about how to apply a security-review mindset. It’s crucial for <a contenteditable="false" data-primary="security" data-secondary="common vulnerabilities in AI-generated code" data-startref="ix_secuAIvul" data-type="indexterm" id="id881"/>catching those “things we can do without thinking.”</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Security Audits"><div class="sect1" id="ch08_security_audits_1752630044621935">
<h1>Security Audits</h1>

<p>Given the types of vulnerabilities outlined, how can you effectively audit and secure our AI-generated code? <a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-type="indexterm" id="ix_secaudi"/>This section looks at several techniques and tools you can employ.</p>

<section data-type="sect2" data-pdf-bookmark="Leverage Automated Security Scanners"><div class="sect2" id="ch08_leverage_automated_security_scanners_1752630044621989">
<h2>Leverage Automated Security Scanners</h2>

<p>Static analysis tools (SASTs) can scan your <a contenteditable="false" data-primary="SAST (static analysis tools)" data-type="indexterm" id="id882"/>code for <a contenteditable="false" data-primary="static analysis tools (SAST)" data-type="indexterm" id="id883"/>known <a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="using automated security scanners" data-type="indexterm" id="id884"/>vulnerability patterns; for example:</p>

<ul>
	<li>
	<p><a href="https://oreil.ly/55ppH">ESLint + security plug-ins</a> can detect insecure functions or unsanitized input in JavaScript and Node code.<a contenteditable="false" data-primary="ESLint and security plug-ins" data-type="indexterm" id="id885"/></p>
	</li>
	<li>
	<p><a href="https://bandit.readthedocs.io">Bandit</a> for Python can flag uses of assert in production, weak cryptography, hard-coded secrets, and more.<a contenteditable="false" data-primary="Bandit (security scanner)" data-type="indexterm" id="id886"/></p>
	</li>
	<li>
	<p><a href="https://github.com/github/codeql">GitHub CodeQL</a> lets you run queries across your codebase to find SQL injection, XSS, and other common patterns.<a contenteditable="false" data-primary="GitHub CodeQL" data-type="indexterm" id="id887"/></p>
	</li>
	<li>
	<p><a href="https://semgrep.dev">Semgrep</a> has rules for many languages, including community-maintained ones for JavaScript, Python, Java, Go, and more, and can spot top issues out of the box.<a contenteditable="false" data-primary="Semgrep" data-type="indexterm" id="id888"/></p>
	</li>
</ul>

<p>You can integrate these tools into your CI/CD or dev pipelines. Run them on your AI-generated code—it won’t catch everything, but it will probably flag the obvious mistakes (e.g., plain-text password checks, unsanitized SQL, insecure crypto). It’s a solid safety net.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Use a Separate AI as a Reviewer"><div class="sect2" id="ch08_use_a_separate_ai_as_a_reviewer_1752630044622038">
<h2>Use a Separate AI as a Reviewer</h2>

<p>Two distinct approaches can leverage AI for security review of generated code, each with unique advantages. <a contenteditable="false" data-primary="code reviews" data-secondary="using separate AI as reviewer for security of AI code" data-type="indexterm" id="id889"/><a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="using separate AI as reviewer" data-type="indexterm" id="id890"/>The first involves using the same AI model that generated the code, asking it to switch perspectives and audit its own output. After generating code, you can prompt the model with something like this:</p>

<blockquote>
<p>Review this code for security vulnerabilities and explain any issues you find.</p>
</blockquote>

<p>This approach often yields surprisingly effective results, as the model can identify common security problems such as plain-text password storage, missing input validation, or potential SQL injection vulnerabilities.</p>

<p>The second approach employs a different AI model as an independent reviewer. For instance, if you generated code using ChatGPT, you might paste that code into Claude or Gemini for security analysis. This cross-model review can surface different perspectives and catch issues the original model might have overlooked, much like how different security tools or human reviewers bring varying expertise and focus areas. Different models may have been trained with different emphases or datasets, potentially catching distinct categories of vulnerabilities.</p>

<p>Both techniques serve as valuable additional layers of security review, complementing but never replacing proper security testing and human expertise. While AI reviewers may occasionally flag false positives or miss subtle vulnerabilities, they excel at catching common security antipatterns quickly. Think of this process as automated pair programming focused specifically on security considerations. The key lies in treating these AI-generated security reviews as another input to your security assessment process rather than as definitive security clearance.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Perform a Human Code Review with a Security Checklist"><div class="sect2" id="ch08_perform_a_human_code_review_with_a_security_checkl_1752630044622090">
<h2>Perform a Human Code Review with a Security Checklist</h2>

<p>If you’re <a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="performing human review for security" data-type="indexterm" id="id891"/>in a team, have a <a contenteditable="false" data-primary="code reviews" data-secondary="performing human code review with security checklist" data-type="indexterm" id="id892"/>checklist for <a contenteditable="false" data-primary="human oversight of AI-assisted coding" data-secondary="performing human code review for security" data-type="indexterm" id="id893"/>reviewing code with an eye to security. AI often produces code that “works” for the expected case but isn’t hardened to deal with malicious cases. For AI-generated code, be sure to consider:</p>

<ul class="checkbox">
	<li>
	<p>Authentication flows: Are they solid?</p>
	</li>
	<li>
	<p>Any place data enters the system: Are we validating inputs?</p>
	</li>
	<li>
	<p>Any place data leaves the system: Are we sanitizing outputs? Are we protecting sensitive data?</p>
	</li>
	<li>
	<p>Use of external APIs: Are we handling failures? Are we exposing keys?</p>
	</li>
	<li>
	<p>Database access: Are we using ORMs safely? Are we using parameterized <span class="keep-together">queries</span>?</p>
	</li>
	<li>
	<p>Memory management in low-level code: If AI is writing C/C++ or Rust, are there overflows? Is there any misuse?</p>
	</li>
</ul>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Penetration Testing and Fuzzing"><div class="sect2" id="ch08_penetration_testing_and_fuzzing_1752630044622139">
<h2>Penetration Testing and Fuzzing</h2>

<p>Use dynamic approaches. <a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="penetrating testing and fuzzing" data-type="indexterm" id="id894"/>For fuzz testing, feed random or specially crafted inputs into your functions or endpoints to see if they break or do weird things.<a contenteditable="false" data-primary="fuzz testing of AI-generated code" data-type="indexterm" id="id895"/><a contenteditable="false" data-primary="penetration testing of AI-generated code" data-type="indexterm" id="id896"/> AI can help generate fuzz cases, or you can use <a href="https://oreil.ly/OoFzT">existing fuzz tools</a>, such as <a href="https://oreil.ly/FvKSU">OSS Fuzz by Google</a>.</p>

<p>Running penetration-testing tools like OWASP’s ZAP <a contenteditable="false" data-primary="ZAP (OWASP), penetrating testing with" data-type="indexterm" id="id897"/>against <a contenteditable="false" data-primary="OWASP’s ZAP" data-type="indexterm" id="id898"/>your AI-made web app can automate scanning for things like XSS and SQL injection vulnerabilities. For example, ZAP might attempt to inject a script and get it reflected, and detect that a certain input isn’t sanitized.</p>

<p>If you’re building an API, tools like Postman <a contenteditable="false" data-primary="APIs" data-secondary="penetration and fuzzing tests on" data-type="indexterm" id="id899"/>or custom scripts can try sending ill-formed data to see how the system behaves: does it throw a 500 error or handle errors gracefully?</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Add Security-Focused Unit Tests"><div class="sect2" id="ch08_add_security_focused_unit_tests_1752630044622187">
<h2>Add Security-Focused Unit Tests</h2>

<p>For critical pieces of code, write tests that assert security properties.<a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="security-focused unit tests" data-type="indexterm" id="id900"/><a contenteditable="false" data-primary="unit tests" data-secondary="security-focused, adding to AI-generated code" data-type="indexterm" id="id901"/> For instance, you might test that your login rate limiter triggers after X bad attempts, or that certain inputs (like <code>"&lt;script&gt;alert(1)&lt;/script&gt;"</code>) come out escaped in the response. To test that unauthorized users cannot access a protected resource, simulate both authorized and unauthorized calls and ensure the app behaves correctly.</p>

<p>You can ask the AI to help generate these tests:</p>

<blockquote>
<p>Write tests to ensure an unauthorized user gets 403 on the /deleteUser endpoint.</p>
</blockquote>

<p>And then run the tests.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Provide Updates to Compensate for Training Cutoffs"><div class="sect2" id="ch08_provide_updates_to_compensate_for_training_cutoffs_1752630044622236">
<h2>Provide Updates to Compensate for Training Cutoffs</h2>

<p>AI models possess a fundamental limitation that directly impacts security: their knowledge freezes at a specific point in time.<a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="updates of AI model training" data-type="indexterm" id="id902"/><a contenteditable="false" data-primary="models (AI)" data-secondary="providing updates to compensate for training cutoffs" data-type="indexterm" id="id903"/> When a model completes training, it cannot learn about vulnerabilities discovered afterward, security patches released subsequently, or new best practices that emerge. This knowledge cutoff creates a critical gap between what the AI knows and current security standards.</p>

<p>Consider a model trained in 2023 generating code in 2025. During those intervening years, numerous security vulnerabilities have been discovered, patched, and documented. New attack vectors have emerged, frameworks have added security features, and best practices have evolved. The AI, however, remains unaware of these developments unless you explicitly provide updated information within your prompts.</p>

<p>This limitation becomes particularly acute with rapidly evolving security standards and vulnerability databases. The <a href="https://oreil.ly/US-uh">OWASP Top 10</a>, for instance, undergoes periodic updates to reflect the changing threat landscape. <a contenteditable="false" data-primary="OWASP Top 10 security vulnerabilities" data-type="indexterm" id="id904"/>If you prompt an AI to “write a secure file upload function,” it might implement reasonable protections based on its training data—perhaps including file type validation, size limits, and storage outside the web root. However, it could miss recently discovered attack vectors or fail to implement newly recommended mitigations.</p>

<p>The solution involves actively supplementing the AI’s knowledge with current security information. When requesting security-sensitive code, include references to current best practices in your prompts. For example, rather than simply asking for secure code, you might prompt:</p>

<blockquote>
<p>Write a file upload function that addresses the security concerns in the 2025 OWASP Top 10, particularly focusing on injection attacks and server-side request forgery.</p>
</blockquote>

<p>This approach grounds the AI’s response in current security standards rather than potentially outdated training data.</p>

<p>Similarly, framework-specific security features often emerge after an AI’s training cutoff. Express.js applications, for instance, benefit significantly from the <a href="https://oreil.ly/WSPar">Helmet middleware</a> for setting security headers.<a contenteditable="false" data-primary="Helmet middleware" data-type="indexterm" id="id905"/> An AI trained before Helmet became standard practice might generate Express applications without this crucial security layer. By explicitly mentioning current security tools and practices in your prompts, you help the AI generate code that aligns with contemporary security standards rather than historical ones.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Optimize Your Logging Practices"><div class="sect2" id="ch08_optimize_your_logging_practices_1752630044622285">
<h2>Optimize Your Logging Practices</h2>

<p>Ensure that the code (AI and human) has good logging, especially around critical operations or potential failure points. <a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="optimizing logging practices" data-type="indexterm" id="id906"/><a contenteditable="false" data-primary="logging" data-secondary="optimizing practices for security purposes" data-type="indexterm" id="id907"/>This helps in debugging issues in production. If an AI wrote a section with minimal logs, consider adding more. For example, if there’s an AI-generated catch block that just swallows an error, change it to log the error (and maybe some context) for visibility. Also, sanitize the logs so they contain no sensitive info.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Use Updated Models or Tools with a Security Focus"><div class="sect2" id="ch08_use_updated_models_or_tools_with_a_security_focus_1752630044622335">
<h2>Use Updated Models or Tools with a Security Focus</h2>

<p>Some AI coding tools aim to blend code generation with built-in security scanning. Snyk is a prime example: it uses a <a href="https://oreil.ly/0ZGFv">hybrid approach</a> combining LLM-generated suggestions with rule-based taint analysis. <a contenteditable="false" data-primary="Snyk" data-type="indexterm" id="id908"/><a contenteditable="false" data-primary="tools" data-secondary="using updated AI-powered tools with security focus" data-type="indexterm" id="id909"/><a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="using updated models or tools with security focus" data-type="indexterm" id="id910"/><a contenteditable="false" data-primary="models (AI)" data-secondary="updated models with security focus, using" data-type="indexterm" id="id911"/>According to Snyk, when you request code (even from LLM libraries like OpenAI, Anthropic, or Hugging Face), Snyk Code tracks potentially unsafe data flows and flags untrusted inputs before they reach sensitive sinks. In practice, that means if an AI suggests a database query, Snyk ensures it’s parameterized, preventing SQL injection—even if you forget to do so yourself. This kind of tool is particularly useful because it works to avoid introducing insecure code through AI-generated suggestions.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Pay Attention to Warnings in Context"><div class="sect2" id="ch08_pay_attention_to_warnings_in_context_1752630044622384">
<h2>Pay Attention to Warnings in Context</h2>

<p>If you’re using an IDE, often you’ll see warnings or squiggly lines to highlight suspicious code.<a contenteditable="false" data-primary="IDEs (integrated development environments)" data-secondary="modern IDEs with IntelliSense, warnings and flags from" data-type="indexterm" id="id912"/><a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="paying attention to warnings in context" data-type="indexterm" id="id913"/> Modern IDEs with IntelliSense can sometimes catch, for instance, a string concatenation of SQL that looks suspicious. Don’t ignore those warnings and flags just because the AI writes them—address the issue. The AI doesn’t have the benefit of those real-time warnings when generating the code.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Slow Down"><div class="sect2" id="ch08_slow_down_1752630044622431">
<h2>Slow Down</h2>

<p>After using AI to generate a lot of code quickly, shift gears and <em>slow down</em> when it’s time for auditing. <a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-tertiary="slowing down for audits" data-type="indexterm" id="id914"/>When you can produce features fast, it’s tempting to chase the next one, but schedule time for a thorough review. Think of it as “AI-accelerated development, human-accelerated security.” Snyk’s <a href="https://oreil.ly/uUExW">best practices</a> recommend scanning AI code right in the IDE, and caution against letting AI’s speed outpace your security checks. <a contenteditable="false" data-primary="Snyk" data-secondary="best practices for security audits of AI-generated code" data-type="indexterm" id="id915"/>In other words, integrate security scanning into your dev loop, so you can catch vulnerabilities as soon as the code is written.</p>

<p>In summary, when you audit AI-generated code, you’ll use many of the same tools you use in traditional development—static analysis, dynamic testing, code review—but you might apply them more frequently, because code is produced more quickly. <em>Treat every AI output as needing inspection</em>.<a contenteditable="false" data-primary="security" data-secondary="audits of AI-generated code" data-startref="ix_secaudi" data-type="indexterm" id="id916"/><a contenteditable="false" data-primary="security" data-startref="ix_secu" data-type="indexterm" id="id917"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Building Effective Testing Frameworks for AI-Generated Systems"><div class="sect1" id="ch08_building_effective_testing_frameworks_for_ai_gener_1752630044622483">
<h1>Building Effective Testing Frameworks <span class="keep-together">for AI-Generated Systems</span></h1>

<p>While security forms one pillar of reliability, the broader concept encompasses the fundamental dependability of your software system.<a contenteditable="false" data-primary="testing" data-secondary="building effective frameworks for AI-generated systems" data-type="indexterm" id="ix_tstbldfrm"/> <em>Reliability</em>, in software architecture terms, addresses critical questions about system failure and its consequences.<a contenteditable="false" data-primary="reliability" data-type="indexterm" id="id918"/> Does your system need to be fail-safe? Is it mission critical in ways that could affect human lives or safety? If the system fails, will it result in significant financial losses for your organization? These considerations determine the rigor required in your development and testing practices.</p>

<p>When you’re building with AI assistance, these reliability stakes remain unchanged. A banking application generated with AI assistance carries the same requirements for transaction accuracy and data integrity as one written entirely by humans. A healthcare system must meet identical standards for patient safety regardless of how its code originated. The AI’s involvement in code generation does not diminish these fundamental reliability requirements.</p>

<p>This reality underscores why comprehensive testing becomes even more critical in AI-assisted development. A strong testing framework ensures that your code performs its intended functions correctly and maintains that correctness as the project evolves. While testing AI-generated code follows the same fundamental principles as testing human-written code, certain nuances and opportunities emerge from the AI development process that warrant specific attention.</p>

<p>The following sections explore how to leverage AI not just in generating code but in creating robust test suites that validate reliability, maintain system stability, and provide confidence that your software will perform correctly when the stakes are highest.</p>

<p>First, embrace automated testing early and often. It’s easy to skip writing tests when development is slow because you want to push features. Ironically, when development is <em>fast</em> (with AI), it’s <em>also</em> easy to skip tests, because new features keep coming at you. But when code is churned out rapidly, that’s precisely when you most need tests to catch regression or integration issues. So after implementing a feature with AI help, get into the habit of immediately writing tests for it (or even using AI to write those tests). This verifies the feature and also guards it as you change things later.</p>

<p>A <a href="https://oreil.ly/Vc8Gd">2022 study</a> found that developers who were using an AI assistant were <em>more confident</em> in the security of the code they wrote even when it was objectively less secure than code written by those without AI assistance. You need to counteract that overconfidence with actual tests.</p>

<p>As I noted in <a data-type="xref" href="ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362">Chapter 4</a>, you can use the AI not just to generate the code but also to produce a suite of tests. This way, AI helps double-check itself. <a contenteditable="false" data-primary="unit tests" data-secondary="writing with AI to test AI-generated code" data-type="indexterm" id="id919"/>It’s like having it do both the implementation and an initial pass at validation. For example, after writing a new module, you could ask:</p>

<blockquote>
<p>Write unit tests for this module, covering edge cases.</p>
</blockquote>

<p>If they pass, great. If they fail, either there’s a bug or the tests expected something else. Investigate and fix either code or test as appropriate.</p>

<p>Be cautious that the AI may assume some output or behavior incorrectly; treat its tests, like its code, as suggestions, not the ground truth. You might need to adjust the test’s expectations to match the intended behavior—but even that process is valuable, because it forces you to define the intended behavior clearly.</p>

<p>Incorporate your test suite into a CI pipeline that runs on every commit. This way, whenever AI-generated code is added or changed, all tests run automatically. If something breaks, you’ll catch it early. Sometimes AI might introduce subtle breaking changes (like changing a function signature or output format slightly), and a robust test suite will detect that. Include security scans in the CI too (like <code>npm audit</code> or static analysis) so that any new introduction of a risky pattern is flagged. Types of tests to try<a contenteditable="false" data-primary="testing" data-secondary="building effective frameworks for AI-generated systems" data-tertiary="types of tests to include" data-type="indexterm" id="id920"/> include:</p>

<dl>
	<dt>Property-based testing and fuzzing</dt>
	<dd>
	<p><em>Property-based testing</em> (with tools like <a href="https://oreil.ly/JcYBf">Hypothesis for Python</a> or <a href="https://fast-check.dev">fast-check for JavaScript</a>) is another valuable technique. <a contenteditable="false" data-primary="property-based testing" data-type="indexterm" id="id921"/>Instead of writing individual test cases with specific inputs and expected outputs, you define high-level properties that your code should always satisfy. <a contenteditable="false" data-primary="fuzz testing of AI-generated code" data-type="indexterm" id="id922"/>The framework then generates a wide range of inputs to check whether those properties hold.</p>
	</dd>
	<dd>
	<p>Take sorting as an example. Rather than asserting that <code>sort([3, 1, 2]) === [1, 2, 3]</code>, you can define properties:</p>

	<ul>
		<li>
		<p>The output should be in order</p>
		</li>
		<li>
		<p>It should contain the same elements as the input</p>
		</li>
	</ul>

	<p>The tool then generates dozens or hundreds of input arrays to test those conditions—and finds edge cases you might not think of manually.</p>
	</dd>
	<dd>
	<p>This can be especially useful for AI-generated code. If your AI writes a function to normalize email addresses (such as by lowercasing the domain), a property test might check that the output is <em>idempotent</em>—meaning running the function twice gives the same result as running it once.<a contenteditable="false" data-primary="idempotence" data-type="indexterm" id="id923"/> If an edge case violates that invariant, the test framework will generate a counterexample to help you diagnose the bug.</p>
	</dd>
	<dt>Load and performance testing</dt>
	<dd>
	<p>AI might write code that’s not optimized. It’s a good idea to test your system under load.<a contenteditable="false" data-primary="performance" data-secondary="testing for AI-generated systems" data-type="indexterm" id="id924"/><a contenteditable="false" data-primary="load testing" data-type="indexterm" id="id925"/> This is reliability in terms of performance. Use tools like JMeter, Locust, or k6 to simulate many requests or heavy data and see if the system holds up. If not, identify the bottlenecks.</p>
	</dd>
	<dd>
	<p>For instance, maybe the AI writes a naive <code>O(n^2)</code> algorithm that works fine on 100 items but will tank at 10,000. Without performance tests, you might not notice that until it’s in production. So incorporate some performance scenarios, if applicable. Time some critical operations with increasing input sizes, or use profiling tools to see where CPU time or memory goes for heavy tasks.</p>
	</dd>
	<dt>Error handling</dt>
	<dd>
	<p>Intentionally cause errors to ensure <a contenteditable="false" data-primary="error handling" data-secondary="testing for AI-generated systems" data-type="indexterm" id="id926"/>the system responds gracefully, such as:</p>

	<ul>
		<li>
		<p>For an API, shut down the database and see if the API returns a friendly error or crashes.<a contenteditable="false" data-primary="APIs" data-secondary="testing error handling in AI-generated systems" data-type="indexterm" id="id927"/> If it crashes, add code (or ask AI to add code) to handle DB connection errors.</p>
		</li>
		<li>
		<p>For the frontend, simulate the backend returning 500 errors and ensure the UI shows an error message, not a blank page or infinite spinner.<a contenteditable="false" data-primary="backends" data-secondary="testing error handling in AI-generated systems" data-type="indexterm" id="id928"/><a contenteditable="false" data-primary="frontends" data-secondary="testing error handling in AI-generated systems" data-type="indexterm" id="id929"/></p>
		</li>
	</ul>

	<p>AI might not think of these failure modes on its own when writing code, so you have to test them and then refine. Testing these scenarios will improve reliability by prompting you to add proper fallback logic, retries, or user feedback.</p>
	</dd>
	<dt>Monitoring and logging</dt>
	<dd>
	<p>Incorporate logging and perhaps use the logs in tests for verification. <a contenteditable="false" data-primary="logging" data-secondary="incorporating and using in tests of AI-generated systems" data-type="indexterm" id="id930"/>For instance, if a certain action should trigger an audit log entry, test for that. AI can generate log lines; verify they print out as expected.</p>
	</dd>
	<dd>
	<p>Also, think about setting up monitoring (like an in-memory simulation of how your service will be monitored in production).<a contenteditable="false" data-primary="monitoring, setting up in AI-generated systems" data-type="indexterm" id="id931"/> For example, you might track if any uncaught exceptions are logged during test runs. If yes, treat it as a test failure; that means there’s some case not properly handled.</p>
	</dd>
	<dt>Maintainability</dt>
	<dd>
	<p>Maintainability testing, like ensuring <a contenteditable="false" data-primary="maintainability" data-secondary="testing for AI-generated systems" data-type="indexterm" id="id932"/>code style and standards, is important. Use linters and formatters to keep code consistent, since AI can produce slightly different styles from different prompts.<a contenteditable="false" data-primary="linters" data-type="indexterm" id="id933"/><a contenteditable="false" data-primary="formatters" data-type="indexterm" id="id934"/> A formatting tool like <a href="https://prettier.io">Prettier</a> or <a href="https://pypi.org/project/black">Black (for Python)</a> can unify style. For more logical consistency and to catch overly complex AI-generated code that might need refactoring, consider adding linting rules that enforce things like function complexity limits. (See <a data-type="xref" href="#ch08_ensuring_maintainability_in_ai_accelerated_codebas_1752630044622595">“Ensuring Maintainability in AI-Accelerated Codebases”</a> for more.)</p>
	</dd>
</dl>

<p>Once your tests are in place, you can refactor AI code more confidently.<a contenteditable="false" data-primary="refactoring" data-secondary="of AI code after testing" data-secondary-sortas="AI" data-type="indexterm" id="id935"/> Perhaps the AI produces a working but clunky solution; you can improve it and rely on tests to ensure you haven’t broken its behavior. You might even ask AI to refactor its own code:</p>

<blockquote>
<p>Refactor this function for clarity while keeping it passing the current tests.</p>
</blockquote>

<p>If your tests are good, you can check that the refactoring didn’t break anything.</p>

<p>Understanding nondeterminism in AI systems requires distinguishing between two fundamentally different scenarios. <a contenteditable="false" data-primary="testing" data-secondary="building effective frameworks for AI-generated systems" data-tertiary="nondeterminism in AI systems" data-type="indexterm" id="id936"/><a contenteditable="false" data-primary="nondeterminism in AI systems" data-type="indexterm" id="id937"/>When AI operates at runtime in production systems, such as a chatbot responding to customer queries or a recommendation engine personalizing content, the outputs can vary even with identical inputs. This variability stems from factors like model temperature settings, random seeds, or evolving model states. Testing such systems requires specialized approaches that account for acceptable variation ranges rather than expecting exact matches.</p>

<p>However, AI-assisted code generation presents a different paradigm entirely. Once an AI generates code and that code is committed to your repository, it becomes as deterministic as any human-written code. The function that calculates tax rates will produce the same output for the same input every time, regardless of whether a human or AI originally wrote it. This determinism is crucial for system reliability and makes traditional testing approaches entirely applicable to AI-generated code.</p>

<p class="pagebreak-before less_space">The more subtle challenge emerges when <a contenteditable="false" data-primary="testing" data-secondary="building effective frameworks for AI-generated systems" data-tertiary="challenges when integrating multiple AI-generated components" data-type="indexterm" id="id938"/>integrating multiple AI-generated components, each potentially created in isolation with different implicit assumptions. Consider a concrete example from an ecommerce system. You might prompt an AI to generate an order processing module, instructing it to handle international orders. <a contenteditable="false" data-primary="date formatting, using AI for" data-secondary="mismatches in AI-generated components generated in isolation" data-type="indexterm" id="id939"/>Separately, you ask the AI to create a shipping calculation service for the same system. The order processing module, following American conventions, formats dates as “12/25/2024” for December 25. Meanwhile, the shipping service, perhaps influenced by European examples in its generation, expects dates formatted as “25/12/2024.” Both components function perfectly in isolation, passing their individual unit tests.</p>

<p>The mismatch only surfaces during integration testing when the order processor passes a date to the shipping calculator. The shipping service interprets “12/01/2024” as January 12 rather than December 1, potentially calculating shipping times based on the wrong month entirely. This type of assumption mismatch is particularly common with AI-generated components because the AI might draw from different examples or conventions when generating each piece independently. Comprehensive integration testing that exercises the actual data flow between components becomes essential for catching these subtle incompatibilities before they cause production failures.</p>

<p>The QA process for AI-assisted projects might require a bit more creativity, since AI can introduce unusual edge cases.<a contenteditable="false" data-primary="edge cases" data-secondary="AI introducing unusual edge cases" data-type="indexterm" id="id940"/><a contenteditable="false" data-primary="quality assurance" data-secondary="process for AI-assisted projects, unusual edge cases and" data-type="indexterm" id="id941"/> For instance, an AI might output a feature you didn’t explicitly consider—if so, test that as well. If it added a hidden behavior, either remove it or properly test it.</p>

<p>Finally, if possible, test your application in an environment similar to production, with a realistic data load. Sometimes performance issues only appear with larger data volumes or higher concurrency.<a contenteditable="false" data-primary="testing" data-secondary="building effective frameworks for AI-generated systems" data-startref="ix_tstbldfrm" data-type="indexterm" id="id942"/> Use those test results to pinpoint inefficiencies.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Performance Optimization"><div class="sect1" id="ch08_performance_optimization_1752630044622537">
<h1>Performance Optimization</h1>

<p>While the AI often writes correct code, it may not always write <em>optimal</em> code. <a contenteditable="false" data-primary="performance" data-secondary="optimization for AI-generated code" data-type="indexterm" id="ix_perfopti"/>LLMs don’t inherently do performance analysis; they typically reproduce what is common in their training data. Therefore, be vigilant about potential performance issues, especially in critical paths or for large-scale use.</p>

<p>You can even chat with the AI for hints about performance optimization:</p>

<ul>
	<li>
	<p>What is the complexity of this code? Can it be improved?</p>
	</li>
	<li>
	<p>This function is slow—any ideas on how to make it faster?</p>
	</li>
</ul>

<p>It might not always be right, but it can sometimes give useful suggestions or at least confirm your thinking.</p>

<p>That said, don’t overoptimize, and don’t optimize prematurely or where it’s not needed. Sometimes the AI solution is perfectly fine, if the data sizes are small or the operation infrequent. Use your profiling data to focus on real bottlenecks and optimize the parts that really need it. The advantage of vibe coding is that you haven’t spent a ton of time handcrafting code from scratch, so you can afford to let some noncritical parts be simple and not superoptimized, as long as they don’t impact user experience or cost. This approach aligns with agile practices: make it work, then make it fast (if needed).</p>

<p>Here are some <a contenteditable="false" data-primary="performance" data-secondary="optimization for AI-generated code" data-tertiary="areas to cover" data-type="indexterm" id="id943"/>areas to cover as you ensure your AI-augmented project runs <span class="keep-together">efficiently</span>:</p>

<dl>
	<dt>Complexity analysis</dt>
	<dd>
	<p>When the AI generates an algorithm, take <a contenteditable="false" data-primary="complexity" data-secondary="analysis of in AI-generated code" data-type="indexterm" id="id944"/>a moment to consider its complexity. Sometimes it will use a brute-force solution where a more efficient algorithm exists. For example, it might double-sort a list because it didn’t recall a single-step method, resulting in O(n log n × 2) where O(n log n) could do (the capital <em>O</em> stands for memory usage). Or it might use nested loops that make an operation O(n<sup>2</sup>) when there’s a known O(n) approach. If you spot something like that, ask for improvements:</p>
	</dd>
	<dd>
	<blockquote>
	<p>Can we optimize this to avoid nested loops? Perhaps use a set for lookups.</p>
	</blockquote>
	</dd>
	<dd>
	<p>The AI often will oblige and give a better solution if you hint at the approach. If not, you might have to implement that part manually.<a contenteditable="false" data-primary="functions" data-secondary="performance optimization in AI-generated code" data-type="indexterm" id="id945"/></p>
	</dd>
	<dd>
	<p>To identify slow functions, run a profiler or measure execution time of key code paths with representative or worst-case data. If something is too slow, you can attempt to optimize manually or with AI assistance:</p>
	</dd>
	<dd>
	<blockquote>
	<p class="fix_tracking">Optimize this function, which is currently a bottleneck; try to reduce its complexity.</p>
	</blockquote>
	</dd>
	<dd>
	<p>The AI might restructure the code for performance. Use tests to make sure it still works.</p>
	</dd>
	<dd>
	<p>For critical algorithms, write a small benchmark harness.<a contenteditable="false" data-primary="algorithms" data-secondary="performance optimization in AI-generated code" data-type="indexterm" id="id946"/> If AI gives you a piece of code to, say, compute something, test it against another approach, or at least measure how it scales with input size. You might decide to rewrite in a more efficient way if needed.</p>
	</dd>
	<dt>Memory usage, leaks, and retention</dt>
	<dd>
	<p>AI-generated solutions might use more memory<a contenteditable="false" data-primary="memory" data-secondary="optimizations for performance in AI-generated code" data-type="indexterm" id="id947"/> than necessary: reading entire files into memory instead of streaming, for example, and thus holding large data structures. If your use case involves big data, check your system’s memory usage and optimize by streaming or chunking if needed. For instance, if you need to process millions of records, you’d want to refactor your AI-generated function <code>loadAllRecords()</code> to process them in batches or stream from the database.</p>
	</dd>
	<dd>
	<p>Also check that the AI-generated code is releasing resources. In languages like Java or C#, maybe it opens a file or DB connection and doesn’t close it. In a frontend single-page app, maybe event listeners aren’t removed, leading to leaks. Tools can help (like Chrome dev tools’ Memory Inspector for frontends or Valgrind for C++ leaks), but often just reading the code helps. Identify these and fix them. If you see an open file handle not closed, add a close in a <code>finally</code> block.</p>
	</dd>
	<dt>Concurrency and parallelism</dt>
	<dd>
	<p>If you’re using languages that support threads or async, look for places where the AI code might be single-threaded when it could be parallel.<a contenteditable="false" data-primary="parallelism" data-type="indexterm" id="id948"/><a contenteditable="false" data-primary="concurrency" data-type="indexterm" id="id949"/> AI might not automatically use async/await where appropriate, and may not know to offload a heavy CPU task to a worker thread.<a contenteditable="false" data-primary="async/await functions" data-type="indexterm" id="id950"/> Identify such opportunities. For example, for I/O-bound tasks in Node or Python, ensure asynchronous usage so that the system doesn’t block. For CPU-bound tasks, maybe the AI can’t help much in code, but you might decide to implement in a more performant language or offload to a background job.</p>
	</dd>
	<dt>Caching</dt>
	<dd>
	<p>A common performance optimization that <a contenteditable="false" data-primary="caching" data-secondary="adding for performance in AI-generated code" data-type="indexterm" id="id951"/>AI doesn’t always automatically add is to cache results of expensive operations. Look at your code: is it recalculating something repeatedly? If so, implement caching (either in-memory or using an external cache like Redis). You can prompt AI:</p>
	</dd>
	<dd>
	<blockquote>
	<p>Add caching to this function to avoid redundant calculations.</p>
	</blockquote>
	</dd>
	<dd>
	<p>It may implement a simple memorization or suggest using a caching library.</p>
	</dd>
	<dt>Database query optimization</dt>
	<dd>
	<p class="fix_tracking">If your application <a contenteditable="false" data-primary="databases" data-secondary="query optimization for AI-generated code" data-type="indexterm" id="id952"/>uses a database, examine the queries the AI creates. Are they using indexes properly? Perhaps the AI wrote <code>SELECT *</code> where only a few columns are needed. Or it’s fetching extensive data to filter in code, creating performance bottlenecks like the N + 1 query problem. These inefficiencies require optimization by pushing more work to the database or leveraging proper indexing.</p>

	<p>For instance, if the generated code calls <code>findOne</code> repeatedly within a loop, resulting in multiple database round trips, you can refactor this into a single batch query using <code>WHERE id IN (...)</code>. Similarly, if the AI omitted index creation in a migration for frequently queried fields, adding those indexes becomes essential for maintaining acceptable performance. The AI often generates functionally correct but suboptimal database interactions that require human expertise to identify and resolve.</p>
	</dd>
</dl>

<p>To illustrate, let’s take an example.<a contenteditable="false" data-primary="performance" data-secondary="optimization for AI-generated code" data-tertiary="example, function merging sorted arrays" data-type="indexterm" id="id953"/> Suppose <a contenteditable="false" data-primary="functions" data-secondary="performance optimization in AI-generated code" data-type="indexterm" id="id954"/>AI writes you a function that merges two sorted arrays by simply concatenating and sorting the result: (O(n log n))—even though there’s a known linear algorithm it could be using to merge two sorted lists (like merge step or merge sort, O(n)). In code review, you realize this could be a bottleneck for large arrays, so you prompt AI to implement the linear merge:</p>

<blockquote>
<p>Optimize the mergeSortedArrays function to perform the merge in linear time without using built-in sort.</p>
</blockquote>

<p>The AI recognizes this as the classic merge algorithm and writes it. The solution passes your tests, so congratulations: you gained performance without sacrificing <span class="keep-together">correctness</span>.</p>

<p>AI-assisted development doesn’t remove the need for performance tuning; it just shifts <em>when</em> you do that tuning. You’ll often get a correct solution first (which is extremely valuable), then turn your attention to measuring and optimizing targeted parts. When you do need to optimize something, the AI can help, as long as you guide it on what you need.<a contenteditable="false" data-primary="performance" data-secondary="optimization for AI-generated code" data-startref="ix_perfopti" data-type="indexterm" id="id955"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Ensuring Maintainability in AI-Accelerated Codebases"><div class="sect1" id="ch08_ensuring_maintainability_in_ai_accelerated_codebas_1752630044622595">
<h1>Ensuring Maintainability in AI-Accelerated Codebases</h1>

<p>A codebase’s <em>maintainability</em> describes how easy it is to modify, extend, and comprehend over time.<a contenteditable="false" data-primary="maintainability" data-secondary="ensuring in AI-accelerated codebases" data-type="indexterm" id="ix_maint"/> Some worry that AI-generated code could be messy or inconsistent, especially if multiple suggestions have varying styles or patterns. This section covers several practices you can use to address these concerns and keep your vibe-coded project clean and maintainable.</p>

<section data-type="sect2" data-pdf-bookmark="While Prompting"><div class="sect2" id="ch08_while_prompting_1752630044622648">
<h2>While Prompting</h2>

<p>As you prepare <a contenteditable="false" data-primary="maintainability" data-secondary="ensuring in AI-accelerated codebases" data-tertiary="considerations while prompting" data-type="indexterm" id="id956"/>your prompts, a few things<a contenteditable="false" data-primary="prompts" data-secondary="considerations for maintainability of AI code while prompting" data-type="indexterm" id="id957"/> to keep in mind:</p>

<dl>
	<dt>Use consistent coding standards</dt>
	<dd>
	<p>Use linters and formatters to enforce a consistent style.<a contenteditable="false" data-primary="coding standards, consistent, using in AI-generated code" data-type="indexterm" id="id958"/> As mentioned, AI might sometimes use different naming conventions or formatting in different outputs.<a contenteditable="false" data-primary="linters" data-type="indexterm" id="id959"/><a contenteditable="false" data-primary="formatters" data-type="indexterm" id="id960"/> Running a formatter (like Prettier for JS, Black for Python, gofmt for Go, etc.) on all code after generation ensures it conforms to a unified style. This makes reading code much easier (no cognitive load switching styles). Additionally, define naming conventions for your project and stick to them. If the AI outputs <code>get_user_data</code> in one place and <code>fetchUserData</code> in another, decide which convention you prefer (<code>snake_case</code> versus <code>camelCase</code>, etc.) and refactor to one style.</p>
	</dd>
	<dt>Use architectural patterns to encourage modularity and avoid sprawl</dt>
	<dd>
	<p>Encourage the<a contenteditable="false" data-primary="architecture" data-secondary="using architectural patterns to encourage modularity and avoid sprawl" data-type="indexterm" id="id961"/> AI to write <a contenteditable="false" data-primary="modularity, encouraging in AI code" data-type="indexterm" id="id962"/>modular code by<a contenteditable="false" data-primary="sprawl, avoiding in AI-generated code" data-type="indexterm" id="id963"/> prompting it to separate concerns. For example, instead of asking it to write one huge file implementing everything, break the work into tasks:</p>

	<ul>
		<li>
		<p>Create a UserService class for user logic.</p>
		</li>
		<li>
		<p>Create a separate module for sending emails.</p>
		</li>
	</ul>

	<p>This leads to a codebase that’s logically divided. It’s easier to maintain when each module has a clear responsibility. You can guide the architecture:</p>
	</dd>
	<dd>
	<blockquote>
	<p>Put database access code in a separate file or class from the API routing code.</p>
	</blockquote>
	</dd>
	<dd>
	<p>Because it’s so very easy to add features when using AI, it’s crucial to guard against feature creep and code sprawl. Without disciplined architectural thinking, you risk your codebase devolving into what software architects call a <em>big ball of mud</em>: an antipattern where <a contenteditable="false" data-primary="big ball of mud architectural antipattern" data-type="indexterm" id="id964"/>code lacks clear structure or boundaries. This risk intensifies with AI assistance, as the friction traditionally associated with adding features disappears, potentially accelerating architectural decay.</p>
	</dd>
	<dd>
	<p>To combat this, ground your AI-assisted development in proven architectural patterns and principles. When instructing AI, explicitly reference the patterns your project follows:</p>

	<ul>
		<li>
		<p>Add this new feature following the repository/service pattern used in the project.</p>
		</li>
		<li>
		<p>Implement this using the hexagonal architecture established in our domain layer.</p>
		</li>
	</ul>

	<p>This specificity helps maintain consistency even as features accumulate rapidly.</p>
	</dd>
</dl>

<p>For developers seeking deeper architectural grounding, several foundational texts provide <a contenteditable="false" data-primary="architecture" data-secondary="foundational texts on" data-type="indexterm" id="id965"/>essential guidance:</p>

<ul>
	<li>
	<p><em>Design Patterns: Elements of Reusable Object-Oriented Software</em> (Addison-Wesley, 1994) by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (the “Gang of Four”) remains the definitive catalog of reusable design solutions.</p>
	</li>
	<li>
	<p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/fundamentals-of-software/9781098175504/"><em>Fundamentals of Software Architecture: An Engineering Approach</em></a> by Mark Richards and Neal Ford offers comprehensive coverage of architectural patterns and principles across technology stacks.</p>
	</li>
	<li>
	<p><em>Domain-Driven Design: Tackling Complexity in the Heart of Software</em> by Eric Evans (Addison-Wesley, 2003) provides crucial techniques for aligning software design with business domains—particularly valuable when AI generates code that must reflect complex business logic.</p>
	</li>
</ul>

<p>These resources equip you to guide AI tools effectively, ensuring generated code adheres to sound architectural principles rather than contributing to technical debt. Remember: AI excels at implementing patterns but cannot determine which patterns are appropriate for your specific context. That architectural judgment remains fundamentally human.</p>
</div></section>

<section class="pagebreak-before" data-type="sect2" data-pdf-bookmark="Working with Code Output"><div class="sect2" id="ch08_working_with_code_output_1752630044622698">
<h2 class="less_space">Working with Code Output</h2>

<p>Once the AI responds with generated code, maintainability<a contenteditable="false" data-primary="maintainability" data-secondary="ensuring in AI-accelerated codebases" data-tertiary="working with code output" data-type="indexterm" id="id966"/> techniques to use include the following:</p>

<dl>
	<dt>Refactor continuously</dt>
	<dd>
	<p>Don’t hesitate to refactor AI-generated code when needed. <a contenteditable="false" data-primary="refactoring" data-secondary="continuous refactoring of AI-accelerated codebase" data-type="indexterm" id="id967"/>Sometimes the first pass is correct but not ideally structured: for example, the AI might write a very long function or duplicate its logic in two places. A common challenge is unintentionally duplicated code: the AI might not realize two functions do similar things and create both. If you notice similar blocks, refactor to one. Tools like code linters can detect duplicates (there are linters for too-similar code). Running those could highlight places to “DRY out” (don’t repeat yourself).</p>
	</dd>
	<dd>
	<p>To ask the AI to help refactor, you could prompt:</p>
	</dd>
	<dd>
	<blockquote>
	<p>Refactor this code to remove duplication and improve clarity.</p>
	</blockquote>
	</dd>
	<dd>
	<p class="fix_tracking">It might create helper functions or simplify some logic. Always test after <span class="keep-together">refactoring</span>.</p>
	</dd>
	<dt>Test</dt>
	<dd>
	<p>This chapter has already covered testing, so I’ll just note that a good test suite makes maintenance easier. <a contenteditable="false" data-primary="testing" data-secondary="helping to ensure maintainability of AI-accelerated codebase" data-type="indexterm" id="id968"/>When you or others modify code in the future (possibly with AI again), your tests will catch if the changes break anything, so you can refactor or change implementations with peace of mind. Testing decouples “what it does” from “how it does it,” giving you flexibility to maintain or improve “how” without altering “what.”</p>
	</dd>
	<dt>Avoid excessive complexity or overrelying on AI-specific constructs</dt>
	<dd>
	<p>Sometimes the AI might use a clever trick or less common function that other developers might not know.<a contenteditable="false" data-primary="constructs, AI-specific, overrelying on" data-type="indexterm" id="id969"/><a contenteditable="false" data-primary="complexity" data-secondary="excessive, avoiding in AI-accelerated codebase" data-type="indexterm" id="id970"/> While that’s not inherently bad, consider maintainability: if an average developer would scratch their head at the code, maybe simplify it. For instance, if AI uses a bit of regex magic or list comprehension that’s too terse, rewrite it in a more explicit loop for clarity (or at least comment it).</p>
	</dd>
	<dd>
	<p>Similarly, an AI trying to be helpful might overengineer a solution, like adding layers that aren’t needed. For instance, maybe a direct approach was fine, but the AI introduced an abstraction that isn’t pulling its weight. Remove it to keep things straightforward. Simpler code is usually easier to maintain.</p>
	</dd>
	<dt>Build in resilience and fallbacks</dt>
	<dd>
	<p>Think about fallback strategies in case of failures.<a contenteditable="false" data-primary="resilience, building into AI-accelerated codebase" data-type="indexterm" id="id971"/> For <a contenteditable="false" data-primary="fallbacks in AI-generated code" data-type="indexterm" id="id972"/>example, if an AI-coded component calls an external API and that API is down or returns unexpected data, do we have a fallback (like using cached data or a default response)? Implementing such resilience patterns (circuit breakers, retries with backoff, etc.) can make the system more robust. The AI likely won’t do this on its own unless asked. Ensure the system can handle partial failures gracefully. One microservice going down shouldn’t take the whole app down, if possible. Use timeouts and fallback logic.</p>
	</dd>
</dl>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Follow-Up"><div class="sect2" id="ch08_follow_up_1752630044622747">
<h2>Follow-Up</h2>

<p>Once you’re satisfied with the code, a few more<a contenteditable="false" data-primary="maintainability" data-secondary="ensuring in AI-accelerated codebases" data-tertiary="follow-up practices" data-type="indexterm" id="id973"/> practices help to keep it maintainable:</p>

<dl>
	<dt>Provide thorough documentation and comments</dt>
	<dd>
	<p>Make sure the code is properly documented.<a contenteditable="false" data-primary="documentation" data-secondary="providing for AI-accelerated codebase" data-type="indexterm" id="id974"/> AI often writes minimal comments unless prompted. <a contenteditable="false" data-primary="comments in AI-generated code" data-type="indexterm" id="id975"/>You can request docstrings or comments with prompts:</p>

	<ul>
		<li>
		<p>Add comments to explain the purpose of each section in this code.</p>
		</li>
		<li>
		<p>Write a docstring for this function.</p>
		</li>
	</ul>

	<p>These can save future readers time. The AI can usually generate fairly good explanations but sometimes misexplains subtle points, so review for accuracy.</p>
	</dd>
	<dd>
	<p>Also consider maintaining a high-level documentation (like a README or design doc) for the project, describing its architecture, main components, and so on. You can largely write this yourself, but AI can help by summarizing the codebase if needed.</p>
	</dd>
	<dd>
	<p>If you encounter some quirk like “The AI always names this parameter weirdly,” mention it in your dev notes for others. It’s part of the new collaborative environment. If it’s just you using the AI-generated code, a few quirks are fine—but if others join the project, they might wonder, “Why is this thing named like that?” Perhaps just standardize those names.</p>
	</dd>
	<dd>
	<p>There’s also an aspect of maintainability in terms of knowing which pieces of code were AI-generated and which were human-written. It’s not strictly necessary to label, but some teams might comment, “Generated with the help of GPT-4 on 2025-05-01” for traceability. Ideally, flag anything you’re unsure about in your PR description: “Used ChatGPT to help with this function; it seems to work, but please check the error-handling logic carefully.”</p>
	</dd>
	<dd>
	<p>This isn’t a widespread practice. It can be helpful during code review, but you might not need it if a human has already reviewed the code and it’s now just code. If you do keep any transcripts or prompts, you could link them in comments for complicated code: “This algorithm derived via GPT-4, based on prompt X; see docs for derivation.” A reviewer doesn’t need to treat it differently in terms of scrutiny (you should scrutinize all code), but it can help to understand the context. For example, if code has a certain style mismatch or an odd idiom, knowing it came from AI might clue the reviewer in that this isn’t a deliberate authorial choice but an AI artifact.</p>
	</dd>
	<dt>Code reviews and team norms</dt>
	<dd>
	<p>If you’re working in a team, have all team members review code—even if one person and AI cowrote it. <a contenteditable="false" data-primary="code reviews" data-secondary="having all team members review code" data-type="indexterm" id="id976"/>They might spot awkward patterns or things that break team norms. Over time, you’ll develop a sense of how to prompt the AI to match your team’s style (maybe including specifics in system prompts or initial guidelines). If multiple developers use AI, make sure everyone knows the desired style patterns so they can prompt accordingly (like “Write this in functional style” or “Use async/await, not callbacks”). See the next section for some tips on code review with AI code.</p>
	</dd>
	<dt>Track technical debt</dt>
	<dd>
	<p>If, during development, you accept an AI solution that you know isn’t ideal, track it as <a contenteditable="false" data-primary="technical debt, tracking in AI-accelerated codebase" data-type="indexterm" id="id977"/>technical debt in your comments or the project to-dos: “TODO: The solution works but is O(n<sup>2</sup>); if data grows, optimize this,” or “TODO: This uses a global variable for simplicity; refine this later.” The AI can even insert TODO comments itself if you ask:</p>
	</dd>
	<dd>
	<blockquote>
	<p>If there are any areas that need future improvement, add to-do  comments.</p>
	</blockquote>
	</dd>
	<dd>
	<p>Just address those to-dos eventually.</p>
	</dd>
	<dt>Learn from AI patterns</dt>
	<dd>
	<p>If AI introduces a design pattern or library you’re not familiar with, take time to learn more about it rather than ignoring it. <a contenteditable="false" data-primary="patterns (AI), learning from" data-type="indexterm" id="id978"/><a contenteditable="false" data-primary="AI patterns, learning from" data-type="indexterm" id="id979"/>Understanding a particular caching approach or a library it uses will help you maintain or modify that part confidently in the future. If it’s too arcane, you might decide to remove it in favor of something you know—but sometimes AI can pleasantly surprise you with a useful library or pattern you didn’t know. If it’s a well-known solution that you and the team can learn, this can even improve maintainability.</p>
	</dd>
</dl>

<p>In practice, maintainability comes down to applying the same good software-engineering principles as always—just applying them to code that was partially written by AI. Fortunately, because AI reduces the grunt work, you may have more time to focus on cleaning up the code and writing docs, which <em>improves</em> maintainability.</p>

<p>Some companies <a href="https://oreil.ly/2lrTW">report</a> that after an initial burst of generating code with AI, they invest time in a “hardening sprint” to refactor and document it all. Consider alternating between generation-heavy sprints and cleanup sprints as a potential strategy.<a contenteditable="false" data-primary="maintainability" data-secondary="ensuring in AI-accelerated codebases" data-startref="ix_maint" data-type="indexterm" id="id980"/></p>
</div></section>
</div></section>

<section class="pagebreak-before" data-type="sect1" data-pdf-bookmark="Code Review Strategies"><div class="sect1" id="ch08_code_review_strategies_1752630044622796">
<h1 class="less_space">Code Review Strategies</h1>

<p>As discussed in <a data-type="xref" href="ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362">Chapter 4</a>, code review is a critical process in traditional development and remains so in AI-assisted development. <a contenteditable="false" data-primary="code reviews" data-secondary="strategies for AI-assisted development" data-type="indexterm" id="ix_cdrev"/>This section discusses some nuances to consider when a chunk of the code under review is machine-suggested. Because AI can produce code so quickly, it’s reasonable to worry that code review will become a bottleneck—but don’t let that worry hamper the review process. It’s crucial to allocate proper time for reviews. Don’t skimp on the assumption that “we wrote it fast, let’s merge fast.” If anything, commit smaller changes more frequently to make reviews easier (generally a good practice anyway). <a contenteditable="false" data-primary="PRs (pull requests)" data-type="indexterm" id="id981"/>Frequent, smaller pull requests (PRs) are easier to review thoroughly than one giant PR. The AI can help break tasks into smaller PRs as well, if you plan accordingly.</p>

<p>Don’t assume code is correct just because “the AI wrote it and the tests pass.” Think critically and try to reason through the logic. If possible, test it mentally or with additional cases outside the provided tests, because tests might not cover everything. You can also run the code and even experiment by running a snippet with a tricky input to see if it behaves.</p>

<p>Code reviews can also be important learning moments. If the AI introduces a novel solution that is actually good, the reviewer might learn something new while verifying its correctness. Similarly, if the AI/human combination does something suboptimal, the reviewer can explain a better approach. Over time, this feedback loop can improve how the team uses AI (like helping everyone understand which things to avoid or ask differently). In a sense, code review helps to close the human learning loop, since the human author should learn and understand anything the AI wrote that is new to them.</p>

<p>When you review code, your first priority should be making sure it meets the requirements and intended design. Does this code do what the feature/bugfix is supposed to? Does it cover any edge cases mentioned in the specifications? If the prompt is off, AI might solve a slightly different problem: maybe it handles a case that wasn’t needed or misses a case. This is normal, but watch that the developer didn’t just accept AI output that only partially addresses the issue. For example, an AI might produce code to format a date but assume a certain time zone, which might or might not align with requirements.</p>

<p>If something in the code isn’t obvious, ask the author to explain how it works or why it’s done that way. If they struggle to explain or reach for “the AI did it and I assume it’s right,” that’s a red flag. The team should understand everything in the codebase. Encourage the author to double-check with the AI or documentation and provide a proper explanation, possibly as a comment in code.</p>

<p>Pay attention to the security and performance vulnerabilities discussed earlier in this chapter, too, and if any known best practice is violated, call it out—like if output isn’t escaped (in web dev) or if you find credentials in the code.</p>

<p>Request changes or refactoring if<a contenteditable="false" data-primary="refactoring" data-secondary="requesting for AI-assisted code that could be simpler" data-type="indexterm" id="id982"/> you see code that works but could be simpler or more in line with team style:</p>

<blockquote>
<p>The AI created 3 separate functions for different user roles that mostly duplicate each other. Can we merge these into one function with a parameter for role?</p>
</blockquote>

<p>The code’s author can then do so (maybe with AI’s help). If the AI suggestion didn’t use the team’s consistent style or standard libraries, mention that too:</p>

<blockquote>
<p>We usually use the requests library for HTTP calls, but this code is using http.client. Let’s stick to requests for consistency.</p>
</blockquote>

<p>The author can then prompt the AI to rewrite using the preferred library.</p>

<p>If the AI has written something really complex, like a tricky algorithm, consider discussing it with another reviewer or the team for a deeper review.</p>

<p>You may want to try some of the emerging tools that use AI to assist in code review—like GitHub’s Copilot for Pull Requests, which can generate summaries and flag potential bugs and other issues.<a contenteditable="false" data-primary="GitHub’s Copilot for Pull Requests" data-type="indexterm" id="id983"/> Such a tool might highlight something like “This code snippet is similar to one in module X with slight differences” (pointing out possible duplication). These hints can complement the human review but should not replace it.</p>

<p>Finally, be respectful and constructive in your reviewing, even when the code has flaws due to AI. Avoid blaming the developer for what could be an AI artifact: while they are still responsible for their code, recognize the context. AI is a tool, and both author and reviewer are working with it. The goal is to improve the code and share knowledge, not point fingers. For example: “This part seems to have a security issue⁠—likely an oversight from the AI suggestion; let’s fix it.”</p>

<p>Ultimately, code review in vibe coding is how we fully exercise the <em>human intelligence</em> side of the human/AI partnership. <a contenteditable="false" data-primary="human-AI pair programming" data-secondary="human intelligence exercised in code reviews" data-type="indexterm" id="id984"/>It’s where oversight and expertise come in to catch what the AI might miss and to keep the quality bar high. It’s also a knowledge-sharing moment for the team, since discussing code in reviews spreads understanding of both the domain and how to best use AI.</p>

<p>Code review also formalizes the concept of “developers as editors” <a href="https://oreil.ly/INPFV">introduced by Grant Gross in <em>CIO</em></a>: the reviewer is an editor, making sure the code is polished and fit for production. <a contenteditable="false" data-primary="developers" data-secondary="developers as editors, code reviews formalizing" data-type="indexterm" id="id985"/>This aligns perfectly with vibe coding as a concept, where the vibes (AI suggestions) are there but human judgment refines them.<a contenteditable="false" data-primary="code reviews" data-secondary="strategies for AI-assisted development" data-startref="ix_cdrev" data-type="indexterm" id="id986"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Best Practices for Reliable Deployment"><div class="sect1" id="ch08_best_practices_for_reliable_deployment_1752630044622847">
<h1>Best Practices for Reliable Deployment</h1>

<p>Once you know your code is secure, tested, and maintainable, you need to deploy it and keep it running reliably in production.<a contenteditable="false" data-primary="deployments" data-secondary="best practices for reliable deployment" data-type="indexterm" id="ix_dply"/></p>

<p>While AI-assisted development doesn’t alter the core principles of software deployment, it does introduce considerations around deployment velocity and operational complexity. For those seeking comprehensive coverage of deployment fundamentals, <em>The DevOps Handbook</em> (IT Revolution Press, 2016), by Gene Kim, Jez Humble, Patrick Debois, John Willis, and Nicole Forsgren, provides the definitive guide, covering everything from continuous integration and deployment pipelines to monitoring, security, and organizational transformation. This foundational knowledge becomes even more critical when AI accelerates your ability to generate deployable code, as the principles ensure your deployment practices can scale with your increased development velocity.</p>

<section data-type="sect2" data-pdf-bookmark="Before and During Deployment"><div class="sect2" id="ch08_before_and_during_deployment_1752630044622907">
<h2>Before and During Deployment</h2>

<p>As you ramp up to deployment, consider <a contenteditable="false" data-primary="deployments" data-secondary="best practices for reliable deployment" data-tertiary="before and during deployment" data-type="indexterm" id="id987"/>the following best practices:</p>

<dl>
	<dt>Automate your CI/CD pipeline</dt>
	<dd>
	<p>Given the fast pace of AI development, a robust <a contenteditable="false" data-primary="CI/CD (continuous integration and continuous deployment) pipelines" data-secondary="automating" data-type="indexterm" id="id988"/>continuous integration/continuous deployment (CI/CD) pipeline is valuable. Every commit (with or without AI-generated code) should be built, tested, and potentially deployed through an automated pipeline. This reduces human error and confirms that all deployment steps (tests, lint, security scans) are consistently run. If AI code introduces something that breaks the build or fails the tests, the CI will catch it immediately. Also, an automated CI/CD pipeline allows for quick iteration, so you can patch any AI-introduced issues and deploy fixes rapidly.</p>
	</dd>
	<dt>Infrastructure as code</dt>
	<dd>
	<p>Use infrastructure as code (Terraform, CloudFormation, etc.) to define your deployment environment. <a contenteditable="false" data-primary="infrastructure as code, using to define deployment environment" data-type="indexterm" id="id989"/>While not directly related to AI coding, it’s part of reliable deployments. You could even use AI to help write Terraform scripts, but treat those with the same caution and testing as other AI code, including perhaps testing them in a sandbox before applying them to production. A valuable starting point is the book <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/terraform-up-and/9781098116736/"><em>Terraform: Up &amp; Running</em></a> (O’Reilly, 2022), by Yevgeniy Brikman, which provides a comprehensive introduction to the principles and practices of IaC with Terraform.</p>
	</dd>
	<dt>Use staged rollouts—and have a rollback plan</dt>
	<dd>
	<p>Use staged rollout strategies <a contenteditable="false" data-primary="staged rollout strategies" data-type="indexterm" id="id990"/>like deploying to a staging environment or a canary release before full production rollout. This way, you can catch anything you’ve overlooked before it affects all users. For example, you might deploy a new <span class="keep-together">AI-coded</span> feature to 5% of users and monitor (with metrics and logs) for any errors or performance issues. If all is good, roll it out to 100% of users.</p>
	</dd>
	<dd>
	<p>Always have a rollback plan.<a contenteditable="false" data-primary="rollback plans" data-type="indexterm" id="id991"/> Despite all tests and reviews, sometimes things slip through. If a new release goes wrong, be ready to revert to the last stable version. If you’re using a containerization strategy like Kubernetes, maintain previous deployments for quick switchback. If it’s a serverless function, keep the previous version alive until you’re confident in the new one.</p>
	</dd>
	<dt>Set up observability</dt>
	<dd>
	<p>Set up comprehensive <a contenteditable="false" data-primary="observability, setting up for deployments" data-type="indexterm" id="id992"/>monitoring in production, of both system metrics and application logs:</p>

	<ul>
		<li>
		<p>Use tools like Sentry to track errors and capture exceptions. If the AI code throws an unexpected error in production (perhaps an edge case wasn’t covered), you’ll get an alert so you can fix it.</p>
		</li>
		<li>
		<p>Use performance-monitoring tools like application performance monitoring (APM) to track response times, throughput, and memory usage. This will show you if any code in the new deployment has introduced a slowdown or memory leak.</p>
		</li>
		<li>
		<p>Monitor availability: for instance, ping the service endpoints to confirm they’re up. If something crashes (maybe due to some untested scenario), an alert should fire, so you can react quickly.</p>
		</li>
	</ul>
	</dd>
	<dt>Stay vigilant about security</dt>
	<dd>
	<p>Make sure that secrets like API keys are handled properly in deployment.<a contenteditable="false" data-primary="security" data-secondary="remaining vigilant about in deployments" data-type="indexterm" id="id993"/> For example, if your AI wrote code that expects a secret in an environment variable, set up that secret in the CI/CD or cloud config, so it’s not accidentally logged or exposed.<a contenteditable="false" data-primary="CI/CD (continuous integration and continuous deployment) pipelines" data-secondary="security in" data-type="indexterm" id="id994"/><a contenteditable="false" data-primary="key management" data-type="indexterm" id="id995"/><a contenteditable="false" data-primary="secret management tools" data-type="indexterm" id="id996"/> Use secret management tools like <a href="https://oreil.ly/NqQ-T">HashiCorp Vault</a> (HashiCorp Vault offers secrets management, key management, and more with many integrations) or <a href="https://oreil.ly/LlYX-">AWS Secrets Manager</a> (AWS Secrets Manager allows you to securely store and rotate secrets like database credentials, API keys, and tokens, and can integrate with CI/CD tools like GitHub). <a contenteditable="false" data-primary="credentials" data-secondary="management of for deployments" data-type="indexterm" id="id997"/>Also, if you’re using container images, scan them for vulnerabilities.</p>
	</dd>
	<dt>Test using techniques like blue-green deployments or shadow testing</dt>
	<dd>
	<p>For major changes, consider a blue-green deploy. <a contenteditable="false" data-primary="blue-green deployments" data-type="indexterm" id="id998"/>This involves setting up two identical production environments: “blue” (the current live version) and “green” (the new version). Traffic is initially directed to the blue environment. Once the green environment is ready and tested, traffic is switched over to it. If any issues arise with the green environment, traffic can be quickly rerouted back to the blue environment, minimizing downtime and risk. This method tests the new version in a full production setting before making it the sole live version.</p>
	</dd>
	<dd>
	<p>Alternatively, if a specific AI-coded algorithm change is risky or you want to validate its behavior with real-world data without impacting users, you could shadow test it. <a contenteditable="false" data-primary="testing" data-secondary="shadow testing via deployment" data-type="indexterm" id="id999"/><a contenteditable="false" data-primary="shadow testing" data-type="indexterm" id="id1000"/>This involves deploying the new version alongside the current live version. Real production inputs are fed to both versions in parallel. However, only the current version’s outputs are shown to users. The outputs from the new (shadow) version are collected and compared against the current version’s results to evaluate its performance, accuracy, and stability. If the shadow version’s results are satisfactory and performance is good, you can then confidently switch it to be the active version.</p>
	</dd>
</dl>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Ongoing Best Practices"><div class="sect2" id="ch08_ongoing_best_practices_1752630044622957">
<h2>Ongoing Best Practices</h2>

<p>After deployment, these strategies can help keep<a contenteditable="false" data-primary="deployments" data-secondary="best practices for reliable deployment" data-tertiary="ongoing practices after deployment" data-type="indexterm" id="id1001"/> everything running reliably:</p>

<dl>
	<dt>Create operational runbooks</dt>
	<dd>
	<p>Provide runbooks for the ops team that <a contenteditable="false" data-primary="runbooks" data-type="indexterm" id="id1002"/>describe <a contenteditable="false" data-primary="operational runbooks" data-type="indexterm" id="id1003"/>any special aspects of the AI-generated parts of the code: “This service uses an AI model for X; if the model output seems erroneous, try restarting service or check the model’s version.” Or “Feature Y heavily uses caching to perform well; if performance issues arise, check the cache hit rate.” Essentially, document any operational considerations that might not be obvious. If AI has introduced a dependency (like using a temp file), note that, so ops will know to monitor disk space and the like.</p>
	</dd>
	<dt>Test in production</dt>
	<dd>
	<p>In addition to testing during development and as part of the rollout, some companies do testing in production (TiP) in safe ways, like running continuous small experiments. <a contenteditable="false" data-primary="testing" data-secondary="in production" data-secondary-sortas="production" data-type="indexterm" id="id1004"/>For instance, you might use feature flags to turn on an AI-generated feature for a small subset of users and see if any error rates change. This overlaps with canary releases, but you can make it more granular using feature toggles.</p>
	</dd>
	<dt>Audit regularly</dt>
	<dd>
	<p>Schedule periodic security and performance audits of the codebase, especially as more AI contributions accumulate. <a contenteditable="false" data-primary="performance" data-secondary="periodic audits of AI-accelerated codebase" data-type="indexterm" id="id1005"/><a contenteditable="false" data-primary="audits" data-secondary="periodic security and performance audits of codebase" data-type="indexterm" id="id1006"/>This is similar to managing tech debt: it helps you catch things that were fine at first but that could turn problematic as the scale or context changes. Watch for “drift,” too—if AI code is generating SQL queries, make sure that your migrations and code stay in sync and that the deployment runs migrations properly before new code takes traffic.</p>
	</dd>
	<dt>Keep humans in the loop</dt>
	<dd>
	<p>The theme continues—humans should monitor the automations. <a contenteditable="false" data-primary="human contribution, maximizing" data-secondary="humans monitoring automations" data-type="indexterm" id="id1007"/>AI might help you write code, but it won’t fix a production incident at 2 a.m. Have someone on call who understands the system. Over time, you might enlist AI for troubleshooting help like analyzing logs (a feature of some emerging tools), but at the end of the day, a human should make decisions about fixes.</p>
	</dd>
	<dt>Learn from failures</dt>
	<dd>
	<p>No process is 100% perfect. <a contenteditable="false" data-primary="failures" data-secondary="learning from" data-type="indexterm" id="id1008"/>If an error gets through your defenses and causes an incident, do a postmortem. Identify if the problem was related to AI usage (like “We trusted the AI code here and it failed under scenario X”), and update your processes and tests to prevent that class of issue. Doing this kind of analysis every time continuously improves reliability.</p>
	</dd>
</dl>

<p>Reliability isn’t just about code, of course; it also involves the infrastructure and operations <em>around</em> the code. <a contenteditable="false" data-primary="reliability" data-secondary="of infrastructure and operations around the code" data-secondary-sortas="infrastructure" data-type="indexterm" id="id1009"/>AI helps mostly on the code side. Robust operational practices (which can be partially assisted by AI) keep the overall system reliable.</p>

<p>In essence, treat an AI-heavy project the same as any high-quality software project when it comes to deployment: employ thorough testing, roll out gradually, monitor heavily, and make sure you can roll back quickly.<a contenteditable="false" data-primary="CI/CD (continuous integration and continuous deployment) pipelines" data-secondary="more frequent deployments with AI-assisted code" data-type="indexterm" id="id1010"/> Because AI can create changes faster, you may end up deploying more frequently (which is fine, if your CI/CD pipeline is good). <a href="https://oreil.ly/ATjYo">Frequent small deployments</a> are actually <a href="https://oreil.ly/Y5uDn">known to reduce risk</a> compared to infrequent big ones. The reason is that each individual change is smaller, making it easier to identify and fix any issues that arise. If a problem occurs, rolling back a small change is also simpler and faster. This approach contrasts with large, infrequent releases where numerous changes are bundled together, making it difficult to pinpoint the cause of any problems and increasing the potential impact of a failed deployment.</p>

<p>By following these best practices, you can be confident that even though a lot of its code was machine-generated, your system as a whole will behave reliably for users. The combination of automated testing, careful deployment, and monitoring closes the loop to catch anything that slipped through earlier stages. As a result, you can reap the speed and productivity benefits of AI development without sacrificing your ability to trust your software in production.<a contenteditable="false" data-primary="deployments" data-secondary="best practices for reliable deployment" data-startref="ix_dply" data-type="indexterm" id="id1011"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary and Next Steps"><div class="sect1" id="ch08_summary_and_next_steps_1752630044623004">
<h1>Summary and Next Steps</h1>

<p>In summary, vibe coding does not remove the need for engineering rigor—it amplifies the productivity of the engineers who apply that rigor. Your mantra should be the old Russian proverb: Trust but verify. Trust the AI to handle the grunt work, but verify everything with your tools and expertise.</p>

<p>Security and reliability are one dimension of responsible development; ethics is another. AI-assisted coding raises important questions about intellectual property, bias, the impact on developer jobs, and more. <a data-type="xref" href="ch09.html#ch09_the_ethical_implications_of_vibe_coding_1752630044848930">Chapter 9</a> will delve into those broader implications. How can you use AI coding tools responsibly and fairly? How do you deal with licensing of AI-generated code and ensure your models and prompts are used ethically?</p>
</div></section>
</div></section></div></div></body></html>