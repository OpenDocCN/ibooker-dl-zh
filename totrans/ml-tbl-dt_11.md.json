["```py\ndf.describe()\n```", "```py\ncounts = df.nunique()                                  ①\nprint(\"unique value counts:\\n\",counts)\n```", "```py\nunique value counts:  \nLocation          112                                  ①\nPrice            4280                                  ②\nRooms              43                                  ③\nBathrooms          17                                  ④\nCar Parks          21                                  ④\nProperty Type      99                                  ⑤\nSize             6190                                  ⑥\nFurnishing          4                                  ⑦\ndtype: int64\n```", "```py\nprint(\"shape \",df.shape)                                 ①\nshape  (53883, 8)                                        ②\n```", "```py\nmissing_values_count = df.isnull().sum()                  ①\nprint(\"missing values before cleanup:\\n\",missing_values_count)\n```", "```py\nmissing values before cleanup:\nLocation             0                                    ①\nPrice              248\nRooms             1706\nBathrooms         2013\nCar Parks        17567                                    ②\nProperty Type       25\nSize              1063\nFurnishing        6930\ndtype: int64\n```", "```py\nMissing values:  \nid                                    0\nname                                 16                  ①\nhost_id                               0\nhost_name                            21                  ②\nneighbourhood_group                   0\nneighbourhood                         0\nlatitude                              0\nlongitude                             0\nroom_type                             0\nprice                                 0\nminimum_nights                        0\nnumber_of_reviews                     0\nlast_review                       10052                  ③\nreviews_per_month                 10052                  ④\ncalculated_host_listings_count        0\navailability_365                      0\n```", "```py\nmisc_col_dict: # default values to replace missing values for general columns \n   Bathrooms: median                                     ①\n   Car Parks: 0                                          ②\n   Furnishing: unknown_furnishing                        ③\n   Property Type: unknown_property\n   Location: unknown_location\n```", "```py\ndef clean_up_misc_cols(df,misc_col_dict): \n for col in misc_col_dict:                           ①\n    if misc_col_dict[col] == 'median':               ②\n      df[col] = df[col].fillna(df[col].median())\n    else:\n      df[col] = df[col].fillna(misc_col_dict[col])   ③\n  return(df)\n```", "```py\ndef clean_up_price_col(df):\n  df.dropna(subset=['Price'], inplace=True)            ①\n  df['Price'] = \\\ndf['Price'].apply(lambda x:\\ \nremove_currency_symbol(\"RM \",x))                       ②\n  df['Price'] = \\\npd.to_numeric(df['Price'].\\\nstr.replace(',',''), errors='coerce')                  ③\n  return(df)\n```", "```py\nRooms              43\n```", "```py\n3           14249      ①\n3+1          8070      ②\n2            5407\n4            5018\n4+1          4404\n5+1          2340\n1            2322\n5            2065\n2+1          1938\n1+1          1191\n6             937\nStudio        874      ③\n6+1           807\n4+2           479\n3+2           477\n5+2           410\n7             358\n7+1           237\n2+2           132\n8             125\n6+             86      ④\n```", "```py\ncategorical: # categorical columns\n      - 'Location'\n#     - 'Rooms'\n      - 'Property Type'\n      - 'Furnishing'\n      - 'Size_type_bin'\ncontinuous: # continuous columns\n      - 'Bathrooms'\n      - 'Car Parks'\n      - 'Rooms'\n      - 'Size'\n```", "```py\ndef clean_up_rooms_col(df,treat_rooms_as_numeric):\n  if treat_rooms_as_numeric:                                ①\n    print(\"Rooms treated as numeric\")\n    df['Rooms'] = df['Rooms'].fillna(\"0\")                   ②\n    df['Rooms'] = \\\ndf['Rooms'].apply(lambda x: x+\"1\" \\\nif x.endswith('+') else x)                                  ③\n    df['Rooms'] = df['Rooms'].replace(\"Studio\", \"1\")        ④\n    df['Rooms']= \\\ndf['Rooms'].replace(\"20 Above\", \"21\")                       ⑤\n    df['Rooms']=\\\ndf['Rooms'].apply(lambda x:eval(str(x)))                    ⑥\n    df['Rooms'] = pd.to_numeric(df['Rooms'], \nerrors='coerce')                                            ⑦\n    # replace missing values with 0\n    df['Rooms'] = df['Rooms'].fillna(0)                     ⑧\n  else:\n    print(\"Rooms treated as non-numeric\")\n    df['Rooms'] = df['Rooms'].fillna(\"unknown_rooms\")       ⑨\n  return(df)\n```", "```py\ndef clean_up_size_col(df,clean_up_list,size_bin_count):\n    df.dropna(subset=['Size'], inplace=True)                  ①\n    df['Size'] = df['Size'].str.lower()                       ②\n    df[['Size_type','Size']] = \\\n    df['Size'].str.split(':',expand=True)                     ③\n    df['Size'] = df['Size'].fillna(\"0\")                       ④\n    df = df[df.Size.str.contains(r'\\d')]                      ⑤\n\n    for string in clean_up_list:                              ⑥\n        df = df[~df.Size.str.contains(string,na=False)]\n\n    df['Size'] = (df['Size'].str.replace(',','')\n                            .str.replace('`','')\n                            .str.replace('@','x')\n                            .str.replace('\\+ sq. ft.','')\n                            )                                 ⑦\n    df['Size'] = (df['Size'].str.replace(' sq. ft.','')\n                            .str.replace('sf sq.ft.','')\n                            .str.replace('ft','')\n                            .str.replace('sq','')\n                            .str.replace(\"xx\",\"*\")\n                            .str.replace(\"x \",\"*\")\n                            .str.replace(\" x\",\"*\")\n                            .str.replace(\"x\",\"*\")\n                            .str.replace(\"X\",\"*\")\n                            .replace('\\'','')\n                            )                                 ⑧\n    df['Size'] = \\\n    df['Size'].apply(lambda x: remove_after_space(x))         ⑨\n    df['Size'] = \\\ndf['Size'].apply(lambda x: eval(str(x)))                      ⑩\n    df['Size'] = df['Size'].fillna(0.0)\n    print(\"min is: \",df['Size'].min())\n    print(\"max is: \",df['Size'].max())\n    bins = np.linspace(df['Size'].min(), \n    df['Size'].max(), size_bin_count)                         ⑪\n    print(\"bins is: \",bins)\n    bin_labels = range(1,size_bin_count+1)                    ⑫\n    print(\"bin_labels is: \",bin_labels)\n    df['Size_bin'] = pd.qcut(df['Size'],\n    size_bin_count, labels=bin_labels)                        ⑬\n    df['Size_type_bin'] = \\\n    df['Size_type']+df['Size_bin'].astype(str)                ⑭\n    return(df)\n```", "```py\nmerged_data.columns = \\\nmerged_data.columns.str.replace(' ', '_')               ①\nmerged_data.columns  = merged_data.columns.str.lower()  ②\nconfig['categorical'] = \\\n[x.replace(\" \", \"_\") for x in \\\nconfig['categorical']]                                  ③\nconfig['continuous'] = \\\n[x.replace(\" \", \"_\") for x in \\\nconfig['continuous']]                                   ④\nconfig['categorical'] = \\\n[x.lower() for x in config['categorical']]              ⑤\nconfig['continuous'] = \\\n[x.lower() for x in config['continuous']]               ⑥\n```", "```py\n# function from https:\n//www.tensorflow.org/tutorials/structured_data/preprocessing_layers\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  df = dataframe.copy()\n  labels = df.pop('target')                              ①\n  df = {key: value[:,tf.newaxis] for key, \n    value in dataframe.items()}                          ②\n  ds = tf.data.Dataset.from_tensor_slices((dict(df), \n    labels))                                             ③\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))          ④\n  ds = ds.batch(batch_size)                              ⑤\n  ds = ds.prefetch(batch_size)                           ⑥\n  return ds\n```", "```py\n# function from \nhttps://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\ndef get_normalization_layer(name, dataset):\n  normalizer = layers.Normalization(axis=None)            ①\n  feature_ds = dataset.map(lambda x, y: x[name])          ②\n  normalizer.adapt(feature_ds)                            ③\n  return normalizer\n```", "```py\n# function from \nhttps://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\ndef get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  if dtype == 'string':                                     ①\n    index = layers.StringLookup(max_tokens=max_tokens)\n  else:\n    index = layers.IntegerLookup(max_tokens=max_tokens)\n  feature_ds = dataset.map(lambda x, y: x[name])            ②\n  index.adapt(feature_ds)                                   ③\n  encoder = \\\nlayers.CategoryEncoding(num_tokens= \\\nindex.vocabulary_size())                                    ④\n  return lambda feature: encoder(index(feature))            ⑤\n```", "```py\n# function from \n# https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\ntrain_ds = df_to_dataset(train, batch_size=batch_size)      ①\nval_ds = df_to_dataset(val, shuffle=False, \n    batch_size=batch_size)                                  ②\ntest_ds = df_to_dataset(test, shuffle=False, \n    batch_size=batch_size)                                  ③\n```", "```py\nall_inputs = []                                              ①\nencoded_features = []                                        ②\n\nfor header in config['continuous']:                          ③\n  numeric_col = tf.keras.Input(shape=(1,), name=header)\n  normalization_layer = get_normalization_layer(header, train_ds)\n  encoded_numeric_col = normalization_layer(numeric_col)\n  all_inputs.append(numeric_col)\n  encoded_features.append(encoded_numeric_col)\n\nfor header in config['categorical']:                         ④\n  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n  encoding_layer = get_category_encoding_layer(name=header,\n                                               dataset=train_ds,\n                                               dtype='string',\n                                               max_tokens=5)\n  encoded_categorical_col = encoding_layer(categorical_col)\n  all_inputs.append(categorical_col)\n  encoded_features.append(encoded_categorical_col)\n```", "```py\n[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Bathrooms')>,\n <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Car_Parks')>,\n <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Rooms')>,\n <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Size')>,\n <KerasTensor: shape=(None, 1) dtype=string (created by layer 'Location')>,\n <KerasTensor: shape=\\\n(None, 1) dtype=string \\\n(created by layer 'Property_Type')>,\n <KerasTensor: shape=(None, 1) dtype=string (created by layer 'Furnishing')>,\n <KerasTensor: shape=\\\n(None, 1) dtype=string \\\n(created by layer 'Size_type_bin')>]\n```", "```py\ncategorical: # categorical columns\n      - 'Location'\n      - 'Property Type'\n      - 'Furnishing'\n      - 'Size_type_bin'\ncontinuous: # continuous columns\n      - 'Bathrooms'\n      - 'Car Parks'\n      - 'Rooms'\n      - 'Size'\n```", "```py\nall_features = \\\ntf.keras.layers.concatenate(encoded_features)         ①\nx = \\\ntf.keras.layers.Dense(32, \n    activation=\"relu\")(all_features)                  ②\nx = tf.keras.layers.Dropout(dropout_rate)(x)          ③\noutput = tf.keras.layers.Dense(1)(x)                  ④\n\nmodel = tf.keras.Model(all_inputs, output) \n```", "```py\ntf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n```", "```py\nRooms              18\nLocation          108\nProperty Type      97\nFurnishing          5\nSize_type_bin      20\n```", "```py\nmodel.compile(optimizer=config['hyperparameters']['optimizer'],\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=config['metrics'])                      ①\nif config['general']['early_stop']:\n   callback_list, save_model_path = set_early_stop(es_monitor, es_mode)\n   model.fit(train_ds, \n             epochs=config['hyperparameters']['epochs'],    \n             validation_data=val_ds,\n             callbacks=callback_list)                         ②\nelse:\n   model.fit(train_ds, \n             epochs=config['hyperparameters']['epochs'], \n             validation_data=val_ds)                          ③\n```", "```py\nloss, accuracy = model.evaluate(test_ds)                  ①\nprint(\"Test Loss\", loss)\nprint(\"Test Accuracy\", accuracy)\nTest Loss 0.2754836082458496                              ②\nTest Accuracy 0.8765323758125305                          ③\n```", "```py\nmodel_file_name = \\\nos.path.join(get_model_path(),config['file_names']['saved_model'])\nmodel.save(model_file_name)                                      ①\nreloaded_model = \\\ntf.keras.models.load_model(model_file_name)                      ②\nsample = {                                                       ③\n    'location': 'Dutamas, Kuala Lumpur',\n    'rooms': 7.0,\n    'property_type': 'Serviced Residence (Intermediate)',\n    'furnishing': 'Partly Furnished',\n    'size_type_bin': 'built-up 1',\n    'bathrooms': 1.0,\n    'car_parks': 1.0,\n    'size': 16805.0,\n}\ninput_dict = \\\n{name: tf.convert_to_tensor([value]) for name, \nvalue in sample.items()}                                         ④\npredictions = reloaded_model.predict(input_dict)                 ⑤\nprob = tf.nn.sigmoid(predictions[0]) \n\nprint(\n    \"This property has a %.1f percent probability of \"\n    \"having a price over the median.\" % (100 * prob)\n)\n```", "```py\nThis property has a 99.4 percent probability\nof having a price over the median.\n```", "```py\nThis property has a 47.5 percent probability \nof having a price over the median.\n```", "```py\nsample = {\n    'location': 'Sentul, Kuala Lumpur',\n    'rooms': 3.0,\n    'property_type': 'Condominium For Sale',\n    'furnishing': 'Partly Furnished',\n    'size_type_bin': 'built-up 1',\n    'bathrooms': 2.0,\n    'car_parks': 0.0,\n    'size': 950.0,\n}\n```", "```py\nThis property has a 12.6 percent probability \nof having a price over the median.\n```", "```py\nmerged_data['price'].median()\n980000.0\n```"]