# 第十五章：优化延迟

嵌入式系统的计算能力有限，这意味着神经网络所需的密集计算可能比大多数其他平台花费更长的时间。由于嵌入式系统通常实时处理传感器数据流，运行速度过慢可能会导致许多问题。假设您试图观察可能仅在短暂时间内发生的事情（比如相机视野中出现的鸟）。如果处理时间太长，您可能会以太慢的速度采样传感器，错过其中一个事件。有时，通过重复观察重叠的传感器数据窗口，可以改善预测的质量，就像唤醒词检测示例在音频数据上运行一秒钟的窗口来进行唤醒词识别，但每次只将窗口向前移动一百毫秒或更少，对结果进行平均。在这些情况下，减少延迟可以帮助我们提高整体准确性。加快模型执行还可以使设备以更低的 CPU 频率运行，或在推理之间进入睡眠状态，从而降低整体能源使用量。

由于延迟是优化的一个重要领域，本章重点介绍了一些不同的技术，可以帮助您减少运行模型所需的时间。

# 首先确保它重要

有可能您的神经网络代码只是整体系统延迟的一小部分，加快它可能对产品的性能没有太大影响。确定是否是这种情况的最简单方法是在应用代码中注释掉对[`tflite::MicroInterpreter::Invoke()`](https://oreil.ly/1dLTn)的调用。这个函数包含了所有的推理计算，并且会阻塞直到网络运行完毕，因此通过移除它，您可以观察它对整体延迟的影响。在理想的情况下，您可以通过计时器日志语句或分析器来计算这种变化，但正如稍后所述，即使只是闪烁 LED 并粗略估计频率差异，也足以让您对速度增加有一个大致的概念。如果运行网络推理和不运行之间的差异很小，那么从优化代码的深度学习部分中获益不大，您应该首先关注应用的其他部分。

# 硬件更改

如果您确实需要加快神经网络代码的速度，首先要问的问题是是否能够使用更强大的硬件设备。对于许多嵌入式产品来说，这可能是不可能的，因为通常在很早的时候或者外部已经确定了要使用哪种硬件平台，但因为从软件角度来看这是最容易改变的因素，所以值得明确考虑。如果您有选择的余地，最大的约束通常是能源、速度和成本。如果可以的话，通过更换使用的芯片来权衡能源或成本以换取速度。您甚至可能在研究中幸运地发现一个新平台，它可以在不失去其他两个主要因素的情况下提供更快的速度！

###### 注意

当神经网络进行训练时，通常会一次发送大量的训练示例，在每个训练步骤中。这样可以进行许多计算优化，而当一次只提交一个样本时是不可能的。例如，一百张图像和标签可能会作为一个单独的训练调用的一部分发送。这些训练数据的集合称为*批次*。

在嵌入式系统中，我们通常一次处理一组传感器读数，实时处理，因此我们不希望等待收集更大的批次再触发推理。这种“单批次”关注意味着我们无法从一些在训练阶段有意义的优化中获益，因此对云端有帮助的硬件架构并不总是适用于我们的用例。

# 模型改进

在切换硬件平台后，对神经网络延迟产生重大影响的最简单方法是在架构层面。如果您能够创建一个足够准确但涉及更少计算的新模型，您可以加速推断而无需进行任何代码更改。通常可以通过降低准确性来换取增加速度，因此，如果您能够从一开始就使用尽可能准确的模型开始，那么您将有更多的空间进行这些权衡。这意味着花时间改进和扩展您的训练数据在整个开发过程中可能非常有帮助，即使在看似无关的任务，如延迟优化方面。

在优化过程代码时，通常更好的做法是花时间改变代码基于的高级算法，而不是在汇编中重写内部循环。对模型架构的关注基于同样的想法；如果可以的话，最好是完全消除工作，而不是提高执行工作的速度。在我们的情况下不同的是，交换机器学习模型比在传统代码中切换算法要容易得多，因为每个模型只是一个接受输入数据并返回数值结果的功能黑盒。在收集了一组良好的数据之后，应该相对容易地在训练脚本中用另一个模型替换一个模型。您甚至可以尝试删除您正在使用的模型中的单个层并观察效果。神经网络往往具有非常良好的退化性能，因此您应该随意尝试许多不同的破坏性更改，并观察它们对准确性和延迟的影响。

## 估算模型延迟

大多数神经网络模型在运行时花费大部分时间在运行大型矩阵乘法或非常接近的等效操作。这是因为每个输入值必须由不同的权重缩放以获得每个输出值，因此，每个网络层的工作量大约等于每个输入值乘以每个输出值的数量。这通常通过讨论网络在单次推断运行中所需的浮点运算数（或 FLOPs）来近似。通常，一个乘加操作（通常在机器码级别是一个单指令）计为两个 FLOPs，即使您执行 8 位或更低精度的量化计算，有时也会看到它们被称为 FLOPs，尽管不再涉及浮点数。可以通过手动逐层计算网络所需的 FLOPs。例如，全连接层所需的 FLOPs 数量等于输入向量的大小乘以输出向量的大小。因此，如果您知道这些维度，您可以计算出所需的工作量。通常在讨论和比较模型架构的论文中可以找到 FLOP 的估计值，比如[MobileNet](https://arxiv.org/abs/1905.02244)。

FLOPs 作为一个粗略的度量单位，用于衡量一个网络执行所需时间的多少，因为其他条件相同，涉及更少计算的模型将以与 FLOPs 差异成比例的速度运行得更快。例如，您可以合理地期望一个需要 1 亿 FLOPs 的模型比 2 亿 FLOP 版本运行速度快两倍。在实践中，这并不完全正确，因为还有其他因素，比如软件对特定层的优化程度会影响延迟，但这是评估不同网络架构的一个很好的起点。这也有助于确定对于您的硬件平台可以期望什么是现实的。如果您能在芯片上以 100 毫秒运行一个 100 万 FLOP 模型，那么您可以做出一个合理的猜测，即需要 1000 万 FLOPs 的不同模型将需要大约一秒来计算。

## 如何加速您的模型

模型架构设计仍然是一个活跃的研究领域，因此目前很难为初学者撰写一份好的指南。最好的起点是找到一些已经设计为高效的现有模型，然后迭代地尝试进行更改。许多模型具有特定的参数，我们可以改变这些参数以影响所需的计算量，比如 MobileNet 的深度通道因子，或者期望的输入大小。在其他情况下，您可能会查看每个层所需的 FLOPs，并尝试删除特别慢的层或用更快的替代方案替换它们（例如使用深度卷积代替普通卷积）。如果可以的话，最好查看在设备上运行时每个层的实际延迟，而不是通过 FLOPs 来估计。尽管这将需要一些在接下来的代码优化部分讨论的性能分析技术。

###### 注意

设计模型架构是困难且耗时的，但最近已经有一些自动化这个过程的进展，比如[MnasNet](https://arxiv.org/abs/1807.11626)，使用遗传算法等方法来改进网络设计。这些方法还没有完全取代人类（它们通常需要以已知的良好架构作为起点，并且需要手动规则来确定使用的搜索空间，例如），但很可能我们将在这个领域看到快速的进展。

已经有像[AutoML](https://cloud.google.com/automl/)这样的服务，允许用户避开训练的许多细节，希望这种趋势会继续下去，这样您就能够选择最适合您的数据和效率权衡的最佳模型。

# 量化

运行神经网络需要进行数十万甚至数百万次计算以进行每次预测。执行这种复杂计算的大多数程序对数值精度非常敏感；否则，错误会累积并导致结果太不准确而无法使用。深度学习模型不同——它们能够在中间计算中承受大量数值精度损失，仍然能够产生整体准确的最终结果。这种特性似乎是它们训练过程的副产品，其中输入很大且充满噪音，因此模型学会了对微不足道的变化具有鲁棒性，并专注于重要的模式。

在实践中，这意味着使用 32 位浮点表示进行操作几乎总是比推断所需的精度更高。训练要求更高一些，因为它需要对权重进行许多小的更改来学习，但即使在那里，16 位表示也被广泛使用。大多数推断应用程序可以产生与浮点等效物无法区分的结果，只需使用 8 位来存储权重和激活值。鉴于我们的许多平台对这些模型依赖的 8 位乘积累加指令提供了强大的支持，这对于嵌入式应用来说是个好消息，因为这些指令在信号处理算法中很常见。

然而，将模型从浮点转换为 8 位并不简单。为了有效地执行计算，8 位值需要线性转换为实数。这对于权重来说很容易，因为我们知道每个层的范围是从训练值中得出的，因此我们可以推导出正确的缩放因子来执行转换。然而，对于激活来说就比较棘手，因为从检查模型参数和架构中并不明显每个层输出的范围是多少。如果我们选择的范围太小，一些输出将被剪切到最小值或最大值，但如果范围太大，输出的精度将比可能的精度小，我们将面临整体结果精度下降的风险。

量化仍然是一个活跃的研究课题，有许多不同的选择，因此 TensorFlow 团队在过去几年中尝试了各种方法。您可以在[Raghuraman Krishnamoorthi 的“为高效推理量化深度卷积网络：白皮书”](https://arxiv.org/pdf/1806.08342.pdf)中看到一些这些实验的讨论，而[量化规范](https://oreil.ly/toF_E)则涵盖了我们现在基于经验使用的推荐方法。

我们将量化过程集中在将模型从 TensorFlow 训练环境转换为 TensorFlow Lite 图的过程中。我们过去推荐了一种量化感知训练方案，但发现这种方法难以使用，我们发现我们可以在导出时使用一些额外的技术获得等效的结果。最容易使用的量化类型是所谓的[训练后权重量化](https://oreil.ly/Tz9D_)。这是将权重量化为 8 位，但激活层保持浮点数的情况。这是有用的，因为它将模型文件大小缩小了 75%，并提供了一些速度优势。这是最容易运行的方法，因为它不需要任何关于激活层范围的知识，但仍然需要快速浮点硬件，这在许多嵌入式平台上并不存在。

[训练后整数量化](https://oreil.ly/LDw-y)意味着模型可以在没有任何浮点计算的情况下执行，这使得它成为我们在本书中涵盖的用例的首选方法。使用它最具挑战性的部分是，在模型导出过程中需要提供一些示例输入，以便通过运行一些典型图像、音频或其他数据来观察激活层输出的范围。正如我们之前讨论过的，如果没有这些范围的估计，就无法准确地量化这些层。过去，我们使用过其他方法，比如在训练期间记录范围或在运行时捕获范围，但这些方法都有缺点，比如使训练变得更加复杂或施加延迟惩罚，因此这是最不好的方法。

如果您回顾一下我们在第十章中导出人员检测器模型的说明，您会看到我们向`converter`对象提供了一个`representative_dataset`函数。这是一个 Python 函数，用于生成激活范围估计过程所需的输入，对于人员检测器模型，我们从训练数据集中加载一些示例图像。不过，对于您训练的每个模型，您都需要弄清楚预期输入，因为每个应用程序的预期输入都会发生变化。此外，很难辨别输入在预处理过程中是如何缩放和转换的，因此创建该函数可能需要一些试错。我们希望未来能够简化这个过程。

在几乎所有平台上运行完全量化的模型都具有很大的延迟优势，但如果您支持一个新设备，您可能需要优化最计算密集的操作，以利用硬件提供的专门指令。如果您正在处理卷积网络，一个很好的起点是[`Conv2D`操作](https://oreil.ly/NrjSo)和[kernel](https://oreil.ly/V27Q-)。您会注意到许多内核有`uint8`和`int8`版本；`uint8`版本是旧的量化方法的残余物，现在不再使用，所有模型现在都应该使用`int8`路径导出。

# 产品设计

你可能不会将产品设计视为优化延迟的一种方式，但实际上这是投入时间的最佳地方之一。关键是要弄清楚你是否可以放宽对网络的要求，无论是速度还是准确性。例如，你可能想使用摄像头以每秒多帧的速度跟踪手势，但如果你有一个需要一秒钟才能运行的身体姿势检测模型，你可能可以使用更快的光学跟踪算法以更高的速率跟踪识别的点，当更准确但不太频繁的神经网络结果可用时进行更新。另一个例子，你可以让微控制器将高级语音识别委托给通过网络访问的云 API，同时保持唤醒词检测在本地设备上运行。在更广泛的层面上，你可能可以通过将不确定性纳入用户界面来放宽网络的准确性要求。用于语音识别系统的唤醒词通常是包含不太可能出现在正常语音中的音节序列的短语。如果你有一个手势系统，也许你可以要求每个序列以竖起大拇指结束以确认命令是有意的？

目标是提供尽可能好的用户体验，因此在系统的其他部分中做任何可以更容忍错误的事情，可以让你有更多的空间来权衡准确性和速度或其他需要改进的属性。

# 代码优化

我们将这个主题放在章节的最后，因为在优化延迟方面有其他方法是你应该首先尝试的，但传统的代码优化是实现可接受性能的重要途径。特别是，TensorFlow Lite for Microcontrollers 的代码已经被编写成在尽可能小的二进制占用空间下运行良好，因此可能有一些优化仅适用于你特定的模型或平台，你可以从中受益。这也是我们鼓励你尽可能推迟代码优化的原因之一，因为如果你更改硬件平台或使用的模型架构，许多这类改变可能不适用，因此首先确定这些事项是至关重要的。

## 性能分析

任何代码优化工作的基础是知道程序中不同部分运行所需的时间。在嵌入式世界中，这可能会很难确定，因为你可能甚至没有一个简单的默认计时器，即使有，记录和返回所需的信息也可能很困难。以下是我们使用过的各种方法，从最容易实现到最棘手的。

### 闪烁

几乎所有的嵌入式开发板上都至少有一个 LED 可以从程序中控制。如果你要测量超过半秒的时间，可以尝试在你想要测量的代码部分开始时点亮 LED，然后在之后关闭它。你可以大致估计花费的时间，使用外部秒表并手动计算在 10 秒内看到多少次闪烁。你也可以将两个开发板并排放置，分别运行不同版本的代码，通过闪烁的频率来估计哪个更快。

### 散弹式性能分析

在大致了解您的应用程序正常运行需要多长时间后，估计特定代码段需要多长时间的最简单方法是将其注释掉，看整体执行速度提高了多少。这被称为*shotgun profiling*，类比于 shotgun debugging，其中您删除大块代码以定位崩溃，当其他信息很少时。对于神经网络调试来说，这可能会非常有效，因为模型执行代码中通常没有数据相关分支，因此通过注释掉其内部实现将任何一个操作变为无操作不应该影响模型其他部分的速度。

### 调试日志

在大多数情况下，您应该能够从嵌入式开发板向主机计算机输出一行文本，因此这似乎是检测代码执行时机的理想方式。不幸的是，与开发机器通信本身可能非常耗时。在 Arm Cortex-M 芯片上，[串行线调试输出](https://oreil.ly/SdsWk)可能需要长达 500 毫秒的时间，延迟变化很大，这使得它对于简单的日志分析方法毫无用处。基于 UART 连接的调试日志通常成本较低，但仍不理想。

### 逻辑分析仪

类似于切换 LED 但更精确，您可以让您的代码打开和关闭 GPIO 引脚，然后使用外部逻辑分析仪（我们过去使用过[Saleae Logic Pro 16](https://oreil.ly/pig8l)）来可视化和测量持续时间。这需要一些布线，设备本身可能很昂贵，但它提供了一种非常灵活的方式来调查程序的延迟，而无需任何软件支持超出一个或多个 GPIO 引脚的控制。

### 计时器

如果您有一个可以提供足够精度的一致当前时间的计时器，您可以记录您感兴趣的代码部分的开始和结束时的时间，并在之后将持续时间输出到日志中，其中任何通信延迟都不会影响结果。出于这个原因，我们考虑在 TensorFlow Lite for Microcontrollers 中需要一个平台无关的计时器接口，但我们认为这会给那些移植到不同平台的人增加太多负担，因为设置计时器可能会很复杂。不幸的是，这意味着您需要探索如何为您正在运行的芯片实现此功能。还有一个缺点是您需要在您想要调查的任何代码周围添加计时器调用，因此需要工作和计划来识别关键部分，并且您需要在探索时间去向的过程中不断重新编译和刷新。

### 分析器

如果您幸运的话，您将使用支持某种外部分析工具的工具链和平台。这些应用程序通常会使用来自您的程序的调试信息，以匹配他们从设备上运行您的程序时收集的执行统计信息。然后，它们将能够可视化哪些函数花费了最多时间，甚至是哪些代码行。这是了解代码中速度瓶颈所在的最快方式，因为您将能够快速探索和放大到重要的函数。

# 优化操作

在确保您使用尽可能简单的模型并确定哪些代码部分花费了最多时间之后，您应该看看如何加快它们的速度。神经网络的大部分执行时间应该花在操作实现内部，因为每个层可能涉及数十万或数百万次计算，因此很可能您已经发现其中一个或多个是瓶颈。

## 寻找已经优化的实现

TensorFlow Lite for Microcontrollers 中所有操作的默认实现都是为了小巧、易懂和可移植，而不是快速的，因此预期您应该能够通过使用更多代码行或内存的方法轻松击败它们。我们在[*kernels/portable_optimized 目录*](https://oreil.ly/fmY8R)中有一组更快的实现，使用了第十三章中描述的子文件夹专业化方法。这些实现不应该有任何平台依赖性，但它们可能使用比参考版本更多的内存。因为它们使用子文件夹专业化，您只需传递`TAGS="portable_optimized"`参数即可生成一个使用这些实现而不是默认实现的项目。

如果您正在使用具有特定于平台的实现的设备，例如通过类似 CMSIS-NN 的库，并且在指定目标时它们没有自动选择，您可以选择通过传递适当的标签来使用这些非可移植版本。但是，您需要查阅平台的文档和 TensorFlow Lite for Microcontrollers 源代码树，以找到相应的内容。

## 编写您自己的优化实现

如果您找不到正在占用大部分时间的操作的优化实现，或者可用的实现速度不够快，您可能需要自己编写。好消息是，您应该能够缩小范围，使工作更容易。您只需要调用几种不同的输入和输出大小以及参数的操作，因此您只需要专注于使这些路径更快，而不是一般情况。例如，我们发现深度卷积参考代码在 SparkFun Edge 开发板上的语音唤醒示例的第一个版本中占用了大部分时间，并且整体运行速度太慢，无法使用。当我们查看代码时，我们发现卷积滤波器的宽度始终为八，这使得可以编写[利用该模式的一些优化代码](https://oreil.ly/Kbx22)。我们可以使用 32 位整数并行获取四个输入值和四个字节中保存的权重。

要开始优化过程，请使用前面描述的子文件夹专业化方法在*kernels*根目录中创建一个新目录。将参考内核实现复制到该子文件夹中，作为您代码的起点。为确保构建正确，请运行与该操作相关的单元测试，并确保它仍然通过；如果您传递了正确的标签，它应该使用新的实现：

```py
make -f tensorflow/lite/micro/tools/make/Makefile test_depthwise_conv_\
  test TAGS="portable_optimized"
```

然后建议为您的操作添加一个新的测试到单元测试代码中，该测试不检查正确性，只报告执行操作所需的时间。拥有这样的基准测试将帮助您验证您的更改是否按照您的预期提高了性能。对于您在分析中看到速度瓶颈的每种情况，您应该为每种情况都有一个基准测试，具有与模型中该点的操作相同的大小和其他参数（尽管权重和输入可以是随机值，因为在大多数情况下，数字不会影响执行延迟）。基准测试代码本身将需要依赖本章前面讨论的一种性能分析方法，最好使用高精度计时器来测量持续时间，但如果没有，至少切换 LED 或逻辑输出。如果您的测量过程的粒度太大，您可能需要在循环中多次执行操作，然后除以迭代次数以捕获实际所需的时间。在编写基准测试后，记录在您进行任何更改之前的延迟，并确保它大致与您从分析应用程序中看到的相匹配。

有了代表性的基准测试数据，现在您应该能够快速迭代潜在的优化。一个很好的第一步是找到初始实现的最内部循环。这是代码中将被最频繁运行的部分，因此对其进行改进将比算法的其他部分产生更大的影响。通过查看代码并找到最深度嵌套的`for`循环（或等效部分），您应该能够识别出这一部分，但值得验证您是否有适当的部分，通过将其注释掉并再次运行基准测试。如果延迟显著下降（希望至少降低 50%），则您已经找到了需要关注的正确区域。例如，从[深度卷积的参考实现](https://oreil.ly/8S4kS)中获取这段代码：

```py
    for (int b = 0; b < batches; ++b) {
      for (int out_y = 0; out_y < output_height; ++out_y) {
        for (int out_x = 0; out_x < output_width; ++out_x) {
          for (int ic = 0; ic < input_depth; ++ic) {
            for (int m = 0; m < depth_multiplier; m++) {
              const int oc = m + ic * depth_multiplier;
              const int in_x_origin = (out_x * stride_width) - pad_width;
              const int in_y_origin = (out_y * stride_height) - pad_height;
              int32 acc = 0;
              for (int filter_y = 0; filter_y < filter_height; ++filter_y) {
                for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
                  const int in_x =
                      in_x_origin + dilation_width_factor * filter_x;
                  const int in_y =
                      in_y_origin + dilation_height_factor * filter_y;
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
                    int32 filter_val = filter_data[Offset(
                        filter_shape, 0, filter_y, filter_x, oc)];
                    acc += (filter_val + filter_offset) *
                           (input_val + input_offset);
                  }
                }
              }
              if (bias_data) {
                acc += bias_data[oc];
              }
              acc = DepthwiseConvRound<output_rounding>(acc, output_multiplier,
                                                        output_shift);
              acc += output_offset;
              acc = std::max(acc, output_activation_min);
              acc = std::min(acc, output_activation_max);
              output_data[Offset(output_shape, b, out_y, out_x, oc)] =
                  static_cast<uint8>(acc);
            }
          }
        }
      }
    }
```

仅通过检查缩进，就可以确定正确的内部循环如下所示：

```py
                  const int in_x =
                      in_x_origin + dilation_width_factor * filter_x;
                  const int in_y =
                      in_y_origin + dilation_height_factor * filter_y;
                  // If the location is outside the bounds of the input image,
                  // use zero as a default value.
                  if ((in_x >= 0) && (in_x < input_width) && (in_y >= 0) &&
                      (in_y < input_height)) {
                    int32 input_val =
                        input_data[Offset(input_shape, b, in_y, in_x, ic)];
                    int32 filter_val = filter_data[Offset(
                        filter_shape, 0, filter_y, filter_x, oc)];
                    acc += (filter_val + filter_offset) *
                           (input_val + input_offset);
                  }
```

这段代码被执行的次数比函数中的其他行要多得多，这是因为它位于所有循环的中间位置，将其注释掉将确认它占用了大部分时间。如果你有逐行分析信息的幸运，这也可以帮助你找到确切的部分。

现在你已经找到了一个高影响区域，目标是尽可能将更多工作移到不太关键的部分。例如，在中间有一个`if`语句，这意味着在每次内部循环迭代时必须执行条件检查，但可以将这部分工作提升到代码的其他部分，以便在外部循环中更少频繁地执行检查。你可能还会注意到一些条件或计算对于你的特定模型和基准测试是不需要的。在语音唤醒词模型中，扩张因子始终为 1，因此涉及它们的乘法可以被跳过，节省更多工作。我们建议您在顶层进行这种参数特定的优化检查，并在参数不符合优化要求时退回到普通的参考实现。这可以加速已知模型，但确保如果您有不符合这些标准的操作，它们至少能正常工作。为了确保您不会意外破坏正确性，值得经常运行操作的单元测试，因为您正在进行更改。

本书的范围超出了覆盖所有优化数值处理代码的方式，但您可以查看[*portable_optimized*](https://oreil.ly/tQkJm)文件夹中的内核，看看一些可能有用的技术。

## 利用硬件特性

到目前为止，我们只讨论了不特定于平台的可移植优化。这是因为重构代码以完全避免工作通常是产生重大影响的最简单方法。它还简化了更专门优化的焦点和范围。您可能会发现自己在像 Cortex-M 设备这样的平台上，具有[SIMD 指令](https://oreil.ly/MBxf5)，这些指令通常对神经网络推断中占用大部分时间的重复计算非常有帮助。您可能会诱惑直接使用内部函数或者甚至汇编来重写内部循环，但要抵制！至少要查看供应商提供的库的文档，看看是否已经有适合的内容来实现算法的较大部分，因为那可能已经高度优化了（尽管可能会错过您可以应用的优化，了解您的操作参数）。如果可以的话，尝试调用现有函数来计算一些常见的东西，比如快速傅立叶变换，而不是编写自己的版本。

如果您已经完成了这些阶段，那么现在是时候尝试您平台的汇编级别了。我们推荐的方法是从逐行将代码替换为其在汇编中的机械等效物开始，一次替换一行，这样您可以在进行过程中验证正确性，而不必一开始就担心加速。在您转换了必要的代码之后，您可以尝试融合操作和其他技术来减少延迟。与更复杂的处理器相比，嵌入式系统的一个优势是它们的行为通常比较简单，没有深层指令流水线或缓存，因此更容易在纸上理解潜在的性能，并建立潜在的汇编级优化，而不会有太多意外副作用的风险。

## 加速器和协处理器

随着机器学习工作负载在嵌入式世界中变得更加重要，我们看到越来越多的系统出现，提供专门的硬件来加速或降低它们所需的功耗。然而，目前还没有明确的编程模型或标准 API，因此并不总是清楚如何将它们与软件框架集成。通过 TensorFlow Lite for Microcontrollers，我们希望支持与主处理器同步工作的硬件的直接集成，但异步组件超出了当前项目的范围。

我们所说的同步是指加速硬件与主 CPU 紧密耦合，共享内存空间，并且操作员实现可以快速调用加速器，并在结果返回之前阻塞。从程序员的角度来看，这种加速器更像是早期 x86 系统上存在的浮点协处理器，而不是另一种更像 GPU 的模型。我们专注于这种同步加速器的原因是它们似乎对我们的低能耗系统最有意义，避免异步协调可以使运行时更简单。

类似协处理器的加速器需要与系统架构中的 CPU 非常接近，才能以如此低的延迟响应。相反的模型是现代 GPU 所使用的模型，其中有一个完全独立的系统，具有自己的控制逻辑，位于总线的另一端。编程这些类型的处理器涉及 CPU 排队一长串命令，这些命令需要相对较长的时间来执行，并在批处理准备就绪后立即发送，但立即继续其他工作，不等待加速器完成。在这种模型中，CPU 和加速器之间的通信延迟是微不足道的，因为发送命令的频率很低，而且没有等待结果。加速器可以从这种方法中受益，因为一次看到很多命令会提供许多重新排列和优化工作的机会，这在任务更加细粒度且需要按顺序执行时很难做到。这对图形渲染非常适用，因为结果根本不需要返回给 CPU；渲染的显示缓冲区只需显示给用户。通过向深度学习训练发送大批量的训练样本，可以确保一次有很多工作要做，并尽可能多地保留在卡上，避免将数据复制回 CPU。随着嵌入式系统变得更加复杂并承担更大的工作负载，我们可能会重新审视框架的要求，并通过类似移动版 TensorFlow Lite 中的委托接口来支持这种流程，但这超出了我们当前版本库的范围。

# 回馈开源

我们始终热衷于看到对 TensorFlow Lite 的贡献，当您努力优化一些框架代码后，您可能会有兴趣将其分享回主线。一个很好的开始是加入[SIG Micro](https://oreil.ly/wrtz-)邮件列表，并发送一封简短的电子邮件总结您所做的工作，以及指向带有您提议更改的 TensorFlow 存储库分支的指针。如果您包括您正在使用的基准测试以及一些内联文档讨论优化将有所帮助的地方，那将会很有帮助。社区应该能够提供反馈；他们将寻找可以在其基础上构建的东西，通常是有用的，并且可以维护和测试。我们迫不及待地想看看您的成果，感谢您考虑开源您的改进！

# 收尾

在本章中，我们介绍了加快模型执行速度所需了解的最重要的事情。最快的代码是根本不运行的代码，所以要记住的关键是在开始优化单个函数之前，在模型和算法级别缩小您正在进行的工作。您可能需要解决延迟问题，然后才能让您的应用程序在真实设备上运行，并测试它是否按照您的意图工作。之后，下一个优先事项可能是确保您的设备具有足够的寿命以便有用——这就是下一章关于优化能源使用的地方将会有用的地方。
