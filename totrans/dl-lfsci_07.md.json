["```py\nwget https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_images.zip\nunzip BBBC005_v1_images.zip\n\n```", "```py\nimage_dir = 'BBBC005_v1_images'\nfiles = []\nlabels = []\nfor f in os.listdir(image_dir):\n if f.endswith('.TIF'):\n  files.append(os.path.join(image_dir, f))\n  labels.append(int(re.findall('_C(.*?)_', f)[0]))\nloader = dc.data.ImageLoader()\ndataset = loader.featurize(files, np.array(labels))\n\n```", "```py\nsplitter = dc.splits.RandomSplitter()\ntrain_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(\n\t  dataset, seed=123)\n\n```", "```py\nlearning_rate = dc.models.tensorgraph.optimizers.ExponentialDecay(0.001, 0.9,\n                                                                 250)\nmodel = dc.models.TensorGraph(learning_rate=learning_rate, model_dir='model')\nfeatures = layers.Feature(shape=(None, 520, 696))\nlabels = layers.Label(shape=(None,))\nprev_layer = features\nfor num_outputs in [16, 32, 64, 128, 256]:\n prev_layer = layers.Conv2D(num_outputs, kernel_size=5, stride=2, \n \t\t\t\t\t\t\tin_layers=prev_layer)\noutput = layers.Dense(1, in_layers=layers.Flatten(prev_layer))\nmodel.add_output(output)\nloss = layers.ReduceSum(layers.L2Loss(in_layers=(output, labels)))\nmodel.set_loss(loss)\n\n```", "```py\nmodel.restore()\n```", "```py\ny_pred = model.predict(test_dataset).flatten()                           \nprint(np.sqrt(np.mean((y_pred-test_dataset.y)**2)))\n\n```", "```py\nmodel.fit(train_dataset, nb_epoch=50)\n\n```", "```py\nwget https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_ground_truth.zip\nunzip BBBC005_v1_ground_truth.zip\n\n```", "```py\nimage_dir = 'BBBC005_v1_images'\nlabel_dir = 'BBBC005_v1_ground_truth'\nrows = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n\t\t'M', 'N', 'O', 'P')\nblurs = (1, 4, 7, 10, 14, 17, 20, 23, 26, 29, 32, 35, 39, 42, 45, 48)\nfiles = []\nlabels = []\nfor f in os.listdir(label_dir):\n if f.endswith('.TIF'):\n  for row, blur in zip(rows, blurs):\n   fname = f.replace('_F1', '_F%d'%blur).replace('_A', '_%s'%row)\n   files.append(os.path.join(image_dir, fname))\n   labels.append(os.path.join(label_dir, f))\nloader = dc.data.ImageLoader()\ndataset = loader.featurize(files, labels)\n\n```", "```py\nsplitter = dc.splits.RandomSplitter()\ntrain_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(\n\t  dataset, seed=123)\n\n```", "```py\nlearning_rate = dc.models.tensorgraph.optimizers.ExponentialDecay(0.01, 0.9, 250)\nmodel = dc.models.TensorGraph(learning_rate=learning_rate, \n\t\t\t\t\t\t\t  model_dir='segmentation')\nfeatures = layers.Feature(shape=(None, 520, 696, 1)) / 255.0\nlabels = layers.Label(shape=(None, 520, 696, 1)) / 255.0\n# Downsample three times.\nconv1 = layers.Conv2D(16, kernel_size=5, stride=2, in_layers=features)\nconv2 = layers.Conv2D(32, kernel_size=5, stride=2, in_layers=conv1)\nconv3 = layers.Conv2D(64, kernel_size=5, stride=2, in_layers=conv2)\n# Do a 1x1 convolution.\nconv4 = layers.Conv2D(64, kernel_size=1, stride=1, in_layers=conv3)\n# Upsample three times.\nconcat1 = layers.Concat(in_layers=[conv3, conv4], axis=3)\ndeconv1 = layers.Conv2DTranspose(32, kernel_size=5, stride=2, in_layers=concat1)\nconcat2 = layers.Concat(in_layers=[conv2, deconv1], axis=3)\ndeconv2 = layers.Conv2DTranspose(16, kernel_size=5, stride=2, in_layers=concat2)\nconcat3 = layers.Concat(in_layers=[conv1, deconv2], axis=3)\ndeconv3 = layers.Conv2DTranspose(1, kernel_size=5, stride=2, in_layers=concat3)\n# Compute the final output.\nconcat4 = layers.Concat(in_layers=[features, deconv3], axis=3)\nlogits = layers.Conv2D(1, kernel_size=5, stride=1, activation_fn=None, \n\t\t\t\t\t   in_layers=concat4)\noutput = layers.Sigmoid(logits)\nmodel.add_output(output)\nloss = layers.ReduceSum(layers.SigmoidCrossEntropy(in_layers=(labels, logits)))\nmodel.set_loss(loss)\n\n```", "```py\nmodel.restore()\n\n```", "```py\nscores = []\nfor x, y, w, id in test_dataset.itersamples():\n y_pred = model.predict_on_batch([x]).squeeze()\n scores.append(np.mean((y>0) == (y_pred>0.5)))\nprint(np.mean(scores))\n\n```", "```py\nmodel.fit(train_dataset, nb_epoch=50, checkpoint_interval=100)\n\n```"]