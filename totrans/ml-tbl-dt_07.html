<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">6 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/>Advanced feature processing methods</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-192"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Processing features with more advanced methods</li>

    <li class="co-summary-bullet">Selecting useful features for lighter, more understandable models</li>

    <li class="co-summary-bullet">Optimizing hyperparameters to make your models shine in performance</li>

    <li class="co-summary-bullet">Mastering the specific characteristics and options from gradient boosting decision trees</li>
  </ul>

  <p class="body">We’ve now discussed decision trees, their characteristics, their limitations, and all their ensemble models, both those based on random resamplings, such as random forests, and those based on boosting, such as gradient boosting. Since boosting solutions are considered the state of the art in tabular data modeling, we have explained how it works and optimized its predictions at length. In particular, we have presented a couple of solid gradient boosting implementations, XGBoost and LightGBM, that are proving the best solutions available to a data scientist dealing with tabular data.</p>

  <p class="body">This chapter will deal with more general topics regarding classical machine learning. However, we will focus on gradient boosting decision trees (GBDTs), especially XGBoost. In the chapter, we discuss more advanced methods for feature processing, such as multivariate imputation of missing values, target encoding for reducing high categorical features to simple numeric ones, and a general way to figure out how to transform or elaborate your features based on how they relate to the target. We will propose a few ways to reduce the number of your features to the essential and optimize your hyperparameters depending on the computational resources available and the model you have chosen. The chapter will then close with a section on only advanced methods and options related to GBDTs.<a id="idTextAnchor004"/><a id="idIndexMarker001"/></p>

  <h2 class="fm-head" id="heading_id_3">6.1 Processing features</h2>

  <p class="body">There are any number of problems you may face when dealing with real-world tabular datasets, and all the methods we’ve discussed so far will produce substandard results if you aren’t adjusting your techniques to address the realities of your data. Here, we’ll consider a few such problems, such as dealing with missing values in the smartest way possible, transforming categorical features with a large number of unique values, and finding a way to reprocess your features after you have trained your model to squeeze out even more performance. This isn’t an exhaustive list, of course, but it should give you some practice in spotting problems and planning an appropriate approach. <a id="idIndexMarker002"/><a id="marker-193"/></p>

  <p class="body">As in the previous chapter, for our explanations and examples, we will rely again on the Airbnb NYC dataset to present practical examples to tackle the most challenging task in tabular data problems. The following listing reprises the data and some key functions and classes we will use again in this chapte<a id="idTextAnchor005"/>r.</p>

  <p class="fm-code-listing-caption">Listing 6.1 Reprising the Airbnb NYC dataset</p>
  <pre class="programlisting">import numpy as np
import pandas as pd
from sklearn.preprocessing import (
    StandardScaler,
    OneHotEncoder,
    OrdinalEncoder
)
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
 
data = pd.read_csv("./AB_NYC_2019.csv")
excluding_list = ['price', 'id', 'latitude', 'longitude', 'host_id', 
                  'last_review', 'name', 'host_name']            <span class="fm-combinumeral">①</span>
low_card_categorical = [
    'neighbourhood_group',
    'room_type'
]                                                                <span class="fm-combinumeral">②</span>
high_card_categorical = ['neighbourhood']                        <span class="fm-combinumeral">③</span>
continuous = [
    'minimum_nights',
    'number_of_reviews',
    'reviews_per_month', 
    'calculated_host_listings_count',
    'availability_365'
]                                                                <span class="fm-combinumeral">④</span>
target_mean = (
    (data["price"] &gt; data["price"].mean())
    .astype(int)
)                                                                <span class="fm-combinumeral">⑤</span>
target_median = (
    (data["price"] &gt; data["price"].median())
    .astype(int)
)                                                                <span class="fm-combinumeral">⑥</span>
target_multiclass = pd.qcut(
    data["price"], q=5, labels=False
)                                                                <span class="fm-combinumeral">⑦</span>
target_regression = data["price"]                                <span class="fm-combinumeral">⑧</span>
categorical_onehot_encoding = OneHotEncoder(handle_unknown='ignore')
categorical_ord_encoding = 
OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan)
numeric_standardization = Pipeline([('StandardScaler', StandardScaler()), 
                                    ('Imputer', 
SimpleImputer(strategy="constant", fill_value=0))])
 
column_transform = ColumnTransformer(
    [
        ('low_card_categories', 
         categorical_onehot_encoding, 
         low_card_categorical),
        ('high_card_categories', 
         categorical_ord_encoding, 
         high_card_categorical),
        ('numeric', 
         numeric_standardization, 
         continuous)
    ],
    remainder='drop',
    verbose_feature_names_out=True,
    sparse_threshold=0.0)                                        <span class="fm-combinumeral">⑨</span>
 
lm_column_transform = ColumnTransformer(
    [
        ('low_card_categories', 
         categorical_onehot_encoding, 
         low_card_categorical),
        ('numeric', 
         numeric_standardization, 
         continuous)
    ],
    remainder='drop',
    verbose_feature_names_out=True,
    sparse_threshold=0.0)                                        <span class="fm-combinumeral">⑩</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> List of excluded columns for feature processing</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> List of categorical columns with low cardinality to be one-hot encoded</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> List of categorical columns with high cardinality to be ordinally encoded</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> List of continuous feature columns</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a binary target indicating whether the price is above the mean (unbalanced binary target)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Creates a binary target indicating whether the price is above the median (balanced binary target)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Creates a multiclass target by quantile binning the price into five classes</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Sets the target for regression as the price column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Creates a column transformer that applies different transformations to different groups of features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Creates a column transformer suitable for linear models</p>

  <p class="body"><a id="marker-194"/>We refer to the explanations presented in the previous chapter for all the details about what the code does. The only addition is a column transformer designed explicitly for linear models. This transformer handles just low cardinality categorical features by performing one-hot encoding, leaving high cardinality categorical ones apar<a id="idTextAnchor006"/>t.</p>

  <h3 class="fm-head1" id="heading_id_4">6.1.1 Multivariate missing data imputation</h3>

  <p class="body">Having missing data in your tabular dataset is a blocking problem because, apart from GBDTs solutions such as XGBoost, LightGBM, and Scikit-learn’s HistGradientBoosting, classical machine learning algorithms do not have any native support for missing values. In addition, even if your GBDTs algorithm of choice can handle missingness, as explained in the next section, you may still find directly imputing the missing values more effective because you can check beforehand how each feature or specific case is handled.<a id="idIndexMarker003"/><a id="marker-195"/><a id="idIndexMarker004"/></p>

  <p class="body">In chapter 2, we discussed simple imputation methods, such as using the mean or the median, and the usefulness of building missing indicators, thus enabling algorithms to spot missing patterns that are present more easily. This section will provide more details about these techniques and multivariate imputation.</p>

  <p class="body">First, unless missing cases depend on unobserved variables such as features you don’t have access to, missing data can be categorized as</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Missing completely at random</i> (<a id="idTextAnchor007"/>MCAR)—In this scenario, data missingness is unrelated to observed and unobserved variables. The missingness occurs randomly across the dataset. <a id="idIndexMarker005"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Missing at random</i> (MAR)—MAR assumes that observed variables, not the unobserved ones, can explain the missingness. In other words, the probability of missingness solely depends on the observed data.<a id="idIndexMarker006"/></p>
    </li>
  </ul>

  <p class="body">When the missing cases depend on the unobserved values of the missing data itself, you fall into the case of missing not at random (MNAR), which requires quite a specialized treatment that is not a topic of this book. However, suppose you understand the mechanism behind some missing not at random missing data, such as when you don’t get information in the census about people who are too rich (because of privacy) or too poor (because of a general lack of access). In that case, you can try to gather new features that hint at their wealth to add to your dataset and fall back to the MAR case.<a id="idIndexMarker007"/></p>

  <p class="body">Generally, you often have cases where missing data is MCAR or MAR. In both cases, apart from simple imputation using an expected value that works perfectly with MCAR, you can better reconstruct your missing data through multivariate imputation. <i class="fm-italics">Multivariate imputation</i> is a method that uses the correlations among predictors in a dataset to impute missing values. It involves building a series of models to estimate the missing values based on the relationships between variables. In this approach, each model treats a feature with missing values as the target variable (by modeling only its known values) and uses the remaining features as predictors. The resulting model is then used to determine what values to replace the missing values in the target. You may set how the algorithm cycles through the features to impute. You usually use the default setting, starting from the features with less missing data and progressing to those with more missing values, which is preferred and most effective.<a id="idIndexMarker008"/></p>

  <p class="body">To handle missing values in the predictors, an initial imputation step is performed using a simple mean or another basic imputation method. Then, through multiple iterations, the imputation process refines the initial estimates by incorporating the results from the imputing models. This iterative process continues until the imputed values reach a state of stability, where further iterations do not lead to significant changes. Multivariate imputation is implemented in Scikit-learn by <code class="fm-code-in-text">Iterative<a class="calibre" id="idTextAnchor008"/>Imputer</code> (<a class="url" href="https://mng.bz/MDZQ">https://mng.bz/MDZQ</a>). Inspired by the R MICE package (Multivariate Imputation by Chained Eq<a id="idTextAnchor009"/>uations: <a class="url" href="https://mng.bz/avEj">https://mng.bz/avEj</a>), it allows both for multivariate imputation and multiple imputations, a common approach in statistics and social sciences where, instead of a single imputed value, you get a distribution of plausible replacements. Multiple imputations are possible with <code class="fm-code-in-text">IterativeImputer</code> by running it multiple times with the <code class="fm-code-in-text">sample_posterior</code> parameter set to True, using a different random seed each time.<a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>

  <p class="body">However, multivariate imputation is the favored choice in data science tabular data applications because it allows building models based on single but precise estimations. In our example, we take the Airbnb NYC dataset’s continuous features and randomly remove 5% of the data, thus mimicking an MCAR situation. Afterward, we run a <code class="fm-code-in-text">SimpleImputer</code>, replacing missing values with a mean and an <code class="fm-code-in-text">IterativeImputer</code>. Finally, we compare, using the mean absolute error (MAE), the features reconstructed by each method against the origin<a id="idTextAnchor010"/>al values.<a id="marker-196"/><a id="idIndexMarker011"/></p>

  <p class="fm-code-listing-caption">Listing 6.2 Multivariate imputation</p>
  <pre class="programlisting">from sklearn.experimental import (
    enable_iterative_imputer
)                                                             <span class="fm-combinumeral">①</span>
from sklearn.impute import SimpleImputer, IterativeImputer
from sklearn.ensemble import RandomForestRegressor
 
Xm = data[continuous].copy()                                  <span class="fm-combinumeral">②</span>
missing_percentage = 0.05
np.random.seed(0)
mask = np.random.rand(*Xm.shape) &lt; missing_percentage         <span class="fm-combinumeral">③</span>
Xm[mask] = np.nan
 
simple_imputer = SimpleImputer()
Xm_si = simple_imputer.fit_transform(Xm)                      <span class="fm-combinumeral">④</span>
 
rf = RandomForestRegressor(random_state=0, n_jobs=-1)         <span class="fm-combinumeral">⑤</span>
multivariate_imputer = IterativeImputer(
    estimator=rf,
    max_iter=1,
    tol=0.01
)                                                             <span class="fm-combinumeral">⑥</span>
Xm_mi = multivariate_imputer.fit_transform(Xm)                <span class="fm-combinumeral">⑦</span>
 
mae = pd.DataFrame(
    {
        "simple": np.mean(
            np.abs(data[continuous] - Xm_si), axis=0
        ),
        "multivariate": np.mean(
            np.abs(data[continuous] - Xm_mi), axis=0
        )
     },
     index = continuous
)                                                             <span class="fm-combinumeral">⑧</span>
print(mae)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports IterativeImputer, which is still experimental and under improvement in Scikit-learn</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a copy of continuous feature data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a mask to randomly mark missing values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Uses a SimpleImputer instance with mean imputation strategy</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Instantiates a RandomForestRegressor for iterative imputation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Creates an IterativeImputer instance with max_iter and tol are the stopping criteria</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Imputes missing data using iterative imputation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Calculates MAE for imputed data and original data</p>

  <p class="body">The result provided by the command <code class="fm-code-in-text">print(mae)</code> is a table that compares the simple imputation with the multivariate imputation method: <a id="idIndexMarker012"/></p>
  <pre class="programlisting">                                       Simple        Multivariate
minimum_nights                         0.347355      0.260156
number_of_reviews                      1.327776      0.858506
reviews_per_month                      0.057980      0.036876
calculated_host_listings_count         0.579423      0.368567
availability_365                       6.025748      4.62264</pre>

  <p class="body"><a id="marker-197"/>The comparison results demonstrate that the multivariate method, specifically the <code class="fm-code-in-text">IterativeImputer</code>, consistently yields lower MAE values than the simple imputation method, even after a single iteration. This indicates that <code class="fm-code-in-text">IterativeImputer</code> is more effective at replacing missing values with fewer errors. To obtain even better estimations, you can increase the <code class="fm-code-in-text">max_iter</code> to a higher number and leave the algorithm to decide if to stop earlier based on the tol values, a tolerance threshold used to check if the results are stable. Increasing the <code class="fm-code-in-text">max_iter</code> will lead to longer imputation times because, as an imputing model, we are using a random forests algorithm. Random forests are usually the most effective way to handle multivariate estimations (a method known in the R community as <i class="fm-italics">MissForest</i>: <a class="url" href="https://rpubs.com/lmorgan95/MissForest">https://rpubs.com/lmorgan95/MissForest</a>). However, you can choose faster methods based on linear models or k-nearest neighbors by simply replacing the <code class="fm-code-in-text">estimator</code> in the <code class="fm-code-in-text">IterativeImputer</code>:<a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">BayesianRidge—Regularized linear regression simply using <code class="fm-code-in-text">BayesianRidge()</code> </p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">RandomForestRegressor—For forests of randomized trees regression, you can set <code class="fm-code-in-text">n_estimators</code>, <code class="fm-code-in-text">max_depth</code>, and <code class="fm-code-in-text">max_features</code> to create shallower trees and thus accelerate the imputation process such as <code class="fm-code-in-text">RandomForestRegressor(n_estimators=30,</code> <code class="fm-code-in-text">max_depth=6,</code> <code class="fm-code-in-text">max_samples=0.5)</code> </p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Nystroem + Ridge—A pipeline with the expansion of a degree 2 polynomial kernel and regularized linear regression by combining different Scikit-learn commands: <code class="fm-code-in-text">make_pipeline(Nystroem(kernel="polynomial",</code> <code class="fm-code-in-text">degree=2,</code> <code class="fm-code-in-text">random_state=0),</code> <code class="fm-code-in-text">Ridge(alpha=1e3))</code> </p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">KNeighborsRegressor—A k-nearest neighbors imputation approach where you decide the number of neighbors to consider, such as <code class="fm-code-in-text">KNeighbors-Regressor(n_neighbors=5)</code> </p>
    </li>
  </ul>

  <p class="body">The estimator you use will affect the quality of the results you obtain and the computation time. As a start, <code class="fm-code-in-text">BayesianRidge</code> is the default choice and also the fastest. If you have more time, <code class="fm-code-in-text">RandomForestRegressor</code> will provide you with better estimates. By jointly inputting multiple variables, <code class="fm-code-in-text">IterativeImputer</code> captures the dependencies between variables more accurately at the price of more computations and written code. For a straightforward, out-of-the-box solution, some GBDT implementations provide native support for handling missing values, which we will discover in the<a id="idTextAnchor011"/> next section.</p>

  <h3 class="fm-head1" id="heading_id_5">6.1.2 Handling missing data with GBDTs</h3>

  <p class="body">Both XGBoost and LightGBM algorithms (and Scikit-learn’s HistGradientBoosting) handle missing values similarly by assigning them to the side that minimizes the loss function the most in each split. XGBoost introduced this technique with its sparsity-aware split finding algorithm, which provides a default direction to use when data is missing, either because it is missing or stored in a sparse matrix where only non-zero values are kept. <a id="idIndexMarker015"/><a id="marker-198"/><a id="idIndexMarker016"/></p>

  <p class="body">Consequently, don’t forget that XGBoost will treat the zeros in a sparse matrix as missing and apply its specific algorithm to handle missing data. Hence, on the one hand, you may find it convenient when analyzing one-hot encoded matrices of categorical variables with high cardinality to create them as sparse matrices because that will save you a lot of memory and computations. On the other hand, you may notice that XGBoost returns completely different models if you analyze some data represented as a dense matrix or as a sparse matrix.</p>

  <p class="body">The difference is in what happens when XGBoost encounters a missing example. During training, the algorithm learns at each split point whether samples with missing values should go to the left or right branching based on the resulting gain. When making predictions, samples with missing values are assigned to the appropriate child accordingly. This allows the algorithm to split on the feature value’s missingness pattern if it is predictive. If there are no missing values for a given feature during training, then samples with missing values are assigned to whichever child has the most samples.</p>

  <p class="body">You can use the missing parameter to specify what value XGBoost will have to consider as missing. This parameter is set to NaN by default, but you can decide on any value you want.</p>

  <p class="body">Another critical thing to remember about XGBoost is that the <code class="fm-code-in-text">gblinear</code> booster, using linear models as base learners, treats missing values as zeros. Suppose you standardize your numeric features, as is often used with linear models. In that case, the <code class="fm-code-in-text">gblinear</code> booster will treat missing values as the average value for that feature because the average takes the zero value in a standardized variable.</p>

  <p class="body">LightGBM employs a similar approach (see <a class="url" href="https://github.com/microsoft/LightGBM/issues/2921">https://github.com/microsoft/LightGBM/issues/2921</a>), using specific parameters:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">LightGBM enables the missing value to be handled by default. Turn it off by setting <code class="fm-code-in-text">use_missing=false</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">LightGBM uses NA (NaN) to represent missing values by default. Change it to use zero by setting <code class="fm-code-in-text">zero_as_missing=true</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">When <code class="fm-code-in-text">zero_as_missing=false</code> (default), the unrecorded values in sparse matrices (and LightSVM) are treated as zeros.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">When <code class="fm-code-in-text">zero_as_missing=true</code>, NA and zeros (including unrecorded values in sparse matrices [and LightSVM]) are treated as missing.</p>
    </li>
  </ul>

  <p class="body">This strategy for handling missing data works well on average, especially if your data is MCAR. This means the pattern of missing instances is completely random and unrelated to any other feature or hidden underlying process. The situation is different with MAR when missingness is related to other features’ values but not to the values of the feature itself and NMAR, where there is a systematic pattern of missing values related to the feature itself and other features. In MAR and NMAR cases, the best solution is to try to impute these values by other means because the XGBoost and LightGBM strategy for missing data may reveal itself as underperforming.</p>

  <p class="body">There are alternatives to imputing missing data, however. For instance, you can create missing data indicators, which are binary features valued in correspondence to missing instances in a variable. Missing data indicators can prove quite valuable if your data is not completely missing at random, and they can work with any classical machine learning algorithm. Another popular solution with decision trees is to assign missing values to an extreme value (usually a negative extreme value) not used by any variable in the dataset. If you use exact splits, not histogram-based ones, where values are reduced into bins, missing data replaced by extreme values can prove an efficient an<a id="idTextAnchor012"/>d easy solution.</p>

  <h3 class="fm-head1" id="heading_id_6">6.1.3 Target encoding</h3>

  <p class="body"><a id="marker-199"/>Categorical features, usually represented in a dataset as strings, can be efficiently dealt with through different strategies. We already mentioned one-hot-encoding in chapter 2 and chapter 4. As one-hot-encoding, all the other strategies for categorical features, whether presenting low or high cardinality, require <i class="fm-italics">encoding</i>, a procedure to transform data numerically into a suitable format for machine learning algorithms. Although there are some similarities, encoding is not to be confused with <i class="fm-italics">embeddings</i>, which is a procedure to reduce high-dimensional data, such as text or images, into a lower-dimensional space while preserving certain characteristics or relationships of the original data. Embeddings are typically learned through neural network-based models and are briefly touched on in our book.<a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>

  <p class="body">The Scikit learn package offers a couple of encoding solutions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">OneHotEncoder</code>—For one-hot encoding (i.e., transforming each unique string value into a binary feature), which is the solution we have up so far used<a id="idIndexMarker022"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">OrdinalEncoder</code>—For ordinal encoding (i.e., transforming the string values in a feature into ordered numeric ones; there is also <code class="fm-code-in-text">LabelEncoder</code>, but it works the same, and it is mainly for transforming categorical targets)<a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>
    </li>
  </ul>

  <p class="body">Generally, one-hot encoding works fine both for linear models and tree-based models, and ordinal encoding works fine for more complex tree-based models, such as random forests and GBDTs, because trees can recursively split on the categorical feature and finally find a set of partitions useful for predictions. However, problems arise with high cardinality categoricals when using one-hot or ordinal encoding. High cardinality is a weak point for both linear and tree-based mode<a id="idTextAnchor013"/>ls. When one-hot encoded, high cardinality categoricals produce sparse matrices that cannot easily be turned into dense ones because of memory limitations. In addition, decision trees with many branching levels may need help splitting ordinally encoded high cardinality categorical features into meaningful partitions for prediction.</p>

  <p class="body">There is no commonly fixed standard to declare when a categorical is high cardinality because that also depends on how many rows your dataset has and how large one-hot encoded features your computer memory could handle. However, high cardinality categorical features generally include IDs, zip codes, and product or geographical names with many unique values. For instance, a reasonable threshold could be over 512, but it may be lower depending on the dataset. Using the rule of thumb that the number of classes in a feature should not exceed 5%–10% of the total rows in the dataset, 512 may be too high for smaller datasets. In such circumstances, standard practice, especially from data science competitions such as Kaggle’s, suggests resorting to <i class="fm-italics">target encoding</i> (also known as <i class="fm-italics">mean encoding</i>). <a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>

  <p class="body">First presented in the paper by Micci-Barreca, “A Preprocessing Scheme for High-Cardinality Attributes in Classification and Prediction Problems” (ACM SIGKDD Explorations Newsletter 3.1, 2001), target encoding is simply a way to transform the values in a categorical feature into their corresponding expected target values. If your problem is a regression, target encoding will use the average target value corresponding to that value in the dataset, with a classification problem: conditional probability or odds ratio. Such a process, bringing about the risk of overfitting for the model when the category has few examples in the dataset, is mitigated by using a weighted average between the expected value for that category (posterior probability of the target) and the average expected value of all the dataset (the prior probability of the target over all the training data).<a id="idIndexMarker027"/><a id="marker-200"/></p>

  <p class="body">Target encoding is available in the<a id="idTextAnchor014"/> category-encoders package (<a class="url" href="https://mng.bz/gave">https://mng.bz/gave</a>), a Scikit-learn compatible project as the <a id="idTextAnchor015"/>target TargetEncoder class (<a class="url" href="https://mng.bz/5glq">https://mng.bz/5glq</a>), and you can install it by a <code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">install</code> <code class="fm-code-in-text">category_encoders</code> command in the shell. In the TargetEncoder class, you have to specify a smoothing parameter (to be fixed at a value above zero) to balance between the posterior probability of the target and the prior probability all over the training data. The best smoothing parameter for your data has to be found by experimentation, or you can rely on another similar encoder, the James Steiner encoder, which guesses the best way to smooth your expected target values based on the variance conditioned by the c<a id="idTextAnchor016"/>ategory you want to encode (<a class="url" href="https://mng.bz/5glq">https://mng.bz/5glq</a>). The James Stenier encoder makes stronger assumptions about your data. You have to decide among different ways to estimate conditional variance by the model parameter (for regression problems, it is advisable to use “independent” and for classification ones, “binary”). Still, it lifts you from experimenting with different blending thresholds as if it were a hyperparameter.<a id="idIndexMarker028"/></p>

  <p class="body">In our example, we use the <code class="fm-code-in-text">neighborhood</code> feature, which has over 200 unique values, and the latitude and longitude coordinates after mapping them into a 100 x 100 grid space. The mapping returns a feature presenting over 2,000 distinct values, which makes it a high cardinality categorical feature without any doubt. In listing 6.3, we first bin latitude and longitude and then combine them by summing them in a way that results in a distinct code for every bin combination of latitude and longitude. Binning is obtained by dividing the range between the feature’s minimum and maximum value into equal parts. Also, the code snippet performs binning on two separate features, resulting in sets of integer values for each feature. The values of one feature are multiplied by a power of 10, which is larger than the maximum value of the other feature. This ensures that a unique value is always obtained when the two sets of values are summed, regardless of the s<a id="idTextAnchor017"/>pecific values being summed.<a id="idIndexMarker029"/><a id="marker-201"/></p>

  <p class="fm-code-listing-caption">Listing 6.3 Creating a high cardinality categorical feature</p>
  <pre class="programlisting">def bin_2_cat(feature, bins=100):
    min_value = feature.min()
    bin_size = (feature.max() - min_value) / bins
    bin_values = (feature - min_value) / bin_size
    return bin_values.astype(int)                         <span class="fm-combinumeral">①</span>
    
data['coordinates'] = (
    bin_2_cat(data['latitude']) * 1000 
    + bin_2_cat(data['longitude']
)                                                         <span class="fm-combinumeral">②</span>
high_card_categorical += ['coordinates']
 
print(data[high_card_categorical].nunique())              <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Function to convert numerical data into categorical bins</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts latitude and longitude to categorical coordinates</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Prints the number of unique values in the high-cardinality categorical features</p>

  <p class="body">The code snippet closes by checking the number of unique values for each feature among the high cardinality ones:</p>
  <pre class="programlisting">neighbourhood     221
coordinates      2259</pre>

  <p class="body">With two categorical features considered high cardinality, we can add to our preprocessing pipeline the <code class="fm-code-in-text">TargetEn<a class="calibre" id="idTextAnchor018"/>coder</code> from category-encoders.<a id="idIndexMarker030"/></p>

  <p class="fm-code-listing-caption">Listing 6.4 Using target encoding in the pipeline</p>
  <pre class="programlisting">from category_encoders.target_encoder import TargetEncoder
from XGBoost import XGBClassifier
from sklearn.model_selection import KFold, cross_validate
from sklearn.metrics import accuracy_score, make_scorer
 
target_encoder = TargetEncoder(cols=high_card_categorical,  <span class="fm-combinumeral">①</span>
                               smoothing=0.5)               <span class="fm-combinumeral">②</span>
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
xgb = XGBClassifier(booster='gbtree',
                    objective='reg:logistic',
                    n_estimators=300,
                    max_depth=4,
                    min_child_weight=3)                     <span class="fm-combinumeral">③</span>
 
column_transform = ColumnTransformer(
    [
        ('low_card_categories', 
         categorical_onehot_encoding, 
         low_card_categorical),
       ('high_card_categories', 
        target_encoder, 
        high_card_categorical),
       ('numeric', 
        numeric_standardization, 
        continuous)
    ],
    remainder='drop',
    verbose_feature_names_out=True,
    sparse_threshold=0.0)                                   <span class="fm-combinumeral">④</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('model', xgb)])                                       <span class="fm-combinumeral">⑤</span>
 
cv_scores = cross_validate(estimator=model_pipeline,
                          X=data,
                          y=target_median,
                          scoring=accuracy,
                          cv=cv,
                          return_train_score=True,
                          return_estimator=True)        <span class="fm-combinumeral">⑥</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})",
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                 <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes TargetEncoder for high cardinality categorical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Smoothes value to blend prior and posterior probabilities</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initializes XGBoost classifier with specific hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines ColumnTransformer for preprocessing features with TargetEncoder for high cardinality categorical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a pipeline that combines preprocessing and modeling</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Performs five-fold cross-validation and obtaining evaluation metrics</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Prints mean accuracy, fit time, and prediction time from cross-validation</p>

  <p class="body"><a id="marker-202"/>When executed, the code procedures the results for running XGBoost on the problem with the extra help of the high cardinality categorical features. The results point to a slight improvement in the accuracy. Later in this chapter, we will investigate the weight of the contribution by target encoding when examining the explainability:<a id="idIndexMarker031"/></p>
  <pre class="programlisting">0.840 (0.004) fit: 4.52 secs pred: 0.06 secs</pre>

  <p class="body">Although target encoding is a convenient procedure because it may quickly transform any categorical into a numeric feature, in doing so, you have to pay attention to keeping all important information from your data. Target encoding renders further modeling of any interaction between features impossible. Let’s say, for instance, that you are working on an advertising response dataset with click results for many websites and advertising formats. If you encode both features, having transformed two high cardinality categoricals with potentially thousands and thousands of values, you may easily create any kind of classical model. However, after encoding, your model, whether linear or tree-based, won’t be able to grasp any possible interaction between the encoded features. In this case, the solution is to create a new feature beforehand, combining the two high cardinality categorical features and then target encode their combination.</p>

  <p class="body">Hence, as for other tools, we should consider the pros and cons of this advanced encoding technicality. In our experience, depending on the situation, before resorting to target encoding, there are a few options for classic machine learning algorithms and for gradient boosting for dealing with high cardinality categorical features:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Just dropping problematic categorical features</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Using a OneHotEncoder</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Using an OrdinalEncoder and treating categories as ordered equidistant quantities</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Using an OrdinalEncoder and relying on the native category support of gradient boosting histogram-based algorithms</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Using target encoding as a last resort</p>
    </li>
  </ul>

  <p class="body">Dropping features is only sometimes considered. However, we already mentioned in chapter 2 how you can evaluate how a nominal feature can contribute to predicting a target utilizing Cramer’s V measure of association.</p>

  <p class="body">When confronted with a high cardinality categorical feature, opting for one-hot encoding is almost necessary for linear models. When dealing with other models, such as decision trees and their ensembles, there might be a more suitable approach. This is because one-hot encoding creates an additional feature for each category value of the categorical feature. This leads to an increased number of split points that the tree-based model must consider during fitting. Consequently, using one-hot encoded data requires more depth in a decision tree to achieve an equivalent split that could be achieved with a single split point using a different way of handling the categorical feature.</p>

  <p class="body">For an ordinal encoder, the categories are encoded as 0, 1, 2, and so on, treating them as continuous features. While this approach can be misleading for linear models, it works effectively for decision trees. Decision trees can accurately split the data based on ordinal encoding, separating the categories according to their relationship with the target variable. This happens with XGBoost, which treats all the features as numerical, continuous features.<a id="marker-203"/></p>

  <p class="body">If we decide to use the native support for categorical features, this option is available in LightGBM and in the version of XGBoo<a id="idTextAnchor019"/>st provided by the H2O.ai library (<a class="url" href="https://mng.bz/6e75">https://mng.bz/6e75</a>). Native categorical support allows these models to handle categorical features more effectively, without converting them into numerical values. In that case, since native handling requires sorting categories, we expect the algorithm to be slightly slower when using native handling of categorical features with respect to treating categories as ordinal numbers. In the native category support, the sorting of the categories of a feature is based on the associated target variance for each category. Once the sorting has happened, the feature can be used as a <a id="idTextAnchor020"/>continuous numerical attribute.</p>

  <h3 class="fm-head1" id="heading_id_7">6.1.4 Transforming numerical data</h3>

  <p class="body">Decision trees can automatically handle nonlinearities and interactions in data. This is because they can split a variable at any point into two parts and then further split them repeatedly. This property comes in particularly handy for handling subtle and deep interactions in the data, with a caveat, because decision trees are quite rough approximators. Under the aspect of precisely modeling complex relationships in data, neural networks with enough examples are better approximators.<a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="marker-204"/><a id="idIndexMarker034"/></p>

  <p class="body">Figure 6.1 shows how a bagged ensemble of decision trees can approximate a nonlinear function. The result is an approximation constructed by a set of if-then-else decision rules that recursively divide the space. However, noise in the data can result in inaccuracies in certain parts of the space. In contrast, a neural network with the same number of nodes as the trees used in the bagged decision trees can provide a smoother and <a id="idTextAnchor021"/>more accurate curve estimation.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.1 Comparison of predictions between a neural network and a bagged trees ensemble for a random dataset with a noisy sine function</p>
  </div>

  <p class="body">Since GBDT is also based on decision trees, it may similarly struggle in shaping nonlinear functions using binary splits. Consequently, when using GBDT, and you know specific nonlinearities or interactions, it benefits you to explicitly define them by using transformations toward linear forms, binning or discretization, and precomputed interactions between features. For nonlinearities, transformations help reduce the number of splits. In addition, computing specific interactions beforehand also reduces the number of splits, which occur at better split points.</p>

  <p class="body">However, before applying such transformations, you need to understand your data. Linearities and nonlinearities, even if there is no relationship with the target, can be easily spotted after you complete fitting your training data by a partial dependence plot (PDP). This model-agnostic chart technique explains how features and targets are related through the model you have trained. <a id="idIndexMarker035"/></p>

  <p class="body">PDPs display how the target output changes based on specific input features while ignoring the effects of other input features. In other words, it shows us the average expected prediction if we set a certain value on all the data points of the specific input feature we are examining. The assumption underlying the analysis is that the input we represent by a PDP is independent of other features. Under such conditions, the PDP represents how the input feature directly affects the target. However, this assumption is often violated in practice, meaning that the input feature we are examining is usually not completely independent of the other features. As a result, the plot typically shows how the target value changes as we vary the value of the input feature, while also reflecting the average effects of the other features in the model.</p>

  <p class="body">In listing 6.5, we explore PDPs’ possible usage and limitations. Given an XGBoost model trained on our Airbnb NYC dataset, we demonstrate how our target changes regarding our numeric features, trying to spot any nonlinearities or other characteristics of the modeled data. The four resulting charts are plotted using matpl<a id="idTextAnchor022"/>otlib axes and are to be analyzed.<a id="marker-205"/></p>

  <p class="fm-code-listing-caption">Listing 6.5 Partial dependence plot</p>
  <pre class="programlisting">from XGBoost import XGBClassifier
import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay
 
xgb = XGBClassifier(booster='gbtree', 
                    objective='reg:logistic', 
                    n_estimators=300, 
                    max_depth=4,
                    min_child_weight=3)
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('XGBoost', xgb)])                                 <span class="fm-combinumeral">①</span>
 
model_pipeline.fit(X=data, y=target_median)
 
fig, axes = plt.subplots(
   nrows=2,
   ncols=2,
   figsize=(8, 4)
)                                                       <span class="fm-combinumeral">②</span>
fig.subplots_adjust(hspace=0.4, wspace=0.2) 
 
PartialDependenceDisplay.from_estimator(
    model_pipeline, 
    X=data, 
    kind='average',                                     <span class="fm-combinumeral">③</span>
    features=[
        'minimum_nights',
        'number_of_reviews', 
        'calculated_host_listings_count',
        'availability_365'
    ],                                                  <span class="fm-combinumeral">④</span>
    ax=axes
)
for ax in axes.flatten():
    ax.axhline(y=0.5, color='red', linestyle='--')      <span class="fm-combinumeral">⑤</span>
    
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a model pipeline combining data processing and XGBoost classifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a 2 × 2 subplot layout</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a partial dependence plot of the average effect</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A list specifying features for the plot</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Adds a red dashed line at y=0.5 on each subplot, a reference line for interpretation</p>

  <p class="body"><a id="marker-206"/>Figure 6.2 shows the four charts. The dashed line marks the classification threshold for one (above or equal to 0.5) and zero (below 0.5). The solid line describes the relationship between the feature value on the x-axis and the target probability on the y-axis. The tick marks on the x-axes point out the distribution deciles of the feature, hinting at ranges denser (where the ticks are next to each other) with values and at those sparser (where the ticks are farther from each other). Ranges sparser with values are less reliable in their estimates. For instance, <code class="fm-code-in-text">minimum_nights</code> and <code class="fm-code-in-text">calculated_host_listings_count</code> display a nonlinear pattern, whereas <code class="fm-code-in-text">number_of_reviews</code> and <code class="fm-code-in-text">availabi<a class="calibre" id="idTextAnchor023"/>lity_365</code> oscillate stationary.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F02_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.2 A panel of PDPs for numeric features</p>
  </div>

  <p class="body">Given such results, you may evaluate to try to transform <code class="fm-code-in-text">minimum_nights</code> and <code class="fm-code-in-text">calculated_host_listings_count</code> using transformative functions by trial and error, such as</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Square or cubic transformations</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Square root or cube root</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Log or exponent transformations</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Tangent, sine, and cosine transformations</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Inverse, squared inverse, cubed inverse, square root inverse, cube root inverse</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Log inverse, exponent inverse, tangent inverse, sine inverse, cosine inverse</p>
    </li>
  </ul>

  <p class="body">However, before rushing to test transformation, it is important to verify if the obtained PDP average curve represents that feature’s behavior under all circumstances. You can verify that using individual conditional expectation (ICE) plots. ICE plots are the single components of the PDP curve. You can obtain ICE plots with a sl<a id="idTextAnchor024"/>ight change in the previous code.<a id="marker-207"/><a id="idIndexMarker036"/></p>

  <p class="fm-code-listing-caption">Listing 6.6 ICE plots</p>
  <pre class="programlisting">import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay
 
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 4))
fig.subplots_adjust(hspace=0.4, wspace=0.2)
 
PartialDependenceDisplay.from_estimator(model_pipeline, 
                                        X=data, 
                                        kind='both',               <span class="fm-combinumeral">①</span>
                                        subsample=30,              <span class="fm-combinumeral">②</span>
                                        features=['minimum_nights', 
                                                  'number_of_reviews', 
  
                                                  'calculated_host_listings_count', 
                                                  'availability_365'],
                                       ax=axes)
 
for ax in axes.flatten():
    ax.axhline(y=0.5, color='red', linestyle='--')
    ax.legend().set_visible(False)
 
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a partial dependence plot showing both individual and average effects</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses a random subset of 30% of data for plotting efficiency</p>

  <p class="body">After running the code, you may examine the results, as seen in figure 6.3. You can see the same PDP average curve as before, represented with a dashed lighter line, and a sample of 30 curves taken randomly from the sample. Suppose you can verify that the sampled curves are being clustered together, approximately reproducing the shape of the average curve. In that case, you have a confirmation that the average PDP curve is representative of the behavior of the feature with respect to the target. Otherwise, as in our example, if the single curve appears different and dispersed, the other features somehow mediate the feature’s relationship because of collinearity or interactions, and you cannot benefit much<a id="idTextAnchor025"/> from transforming the feature.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F03_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.3 A panel of ICE plots for numeric features</p>
  </div>

  <p class="body">Up to now, we have just used PDP for numeric features. Still, you can also apply them to binary and categorical features after encoding them by one-hot encoding. In this case, you have first to compute the curve value by the stand-alone function <code class="fm-code-in-text">partial_dependence</code> and afterward represent the obtained values as bars (for PDP average curves) or boxplots (for PDP and ICE curves together). In the following listing, we extract the necessary values and create a box plot representation for the single l<a id="idTextAnchor026"/>evels of the <code class="fm-code-in-text">neighbourhood_group</code>.<a id="idIndexMarker037"/><a id="marker-208"/></p>

  <p class="fm-code-listing-caption">Listing 6.7 Partial dependence plot for binary features</p>
  <pre class="programlisting">from sklearn.inspection import partial_dependence         <span class="fm-combinumeral">①</span>
import matplotlib.pyplot as plt
 
pd_ice = partial_dependence(model_pipeline, X=data, 
                            features=['neighbourhood_group'], 
                            kind='both')
 
fig = plt.figure(figsize=(8, 5))
ax = fig.add_subplot(1, 1, 1)
labels = np.ravel(pd_ice['values'])
plt.boxplot(
    pd_ice["individual"].squeeze(),
    labels=labels
)                                                         <span class="fm-combinumeral">②</span>
ax.axhline(y=0.5, color='red', linestyle='--')            <span class="fm-combinumeral">③</span>
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports the partial_dependence function that computes the curve values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a box plot of individual ICE curves</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Adds a red dashed line at y = 0.5 on each subplot, a reference line for interpretation</p>

  <p class="body">Figure 6.4 shows the result, providing insights on how a flat’s location in Manhattan is usually associated with higher prices. The other locations are associated with lower prices, according to the model. However, Brooklyn shows the largest variability, with sometimes higher prices similar to Manhattan, clearly depending on other factors related to the exact position <a id="idTextAnchor027"/>or characteristics of the flat.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F04_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.4 For each binary feature, the boxplot associated target values obtained by PDP</p>
  </div>

  <p class="body">As with numeric features, PDP curves also provide useful insight on how to power up your model. For instance, they can be used to aggregate the levels of a categorical feature that behave the same—in our example, Bronx, Staten Island, and probably also Queens.</p>

  <p class="body">PDPs show us what we can expect from the target output based on the input features we’re interested in. They also help us understand the relationship between the target response and the input feature of interest, whether linear or nonlinear. By observing the shape of the curve drawn by the analysis, we can also figure out what transformation could linearize it. When providing the <code class="fm-code-in-text">features</code> parameter of the <code class="fm-code-in-text">PartialDependenceDisplay</code> function with tuples of features, the function will output a contour map showing the conjoint effects of two specific features. Discovering interactions this way is long and tedious, especially if you have many features to explore. A solution would be to automatically discover the potential interactions and then test them with the PDP conjoint chart. Detecting interactions automatically using XGBoost is straightforward by using a project such as XGBoost Feature Interactions Reshaped (XGBFIR; <a class="url" href="https://github.com/limexp/xgbfir">https://github.com/limexp/xgbfir</a>). The following listing shows an example you can run after installing the package by <code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">i<a class="calibre" id="idTextAnchor028"/>nstall</code> <code class="fm-code-in-text">xgbfir</code> on a command shell.<a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="marker-209"/><a id="idIndexMarker040"/></p>

  <p class="fm-code-listing-caption">Listing 6.8 Discovering interactions by XGBFIR</p>
  <pre class="programlisting">import xgbfir
xgbfir.saveXgbFI(
    model_pipeline['XGBoost'],
    feature_names=(
        model_pipeline['processing']
        .get_feature_names_out()
    ),
    OutputXlsxFile='fir.xlsx')                                    <span class="fm-combinumeral">①</span>
fir = pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 1') <span class="fm-combinumeral">②</span>
result = fir[["Interaction", "Gain"]].sort_values(by="Gain", 
  
ascending=False).head(10).round(2)                                <span class="fm-combinumeral">③</span>
for index, row in result.iterrows():
    print(f"{row['Interaction']}")
 
PartialDependenceDisplay.from_estimator(
    model_pipeline,
    X=data,
    kind='average',
    features=[(
        'minimum_nights',
        'calculated_host_listings_count')])                       <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates a report with xgbfir and saves it to an Excel file</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reads the Excel file created in the previous steps</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Extracts and sorts based on split gain the “Interaction” and “Gain” columns from the feature interaction report</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Generates a partial dependence plot for the features “minimum_nights” and “calculated_host_listings_count”</p>

  <p class="body">The code will print a series of interactions. If you work with a linear model, each interaction returned by XGBFIR should be tested because they could enhance your model’s performance. If you work with decision trees, you can ignore the ones that involve a binary feature and concentrate on only numeric ones. An example is the interaction between <code class="fm-code-in-text">minimum_nights</code> and <code class="fm-code-in-text">calculated_host_listings_count</code>. Figure 6.5 shows how combining them with specific values is strongly associated <a id="idTextAnchor029"/>with a positive target response.<a id="marker-210"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F05_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.5 Conjoint PDP for two numeric features</p>
  </div>

  <p class="body">In such cases, combining the numeric features by multiplying them will optimize your GDBT mode<a id="idTextAnchor030"/>l much faster and more effectively.</p>

  <h2 class="fm-head" id="heading_id_8">6.2 Selecting features</h2>

  <p class="body">Feature selection is not always necessary. Still, when it is, it plays a vital role in identifying the most valuable features for training among the existing set of features, whether they derive directly from the data extraction or are the product of your feature engineering work. By employing effective feature selection techniques, you can pinpoint and retain the most relevant features that contribute significantly to the machine learning process.<a id="idIndexMarker041"/><a id="marker-211"/></p>

  <p class="body">In chapter 2, section 2.2.3, we discussed avoiding collecting irrelevant and redundant features for the tasks based on your knowledge of the problem and exploratory data analysis. In the subsequent chapters, we discussed machine learning algorithms that handle irrelevant and redundant features.</p>

  <p class="body">In classic machine learning, we have a large set of algorithms, including the family of linear models, that are particularly susceptible to irrelevant and redundant features, reducing performance and accuracy. Uninformative and noisy features, deemed irrelevant because they lack a meaningful association with the target of the learning task, can pose significant challenges for linear models. This is due to the possibility of random alignment between the feature values and the target, which can mislead the algorithm and assign undue importance to these features. Linear models utilize all the features provided, making them particularly vulnerable since the more noisy features there are, the more the results will be degraded. Ensembles based on decision trees are instead less affected by irrelevant and redundant features because they automatically select what features to use and ignore. This also happens with deep learning. However, deep learning may not be as robust as decision tree ensemble methods when dealing with noisy or irrelevant features on tabular data. For optimal performance under such conditions, large amounts of data are required, as well as careful choice of architecture, such as using dropout, regularization, or batch normalization layers, and tuning of learning rates.</p>

  <p class="body">Feature selection benefits classic machine learning algorithms such as linear models. However, it is also valuable in the case of decision tree-based ensembles and deep learning architectures, and it should not be ignored that it makes a machine learning process faster because of fewer columns to handle. By selecting the features before training, these complex algorithms can achieve improved clarity and ease of explanation by distilling the most relevant and informative features and enabling a clearer understanding of the underlying patterns and relationships captured by the models. This simplification enhances interpretability and facilitates the communication of the algorithm’s decision-making process.</p>

  <p class="body">In the following sections, we discuss and test a few solutions that can work well, stand-alone or sequentially, to select only essential features for solving your tabular data problem with the best results. We discuss algorithms for figuring out both relevant features (the all-relevant set), which may lead to redundant but useful sets of features, from algorithms to select minimal subsets of features (the nonredundant set) that produce models comparable to the set of relevant features but with the added advantage of increased interpretabilit<a id="idTextAnchor031"/>y due to a fewer number of features.</p>

  <h3 class="fm-head1" id="heading_id_9">6.2.1 Stability selection for linear models</h3>

  <p class="body"><i class="fm-italics">Stability selection</i> is based on the idea that if you use a variable selection procedure, you won’t always get the same results if you subsample or bootstrap your data because of variability in the process itself. For instance, if you use L1 regularization for feature selection in a linear model, you may find that different samples may return different non-zero coefficients, especially for highly correlated features. <a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="marker-212"/></p>

  <p class="body">As we have discussed, the L1 regularization penalty results in sparsity in the coefficient estimates. It works by adding a penalty term to the loss function, the sum of the absolute values of the coefficients. Such a penalty term imposes a constraint on the sum of the absolute magnitudes of the coefficients, promoting some coefficients to become exactly zero. Consequently, the L1 regularization can effectively select features by shrinking some coefficients to zero and excluding the corresponding features from the model. In the presence of highly correlated features, the L1 regularization may face difficulty selecting a unique set of features due to their similarity in their contributions to the target variable. Here, chance plays a role in the fact that certain features get non-zero coefficients depending on what data you have in your sample. However, this can be used to our advantage.</p>

  <p class="body">By introducing randomness through data sampling, stability selection aims to identify features that consistently appear important across multiple subsets, indicating their robustness and reducing the likelihood of selecting features by chance or noise. Stability selection will provide a useful set of features, not a minimal one. By ruling out unimportant features, stability selection ensures that all the relevant features are identified, thus making it a perfect algorithm for the first step in reducing the number of your features.</p>

  <p class="body">Presented in the paper by Meinshausen and Büehlmann (<a class="url" href="https://arxiv.org/abs/0809.2932">https://arxiv.org/abs/0809.2932</a>), for some time, stability selection has been offered as part of Scikit-learn and then maintained among the Scikit-learn compatible projects. We can easily replicate its procedures using Scikit-learn’s <code class="fm-code-in-text">BaggingClassifier</code> with <code class="fm-code-in-text">LogisticRegression</code> with L1 regularization for a classification problem. You can also adopt the same code for regression problems, using <code class="fm-code-in-text">BaggingRegressor</code> with the L1 regression class, <code class="fm-code-in-text">Lasso</code>.</p>

  <p class="body">In our implementation, we test a series of C values for the L1 logistic regression against bootstrap resamples. The procedure creates a series of logistic regression coefficients that we can sum, average, or count how many times they differ from zero. Given that we are using a mix of binary and continuous features, we find it more useful to count the times the coefficient associated with a variable has an absolute value above a threshold. Thus, we can finally deem all the features that, most of the time, tend to have a relevant coefficient as relevant, whi<a id="idTextAnchor032"/>ch can affect the resulting prediction.</p>

  <p class="fm-code-listing-caption">Listing 6.9 Stability selection</p>
  <pre class="programlisting">import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier
 
lambda_grid=np.logspace(-4, -1, 10)                       <span class="fm-combinumeral">①</span>
sparse_coef = list()
 
for modeling_c in lambda_grid:
    estimator = LogisticRegression(
        solver='liblinear',
        penalty='l1',
        C=modeling_c
    )                                                     <span class="fm-combinumeral">②</span>
    model = BaggingClassifier(
        estimator, 
        n_estimators=100,
        bootstrap=True
    )                                                     <span class="fm-combinumeral">③</span>
    model_pipeline = Pipeline(
        [('processing', lm_column_transform),
         ('standardize', StandardScaler()),               <span class="fm-combinumeral">④</span>
         ('modeling', model)])
    model_pipeline.fit(data, target_median)
    sparse_coef += [estimator.coef_.ravel() for estimator in 
model_pipeline["modeling"].estimators_]
    
epsilon = 1e-2                                            <span class="fm-combinumeral">⑤</span>
threshold = 0.5                                           <span class="fm-combinumeral">⑥</span>
 
non_zero = (np.abs(sparse_coef) &gt; epsilon).mean(axis=0)
feature_names = model_pipeline["processing"].get_feature_names_out()
print(non_zero)
print(feature_names[non_zero &gt; threshold])</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates a grid of lambda values using a logarithmic scale for use in L1 regularization</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a Logistic Regression estimator with L1 (Lasso) penalty</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a BaggingClassifier that uses the Logistic Regression estimator as its base model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Standardizes after data processing renders all the coefficients comparable, no matter the scale</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets a small value as epsilon for a threshold</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sets a threshold value for selecting significant coefficients</p>

  <p class="body"><a id="marker-213"/>The output highlights both the distributions of relevant coefficients and the selected features:</p>
  <pre class="programlisting">[0.635 0.    0.9   0.7   0.592 1.    0.    0.6   0.593 0.444 0.6   0.506 0.7  ]
['low_card_categories__neighbourhood_group_Bronx'
 'low_card_categories__neighbourhood_group_Manhattan'
 'low_card_categories__neighbourhood_group_Queens'
 'low_card_categories__neighbourhood_group_Staten Island'
 'low_card_categories__room_type_Entire home/apt'
 'low_card_categories__room_type_Shared room' 'numeric__minimum_nights'
 'numeric__reviews_per_month' 'numeric__calculated_host_listings_count'
 'Numeric__availability_365']</pre>

  <p class="body">Stability selection offers several advantages. It can handle high-dimensional data, avoid overfitting by incorporating randomness, and provide a measure of feature importance that considers the selection process’s stability. It is commonly used in applications with large features, such as genomics, text mining, or image analysis. On the other hand, the selection algorithm is limited to classic machine learning algorithms that use L1 regularization and return a set of coefficients, which are among the ones we discussed before: logistic regression and lasso regression. You can extend the concept proposed by stability selection by using feature importance (many ensemble models estimate feature importance), such as done in Sciki<a id="idTextAnchor033"/>t-learn by the command <code class="fm-code-in-text">SelectFromModel</code> (<a class="url" href="https://mng.bz/oKej">https://mng.bz/oKej</a>), but things will get trickier because you’ll have to figure how what makes an importance estimate relevant and what selection threshold to use. In the next section, we reprise how feature importance works, and we present Boruta. Using a solid automatic feature selection procedure, this algorithm can figure out the relevant features for a decision-tree ensemble, such <a id="idTextAnchor034"/>as random forests or gradient boosting.<a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>

  <h3 class="fm-head1" id="heading_id_10">6.2.2 Shadow features and Boruta</h3>

  <p class="body">Boruta is a smart procedure for determining whether a feature is relevant in a machine learning problem by relying on the internal parameters of the model, such as coefficients in linear models or importance values based on gain, such as in decision trees and their ensembles. It was first published in “Feature Selection with the Boruta Package” by Miron B. Kursa and Witold R. Rudnicki [<i class="fm-italics">Journal of Statistical Software</i> 36 (2010): 1-13]; for a copy of the article, see <a class="url" href="https://www.jstatsoft.org/article/view/v036i11">https://www.jstatsoft.org/article/view/v036i11</a>.<a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="marker-214"/><a id="idIndexMarker050"/></p>

  <p class="body">Boruta, although innovative, presents quite a few analogies with stability selection. It can be used only with decision tree-based ensembles. To measure the relevance of a feature, as in stability selection, we look for non-zero coefficients. In Boruta, we count the times when the importance of a feature exceeds the highest importance obtained by shadow features. We call them hits. Shadow features are random versions of the feature themselves (basically shuffled features), which, given that they are random, should attain any importance just by chance. If any feature cannot exceed the same importance of a shadow feature, it cannot be considered more predictive than any random sequence of values.</p>

  <p class="body">A threshold for selection, usually a minimum number of occurrences of non-zero coefficients in stability selection, is given in Boruta by how the number of hits translates into a binomial distribution. A significance threshold for retaining or dropping a feature test according to the distribution if the number of hits can prove that the feature is consistently better than any random construct.</p>

  <p class="body">Listing 6.10 shows an example using Boruta to select all the relevant features for an XGBoost classification on the Airbnb NYC dataset. Boruta in the BorutaPy implementation (<a class="url" href="https://github.com/scikit-learn-contrib/boruta_py">https://github.com/scikit-learn-contrib/boruta_py</a>) has some limitations because, apart from working only with tree-based models, such as random forests or gradient boosting (no matter what the implementation is), it cannot work with pipelines. Hence, we first had to transform the data and then run Boruta on the transformed features as we were training the final model. Boruta takes as key parameters the estimator—that is, the model you want to use, the number of decision trees in the ensemble, and the <code class="fm-code-in-text">n_estimators</code> hyperparameter, which can be left empty, set to an integer, or set to “auto” where the number of trees is decided upon the size of the dataset. Other important parameters in Boruta are <code class="fm-code-in-text">max_iter</code>, the number of rounds of testing, usually set to 100, and the alpha threshold for the binomial test, which can be increased from 0.05 to allow for more features to be retained or<a id="idTextAnchor035"/> decreased for more features to be discarded.<a id="marker-215"/><a id="idIndexMarker051"/></p>

  <p class="fm-code-listing-caption">Listing 6.10 Boruta selection</p>
  <pre class="programlisting">from XGBoost import XGBClassifier
from boruta import BorutaPy
 
xgb = XGBClassifier(booster='gbtree', 
                    objective='reg:logistic', 
                    n_estimators=300, 
                    max_depth=4,
                    min_child_weight=3)
 
X = column_transform.fit_transform(data, target_median)                   <span class="fm-combinumeral">①</span>
boruta_selector = BorutaPy(estimator=xgb, n_estimators='auto', verbose=2) <span class="fm-combinumeral">②</span>
boruta_selector.fit(X, target_median)                                     <span class="fm-combinumeral">③</span>
selected_features = boruta_selector.support_                              <span class="fm-combinumeral">④</span>
selected_data = column_transform.get_feature_names_out()[selected_features]
print(selected_data)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Transforms the input data, performing any necessary preprocessing steps</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Initializes a BorutaPy feature selection object using an XGBoost classifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Fits the Boruta feature selector</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Retrieves a Boolean mask of selected features determined by the Boruta feature selector</p>

  <p class="body">After a few iterations, you should get the results of only a single feature discarded as not relevant to the problem:</p>
  <pre class="programlisting">Iteration:    50 / 100
Confirmed:    13
Tentative:    0
Rejected:    1
['low_card_categories__neighbourhood_group_Bronx'
 'low_card_categories__neighbourhood_group_Brooklyn'
 'low_card_categories__neighbourhood_group_Manhattan'
 'low_card_categories__neighbourhood_group_Queens'
 'low_card_categories__room_type_Entire home/apt'
 'low_card_categories__room_type_Private room'
 'low_card_categories__room_type_Shared room'
 'high_card_categories__neighbourhood' 'numeric__minimum_nights'
 'numeric__number_of_reviews' 'numeric__reviews_per_month'
 'numeric__calculated_host_listings_count' 'numeric__availability_365']</pre>

  <p class="body">The same procedure can be executed using LightGBM as predictor, instead of XGBoost:</p>
  <pre class="programlisting">from lightgbm import LGBMClassifier
 
lgbm = LGBMClassifier(boosting_type='gbdt', 
                      n_estimators=300, 
                      max_depth=4,
                      min_child_samples=3)
 
boruta_selector = BorutaPy(estimator=lgbm, n_estimators='auto', verbose=2) <span class="fm-combinumeral">①</span>
boruta_selector.fit(X, target_median)
selected_features = boruta_selector.support_
selected_data = column_transform.get_feature_names_out()[selected_features]
print(selected_data)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes a BorutaPy feature selection object using the provided LightGBM classifier</p>

  <p class="body">The result is reached after only 9 iterations, and this time, we have an increased number of rejected features:</p>
  <pre class="programlisting">Iteration:     9 / 100
Confirmed:     8
Tentative:     0
Rejected:      6
['low_card_categories__neighbourhood_group_Manhattan'
 'low_card_categories__room_type_Entire home/apt'
 'high_card_categories__neighbourhood' 'numeric__minimum_nights'
 'numeric__number_of_reviews' 'numeric__reviews_per_month'
 'numeric__calculated_host_listings_count' 'numeric__availability_365']</pre>

  <p class="body">LightGBM not only converges more quickly, but the way it splits allows, in this problem, the creation of a performing model with many fewer features than XGBoost.</p>

  <p class="body">In our example, we have trained on all the available data. Still, you can use Boruta even in a cross-validation loop, where you can consolidate a result for the dataset by using all the selected features in all the folds or by only those selected at leas<a id="idTextAnchor036"/>t a minimum number of times across the folds. <a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="marker-216"/><a id="idIndexMarker054"/></p>

  <h3 class="fm-head1" id="heading_id_11">6.2.3 Forward and backward selection</h3>

  <p class="body">One limitation of Boruta is that it selects all the relevant features for your problem but not the essential ones. This means you may end up with a list with redundant and highly correlated features that can be cut to a shorter selection. After applying Boruta, we suggest resorting to a sequential feature selection procedure, as implemented in the Scikit-learn function <code class="fm-code-in-text">SequentialFeatureSelector</code>. This procedure adds by forward selection or removes by backward elimination features from your selection based on their performance on the prediction in a greedy fashion—that is, always picking up the best-performing choice, based on the cross-validation score, in terms of addition or discard. The technique relies on the learning algorithm and its objective function. Hence, its selection will always be among the best possible. Since it is a greedy procedure, there is always the risk of choosing a local optimum set.<a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/></p>

  <p class="body">Sequential selection is a very effective procedure in reducing the number of features you have to deal with. Still, it is quite time-consuming because the algorithm has to evaluate all the candidates at each round. In the forward procedure, this will turn slower and slower as you proceed because, despite having fewer candidates to evaluate at each round, the increasing number of features used will slow down the training. However, in the backward procedure, you start slow and tend to accelerate after discarding a number of features. The backward procedure may be impractical if you start from many features to evaluate and the training is very slow.</p>

  <p class="body">As a stopping rule for the procedure, you may set a certain number of features, or you can leave the selection algorithm to find out the point at which adding or removing a feature bears no more improvements to the predictions. A tolerance threshold helps give the algorithm a certain freedom to proceed or not: the larger the tolerance, the more likely the algorithm will proceed in its operations, even if adding or removing a feature somehow deteriorates the performance.</p>

  <p class="body">In listing 6.11, we apply a forward selection to an XGBoost model trained on the Airbnb NYC dataset. The selection algorithm is set free to determine the correct number of features to add, and the low tolerance (set to 0.0001 on accuracy measures ranging from 0 to 1) should stop it at the first s<a id="idTextAnchor037"/>igns of deterioration of the predictive performances.<a id="marker-217"/></p>

  <p class="fm-code-listing-caption">Listing 6.11 Forward selection</p>
  <pre class="programlisting">from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import accuracy_score, make_scorer
from XGBoost import XGBClassifier
 
xgb = XGBClassifier(booster='gbtree', 
                    objective='reg:logistic', 
                    n_estimators=300, 
                    max_depth=4,
                    min_child_weight=3)
 
cv = KFold(5, shuffle=True, random_state=0)                  <span class="fm-combinumeral">①</span>
accuracy = make_scorer(accuracy_score)                       <span class="fm-combinumeral">②</span>
X = column_transform.fit_transform(data, target_median)
selector = SequentialFeatureSelector(
     estimator=xgb,
     n_features_to_select="auto",
     tol=0.0001,                                             <span class="fm-combinumeral">③</span>
     direction="forward",                                    <span class="fm-combinumeral">④</span>
     scoring=accuracy,
     cv=cv
)
selector.fit(X, target_median)
selected_features = selector.support_                        <span class="fm-combinumeral">⑤</span>
selected_data = column_transform.get_feature_names_out()[selected_features]
print(selected_data)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes a KFold cross-validation splitter object with five folds</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a scoring function for use in the feature selection process</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sets a tolerance value used by the sequential feature selector to determine convergence during the search</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Specifies the direction of feature selection (which is “forward” in this case)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Retrieves a boolean mask of selected features</p>

  <p class="body">The obtained results point out to six features to be used: three binary ones, a high cardinality categorical, and two numeric ones:</p>
  <pre class="programlisting">['low_card_categories__neighbourhood_group_Bronx'
 'low_card_categories__room_type_Entire home/apt'
 'low_card_categories__room_type_Shared room'
 'high_card_categories__neighbourhood' 'numeric__minimum_nights'
 'numeric__number_of_reviews' 'numeric__reviews_per_month'
 'numeric__calculated_host_listings_count' 'numeric__availability_365']</pre>

  <p class="body">We can replicate the experiment in a backward fashion by running the following commands:</p>
  <pre class="programlisting">selector = SequentialFeatureSelector(
     estimator=xgb,
     n_features_to_select="auto",
     tol=0.0001,
     direction="backward",                                  <span class="fm-combinumeral">①</span>
     scoring=accuracy,
     cv=cv
)
selector.fit(X, target_median)
selected_features = selector.support_
selected_data = column_transform.get_feature_names_out()[selected_features]
print(selected_data)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Specifies the direction of feature selection (which is “backward” in this case)</p>

  <p class="body">The resulting selection is made of nine features, many of those already seen in the set resulting from the forward selection:</p>
  <pre class="programlisting">['low_card_categories__neighbourhood_group_Bronx'
 'low_card_categories__neighbourhood_group_Manhattan'
 'low_card_categories__neighbourhood_group_Queens'
 'low_card_categories__neighbourhood_group_Staten Island'
 'low_card_categories__room_type_Entire home/apt'
 'low_card_categories__room_type_Shared room'
 'high_card_categories__neighbourhood' 'numeric__minimum_nights'
 'numeric__number_of_reviews' 'numeric__reviews_per_month'
 'numeric__calculated_host_listings_count' 'numeric__availability_365']</pre>

  <p class="body">In our own experience, choosing a forward or backward selection depends on the need you may have to risk leaving out some slightly important feature from your chosen set. With forward addition, you are sure to keep only the essential features but risk leaving something marginally relevant out. With the backward elimination, you are assured that all the key features are in the set, allowing for some redundancy.</p>

  <p class="body">Besides choosing a forward or backward procedure, sequential selection will help you build models faster in training and prediction and will be much easier to interpret and maintai<a id="idTextAnchor038"/>n because of the limited number of features involved.<a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="marker-218"/></p>

  <h2 class="fm-head" id="heading_id_12">6.3 Optimizing hyperparameters</h2>

  <p class="body">Feature engineering can improve the results you obtain from your classical machine learning models. Creating new features can reveal the underlying patterns and relationships in data that the models cannot grasp because of their limitations. Feature selection can improve your models’ results by removing unuseful and redundant features for the problem, thus reducing the noise and spurious signals in data. Finally, by optimizing hyperparameters, you can gain another performance boost and have your classical machine learning model shine on the tabular data problem you are dealing with.<a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>

  <p class="body">As discussed in chapter 4, hyperparameters are those settings that work under the hood of all machine learning algorithms and determine how specifically they can work. From an abstract point of view, each machine learning algorithm potentially offers a limited, yet still wide, range of functional forms—that is, the mathematical ways you can relate your predictor variables to your outcome. Straight out of the box, a machine learning algorithm may less or more match the functional form required by your specific machine learning problem.</p>

  <p class="body">For instance, if you are using a gradient boosting algorithm to solve a classification problem, it may be that the default number of iterations or how its trees are grown do not match the requirements of the problem. You may need fewer or more iterations and tree growth than specified by the default values. By opportunely setting its hyperparameters, you can find the best settings that work better with your problem.</p>

  <p class="body">However, it is not just a matter of twiddling all the many knobs that an algorithm presents until it gets the results you expect. Sometimes the knobs are too many to be tested together, and even if you manage to test enough of them, if not done properly, it will result in overfitting your data and, on the contrary, obtaining worse results. You need a systematic approach after defining one or more evaluation metrics:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Defining a search space containing the hyperparameters whose effects you want to explore and the boundaries of the values to test</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Building a proper cross-validation scheme to ensure that you are discovering a solution that generalizes beyond the data you have</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Choosing a search algorithm that, by a proper strategy, will find out in less time and with less cost—for instance, in terms of computations—the solution you need</p>
    </li>
  </ul>

  <p class="body">In the following subsections, under the light of different search strategies, we discuss the way you can accomplish tuning some of the<a id="idTextAnchor039"/> classical machine learning algorithms we have seen so far.<a id="marker-219"/></p>

  <h3 class="fm-head1" id="heading_id_13">6.3.1 Searching systematically</h3>

  <p class="body">Grid search works through all the possible combinations of hyperparameters’ values. For every hyperparameter you want to test, you pick a sequence of values and iterate through all their combinations exhaustively. In the end, you pick the combination that returns the best results.<a id="idIndexMarker065"/><a id="idIndexMarker066"/></p>

  <p class="body">In listing 6.12, we apply it to a logistic regression model, helping to choose the kind of regularization and the settings of L1 and L2 regularization values. The most important part of the code is the search grid, which is a list containing one or multiple dictionaries. Instead, each dictionary is a search space, a sequence of hyperparameters (the keys of the dictionary) associated with a list of a generator of values, which are the possible values you want to test (the values of the dictionary). Structuring one or more search spaces is a common practice across all the optimization methods, whether they are from Scikit-learn or not. Just notice how the name of the hyperparameters are formulated in the form <code class="fm-code-in-text">model__name_of_the_hyperparameter</code> because we are optimizing a pipeline and addressing parameters that are first internal to the pipeline and then of the model. We will com<a id="idTextAnchor040"/>e back to this in the next subsection with more explanations.<a id="marker-220"/></p>

  <p class="fm-code-listing-caption">Listing 6.12 Grid search</p>
  <pre class="programlisting">from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.metrics import make_scorer
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = LogisticRegression(solver="saga", max_iter=5_000)
 
model_pipeline = Pipeline(
    [('processing', lm_column_transform),
     ('model', model)])
 
search_grid = [
    {"model__penalty": [None]},
    {"model__penalty": ["l1", "l2"], "model__C": np.logspace(-4, 4, 10)},
    {"model__penalty": ["elasticnet"], "model__C": np.logspace(-4, 4, 10), 
     "model__l1_ratio": [.1, .3, .5, .7, .9, .95, .99]},
]                                                             <span class="fm-combinumeral">①</span>
 
search_func = GridSearchCV(estimator=model_pipeline,          <span class="fm-combinumeral">②</span>
                           param_grid=search_grid, 
                           scoring=accuracy, 
                           n_jobs=-1, 
                           cv=cv)
 
search_func.fit(X=data, y=target_median)
print (search_func.best_params_)                              <span class="fm-combinumeral">③</span>
print (search_func.best_score_)                               <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A list of dictionaries specifying a search grid of hyperparameters for the logistic regression model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Initializes a GridSearchCV object using the defined search grid</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Prints the best hyperparameters found by the grid search</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints the best score achieved by the model using the best hyperparameters found during the grid search</p>

  <p class="body">After testing all the combinations, the grid search procedure returns that the best set of hyperparameters is just not to use any penalty at all. It returns the best cross-validated score in support of its report:</p>
  <pre class="programlisting">{'model__penalty': None}
0.8210860006135597</pre>

  <p class="body">Grid search is effective when your hyperparameters are few; they take discrete values, and you can parallelize in memory the testing operations because your dataset is not too large.</p>

  <p class="body">First, the more combinations, the more tests you have to take and the longer and more computations you’ll have to spend. It could be a serious problem if you need to test many hyperparameters and suspect some of them are irrelevant for properly tuning your algorithm. When you add a hyperparameter to the grid search, you must make all the other hyperparameters cycle through it, which can turn into a waste of energy if the hyperparameter you are testing is irrelevant.</p>

  <p class="body">In addition, if a parameter takes continuous values, you must decide how to turn its continuous search space into a discrete one. Usually, this is done by uniformly dividing the continuum of values into discrete values, but by doing so without any knowledge of the way the algorithm behaves with respect to that hyperparameter and its values may again turn into wasting multiple computations on testing values that cannot improve the algorithm performances.</p>

  <p class="body">The last aspect to consider is using multiple cores and parallelizing their operations. Grid search is completely unaware of the results each test obtains. The results are only ranked at the end, and you are offered the best result. Hence, grid search is fine if your algorithm naturally works on a single core. However, suppose your algorithm uses multiple threads and cores, such as a random forest or an XGBoost. In that case, you have to trade off between having the algorithm running at full speed or having the optimization procedure go parallel and speedier. Usually, the best choice is to push the algorithm to run faster by using parallel running. Regardless of whether you decide to take advantage of the parallelization capabilities of the algorithm or those of the search procedure, grid search is not the best-performing option when working with a multicore algorithm.</p>

  <p class="body">Based on our experience and the limits of the grid search strategy, we deem it the best fit for testing linear models since they are easily parallelizable and have few limited paramete<a id="idTextAnchor041"/>rs, often characterized by taking Boolean or discrete values.<a id="idIndexMarker067"/><a id="marker-221"/><a id="idIndexMarker068"/></p>

  <h3 class="fm-head1" id="heading_id_14">6.3.2 Using random trials</h3>

  <p class="body">Important limitations when using grid search are that<a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">You need to discretize continuous hyperparameters.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">If a hyperparameter is irrelevant to the problem, you are going to have many trials wasted as they test the space of the irrelevant feature.</p>
    </li>
  </ul>

  <p class="body">For these reasons, the idea of sampling the search space randomly is rooted in the machine learning community. As described in the paper “Random Search for Hyper-Parameter Optimization” by James <a id="idTextAnchor042"/>Bergstra and Yoshua Bengio (<i class="fm-italics">Journal of Machine Learning Research;</i> <a class="url" href="https://mng.bz/nRg8">https://mng.bz/nRg8</a>), random search optimization becomes the standard optimization when you have many hyperparameters and you don’t know exactly how they affect the results or how they work together.</p>

  <p class="body">In our example, we reprise our classification problem using an XGBoost classifier. XGBoost, as with other gradient boosting implementations, features several hyperparameters that can be deemed important, and you should try to test them to check if your model’s performance can be improved. In the example, we also make things a bit more sophisticated because we operate by the XGBoost model wrapped into a pipeline, thus requiring a specific way to address hyperparameters. Since each element in the pipeline has a name, you must address each parameter in a part of the pipeline by the name of the element in the pipeline, two underlines, and then the name of the hyperparameter. For instance, in our example, XGBoost is in a part of the pipeline named “xgb.” To address the hyperparameter <code class="fm-code-in-text">n_estimators</code> of XGBoost, just use the label <code class="fm-code-in-text">xgb__n_estimators</code> in your search space. The idea is to demonstrate how to optimize a model and its pipeline without testing all t<a id="idTextAnchor043"/>he possible choices influencing a model’s predictive performance.<a id="marker-222"/></p>

  <p class="fm-code-listing-caption">Listing 6.13 Random search</p>
  <pre class="programlisting">from sklearn.utils.fixes import loguniform
from sklearn.model_selection import KFold, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import make_scorer
from XGBoost import XGBClassifier
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
 
xgb = XGBClassifier(booster='gbtree', objective='reg:logistic')
model_pipeline = Pipeline(
    [('processing', column_transform), ('xgb', xgb)]
)                                                               <span class="fm-combinumeral">①</span>
 
search_dict = {                                                 <span class="fm-combinumeral">②</span>
    'xgb__n_estimators': np.arange(100, 2000, 100), 
    'xgb__learning_rate': loguniform(0.01, 1),
    'xgb__max_depth': np.arange(1, 8),
    'xgb__subsample': np.arange(0.1, 0.9, 0.05),
    'xgb__colsample_bytree': np.arange(0.1, 0.9, 0.05),
    'xgb__reg_lambda': loguniform(1e-9, 100),
    'xgb__reg_alpha': loguniform(1e-9, 100)
}
 
search_func = RandomizedSearchCV(estimator=model_pipeline, 
                                 param_distributions=search_dict, 
                                 n_iter=60,                     <span class="fm-combinumeral">③</span>
                                 scoring=accuracy, 
                                 n_jobs=1,                      <span class="fm-combinumeral">④</span>
                                 cv=cv, 
                                 random_state=0)
 
search_func.fit(X=data, y=target_median)
print (search_func.best_params_)                                <span class="fm-combinumeral">⑤</span>
print (search_func.best_score_)                                 <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a pipeline that combines data processing and the XGBoost classifier</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A dictionary containing various hyperparameters with their search spaces for the RandomizedSearchCV</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Specifies the number of iterations for the random search process</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Specifies the number of parallel jobs to run for the search</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the best hyperparameters found by the RandomizedSearchCV</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Prints the best score achieved using the best hyperparameters found during the random search</p>

  <p class="body">After a while (the code runs in about one hour in a Google Colab instance), we get the set of the best parameters and the cross-validated score obtained:</p>
  <pre class="programlisting">{'xgb__colsample_bytree': 0.3500000000000001,
 'xgb__learning_rate': 0.020045491299569684,
 'xgb__max_depth': 6,
 'xgb__n_estimators': 1800,
 'xgb__reg_alpha': 3.437821898520205e-08,
 'xgb__reg_lambda': 0.021708909914764426,
 'xgb__subsample': 0.1}
0.8399836384088353</pre>

  <p class="body">Random search optimization, in spite of its simplicity and the fact it relies on randomness, really works, and it provides the best optimization in many situations. Many AutoML systems rely on this optimization strategy when there are many hyperparameters to tune (see, for instance, “Google Vizier: A Service for Black-Box Optimization,” by D. Golovinb et al., 2017 at <a class="url" href="https://mng.bz/8OrZ">https://mng.bz/8OrZ</a>). Compared to grid search, which works well when you have a limited set of hyperparameters that you expect to be impactful and a limited set of values to test, random search works the best when you have too many values to tune, without prior knowledge of how they work.</p>

  <p class="body">All you have to do is rely on enough random tests to have a good combination emerge, which may not take long. In our experience, 30 to 60 random draws usually suffice for good optimization. One strong point of random search optimization is that it works well for complex problems and is not affected by irrelevant hyperparameters. The number of relevant ones determines how fast you can find a good solution. The algorithm is also suited for parallel search on different computers or instances (you pick the best result among all). Still, this positive point comes with the limitation that since tests ar<a id="idTextAnchor044"/>e independent, they do not inform each other about their results.<a id="idIndexMarker071"/><a id="marker-223"/><a id="idIndexMarker072"/></p>

  <h3 class="fm-head1" id="heading_id_15">6.3.3 Reducing the computational burden</h3>

  <p class="body">Both grid search and random search do not utilize the outcomes from previous experiments. Grid search strictly adheres to a predefined procedure, while random search conducts a set of independent tests. In both cases, the prior results are not considered or used in any way during the search process. <i class="fm-italics">Successive halving</i>, a wrapper of both strategies, can instead take advantage of knowing the prior results. The idea is like that of a tournament where you first hold many rounds and put forth few resources to test different hyperparameters’ values. Then, as you progress and drop the values that underperform, you invest more resources to test the remaining values thoroughly. Usually, the resource you initially dilute and then later concentrate on is the number of training examples. More examples imply certain results from a hyperparameter test, but it costs more computational power.<a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="idIndexMarker075"/></p>

  <p class="body">Available as <code class="fm-code-in-text">HalvingGridSearchCV</code> and <code class="fm-code-in-text">HalvingRandomSearchCV</code> in Scikit-learn, in listing 6.14, we test the random search variant to verify if we can obtain similar optimization results at a fraction of the time. As stated, we use the number of samples as a scarce resource to optimize, using just the 30% available at the start. In addition, we instruct the algorithm to start from 20 initial candidates and to decrease the <a id="idTextAnchor045"/>number of candidates by three times at each round (from 20 to 6 to 2).<a id="idIndexMarker076"/><a id="idIndexMarker077"/></p>

  <p class="fm-code-listing-caption">Listing 6.14 Halving random search</p>
  <pre class="programlisting">from sklearn.experimental import (
    enable_halving_search_cv
)                                                           <span class="fm-combinumeral">①</span>
from sklearn.model_selection import HalvingRandomSearchCV
 
search_func = HalvingRandomSearchCV(
    estimator=model_pipeline,
    param_distributions=search_dict,
    resource='n_samples',                                   <span class="fm-combinumeral">②</span>
    n_candidates=20,                                        <span class="fm-combinumeral">③</span>
    factor=3,                                               <span class="fm-combinumeral">④</span>
    min_resources=int(len(data) * 0.3),                     <span class="fm-combinumeral">⑤</span>
    max_resources=len(data),                                <span class="fm-combinumeral">⑥</span>
    scoring=accuracy,
    n_jobs=1,
    cv=cv,
    random_state=0
)
search_func.fit(X=data, y=target_median)
print (search_func.best_params_)
print (search_func.best_score_)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Enables the experimental HalvingRandomSearchCV module</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Specifies that the resource being used for halving is the number of samples</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sets the number of candidates that will be sampled and evaluated at the first iteration</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Determines the factor by which the number of candidates will be reduced in each iteration</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets the minimum number of resources (samples) that will be used in the halving process</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sets the maximum number of resources (samples) that will be used in the halving process</p>

  <p class="body">The following are the results you obtain, at a fraction of the time previously required (in Google Colab the procedure runs in about 10 minutes):</p>
  <pre class="programlisting">{'xgb__colsample_bytree': 0.6500000000000001,
 'xgb__learning_rate': 0.02714215181104359,
 'xgb__max_depth': 7,
 'xgb__n_estimators': 400,
 'xgb__reg_alpha': 3.281921389446602,
 'xgb__reg_lambda': 0.00039687940902191534,
 'xgb__subsample': 0.8000000000000002}
0.8398409090909091</pre>

  <p class="body">In our experience with this optimization strategy, the strategy is to set the initial round in a way that it can catch some good hyperparameters. Hence it is important to have the highest possible number of candidates running at a minimum of resources, although not so low to compromise the results of optimization. Setting as little as 1,000 starting samples should work sufficiently well if the factor parameter is reduced. This determines the proportion of candidates selected for each subsequent ite<a id="idTextAnchor046"/>ration to two instead of three, thus going to a longer number of rounds.<a id="marker-224"/></p>

  <h3 class="fm-head1" id="heading_id_16">6.3.4 Extending your search by Bayesian methods</h3>

  <p class="body">Another optimization strategy that makes informed choices is Bayesian optimization. Introduced in the paper “Practical Bayesian Optimization of Machine Learning Algorithms” by Snoek, Larochelle, and Adams (<a class="url" href="https://arxiv.org/abs/1206.2944">https://arxiv.org/abs/1206.2944</a>), the idea behind this optimization strategy is to understand how the hyperparameters of a model work, by building a model of themselves. The algorithm optimizes a proxy function, called the surrogate function, to increase the algorithm’s performance. Of course, the surrogate function is updated by the feedback from the objective function of the machine learning model under optimization. However, the Bayesian optimization algorithm’s decisions are solely based on the surrogate function. <a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/></p>

  <p class="body">In particular, it is another that alternates exploration with exploitation: the acquisition function. The acquisition function reports how much exploring a certain combination of parameters is promising and how much is uncertain, based on the surrogate function. Exploration implies trying combinations of the parameters that have never been tried before, and this happens when there is much uncertainty, and thus consequently hope, in certain areas of the search space that need at least to be tried to improve the surrogate function. On the contrary, exploitation happens when the acquisition function ensures that the algorithm can improve performance when trying a certain set of hyperparameters.</p>

  <p class="body">As the “Bayesian” in the name implies, and from our brief description of how the Bayesian optimization works under the hood, the process is influenced by prior expectations and subsequently corrected by posterior observations in a fine-tuning cycle. The surrogate function in all of this is nothing more than a model of our model. Usually, Gaussian processes are chosen as a model for the surrogate function. Still, ther<a id="idTextAnchor047"/>e are alternatives, such as using tree algorithms such as random forests or tree-structured Parzen estimators, which are a multivariate distribution capable of describing the behavior of the hyperparameters in our model. Packages such as Scikit-optimize (<a class="url" href="https://scikit-optimize.github.io/stable/">https://scikit-optimize.github.io/stable/</a>) or KerasTuner (<a class="url" href="https://keras.io/keras_tuner/">https://keras.io/keras_tuner/</a>) use Gaussian processes, with Scikit-optimize also capable of using tree ensembles and KerasTuner using multiarmed bandits, as well. Optuna, an optimization framework developed by Preferred Networks, a Japanese AI research and development company, instead uses tree-structured Parzen estimators. Initially released in May 2019 as an open-source project, Optuna is particularly popular in the Python machine learning community due to its simplicity, versatility, and integration with popular machine learning libraries such as TensorFlow, PyTorch, and Scikit-learn.</p>

  <p class="body"><a id="marker-225"/>In our example, we use Optuna to improve our XGBoost classifier. When using Optuna, you just set up a study and provide its running parameters, such as the number of trials, <code class="fm-code-in-text">n_trials</code>, and the direction parameter if you want to minimize or maximize your objective function. Behind the scenes, all the heavy lifting is done by the objective function, which you define and returns an evaluation. The objective function expects just an input parameter, trial, which Optuna provides. By the trial parameter, you define the values of the hyperparameters to test. Then, you just test them as you like because it is up to you inside the objective function to decide if to apply a cross-validation, a simple test on a sample, or anything else. This flexibility also allows you to run complex optimizations where certain hyperparameters are used or depend <a id="idTextAnchor048"/>on others and their values. It is up to you to code the procedure you want.<a id="idIndexMarker081"/></p>

  <p class="fm-code-listing-caption">Listing 6.15 Bayesian search with Optuna</p>
  <pre class="programlisting">import optuna
from XGBoost import XGBClassifier
from sklearn.model_selection import cross_validate
 
def objective(trial):
    
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
        'learning_rate': trial.suggest_float(
            'learning_rate', 0.01, 1.0, log=True
        ),
        'subsample': trial.suggest_float('subsample', 0.1, 1.0),
        'colsample_bytree': trial.suggest_float(
            'colsample_bytree', 0.1, 1.0
        ),
        'max_depth': trial.suggest_int('max_depth', 1, 7),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
        'reg_lambda': trial.suggest_float(
            'reg_lambda', 1e-9, 100.0, log=True
         ),
        'reg_alpha': trial.suggest_float(
            'reg_alpha', 1e-9, 100.0, log=True
         ),
    }                                                          <span class="fm-combinumeral">①</span>
    
    xgb = XGBClassifier(
        booster='gbtree', 
        objective='reg:logistic',
        **params
    )                                                          <span class="fm-combinumeral">②</span>
    model_pipeline = Pipeline(
        [('processing', column_transform), ('xgb', xgb)]
    )
    accuracy = make_scorer(accuracy_score)
    cv = KFold(5, shuffle=True, random_state=0)
 
    cv_scores = cross_validate(estimator=model_pipeline, 
                               X=data, 
                               y=target_median,
                               scoring=accuracy,
                               cv=cv)                          <span class="fm-combinumeral">③</span>
 
    cv_accuracy = np.mean(cv_scores['test_score'])
    return cv_accuracy                                         <span class="fm-combinumeral">④</span>
 
study = optuna.create_study(direction="maximize")              <span class="fm-combinumeral">⑤</span>
study.optimize(objective, n_trials=60)                         <span class="fm-combinumeral">⑥</span>
print(study.best_value)                                        <span class="fm-combinumeral">⑦</span>
print(study.best_params)                                       <span class="fm-combinumeral">⑧</span><a id="marker-226"/></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A dictionary defining the search space for hyperparameters for Optuna</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates an XGBoost classifier with hyperparameters suggested by Optuna</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Performs cross-validation to evaluate the model’s performance using the hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A function acting as the objective value for optimization by returning the mean accuracy score from cross-validation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates an Optuna study object with the goal of maximizing the objective function</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Starts the optimization process using the defined objective function and a maximum of 60 trials</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Prints the best-achieved value of the objective function</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Prints the best hyperparameters found by Optuna</p>

  <p class="body">On a Google Colab instance, the process can take up to two hours, but the results are by far the best in class you can obtain from hyperparameter optimization:</p>
  <pre class="programlisting">{'n_estimators': 1434, 
 'learning_rate': 0.013268588739778429,
 'subsample': 0.782534239551612,
 'colsample_bytree': 0.9427647573058971,
 'max_depth': 7,
 'min_child_weight': 2,
 'reg_lambda': 2.3123673571345327e-06,
 'reg_alpha': 1.8176941971395193e-05}
0.8419879333265161</pre>

  <p class="body">As an extra feature offered by Optuna, with a few simple additions to the previous code, you can store your study in a project database and restart optimization at any time. Optuna can integrate its optimization procedures with SQLite if, at the time of the creation of the study, you declare the name of the study and a target database:</p>
  <pre class="programlisting">sqlite_db = "sqlite:///sqlite.db"                                      <span class="fm-combinumeral">①</span>
study_name = "optimize_XGBoost"                                        <span class="fm-combinumeral">②</span>
study = optuna.create_study(storage=sqlite_db, study_name=study_name, 
                            direction="maximize", load_if_exists=True) <span class="fm-combinumeral">③</span>
study.optimize(objective, n_trials=60)
 
print(study.best_params)
print(study.best_value)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines the path to the SQLite database where Optuna will store study-related information</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Provides a name for the Optuna study</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates an Optuna study object and connects it to the SQLite database</p>

  <p class="body"><a id="marker-227"/>Regarding the specification of the SQLite storage database, <code class="fm-code-in-text">sqlite://</code> is a Uniform Resource Identifier (URI) scheme used to specify the protocol or mechanism for connecting to an SQLite database. In the context of the URI scheme, <code class="fm-code-in-text">sqlite://</code> indicates that the database connection will be established using the SQLite database engine. When using this URI scheme, the <code class="fm-code-in-text">sqlite://+</code> portion is followed by the path to the SQLite database file. In your example, <code class="fm-code-in-text">sqlite:///sqlite.db</code> specifies that the SQLite database file is named <code class="fm-code-in-text">sqlite.db</code> and is located in the current directory. The three slashes (<code class="fm-code-in-text">///</code>) after <code class="fm-code-in-text">sqlite:</code> are optional and indicate that the path is relative to the current directory.</p>

  <p class="body">Once the study has been completed, you can also obtain useful visualization regarding the results of the iterations and gain insights useful on successive runs of the same search. For instance, you can explore the optimization history and check if you have reached a plateau in the optimization or if going on with more iterations is advisable:</p>
  <pre class="programlisting">fig = optuna.visualization.plot_optimization_history(study)
fig.show()</pre>

  <p class="body">Figure 6.6 shows how our optimization proceeded. After a few iterations, the optimization reached a good result, but then it struggled to progress through the rest of the available iterations. Under such conditions, further progress in optim<a id="idTextAnchor049"/>ization is improbable because any gains cannot be but tiny at this point.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F06_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.6 History of optimization results across the trials</p>
  </div>

  <p class="body">Another useful plot depicts how the hyperparameters have been determinant in the resulting optimum settings:</p>
  <pre class="programlisting">fig = optuna.visualization.plot_param_importances(study)
fig.show()</pre>

  <p class="body"><a id="marker-228"/>Figure 6.7 shows the estimated importance of our optimization of the XGBoost algorithm. The results appear dominated by the <code class="fm-code-in-text">max_depth</code> hyperparameter and somehow by the subsample values. Such an outcome suggests that the algorithm is susceptible to the depth of the trees and that increasing the depth significantly affects the optimization results. This could indicate that the data contains complex patterns that require deeper trees to capture, and the sweet point of seven found by the o<a id="idTextAnchor050"/>ptimization marks a point after which the algorithm starts overfitting.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F07_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.7 A plot chart of hyperparameter estimated importance during the Optuna optimization process</p>
  </div>

  <p class="body">Understanding why your XGBoost (or LightGBM) behaves better under certain conditions differs from problem to problem. However, being able to understand why, explain the reasons to others (such as the stakeholders), and take steps to adjust your data or optimization settings is indeed an invaluable feature offered by Optuna in comparison to other optimization methods.</p>

  <p class="body">After completing the panoramic illustration on optimization techniques, we are left dealing with the case when you don’t want to set up anything complicated to make your machine learning algorithms work but y<a id="idTextAnchor051"/>ou need some direction on how to make fast adjustments by trial and error.<a id="idIndexMarker082"/><a id="idIndexMarker083"/><a id="idIndexMarker084"/></p>

  <h3 class="fm-head1" id="heading_id_17">6.3.5 Manually setting hyperparameters</h3>

  <p class="body">Despite the efficiency of the previously described optimization strategies, you may not be surprised to read that we know that many practitioners still tune the settings of their models by intuition and trial and error. Such a procedure seems particularly well-grounded during the experimentation phase when you try to make everything work reasonably as you look for ways to improve your solution in various iterations. A thorough optimization is, therefore, left after the processing and experimentation iterations have been completed.<a id="idIndexMarker085"/><a id="idIndexMarker086"/><a id="marker-229"/></p>

  <p class="body">The book’s appendices provide a comprehensive guide to the key parameters of the machine learning algorithms covered thus far. We begin with linear models, such as linear or logistic regressions, which can be effectively tuned using a grid search due to their limited number of parameters and ease of discretization. A table covers random forests and extremely randomized trees, as they share similar hyperparameters, being based on the same bootstrapped ensemble approach.</p>

  <p class="body">Regarding GBDTs, we have different sets of hyperparameters depending on the specific implementation. For your convenience, we have selected the most essential ones. Feel free to use them along with the proposed ranges for manual or automatic optimization. The guide starts with HistGradientBoosting and then covers XGBoost and LightGBM. It’s important to note that XGBoost <a id="idTextAnchor052"/>has a larger set of relevant hyperparameters (you can find the complete list at <a class="url" href="https://mng.bz/6e7e">https://mng.bz/6e7e</a>). Lastly, we include the list of hyperparameters for L<a id="idTextAnchor053"/>ightGBM, which differs slightly from XGBoost (you can find the complete list at <a class="url" href="https://mng.bz/vK8q">https://mng.bz/vK8q</a>). This comprehensive guide will aid you in effectively tuning the machine learning algorithms and optimizing their performance based on the specific hyperparameter settings.</p>

  <p class="body">As for manually tuning GBDTs, the models tend to work worst out of the box, so you should be aware of a few tricks of the trade. Let’s begin with a 1999 paper titled “Greedy Function Approximation: A Gradient Boosting Machine” by Jerome Friedman. In this paper, Friedman discusses the tradeoff between the number of trees and the learning rate. It was observed that lower learning rates tend to result in higher optimal numbers of trees. Furthermore, it is advisable to reduce the learning rate when increasing the maximum depth of the decision trees in your model. This precautionary measure is because deeper trees introduce more complexity, potentially leading to overfitting. Overfitting occurs when the model becomes excessively tailored to the training data and performs poorly on unseen data. By simultaneously reducing the learning rate, you can mitigate this risk. This is because a lower learning rate translates to smaller and more cautious updates to the model. This gradual learning process allows for finer adjustments, helping the model strike a better balance between capturing complex relationships and avoiding overfitting.</p>

  <p class="body">Another great resource for hints on manually adjusting parameters in a GBDT is Owen Zhang’s talk to the NYC Data Science Academy in 2015 titled “Winning Data Science Competitions.” Owen, previously a top competitor on Kaggle, provided a few interesting tips:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Decide the number of trees to use based on the dataset size (usually in the range of 100 to 1,000) and keep it fixed during the optimization. Prefer fewer trees to more.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Test the learning rate in the range from 2 to 10 divided by the number of trees. Hence, for 1,000 trees, test learning rates in the interval from 0.002 to 0.01.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Test row sampling on 0.5, 0.75, 1.0 values.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Test column sampling on 0.4, 0.6, 0.8, 1.0 values.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Test max tree depth on 4, 6, 8, 10 values.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Tune the minimum leaf weight/count as an approximate ratio of 3 over the square root of the percentage of the rarest class you have to predict. Therefore, if the class you need to predict has a 10% coverage in the data, you should set the minimum leaf weight/count to about 9. This figure is calculated by dividing 3 by the square root of 0.1 (since 10% coverage is 0.1 as a decimal).</p>
    </li>
  </ul>

  <p class="body"><a id="marker-230"/>In the concluding section, we keep exploring some id<a id="idTextAnchor054"/>eas and tricks to master even better GBDTs when solving tabular data problems. <a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/><a id="idIndexMarker090"/></p>

  <h2 class="fm-head" id="heading_id_18">6.4 Mastering gradient boosting</h2>

  <p class="body">Having discussed how gradient boosting works and its implementations, we close this chapter with suggestions about how to use gradient boosting at its best, understand h<a id="idTextAnchor055"/>ow it works under the hood, and speed it up to cut time at training and prediction.</p>

  <h3 class="fm-head1" id="heading_id_19">6.4.1 Deciding between XGBoost and LightGBM</h3>

  <p class="body">When considering using gradient boosting for your data problem, XGBoost and LightGBM (along with HistGradientBoosting) are among the most popular and high-performing implementations of histogram gradient boosted machines. Despite their being so powerful, in our experience, you never can a priori go for XGBoost or LightGBM or just generally favor GBDTs in regards to other classical or deep learning solutions because of the no free lunch theorem in machine learning: there is no universal learning algorithm that performs best for all possible problems. Hence, stating that “XGBoost is all you need” for tabular data problems is surely a catchy phrase, but it may not always fit your specific problem or situation with data. GDBTs often tend to overperform other solutions for tabular data. Thus, starting with them, but not limited to them, is a good choice. Returning to specific implementations, while it is always advisable to test any algorithm on your data and make your own decisions, there are also a few other criteria to consider when deciding whether to try one implementation first or the other. We<a id="idTextAnchor056"/> have validated them based on our own experience. They are summarized in table 6.1.<a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="marker-231"/></p>

  <p class="fm-table-caption">Table 6.1 Criteria to consider when using GBDTs</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="25%"/>
      <col class="contenttable-0-col" span="1" width="75%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Type</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Description</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Amount of data</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">XGBoost works fine for all tabular problems; LightGBM, because of its leaf-wise splitting method that can create deeper trees, tends to overfit more often with smaller datasets.</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Scalability</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">XGBoost is more scalable and GPU ready; LightGBM struggles more.</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Speed of experimentation</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">On CPUs, LightGBM is undoubtedly faster than XGBoost.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">The availability of large amounts of data is the first criterion to consider. LightGBM uses leaf-wise (vertical) growth, which can result in overfitting. The tendency to overfit the data available explains well the algorithm’s success in Kaggle competitions. Hence, LightGBM works better when you have a lot of data available. In contrast, XGBoost builds more robust models than LightGBM on smaller data samples.</p>

  <p class="body">Another criterion to consider is whether you have access to multiple GPUs and strong CPUs or more limited access to computational resources. If you have plenty of resources, XGBoost is more scalable, making it a better option for use in institutional or business settings. However, if you prefer to focus on experimentation and feature engineering and cannot access GPUs, LightGBM makes more sense because of its faster training time. You can use the saved training time to improve the robustness of your final model. If you have limited resources, such as a stand-alone computer, you should consider that the training time for XGBoost increases linearly with the sample size, while LightGBM requires a much smaller fraction of training time.</p>

  <h3 class="fm-head1" id="heading_id_20">6.4.2 <a id="idTextAnchor057"/>Exploring tree structures</h3>

  <p class="body">As previously discussed, GBDTs are complicated algorithms, not inexplicable or unreplicable ones. You just need to reproduce the various decision trees they are made of in a more performing way and combine them to obtain your fast predictions. Both XGboost and LightGBM allow for exploring and extracting their model’s structure. In listing 6.16, we take a few steps to demonstrate that. After dumping an XGBoost simple solution on a JSON file, we navigate inside its structure like a graph, using a depth-first search strategy. In depth-first search, the algorithm explores each branch as far as possible before backtracking.<a id="idIndexMarker094"/><a id="idIndexMarker095"/></p>

  <p class="body"><a id="marker-232"/>Taking a closer look at the code in listing 6.16, you can notice that in the <code class="fm-code-in-text">traverse_xgb_tree</code> function, the code recursively explores the tree by first traversing the left subtree (<code class="fm-code-in-text">tree['children'][0]</code>) and then the right subtree (<code class="fm-code-in-text">tree['children'][1]</code>). This is evident from the recursive calls <code class="fm-code-in-text">traverse_xgb_tree(tree['children'][0])</code> and <code class="fm-code-in-text">traverse_xgb_tree(tree['children'][1])<a class="calibre" id="idTextAnchor058"/></code>.<a id="idIndexMarker096"/></p>

  <p class="fm-code-listing-caption">Listing 6.16 Extracting XGBoost tree structure</p>
  <pre class="programlisting">import json
import matplotlib.pyplot as plt
from XGBoost import XGBClassifier, plot_tree
from collections import namedtuple
 
xgb = XGBClassifier(booster='gbtree',
                    objective='reg:logistic',
                    n_estimators=10,
                    max_depth=3)                           <span class="fm-combinumeral">①</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('XGBoost', xgb)])                                    <span class="fm-combinumeral">②</span>
 
model_pipeline.fit(X=data, y=target_median)
model = model_pipeline["XGBoost"]
tree_info = model.get_booster().dump_model(
    "xgb_model.json",
    with_stats=True,
    dump_format="json"
)                                                          <span class="fm-combinumeral">③</span>
 
fig, ax = plt.subplots(figsize=(12, 15), dpi=300)
ax = plot_tree(
    model, num_trees=0, ax=ax, rankdir='LR
)                                                          <span class="fm-combinumeral">④</span>
plt.show()
 
with open("xgb_model.json", "r") as f:
    json_model = json.loads(f.read())                      <span class="fm-combinumeral">⑤</span>
 
print(f"Number of trees: {len(json_model)}")
tree_structure = json_model[0]                             <span class="fm-combinumeral">⑥</span>
 
Split = namedtuple("SplitNode", "feature origin gain count threshold")
Leaf = namedtuple("LeafNode", "index origin count")
 
def extract_xgb_node_info(tree):
    return [tree['split'], tree['origin'], tree['gain'],
            tree['cover'], tree['split_condition']]        <span class="fm-combinumeral">⑦</span>
 
def extract_xgb_leaf_info(tree):
    return (
        [tree['nodeid'], 
         tree['origin'], 
         tree['cover']
        ]
)                                                          <span class="fm-combinumeral">⑧</span>
 
def traverse_xgb_tree(tree):                               <span class="fm-combinumeral">⑨</span><a id="marker-233"/>
    if not 'origin' in tree:
        tree['origin'] = "="
    if not 'children' in tree:
        return [[Leaf(*extract_xgb_leaf_info(tree))]]
    left_branch = tree['children'][0]
    right_branch = tree['children'][1]
    left_branch['origin'] = '&lt;'
    right_branch['origin'] = '&gt;='
    left_paths = traverse_xgb_tree(left_branch)
    right_paths = traverse_xgb_tree(right_branch)
    node_info = [Split(*extract_xgb_node_info(tree))]
    return [node_info + path for path in left_paths + right_paths]
 
paths = traverse_xgb_tree(tree_structure)
 
print(f"Number of paths on tree: {len(paths)}")
print("Path 0:", paths[0])</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates an XGBoost classifier limited to 10 estimators and trees of three levels</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts the XGBoost model from the pipeline</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Dumps the XGBoost model’s information (the booster) into a JSON file</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a plot of the first tree in the ensemble</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Retrieves the JSON structure from disk with the model’s information</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Prints the number of trees in the model and extracts the structure of the first tree</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Function extracting various information from a split node in the tree structure</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Function extracting information from a leaf node in the tree structure</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Function traversing the tree structure recursively to extract paths</p>

  <p class="body">The code trains a XGBoost model, saves its tree structure, processes the structure into a readable way, and presents the results to the user:</p>
  <pre class="programlisting">Number of trees: 10
Number of paths on tree: 8
Path 0: [SplitNode(
             feature='f5', 
             origin='=', 
             gain=19998.9316, 
             count=12223.75,
             threshold=0.5),
             SplitNode(
                 feature='f2',
                 origin='&lt;',
                 gain=965.524414,
                 count=5871.5,
                 threshold=0.5
             ), 
            SplitNode(
                 feature='f13',
                 origin='&lt;',
                 gain=66.1962891,
                 count=3756,
                 threshold=1.88965869
            ), 
            LeafNode(
                 index=7,
                 origin='&lt;',
                 count=3528)
]</pre>

  <p class="body">Figure 6.8 compares the obtained outputs with the graphical representation of the complete tree, as provided by the <code class="fm-code-in-text">plot_tree</code> from the XGBoost package itself.<a id="idIndexMarker097"/><a id="marker-234"/><a id="idTextAnchor059"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F08_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.8 XGBoost’s <code class="fm-code-in-text">plot_tree</code> output</p>
  </div>

  <p class="body">From the 10 trees built in the model, the code presents the first tree and, among the 8 different paths available from the sample to the prediction leaf, it represents the first path. Visually, this path is the leftmost one. The path is made up of different nodes in sequence. The code reports the name of the used feature, the split branch origin from the previous node (in XGBoost, minor always stands for the left branch, and major is equal to the right branch), the cut threshold, the gain with respect to the objective function, and the resulting reduction in the sample given the split of the dataset. All this information allows you to perfectly replicate the results of every tree of an XGBoost model.</p>

  <p class="body">We can also extract the same tree structure from LightGBM, though the approach is a bit different because the LightGBM package follows a few slightly different conventions. For instance, XGBoost always splits the minus than the threshold on the left; LightGBM instead, for each node, defines a rule using minus or major-equal and threshold and splits on the left if the rule is true and on the right if it is fals<a id="idTextAnchor060"/>e.</p>

  <p class="fm-code-listing-caption">Listing 6.17 Extracting LightGBM tree structure<a id="marker-235"/></p>
  <pre class="programlisting">from lightgbm import LGBMClassifier, plot_tree
 
lgbm = LGBMClassifier(boosting_type='gbdt', 
                      n_estimators=10, 
                      max_depth=3)
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('lightgbm', lgbm)])
 
model_pipeline.fit(X=data, y=target_median)
model = model_pipeline["lightgbm"]
 
tree_info = model._Booster.dump_model()["tree_info"]           <span class="fm-combinumeral">①</span>
tree_structure = tree_info[0]['tree_structure']                <span class="fm-combinumeral">②</span>
plot_tree(
    booster=model._Booster,
    tree_index=0,
    dpi=600
)                                                              <span class="fm-combinumeral">③</span>
 
Split = namedtuple(
    "SplitNode",
    "feature origin decision_type threshold gain count"
)
Leaf = namedtuple("LeafNode", "index origin count value")
 
def extract_lgbm_node_info(tree):                              <span class="fm-combinumeral">④</span>
    return [tree['split_feature'], tree['origin'], tree['decision_type'],
            tree['threshold'], tree['split_gain'], tree['internal_count']]
 
def extract_lgbm_leaf_info(tree):                              <span class="fm-combinumeral">⑤</span>
    return [
         tree['leaf_index'],
         tree['origin'],
         tree['leaf_count'], 
         tree['leaf_value']
    ]
 
def traverse_lgbm_tree(tree):                                  <span class="fm-combinumeral">⑥</span>
    if not 'origin' in tree:
        tree['origin'] = ""
    if not 'left_child' in tree and not 'right_child' in tree:
        return [[Leaf(*extract_lgbm_leaf_info(tree))]]
    left_branch = tree['left_child']
    right_branch = tree['right_child']
    left_branch['origin'] = 'yes'
    right_branch['origin'] = 'no'
    left_paths = traverse_lgbm_tree(left_branch)
    right_paths = traverse_lgbm_tree(right_branch)
    node_info = [Split(*extract_lgbm_node_info(tree))]
    return [node_info + path for path in left_paths + right_paths]
 
paths = traverse_lgbm_tree(tree_structure)
print(paths[0])</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Extracts the tree information from the LightGBM model booster</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts the structure of the first tree from the tree information</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Plots the first tree in the ensemble using the plot_tree function</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Function extracting various information from a split node in the LightGBM tree structure</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Function extracting information from a leaf node in the LightGBM tree structure</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Function recursively traversing the LightGBM tree structure to extract paths</p>

  <p class="body">The results from this exploration report a path from the structure of the first decision tree in the ensemble:</p>
  <pre class="programlisting">[SplitNode(
    feature=5, 
    origin='',
    decision_type='&lt;=',
    threshold=1.0000000180025095e-35,
    gain=20002.19921875,
    count=48895),
 SplitNode(
    feature=2,
    origin='yes',
    decision_type='&lt;=',
    threshold=1.0000000180025095e-35,
    gain=967.0560302734375,
    count=23486),
 SplitNode(
    feature=13,
    origin='yes',
    decision_type='&lt;=',
    threshold=1.8896587976897459,
    gain=67.53350067138672,
    count=15024), 
 LeafNode(
    index=0,
    origin='yes',
    count=14112,
    value=-0.16892421857257725)
]</pre>

  <p class="body">Figure 6.9 shows the entire tree plotted by the <code class="fm-code-in-text">plot_tree</code> function, this time from the LightGBM package.<a id="idIndexMarker098"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH06_F09_Ryan2.png"/></p>

    <p class="figurecaption">Figure 6.9 LightGBM’s <code class="fm-code-in-text">plot_tree</code> output</p>
  </div>

  <p class="body">The tree is plotted horizontally from left to right. We can check that the path returned by the code is the uppermost one, ending in leaf 0<a id="idTextAnchor061"/>. <a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="marker-236"/></p>

  <h3 class="fm-head1" id="heading_id_21">6.4.3 Speeding up by GBDTs and compiling</h3>

  <p class="body">When the number of cases or the available features are many, even the faster LightGBM may take a long time to train a model on such data. At training time, you can overcome long waits by reducing the cases and features handled by smaller values of the parameter <code class="fm-code-in-text">subsample</code> for limiting the cases involved in building each decision tree and the parameter <code class="fm-code-in-text">colsample_bytree</code> for limiting the number of features considered at tree splitting time. However, reducing cases or features may not be optimal for getting the best results from your model. An alternative is using GPUs, which are widespread because they utilize deep learning models. GPUs can speed up training operations, especially with XGBoost, and, in a lesser but significant way, with LightGBM models.<a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="idIndexMarker103"/><a id="idIndexMarker104"/></p>

  <p class="body">With XGBoost, from a modeling point of view, using your GPU is quite straightforward: you just need to specify <code class="fm-code-in-text">"gpu_hist"</code> as a value for the <code class="fm-code-in-text">tree_method</code> parameter. With the new 2.0.0 version, such a method is, however, deprecated, and users can now instead specify the used device by the parameter <code class="fm-code-in-text">device</code>. You can set it to <code class="fm-code-in-text">"cpu"</code> for XGBoost to execute on CPU or <code class="fm-code-in-text">device="cuda"</code> as well as <code class="fm-code-in-text">device="gpu"</code> to have it run on a CUDA-powered GPU, which is the only option at the moment, but in the future, more GPU types will be supported. If you have multiple GPUs, you can specify their ordinal to choose a particular one; for instance, <code class="fm-code-in-text">device="cuda:1"</code> will execute on your second GPU device.</p>

  <p class="body">For XGBoost to perform, you need at least CUDA 11.00 installed and a GPU with a compute capability 5.0. If you have more GPUs available, you can specify which to use by the <code class="fm-code-in-text">gpu_id</code> parameter, which represents the GPU device ordinal reported by CUDA runtime (usually set to zero if you have a single GPU). In this way, XGBoost moves the growth of decision trees to the GPU memory and processors, thus obtaining a relevant speed of operations, especially feature histograms, as described in the paper “Accelerating the XGBoost Algorithm Using GPU Computing” by Mitchell and Frank (<a class="url" href="https://peerj.com/articles/cs-127/">https://peerj.com/articles/cs-127/</a>).</p>

  <p class="body"><a id="marker-237"/>Once GPU trains a model, it can be used for prediction on a machine with a GPU. All you have to set is the <code class="fm-code-in-text">predictor</code> parameter to <code class="fm-code-in-text">gpu_predictor</code> or to <code class="fm-code-in-text">cpu_predictor</code> if you want to use your CPU. Selecting the predictor parameter to GPU can also speed up things when you have to compute SHAP values and SHAP interaction values for model interpretability:</p>
  <pre class="programlisting">model.set_param({"predictor": "gpu_predictor"})
shap_values = model.predict(X, pred_contribs=True)
shap_interaction_values = model.predict(X, pred_interactions=True)</pre>

  <p class="body">Although using a GPU with XGBoost is easy, it becomes a little bit trickier with LightGBM. LightGBM doesn’t have an option for GPU running but rather requires a special version of itself to be compiled for the purpose. Depending on your operating system (Windows, Linux/Ubuntu, MacOS), the compilation may be less or more challenging. Instructions are avai<a id="idTextAnchor062"/><a id="idTextAnchor063"/>lable at <a class="url" href="https://mng.bz/nRg5">https://mng.bz/nRg5</a> for POSIX system<a id="idTextAnchor064"/>s and at <a class="url" href="https://mng.bz/vK8p">https://mng.bz/vK8p</a> for Windows systems. However, if you have all the prerequisites ready on your system as stated by the instruc<a id="idTextAnchor065"/>tions at <a class="url" href="https://mng.bz/4aJg">https://mng.bz/4aJg</a>, you can just require to directly install it using the pip install instruction on your shell or command prompt:</p>
  <pre class="programlisting">pip install lightgbm --install-option=--gpu</pre>

  <p class="body">Once everything has been installed, you need to set the parameter <code class="fm-code-in-text">device</code> to <code class="fm-code-in-text">gpu</code> Don’t expect astonishing performance improvements, however. As stated by LightGBM auth<a id="idTextAnchor066"/>ors (see <a class="url" href="https://mng.bz/vK8p">https://mng.bz/vK8p</a>), the best results are obtained on large-scale and dense datasets because of the inefficient data turnover that causes latencies when working on smaller datasets. In addition, setting a lower number of bins for the histogram algorithm will make the GPU work more efficiently with the LightGBM. The suggestion is to set <code class="fm-code-in-text">max_bin=15</code> and single precision, <code class="fm-code-in-text">gpu_use_dp=false</code>, for the best performances.</p>

  <p class="body"><a id="marker-238"/>GPUs are quite useful for speeding up training, but there are more options at prediction time. With tree structures so readily available, as we have seen in the previous section, it has been possible for specific projects to use such information for rebuilding prediction trees using more performing programming languages such as C, JAVA, or LLVM that can turn your model into pure assembly code. Such tree-compiling projects aim for fast prediction and easier deployment. Examples are Treelite (<a class="url" href="https://github.com/dmlc/treelite">https://github.com/dmlc/treelite</a>), which can read models produced by XGBoost, LightGBM, and even Scikit-learn, and lleaves (<a class="url" href="https://github.com/siboehm/lleaves">https://github.com/siboehm/lleaves</a>), which is a project for LightGBM only.</p>

  <p class="body">Starting from Treelite, this project strives to be a universal model exchange and serialization format for decision tree forests. It compiles your GBDT into C or Java with the least possible dependencies, so you can easily deploy it into any system. To have it tested, you must install a few packages at the command line: <code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">install</code> <code class="fm-code-in-text">tl2cgen treelite treelite_r<a class="calibre" id="idTextAnchor067"/>untime</code>.</p>

  <p class="fm-code-listing-caption">Listing 6.18 XGBoost prediction speedup by Treelite</p>
  <pre class="programlisting">import treelite
import treelite_runtime
import tl2cgen
 
xgb = XGBClassifier(booster='gbtree',
                      objective='reg:logistic',
                      n_estimators=10,
                      max_depth=3)
 
model_pipeline = Pipeline(
     [('processing', column_transform),
     ('XGBoost', xgb)])
 
model_pipeline.fit(X=data, y=target_median)
model = model_pipeline["XGBoost"]
 
model.save_model("./xgb_model.json")                          <span class="fm-combinumeral">①</span>
treelite_model = treelite.Model.load("./xgb_model.json", 
model_format="XGBoost_json")                                  <span class="fm-combinumeral">②</span>
tl2cgen.generate_c_code(treelite_model, dirpath="./", 
params={"parallel_comp": 4})
tl2cgen.export_lib(treelite_model, toolchain="gcc", 
libpath="./xgb_model.so",                                     <span class="fm-combinumeral">③</span>
                   params={"parallel_comp": 4})
 
predictor = tl2cgen.Predictor("./xgb_model.so")
X = model_pipeline["processing"].transform(data)              <span class="fm-combinumeral">④</span>
dmat = tl2cgen.DMatrix(X)                                     <span class="fm-combinumeral">⑤</span>
predictor.predict(dmat) </pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Saves the XGBoost model to a JSON file</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Loads the XGBoost model in Treelite format from the JSON file</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Generates C code from the Treelite model and exports it as a shared library</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Transforms the input data using the preprocessing steps defined in the pipeline</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a Treelite DMatrix from the transformed data, compatible with the exported Treelite model</p>

  <p class="body">The result is a compiled model that, inside a Python script, can return predictions in a much faster fashion. Predictors must always be transformed beforehand since the pipeline is not part of the compiling. Only the model is. In addition, you also have to convert the data in DMatrix format, the native XGBoost data format, before it is sent to the compiled model.</p>

  <p class="body">Developed by Simon Boehm, lleaves promises x10 speed up by LLVM compiling into assembly based on the text tree structure that can be outputted from a LightGBM model. After having installed the package by a <code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">install</code> <code class="fm-code-in-text">leaves</code> instruction on the command line, you can obtain a speed up by following thes<a id="marker-239"/><a id="idTextAnchor068"/>e steps.</p>

  <p class="fm-code-listing-caption">Listing 6.19 LightGBM prediction speedup by <code class="fm-code-in-text">lleaves</code> </p>
  <pre class="programlisting">import lleaves
 
lgbm = LGBMClassifier(boosting_type='gbdt',
                      n_estimators=10,
                      max_depth=3)
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('lightgbm', lgbm)])
 
model_pipeline.fit(X=data, y=target_median)
model = model_pipeline["lightgbm"]
 
model.booster_.save_model('lgb_model.txt')                     <span class="fm-combinumeral">①</span>
 
llvm_model = lleaves.Model(model_file="lgb_model.txt")         <span class="fm-combinumeral">②</span>
llvm_model.compile()                                           <span class="fm-combinumeral">③</span>
X = model_pipeline["processing"].transform(data)               <span class="fm-combinumeral">④</span>
llvm_model.predict(X)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Saves the LightGBM model to a text file</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Loads the LightGBM model using the lleaves library</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Compiles the loaded LightGBM model into LLVM representation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Transforms the input data using the preprocessing steps defined in the pipeline</p>

  <p class="body">Also, in this case, the model is compiled and can predict in a faster way inside a Python script. From a general point of view, <code class="fm-code-in-text">lleaves</code>, though limited only to LightGBM, is a compiling solution that requires many fewer settings and specifications from the user, resulting in a much simpler and straightforwar<a id="idTextAnchor069"/>d usage.<a id="marker-240"/></p>

  <h2 class="fm-head" id="heading_id_22">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Among processing problems, missing data is one of the most problematic. If your data is MCR or is just MAR because missing patterns are related to the other features, multivariate imputation can use the correlations among predictors in a dataset to impute missing values.<a id="idIndexMarker105"/><a id="idIndexMarker106"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Both XGBoost and LightGBM algorithms automatically handle missing data by assigning them to the side that minimizes the loss function the most in each split.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">When a categorical presents high cardinality because of its many labels, you can use target encoding, which gained popularity in Kaggle competitions. Target encoding is a way to transform the values in a categorical feature into their corresponding expected target values.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PDP is a model-agnostic chart technique that explains how features and the target are related by means of the model you have trained. It is beneficial because it helps you better model the relationship between the predictive feature and the target if you notice it is nonlinear and complex.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">XGBoost, thanks to packages such as XGBFIR, can inform you about the most important interactions between predictive features.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">By employing effective feature selection techniques, you can pinpoint and retain the most relevant features that contribute significantly to the machine learning process. Standard techniques to handle feature selection are stability selection based on L1 regularization for linear models, iterative selection, and Boruta for tree ensembles:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">Based on L1 regularization, stability selection aims to identify features that consistently appear as important across multiple subsets, indicating their robustness and reducing the likelihood of selecting features by chance or noise.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Boruta is a procedure to determine if a feature is relevant in a machine learning problem by relying on the internal parameters of the model, such as coefficients in linear models or importance values based on gain, such as in decision trees and their ensembles.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Iterative selection additions by forward selection, or removes by backward elimination, features from your selection based on their performance on the prediction in a greedy fashion, leaving only the essential features for your prediction.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">By optimizing hyperparameters, you can gain another performance boost to your classical machine learning model. Apart from manually setting the hyperparameters, depending on the model you are working on, grid search, random search, successive halving, and Bayesian optimization are popular optimization methods within the data science community:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">Grid search simply works by searching through all the possible combinations of hyperparameters’ values. For every hyperparameter you want to test, you pick a sequence of values and iterate through all their combinations exhaustively.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Random search optimization decides what values to test by randomly drawing them from the search space. The technique is particularly effective if you know little about your hyperparameters, if there are many of them, and if some are irrelevant but you don’t know which ones.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Successive halving is a wrapper of the previously discussed strategies. It works as a tournament between sets of hyperparameters, where first, they are tested using a few computational resources. Then, only a fraction of the best is further tested using more resources. In the end, there will be only one surviving set of hyperparameters.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Bayesian optimization uses informed search to find the best set of hyperparameters. It builds a model of the hyperparameter’s behavior based on prior knowledge of how it works on the data problem. Then it sets a series of experiments to explore further and refine its own internal model, exploit the previous trials, and validate the actual performances of a solution.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Both XGBoost and LightGBM have specific settings and options that are not commonly found in other machine learning algorithms, such as the possibility of extracting and representing their internal structure and speeding up their execution by GPU use and compiling.<a id="marker-241"/></p>
    </li>
  </ul>
</body></html>