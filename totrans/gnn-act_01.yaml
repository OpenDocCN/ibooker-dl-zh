- en: 1 Discovering graph neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 发现图神经网络
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining graphs and graph neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义图和图神经网络
- en: Understanding why people are excited about graph neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么人们对图神经网络感到兴奋
- en: Recognizing when to use graph neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到何时使用图神经网络
- en: Taking a big picture look at solving a problem with a graph neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从宏观角度审视使用图神经网络解决问题
- en: For data practitioners, the fields of machine learning and data science initially
    excite us because of the potential to draw nonintuitive and useful insights from
    data. In particular, the insights from machine learning and deep learning promise
    to enhance our understanding of the world. For the working engineer, these tools
    promise to deliver business value in unprecedented ways.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据从业者来说，机器学习和数据科学领域最初之所以令人兴奋，是因为从数据中提取非直观且有用的见解的潜力。特别是，机器学习和深度学习的见解有望增强我们对世界的理解。对于工作的工程师来说，这些工具承诺以前所未有的方式创造商业价值。
- en: Experience deviates from this ideal. Real-world data is usually messy, dirty
    and biased. Furthermore, statistical methods and learning systems come with their
    own set of limitations. An essential role of the practitioner is to comprehend
    these limitations and bridge the gap between real data and a feasible solution.
    For example, we may want to predict fraudulent activity in a bank, but we first
    need to make sure that our training data has been correctly labeled. Even more
    importantly, we’ll need to check that our models won’t incorrectly assign fraudulent
    activity to normal behaviors, possibly due to some hidden confounders in the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 经验与这一理想有所偏差。现实世界的数据通常杂乱无章、污秽且存在偏见。此外，统计方法和学习系统都带有自己的局限性。实践者的一个基本作用是理解这些局限性，弥合真实数据与可行解决方案之间的差距。例如，我们可能想要预测银行的欺诈活动，但首先需要确保我们的训练数据已被正确标记。更重要的是，我们还需要检查我们的模型不会错误地将欺诈活动分配给正常行为，这可能是由于数据中的一些隐藏混杂因素。
- en: For graph data, until recently, bridging this gap has been particularly challenging.
    Graphs are a data structure that is rich with information and especially adept
    at capturing the intricacies of data where relationships play a crucial role.
    Graphs are omnipresent, with relational data appearing in different forms such
    as atoms in molecules (nature), social networks (society), and even models the
    connection of web pages on the internet (technology) [1]. It’s important to note
    that the term *relational* here doesn’t refer to *relational databases*, but rather
    to data where relationships are of significance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图数据，直到最近，弥合这一差距一直特别具有挑战性。图是一种信息丰富的数据结构，特别擅长捕捉数据中关系所扮演的关键角色。图无处不在，关系数据以不同的形式出现，如分子中的原子（自然）、社交网络（社会），甚至互联网上网页之间的连接模型（技术）[1]。需要注意的是，这里的“关系”一词并不指代“关系数据库”，而是指关系中具有显著意义的数据。
- en: Previously, if you wanted to incorporate relational features from a graph into
    a deep learning model, it had to be done in an indirect way, with different models
    used to process, analyze, and then use the graph data. These separate models often
    couldn’t be easily scaled and had trouble taking into account all the node and
    edge properties of graph data. To make the best use of this rich and ubiquitous
    data type for machine learning, we needed a specialized machine learning technique
    specifically designed for the distinct qualities of graphs and relational data.
    This is the gap that graph neural networks (GNNs) fill.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，如果你想在深度学习模型中整合图中的关系特征，必须以间接的方式进行，使用不同的模型来处理、分析和然后使用图数据。这些独立的模型通常难以扩展，并且难以考虑到图数据的所有节点和边属性。为了充分利用这种丰富且普遍存在的数据类型进行机器学习，我们需要一种专门为图的独特品质和关系数据设计的机器学习技术。这正是图神经网络（GNNs）填补的差距。
- en: The deep learning field often contains a lot of hype around new technologies
    and methods. However, GNNs are widely recognized as a genuine leap forward for
    graph-based learning [2]. This doesn’t mean that GNNs are a silver bullet. Careful
    comparisons should be done between predictive results derived from GNNs and other
    machine learning and deep learning methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域常常围绕新技术和方法存在很多炒作。然而，GNNs被广泛认为是基于图学习的一个真正的飞跃[2]。这并不意味着GNNs是一劳永逸的解决方案。应该仔细比较GNNs得出的预测结果与其他机器学习和深度学习方法。
- en: The key thing to remember is that if your data science problem involves data
    that can be structured as a graph—that is, the data is connected or relational—then
    GNNs could offer a valuable approach, even if you weren’t aware that something
    was missing in your approach. GNNs can be designed to handle very large data,
    to scale, and to adapt to graphs of different sizes and shapes. This can make
    working with relationship-centric data easier and more efficient, as well as yield
    richer results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的关键点是，如果你的数据科学问题涉及可以结构化为图的数据——也就是说，数据是连接的或相关的——那么GNNs可以提供一种有价值的解决方案，即使你还没有意识到你的方法中缺少了什么。GNNs可以设计来处理非常大的数据，进行扩展，并适应不同大小和形状的图。这可以使处理以关系为中心的数据更加容易和高效，同时产生更丰富的结果。
- en: The standout advantages of GNNs are why data scientists and engineers are increasingly
    recognizing the importance of mastering them. GNNs have the ability to unveil
    unique insights from relational data—from identifying new drug candidates to optimizing
    ETA prediction accuracy in your Google Maps app—acting as a catalyst for discovery
    and innovation, and empowering professionals to push the boundaries of conventional
    data analysis. Their diverse applicability spans various fields, offering professionals
    a versatile tool that is as relevant in e-commerce (e.g., recommendation engines)
    as it is in bioinformatics (e.g., drug toxicity prediction). Proficiency in GNNs
    equips data professionals with a multifaceted tool for enhanced, accurate, and
    innovative data analysis of graphs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs的突出优势是为什么数据科学家和工程师越来越认识到掌握它们的重要性。GNNs能够从关系数据中揭示独特的见解——从识别新的药物候选者到优化Google
    Maps应用中的ETA预测准确性——作为发现和创新的催化剂，并赋予专业人士推动传统数据分析边界的力量。它们的广泛应用跨越各个领域，为专业人士提供了一种多功能的工具，在电子商务（例如，推荐引擎）和生物信息学（例如，药物毒性预测）中同样相关。GNNs的熟练掌握使数据专业人士能够进行增强、准确和创新的图数据分析。
- en: For all these reasons, GNNs are now the popular choice for recommender engines,
    analyzing social networks, detecting fraud, understanding how biomolecules behave,
    and many other practical examples that we’ll meet over the course of this book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些原因，图神经网络（GNNs）现在已成为推荐引擎、分析社交网络、检测欺诈、理解生物分子行为以及本书中我们将遇到的其他许多实际应用的流行选择。
- en: 1.1 Goals of this book
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 本书的目标
- en: '*Graph Neural Networks in Action* is aimed at practitioners who want to begin
    to deploy GNNs to solve real problems. This could be a machine learning engineer
    not familiar with graph data structures, a data scientist who hasn’t yet tried
    GNNs, or even a software engineer who may be unfamiliar with either. Throughout
    this book, we’ll be covering topics from the basics of graphs all the way to more
    complex GNN models. We’ll be building up the architecture of a GNN, step-by-step.
    This includes the overall architecture of a GNN and the critical aspect of message
    passing. We then go on to add different features and extensions to these basic
    aspects, such as introducing convolution and sampling, attention mechanisms, a
    generative model, and operating on dynamic graphs. When building our GNNs, we’ll
    be working in Python and using some standard libraries. GNN libraries are either
    standalone or use TensorFlow or PyTorch as a backend. In this text, the focus
    will be on PyTorch Geometric (PyG). Other popular libraries include Deep Graph
    Library (DGL, a standalone library) and Spektral (which uses Keras and TensorFlow
    as a backend). There is also Jraph for JAX users.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 《图神经网络实战》（Graph Neural Networks in Action）旨在帮助实践者开始部署GNNs来解决实际问题。这可能是一个不熟悉图数据结构的机器学习工程师，一个尚未尝试GNNs的数据科学家，甚至可能是一个可能对两者都不熟悉的软件工程师。在本书中，我们将从图的基本知识一直覆盖到更复杂的GNN模型。我们将逐步构建GNN的架构。这包括GNN的整体架构和消息传递的关键方面。然后我们继续添加不同的功能和扩展到这些基本方面，例如引入卷积和采样、注意力机制、生成模型以及在动态图上操作。在构建我们的GNNs时，我们将使用Python和一些标准库。GNN库要么是独立的，要么使用TensorFlow或PyTorch作为后端。在本文本中，重点将放在PyTorch
    Geometric（PyG）上。其他流行的库包括深度图库（Deep Graph Library，DGL，一个独立的库）和Spektral（它使用Keras和TensorFlow作为后端）。还有Jraph，适用于JAX用户。
- en: Our aim throughout this book is to enable you to
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是使你能够
- en: assess the suitability of a GNN solution for your problem.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估GNN解决方案对你问题的适用性。
- en: understand when traditional neural networks won’t perform as well as a GNN for
    graph structured data and when GNNs may not be the best tool for tabular data.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解何时传统的神经网络在处理图结构数据时不如GNN表现得好，以及何时GNN可能不是表格数据的最佳工具。
- en: design and implement a GNN architecture to solve problems specific to you.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和实现一个GNN架构来解决你特有的问题。
- en: make clear the limitations of GNNs.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确GNNs的限制。
- en: This book is weighted toward implementation using programming. We also devote
    some time on essential theory and concepts, so that the techniques covered can
    be sufficiently understood. These are covered in an “Under the Hood” section at
    the end of most chapters to separate the technical reasons from the actual implementation.
    There are many different models and packages that build on the key concepts we
    introduce in this book. So, this book shouldn’t be seen as a comprehensive review
    of all GNN methods and models, which could run to several thousands of pages,
    but rather the starting point for the curious and eager-to-learn practitioner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书侧重于使用编程实现。我们也花了一些时间在必要理论和概念上，以确保所涵盖的技术能够被充分理解。这些内容在大多数章节的“内部机制”部分进行介绍，以区分技术原因和实际实现。本书基于我们在书中介绍的关键概念，有许多不同的模型和包。因此，本书不应被视为对所有GNN方法和模型的全面回顾，这可能需要数千页，而应被视为好奇和渴望学习的实践者的起点。
- en: The book is divided into three parts. Part 1 covers the basics of GNNs, especially
    the ways in which they differ from other neural networks, such as *message passing*
    and *embeddings*, which have specific meaning for GNNs. Part 2, the heart of the
    book, goes over the models themselves, where we cover a handful of key model types.
    Then, in part 3, we’ll go into more detail with some of the harder models and
    concepts, including how to scale graphs and deal with temporal data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为三个部分。第一部分涵盖了GNNs的基础知识，特别是它们与其他神经网络（如*消息传递*和*嵌入*）的不同之处，这些对于GNNs具有特定的含义。第二部分，本书的核心内容，介绍了模型本身，其中我们涵盖了几个关键模型类型。然后，在第三部分，我们将对一些更复杂的模型和概念进行更详细的探讨，包括如何扩展图以及处理时间数据。
- en: '*Graph Neural Networks in Action* is designed for people to jump quickly into
    this new field and start building applications. Our aim for this book is to reduce
    the friction of implementing new technologies by filling in the gaps and answering
    key development questions whose answers may not be easy to find or may not be
    covered elsewhere at all. Each method is introduced through an example application
    so you can understand how GNNs are applied in practice. We strongly advise you
    to try out the code for yourself along the way.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 《图神经网络实战》旨在帮助人们快速进入这个新领域并开始构建应用。本书的目标是通过填补空白和回答关键开发问题来减少实现新技术的摩擦，这些问题可能不容易找到或根本未在其他地方涉及。每种方法都是通过一个示例应用来介绍的，这样你可以了解GNNs在实际中的应用。我们强烈建议你在过程中亲自尝试代码。
- en: 1.1.1 Catching up on graph fundamentals
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 补充图的基本知识
- en: Yes, you do need to understand the basics of graphs before you can understand
    GNNs. Yet our goal for this book is to teach GNNs to deep learning practitioners
    and builders of traditional neural networks who may not know much about graphs.
    At the same time, we also recognize that readers of this book may vary enormously
    in their knowledge of graphs. How to address these differences and make sure everyone
    has what they need to make the most of this book? In this chapter, we provide
    an introduction to the fundamental graph concepts that are most essential to understanding
    GNNs. If you’re well-versed in graphs, you may choose to skip this section, although
    we recommend skimming through as we cover some specific terminology and use-cases
    that will be helpful to understand for the remainder of the book. For those of
    you who have more questions about graphs, we’ve also included a full tutorial
    on basic graph concepts and terminology in appendix A. This primer should also
    serve as a reference for looking up specific concepts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，在理解GNNs之前，你需要了解图的基本知识。然而，本书的目标是教授GNNs给深度学习实践者和传统神经网络的构建者，他们可能对图知之甚少。同时，我们也认识到本书的读者在图的知识上可能存在巨大差异。如何解决这些差异并确保每个人都能充分利用本书？在本章中，我们提供了对理解GNNs最基本图概念的介绍。如果你对图非常熟悉，你可以选择跳过这一部分，尽管我们建议快速浏览，因为我们涵盖了一些对理解本书剩余部分有帮助的特定术语和用例。对于那些对图有更多疑问的人，我们在附录A中也包含了一个关于基本图概念和术语的完整教程。这个入门指南也应当作为查找特定概念的参考。
- en: After the refresher on key concepts in graphs and graph learning, we’ll look
    into some case studies in several fields where GNNs are being successfully applied.
    Then, we’ll break down those specific cases to see what makes a good case for
    using a GNN, as well as how to know if you have a GNN problem on your hands. At
    the end of the chapter, we introduce the mechanics of GNNs, the barebone skeleton
    that the rest of the book will add to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了图和图学习中的关键概念之后，我们将探讨几个领域中的案例研究，在这些领域中 GNN 正在成功应用。然后，我们将分析这些特定案例，看看是什么使得 GNN
    成为一种好的应用案例，以及如何知道你手头是否有 GNN 问题。在本章末尾，我们将介绍 GNN 的机制，这是本书其余部分将添加的骨架。
- en: 1.2 Graph-based learning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 基于图的学习
- en: This section defines graphs, graph-based learning, and some fundamentals of
    GNNs, including the basic structure of a graph and a taxonomy of different types
    of graphs. Then, we’ll review graph-based learning, putting GNNs in context with
    other learning methods. Finally, we’ll explain the value of graphs, ending with
    an example of data derived from the Titanic dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节定义了图、基于图的学习和 GNN 的某些基本原理，包括图的基本结构和不同类型图的分类。然后，我们将回顾基于图的学习，将 GNN 与其他学习方法联系起来。最后，我们将解释图的价值，并以泰坦尼克号数据集的数据为例。
- en: 1.2.1 What are graphs?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 什么是图？
- en: Graphs are data structures with elements, expressed as *nodes or vertices*,
    and relationships between elements, expressed as *edges or links*, as shown in
    figure 1.1\. All nodes in the graph will have additional *feature data*. This
    is node-specific data, relating to things such as the names or ages of individuals
    in a social network. The links are key to the power of relational data, as they
    allow us to learn more about the system, give new tools for analyzing data, and
    predict new properties from it. This is in contrast to tabular data such as a
    database table, dataframe, or spreadsheet, where the data is fixed in rows and
    columns.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图是包含元素的数据结构，用 *节点或顶点* 表示，元素之间的关系用 *边或链接* 表示，如图 1.1 所示。图中的所有节点都将有额外的 *特征数据*。这是节点特定的数据，与社交网络中个人的姓名或年龄等相关。链接是关系数据力量的关键，因为它们使我们能够了解系统，为数据分析提供新的工具，并从中预测新的属性。这与数据库表、数据框或电子表格等表格数据形成对比，其中数据固定在行和列中。
- en: '![figure](../Images/1-1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-1.png)'
- en: Figure 1.1 A graph. Individual elements, represented here by letters A through
    E, are nodes, also called vertices, and their relationships are described by edges,
    also known as links.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.1 一个图。图中由字母 A 到 E 表示的各个元素是节点，也称为顶点，它们之间的关系由边描述，也称为链接。
- en: To describe and learn from the edges between the nodes, we need a way to write
    them down. This can be done explicitly, stating that the A node is connected to
    B and E, and that the B node is connected to A, C, D, and E. Quickly, we can see
    that describing things in this way becomes unwieldy and that we might be repeating
    redundant information (that A is connected to B and that B is connected to A).
    Luckily, there are many mathematical formalisms for describing relations in graphs.
    One of the most common is to describe the *adjacency matrix*, which we write out
    in table 1.1\. Notice that the adjacency matrix is symmetric across the diagonal
    and that all values are ones or zero.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述和从节点之间的边中学习，我们需要一种方法来记录它们。这可以通过明确声明 A 节点连接到 B 和 E，以及 B 节点连接到 A、C、D 和 E 来完成。很快，我们可以看到以这种方式描述事物变得难以管理，我们可能会重复冗余信息（A
    连接到 B 和 B 连接到 A）。幸运的是，有许多数学形式用于描述图中的关系。其中最常见的一种是描述 *邻接矩阵*，我们在表 1.1 中将其列出。请注意，邻接矩阵在对角线两侧是对称的，并且所有值都是
    1 或 0。
- en: Table 1.1 The adjacency matrix for the simple graph in figure 1.1
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 1.1 图 1.1 中简单图的邻接矩阵
- en: '|  | A | B | C | D | E |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | A | B | C | D | E |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **A**  | 0  | 1  | 0  | 0  | 1  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **A**  | 0  | 1  | 0  | 0  | 1  |'
- en: '| **B**  | 1  | 0  | 1  | 1  | 1  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **B**  | 1  | 0  | 1  | 1  | 1  |'
- en: '| **C**  | 0  | 1  | 0  | 1  | 0  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **C**  | 0  | 1  | 0  | 1  | 0  |'
- en: '| **D**  | 0  | 1  | 1  | 0  | 1  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **D**  | 0  | 1  | 1  | 0  | 1  |'
- en: '| **E**  | 1  | 1  | 0  | 1  | 0  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **E**  | 1  | 1  | 0  | 1  | 0  |'
- en: The adjacency matrix of a graph is an important concept that makes it easy to
    observe all the connections of a graph in a single table [3]. Here, we assumed
    that there is no directionality in our graph; that is, if 0 is connected to 1,
    then 1 is also connected to 0\. This is known as an *undirected graph*. Undirected
    graphs can be easily inferred from an adjacency matrix because, in this case,
    the matrix is symmetric across the diagonal (e.g., in table 1.1, the upper-right
    triangle is reflected onto the bottom left).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图的邻接矩阵是一个重要的概念，它使得在一个表格中观察图的所有连接变得容易[3]。在这里，我们假设我们的图没有方向性；也就是说，如果0连接到1，那么1也连接到0。这被称为*无向图*。无向图可以从邻接矩阵中轻松推断出来，因为在这种情况下，矩阵在对角线上是对称的（例如，在表1.1中，右上角被反射到左下角）。
- en: We also assume here that all the relations between nodes are identical. If we
    wanted the relation of nodes B–E to mean more than the relation of nodes B–A,
    then we could increase the weight of this edge. This translates to increasing
    the value in the adjacency matrix, making the entry for the B–A edge in table
    1.1 equal to 10 instead of 1, for example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设节点之间的关系都是相同的。如果我们想让节点B-E的关系比节点B-A的关系更重要，我们可以增加这条边的权重。这相当于增加邻接矩阵中的值，例如，使表1.1中B-A边的条目等于10而不是1。
- en: Graphs where all relations are of equal importance are known as *unweighted
    graphs* and can also be easily observed from the adjacency matrix because all
    graph entries are either 1s or 0s. Graphs where edges have multiple values are
    known as *weighted.*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关系都同等重要的图称为*无权图*，并且可以从邻接矩阵中轻松观察到，因为所有图条目要么是1，要么是0。具有多个值的边的图称为*加权图*。
- en: If any of the nodes in the graph don’t have an edge that connects to itself,
    then the nodes will also have 0s at their own value in the adjacency matrix (0s
    along the diagonal). This means a graph doesn’t have self-loops. A *self-loop*
    occurs when a node has an edge that connects to that same node. To add a self-loop,
    we just make the value for that node nonzero at its position in the diagonal.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图中任何节点都没有连接到自身的边，那么这些节点在其邻接矩阵中的值也将是0（对角线上的0）。这意味着图中没有自环。当一个节点有一个连接到同一节点的边时，就发生了*自环*。要添加自环，我们只需将该节点在对角线上的值设置为非零。
- en: In practice, an adjacency matrix is only one of many ways to describe relations
    in a graph. Others include adjacency lists, edge lists, or an incidence matrix.
    Understanding these types of data structures well is vital to graph-based learning.
    If you’re unfamiliar with these terms, or need a refresher, we recommend looking
    through appendix A, which has additional details and explanations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，邻接矩阵只是描述图中关系的许多方法之一。其他包括邻接表、边列表或关联矩阵。充分理解这些数据结构对于基于图的学习至关重要。如果您不熟悉这些术语或需要复习，我们建议查阅附录A，其中包含更多细节和解释。
- en: 1.2.2 Different types of graphs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 不同类型的图
- en: Understanding the many different types of graphs can help us work out what methods
    to use to analyze and transform the graph, and what machine learning methods to
    apply. In the following, we give a very quick overview of some of the most common
    properties for graphs to have. As before, we recommend you look through appendix
    A for further information.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 了解许多不同类型的图可以帮助我们确定使用哪些方法来分析和转换图，以及应用哪些机器学习方法。以下，我们给出了一些最常见的图属性的快速概述。和之前一样，我们建议您查阅附录A以获取更多信息。
- en: Homogeneous and heterogeneous graphs
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同质图和异质图
- en: The most basic graphs are *homogenous graphs*, which are made up of one type
    of node and one type of edge. Consider a homogeneous graph that describes a recruitment
    network. In this type of graph, the nodes would represent job candidates, and
    the edges would represent relationships between the candidates.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的图是*同质图*，由一种类型的节点和一种类型的边组成。考虑一个描述招聘网络的同质图。在这种类型的图中，节点将代表求职者，边将代表候选人之间的关系。
- en: If we want to expand the power of our graph to describe our recruitment network,
    we could give it more types of nodes and edges, making it a *heterogeneous graph*.
    With this expansion, some nodes may be candidates and others may be companies.
    Edges could now consist of relationships between candidates and current or past
    employment of job candidates at the companies. See figure 1.2 for a comparison
    of a homogeneous graph (all nodes or edges have the same shade) with a heterogeneous
    graph (nodes and edges have a variety of shades).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要扩展我们图的能力来描述我们的招聘网络，我们可以给它更多的节点和边类型，使其成为一个**异质图**。通过这种扩展，一些节点可能是候选人，而其他节点可能是公司。边现在可以由候选人与公司当前或过去的就业关系组成。参见图1.2中同质图（所有节点或边具有相同的阴影）与异质图（节点和边具有各种阴影）的比较。
- en: '![figure](../Images/1-2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-2.png)'
- en: Figure 1.2 A homogeneous graph and a heterogeneous graph. Here, the shade of
    a node or edge represents its type or class. For the homogeneous graph, all nodes
    are of the same type, and all edges are of the same type. For the heterogeneous
    graph, nodes and edges have multiple types.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 同质图和异质图。在此，节点或边的阴影代表其类型或类别。对于同质图，所有节点都是同一类型，所有边也都是同一类型。对于异质图，节点和边有多种类型。
- en: Bipartite graphs
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二部图
- en: Similar to heterogeneous graphs, *bipartite graphs* also can be separated or
    partitioned into different subsets. However, bipartite graphs (figure 1.3) have
    a very specific network structure such that nodes in each subset connect to nodes
    outside of their subset and not inside. Later, we’ll be discussing recommendation
    systems and the Pinterest graph. This graph is bipartite because one set of nodes
    (pins) connects another set of nodes (boards) but not to nodes within their set
    (pins).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与异质图类似，**二部图**也可以被分离或划分为不同的子集。然而，二部图（图1.3）具有一个非常特定的网络结构，即每个子集中的节点连接到其子集之外的节点，而不是内部的节点。稍后，我们将讨论推荐系统和Pinterest图。这个图是二部图，因为一组节点（图钉）连接另一组节点（版面），但不是它们自己组内的节点（图钉）。
- en: '![figure](../Images/1-3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-3.png)'
- en: Figure 1.3 A bipartite graph. There are two types of nodes (two shades of circles).
    In a bipartite graph, nodes can’t be connected to nodes of the same type. This
    is also an example of a heterogeneous graph.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3 二部图。有两种类型的节点（两种阴影的圆圈）。在二部图中，节点不能连接到同一类型的节点。这也是一个异质图的例子。
- en: Cyclic graphs, acyclic graphs, and directed acyclic graphs
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环图、无环图和有向无环图
- en: A graph is *cyclic* if it allows you to start at a node, travel along its edges,
    and return to the starting node without retracing any steps, creating a circular
    path within the graph. In contrast, in an *acyclic* graph, no matter which path
    you take from any starting node, you can’t return to the starting point without
    backtracking. These graphs, as shown in figure 1.4, often resemble tree-like structures
    or paths that don’t loop back on themselves.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个图是**循环的**，那么你可以从一个节点开始，沿着其边旅行，不重复任何步骤，返回到起始节点，在图中创建一个环形路径。相比之下，在一个**无环**图中，无论从任何起始节点选择哪条路径，你都不能不回溯就返回到起始点。这些图，如图1.4所示，通常类似于树状结构或没有回环的路径。
- en: While both cyclic and acyclic graphs can be either undirected or directed, a
    *directed acyclic graph (DAG)* is a specific type of acyclic graph that is exclusively
    directed. In a DAG, all edges have a direction, and no cycles are allowed. DAGs
    represent one-way relationships where you can’t follow the arrows and end up back
    at the starting point. This characteristic makes DAGs essential in causal analysis,
    as they reflect causal structures where causality is assumed to be unidirectional.
    For example, A can cause B, but B can’t simultaneously cause A. This unidirectional
    nature aligns perfectly with the structure of DAGs, making them ideal for modeling
    workflow processes, dependency chains, and causal relationships in various fields.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有向图和无向图都可以是有向的或无向的，但**有向无环图（DAG**）是一种特定的无环图，它仅限于有向。在有向无环图中，所有边都有方向，不允许有环。DAGs代表单向关系，你不能跟随箭头并最终回到起点。这种特性使得DAGs在因果分析中至关重要，因为它们反映了假设因果是单向的因果结构。例如，A可以导致B，但B不能同时导致A。这种单向性质与DAGs的结构完美契合，使它们成为建模各种领域中的工作流程过程、依赖链和因果关系的理想选择。
- en: '![figure](../Images/1-4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-4.png)'
- en: Figure 1.4 A cyclic graph (left), an acyclic graph (right), and a DAG (bottom).
    In the cyclic graph, the cycle is shown by the arrows (directed edges) connecting
    nodes A-E-D-C-B-A. Note that two nodes, G and F are part of the graph, but not
    part of its defining cycle. The acyclic graph is composed of undirected edges,
    and no cycle is possible. In the DAG, all directed edges flow in one direction,
    from A to F.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 展示了循环图（左）、无环图（右）和DAG（底部）。在循环图中，通过连接节点A-E-D-C-B-A的箭头（有向边）显示了循环。请注意，节点G和F是图的一部分，但不是其定义循环的一部分。无环图由无向边组成，不可能存在循环。在DAG中，所有有向边都流向一个方向，从A到F。
- en: Knowledge graphs
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识图谱
- en: A *knowledge graph* is a specialized type of heterogeneous graph that represents
    data with enriched semantic meaning, capturing not only the relationships between
    different entities but also the context and nature of these relationships. Unlike
    conventional graphs, which primarily emphasize structure and connectivity, a knowledge
    graph incorporates metadata and follows specific schemas to provide deeper contextual
    information. This allows for advanced reasoning and querying capabilities, such
    as identifying patterns, uncovering specific types of connections, or inferring
    new relationships.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*知识图谱*是一种特殊类型的异构图，它通过丰富的语义意义来表示数据，不仅捕捉不同实体之间的关系，还捕捉这些关系的上下文和本质。与主要强调结构和连接性的传统图不同，知识图谱结合了元数据和遵循特定模式，以提供更深入的上下文信息。这允许进行高级推理和查询功能，例如识别模式、揭示特定类型的连接或推断新的关系。'
- en: In the example of an academic research network at a university, a knowledge
    graph might represent various entities such as Professors, Students, Papers, and
    Research Topics, and explicitly define the relationships between them. For instance,
    Professors and Students could be associated with Papers through an Authorship
    relationship, while Professors might also Supervise Students. Furthermore, the
    graph would reflect hierarchical structures, such as Professors and Students being
    categorized under Departments. You can see this knowledge graph depicted in figure
    1.5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在大学学术研究网络的例子中，知识图谱可能表示各种实体，如教授、学生、论文和研究主题，并明确定义它们之间的关系。例如，教授和学生可以通过作者关系与论文相关联，而教授也可能指导学生。此外，图将反映层级结构，如教授和学生被归类在系别下。您可以在图1.5中看到这个知识图谱的表示。
- en: '![figure](../Images/1-5.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-5.png)'
- en: Figure 1.5 A knowledge graph representing an academic research network within
    a university’s physics department. The graph illustrates both hierarchical relationships,
    such as professors and students as members of the department, and behavioral relationships,
    such as professors supervising students and authoring papers. Entities such as
    Professors, Students, Papers, and Topics are connected through semantically meaningful
    relationships (Supervises, Wrote, Inspires). Entities also have detailed features
    (Name, Department, Type) providing further context. The semantic connections and
    features enable advanced querying and analysis of complex academic interactions.
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5 表示大学物理系内部学术研究网络的图谱。该图谱展示了层级关系，例如教授和学生作为系成员，以及行为关系，例如教授指导学生和撰写论文。实体如教授、学生、论文和主题通过语义上有意义的关系（指导、撰写、启发）相互连接（Supervises,
    Wrote, Inspires）。实体还具有详细特征（姓名、系别、类型），提供更多上下文。语义连接和特征使得对复杂的学术互动进行高级查询和分析成为可能。
- en: A key feature of knowledge graphs is their ability to provide explicit context.
    Unlike conventional heterogeneous graphs, which display different types of entities
    and their basic connections without detailed semantic meaning, knowledge graphs
    go further by defining the specific types and meanings of relationships. For example,
    while a traditional graph might show that Professors are connected to Departments
    or that Students are linked to Papers, a knowledge graph would specify that Professors
    supervise Students or that Students and Professors Wrote Papers. This added layer
    of meaning enables more powerful querying and analysis, making knowledge graphs
    particularly valuable in fields such as natural language processing, recommendation
    systems, and academic research analysis.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱的一个关键特征是它们提供明确上下文的能力。与传统的异构图不同，传统的异构图显示不同类型的实体及其基本连接，但没有详细的语义意义，知识图谱通过定义特定类型和关系的具体类型和意义更进一步。例如，虽然传统的图可能显示教授与系部相连或学生与论文相链接，但知识图谱会具体说明教授监督学生或学生和教授共同撰写论文。这一层额外的意义使得查询和分析更加强大，使得知识图谱在自然语言处理、推荐系统和学术研究分析等领域特别有价值。
- en: Hypergraphs
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超图
- en: One of the more complex and difficult graphs to work with is the hypergraph.
    *Hypergraphs* are those where a single edge can be connected to multiple different
    nodes. For graphs that aren’t hypergraphs, edges are used to connect exactly two
    nodes (or a node to itself for self-loops). As shown in figure 1.6, edges in a
    hypergraph can connect between any number of nodes. The complexity of a hypergraph
    is reflected in its adjacency data. For typical graphs, network connectivity is
    represented by a two-dimensional adjacency matrix. For hypergraphs, the adjacency
    matrix extends to a higher dimensional tensor, referred to as an *incidence tensor*.
    This tensor is N-dimensional, where N is the maximum number of nodes connected
    by a single edge. An example of a hypergraph might be a communication platform
    that allows for group chats as well as single person conversations. In an ordinary
    graph, edges would only connect two people. In a hypergraph, one hyperedge could
    connect multiple people, representing a group chat.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有需要处理的图中，超图是较为复杂和困难的一种。*超图*是指单条边可以连接多个不同节点的图。对于不是超图的图，边用于连接恰好两个节点（或节点自身形成自环）。如图1.6所示，超图中的边可以连接任意数量的节点。超图的复杂性体现在其邻接数据上。对于典型图，网络连通性由二维邻接矩阵表示。对于超图，邻接矩阵扩展到更高维的张量，称为*关联张量*。这个张量是N维的，其中N是单条边连接的最大节点数。一个超图的例子可能是一个允许进行群聊以及单个人对话的通信平台。在普通图中，边只会连接两个人。在超图中，一个超边可以连接多个人，表示一个群聊。
- en: '![figure](../Images/1-6.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-6.png)'
- en: Figure 1.6 One undirected hypergraph, illustrated in two ways. On the left,
    we have a graph whose edges are represented by shaded areas, marked by letters,
    and whose vertices are dots, marked by numbers. On the right, we have a graph
    whose edge lines (marked by letters) connect up to 3 nodes (circles marked by
    numbers). Node 8 has no edge. Node 7 has a self-loop.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6 一种无向超图，以两种方式展示。在左侧，我们有一个图，其边由阴影区域表示，用字母标记，其顶点由点表示，用数字标记。在右侧，我们有一个图，其边线（用字母标记）连接最多3个节点（用数字标记的圆圈）。节点8没有边。节点7有一个自环。
- en: 1.2.3 Graph-based learning
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 基于图的学习
- en: As we’ll see in the rest of this chapter, graphs are ubiquitous in our everyday
    life. *Graph-based learning* takes graphs as input data to build models that give
    insight into questions about this data. Later in this chapter, we look at different
    examples of graph data as well as at the sort of questions and tasks we can use
    graph-based learning to answer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本章的其余部分看到的那样，图在我们的日常生活中无处不在。*基于图的学习*将图作为输入数据来构建模型，以洞察关于这些数据的问题。在本章的后面部分，我们将探讨不同的图数据示例，以及我们可以使用基于图的学习来回答的问题和任务。
- en: Graph-based learning uses a variety of machine learning methods to build *representations*
    of graphs. These representations are then used for downstream tasks such as node
    or link prediction or graph classification. In chapter 2, you’ll learn about one
    of the essential tools in graph-based learning, building embeddings. Briefly,
    embeddings are *low-dimensional* vector representations. We can build an embedding
    of different nodes, edges, or entire graphs, and there are a number of different
    ways to do this such as the Node2Vec (N2V) or DeepWalk algorithms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的学习使用各种机器学习方法来构建图的*表示*。这些表示随后用于下游任务，如节点或链接预测或图分类。在第2章中，你将了解基于图学习中的一个基本工具，即构建嵌入。简而言之，嵌入是*低维*向量表示。我们可以构建不同节点、边或整个图的嵌入，并且有几种不同的方法可以实现这一点，例如Node2Vec
    (N2V)或DeepWalk算法。
- en: Methods for analysis on graph data have been around for a long time, at least
    as early as the 1950s when *clique methods* used certain features of a graph to
    identify subsets or communities in the graph data [4].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据上的分析方法已经存在很长时间了，至少早在20世纪50年代，当*团方法*使用图的某些特征来识别图数据中的子集或社区时[4]。
- en: One of the most famous graph-based algorithms is PageRank, which was developed
    by Larry Page and Sergey Brin in 1996 and formed the basis for Google’s search
    algorithms. Some believe that this algorithm was a key element in the company’s
    meteoric rise in the following years. This highlights that a successful graph-based
    learning algorithm can have a huge effect.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的基于图的算法之一是PageRank，它由拉里·佩奇和谢尔盖·布林在1996年开发，并成为谷歌搜索算法的基础。有些人认为这个算法是公司在随后的几年中迅速崛起的关键因素。这突显了成功的基于图的学习算法可以产生巨大的影响。
- en: 'These methods are only a small subset of graph-based learning and analysis
    techniques. Others include belief propagation [5], graph kernel methods [6], label
    propagation [7], and isomaps [8]. However, in this book, we’ll focus on one of
    the newest and most exciting additions to the family of graph-based learning techniques:
    GNNs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法只是基于图的学习和分析技术的一个小子集。其他包括信念传播[5]、图核方法[6]、标签传播[7]和等距映射[8]。然而，在这本书中，我们将重点关注基于图的学习技术家族中最新且最激动人心的补充之一：GNN。
- en: 1.2.4 What is a GNN?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 什么是GNN？
- en: GNNs combine graph-based learning with deep learning. This means that neural
    networks are used to build embeddings and process the relational data. An overview
    of the inner workings of a GNN is shown in figure 1.7\.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: GNN结合了基于图的学习和深度学习。这意味着神经网络被用来构建嵌入并处理关系数据。GNN内部工作原理的概述如图1.7所示。
- en: GNNs allows you to represent and learn from graphs, including their constituent
    nodes, edges, and features. In particular, many methods of GNNs are built specifically
    to scale effectively with the size and complexity of a graph. This means that
    GNNs can operate on huge graphs, as we’ll discuss. In this sense, GNNs provide
    analogous advantages to relational data as convolutional neural networks have
    given for image-based data and computer vision.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GNN允许你表示和学习图，包括它们的构成节点、边和特征。特别是，许多GNN方法专门设计用来有效地扩展到图的大小和复杂性。这意味着GNN可以在非常大的图上运行，正如我们将讨论的。在这方面，GNN为关系数据提供了与卷积神经网络为基于图像的数据和计算机视觉所提供的类似的优势。
- en: '![figure](../Images/1-7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-7.png)'
- en: Figure 1.7 An overview of how GNNs work. An input graph is passed to a GNN.
    The GNN then uses neural networks to transform graph features such as nodes or
    edges into nonlinear embeddings through a process known as message passing. These
    embeddings are then tuned to specific unknown properties using training data.
    After the GNN is trained, it can predict unknown features of a graph.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7 GNN工作原理概述。一个输入图被传递给GNN。然后GNN使用神经网络通过称为消息传递的过程将图特征（如节点或边）转换为非线性嵌入。然后使用训练数据对这些嵌入进行调整，以适应特定的未知属性。GNN训练完成后，它可以预测图的未知特征。
- en: Historically, applying traditional machine learning methods to graph data structures
    has been challenging because graph data, when represented in grid-like formats
    and data structures, can lead to massive repetitions of data. To address this,
    graph-based learning focuses on approaches that are *permutation invariant*. This
    means that the machine learning method is uninfluenced by the ordering of the
    graph representation. In concrete terms, it means that we can shuffle the rows
    and columns of the adjacency matrix without affecting our algorithm’s performance.
    Whenever we’re working with data that contains relational data, that is, has an
    adjacency matrix, then we want to use a machine learning method that is permutation
    invariant to make our method more general and efficient. Although GNNs can be
    applied to all graph data, GNNs are especially useful because they can deal with
    huge graph datasets and typically perform better than other machine learning methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，将传统的机器学习方法应用于图数据结构一直具有挑战性，因为当图数据以网格状格式和数据结构表示时，可能会导致数据的大量重复。为了解决这个问题，基于图的学习专注于**排列不变性**的方法。这意味着机器学习方法不受图表示顺序的影响。具体来说，这意味着我们可以随意打乱邻接矩阵的行和列，而不会影响算法的性能。每当我们在处理包含关系数据的数据时，也就是说，具有邻接矩阵的数据时，我们希望使用排列不变的机器学习方法，使我们的方法更加通用和高效。尽管GNNs可以应用于所有图数据，但GNNs特别有用，因为它们可以处理巨大的图数据集，并且通常比其他机器学习方法表现更好。
- en: Permutation invariances are a type of *inductive bias*, or an algorithm’s learning
    bias, and are powerful tools for designing machine learning algorithms [1]. The
    need for permutation-invariant approaches is one of the central reasons that graph-based
    learning has increased in popularity in recent years.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 排列不变性是一种**归纳偏差**，或算法的学习偏差，是设计机器学习算法的强大工具 [1]。对排列不变性方法的需求是近年来基于图的学习增加受欢迎程度的核心原因之一。
- en: Being designed for permutation-invariant data comes with some drawbacks along
    with its advantages. GNNs aren’t as well suited for other data, such as images
    or tables. While this might seem obvious, images and tables are *not* permutation
    invariant and therefore not a good fit for GNNs. If we shuffle the rows and columns
    of an image, then we scramble the input. Instead, machine learning algorithms
    for images seek *translational invariance*, which means that we can translate
    (shift) the object in an image, and it won’t affect the performance of the algorithm.
    Other neural networks, such as convolutional neural networks (CNNs) typically
    perform much better on images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于排列不变性数据带来了一些缺点，同时也带来了优点。GNNs并不适合其他数据，如图像或表格。虽然这看起来可能很明显，但图像和表格**不是**排列不变的，因此不适合GNNs。如果我们打乱图像的行和列，那么就会打乱输入。相反，图像的机器学习算法寻求**平移不变性**，这意味着我们可以平移（移动）图像中的对象，而不会影响算法的性能。其他神经网络，如卷积神经网络（CNNs），通常在图像上表现更好。
- en: 1.2.5 Differences between tabular and graph data
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.5 表格数据和图数据之间的差异
- en: Graph data includes all data with some relational content, making it a powerful
    way to represent complex connections. While graph data might initially seem distinct
    from traditional tabular data, many datasets that are typically represented in
    tables can be re-created as graphs with some data engineering and imagination.
    Let’s take a closer look at the Titanic dataset, a classic example in machine
    learning, and explore how it can be transformed from a table format to a graph
    format.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据包括所有具有某种关系内容的数据，使其成为表示复杂连接的有力方式。虽然图数据最初可能看起来与传统表格数据不同，但许多通常以表格形式表示的数据集可以通过一些数据工程和想象力重新创建为图。让我们更仔细地看看泰坦尼克号数据集，它是机器学习中的一个经典例子，并探讨它如何从表格格式转换为图格式。
- en: The Titanic dataset describes passengers on the Titanic, a ship that famously
    met an untimely end when it collided with an iceberg. Historically, this dataset
    has been analyzed in tabular format, containing rows for each passenger with columns
    representing features such as age, gender, fare, class, and survival status. However,
    the dataset also contains rich, unexplored relationships that aren’t immediately
    visible in a table format, as shown in figure 1.8.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号数据集描述了泰坦尼克号上的乘客，这艘船因与冰山相撞而著名地遭遇了不幸的结局。历史上，这个数据集一直是用表格格式分析的，包含每名乘客的行，列代表年龄、性别、票价、等级和生存状态等特征。然而，该数据集还包含丰富的、在表格格式中不立即可见的关系，如图1.8所示。
- en: '![figure](../Images/1-8.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-8.png)'
- en: Figure 1.8 The Titanic Dataset is usually displayed and analyzed using a table
    format.
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8 泰坦尼克号数据集通常以表格格式显示和分析。
- en: Recasting the Titanic dataset as a graph
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将泰坦尼克号数据集重新构造成图
- en: 'To transform the Titanic dataset into a graph, we need to consider how to represent
    the underlying relationships between passengers as nodes and edges:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要将泰坦尼克号数据集转换为图，我们需要考虑如何将乘客之间的基本关系表示为节点和边：
- en: '*Nodes*—In the graph, each passenger can be represented as a node. We can also
    introduce nodes for other entities, such as cabins, families, or even groups such
    as “third-class passengers.”'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点*——在图中，每个乘客都可以表示为一个节点。我们还可以为其他实体引入节点，例如客舱、家庭，甚至如“三等乘客”这样的群体。'
- en: '*Edges*—Edges represent the relationships or connections between these nodes.
    For example:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边*——边代表这些节点之间的关系或连接。例如：'
- en: Passengers who are family members (siblings, spouses, parents, or children)
    based on the available data
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据可用数据，是家庭成员（兄弟姐妹、配偶、父母或子女）的乘客
- en: Passengers who share a cabin or were traveling together
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享同一客舱或曾一同旅行的乘客
- en: Social or business relationships that might be inferred from shared ticket numbers,
    last names, or other identifying features
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能从共享的票号、姓氏或其他识别特征中推断出的社会或商业关系
- en: To construct this graph, we need to use the existing information in the table
    and potentially enrich it with secondary data sources or assumptions (e.g., linking
    last names to create family groups). This process converts the tabular data into
    a graph-based structure, shown in figure 1.9, where each edge and node encapsulates
    meaningful relational data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这个图，我们需要使用表格中的现有信息，并可能通过次要数据源或假设（例如，将姓氏链接以创建家庭群体）来丰富它。这个过程将表格数据转换为基于图的架构，如图1.9所示，其中每个边和节点封装了有意义的关联数据。
- en: '![figure](../Images/1-9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-9.png)'
- en: 'Figure 1.9 The Titanic dataset, showing the family relationships of the people
    on the Titanic visualized as a graph (Source: Matt Hagy). Here, we can see that
    there was a rich social network as well as many passengers with unknown family
    ties.'
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.9 泰坦尼克号数据集，展示了泰坦尼克号上人们的家庭关系，以图的形式可视化（来源：Matt Hagy）。在这里，我们可以看到存在丰富的社交网络以及许多具有未知家庭联系的乘客。
- en: How graph data adds depth and meaning
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图数据如何增加深度和意义
- en: 'Once the dataset is represented as a graph, it provides a much deeper view
    of the social and familial connections between the passengers. For example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被表示为图，它就提供了对乘客之间社会和家庭联系的更深入视角。例如：
- en: '*Family relationships*—The graph clearly shows how certain passengers were
    related (e.g., as parents, children, or siblings). This could help us understand
    survival patterns, as family members might have behaved differently in a crisis
    than individuals traveling alone.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*家庭关系*——图清楚地显示了某些乘客之间的关系（例如，作为父母、子女或兄弟姐妹）。这有助于我们理解生存模式，因为家庭成员在危机中可能的行为与独自旅行的个体可能不同。'
- en: '*Social networks*—Beyond families, the graph could reveal broader social networks
    (e.g., friendships or business connections), which could be important factors
    in analyzing behavior and outcomes.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社交网络*——除了家庭之外，图可能揭示更广泛的社会网络（例如，友谊或商业联系），这些可能是分析行为和结果的重要因素。'
- en: '*Community insights*—The graph structure also allows for community detection
    algorithms to identify clusters of related or connected passengers, which may
    reveal new insights into survival rates, rescue patterns, or other behaviors.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社区洞察*——图结构还允许社区检测算法识别相关或连接的乘客集群，这可能揭示关于生存率、救援模式或其他行为的新的见解。'
- en: Graph representations add depth by specifying connections that might not be
    obvious in a tabular format. For example, understanding who traveled together,
    who shared a cabin, or who had social or family ties can provide more context
    on survival rates and passenger behavior. This is crucial for tasks such as node
    prediction, where we want to predict attributes or outcomes based on the relationships
    represented in the graph.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示通过指定在表格格式中可能不明显的关系来增加深度。例如，了解谁曾一同旅行、谁共享了客舱，或者谁有社会或家庭联系，可以提供更多关于生存率和乘客行为的背景信息。这对于诸如节点预测等任务至关重要，在这些任务中，我们希望根据图中表示的关系来预测属性或结果。
- en: By creating an adjacency matrix or defining graph edges and nodes based on the
    relationships in the dataset, we can transition from simple data analysis to more
    sophisticated graph-based learning methods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建邻接矩阵或根据数据集中的关系定义图中的边和节点，我们可以从简单的数据分析过渡到更复杂的基于图的学习方法。
- en: '1.3 GNN applications: Case studies'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 GNN应用：案例研究
- en: As we’ve seen, GNNs are neural networks designed to work on relational data.
    They give new ways for relational data to be transformed and manipulated, by being
    easier to scale and more accurate than previous graph-based learning methods.
    In the following, we discuss some exciting applications of GNNs, to see, at a
    high level, how this class of models are solving real-world problems. Links to
    source papers are listed at the end of the book if you want to learn more about
    these particular projects.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，GNN是设计用于处理关系数据的神经网络。它们通过比以前的基于图的学习方法更容易扩展和更准确，为关系数据的转换和处理提供了新的方法。在以下内容中，我们将讨论一些GNN的激动人心的应用，从高层次上了解这类模型是如何解决现实世界问题的。如果您想了解更多关于这些特定项目的详细信息，可以在本书末尾找到相关论文的链接。
- en: 1.3.1 Recommendation engines
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 推荐引擎
- en: Enterprise graphs can exceed billions of nodes and many billions of edges. On
    the other hand, many GNNs are benchmarked on datasets that consist of fewer than
    a million nodes. When applying GNNs to large graphs, adjustments of the training
    and inference algorithms and storage techniques all have to be made. (You can
    learn more about the specifics of scaling GNNs in chapter 7.)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 企业图可以超过数十亿个节点和数十亿条边。另一方面，许多GNN在包含不到一百万个节点的数据集上进行基准测试。当将GNN应用于大型图时，必须调整训练和推理算法以及存储技术。（您可以在第7章中了解更多关于扩展GNN的具体细节。）
- en: 'One of the most well-known industry examples of GNNs is their use as recommendation
    engines. For instance, Pinterest is a social media platform for finding and sharing
    images and ideas. There are two major concepts to Pinterest’s users: collections
    or categories of ideas, called *boards* (like a bulletin board); and objects a
    user wants to bookmark called *pins*. Pins include images, videos, and website
    URLs. A user board focused on dogs might then include pins of pet photos, puppy
    videos, or dog-related website links. A board’s pins aren’t exclusive to it; a
    pet drawing that was pinned to the Dogs board could also be pinned to a Puppies
    board, as shown in figure 1.10.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: GNN最著名的行业应用之一是它们作为推荐引擎的使用。例如，Pinterest是一个用于寻找和分享图像和想法的社会媒体平台。对于Pinterest的用户有两个主要概念：称为*版面*的想法集合或类别（就像公告板一样）；以及用户想要书签的对象，称为*图钉*。图钉包括图像、视频和网站URL。一个专注于狗的用户版面可能包括宠物照片、小狗视频或与狗相关的网站链接。版面的图钉并不局限于它；如图1.10所示，一个被图钉到狗版面的宠物画也可能被图钉到小狗版面。
- en: '![figure](../Images/1-10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-10.png)'
- en: Figure 1.10 A bipartite graph that is like the Pinterest graph. Nodes in this
    case are the pins and boards.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.10 一个类似于Pinterest图的二分图。本例中的节点是图钉和版面。
- en: As of this writing, Pinterest has 400 million active users who have likely pinned
    tens if not hundreds of items per user. One imperative of Pinterest is to help
    their users find content of interest via recommendations. Such recommendations
    should not only take into account image data and user tags but also draw insights
    from the relationships between pins and boards.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Pinterest有4亿活跃用户，他们可能每人已经图钉了数十甚至数百个物品。Pinterest的一个紧迫任务是帮助他们通过推荐找到感兴趣的内容。这样的推荐不仅应该考虑图像数据和用户标签，还应该从图钉和版面之间的关系中汲取见解。
- en: One way to interpret the relationships between pins and boards is as a *bipartite
    graph*, which we discussed earlier. For the Pinterest graph, all the pins are
    connected to boards, but no pin is connected to another pin, and no board is connected
    to another board. Pins and boards are two classes of nodes. Members of these classes
    can be linked to members of the other class, but not to members of the same class.
    The Pinterest graph was reported to have 3 billion nodes and 18 billion edges.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解释图钉和版面之间关系的一种方式是将其视为*二分图*，这是我们之前讨论过的。对于Pinterest图，所有图钉都与版面相连，但没有图钉与另一个图钉相连，也没有版面与另一个版面相连。图钉和版面是两种节点类别。这些类别的成员可以与其他类别的成员相连接，但不能与同一类别的成员相连接。据报道，Pinterest图有30亿个节点和180亿条边。
- en: PinSage, a graph convolutional network (GCN), was one of the first documented
    highly scaled GNNs used in an enterprise system [9]. This was used in Pinterest’s
    recommendation systems to overcome past challenges of applying graph-learning
    models to massive graphs. Compared to baseline methods, tests on this system showed
    it improved user engagement by 30%. Specifically, PinSage was used to predict
    which objects should be recommended to be included in a user’s graph. However,
    GNNs can also be used to predict what an object is, such as whether it contains
    a dog or mountain, based on the rest of the nodes in the graph and how they are
    connected. We’ll be doing a deep dive on GCNs, of which PinSage is an extension,
    in chapter 3.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: PinSage，一种图卷积网络（GCN），是第一个在企业系统中记录的高规模GNN之一[9]。它在Pinterest的推荐系统中被用来克服将图学习模型应用于大规模图的传统挑战。与基线方法相比，该系统的测试表明，它提高了用户参与度30%。具体来说，PinSage被用来预测应推荐给用户包含在其图中的哪些对象。然而，GNNs也可以用来预测一个对象是什么，例如它是否包含狗或山，基于图中其余节点及其连接方式。我们将在第3章中深入探讨GCNs，其中PinSage是其扩展。
- en: 1.3.2 Drug discovery and molecular science
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 药物发现与分子科学
- en: In chemistry and molecular sciences, a prominent problem has been representing
    molecules in a general, application-agnostic way, and inferring possible interfaces
    between molecules, such as proteins. For molecule representation, we can see that
    the drawings of molecules that are common in high school chemistry classes bear
    resemblance to a graph structure, consisting of nodes (atoms) and edges (atomic
    bonds), as shown in figure 1.11.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在化学和分子科学中，一个突出的问题是以通用、与应用无关的方式表示分子，并推断分子之间（如蛋白质）的可能界面。对于分子表示，我们可以看到在高中化学课堂上常见的分子结构图与图结构相似，由节点（原子）和边（原子键）组成，如图1.11所示。
- en: '![figure](../Images/1-11.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-11.png)'
- en: Figure 1.11 In this molecule, we can see individual atoms as nodes and the atomic
    bonds as edges.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.11 在这个分子中，我们可以将单个原子视为节点，将原子键视为边。
- en: Applying GNNs to these structures can, in certain circumstances, outperform
    traditional “fingerprint” methods for determining the properties of a molecule.
    These traditional methods involve the creation of features by domain experts to
    capture a molecule’s properties, such as interpreting the presence or absence
    of certain molecules or atoms [10]. GNNs learn new data-driven features that can
    be used to group certain molecules together in new and unexpected ways or even
    to propose new molecules for synthesis. This is extremely important for predicting
    whether a chemical is toxic or safe for use or whether it has some downstream
    effects that can affect disease progression. Therefore, GNNs have shown themselves
    to be incredibly useful in the field of drug discovery.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，将GNNs应用于这些结构可以优于传统的“指纹”方法，以确定分子的属性。这些传统方法涉及领域专家创建特征来捕捉分子的属性，例如解释某些分子或原子的存在与否[10]。GNNs学习新的数据驱动特征，可以以新的和意想不到的方式将某些分子分组在一起，甚至可以提出新的分子用于合成。这对于预测化学物质是否有毒性或是否安全使用，或者它是否有可能影响疾病进展的下游效应至关重要。因此，GNNs在药物发现领域已被证明是非常有用的。
- en: Drug discovery, especially for GNNs, can be understood as a graph prediction
    problem. *Graph prediction* tasks are those that require learning and predicting
    properties about the entire graph. For drug discovery, the aim is to predict properties
    such as toxicity or treatment effectiveness (discriminative) or to suggest entirely
    new graphs that should be synthesized and tested (generative). To suggest these
    new graphs, drug discovery methods often combine GNNs with other generative models
    such as variational graph autoencoders (VGAEs), as shown, for example, in figure
    1.12\. We’ll describe VGAEs in more detail in chapter 5 and show how we can use
    these to predict molecules.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 药物发现，特别是对于图神经网络（GNNs），可以理解为图预测问题。*图预测*任务是需要学习和预测整个图属性的任务。对于药物发现，目标是预测诸如毒性或治疗有效性（判别性）等属性，或者提出应合成和测试的全新图（生成性）。为了提出这些新图，药物发现方法通常将GNNs与其他生成模型（如变分图自动编码器（VGAEs））相结合，例如图1.12所示。我们将在第5章中更详细地描述VGAEs，并展示如何使用这些模型来预测分子。
- en: '![figure](../Images/1-12.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-12.png)'
- en: Figure 1.12 A GNN system used to predict new molecules [11]. The workflow here
    starts on the left with a representation of a molecule as a graph. In the middle
    parts of the figure, this graph representation is transformed via a GNN into a
    latent representation. The latent representation is then transformed back to the
    molecule to ensure that the latent space can be decoded (right).
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.12展示了用于预测新分子的GNN系统 [11]。这里的流程从左边的分子作为图的表示开始。在图的中部部分，这个图表示通过GNN转换为潜在表示。然后，潜在表示被转换回分子，以确保潜在空间可以被解码（右图）。
- en: 1.3.3 Mechanical reasoning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 机械推理
- en: We develop rudimentary intuition about mechanics and physics of the world around
    us at a remarkably young age and without any formal training in the subject. We
    don’t need to write down a set of equations to know how to catch a bouncing ball.
    We don’t even have to be in the presence of a physical ball. Given a series of
    snapshots of a bouncing ball, we can predict reasonably well where the ball is
    going to end up.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在非常年轻的时候，甚至在没有任何正式训练的情况下，就发展了对周围世界力学和物理的初步直觉。我们不需要写下一系列方程式就能知道如何接住弹跳的球。我们甚至不需要一个实际的球。给定一系列弹跳球的快照，我们能够合理地预测球最终会落在何处。
- en: While these problems might seem trivial for us, they are critical for many physical
    industries, including manufacturing and autonomous driving. For example, autonomous
    driving systems need to anticipate what will happen in a traffic scene consisting
    of many moving objects. Until recently, this task was typically treated as a problem
    of computer vision. However, more recent approaches have begun to use GNNs [12].
    These GNN-based methods demonstrate that including relational information, such
    as how limbs are connected, can enable algorithms to develop physical intuition
    about how a person or animal moves with higher accuracy and less data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些问题对我们来说可能看似微不足道，但对于许多物理行业，包括制造业和自动驾驶，它们是至关重要的。例如，自动驾驶系统需要预测由许多移动物体组成的交通场景中会发生什么。直到最近，这项任务通常被视为计算机视觉问题。然而，最近的方法已经开始使用GNN
    [12]。这些基于GNN的方法表明，包括关系信息，如肢体如何连接，可以使算法以更高的准确性和更少的数据开发出关于人或动物如何移动的物理直觉。
- en: In figure 1.13, we give an example of how a body can be thought of as a “mechanical”
    graph. The input graphs for these physical reasoning systems have elements that
    reflect the problem. For instance, when reasoning about a human or animal body,
    a graph could consist of nodes that represent points on the body where limbs connect.
    For systems of free bodies, the nodes of a graph could be individual objects such
    as bouncing balls. The edges of the graph then represent the physical relationship
    (e.g., gravitational forces, elastic springs, or rigid connections) between the
    nodes. Given these inputs, GNNs learn to predict future states of a set of objects
    without explicitly calling on physical/mechanical laws [13]. These methods are
    a form of *edge prediction*; that is, they predict how the nodes connect over
    time. Furthermore, these models have to be dynamic to account for the temporal
    evolution of the system. We consider these problems in detail in chapter 6.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.13中，我们给出了一个例子，说明一个物体可以被看作是一个“机械”图。这些物理推理系统的输入图具有反映问题的元素。例如，当推理关于人或动物的身体时，图可以由表示身体上肢体连接点的节点组成。对于自由体系统，图的节点可以是单个物体，如弹跳球。图的边代表节点之间的物理关系（例如，重力、弹性弹簧或刚性连接）。给定这些输入，GNN学习预测一组物体的未来状态，而不需要明确调用物理/力学定律
    [13]。这些方法是一种*边预测*形式；也就是说，它们预测节点随时间如何连接。此外，这些模型必须是动态的，以解释系统的时态演变。我们将在第6章详细讨论这些问题。
- en: '![figure](../Images/1-13.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-13.png)'
- en: Figure 1.13 A graph representation of a mechanical body, taken from Sanchez-Gonzalez
    [13]. The body’s segments are represented as nodes, and the mechanical forces
    binding them are edges.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.13展示了机械体的图形表示，摘自Sanchez-Gonzalez [13]。该物体的各个部分被表示为节点，而将它们连接在一起的机械力则表示为边。
- en: 1.4 When to use a GNN?
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 何时使用GNN？
- en: Now that we’ve explored real-world applications of GNNs, let’s identify some
    underlying characteristics that make problems suitable for graph-based solutions.
    While the cases of the previous section clearly involved data that was naturally
    modeled as a graph, it’s crucial to recognize that GNNs can also be effectively
    applied to problems where the graph-like nature may not be immediately obvious.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经探讨了图神经网络（GNNs）在现实世界中的应用，让我们来识别一些使问题适合基于图解决方案的潜在特征。虽然上一节中的案例明显涉及自然建模为图的数据，但认识到GNNs也可以有效地应用于图形性质可能并不立即明显的问题至关重要。
- en: 'So, instead of simply stating that GNNs are useful for graph problems, this
    section will help you recognize patterns and relationships within your data that
    could benefit from graph-based modeling, even if those relationships aren’t immediately
    apparent. Essentially, there are three types of criteria for identifying GNN problems:
    implicit relationships and interdependencies; high dimensionality and sparsity;
    and complex nonlocal interactions.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本节将帮助您识别数据中的模式和关系，即使这些关系并不立即明显，这些模式和关系也可能从基于图的建模中受益。本质上，识别GNN问题的有三个标准：隐含关系和相互依赖性；高维度和稀疏性；以及复杂的非局部交互。
- en: 1.4.1 Implicit relationships and interdependencies
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 隐含关系和相互依赖性
- en: Graphs are versatile data structures that can model a wide range of relationships.
    Even when a problem doesn’t initially appear to be graph-like, even if your dataset
    is tabular, it’s beneficial to explore whether implicit relationships or interdependencies
    might exist that could be represented explicitly. Implicit relationships are connections
    that aren’t immediately documented or obvious within the data but can still play
    a significant role in understanding the underlying patterns and behaviors.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图是灵活的数据结构，可以模拟广泛的关系。即使问题最初看起来不是图形化的，即使你的数据集是表格的，探索是否存在可能明确表示的隐含关系或相互依赖性也是有益的。隐含关系是那些在数据中未立即记录或明显的连接，但仍然在理解潜在的模式和行为的理解中扮演着重要角色。
- en: Key indicators
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键指标
- en: To determine if your problem might benefit from modeling implicit relationships
    with graphs, consider whether there are hidden or indirect connections between
    entities in your dataset. For example, in customer behavior analysis, customers
    may appear as independent entities in a tabular dataset containing their purchases,
    demographics, and other details. However, they could be connected through social
    media influence, peer recommendations, or shared purchasing patterns, forming
    an underlying network of interactions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定你的问题是否可能从使用图来建模隐含关系中获得益处，考虑一下在你的数据集中是否存在实体之间的隐藏或间接联系。例如，在客户行为分析中，客户可能看起来在包含他们的购买、人口统计和其他细节的表格数据集中是独立的实体。然而，他们可能通过社交媒体影响、同伴推荐或共享的购买模式相互连接，形成一个潜在的交互网络。
- en: Another indicator is the presence of entities that share common attributes or
    activities without a direct or documented relationship. In the case of investors,
    for example, two or more investors may not have any formal connection but might
    frequently co-invest in the same companies under similar conditions. Such patterns
    of co-investment could indicate a shared strategy or influence. In this scenario,
    a graph representation can be created where nodes represent individual investors,
    and edges are formed between nodes when two or more investors co-invest in the
    same company. Additional attributes, such as investment size, timing, or the types
    of companies invested in can be added to nodes or edges, allowing GNNs to identify
    patterns, trends, or even potential collaboration opportunities.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个指标是存在共享共同属性或活动但没有直接或记录关系的实体。例如，在投资者的情况下，两个或更多的投资者可能没有任何正式的联系，但在类似条件下可能会频繁共同投资于同一公司。这种共同投资模式可能表明共享策略或影响。在这种情况下，可以创建一个图表示，其中节点代表个别投资者，当两个或更多投资者共同投资于同一公司时，节点之间形成边。可以添加到节点或边上的附加属性，如投资规模、时间或投资的公司类型，从而使GNNs能够识别模式、趋势，甚至潜在的合作机会。
- en: Additionally, consider whether the data involves entities that are interconnected
    through shared references or co-occurrence patterns. Document and text data may
    not immediately suggest a graph structure, but if documents cite each other or
    share common topics or authors, they can be represented as nodes in a graph, with
    edges reflecting these relationships. Similarly, terms within documents can form
    co-occurrence networks, which are useful for tasks such as keyword extraction,
    document classification, or topic modeling.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑数据是否涉及通过共享引用或共现模式相互连接的实体。文档和文本数据可能不会立即表明图结构，但如果文档相互引用或共享共同的主题或作者，它们可以表示为图中的节点，边反映了这些关系。同样，文档中的术语可以形成共现网络，这对于诸如关键词提取、文档分类或主题建模等任务是有用的。
- en: By identifying these key indicators in your data, you can uncover hidden or
    implicit relationships that can be represented explicitly through graphs. Such
    representations allow for more advanced analyses using GNNs, which can effectively
    capture and model these relationships, leading to more accurate predictions and
    deeper insights into the data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过识别你数据中的这些关键指标，你可以揭示可以通过图明确表示的隐藏或隐含关系。这种表示允许使用GNNs进行更高级的分析，这些GNNs可以有效地捕捉和建模这些关系，从而实现更准确的预测并更深入地了解数据。
- en: 1.4.2 High dimensionality and sparsity
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 高维性和稀疏性
- en: Graph-based models are particularly effective in handling high-dimensional data
    where many features may be sparse or missing. These models excel in situations
    where there are underlying structures connecting sparse entities, allowing for
    more meaningful analysis and improved performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图模型在处理高维数据方面特别有效，在这些数据中，许多特征可能是稀疏或缺失的。这些模型在存在连接稀疏实体的潜在结构的情况下表现出色，这允许进行更有意义的分析并提高性能。
- en: Key indicators
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键指标
- en: To determine if your problem involves high-dimensional and sparse data suitable
    for GNNs, consider whether your dataset contains numerous entities with limited
    direct interactions or relationships. For example, in recommender systems, user-item
    interaction data may appear tabular, but it’s inherently sparse—most users only
    interact with a small subset of the available items. By representing users and
    items as nodes and representing their interactions (e.g., purchases or clicks)
    as edges, GNNs can exploit network effects to make more accurate recommendations.
    These models can also address the cold-start problem by uncovering both explicit
    and implicit relationships, leading to better performance in recommending new
    items to users or engaging new users with existing items.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定你的问题是否涉及适合GNNs的高维和稀疏数据，考虑你的数据集是否包含大量具有有限直接交互或关系的实体。例如，在推荐系统中，用户-项目交互数据可能看起来是表格形式的，但它是固有的稀疏的——大多数用户只与可用项目的一小部分进行交互。通过将用户和项目表示为节点，并将它们的交互（例如，购买或点击）表示为边，GNNs可以利用网络效应来做出更准确的推荐。这些模型还可以通过揭示显性和隐性的关系来解决冷启动问题，从而在向用户推荐新项目或与现有项目吸引新用户方面表现出更好的性能。
- en: Another indicator that your problem may be suitable for graph-based models is
    when the data represents entities that are sparsely connected but share significant
    characteristics. In drug discovery, for example, molecules are represented as
    graphs, with atoms as nodes and chemical bonds as edges. This representation captures
    the inherent sparsity of molecular structures, where most atoms form only a few
    bonds, and large portions of the molecule may be distant from each other in the
    graph. Traditional machine learning methods often struggle to predict properties
    of new molecules due to this sparsity, as they don’t account for the full structural
    context.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个表明你的问题可能适合基于图模型的指标是，当数据表示的是稀疏连接但具有显著特征的实体时。例如，在药物发现中，分子被表示为图，原子作为节点，化学键作为边。这种表示捕捉了分子结构的固有稀疏性，其中大多数原子只形成少数键，分子的大部分部分在图中可能相距甚远。由于这种稀疏性，传统的机器学习方法往往难以预测新分子的属性，因为它们没有考虑到完整的结构背景。
- en: Graph-based models, particularly GNNs, overcome these challenges by capturing
    both local atomic environments and global molecular structures. GNNs learn hierarchical
    features from fine-grained atomic interactions to broader molecular properties,
    and their ability to remain invariant to the ordering of atoms ensures consistent
    predictions. By using the graph structure of molecules, GNNs make accurate predictions
    from sparse, connected data, thereby accelerating the drug discovery process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的模型，尤其是GNNs，通过捕捉局部原子环境和全局分子结构来克服这些挑战。GNNs从细粒度原子交互中学习层次特征，以更广泛的分子属性，并且它们对原子顺序的不变性确保了一致的预测。通过使用分子的图结构，GNNs可以从稀疏、连接的数据中做出准确的预测，从而加速药物发现过程。
- en: By recognizing these key indicators in your data, you can identify situations
    where graph-based models can effectively handle high-dimensional and sparse datasets.
    Representing such data as graphs allows GNNs to capture and use underlying structures,
    resulting in more accurate predictions and deeper insights across various applications.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过识别数据中的这些关键指标，你可以确定哪些情况下基于图模型可以有效地处理高维和稀疏数据集。将这些数据表示为图，允许GNNs捕捉和使用底层结构，从而在各种应用中实现更准确的预测和更深入的洞察。
- en: 1.4.3 Complex, nonlocal interactions
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 复杂、非局部交互
- en: Certain problems require understanding how distant elements in a dataset influence
    each other. In these cases, GNNs provide a framework to capture these complex
    interactions, where the predicted value or label of a particular data point depends
    not just on the features of its immediate neighbors but also on those of other
    related data points. This capability is especially useful when relationships extend
    beyond direct connections to involve multiple levels or degrees of separation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 某些问题需要理解数据集中遥远元素之间是如何相互影响的。在这些情况下，GNNs提供了一个框架来捕捉这些复杂的交互，其中特定数据点的预测值或标签不仅取决于其直接邻居的特征，还取决于其他相关数据点的特征。这种能力在关系超越了直接连接，涉及多个层级或分离度时特别有用。
- en: However, some standard GNNs, which rely primarily on local message passing,
    may struggle to capture long-range dependencies effectively. Advanced architectures
    or modifications, such as those incorporating global attention, nonlocal aggregation,
    or hierarchical message-passing, can better address these challenges [14].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些主要依赖于局部消息传递的标准GNNs可能难以有效地捕捉长距离依赖。通过结合全局注意力、非局部聚合或分层消息传递等高级架构或修改，可以更好地解决这些挑战
    [14]。
- en: Key indicators
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键指标
- en: To determine if your problem involves complex, nonlocal interactions suitable
    for GNNs, consider whether the outcome or behavior of one entity depends on the
    attributes or actions of entities that aren’t directly connected to it but may
    be indirectly connected through other entities. For example, in supply chain optimization,
    a delay in one supplier may not only affect its immediate downstream customers
    but could cascade through multiple levels of the network, influencing distributors
    and final consumers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定你的问题是否涉及适合GNNs的复杂、非局部交互，考虑一个实体的结果或行为是否依赖于与其没有直接连接但可能通过其他实体间接连接的实体的属性或行为。例如，在供应链优化中，一个供应商的延误不仅可能影响其直接下游客户，还可能通过网络的多个层级级联，影响分销商和最终消费者。
- en: Another indicator is whether the problem involves scenarios where information,
    influence, or effects propagate through a network over time. In healthcare and
    epidemiology, for instance, a disease outbreak might spread from a small cluster
    of patients through their interactions with shared healthcare providers, common
    environments, or overlapping social networks. Such propagation requires an approach
    that captures the indirect transmission pathways of information or effects.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个指标是问题是否涉及信息、影响或效应随时间通过网络传播的场景。例如，在医疗保健和流行病学中，疾病爆发可能通过患者与共享医疗保健提供者、共同环境或重叠的社会网络的互动从一个小的患者群传播开来。这种传播需要一种能够捕捉信息或效应间接传播途径的方法。
- en: 'To close this section, in determining whether your problem is a good candidate
    for a GNN, ask yourself these questions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本节之前，在确定你的问题是否适合GNN时，请自问以下问题：
- en: Are there implicit relationships or interdependencies in my data that I could
    model?
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的数据中是否存在我可以建模的隐含关系或相互依赖？
- en: Do the interactions between entities exhibit complex, nonlocal dependencies
    that go beyond immediate connections?
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体之间的交互是否表现出超越直接连接的复杂、非局部依赖？
- en: Is the data high-dimensional and sparse, with a need to capture underlying relational
    structures?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否是高维且稀疏的，需要捕捉潜在的关联结构？
- en: If the answer to any of these questions is yes, consider framing your problem
    as a graph and applying GNNs to unlock new insights and predictive capabilities.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些问题的答案中的任何一个为是，考虑将你的问题构建为图，并应用 GNN 来解锁新的见解和预测能力。
- en: 1.5 Understanding how GNNs operate
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 理解 GNN 的工作原理
- en: In this section, we’ll explore how GNNs work, starting from the initial collection
    of raw data to the final deployment of trained models. We’ll examine each step,
    highlighting the processes of data handling, model building, and the unique message-passing
    technique that sets GNNs apart from traditional deep learning models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 GNN 的工作原理，从最初收集原始数据到最终部署训练好的模型。我们将检查每个步骤，突出数据处理、模型构建以及将 GNN 区别于传统深度学习模型的独特消息传递技术。
- en: 1.5.1 Mental model for training a GNN
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 训练 GNN 的思维模型
- en: Our mental model covers the data sourcing, graph representation, preprocessing,
    and model development workflow. We start with raw data and end up with a trained
    GNN model and its outputs. Figure 1.14 illustrates and visualizes topics related
    to these stages, annotated with the chapters in which these topics appear.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的思维模型涵盖了数据来源、图表示、预处理和模型开发工作流程。我们从原始数据开始，最终得到一个训练好的 GNN 模型及其输出。图 1.14 说明了与这些阶段相关的话题，并标注了这些话题出现在哪些章节中。
- en: '![figure](../Images/1-14.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-14.png)'
- en: Figure 1.14 Mental model of the GNN project. We start with raw data, which is
    transformed into a graph data model that can be stored in a graph database or
    used in a graph processing system. From the graph processing system (and some
    graph databases), exploratory data analysis and visualization can be done. Finally,
    for graph machine learning, data is preprocessed into a form that can be submitted
    for training.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.14 GNN 项目的思维模型。我们从原始数据开始，将其转换为可以存储在图数据库中或用于图处理系统的图数据模型。从图处理系统（以及一些图数据库）中，可以进行探索性数据分析和可视化。最后，为了进行图机器学习，数据需要预处理成可以提交进行训练的形式。
- en: While not all workflows include every step or stage of this process, most will
    incorporate at least some elements. At different stages of a model development
    project, different parts of this process will typically be used. For example,
    when *training* a model, data analysis and visualization may be needed to make
    design decisions, but when *deploying* a model, it may only be necessary to stream
    raw data and quickly preprocess it for ingestion into a model. Though this book
    touches on the earlier stages in this mental model, the bulk of the book is focused
    on how to train different types of GNNs. When the other topics are discussed,
    they serve to support this main focus.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非所有工作流程都包括这个过程的每个步骤或阶段，但大多数都会包含至少一些元素。在模型开发项目的不同阶段，通常将使用这个过程中的不同部分。例如，当*训练*模型时，可能需要进行数据分析可视化以做出设计决策，但当*部署*模型时，可能只需要流式传输原始数据并快速预处理它以便将其摄入模型。尽管本书涉及了这个思维模型中的早期阶段，但本书的大部分内容集中在如何训练不同类型的
    GNN。当讨论其他主题时，它们服务于支持这个主要焦点。
- en: The mental model shows the core tasks of applying GNNs to machine learning problems,
    and we’ll be returning to this process repeatedly throughout the rest of the book.
    Let’s examine this diagram from end to end.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 思维模型展示了将 GNN 应用到机器学习问题中的核心任务，我们将在本书的其余部分反复回到这个过程。让我们从头到尾检查这个图。
- en: The first step in training a GNN is structuring this raw data into a graph format,
    if it isn’t already. This requires deciding which entities in the data to represent
    as nodes and edges, as well as determining the features to assign to them. Decisions
    must also be made about data storage—whether to use a graph database, processing
    system, or other formats.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 GNN 的第一步是将这些原始数据结构化为图格式，如果它还不是的话。这需要决定在数据中哪些实体应表示为节点和边，以及确定要分配给它们的特征。还必须做出关于数据存储的决定——是使用图数据库、处理系统还是其他格式。
- en: For machine learning, the data must be preprocessed for training and inference,
    involving tasks such as sampling, batching, and splitting the data into training,
    validation, and test sets. Throughout this book, we use PyTorch Geometric (PyG),
    which offers specialized classes for preprocessing and data splitting while preserving
    the graph’s structure. Preprocessing is covered in most chapters, with more in-depth
    explanations available in appendix B.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习，数据必须在训练和推理前进行预处理，包括采样、批处理以及将数据分割成训练集、验证集和测试集等任务。在这本书中，我们使用PyTorch Geometric（PyG），它提供了用于预处理和数据分割的专用类，同时保留图的架构。预处理在大多数章节中都有涉及，更深入的解释可以在附录B中找到。
- en: 'After processing the data, we can then move on to the model training. In this
    book, we cover several architectures and training types:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完数据后，我们就可以继续进行模型训练。在这本书中，我们涵盖了几个架构和训练类型：
- en: Chapters 2 and 3 discuss convolutional GNNs, where we first use a GCN layer
    to produce graph embeddings (chapter 2) and then train a full GCN and GraphSAGE
    models (chapter 3).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2章和第3章讨论了卷积GNN，我们首先使用GCN层来生成图嵌入（第2章），然后在第3章中训练完整的GCN和GraphSAGE模型。
- en: Chapter 4 explains graph attention networks (GATs), which adds attention to
    our GNNs.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4章解释了图注意力网络（GATs），它为我们的GNN添加了注意力。
- en: Chapter 5 introduces GNNs for unsupervised and generative problems, where we
    train and use a variational graph autoencoder (VGAE).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5章介绍了用于无监督和生成问题的GNN，其中我们训练并使用变分图自动编码器（VGAE）。
- en: Chapter 6 then explores the advanced concept of spatiotemporal GNNs, based on
    graphs that evolve over time. We train a neural relational inference (NRI) model,
    which combines an autoencoder structure with a recurrent neural network.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6章接着探讨了时空GNN的高级概念，基于随时间演变的图。我们训练了一个神经关系推理（NRI）模型，该模型结合了自动编码器结构和循环神经网络。
- en: Most of the examples provided for the GNNs mentioned so far are illustrated
    with code examples which use small-scale graphs that can fit into memory on a
    laptop or desktop computer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，提供的GNN示例大多数都是通过代码示例来展示的，这些示例使用的是可以适应笔记本电脑或台式计算机内存的小规模图。
- en: In chapter 7, we delve into strategies for handling data that exceeds the processing
    capacity of a single machine.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第7章中，我们深入探讨了处理超出单机处理能力的数据的策略。
- en: In chapter 8, we close with some considerations for graph and GNN projects,
    such as practical aspects of working with graph data, as well as how to convert
    nongraph data into a graph format.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第8章中，我们总结了关于图和GNN项目的考虑因素，例如与图数据一起工作的实际方面，以及如何将非图数据转换为图格式。
- en: 1.5.2 Unique mechanisms of a GNN model
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.2 GNN模型独特的机制
- en: Although there are a variety of GNN architectures at this point, they all tackle
    the same problem of dealing with graph data in a way that is permutation invariant.
    They do this via encoding and exchanging information across the graph structure
    during the learning process.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此时有各种各样的GNN架构，但它们都解决相同的问题，即以排列不变的方式处理图数据。它们通过在学习过程中对图结构进行编码和交换信息来实现这一点。
- en: In a conventional neural network, we first need to initialize a set of parameters
    and functions. These include the number of layers, the size of the layers, the
    learning rate, the loss function, the batch size, and other hyperparameters. (These
    are all treated in detail in other books on deep learning, so we assume you’re
    familiar with these terms.) Once we’ve defined these features, we then train our
    network by iteratively updating the weights of the network, as shown in figure
    1.15.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的神经网络中，我们首先需要初始化一组参数和函数。这包括层数、层的大小、学习率、损失函数、批大小以及其他超参数。（这些在其他关于深度学习的书籍中都有详细讨论，所以我们假设你熟悉这些术语。）一旦我们定义了这些特性，我们就通过迭代更新网络的权重来训练我们的网络，如图1.15所示。
- en: '![figure](../Images/1-15.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-15.png)'
- en: Figure 1.15 Process for training a GNN, which is similar to training most other
    deep learning models
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.15 训练GNN的过程，这与训练大多数其他深度学习模型类似
- en: 'Explicitly, we perform the following steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 明确地说，我们执行以下步骤：
- en: Input our data.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入我们的数据。
- en: Pass the data through neural network layers that transform the data according
    to the parameters of the layer and an activation rule.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据通过神经网络层传递，这些层根据层的参数和激活规则转换数据。
- en: Output a representation from the final layer of the network.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络的最后一层输出一个表示。
- en: Backpropagate the error, and adjust the parameters accordingly.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播错误，并相应地调整参数。
- en: Repeat these steps a fixed number of *epochs* (the process by which data is
    passed forward and backward to train a neural network).
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这些步骤固定数量的**epoch**（数据正向和反向传递以训练神经网络的流程）。
- en: For tabular data, these steps are exactly as listed, as shown in figure 1.16\.
    For graph-based or relational data, these steps are similar except that each epoch
    relates to one iteration of message passing, which is described in the next subsection.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表格数据，这些步骤与列表中列出的完全一致，如图1.16所示。对于基于图或关系的数据，这些步骤类似，但每个epoch对应于一次消息传递迭代，这将在下一小节中描述。
- en: '![figure](../Images/1-16.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-16.png)'
- en: Figure 1.16 Comparison of (simple) non-GNN (above) and GNN (below). GNNs have
    a layer that distributes data among its vertices.
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.16比较了（简单的）非GNN（上方）和GNN（下方）。GNN有一个层，它在其顶点之间分配数据。
- en: 1.5.3 Message passing
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.3 消息传递
- en: '*Message passing*, which is touched on throughout the book, is a central mechanism
    in GNNs that enables nodes to communicate and share information across a graph
    [15]. This process allows GNNs to learn rich, informative representations of graph-structured
    data, which is essential for tasks such as node classification, link prediction,
    and graph-level prediction. Figure 1.17 illustrates the steps involved in a typical
    message-passing layer.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本书多次提到的**消息传递**是GNN中的一个核心机制，它使得节点能够在图中进行通信和共享信息[15]。这个过程允许GNN学习丰富的、信息丰富的图结构数据的表示，这对于节点分类、链接预测和图级预测等任务至关重要。图1.17说明了典型消息传递层中涉及的步骤。
- en: '![figure](../Images/1-17.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-17.png)'
- en: Figure 1.17 Elements of our message passing layer. Each message passing layer
    consists of an aggregation, a transformation, and an update step.
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.17展示了我们的消息传递层的元素。每个消息传递层由聚合、转换和更新步骤组成。
- en: The message-passing process begins with the Input (step 1) of the initial graph,
    where every node and edge have their own features. In the Collect step (step 2),
    each node gathers information from its immediate neighbors—these pieces of information
    are referred to as “messages.” This step ensures that each node has access to
    the features of its neighbors, which are crucial for understanding the local graph
    structure.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递过程从初始图的输入（步骤1）开始，其中每个节点和边都有自己的特征。在收集步骤（步骤2）中，每个节点从其直接邻居那里收集信息——这些信息被称为“消息”。这一步骤确保每个节点都能访问其邻居的特征，这对于理解局部图结构至关重要。
- en: Next, in the Aggregate step (step 3), the collected messages from neighboring
    nodes are combined using an invariant function, such as sum, mean, or max. This
    aggregation consolidates the information from a node’s neighborhood into a single
    vector, capturing the most relevant details about its local environment.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在聚合步骤（步骤3）中，使用不变函数（如求和、平均值或最大值）将来自邻居节点的收集到的消息组合起来。这种聚合将来自节点邻域的信息合并成一个向量，捕捉其局部环境中最相关的细节。
- en: In the Transform step (step 4), the aggregated messages are processed by a neural
    network to produce a new representation for each node. This transformation allows
    the GNN to learn complex interactions and patterns within the graph by applying
    nonlinear functions to the aggregated information.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换步骤（步骤4）中，聚合的消息通过神经网络进行处理，为每个节点生成一个新的表示。这种转换允许GNN通过将非线性函数应用于聚合信息来学习图中的复杂交互和模式。
- en: Finally, during the Update step (step 5), the features of each node in the graph
    are replaced or updated with these new representations. This completes one round
    of message passing, incorporating information from neighboring nodes to refine
    each node’s features.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在更新步骤（步骤5）中，图中每个节点的特征被用这些新的表示替换或更新。这完成了一轮消息传递，结合了来自邻居节点的信息以细化每个节点的特征。
- en: Each message-passing layer in a GNN allows nodes to gather information from
    nodes that are further away, or more “hops” away, in the graph. Repeating these
    steps over multiple layers enables the GNN to capture more complex dependencies
    and long-range interactions within the graph.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GNN中的每一层消息传递层都允许节点从图中更远的节点或更多“跳数”的节点那里收集信息。通过在多层上重复这些步骤，GNN能够捕捉到图中的更复杂依赖关系和长距离相互作用。
- en: By using message passing, GNNs efficiently encode the graph structure and data
    into useful representations for a variety of downstream tasks. Advanced architectures,
    such as those incorporating global attention or hierarchical message passing,
    further enhance the model’s ability to capture long-range dependencies across
    the graph, enabling more robust performance on diverse applications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用消息传递，图神经网络（GNNs）有效地将图结构和数据编码成对各种下游任务有用的表示。高级架构，如那些结合全局注意力或分层消息传递的架构，进一步增强了模型捕捉图中长距离依赖关系的能力，从而在多样化的应用中实现更稳健的性能。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Graph neural networks (GNNs) are specialized tools for handling relational,
    or relationship-centric, data, particularly in scenarios where traditional neural
    networks struggle due to the complexity and diversity of graph structures.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）是专门用于处理关系型或以关系为中心的数据的工具，尤其是在传统神经网络因图结构的复杂性和多样性而难以处理的情况下。
- en: GNNs have found significant applications in areas such as recommendation engines,
    drug discovery, and mechanical reasoning, showcasing their versatility in handling
    large and complex relational data for enhanced insights and predictions.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GNNs在推荐引擎、药物发现和机械推理等领域找到了显著的应用，展示了它们在处理大型和复杂关系数据以增强洞察力和预测方面的多功能性。
- en: Specific GNN tasks include node prediction, edge prediction, graph prediction,
    and graph representation through embedding techniques.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定的GNN任务包括节点预测、边预测、图预测以及通过嵌入技术进行图表示。
- en: GNNs are best used when data is represented as a graph, indicating a strong
    emphasis on relationships and connections between data points. They aren’t ideal
    for individual, standalone data entries where relational information is insignificant.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据以图的形式表示，表明对数据点之间关系和连接的强烈关注时，GNNs的使用最为理想。它们不适用于关系信息不重要的单个、独立的数据条目。
- en: When deciding if a GNN solution is a good fit for your problem, consider cases
    that have characteristics such as implicit relationships, high-dimensionality,
    sparsity, and complex nonlocal interactions. By understanding these fundamentals,
    practitioners can evaluate the suitability of GNNs for their specific problems,
    implement them effectively, and recognize their tradeoffs and limitations in real-world
    applications.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当决定GNN解决方案是否适合你的问题时，考虑具有隐含关系、高维度、稀疏性和复杂非局部交互等特征的案例。通过理解这些基本原理，从业者可以评估GNNs对其特定问题的适用性，有效地实施它们，并在实际应用中认识到它们的权衡和限制。
- en: Message passing is a core mechanism of GNNs, which enables them to encode and
    exchange information across a graph’s structure, allowing for meaningful node,
    edge, and graph-level predictions. Each layer of a GNN represents one step of
    message passing, with various aggregation functions to combine messages effectively,
    providing insights and representations useful for machine learning tasks.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息传递是GNNs的核心机制，它使它们能够在图结构中编码和交换信息，从而允许进行有意义的节点、边和图级别预测。GNN的每一层代表消息传递的一个步骤，使用各种聚合函数有效地组合消息，为机器学习任务提供洞察力和表示。
