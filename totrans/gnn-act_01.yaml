- en: 1 Discovering graph neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 发现图神经网络
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining graphs and graph neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义图和图神经网络
- en: Understanding why people are excited about graph neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么人们对图神经网络感到兴奋
- en: Recognizing when to use graph neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到何时使用图神经网络
- en: Taking a big picture look at solving a problem with a graph neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从宏观角度审视使用图神经网络解决问题
- en: For data practitioners, the fields of machine learning and data science initially
    excite us because of the potential to draw nonintuitive and useful insights from
    data. In particular, the insights from machine learning and deep learning promise
    to enhance our understanding of the world. For the working engineer, these tools
    promise to deliver business value in unprecedented ways.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据从业者来说，机器学习和数据科学领域最初之所以令人兴奋，是因为从数据中提取非直观且有用的见解的潜力。特别是，机器学习和深度学习的见解有望增强我们对世界的理解。对于工作的工程师来说，这些工具承诺以前所未有的方式创造商业价值。
- en: Experience deviates from this ideal. Real-world data is usually messy, dirty
    and biased. Furthermore, statistical methods and learning systems come with their
    own set of limitations. An essential role of the practitioner is to comprehend
    these limitations and bridge the gap between real data and a feasible solution.
    For example, we may want to predict fraudulent activity in a bank, but we first
    need to make sure that our training data has been correctly labeled. Even more
    importantly, we’ll need to check that our models won’t incorrectly assign fraudulent
    activity to normal behaviors, possibly due to some hidden confounders in the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 经验与这一理想有所偏差。现实世界的数据通常杂乱无章、污秽且存在偏见。此外，统计方法和学习系统都带有自己的局限性。实践者的一个基本作用是理解这些局限性，弥合真实数据与可行解决方案之间的差距。例如，我们可能想要预测银行的欺诈活动，但首先需要确保我们的训练数据已被正确标记。更重要的是，我们还需要检查我们的模型不会错误地将欺诈活动分配给正常行为，这可能是由于数据中的一些隐藏混杂因素。
- en: For graph data, until recently, bridging this gap has been particularly challenging.
    Graphs are a data structure that is rich with information and especially adept
    at capturing the intricacies of data where relationships play a crucial role.
    Graphs are omnipresent, with relational data appearing in different forms such
    as atoms in molecules (nature), social networks (society), and even models the
    connection of web pages on the internet (technology) [1]. It’s important to note
    that the term *relational* here doesn’t refer to *relational databases*, but rather
    to data where relationships are of significance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图数据，直到最近，弥合这一差距一直特别具有挑战性。图是一种信息丰富的数据结构，特别擅长捕捉数据中关系所扮演的关键角色。图无处不在，关系数据以不同的形式出现，如分子中的原子（自然）、社交网络（社会），甚至互联网上网页之间的连接模型（技术）[1]。需要注意的是，这里的“关系”一词并不指代“关系数据库”，而是指关系中具有显著意义的数据。
- en: Previously, if you wanted to incorporate relational features from a graph into
    a deep learning model, it had to be done in an indirect way, with different models
    used to process, analyze, and then use the graph data. These separate models often
    couldn’t be easily scaled and had trouble taking into account all the node and
    edge properties of graph data. To make the best use of this rich and ubiquitous
    data type for machine learning, we needed a specialized machine learning technique
    specifically designed for the distinct qualities of graphs and relational data.
    This is the gap that graph neural networks (GNNs) fill.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，如果你想在深度学习模型中整合图中的关系特征，必须以间接的方式进行，使用不同的模型来处理、分析和然后使用图数据。这些独立的模型通常难以扩展，并且难以考虑到图数据的所有节点和边属性。为了充分利用这种丰富且普遍存在的数据类型进行机器学习，我们需要一种专门为图的独特品质和关系数据设计的机器学习技术。这正是图神经网络（GNNs）填补的差距。
- en: The deep learning field often contains a lot of hype around new technologies
    and methods. However, GNNs are widely recognized as a genuine leap forward for
    graph-based learning [2]. This doesn’t mean that GNNs are a silver bullet. Careful
    comparisons should be done between predictive results derived from GNNs and other
    machine learning and deep learning methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域常常围绕新技术和方法存在很多炒作。然而，GNNs被广泛认为是基于图学习的一个真正的飞跃[2]。这并不意味着GNNs是一劳永逸的解决方案。应该仔细比较GNNs得出的预测结果与其他机器学习和深度学习方法。
- en: The key thing to remember is that if your data science problem involves data
    that can be structured as a graph—that is, the data is connected or relational—then
    GNNs could offer a valuable approach, even if you weren’t aware that something
    was missing in your approach. GNNs can be designed to handle very large data,
    to scale, and to adapt to graphs of different sizes and shapes. This can make
    working with relationship-centric data easier and more efficient, as well as yield
    richer results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的关键点是，如果你的数据科学问题涉及可以结构化为图的数据——也就是说，数据是连接的或相关的——那么GNNs可以提供一种有价值的解决方案，即使你还没有意识到你的方法中缺少了什么。GNNs可以设计来处理非常大的数据，进行扩展，并适应不同大小和形状的图。这可以使处理以关系为中心的数据更加容易和高效，同时产生更丰富的结果。
- en: The standout advantages of GNNs are why data scientists and engineers are increasingly
    recognizing the importance of mastering them. GNNs have the ability to unveil
    unique insights from relational data—from identifying new drug candidates to optimizing
    ETA prediction accuracy in your Google Maps app—acting as a catalyst for discovery
    and innovation, and empowering professionals to push the boundaries of conventional
    data analysis. Their diverse applicability spans various fields, offering professionals
    a versatile tool that is as relevant in e-commerce (e.g., recommendation engines)
    as it is in bioinformatics (e.g., drug toxicity prediction). Proficiency in GNNs
    equips data professionals with a multifaceted tool for enhanced, accurate, and
    innovative data analysis of graphs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs的突出优势是为什么数据科学家和工程师越来越认识到掌握它们的重要性。GNNs能够从关系数据中揭示独特的见解——从识别新的药物候选者到优化Google
    Maps应用中的ETA预测准确性——作为发现和创新的催化剂，并赋予专业人士推动传统数据分析边界的力量。它们的广泛应用跨越各个领域，为专业人士提供了一种多功能的工具，在电子商务（例如，推荐引擎）和生物信息学（例如，药物毒性预测）中同样相关。GNNs的熟练掌握使数据专业人士能够进行增强、准确和创新的图数据分析。
- en: For all these reasons, GNNs are now the popular choice for recommender engines,
    analyzing social networks, detecting fraud, understanding how biomolecules behave,
    and many other practical examples that we’ll meet over the course of this book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些原因，图神经网络（GNNs）现在已成为推荐引擎、分析社交网络、检测欺诈、理解生物分子行为以及本书中我们将遇到的其他许多实际应用的流行选择。
- en: 1.1 Goals of this book
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 本书的目标
- en: '*Graph Neural Networks in Action* is aimed at practitioners who want to begin
    to deploy GNNs to solve real problems. This could be a machine learning engineer
    not familiar with graph data structures, a data scientist who hasn’t yet tried
    GNNs, or even a software engineer who may be unfamiliar with either. Throughout
    this book, we’ll be covering topics from the basics of graphs all the way to more
    complex GNN models. We’ll be building up the architecture of a GNN, step-by-step.
    This includes the overall architecture of a GNN and the critical aspect of message
    passing. We then go on to add different features and extensions to these basic
    aspects, such as introducing convolution and sampling, attention mechanisms, a
    generative model, and operating on dynamic graphs. When building our GNNs, we’ll
    be working in Python and using some standard libraries. GNN libraries are either
    standalone or use TensorFlow or PyTorch as a backend. In this text, the focus
    will be on PyTorch Geometric (PyG). Other popular libraries include Deep Graph
    Library (DGL, a standalone library) and Spektral (which uses Keras and TensorFlow
    as a backend). There is also Jraph for JAX users.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 《图神经网络实战》（Graph Neural Networks in Action）旨在帮助实践者开始部署GNNs来解决实际问题。这可能是一个不熟悉图数据结构的机器学习工程师，一个尚未尝试GNNs的数据科学家，甚至可能是一个可能对两者都不熟悉的软件工程师。在本书中，我们将从图的基本知识一直覆盖到更复杂的GNN模型。我们将逐步构建GNN的架构。这包括GNN的整体架构和消息传递的关键方面。然后我们继续添加不同的功能和扩展到这些基本方面，例如引入卷积和采样、注意力机制、生成模型以及在动态图上操作。在构建我们的GNNs时，我们将使用Python和一些标准库。GNN库要么是独立的，要么使用TensorFlow或PyTorch作为后端。在本文本中，重点将放在PyTorch
    Geometric（PyG）上。其他流行的库包括深度图库（Deep Graph Library，DGL，一个独立的库）和Spektral（它使用Keras和TensorFlow作为后端）。还有Jraph，适用于JAX用户。
- en: Our aim throughout this book is to enable you to
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是使你能够
- en: assess the suitability of a GNN solution for your problem.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估GNN解决方案对你问题的适用性。
- en: understand when traditional neural networks won’t perform as well as a GNN for
    graph structured data and when GNNs may not be the best tool for tabular data.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解何时传统的神经网络在处理图结构数据时不如GNN表现得好，以及何时GNN可能不是表格数据的最佳工具。
- en: design and implement a GNN architecture to solve problems specific to you.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和实现一个GNN架构来解决你特有的问题。
- en: make clear the limitations of GNNs.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确GNNs的限制。
- en: This book is weighted toward implementation using programming. We also devote
    some time on essential theory and concepts, so that the techniques covered can
    be sufficiently understood. These are covered in an “Under the Hood” section at
    the end of most chapters to separate the technical reasons from the actual implementation.
    There are many different models and packages that build on the key concepts we
    introduce in this book. So, this book shouldn’t be seen as a comprehensive review
    of all GNN methods and models, which could run to several thousands of pages,
    but rather the starting point for the curious and eager-to-learn practitioner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书侧重于使用编程实现。我们也花了一些时间在必要理论和概念上，以确保所涵盖的技术能够被充分理解。这些内容在大多数章节的“内部机制”部分进行介绍，以区分技术原因和实际实现。本书基于我们在书中介绍的关键概念，有许多不同的模型和包。因此，本书不应被视为对所有GNN方法和模型的全面回顾，这可能需要数千页，而应被视为好奇和渴望学习的实践者的起点。
- en: The book is divided into three parts. Part 1 covers the basics of GNNs, especially
    the ways in which they differ from other neural networks, such as *message passing*
    and *embeddings*, which have specific meaning for GNNs. Part 2, the heart of the
    book, goes over the models themselves, where we cover a handful of key model types.
    Then, in part 3, we’ll go into more detail with some of the harder models and
    concepts, including how to scale graphs and deal with temporal data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为三个部分。第一部分涵盖了GNNs的基础知识，特别是它们与其他神经网络（如*消息传递*和*嵌入*）的不同之处，这些对于GNNs具有特定的含义。第二部分，本书的核心内容，介绍了模型本身，其中我们涵盖了几个关键模型类型。然后，在第三部分，我们将对一些更复杂的模型和概念进行更详细的探讨，包括如何扩展图以及处理时间数据。
- en: '*Graph Neural Networks in Action* is designed for people to jump quickly into
    this new field and start building applications. Our aim for this book is to reduce
    the friction of implementing new technologies by filling in the gaps and answering
    key development questions whose answers may not be easy to find or may not be
    covered elsewhere at all. Each method is introduced through an example application
    so you can understand how GNNs are applied in practice. We strongly advise you
    to try out the code for yourself along the way.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 《图神经网络实战》旨在帮助人们快速进入这个新领域并开始构建应用。本书的目标是通过填补空白和回答关键开发问题来减少实现新技术的摩擦，这些问题可能不容易找到或根本未在其他地方涉及。每种方法都是通过一个示例应用来介绍的，这样你可以了解GNNs在实际中的应用。我们强烈建议你在过程中亲自尝试代码。
- en: 1.1.1 Catching up on graph fundamentals
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 补充图的基本知识
- en: Yes, you do need to understand the basics of graphs before you can understand
    GNNs. Yet our goal for this book is to teach GNNs to deep learning practitioners
    and builders of traditional neural networks who may not know much about graphs.
    At the same time, we also recognize that readers of this book may vary enormously
    in their knowledge of graphs. How to address these differences and make sure everyone
    has what they need to make the most of this book? In this chapter, we provide
    an introduction to the fundamental graph concepts that are most essential to understanding
    GNNs. If you’re well-versed in graphs, you may choose to skip this section, although
    we recommend skimming through as we cover some specific terminology and use-cases
    that will be helpful to understand for the remainder of the book. For those of
    you who have more questions about graphs, we’ve also included a full tutorial
    on basic graph concepts and terminology in appendix A. This primer should also
    serve as a reference for looking up specific concepts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，在理解GNNs之前，你需要了解图的基本知识。然而，本书的目标是教授GNNs给深度学习实践者和传统神经网络的构建者，他们可能对图知之甚少。同时，我们也认识到本书的读者在图的知识上可能存在巨大差异。如何解决这些差异并确保每个人都能充分利用本书？在本章中，我们提供了对理解GNNs最基本图概念的介绍。如果你对图非常熟悉，你可以选择跳过这一部分，尽管我们建议快速浏览，因为我们涵盖了一些对理解本书剩余部分有帮助的特定术语和用例。对于那些对图有更多疑问的人，我们在附录A中也包含了一个关于基本图概念和术语的完整教程。这个入门指南也应当作为查找特定概念的参考。
- en: After the refresher on key concepts in graphs and graph learning, we’ll look
    into some case studies in several fields where GNNs are being successfully applied.
    Then, we’ll break down those specific cases to see what makes a good case for
    using a GNN, as well as how to know if you have a GNN problem on your hands. At
    the end of the chapter, we introduce the mechanics of GNNs, the barebone skeleton
    that the rest of the book will add to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Graph-based learning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section defines graphs, graph-based learning, and some fundamentals of
    GNNs, including the basic structure of a graph and a taxonomy of different types
    of graphs. Then, we’ll review graph-based learning, putting GNNs in context with
    other learning methods. Finally, we’ll explain the value of graphs, ending with
    an example of data derived from the Titanic dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 What are graphs?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graphs are data structures with elements, expressed as *nodes or vertices*,
    and relationships between elements, expressed as *edges or links*, as shown in
    figure 1.1\. All nodes in the graph will have additional *feature data*. This
    is node-specific data, relating to things such as the names or ages of individuals
    in a social network. The links are key to the power of relational data, as they
    allow us to learn more about the system, give new tools for analyzing data, and
    predict new properties from it. This is in contrast to tabular data such as a
    database table, dataframe, or spreadsheet, where the data is fixed in rows and
    columns.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 A graph. Individual elements, represented here by letters A through
    E, are nodes, also called vertices, and their relationships are described by edges,
    also known as links.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To describe and learn from the edges between the nodes, we need a way to write
    them down. This can be done explicitly, stating that the A node is connected to
    B and E, and that the B node is connected to A, C, D, and E. Quickly, we can see
    that describing things in this way becomes unwieldy and that we might be repeating
    redundant information (that A is connected to B and that B is connected to A).
    Luckily, there are many mathematical formalisms for describing relations in graphs.
    One of the most common is to describe the *adjacency matrix*, which we write out
    in table 1.1\. Notice that the adjacency matrix is symmetric across the diagonal
    and that all values are ones or zero.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.1 The adjacency matrix for the simple graph in figure 1.1
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '|  | A | B | C | D | E |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| **A**  | 0  | 1  | 0  | 0  | 1  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| **B**  | 1  | 0  | 1  | 1  | 1  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| **C**  | 0  | 1  | 0  | 1  | 0  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| **D**  | 0  | 1  | 1  | 0  | 1  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| **E**  | 1  | 1  | 0  | 1  | 0  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: The adjacency matrix of a graph is an important concept that makes it easy to
    observe all the connections of a graph in a single table [3]. Here, we assumed
    that there is no directionality in our graph; that is, if 0 is connected to 1,
    then 1 is also connected to 0\. This is known as an *undirected graph*. Undirected
    graphs can be easily inferred from an adjacency matrix because, in this case,
    the matrix is symmetric across the diagonal (e.g., in table 1.1, the upper-right
    triangle is reflected onto the bottom left).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图的邻接矩阵是一个重要的概念，它使得在一个表格中观察图的所有连接变得容易[3]。在这里，我们假设我们的图没有方向性；也就是说，如果0连接到1，那么1也连接到0。这被称为*无向图*。无向图可以从邻接矩阵中轻松推断出来，因为在这种情况下，矩阵在对角线上是对称的（例如，在表1.1中，右上角被反射到左下角）。
- en: We also assume here that all the relations between nodes are identical. If we
    wanted the relation of nodes B–E to mean more than the relation of nodes B–A,
    then we could increase the weight of this edge. This translates to increasing
    the value in the adjacency matrix, making the entry for the B–A edge in table
    1.1 equal to 10 instead of 1, for example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设节点之间的关系都是相同的。如果我们想让节点B-E的关系比节点B-A的关系更重要，我们可以增加这条边的权重。这相当于增加邻接矩阵中的值，例如，使表1.1中B-A边的条目等于10而不是1。
- en: Graphs where all relations are of equal importance are known as *unweighted
    graphs* and can also be easily observed from the adjacency matrix because all
    graph entries are either 1s or 0s. Graphs where edges have multiple values are
    known as *weighted.*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关系都同等重要的图称为*无权图*，并且可以从邻接矩阵中轻松观察到，因为所有图条目要么是1，要么是0。具有多个值的边的图称为*加权图*。
- en: If any of the nodes in the graph don’t have an edge that connects to itself,
    then the nodes will also have 0s at their own value in the adjacency matrix (0s
    along the diagonal). This means a graph doesn’t have self-loops. A *self-loop*
    occurs when a node has an edge that connects to that same node. To add a self-loop,
    we just make the value for that node nonzero at its position in the diagonal.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图中任何节点都没有连接到自身的边，那么这些节点在其邻接矩阵中的值也将是0（对角线上的0）。这意味着图中没有自环。当一个节点有一个连接到同一节点的边时，就发生了*自环*。要添加自环，我们只需将该节点在对角线上的值设置为非零。
- en: In practice, an adjacency matrix is only one of many ways to describe relations
    in a graph. Others include adjacency lists, edge lists, or an incidence matrix.
    Understanding these types of data structures well is vital to graph-based learning.
    If you’re unfamiliar with these terms, or need a refresher, we recommend looking
    through appendix A, which has additional details and explanations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，邻接矩阵只是描述图中关系的许多方法之一。其他包括邻接表、边列表或关联矩阵。充分理解这些数据结构对于基于图的学习至关重要。如果您不熟悉这些术语或需要复习，我们建议查阅附录A，其中包含更多细节和解释。
- en: 1.2.2 Different types of graphs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 不同类型的图
- en: Understanding the many different types of graphs can help us work out what methods
    to use to analyze and transform the graph, and what machine learning methods to
    apply. In the following, we give a very quick overview of some of the most common
    properties for graphs to have. As before, we recommend you look through appendix
    A for further information.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 了解许多不同类型的图可以帮助我们确定使用哪些方法来分析和转换图，以及应用哪些机器学习方法。以下，我们给出了一些最常见的图属性的快速概述。和之前一样，我们建议您查阅附录A以获取更多信息。
- en: Homogeneous and heterogeneous graphs
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同质图和异质图
- en: The most basic graphs are *homogenous graphs*, which are made up of one type
    of node and one type of edge. Consider a homogeneous graph that describes a recruitment
    network. In this type of graph, the nodes would represent job candidates, and
    the edges would represent relationships between the candidates.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的图是*同质图*，由一种类型的节点和一种类型的边组成。考虑一个描述招聘网络的同质图。在这种类型的图中，节点将代表求职者，边将代表候选人之间的关系。
- en: If we want to expand the power of our graph to describe our recruitment network,
    we could give it more types of nodes and edges, making it a *heterogeneous graph*.
    With this expansion, some nodes may be candidates and others may be companies.
    Edges could now consist of relationships between candidates and current or past
    employment of job candidates at the companies. See figure 1.2 for a comparison
    of a homogeneous graph (all nodes or edges have the same shade) with a heterogeneous
    graph (nodes and edges have a variety of shades).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要扩展我们图的能力来描述我们的招聘网络，我们可以给它更多的节点和边类型，使其成为一个**异质图**。通过这种扩展，一些节点可能是候选人，而其他节点可能是公司。边现在可以由候选人与公司当前或过去的就业关系组成。参见图1.2中同质图（所有节点或边具有相同的阴影）与异质图（节点和边具有各种阴影）的比较。
- en: '![figure](../Images/1-2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-2.png)'
- en: Figure 1.2 A homogeneous graph and a heterogeneous graph. Here, the shade of
    a node or edge represents its type or class. For the homogeneous graph, all nodes
    are of the same type, and all edges are of the same type. For the heterogeneous
    graph, nodes and edges have multiple types.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 同质图和异质图。在此，节点或边的阴影代表其类型或类别。对于同质图，所有节点都是同一类型，所有边也都是同一类型。对于异质图，节点和边有多种类型。
- en: Bipartite graphs
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二部图
- en: Similar to heterogeneous graphs, *bipartite graphs* also can be separated or
    partitioned into different subsets. However, bipartite graphs (figure 1.3) have
    a very specific network structure such that nodes in each subset connect to nodes
    outside of their subset and not inside. Later, we’ll be discussing recommendation
    systems and the Pinterest graph. This graph is bipartite because one set of nodes
    (pins) connects another set of nodes (boards) but not to nodes within their set
    (pins).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与异质图类似，**二部图**也可以被分离或划分为不同的子集。然而，二部图（图1.3）具有一个非常特定的网络结构，即每个子集中的节点连接到其子集之外的节点，而不是内部的节点。稍后，我们将讨论推荐系统和Pinterest图。这个图是二部图，因为一组节点（图钉）连接另一组节点（版面），但不是它们自己组内的节点（图钉）。
- en: '![figure](../Images/1-3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-3.png)'
- en: Figure 1.3 A bipartite graph. There are two types of nodes (two shades of circles).
    In a bipartite graph, nodes can’t be connected to nodes of the same type. This
    is also an example of a heterogeneous graph.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3 二部图。有两种类型的节点（两种阴影的圆圈）。在二部图中，节点不能连接到同一类型的节点。这也是一个异质图的例子。
- en: Cyclic graphs, acyclic graphs, and directed acyclic graphs
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环图、无环图和有向无环图
- en: A graph is *cyclic* if it allows you to start at a node, travel along its edges,
    and return to the starting node without retracing any steps, creating a circular
    path within the graph. In contrast, in an *acyclic* graph, no matter which path
    you take from any starting node, you can’t return to the starting point without
    backtracking. These graphs, as shown in figure 1.4, often resemble tree-like structures
    or paths that don’t loop back on themselves.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个图是**循环的**，那么你可以从一个节点开始，沿着其边旅行，不重复任何步骤，返回到起始节点，在图中创建一个环形路径。相比之下，在一个**无环**图中，无论从任何起始节点选择哪条路径，你都不能不回溯就返回到起始点。这些图，如图1.4所示，通常类似于树状结构或没有回环的路径。
- en: While both cyclic and acyclic graphs can be either undirected or directed, a
    *directed acyclic graph (DAG)* is a specific type of acyclic graph that is exclusively
    directed. In a DAG, all edges have a direction, and no cycles are allowed. DAGs
    represent one-way relationships where you can’t follow the arrows and end up back
    at the starting point. This characteristic makes DAGs essential in causal analysis,
    as they reflect causal structures where causality is assumed to be unidirectional.
    For example, A can cause B, but B can’t simultaneously cause A. This unidirectional
    nature aligns perfectly with the structure of DAGs, making them ideal for modeling
    workflow processes, dependency chains, and causal relationships in various fields.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有向图和无向图都可以是有向的或无向的，但**有向无环图（DAG**）是一种特定的无环图，它仅限于有向。在有向无环图中，所有边都有方向，不允许有环。DAGs代表单向关系，你不能跟随箭头并最终回到起点。这种特性使得DAGs在因果分析中至关重要，因为它们反映了假设因果是单向的因果结构。例如，A可以导致B，但B不能同时导致A。这种单向性质与DAGs的结构完美契合，使它们成为建模各种领域中的工作流程过程、依赖链和因果关系的理想选择。
- en: '![figure](../Images/1-4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-4.png)'
- en: Figure 1.4 A cyclic graph (left), an acyclic graph (right), and a DAG (bottom).
    In the cyclic graph, the cycle is shown by the arrows (directed edges) connecting
    nodes A-E-D-C-B-A. Note that two nodes, G and F are part of the graph, but not
    part of its defining cycle. The acyclic graph is composed of undirected edges,
    and no cycle is possible. In the DAG, all directed edges flow in one direction,
    from A to F.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 展示了循环图（左）、无环图（右）和DAG（底部）。在循环图中，通过连接节点A-E-D-C-B-A的箭头（有向边）显示了循环。请注意，节点G和F是图的一部分，但不是其定义循环的一部分。无环图由无向边组成，不可能存在循环。在DAG中，所有有向边都流向一个方向，从A到F。
- en: Knowledge graphs
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识图谱
- en: A *knowledge graph* is a specialized type of heterogeneous graph that represents
    data with enriched semantic meaning, capturing not only the relationships between
    different entities but also the context and nature of these relationships. Unlike
    conventional graphs, which primarily emphasize structure and connectivity, a knowledge
    graph incorporates metadata and follows specific schemas to provide deeper contextual
    information. This allows for advanced reasoning and querying capabilities, such
    as identifying patterns, uncovering specific types of connections, or inferring
    new relationships.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*知识图谱*是一种特殊类型的异构图，它通过丰富的语义意义来表示数据，不仅捕捉不同实体之间的关系，还捕捉这些关系的上下文和本质。与主要强调结构和连接性的传统图不同，知识图谱结合了元数据和遵循特定模式，以提供更深入的上下文信息。这允许进行高级推理和查询功能，例如识别模式、揭示特定类型的连接或推断新的关系。'
- en: In the example of an academic research network at a university, a knowledge
    graph might represent various entities such as Professors, Students, Papers, and
    Research Topics, and explicitly define the relationships between them. For instance,
    Professors and Students could be associated with Papers through an Authorship
    relationship, while Professors might also Supervise Students. Furthermore, the
    graph would reflect hierarchical structures, such as Professors and Students being
    categorized under Departments. You can see this knowledge graph depicted in figure
    1.5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在大学学术研究网络的例子中，知识图谱可能表示各种实体，如教授、学生、论文和研究主题，并明确定义它们之间的关系。例如，教授和学生可以通过作者关系与论文相关联，而教授也可能指导学生。此外，图将反映层级结构，如教授和学生被归类在系别下。您可以在图1.5中看到这个知识图谱的表示。
- en: '![figure](../Images/1-5.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/1-5.png)'
- en: Figure 1.5 A knowledge graph representing an academic research network within
    a university’s physics department. The graph illustrates both hierarchical relationships,
    such as professors and students as members of the department, and behavioral relationships,
    such as professors supervising students and authoring papers. Entities such as
    Professors, Students, Papers, and Topics are connected through semantically meaningful
    relationships (Supervises, Wrote, Inspires). Entities also have detailed features
    (Name, Department, Type) providing further context. The semantic connections and
    features enable advanced querying and analysis of complex academic interactions.
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5 表示大学物理系内部学术研究网络的图谱。该图谱展示了层级关系，例如教授和学生作为系成员，以及行为关系，例如教授指导学生和撰写论文。实体如教授、学生、论文和主题通过语义上有意义的关系（指导、撰写、启发）相互连接（Supervises,
    Wrote, Inspires）。实体还具有详细特征（姓名、系别、类型），提供更多上下文。语义连接和特征使得对复杂的学术互动进行高级查询和分析成为可能。
- en: A key feature of knowledge graphs is their ability to provide explicit context.
    Unlike conventional heterogeneous graphs, which display different types of entities
    and their basic connections without detailed semantic meaning, knowledge graphs
    go further by defining the specific types and meanings of relationships. For example,
    while a traditional graph might show that Professors are connected to Departments
    or that Students are linked to Papers, a knowledge graph would specify that Professors
    supervise Students or that Students and Professors Wrote Papers. This added layer
    of meaning enables more powerful querying and analysis, making knowledge graphs
    particularly valuable in fields such as natural language processing, recommendation
    systems, and academic research analysis.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱的一个关键特征是它们提供明确上下文的能力。与传统的异构图不同，传统的异构图显示不同类型的实体及其基本连接，但没有详细的语义意义，知识图谱通过定义特定类型和关系的具体类型和意义更进一步。例如，虽然传统的图可能显示教授与系部相连或学生与论文相链接，但知识图谱会具体说明教授监督学生或学生和教授共同撰写论文。这一层额外的意义使得查询和分析更加强大，使得知识图谱在自然语言处理、推荐系统和学术研究分析等领域特别有价值。
- en: Hypergraphs
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超图
- en: One of the more complex and difficult graphs to work with is the hypergraph.
    *Hypergraphs* are those where a single edge can be connected to multiple different
    nodes. For graphs that aren’t hypergraphs, edges are used to connect exactly two
    nodes (or a node to itself for self-loops). As shown in figure 1.6, edges in a
    hypergraph can connect between any number of nodes. The complexity of a hypergraph
    is reflected in its adjacency data. For typical graphs, network connectivity is
    represented by a two-dimensional adjacency matrix. For hypergraphs, the adjacency
    matrix extends to a higher dimensional tensor, referred to as an *incidence tensor*.
    This tensor is N-dimensional, where N is the maximum number of nodes connected
    by a single edge. An example of a hypergraph might be a communication platform
    that allows for group chats as well as single person conversations. In an ordinary
    graph, edges would only connect two people. In a hypergraph, one hyperedge could
    connect multiple people, representing a group chat.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有需要处理的图中，超图是较为复杂和困难的一种。*超图*是指单条边可以连接多个不同节点的图。对于不是超图的图，边用于连接恰好两个节点（或节点自身形成自环）。如图1.6所示，超图中的边可以连接任意数量的节点。超图的复杂性体现在其邻接数据上。对于典型图，网络连通性由二维邻接矩阵表示。对于超图，邻接矩阵扩展到更高维的张量，称为*关联张量*。这个张量是N维的，其中N是单条边连接的最大节点数。一个超图的例子可能是一个允许进行群聊以及单个人对话的通信平台。在普通图中，边只会连接两个人。在超图中，一个超边可以连接多个人，表示一个群聊。
- en: '![figure](../Images/1-6.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1-6.png)'
- en: Figure 1.6 One undirected hypergraph, illustrated in two ways. On the left,
    we have a graph whose edges are represented by shaded areas, marked by letters,
    and whose vertices are dots, marked by numbers. On the right, we have a graph
    whose edge lines (marked by letters) connect up to 3 nodes (circles marked by
    numbers). Node 8 has no edge. Node 7 has a self-loop.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6 一种无向超图，以两种方式展示。在左侧，我们有一个图，其边由阴影区域表示，用字母标记，其顶点由点表示，用数字标记。在右侧，我们有一个图，其边线（用字母标记）连接最多3个节点（用数字标记的圆圈）。节点8没有边。节点7有一个自环。
- en: 1.2.3 Graph-based learning
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 基于图的学习
- en: As we’ll see in the rest of this chapter, graphs are ubiquitous in our everyday
    life. *Graph-based learning* takes graphs as input data to build models that give
    insight into questions about this data. Later in this chapter, we look at different
    examples of graph data as well as at the sort of questions and tasks we can use
    graph-based learning to answer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本章的其余部分看到的那样，图在我们的日常生活中无处不在。*基于图的学习*将图作为输入数据来构建模型，以洞察关于这些数据的问题。在本章的后面部分，我们将探讨不同的图数据示例，以及我们可以使用基于图的学习来回答的问题和任务。
- en: Graph-based learning uses a variety of machine learning methods to build *representations*
    of graphs. These representations are then used for downstream tasks such as node
    or link prediction or graph classification. In chapter 2, you’ll learn about one
    of the essential tools in graph-based learning, building embeddings. Briefly,
    embeddings are *low-dimensional* vector representations. We can build an embedding
    of different nodes, edges, or entire graphs, and there are a number of different
    ways to do this such as the Node2Vec (N2V) or DeepWalk algorithms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Methods for analysis on graph data have been around for a long time, at least
    as early as the 1950s when *clique methods* used certain features of a graph to
    identify subsets or communities in the graph data [4].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: One of the most famous graph-based algorithms is PageRank, which was developed
    by Larry Page and Sergey Brin in 1996 and formed the basis for Google’s search
    algorithms. Some believe that this algorithm was a key element in the company’s
    meteoric rise in the following years. This highlights that a successful graph-based
    learning algorithm can have a huge effect.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'These methods are only a small subset of graph-based learning and analysis
    techniques. Others include belief propagation [5], graph kernel methods [6], label
    propagation [7], and isomaps [8]. However, in this book, we’ll focus on one of
    the newest and most exciting additions to the family of graph-based learning techniques:
    GNNs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 What is a GNN?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GNNs combine graph-based learning with deep learning. This means that neural
    networks are used to build embeddings and process the relational data. An overview
    of the inner workings of a GNN is shown in figure 1.7\.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: GNNs allows you to represent and learn from graphs, including their constituent
    nodes, edges, and features. In particular, many methods of GNNs are built specifically
    to scale effectively with the size and complexity of a graph. This means that
    GNNs can operate on huge graphs, as we’ll discuss. In this sense, GNNs provide
    analogous advantages to relational data as convolutional neural networks have
    given for image-based data and computer vision.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 An overview of how GNNs work. An input graph is passed to a GNN.
    The GNN then uses neural networks to transform graph features such as nodes or
    edges into nonlinear embeddings through a process known as message passing. These
    embeddings are then tuned to specific unknown properties using training data.
    After the GNN is trained, it can predict unknown features of a graph.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Historically, applying traditional machine learning methods to graph data structures
    has been challenging because graph data, when represented in grid-like formats
    and data structures, can lead to massive repetitions of data. To address this,
    graph-based learning focuses on approaches that are *permutation invariant*. This
    means that the machine learning method is uninfluenced by the ordering of the
    graph representation. In concrete terms, it means that we can shuffle the rows
    and columns of the adjacency matrix without affecting our algorithm’s performance.
    Whenever we’re working with data that contains relational data, that is, has an
    adjacency matrix, then we want to use a machine learning method that is permutation
    invariant to make our method more general and efficient. Although GNNs can be
    applied to all graph data, GNNs are especially useful because they can deal with
    huge graph datasets and typically perform better than other machine learning methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Permutation invariances are a type of *inductive bias*, or an algorithm’s learning
    bias, and are powerful tools for designing machine learning algorithms [1]. The
    need for permutation-invariant approaches is one of the central reasons that graph-based
    learning has increased in popularity in recent years.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Being designed for permutation-invariant data comes with some drawbacks along
    with its advantages. GNNs aren’t as well suited for other data, such as images
    or tables. While this might seem obvious, images and tables are *not* permutation
    invariant and therefore not a good fit for GNNs. If we shuffle the rows and columns
    of an image, then we scramble the input. Instead, machine learning algorithms
    for images seek *translational invariance*, which means that we can translate
    (shift) the object in an image, and it won’t affect the performance of the algorithm.
    Other neural networks, such as convolutional neural networks (CNNs) typically
    perform much better on images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Differences between tabular and graph data
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph data includes all data with some relational content, making it a powerful
    way to represent complex connections. While graph data might initially seem distinct
    from traditional tabular data, many datasets that are typically represented in
    tables can be re-created as graphs with some data engineering and imagination.
    Let’s take a closer look at the Titanic dataset, a classic example in machine
    learning, and explore how it can be transformed from a table format to a graph
    format.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The Titanic dataset describes passengers on the Titanic, a ship that famously
    met an untimely end when it collided with an iceberg. Historically, this dataset
    has been analyzed in tabular format, containing rows for each passenger with columns
    representing features such as age, gender, fare, class, and survival status. However,
    the dataset also contains rich, unexplored relationships that aren’t immediately
    visible in a table format, as shown in figure 1.8.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-8.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 The Titanic Dataset is usually displayed and analyzed using a table
    format.
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recasting the Titanic dataset as a graph
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To transform the Titanic dataset into a graph, we need to consider how to represent
    the underlying relationships between passengers as nodes and edges:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '*Nodes*—In the graph, each passenger can be represented as a node. We can also
    introduce nodes for other entities, such as cabins, families, or even groups such
    as “third-class passengers.”'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edges*—Edges represent the relationships or connections between these nodes.
    For example:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passengers who are family members (siblings, spouses, parents, or children)
    based on the available data
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Passengers who share a cabin or were traveling together
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Social or business relationships that might be inferred from shared ticket numbers,
    last names, or other identifying features
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To construct this graph, we need to use the existing information in the table
    and potentially enrich it with secondary data sources or assumptions (e.g., linking
    last names to create family groups). This process converts the tabular data into
    a graph-based structure, shown in figure 1.9, where each edge and node encapsulates
    meaningful relational data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9 The Titanic dataset, showing the family relationships of the people
    on the Titanic visualized as a graph (Source: Matt Hagy). Here, we can see that
    there was a rich social network as well as many passengers with unknown family
    ties.'
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: How graph data adds depth and meaning
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the dataset is represented as a graph, it provides a much deeper view
    of the social and familial connections between the passengers. For example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '*Family relationships*—The graph clearly shows how certain passengers were
    related (e.g., as parents, children, or siblings). This could help us understand
    survival patterns, as family members might have behaved differently in a crisis
    than individuals traveling alone.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Social networks*—Beyond families, the graph could reveal broader social networks
    (e.g., friendships or business connections), which could be important factors
    in analyzing behavior and outcomes.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Community insights*—The graph structure also allows for community detection
    algorithms to identify clusters of related or connected passengers, which may
    reveal new insights into survival rates, rescue patterns, or other behaviors.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph representations add depth by specifying connections that might not be
    obvious in a tabular format. For example, understanding who traveled together,
    who shared a cabin, or who had social or family ties can provide more context
    on survival rates and passenger behavior. This is crucial for tasks such as node
    prediction, where we want to predict attributes or outcomes based on the relationships
    represented in the graph.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: By creating an adjacency matrix or defining graph edges and nodes based on the
    relationships in the dataset, we can transition from simple data analysis to more
    sophisticated graph-based learning methods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '1.3 GNN applications: Case studies'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve seen, GNNs are neural networks designed to work on relational data.
    They give new ways for relational data to be transformed and manipulated, by being
    easier to scale and more accurate than previous graph-based learning methods.
    In the following, we discuss some exciting applications of GNNs, to see, at a
    high level, how this class of models are solving real-world problems. Links to
    source papers are listed at the end of the book if you want to learn more about
    these particular projects.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Recommendation engines
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enterprise graphs can exceed billions of nodes and many billions of edges. On
    the other hand, many GNNs are benchmarked on datasets that consist of fewer than
    a million nodes. When applying GNNs to large graphs, adjustments of the training
    and inference algorithms and storage techniques all have to be made. (You can
    learn more about the specifics of scaling GNNs in chapter 7.)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most well-known industry examples of GNNs is their use as recommendation
    engines. For instance, Pinterest is a social media platform for finding and sharing
    images and ideas. There are two major concepts to Pinterest’s users: collections
    or categories of ideas, called *boards* (like a bulletin board); and objects a
    user wants to bookmark called *pins*. Pins include images, videos, and website
    URLs. A user board focused on dogs might then include pins of pet photos, puppy
    videos, or dog-related website links. A board’s pins aren’t exclusive to it; a
    pet drawing that was pinned to the Dogs board could also be pinned to a Puppies
    board, as shown in figure 1.10.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 A bipartite graph that is like the Pinterest graph. Nodes in this
    case are the pins and boards.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As of this writing, Pinterest has 400 million active users who have likely pinned
    tens if not hundreds of items per user. One imperative of Pinterest is to help
    their users find content of interest via recommendations. Such recommendations
    should not only take into account image data and user tags but also draw insights
    from the relationships between pins and boards.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: One way to interpret the relationships between pins and boards is as a *bipartite
    graph*, which we discussed earlier. For the Pinterest graph, all the pins are
    connected to boards, but no pin is connected to another pin, and no board is connected
    to another board. Pins and boards are two classes of nodes. Members of these classes
    can be linked to members of the other class, but not to members of the same class.
    The Pinterest graph was reported to have 3 billion nodes and 18 billion edges.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: PinSage, a graph convolutional network (GCN), was one of the first documented
    highly scaled GNNs used in an enterprise system [9]. This was used in Pinterest’s
    recommendation systems to overcome past challenges of applying graph-learning
    models to massive graphs. Compared to baseline methods, tests on this system showed
    it improved user engagement by 30%. Specifically, PinSage was used to predict
    which objects should be recommended to be included in a user’s graph. However,
    GNNs can also be used to predict what an object is, such as whether it contains
    a dog or mountain, based on the rest of the nodes in the graph and how they are
    connected. We’ll be doing a deep dive on GCNs, of which PinSage is an extension,
    in chapter 3.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Drug discovery and molecular science
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chemistry and molecular sciences, a prominent problem has been representing
    molecules in a general, application-agnostic way, and inferring possible interfaces
    between molecules, such as proteins. For molecule representation, we can see that
    the drawings of molecules that are common in high school chemistry classes bear
    resemblance to a graph structure, consisting of nodes (atoms) and edges (atomic
    bonds), as shown in figure 1.11.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-11.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 In this molecule, we can see individual atoms as nodes and the atomic
    bonds as edges.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Applying GNNs to these structures can, in certain circumstances, outperform
    traditional “fingerprint” methods for determining the properties of a molecule.
    These traditional methods involve the creation of features by domain experts to
    capture a molecule’s properties, such as interpreting the presence or absence
    of certain molecules or atoms [10]. GNNs learn new data-driven features that can
    be used to group certain molecules together in new and unexpected ways or even
    to propose new molecules for synthesis. This is extremely important for predicting
    whether a chemical is toxic or safe for use or whether it has some downstream
    effects that can affect disease progression. Therefore, GNNs have shown themselves
    to be incredibly useful in the field of drug discovery.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Drug discovery, especially for GNNs, can be understood as a graph prediction
    problem. *Graph prediction* tasks are those that require learning and predicting
    properties about the entire graph. For drug discovery, the aim is to predict properties
    such as toxicity or treatment effectiveness (discriminative) or to suggest entirely
    new graphs that should be synthesized and tested (generative). To suggest these
    new graphs, drug discovery methods often combine GNNs with other generative models
    such as variational graph autoencoders (VGAEs), as shown, for example, in figure
    1.12\. We’ll describe VGAEs in more detail in chapter 5 and show how we can use
    these to predict molecules.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-12.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 A GNN system used to predict new molecules [11]. The workflow here
    starts on the left with a representation of a molecule as a graph. In the middle
    parts of the figure, this graph representation is transformed via a GNN into a
    latent representation. The latent representation is then transformed back to the
    molecule to ensure that the latent space can be decoded (right).
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.3.3 Mechanical reasoning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We develop rudimentary intuition about mechanics and physics of the world around
    us at a remarkably young age and without any formal training in the subject. We
    don’t need to write down a set of equations to know how to catch a bouncing ball.
    We don’t even have to be in the presence of a physical ball. Given a series of
    snapshots of a bouncing ball, we can predict reasonably well where the ball is
    going to end up.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: While these problems might seem trivial for us, they are critical for many physical
    industries, including manufacturing and autonomous driving. For example, autonomous
    driving systems need to anticipate what will happen in a traffic scene consisting
    of many moving objects. Until recently, this task was typically treated as a problem
    of computer vision. However, more recent approaches have begun to use GNNs [12].
    These GNN-based methods demonstrate that including relational information, such
    as how limbs are connected, can enable algorithms to develop physical intuition
    about how a person or animal moves with higher accuracy and less data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In figure 1.13, we give an example of how a body can be thought of as a “mechanical”
    graph. The input graphs for these physical reasoning systems have elements that
    reflect the problem. For instance, when reasoning about a human or animal body,
    a graph could consist of nodes that represent points on the body where limbs connect.
    For systems of free bodies, the nodes of a graph could be individual objects such
    as bouncing balls. The edges of the graph then represent the physical relationship
    (e.g., gravitational forces, elastic springs, or rigid connections) between the
    nodes. Given these inputs, GNNs learn to predict future states of a set of objects
    without explicitly calling on physical/mechanical laws [13]. These methods are
    a form of *edge prediction*; that is, they predict how the nodes connect over
    time. Furthermore, these models have to be dynamic to account for the temporal
    evolution of the system. We consider these problems in detail in chapter 6.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-13.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 A graph representation of a mechanical body, taken from Sanchez-Gonzalez
    [13]. The body’s segments are represented as nodes, and the mechanical forces
    binding them are edges.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.4 When to use a GNN?
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve explored real-world applications of GNNs, let’s identify some
    underlying characteristics that make problems suitable for graph-based solutions.
    While the cases of the previous section clearly involved data that was naturally
    modeled as a graph, it’s crucial to recognize that GNNs can also be effectively
    applied to problems where the graph-like nature may not be immediately obvious.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'So, instead of simply stating that GNNs are useful for graph problems, this
    section will help you recognize patterns and relationships within your data that
    could benefit from graph-based modeling, even if those relationships aren’t immediately
    apparent. Essentially, there are three types of criteria for identifying GNN problems:
    implicit relationships and interdependencies; high dimensionality and sparsity;
    and complex nonlocal interactions.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1 Implicit relationships and interdependencies
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graphs are versatile data structures that can model a wide range of relationships.
    Even when a problem doesn’t initially appear to be graph-like, even if your dataset
    is tabular, it’s beneficial to explore whether implicit relationships or interdependencies
    might exist that could be represented explicitly. Implicit relationships are connections
    that aren’t immediately documented or obvious within the data but can still play
    a significant role in understanding the underlying patterns and behaviors.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Key indicators
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To determine if your problem might benefit from modeling implicit relationships
    with graphs, consider whether there are hidden or indirect connections between
    entities in your dataset. For example, in customer behavior analysis, customers
    may appear as independent entities in a tabular dataset containing their purchases,
    demographics, and other details. However, they could be connected through social
    media influence, peer recommendations, or shared purchasing patterns, forming
    an underlying network of interactions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Another indicator is the presence of entities that share common attributes or
    activities without a direct or documented relationship. In the case of investors,
    for example, two or more investors may not have any formal connection but might
    frequently co-invest in the same companies under similar conditions. Such patterns
    of co-investment could indicate a shared strategy or influence. In this scenario,
    a graph representation can be created where nodes represent individual investors,
    and edges are formed between nodes when two or more investors co-invest in the
    same company. Additional attributes, such as investment size, timing, or the types
    of companies invested in can be added to nodes or edges, allowing GNNs to identify
    patterns, trends, or even potential collaboration opportunities.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, consider whether the data involves entities that are interconnected
    through shared references or co-occurrence patterns. Document and text data may
    not immediately suggest a graph structure, but if documents cite each other or
    share common topics or authors, they can be represented as nodes in a graph, with
    edges reflecting these relationships. Similarly, terms within documents can form
    co-occurrence networks, which are useful for tasks such as keyword extraction,
    document classification, or topic modeling.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: By identifying these key indicators in your data, you can uncover hidden or
    implicit relationships that can be represented explicitly through graphs. Such
    representations allow for more advanced analyses using GNNs, which can effectively
    capture and model these relationships, leading to more accurate predictions and
    deeper insights into the data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2 High dimensionality and sparsity
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph-based models are particularly effective in handling high-dimensional data
    where many features may be sparse or missing. These models excel in situations
    where there are underlying structures connecting sparse entities, allowing for
    more meaningful analysis and improved performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Key indicators
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To determine if your problem involves high-dimensional and sparse data suitable
    for GNNs, consider whether your dataset contains numerous entities with limited
    direct interactions or relationships. For example, in recommender systems, user-item
    interaction data may appear tabular, but it’s inherently sparse—most users only
    interact with a small subset of the available items. By representing users and
    items as nodes and representing their interactions (e.g., purchases or clicks)
    as edges, GNNs can exploit network effects to make more accurate recommendations.
    These models can also address the cold-start problem by uncovering both explicit
    and implicit relationships, leading to better performance in recommending new
    items to users or engaging new users with existing items.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Another indicator that your problem may be suitable for graph-based models is
    when the data represents entities that are sparsely connected but share significant
    characteristics. In drug discovery, for example, molecules are represented as
    graphs, with atoms as nodes and chemical bonds as edges. This representation captures
    the inherent sparsity of molecular structures, where most atoms form only a few
    bonds, and large portions of the molecule may be distant from each other in the
    graph. Traditional machine learning methods often struggle to predict properties
    of new molecules due to this sparsity, as they don’t account for the full structural
    context.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based models, particularly GNNs, overcome these challenges by capturing
    both local atomic environments and global molecular structures. GNNs learn hierarchical
    features from fine-grained atomic interactions to broader molecular properties,
    and their ability to remain invariant to the ordering of atoms ensures consistent
    predictions. By using the graph structure of molecules, GNNs make accurate predictions
    from sparse, connected data, thereby accelerating the drug discovery process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: By recognizing these key indicators in your data, you can identify situations
    where graph-based models can effectively handle high-dimensional and sparse datasets.
    Representing such data as graphs allows GNNs to capture and use underlying structures,
    resulting in more accurate predictions and deeper insights across various applications.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.3 Complex, nonlocal interactions
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Certain problems require understanding how distant elements in a dataset influence
    each other. In these cases, GNNs provide a framework to capture these complex
    interactions, where the predicted value or label of a particular data point depends
    not just on the features of its immediate neighbors but also on those of other
    related data points. This capability is especially useful when relationships extend
    beyond direct connections to involve multiple levels or degrees of separation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: However, some standard GNNs, which rely primarily on local message passing,
    may struggle to capture long-range dependencies effectively. Advanced architectures
    or modifications, such as those incorporating global attention, nonlocal aggregation,
    or hierarchical message-passing, can better address these challenges [14].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Key indicators
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To determine if your problem involves complex, nonlocal interactions suitable
    for GNNs, consider whether the outcome or behavior of one entity depends on the
    attributes or actions of entities that aren’t directly connected to it but may
    be indirectly connected through other entities. For example, in supply chain optimization,
    a delay in one supplier may not only affect its immediate downstream customers
    but could cascade through multiple levels of the network, influencing distributors
    and final consumers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Another indicator is whether the problem involves scenarios where information,
    influence, or effects propagate through a network over time. In healthcare and
    epidemiology, for instance, a disease outbreak might spread from a small cluster
    of patients through their interactions with shared healthcare providers, common
    environments, or overlapping social networks. Such propagation requires an approach
    that captures the indirect transmission pathways of information or effects.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'To close this section, in determining whether your problem is a good candidate
    for a GNN, ask yourself these questions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Are there implicit relationships or interdependencies in my data that I could
    model?
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do the interactions between entities exhibit complex, nonlocal dependencies
    that go beyond immediate connections?
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data high-dimensional and sparse, with a need to capture underlying relational
    structures?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the answer to any of these questions is yes, consider framing your problem
    as a graph and applying GNNs to unlock new insights and predictive capabilities.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Understanding how GNNs operate
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll explore how GNNs work, starting from the initial collection
    of raw data to the final deployment of trained models. We’ll examine each step,
    highlighting the processes of data handling, model building, and the unique message-passing
    technique that sets GNNs apart from traditional deep learning models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.1 Mental model for training a GNN
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our mental model covers the data sourcing, graph representation, preprocessing,
    and model development workflow. We start with raw data and end up with a trained
    GNN model and its outputs. Figure 1.14 illustrates and visualizes topics related
    to these stages, annotated with the chapters in which these topics appear.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-14.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 Mental model of the GNN project. We start with raw data, which is
    transformed into a graph data model that can be stored in a graph database or
    used in a graph processing system. From the graph processing system (and some
    graph databases), exploratory data analysis and visualization can be done. Finally,
    for graph machine learning, data is preprocessed into a form that can be submitted
    for training.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While not all workflows include every step or stage of this process, most will
    incorporate at least some elements. At different stages of a model development
    project, different parts of this process will typically be used. For example,
    when *training* a model, data analysis and visualization may be needed to make
    design decisions, but when *deploying* a model, it may only be necessary to stream
    raw data and quickly preprocess it for ingestion into a model. Though this book
    touches on the earlier stages in this mental model, the bulk of the book is focused
    on how to train different types of GNNs. When the other topics are discussed,
    they serve to support this main focus.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The mental model shows the core tasks of applying GNNs to machine learning problems,
    and we’ll be returning to this process repeatedly throughout the rest of the book.
    Let’s examine this diagram from end to end.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The first step in training a GNN is structuring this raw data into a graph format,
    if it isn’t already. This requires deciding which entities in the data to represent
    as nodes and edges, as well as determining the features to assign to them. Decisions
    must also be made about data storage—whether to use a graph database, processing
    system, or other formats.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: For machine learning, the data must be preprocessed for training and inference,
    involving tasks such as sampling, batching, and splitting the data into training,
    validation, and test sets. Throughout this book, we use PyTorch Geometric (PyG),
    which offers specialized classes for preprocessing and data splitting while preserving
    the graph’s structure. Preprocessing is covered in most chapters, with more in-depth
    explanations available in appendix B.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'After processing the data, we can then move on to the model training. In this
    book, we cover several architectures and training types:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Chapters 2 and 3 discuss convolutional GNNs, where we first use a GCN layer
    to produce graph embeddings (chapter 2) and then train a full GCN and GraphSAGE
    models (chapter 3).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 4 explains graph attention networks (GATs), which adds attention to
    our GNNs.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 5 introduces GNNs for unsupervised and generative problems, where we
    train and use a variational graph autoencoder (VGAE).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 6 then explores the advanced concept of spatiotemporal GNNs, based on
    graphs that evolve over time. We train a neural relational inference (NRI) model,
    which combines an autoencoder structure with a recurrent neural network.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the examples provided for the GNNs mentioned so far are illustrated
    with code examples which use small-scale graphs that can fit into memory on a
    laptop or desktop computer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 7, we delve into strategies for handling data that exceeds the processing
    capacity of a single machine.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 8, we close with some considerations for graph and GNN projects,
    such as practical aspects of working with graph data, as well as how to convert
    nongraph data into a graph format.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.5.2 Unique mechanisms of a GNN model
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although there are a variety of GNN architectures at this point, they all tackle
    the same problem of dealing with graph data in a way that is permutation invariant.
    They do this via encoding and exchanging information across the graph structure
    during the learning process.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: In a conventional neural network, we first need to initialize a set of parameters
    and functions. These include the number of layers, the size of the layers, the
    learning rate, the loss function, the batch size, and other hyperparameters. (These
    are all treated in detail in other books on deep learning, so we assume you’re
    familiar with these terms.) Once we’ve defined these features, we then train our
    network by iteratively updating the weights of the network, as shown in figure
    1.15.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-15.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 Process for training a GNN, which is similar to training most other
    deep learning models
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Explicitly, we perform the following steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Input our data.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the data through neural network layers that transform the data according
    to the parameters of the layer and an activation rule.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output a representation from the final layer of the network.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the error, and adjust the parameters accordingly.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat these steps a fixed number of *epochs* (the process by which data is
    passed forward and backward to train a neural network).
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For tabular data, these steps are exactly as listed, as shown in figure 1.16\.
    For graph-based or relational data, these steps are similar except that each epoch
    relates to one iteration of message passing, which is described in the next subsection.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-16.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 Comparison of (simple) non-GNN (above) and GNN (below). GNNs have
    a layer that distributes data among its vertices.
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.5.3 Message passing
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Message passing*, which is touched on throughout the book, is a central mechanism
    in GNNs that enables nodes to communicate and share information across a graph
    [15]. This process allows GNNs to learn rich, informative representations of graph-structured
    data, which is essential for tasks such as node classification, link prediction,
    and graph-level prediction. Figure 1.17 illustrates the steps involved in a typical
    message-passing layer.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-17.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 Elements of our message passing layer. Each message passing layer
    consists of an aggregation, a transformation, and an update step.
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The message-passing process begins with the Input (step 1) of the initial graph,
    where every node and edge have their own features. In the Collect step (step 2),
    each node gathers information from its immediate neighbors—these pieces of information
    are referred to as “messages.” This step ensures that each node has access to
    the features of its neighbors, which are crucial for understanding the local graph
    structure.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the Aggregate step (step 3), the collected messages from neighboring
    nodes are combined using an invariant function, such as sum, mean, or max. This
    aggregation consolidates the information from a node’s neighborhood into a single
    vector, capturing the most relevant details about its local environment.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: In the Transform step (step 4), the aggregated messages are processed by a neural
    network to produce a new representation for each node. This transformation allows
    the GNN to learn complex interactions and patterns within the graph by applying
    nonlinear functions to the aggregated information.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Finally, during the Update step (step 5), the features of each node in the graph
    are replaced or updated with these new representations. This completes one round
    of message passing, incorporating information from neighboring nodes to refine
    each node’s features.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Each message-passing layer in a GNN allows nodes to gather information from
    nodes that are further away, or more “hops” away, in the graph. Repeating these
    steps over multiple layers enables the GNN to capture more complex dependencies
    and long-range interactions within the graph.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: By using message passing, GNNs efficiently encode the graph structure and data
    into useful representations for a variety of downstream tasks. Advanced architectures,
    such as those incorporating global attention or hierarchical message passing,
    further enhance the model’s ability to capture long-range dependencies across
    the graph, enabling more robust performance on diverse applications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph neural networks (GNNs) are specialized tools for handling relational,
    or relationship-centric, data, particularly in scenarios where traditional neural
    networks struggle due to the complexity and diversity of graph structures.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GNNs have found significant applications in areas such as recommendation engines,
    drug discovery, and mechanical reasoning, showcasing their versatility in handling
    large and complex relational data for enhanced insights and predictions.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific GNN tasks include node prediction, edge prediction, graph prediction,
    and graph representation through embedding techniques.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GNNs are best used when data is represented as a graph, indicating a strong
    emphasis on relationships and connections between data points. They aren’t ideal
    for individual, standalone data entries where relational information is insignificant.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When deciding if a GNN solution is a good fit for your problem, consider cases
    that have characteristics such as implicit relationships, high-dimensionality,
    sparsity, and complex nonlocal interactions. By understanding these fundamentals,
    practitioners can evaluate the suitability of GNNs for their specific problems,
    implement them effectively, and recognize their tradeoffs and limitations in real-world
    applications.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message passing is a core mechanism of GNNs, which enables them to encode and
    exchange information across a graph’s structure, allowing for meaningful node,
    edge, and graph-level predictions. Each layer of a GNN represents one step of
    message passing, with various aggregation functions to combine messages effectively,
    providing insights and representations useful for machine learning tasks.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
