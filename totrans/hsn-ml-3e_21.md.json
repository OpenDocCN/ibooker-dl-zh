["```py\nfrom pathlib import Path\nimport tensorflow as tf\n\nX_train, X_valid, X_test = [...]  # load and split the MNIST dataset\nmodel = [...]  # build & train an MNIST model (also handles image preprocessing)\n\nmodel_name = \"my_mnist_model\"\nmodel_version = \"0001\"\nmodel_path = Path(model_name) / model_version\nmodel.save(model_path, save_format=\"tf\")\n```", "```py\n$ saved_model_cli show --dir my_mnist_model/0001\nThe given SavedModel contains the following tag-sets:\n'serve'\n\n```", "```py\n$ saved_model_cli show --dir 0001/my_mnist_model --tag_set serve\nThe given SavedModel MetaGraphDef contains SignatureDefs with these keys:\nSignatureDef key: \"__saved_model_init_op\"\nSignatureDef key: \"serving_default\"\n\n```", "```py\n$ saved_model_cli show --dir 0001/my_mnist_model --tag_set serve \\\n                       --signature_def serving_default\nThe given SavedModel SignatureDef contains the following input(s):\n inputs['flatten_input'] tensor_info:\n dtype: DT_UINT8\n shape: (-1, 28, 28)\n name: serving_default_flatten_input:0\nThe given SavedModel SignatureDef contains the following output(s):\n outputs['dense_1'] tensor_info:\n dtype: DT_FLOAT\n shape: (-1, 10)\n name: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict\n\n```", "```py\nurl = \"https://storage.googleapis.com/tensorflow-serving-apt\"\nsrc = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n!echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n!curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n!apt update -q && apt-get install -y tensorflow-model-server\n%pip install -q -U tensorflow-serving-api\n```", "```py\nimport os\n\nos.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())\n```", "```py\n%%bash --bg\ntensorflow_model_server \\\n     --port=8500 \\\n     --rest_api_port=8501 \\\n     --model_name=my_mnist_model \\\n     --model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1\n```", "```py\nimport json\n\nX_new = X_test[:3]  # pretend we have 3 new digit images to classify\nrequest_json = json.dumps({\n    \"signature_name\": \"serving_default\",\n    \"instances\": X_new.tolist(),\n})\n```", "```py\n>>> request_json\n'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, ... ]]]}'\n```", "```py\nimport requests\n\nserver_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\nresponse = requests.post(server_url, data=request_json)\nresponse.raise_for_status()  # raise an exception in case of error\nresponse = response.json()\n```", "```py\n>>> import numpy as np\n>>> y_proba = np.array(response[\"predictions\"])\n>>> y_proba.round(2)\narray([[0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 1\\.  , 0\\.  , 0\\.  ],\n [0\\.  , 0\\.  , 0.99, 0.01, 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  ],\n [0\\.  , 0.97, 0.01, 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0.01, 0\\.  , 0\\.  ]])\n```", "```py\nfrom tensorflow_serving.apis.predict_pb2 import PredictRequest\n\nrequest = PredictRequest()\nrequest.model_spec.name = model_name\nrequest.model_spec.signature_name = \"serving_default\"\ninput_name = model.input_names[0]  # == \"flatten_input\"\nrequest.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n```", "```py\nimport grpc\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\nchannel = grpc.insecure_channel('localhost:8500')\npredict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\nresponse = predict_service.Predict(request, timeout=10.0)\n```", "```py\noutput_name = model.output_names[0]  # == \"dense_1\"\noutputs_proto = response.outputs[output_name]\ny_proba = tf.make_ndarray(outputs_proto)\n```", "```py\nmodel = [...]  # build and train a new MNIST model version\n\nmodel_version = \"0002\"\nmodel_path = Path(model_name) / model_version\nmodel.save(model_path, save_format=\"tf\")\n```", "```py\n[...]\nReading SavedModel from: /models/my_mnist_model/0002\nReading meta graph with tags { serve }\n[...]\nSuccessfully loaded servable version {name: my_mnist_model version: 2}\nQuiescing servable version {name: my_mnist_model version: 1}\nDone quiescing servable version {name: my_mnist_model version: 1}\nUnloading servable version {name: my_mnist_model version: 1}\n\n```", "```py\nfrom google.colab import auth\n\nauth.authenticate_user()\n```", "```py\nfrom google.cloud import storage\n\nproject_id = \"my_project\"  # change this to your project ID\nbucket_name = \"my_bucket\"  # change this to a unique bucket name\nlocation = \"us-central1\"\n\nstorage_client = storage.Client(project=project_id)\nbucket = storage_client.create_bucket(bucket_name, location=location)\n```", "```py\ndef upload_directory(bucket, dirpath):\n    dirpath = Path(dirpath)\n    for filepath in dirpath.glob(\"**/*\"):\n        if filepath.is_file():\n            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n            blob.upload_from_filename(filepath)\n\nupload_directory(bucket, \"my_mnist_model\")\n```", "```py\n!gsutil -m cp -r my_mnist_model gs://{bucket_name}/\n```", "```py\nfrom google.cloud import aiplatform\n\nserver_image = \"gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\"\n\naiplatform.init(project=project_id, location=location)\nmnist_model = aiplatform.Model.upload(\n    display_name=\"mnist\",\n    artifact_uri=f\"gs://{bucket_name}/my_mnist_model/0001\",\n    serving_container_image_uri=server_image,\n)\n```", "```py\nendpoint = aiplatform.Endpoint.create(display_name=\"mnist-endpoint\")\n\nendpoint.deploy(\n    mnist_model,\n    min_replica_count=1,\n    max_replica_count=5,\n    machine_type=\"n1-standard-4\",\n    accelerator_type=\"NVIDIA_TESLA_K80\",\n    accelerator_count=1\n)\n```", "```py\nresponse = endpoint.predict(instances=X_new.tolist())\n```", "```py\n>>> import numpy as np\n>>> np.round(response.predictions, 2)\narray([[0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 1\\.  , 0\\.  , 0\\.  ],\n [0\\.  , 0\\.  , 0.99, 0.01, 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0\\.  ],\n [0\\.  , 0.97, 0.01, 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0.01, 0\\.  , 0\\.  ]])\n```", "```py\nendpoint.undeploy_all()  # undeploy all models from the endpoint\nendpoint.delete()\n```", "```py\nbatch_path = Path(\"my_mnist_batch\")\nbatch_path.mkdir(exist_ok=True)\nwith open(batch_path / \"my_mnist_batch.jsonl\", \"w\") as jsonl_file:\n    for image in X_test[:100].tolist():\n        jsonl_file.write(json.dumps(image))\n        jsonl_file.write(\"\\n\")\n\nupload_directory(bucket, batch_path)\n```", "```py\nbatch_prediction_job = mnist_model.batch_predict(\n    job_display_name=\"my_batch_prediction_job\",\n    machine_type=\"n1-standard-4\",\n    starting_replica_count=1,\n    max_replica_count=5,\n    accelerator_type=\"NVIDIA_TESLA_K80\",\n    accelerator_count=1,\n    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n    sync=True  # set to False if you don't want to wait for completion\n)\n```", "```py\ny_probas = []\nfor blob in batch_prediction_job.iter_outputs():\n    if \"prediction.results\" in blob.name:\n        for line in blob.download_as_text().splitlines():\n            y_proba = json.loads(line)[\"prediction\"]\n            y_probas.append(y_proba)\n```", "```py\n>>> y_pred = np.argmax(y_probas, axis=1)\n>>> accuracy = np.sum(y_pred == y_test[:100]) / 100\n0.98\n```", "```py\nfor prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\n    blobs = bucket.list_blobs(prefix=prefix)\n    for blob in blobs:\n        blob.delete()\n\nbucket.delete()  # if the bucket is empty\nbatch_prediction_job.delete()\n```", "```py\nconverter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\ntflite_model = converter.convert()\nwith open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n```", "```py\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n```", "```py\nimport \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\";\nimport \"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\";\n\nconst image = document.getElementById(\"image\");\n\nmobilenet.load().then(model => {\n    model.classify(image).then(predictions => {\n        for (var i = 0; i < predictions.length; i++) {\n            let className = predictions[i].className\n            let proba = (predictions[i].probability * 100).toFixed(1)\n            console.log(className + \" : \" + proba + \"%\");\n        }\n    });\n});\n```", "```py\n$ nvidia-smi\nSun Apr 10 04:52:10 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n```", "```py\n>>> physical_gpus = tf.config.list_physical_devices(\"GPU\")\n>>> physical_gpus\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n```", "```py\n$ CUDA_DEVICE_ORDER=PCI_BUS_IDCUDA_VISIBLE_DEVICES=0,1python3program_1.py*`#` `and``in``another``terminal:`*$ CUDA_DEVICE_ORDER=PCI_BUS_IDCUDA_VISIBLE_DEVICES=3,2python3program_2.py\n```", "```py\nfor gpu in physical_gpus:\n    tf.config.set_logical_device_configuration(\n        gpu,\n        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n    )\n```", "```py\nfor gpu in physical_gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n```", "```py\ntf.config.set_logical_device_configuration(\n    physical_gpus[0],\n    [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n     tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n)\n```", "```py\n>>> logical_gpus = tf.config.list_logical_devices(\"GPU\")\n>>> logical_gpus\n[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n LogicalDevice(name='/device:GPU:1', device_type='GPU')]\n```", "```py\n>>> a = tf.Variable([1., 2., 3.])  # float32 variable goes to the GPU\n>>> a.device\n'/job:localhost/replica:0/task:0/device:GPU:0'\n>>> b = tf.Variable([1, 2, 3])  # int32 variable goes to the CPU\n>>> b.device\n'/job:localhost/replica:0/task:0/device:CPU:0'\n```", "```py\n>>> with tf.device(\"/cpu:0\"):\n...     c = tf.Variable([1., 2., 3.])\n...\n>>> c.device\n'/job:localhost/replica:0/task:0/device:CPU:0'\n```", "```py\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    model = tf.keras.Sequential([...])  # create a Keras model normally\n    model.compile([...])  # compile the model normally\n\nbatch_size = 100  # preferably divisible by the number of replicas\nmodel.fit(X_train, y_train, epochs=10,\n          validation_data=(X_valid, y_valid), batch_size=batch_size)\n```", "```py\n>>> type(model.weights[0])\ntensorflow.python.distribute.values.MirroredVariable\n```", "```py\nwith strategy.scope():\n    model = tf.keras.models.load_model(\"my_mirrored_model\")\n```", "```py\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n```", "```py\nstrategy = tf.distribute.experimental.CentralStorageStrategy()\n```", "```py\ncluster_spec = {\n    \"worker\": [\n        \"machine-a.example.com:2222\",     # /job:worker/task:0\n        \"machine-b.example.com:2222\"      # /job:worker/task:1\n    ],\n    \"ps\": [\"machine-a.example.com:2221\"]  # /job:ps/task:0\n}\n```", "```py\nos.environ[\"TF_CONFIG\"] = json.dumps({\n    \"cluster\": cluster_spec,\n    \"task\": {\"type\": \"worker\", \"index\": 0}\n})\n```", "```py\nimport tempfile\nimport tensorflow as tf\n\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()  # at the start!\nresolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\nprint(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n[...] # load and split the MNIST dataset\n\nwith strategy.scope():\n    model = tf.keras.Sequential([...])  # build the Keras model\n    model.compile([...])  # compile the model\n\nmodel.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n\nif resolver.task_id == 0:  # the chief saves the model to the right location\n    model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\nelse:\n    tmpdir = tempfile.mkdtemp()  # other workers save to a temporary directory\n    model.save(tmpdir, save_format=\"tf\")\n    tf.io.gfile.rmtree(tmpdir)  # and we can delete this directory at the end!\n```", "```py\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(\n    communication_options=tf.distribute.experimental.CommunicationOptions(\n        implementation=tf.distribute.experimental.CollectiveCommunication.NCCL))\n```", "```py\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\n```", "```py\nimport os\n[...]  # other imports, create MultiWorkerMirroredStrategy, and resolver\n\nif resolver.task_type == \"chief\":\n    model_dir = os.getenv(\"AIP_MODEL_DIR\")  # paths provided by Vertex AI\n    tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n    checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\nelse:\n    tmp_dir = Path(tempfile.mkdtemp())  # other workers use temporary dirs\n    model_dir = tmp_dir / \"model\"\n    tensorboard_log_dir = tmp_dir / \"logs\"\n    checkpoint_dir = tmp_dir / \"ckpt\"\n\ncallbacks = [tf.keras.callbacks.TensorBoard(tensorboard_log_dir),\n             tf.keras.callbacks.ModelCheckpoint(checkpoint_dir)]\n[...]  # build and  compile using the strategy scope, just like earlier\nmodel.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10,\n          callbacks=callbacks)\nmodel.save(model_dir, save_format=\"tf\")\n```", "```py\ncustom_training_job = aiplatform.CustomTrainingJob(\n    display_name=\"my_custom_training_job\",\n    script_path=\"my_vertex_ai_training_task.py\",\n    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n    model_serving_container_image_uri=server_image,\n    requirements=[\"gcsfs==2022.3.0\"],  # not needed, this is just an example\n    staging_bucket=f\"gs://{bucket_name}/staging\"\n)\n```", "```py\nmnist_model2 = custom_training_job.run(\n    machine_type=\"n1-standard-4\",\n    replica_count=2,\n    accelerator_type=\"NVIDIA_TESLA_K80\",\n    accelerator_count=2,\n)\n```", "```py\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--n_hidden\", type=int, default=2)\nparser.add_argument(\"--n_neurons\", type=int, default=256)\nparser.add_argument(\"--learning_rate\", type=float, default=1e-2)\nparser.add_argument(\"--optimizer\", default=\"adam\")\nargs = parser.parse_args()\n```", "```py\nimport tensorflow as tf\n\ndef build_model(args):\n    with tf.distribute.MirroredStrategy().scope():\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n        for _ in range(args.n_hidden):\n            model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\n        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n        opt = tf.keras.optimizers.get(args.optimizer)\n        opt.learning_rate = args.learning_rate\n        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n                      metrics=[\"accuracy\"])\n        return model\n\n[...]  # load the dataset\nmodel = build_model(args)\nhistory = model.fit([...])\n```", "```py\nimport hypertune\n\nhypertune = hypertune.HyperTune()\nhypertune.report_hyperparameter_tuning_metric(\n    hyperparameter_metric_tag=\"accuracy\",  # name of the reported metric\n    metric_value=max(history.history[\"val_accuracy\"]),  # metric value\n    global_step=model.optimizer.iterations.numpy(),\n)\n```", "```py\ntrial_job = aiplatform.CustomJob.from_local_script(\n    display_name=\"my_search_trial_job\",\n    script_path=\"my_vertex_ai_trial.py\",  # path to your training script\n    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n    staging_bucket=f\"gs://{bucket_name}/staging\",\n    accelerator_type=\"NVIDIA_TESLA_K80\",\n    accelerator_count=2,  # in this example, each trial will have 2 GPUs\n)\n```", "```py\nfrom google.cloud.aiplatform import hyperparameter_tuning as hpt\n\nhp_job = aiplatform.HyperparameterTuningJob(\n    display_name=\"my_hp_search_job\",\n    custom_job=trial_job,\n    metric_spec={\"accuracy\": \"maximize\"},\n    parameter_spec={\n        \"learning_rate\": hpt.DoubleParameterSpec(min=1e-3, max=10, scale=\"log\"),\n        \"n_neurons\": hpt.IntegerParameterSpec(min=1, max=300, scale=\"linear\"),\n        \"n_hidden\": hpt.IntegerParameterSpec(min=1, max=10, scale=\"linear\"),\n        \"optimizer\": hpt.CategoricalParameterSpec([\"sgd\", \"adam\"]),\n    },\n    max_trial_count=100,\n    parallel_trial_count=20,\n)\nhp_job.run()\n```", "```py\ndef get_final_metric(trial, metric_id):\n    for metric in trial.final_measurement.metrics:\n        if metric.metric_id == metric_id:\n            return metric.value\n\ntrials = hp_job.trials\ntrial_accuracies = [get_final_metric(trial, \"accuracy\") for trial in trials]\nbest_trial = trials[np.argmax(trial_accuracies)]\n```", "```py\n>>> max(trial_accuracies)\n0.977400004863739\n>>> best_trial.id\n'98'\n>>> best_trial.parameters\n[parameter_id: \"learning_rate\" value { number_value: 0.001 },\n parameter_id: \"n_hidden\" value { number_value: 8.0 },\n parameter_id: \"n_neurons\" value { number_value: 216.0 },\n parameter_id: \"optimizer\" value { string_value: \"adam\" }\n]\n```"]