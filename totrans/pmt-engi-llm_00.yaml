- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Since OpenAI introduced GPT-2 in early 2019, large language models (LLMs) have
    rapidly changed our world. In 2019, if you, as a coder, had a technical question,
    then you would search the internet for an answer. More often than not, there would
    be no answer, leaving only the option to post on some question-and-answer (Q&A)
    forum in the possibly vain hope that *someone* might answer you. But today, instead
    of breaking your flow, you just ask an LLM assistant for direct commentary on
    the code you’re working on. Moreover, you can even engage in a pairing session
    where the assistant writes the code to your specifications. This is just in the
    field of software engineering, and similar tectonic shifts are beginning to be
    felt in almost any field that you can name.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自从OpenAI在2019年初推出GPT-2以来，大型语言模型（LLMs）迅速改变了我们的世界。在2019年，如果你作为一个程序员，遇到技术问题，你会上网寻找答案。很多时候，你找不到答案，只能寄希望于在一些问答（Q&A）论坛上发帖，希望能有人回答你。但如今，你不必打断你的工作流程，只需向LLM助手询问你正在工作的代码的直接评论。此外，你甚至可以进行配对会话，助手根据你的要求编写代码。这仅仅是在软件工程领域，几乎在你能命名的任何领域，类似的板块式变革都开始感受到。
- en: The reason that this revolution is taking place is because the LLM is truly
    a revolutionary technology that makes it possible to achieve in software what
    formerly could be done only through human interaction. LLMs can generate content,
    answer questions, extract tabular data from natural language text, summarize text,
    classify documents, translate, and (in principle) do just about anything that
    you can do with text—except that LLMs will do it many orders of magnitude faster
    and never stop for a break.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这场革命发生的原因是因为LLM确实是一项革命性的技术，它使得在软件领域实现以前只能通过人类交互才能完成的事情成为可能。LLMs可以生成内容、回答问题、从自然语言文本中提取表格数据、总结文本、分类文档、翻译，并且在原则上可以做任何你可以用文本做的事情——除了LLMs会以许多数量级更快地完成，并且永远不会停下来休息。
- en: For entrepreneurs, this opens endless doors of opportunity in every field imaginable.
    But before you can take advantage of these opportunities, you have to be prepared.
    This book serves as a guide to help you understand LLMs, interact with them through
    prompt engineering, and build applications that will bring value to your users,
    your company, or yourself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业家来说，这为每个可想象的领域打开了无尽的机遇之门。但在你能够利用这些机遇之前，你必须做好准备。这本书作为指南，帮助你了解LLMs，通过提示工程与他们互动，并构建能够为你的用户、公司或你自己带来价值的应用程序。
- en: Who Is This Book For?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书是为谁而写的？
- en: This book is written for application engineers. If you build software products
    that customers use, then this book is for you. If you build internal applications
    or data-processing workflows, then this book is also for you. The reason that
    we are being so inclusive is because we believe that the usage of LLMs will soon
    become ubiquitous. Even if your day-to-day work doesn’t involve prompt engineering
    or LLM workflow design, your codebase will be filled with usages of LLMs, and
    you’ll need to understand how to interact with them just to get your job done.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是为应用工程师编写的。如果你构建的是客户使用的软件产品，那么这本书就是为你而写的。如果你构建的是内部应用或数据处理工作流程，这本书也适合你。我们之所以如此包容，是因为我们相信LLMs的使用将很快变得无处不在。即使你的日常工作不涉及提示工程或LLM工作流程设计，你的代码库也将充满LLMs的使用，你需要了解如何与他们交互才能完成你的工作。
- en: However, a subset of application engineers will be the dedicated LLM wranglers—these
    are the *prompt engineers*. It’s their job to convert problems into a packet of
    information that the LLM can understand—which we call the *prompt*—and then convert
    the LLM completions back into results that bring value to those who use the application.
    If this is your current role—or if you want this to be your role—then this book
    is *especially* for you.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，应用工程师的一个子集将是专门的LLM驯服者——这些是**提示工程师**。他们的工作是把问题转换成LLM可以理解的信息包——我们称之为**提示**——然后将LLM的完成内容转换成对应用程序使用者有价值的结果。如果你目前就是这个角色——或者你希望成为这个角色——那么这本书**特别**适合你。
- en: LLMs are very approachable—you speak with them in natural language. So, for
    this book, you won’t be expected to know everything about machine learning. But
    you do need to have a good grasp of basic engineering principles—you need to know
    how to program and how to use an API. Another prerequisite for this book is the
    ability to empathize, because unlike with any technology before, you need to understand
    how LLMs “think” so that you can guide them to generate the content you need.
    This book will show you how.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）非常易于接近——你用自然语言与他们交谈。因此，对于这本书，你不需要了解机器学习的所有内容。但你确实需要掌握基本的工程原理——你需要知道如何编程和如何使用API。这本书的另一个先决条件是具有同理心，因为与之前任何技术不同，你需要理解LLM“思考”的方式，以便引导它们生成所需的内容。这本书将向你展示如何做到这一点。
- en: What You Will Learn
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你将学到什么
- en: The goal of this book is to equip you with all the theory, techniques, tips,
    and tricks you need to master prompt engineering and build successful LLM applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的目标是为你提供所有你需要掌握提示工程和构建成功的LLM应用的理论、技术、技巧和窍门。
- en: In [Part I](part01.html#part01) of the book, we convey a foundational understanding
    of LLMs, their inner workings, and their functionality as text completion engines.
    We cover the extension of LLMs to their new role as chat engines, and we present
    a high-level approach to LLM application development.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的[第一部分](part01.html#part01)中，我们传达了对LLM的基础理解，它们的内部工作原理以及作为文本完成引擎的功能。我们涵盖了LLM扩展到作为聊天引擎的新角色，并介绍了LLM应用开发的总体方法。
- en: In [Part II](part02.html#part02), we introduce the core techniques for prompt
    engineering—how to source context information, rank its importance for the task
    at hand, pack the prompt (without overloading it), and organize everything into
    a template that will result in high-quality completions that elicit the answer
    you need.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二部分](part02.html#part02)中，我们介绍了提示工程的核心技术——如何获取上下文信息，为当前任务排序其重要性，打包提示（避免过载），并将所有内容组织成模板，从而生成高质量的完成内容，以获得所需的答案。
- en: In [Part III](part03.html#part03), we move to more advanced techniques. We assemble
    loops, pipelines, and workflows of LLM inference to create conversational agency
    and LLM-driven workflows, and we then explain techniques for evaluating LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三部分](part03.html#part03)中，我们转向更高级的技术。我们组装LLM推理的循环、管道和工作流程以创建对话代理和LLM驱动的流程，然后解释评估LLM的技术。
- en: 'Throughout this book, we highlight one principle that underlies all others:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们强调了一个所有其他原则都基于的原则：
- en: At their core, LLMs are just text completion engines that mimic the text they
    see during their training.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本质上，LLM只是模仿它们在训练期间看到的文本的文本完成引擎。
- en: 'If you process that statement deeply, then you’ll arrive at the same conclusions
    that we share throughout this book: when you want an LLM to behave a certain way,
    you have to shape the prompt to resemble patterns seen in training data—use clear
    language, rely upon existing patterns rather than creating new ones, and don’t
    drown the LLM in superfluous content. Once you master prompt engineering, you
    can build upon these skills by creating conversation agency and workflows—the
    dominant paradigms for LLM applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你深入思考这个陈述，那么你将得出与我们在这本书中分享的相同的结论：当你想让LLM以某种方式表现时，你必须塑造提示以类似于训练数据中看到的模式——使用清晰的语言，依赖于现有模式而不是创建新模式，并且不要让LLM淹没在多余的内容中。一旦你掌握了提示工程，你就可以通过创建对话代理和工作流程来构建这些技能——这是LLM应用的主要范例。
- en: Conventions Used in This Book
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`等宽`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序列表，以及段落内引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: '*`Constant width italic`*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*`等宽斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应替换为用户提供的值或由上下文确定的值的文本。
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般性说明。
- en: Warning
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示警告或注意。
- en: O’Reilly Online Learning
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 40多年来，[*O’Reilly Media*](https://oreilly.com)一直为科技公司提供技术和商业培训、知识和洞察力，以帮助公司取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境以及来自O’Reilly和200多家其他出版商的大量文本和视频。欲了解更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题寄给出版社：
- en: O’Reilly Media, Inc.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加文斯顿北街1005号
- en: 800-889-8969 (in the United States or Canada)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-889-8969（美国或加拿大）
- en: 707-827-7019 (international or local)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-827-7019（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*support@oreilly.com*](mailto:support@oreilly.com)'
- en: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/PromptEngForLLMs*](https://oreil.ly/PromptEngForLLMs).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书有一个网页，其中列出了勘误表、示例和任何其他附加信息。您可以通过[*https://oreil.ly/PromptEngForLLMs*](https://oreil.ly/PromptEngForLLMs)访问此页面。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 欲获取有关我们书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinkedIn上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。
- en: Acknowledgments
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Thank you to our technical reviewers, Leonie Monigatti, Benjamin Muskalla, David
    Foster, and Balaji Dhamodharan; our technical editor, Sara Verdi; and our development
    editor, Sara Hunter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们的技术审稿人，莱奥妮·莫尼加蒂、本杰明·穆斯卡拉、大卫·福斯特和巴拉吉·达莫达拉南；我们的技术编辑，萨拉·韦尔迪；以及我们的开发编辑，萨拉·亨特。
- en: From John
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自约翰
- en: To Kumiko—my immeasurable love and thanks. I swore I’d never write a book again,
    but I did, and you supported me patiently through this foolishness once more.
    To Meg and Bo—*Papa’s done with work for today!* Let’s go play.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 致库米科——我无法衡量的爱和感激。我发誓我再也不会写书了，但我还是写了，而你再次耐心地支持我度过这次愚蠢的行为。给梅格和博——*爸爸今天的工作做完了！*我们去玩吧。
- en: From Albert
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自阿尔伯特
- en: To Annika, Fiona, and Loki—may your prompts never falter!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 致安尼卡、菲奥娜和洛基——愿你们的提示永不失效！
