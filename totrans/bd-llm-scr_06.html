<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span></span> <span class="chapter-title-text">Fine-tuning to follow instructions</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">The instruction fine-tuning process of LLMs</li> 
    <li class="readable-text" id="p3">Preparing a dataset for supervised instruction fine-tuning</li> 
    <li class="readable-text" id="p4">Organizing instruction data in training batches</li> 
    <li class="readable-text" id="p5">Loading a pretrained LLM and fine-tuning it to follow human instructions</li> 
    <li class="readable-text" id="p6">Extracting LLM-generated instruction responses for evaluation</li> 
    <li class="readable-text" id="p7">Evaluating an instruction-fine-tuned LLM</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>Previously, we implemented the LLM architecture, carried out pretraining, and imported pretrained weights from external sources into our model. Then, we focused on fine-tuning our LLM for a specific classification task: distinguishing between spam and non-spam text messages. Now we’ll implement the process for fine-tuning an LLM to follow human instructions, as illustrated in figure 7.1. Instruction fine-tuning is one of the main techniques behind developing LLMs for chatbot applications, personal assistants, and other conversational tasks.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p9">  
   <img alt="figure" src="../Images/7-1.png" width="1012" height="549"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.1</span> The three main stages of coding an LLM. This chapter focuses on step 9 of stage 3: fine-tuning a pretrained LLM to follow human instructions.</h5>
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Figure 7.1 shows two main ways of fine-tuning an LLM: fine-tuning for classification (step 8) and fine-tuning an LLM to follow instructions (step 9). We implemented step 8 in chapter 6. Now we will fine-tune an LLM using an <em>instruction dataset</em>.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.1</span> Introduction to instruction fine-tuning</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>We now know that pretraining an LLM involves a training procedure where it learns to generate one word at a time. The resulting pretrained LLM is capable of <em>text completion</em>, meaning it can finish sentences or write text paragraphs given a fragment as input. However, pretrained LLMs often struggle with specific instructions, such as “Fix the grammar in this text” or “Convert this text into passive voice.” Later, we will examine a concrete example where we load the pretrained LLM as the basis for <em>instruction fine-tuning</em>, also known as <em>supervised instruction fine-tuning</em>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>Here, we focus on improving the LLM’s ability to follow such instructions and generate a desired response, as illustrated in figure 7.2. Preparing the dataset is a key aspect of instruction fine-tuning. Then we’ll complete all the steps in the three stages of the instruction fine-tuning process, beginning with the dataset preparation, as shown in figure 7.3.<span class="aframe-location"/><span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p14">  
   <img alt="figure" src="../Images/7-2.png" width="742" height="367"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.2</span> Examples of instructions that are processed by an LLM to generate desired responses</h5>
  </div> 
  <div class="browsable-container figure-container" id="p15">  
   <img alt="figure" src="../Images/7-3.png" width="1100" height="813"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.3</span> The three-stage process for instruction fine-tuning an LLM. Stage 1 involves dataset preparation, stage 2 focuses on model setup and fine-tuning, and stage 3 covers the evaluation of the model. We will begin with step 1 of stage 1: downloading and formatting the dataset.</h5>
  </div> 
  <div class="readable-text" id="p16"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.2</span> Preparing a dataset for supervised instruction fine-tuning</h2> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Let’s download and format the instruction dataset for instruction fine-tuning a pretrained LLM. The dataset consists of 1,100 <em>instruction–response pairs</em> similar to those in figure 7.2. This dataset was created specifically for this book, but interested readers can find alternative, publicly available instruction datasets in appendix B.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>The following code implements and executes a function to download this dataset, which is a relatively small file (only 204 KB) in JSON format. JSON, or JavaScript Object Notation, mirrors the structure of Python dictionaries, providing a simple structure for data interchange that is both human readable and machine friendly.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p19"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.1</span> Downloading the dataset</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import json
import os
import urllib

def download_and_load_file(file_path, url):
    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    
    with open(file_path, "r") as file:
        data = json.load(file)
    return data

file_path = "instruction-data.json"
url = (
    "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch"
    "/main/ch07/01_main-chapter-code/instruction-data.json"
)


data = download_and_load_file(file_path, url)
print("Number of entries:", len(data))</pre> 
   </div> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>The output of executing the preceding code is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p21"> 
   <div class="code-area-container"> 
    <pre class="code-area">Number of entries: 1100</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>The <code>data</code> list that we loaded from the JSON file contains the 1,100 entries of the instruction dataset. Let’s print one of the entries to see how each entry is structured:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p23"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Example entry:\n", data[50])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>The content of the example entry is </p> 
  </div> 
  <div class="browsable-container listing-container" id="p25"> 
   <div class="code-area-container"> 
    <pre class="code-area">Example entry:
 {'instruction': 'Identify the correct spelling of the following word.',
  'input': 'Ocassion', 'output': "The correct spelling is 'Occasion.'"}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>As we can see, the example entries are Python dictionary objects containing an <code>'instruction'</code>, <code>'input'</code>, and <code>'output'</code>. Let’s take a look at another example:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p27"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Another example entry:\n", data[999])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Based on the contents of this entry, the <code>'input'</code> field may occasionally be empty:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <div class="code-area-container"> 
    <pre class="code-area">Another example entry:
 {'instruction': "What is an antonym of 'complicated'?", 
  'input': '',
  'output': "An antonym of 'complicated' is 'simple'."}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Instruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are explicitly provided. There are various methods to format these entries for LLMs. Figure 7.4 <span class="aframe-location"/>illustrates two different example formats, often referred to as <em>prompt styles</em>, used in the training of notable LLMs such as Alpaca and Phi-3. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p31">  
   <img alt="figure" src="../Images/7-4.png" width="1012" height="634"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.4</span> Comparison of prompt styles for instruction fine-tuning in LLMs. The Alpaca style (left) uses a structured format with defined sections for instruction, input, and response, while the Phi-3 style (right) employs a simpler format with designated <code>&lt;|user|&gt;</code> and <code>&lt;|assistant|&gt;</code> tokens.</h5>
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning process. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt styles. The rest of this chapter uses the Alpaca prompt style since it is one of the most popular ones, largely because it helped define the original approach to fine-tuning.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p33"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.1 Changing prompt styles </h5> 
   </div> 
   <div class="readable-text" id="p34"> 
    <p>After fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt style shown in figure 7.4 and observe whether it affects the response quality of the model.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Let’s define a <code>format_input</code> function that we can use to convert the entries in the <code>data</code> list into the Alpaca-style input format. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p36"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.2</span> Implementing the prompt formatting function</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = (
        f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""
    )
    return instruction_text + input_text</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>This <code>format_input</code> function takes a dictionary <code>entry</code> as input and constructs a formatted string. Let’s test it to dataset entry <code>data[50]</code>, which we looked at earlier:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p38"> 
   <div class="code-area-container"> 
    <pre class="code-area">model_input = format_input(data[50])
desired_response = f"\n\n### Response:\n{data[50]['output']}"
print(model_input + desired_response)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>The formatted input looks like as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <div class="code-area-container"> 
    <pre class="code-area">Below is an instruction that describes a task. Write a response that 
appropriately completes the request.

### Instruction:
Identify the correct spelling of the following word.

### Input:
Ocassion

### Response:
The correct spelling is 'Occasion.'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Note that the <code>format_input</code> skips the optional <code>###</code> <code>Input:</code> section if the <code>'input'</code> field is empty, which we can test out by applying the <code>format_input</code> function to entry <code>data[999]</code> that we inspected earlier:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <div class="code-area-container"> 
    <pre class="code-area">model_input = format_input(data[999])
desired_response = f"\n\n### Response:\n{data[999]['output']}"
print(model_input + desired_response)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>The output shows that entries with an empty <code>'input'</code> field don’t contain an <code>###</code> <code>Input:</code> section in the formatted input:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <div class="code-area-container"> 
    <pre class="code-area">Below is an instruction that describes a task. Write a response that 
appropriately completes the request.

### Instruction:
What is an antonym of 'complicated'?

### Response:
An antonym of 'complicated' is 'simple'.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Before we move on to setting up the PyTorch data loaders in the next section, let’s divide the dataset into training, validation, and test sets analogous to what we have done with the spam classification dataset in the previous chapter. The following listing shows how we calculate the portions.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p46"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.3</span> Partitioning the dataset</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">train_portion = int(len(data) * 0.85)   <span class="aframe-location"/> #1
test_portion = int(len(data) * 0.1)           <span class="aframe-location"/> #2
val_portion = len(data) - train_portion - test_portion   <span class="aframe-location"/> #3

train_data = data[:train_portion]
test_data = data[train_portion:train_portion + test_portion]
val_data = data[train_portion + test_portion:]

print("Training set length:", len(train_data))
print("Validation set length:", len(val_data))
print("Test set length:", len(test_data))</pre> 
    <div class="code-annotations-overlay-container">
     #1 Use 85% of the data for training
     <br/>#2 Use 10% for testing
     <br/>#3 Use remaining 5% for validation
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>This partitioning results in the following dataset sizes:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p48"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training set length: 935
Validation set length: 55
Test set length: 110</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Having successfully downloaded and partitioned the dataset and gained a clear understanding of the dataset prompt formatting, we are now ready for the core implementation of the instruction fine-tuning process. Next, we focus on developing the method for constructing the training batches for fine-tuning the LLM.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.3</span> Organizing data into training batches</h2> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>As we progress into the implementation phase of our instruction fine-tuning process, the next step, illustrated in figure 7.5, focuses on constructing the training batches effectively. This involves defining a method that will ensure our model receives the formatted training data during the fine-tuning process.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/7-5.png" width="917" height="604"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.5</span> The three-stage process for instruction fine-tuning an LLM. Next, we look at step 2 of stage 1: assembling the training batches.</h5>
  </div> 
  <div class="readable-text" id="p53"> 
   <p>In the previous chapter, the training batches were created automatically by the PyTorch <code>DataLoader</code> class, which employs a default <em>collate</em> function to combine lists of samples into batches. A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the model during training. </p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>However, the batching process for instruction fine-tuning is a bit more involved and requires us to create our own custom collate function that we will later plug into the <code>DataLoader</code>. We implement this custom collate function to handle the specific requirements and formatting of our instruction fine-tuning dataset. </p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>Let’s tackle the <em>batching process</em> in several steps, including coding the custom collate function, as illustrated in figure 7.6. First, to implement steps 2.1 and 2.2, we code an <code>InstructionDataset</code> class that applies <code>format_input</code> and <em>pretokenizes</em> all inputs in the dataset, similar to the <code>SpamDataset</code> in chapter 6. This two-step process, detailed in figure 7.7, is implemented in the <code>__init__</code> constructor method of the <code>InstructionDataset</code>.<span class="aframe-location"/><span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p56">  
   <img alt="figure" src="../Images/7-6.png" width="877" height="774"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.6</span> The five substeps involved in implementing the batching process: (2.1) applying the prompt template, (2.2) using tokenization from previous chapters, (2.3) adding padding tokens, (2.4) creating target token IDs, and (2.5) replacing <code>-100</code> placeholder tokens to mask padding tokens in the loss function.</h5>
  </div> 
  <div class="browsable-container figure-container" id="p57">  
   <img alt="figure" src="../Images/7-7.png" width="1007" height="614"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.7</span> The first two steps involved in implementing the batching process. Entries are first formatted using a specific prompt template (2.1) and then tokenized (2.2), resulting in a sequence of token IDs that the model can process.</h5>
  </div> 
  <div class="browsable-container listing-container" id="p58"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.4</span> Implementing an instruction dataset class</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
from torch.utils.data import Dataset

class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.encoded_texts = []
        for entry in data:        <span class="aframe-location"/> #1
            instruction_plus_input = format_input(entry)
            response_text = f"\n\n### Response:\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Pretokenizes texts
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Similar to the approach used for classification fine-tuning, we want to accelerate training by collecting multiple training examples in a batch, which necessitates padding all inputs to a similar length. As with classification fine-tuning, we use the <code>&lt;|endoftext|&gt;</code> token as a padding token. </p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>Instead of appending the <code>&lt;|endoftext|&gt;</code> tokens to the text inputs, we can append the token ID corresponding to <code>&lt;|endoftext|&gt;</code> to the pretokenized inputs directly. We can use the tokenizer’s <code>.encode</code> method on an <code>&lt;|endoftext|&gt;</code> token to remind us which token ID we should use:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p61"> 
   <div class="code-area-container"> 
    <pre class="code-area">import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")
print(tokenizer.encode("&lt;|endoftext|&gt;", allowed_special={"&lt;|endoftext|&gt;"}))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>The resulting token ID is <code>50256</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p63"> 
   <p>Moving on to step 2.3 of the process (see figure 7.6), we adopt a more sophisticated approach by developing a custom collate function that we can pass to the data loader. This custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths, as demonstrated in figure 7.8. This approach minimizes unnecessary padding by only extending sequences to match the longest one in each batch, not the whole dataset.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p64">  
   <img alt="figure" src="../Images/7-8.png" width="1007" height="634"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.8</span> The padding of training examples in batches using token ID <code>50256</code> to ensure uniform length within each batch. Each batch may have different lengths, as shown by the first and second.</h5>
  </div> 
  <div class="readable-text" id="p65"> 
   <p>We can implement the padding process with a custom collate function:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p66"> 
   <div class="code-area-container"> 
    <pre class="code-area">def custom_collate_draft_1(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)  <span class="aframe-location"/> #1
    inputs_lst = []

    for item in batch:    <span class="aframe-location"/> #2
        new_item = item.copy()
        new_item += [pad_token_id]


        padded = (
            new_item + [pad_token_id] * 
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])   <span class="aframe-location"/> #3
        inputs_lst.append(inputs)

    inputs_tensor = torch.stack(inputs_lst).to(device)    <span class="aframe-location"/> #4
    return inputs_tensor</pre> 
    <div class="code-annotations-overlay-container">
     #1 Finds the longest sequence in the batch
     <br/>#2 Pads and prepares inputs
     <br/>#3 Removes extra padded token added earlier
     <br/>#4 Converts the list of inputs to a tensor and transfers it to the target device
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>The <code>custom_collate_draft_1</code> we implemented is designed to be integrated into a PyTorch <code>DataLoader</code>, but it can also function as a standalone tool. Here, we use it independently to test and verify that it operates as intended. Let’s try it on three different inputs that we want to assemble into a batch, where each example gets padded to the same length:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p68"> 
   <div class="code-area-container"> 
    <pre class="code-area">inputs_1 = [0, 1, 2, 3, 4]
inputs_2 = [5, 6]
inputs_3 = [7, 8, 9]
batch = (
    inputs_1,
    inputs_2,
    inputs_3
)
print(custom_collate_draft_1(batch))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>The resulting batch looks like the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p70"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[    0,     1,     2,     3,     4],  
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>This output shows all inputs have been padded to the length of the longest input list, <code>inputs_1</code>, containing five token IDs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p72"> 
   <p>We have just implemented our first custom collate function to create batches from lists of inputs. However, as we previously learned, we also need to create batches with the target token IDs corresponding to the batch of input IDs. These target IDs, as shown in figure 7.9, are crucial because they represent what we want the model to generate and what we need during training to calculate the loss for the weight updates. That is, we modify our custom collate function to return the target token IDs in addition to the input token IDs.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p73">  
   <img alt="figure" src="../Images/7-9.png" width="919" height="773"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.9</span> The five substeps involved in implementing the batching process. We are now focusing on step 2.4, the creation of target token IDs. This step is essential as it enables the model to learn and predict the tokens it needs to generate.</h5>
  </div> 
  <div class="readable-text" id="p74"> 
   <p>Similar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right. This setup, as shown in figure 7.10, allows the LLM to learn how to predict the next token in a sequence.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p75">  
   <img alt="figure" src="../Images/7-10.png" width="1100" height="1308"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.10</span> The input and target token alignment used in the instruction fine-tuning process of an LLM. For each input sequence, the corresponding target sequence is created by shifting the token IDs one position to the right, omitting the first token of the input, and appending an end-of-text token. </h5>
  </div> 
  <div class="readable-text" id="p76"> 
   <p>The following updated collate function generates the target token IDs from the input token IDs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <div class="code-area-container"> 
    <pre class="code-area">def custom_collate_draft_2(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]

        padded = (
            new_item + [pad_token_id] * 
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])    <span class="aframe-location"/> #1
        targets = torch.tensor(padded[1:])   <span class="aframe-location"/> #2
        inputs_lst.append(inputs)
        targets_lst.append(targets)

    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor

inputs, targets = custom_collate_draft_2(batch)
print(inputs)
print(targets)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Truncates the last token for inputs
     <br/>#2 Shifts +1 to the right for targets
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>Applied to the example <code>batch</code> consisting of three input lists we defined earlier, the new <code>custom_collate_draft_2</code> function now returns the input and the target batch:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p79"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[    0,     1,     2,     3,     4],   <span class="aframe-location"/> #1
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],  <span class="aframe-location"/> #2
        [    6, 50256, 50256, 50256, 50256],
        [    8,     9, 50256, 50256, 50256]])</pre> 
    <div class="code-annotations-overlay-container">
     #1 The first tensor represents inputs.
     <br/>#2 The second tensor represents the targets.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>In the next step, we assign a <code>-100</code> placeholder value to all padding tokens, as highlighted in figure 7.11. This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning. We will discuss this process in more detail after we implement this modification. (When fine-tuning for classification, we did not have to worry about this since we only trained the model based on the last output token.)<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p81">  
   <img alt="figure" src="../Images/7-11.png" width="1100" height="929"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.11</span> The five substeps involved in implementing the batching process. After creating the target sequence by shifting token IDs one position to the right and appending an end-of-text token, in step 2.5, we replace the end-of-text padding tokens with a placeholder value (<code>-100</code>).</h5>
  </div> 
  <div class="readable-text intended-text" id="p82"> 
   <p>However, note that we retain one end-of-text token, ID <code>50256</code>, in the target list, as depicted in figure 7.12. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated response is complete.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p83">  
   <img alt="figure" src="../Images/7-12.png" width="1007" height="349"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.12</span> Step 2.4 in the token replacement process in the target batch for the training data preparation. We replace all but the first instance of the end-of-text token, which we use as padding, with the placeholder value <code>-100</code>, while keeping the initial end-of-text token in each target sequence.</h5>
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>In the following listing, we modify our custom collate function to replace tokens with ID <code>50256</code> with <code>-100</code> in the target lists. Additionally, we introduce an <code>allowed_ max_length</code> parameter to optionally limit the length of the samples. This adjustment will be useful if you plan to work with your own datasets that exceed the 1,024-token context size supported by the GPT-2 model. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.5</span> Implementing a custom batch collate function</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]


        padded = (                              <span class="aframe-location"/> #1
            new_item + [pad_token_id] *          #1
            (batch_max_length - len(new_item))   #1
        )
        inputs = torch.tensor(padded[:-1])     <span class="aframe-location"/> #2
        targets = torch.tensor(padded[1:])    <span class="aframe-location"/> #3

        mask = targets == pad_token_id             <span class="aframe-location"/> #4
        indices = torch.nonzero(mask).squeeze()     #4
        if indices.numel() &gt; 1:                     #4
            targets[indices[1:]] = ignore_index     #4

        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]      <span class="aframe-location"/> #5
            targets = targets[:allowed_max_length]     #5

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor</pre> 
    <div class="code-annotations-overlay-container">
     #1 Pads sequences to max_length
     <br/>#2 Truncates the last token for inputs
     <br/>#3 Shifts +1 to the right for targets
     <br/>#4 Replaces all but the first padding tokens in targets by ignore_index
     <br/>#5 Optionally truncates to the maximum sequence length
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Again, let’s try the collate function on the sample batch that we created earlier to check that it works as intended:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">inputs, targets = custom_collate_fn(batch)
print(inputs)
print(targets)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>The results are as follows, where the first tensor represents the inputs and the second tensor represents the targets:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p89"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256,  -100,  -100,  -100],
        [    8,     9, 50256,  -100,  -100]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>The modified collate function works as expected, altering the target list by inserting the token ID <code>-100</code>. What is the logic behind this adjustment? Let’s explore the underlying purpose of this modification.</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>For demonstration purposes, consider the following simple and self-contained example where each output logit corresponds to a potential token from the model’s vocabulary. Here’s how we might calculate the cross entropy loss<em> </em>(introduced in chapter 5) during training when the model predicts a sequence of tokens, which is similar to what we did when we pretrained the model and fine-tuned it for classification:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">logits_1 = torch.tensor(
    [[-1.0, 1.0],    <span class="aframe-location"/> #1
     [-0.5, 1.5]]     <span class="aframe-location"/> #2
)
targets_1 = torch.tensor([0, 1]) # Correct token indices to generate
loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)
print(loss_1)</pre> 
    <div class="code-annotations-overlay-container">
     #1 predictions for 1st token 
     <br/>#2 predictions for 2nd token
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>The loss value calculated by the previous code is <code>1.1269</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor(1.1269)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>As we would expect, adding an additional token ID affects the loss calculation:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p96"> 
   <div class="code-area-container"> 
    <pre class="code-area">logits_2 = torch.tensor(
    [[-1.0, 1.0],
     [-0.5, 1.5],
     [-0.5, 1.5]]     <span class="aframe-location"/> #1
)
targets_2 = torch.tensor([0, 1, 1])
loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)
print(loss_2)</pre> 
    <div class="code-annotations-overlay-container">
     #1 New third token ID prediction
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>After adding the third token, the loss value is <code>0.7936</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>So far, we have carried out some more or less obvious example calculations using the cross entropy loss function in PyTorch, the same loss function we used in the training functions for pretraining and fine-tuning for classification. Now let’s get to the interesting part and see what happens if we replace the third target token ID with <code>-100</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p99"> 
   <div class="code-area-container"> 
    <pre class="code-area">targets_3 = torch.tensor([0, 1, -100])
loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)
print(loss_3)
print("loss_1 == loss_3:", loss_1 == loss_3)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>The resulting output is </p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor(1.1269)
loss_1 == loss_3: tensor(True)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>The resulting loss on these three training examples is identical to the loss we calculated from the two training examples earlier. In other words, the cross entropy loss function ignored the third entry in the <code>targets_3</code> vector, the token ID corresponding to <code>-100</code>. (Interested readers can try to replace the <code>-100</code> value with another token ID that is not <code>0</code> or <code>1</code>; it will result in an error.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p103"> 
   <p>So what’s so special about <code>-100</code> that it’s ignored by the cross entropy loss? The default setting of the cross entropy function in PyTorch is <code>cross_entropy(...,</code> <code>ignore_index=-100)</code>. This means that it ignores targets labeled with <code>-100</code>. We take advantage of this <code>ignore_index</code> to ignore the additional end-of-text (padding) tokens that we used to pad the training examples to have the same length in each batch. However, we want to keep one <code>50256</code> (end-of-text) token ID in the targets because it helps the LLM to learn to generate end-of-text tokens, which we can use as an indicator that a response is complete.</p> 
  </div> 
  <div class="readable-text intended-text" id="p104"> 
   <p>In addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in figure 7.13. By masking out the LLM’s target token IDs corresponding to the instruction, the cross entropy loss is only computed for the generated response target IDs. Thus, the model is trained to focus on generating accurate responses rather than memorizing instructions, which can help reduce overfitting.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p105">  
   <img alt="figure" src="../Images/7-13.png" width="1007" height="494"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.13</span> Left: The formatted input text we tokenize and then feed to the LLM during training. Right: The target text we prepare for the LLM where we can optionally mask out the instruction section, which means replacing the corresponding token IDs with the <code>-100</code> <code>ignore_index</code> value.</h5>
  </div> 
  <div class="readable-text" id="p106"> 
   <p>As of this writing, researchers are divided on whether masking the instructions is universally beneficial during instruction fine-tuning. For instance, the 2024 paper by Shi et al., “Instruction Tuning With Loss Over Instructions” (<a href="https://arxiv.org/abs/2405.14394">https://arxiv.org/abs/2405.14394</a>), demonstrated that not masking the instructions benefits the LLM performance (see appendix B for more details). Here, we will not apply masking and leave it as an optional exercise for interested readers.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p107"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.2 Instruction and input masking</h5> 
   </div> 
   <div class="readable-text" id="p108"> 
    <p>After completing the chapter and fine-tuning the model with <code>InstructionDataset</code>, replace the instruction and input tokens with the <code>-100</code> mask to use the instruction masking method illustrated in figure 7.13. Then evaluate whether this has a positive effect on model performance.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.4</span> Creating data loaders for an instruction dataset</h2> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>We have completed several stages to implement an <code>InstructionDataset</code> class and a <code>custom_collate_fn</code> function for the instruction dataset. As shown in figure 7.14,<span class="aframe-location"/> we are ready to reap the fruits of our labor by simply plugging both <code>InstructionDataset</code> objects and the <code>custom_collate_fn</code> function into PyTorch data loaders. These loaders will automatically shuffle and organize the batches for the LLM instruction fine-tuning process.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p111">  
   <img alt="figure" src="../Images/7-14.png" width="915" height="579"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.14</span> The three-stage process for instruction fine-tuning an LLM. Thus far, we have prepared the dataset and implemented a custom collate function to batch the instruction dataset. Now, we can create and apply the data loaders to the training, validation, and test sets needed for the LLM instruction fine-tuning and evaluation.</h5>
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>Before we implement the data loader creation step, we have to briefly talk about the <code>device</code> setting of the <code>custom_collate_fn</code>. The <code>custom_collate_fn</code> includes code to move the input and target tensors (for example, <code>torch.stack(inputs_lst).to (device)</code>) to a specified device, which can be either <code>"cpu"</code> or <code>"cuda"</code> (for NVIDIA GPUs) or, optionally, <code>"mps"</code> for Macs with Apple Silicon chips. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p113"> 
   <p><span class="print-book-callout-head">Note</span>  Using an <code>"mps"</code> device may result in numerical differences compared to the contents of this chapter, as Apple Silicon support in PyTorch is still experimental.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Previously, we moved the data onto the target device (for example, the GPU memory when <code>device="cuda"</code>) in the main training loop. Having this as part of the collate function offers the advantage of performing this device transfer process as a background process outside the training loop, preventing it from blocking the GPU during model training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>The following code initializes the <code>device</code> variable:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p116"> 
   <div class="code-area-container"> 
    <pre class="code-area">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# if torch.backends.mps.is_available():  <span class="aframe-location"/> #1
#     device = torch.device("mps")"      
print("Device:", device)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Uncomments these two lines to use the GPU on an Apple Silicon chip
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>This will either print <code>"Device:</code> <code>cpu"</code> or <code>"Device:</code> <code>cuda"</code>, depending on your machine.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Next, to reuse the chosen device setting in <code>custom_collate_fn</code> when we plug it into the PyTorch <code>DataLoader</code> class, we use the <code>partial</code> function from Python’s <code>functools</code> standard library to create a new version of the function with the device argument prefilled. Additionally, we set the <code>allowed_max_length</code> to <code>1024</code>, which truncates the data to the maximum context length supported by the GPT-2 model, which we will fine-tune later:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <div class="code-area-container"> 
    <pre class="code-area">from functools import partial

customized_collate_fn = partial(
    custom_collate_fn,
    device=device,
    allowed_max_length=1024
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Next, we can set up the data loaders as we did previously, but this time, we will use our custom collate function for the batching process.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.6</span> Initializing the data loaders</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from torch.utils.data import DataLoader

num_workers = 0     <span class="aframe-location"/> #1
batch_size = 8

torch.manual_seed(123)

train_dataset = InstructionDataset(train_data, tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers
)

val_dataset = InstructionDataset(val_data, tokenizer)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

test_dataset = InstructionDataset(test_data, tokenizer)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)</pre> 
    <div class="code-annotations-overlay-container">
     #1 You can try to increase this number if parallel Python processes are supported by your operating system.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Let’s examine the dimensions of the input and target batches generated by the training loader:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p123"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Train loader:")
for inputs, targets in train_loader:
    print(inputs.shape, targets.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>The output is as follows (truncated to conserve space):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p125"> 
   <div class="code-area-container"> 
    <pre class="code-area">Train loader:
torch.Size([8, 61]) torch.Size([8, 61])
torch.Size([8, 76]) torch.Size([8, 76])
torch.Size([8, 73]) torch.Size([8, 73])
...
torch.Size([8, 74]) torch.Size([8, 74])
torch.Size([8, 69]) torch.Size([8, 69])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>This output shows that the first input and target batch have dimensions 8 × 61, where 8 represents the batch size and 61 is the number of tokens in each training example in this batch. The second input and target batch have a different number of tokens—for instance, 76. Thanks to our custom collate function, the data loader is able to create batches of different lengths. In the next section, we load a pretrained LLM that we can then fine-tune with this data loader.</p> 
  </div> 
  <div class="readable-text" id="p127"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.5</span> Loading a pretrained LLM</h2> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>We have spent a lot of time preparing the dataset for instruction fine-tuning, which is a key aspect of the supervised fine-tuning process. Many other aspects are the same as in pretraining, allowing us to reuse much of the code from earlier chapters.</p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>Before beginning instruction fine-tuning, we must first load a pretrained GPT model that we want to fine-tune (see figure 7.15), a process we have undertaken previously. However, instead of using the smallest 124-million-parameter model as before, we load the medium-sized model with 355 million parameters. The reason for this choice is that the 124-million-parameter model<span class="aframe-location"/> is too limited in capacity to achieve satisfactory results via instruction fine-tuning. Specifically, smaller models lack the necessary capacity to learn and retain the intricate patterns and nuanced behaviors required for high-quality instruction-following tasks. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p130">  
   <img alt="figure" src="../Images/7-15.png" width="812" height="574"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.15</span> The three-stage process for instruction fine-tuning an LLM. After the dataset preparation, the process of fine-tuning an LLM for instruction-following begins with loading a pretrained LLM, which serves as the foundation for subsequent training. </h5>
  </div> 
  <div class="readable-text intended-text" id="p131"> 
   <p>Loading our pretrained models requires the same code as when we pretrained the data (section 5.5) and fine-tuned it for classification (section 6.4), except that we now specify <code>"gpt2-medium</code> <code>(355M)"</code> instead of <code>"gpt2-small</code> <code>(124M)"</code>. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p132"> 
   <p><span class="print-book-callout-head">Note</span>  Executing this code will initiate the download of the medium-sized GPT model, which has a storage requirement of approximately 1.42 gigabytes. This is roughly three times larger than the storage space needed for the small model.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p133"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.7</span> Loading the pretrained model</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from gpt_download import download_and_load_gpt2
from chapter04 import GPTModel
from chapter05 import load_weights_into_gpt

BASE_CONFIG = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "drop_rate": 0.0,        # Dropout rate
    "qkv_bias": True         # Query-key-value bias
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

CHOOSE_MODEL = "gpt2-medium (355M)"
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")

settings, params = download_and_load_gpt2(
    model_size=model_size, 
    models_dir="gpt2"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval();</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>After executing the code, several files will be downloaded:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p135"> 
   <div class="code-area-container"> 
    <pre class="code-area">checkpoint: 100%|██████████| 77.0/77.0 [00:00&lt;00:00, 156kiB/s]
encoder.json: 100%|██████████| 1.04M/1.04M [00:02&lt;00:00, 467kiB/s]
hparams.json: 100%|██████████| 91.0/91.0 [00:00&lt;00:00, 198kiB/s]
model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G 
[05:50&lt;00:00, 4.05MiB/s]
model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00&lt;00:00, 18.1MiB/s]
model.ckpt.meta: 100%|██████████| 927k/927k [00:02&lt;00:00, 454kiB/s]
vocab.bpe: 100%|██████████| 456k/456k [00:01&lt;00:00, 283kiB/s]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>Now, let’s take a moment to assess the pretrained LLM’s performance on one of the validation tasks by comparing its output to the expected response. This will give us a baseline understanding of how well the model performs on an instruction-following task right out of the box, prior to fine-tuning, and will help us appreciate the effect of fine-tuning later on. We will use the first example from the validation set for this assessment:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p137"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
input_text = format_input(val_data[0])
print(input_text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>The content of the instruction is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p139"> 
   <div class="code-area-container"> 
    <pre class="code-area">Below is an instruction that describes a task. Write a response that 
appropriately completes the request.

### Instruction:
Convert the active sentence to passive: 'The chef cooks the meal every day.'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>Next we generate the model’s response using the same <code>generate</code> function we used to pretrain the model in chapter 5:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p141"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter05 import generate, text_to_token_ids, token_ids_to_text

token_ids = generate(
    model=model,
    idx=text_to_token_ids(input_text, tokenizer),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
    eos_id=50256,
)
generated_text = token_ids_to_text(token_ids, tokenizer)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>The <code>generate</code> function returns the combined input and output text. This behavior was previously convenient since pretrained LLMs are primarily designed as text-completion models, where the input and output are concatenated to create coherent and legible text. However, when evaluating the model’s performance on a specific task, we often want to focus solely on the model’s generated response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p143"> 
   <p>To isolate the model’s response text, we need to subtract the length of the input instruction from the start of the <code>generated_text</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p144"> 
   <div class="code-area-container"> 
    <pre class="code-area">response_text = generated_text[len(input_text):].strip()
print(response_text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>This code removes the input text from the beginning of the <code>generated_text</code>, leaving us with only the model’s generated response. The <code>strip()</code> function is then applied to remove any leading or trailing whitespace characters. The output is </p> 
  </div> 
  <div class="browsable-container listing-container" id="p146"> 
   <div class="code-area-container"> 
    <pre class="code-area">### Response:

The chef cooks the meal every day.

### Instruction:

Convert the active sentence to passive: 'The chef cooks the</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>This output shows that the pretrained model is not yet capable of correctly following the given instruction. While it does create a Response section, it simply repeats the original input sentence and part of the instruction, failing to convert the active sentence to passive voice as requested. So, let’s now implement the fine-tuning process to improve the model’s ability to comprehend and appropriately respond to such requests.</p> 
  </div> 
  <div class="readable-text" id="p148"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.6</span> Fine-tuning the LLM on instruction data</h2> 
  </div> 
  <div class="readable-text" id="p149"> 
   <p>It’s time to fine-tune the LLM for instructions (figure 7.16). We will take the loaded pretrained model in the previous section and further train it using the previously prepared instruction dataset prepared earlier in this chapter. We already did all the hard work when we <span class="aframe-location"/>implemented the instruction dataset processing at the beginning of this chapter. For the fine-tuning process itself, we can reuse the loss calculation and training functions implemented in chapter 5:</p> 
  </div> 
  <div class="browsable-container figure-container" id="p150">  
   <img alt="figure" src="../Images/7-16.png" width="1009" height="449"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.16</span> The three-stage process for instruction fine-tuning an LLM. In step 5, we train the pretrained model we previously loaded on the instruction dataset we prepared earlier.</h5>
  </div> 
  <div class="browsable-container listing-container" id="p151"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter05 import (
    calc_loss_loader,
    train_model_simple
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p>Before we begin training, let’s calculate the initial loss for the training and validation sets:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p153"> 
   <div class="code-area-container"> 
    <pre class="code-area">model.to(device)
torch.manual_seed(123)

with torch.no_grad():
    train_loss = calc_loss_loader(
        train_loader, model, device, num_batches=5
    )
    val_loss = calc_loss_loader(
        val_loader, model, device, num_batches=5
)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p>The initial loss values are as follows; as previously, our goal is to minimize the loss:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p155"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training loss: 3.825908660888672
Validation loss: 3.7619335651397705</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p156"> 
    <h5 class=" callout-container-h5 readable-text-h5">Dealing with hardware limitations</h5> 
   </div> 
   <div class="readable-text" id="p157"> 
    <p>Using and training a larger model like GPT-2 medium (355 million parameters) is more computationally intensive than the smaller GPT-2 model (124 million parameters). If you encounter problems due to hardware limitations, you can switch to the smaller model by changing <code>CHOOSE_MODEL</code> <code>=</code> <code>"gpt2-medium</code> <code>(355M)"</code> to <code>CHOOSE_MODEL</code> <code>=</code> <code>"gpt2-small</code> <code>(124M)"</code> (see section 7.5). Alternatively, to speed up the model training, consider using a GPU. The following supplementary section in this book’s code repository lists several options for using cloud GPUs: <a href="https://mng.bz/EOEq">https://mng.bz/EOEq</a>.</p> 
   </div> 
   <div class="readable-text" id="p158"> 
    <p>The following table provides reference run times for training each model on various devices, including CPUs and GPUs, for GPT-2. Running this code on a compatible GPU requires no code changes and can significantly speed up training. For the results shown in this chapter, I used the GPT-2 medium model and trained it on an A100 GPU.</p> 
   </div> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Model name 
       </div></th> 
      <th> 
       <div>
         Device 
       </div></th> 
      <th> 
       <div>
         Run time for two epochs 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  gpt2-medium (355M) <br/></td> 
      <td>  CPU (M3 MacBook Air) <br/></td> 
      <td>  15.78 minutes <br/></td> 
     </tr> 
     <tr> 
      <td>  gpt2-medium (355M) <br/></td> 
      <td>  GPU (NVIDIA L4) <br/></td> 
      <td>  1.83 minutes <br/></td> 
     </tr> 
     <tr> 
      <td>  gpt2-medium (355M) <br/></td> 
      <td>  GPU (NVIDIA A100) <br/></td> 
      <td>  0.86 minutes <br/></td> 
     </tr> 
     <tr> 
      <td>  gpt2-small (124M) <br/></td> 
      <td>  CPU (M3 MacBook Air) <br/></td> 
      <td>  5.74 minutes <br/></td> 
     </tr> 
     <tr> 
      <td>  gpt2-small (124M) <br/></td> 
      <td>  GPU (NVIDIA L4) <br/></td> 
      <td>  0.69 minutes <br/></td> 
     </tr> 
     <tr> 
      <td>  gpt2-small (124M) <br/></td> 
      <td>  GPU (NVIDIA A100) <br/></td> 
      <td>  0.39 minutes <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>With the model and data loaders prepared, we can now proceed to train the model. The code in listing 7.8 sets up the training process, including initializing the optimizer, setting the number of epochs, and defining the evaluation frequency and starting context to evaluate generated LLM responses during training based on the first validation set instruction (<code>val_data[0]</code>) we looked at in section 7.5.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p161"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.8</span> Instruction fine-tuning the pretrained LLM</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import time

start_time = time.time()
torch.manual_seed(123)
optimizer = torch.optim.AdamW(
    model.parameters(), lr=0.00005, weight_decay=0.1
)
num_epochs = 2

train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context=format_input(val_data[0]), tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>The following output displays the training progress over two epochs, where a steady decrease in losses indicates improving ability to follow instructions and generate appropriate responses:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p163"> 
   <div class="code-area-container"> 
    <pre class="code-area">Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626
Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103
Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944
Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906
...
Ep 1 (Step 000115): Train loss 0.520, Val loss 0.665
Below is an instruction that describes a task. Write a response that 
appropriately completes the request.  ### Instruction: Convert the 
active sentence to passive: 'The chef cooks the meal every day.' 
### Response: The meal is prepared every day by the chef.&lt;|endoftext|&gt;
The following is an instruction that describes a task. 
Write a response that appropriately completes the request.  
### Instruction: Convert the active sentence to passive:
Ep 2 (Step 000120): Train loss 0.438, Val loss 0.670
Ep 2 (Step 000125): Train loss 0.453, Val loss 0.685
Ep 2 (Step 000130): Train loss 0.448, Val loss 0.681
Ep 2 (Step 000135): Train loss 0.408, Val loss 0.677
...
Ep 2 (Step 000230): Train loss 0.300, Val loss 0.657
Below is an instruction that describes a task. Write a response 
that appropriately completes the request.  ### Instruction: 
Convert the active sentence to passive: 'The chef cooks the meal 
every day.'  ### Response: The meal is cooked every day by the 
chef.&lt;|endoftext|&gt;The following is an instruction that describes 
a task. Write a response that appropriately completes the request.  
### Instruction: What is the capital of the United Kingdom
Training completed in 0.87 minutes.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p164"> 
   <p>The training output shows that the model is learning effectively, as we can tell based on the consistently decreasing training and validation loss values over the two epochs. This result suggests that the model is gradually improving its ability to understand and follow the provided instructions. (Since the model demonstrated effective learning within these two epochs, extending the training to a third epoch or more is not essential and may even be counterproductive as it could lead to increased overfitting.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p165"> 
   <p>Moreover, the generated responses at the end of each epoch let us inspect the model’s progress in correctly executing the given task in the validation set example. In this case, the model successfully converts the active sentence <code>"The</code> <code>chef</code> <code>cooks</code> <code>the</code> <code>meal</code> <code>every</code> <code>day."</code> into its passive voice counterpart: <code>"The</code> <code>meal</code> <code>is</code> <code>cooked</code> <code>every</code> <code>day</code> <code>by</code> <code>the</code> <code>chef."</code></p> 
  </div> 
  <div class="readable-text intended-text" id="p166"> 
   <p>We will revisit and evaluate the response quality of the model in more detail later. For now, let’s examine the training and validation loss curves to gain additional insights into the model’s learning process. For this, we use the same <code>plot_losses</code> function we used for pretraining:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p167"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter05 import plot_losses
epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>From the loss plot shown in figure 7.17, we can see that the model’s performance on both the training and validation sets improves substantially over the course of training. The rapid decrease in losses during the initial phase indicates that the model quickly learns meaningful patterns and representations from the data. Then, as training progresses to the second epoch, the losses continue to decrease but at a slower rate, suggesting that the model is fine-tuning its learned representations and converging to a stable solution. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p169">  
   <img alt="figure" src="../Images/7-17.png" width="542" height="309"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.17</span> The training and validation loss trends over two epochs. The solid line represents the training loss, showing a sharp decrease before stabilizing, while the dotted line represents the validation loss, which follows a similar pattern. </h5>
  </div> 
  <div class="readable-text" id="p170"> 
   <p>While the loss plot in figure 7.17 indicates that the model is training effectively, the most crucial aspect is its performance in terms of response quality and correctness. So, next, let’s extract the responses and store them in a format that allows us to evaluate and quantify the response quality.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p171"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.3 Fine-tuning on the original Alpaca dataset </h5> 
   </div> 
   <div class="readable-text" id="p172"> 
    <p>The Alpaca dataset, by researchers at Stanford, is one of the earliest and most popular openly shared instruction datasets, consisting of 52,002 entries. As an alternative to the <code>instruction-data.json</code> file we use here, consider fine-tuning an LLM on this dataset. The dataset is available at <a href="https://mng.bz/NBnE">https://mng.bz/NBnE</a>.</p> 
   </div> 
   <div class="readable-text" id="p173"> 
    <p>This dataset contains 52,002 entries, which is approximately 50 times more than those we used here, and most entries are longer. Thus, I highly recommend using a GPU to conduct the training, which will accelerate the fine-tuning process. If you encounter out-of-memory errors, consider reducing the <code>batch_size</code> from 8 to 4, 2, or even 1. Lowering the <code>allowed_max_length</code> from 1,024 to 512 or 256 can also help manage memory problems.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p174"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.7</span> Extracting and saving responses</h2> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>Having fine-tuned the LLM on the training portion of the instruction dataset, we are now ready to evaluate its performance on the held-out test set. First, we extract the model-generated responses for each input in the test dataset and collect them for manual analysis, and then we evaluate the LLM to quantify the quality of the responses, as highlighted in figure 7.18.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p176">  
   <img alt="figure" src="../Images/7-18.png" width="1100" height="859"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.18</span> The three-stage process for instruction fine-tuning the LLM. In the first two steps of stage 3, we extract and collect the model responses on the held-out test dataset for further analysis and then evaluate the model to quantify the performance of the instruction-fine-tuned LLM.</h5>
  </div> 
  <div class="readable-text" id="p177"> 
   <p>To complete the response instruction step, we use the <code>generate</code> function. We then print the model responses alongside the expected test set answers for the first three test set entries, presenting them side by side for comparison:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p178"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)

for entry in test_data[:3]:     <span class="aframe-location"/> #1
    input_text = format_input(entry)
    token_ids = generate(              <span class="aframe-location"/> #2
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)

    response_text = (
        generated_text[len(input_text):]
        .replace("### Response:", "")
        .strip()
    )
    print(input_text)
    print(f"\nCorrect response:\n&gt;&gt; {entry['output']}")
    print(f"\nModel response:\n&gt;&gt; {response_text.strip()}")
    print("-------------------------------------")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Iterates over the first three test set samples
     <br/>#2 Uses the generate function imported in section 7.5
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>As mentioned earlier, the <code>generate</code> function returns the combined input and output text, so we use slicing and the <code>.replace()</code> method on the <code>generated_text</code> contents to extract the model’s response. The instructions, followed by the given test set response and model response, are shown next.</p> 
   <img alt="figure" src="../Images/7_prompt.png" width="1100" height="1241"/> 
  </div> 
  <div class="readable-text" id="p180"> 
   <p>As we can see based on the test set instructions, given responses, and the model’s responses, the model performs relatively well. The answers to the first and last instructions are clearly correct, while the second answer is close but not entirely accurate. The model answers with “cumulus cloud” instead of “cumulonimbus,” although it’s worth noting that cumulus clouds can develop into cumulonimbus clouds, which are capable of producing thunderstorms.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>Most importantly, model evaluation is not as straightforward as it is for classification fine-tuning, where we simply calculate the percentage of correct spam/non-spam class labels to obtain the classification’s accuracy. In practice, instruction-fine-tuned LLMs such as chatbots are evaluated via multiple approaches:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p182"> Short-answer and multiple-choice benchmarks, such as Measuring Massive Multitask Language Understanding (MMLU; <a href="https://arxiv.org/abs/2009.03300">https://arxiv.org/abs/2009.03300</a>), which test the general knowledge of a model. </li> 
   <li class="readable-text" id="p183"> Human preference comparison to other LLMs, such as LMSYS chatbot arena (<a href="https://arena.lmsys.org">https://arena.lmsys.org</a>). </li> 
   <li class="readable-text" id="p184"> Automated conversational benchmarks, where another LLM like GPT-4 is used to evaluate the responses, such as AlpacaEval (<a href="https://tatsu-lab.github.io/alpaca_eval/">https://tatsu-lab.github.io/alpaca_eval/</a>). </li> 
  </ul> 
  <div class="readable-text" id="p185"> 
   <p>In practice, it can be useful to consider all three types of evaluation methods: multiple-choice question answering, human evaluation, and automated metrics that measure conversational performance. However, since we are primarily interested in assessing conversational performance rather than just the ability to answer multiple-choice questions, human evaluation and automated metrics may be more relevant.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p186"> 
    <h5 class=" callout-container-h5 readable-text-h5">Conversational performance</h5> 
   </div> 
   <div class="readable-text" id="p187"> 
    <p>Conversational performance of LLMs refers to their ability to engage in human-like communication by understanding context, nuance, and intent. It encompasses skills such as providing relevant and coherent responses, maintaining consistency, and adapting to different topics and styles of interaction.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>Human evaluation, while providing valuable insights, can be relatively laborious and time-consuming, especially when dealing with a large number of responses. For instance, reading and assigning ratings to all 1,100 responses would require a significant amount of effort.</p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>So, considering the scale of the task at hand, we will implement an approach similar to automated conversational benchmarks, which involves evaluating the responses automatically using another LLM. This method will allow us to efficiently assess the quality of the generated responses without the need for extensive human involvement, thereby saving time and resources while still obtaining meaningful performance indicators.</p> 
  </div> 
  <div class="readable-text intended-text" id="p190"> 
   <p>Let’s employ an approach inspired by AlpacaEval, using another LLM to evaluate our fine-tuned model’s responses. However, instead of relying on a publicly available benchmark dataset, we use our own custom test set. This customization allows for a more targeted and relevant assessment of the model’s performance within the context of our intended use cases, represented in our instruction dataset.</p> 
  </div> 
  <div class="readable-text intended-text" id="p191"> 
   <p>To prepare the responses for this evaluation process, we append the generated model responses to the <code>test_set</code> dictionary and save the updated data as an <code>"instruction-data-with-response.json"</code> file for record keeping. Additionally, by saving this file, we can easily load and analyze the responses in separate Python sessions later on if needed.</p> 
  </div> 
  <div class="readable-text intended-text" id="p192"> 
   <p>The following code listing uses the <code>generate</code> method in the same manner as before; however, we now iterate over the entire <code>test_set</code>. Also, instead of printing the model responses, we add them to the <code>test_set</code> dictionary.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p193"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.9</span> Generating test set responses</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from tqdm import tqdm

for i, entry in tqdm(enumerate(test_data), total=len(test_data)):
    input_text = format_input(entry)

    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)

    response_text = (
        generated_text[len(input_text):]
        .replace("### Response:", "")
        .strip()
    )
    test_data[i]["model_response"] = response_text

with open("instruction-data-with-response.json", "w") as file:
    json.dump(test_data, file, indent=4)        <span class="aframe-location"/> #1</pre> 
    <div class="code-annotations-overlay-container">
     #1 indent for pretty-printing
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p194"> 
   <p>Processing the dataset takes about 1 minute on an A100 GPU and 6 minutes on an M3 MacBook Air: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p195"> 
   <div class="code-area-container"> 
    <pre class="code-area">100%|██████████| 110/110 [01:05&lt;00:00,  1.68it/s]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p196"> 
   <p>Let’s verify that the responses have been correctly added to the <code>test_set</code> dictionary by examining one of the entries:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p197"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(test_data[0])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>The output shows that the <code>model_response</code> has been added correctly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p199"> 
   <div class="code-area-container"> 
    <pre class="code-area">{'instruction': 'Rewrite the sentence using a simile.', 
 'input': 'The car is very fast.', 
 'output': 'The car is as fast as lightning.', 
 'model_response': 'The car is as fast as a bullet.'}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p200"> 
   <p>Finally, we save the model as <code>gpt2-medium355M-sft.pth</code> file to be able to reuse it in future projects:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p201"> 
   <div class="code-area-container"> 
    <pre class="code-area">import re

file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth"     <span class="aframe-location"/> #1
torch.save(model.state_dict(), file_name)
print(f"Model saved as {file_name}")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Removes white spaces and parentheses from file name
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p202"> 
   <p>The saved model can then be loaded via <code>model.load_state_dict(torch.load("gpt2 -medium355M-sft.pth")</code>).</p> 
  </div> 
  <div class="readable-text" id="p203"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.8</span> Evaluating the fine-tuned LLM</h2> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>Previously, we judged the performance of an instruction-fine-tuned model by looking at its responses on three examples of the test set. While this gives us a rough idea of how well the model performs, this method does not scale well to larger amounts of responses. So, we implement a method to automate the response evaluation of the fine-tuned LLM using another, larger LLM, as highlighted in figure 7.19.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p205">  
   <img alt="figure" src="../Images/7-19.png" width="1100" height="848"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.19</span> The three-stage process for instruction fine-tuning the LLM. In this last step of the instruction-fine-tuning pipeline, we implement a method to quantify the performance of the fine-tuned model by scoring the responses it generated for the test.</h5>
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>To evaluate test set responses in an automated fashion, we utilize an existing instruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI. This model can be run locally using the open source Ollama application (<a href="https://ollama.com">https://ollama.com</a>).</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p207"> 
   <p><span class="print-book-callout-head">NOTE</span>  Ollama is an efficient application for running LLMs on a laptop. It serves as a wrapper around the open source llama.cpp library (<a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>), which implements LLMs in pure C/C++ to maximize efficiency. However, Ollama is only a tool for generating text using LLMs (inference) and does not support training or fine-tuning LLMs.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p208"> 
    <h5 class=" callout-container-h5 readable-text-h5">Using larger LLMs via web APIs </h5> 
   </div> 
   <div class="readable-text" id="p209"> 
    <p>The 8-billion-parameter Llama 3 model is a very capable LLM that runs locally. However, it’s not as capable as large proprietary LLMs such as GPT-4 offered by OpenAI. For readers interested in exploring how to utilize GPT-4 through the OpenAI API to assess generated model responses, an optional code notebook is available within the supplementary materials accompanying this book at <a href="https://mng.bz/BgEv">https://mng.bz/BgEv</a>.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p210"> 
   <p>To execute the following code, install Ollama by visiting <a href="https://ollama.com">https://ollama.com</a>and follow the provided instructions for your operating system:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p211"> <em>For macOS and Windows users</em>—Open the downloaded Ollama application. If prompted to install command-line usage, select Yes. </li> 
   <li class="readable-text" id="p212"> <em>For Linux users</em>—Use the installation command available on the Ollama website. </li> 
  </ul> 
  <div class="readable-text" id="p213"> 
   <p>Before implementing the model evaluation code, let’s first download the Llama 3 model and verify that Ollama is functioning correctly by using it from the command-line terminal. To use Ollama from the command line, you must either start the Ollama application or run <code>ollama</code> <code>serve</code> in a separate terminal, as shown in figure 7.20.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p214">  
   <img alt="figure" src="../Images/7-20.png" width="1012" height="609"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.20</span> Two options for running Ollama. The left panel illustrates starting Ollama using <code>ollama</code> <code>serve</code>. The right panel shows a second option in macOS, running the Ollama application in the background instead of using the <code>ollama</code> <code>serve</code> command to start the application.</h5>
  </div> 
  <div class="readable-text" id="p215"> 
   <p>With the Ollama application or <code>ollama</code> <code>serve</code> running in a different terminal, execute the following command on the command line (not in a Python session) to try out the 8-billion-parameter Llama 3 model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p216"> 
   <div class="code-area-container"> 
    <pre class="code-area">ollama run llama3</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p217"> 
   <p>The first time you execute this command, this model, which takes up 4.7 GB of storage space, will be automatically downloaded. The output looks like the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container"> 
    <pre class="code-area">pulling manifest
pulling 6a0746a1ec1a... 100% |████████████████| 4.7 GB
pulling 4fa551d4f938... 100% |████████████████|  12 KB
pulling 8ab4849b038c... 100% |████████████████|  254 B
pulling 577073ffcc6c... 100% |████████████████|  110 B
pulling 3f8eb4da87fa... 100% |████████████████|  485 B
verifying sha256 digest
writing manifest
removing any unused layers
success</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p219"> 
    <h5 class=" callout-container-h5 readable-text-h5">Alternative Ollama models </h5> 
   </div> 
   <div class="readable-text" id="p220"> 
    <p>The <code>llama3</code> in the <code>ollama</code> <code>run</code> <code>llama3</code> command refers to the instruction-fine-tuned 8-billion-parameter Llama 3 model. Using Ollama with the <code>llama3</code> model requires approximately 16 GB of RAM. If your machine does not have sufficient RAM, you can try using a smaller model, such as the 3.8-billion-parameter <code>phi3</code> model via <code>ollama</code> <code>run</code> <code>llama3</code>, which only requires around 8 GB of RAM.</p> 
   </div> 
   <div class="readable-text" id="p221"> 
    <p>For more powerful computers, you can also use the larger 70-billion-parameter Llama 3 model by replacing <code>llama3</code> with <code>llama3:70b</code>. However, this model requires significantly more computational resources.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p222"> 
   <p>Once the model download is complete, we are presented with a command-line interface that allows us to interact with the model. For example, try asking the model, “What do llamas eat?”</p> 
  </div> 
  <div class="browsable-container listing-container" id="p223"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;&gt; What do llamas eat?
Llamas are ruminant animals, which means they have a four-chambered
stomach and eat plants that are high in fiber. In the wild, 
llamas typically feed on:

1. Grasses: They love to graze on various types of grasses, including tall
grasses, wheat, oats, and barley.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p224"> 
   <p>Note that the response you see might differ since Ollama is not deterministic as of this writing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p225"> 
   <p>You can end this <code>ollama</code> <code>run</code> <code>llama3</code> session using the input <code>/bye</code>. However, make sure to keep the <code>ollama</code> <code>serve</code> command or the Ollama application running for the remainder of this chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p226"> 
   <p>The following code verifies that the Ollama session is running properly before we use Ollama to evaluate the test set responses:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p227"> 
   <div class="code-area-container"> 
    <pre class="code-area">import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

ollama_running = check_if_running("ollama")

if not ollama_running:
    raise RuntimeError(
        "Ollama not running. Launch ollama before proceeding."
)
print("Ollama running:", check_if_running("ollama"))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p228"> 
   <p>Ensure that the output from executing the previous code displays <code>Ollama</code> <code>running:</code> <code>True</code>. If it shows <code>False</code>, verify that the <code>ollama</code> <code>serve</code> command or the Ollama application is actively running.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p229"> 
    <h5 class=" callout-container-h5 readable-text-h5">Running the code in a new Python session </h5> 
   </div> 
   <div class="readable-text" id="p230"> 
    <p>If you already closed your Python session or if you prefer to execute the remaining code in a different Python session, use the following code, which loads the instruction and response data file we previously created and redefines the <code>format_input</code> function we used earlier (the <code>tqdm</code> progress bar utility is used later): </p> 
   </div> 
   <div class="browsable-container listing-container" id="p231"> 
    <div class="code-area-container"> 
     <pre class="code-area">import json
from tqdm import tqdm

file_path = "instruction-data-with-response.json"
with open(file_path, "r") as file:
    test_data = json.load(file)

def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )


    input_text = (
        f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""
    )
    return instruction_text + input_text</pre>  
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>An alternative to the <code>ollama</code> <code>run</code> command for interacting with the model is through its REST API using Python. The <code>query_model</code> function shown in the following listing demonstrates how to use the API.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p233"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.10</span> Querying a local Ollama model</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import urllib.request

def query_model(
    prompt, 
    model="llama3", 
    url="http://localhost:11434/api/chat"
):
    data = {            <span class="aframe-location"/> #1
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "options": {        <span class="aframe-location"/> #2
            "seed": 123,
            "temperature": 0,
            "num_ctx": 2048
        }
    }


    payload = json.dumps(data).encode("utf-8")   <span class="aframe-location"/> #3
    request = urllib.request.Request(                      <span class="aframe-location"/> #4
        url,                                                #4
        data=payload,                                       #4
        method="POST"                                       #4
    ) #4

    request.add_header("Content-Type", "application/json")   #4

    response_data = ""
    with urllib.request.urlopen(request) as response:  <span class="aframe-location"/> #5
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]

    return response_data</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates the data payload as a dictionary
     <br/>#2 Settings for deterministic responses
     <br/>#3 Converts the dictionary to a JSON-formatted string and encodes it to bytes
     <br/>#4 Creates a request object, setting the method to POST and adding necessary headers
     <br/>#5 Sends the request and captures the response
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p234"> 
   <p>Before running the subsequent code cells in this notebook, ensure that Ollama is still running. The previous code cells should print <code>"Ollama</code> <code>running:</code> <code>True"</code> to confirm that the model is active and ready to receive requests.</p> 
  </div> 
  <div class="readable-text intended-text" id="p235"> 
   <p>The following is an example of how to use the <code>query_model</code> function we just implemented:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p236"> 
   <div class="code-area-container"> 
    <pre class="code-area">model = "llama3"
result = query_model("What do Llamas eat?", model)
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p237"> 
   <p>The resulting response is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p238"> 
   <div class="code-area-container"> 
    <pre class="code-area">Llamas are ruminant animals, which means they have a four-chambered 
stomach that allows them to digest plant-based foods. Their diet 
typically consists of:

1. Grasses: Llamas love to graze on grasses, including tall grasses, 
short grasses, and even weeds.
...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p239"> 
   <p>Using the <code>query_model</code> function defined earlier, we can evaluate the responses generated by our fine-tuned model that prompts the Llama 3 model to rate our fine-tuned model’s responses on a scale from 0 to 100 based on the given test set response as reference. </p> 
  </div> 
  <div class="readable-text intended-text" id="p240"> 
   <p>First, we apply this approach to the first three examples from the test set that we previously examined:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p241"> 
   <div class="code-area-container"> 
    <pre class="code-area">for entry in test_data[:3]:
    prompt = (
        f"Given the input `{format_input(entry)}` "
        f"and correct output `{entry['output']}`, "
        f"score the model response `{entry['model_response']}`"
        f" on a scale from 0 to 100, where 100 is the best score. "
    )
    print("\nDataset response:")
    print("&gt;&gt;", entry['output'])
    print("\nModel response:")
    print("&gt;&gt;", entry["model_response"])
    print("\nScore:")
    print("&gt;&gt;", query_model(prompt))
    print("\n-------------------------")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p242"> 
   <p>This code prints outputs similar to the following (as of this writing, Ollama is not fully deterministic, so the generated texts may vary):</p> 
   <img alt="figure" src="../Images/7_prompt2.png" width="1100" height="1502"/> 
  </div> 
  <div class="readable-text" id="p243"> 
   <p>The generated responses show that the Llama 3 model provides reasonable evaluations and is capable of assigning partial points when a model’s answer is not entirely correct. For instance, if we consider the evaluation of the “cumulus cloud” answer, the model acknowledges the partial correctness of the response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p244"> 
   <p>The previous prompt returns highly detailed evaluations in addition to the score. We can modify the prompt to just generate integer scores ranging from 0 to 100, where 100 represents the best possible score. This modification allows us to calculate an average score for our model, which serves as a more concise and quantitative assessment of its performance. The <code>generate_model_scores</code> function shown in the following listing uses a modified prompt telling the model to <code>"Respond</code> <code>with</code> <code>the</code> <code>integer</code> <code>number</code> <code>only."</code></p> 
  </div> 
  <div class="browsable-container listing-container" id="p245"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 7.11</span> Evaluating the instruction fine-tuning LLM</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        prompt = (
            f"Given the input `{format_input(entry)}` "
            f"and correct output `{entry['output']}`, "
            f"score the model response `{entry[json_key]}`"
            f" on a scale from 0 to 100, where 100 is the best score. "
            f"Respond with the integer number only."  <span class="aframe-location"/> #1
        )
        score = query_model(prompt, model)
        try:
            scores.append(int(score))
        except ValueError:
            print(f"Could not convert score: {score}")
            continue

    return scores</pre> 
    <div class="code-annotations-overlay-container">
     #1 Modified instruction line to only return the score
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p246"> 
   <p>Let’s now apply the <code>generate_model_scores</code> function to the entire <code>test_data</code> set, which takes about 1 minute on a M3 Macbook Air:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p247"> 
   <div class="code-area-container"> 
    <pre class="code-area">scores = generate_model_scores(test_data, "model_response")
print(f"Number of scores: {len(scores)} of {len(test_data)}")
print(f"Average score: {sum(scores)/len(scores):.2f}\n")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p248"> 
   <p>The results are as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p249"> 
   <div class="code-area-container"> 
    <pre class="code-area">Scoring entries: 100%|████████████████████████| 110/110 
[01:10&lt;00:00,  1.56it/s]
Number of scores: 110 of 110
Average score: 50.32</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p250"> 
   <p>The evaluation output shows that our fine-tuned model achieves an average score above 50, which provides a useful benchmark for comparison against other models or for experimenting with different training configurations to improve the model’s performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p251"> 
   <p>It’s worth noting that Ollama is not entirely deterministic across operating systems at the time of this writing, which means that the scores you obtain might vary slightly from the previous scores. To obtain more robust results, you can repeat the evaluation multiple times and average the resulting scores.</p> 
  </div> 
  <div class="readable-text intended-text" id="p252"> 
   <p>To further improve our model’s performance, we can explore various strategies, such as</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p253"> Adjusting the hyperparameters during fine-tuning, such as the learning rate, batch size, or number of epochs </li> 
   <li class="readable-text" id="p254"> Increasing the size of the training dataset or diversifying the examples to cover a broader range of topics and styles </li> 
   <li class="readable-text" id="p255"> Experimenting with different prompts or instruction formats to guide the model’s responses more effectively </li> 
   <li class="readable-text" id="p256"> Using a larger pretrained model, which may have greater capacity to capture complex patterns and generate more accurate responses </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p257"> 
   <p><span class="print-book-callout-head">NOTE</span>  For reference, when using the methodology described herein, the Llama 3 8B base model, without any fine-tuning, achieves an average score of 58.51 on the test set. The Llama 3 8B instruct model, which has been fine-tuned on a general instruction-following dataset, achieves an impressive average score of 82.6.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p258"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.4 Parameter-efficient fine-tuning with LoRA </h5> 
   </div> 
   <div class="readable-text" id="p259"> 
    <p>To instruction fine-tune an LLM more efficiently, modify the code in this chapter to use the low-rank adaptation method (LoRA) from appendix E. Compare the training run time and model performance before and after the modification.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p260"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.9</span> Conclusions</h2> 
  </div> 
  <div class="readable-text" id="p261"> 
   <p>This chapter marks the conclusion of our journey through the LLM development cycle. We have covered all the essential steps, including implementing an LLM architecture, pretraining an LLM, and fine-tuning it for specific tasks, as summarized in figure 7.21. Let’s discuss some ideas for what to look into next.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p262">  
   <img alt="figure" src="../Images/7-21.png" width="1012" height="549"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.21</span> The three main stages of coding an LLM. </h5>
  </div> 
  <div class="readable-text" id="p263"> 
   <h3 class=" readable-text-h3"><span class="num-string">7.9.1</span> What’s next?</h3> 
  </div> 
  <div class="readable-text" id="p264"> 
   <p>While we covered the most essential steps, there is an optional step that can be performed after instruction fine-tuning: preference fine-tuning. Preference fine-tuning is particularly useful for customizing a model to better align with specific user preferences. If you are interested in exploring this further, see the <code>04_preference-tuning-with-dpo</code> folder in this book’s supplementary GitHub repository at <a href="https://mng.bz/dZwD">https://mng.bz/dZwD</a>.</p> 
  </div> 
  <div class="readable-text" id="p265"> 
   <p>In addition to the main content covered in this book, the GitHub repository also contains a large selection of bonus material that you may find valuable. To learn more about these additional resources, visit the Bonus Material section on the repository’s README page: <a href="https://mng.bz/r12g">https://mng.bz/r12g</a>.</p> 
  </div> 
  <div class="readable-text" id="p266"> 
   <h3 class=" readable-text-h3"><span class="num-string">7.9.2</span> Staying up to date in a fast-moving field</h3> 
  </div> 
  <div class="readable-text" id="p267"> 
   <p>The fields of AI and LLM research are evolving at a rapid (and, depending on who you ask, exciting) pace. One way to keep up with the latest advancements is to explore recent research papers on arXiv at <a href="https://arxiv.org/list/cs.LG/recent">https://arxiv.org/list/cs.LG/recent</a>. Additionally, many researchers and practitioners are very active in sharing and discussing the latest developments on social media platforms like X (formerly Twitter) and Reddit. The subreddit r/LocalLLaMA, in particular, is a good resource for connecting with the community and staying informed about the latest tools and trends. I also regularly share insights and write about the latest in LLM research on my blog, available at <a href="https://magazine.sebastianraschka.com">https://magazine.sebastianraschka.com</a> and <a href="https://sebastianraschka.com/blog/">https://sebastianraschka.com/blog/</a>.</p> 
  </div> 
  <div class="readable-text" id="p268"> 
   <h3 class=" readable-text-h3"><span class="num-string">7.9.3</span> Final words</h3> 
  </div> 
  <div class="readable-text" id="p269"> 
   <p>I hope you have enjoyed this journey of implementing an LLM from the ground up and coding the pretraining and fine-tuning functions from scratch. In my opinion, building an LLM from scratch is the most effective way to gain a deep understanding of how LLMs work. I hope that this hands-on approach has provided you with valuable insights and a solid foundation in LLM development.</p> 
  </div> 
  <div class="readable-text intended-text" id="p270"> 
   <p>While the primary purpose of this book is educational, you may be interested in utilizing different and more powerful LLMs for real-world applications. For this, I recommend exploring popular tools such as Axolotl (<a href="https://github.com/OpenAccess-AI-Collective/axolotl">https://github.com/OpenAccess-AI-Collective/axolotl</a>) or LitGPT (<a href="https://github.com/Lightning-AI/litgpt">https://github.com/Lightning-AI/litgpt</a>), which I am actively involved in developing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p271"> 
   <p>Thank you for joining me on this learning journey, and I wish you all the best in your future endeavors in the exciting field of LLMs and AI!</p> 
  </div> 
  <div class="readable-text" id="p272"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p273"> The instruction-fine-tuning process adapts a pretrained LLM to follow human instructions and generate desired responses. </li> 
   <li class="readable-text" id="p274"> Preparing the dataset involves downloading an instruction-response dataset, formatting the entries, and splitting it into train, validation, and test sets. </li> 
   <li class="readable-text" id="p275"> Training batches are constructed using a custom collate function that pads sequences, creates target token IDs, and masks padding tokens. </li> 
   <li class="readable-text" id="p276"> We load a pretrained GPT-2 medium model with 355 million parameters to serve as the starting point for instruction fine-tuning. </li> 
   <li class="readable-text" id="p277"> The pretrained model is fine-tuned on the instruction dataset using a training loop similar to pretraining. </li> 
   <li class="readable-text" id="p278"> Evaluation involves extracting model responses on a test set and scoring them (for example, using another LLM). </li> 
   <li class="readable-text" id="p279"> The Ollama application with an 8-billion-parameter Llama model can be used to automatically score the fine-tuned model’s responses on the test set, providing an average score to quantify performance. </li> 
  </ul>
 </div></div></body></html>