<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">appendix B</span></span> <span class="chapter-title-text">Responsible AI tools</span></h1> 
  </div> 
  <div class="readable-text" id="p2"> 
   <p>As generative AI models have become increasingly prevalent in enterprises, ensuring that they are developed and deployed responsibly is essential. Responsible AI (RAI) practices can help organizations build stakeholder trust, meet regulatory requirements, and avoid unintended consequences. Fortunately, many tools are available to support developers and architects in integrating RAI principles into their AI systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p3"> 
   <p>The following sections outline some of these tools and frameworks, which can help ensure transparency, fairness, interpretability, and security in AI.</p> 
  </div> 
  <div class="readable-text" id="p4"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_255"><span class="num-string">B.1</span> Model card</h2> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>A model card is a special type of documentation accompanying an AI model. It provides a standardized information set about the model’s purpose, performance, training data, ethical considerations, and more. It’s akin to a product data sheet, offering transparency and facilitating responsible AI practices.</p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>While it might seem odd to think of model cards as an RAI tool, they serve an important role in the context of RAI. Model cards are considered an essential RAI tool. They help stakeholders understand the capabilities and limitations of GenAI models, such as those based on GPT architectures, ensuring that these powerful tools are used ethically and effectively:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p7"> <em>Promoting transparency</em><em> </em>—They detail the model’s characteristics, limitations, and ideal use cases. </li> 
   <li class="readable-text" id="p8"> <em>Encouraging accountability</em><em> </em>—By documenting the model’s development process, model cards help ensure creators remain accountable for their AI systems. </li> 
   <li class="readable-text" id="p9"> <em>Facilitating informed use</em><em> </em>—They provide users with the necessary information to understand how the model should be used, thus preventing misuse. </li> 
  </ul> 
  <div class="readable-text" id="p10"> 
   <p>For large language models (LLMs), model cards typically include the following details:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p11"> <em>Model details</em><em> </em>—Information about the model’s architecture, size, training data, and training procedures </li> 
   <li class="readable-text" id="p12"> <em>Intended use</em><em> </em>—A description of the tasks the model is designed for and any limitations on its intended use </li> 
   <li class="readable-text" id="p13"> <em>Performance metrics</em><em> </em>—Benchmarks and evaluation results showing how the model performs on various tasks </li> 
   <li class="readable-text" id="p14"> <em>Ethical considerations</em><em> </em>—Any ethical concerns related to the model’s use, including potential biases </li> 
   <li class="readable-text" id="p15"> <em>Caveats and recommendations</em><em> </em>—Any warnings or suggestions for users of the model </li> 
  </ul> 
  <div class="readable-text" id="p16"> 
   <p>For example, OpenAI’s GPT-4 model card is called a system card and is 60 pages long. It calls out multiple risks related to safety challenges, such as hallucinations, harmful content, potential for risky emergent behaviors, overreliance, and so forth. More details on model cards can be found at <a href="https://mng.bz/M1gB">https://mng.bz/M1gB</a>.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_256"><span class="num-string">B.2</span> Transparency notes</h2> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>A transparency note is a document outlining the capabilities, limitations, and environmental impact of AI technology. It’s designed to clarify how an AI system works, which is crucial for responsible AI implementation. Transparency notes are practical tools for applying AI principles and guiding the responsible use and deployment of AI technologies. Enterprises should consider transparency notes as part of their AI development for a couple of reasons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p19"> <em>Understanding AI systems</em><em> </em>—They should understand that an AI system includes not just the technology but also the users, those affected by it, and the environment in which it’s deployed. </li> 
   <li class="readable-text" id="p20"> <em>Informed deployment</em><em> </em>—Transparency notes can help enterprises make informed decisions about developing and deploying AI systems, ensuring they are suitable for their intended use. </li> 
  </ul> 
  <div class="readable-text" id="p21"> 
   <p>Transparency notes are a helpful way to enhance transparency and accountability, which are important for the ethical creation and use of AI systems. For instance, Azure OpenAI Service’s transparency notes explain system features, boundaries, applications, and best practices to optimize system performance. You can access these transparency notes at <a href="https://mng.bz/aVKm">https://mng.bz/aVKm</a>.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_257"><span class="num-string">B.3</span> HAX Toolkit</h2> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>The HAX Toolkit (<a href="https://aka.ms/haxtoolkit">https://aka.ms/haxtoolkit</a>), developed by Microsoft Research in collaboration with Aether, Microsoft’s advisory body on AI ethics and effects in engineering and research, is a suite of practical tools designed to facilitate the creation of responsible human–AI experiences. It includes guidelines for human–AI interaction, a workbook, design patterns, a playbook, and a design library, all aimed at helping teams strategically create AI technologies that interact with people.</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>Enterprises should consider the HAX Toolkit a valuable resource when implementing RAI in their AI development process. It provides actionable guidance grounded in research and validated through practical application. The toolkit can help teams prioritize guidelines, plan resources, and address common design challenges. It also prepares for unforeseen errors, ensuring that AI systems are developed with a human-centered approach and aligned with responsible AI principles.</p> 
  </div> 
  <div class="readable-text intended-text" id="p25"> 
   <p>The HAX Toolkit addresses bias and fairness in AI systems by providing practical tools that translate knowledge of human–AI interaction into actionable guidance for AI creators. It helps teams prioritize guidelines and plan resources to address priorities, including bias and fairness. The toolkit includes</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p26"> <em>Guidelines for human–AI interaction</em><em> </em>—Best practices for how AI applications should interact with people </li> 
   <li class="readable-text" id="p27"> <em>HAX workbook</em><em> </em>—Helps teams prioritize guidelines and plan the time and resources needed to address high-priority items </li> 
   <li class="readable-text" id="p28"> <em>HAX design patterns</em><em> </em>—Offer flexible solutions for common problems in designing human–AI systems </li> 
   <li class="readable-text" id="p29"> <em>HAX playbook</em><em> </em>—Assists teams in identifying and planning for unforeseen errors, such as transcription errors or false positives, which can be sources of bias </li> 
   <li class="readable-text" id="p30"> <em>HAX design library</em><em> </em>—A searchable database of design patterns and implementation examples </li> 
  </ul> 
  <div class="readable-text" id="p31"> 
   <p>By utilizing these resources, teams can ensure that their AI systems are designed with a human-centered approach that considers fairness and mitigates bias throughout the AI application lifecycle. More details can be found at <a href="https://mng.bz/gAxv">https://mng.bz/gAxv</a>.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_258"><span class="num-string">B.4</span> Responsible AI Toolbox</h2> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>The Responsible AI Toolbox (<a href="https://mng.bz/5OWO">https://mng.bz/5OWO</a>) is a suite of tools provided by Microsoft to help operationalize RAI practices. It includes integrated tools and functionalities enabling users to assess their AI models and make more efficient user-facing decisions. The toolbox is designed to be flexible and model agnostic, which means it can be used with various AI models, including generative ones.</p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>The toolbox provides interfaces and libraries that empower AI system developers and stakeholders to develop and monitor AI more responsibly. Its capabilities can benefit enterprises looking to ensure that their use of generative AI aligns with RAI principles. One key area covered by it is the Responsible AI Dashboard, which combines various RAI capabilities to help practitioners optimize their machine learning (ML) models for fairness, explainability, and other desired characteristics.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>It’s designed to assist in assessing and debugging ML models, providing insights to help business decision-makers make more informed decisions. It combines several advanced tools in domains such as model reliability, interpretability, fairness, and compliance, giving a complete evaluation and troubleshooting of models for data-based decisions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>This is mainly used for conventional ML models rather than generative models such as LLMs. Still, sometimes these models work together in a workflow, enhancing each other, and in that situation, the RAI dashboard is useful. The dashboard helps with the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p37"> <em>Holistic assessment</em><em> </em>—It provides a single interface for various RAI tools, enabling a complete evaluation of ML models. </li> 
   <li class="readable-text" id="p38"> <em>Customizable interface</em><em> </em>—Users can tailor the dashboard to include only the relevant tools for their use case. </li> 
   <li class="readable-text" id="p39"> <em>Model debugging</em><em> </em>—It supports model debugging through stages of assessment, understanding, and mitigation, focusing on model reliability, interpretability, fairness, and compliance. </li> 
  </ul> 
  <div class="readable-text" id="p40"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_259"><span class="num-string">B.5</span> Learning Interpretability Tool (LIT)</h2> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>The Learning Interpretability Tool (LIT) is an open source tool (<a href="https://pair-code.github.io/lit">https://pair-code.github.io/lit</a>) designed to help us understand and interpret ML models. It supports various data types, including text, image, and tabular data, and can be used with different ML frameworks such as TensorFlow and PyTorch. </p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>LIT is part of the broader Responsible GenAI Toolkit (<a href="https://ai.google.dev/responsible">https://ai.google.dev/responsible</a>) from Google, designed to work on Google Cloud. LIT provides features such as</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p43"> <em>Local explanations</em><em> </em>—Produced through salience maps, attention visualization, and model predictions </li> 
   <li class="readable-text" id="p44"> <em>Aggregate analysis</em><em> </em>—Including custom metrics, slicing, binning, and visualization of embedding spaces </li> 
   <li class="readable-text" id="p45"> <em>Counterfactual generation</em><em> </em>—Used to create and evaluate new examples dynamically </li> 
  </ul> 
  <div class="readable-text" id="p46"> 
   <p>Enterprises can use LIT with generative AI applications to debug and analyze models, helping them understand why and how models behave the way they do. LIT can also help improve model outputs by using interpretability techniques such as sequence salience to analyze the impact of prompt designs on model outputs and test hypothesized improvements. By analyzing and documenting the behavior of generative models, enterprises can align with RAI principles.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_260"><span class="num-string">B.6</span> AI Fairness 360</h2> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>AI Fairness 360 (AIF360 Paper: <a href="https://arxiv.org/abs/1810.01943">https://arxiv.org/abs/1810.01943</a>) is an open source toolkit (<a href="https://github.com/Trusted-AI/AIF360">https://github.com/Trusted-AI/AIF360</a>) that helps users check, measure, and reduce discrimination and bias in ML models at any stage of the AI application lifecycle. It offers a full set of fairness metrics and bias mitigation algorithms created by the research community to deal with bias in AI systems. It can be part of the AI development process to monitor and reduce unwanted biases. With AIF360, organizations can quantify bias by using over 70 fairness metrics. After we measure bias, then AIF360 can help eliminate it by applying advanced algorithms to decrease bias in training data and models and ensure compliance by following ethical standards and regulations and showing efforts to address AI fairness. </p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>Finally, AIF360 helps with red-teaming, and enterprises build trust with users and stakeholders by ensuring their fair and equitable AI systems.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_261"><span class="num-string">B.7</span> C2PA</h2> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>The Coalition for Content Provenance and Authenticity (C2PA) is a project that develops technical standards to certify the source and history of media content online. It aims to prevent the spread of misleading information by providing a way to trace the origin of different types of media, such as images, videos, and documents. The standard is a collaboration between major tech companies, and it enables content creators to attach cryptographically signed metadata, C2PA manifests, to digital assets. This metadata can verify the content’s origin and any subsequent edits, increasing trust and authenticity in digital media.</p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>C2PA allows the creation of content credentials for a digital media file, which shows the creation process, including the creator’s identity and the tools used. These credentials are then secured with digital signatures to prevent tampering. When the media is shared, the embedded C2PA metadata enables others to check the authenticity of the media and any changes that have been made. A few open source tools, such as c2patool, can help with this task (<a href="https://github.com/contentauth/c2patool">https://github.com/contentauth/c2patool</a>).</p> 
  </div>
 </div></div></body></html>