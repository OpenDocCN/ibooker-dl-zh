<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">4 <a id="idTextAnchor002"/><a id="idTextAnchor003"/>Classical algorithms for tabular data</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-91"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">An introduction to Scikit-learn</li>

    <li class="co-summary-bullet">Exploring and processing features of the Airbnb NYC dataset</li>

    <li class="co-summary-bullet">Some classic machine learning techniques</li>
  </ul>

  <p class="body">Depending on the problem, classic machine learning algorithms are often the most practical approach to working with tabular data. With decades of research and practice behind these tools and algorithms, there is a rich palette of solutions to choose from.</p>

  <p class="body">In this chapter, we’ll cover essential algorithms in classical machine learning for making predictions using tabular data. We have focused on the linear models because they are still the most common solutions for both a challenging baseline and a solid and robust model in production. In addition, discussing linear models helps us build concepts and ideas that we can find in deep learning architectures and in more advanced machine learning algorithms, such as gradient-boosting decision trees (which will be one of the topics of the next chapter).</p>

  <p class="body">We’ll also give you a quick introduction to Scikit-learn, a powerful and versatile machine learning library that we’ll use to continue exploring the Airbnb NYC dataset. We’ll stay away from lengthy mathematical definitions and textbook details in favor of examples and practical recommendations for applying these models to tabular data problems.<a id="idTextAnchor004"/></p>

  <h2 class="fm-head" id="heading_id_3">4.1 Introducing Scikit-learn</h2>

  <p class="body">Scikit-learn is an open-source library for classic machine learning. It started in 2007 as a Google Summer of Code project by David Cournapeau, and it later became part of the SciKits (short for Scipy Toolkits: <a class="url" href="https://projects.scipy.org/scikits.html">https://projects.scipy.org/scikits.html</a>) until the INRIA (Institut National de Recherche en Informatique et en Automatique) and its foundation took the leadership of the project and of its development. We provide a short example of how Scikit-learn can quickly solve most machine learning problems. In our starting example,<a id="idIndexMarker001"/><a id="marker-92"/><a id="idIndexMarker002"/></p>

  <ol class="calibre7">
    <li class="fm-list-bullet">
      <p class="list">We create a synthetic dataset for a classification problem with a binary balanced target with half labels positive and half negative.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">We set up a pipeline standardizing the features and passing them to a logistic regression model, one of the simplest and most effective statistical-based machine learning algorithms for classification problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">We evaluate its performance using cross-validation.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Finally, assured by the cross-validation results that our work with the problem is fine, we train a model on all the available data.</p>
    </li>
  </ol>

  <p class="body">Listing 4.1 shows the complete listing and most of the features offered by Scikit-learn applied in a simple classification problem based on synthetically generated data. After creating the data, we define a pipeline, putting together statistical standardization with a basic model, logistic regression, for classification. Everything is first sent into a function that automatically estimates its performance on an evaluation metric, the accuracy, and the time its predictions are correct. Finally, figuring that its evaluated performances are suitable, we refit the same machine learning algorithm with all the dat<a id="idTextAnchor005"/>a.</p>

  <p class="fm-code-listing-caption">Listing 4.1 Example using Scikit-learn for a classification problem</p>
  <pre class="programlisting">import numpy as np
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
from sklearn.pipeline import Pipeline
 
X, y = make_classification(n_features=32,
                           n_redundant=0,
                           n_informative=24,
                           random_state=1,
                           n_clusters_per_class=1
                           )                            <span class="fm-combinumeral">①</span>
 
model = LogisticRegression()                            <span class="fm-combinumeral">②</span>
 
pipeline = Pipeline(
    [('processing', StandardScaler()),
     ('modeling', model)])                              <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=pipeline, 
                           X=X, 
                           y=y,
                           scoring="accuracy",
                           cv=5)                        <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
print(f"accuracy: {mean_cv:0.3f} ({std_cv:0.3f})")      <span class="fm-combinumeral">⑤</span>
 
model.fit(X, y) </pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates a synthetic dataset with specified characteristics</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates an instance of the LogisticRegression model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a pipeline that sequentially applies standard scaling and the logistic regression model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Performs a five-fold cross-validation using the defined pipeline, calculating accuracy scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the test accuracy scores from cross-validation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Fits the logistic regression model to the entire dataset X with corresponding labels y</p>

  <p class="body">The resulting output reports the obtained cross-validation accuracy on the classification:</p>
  <pre class="programlisting">accuracy 0.900 (0.032)</pre>

  <p class="body">The key point here is not the model but the procedure of doing things, which is standard for all tabular problems, whether you work with classical machine learning models or cutting-edge deep learning algorithms. Scikit-learn perfectly embeds such a procedure in its API, thus demonstrating a versatile and indispensable tool for tabular data problems. In the following sections, we will explore its characteristics and workings since we will reuse its procedures multiple times in our examples in the boo<a id="idTextAnchor006"/>k.<a id="marker-93"/></p>

  <h3 class="fm-head1" id="heading_id_4">4.1.1 Common features of Scikit-learn packages</h3>

  <p class="body">The key characteristics of the Scikit-learn package are<a id="idIndexMarker003"/><a id="idIndexMarker004"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">It offers a wide range of models for classification and regression, as well as functions for clustering, dimensionality reduction, preprocessing, and model selection. Most models will work in-memory when data is processed in the computer memory and out-of-core when data cannot fit into memory and is accessed from disk, allowing learning from data that exceeds your available computer memory.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Across its range of models, it presents a consistent API (class methods such as <code class="fm-code-in-text">fit</code>, <code class="fm-code-in-text">partial_fit</code>, <code class="fm-code-in-text">predict</code>, <code class="fm-code-in-text">predict_proba</code>, <code class="fm-code-in-text">transform</code>) that can be quickly learned and reused and that focuses exclusively on the transformations and processes necessary for a model to learn from data and predict from it. Scikit-learn’s API also offers automatic segregation of train and test data, the ability to chain and reuse its elements in a data pipeline, and accessibility of its parameters by simply inspecting the used class’s public attributes. <a id="idIndexMarker005"/><a id="idIndexMarker006"/><a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="idIndexMarker009"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Initially working on NumPy arrays and sparse matrices, Scikit-learn later extended to pandas DataFrames, enabling the practitioner to use them as inputs. In later versions (since version 1.1.3), you can retain key DataFrame characteristics, such as the name of columns and the transformations operated by Scikit-learn functions and classes. The support recently provided by Scikit-learn for pandas DataFrames has been long yearned for and is indeed essential for the topic of our book, tabular data.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">To define the working parameters of each Scikit-learn class, you just use standard Python types and classes (strings, floats, lists). In addition, the default values of all such parameters are already set to a proper value for you to create a baseline to start with and improve.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Thanks to a core group of top contributors (such as Andreas Mueller, Oliver Grisel, Fabian Pedregosa, Gael Varoquaux, and Gilles Loupe), Scikit-learn is in continuous development. There is constant debugging, and new functionalities and new models are added every time or old ones are excluded based on their robustness and scalability.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The package also presents extensive and easily accessible documentation with examples you can consult online (<a class="url" href="https://scikit-learn.org/stable/user_guide.html">https://scikit-learn.org/stable/user_guide.html</a>) or offline using the <code class="fm-code-in-text">help()</code> command.<a id="idIndexMarker010"/></p>
    </li>
  </ul>

  <p class="body">Depending on your operating system and installation preferences, if you want to install Scikit-learn, you just need to follow the instructions at <a class="url" href="https://scikit-learn.org/stable/install.html">https://scikit-learn.org/stable/install.html</a>. Together with pandas (<a class="url" href="https://pandas.pydata.org/">https://pandas.pydata.org/</a>), Scikit-learn is the core library for tabular data analysis and modeling. It offers a vast range of machine learning and statistical algorithms exclusively for structured data; in fact, the input has to be a pandas Dataframe, a NumPy array, or a sparse matrix to choose from. These algorithms are all well-established because the Scikit-learn team decided to include any algorithm in the package based on “at least three years since publication, 200+ citations, and wide use and usefulness” criteria. For more details on the algorithm inclusion requirements in Scikit-<a id="idTextAnchor007"/>learn, see <a class="url" href="https://mng.bz/8OMw">https://mng.bz/8OMw</a>.<a id="idTextAnchor008"/><a id="marker-94"/></p>

  <h3 class="fm-head1" id="heading_id_5">4.1.2 Common Scikit-learn interface</h3>

  <p class="body">The other key aspect of Scikit-learn that makes it so apt for tabular data problems is its current estimator API, the <i class="fm-italics">fit, predict/transform</i> interface. Such an estimator API is not just limited to Scikit-learn, and it is widely recognized as the most effective approach to handling training and test data. Many other projects have adopt<a id="idTextAnchor009"/>ed it (see <a class="url" href="https://mng.bz/EaWO">https://mng.bz/EaWO</a>). In fact, following Scikit-learn API, you automatically incorporate all the best practices in your data science project. In particular, you strictly separate training from validation and test data, an indispensable step for the success of any tabular data modeling, as we will demonstrate in the next section by reprising the Airbnb NYC dataset. <a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>

  <p class="body">Before delving into more practical examples, we provide some basics about Scikit-learn estimators. First, we distinguish four kinds of objects in Scikit-learn, each with a different interface. One class can implement multiple objects at the same time. Estimators are just one of them, though they are the most important ones because most of the Scikit-learn classes are estimators. In the following example, we define a machine learning estimator, a logistic regression (to be later discussed in this same chapter) for classification using the LogisticRegression class offered by Scikit-learn:</p>
  <pre class="programlisting">from sklearn.linear_model import LogisticRegression
model = LogisticRegression(C=1.0)</pre>

  <p class="body">An <i class="fm-italics">estimator</i> is an object focused on learning from data using the .fit method. It can be applied to supervised learning, relating data to a target, or to unsupervised learning where only data is involved: <a id="idIndexMarker014"/><a id="marker-95"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">For supervised learning: <code class="fm-code-in-text">estimator</code> = <code class="fm-code-in-text">estimator.fit(data, targets)</code> </p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">For unsupervised learning: <code class="fm-code-in-text">estimator</code> = <code class="fm-code-in-text">estimator.fit(data)</code> </p>
    </li>
  </ul>

  <p class="body">Under the hood, an estimator uses data to estimate some parameters that serve for later mapping back data to predictions or transforming it. The parameters and other information collected in the process are made available as object attributes.</p>

  <p class="body">Other typical Scikit-learn objects include the following:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Transformer</i> is an object focused on mapping a transformation on data:<a id="idIndexMarker015"/></p>
    </li>
  </ul>
  <pre class="programlistinge"><code class="fm-code-in-text">transformed_data = transformer.transform(data)</code></pre>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Predictor</i> is an object focussed on mapping a predicted response given some data by the methods <code class="fm-code-in-text">.predict</code> (predicting a general outcome) and <code class="fm-code-in-text">.predict_proba</code> (predicting a probability): <a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>
    </li>
  </ul>
  <pre class="programlistinge"><code class="fm-code-in-text">prediction = predictor.predict(data)</code> 
probability = predictor.predict_proba(data)</pre>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Model</i> is an object focused on providing the goodness of fit in respect of some data, typical of many statistical methods, by the method <code class="fm-code-in-text">.score</code>:<a id="idIndexMarker019"/><a id="idIndexMarker020"/></p>
    </li>
  </ul>
  <pre class="programlistinge">score = model.score(data)</pre>

  <p class="body">Whether you need an estimator or a transformer, each class is always instantiated by assigning it to a variable and specifying its parameters.</p>

  <p class="body">Under the hood, all these classes store parameters for their task. Some parameters are learned directly from the data and are commonly referred to as the weights or parameters of the models. You can think of these as the coefficients in a mathematical formulation: unknown values to be determined by data and computations. Others are given by the user at instantiation and can be configuration or initialization settings or parameters that influence how the algorithm learns from data. We usually refer to the latter ones as <i class="fm-italics">hyperparameters</i>. They tend to differ depending on the machine learning model; hence, we will discuss the most important ones when explaining each algorithm.<a id="idIndexMarker021"/></p>

  <p class="body">Configuration and setting parameters are similar for all the algorithms. For instance, the <code class="fm-code-in-text">random_state</code> setting helps to define a random seed for replicating the exact behavior of the model when using the same data. The results won’t change in different runs thanks to setting a random seed. The configuration parameter <code class="fm-code-in-text">n_jobs</code> will allow you to set how many CPU processors you want to be used in the computations, thus speeding up the time necessary for the model to complete its work but preventing you from doing other computer operations simultaneously. Depending on the algorithm, other available settings of the same kind may define the tolerance or the memory cache used by the model.<a id="idIndexMarker022"/><a id="marker-96"/><a id="idIndexMarker023"/></p>

  <p class="body">As we mentioned, some of these hyperparameters affect how the model operates and others how it learns from data. Let’s reprise our previous example:</p>
  <pre class="programlisting">from sklearn.linear_model import LogisticRegression
model = LogisticRegression(C=1.0)</pre>

  <p class="body">Among the hyperparameters that affect how the model learns from data, in our example, we can quote the C parameter, which, by taking different values, instructs the machine learning algorithm to apply some constraints in elaborating patterns from the data. We will address all the parameters to be fixed for each machine learning algorithm as we present them. It is important to notice that you usually set the hyperparameters at the time when the class is instantiated.</p>

  <p class="body">After the class instantiation, you usually provide the data to learn from and some limited instructio<a id="idTextAnchor010"/>n on how to deal with it—for instance, by giving different weights to each data example. At this stage, we say you train or fit the class on data. This phase is commonly mentioned as “fitting an estimator,” and it is done by providing data as a NumPy array, a sparse matrix, or a pandas DataFrame to the <code class="fm-code-in-text">.fit</code> method:<a id="idIndexMarker024"/></p>
  <pre class="programlisting">X = [[-1, -1], [-2, -1], [1, 1], [2, 1]]
y = [1, 1, 0, 0]
model.fit(X, y)</pre>

  <p class="body">Since training a model requires mapping an answer to some data, the <code class="fm-code-in-text">.fit</code> method inputs the data matrix and the answer vector. Such behavior is more than typical to models because some other Scikit-learn classes input data. The <code class="fm-code-in-text">.fit</code> method is also common to all transformative classes in Scikit-learn. For instance, fitting just data is typical of all the classes dealing with preprocess<a id="idTextAnchor011"/>ing, as you can check at <a class="url" href="https://mng.bz/N161">https://mng.bz/N161</a>, because transformations also require learning some information from features. For example, if you need to standardize data, you must first learn the standard deviation and the mean of each numeric feature in the data. The Scikit<a id="idTextAnchor012"/>-learn’s StandardScaler (<a class="url" href="https://mng.bz/DMgw">https://mng.bz/DMgw</a>) does exactly this:<a id="idIndexMarker025"/></p>
  <pre class="programlisting">from sklearn.preprocessing import StandardScaler
processing = StandardScaler().fit(X)</pre>

  <p class="body">In our example, we instantiate the class necessary for standardizing the data (StandardScaler), and we immediately afterward fit the data itself. Since the <code class="fm-code-in-text">.fit</code> method returns the instantiated class we used for the fitting procedure, you can safely get in return the class with all the learned parameters by combining these two steps. Such an approach will be helpful when building data pipelines and training models because it helps you separate the activities that learn something from data from the actions that apply what they learned to new data. This way, you won’t mistake mixing information from training and validation or test data.</p>

  <p class="body">Depending on the complexity of the underlying operations and the quantity of provided data, fitting a model or a function processing data may take some time. After the fitting has been completed, many more attributes will become available for you to use afterward, depending on the algorithm you used.</p>

  <p class="body">For a trained model, you will obtain a vector of responses of predictions based on any new data by applying the <code class="fm-code-in-text">.predict</code> method. This will work both for a classification or a regression problem:<a id="idIndexMarker026"/></p>
  <pre class="programlisting">X_test = [[-1, 1], [2, -1]]
model.predict(X_test)</pre>

  <p class="body">Suppose you are working on a classification; instead, you must get the probability that a certain class is a correct prediction for a new sample. In that case, you need to use the <code class="fm-code-in-text">.predict_proba</code> method, which is available only to certain models:<a id="idIndexMarker027"/><a id="marker-97"/></p>
  <pre class="programlisting">model.predict_proba(X_test)</pre>

  <p class="body">Classes that process data, instead, do not have a <code class="fm-code-in-text">.predict</code> method. Still, they use the <code class="fm-code-in-text">.transform</code> one, which returns transformed data if the class has been previously instantiated and fitted with some training data for learning the key parameters necessary for the transformation:<a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
  <pre class="programlisting">processing.transform(X)</pre>

  <p class="body">Since the transformation is often applied on the very same data that provided the key parameters, the <code class="fm-code-in-text">.fit_transform</code> method, which concatenates the two fit and transform phases, will result in a handy shortcut<a id="idTextAnchor013"/>:<a id="idIndexMarker030"/><a id="idIndexMarker031"/><a id="idIndexMarker032"/></p>
  <pre class="programlisting">processing.fit_transform(X)</pre>

  <h3 class="fm-head1" id="heading_id_6">4.1.3 Introduction to Scikit-learn pipelines</h3>

  <p class="body">You can also wrap a sequence of transformations and then predictions, selectively deciding what to transform and joining different sequences of transformations by using utility functions offered b<a id="idTextAnchor014"/>y Scikit-learn, such as<a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Pipeline</i> (<a class="url" href="https://mng.bz/lYx8">https<span id="idTextAnchor015">://mng.bz/lYx8</span></a>)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">ColumnTransformer</i> (<a class="url" href="https://mng.bz/BXM8"><span id="idTextAnchor016">https://mng.bz/BXM8</span></a>)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">FeatureUnion</i> (<a class="url" href="https://mng.bz/dX2O">https://mng.bz/dX2O</a>)</p>
    </li>
  </ul>

  <p class="body">The Pipeline command allows you to create a sequence of Scikit-learn classes that results in a series of transformations of the data, and it can end up with a model and its predictions. In this way, you can integrate any model with the transformations it requires for the data and deal with all the involved parameters at once—those of the transformations and those of the model itself. The Pipeline command is the core command to move tabular data from source to predictions in the Scikit-learn package. To set it, at instantiation time, you just need to provide a list of tuples, each containing the name of the step in the pipeline and the Scikit-learn class or model to be executed. Once instantiated, you can use it following the common API specifications of Scikit-learn (fit, transform/predict). The pipeline will execute all the predefined steps in sequence, returning the final result. Naturally, you can access, inspect, and tune the single steps of the pipeline sequence for better results and performance, but you can handle the pipeline as a single macro command.<a id="marker-98"/><a id="idIndexMarker035"/></p>

  <p class="body">However, tabular columns may have different types and require quite different transformation sequences, or you may have devised two different ways to process your data that you would like to combine. ColumnTrasformer and FeatureUnion are Scikit-learn commands that can help you in such occurrences. ColumnTrasformer allows you to apply a certain transformation or pipeline of transformations only on certain columns (which you can define by their name or position index in the columns’ sequence). The command takes a list of tuples, as the Pipeline command, but it requires a name for the transformation, a Scikit-learn class for executing it, and a list of column names or indexes to which the transformation should be applied. Since it is just a transformative command, its ideal usage is inside a pipeline, where its transformations can be part of the data feeding of a model. FeatureUnion, instead, is just an easy way to concatenate the results of two distinct pipelines. You may achieve the same result with a simp<a id="idTextAnchor017"/>le NumPy command such as <code class="fm-code-in-text">np.hstack</code> (<a class="url" href="https://mng.bz/rKJD">https://mng.bz/rKJD</a>). However, when using FeatureUnion you have the advantage that the command can fit into a Scikit-learn pipeline and hence automatically be used as part of the data feeding to the model.<a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>

  <p class="body">The modularity of operations and API consistency offered by Scikit-learn and its Pipeline, ColumnTrasformer, and FeatureUnion will allow you to easily create complex data transformations to be handled as a single command, thus making your code highly readable, compact, and easily maintainable. In the next section, we will return to the Airbnb NYC dataset we used. We will create a series of transforming sequences in Scikit-learn that will allow us to demonstrate how Scikit-learn and its pipeline functions are the right choices for tackling your tabular data problems. We will also point out how easily you can switch between the different options for machine learning with tabular d<a id="idTextAnchor018"/>ata thanks to a well-defined pipeline.<a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>

  <h2 class="fm-head" id="heading_id_7">4.2 Exploring and processing features of the Airbnb NYC dataset</h2>

  <p class="body">The previously introduced Airbnb NYC dataset is a perfect example for demonstrative purposes because it is a dataset representative of a real-world problem and because of its various types of columns. We will have to create and combine different pipelines to handle the different features, and the following chapters will give us a chance to present even more advanced processing techniques than the ones you can find in this chapter.<a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>

  <p class="body">For the moment, we will place the features we will deal with into a list named <code class="fm-code-in-text">excluding_list</code>. They are features, such as the latitude and longitude degrees or the data of the last review (<code class="fm-code-in-text">last_review</code>), which need special ad hoc processing. Also, the dataset presents a few possible columns that may act as targets: the price, the availability of the listed properties (<code class="fm-code-in-text">availability_365</code>), and the number of reviews (<code class="fm-code-in-text">number_of_reviews</code>). For our purposes, we prefer to use the price. Because it is a continuous set of values above zero, we can immediately use it as a regression target. In addition, by applying a split on the mean or the median, or binning the values into deciles, we can quickly turn the price variable into a binary or multiclass classification target. Apart from price, we use all the other features as predictive ones or for more advanced feature engineering.<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="marker-99"/></p>

  <p class="body">In the following subsection, we will demonstrate a step-by-step approach to exploring the dataset, filtering the dataset based on the useful columns, and setting up our target variables. In principle, we will follow the hints and examples provided in chapter 2 when discussing <i class="fm-italics">exploratory data analysis</i> (EDA). In the next section, we will take advantage of our discoveries and prepare suitable data pipelines that will be reused in the following paragraphs when revising the different <a id="idTextAnchor019"/>options for machine learning for tabular data.<a id="idIndexMarker047"/></p>

  <h3 class="fm-head1" id="heading_id_8">4.2.1 Dataset exploration</h3>

  <p class="body">As a first step in exploring the dataset, we import the relevant packages (NumPy and pandas), define the list of excluded features as well as separate lists for categorical and continuous features based on our prior knowledge built in the previous chapter, and load the data from our current working directory. The code to be executed is<a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>
  <pre class="programlisting">import numpy as np
import pandas as pd
excluding_list = ['price', 'id', 'latitude', 
                  'longitude', 'host_id', 
                  'last_review', 'name', 
                  'host_name']                       <span class="fm-combinumeral">①</span>
categorical = ['neighbourhood_group',
               'neighbourhood',
               'room_type']                          <span class="fm-combinumeral">②</span>
continuous = ['minimum_nights',
              'number_of_reviews',
              'reviews_per_month',
              'Calculated_host_listings_count']      <span class="fm-combinumeral">③</span>
data = pd.read_csv("./AB_NYC_2019.csv")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> List of column names to be excluded from the analysis</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> List of names of columns that likely represent categorical variables in the dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> List of names of columns that represent continuous numerical variables in the dataset</p>

  <p class="body">Once the code snippet has completed the loading of the data, we first check how many rows and columns have been returned in the data frame:</p>
  <pre class="programlisting">data.shape</pre>

  <p class="body">We will get 48,895 rows available—a fair number for a tabular problem, allowing us to use any available learning algorithm—and 16 columns. Since we are interested only in some of the columns—the ones we defined in the variables named categorical and continuous—we start by refining the classification of categorical features into low cardinality and high cardinality ones based on the number of unique values they have:</p>
  <pre class="programlisting">data[categorical].nunique()</pre>

  <p class="body">The command results in the following output:</p>
  <pre class="programlisting">neighbourhood_group      5
neighbourhood          221
room_type                3</pre>

  <p class="body"><a id="marker-100"/>Our standard approach when dealing with categorical features is to apply <i class="fm-italics">one-hot encoding</i>, creating one binary variable for each unique value in the original feature. However, by using one-hot encoding, features presenting over 20 unique values will result in an excessive number of columns in the dataset and data sparsity. You have sparsity in your data when your data is predominantly of zero values, which is a problem, especially for neural networks and generally for online algorithms because learning becomes more difficult. In chapter 6, we will present techniques, such as target encoding, to deal with features with too many unique values, called <i class="fm-italics">high cardinality categorical features</i>. For the examples in this chapter, we will separate the low from the high cardinality categorical features and process only the low cardinality ones:<a id="idIndexMarker050"/><a id="idIndexMarker051"/></p>
  <pre class="programlisting">low_card_categorical = ['neighbourhood_group', 'room_type']
high_card_categorical = ['neighbourhood']</pre>

  <p class="body">Next, having defined that (for the moment, we will be working only with numeric and low cardinality categorical features), we need to figure out if there are any missing cases in our data. The following command asks to flag true missing values and then computes a count of them across features:</p>
  <pre class="programlisting">data[low_card_categorical + continuous].isna().sum()</pre>

  <p class="body">We obtain the following result that points out a problem only with the <code class="fm-code-in-text">reviews_per_month</code> feature:<a id="idIndexMarker052"/></p>
  <pre class="programlisting">neighbourhood_group                   0
room_type                             0
minimum_nights                        0
number_of_reviews                     0
reviews_per_month                 10052
calculated_host_listings_count        0
availability_365                      0</pre>

  <p class="body">As we mentioned in chapter 2, dealing with missing values shouldn’t be an automated procedure; rather, it requires some reflection on the data scientist’s part to determine if there is some reason for them to be missing. In this case, it becomes evident that there is a processing problem with the data at the source because if you check the minimum value, this will result in a value above zero:</p>
  <pre class="programlisting">data.reviews_per_month.min()</pre>

  <p class="body">The minimum reported is 0.01. Here we have a missing value when there are not enough reviews to make statistics. Hence, we could replace the missing value on this feature with a zero value. Having filtered our features to be used for predictions and having checked missing values because most machine learning algorithms won’t work in the presence of missing input data, apart from a few such as the gradient boosting implementations XGBoost or LightGBM (discussed in the next chapter), we can proceed to check about our target. This part of EDA, <i class="fm-italics">target analysis</i>, is often overlooked, yet it is quite important because, in tabular problems, not all machine learning algorithms can handle the same kind of targets. For example, targets with many zeros, fat tails, and multiple mode values are difficult for certain models and result in your model underfitting. Let’s start by checking the distribution of the price feature. A histogram, plotting the frequency of values falling into ranges of values (called bins), is particularly helpful in figuring out how your data distributes. For instance, a histogram can tell you if your data resembles a known distribution, such as the normal distribution, or highlight at around what values there are peaks and where the data is denser (see figure 4.1). If you are working with a pandas DataFrame, the plot can be made just by calling the <code class="fm-code-in-text">hist</code> method that depicts data distribution by plotting the frequency of values falling into ran<a id="idTextAnchor020"/>ge<a id="idTextAnchor021"/>s of values (bins): <a id="idIndexMarker053"/><a id="marker-101"/><a id="idIndexMarker054"/></p>
  <pre class="programlisting">data[["price"]].hist(bins=10)</pre>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.1 A histogram describing how the Price feature is distributed</p>
  </div>

  <p class="body">The distribution shown in figure 4.1 is extremely skewed to the right, with many outlying values because the plotted values range to 10,000. However, just before 2,000, it is hard to distinguish any bar depicting frequencies. This becomes even more evident by plotting a boxplot, which is a very useful tool when one wants to visualize where the core part of the distribution of a variable lies. A <i class="fm-italics">boxplot</i> for a variable is a plot where the key measurements of the distribution are depicted as a box with “whiskers”: two lines outside the box that stretch to the expected limits of the variables’ distribution. The box is delimited by the interquartile range (IQR), determined by the 25th and 75th percentiles, and split into two by the line of the median. The whiskers stretch up and down to values 1.5 times the IQR. Everything above or below the whiskers’ edges is considered an <i class="fm-italics">outlier</i>: an unusual or unexpected value. Let’s plot a boxplot for the price variable, again using a built-in method in pandas DataFrame, the boxplot <a id="idTextAnchor022"/>method (see figure 4.2):<a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="marker-102"/></p>
  <pre class="programlisting">data[["price"]].boxplot()</pre>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F02_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.2 A boxplot highlighting the distribution of the Price feature and its right heavy tail on large price values</p>
  </div>

  <p class="body">Not surprisingly, the box and whiskers are squeezed in the lower part of the chart and are almost indistinguishable from each other. A long queue of outliers elongates from the upper limit of the upper extremity of the boxplot. This is an evident case of a right-skewed distribution. In such cases, a standard solution to remediate the variable is to transform the target using a logarithm transformation. It is common practice to add a constant to offset the values into the positive number field for handling values of zero and below. In our case, it is unnecessary, since all the values are positive and above zero. In the following code snippet, we represent the transformed price feature by application of a logarithmic transformation (see figures 4.3 and 4.4):</p>
  <pre class="programlisting">np.log1p(data["price"]).hist(bins=20)
dat<a id="idTextAnchor023"/>a[["price"]].apply(lambda x: np.log1p(x)).boxplot()</pre>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F03_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.3 A histogram of the Price feature being more symmetrical after log transformation<a id="idTextAnchor024"/><a id="marker-103"/></p>
  </div>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F04_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.4 A boxplot of the Price feature after log transformation signaling the persistence of extreme values at both the tails of the distribution</p>
  </div>

  <p class="body">Now the distribution, represented both by the new histogram and the boxplot, is more symmetric, though it is evident that there are outlying observations on both sides of the distribution. Since our aim is illustrative, we can ignore the original distribution and focus on a meaningful target representation. For instance, we can keep only the price values below 1,000 (see figure 4.5). In the following code snippet, we produce a histogram focused only on price values below 1,000:</p>
  <pre class="programlisting"><a id="idTextAnchor025"/>data[["price"]][data.price &lt;= 1000].hist(bins=20)</pre>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F05_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.5 A histogram of the Price feature for values under 1,000 still showing a right-skewed long tail</p>
  </div>

  <p class="body">Here the represented distribution is still right-skewed, but it resembles more common distributions found in e-commerce or other sales with long-tail products. In addition, if we focus on the range between 50 and 200, the distribution will appear more uniform (see figure 4.6). In the following code snippet, we restrict our focus further only to prices between 50 and 200 and plot the relative histogram:</p>
  <pre class="programlisting">data[["price"]][(data.p<a id="idTextAnchor026"/>rice &gt;= 50) &amp; (data.price &lt;= 200)].hist(bins=20)</pre>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F06_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.6 A histogram of the Price feature for values between 50 and 200, showing distributed values across the range</p>
  </div>

  <p class="body"><a id="marker-104"/>Therefore, we can create two masking variables, made of booleans, that can help us filter the target according to the type of algorithm we would like to test. The <code class="fm-code-in-text">price_capped</code> variable will be instrumental when demonstrating how certain machine learning algorithms can handle long tails easily:<a id="idIndexMarker057"/></p>
  <pre class="programlisting">price_capped = data.price &lt;= 1000
price_window = (data.price &gt;= 50) &amp; (data.price &lt;= 200)</pre>

  <p class="body">Figure 4.7 shows the boxplot relative to the capped price, which presents right-side<a id="idTextAnchor027"/>d outliers, but at least the boxplot is visible.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F07_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.7 A boxplot of the Price feature for values under 1,000 showing a long tail of extreme values in its right tail</p>
  </div>

  <p class="body">Figure 4.8 shows the boxplot relative to the windowed price, showing no sign of ou<a id="idTextAnchor028"/>tliers:<a id="marker-105"/></p>
  <pre class="programlisting">data[["price"]][price_window].boxplot()</pre>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F08_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.8 A boxplot of the Price feature for values between 50 and 200 showing a slightly right-skewed distribution with no extreme values</p>
  </div>

  <p class="body">After completing our exploration of the predictors and the target, we are ready to prepare four different targets that will be used along with our examples:</p>
  <pre class="programlisting">target_mean = (data["price"] &gt; data["price"].mean()).astype(int)
target_median = (data["price"] &gt; data["price"].median()).astype(int)
target_multiclass = pd.qcut(data["price"], q=5, labels=False)
target_regression = data["price"]</pre>

  <p class="body">We prepared two binary targets, <code class="fm-code-in-text">target_mean</code> and <code class="fm-code-in-text">target_median</code>, and a multiclass target with five distinct classes based on percentiles for classification purposes.<a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>

  <p class="body">In particular, it is important to notice that our <code class="fm-code-in-text">target_median</code> is a binary balanced target. Hence, we can safely use accuracy as a good performance measurement. As a test, you get an almost equal number of cases for the positive and negative classes if you try to count the values:<a id="idIndexMarker060"/></p>
  <pre class="programlisting">target_median.value_counts()</pre>

  <p class="body">You get the result</p>
  <pre class="programlisting">0    24472
1    24423</pre>

  <p class="body">Instead, if you try doing the same on the <code class="fm-code-in-text">target_mean</code> target variable, you get<a id="idIndexMarker061"/></p>
  <pre class="programlisting">target_mean.value_counts()</pre>

  <p class="body">You will obtain a distribution that is imbalanced toward the negative cases; that is, there are more cases below the mean because of the skewed distribution we previously observed:</p>
  <pre class="programlisting">0    34016
1    14879</pre>

  <p class="body">In such a case, when evaluating the results of a machine learning classifier, we prefer to use metrics such as the Receiver Operating Characteristic Area Under the Curve (ROC-AUC) or Average Precision—both quite sensible for ordering. Finally, as for the multiclass target, counting the cases for each one of the five classes reveals that they are also balanced in distribution:<a id="idIndexMarker062"/><a id="marker-106"/></p>
  <pre class="programlisting">target_multiclass.value_counts()</pre>

  <p class="body">This command returns the result of</p>
  <pre class="programlisting">0    10063
1     9835
2     9804
3    10809
4     8384</pre>

  <p class="body">As for the regression target, <code class="fm-code-in-text">target_regression</code> is the original target without transformations. However, we will use subsets of it and accordingly transform them based on the machine learning algorithm we will demonstrate.<a id="idIndexMarker063"/></p>

  <p class="body">Having completed our exploration of the data, the target, and some basic feature selection in the next paragraph, using a building blocks approach, we will prepare a few pipelines to accompany our discovery of differ<a id="idTextAnchor029"/>ent machine learning options for tabular data problems.<a id="idIndexMarker064"/><a id="idIndexMarker065"/></p>

  <h3 class="fm-head1" id="heading_id_9">4.2.2 Pipelines preparation</h3>

  <p class="body">We will use the previously seen Pipeline and ColumnTransformer classes from Scikit-learn to prepare the pipelines. In a building blocks approach, we first create the different operations to be applied to the other data types that characterize features in a tabular dataset.<a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="marker-107"/></p>

  <p class="body">The following code defines three core procedures that will be reused multiple times in this chapter:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Categorical one-hot encoding</i>—Categorical features are transformed into binary ones. If a value has never been seen before, it will be ignored.<a id="idIndexMarker069"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Numeric pass-through</i>—Numeric features are imputed using zero as a value.<a id="idIndexMarker070"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Numeric standardization</i>—After imputing missing values, numeric features are rescaled by subtracting their mean and dividing them by their standard deviation <a id="idIndexMarker071"/></p>
    </li>
  </ul>

  <p class="body">The co<a id="idTextAnchor030"/>de defining these procedures is shown in the following listing.</p>

  <p class="fm-code-listing-caption">Listing 4.2 Setting up building blocks for tabular learning pipelines</p>
  <pre class="programlisting">from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
 
categorical_onehot_encoding = OneHotEncoder(
       handle_unknown='ignore')                       <span class="fm-combinumeral">①</span>
numeric_passthrough = SimpleImputer(
       strategy="constant", fill_value=0)             <span class="fm-combinumeral">②</span>
numeric_standardization = Pipeline([
       ("imputation", SimpleImputer(strategy="constant", fill_value=0)),
       ("standardizing", StandardScaler())
       ])                                             <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts categorical features into one-hot encoded format</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Replaces missing numeric values with zero</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Pipeline replaces missing numeric values with zero and standardizes the features</p>

  <p class="body">At this point, we can compose specific transformation pipelines that handle the data according to our needs for each machine learning algorithm. For instance, in this example, we set a pipeline that will one-hot encode low categorical features and just impute missing values as zero for numeric ones. Such a pipeline is made by the ColumnTransformer function, a glue function that combines operations applied on different sets of features simultaneously. This is an excellent transformative strategy suitable for most machine learning models:<a id="idIndexMarker072"/></p>
  <pre class="programlisting">column_transform = ColumnTransformer(
                [('categories', 
                  categorical_onehot_encoding, 
                  low_card_categorical),              <span class="fm-combinumeral">①</span>
                 ('numeric', 
                  numeric_passthrough, 
                  continuous),                        <span class="fm-combinumeral">②</span>
                ],
    remainder='drop',                                 <span class="fm-combinumeral">③</span>
    verbose_feature_names_out=False,                  <span class="fm-combinumeral">④</span>
    sparse_threshold=0.0                              <span class="fm-combinumeral">⑤</span>
)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> First step of the pipeline: one-hot encoding categorical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Second step of the pipeline: handling numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The features not processed by the pipeline are dropped from the result.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Names of the features are kept as they originally are.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The result is always a dense matrix (i.e., a NumPy array)</p>

  <p class="body">We can immediately run this code snippet and check how this pipeline transforms our Airbnb NYC data:</p>
  <pre class="programlisting">X = column_transform.fit_transform(data)
print(type(X), X.dtype, X.shape)</pre>

  <p class="body">The result is that the output is now a NumPy array made of floats and that the shape has increased to 13 columns. In fact, because of one-hot encoding, each value in the categorical features has turned into a separate feature:</p>
  <pre class="programlisting">&lt;class 'numpy.ndarray'&gt; float64 (48895, 13)</pre>

  <p class="body">The following section will explore the main machine learning techniques for tabular data. Each will be accompanied by its column transforming class, <a id="idTextAnchor031"/>which will be integrated into the pipeline containing the model.<a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="marker-108"/></p>

  <h2 class="fm-head" id="heading_id_10">4.3 Classical machine learning</h2>

  <p class="body">To explain the different models from the classic machine learning techniques for tabular data, we will first introduce the core characteristics of the algorithm, and then demonstrate a code snippet, seeing it at work on our reference tabular problem, the Airbnb NYC dataset. The following are some best practices that we will use in our examples to allow reproducibility and comparability of the different approaches:<a id="idIndexMarker075"/><a id="idIndexMarker076"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">We define a pipeline incorporating both data transformation and modeling.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">We set an error measure, such as root mean squared error (RMSE) for regression or accuracy for classification, and measure it using the same cross-validation strategy.<a id="idIndexMarker077"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">We report the average and standard deviation—crucial to figure out if the model has a constant performance across different data samples—of the cross-validated estimate of the error.</p>
    </li>
  </ul>

  <p class="body">In the previous section, we introduced the different tools Scikit-learn offers for building data pipelines integrating feature processing and machine learning models. In this section, we will introduce the recommended evaluation measures and how the cross-validation estimate by the Scikit-learn <code class="fm-code-in-text">cross_validate</code> command works.<a id="idIndexMarker078"/></p>

  <p class="body">Let’s review <i class="fm-italics">evaluation metrics</i> first. We decided to use RMSE, a common measure for regression tasks, and accuracy, another standard measure for balanced binary and multiclass classification problems when the classes have approximately the same sample sizes. In subsequent chapters, we will also use metrics suitable for unbalanced classification problems, such as ROC-AUC and average precision.<a id="idIndexMarker079"/><a id="marker-109"/></p>

  <p class="body"><i class="fm-italics">Cross-validation</i> is the de facto standard in data science when you intend to estimate the expected performance of a machine learning model on any data different from the training data but drawn from the same data distribution. It is important to note that cross-validation estimates the future performance of your model based on the idea that your data may change in the future but won’t be radically different. To work correctly, the model expects that you will use the same features in the future and that they will have the same unique values (if a categorical feature) with similar distributions (both for categorical and numeric features) and, most importantly, that features will be in the same relation with your target variable.<a id="idIndexMarker080"/></p>

  <p class="body">The assumption that data distributions will remain consistent in the future is frequently not true because economic dynamics, consumer markets, and social and political situations change rapidly in the real world. In the real world, your model may experience concept drifting, when the modeled relationships between features and targets no longer represent reality. Hence, your model will underperform when dealing with new data. Cross-validation is the best tool to evaluate your models at the time of their creation because it is based on your available information at that moment and because, if well designed, it is not influenced by the ability of your machine learning model to overfit the training data. Its usefulness stays true even after cross-validated results are disproved compared to future performances, usually because the underlying data distribution has changed. In addition, alternative methods, such as leave-one-out or bootstrapping, offer better estimates with increasing computational costs, whereas more straightforward methods, such as train/test split, are less reliable in their estimates.</p>

  <p class="body">In its most uncomplicated flavor, the <i class="fm-italics">k-fol<a id="idTextAnchor032"/>d cross-validation</i> (implemented in Scikit-learn with the KFold function: <a class="url" href="https://mng.bz/VVM0">https://mng.bz/VVM0</a>) is based on the splitting of your available training data into k partitions and the building of k versions of your model fed each time by different sets of k-1 partitions and then tested on the remaining left out partition (the out-of-sample performance). The average and standard deviation of the resulting k scores will provide an estimate and a quantification of its uncertainty level to be used as a model estimate for expected performance on future unseen data. Figure 4.9 illustrates the k-fold validation when k is set to 5: each row represents the data partitioning at each fold. The validation part of a fold is always distinct f<a id="idTextAnchor033"/>rom the others, and the training part is always differently composed.<a id="idIndexMarker081"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F09_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.9 How data is distributed between train and validation across the folds of a five-fold cross-validation strategy</p>
  </div>

  <p class="body">Setting the correct value to k is a matter of how much training data you have available, how computationally costly it is to train your model on it, how the sample you received catches all the possible variations of the data distribution you want to model, and for what purpose you intend to get a performance estimate. As a general rule of thumb, values of k such as 5 or 10 are optimal choices, with k = 10 being more suitable for precise performance evaluation and k = 5 a good value compromising precision and computation costs for activities such as model, features, and hyperparameters evaluation (hence it will be used for our examples).</p>

  <p class="body">To get a general performance estimation for your model, you can build the necessary cross-validation iterations using a series of iterations on the KFold function (or its vari<a id="idTextAnchor034"/>ations, offering sample stratification or control on the time <a id="idTextAnchor035"/>dimension: <a class="url" href="https://mng.bz/xKne">https://mng.bz/xKne</a>) or rely on the <code class="fm-code-in-text">cross_validate</code> procedure (<a class="url" href="https://mng.bz/AQyK">https://mng.bz/AQyK</a>) that will handle everything for you and just return the results. For our purposes of testing different algorithms, <code class="fm-code-in-text">cross_validate</code> is quite handy because, given the proper parameters, it will produce a series of metrics:<a id="idIndexMarker082"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Cross-validation test scores (out-of-sample performance)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Cross-validation train scores (in-sample performance)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Fit time and predict time (to evaluate the computational cost)</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The trained estimators on the different cross-validation folds</p>
    </li>
  </ul>

  <p class="body"><a id="marker-110"/>All we have to do is provide an estimator, which can be any Scikit-learn object with a fit method, predictors and target, a cross-validation strategy, and a single or multiple scoring functions in a list. This estimator should be provid<a id="idTextAnchor036"/>ed in the form of a callable to be created using the <code class="fm-code-in-text">make_scorer</code> command (<a class="url" href="https://mng.bz/ZlAO">https://mng.bz/ZlAO</a>). In the next section, we will start seeing how we can get cross-validated performance estimates using such inputs, starting with classical machi<a id="idTextAnchor037"/>ne learning algorithms such as linear regression and logistic regression.<a id="idIndexMarker083"/></p>

  <h3 class="fm-head1" id="heading_id_11">4.3.1 Linear and logistic regression</h3>

  <p class="body">In <i class="fm-italics">linear regression</i>, a statistical method that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data, you first have all your features converted to numeric ones and put them into a matrix, including one-hot encoded categorical features. The algorithm’s goal is to optimally find the weight values in a column vector (the coefficients) so that, when multiplied against the matrix of features, you get a vector of results best approximating your targets (the predictions). In other words, the algorithm strives to minimize the residual sum of squares between the targets and the predictions obtained by multiplying features with the weight vector. In the process, you can consider using a prediction baseline (the so-called intercept or bias) or placing constraints on the weight values for them to be only positive. <a id="idIndexMarker084"/><a id="idIndexMarker085"/><a id="idIndexMarker086"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="marker-111"/><a id="idIndexMarker089"/></p>

  <p class="body">Since the linear regression algorithm is just a weighted summation, you have to take care of three key aspects:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Ensure there are no missing values since they cannot be used for multiplications or additions unless you have imputed them to some value.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Ensure you have handled outliers because they can affect the algorithm’s work both in training and prediction.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Validate that the features and the target are linearly related as much as possible (i.e., they have a good Pearson correlation): features weakly related to the target tend just to add noise to the model, and they tend to make it underfit or even, when in high numbers, overfit.</p>
    </li>
  </ul>

  <p class="body">Since a summation of your weighted features gives the prediction, it is easy to determine the most significant effect on the predicted output and how each feature contributes to it. Observing the coefficients relative to each feature gives you insight into how the algorithm behaves. Such understanding can prove valuable when you have to explain how the model works to regulatory authorities or stakeholders and when you want to check if the predictions are justifiable from the point of view of a hypothesis or expert knowledge of the domain.</p>

  <p class="body">However, there are also hidden perils in the easy way that a regression model shows how it works under the hood. When two or more features in the data are highly correlated, a condition known as “multicollinearity” in statistics, the interpretation in a regression model can be much more complicated, even if both features effectively contribute to the prediction. Usually, only one of many takes a notable coefficient, whereas the others take small values as if they were unrelated to the target. In reality, the opposite is often true, and the relative ease in understanding the role of a feature in a regression prediction can lead to important conceptual misunderstanding.</p>

  <p class="body">Another great advantage of the linear regression algorithm is that, since it is just some multiplications and summations, it is a breeze to implement it on any software platform, even by hand-coding it in a script. Other machine learning algorithms are more complex to replicate, and hence, implementation from scratch of algorithms more complicated than a linear regression may be susceptible to errors and bugs. However, though unfeasible for delivering your projects, we have to note that hand-coding any machine learning model can be a valuable learning experience, allowing you to gain a deeper understanding of the inner workings of the algorithm and making yourself more equipped to troubleshoot and optimize the performance of the similar models in the future. We present some manageable from-scratch implementations of some algorithms for learning purposes in chapter 5.</p>

  <p class="body"><a id="marker-112"/>We will start with an example of a linear regression model applied end to end to our Airbnb NYC data. The example follows the schema proposed in figure 4.10, a schema that we will replicate for every classical machine learning algorithm we will present and that is<a id="idTextAnchor038"/> based on Scikit-learn’s pipelines and cross-validation evaluation functions.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F10_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.10 Schema of how we will organize the examples for classical machine learning algorithms</p>
  </div>

  <p class="body">The schema is quite linear. The input from a comma-separated values file first goes through a ColumnTransformer, which constitutes the data preparation part, which applies transformation on data, discards data, or lets it pass as it is, based on column names and then a machine learning model. Both are wrapped into a pipeline tested by a <code class="fm-code-in-text">cross_validate</code> function that executes cross-validation and records computation times, trained models, and performances on a certain number of folds. Finally, the results are selected to demonstrate how the model worked. In addition, we can access, passing by the pipeline, the model coefficients and weights to get more insights into the functionalities of the algorithm we tested.<a id="idIndexMarker090"/></p>

  <p class="body">Applying such a schema, we just use a vanilla linear regression model in listing 4.3 since this algorithm usually does not need to specify any parameter. For special applications related to model interpretability, you could have specified the <code class="fm-code-in-text">fit_intercept</code> to be false to remove the intercept from the model and derive all the predictions from the featu<a id="idTextAnchor039"/>res only or the positive parameter to be true to get only positive coefficients.</p>

  <p class="fm-code-listing-caption">Listing 4.3 Linear regression</p>
  <pre class="programlisting">from sklearn.linear_model import LinearRegression
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_passthrough, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                   <span class="fm-combinumeral">①</span>
 
model = LinearRegression()                                  <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)]                                   <span class="fm-combinumeral">③</span>
)
 
cv = KFold(5, shuffle=True, random_state=0)                 <span class="fm-combinumeral">④</span>
rmse =  make_scorer(mean_squared_error, 
                    squared=False)                          <span class="fm-combinumeral">⑤</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data[price_window], 
                           y=target_regression[price_window],
                           scoring=rmse,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)           <span class="fm-combinumeral">⑥</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}", 
      f"secs pred: {score_time:0.2f} secs")                 <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> ColumnTransformer, transforming data into numeric features and imputing missing data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Vanilla linear regression model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Pipeline assembling ColumnTransformer and model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Cross-validation strategy based on five folds and random sampling</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Function for evaluation metric derived from mean squared error</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Automated cross-validate procedure</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Reports the results in terms of evaluation metric, standard deviation, fitting, and prediction time</p>

  <p class="body"><a id="marker-113"/>Running the listed code will produce the following RMSE results:</p>
  <pre class="programlisting">33.949 (0.274) fit: 0.06 secs pred: 0.01 secs</pre>

  <p class="body">That’s a good result, obtained in a minimal time (using a standard Google Colab instance or a Kaggle notebook), and can act as a baseline for more sophisticated attempts. For example, if you try to run the code in listing 4.4, you will realize that you can get similar results with fewer but accurately prepared features. That’s called <i class="fm-italics">feature engineering,</i> and the interesting point of doing it is that you can get better results or the same results but with fewer features meaningful for domain or business experts. For example, we create various new features in the code listing by generating binary features relative to specific<a id="idTextAnchor040"/> values, combining features, and transforming them using a logarithmic function.<a id="idIndexMarker091"/></p>

  <p class="fm-code-listing-caption">Listing 4.4 Customized data preparation for linear regression</p>
  <pre class="programlisting">data_2 = data[[]].copy()                                            <span class="fm-combinumeral">①</span>
data_2['neighbourhood_group_Manhattan'] = ( 
   (data['neighbourhood_group']=='Manhattan')
   .astype(int))                                                    <span class="fm-combinumeral">②</span>
data_2['neighbourhood_group_Queens'] = (
                           
(data['neighbourhood_group']=='Queens').astype(int))                <span class="fm-combinumeral">③</span>
data_2['room_type_Entire home/apt'] = (
                           (data['room_type']=='Entire 
home/apt').astype(int))                                             <span class="fm-combinumeral">④</span>
data_2['minimum_nights_log'] = np.log1p(
                        data["minimum_nights"])                     <span class="fm-combinumeral">⑤</span>
data_2['number_of_reviews_log'] = np.log1p(
                        data["number_of_reviews"])                  <span class="fm-combinumeral">⑥</span>
label1 = 'neighbourhood_group_Manhattan*room_type_Entire home/apt'
data_2[label1] = (
   data_2['neighbourhood_group_Manhattan'] *
   data_2['room_type_Entire home/apt'])                             <span class="fm-combinumeral">⑦</span>
label2 = 'availability_365*neighbourhood_group_Manhattan'
data_2[label2] = (data['availability_365'] *
   data_2['neighbourhood_group_Manhattan'])                         <span class="fm-combinumeral">⑧</span>
label3 = 'availability_365*room_type_Entire home/apt'
data_2[label3] = (data['availability_365'] *
   data_2['room_type_Entire home/apt'])                             <span class="fm-combinumeral">⑨</span>
 
rmse = make_scorer(mean_squared_error, squared=False)
cv = KFold(5, shuffle=True, random_state=0)
 
cv_scores = cross_validate(estimator=LinearRegression(), 
                           X=data_2[price_window], 
                           y=target_regression[price_window],
                           scoring=rmse,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
print(f"{mean_cv:0.5f}, {std_cv:0.5f}")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates an empty DataFrame</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A binary column indicating whether the 'neighbourhood_group' is 'Manhattan'</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> A binary column indicating whether the 'neighbourhood_group' is 'Queens'</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A binary column indicating whether the 'room_type' is 'Entire home/apt'</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> A column containing the natural logarithm of the values in the 'minimum_nights' column plus 1</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> A column containing the natural logarithm of the values in the 'number_of_reviews' column plus 1</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> A product of the binary 'neighbourhood_group_Manhattan' and 'room_type_Entire home/apt' columns</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> A product of 'availability_365' and the binary 'neighbourhood_group_Manhattan' column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> A product of 'availability_365' and the binary 'room_type_Entire home/apt' column</p>

  <p class="body"><a id="marker-114"/>The resulting RMSE is</p>
  <pre class="programlisting">33.937 (0.240)</pre>

  <p class="body">Though the result is comparable to the previous experiment, this time you are using a dataset with fewer features that have been created by specific transformations, such as the one-hot encoding of categorical features, the transformations applied to numeric ones by specific functions (i.e., cubed, squared, logarithm, or square root transformation) and by multiplying features together. In our experience, a model presenting fewer, more meaningful features generated by reasoned feature engineering and domain expertise is usually more accepted by business users, even if it has comparable or even less predictive performance than a purely data-driven one.</p>

  <p class="body">Multiplying features together is an operation that you find only when working with linear regression models; the obtained result is called interactions between features. Interactions work by multiplying two or more features to get a new one. All such transformations on the features are intended to render the relationship between each feature and the target as linear as possible. Good results can be obtained automatically or based on your knowledge of the data and the problem. Applying such transformations to the features is typical of the family of linear regression models. They have little or no effect on the more complex algorithms we will explore later in this chapter and subsequent chapters. Investing time in defining how the features should be expressed is both an advantage and a disadvantage of linear regression models. However, there are ways to automatically perform it using regularization, as we will propose in the next section.</p>

  <p class="body">The next section will discuss regularization in linear models (linear regression and logistic regression). Regularization is the best solution to implement when you have many features and their reciprocal multicollinearity (you have multicollinearity when two predictors are highly correlated with one another) doesn’t allow the linear regression model to find the best coefficients for the prediction because they are unstable and unreliable<a id="idTextAnchor041"/>—for instance, showing a coefficient you didn’t expect in terms of sign and size.<a id="marker-115"/></p>

  <h3 class="fm-head1" id="heading_id_12">4.3.2 Regularized methods</h3>

  <p class="body">Linear regression models are usually simple enough for humans to understand directly as formulas of coefficients applied to features. This means that, when applied to a real-world problem, they can turn out to be a rough approximation of complex dynamics and thus systematically miss correct predictions. Technically, they are models with a high bias. A remedy for this is to make their formulations more complex by adding more and more features and their transformations (logarithmic, squared, root transformations, and so on) and by making features interact with many others (through multiplication). In this way, a linear regression model can diminish its bias and become a better predictor. At the same time, however, the variance of the model will also increase, and it can start overfitting.<a id="idIndexMarker092"/><a id="idIndexMarker093"/></p>

  <p class="body">Occam’s razor principle, which states that <a id="idTextAnchor042"/>among competing hypotheses, the one with the fewest assumptions should be selected (<a class="url" href="https://mng.bz/RV40">https://mng.bz/RV40</a>), works perfectly for linear models, whereas it doesn’t matter for neural networks applied to tabular data where the more complex, the better. Hence, linear models should be as simple as possible to meet the needs of the problem. Here is where regularization enters the scene, helping you reduce the complexity of a linear model until it fits the problem. Regularization is a technique used to reduce overfitting in machine learning by limiting the complexity of the model, thus effectively improving its generalization performance. Regularization works because the linear regression model is penalized as it looks for the best coefficients for its predictions. The used penalization is based on the summation of the coefficients. Therefore, the regression model is incentivized to keep them as small as possible, if not to set them to zero. Constraining regression coefficients to limit their magnitude has two significant effects:<a id="idIndexMarker094"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">It avoids any form of data memorization and overfitting (i.e., certain specific coefficient values to be taken when there is a large number of features compared to the available examples).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">As coefficient shrinking happens, estimates are stabilized because multicollinear features will have the values of their coefficients resized or concentrated on only one of the features.</p>
    </li>
  </ul>

  <p class="body">In the optimization process, coefficients are updated multiple times, and these steps are called iterations. At each step, each regression coefficient incorporates a correction toward its optimal value. The optimal value is determined by the gradient, which can be intended as a number representing the direction that greatly improves the coefficient at that step. A more detailed explanation closes this chapter. Penalization is a form of constraint that forces the weights deriving from the optimization of the model to have specific characteristics. We have two variants of regularization:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The first variant is where the penalization is computed by summing the absolute values of the coefficients: this is called L1 regularization. It makes the coefficients sparse because it can push some coefficients to zero, making their related features irrelevant.<a id="idIndexMarker095"/><a id="marker-116"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The second option is where the penalization is computed by summing the squared coefficients: this is called L2 regularization, and its effect is generally to reduce the size of the coefficients (it is also relatively fast to compute).<a id="idIndexMarker096"/></p>
    </li>
  </ul>

  <p class="body">L1 regularization (or Lasso regression) pushes many coefficients to zero values, thus operating an implicit selection of the useful features (setting a coefficient to zero means that a feature doesn’t play any role in prediction). In addition, coefficients are always pushed toward zero w<a id="idTextAnchor043"/>ith the same strength (technically, the gradients toward the solution are always <span class="times">+1</span> or <span class="times">–1</span>). Hence, through the optimization steps, the features less associated with the target tend quickly to be assigned a zero coefficient and become totally irrelevant regarding the predictions. In short, if two or more features are multicollinear and all quite predictive, by applying L1 regularization, you will have only one of them with a coefficient different from zero.<a id="idIndexMarker097"/></p>

  <p class="body">Instead, in L2 regularization (or Ridge regression), the fact that coefficients are squared prevents negative and positive values from canceling each other in the penalization and puts more weight on larger coefficients. The result is a set of generally smaller coefficients, and multicollinear features tend to have similar coefficient values. All the features involved are included in the summation. You can notice better important features because, contrary to what happens with standard regression, the role of a feature in the prediction is not hidden by its correlation with other features. L2 regularization tends to attenuate the coefficients. It does so proportionally during the optimization steps; technically, the gradients toward the solution tend to be smaller and smaller. Hence, coefficients can reach the zero value or be near it. Still, even if the feature must be completely irrelevant to the prediction, it takes many optimization iterations and is quite time-consuming. Consequently, reprising the previous example of two or more multicollinear features in L2 regularization, instead of L1 regression that keeps only one non-zero coefficient, all the features would have a non-zero, similar size coefficient.<a id="idIndexMarker098"/></p>

  <p class="body">In our example, we first try to create new features through systematic interactions between our available features and then perform an L2 and L1 penalized regression to compare <a id="idTextAnchor044"/>their results and resulting coefficients. PolynomialFeatures is a Scikit-learn function (<a class="url" href="https://mng.bz/2ynd">https://mng.bz/2ynd</a>) that automatically creates multiplications between features by multiplying them many times with other features and by themselves. The process is reminiscent of the mathematical <i class="fm-italics">polynomial expansion</i> where a power of sums is expressed into its single terms:<a id="marker-117"/><a id="idIndexMarker099"/></p>

  <p class="fm-equation">(<i class="timestalic">a</i> + <i class="timestalic">b</i>)<sup class="fm-superscript">2</sup> = <i class="timestalic">a</i><sup class="fm-superscript">2</sup> + 2<i class="timestalic">ab</i> + <i class="timestalic">b</i><sup class="fm-superscript">2</sup></p>

  <p class="body">Scikit-learn makes it easier because when you state a degree, the function automatically creates the polynomial expansions up to that degree. You can decide whether to keep only the interactions. Such a process is interesting for a regression model because</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Interactions</i> help the regression model to better take into account the conjoint values of more features since features usually do not relate to the target in isolation but in synergy with others.<a id="idIndexMarker100"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The set of <i class="fm-italics">powers</i> of a feature helps to model it as a curve. For instance, <span class="times">a + a<sup class="fm-superscript">2</sup></span> is a curve in the shape of a parabola. <a id="idIndexMarker101"/></p>
    </li>
  </ul>

  <p class="body">Though using polynomial expansion can avoid the heavy task of creating specific features for your problem, it has a downside because it dramatically increases the number of features your model uses. More features usually provide more predictive power, but they also mean more noise, more multicollinearity, and more chances that the model has just to memorize examples and overfit the problem. Applying penalties can help us fix this problem with the L2 penalty and select only the features to be kept with the L1 penalty.</p>

  <p class="body">In the code in listing 4.5, we test applying L2 and, successively in listing 4.6, L1 regularization to the same polynomial expansion. It is important to note the effect of each kind of regularization. In this first example, we apply L2 regularization (Ridge). Since regularization makes sense if you have plenty of features for your prediction, we create new features from the old ones using a polynomial expansion. Our ridge mo<a id="idTextAnchor045"/>del is then set to a high alpha value to handle the increased number of collinear features.</p>

  <p class="fm-code-listing-caption">Listing 4.5 L2 regularized linear regression</p>
  <pre class="programlisting">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge, Lasso
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_passthrough, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)
 
polynomial_expansion = PolynomialFeatures(degree=2)              <span class="fm-combinumeral">①</span>
 
model = Ridge(alpha=2500.0)                                      <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('polynomial_expansion', polynomial_expansion),
     ('standardizing', numeric_standardization),
     ('modeling', model)]
)                                                                <span class="fm-combinumeral">③</span>
 
cv = KFold(5, shuffle=True, random_state=0)
rmse =  make_scorer(mean_squared_error, squared=False)
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data[price_window], 
                           y=target_regression[price_window],
                           scoring=rmse,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)                <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs") <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> PolynomialFeatures instance performing second-degree polynomial expansion on the features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A Ridge regression model instance with a regularization strength (alpha) of 2,500</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Pipeline for column transformation, polynomial expansion, standardization, and Ridge regression modeling</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Five-fold cross-validation using the defined pipeline and calculating RMSE scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the test RMSE scores from cross-validation</p>

  <p class="body"><a id="marker-118"/>The script results in the following output:</p>
  <pre class="programlisting">33.738 (0.275) fit: 0.13 secs pred: 0.03 secs</pre>

  <p class="body">If we count the number of non-zero coefficients (after rounding to five decimals to exclude extremely small values), we get</p>
  <pre class="programlisting">(cv_scores['estimator'][0]['modeling'].coef_.round(5)!=0).sum()</pre>

  <p class="body">Ninety-one coefficients out of 105 have non-zero values.</p>

  <p class="body">In the next example, we apply an L1 regularization and compare the results with the previous example. The p<a id="idTextAnchor046"/>rocedure is the same as the last code listing, though we resort to a lasso model this time.</p>

  <p class="fm-code-listing-caption">Listing 4.6 L1 regularized linear regression</p>
  <pre class="programlisting">model = Lasso(alpha=0.1)                                         <span class="fm-combinumeral">①</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('polynomial_expansion', polynomial_expansion),
     ('standardizing', numeric_standardization),
     ('modeling', model)]
)                                                                <span class="fm-combinumeral">②</span>
 
cv = KFold(5, shuffle=True, random_state=0)
rmse =  make_scorer(mean_squared_error, squared=False)
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data[price_window], 
                           y=target_regression[price_window],
                           scoring=rmse,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)                <span class="fm-combinumeral">③</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs") <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A Lasso regression model instance with a regularization strength (alpha) of 0.1</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Pipeline applying column transformation, polynomial expansion, standardization, and Lasso regression modeling</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Five-fold cross-validation using the defined pipeline and calculating RMSE scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints the mean and standard deviation of the test RMSE scores from cross-validation</p>

  <p class="body">The resulting output is</p>
  <pre class="programlisting">33.718 (0.269) fit: 0.64 secs pred: 0.03 secs</pre>

  <p class="body">If we check how many coefficients have non-zero-values by taking the first model built by the cross-validation cycle, this time we have fewer:</p>
  <pre class="programlisting">(cv_scores['estimator'][0]['modeling'].coef_.round(5) !=0).sum()</pre>

  <p class="body"><a id="marker-119"/>With 53 non-zero coefficients, the number of working coefficients has been halved. By increasing the alpha parameter of the Lasso call, we can obtain an even sharper reduction of used coefficients, albeit at the price of a higher computation time. There’s a sweet spot after which applying a higher L1 penalty doesn’t improve the prediction results. For prediction purposes, you have to find th<a id="idTextAnchor047"/>e correct alpha by trial and erro<a id="idTextAnchor048"/>r or using convenient automatic functions such as LassoCV (<a class="url" href="https://mng.bz/1XoV">https://mng.bz/1XoV</a>) or RidgeCV (<a class="url" href="https://mng.bz/Pdn9">https://mng.bz/Pdn9</a>) that will do the experimentation for you.<a id="idIndexMarker102"/><a id="idIndexMarker103"/></p>

  <p class="body">Interestingly, regularization is also used in neural networks. Neural networks use sequential matrix multiplications based on matrices of coefficients to transit from features to predictions, which is an extension of the working of linear regression. Neural networks have more complexities, though; yet in such an aspect of matrix multiplication, they resemble a regression model. Based on similar workings, you may find it beneficial for your tabular data problem to fit a deep learning architecture and, in doing so, to apply an L2 penalty, so the coefficients of the network are attenuated and distributed, and/or an L1 penalty, so coefficients are instead sparse with many of them set to zeros. In the next section, we will co<a id="idTextAnchor049"/>ntinue our discussion of linear models by discovering how to solve a classification problem.</p>

  <h3 class="fm-head1" id="heading_id_13">4.3.3 Logistic regression</h3>

  <p class="body">The linear regression model can be effectively extended to classification. In a binary classification problem, where you have two classes (a positive one and a negative one), you use the same approach as in a regression (feature matrix, vector of coefficients, bias). Still, you transf<a id="idTextAnchor050"/>orm the target using the logit function (for details about this statistical distribution, see <a class="url" href="https://mng.bz/JY20">https://mng.bz/JY20</a>). The transformative function is called the <i class="fm-italics">link function</i>. On the optimization side, the algorithm us<a id="idTextAnchor051"/>es as a reference the Bernoulli conditional distribution (for revising this distribution, see <a class="url" href="https://mng.bz/wJoq">https://mng.bz/wJoq</a>) instead of the normal distribution. As a result, you get output values ranging from 0 to 1, representing the probability that the sample belongs to the positive class. This is called logistic regression. Logistic regression is quite an intuitive and practical approach to solving binary classification problems and multiclass and multilabel ones.<a id="idIndexMarker104"/><a id="idIndexMarker105"/><a id="idIndexMarker106"/><a id="marker-120"/></p>

  <p class="body">In listing 4.7, we replicate the same approach as seen with linear regression—this time trying to build a model to guess if an example has a target value above the median. Please note that transformations are the same, though we use a logistic regression model this time. Our target is a class that tells if the target value is above the median. Such a t<a id="idTextAnchor052"/>arget is a binary balanced outcome, where half of the labels are positive and half are negative.</p>

  <p class="fm-code-listing-caption">Listing 4.7 Logistic regression</p>
  <pre class="programlisting">from sklearn.linear_model import LogisticRegression
from sklearn.metrics import make_scorer, accuracy_scorehttps://mng.bz/JY20
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = LogisticRegression(solver="saga",
                           penalty=None,
                           max_iter=1_000)                       <span class="fm-combinumeral">①</span>
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_standardization, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                        <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                       <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_median,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)                <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs") <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A logistic regression model instance with the “saga” solver, no penalty, and a maximum of 1,000 iterations</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A column transformer applying one-hot encoding to categorical features and standardization to numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> A pipeline that sequentially applies column transformation and logistic regression modeling</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Five-fold cross-validation using the defined pipeline and calculating accuracy scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the test accuracy scores from cross-validation</p>

  <p class="body">The script results in the following scores:</p>
  <pre class="programlisting">0.821 (0.004) fit: 3.00 secs pred: 0.02 secs</pre>

  <p class="body"><a id="marker-121"/>As the feature processing is the same, we just focus on noticing how the logistic regression has some specific parameters with respect to linear regression. In particular, you can set the penalty directly without changing the algorithm and decide what optimizer will be used (using the parameter solver). Each optimizer allows specific penalties, and it can be more or less efficient based on the characteristics of your data:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">lbfgs for L2 or no penalty.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">liblinear for L1 and L2 penalties—better for smaller datasets, limited to one-versus-rest schemes for multiclass problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">newton-cg for L2 or no penalty.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">newton-cholesky for L2 or no penalty.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">sag for L2 or no penalty—ideal for large datasets. It requires standardized features (or features all with similar scale/standard deviation).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">saga for no penalty, L1, L2, elasticnet (a mix of L1 and L2) penalties—ideal for large datasets, it requires standardized features (or features all with similar scale/standard deviation).</p>
    </li>
  </ul>

  <p class="body">In listing 4.8, we use an L2 penalty on the multiclass target to test how multiple targets are easily dealt with using the <code class="fm-code-in-text">multi_class</code> parameter set to “ovr” (one-versus-rest), a solution that takes a multiclass problem and builds a binary model for each of the classes to be predicted. At prediction time, prediction probabilities across all the classes are normalized to sum 1.0, and the class corresponding to the highest probability is taken and the predicted class. Such an approach is analogous to the softmax function approach used in neural networks where a vector of arbitrary real values is turned into a probability d<a id="idTextAnchor053"/>istribution, where the<a id="idTextAnchor054"/> sum of all elements is 1 (for a more detailed explanation of softmax, see <a class="url" href="https://mng.bz/qxYw">https://mng.bz/qxYw</a>). The alternative to the one-versus-rest approach is the multinomial option, where a single regression model directly models the probability distribution across all classes simultaneously.</p>

  <p class="body">The multinomial approach is preferred when inter-class relationships are important (e.g., for ranking or confidence-based decisions) or when a compact, single-model solution is desired.<a id="idIndexMarker107"/><a id="marker-122"/></p>

  <p class="fm-code-listing-caption">Listing 4.8 L2 regularized multiclass linear regression</p>
  <pre class="programlisting">from sklearn.linear_model import LogisticRegression
from sklearn.metrics import make_scorer, accuracy_score
 
accuracy = make_scorer(accuracy_score)
cv = KFold(5, shuffle=True, random_state=0)
model = LogisticRegression(penalty="l2", C=0.1, solver="sag", 
multi_class="ovr", max_iter=1_000)                               <span class="fm-combinumeral">①</span>
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_standardization, continuous)],
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)                                        <span class="fm-combinumeral">②</span>
 
model_pipeline = Pipeline(
    [('processing', column_transform),
     ('modeling', model)])                                       <span class="fm-combinumeral">③</span>
 
cv_scores = cross_validate(estimator=model_pipeline, 
                           X=data, 
                           y=target_multiclass,
                           scoring=accuracy,
                           cv=cv, 
                           return_train_score=True,
                           return_estimator=True)                <span class="fm-combinumeral">④</span>
 
mean_cv = np.mean(cv_scores['test_score'])
std_cv = np.std(cv_scores['test_score'])
fit_time = np.mean(cv_scores['fit_time'])
score_time = np.mean(cv_scores['score_time'])
print(f"{mean_cv:0.3f} ({std_cv:0.3f})", 
      f"fit: {fit_time:0.2f}",
      f"secs pred: {score_time:0.2f} secs")                      <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Logistic regression model instance with L2 penalty, regularization C=0.1, “sag” solver, “ovr” multiclass strategy</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Column transformer that applies one-hot encoding to categorical features and standardization to numeric features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Pipeline that sequentially applies column transformation and logistic regression modeling</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Cross-validation using the defined pipeline and calculating accuracy scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the mean and standard deviation of the test accuracy scores from cross-validation</p>

  <p class="body">Predicting the target as a class is certainly more complicated than guessing if the target price is over a threshold value or not:</p>
  <pre class="programlisting">0.435 (0.002) fit: 31.08 secs pred: 0.02 secs</pre>

  <p class="body">From this output, it is important to notice how the training time for a cross-validation fold has skyrocketed 10 times more. The reason is that applying a penalty to the coefficients involves more iterations of the algorithm’s optimization processes before reaching a stable result and because now a model for each class is being built. As a general rule, consider that penalization requires longer computations for the L2 penalty and even longer for the L1 penalty. By setting the <code class="fm-code-in-text">max_iter</code> parameter, you can impose a limit to the algorithm’s iterations, but be aware that the result you obtai<a id="idTextAnchor055"/>n by cutting off the time required for the algorithm to converge won’t be assured to be the best.<a id="idIndexMarker108"/></p>

  <h3 class="fm-head1" id="heading_id_14">4.3.4 Generalized linear methods</h3>

  <p class="body">The idea of extending linear regression to binary classification by logit transformation can be applied to distributions other than Bernoulli conditional distribution. This is dictated by the target that may represent categorical data, count data, or other data whose distribution is known not to be from a normal distribution. As we have seen in the previous paragraph, multiclass problems can be modeled using the Bernoulli distribution (the one-versus-rest strategy of fitting multiple logistic regressions) and the multinomial one. Other problems, more typical of domains such as finance or insurance, require different approaches. For instance, the Scikit-learn packa<a id="idTextAnchor056"/>ge mentions a few real-world applications and their best-fitting distributions (for reference, see <a class="url" href="https://mng.bz/7py9">https://mng.bz/7py9</a>):<a id="idIndexMarker109"/><a id="marker-123"/><a id="idIndexMarker110"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Climate modeling</i>—Number of rain events per year (Poisson distribution for count data and discrete events). The Poisson distribution is used for modeling events such as the number of calls to a call center or the number of customers visiting a restaurant), amount of rainfall per event (using Gamma distribution, a theoretical distribution useful for modeling because of its skewness and long tail), or total rain precipitation per year (Tweedie distribution, a distribution which is a compound of Poisson and Gamma distributions).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Risk modeling or insurance policy pricing</i>—Number of claim events or policyholder per year (Poisson), cost per event (Gamma), the total cost per policyholder per year (Tweedie).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Predictive maintenance</i>—Number of production interruption events per year (Poisson), the duration of interruption (Gamma), and the total interruption time per year (Tweedie).</p>
    </li>
  </ul>

  <p class="body">Figure 4.11 shows the three distributions—Poisson, Tweedie, and Gamma—for different averages. The Tweedie distri<a id="idTextAnchor057"/>bution is calculated for power equal to 1.5, a blend between the Poisson and Gamma distributions.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F11_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.11 Comparing Poisson, Tweedie, and Gamma distributions at different mean values (mu) of the distribution</p>
  </div>

  <p class="body">Of course, you may try any distribution you want—even a plain regression model—for any such situation. However, approaching each of them using the appropriate generalized linear model that optimizes that specific distribution assures the best result in most cases.</p>

  <p class="body">We don’t enter into the specifics of each distribution<a id="idTextAnchor058"/>; you just need to know that the Swiss Army Knife of general linear models is the <code class="fm-code-in-text">TweedieRegressor</code> (<a class="url" href="https://mng.bz/mGOr">https://mng.bz/mGOr</a>). This Scikit-learn implementation, depending on the power parameter, can allow you to quickly test normal distribution (a regular regression), Poisson distribution (<a class="url" href="https://mng.bz/4a4w">https://mng.bz/4a4w</a>), Gamma distribution (<a class="url" href="https://mng.bz/QDvG">https://mng.bz/QDvG</a>), Inverse Gaussian distribution (for nonnegative positively skewed data), and a blend of Gamma and Poisson (the Tweedie distribution) (see table 4.1).<a id="idIndexMarker111"/><a id="marker-124"/></p>

  <p class="fm-table-caption">Table 4.1 Power values and their corresponding statistical distributions</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="25%"/>
      <col class="contenttable-0-col" span="1" width="75%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Power</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Distribution</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">0</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Normal</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">1</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Poisson</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">(1,2)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Compound Poisson Gamma</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">2</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Gamma</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">3</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Inverse Gaussian</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">In listing 4.9, we test the different distributions offered by the TweedieRegressor on the entire distribution of prices of the Airbnb NYC dataset, a model fitting that we previously avoided because of the heavy distribution tails revealed by the EDA. We do so by testing each of these distributions one by one on the full range of price values since we are confident that using such specialized distribution will solve our problem of a target with heavy tails. It is important to remember that such distributions have limitations due to their formulations:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Norma</i><i class="fm-italics">l</i>—Any kind of value</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Poisso</i><i class="fm-italics">n</i>—Zero or positive values</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Tweedie, Gamma, Inverse Gaussia</i><i class="fm-italics">n</i>—Only non-zero positive values</p>
    </li>
  </ul>

  <p class="body">This implies that you must adapt your data if you have negative or zero values by adding an offset value. Hence, depending on the modeled distribution, we clip the target values to a lower bound based on the aforementioned limitations.<a id="marker-125"/></p>

  <p class="fm-code-listing-caption"><a id="idTextAnchor059"/>Listing 4.9 Tweedie regression</p>
  <pre class="programlisting">from sklearn.linear_model import TweedieRegressor
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate
 
experiments = [
     ['normal', 0, float('-inf')],
     ['poisson', 1, 0.0],
     ['tweedie', 1.5, 0.1],
     ['gamma', 2, 0.1],
     ['inverse gaussian', 3, 0.1]]                           <span class="fm-combinumeral">①</span>
 
for experiment, power, min_val in experiments:               <span class="fm-combinumeral">②</span>
    
    column_transform = ColumnTransformer(
        [('categories', categorical_onehot_encoding, low_card_categorical),
         ('numeric', numeric_standardization, continuous)],
        remainder='drop',
        verbose_feature_names_out=False,
        sparse_threshold=0.0)
 
    model = TweedieRegressor(power=power, 
                             max_iter=1_000)                 <span class="fm-combinumeral">③</span>
 
    model_pipeline = Pipeline(
        [('processing', column_transform),
         ('modeling', model)])
 
    cv = KFold(5, shuffle=True, random_state=0)
    rmse =  make_scorer(mean_squared_error, squared=False)
 
    cv_scores = cross_validate(estimator=model_pipeline, 
                   X=data, 
                            y=target_regression.clip(
                                   lower=min_val),           <span class="fm-combinumeral">④</span>
                               scoring=rmse,
                               cv=cv, 
                               return_train_score=True,
                               return_estimator=True)
 
    mean_cv = np.mean(cv_scores['test_score'])
    std_cv = np.std(cv_scores['test_score'])
    fit_time = np.mean(cv_scores['fit_time'])
    score_time = np.mean(cv_scores['score_time'])
    print(f"{experiment:18}: {mean_cv:0.3f} ({std_cv:0.3f})", 
          f"fit: {fit_time:0.2f}",
          f"secs pred: {score_time:0.2f} secs")              <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A list of experiments, made of a distribution name, power parameter, and minimum target value</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Loops through the experiments list with distribution names and power parameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instance of the TweedieRegressor model with the specified power parameter for the current experiment</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Clips the target regression data to a minimum value, according to the used distribution</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the experiment name along with the results from cross-validation</p>

  <p class="body">The resulting best-fitting are the Poisson and Tweedie with power 1.5 distributions:</p>
  <pre class="programlisting">normal            : 233.858 (15.826) fit: 0.13 secs pred: 0.03 secs
poisson           : 229.189 (16.075) fit: 0.66 secs pred: 0.03 secs
tweedie           : 229.607 (16.047) fit: 0.46 secs pred: 0.03 secs
gamma             : 233.991 (15.828) fit: 0.22 secs pred: 0.03 secs
inverse gaussian  : 239.577 (15.453) fit: 0.18 secs pred: 0.03 secs</pre>

  <p class="body">It is crucial to remember that the secret of the performances of the generalized linear models lies in the specific distribution they strive to model during the optimization phase. When faced with similar problems, we could resort to similar distributions on some more advanced algorithms than generalized linear models, particularly on gradient boosting implementations such as XGBoost or LightGBM, which will be discussed in the next chapter. In the next section, we will deal with a different approach related to large datasets.</p>

  <h3 class="fm-head1" id="heading_id_15">4.3.5 <a id="idTextAnchor060"/>Handling large datasets with stochastic gradient descent</h3>

  <p class="body"><a id="marker-126"/>When your tabular dataset cannot fit into your system’s memory, whether it is a cloud instance or your desktop computer, your options in modeling shrink. Apart from deep learning solutions, which will be discussed in the third part of this book, one other option, using classical machine learning, is to resort to out-of-core learning. In out-of-core learning, you keep your data in its storage (for instance, your data warehouse), and you have your model learn from it bit by bit, using small samples extracted from your data, called <i class="fm-italics">batches</i>. This is practically feasible because modern data storage allows for the picking of specific data samples at a certain cost in terms of latency: the time interval between the initiation of a data-related operation and its completion or response. In addition, there are also tools for handling and processing data on the fly (for instance, Apache Kafka or Amazon Kinetics) that can redirect the data to out-of-core learning algorithms.<a id="idIndexMarker112"/><a id="idIndexMarker113"/><a id="idIndexMarker114"/><a id="idIndexMarker115"/></p>

  <p class="body">It is also algorithmically feasible because of linear/logistic regression models. Both models are made up of additions of coefficients relative to the features you use for learning. Out-of-core learning involves first estimating these coefficients using some small samples from the data and then updating such coefficients using more and more batches extracted from your data. In the end, though the process is particularly long, your final estimated coefficient would not be much different from the ones you would have obtained if you could have fit all the data into the memory.</p>

  <p class="body">How many such batches you have to use for your out-of-core modeling, and if you have to reuse them multiple times, is a matter of empirical experimentation: it depends on the problem and the data you are using. Though providing new batches of unseen data may simply prolong your training phase, having the algorithm see the same batches again may cause it to overfit. Unfortunately, in most situations, you need to reiterate the same batches multiple times because out-of-core learning is not as straightforward as when optimizing; it takes a long time, and you may need more passes on the same data, even if we are talking about massive amounts of it. Fortunately, you can rely on regularization techniques, such as L1 and L2 regularization, to avoid overfitting.</p>

  <p class="body">In listing 4.10, we reprise our logistic regression example and make it out-of-core. First, we split our data into a training set and a test set since it is complicated to create a cross-validation procedure when using out-of-core learning strategies. In real out-of-core learning settings, cross-validation is not just complicated but often infeasible because, in such settings, you often handle examples a single time. After all, they are streamed from sources and often discarded. The usual validation strategy is to collect a list of examples for testing purposes or to use a batch of every n-one as an out-of-sample testing batch. In our example, we prefer reserving a test set.</p>

  <p class="fm-code-listing-caption">Listing 4.10 Out-of-core logistic regression with L2 regularization<a id="marker-127"/></p>
  <pre class="programlisting">from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.utils import gen_batches
from sklearn.metrics import accuracy_score
 
def generate_batches(X, 
                     y, 
                     batch_size, 
                     random_state):                                 <span class="fm-combinumeral">①</span>
    """split dataset into batches """
    examples = len(X)
    batches = gen_batches(n=examples, 
                          batch_size=batch_size)                    <span class="fm-combinumeral">②</span>
    sequence = np.arange(examples)
    if random_state:
        np.random.seed(random_state)                                <span class="fm-combinumeral">③</span>
        np.random.shuffle(sequence)
    
    for batch in batches:
        items = sequence[batch]
        yield(X.iloc[items], y.iloc[items])                         <span class="fm-combinumeral">④</span>
        
model = SGDClassifier(loss="log_loss", 
                      average=True,
                      penalty='l2', 
                      alpha=0.001)                                  <span class="fm-combinumeral">⑤</span>
 
column_transform = ColumnTransformer(
    [('categories', categorical_onehot_encoding, low_card_categorical),
     ('numeric', numeric_standardization, continuous)], 
    remainder='drop',
    verbose_feature_names_out=False,
    sparse_threshold=0.0)
 
X_train, X_test, y_train, y_test = train_test_split(data, target_median, 
                                                    test_size=0.20, 
                                                    random_state=0) <span class="fm-combinumeral">⑥</span>
iterations = 10
for j in range(iterations):                                         <span class="fm-combinumeral">⑦</span>
    generator = generate_batches(X_train, y_train, batch_size=256, 
random_state=j)
    for k, (Xt, yt) in enumerate(generator):
        if k == 0:
            column_transform.fit(Xt)
                    Xt = column_transform.transform(Xt)
        if k == 0:
            model.partial_fit(Xt, yt, classes=(0, 1))               <span class="fm-combinumeral">⑧</span>
        else:
            model.partial_fit(Xt, yt)                               <span class="fm-combinumeral">⑨</span>
            
predictions = model.predict(column_transform.transform(X_test)) 
score = accuracy_score(y_true=y_test, y_pred=predictions)
print(f"Accuracy on test set: {score:0.3f}")                        <span class="fm-combinumeral">⑩</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a function to generate batches of data for training</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates batches of data indices for processing</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Shuffles the sequence of examples if a random state is provided</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Yields batches of input features and corresponding labels</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates an instance of the SGDClassifier model with logistic loss, averaging, L2 penalty, and alpha regularization</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Splits the data and target into training and testing sets using an 80-20 ratio</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Iterates through training data batches, fitting the column transformer on the first batch</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Uses partial fitting to train the model on the first batch, specifying the classes</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Uses partial fitting to further train the model on subsequent batches</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Prints accuracy score of test data predictions</p>

  <p class="body">The train data is then split into multiple batches, and each batch is proposed for learning to the <i class="fm-italics">stochastic gradient descent</i> (SGD) algorithm. SGD is not a stand-alone algorithm but an optimization procedure for linear models, optimizing the model weights by iteratively learning them from small batches of the data or even just single examples taken alone. It is based on the <i class="fm-italics">gradient descent</i> optimization procedure and is also used in deep learning. Gradient descent starts with an initial guess for the model weights and computes the error. The next step involves computing the gradient of the error, which is obtained by taking the negative of the vector that contains the partial derivatives of the error with respect to the model weights. Since the gradient can be interpreted as taking the steepest descent on an error surface, a common example for gradient descent is always to figure it as descending from highs in the mountains to the lowest valley by taking the steepest path downward. The “mountains” in this analogy represent the error surface, and the “lowest valley” represents the minimum of the error function. Figure 4.12 visually represents this process by the progressive descent from a random high place to the lowest point in a bowl-shaped error curve.</p>

  <p class="body"><a id="marker-128"/>Besides the analogy, it is important to remember that the gradient determines how the weights should be adjusted to reduce the error at that step. Through repeated iterations, the error can be minimized by adjusting the model’s weights. However, how the weights are updated can significantly affect the outcome. If the updates are too large and decisive, the algorithm may take overly wide steps, potentially causing the model to overshoot the target and climb the error curve. In the worst-case scenario, this can result in a continuous worsening of the error, with no possibility of recovery. Conversely, taking smaller steps is generally safer but may be computationally burdensome. The size of such steps is decided by the learning rate, a parameter that regulates how the updates are done.<a id="idTextAnchor061"/><a id="idIndexMarker116"/><a id="idIndexMarker117"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F12_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.12 Gradient descent optimization in action in a simple optimization landscape</p>
  </div>

  <p class="body"><a id="marker-129"/>Linear models can be optimized easily using gradient descent because their error surface is simple and bowl-shaped. However, more complex models like gradient boosting (which will be discussed in the next chapter) and deep learning architectures may encounter challenges in optimization due to their higher complexity, with interrelated parameters and a more complex error landscape. Depending on the starting point, as illustrated in figure 4.13, these models may become stuck in a local minimum or plateau during optimization, leading to suboptimal results<a id="idTextAnchor062"/>.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH04_F13_Ryan2.png"/></p>

    <p class="figurecaption">Figure 4.13 Gradient descent in a complex error landscape showing how local minima and plateaus can lead to less-than-optimal solutions</p>
  </div>

  <p class="body">Learning a linear model by SGD is made possible using Scikit-learn’s method <code class="fm-code-in-text">partial_fit</code>, which, after an informed start (the algorithm needs to know the target labels), can learn by partially fitting one batch after the other. The same procedure is repeated multiple times, called iterations or epochs, to consolidate and improve the learning, though repeating the same examples too often may also cause overfitting. The algorithm will see, though in a different order, the same examples multiple times and update its coefficients every time. To avoid abrupt changes in the coefficients, which frequently occur when an outlier is present in the batch, the updated coefficients are not substituted for the existing ones. Instead, they are averaged together, allowing a more gradual transiti<a id="idTextAnchor063"/>on.<a id="idIndexMarker118"/></p>

  <p class="body">After all the learning process is completed, you will get the following result:</p>
  <pre class="programlisting">Accuracy on test set: 0.818</pre>

  <p class="body">The result is quite comparable with in-core learning logistic regression. Out-of-core learning, though limited to only the simplest machine learning algorithms such as linear or logistic regression, is an effective way to train on your tabular data when too many samples cannot fit into memory. All deep learning solutions also use the idea of a stream of batches, and it will be discussed again in the chapters devoted to deep neural network methods for tabular data, together with strategies such as <i class="fm-italics">early stopping</i>, a technique interrupting the iterations over data when necessary to avoid overfitting the data because of an excessive exposition of the algorithm to examples seen in previous iterations. <a id="idIndexMarker119"/></p>

  <p class="body">We can now anticipate that a fundamental recipe of such learning strategies is the randomization of the order of the examples. Since the optimization is progressive, if your data is ordered in a specific way, it will cause a biased optimization, which may result in suboptimal learning. Repeating the same batches in the same order can negatively influence your results. Hence, randomizing the order is critical for a better-trained algorithm. Another important point with SGD, however, is the data preparation phase. In such a phase, you should include all feature rescaling operations, because the optimization process is sensible to the scale of features and all the feature engineering and feature interaction computations, and set it in a way as deterministic as possible since it could be difficult to use global parameters, such as the maximum/minimum or average and the standard deviation of a feature, when your data is split into multiple batches.<a id="marker-130"/><a id="idTextAnchor064"/></p>

  <h3 class="fm-head1" id="heading_id_16">4.3.6 Choosing your algorithm</h3>

  <p class="body">As a general rule of thumb, you should first consider that machine learning algorithms scale differently based on how many rows and columns you have. Starting from the number of available rows, you must strictly resort to simple rule-based or statistical-based algorithms when operating with about or fewer than <span class="times">10<sup class="fm-superscript">2</sup></span> rows of data. For up to <span class="times">10<sup class="fm-superscript">3</sup></span> rows, models based on linear combinations, such as linear and logistic regression, are best suited because they tend not to overfit the little data available. You usually cannot tell what algorithm will work better from about <span class="times">10<sup class="fm-superscript">3</sup></span> to <span class="times">10<sup class="fm-superscript">4</sup> – 10<sup class="fm-superscript">5</sup></span> rows. Hence, it is all a matter of testing and experimenting. Here, deep learning solutions may outrun other choices only if there is some structure to exploit, such as an ordered series of information or a hierarchical structure. Up to <span class="times">10<sup class="fm-superscript">9</sup></span> rows, solutions from the gradient boosting family are likely the most effective. Again, you may find that something like out-of-core learning is a much better solution for specific problems, such as in the advertisement industry, where you have many fixed interactions that you need to estimate—for instance, between display devices, websites, and advertisements. <a id="idIndexMarker120"/></p>

  <p class="body">Out-of-core learning refers to a learning strategy that certain machine learning algorithms can adopt when learning from data: instead of learning all at once from the data, they learn bit by bit from smaller samples of the data, the batches, or even from single examples, one by one, which is also mentioned as online learning. Finally, in our experience, in situations with datasets above <span class="times">10<sup class="fm-superscript">9</sup></span> rows, deep learning solutions, and some out-of-core learning algorithms tend to perform better because they can effectively deal with such an amount of data, whereas other machine learning algorithms may be forced to learn from subsamples from the data or find other suboptimal solutions.</p>

  <p class="body">Regarding columns, we find that some algorithms need to scale better with datasets characterized by multiple columns, especially if they present sparse information—that is, many binary features. The sparser the datasets, which can be measured by the percentage of zeros values in relation to the total numeric values in the dataset, the earlier you may need to apply online learning algorithms or deep learning.</p>

  <p class="body">However, apart from scalability reasons, which relate to memory and computational complexity, each machine learning solution also suits different needs in terms of model control, openness, and understandability of the solution. In such a way, the variety of needs in tabular problems and of models in machine learning defies the notion of one best algorithm that is all you need for your work. In other words, it is not just that you need to try more machine learning models because “there’s no free lunch,” as stated in the well-known theorem by David Wolpert and William Macready (see <a class="url" href="http://www.no-free-lunch.org">http://www.no-free-lunch.org</a> for more details). More often than expected, there are cases where the underdog algorithm surprisingly beats the best-in-class algorithm. The necessity of more algorithms is mostly dictated because, as you change perspective on your problem as an artisan/artist creating their work from different angles, you may need different tools for the task.</p>

  <p class="body"><a id="marker-131"/>In the next chapter, we will present a more powerful class of machine learning algorithms, the ensembles, and finally, the gradient boosting family and its successful and popular implementations, such as XGBoost and Lig<a id="idTextAnchor065"/>htGBM.</p>

  <h2 class="fm-head" id="heading_id_17">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Determining what machine learning algorithm involves several factors: the number of examples and features, the expected performances, speed at prediction time, and interpretability. As a general rule of thumb,</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">Statistical machine learning is suitable for datasets with few cases.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Classical machine learning is suitable for datasets with a moderate number of cases.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Gradient boosting algorithms are particularly effective for datasets with a moderate to large number of cases.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Deep learning solutions are the most feasible and effective for datasets with massive amounts of data.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Scikit-learn is an open-source library for machine learning that offers a wide range of models for classification and regression, as well as functions for clustering, dimensionality reduction, preprocessing, and model selection. We can summarize its core advantages as follows:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">Consistent API across models</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Supports in-memory and out-of-core learning</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Supports working with pandas DataFrames</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Ideal for tabular problems</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Easy to install</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Extensive documentation</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Linear regression is the summation of weighted features that have been converted to numeric values (one-hot encoding for categorical features):<a id="marker-132"/></p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">The algorithm finds optimal weight values (coefficients) to minimize the residual sum of squares between targets and predictions.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Linear regression is easy to explain and understand how each feature contributes to the final result.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">A high correlation between features (multicollinearity) can cause conceptual misunderstanding.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Linear regression is computationally simple and easy to implement.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Linear regression is limited in its ability to fit complex problems with nonlinear data unless features are carefully prepared beforehand with feature engineering, such as creating polynomial features, which can help linear regression capture nonlinear relationships.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Regularization is used to prevent overfitting by reducing the complexity of a regression model and improving its generalization performance. There are two types of regularization:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">L1 regularization (or Lasso regression) pushes many coefficients to zero values, thus making some features irrelevant in the model.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">L2 regularization generally reduces the size of coefficients.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">L1 regularization can be helpful for feature selection, while L2 regularization reduces overfitting when using many features while being faster to compute.</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Linear regression can be extended to classification problems using the logit function to transform the target and the Bernoulli conditional distribution to optimize the algorithm. This results in a logistic regression model that can be used for binary classification, multiclass, and multilabel problems. Logistic regression is easy to implement and understand but has the same limitations as linear regression.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The same approach of transforming the target can be applied to other distributions as well, such as Poisson and Gamma, depending on the nature of the data. The resulting generalized linear models can be used for various real-world applications, such as climate modeling, risk modeling, and predictive maintenance. However, it’s important to note that the results may not be optimal without a proper understanding of the specific distribution applied to each situation.<a id="marker-133"/></p>
    </li>
  </ul>
</body></html>