["```py\ncd ~  \nmkdir ml_pipeline\ncd ml_pipeline\n```", "```py\ncd ~/ml_pipeline  \nls\n```", "```py\nexport \\\nGOOGLE_APPLICATION_CREDENTIALS=\\\n'/home/ryanmark2023/ml_pipeline/\\\nfirst-project-ml-tabular-039ff1f820a8.json'\n```", "```py\n$ echo $GOOGLE_APPLICATION_CREDENTIALS\n```", "```py\nparser = argparse.ArgumentParser()                           ①\nparser.add_argument(\n        '--config_bucket',                                   ②\n        help='Config details',\n        required=True\n    )\nargs = parser.parse_args().__dict__                          ③\nconfig_bucket = args['config_bucket']                        ④\n```", "```py\nbucket_name = config_bucket.split(\"/\")[2]                    ①\nobject_name = \"/\".join(config_bucket.split(\"/\")[3:])         ②\nstorage_client2 = storage.Client()                           ③\nbucket = storage_client2.bucket(bucket_name)                 ④\nblob_out = bucket.blob(object_name)                          ⑤\ndestination_file_name = 'config.yml'                         ⑥\nblob_out.\\\ndownload_to_filename(destination_file_name)                  ⑦\ntry:\n    with open (destination_file_name, 'r') as c_file:\n        config = yaml.safe_load(c_file)                      ⑧\nexcept Exception as e:\n    print('Error reading the config file')\n```", "```py\ndef assign_container_env_variables():\n    OUTPUT_MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")             ①\n    TRAIN_DATA_PATTERN = \\\nos.getenv(\"AIP_TRAINING_DATA_URI\")                            ②\n    EVAL_DATA_PATTERN = \\\nos.getenv(\"AIP_VALIDATION_DATA_URI\")                          ③\n    TEST_DATA_PATTERN = \\\nos.getenv(\"AIP_TEST_DATA_URI\")                                ④\n    return OUTPUT_MODEL_DIR, TRAIN_DATA_PATTERN, \\\nEVAL_DATA_PATTERN, TEST_DATA_PATTERN\n```", "```py\nbucket_pattern = tracer_pattern.split(\"/\")[2]                  ①\npattern = \"/\".join(tracer_pattern.split(\"/\")[3:])              ②\npattern_client = storage.Client()                              ③\nbucket = pattern_client.get_bucket(bucket_pattern)\nblobs = bucket.list_blobs()                                    ④\nmatching_files = [f\"gs://{bucket_pattern}/{blob.name}\" \\\nfor blob in blobs if fnmatch.fnmatch(blob.name, pattern)]      ⑤\nmerged_data = \\\npd.concat([pd.read_csv(f) for f in matching_files], \nignore_index=True)                                             ⑥\n```", "```py\ntf.saved_model.save(model, OUTPUT_MODEL_DIR)\n```", "```py\nmodel_args = ['--config_bucket', config['config_bucket_path']]\n```", "```py\ndef create_job(config):\n    model_display_name = '{}-{}'.format(config['ENDPOINT_NAME'], TIMESTAMP)\n    job = aiplatform.CustomTrainingJob(\n            display_name='train-{}'.format(model_display_name),\n            script_path = config['script_path'],\n            container_uri=config['train_image'],            ①\n            staging_bucket = config['staging_path'],\n            requirements=['gcsfs'],                         ②\n            model_serving_container_image_uri= \\\nconfig['deploy_image']                                      ③\n    ) \n    return job\n```", "```py\ndataset_path = \\\n'projects/'+config['project_id']+\\\n'/locations/'+config['region']+\\\n'/datasets/'+config['dataset_id']\n ds = aiplatform.TabularDataset(dataset_path)\n```", "```py\ndef run_job(job, ds, model_args,config):\n    model_display_name = \\\n'{}-{}'.format(config['ENDPOINT_NAME'], TIMESTAMP)\n    model = job.run(\n        dataset=ds,                                           ①\n        training_fraction_split = \\\nconfig['training_fraction_split'],                            ②\n        validation_fraction_split = config['validation_fraction_split'],\n        test_fraction_split = config['test_fraction_split'],\n        model_display_name=model_display_name,\n        args=model_args,                                      ③\n        machine_type= config['machine_type'] \n    )\n    return model\n```", "```py\ndef deploy_model(model,config):\n    endpoints = aiplatform.Endpoint.list(                      ①\n        filter='display_name=\"{}\"'.format(config['ENDPOINT_NAME']),\n        order_by='create_time desc',\n        project=config['project_id'], \n        location=config['region']\n    )\n    endpoint = aiplatform.Endpoint.create(                     ②\n         display_name=config['ENDPOINT_NAME'], \n         project=config['project_id'], \n         location=config['region']\n        )\n    model.deploy(                                              ③\n        endpoint=endpoint,\n        traffic_split={\"0\": 100},\n        machine_type=config['machine_type_deploy'],\n        min_replica_count=1,\n        max_replica_count=1,\n    )\n```", "```py\n    start_time = time.time()\n    config = get_pipeline_config('pipeline_config.yml')          ①\n    model_args = ['--config_bucket', config['config_bucket_path']]\n    job = create_job(config)\n    dataset_path = \\\n'projects/'+config['project_id']+\\\n'/locations/'+config['region']+\\\n'/datasets/'+config['dataset_id']\n    ds = aiplatform.TabularDataset(dataset_path)\n    model = run_job(job, ds, model_args,config)                  ②\n    if config['deploy_model']:\n        deploy_model(model,config)                               ③\n    print(\"pipeline completed\")\n```", "```py\npython pipeline_script.py\n```", "```py\nendpoint:\n   project: \"1028332300603\"\n   endpoint_id: \"1447850105594970112\"\n   location: \"us-central1\"\n```", "```py\npython flask_endpoint_deploy.py\n```", "```py\npip install protobuf==3.20.*\n```", "```py\ndef get_pipeline_config(path_to_yaml):\n    '''ingest the config yaml file\n    Args:\n        path_to_yaml: yaml file containing parameters for the pipeline script\n\n    Returns:\n        config: dictionary containing parameters read from the config file\n    '''\n```", "```py\ndef get_pipeline_config(path_to_yaml):\n    '''ingest the config yaml file\n    Args:\n        path_to_yaml: yaml file containing parameters for the pipeline script\n\n    Returns:\n        config: dictionary containing parameters read from the config file\n    '''\n    with open(path_to_yaml) as file:                     ①\n        config = yaml.safe_load(file)\n\n    return config                                        ②\n```", "```py\ndef get_pipeline_config(path_to_yaml):\n    '''ingest the config yaml file\n    Args:\n        path_to_yaml: yaml file containing parameters for the pipeline script\n\n    Returns:\n        config: dictionary containing parameters read from the config file\n    '''\n    print(\"path_to_yaml \"+path_to_yaml)\n    try:                                                  ①\n        with open (path_to_yaml, 'r') as c_file:          ②\n            config = yaml.safe_load(c_file)\n    except Exception as e:\n        print('Error reading the config file')\n    return config\n```", "```py\nif __name__ == '__main__':\n    start_time = time.time()\n    # load pipeline config parameters\n    config = get_pipeline_config('pipeline_config.yml')\n    # all the arguments sent to the training \n    #script run in the container are sent via\n    # a yaml file in Cloud Storage whose URI is the single argument sent\n    model_args = ['--config_bucket', config['config_bucket_path']]\n    print(\"model_args: \",model_args)\n    # create a CustomTrainingJob object\n    job = create_job(config)\n    # define TabularDataset object to use in running CustomTrainingJob\n    dataset_path = \\\n'projects/'+config['project_id']+\\\n'/locations/'+config['region']+\\\n'/datasets/'+config['dataset_id']\n    ds = aiplatform.TabularDataset(dataset_path)\n    # run the CustomTrainingJob object to get a trained model\n    model = run_job(job, ds, model_args,config)\n    print(\"deployment starting\")\n    # deploy model to a Vertex AI endpoint\n    if config['deploy_model']:\n        deploy_model(model,config)\n    print(\"pipeline completed\")\n    # show time taken by script\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n```", "```py\n{\n\"input_text\":\"TRANSCRIPT: \\nREASON FOR CONSULTATION: , \nLoculated left effusion, \nmultilobar pneumonia.\\n\\n LABEL:\",                      ①\n\"output_text\":\"Consult - History and Phy.\"              ②\n}\n```", "```py\nTRANSCRIPT: \\nIMPRESSION:  ,EEG during wakefulness, \ndrowsiness, and sleep with synchronous \nvideo monitoring demonstrated no evidence \nof focal or epileptogenic activity.\\n\\n LABEL:\n```"]