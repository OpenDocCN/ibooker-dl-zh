- en: Chapter 2\. Introduction to Large Language Models for Text Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章. 文本生成的大语言模型简介
- en: In artificial intelligence, a recent focus has been the evolution of large language
    models. Unlike their less-flexible predecessors, LLMs are capable of handling
    and learning from a much larger volume of data, resulting in the emergent capability
    of producing text that closely resembles human language output. These models have
    generalized across diverse applications, from writing content to automating software
    development and enabling real-time interactive chatbot experiences.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能领域，最近的研究重点是大语言模型的演变。与它们不那么灵活的前辈相比，LLM能够处理和从大量数据中学习，从而产生了产生与人类语言输出非常相似文本的能力。这些模型已经应用于各种不同的应用，从撰写内容到自动化软件开发，以及实现实时交互式聊天机器人体验。
- en: What Are Text Generation Models?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成模型是什么？
- en: Text generation models utilize advanced algorithms to understand the meaning
    in text and produce outputs that are often indistinguishable from human work.
    If you’ve ever interacted with [ChatGPT](https://chat.openai.com) or marveled
    at its ability to craft coherent and contextually relevant sentences, you’ve witnessed
    the power of an LLM in action.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成模型利用高级算法来理解文本中的含义，并产生通常与人类工作难以区分的输出。如果你曾经与[ChatGPT](https://chat.openai.com)互动或对其构建连贯且上下文相关的句子能力感到惊奇，你见证了LLM的实际力量。
- en: In natural language processing (NLP) and LLMs, the fundamental linguistic unit
    is a *token*. [Tokens](https://oreil.ly/3fOsM) can represent sentences, words,
    or even subwords such as a set of characters. A useful way to understand the size
    of text data is by looking at the number of tokens it comprises; for instance,
    a text of 100 tokens roughly equates to about 75 words. This comparison can be
    essential for managing the processing limits of LLMs as different models may have
    varying token capacities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）和LLM中，基本的语言单位是*标记*。[标记](https://oreil.ly/3fOsM)可以代表句子、单词，甚至子词，如一组字符。理解文本数据大小的一个有用方法是查看它包含的标记数量；例如，100个标记的文本大约相当于75个单词。这种比较对于管理LLM的处理限制至关重要，因为不同的模型可能有不同的标记容量。
- en: '*Tokenization*, the process of breaking down text into tokens, is a crucial
    step in preparing data for NLP tasks. Several methods can be used for tokenization,
    including [Byte-Pair Encoding (BPE)](https://oreil.ly/iSOp7), WordPiece, and SentencePiece.
    Each of these methods has its unique advantages and is suited to particular use
    cases. BPE is commonly used due to its efficiency in handling a wide range of
    vocabulary while keeping the number of tokens manageable.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*标记化*，即将文本分解成标记的过程，是准备数据用于NLP任务的关键步骤。可以用于标记化的方法包括[字节对编码（BPE）](https://oreil.ly/iSOp7)，WordPiece和SentencePiece。每种方法都有其独特的优势，适用于特定的用例。BPE因其处理广泛词汇量的效率而常用，同时保持标记数量可管理。'
- en: BPE begins by viewing a text as a series of individual characters. Over time,
    it combines characters that frequently appear together into single units, or tokens.
    To understand this better, consider the word *apple*. Initially, BPE might see
    it as *a*, *p*, *p*, *l*, and *e*. But after noticing that *p* often comes after
    *a* and before *l* in the dataset, it might combine them and treat *appl* as a
    single token in future instances.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: BPE（字节对编码）首先将文本视为一系列单个字符。随着时间的推移，它将频繁一起出现的字符组合成单个单元，或称为标记。为了更好地理解这一点，可以考虑单词*apple*。最初，BPE可能将其视为*a*，*p*，*p*，*l*和*e*。但注意到*p*在数据集中经常出现在*a*之后和*l*之前，它可能会将它们组合起来，并在未来的实例中将*appl*视为一个单独的标记。
- en: This approach helps LLMs recognize and generate words or phrases, even if they
    weren’t common in the training data, making the models more adaptable and versatile.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有助于LLM识别和生成单词或短语，即使它们在训练数据中并不常见，这使得模型更具适应性和多功能性。
- en: Understanding the workings of LLMs requires a grasp of the underlying mathematical
    principles that power these systems. Although the computations can be complex,
    we can simplify the core elements to provide an intuitive understanding of how
    these models operate. Particularly within a business context, the accuracy and
    reliability of LLMs are paramount.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理解大型语言模型（LLM）的工作原理需要掌握支撑这些系统的底层数学原理。尽管计算可能很复杂，但我们仍可以将核心元素简化，以提供对这些模型如何运作的直观理解。特别是在商业环境中，LLM的准确性和可靠性至关重要。
- en: A significant part of achieving this reliability lies in the pretraining and
    fine-tuning phases of LLM development. Initially, models are trained on vast datasets
    during the pretraining phase, acquiring a broad understanding of language. Subsequently,
    in the fine-tuning phase, models are adapted for specific tasks, honing their
    capabilities to provide accurate and reliable outputs for specialized applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这种可靠性的重要部分在于LLM开发的前训练和微调阶段。最初，模型在前训练阶段在庞大的数据集上进行训练，获得对语言的广泛理解。随后，在微调阶段，模型被适应于特定任务，磨练其能力，为专门的应用提供准确和可靠的结果。
- en: 'Vector Representations: The Numerical Essence of Language'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量表示：语言的数值本质
- en: 'In the realm of NLP, words aren’t just alphabetic symbols. They can be tokenized
    and then represented in a numerical form, known as *vectors*. These vectors are
    multi-dimensional arrays of numbers that capture the semantic and syntactic relations:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP领域，单词不仅仅是字母符号。它们可以被标记化，然后以数值形式表示，称为*向量*。这些向量是多维数组，包含数字，捕捉语义和句法关系：
- en: $w right-arrow bold v equals left-bracket v 1 comma v 2 comma ellipsis comma
    v Subscript n Baseline right-bracket$
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: $w right-arrow bold v equals left-bracket v 1 comma v 2 comma ellipsis comma
    v Subscript n Baseline right-bracket$
- en: Creating word vectors, also known as *word embeddings*, relies on intricate
    patterns within language. During an intensive training phase, models are designed
    to identify and learn these patterns, ensuring that words with similar meanings
    are mapped close to one another in a high-dimensional space ([Figure 2-1](#figure-2-1)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 创建单词向量，也称为*词嵌入*，依赖于语言中的复杂模式。在密集的训练阶段，模型被设计用来识别和学习这些模式，确保具有相似意义的单词在多维空间中彼此靠近（[图2-1](#figure-2-1)）。
- en: '![Word Embeddings](assets/pega_0201.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Word Embeddings](assets/pega_0201.png)'
- en: Figure 2-1\. Semantic proximity of word vectors within a word embedding space
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 单词嵌入空间中单词向量的语义邻近性
- en: The beauty of this approach is its ability to capture nuanced relationships
    between words and calculate their distance. When we examine word embeddings, it
    becomes evident that words with similar or related meanings like *virtue* and
    *moral* or *walked* and *walking* are situated near each other. This spatial closeness
    in the embedding space becomes a powerful tool in various NLP tasks, enabling
    models to understand context, semantics, and the intricate web of relationships
    that form language.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的美丽之处在于它能够捕捉单词之间的细微关系并计算它们的距离。当我们检查词嵌入时，很明显，具有相似或相关意义的单词，如*virtue*和*moral*或*walked*和*walking*，都位于彼此附近。这种嵌入空间中的空间接近性成为各种NLP任务中的强大工具，使模型能够理解上下文、语义以及构成语言的复杂关系网。
- en: 'Transformer Architecture: Orchestrating Contextual Relationships'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer架构：协调上下文关系
- en: Before we go deep into the mechanics of transformer architectures, let’s build
    a foundational understanding. In simple terms, when we have a sentence, say, *The
    cat sat on the mat*, each word in this sentence gets converted into its numerical
    vector representation. So, *cat* might become a series of numbers, as does *sat*,
    *on*, and *mat*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨transformer架构的机制之前，让我们建立一个基础的理解。简单来说，当我们有一个句子，比如说，*The cat sat on the
    mat*，这个句子中的每个单词都会被转换成其数值向量表示。所以，*cat*可能变成一系列数字，*sat*、*on*和*mat*也是如此。
- en: As you’ll explore in detail later in this chapter, the transformer architecture
    takes these word vectors and understands their relationships—both in structure
    (syntax) and meaning (semantics). There are many types of transformers; [Figure 2-2](#figure-2-2)
    showcases both BERT and GPT’s architecture. Additionally, a transformer doesn’t
    just see words in isolation; it looks at *cat* and knows it’s related to *sat*
    and *mat* in a specific way in this sentence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在本章后面详细探索的那样，transformer架构将这些单词向量理解它们之间的关系——在结构（句法）和意义（语义）上。有许多类型的transformer；[图2-2](#figure-2-2)展示了BERT和GPT的架构。此外，transformer不仅孤立地看待单词；它观察*cat*并知道在这个句子中以特定方式与*sat*和*mat*相关。
- en: '![BERT and GPT architecture](assets/pega_0202.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![BERT和GPT架构](assets/pega_0202.png)'
- en: Figure 2-2\. BERT uses an encoder for input data, while GPT has a decoder for
    output
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. BERT使用编码器处理输入数据，而GPT有解码器处理输出
- en: 'When the transformer processes these vectors, it uses mathematical operations
    to understand the relationships between the words, thereby producing new vectors
    with rich, contextual information:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当transformer处理这些向量时，它使用数学运算来理解单词之间的关系，从而产生具有丰富上下文信息的新向量：
- en: $bold v prime Subscript i Baseline equals Transformer left-parenthesis bold
    v 1 comma bold v 2 comma ellipsis comma bold v Subscript m Baseline right-parenthesis$
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: $bold v prime Subscript i Baseline equals Transformer left-parenthesis bold
    v 1 comma bold v 2 comma ellipsis comma bold v Subscript m Baseline right-parenthesis$
- en: One of the remarkable features of transformers is their ability to comprehend
    the nuanced contextual meanings of words. The [self-attention](https://oreil.ly/xuovP)
    mechanism in transformers lets each word in a sentence look at all other words
    to understand its context better. Think of it like each word casting votes on
    the importance of other words for its meaning. By considering the entire sentence,
    transformers can more accurately determine the role and meaning of each word,
    making their *interpretations more contextually rich.*
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的一个显著特点是它们理解词语细微语境意义的能力。Transformer 中的 [自注意力](https://oreil.ly/xuovP)
    机制允许句子中的每个词查看所有其他词，以更好地理解其语境。想象一下，每个词都在为其他词对其意义的重要性投票。通过考虑整个句子，Transformer 可以更准确地确定每个词的角色和意义，使它们的*解释更加语境丰富*。
- en: 'Probabilistic Text Generation: The Decision Mechanism'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率文本生成：决策机制
- en: 'After the transformer understands the context of the given text, it moves on
    to generating new text, guided by the concept of likelihood or probability. In
    mathematical terms, the model calculates how likely each possible next word is
    to follow the current sequence of words and picks the one that is most likely:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 理解了给定文本的语境之后，它继续生成新的文本，受似然或概率概念指导。从数学的角度来看，模型计算每个可能的下一个词跟随当前词序列的可能性，并选择最有可能的那个词：
- en: $w Subscript next Baseline equals argmax upper P left-parenthesis w vertical-bar
    w 1 comma w 2 comma ellipsis comma w Subscript m Baseline right-parenthesis$
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: $w Subscript next Baseline equals argmax upper P left-parenthesis w vertical-bar
    w 1 comma w 2 comma ellipsis comma w Subscript m Baseline right-parenthesis$
- en: By repeating this process, as shown in [Figure 2-3](#figure-2-3), the model
    generates a coherent and contextually relevant string of text as its output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重复此过程，如图 [图 2-3](#figure-2-3) 所示，模型生成一个连贯且与语境相关的文本字符串作为其输出。
- en: '![An illustrative overview of how text is generated using transformer models
    like GPT-4.](assets/pega_0203.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![使用类似 GPT-4 的 Transformer 模型生成文本的说明性概述](assets/pega_0203.png)'
- en: Figure 2-3\. How text is generated using transformer models such as GPT-4
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 使用类似 GPT-4 的 Transformer 模型生成文本的方式
- en: The mechanisms driving LLMs are rooted in vector mathematics, linear transformations,
    and probabilistic models. While the under-the-hood operations are computationally
    intensive, the core concepts are built on these mathematical principles, offering
    a foundational understanding that bridges the gap between technical complexity
    and business applicability.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动大型语言模型（LLM）的机制根植于向量数学、线性变换和概率模型。虽然底层的操作计算密集，但核心概念建立在这些数学原理之上，提供了一个基础理解，架起了技术复杂性与商业应用之间的桥梁。
- en: 'Historical Underpinnings: The Rise of Transformer Architectures'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史背景：Transformer 架构的兴起
- en: Language models like ChatGPT (the *GPT* stands for *generative pretrained transformer*)
    didn’t magically emerge. They’re the culmination of years of progress in the field
    of NLP, with particular acceleration since the late 2010s. At the heart of this
    advancement is the introduction of transformer architectures, which were detailed
    in the groundbreaking paper [“Attention Is All You Need”](https://oreil.ly/6NNbg)
    by the Google Brain team.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 ChatGPT（*GPT* 代表 *生成预训练 Transformer*）的语言模型并非凭空出现。它们是 NLP 领域多年进步的结晶，特别是自 2010
    年代后期以来的加速发展。这一进步的核心是引入了 transformer 架构，这在谷歌 Brain 团队开创性的论文 [“Attention Is All
    You Need”](https://oreil.ly/6NNbg) 中有详细阐述。
- en: The real breakthrough of transformer architectures was the concept of *attention*.
    Traditional models processed text sequentially, which limited their understanding
    of language structure especially over long distances of text. Attention transformed
    this by allowing models to directly relate distant words to one another irrespective
    of their positions in the text. This was a groundbreaking proposition. It meant
    that words and their context didn’t have to move through the entire model to affect
    each other. This not only significantly improved the models’ text comprehension
    but also made them much more efficient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构的真正突破是“注意力”的概念。传统模型按顺序处理文本，这限制了它们对语言结构的理解，尤其是在文本的较长距离上。注意力通过允许模型直接将文本中位置较远的词语联系起来，而不管它们在文本中的位置，从而改变了这一点。这是一个开创性的观点。这意味着词语及其上下文不必在整个模型中移动才能相互影响。这不仅显著提高了模型的文本理解能力，而且使它们变得更加高效。
- en: This attention mechanism played a vital role in expanding the models’ capacity
    to detect long-range dependencies in text. This was crucial for generating outputs
    that were not just contextually accurate and fluent, but also coherent over longer
    stretches.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种注意力机制在扩展模型检测文本中长距离依赖关系的能力方面发挥了至关重要的作用。这对于生成不仅上下文准确和流畅，而且在更长段落中也是连贯的输出至关重要。
- en: According to AI pioneer and educator [Andrew Ng](https://oreil.ly/JQd53), much
    of the early NLP research, including the fundamental work on transformers, received
    significant funding from United States military intelligence agencies. Their keen
    interest in tools like machine translation and speech recognition, primarily for
    intelligence purposes, inadvertently paved the way for developments that transcended
    just translation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据人工智能先驱和教育家[安德鲁·吴](https://oreil.ly/JQd53)的说法，早期的许多自然语言处理研究，包括对变换器的基本研究，都得到了美国军事情报机构的重大资助。他们对机器翻译和语音识别等工具的浓厚兴趣，主要是出于情报目的，无意中为超越翻译本身的发展开辟了道路。
- en: Training LLMs requires extensive computational resources. These models are fed
    with vast amounts of data, ranging from terabytes to petabytes, including internet
    content, academic papers, books, and more niche datasets tailored for specific
    purposes. It’s important to note, however, that the data used to train LLMs can
    carry *inherent biases from their sources*. Thus, users should exercise caution
    and ideally employ human oversight when leveraging these models, ensuring responsible
    and ethical AI applications.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLM需要大量的计算资源。这些模型被大量数据喂养，从千兆字节到拍字节不等，包括互联网内容、学术论文、书籍以及为特定目的定制的更窄数据集。然而，需要注意的是，用于训练LLM的数据可能携带其来源的*固有偏差*。因此，在使用这些模型时，用户应谨慎行事，并最好采用人工监督，以确保负责任和道德的AI应用。
- en: OpenAI’s GPT-4, for example, boasts an estimated [1.7 trillion parameters](https://oreil.ly/pZvMo),
    which is equivalent to an Excel spreadsheet that stretches across thirty thousand
    soccer fields. *Parameters* in the context of neural networks are the weights
    and biases adjusted throughout the training process, allowing the model to represent
    and generate complex patterns based on the data it’s trained on. The training
    cost for GPT-4 was estimated to be in the order of [$63 million](https://oreil.ly/_NAq5),
    and the training data would fill about [650 kilometers of bookshelves full of
    books](https://oreil.ly/D7jL5).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI的GPT-4拥有估计的[1.7万亿参数](https://oreil.ly/pZvMo)，相当于一个覆盖三万足球场的Excel电子表格。在神经网络中，“参数”是指在训练过程中调整的权重和偏差，使模型能够根据其训练数据表示和生成复杂模式。GPT-4的训练成本估计为[6300万美元](https://oreil.ly/_NAq5)，而训练数据将填满大约[650公里书架的书](https://oreil.ly/D7jL5)。
- en: To meet these requirements, major technological companies such as Microsoft,
    Meta, and Google have invested heavily, making LLM development a high-stakes endeavor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些要求，像微软、Meta和谷歌这样的大型科技公司投入了大量资金，使得大型语言模型（LLM）的开发成为一场高风险的竞赛。
- en: The rise of LLMs has provided an increased demand for the hardware industry,
    particularly companies specializing in graphics processing units (GPUs). NVIDIA,
    for instance, has become almost synonymous with high-performance GPUs that are
    essential for training LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的兴起为硬件行业带来了更高的需求，尤其是专注于图形处理单元（GPU）的公司。例如，英伟达几乎成了高性能GPU的同义词，这对于训练LLM至关重要。
- en: The demand for powerful, efficient GPUs has skyrocketed as companies strive
    to build ever-larger and more complex models. It’s not just the raw computational
    power that’s sought after. GPUs also need to be fine-tuned for tasks endemic to
    machine learning, like tensor operations. *Tensors*, in a machine learning context,
    are multidimensional arrays of data, and operations on them are foundational to
    neural network computations. This emphasis on specialized capabilities has given
    rise to tailored hardware such as NVIDIA’s H100 Tensor Core GPUs, explicitly crafted
    to expedite machine learning workloads.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公司努力构建更大、更复杂的模型，对强大、高效的GPU的需求急剧上升。人们追求的不仅仅是原始的计算能力。GPU还需要针对机器学习中的特定任务进行微调，如张量运算。在机器学习环境中，“张量”是多维数据数组，对它们的操作是神经网络计算的基础。这种对专用能力的重视催生了定制的硬件，例如NVIDIA的H100张量核心GPU，它专门设计用来加速机器学习工作负载。
- en: Furthermore, the overwhelming demand often outstrips the supply of these top-tier
    GPUs, sending prices on an upward trajectory. This supply-demand interplay has
    transformed the GPU market into a fiercely competitive and profitable arena. Here,
    an eclectic clientele, ranging from tech behemoths to academic researchers, scramble
    to procure the most advanced hardware.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对顶级GPU的需求往往超过了供应，导致价格呈上升趋势。这种供需互动已经将GPU市场转变为一个竞争激烈且有利可图的领域。在这里，从科技巨头到学术研究人员，各种客户都在争相采购最先进的硬件。
- en: This surge in demand has sparked a wave of innovation beyond just GPUs. Companies
    are now focusing on creating dedicated AI hardware, such as Google’s Tensor Processing
    Units (TPUs), to cater to the growing computational needs of AI models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种需求的激增引发了一股创新浪潮，而不仅仅是针对GPU。公司现在正专注于创建专门的AI硬件，例如谷歌的Tensor处理单元（TPUs），以满足AI模型不断增长的计算需求。
- en: This evolving landscape underscores not just the symbiotic ties between software
    and hardware in the AI sphere but also spotlights the ripple effect of the LLM
    *gold rush*. It’s steering innovations and funneling investments into various
    sectors, especially those offering the fundamental components for crafting these
    models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不断演变的地形不仅强调了人工智能领域软件与硬件之间的共生关系，而且也突出了LLM“淘金热”的连锁反应。它正引导创新并将投资引导到各个领域，尤其是那些提供构建这些模型的基本组件的领域。
- en: OpenAI’s Generative Pretrained Transformers
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI的生成预训练转换器
- en: Founded with a mission to ensure that artificial general intelligence benefits
    all of humanity, [OpenAI](https://openai.com) has recently been at the forefront
    of the AI revolution. One of their most groundbreaking contributions has been
    the GPT series of models, which have substantially redefined the boundaries of
    what LLMs can achieve.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI](https://openai.com)的成立宗旨是确保通用人工智能的益处惠及全人类，最近它一直是人工智能革命的先锋。他们最具有突破性的贡献之一是GPT系列模型，这些模型在很大程度上重新定义了LLM能够实现的范围。'
- en: The original GPT model by OpenAI was more than a mere research output; it was
    a compelling demonstration of the potential of transformer-based architectures.
    This model showcased the initial steps toward making machines understand and generate
    human-like language, laying the foundation for future advancements.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的原始GPT模型不仅仅是一个研究成果；它是对基于转换器架构潜力的有力展示。这个模型展示了机器理解并生成类似人类语言的初步步骤，为未来的进步奠定了基础。
- en: The unveiling of GPT-2 was met with both anticipation and caution. Recognizing
    the model’s powerful capabilities, OpenAI initially hesitated in releasing it
    due to concerns about its potential misuse. Such was the might of GPT-2 that ethical
    concerns took center stage, which might look quaint compared to the power of today’s
    models. However, when OpenAI decided to release the project as [open-source](https://oreil.ly/evOQE),
    it didn’t just mean making the code public. It allowed businesses and researchers
    to use these pretrained models as building blocks, incorporating AI into their
    applications without starting from scratch. This move democratized access to high-level
    natural language processing capabilities, spurring innovation across various domains.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的发布引起了人们的期待和谨慎。OpenAI在认识到该模型强大能力的同时，由于担心其潜在的误用，最初犹豫是否发布。GPT-2的强大力量使得伦理问题成为焦点，与今天模型的威力相比可能显得有些过时。然而，当OpenAI决定将项目作为[开源](https://oreil.ly/evOQE)发布时，并不仅仅意味着公开代码。它允许企业和研究人员将这些预训练模型作为构建块使用，将AI融入他们的应用中而无需从头开始。这一举措使高级自然语言处理能力更加民主化，促进了各个领域的创新。
- en: After GPT-2, OpenAI decided to focus on releasing paid, closed-source models.
    GPT-3’s arrival marked a monumental stride in the progression of LLMs. It garnered
    significant media attention, not just for its technical prowess but also for the
    societal implications of its capabilities. This model could produce text so convincing
    that it often became indistinguishable from human-written content. From crafting
    intricate pieces of literature to churning out operational code snippets, GPT-3
    exemplified the seemingly boundless potential of AI.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-2之后，OpenAI决定专注于发布付费的闭源模型。GPT-3的推出标志着LLM进步中的一个重大步伐。它不仅因其技术实力而受到媒体的关注，还因其能力的社会影响而备受瞩目。该模型能够生成如此逼真的文本，以至于它常常与人类撰写的文本难以区分。从创作复杂的文学作品到生成操作代码片段，GPT-3展示了AI看似无限的潜力。
- en: GPT-3.5-turbo and ChatGPT
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3.5-turbo和ChatGPT
- en: Bolstered by Microsoft’s significant investment in their company, OpenAI introduced
    GPT-3.5-turbo, an optimized version of its already exceptional predecessor. Following
    a [$1 billion injection](https://oreil.ly/1C8qm) from Microsoft in 2019, which
    later increased to a hefty $13 billion for a 49% stake in OpenAI’s for-profit
    arm, OpenAI used these resources to develop GPT-3.5-turbo, which offered improved
    efficiency and affordability, effectively making LLMs more accessible for a broader
    range of use cases.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在微软对其公司进行重大投资的支持下，OpenAI推出了GPT-3.5-turbo，这是其卓越前身的优化版本。2019年，微软注入了10亿美元，后来增加到130亿美元以获得OpenAI营利性部门的49%股份，OpenAI利用这些资源开发了GPT-3.5-turbo，它提供了更高的效率和更低的成本，使得大型语言模型（LLM）对更广泛的用例更加可及。
- en: OpenAI wanted to gather more world feedback for fine-tuning, and so [ChatGPT](https://chat.openai.com)
    was born. Unlike its general-purpose siblings, [ChatGPT was fine-tuned](https://oreil.ly/6ib-Q)
    to excel in conversational contexts, enabling a dialogue between humans and machines
    that felt natural and meaningful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI希望收集更多世界反馈以进行微调，因此[ChatGPT](https://chat.openai.com)应运而生。与通用型兄弟不同，[ChatGPT经过微调](https://oreil.ly/6ib-Q)以在对话环境中表现出色，使人类与机器之间的对话既自然又富有意义。
- en: '[Figure 2-4](#figure-2-4) shows the training process for ChatGPT, which involves
    three main steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-4](#figure-2-4)展示了ChatGPT的训练过程，涉及三个主要步骤：'
- en: Collection of demonstration data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 演示数据的收集
- en: In this step, human labelers provide examples of the desired model behavior
    on a distribution of prompts. The labelers are trained on the project and follow
    specific instructions to annotate the prompts accurately.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，人类标注员提供了一系列提示的期望模型行为的示例。标注员在项目上进行培训，并遵循具体指令以准确标注提示。
- en: Training a supervised policy
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 训练监督策略
- en: The demonstration data collected in the previous step is used to fine-tune a
    pretrained GPT-3 model using supervised learning. In supervised learning, models
    are trained on a labeled dataset where the correct answers are provided. This
    step helps the model to learn to follow the given instructions and produce outputs
    that align with the desired behavior.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上一步收集的演示数据用于使用监督学习微调预训练的GPT-3模型。在监督学习中，模型在提供正确答案的标记数据集上进行训练。这一步骤有助于模型学习遵循给定的指令并产生符合期望行为的输出。
- en: Collection of comparison data and reinforcement learning
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 比较数据的收集和强化学习
- en: In this step, a dataset of model outputs is collected, and human labelers rank
    the outputs based on their preference. A reward model is then trained to predict
    which outputs the labelers would prefer. Finally, reinforcement learning techniques,
    specifically the Proximal Policy Optimization (PPO) algorithm, are used to optimize
    the supervised policy to maximize the reward from the reward model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，收集了一个模型输出的数据集，并且人类标注者根据他们的偏好对这些输出进行排名。然后训练一个奖励模型来预测标注者会偏好哪些输出。最后，使用强化学习技术，特别是近端策略优化（PPO）算法，来优化监督策略，以最大化奖励模型中的奖励。
- en: This training process allows the ChatGPT model to align its behavior with human
    intent. The use of reinforcement learning with human feedback helped create a
    model that is more helpful, honest, and safe compared to the pretrained GPT-3
    model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练过程允许ChatGPT模型使其行为与人类意图保持一致。使用带有人类反馈的强化学习有助于创建一个比预训练的GPT-3模型更有帮助、更诚实、更安全的模型。
- en: '![ChatGPT Fine Tuning Approach](assets/pega_0204.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT微调方法](assets/pega_0204.png)'
- en: Figure 2-4\. The fine-tuning process for ChatGPT
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. ChatGPT的微调过程
- en: According to a [UBS study](https://oreil.ly/2Ivq2), by January 2023 ChatGPT
    set a new benchmark, amassing 100 million active users and becoming the fastest-growing
    consumer application in internet history. ChatGPT is now a go-to for customer
    service, virtual assistance, and numerous other applications that require the
    finesse of human-like conversation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一份[UBS研究](https://oreil.ly/2Ivq2)，到2023年1月，ChatGPT设定了新的基准，积累了1亿活跃用户，成为互联网历史上增长最快的消费应用。ChatGPT现在已成为客户服务、虚拟助手和许多其他需要类似人类对话技巧的应用的首选。
- en: GPT-4
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4
- en: In 2024, OpenAI released GPT-4, which excels in understanding complex queries
    and generating contextually relevant and coherent text. For example, GPT-4 scored
    in the 90th percentile of the bar exam with a score of 298 out of 400\. Currently,
    GPT-3.5-turbo is free to use in ChatGPT, but GPT-4 requires a [monthly payment](https://oreil.ly/UOEBM).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年，OpenAI发布了GPT-4，它在理解复杂查询和生成上下文相关且连贯的文本方面表现出色。例如，GPT-4在律师资格考试中得分达到90分，总分400分。目前，GPT-3.5-turbo在ChatGPT中免费使用，但GPT-4需要[每月支付](https://oreil.ly/UOEBM)。
- en: GPT-4 uses a [mixture-of-experts approach](https://oreil.ly/v45LZ); it goes
    beyond relying on a single model’s inference to produce even more accurate and
    insightful results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4采用[mixture-of-experts方法](https://oreil.ly/v45LZ)；它不仅超越了依赖单个模型的推理来产生更准确和有洞察力的结果。
- en: On May 13, 2024, OpenAI introduced [GPT-4o](https://oreil.ly/4ttmq), an advanced
    model capable of processing and reasoning across text, audio, and vision inputs
    in real time. This model offers enhanced performance, particularly in vision and
    audio understanding; it is also faster and more cost-effective than its predecessors
    due to its ability to process all three modalities in one neural network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年5月13日，OpenAI推出了[GPT-4o](https://oreil.ly/4ttmq)，这是一个能够实时处理和推理文本、音频和视觉输入的高级模型。该模型在视觉和音频理解方面提供了增强的性能；由于其能够在单个神经网络中处理所有三种模态，因此它比其前辈更快、更经济。
- en: Google’s Gemini
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌的Gemini
- en: After Google lost search market share due to ChatGPT usage, it initially released
    Bard on March 21, 2023\. Bard was a bit [rough around the edges](https://oreil.ly/Sj24h)
    and definitely didn’t initially have the same high-quality LLM responses that
    ChatGPT offered ([Figure 2-5](#figure-2-5)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ChatGPT的使用导致谷歌失去了搜索市场份额，它最初于2023年3月21日发布了Bard。Bard起初有些[粗糙](https://oreil.ly/Sj24h)，并且最初并没有提供与ChatGPT相同的高质量LLM响应（[图2-5](#figure-2-5)）。
- en: Google has kept adding extra features over time including code generation, visual
    AI, real-time search, and voice into Bard, bringing it closer to ChatGPT in terms
    of quality.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌一直在为Bard添加额外功能，包括代码生成、视觉AI、实时搜索和语音，使其在质量上更接近ChatGPT。
- en: On March 14, 2023, Google released [PaLM API](https://oreil.ly/EbI8-), allowing
    developers to access it on Google Cloud Platform. In April 2023, Amazon Web Services
    (AWS) released similar services such as [Amazon Bedrock](https://oreil.ly/4fNQX)
    and [Amazon’s Titan FMs](https://oreil.ly/FJ-7D). Google [rebranded Bard to Gemini](https://oreil.ly/EO42O)
    for their v1.5 release in February 2024 and started to get results similar to
    GPT-4.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年3月14日，谷歌发布了[PaLM API](https://oreil.ly/EbI8-)，允许开发者在谷歌云平台上访问它。2023年4月，亚马逊网络服务（AWS）发布了类似的服务，如[Amazon
    Bedrock](https://oreil.ly/4fNQX)和[亚马逊的Titan FMs](https://oreil.ly/FJ-7D)。谷歌在2024年2月的v1.5版本中将Bard重新命名为Gemini，并开始获得与GPT-4类似的结果。
- en: '![Google''s bard which is a similar application to ChatGPT.](assets/pega_0205.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![谷歌的 Bard，这是一个类似于 ChatGPT 的应用。](assets/pega_0205.png)'
- en: Figure 2-5\. Bard hallucinating results about the James Webb Space Telescope
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. Bard 对詹姆斯·韦伯太空望远镜的幻想结果
- en: Also, Google released two smaller [open source models](https://oreil.ly/LWIwv)
    based on the same architecture as Gemini. OpenAI is finally no longer the only
    obvious option for software engineers to integrate state-of-the-art LLMs into
    their applications.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，谷歌还发布了基于与 Gemini 相同架构的两个较小的 [开源模型](https://oreil.ly/LWIwv)。OpenAI 终于不再是软件工程师将最先进的
    LLM 集成到其应用程序中的唯一明显选择。
- en: Meta’s Llama and Open Source
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta 的 Llama 和开源
- en: Meta’s approach to language models differs significantly from other competitors
    in the industry. By sequentially releasing open source models [Llama](https://oreil.ly/LroPn),
    [Llama 2](https://oreil.ly/NeZLw) and [Llama 3](https://oreil.ly/Vwlo-), Meta
    aims to foster a more inclusive and collaborative AI development ecosystem.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 对语言模型的方法与行业中的其他竞争对手存在显著差异。通过依次发布开源模型 [Llama](https://oreil.ly/LroPn)，[Llama
    2](https://oreil.ly/NeZLw) 和 [Llama 3](https://oreil.ly/Vwlo-)，Meta 旨在培养一个更加包容和协作的人工智能开发生态系统。
- en: The open source nature of Llama 2 and Llama 3 has significant implications for
    the broader tech industry, especially for large enterprises. The transparency
    and collaborative ethos encourage rapid innovation, as problems and vulnerabilities
    can be quickly identified and addressed by the global developer community. As
    these models become more robust and secure, large corporations can adopt them
    with increased confidence.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 和 Llama 3 的开源特性对更广泛的科技行业具有重大影响，尤其是对大型企业而言。透明度和协作精神鼓励快速创新，因为问题和漏洞可以由全球开发者社区迅速识别和解决。随着这些模型变得更加稳健和安全，大型企业可以更加有信心地采用它们。
- en: Meta’s open source strategy not only democratizes access to state-of-the-art
    AI technologies but also has the potential to make a meaningful impact across
    the industry. By setting the stage for a collaborative, transparent, and decentralized
    development process, Llama 2 and Llama 3 are pioneering models that could very
    well define the future of generative AI. The models are available in 7, 8 and
    70 billion parameter versions on AWS, Google Cloud, Hugging Face, and other platforms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 的开源策略不仅使最先进的 AI 技术的访问民主化，而且有可能在整个行业中产生重大影响。通过为协作、透明和去中心化的开发过程奠定基础，Llama
    2 和 Llama 3 是开创性的模型，它们很可能定义了生成式 AI 的未来。这些模型在 AWS、Google Cloud、Hugging Face 和其他平台上有
    70 亿、80 亿和 700 亿参数版本。
- en: The open source nature of these models presents a double-edged sword. On one
    hand, it levels the playing field. This means that even smaller developers have
    the opportunity to contribute to innovation, improving and applying open source
    models to practical business applications. This kind of decentralized innovation
    could lead to breakthroughs that might not occur within the walled gardens of
    a single organization, enhancing the models’ capabilities and applications.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的开放源代码特性是一把双刃剑。一方面，它使竞争更加公平。这意味着即使是较小的开发者也有机会为创新做出贡献，改进和应用开源模型到实际商业应用中。这种去中心化的创新可能导致在单一组织围墙花园内可能不会发生的突破，从而增强模型的能力和应用。
- en: However, the same openness that makes this possible also poses potential risks,
    as it could allow malicious actors to exploit this technology for detrimental
    ends. This indeed is a concern that organizations like OpenAI share, suggesting
    that some degree of control and restriction can actually serve to mitigate the
    dangerous applications of these powerful tools.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正是这种使这一切成为可能的开源性也带来了潜在的风险，因为它可能允许恶意行为者利用这项技术达到有害的目的。这确实是像 OpenAI 这样的组织所关注的问题，他们建议一定程度上的控制和限制实际上可以用来减轻这些强大工具的危险应用。
- en: Leveraging Quantization and LoRA
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用量化（Quantization）和 LoRA（Low-Rank Adaptation）
- en: One of the game-changing aspects of these open source models is the potential
    for [quantization](https://oreil.ly/bkWXk) and the use of [LoRA](https://oreil.ly/zORsB)
    (low-rank approximations). These techniques allow developers to fit the models
    into smaller hardware footprints. Quantization helps to reduce the numerical precision
    of the model’s parameters, thereby shrinking the overall size of the model without
    a significant loss in performance. Meanwhile, LoRA assists in optimizing the network’s
    architecture, making it more efficient to run on consumer-grade hardware.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些开源模型中具有颠覆性的一个方面是[量化](https://oreil.ly/bkWXk)和[LoRA](https://oreil.ly/zORsB)（低秩近似）的潜力。这些技术允许开发者将模型适配到更小的硬件占用空间。量化有助于降低模型参数的数值精度，从而在不显著影响性能的情况下缩小模型的整体大小。同时，LoRA有助于优化网络的架构，使其在消费级硬件上运行更加高效。
- en: Such optimizations make fine-tuning these LLMs increasingly feasible on consumer
    hardware. This is a critical development because it allows for greater experimentation
    and adaptability. No longer confined to high-powered data centers, individual
    developers, small businesses, and start-ups can now work on these models in more
    resource-constrained environments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的优化使得在消费级硬件上微调这些LLM变得越来越可行。这是一个关键的发展，因为它允许进行更多的实验和适应性调整。不再局限于高性能数据中心，个人开发者、小型企业和初创公司现在可以在资源受限的环境中工作这些模型。
- en: Mistral
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mistral
- en: Mistral 7B, a brainchild of French start-up [Mistral AI](https://mistral.ai),
    emerges as a powerhouse in the generative AI domain, with its 7.3 billion parameters
    making a significant impact. This model is not just about size; it’s about efficiency
    and capability, promising a bright future for open source large language models
    and their applicability across a myriad of use cases. The key to its efficiency
    is the implementation of sliding window attention, a technique released under
    a permissive Apache open source license. Many AI engineers have fine-tuned on
    top of this model as a base, including the impressive [Zephr 7b beta](https://oreil.ly/Lg6_r)
    model. There is also [Mixtral 8x7b](https://oreil.ly/itsJG), a mixture of experts
    model (similar to the architecture of GPT-4), which achieves results similar to
    GPT-3.5-turbo.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B，法国初创公司[Mistral AI](https://mistral.ai)的杰作，在生成式AI领域崭露头角，其73亿个参数产生了重大影响。这个模型不仅仅关乎规模；它关乎效率和能力，为开源大型语言模型及其在众多用例中的应用前景提供了光明。其效率的关键在于滑动窗口注意力的实现，这是一种在宽松的Apache开源许可证下发布的技巧。许多AI工程师以此模型为基础进行了微调，包括令人印象深刻的[Zephr
    7b beta](https://oreil.ly/Lg6_r)模型。还有[Mixtral 8x7b](https://oreil.ly/itsJG)，这是一个专家混合模型（类似于GPT-4的架构），其结果与GPT-3.5-turbo相似。
- en: For a more detailed and up-to-date comparison of open source models and their
    performance metrics, visit the Chatbot [Arena Leaderboard](https://oreil.ly/ttiji)
    hosted by Hugging Face.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更详细和最新的开源模型及其性能指标的比较，请访问由Hugging Face主办的聊天机器人[排行榜](https://oreil.ly/ttiji)。
- en: 'Anthropic: Claude'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Anthropic: Claude'
- en: Released on July 11, 2023, [Claude 2](https://claude.ai/login) is setting itself
    apart from other prominent LLMs such as ChatGPT and LLaMA, with its pioneering
    [Constitutional AI](https://oreil.ly/Tim9W) approach to AI safety and alignment—training
    the model using a list of rules or values. A notable enhancement in Claude 2 was
    its expanded context window of 100,000 tokens, as well as the ability to upload
    files. In the realm of generative AI, a *context window* refers to the amount
    of text or data the model can actively consider or keep in mind when generating
    a response. With a larger context window, the model can understand and generate
    based on a broader context.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年7月11日发布的[Claude 2](https://claude.ai/login)通过其开创性的[宪法AI](https://oreil.ly/Tim9W)方法在AI安全和一致性方面脱颖而出——使用一系列规则或价值观来训练模型。Claude
    2的一个显著改进是其扩展的上下文窗口，达到10万个标记，以及上传文件的能力。在生成式AI领域，*上下文窗口*指的是模型在生成响应时可以积极考虑或记住的文本或数据量。拥有更大的上下文窗口，模型可以基于更广泛的环境进行理解和生成。
- en: This advancement garnered significant enthusiasm from AI engineers, as it opened
    up avenues for new and more intricate use cases. For instance, Claude 2’s augmented
    ability to process more information at once makes it adept at summarizing extensive
    documents or sustaining in-depth conversations. The advantage was short-lived,
    as OpenAI released their 128K version of GPT-4 only [six months later](https://oreil.ly/BWxrn).
    However, the fierce competition between rivals is pushing the field forward.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这一进步引起了AI工程师的极大热情，因为它为新的更复杂的用例开辟了道路。例如，Claude 2同时处理更多信息的能力使其擅长总结大量文档或维持深入对话。然而，这一优势很快就被OpenAI发布的GPT-4
    128K版本所取代，仅[六个月后](https://oreil.ly/BWxrn)。然而，竞争对手之间的激烈竞争正在推动该领域向前发展。
- en: The next generation of Claude included [Opus](https://oreil.ly/NH0jh), the first
    model to rival GPT-4 in terms of intelligence, as well as Haiku, a smaller model
    that is lightning-fast with the competitive price of $0.25 per million tokens
    (half the cost of GPT-3.5-turbo at the time).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Claude的下一代包括了[Opus](https://oreil.ly/NH0jh)，这是第一个在智能方面与GPT-4相媲美的模型，以及Haiku，这是一个更小的模型，速度极快，价格为每百万个标记0.25美元（当时是GPT-3.5-turbo成本的一半）。
- en: GPT-4V(ision)
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4V(视觉)
- en: In a significant leap forward, on September 23, 2023, OpenAI expanded the capabilities
    of GPT-4 with the introduction of Vision, enabling users to instruct GPT-4 to
    analyze images alongside text. This innovation was also reflected in the update
    to ChatGPT’s interface, which now supports the inclusion of both images and text
    as user inputs. This development signifies a major trend toward *multimodal models*,
    which can seamlessly process and understand multiple types of data, such as images
    and text, within a single context.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个重大的飞跃中，2023年9月23日，OpenAI通过引入视觉功能扩展了GPT-4的能力，使用户能够指令GPT-4分析图像和文本。这一创新也体现在ChatGPT界面的更新中，现在它支持包含图像和文本作为用户输入。这一发展标志着向*多模态模型*的重大趋势转变，这些模型可以在单一语境中无缝处理和理解多种类型的数据，如图像和文本。
- en: Model Comparison
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型比较
- en: The market for LLMs is dominated by OpenAI at the time of writing, with its
    state-of-the-art GPT-4 model widely considered to have a significant lead. The
    closest competitor is Anthropic, and there is widespread excitement at the potential
    of smaller open source models such as Llama and Mistral, particularly with respects
    to fine-tuning. Although commentators expect OpenAI to continue to deliver world-beating
    models in the future, as open source models get *good enough* at more tasks, AI
    workloads may shift toward local fine-tuned models. With advances in model performance
    and quantization (methods for trading off accuracy versus size and compute cost),
    it may be possible to one day run LLMs on your mobile phone or other devices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，LLM市场由OpenAI主导，其最先进的GPT-4模型被广泛认为具有显著领先优势。最接近的竞争对手是Anthropic，人们对Llama和Mistral等小型开源模型在微调方面的潜力感到兴奋。尽管评论家们预计OpenAI未来将继续推出世界领先的模型，但随着开源模型在更多任务上达到*足够好*的水平，AI工作负载可能会转向本地微调模型。随着模型性能和量化（在精度、大小和计算成本之间权衡的方法）的进步，未来某一天在您的手机或其他设备上运行LLM可能成为可能。
- en: For now, the best way to get a sense for what the models are good at is to run
    the same prompt across multiple models and compare the responses. One thing that
    regularly stands out in our work is that GPT-4 is much better at following instructions,
    as is demonstrated in the following example where it was the only model to respond
    in the right format, with names that matched the examples (starting with the letter
    *i*), as desired.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，了解模型擅长什么最好的方法是在多个模型上运行相同的提示并比较响应。在我们工作中经常突出的一点是，GPT-4在遵循指令方面表现得更好，如下面的例子所示，它是唯一一个以正确格式、以字母*i*开头并符合要求的名称响应的模型。
- en: 'Input:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output (GPT-4):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（GPT-4）：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output (Claude 3):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（Claude 3）：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Output (Llama 3 70b):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（Llama 3 70b）：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The journey of LLMs from their inception to their current advanced states is
    a tale of relentless innovation, collaboration, and intense competition. As these
    models continue to evolve, they are likely to become even more integral parts
    of our daily lives, changing the way we interact with technology and even with
    each other.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLM的诞生到其当前的高级状态，这是一个关于不懈创新、合作和激烈竞争的故事。随着这些模型继续进化，它们很可能会成为我们日常生活中的重要组成部分，改变我们与技术以及彼此互动的方式。
- en: By understanding the historical context and capabilities of these models, you
    can better appreciate the tools at our disposal for various applications, from
    prompt engineering to the development of intelligent virtual agents. It’s important
    to note, however, that while these models offer expansive possibilities, data
    privacy remains a crucial concern. If these models use your data for retraining
    or fine-tuning, exercise caution and refrain from inputting sensitive information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些模型的历史背景和功能，您可以更好地欣赏我们可用于各种应用的工具，从提示工程到智能虚拟代理的开发。然而，需要注意的是，尽管这些模型提供了广阔的可能性，数据隐私仍然是一个关键的关注点。如果这些模型使用您的数据进行再训练或微调，请谨慎行事，并避免输入敏感信息。
- en: In the next chapter, you will learn all the basic prompt engineering techniques
    for working with text LLMs. You’ll learn the essential skills needed to get the
    most out of powerful language models like GPT-4\. Exciting insights and practical
    methods await you as you unlock the true potential of generative AI.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习所有基本的提示工程技术，用于与文本LLMs（大型语言模型）一起工作。您将学习到充分利用像GPT-4这样的强大语言模型所需的基本技能。随着您解锁生成式AI的真正潜力，您将迎来令人兴奋的见解和实用的方法。
