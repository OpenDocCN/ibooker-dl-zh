<html><head></head><body><section data-pdf-bookmark="Chapter 18. Introduction to RAG" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch18_introduction_to_rag_1748550073472936">&#13;
      <h1><span class="label">Chapter 18. </span>Introduction to RAG</h1>&#13;
      <p>Remember that first time you <a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="about" data-type="indexterm" id="ch18rag"/><a contenteditable="false" data-primary="generative AI" data-secondary="retrieval-augmented generation" data-tertiary="about" data-type="indexterm" id="ch18rag2"/>chatted with an LLM like ChatGPT—and how it was extremely insightful about things you didn’t expect it to know? I had worked with LLMs before the release of ChatGPT and on projects that highlighted LLM abilities, and I <em>still</em> was surprised by what they could do. Remember the famous on-stage demonstration by Google, where the CEO had a conversation with the planet Pluto? It was one of those fundamental mind shifts in the possibilities of AI that we’re <em>still</em> exploring as it continues to evolve.</p>&#13;
      <p>But, despite all that brilliance, there were still limitations, and the more I and others worked with LLMs, the more we encountered them. The transformer-based architecture that we discussed in <a data-type="xref" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">Chapter 15</a> was brilliant at snarfing up text data, creating QKV mappings from it, and learning how to artificially understand the semantics of the text as a result. But despite the volume of text used to build those mappings, there was—and always is—one blind spot:<a contenteditable="false" data-primary="hallucinations from LLMs" data-secondary="model not trained on private data" data-type="indexterm" id="id1851"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="hallucinations" data-tertiary="model not trained on private data" data-type="indexterm" id="id1852"/><a contenteditable="false" data-primary="private data" data-secondary="hallucinations from" data-type="indexterm" id="id1853"/> private data. In particular, if there is data that you want to work with that the model was not trained on, you’re at a major risk of hallucination!</p>&#13;
      <p>Gaining skills to help mitigate this blind spot could potentially be the <em>most</em> valuable thing you can do as a software developer.</p>&#13;
      <p>For this chapter, I want you to think about AI models and in particular large generative models like LLMs <em>differently</em>. Stop seeing them as intelligent and knowledgeable and start seeing them as <em>utilities</em> to help you parse your data better. Think of everything they have learned not as a knowledge base in and of itself but as a way that they have generalized understanding of language by being extensively well read.</p>&#13;
      <p>I call this <em>artificial understanding,</em> as a complementary technology to AI.<a contenteditable="false" data-primary="artificial understanding" data-type="indexterm" id="id1854"/> </p>&#13;
      <p>Then, once you treat your favorite LLM as an engine for artificial understanding, you can start having it understand your private text—stuff that wasn’t in its training set—and through that understanding, process your text in new and interesting ways.</p>&#13;
      <p>Let’s explore this with a scenario. Imagine you’re discussing your favorite sci-fi novel with an AI model. You want to ask about characters, plot, theme, and stuff like that, but the model struggles with the specifics, offering only general responses—or worse, hallucinating them. For example, take a look at <a data-type="xref" href="#ch18_figure_1_1748550073457538">Figure 18-1</a>, which shows the results I got when I was chatting with ChatGPT about a character from a novel called <em>Space Cadets.</em></p>&#13;
      <figure><div class="figure" id="ch18_figure_1_1748550073457538">&#13;
        <img src="assets/aiml_1801.png"/>&#13;
        <h6><span class="label">Figure 18-1. </span>Chatting with GPT about a character</h6>&#13;
      </div></figure>&#13;
      <p>This is all very interesting—except that it’s wrong. First of all, the character is from <em>North </em>Korea, not <em>South </em>Korea. </p>&#13;
      <p>GPT is being confidently incorrect. Why? Because this novel isn’t in the training set! I wrote it in 2014, and it was published by a small press that folded just a few months afterward. As such, it’s relatively obscure and the perfect fodder for us to use to explore RAG. By the end of this chapter, you’ll have used your PyTorch skills to create an application that is much smarter at understanding this novel and, indeed, the character in question. And yes, you’ll have the full novel to work with!</p>&#13;
      <p>A small aside: when I first used an LLM for tasks like this, my mind was blown.<a contenteditable="false" data-primary="artificial understanding" data-type="indexterm" id="id1855"/> Its ability to <em>artificially understand</em> the contents and context of my own writing was like having a partner beside me to critique my work and to help me dig deep into the characters and themes. The book ends on a cliffhanger, and I never came back to write any sequels. Having conversations with an LLM about the character arcs, etc., gave me a whole new fount of wisdom about where it could go. </p>&#13;
      <p>And of course, you aren’t limited to works of fiction. Almost every business has a trove of internal intelligence that’s locked up in documents that would take a human too much time to read, index, cross-correlate, and understand to be able to answer queries—so the ability of an LLM to artificially understand them to help you mine the text for knowledge is second-to-none. </p>&#13;
      <p>That’s why I’m excited about RAG. And I hope you will be, too, after you finish this chapter.<a contenteditable="false" data-primary="" data-startref="ch18rag" data-type="indexterm" id="id1856"/><a contenteditable="false" data-primary="" data-startref="ch18rag2" data-type="indexterm" id="id1857"/></p>&#13;
      <section data-pdf-bookmark="What Is RAG?" data-type="sect1"><div class="sect1" id="ch18_what_is_rag_1748550073473226">&#13;
        <h1>What Is RAG?</h1>&#13;
        <p>The acronym <em>RAG</em> stands for <em>retrieval augmented generation</em>, which<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="explaining what RAG is" data-type="indexterm" id="id1858"/><a contenteditable="false" data-primary="generative AI" data-secondary="retrieval-augmented generation" data-tertiary="explaining what RAG is" data-type="indexterm" id="id1859"/> works to bridge the knowledge gap between what an LLM has been trained on and private data you own that it doesn’t have mappings for. At query time, as well as with a prompt like “Tell me about the character…,” we’ll also feed it information snippets from the local datastore. So, for example, if we’re querying about a character from a novel, local data might include things like her hometown, her favorite food, her values, and how she speaks. When we pass <em>that</em> data along with the query, a lot of it <em>is</em> in the training set for the LLM, and as such, the LLM can have a much more informed opinion about her. Not least, the mistake the LLM made in <a data-type="xref" href="#ch18_figure_1_1748550073457538">Figure 18-1</a> can be mitigated—when the LLM is given her hometown, it can at least get the country right!</p>&#13;
        <p><a data-type="xref" href="#ch18_figure_2_1748550073457600">Figure 18-2</a> shows the flow of a typical query to an LLM. It’s quite basic: you pass in a prompt and the transformers do their magic by going through the knowledge that the LLM learned to produce QKV values to generate a response. </p>&#13;
        <figure><div class="figure" id="ch18_figure_2_1748550073457600">&#13;
          <img alt="" src="assets/aiml_1802.png"/>&#13;
          <h6><span class="label">Figure 18-2. </span>Typical flow of a query to an LLM</h6>&#13;
        </div></figure>&#13;
        <p>As we’ve demonstrated , if the LLM doesn’t have much knowledge of the specifics, it will fill in the gaps—and it does a pretty good job. For example, even though it got her nationality wrong in the example shown in <a data-type="xref" href="#ch18_figure_1_1748550073457538">Figure 18-1</a>, it was at least able to infer that her name is Korean! </p>&#13;
        <p class="pagebreak-before">With RAG, we change this flow to augment the query with extra information that we bundle in (see <a data-type="xref" href="#ch18_figure_3_1748550073457629">Figure 18-3</a>). We do this by having a local database of the content of the book, and then we search that for things that are <em>similar</em> to the query. You’ll see the details of how that works shortly. </p>&#13;
        <figure><div class="figure" id="ch18_figure_3_1748550073457629">&#13;
          <img alt="" src="assets/aiml_1803.png"/>&#13;
          <h6><span class="label">Figure 18-3. </span>Typical flow of a RAG query with an LLM</h6>&#13;
        </div></figure>&#13;
        <p>The goal here is to enhance the initial prompt with a lot of additional context. So, scenes in the book might have her mention her hometown, her family history, favorite foods, why she likes people or things, etc. When that is passed to the LLM along with the query, the LLM has a lot more to work with—including things that it <em>has</em> learned about, so its interpretation of the character becomes a lot more intelligent. It therefore <em>artificially understands</em> the content better.</p>&#13;
        <p>The key to all of this, of course, is in being able to retrieve the best information to bundle with the prompt to make the most of the LLM. You can achieve this by storing content from the source material (in this case, the book) in a way that lets you do searches for things that are semantically relevant. To that end, you’ll use a vector store. We’ll explore that next.</p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Getting Started with RAG" data-type="sect1"><div class="sect1" id="ch18_getting_started_with_rag_1748550073473315">&#13;
        <h1>Getting Started with RAG</h1>&#13;
        <p>To get started, let’s first explore<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-type="indexterm" id="ch18strtall"/><a contenteditable="false" data-primary="generative AI" data-secondary="retrieval-augmented generation" data-tertiary="getting started with RAG" data-type="indexterm" id="ch18strtall2"/> how to create a vector database. To do this, you’ll use a database engine that supports vectors and similarity search. </p>&#13;
        <p>These work with the idea of storing text as vectors that represent it by using embeddings. We saw these in action in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>. <a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="API by LangChain" data-type="indexterm" id="id1860"/><a contenteditable="false" data-primary="LangChain RAG API" data-type="indexterm" id="id1861"/><a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="Chroma vector store database" data-type="indexterm" id="id1862"/><a contenteditable="false" data-primary="APIs" data-secondary="RAG API by LangChain" data-type="indexterm" id="id1863"/><a contenteditable="false" data-primary="Chroma" data-type="indexterm" id="id1864"/><a contenteditable="false" data-primary="vector store database Chroma" data-type="indexterm" id="id1865"/>For simplicity, you’ll start by using a pre-built, pre-learned set of embeddings from OpenAI with an API provided by LangChain. These will be combined with a vector store database called Chroma that is free and open source.</p>&#13;
        <p>Let’s include the following imports:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">PyPDFLoader</code>&#13;
<code class="kn">from</code> <code class="nn">langchain.text_splitter</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_community.embeddings</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_community.vectorstores</code> <code class="kn">import</code> <code class="n">Chroma</code></pre>&#13;
        <p>The <code>PyPDFLoader</code>, as its name suggests, is used for managing PDF files in Python. I’m providing the book as a PDF, so we’ll need this.</p>&#13;
        <p>The <code>RecursiveCharacterTextSplitter</code> is a really useful class for slicing the book up into text chunks. It provides flexibility on the size of the chunk and the overlap between chunks. We’ll explore that in detail a little later.</p>&#13;
        <p>The <code>OpenAIEmbeddings</code> class <a contenteditable="false" data-primary="OpenAIEmbeddings class" data-type="indexterm" id="id1866"/><a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="OpenAIEmbeddings class" data-type="indexterm" id="id1867"/><a contenteditable="false" data-primary="embeddings" data-secondary="OpenAIEmbeddings class" data-type="indexterm" id="id1868"/><a contenteditable="false" data-primary="pretrained embeddings" data-secondary="OpenAIEmbeddings class" data-type="indexterm" id="id1869"/><a contenteditable="false" data-primary="embeddings" data-secondary="pretrained embeddings" data-tertiary="OpenAIEmbeddings class" data-type="indexterm" id="id1870"/>gives us access to the embeddings learned by Open AI while training GPT, and it’s a nice shortcut to make things quicker for us. We don’t need to learn our own embeddings for this application—as long as our text is encoded in a set of embeddings and our prompt uses the same ones, we can use them for similarity search. There are lots of options for this, and Hugging Face is a great repository where you can look for the latest and greatest.</p>&#13;
        <p>Finally, the <code>Chroma</code> database provides us with the ability to store and search text based on similarity. </p>&#13;
        <section data-pdf-bookmark="Understanding Similarity" data-type="sect2"><div class="sect2" id="ch18_understanding_similarity_1748550073473390">&#13;
          <h2>Understanding Similarity</h2>&#13;
          <p>We’ve mentioned similarity<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="similarity explained" data-type="indexterm" id="id1871"/><a contenteditable="false" data-primary="similarity" data-type="indexterm" id="id1872"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="cosine similarity" data-type="indexterm" id="id1873"/><a contenteditable="false" data-primary="cosine similarity" data-type="indexterm" id="id1874"/> a few times now, and it’s important for you to understand where it can be useful for you. Recall that in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>, we discussed how embeddings can be used to turn words into vectors. A simple representation of this is shown in <a data-type="xref" href="#ch18_figure_4_1748550073457653">Figure 18-4</a>.</p>&#13;
          <figure><div class="figure" id="ch18_figure_4_1748550073457653">&#13;
            <img alt="" src="assets/aiml_1804.png"/>&#13;
            <h6><span class="label">Figure 18-4. </span>Words as vectors</h6>&#13;
          </div></figure>&#13;
          <p>Here, we plot the words <em>Awesome</em>, <em>Great</em>, and <em>Terrible</em> based on their learned vectors. It’s an oversimplification in two dimensions, but hopefully it’s enough to demonstrate the concept. In this case, we can visualize that <em>Awesome</em> and <em>Great</em> are similar because they’re close to each other, but we can quantify that by looking at the angle of the vectors between them. Taking a function of that angle, like its <em>cosine,</em> can give us a great indication of how close the vectors are to each other. Similarly, if we look at the word <em>Terrible</em>, the angle between <em>Awesome</em> and <em>Terrible</em> is very large, indicating that the two words aren’t similar. </p>&#13;
          <p>This process is called <em>cosine similarity</em>, and we’ll be using it as we create our RAG. We’ll split the book into chunks, calculate the embedding for those chunks, and store them in the database. Then, by using a store (ChromaDB, in this case) that provides a search based on cosine similarity, we’ll have the key to our RAG.</p>&#13;
          <p>There are many different ways to calculate similarity, with cosine similarity being one of them. It’s worth looking into these other ways to fine-tune your RAG solution, but for the rest of this chapter, I’ll use cosine similarity because of its simplicity.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Creating the Database" data-type="sect2"><div class="sect2" id="ch18_creating_the_database_1748550073473485">&#13;
          <h2>Creating the Database</h2>&#13;
          <p>To create the vector store,<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="creating the database" data-type="indexterm" id="ch18ragdb"/><a contenteditable="false" data-primary="vector store database Chroma" data-secondary="creating the database" data-type="indexterm" id="ch18ragdb2"/><a contenteditable="false" data-primary="PDF loaded and processed into embeddings" data-type="indexterm" id="ch18ragdb3"/><a contenteditable="false" data-primary="embeddings" data-secondary="OpenAIEmbeddings class" data-tertiary="PDF processed into embeddings" data-type="indexterm" id="ch18ragdb4"/><a contenteditable="false" data-primary="OpenAIEmbeddings class" data-secondary="PDF processed into embeddings" data-type="indexterm" id="ch18ragdb5"/><a contenteditable="false" data-primary="pretrained embeddings" data-secondary="OpenAIEmbeddings class" data-tertiary="PDF processed into embeddings" data-type="indexterm" id="ch18ragdb6"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="embeddings" data-tertiary="PDF processed into" data-type="indexterm" id="ch18ragdb7"/><a contenteditable="false" data-primary="embeddings" data-secondary="pretrained embeddings" data-tertiary="PDF processed into embeddings" data-type="indexterm" id="ch18ragdb8"/> we’ll go through the process of loading the PDF file, splitting it into chunks, calculating the chunks’ embeddings, and then storing them. Let’s look at this step-by-step.</p>&#13;
          <p>First, we’ll load the PDF file by using <code>PyPDFLoader</code>:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the PDF file</code>&#13;
<code class="n">loader</code> <code class="o">=</code> <code class="n">PyPDFLoader</code><code class="p">(</code><code class="n">pdf_path</code><code class="p">)</code>&#13;
<code class="n">documents</code> <code class="o">=</code> <code class="n">loader</code><code class="o">.</code><code class="n">load</code><code class="p">()</code></pre>&#13;
          <p>Next, we’ll set up a text splitter that reads what we’ll use to chunk the text. An important part of your application will be establishing the appropriate sizes of chunks: </p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Split the documents into chunks</code>&#13;
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code>&#13;
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>&#13;
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code>&#13;
    <code class="n">length_function</code><code class="o">=</code><code class="nb">len</code><code class="p">,</code>&#13;
    <code class="n">add_start_index</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
<code class="p">)</code></pre>&#13;
          <p>In this case, the code will split the text into chunks of one thousand characters. But it uses a recursive strategy to calculate the split, in which it tries to do it on the natural boundaries in the text, rather than making hard cuts at exactly one thousand characters. It tries to split on newlines first, then on sentences, then on punctuation, and then on spaces. As a last resort, it will split in the middle of a word. </p>&#13;
          <p>The overlap means that the next chunk won’t start at the immediate next character but around two hundred characters back. If we have these overlaps, some text will be included twice in the data—and that’s OK. It means that we won’t lose content by splitting in the middle of a sentence, etc. You should explore the size of the chunk and overlap based on what suits your scenario. Larger chunks like this will be faster to search because there will be fewer chunks than if they were smaller, but it also lowers the likelihood of the chunks being very similar to your prompt if the prompt is shorter than the chunk size.</p>&#13;
          <p>The splitter provides the ability for you to specify your own length function if you want to measure length differently. In this case, I’m just using Python’s default <code>len</code> function. Typically, for a RAG like this, you may not need to override the <code>len</code> function, but the idea is that different models and encoders may count tokens in different ways. For example, GPT 3.5 recognizes a phrase like <code>lol</code> as a single token, but an emoji can be four tokens. </p>&#13;
          <p>The <code>add_start_index</code> parameter adds metadata to each chunk, indicating where it was located in the original text. This is useful for debugging, in which you can trace back where each chunk came from or provide things like citations.</p>&#13;
          <p>Once you’ve specified the text, you can use it to split the PDF into multiple texts:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">texts</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">documents</code><code class="p">)</code></pre>&#13;
          <p>Now that you have the texts, you can turn them into embeddings by using the <code>OpenAIEmbeddings</code> class, and you can also specify that you want a vector store using Chroma by passing it the documents:<a contenteditable="false" data-primary="OpenAIEmbeddings class" data-secondary="OPENAI_API_KEY" data-type="indexterm" id="id1875"/><a contenteditable="false" data-primary="OPENAI_API_KEY environment variable" data-type="indexterm" id="id1876"/><a contenteditable="false" data-primary="environment variables" data-secondary="OPENAI_API_KEY" data-type="indexterm" id="id1877"/><a contenteditable="false" data-primary="embeddings" data-secondary="OpenAIEmbeddings class" data-tertiary="OPENAI_API_KEY environment variable" data-type="indexterm" id="id1878"/><a contenteditable="false" data-primary="embeddings" data-secondary="pretrained embeddings" data-tertiary="OPENAI_API_KEY environment variable" data-type="indexterm" id="id1879"/><a contenteditable="false" data-primary="pretrained embeddings" data-secondary="OpenAIEmbeddings class" data-tertiary="OPENAI_API_KEY environment variable" data-type="indexterm" id="id1880"/><a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="OPENAI_API_KEY environment variable" data-type="indexterm" id="id1881"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Initialize OpenAI embeddings</code>&#13;
<code class="c1"># Make sure to set your OPENAI_API_KEY environment variable</code>&#13;
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
 &#13;
<code class="c1"># Create and persist the vector store</code>&#13;
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">Chroma</code><code class="o">.</code><code class="n">from_documents</code><code class="p">(</code>&#13;
    <code class="n">documents</code><code class="o">=</code><code class="n">texts</code><code class="p">,</code>&#13;
    <code class="n">embedding</code><code class="o">=</code><code class="n">embeddings</code><code class="p">,</code>&#13;
    <code class="n">persist_directory</code><code class="o">=</code><code class="n">persist_directory</code>&#13;
<code class="p">)</code></pre>&#13;
          <p>As shown, you then simply pass the texts and embeddings you specified and a directory to store the embeddings. Then save the vector store to disk with this:<a contenteditable="false" data-primary="vector store database Chroma" data-secondary="saving to disk" data-type="indexterm" id="id1882"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Persist the vector store</code>&#13;
<code class="n">vectorstore</code><code class="o">.</code><code class="n">persist</code><code class="p">()</code></pre>&#13;
          <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
            <p>The <code>OpenAIEmbeddings</code> requires an <code>OPENAI_API_KEY</code> environment variable. You can get one at the <a href="https://oreil.ly/41hwI">Open AIPlatform website</a> and then follow the instructions for your operating system by setting one. Make sure you name it exactly as shown.</p>&#13;
          </div>&#13;
          &#13;
          <p>The underlying database<a contenteditable="false" data-primary="vector store database Chroma" data-secondary="SQLite3" data-type="indexterm" id="id1883"/><a contenteditable="false" data-primary="SQLite vector store database" data-type="indexterm" id="id1884"/> is an SQLite3 one (see <a data-type="xref" href="#ch18_figure_5_1748550073457676">Figure 18-5</a>).</p>&#13;
          <figure><div class="figure" id="ch18_figure_5_1748550073457676">&#13;
            <img src="assets/aiml_1805.png"/>&#13;
            <h6><span class="label">Figure 18-5. </span>The directory containing the ChromaDB content</h6>&#13;
          </div></figure>&#13;
          <p>This gives you the ability<a contenteditable="false" data-primary="DB Browser for SQLite" data-type="indexterm" id="id1885"/> to browse and inspect the database by using any tools that work with SQLite. So, for example, you can use the free <a href="https://sqlitebrowser.org">DB Browser for SQLite</a> to access the data (see <a data-type="xref" href="#ch18_figure_6_1748550073457698">Figure 18-6</a>).</p>&#13;
          <figure><div class="figure" id="ch18_figure_6_1748550073457698">&#13;
            <img src="assets/aiml_1806.png"/>&#13;
            <h6><span class="label">Figure 18-6. </span>Browsing data in the SQLite browser</h6>&#13;
          </div></figure>&#13;
          <p>Now that we have the vector store, let’s explore what happens when we want to search it for similar text.<a contenteditable="false" data-primary="" data-startref="ch18ragdb" data-type="indexterm" id="id1886"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb2" data-type="indexterm" id="id1887"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb3" data-type="indexterm" id="id1888"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb4" data-type="indexterm" id="id1889"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb5" data-type="indexterm" id="id1890"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb6" data-type="indexterm" id="id1891"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb7" data-type="indexterm" id="id1892"/><a contenteditable="false" data-primary="" data-startref="ch18ragdb8" data-type="indexterm" id="id1893"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Performing a Similarity Search" data-type="sect2"><div class="sect2" id="ch18_performing_a_similarity_search_1748550073473554">&#13;
          <h2>Performing a Similarity Search</h2>&#13;
          <p>Once you have the vector store set up, it’s easy to search it.<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="similarity search of vector store" data-type="indexterm" id="id1894"/><a contenteditable="false" data-primary="vector store database Chroma" data-secondary="similarity search" data-type="indexterm" id="id1895"/> </p>&#13;
          <p>Here’s a function you can use to perform a similarity search with the vector store:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">search_vectorstore</code><code class="p">(</code><code class="n">vectorstore</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">3</code><code class="p">):</code>&#13;
    <code class="n">results</code> <code class="o">=</code> <code class="n">vectorstore</code><code class="o">.</code><code class="n">similarity_search</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">results</code></pre>&#13;
          <p>As you can see, it’s pretty straightforward! You can override or extend some of the functionality if you like with optional parameters, including the following:</p>&#13;
          <dl>&#13;
            <dt>Search_type </dt>&#13;
            <dd>&#13;
              <p>This defaults to <code>similarity</code> but can also be <code>mmr</code> for <em>maximum marginal relevance</em> (MMR), which is worth experimenting with as you build out production systems. MMR is particularly useful when you want to avoid redundant results.</p>&#13;
            </dd>&#13;
            <dt>Distance_metric </dt>&#13;
            <dd>&#13;
              <p>This defaults to <code>cosine</code>, as we saw earlier, but it can also be <code>l2</code>, which is the <em> distance</em>—effectively, the straight-line distance between the two vectors in the embedding space. Alternatively, it can be <code>ip</code> for <em>inner product</em>, which provides a very fast calculation but at the cost of lower accuracy.</p>&#13;
            </dd>&#13;
            <dt>Lambda_mult </dt>&#13;
            <dd>&#13;
              <p>This is an optional value between 0 and 1 that you use to control the strictness of the distance measurement. A value of 1.0 will give highly relevant scores, and a value of 0.0 will give much more diverse scores.</p>&#13;
            </dd>&#13;
          </dl>&#13;
          <p>As you build systems, I recommend that you try multiple approaches to see which works best for your scenario.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Putting It All Together" data-type="sect2"><div class="sect2" id="ch18_putting_it_all_together_1748550073473627">&#13;
          <h2>Putting It All Together</h2>&#13;
          <p>Now, you can use code like<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="getting started with RAG" data-tertiary="putting it all together" data-type="indexterm" id="id1896"/> the following to take your PDF, slice and store it as vectors in the store, and run a query against it:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Path to your PDF file</code>&#13;
<code class="n">pdf_path</code> <code class="o">=</code> <code class="s2">"space-cadets-2020-master.pdf"</code>&#13;
 &#13;
<code class="c1"># Create the vector store</code>&#13;
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">create_vectorstore</code><code class="p">(</code><code class="n">pdf_path</code><code class="p">)</code>&#13;
 &#13;
<code class="c1"># Example search</code>&#13;
<code class="n">query</code> <code class="o">=</code> <code class="s2">"Give me some details about Soo-Kyung Kim. </code><code class="w"/>&#13;
         <code class="n">Where</code> <code class="ow">is</code> <code class="n">she</code> <code class="n">from</code><code class="p">,</code> <code class="n">what</code> <code class="n">does</code> <code class="n">she</code> <code class="n">like</code><code class="p">,</code> <code class="n">tell</code> <code class="n">me</code> <code class="nb">all</code> <code class="n">about</code> <code class="n">her</code><code class="err">?</code><code class="s2">"</code><code class="w"/>&#13;
<code class="n">results</code> <code class="o">=</code> <code class="n">search_vectorstore</code><code class="p">(</code><code class="n">vectorstore</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="mi">5</code><code class="p">)</code></pre>&#13;
          <p>When running this, I got detailed results about her character. Here are some snippets:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="err">“</code><code class="n">I</code> <code class="n">think</code> <code class="n">we</code> <code class="n">are</code> <code class="n">going</code> <code class="n">to</code> <code class="n">be</code> <code class="n">good</code> <code class="n">friends</code><code class="p">,</code><code class="err">”</code> <code class="n">said</code> <code class="n">Soo</code><code class="o">-</code><code class="n">Kyung</code><code class="o">.</code> <code class="err">“</code><code class="n">I</code> <code class="n">like</code> <code class="n">how</code>&#13;
<code class="n">you</code> <code class="n">are</code> <code class="n">straightforward</code><code class="o">.</code> <code class="n">I</code> <code class="n">am</code> <code class="n">too</code><code class="p">,</code> <code class="n">but</code> <code class="n">that</code> <code class="n">intimidates</code> <code class="n">some</code> <code class="n">people</code><code class="o">.</code><code class="err">”</code>&#13;
<code class="err">“</code><code class="n">So</code> <code class="n">where</code> <code class="n">are</code> <code class="n">you</code> <code class="n">from</code><code class="err">?”</code>&#13;
<code class="err">“</code><code class="n">I</code> <code class="n">am</code> <code class="kn">from</code> <code class="nn">a</code> <code class="n">small</code> <code class="n">village</code> <code class="n">called</code> <code class="n">Sijungho</code><code class="p">,</code><code class="err">”</code> <code class="n">continued</code> <code class="n">Soo</code> <code class="o">-</code>&#13;
<code class="n">Kyung</code><code class="o">.</code> <code class="err">“</code><code class="n">There</code><code class="err">’</code><code class="n">s</code> <code class="ow">not</code> <code class="n">much</code> <code class="n">to</code> <code class="n">see</code> <code class="n">there</code><code class="o">.</code><code class="err">”</code>&#13;
<code class="err">“</code><code class="n">Sounds</code> <code class="n">Korean</code><code class="p">,</code><code class="err">”</code> <code class="n">said</code> <code class="n">Aisha</code><code class="o">.</code> <code class="err">“</code><code class="n">You</code> <code class="kn">from</code> <code class="nn">South</code> <code class="n">Korea</code><code class="err">?”</code>&#13;
<code class="err">“</code><code class="n">North</code> <code class="n">Korea</code><code class="p">,</code><code class="err">”</code> <code class="n">corrected</code> <code class="n">Soo</code> <code class="o">-</code><code class="n">Kyung</code><code class="o">.</code> <code class="err">“</code><code class="n">I</code><code class="err">’</code><code class="n">ve</code> <code class="n">never</code> <code class="n">even</code> <code class="n">been</code> <code class="n">to</code>&#13;
<code class="n">South</code> <code class="n">Korea</code><code class="o">.</code><code class="err">”</code>&#13;
 </pre>&#13;
          <p>So, when we’re making a query about the character to an LLM, we have all this extra content. We’ll explore that next.<a contenteditable="false" data-primary="" data-startref="ch18strtall" data-type="indexterm" id="id1897"/><a contenteditable="false" data-primary="" data-startref="ch18strtall2" data-type="indexterm" id="id1898"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Using RAG Content with an LLM" data-type="sect1"><div class="sect1" id="ch18_using_rag_content_with_an_llm_1748550073473698">&#13;
        <h1>Using RAG Content with an LLM</h1>&#13;
        <p>Now that you’ve created<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="using RAG content with an LLM" data-type="indexterm" id="ch18use"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="RAG content used with" data-type="indexterm" id="ch18use2"/><a contenteditable="false" data-primary="vector store database Chroma" data-secondary="RAG content used with LLMs" data-type="indexterm" id="ch18use3"/><a contenteditable="false" data-primary="generative AI" data-secondary="retrieval-augmented generation" data-tertiary="using RAG content with an LLM" data-type="indexterm" id="ch18use4"/> a vector store and stored the book in it, let’s explore how you would read snippets back from the store, add them to a prompt, and get data back. We’ll use a local Ollama server to keep things simple. For more on Ollama, see <span class="keep-together"><a data-type="xref" href="ch17.html#ch17_serving_llms_with_ollama_1748550058915113">Chapter 17</a>.</span></p>&#13;
        <p>First, let’s load the vector store that we created in the previous step:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">load_vectorstore</code><code class="p">(</code><code class="n">persist_directory</code><code class="o">=</code><code class="s2">"./chroma_db"</code><code class="p">):</code>&#13;
    <code class="n">embeddings</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
 &#13;
    <code class="c1"># Load existing vector store</code>&#13;
    <code class="n">vectorstore</code> <code class="o">=</code> <code class="n">Chroma</code><code class="p">(</code>&#13;
        <code class="n">persist_directory</code><code class="o">=</code><code class="n">persist_directory</code><code class="p">,</code>&#13;
        <code class="n">embedding_function</code><code class="o">=</code><code class="n">embeddings</code>&#13;
    <code class="p">)</code>&#13;
 &#13;
    <code class="k">return</code> <code class="n">vectorstore</code></pre>&#13;
        <p>You <em>must</em> use the same embeddings as those you used when you created the vector store. Otherwise, there will be a mismatch when you try to encode your prompt and search for stuff similar to it. </p>&#13;
        <p>In this case, I’m using the OpenAIEmbeddings, but it’s entirely up to you how to approach this. There are many embeddings available in open source on Hugging Face, or you could use things like the GLoVE embeddings we explored in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>.</p>&#13;
        <p>ChromaDB persisted the embeddings in an SQLite database at a specific directory. Make sure you embed that, and then all you have to do is pass this and your embedding function to Chroma to get a reference to your database.</p>&#13;
        <p class="pagebreak-before less_space">To search the vector store, you’ll use the same code as earlier:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">search_vectorstore</code><code class="p">(</code><code class="n">vectorstore</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">3</code><code class="p">):</code>&#13;
    <code class="n">results</code> <code class="o">=</code> <code class="n">vectorstore</code><code class="o">.</code><code class="n">similarity_search</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">results</code></pre>&#13;
        <p>Next, input a query. For example, input this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">query</code> <code class="o">=</code> <code class="s2">"Please tell me all about Soo-Kyung Kim."</code></pre>&#13;
        <p>At this point, you have all the pieces you need to do a RAG query, which you can do like this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Example query</code>&#13;
<code class="n">query</code> <code class="o">=</code> <code class="s2">"Please tell me all about Soo-Kyung Kim."</code>&#13;
 &#13;
<code class="c1"># Perform RAG query</code>&#13;
<code class="n">answer</code><code class="p">,</code> <code class="n">sources</code> <code class="o">=</code> <code class="n">rag_query</code><code class="p">(</code><code class="n">vectorstore</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">num_contexts</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>&#13;
        <p>Here, you create a helper function that will pass the query and the vector store, and you also have a parameter with the number of items to find in the vector store. The app will return the answer (from the LLM) as well as a list of sources from the data that it used to augment the query.</p>&#13;
        <p>Let’s explore this function in depth:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">rag_query</code><code class="p">(</code><code class="n">vectorstore</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">num_contexts</code><code class="o">=</code><code class="mi">3</code><code class="p">):</code>&#13;
    <code class="c1"># Retrieve relevant documents</code>&#13;
    <code class="n">relevant_docs</code> <code class="o">=</code> <code class="n">search_vectorstore</code><code class="p">(</code><code class="n">vectorstore</code><code class="p">,</code> <code class="n">query</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="n">num_contexts</code><code class="p">)</code>&#13;
 &#13;
    <code class="c1"># Combine context from retrieved documents</code>&#13;
    <code class="n">context</code> <code class="o">=</code> <code class="s2">"</code><code class="se">\n\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">doc</code><code class="o">.</code><code class="n">page_content</code> <code class="k">for</code> <code class="n">doc</code> <code class="ow">in</code> <code class="n">relevant_docs</code><code class="p">])</code>&#13;
 &#13;
    <code class="c1"># Generate response using Ollama</code>&#13;
    <code class="n">response</code> <code class="o">=</code> <code class="n">query_ollama</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">context</code><code class="p">)</code>&#13;
 &#13;
    <code class="k">return</code> <code class="n">response</code><code class="p">,</code> <code class="n">relevant_docs</code></pre>&#13;
        <p>You’ll start by searching the vector store with the code provided earlier. This will give you the decoded chunks from the datastore as strings, and you should call these <span class="keep-together"><code>relevant_docs</code>.</span> </p>&#13;
        <p>You’ll then create the context string by joining the chunks together with some new line characters to separate them. It’s as simple as that. </p>&#13;
        <p>Now, the query and the context will be used in a call to Ollama. Let’s see how that will work.</p>&#13;
        <p>Start by defining the function:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">query_ollama</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">context</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="s2">"llama3.1:latest"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mf">0.7</code><code class="p">):</code>&#13;
 &#13;
    <code class="n">ollama_url</code> <code class="o">=</code> <code class="s2">"http://localhost:11434/api/chat"</code></pre>&#13;
        <p>Here, you can set the function to accept the prompt and context. I’ve added a couple of optional parameters that, if they’re not set, will use the defaults. The first is the model. To get a list of available models on your server, you can just use “ollama list” from the command prompt. The <code>temperature</code> parameter indicates how deterministic your response will be: the smaller the number, the more deterministic the answer, and the higher the number, the more creative the answer. I set a default of 0.7, which gives some flexibility to the model to make it natural sounding while staying relevant. But when you use smaller models in Ollama (like <code>llama3.1</code>, as shown), it does make hallucination more likely.</p>&#13;
        <p>You’ll also want to specify the <code>ollama_url</code> endpoint, as shown in <a data-type="xref" href="ch17.html#ch17_serving_llms_with_ollama_1748550058915113">Chapter 17</a>.</p>&#13;
        <p class="less_space">Next, you create the messages that will be used to interact with the model. </p>&#13;
        <p>The structure of conversations with a model typically looks like the one in <a data-type="xref" href="#ch18_figure_7_1748550073457720">Figure 18-7</a>. The model will optionally be primed with a system message that gives it instructions on how to behave. It will then have an initial message that it emits to the user, like, “Welcome to the Chat. How can I help?” The user will then respond with a prompt asking the model to do something, to which the model will respond, and so on. </p>&#13;
        <figure><div class="figure" id="ch18_figure_7_1748550073457720">&#13;
          <img alt="" src="assets/aiml_1807.png"/>&#13;
          <h6><span class="label">Figure 18-7. </span>Anatomy of a conversation with a model</h6>&#13;
        </div></figure>&#13;
        <p>The <em>memory</em> of the conversation<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="using RAG content with an LLM" data-tertiary="memory of the conversation" data-type="indexterm" id="id1899"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="RAG content used with" data-tertiary="memory of the conversation" data-type="indexterm" id="id1900"/><a contenteditable="false" data-primary="vector store database Chroma" data-secondary="RAG content used with LLMs" data-tertiary="memory of the conversation" data-type="indexterm" id="id1901"/><a contenteditable="false" data-primary="JSON" data-secondary="memory of the RAG conversation" data-type="indexterm" id="id1902"/> will be a JSON document with each of the roles prefixed by a <code>role</code> value. The initial message will have the <code>system</code> role, the model messages will have the <code>model</code> role, and the user messages will have the <code>user</code> role.</p>&#13;
        <p>So, for the simple RAG app we’re creating, we can create an instance of a conversation like this—passing the system message and the user message, which will be composed of the prompt and the context, like this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting">    <code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>&#13;
        <code class="p">{</code>&#13;
            <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>&#13;
            <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"You are a helpful AI assistant. </code><code class="w"/>&#13;
                        <code class="n">Use</code> <code class="n">the</code> <code class="n">provided</code> <code class="n">context</code> <code class="n">to</code> <code class="n">answer</code> <code class="n">questions</code><code class="o">.</code> &#13;
                        <code class="n">If</code> <code class="n">you</code> <code class="n">cannot</code> <code class="n">find</code> <code class="n">the</code> <code class="n">answer</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">context</code><code class="p">,</code> <code class="n">say</code> <code class="n">so</code><code class="o">.</code> &#13;
                        <code class="n">Only</code> <code class="n">use</code> <code class="n">information</code> <code class="kn">from</code> <code class="nn">the</code> <code class="n">provided</code> <code class="n">context</code><code class="o">.</code><code class="s2">"</code><code class="w"/>&#13;
        <code class="p">},</code>&#13;
        <code class="p">{</code>&#13;
            <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>&#13;
            <code class="s2">"content"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"Context:</code><code class="se">\n</code><code class="si">{</code><code class="n">context</code><code class="si">}</code><code class="se">\n\n</code><code class="s2">Question: </code><code class="si">{</code><code class="n">prompt</code><code class="si">}</code><code class="s2">"</code>&#13;
        <code class="p">}</code>&#13;
    <code class="p">]</code></pre>&#13;
        <p>Depending on how you set up the system role, you’ll get very different behavior. In this case, I used a prompt that gets it to heavily focus on the provided context. You don’t <em>need</em> to do this, and by working with this prompt, you might get much better results.</p>&#13;
        <p>Within the user role, this is just as simple as creating a string with <code>Context:</code> and <span class="keep-together"><code>Question:</code></span> content that you paste the context and prompt into.</p>&#13;
        <p>From this, you can now create<a contenteditable="false" data-primary="Ollama" data-secondary="RAG content used in LLMs" data-type="indexterm" id="id1903"/> a JSON payload to pass to Ollama that contains the desired model, the messages, the temperature, and the stream (which must be set to <code>False</code> if you want to get a single answer back):</p>&#13;
        <pre data-code-language="python" data-type="programlisting">    <code class="n">payload</code> <code class="o">=</code> <code class="p">{</code>&#13;
        <code class="s2">"model"</code><code class="p">:</code> <code class="n">model</code><code class="p">,</code>&#13;
        <code class="s2">"messages"</code><code class="p">:</code> <code class="n">messages</code><code class="p">,</code>&#13;
        <code class="s2">"stream"</code><code class="p">:</code> <code class="kc">False</code><code class="p">,</code>&#13;
        <code class="s2">"temperature"</code><code class="p">:</code> <code class="n">temperature</code>&#13;
    <code class="p">}</code></pre>&#13;
        <p>Note also that the desired model<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="RAG content used with" data-tertiary="model installed in Ollama" data-type="indexterm" id="id1904"/><a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="using RAG content with an LLM" data-tertiary="model installed in Ollama" data-type="indexterm" id="id1905"/><a contenteditable="false" data-primary="vector store database Chroma" data-secondary="RAG content used with LLMs" data-tertiary="model installed in Ollama" data-type="indexterm" id="id1906"/> must be installed in Ollama or you’ll get an error, so see <a data-type="xref" href="ch17.html#ch17_serving_llms_with_ollama_1748550058915113">Chapter 17</a> for adding models to Ollama.</p>&#13;
        <p>Then, you simply have to use an HTTP post to the Ollama URL, passing it the payload. When you get the response, you can query the returned message—where there’ll now be new content added by the model. This content will contain your answer!</p>&#13;
        <pre data-code-language="python" data-type="programlisting">    <code class="k">try</code><code class="p">:</code>&#13;
        <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">ollama_url</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="n">payload</code><code class="p">)</code>&#13;
        <code class="n">response</code><code class="o">.</code><code class="n">raise_for_status</code><code class="p">()</code>&#13;
        <code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()[</code><code class="s2">"message"</code><code class="p">][</code><code class="s2">"content"</code><code class="p">]</code>&#13;
    <code class="k">except</code> <code class="n">requests</code><code class="o">.</code><code class="n">exceptions</code><code class="o">.</code><code class="n">RequestException</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="sa">f</code><code class="s2">"Error querying Ollama: </code><code class="si">{</code><code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code></pre>&#13;
        <p>In this case, I used Llama 3.1 and got some excellent answers. Here’s an example:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">Based</code> <code class="n">on</code> <code class="n">the</code> <code class="n">provided</code> <code class="n">context</code><code class="p">,</code> <code class="n">here</code><code class="s1">'s what can be gathered about Soo-Kyung Kim:</code><code class="w"/>&#13;
 &#13;
<code class="mf">1.</code> <code class="n">She</code> <code class="ow">is</code> <code class="kn">from</code> <code class="nn">North</code> <code class="n">Korea</code><code class="o">.</code>&#13;
<code class="mf">2.</code> <code class="n">She</code> <code class="n">has</code> <code class="n">been</code> <code class="n">trained</code> <code class="ow">in</code> <code class="n">various</code> <code class="n">skills</code><code class="p">,</code> <code class="n">including</code> <code class="n">science</code><code class="p">,</code> <code class="n">technology</code><code class="p">,</code> &#13;
<code class="n">martial</code> <code class="n">arts</code><code class="p">,</code> <code class="n">languages</code><code class="p">,</code> <code class="n">piloting</code><code class="p">,</code> <code class="ow">and</code> <code class="n">strategy</code><code class="o">.</code>&#13;
<code class="mf">3.</code> <code class="n">Her</code> <code class="n">family</code> <code class="n">name</code> <code class="s2">"Kim"</code> <code class="ow">is</code> <code class="n">significant</code><code class="p">,</code> <code class="k">as</code> <code class="n">it</code> <code class="ow">is</code> <code class="n">the</code> <code class="n">name</code> <code class="n">of</code> <code class="n">the</code> <code class="n">ruling</code> <code class="n">family</code> &#13;
   <code class="n">of</code> <code class="n">the</code> <code class="n">Democratic</code> <code class="n">People</code><code class="s1">'s Republic of Korea (North Korea).</code><code class="w"/>&#13;
<code class="mf">4.</code> <code class="n">Soo</code><code class="o">-</code><code class="n">Kyung</code><code class="s1">'s presence on the space academy may be related to her exceptional </code><code class="w"/>&#13;
   <code class="n">abilities</code><code class="p">,</code> <code class="n">but</code> <code class="n">there</code> <code class="ow">is</code> <code class="n">also</code> <code class="n">a</code> <code class="n">suggestion</code> <code class="n">that</code> <code class="n">she</code> <code class="n">was</code> <code class="n">chosen</code> <code class="k">for</code> <code class="n">other</code> &#13;
   <code class="n">reasons</code><code class="o">.</code>&#13;
<code class="err">…</code></pre>&#13;
        <p>Your results will vary, based on the temperature, the slicing size for the chunks, and various other factors. </p>&#13;
        <p>One thing to note is that you can also use a <em>really</em> small model like Gemma2b and still get really good results. However, the context window of a model this small could have issues when you’re retrieving and augmenting your query with lots of information. As you saw earlier in this chapter, we were using one-thousand-character chunks, and we’re retrieving the 10 closest ones to the prompt. This is already in order of 10 k characters, and depending on the tokenization strategy, that could be more than 10 k tokens. Given that the context window for that model is only 2 k tokens, you could hit a problem. Watch out for that! </p>&#13;
        <section data-pdf-bookmark="Extending to Hosted Models" data-type="sect2"><div class="sect2" id="ch18_extending_to_hosted_models_1748550073473768">&#13;
          <h2>Extending to Hosted Models</h2>&#13;
          <p>In the example we just walked through,<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="using RAG content with an LLM" data-tertiary="hosted models" data-type="indexterm" id="id1907"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="RAG content used with" data-tertiary="hosted models" data-type="indexterm" id="id1908"/><a contenteditable="false" data-primary="vector store database Chroma" data-secondary="RAG content used with LLMs" data-tertiary="hosted models" data-type="indexterm" id="id1909"/> we used smaller models like Llama and Gemma to perform RAG on a local Ollama server. If you want to use larger, hosted models like GPT, the process is exactly the same. One change I would make, though, is with the system prompt. Given that these models have huge amounts of parameters that have learned a lot, it’s good to unshackle them a bit and not expect them to be limited solely to the context provided!</p>&#13;
          <p>For example, for GPT, you can import classes that support OpenAI’s GPT models like this:<a contenteditable="false" data-primary="GPT model" data-secondary="ChatOpenAI class" data-type="indexterm" id="id1910"/><a contenteditable="false" data-primary="ChatOpenAI class" data-type="indexterm" id="id1911"/><a contenteditable="false" data-primary="LangChain RAG API" data-secondary="ChatOpenAI class" data-type="indexterm" id="id1912"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code></pre>&#13;
          <p>You can then instantiate this class like this:<a contenteditable="false" data-primary="GPT model" data-secondary="ChatOpenAI class" data-tertiary="RAG content with hosted LLM" data-type="indexterm" id="id1913"/></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">chat</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code>&#13;
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>&#13;
    <code class="n">temperature</code><code class="o">=</code><code class="n">temperature</code>&#13;
<code class="p">)</code></pre>&#13;
          <p>The model value is a string containing the name of the model you want to use. For example, you could use <code>gpt-3.5-turbo</code> or <code>gpt-4</code>. Check the <a href="https://oreil.ly/SVBXr">OpenAI API documentation for model versions</a> available at the time you’re reading this.</p> &#13;
          <p>Then, you can create the prompt very simply. First, create a prompt template to hold the system and user prompts:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Create prompt template</code>&#13;
<code class="n">prompt_template</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">([</code>&#13;
    <code class="p">(</code><code class="s2">"system"</code><code class="p">,</code> <code class="s2">"You are a helpful AI assistant. </code><code class="w"/>&#13;
                <code class="n">Use</code> <code class="n">the</code> <code class="n">following</code> <code class="n">context</code> <code class="n">to</code> <code class="n">answer</code> <code class="n">questions</code><code class="o">.</code> <code class="s2">"</code><code class="w"/>&#13;
               <code class="s2">"Please provide as much detail as possible in a comprehensive </code><code class="w"/>&#13;
               <code class="n">answer</code><code class="o">.</code><code class="s2">"),</code><code class="w"/>&#13;
    <code class="p">(</code><code class="s2">"system"</code><code class="p">,</code> <code class="s2">"Context:</code><code class="se">\n</code><code class="si">{context}</code><code class="s2">"</code><code class="p">),</code>&#13;
    <code class="p">(</code><code class="s2">"user"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{question}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="p">])</code></pre>&#13;
          <p>Then, you can make the formatted prompt with the details of the context and prompt:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Format the prompt with the context and question</code>&#13;
<code class="n">formatted_prompt</code> <code class="o">=</code> <code class="n">prompt_template</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>&#13;
    <code class="n">context</code><code class="o">=</code><code class="n">context</code><code class="p">,</code>&#13;
    <code class="n">question</code><code class="o">=</code><code class="n">prompt</code>&#13;
<code class="p">)</code>&#13;
 </pre>&#13;
          <p>Finally, you can invoke the GPT chat with the formatted prompt and get the response:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Get the response</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">chat</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted_prompt</code><code class="p">)</code>&#13;
<code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">content</code></pre>&#13;
          <p>Now, as long as you ensure that you have an <code>OPENAI_API_KEY</code> environment variable, as discussed earlier, you’re RAGging against GPT! Please pay attention to the pricing on OpenAI for using the available models.<a contenteditable="false" data-primary="" data-startref="ch18use" data-type="indexterm" id="id1914"/><a contenteditable="false" data-primary="" data-startref="ch18use2" data-type="indexterm" id="id1915"/><a contenteditable="false" data-primary="" data-startref="ch18use3" data-type="indexterm" id="id1916"/><a contenteditable="false" data-primary="" data-startref="ch18use4" data-type="indexterm" id="id1917"/> </p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch18_summary_1748550073473830">&#13;
        <h1>Summary</h1>&#13;
        <p>In this chapter, you dipped your toes into the RAG waters, where you learned a powerful technique that enhances the capabilities of LLMs by combining their general understanding skills with local, private data. You saw how RAG works by creating a vector database with the contents of a book, and then you searched that database for information that was relevant to your given prompts. </p>&#13;
        <p>We also explored querying a character from the book to learn more about her—and despite models like Llama and GPT not being trained on content about her, they were able to artificially understand the text and provide great information and analysis. </p>&#13;
        <p>You also explored tools like ChromaDB (for vector storage) and pretrained embeddings (such as OpenAIs for vector encoding of text allowing similarity searches). You also explored various models that could be enhanced by using RAG, both small and local ones (like Llama and Gemma with Ollama) and large hosted models (like GPT via the OpenAI API). This took you through the process end to end: slicing text, encoding it, storing it, searching it based on similarity, and bundling it with a prompt to a model to perform RAG.</p>&#13;
        <p>In the next chapter, we’ll shift gears a bit to another exciting aspect of AI: generative image models. We’ll explore a number of different models that provide images from text prompts, and we’ll dig down a little into how they work.</p>&#13;
      </div></section>&#13;
    </div></section></body></html>