<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span></span> <span class="chapter-title-text">Empowering agents with actions</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">How an agent acts outside of itself using actions</li> 
    <li class="readable-text" id="p3">Defining and using OpenAI functions</li> 
    <li class="readable-text" id="p4">The Semantic Kernel and how to use semantic functions</li> 
    <li class="readable-text" id="p5">Synergizing semantic and native functions</li> 
    <li class="readable-text" id="p6">Instantiating a GPT interface with Semantic Kernel </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In this chapter, we explore actions through the use of functions and how agents can use them as well. We’ll start by looking at OpenAI function calling and then quickly move on to another project from Microsoft called Semantic Kernel (SK), which we’ll use to build and manage skills and functions for agents or as agents.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>We’ll finish the chapter using SK to host our first agent system. This will be a complete chapter with plenty of annotated code examples. </p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_55"><span class="num-string">5.1</span> Defining agent actions</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>ChatGPT plugins were first introduced to provide a session with abilities, skills, or tools. With a plugin, you can search the web or create spreadsheets or graphs. Plugins provide ChatGPT with the means to extend the platform.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>Figure 5.1 shows how a ChatGPT plugin works. In this example, a new movie recommender plugin has been installed in ChatGPT. When a user asks ChatGPT to recommend a new movie, the large language model (LLM) recognizes that it has a plugin to manage that action. It then breaks down the user request into actionable parameters, which it passes to the new movie recommender.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p12">  
   <img alt="figure" src="../Images/5-1.png" width="1009" height="804"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.1</span> How a ChatGPT plugin operates and how plugins and other external tools (e.g., APIs) align with the Use External Tools prompt engineering strategy</h5>
  </div> 
  <div class="readable-text" id="p13"> 
   <p>The recommender then scrapes a website showcasing new movies and appends that information to a new prompt request to an LLM. With this information, the LLM responds to the recommender, which passes this back to ChatGPT. ChatGPT then responds to the user with the recommended request.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>We can think of plugins as proxies for actions. A plugin generally encapsulates one or more abilities, such as calling an API or scraping a website. Actions, therefore, are extensions of plugins—they give a plugin its abilities.</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>AI agents can be considered plugins and consumers of plugins, tools, skills, and other agents. Adding skills, functions, and tools to an agent/plugin allows it to execute well-defined actions—figure 5.2 highlights where agent actions occur and their interaction with LLMs and other systems.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p16">  
   <img alt="figure" src="../Images/5-2.png" width="1012" height="646"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.2</span> How an agent uses actions to perform external tasks</h5>
  </div> 
  <div class="readable-text" id="p17"> 
   <p>An agent action is an ability that allows it to use a function, skill, or tool. What gets confusing is that different frameworks use different terminology. We’ll define an action as anything an agent can do to establish some basic definitions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>ChatGPT plugins and functions represent an actionable ability that ChatGPT or an agent system can use to perform additional actions. Now let’s examine the basis for OpenAI plugins and the function definition.</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_56"><span class="num-string">5.2</span> Executing OpenAI functions</h2> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>OpenAI, with the enablement of plugins, introduced a structure specification for defining the interface between functions/plugins an LLM could action. This specification is becoming a standard that LLM systems can follow to provide actionable systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>These same function definitions are now also being used to define plugins for ChatGPT and other systems. Next, we’ll explore how to use functions directly with an LLM call.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_57"><span class="num-string">5.2.1</span> Adding functions to LLM API calls</h3> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Figure 5.3 demonstrates how an LLM recognizes and uses the function definition to cast its response as the function call.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p24">  
   <img alt="figure" src="../Images/5-3.png" width="1012" height="522"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.3</span> How a single LLM request, including tools, gets interpreted by an LLM</h5>
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Listing 5.1 shows the details of an LLM API call using tools and a function definition. Adding a function definition allows the LLM to reply regarding the function’s input parameters. This means the LLM will identify the correct function and parse the relevant parameters for the user’s request.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p26"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.1</span> <code>first_function.py</code> (API call)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[{"role": "system",
                   "content": "You are a helpful assistant."},
                  {"role": "user", "content": user_message}],
        temperature=0.7,
        tools=[    <span class="aframe-location"/> #1
            {
                "type": "function",    <span class="aframe-location"/> #2
                "function": {
                    "name": "recommend",
                    "description": "Provide a … topic.",    <span class="aframe-location"/> #3
                    "parameters": {
                        "type": "object",    <span class="aframe-location"/> #4
                        "properties": {
                            "topic": {
                                "type": "string",
                                "description": 
                                   "The topic,… for.",    <span class="aframe-location"/> #5
                            },
                            "rating": {
                                "type": "string",
                                "description": 
                          "The rating … given.",    #5
                                "enum": ["good",
                                         "bad", 
                                         "terrible"]    <span class="aframe-location"/> #6
                                },
                        },
                        "required": ["topic"],
                    },
                },
                }
            ]
        )</pre> 
    <div class="code-annotations-overlay-container">
     #1 New parameter called tools
     <br/>#2 Sets the type of tool to function
     <br/>#3 Provides an excellent description of what the function does
     <br/>#4 Defines the type of parameters for input; an object represents a JSON document.
     <br/>#5 Excellent descriptions for each input parameter
     <br/>#6 You can even describe in terms of enumerations.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>To see how this works, open Visual Studio Code (VS Code) to the book’s source code folder: <code>chapter_4/first_function.py</code>. It’s a good practice to open the relevant chapter folder in VS Code to create a new Python environment and install the <code>requirements.txt</code> file. If you need assistance with this, consult appendix B.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>Before starting, correctly set up an <code>.env</code> file in the <code>chapter_4</code> folder with your API credentials. Function calling is an extra capability provided by the LLM commercial service. At the time of writing, this feature wasn’t an option for open source LLM deployments.</p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>Next, we’ll look at the bottom of the code in <code>first_function.py,</code> as shown in listing 5.2. Here are just two examples of calls made to an LLM using the request previously specified in listing 5.1. Here, each request shows the generated output from running the example.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p30"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.2</span> <code>first_function.py</code> (exercising the API)</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">user = "Can you please recommend me a time travel movie?"
response = ask_chatgpt(user)    <span class="aframe-location"/> #1
print(response)

<strong>###Output</strong>
Function(arguments='{"topic":"time travel movie"}', 
                      name='recommend')    <span class="aframe-location"/> #2

user = "Can you please recommend me a good time travel movie?"
response = ask_chatgpt(user)    <span class="aframe-location"/> #3
print(response)

<strong>###Output</strong>
Function(arguments='{"topic":"time travel movie",
                     "rating":"good"}',
 name='recommend')    <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Previously defined function
     <br/>#2 Returned in the name of the function to call and the extracted input parameters
     <br/>#3 Previously defined function
     <br/>#4 Returned in the name of the function to call and the extracted input parameters
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>Run the <code>first_function.py</code> Python script in VS Code using the debugger (F5) or the terminal to see the same results. Here, the LLM parses the input request to match any registered tools. In this case, the tool is the single function definition, that is, the recommended function. The LLM extracts the input parameters from this function and parses those from the request. Then, it replies with the named function and designated input parameters.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p32"> 
   <p><span class="print-book-callout-head">NOTE</span>  The actual function isn’t being called. The LLM only returns the suggested function and the relevant input parameters. The name and parameters must be extracted and passed into a function matching the signature to act on the function. We’ll look at an example of this in the next section.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_58"><span class="num-string">5.2.2</span> Actioning function calls</h3> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Now that we understand that an LLM doesn’t execute the function or plugin directly, we can look at an example that executes the tools. Keeping with the recommender theme, we’ll look at another example that adds a Python function for simple recommendations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>Figure 5.4 shows how this simple example will work. We’ll submit a single request that includes a tool function definition, asking for three recommendations. The LLM, in turn, will reply with the three function calls with input parameters (time travel, recipe, and gift). The results from executing the functions are then passed back to the LLM, which converts them back to natural language and returns a reply.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p36">  
   <img alt="figure" src="../Images/5-4.png" width="1012" height="526"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.4</span> A sample request returns three tool function calls and then submits the results back to the LLM to return a natural language response.</h5>
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Now that we understand the example, open <code>parallel_functions.py</code> in VS Code. Listing 5.3 shows the Python function that you want to call to give recommendations.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p38"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.3</span> <code>parallel_functions.py</code> (recommend function)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def recommend(topic, rating="good"):
    if "time travel" in topic.lower():    <span class="aframe-location"/> #1
        return json.dumps({"topic": "time travel",
                           "recommendation": "Back to the Future",
                           "rating": rating})
    elif "recipe" in topic.lower():    #1
        return json.dumps({"topic": "recipe",
                           "recommendation": "The best thing … ate.",
                           "rating": rating})
    elif "gift" in topic.lower():      #1
        return json.dumps({"topic": "gift",
                           "recommendation": "A glorious new...",
                           "rating": rating})
    else:    <span class="aframe-location"/> #2
        return json.dumps({"topic": topic,
                           "recommendation": "unknown"})    <span class="aframe-location"/> #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Checks to see if the string is contained within the topic input
     <br/>#2 If no topic is detected, returns the default
     <br/>#3 Returns a JSON object
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Next, we’ll look at the function called <code>run_conversation</code>, where all the work starts with the request construction.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.4</span> <code>parallel_functions.py</code> (<code>run_conversation</code>, <code>request</code>)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">user = """Can you please make recommendations for the following:
1. Time travel movies
2. Recipes
3. Gifts"""    <span class="aframe-location"/> #1
messages = [{"role": "user", "content": user}]    <span class="aframe-location"/> #2
tools = [    <span class="aframe-location"/> #3
    {
        "type": "function",
        "function": {
            "name": "recommend",
            "description": 
                "Provide a recommendation for any topic.",
            "parameters": {
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": 
                              "The topic, … recommendation for.",
                        },
                        "rating": {
                            "type": "string",
                            "description": "The rating … was given.",
                            "enum": ["good", "bad", "terrible"]
                            },
                        },
                "required": ["topic"],
                },
            },
        }
    ]</pre> 
    <div class="code-annotations-overlay-container">
     #1 The user message asks for three recommendations.
     <br/>#2 Note that there is no system message.
     <br/>#3 Adds the function definition to the tools part of the request
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Listing 5.5 shows the request being made, which we’ve covered before, but there are a few things to note. This call uses a lower model such as GPT-3.5 because delegating functions is a more straightforward task and can be done using older, cheaper, less sophisticated language models.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.5</span> <code>parallel_functions.py</code> (<code>run_conversation</code>, API call)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.chat.completions.create(
    model="gpt-3.5-turbo-1106",    <span class="aframe-location"/> #1
    messages=messages,    <span class="aframe-location"/> #2
    tools=tools,     #2
    tool_choice="auto",  <span class="aframe-location"/> #3
)
response_message = response.choices[0].message    <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 LLMs that delegate to functions can be simpler models.
     <br/>#2 Adds the messages and tools definitions
     <br/>#3 auto is the default.
     <br/>#4 The returned message from the LLM
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>At this point, after the API call, the response should hold the information for the required function calls. Remember, we asked the LLM to provide us with three recommendations, which means it should also provide us with three function call outputs, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.6</span> <code>parallel_functions.py</code> (<code>run_conversation</code>, <code>tool_calls</code>)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">tool_calls = response_message.tool_calls    <span class="aframe-location"/> #1
if tool_calls:    #1
    available_functions = {
        "recommend": recommend,
    }    <span class="aframe-location"/> #2
    # Step 4: send the info for each function call and function response to 
the model
    for tool_call in tool_calls:    <span class="aframe-location"/> #3
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)
        function_response = function_to_call(
            topic=function_args.get("topic"),    <span class="aframe-location"/> #4
            rating=function_args.get("rating"),
        )
        messages.append(    <span class="aframe-location"/> #5
            {
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": function_response,
            }
        )  # extend conversation with function response
    second_response = client.chat.completions.create(    <span class="aframe-location"/> #6
        model="gpt-3.5-turbo-1106",
        messages=messages,
    )
    return second_response.choices[0].message.content  #6</pre> 
    <div class="code-annotations-overlay-container">
     #1 If the response contains tool calls, execute them.
     <br/>#2 Only one function but could contain several
     <br/>#3 Loops through the calls and replays the content back to the LLM
     <br/>#4 Executes the recommend function from extracted parameters
     <br/>#5 Appends the results of each function call to the set of messages
     <br/>#6 Sends another request to the LLM with updated information and returns the message reply
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>The tool call outputs and the calls to the recommender function results are appended to the messages. Notice how messages now also contain the history of the first call. This is then passed back to the LLM to construct a reply in natural language.</p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p>Debug this example in VS Code by pressing the F5 key with the file open. The following listing shows the output of running <code>parallel_functions.py</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.7</span> <code>parallel_functions.py</code> (output)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">Here are some recommendations for you:

1. Time travel movies: "Back to the Future"
2. Recipes: "The best thing you ever ate."
3. Gifts: "A glorious new..." (the recommendation was cut off, so I 
couldn't provide the full recommendation)

I hope you find these recommendations helpful! Let me know if you need 
more information.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>This completes this simple demonstration. For more advanced applications, the functions could do any number of things, from scraping websites to calling search engines to completing far more complex tasks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>Functions are an excellent way to cast outputs for a particular task. However, the work of handling functions or tools and making secondary calls can be done in a cleaner and more efficient way. The following section will uncover a more robust system of adding actions to agents.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_59"><span class="num-string">5.3</span> Introducing Semantic Kernel</h2> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Semantic Kernel (SK) is another open source project from Microsoft intended to help build AI applications, which we call agents. At its core, the project is best used to define actions, or what the platform calls <em>semantic plugins</em>, which are wrappers for skills and functions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>Figure 5.5 shows how the SK can be used as a plugin and a consumer of OpenAI plugins. The SK relies on the OpenAI plugin definition to define a plugin. That way, it can consume and publish itself or other plugins to other systems.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p53">  
   <img alt="figure" src="../Images/5-5.png" width="797" height="607"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.5</span> How the Semantic Kernel integrates as a plugin and can also consume plugins</h5>
  </div> 
  <div class="readable-text" id="p54"> 
   <p>An OpenAI plugin definition maps precisely to the function definitions in listing 5.4. This means that SK is the orchestrator of API tool calls, aka plugins. That also means that SK can help organize multiple plugins with a chat interface or an agent.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p55"> 
   <p><span class="print-book-callout-head">Note </span> The team at SK originally labeled the functional modules as <em>skills.</em> However, to be more consistent with OpenAI, they have since renamed <em>skills</em> to <em>plugins.</em> What is more confusing is that the code still uses the term <em>skills.</em> Therefore, throughout this chapter, we’ll use <em>skills</em> and <em>plugins</em> to mean the same thing.</p> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>SK is a useful tool for managing multiple plugins (actions for agents) and, as we’ll see later, can also assist with memory and planning tools. For this chapter, we’ll focus on the actions/plugins. In the next section, we look at how to get started using SK.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_60"><span class="num-string">5.3.1</span> Getting started with SK semantic functions</h3> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>SK is easy to install and works within Python, Java, and C#. This is excellent news as it also allows plugins developed in one language to be consumed in a different language. However, you can’t yet develop a native function in one language and use it in another.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>We’ll continue from where we left off for the Python environment using the <code>chapter_4</code> workspace in VS Code. Be sure you have a workspace configured if you want to explore and run any examples.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>Listing 5.8 shows how to install SK from a terminal within VS Code. You can also install the SK extension for VS Code. The extension can be a helpful tool to create plugins/skills, but it isn’t required.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p61"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.8</span> Installing Semantic Kernel </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">pip uninstall semantic-kernel    <span class="aframe-location"/> #1

git clone https://github.com/microsoft/semantic-kernel.git    <span class="aframe-location"/> #2

cd semantic-kernel/python    <span class="aframe-location"/> #3

pip install -e .    <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Uninstalls any previous installations of SK
     <br/>#2 Clones the repository to a local folder
     <br/>#3 Changes to the source folder
     <br/>#4 Installs the editable package from the source folder
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Once you finish the installation, open <code>SK_connecting.py</code> in VS Code. Listing 5.9 shows a demo of running an example quickly through SK. The example creates a chat completion service using either OpenAI or Azure OpenAI.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p63"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.9</span> <code>SK_connecting.py</code></h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import semantic_kernel as sk

selected_service = "OpenAI"    <span class="aframe-location"/> #1
kernel = sk.Kernel()    <span class="aframe-location"/> #2

service_id = None
if selected_service == "OpenAI":
    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion

    api_key, org_id = sk.openai_settings_from_dot_env()    <span class="aframe-location"/> #3
    service_id = "oai_chat_gpt"
    kernel.add_service(
        OpenAIChatCompletion(
            service_id=service_id,
            ai_model_id="gpt-3.5-turbo-1106",
            api_key=api_key,
            org_id=org_id,
        ),
    )
elif selected_service == "AzureOpenAI":
    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion

    deployment, api_key, endpoint = 
<span class="">↪</span> sk.azure_openai_settings_from_dot_env()  <span class="aframe-location"/> #4
    service_id = "aoai_chat_completion"
    kernel.add_service(
        AzureChatCompletion(
            service_id=service_id,
            deployment_name=deployment,
            endpoint=endpoint,
            api_key=api_key,
        ),
    )

#This function is currently broken
async def run_prompt():
    result = await kernel.invoke_prompt( 
              <span class="">↪</span> prompt="recommend a movie about 
<span class="">↪</span> time travel")    <span class="aframe-location"/> #5
    print(result)

# Use asyncio.run to execute the async function
asyncio.run(run_prompt())    <span class="aframe-location"/> #6

###Output
One highly recommended time travel movie is "Back to the Future" (1985) 
directed by Robert Zemeckis. This classic film follows the adventures of 
teenager Marty McFly (Michael J. Fox)…</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets the service you’re using (OpenAI or Azure OpenAI)
     <br/>#2 Creates the kernel
     <br/>#3 Loads secrets from the .env file and sets them on the chat service
     <br/>#4 Loads secrets from the .env file and sets them on the chat service
     <br/>#5 Invokes the prompt
     <br/>#6 Calls the function asynchronously
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Run the example by pressing F5 (debugging), and you should see an output similar to listing 5.9. This example demonstrates how a semantic function can be created with SK and executed. A semantic function is the equivalent of a prompt template in prompt flow, another Microsoft tool. In this example, we define a simple prompt as a function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>It’s important to note that this semantic function isn’t defined as a plugin. However, the kernel can create the function as a self-contained semantic element that can be executed against an LLM. Semantic functions can be used alone or registered as plugins, as you’ll see later. Let’s jump to the next section, where we introduce contextual variables.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_61"><span class="num-string">5.3.2</span> Semantic functions and context variables</h3> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Expanding on the previous example, we can look at adding contextual variables to the semantic function. This pattern of adding placeholders to prompt templates is one we’ll review over and over. In this example, we look at a prompt template that has placeholders for subject, genre, format, and custom.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>Open <code>SK_context_variables.py</code> in VS Code, as shown in the next listing. The prompt is equivalent to setting aside a <code>system</code> and <code>user</code> section of the prompt.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p69"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.10</span> <code>SK_context_variables.py</code></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">#top section omitted…
prompt = """    <span class="aframe-location"/> #1
system:

You have vast knowledge of everything and can recommend anything provided 
you are given the following criteria, the subject, genre, format and any 
other custom information.

user:
Please recommend a {{$format}} with the subject {{$subject}} and {{$genre}}.
Include the following custom information: {{$custom}}
"""

prompt_template_config = sk.PromptTemplateConfig(    <span class="aframe-location"/> #2
    template=prompt,
    name="tldr",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(
            name="format", 
            description="The format to recommend", 
            is_required=True
        ),
        InputVariable(
            name="suject", 
            description="The subject to recommend", 
            is_required=True
        ),
        InputVariable(
            name="genre", 
            description="The genre to recommend", 
            is_required=True
        ),
        InputVariable(
            name="custom",
            description="Any custom information [CA]
                       to enhance the recommendation",
            is_required=True,
        ),
    ],
    execution_settings=execution_settings,
)

recommend_function = kernel.create_function_from_prompt(    <span class="aframe-location"/> #3
    prompt_template_config=prompt_template_config,
    function_name="Recommend_Movies",
    plugin_name="Recommendation",
)

async def run_recommendation(    <span class="aframe-location"/> #4
    subject="time travel",
    format="movie", 
    genre="medieval", 
           custom="must be a comedy"
):
    recommendation = await kernel.invoke(
        recommend_function,
        sk.KernelArguments(subject=subject,
                      format=format, 
                      genre=genre, 
                      custom=custom),    <span class="aframe-location"/> #5
    )
    print(recommendation)


# Use asyncio.run to execute the async function
asyncio.run(run_recommendation())    #5

###Output
One movie that fits the criteria of being about time travel, set in a 
medieval period, and being a comedy is "The Visitors" (Les Visiteurs) 
from 1993. This French film, directed by Jean-Marie Poiré, follows a 
knight and his squire who are transported to the modern era by a 
wizard’s spell gone wrong.…</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines a prompt with placeholders
     <br/>#2 Configures a prompt template and input variable definitions
     <br/>#3 Creates a kernel function from the prompt
     <br/>#4 Creates an asynchronous function to wrap the function call
     <br/>#5 Sets the kernel function arguments
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>Go ahead and debug this example (F5), and wait for the output to be generated. That is the basis for setting up SK and creating and exercising semantic functions. In the next section, we move on to see how a semantic function can be registered as a skill/plugin.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_62"><span class="num-string">5.4</span> Synergizing semantic and native functions</h2> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Semantic functions encapsulate a prompt/profile and execute through interaction with an LLM. Native functions are the encapsulation of code that may perform anything from scraping websites to searching the web. Both semantic and native functions can register as plugins/skills in the SK kernel.</p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>A function, semantic or native, can be registered as a plugin and used the same way we registered the earlier function directly with our API calls. When a function is registered as a plugin, it becomes accessible to chat or agent interfaces, depending on the use case. The next section looks at how a semantic function is created and registered with the kernel.</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_63"><span class="num-string">5.4.1</span> Creating and registering a semantic skill/plugin</h3> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>The VS Code extension for SK provides helpful tools for creating plugins/skills. In this section, we’ll use the SK extension to create a plugin/skill and then edit the components of that extension. After that, we’ll register and execute the plugin in the SK.</p> 
  </div> 
  <div class="readable-text intended-text" id="p76"> 
   <p>Figure 5.6 shows the process for creating a new skill within VS Code using the SK extension. (Refer to appendix B for directions if you need to install this extension.) You’ll then be given the option for the skill/plugin folder to place the function. Always group functions that are similar together. After creating a skill, enter the name and description of the function you want to develop. Be sure to describe the function as if the LLM were going to use it.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p77">  
   <img alt="figure" src="../Images/5-6.png" width="1012" height="749"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.6</span> The process of creating a new skill/plugin</h5>
  </div> 
  <div class="readable-text" id="p78"> 
   <p>You can see the completed skills and functions by opening the <code>skills/plugin</code> folder and reviewing the files. We’ll follow the previously constructed example, so open the <code>skills/Recommender/Recommend_Movies</code> folder, as shown in figure 5.7. Inside this folder is a <code>config.json</code> file, the function description, and the semantic function/prompt in a file called <code>skprompt.txt</code>.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p79">  
   <img alt="figure" src="../Images/5-7.png" width="597" height="299"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.7</span> The file and folder structure of a semantic function skill/plugin</h5>
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p>Listing 5.11 shows the contents of the semantic function definition, also known as the plugin definition. Note that the type is marked as <code>completion</code> and not of type <code>function</code> because this is a semantic function. We would define a native function as a type function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p81"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.11</span> <code>Recommend_Movies/config.json</code></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
    "schema": 1,
    "type": "completion",    <span class="aframe-location"/> #1
    "description": "A function to recommend movies based on users list of 
previously seen movies.",
    "completion": {    <span class="aframe-location"/> #2
        "max_tokens": 256,
        "temperature": 0,
        "top_p": 0,
        "presence_penalty": 0,
        "frequency_penalty": 0
    },
    "input": {
        "parameters": [
            {
                "name": "input",    <span class="aframe-location"/> #3
                "description": "The users list of previously seen movies.",
                "defaultValue": ""
            }
        ]
    },
    "default_backends": []
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 Semantic functions are functions of type completion.
     <br/>#2 We can also set the completion parameters for how the function is called.
     <br/>#3 Defines the parameters input into the semantic function
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>Next, we can look at the definition of the semantic function prompt, as shown in listing 5.12. The format is a little different, but what we see here matches the earlier examples using templating. This prompt recommends movies based on a list of movies the user has previously seen.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.12</span> <code>Recommend_Movies/skprompt.txt</code></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">You are a wise movie recommender and you have been asked to recommend a 
movie to a user.
You are provided a list of movies that the user has watched before.
You want to recommend a movie that the user has not watched before.
[INPUT]
{{$input}}
[END INPUT]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Now, we’ll dive into the code that loads the skill/plugin and executes it in a simple example. Open the <code>SK_first_skill.py</code> file in VS Code. The following listing shows an abridged version highlighting the new sections.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.13</span> SK_first_skill.py (abridged listing)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">kernel = sk.Kernel()

plugins_directory = "plugins"

recommender = kernel.import_plugin_from_prompt_directory(
    plugins_directory,
    "Recommender",
)    <span class="aframe-location"/> #1

recommend = recommender["Recommend_Movies"]

seen_movie_list = [    <span class="aframe-location"/> #2
    "Back to the Future",
    "The Terminator",
    "12 Monkeys",
    "Looper",
    "Groundhog Day",
    "Primer",
    "Donnie Darko",
    "Interstellar",
    "Time Bandits",
    "Doctor Strange",
]


async def run():
    result = await kernel.invoke(
        recommend,
        sk.KernelArguments(    <span class="aframe-location"/> #3
            settings=execution_settings, input=", ".join(seen_movie_list)
        ),
    )
    print(result)


asyncio.run(run())    <span class="aframe-location"/> #4

###Output
Based on the list of movies you've provided, it seems you have an 
interest in science fiction, time travel, and mind-bending narratives. 
Given that you've watched a mix of classics and modern films in this 
genre, I would recommend the following movie that you have not watched 
before:

"Edge of Tomorrow" (also known as "Live Die Repeat: Edge of Tomorrow")…</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads the prompt from the plugins folder
     <br/>#2 List of user’s previously seen movies
     <br/>#3 Input is set to joined list of seen movies.
     <br/>#4 Function is executed asynchronously.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>The code loads the skill/plugin from the <code>skills</code> directory and the <code>plugin</code> folder. When a skill is loaded into the kernel and not just created, it becomes a registered plugin. That means it can be executed directly as is done here or through an LLM chat conversation via the plugin interface.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>Run the code (F5), and you should see an output like listing 5.13. We now have a simple semantic function that can be hosted as a plugin. However, this function requires users to input a complete list of movies they have watched. We’ll look at a means to fix this by introducing native functions in the next section.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_64"><span class="num-string">5.4.2</span> Applying native functions</h3> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>As stated, native functions are code that can do anything. In the following example, we’ll introduce a native function to assist the semantic function we built earlier.</p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>This native function will load a list of movies the user has previously seen, from a file. While this function introduces the concept of memory, we’ll defer that discussion until chapter 8. Consider this new native function as any code that could virtually do anything.</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>Native functions can be created and registered using the SK extension. For this example, we’ll create a native function directly in code to make the example easier to follow.</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>Open <code>SK_native_functions.py</code> in VS Code. We’ll start by looking at how the native function is defined. A native function is typically defined within a class, which simplifies managing and instantiating native functions.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p93"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.14</span> <code>SK_native_functions.py</code> (<code>MySeenMovieDatabase</code>)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class MySeenMoviesDatabase:
    """
    Description: Manages the list of users seen movies.    <span class="aframe-location"/> #1
    """
    @kernel_function(    <span class="aframe-location"/> #2
        description="Loads a list of movies … user has already seen",
        name="LoadSeenMovies",
    )
    def load_seen_movies(self) -&gt; str:    <span class="aframe-location"/> #3
        try:
            with open("seen_movies.txt", 'r') as file:    <span class="aframe-location"/> #4
                lines = [line.strip() for line in file.readlines()]
                comma_separated_string = ', '.join(lines)
            return comma_separated_string
        except Exception as e:
            print(f"Error reading file: {e}")
            return None</pre> 
    <div class="code-annotations-overlay-container">
     #1 Provides a description for the container class
     <br/>#2 Uses a decorator to provide function description and name
     <br/>#3 The actual function returns a list of movies in a comma-separated string.
     <br/>#4 Loads seen movies from the text file
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>With the native function defined, we can see how it’s used by scrolling down in the file, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p95"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.15</span> <code>SK_native_functions</code> (remaining code)</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">plugins_directory = "plugins"

recommender = kernel.import_plugin_from_prompt_directory(
    plugins_directory,
    "Recommender",
)    <span class="aframe-location"/> #1

recommend = recommender["Recommend_Movies"]

seen_movies_plugin = kernel.import_plugin_from_object(
    MySeenMoviesDatabase(), "SeenMoviesPlugin"
)    <span class="aframe-location"/> #2

load_seen_movies = seen_movies_plugin["LoadSeenMovies"]    <span class="aframe-location"/> #3

async def show_seen_movies():
    seen_movie_list = await load_seen_movies(kernel)
    return seen_movie_list

seen_movie_list = asyncio.run(show_seen_movies())    <span class="aframe-location"/> #4
print(seen_movie_list)

async def run():     <span class="aframe-location"/> #5
    result = await kernel.invoke(
        recommend,
        sk.KernelArguments(
                settings=execution_settings,
                input=seen_movie_list),
    )
    print(result)


asyncio.run(run())    #5

###Output
The Matrix, The Matrix Reloaded, The Matrix Revolutions, The Matrix 
Resurrections – <code><em>output from print statement</em></code>
Based on your interest in the "The Matrix" series, it seems you enjoy 
science fiction films with a strong philosophical undertone and action 
elements. Given that you've watched all</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads the semantic function as shown previously
     <br/>#2 Imports the skill into the kernel and registers the function as a plugin
     <br/>#3 Loads the native function
     <br/>#4 Executes the function and returns the list as a string
     <br/>#5 Wraps the plugin call in an asynchronous function and executes
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>One important aspect to note is how the native function was imported into the kernel. The act of importing to the kernel registers that function as a plugin/skill. This means the function can be used as a skill from the kernel through other conversations or interactions. We’ll see how to embed a native function within a semantic function in the next section.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_65"><span class="num-string">5.4.3</span> Embedding native functions within semantic functions</h3> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>There are plenty of powerful features within SK, but one beneficial feature is the ability to embed native or semantic functions within other semantic functions. The following listing shows how a native function can be embedded within a semantic function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p99"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.16</span> <code>SK_semantic_native_functions.py</code> (<code>skprompt</code>)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">sk_prompt = """
You are a wise movie recommender and you have been asked to recommend a 
movie to a user.
You have a list of movies that the user has watched before.
You want to recommend a movie that 
the user has not watched before.    <span class="aframe-location"/> #1
Movie List: {{MySeenMoviesDatabase.LoadSeenMovies}}.    <span class="aframe-location"/> #2
"""</pre> 
    <div class="code-annotations-overlay-container">
     #1 The exact instruction text as previous
     <br/>#2 The native function is referenced and identified by class name and function name.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>The next example, <code>SK_semantic_native_functions.py</code>, uses inline native and semantic functions. Open the file in VS Code, and the following listing shows the code to create, register, and execute the functions.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.17</span> <code>SK_semantic_native_functions.py</code> (abridged)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">prompt_template_config = sk.PromptTemplateConfig(
    template=sk_prompt,
    name="tldr",
    template_format="semantic-kernel",
    execution_settings=execution_settings,
)    <span class="aframe-location"/> #1

recommend_function = kernel.create_function_from_prompt(
    prompt_template_config=prompt_template_config,
    function_name="Recommend_Movies",
    plugin_name="Recommendation",
)    <span class="aframe-location"/> #2


async def run_recommendation():    <span class="aframe-location"/> #3
    recommendation = await kernel.invoke(
        recommend_function,
        sk.KernelArguments(),
    )
    print(recommendation)


# Use asyncio.run to execute the async function
asyncio.run(run_recommendation())
###Output
Based on the list provided, it seems the user is a fan of the Matrix 
franchise. Since they have watched all four existing Matrix movies, I 
would recommend a…</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates the prompt template config for the prompt
     <br/>#2 Creates an inline semantic function from the prompt
     <br/>#3 Executes the semantic function asynchronously
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>Run the code, and you should see an output like listing 5.17. One important aspect to note is that the native function is registered with the kernel, but the semantic function is not. This is important because function creation doesn’t register a function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p103"> 
   <p>For this example to work correctly, the native function must be registered with the kernel, which uses the <code>import_plugin</code> function call—the first line in listing 5.17. However, the semantic function itself isn’t registered. An easy way to register the function is to make it a plugin and import it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p104"> 
   <p>These simple exercises showcase ways to integrate plugins and skills into chat or agent interfaces. In the next section, we’ll look at a complete example demonstrating adding a plugin representing a service or GPT interface to a chat function.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_66"><span class="num-string">5.5</span> Semantic Kernel as an interactive service agent</h2> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>In chapter 1, we introduced the concept of the GPT interface—a new paradigm in connecting services and other components to LLMs via plugins and semantic layers. SK provides an excellent abstraction for converting any service to a GPT interface.</p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>Figure 5.8 shows a GPT interface constructed around an API service called The Movie Database (TMDB; <a href="http://www.themoviedb.org">www.themoviedb.org</a>). The TMDB site provides a free API that exposes information about movies and TV shows.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p108">  
   <img alt="figure" src="../Images/5-8.png" width="897" height="494"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.8</span> This layer architecture diagram shows the role of a GPT interface and the Semantic Kernel being exposed to chat or agent interfaces.</h5>
  </div> 
  <div class="readable-text" id="p109"> 
   <p>To follow along with the exercises in this section, you must register for a free account from TMDB and create an API key. Instructions for getting an API key can be found at the TMDB website (<a href="http://www.themoviedb.org">www.themoviedb.org</a>) or by asking a GPT-4 turbo or a more recent LLM.</p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>Over the next set of subsections, we’ll create a GPT interface using an SK set of native functions. Then, we’ll use the SK kernel to test the interface and, later in this chapter, implement it as plugins into a chat function. In the next section, we look at building a GPT interface against the TMDB API.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_67"><span class="num-string">5.5.1</span> Building a semantic GPT interface</h3> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>TMDB is an excellent service, but it provides no semantic services or services that can be plugged into ChatGPT or an agent. To do that, we must wrap the API calls that TMDB exposes in a semantic service layer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>A semantic service layer is a GPT interface that exposes functions through natural language. As discussed, to expose functions to ChatGPT or other interfaces such as agents, they must be defined as plugins. Fortunately, SK can create the plugins for us automatically, given that we write our semantic service layer correctly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>A native plugin or set of skills can act as a semantic layer. To create a native plugin, create a new plugin folder, and put a Python file holding a class containing the set of native functions inside that folder. The SK extension currently doesn’t do this well, so manually creating the module works best.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>Figure 5.9 shows the structure of the new plugin called <code>Movies</code> and the semantic service layer called <code>tmdb.py</code>. For native functions, the parent folder’s name (<code>Movies</code>) is used in the import.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p116">  
   <img alt="figure" src="../Images/5-9.png" width="532" height="159"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.9</span> The folder and file structure of the TMDB plugin</h5>
  </div> 
  <div class="readable-text" id="p117"> 
   <p>Open the <code>tmdb.py</code> file in VS Code, and look at the top of the file, as shown in listing 5.18. This file contains a class called <code>TMDbService</code>, which exposes several functions that map to API endpoint calls. The idea is to map the various relevant API function calls in this semantic service layer. This will expose the functions as plugins for a chat or agent interface.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p118"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.18</span> <code>tmdb.py</code> (top of file)</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from semantic_kernel.functions import kernel_funct
import requests
import inspect


def print_function_call():    <span class="aframe-location"/> #1
    #omitted …


class TMDbService:    <span class="aframe-location"/> #2
    def __init__(self):
        # enter your TMDb API key here
        self.api_key = "your-TMDb-api-key"


    @kernel_function(     #2
        description="Gets the movie genre ID for a given genre name",
        name="get_movie_genre_id",
        input_description="The movie genre name of the genre_id to get",
        )
    def get_movie_genre_id(self, genre_name: str) -&gt; str:    <span class="aframe-location"/> #3
        print_function_call()
        base_url = "https://api.themoviedb.org/3"
        endpoint = f"{base_url}/genre/movie/list<span class="">↪</span>
                     <span class="">↪</span> ?api_key={self.api_key}&amp;language=en-US"

        response = requests.get(endpoint)    <span class="aframe-location"/> #4
        if response.status_code == 200:    #4
            genres = response.json()['genres']
            for genre in genres:
                if genre_name.lower() in genre['name'].lower():
                    return str(genre['id'])    <span class="aframe-location"/> #5
        return None</pre> 
    <div class="code-annotations-overlay-container">
     #1 Prints the calls to the functions for debugging
     <br/>#2 Top-level service and decorator used to describe the function (good descriptions are important)
     <br/>#3 Function wrapped in semantic wrapper; should return str
     <br/>#4 Calls the API endpoint, and, if good (code 200), checks for matching genre
     <br/>#5 Found the genre, returns the id
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>The bulk of the code for the <code>TMDbService</code> and the functions to call the TMDB endpoints was written with the help of GPT-4 Turbo. Then, each function was wrapped with the <code>sk_function</code> decorator to expose it semantically.</p> 
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>A few of the TMDB API calls have been mapped semantically. Listing 5.19 shows another example of a function exposed to the semantic service layer. This function pulls a current top 10 list of movies playing for a particular genre.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.19</span> <code>tmdb.py</code> (<code>get_top_movies_by_genre</code>)</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">@kernel_function(    <span class="aframe-location"/> #1
        description="””
Gets a list of currently playing movies for a given genre””",
        name="get_top_movies_by_genre",
        input_description="The genre of the movies to get",
        )
    def get_top_movies_by_genre(self, genre: str) -&gt; str:
        print_function_call()
        genre_id = self.get_movie_genre_id(genre)    <span class="aframe-location"/> #2
        if genre_id:
            base_url = "https://api.themoviedb.org/3
            playing_movies_endpoint = f"{base_url}/movie/now_playing?<span class="">↪</span>
<span class="">↪</span> api_key={self.api_key}&amp;language=en-US"
            response = requests.get(
                          playing_movies_endpoint)    <span class="aframe-location"/> #3
            if response.status_code != 200:
                return ""

            playing_movies = response.json()['results'
            for movie in playing_movies:    <span class="aframe-location"/> #4
                movie['genre_ids'] = [str(genre_id)  
                      <span class="">↪</span> for genre_id in movie['genre_ids']]
            filtered_movies = [movie for movie <span class="">↪</span>
<span class="">↪</span> in playing_movies if genre_id <span class="">↪</span>
<span class="">↪</span> in movie['genre_ids']][:10]    <span class="aframe-location"/> #5
            results = ", ".join([movie['title'] for movie in filtered_movies])
            return results
        else:
            return ""</pre> 
    <div class="code-annotations-overlay-container">
     #1 Decorates the function with descriptions
     <br/>#2 Finds the genre id for the given genre name
     <br/>#3 Gets a list of currently playing movies
     <br/>#4 Converts genre_ids to strings
     <br/>#5 Checks to see if the genre id matches movie genres
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Look through the various other API calls mapped semantically. As you can see, there is a well-defined pattern for converting API calls to a semantic service. Before we run the full service, we’ll test each of the functions in the next section.</p> 
  </div> 
  <div class="readable-text" id="p123"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_68"><span class="num-string">5.5.2</span> Testing semantic services</h3> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>In a real-world application, you’ll likely want to write a complete set of unit or integration tests for each semantic service function. We won’t do that here; instead, we’ll write a quick helper script to test the various functions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p125"> 
   <p>Open <code>test_tmdb_service.py</code> in VS Code, and review the code, as shown in listing 5.20. You can comment and uncomment any functions to test them in isolation. Be sure to have only one function uncommented at a time.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p126"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.20</span> <code>test_tmdb_service.py</code></h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import semantic_kernel as sk
from plugins.Movies.tmdb import TMDbService

async def main():
    kernel = sk.Kernel()    <span class="aframe-location"/> #1

    tmdb_service = kernel.import_plugin_from_object <span class="">↪</span>
<span class="">↪</span> (TMDbService(), "TMDBService")    <span class="aframe-location"/> #2

    print(
        await tmdb_service["get_movie_genre_id"](
            kernel, sk.KernelArguments(
                            genre_name="action")    <span class="aframe-location"/> #3
        )
    )    <span class="aframe-location"/> #4
    print(
        await tmdb_service["get_tv_show_genre_id"](
            kernel, sk.KernelArguments(
                            genre_name="action")    <span class="aframe-location"/> #5
        )
    )    <span class="aframe-location"/> #6
    print(
        await tmdb_service["get_top_movies_by_genre"](
            kernel, sk.KernelArguments(
                            genre_name="action")    <span class="aframe-location"/> #7
        )
    )    <span class="aframe-location"/> #8
    print(
        await tmdb_service["get_top_tv_shows_by_genre"](
            kernel, sk.KernelArguments(
                            genre_name="action")    #7
        )
    )
    print(await tmdb_service["get_movie_genres"](
kernel, sk.KernelArguments()))                       <span class="aframe-location"/> #9
    print(await tmdb_service["get_tv_show_genres"](
kernel, sk.KernelArguments()))                       #9


# Run the main function
if __name__ == "__main__":
    import asyncio

    asyncio.run(main())    <span class="aframe-location"/> #10

###Output
Function name: get_top_tv_shows_by_genre    <span class="aframe-location"/> #11
Arguments:
  self = &lt;skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0&gt;
  genre = action
Function name: get_tv_show_genre_id    #11
Arguments:
  self = &lt;skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0&gt;
  genre_name = action
Arcane, One Piece, Rick and Morty, Avatar: The Last Airbender, Fullmetal 
Alchemist: Brotherhood, Demon Slayer: Kimetsu no Yaiba, Invincible, 
Attack on Titan, My Hero Academia, Fighting Spirit, The Owl House</pre> 
    <div class="code-annotations-overlay-container">
     #1 Instantiates the kernel
     <br/>#2 Imports the plugin service
     <br/>#3 Inputs parameter to functions, when needed
     <br/>#4 Executes and tests the various functions
     <br/>#5 Inputs parameter to functions, when needed
     <br/>#6 Executes and tests the various functions
     <br/>#7 Inputs parameter to functions, when needed
     <br/>#8 Executes and tests the various functions
     <br/>#9 Executes and tests the various functions
     <br/>#10 Executes main asynchronously
     <br/>#11 Calls print function details to notify when the function is being called
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p127"> 
   <p>The real power of SK is shown in this test. Notice how the <code>TMDbService</code> class is imported as a plugin, but we don’t have to define any plugin configurations other than what we already did? By just writing one class that wrapped a few API functions, we’ve exposed part of the TMDB API semantically. Now, with the functions exposed, we can look at how they can be used as plugins for a chat interface in the next section.</p> 
  </div> 
  <div class="readable-text" id="p128"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_69"><span class="num-string">5.5.3</span> Interactive chat with the semantic service layer</h3> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>With the TMDB functions exposed semantically, we can move on to integrating them into a chat interface. This will allow us to converse naturally in this interface to get various information, such as current top movies.</p> 
  </div> 
  <div class="readable-text intended-text" id="p130"> 
   <p>Open <code>SK_service_chat.py</code> in VS Code. Scroll down to the start of the new section of code that creates the functions, as shown in listing 5.21. The functions created here are now exposed as plugins, except we filter out the chat function, which we don’t want to expose as a plugin. The chat function here allows the user to converse directly with the LLM and shouldn’t be a plugin.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p131"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.21</span> <code>SK_service_chat.py</code> (function setup)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">system_message = "You are a helpful AI assistant."

tmdb_service = kernel.import_plugin_from_object(
TMDbService(), "TMDBService")    <span class="aframe-location"/> #1

# extracted section of code
execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(
        service_id=service_id,
        ai_model_id=model_id,
        max_tokens=2000,
        temperature=0.7,
        top_p=0.8,
        tool_choice="auto",
        tools=get_tool_call_object(
            kernel, {"exclude_plugin": ["ChatBot"]}),    <span class="aframe-location"/> #2
    )

prompt_config = sk.PromptTemplateConfig.from_completion_parameters(
    max_tokens=2000,
    temperature=0.7,
    top_p=0.8,
    function_call="auto",
    chat_system_prompt=system_message,
)    <span class="aframe-location"/> #3
prompt_template = OpenAIChatPromptTemplate(
    "{{$user_input}}", kernel.prompt_template_engine, prompt_config
)    <span class="aframe-location"/> #4

history = ChatHistory()

history.add_system_message("You recommend movies and TV Shows.")
history.add_user_message("Hi there, who are you?")
history.add_assistant_message(
    "I am Rudy, the recommender chat bot. I'm trying to figure out what 
people need."
)    <span class="aframe-location"/> #5

chat_function = kernel.create_function_from_prompt(
    prompt_template_config=prompt_template,
    plugin_name="ChatBot",
    function_name="Chat",
)    <span class="aframe-location"/> #6</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports the TMDbService as a plugin
     <br/>#2 Configures the execution settings and adds filtered tools
     <br/>#3 Configures the prompt configuration
     <br/>#4 Defines the input template and takes full strings as user input
     <br/>#5 Adds the chat history object and populates some history
     <br/>#6 Creates the chat function
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>Next, we can continue by scrolling in the same file to review the chat function, as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p133"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.22</span> <code>SK_service_chat.py</code> (chat function)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">async def chat() -&gt; bool:
    try:
        user_input = input("User:&gt; ")    <span class="aframe-location"/> #1
    except KeyboardInterrupt:
        print("\n\nExiting chat...")
        return False
    except EOFError:
        print("\n\nExiting chat...")
        return False

    if user_input == "exit":    <span class="aframe-location"/> #2
        print("\n\nExiting chat...")
        return False
    arguments = sk.KernelArguments(    <span class="aframe-location"/> #3
        user_input=user_input,
        history=("\n").join(
           [f"{msg.role}: {msg.content}" for msg in history]),
    )
    result = await chat_completion_with_tool_call(    <span class="aframe-location"/> #4
        kernel=kernel,
        arguments=arguments,
        chat_plugin_name="ChatBot",
        chat_function_name="Chat",
        chat_history=history,
    )
    print(f"AI Agent:&gt; {result}")
    return True</pre> 
    <div class="code-annotations-overlay-container">
     #1 Input is taken directly from the terminal/console.
     <br/>#2 If the user types exit, then exit the chat.
     <br/>#3 Creates arguments to pass to the function
     <br/>#4 Uses the utility function to call the function and execute the tool
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>Lastly, scroll down to the bottom of the file, and review the primary function. This is the code that calls the chat function in a loop.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p135"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.23</span> <code>SK_service_chat.py</code> (main function)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">async def main() -&gt; None:
    chatting = True
    context = kernel.create_new_context()

    print("Welcome to your first AI Agent\    <span class="aframe-location"/> #1
\n  Type 'exit' to exit.\
\n  Ask to get a list of currently playing movies by genre."
    )
    while chatting:    <span class="aframe-location"/> #2
        chatting, context = await chat(context)    <span class="aframe-location"/> #3


if __name__ == "__main__":
    asyncio.run(main())</pre> 
    <div class="code-annotations-overlay-container">
     #1 Introduction to the user
     <br/>#2 Continues until chatting is False
     <br/>#3 Calls the chat function asynchronously
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>Run the chat interface, run the file (F5), and then ask about movies or television shows of a particular genre. An example conversation session is shown in listing 5.24. This output shows how a request to list movies from two genres made the chat interface make multiple calls to the <code>get_top_movie_by_genre</code> function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p137"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.24</span> <code>SK_service_chat.py</code> (example conversation) </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">Welcome to your first AI Agent
  Type 'exit' to exit.
  Ask to get a list of currently playing movies by genre.
User:&gt; Input: can you give me list of the current top playing movies for 
the action and comedy genres?

Function name: get_top_movies_by_genre    <span class="aframe-location"/> #1
Arguments:
  genre = action
Function name: get_movie_genre_id    <span class="aframe-location"/> #2
Arguments:
  genre_name = action
Function name: get_top_movies_by_genre    #1
Arguments:
  genre = comedy
Function name: get_movie_genre_id    #2
Arguments:
  genre_name = comedy
Agent:&gt; Here are the current top-playing movies 
for the action and comedy genres:

**Action:**    <span class="aframe-location"/> #3
1. The Hunger Games: The Ballad of Songbirds &amp; Snakes
2. Rebel Moon - Part One: A Child of Fire
3. Aquaman and the Lost Kingdom
4. Silent Night
5. The Family Plan
6. Freelance
7. Migration
8. Sound of Freedom
9. Godzilla Minus One

**Comedy:**    <span class="aframe-location"/> #4
1. The Family Plan
2. Wonka
3. Freelance
4. Saltburn
5. Chicken Run: Dawn of the Nugget
6. Trolls Band Together
7. There's Something in the Barn
8. Migration

Please note that some movies may overlap in both genres, such as 
"The Family Plan" and "Freelance ."</pre> 
    <div class="code-annotations-overlay-container">
     #1 LLM makes two calls to get_top_movies_by_genre.
     <br/>#2 Internal call to get the genre id
     <br/>#3 List of the top current action movies
     <br/>#4 List of the top current comedy movies
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>Be sure to explore the chat interface’s boundaries and what you can ask for from the TMDB service. For example, try asking for a list of genres for movies or television shows. This service is a good first try, but we can perhaps do better, as we’ll see in the next section.</p> 
  </div> 
  <div class="readable-text" id="p139"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_70"><span class="num-string">5.6</span> Thinking semantically when writing semantic services</h2> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>Now we’ve seen an excellent demonstration of converting an API into a semantic service interface. As it is, the functions return the titles of the top movies and television shows currently playing. However, by just returning the titles, we’re limiting the ability of the LLM to parse the results on its own.</p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>Therefore, we’ll create a v2 version of <code>TMDbService</code> to correct this and return the results as JSON strings. Open the file <code>tmdb_v2.py</code> in VS Code, and scroll down to the <code>get_top_movies_by_genre</code> function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p142"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.25</span> <code>tmdb_v2.py</code> (<code>get_top_movies_by_genre</code>)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def get_top_movies_by_genre(self, genre: str) -&gt; str:
        print_function_call()
        genre_id = self.get_movie_genre_id(genre)
        if genre_id:
            #same code …

            return json.dumps(filtered_movies)    <span class="aframe-location"/> #1
        else:
            return ""</pre> 
    <div class="code-annotations-overlay-container">
     #1 Now returns a filtered list as a JSON string
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>Now open <code>SK_service_chat.py</code> in VS Code, and comment and uncomment the line shown in listing 5.26. This will then use version 2 of the <code>TMDbService</code> that outputs results as full JSON documents in a single string.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p144"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.26</span> <code>SK_service_chat.py</code> (modifying imports)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">#from skills.Movies.tmdb import TMDbService    <span class="aframe-location"/> #1
from skills.Movies.tmdb_v2 import TMDbService    <span class="aframe-location"/> #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Comment out this line.
     <br/>#2 Uncomment this line to use version 2 of the service.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>Rerun the <code>SK_service_chat.py</code> file in VS Code, and alter your query slightly, as shown by the output in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p146"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.27</span> <code>SK_service_chat.py</code> (<code>TMDb_v2</code> service output)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">User:&gt; get a list of currently playing movies for the 
action genre and only return movies about space    <span class="aframe-location"/> #1
Agent:&gt; To find currently playing action movies that are specifically 
about space, I will need to manually filter the provided list for those 
that have space-related themes in their overview. Here's what fits that 
criteria from the list:

1. **Rebel Moon - Part One: A Child of Fire**    <span class="aframe-location"/> #2
   - Release Date: 2023-12-15
   - Overview: When a peaceful colony on the edge of the galaxy finds 
itself threatened by the armies of the tyrannical Regent Balisarius, 
they dispatch Kora, a young woman with a mysterious past, to seek out 
warriors from neighboring planets to help them take a stand.

This is the only movie from the provided list that clearly mentions a 
space-related theme in its overview. …</pre> 
    <div class="code-annotations-overlay-container">
     #1 New query asks to include an additional filter for space
     <br/>#2 The LLM calls the service and then reviews the returned results that match the filter.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>Because the semantic service functions now return the complete movie listing in JSON, the LLM can apply additional filtering. This is the real power of semantic services, allowing you to process the data through the LLM. We won’t see this power by just returning a list of titles.</p> 
  </div> 
  <div class="readable-text intended-text" id="p148"> 
   <p>This last exercise demonstrated the change in mentality you need to make when writing semantic service layers. Generally, you’ll typically want to return as much information as possible. Returning more information takes advantage of the LLM abilities to filter, sort, and transform data independently. In the next chapter, we’ll explore building autonomous agents using behavior trees.</p> 
  </div> 
  <div class="readable-text" id="p149"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_71"><span class="num-string">5.7</span> Exercises</h2> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>Complete the following exercises to improve your knowledge of the material:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p151"> <em>Exercise 1</em>—Creating a Basic Plugin for Temperature Conversion  </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p152"> 
   <p><em>Objective </em>—Familiarize yourself with creating a simple plugin for the OpenAI chat completions API. </p> 
  </div> 
  <div class="readable-text list-body-item" id="p153"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p154"> Develop a plugin that converts temperatures between Celsius and Fahrenheit. </li> 
     <li class="readable-text" id="p155"> Test the plugin by integrating it into a simple OpenAI chat session where users can ask for temperature conversions. </li> 
    </ul></li> 
   <li class="readable-text" id="p156"> <em>Exercise 2</em>—Developing a Weather Information Plugin  </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p157"> 
   <p><em>Objective </em>—Learn to create a plugin that performs a unique task. </p> 
  </div> 
  <div class="readable-text list-body-item" id="p158"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p159"> Create a plugin for the OpenAI chat completions API that fetches weather information from a public API. </li> 
     <li class="readable-text" id="p160"> Ensure the plugin can handle user requests for current weather conditions in different cities. </li> 
    </ul></li> 
   <li class="readable-text" id="p161"> <em>Exercise 3</em>—Crafting a Creative Semantic Function  </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p162"> 
   <p><em>Objective </em>—Explore the creation of semantic functions. </p> 
  </div> 
  <div class="readable-text list-body-item" id="p163"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p164"> Develop a semantic function that writes a poem or tells a children’s story based on user input. </li> 
     <li class="readable-text" id="p165"> Test the function in a chat session to ensure it generates creative and coherent outputs. </li> 
    </ul></li> 
   <li class="readable-text" id="p166"> <em>Exercise 4</em>—Enhancing Semantic Functions with Native Functions  </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p167"> 
   <p><em>Objective </em>—Understand how to combine semantic and native functions. </p> 
  </div> 
  <div class="readable-text list-body-item" id="p168"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p169"> Create a semantic function that uses a native function to enhance its capabilities. </li> 
     <li class="readable-text" id="p170"> For example, develop a semantic function that generates a meal plan and uses a native function to fetch nutritional information for the ingredients. </li> 
    </ul></li> 
   <li class="readable-text" id="p171"> <em>Exercise 5</em>—Wrapping an Existing Web API with Semantic Kernel </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p172"> 
   <p><em>Objective </em>—Learn to wrap existing web APIs as semantic service plugins. </p> 
  </div> 
  <div class="readable-text list-body-item" id="p173"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p174"> Use SK to wrap a news API and expose it as a semantic service plugin in a chat agent. </li> 
     <li class="readable-text" id="p175"> Ensure the plugin can handle user requests for the latest news articles on various topics. </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p176"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_72">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p177"> Agent actions extend the capabilities of an agent system, such as ChatGPT. This includes the ability to add plugins to ChatGPT and LLMs to function as proxies for actions. </li> 
   <li class="readable-text" id="p178"> OpenAI supports function definitions and plugins within an OpenAI API session. This includes adding function definitions to LLM API calls and understanding how these functions allow the LLM to perform additional actions. </li> 
   <li class="readable-text" id="p179"> The Semantic Kernel (SK) is an open source project from Microsoft that can be used to build AI applications and agent systems. This includes the role of semantic plugins in defining native and semantic functions. </li> 
   <li class="readable-text" id="p180"> Semantic functions encapsulate the prompt/profile template used to engage an LLM. </li> 
   <li class="readable-text" id="p181"> Native functions encapsulate code that performs or executes an action using an API or other interface. </li> 
   <li class="readable-text" id="p182"> Semantic functions can be combined with other semantic or native functions and layered within one another as execution stages. </li> 
   <li class="readable-text" id="p183"> SK can be used to create a GPT interface over the top of API calls in a semantic service layer and expose them as chat or agent interface plugins. </li> 
   <li class="readable-text" id="p184"> Semantic services represent the interaction between LLMs and plugins, as well as the practical implementation of these concepts in creating efficient AI agents. </li> 
  </ul>
 </div></div></body></html>