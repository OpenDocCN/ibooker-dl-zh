- en: appendix B Responsible AI tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As generative AI models have become increasingly prevalent in enterprises, ensuring
    that they are developed and deployed responsibly is essential. Responsible AI
    (RAI) practices can help organizations build stakeholder trust, meet regulatory
    requirements, and avoid unintended consequences. Fortunately, many tools are available
    to support developers and architects in integrating RAI principles into their
    AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections outline some of these tools and frameworks, which can
    help ensure transparency, fairness, interpretability, and security in AI.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Model card
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A model card is a special type of documentation accompanying an AI model. It
    provides a standardized information set about the model’s purpose, performance,
    training data, ethical considerations, and more. It’s akin to a product data sheet,
    offering transparency and facilitating responsible AI practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it might seem odd to think of model cards as an RAI tool, they serve
    an important role in the context of RAI. Model cards are considered an essential
    RAI tool. They help stakeholders understand the capabilities and limitations of
    GenAI models, such as those based on GPT architectures, ensuring that these powerful
    tools are used ethically and effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Promoting transparency*—They detail the model’s characteristics, limitations,
    and ideal use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Encouraging accountability*—By documenting the model’s development process,
    model cards help ensure creators remain accountable for their AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Facilitating informed use*—They provide users with the necessary information
    to understand how the model should be used, thus preventing misuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For large language models (LLMs), model cards typically include the following
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model details*—Information about the model’s architecture, size, training
    data, and training procedures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intended use*—A description of the tasks the model is designed for and any
    limitations on its intended use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance metrics*—Benchmarks and evaluation results showing how the model
    performs on various tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethical considerations*—Any ethical concerns related to the model’s use, including
    potential biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Caveats and recommendations*—Any warnings or suggestions for users of the
    model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, OpenAI’s GPT-4 model card is called a system card and is 60 pages
    long. It calls out multiple risks related to safety challenges, such as hallucinations,
    harmful content, potential for risky emergent behaviors, overreliance, and so
    forth. More details on model cards can be found at [https://mng.bz/M1gB](https://mng.bz/M1gB).
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Transparency notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A transparency note is a document outlining the capabilities, limitations,
    and environmental impact of AI technology. It’s designed to clarify how an AI
    system works, which is crucial for responsible AI implementation. Transparency
    notes are practical tools for applying AI principles and guiding the responsible
    use and deployment of AI technologies. Enterprises should consider transparency
    notes as part of their AI development for a couple of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding AI systems*—They should understand that an AI system includes
    not just the technology but also the users, those affected by it, and the environment
    in which it’s deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Informed deployment*—Transparency notes can help enterprises make informed
    decisions about developing and deploying AI systems, ensuring they are suitable
    for their intended use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparency notes are a helpful way to enhance transparency and accountability,
    which are important for the ethical creation and use of AI systems. For instance,
    Azure OpenAI Service’s transparency notes explain system features, boundaries,
    applications, and best practices to optimize system performance. You can access
    these transparency notes at [https://mng.bz/aVKm](https://mng.bz/aVKm).
  prefs: []
  type: TYPE_NORMAL
- en: B.3 HAX Toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The HAX Toolkit ([https://aka.ms/haxtoolkit](https://aka.ms/haxtoolkit)), developed
    by Microsoft Research in collaboration with Aether, Microsoft’s advisory body
    on AI ethics and effects in engineering and research, is a suite of practical
    tools designed to facilitate the creation of responsible human–AI experiences.
    It includes guidelines for human–AI interaction, a workbook, design patterns,
    a playbook, and a design library, all aimed at helping teams strategically create
    AI technologies that interact with people.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises should consider the HAX Toolkit a valuable resource when implementing
    RAI in their AI development process. It provides actionable guidance grounded
    in research and validated through practical application. The toolkit can help
    teams prioritize guidelines, plan resources, and address common design challenges.
    It also prepares for unforeseen errors, ensuring that AI systems are developed
    with a human-centered approach and aligned with responsible AI principles.
  prefs: []
  type: TYPE_NORMAL
- en: The HAX Toolkit addresses bias and fairness in AI systems by providing practical
    tools that translate knowledge of human–AI interaction into actionable guidance
    for AI creators. It helps teams prioritize guidelines and plan resources to address
    priorities, including bias and fairness. The toolkit includes
  prefs: []
  type: TYPE_NORMAL
- en: '*Guidelines for human–AI interaction*—Best practices for how AI applications
    should interact with people'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HAX workbook*—Helps teams prioritize guidelines and plan the time and resources
    needed to address high-priority items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HAX design patterns*—Offer flexible solutions for common problems in designing
    human–AI systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HAX playbook*—Assists teams in identifying and planning for unforeseen errors,
    such as transcription errors or false positives, which can be sources of bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HAX design library*—A searchable database of design patterns and implementation
    examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By utilizing these resources, teams can ensure that their AI systems are designed
    with a human-centered approach that considers fairness and mitigates bias throughout
    the AI application lifecycle. More details can be found at [https://mng.bz/gAxv](https://mng.bz/gAxv).
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Responsible AI Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Responsible AI Toolbox ([https://mng.bz/5OWO](https://mng.bz/5OWO)) is a
    suite of tools provided by Microsoft to help operationalize RAI practices. It
    includes integrated tools and functionalities enabling users to assess their AI
    models and make more efficient user-facing decisions. The toolbox is designed
    to be flexible and model agnostic, which means it can be used with various AI
    models, including generative ones.
  prefs: []
  type: TYPE_NORMAL
- en: The toolbox provides interfaces and libraries that empower AI system developers
    and stakeholders to develop and monitor AI more responsibly. Its capabilities
    can benefit enterprises looking to ensure that their use of generative AI aligns
    with RAI principles. One key area covered by it is the Responsible AI Dashboard,
    which combines various RAI capabilities to help practitioners optimize their machine
    learning (ML) models for fairness, explainability, and other desired characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: It’s designed to assist in assessing and debugging ML models, providing insights
    to help business decision-makers make more informed decisions. It combines several
    advanced tools in domains such as model reliability, interpretability, fairness,
    and compliance, giving a complete evaluation and troubleshooting of models for
    data-based decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is mainly used for conventional ML models rather than generative models
    such as LLMs. Still, sometimes these models work together in a workflow, enhancing
    each other, and in that situation, the RAI dashboard is useful. The dashboard
    helps with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Holistic assessment*—It provides a single interface for various RAI tools,
    enabling a complete evaluation of ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Customizable interface*—Users can tailor the dashboard to include only the
    relevant tools for their use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model debugging*—It supports model debugging through stages of assessment,
    understanding, and mitigation, focusing on model reliability, interpretability,
    fairness, and compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B.5 Learning Interpretability Tool (LIT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Learning Interpretability Tool (LIT) is an open source tool ([https://pair-code.github.io/lit](https://pair-code.github.io/lit))
    designed to help us understand and interpret ML models. It supports various data
    types, including text, image, and tabular data, and can be used with different
    ML frameworks such as TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: LIT is part of the broader Responsible GenAI Toolkit ([https://ai.google.dev/responsible](https://ai.google.dev/responsible))
    from Google, designed to work on Google Cloud. LIT provides features such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Local explanations*—Produced through salience maps, attention visualization,
    and model predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Aggregate analysis*—Including custom metrics, slicing, binning, and visualization
    of embedding spaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Counterfactual generation*—Used to create and evaluate new examples dynamically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enterprises can use LIT with generative AI applications to debug and analyze
    models, helping them understand why and how models behave the way they do. LIT
    can also help improve model outputs by using interpretability techniques such
    as sequence salience to analyze the impact of prompt designs on model outputs
    and test hypothesized improvements. By analyzing and documenting the behavior
    of generative models, enterprises can align with RAI principles.
  prefs: []
  type: TYPE_NORMAL
- en: B.6 AI Fairness 360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI Fairness 360 (AIF360 Paper: [https://arxiv.org/abs/1810.01943](https://arxiv.org/abs/1810.01943))
    is an open source toolkit ([https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360))
    that helps users check, measure, and reduce discrimination and bias in ML models
    at any stage of the AI application lifecycle. It offers a full set of fairness
    metrics and bias mitigation algorithms created by the research community to deal
    with bias in AI systems. It can be part of the AI development process to monitor
    and reduce unwanted biases. With AIF360, organizations can quantify bias by using
    over 70 fairness metrics. After we measure bias, then AIF360 can help eliminate
    it by applying advanced algorithms to decrease bias in training data and models
    and ensure compliance by following ethical standards and regulations and showing
    efforts to address AI fairness.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, AIF360 helps with red-teaming, and enterprises build trust with users
    and stakeholders by ensuring their fair and equitable AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: B.7 C2PA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Coalition for Content Provenance and Authenticity (C2PA) is a project that
    develops technical standards to certify the source and history of media content
    online. It aims to prevent the spread of misleading information by providing a
    way to trace the origin of different types of media, such as images, videos, and
    documents. The standard is a collaboration between major tech companies, and it
    enables content creators to attach cryptographically signed metadata, C2PA manifests,
    to digital assets. This metadata can verify the content’s origin and any subsequent
    edits, increasing trust and authenticity in digital media.
  prefs: []
  type: TYPE_NORMAL
- en: C2PA allows the creation of content credentials for a digital media file, which
    shows the creation process, including the creator’s identity and the tools used.
    These credentials are then secured with digital signatures to prevent tampering.
    When the media is shared, the embedded C2PA metadata enables others to check the
    authenticity of the media and any changes that have been made. A few open source
    tools, such as c2patool, can help with this task ([https://github.com/contentauth/c2patool](https://github.com/contentauth/c2patool)).
  prefs: []
  type: TYPE_NORMAL
