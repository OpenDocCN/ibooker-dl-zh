- en: appendix B Responsible AI tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B 负责任的人工智能工具
- en: As generative AI models have become increasingly prevalent in enterprises, ensuring
    that they are developed and deployed responsibly is essential. Responsible AI
    (RAI) practices can help organizations build stakeholder trust, meet regulatory
    requirements, and avoid unintended consequences. Fortunately, many tools are available
    to support developers and architects in integrating RAI principles into their
    AI systems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成式人工智能模型在企业中的日益普及，确保它们负责任地开发和部署至关重要。负责任的人工智能（RAI）实践可以帮助组织建立利益相关者的信任，满足监管要求，并避免意外后果。幸运的是，许多工具可供开发者和技术架构师使用，以将RAI原则融入他们的AI系统中。
- en: The following sections outline some of these tools and frameworks, which can
    help ensure transparency, fairness, interpretability, and security in AI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分概述了一些这些工具和框架，它们可以帮助确保AI中的透明度、公平性、可解释性和安全性。
- en: B.1 Model card
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 模型卡
- en: A model card is a special type of documentation accompanying an AI model. It
    provides a standardized information set about the model’s purpose, performance,
    training data, ethical considerations, and more. It’s akin to a product data sheet,
    offering transparency and facilitating responsible AI practices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡是伴随AI模型的一种特殊文档。它提供有关模型目的、性能、训练数据、伦理考量等方面的标准化信息集。它类似于产品数据表，提供透明度并促进负责任的人工智能实践。
- en: 'While it might seem odd to think of model cards as an RAI tool, they serve
    an important role in the context of RAI. Model cards are considered an essential
    RAI tool. They help stakeholders understand the capabilities and limitations of
    GenAI models, such as those based on GPT architectures, ensuring that these powerful
    tools are used ethically and effectively:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将模型卡视为RAI工具可能看似奇怪，但它们在RAI的背景下发挥着重要作用。模型卡被视为RAI的必要工具。它们帮助利益相关者了解基于GPT架构等生成式AI模型的性能和局限性，确保这些强大的工具被道德和有效地使用：
- en: '*Promoting transparency*—They detail the model’s characteristics, limitations,
    and ideal use cases.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*促进透明度*——它们详细说明了模型的特点、局限性和理想用例。'
- en: '*Encouraging accountability*—By documenting the model’s development process,
    model cards help ensure creators remain accountable for their AI systems.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鼓励问责制*——通过记录模型的发展过程，模型卡有助于确保创作者对其AI系统负责。'
- en: '*Facilitating informed use*—They provide users with the necessary information
    to understand how the model should be used, thus preventing misuse.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*促进明智使用*——它们为用户提供必要的信息，以了解如何使用模型，从而防止误用。'
- en: 'For large language models (LLMs), model cards typically include the following
    details:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLMs），模型卡通常包括以下详细信息：
- en: '*Model details*—Information about the model’s architecture, size, training
    data, and training procedures'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型详情*——有关模型架构、大小、训练数据和训练程序的信息'
- en: '*Intended use*—A description of the tasks the model is designed for and any
    limitations on its intended use'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预期用途*——描述模型设计用于的任务及其预期用途的限制'
- en: '*Performance metrics*—Benchmarks and evaluation results showing how the model
    performs on various tasks'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能指标*——显示模型在各项任务上表现如何的基准和评估结果'
- en: '*Ethical considerations*—Any ethical concerns related to the model’s use, including
    potential biases'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*伦理考量*——与模型使用相关的任何伦理问题，包括潜在的偏见'
- en: '*Caveats and recommendations*—Any warnings or suggestions for users of the
    model'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意事项和建议*——对模型用户的任何警告或建议'
- en: For example, OpenAI’s GPT-4 model card is called a system card and is 60 pages
    long. It calls out multiple risks related to safety challenges, such as hallucinations,
    harmful content, potential for risky emergent behaviors, overreliance, and so
    forth. More details on model cards can be found at [https://mng.bz/M1gB](https://mng.bz/M1gB).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI的GPT-4模型卡被称为系统卡，长达60页。它指出了与安全挑战相关的多个风险，例如幻觉、有害内容、可能的风险涌现行为、过度依赖等。有关模型卡的更多详细信息，请参阅[https://mng.bz/M1gB](https://mng.bz/M1gB)。
- en: B.2 Transparency notes
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 透明度笔记
- en: 'A transparency note is a document outlining the capabilities, limitations,
    and environmental impact of AI technology. It’s designed to clarify how an AI
    system works, which is crucial for responsible AI implementation. Transparency
    notes are practical tools for applying AI principles and guiding the responsible
    use and deployment of AI technologies. Enterprises should consider transparency
    notes as part of their AI development for a couple of reasons:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度笔记是一份概述人工智能技术能力、局限性和环境影响文档。它旨在阐明人工智能系统的工作原理，这对于负责任地实施人工智能至关重要。透明度笔记是应用人工智能原则和指导人工智能技术的负责任使用和部署的实用工具。企业应考虑以下原因将透明度笔记作为其人工智能开发的一部分：
- en: '*Understanding AI systems*—They should understand that an AI system includes
    not just the technology but also the users, those affected by it, and the environment
    in which it’s deployed.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解人工智能系统*—他们应该了解人工智能系统不仅包括技术，还包括用户、受其影响的人以及其部署的环境。'
- en: '*Informed deployment*—Transparency notes can help enterprises make informed
    decisions about developing and deploying AI systems, ensuring they are suitable
    for their intended use.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知情部署*—透明度笔记可以帮助企业就开发部署人工智能系统做出明智的决定，确保它们适合其预期用途。'
- en: Transparency notes are a helpful way to enhance transparency and accountability,
    which are important for the ethical creation and use of AI systems. For instance,
    Azure OpenAI Service’s transparency notes explain system features, boundaries,
    applications, and best practices to optimize system performance. You can access
    these transparency notes at [https://mng.bz/aVKm](https://mng.bz/aVKm).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度笔记是一种有助于提高透明度和问责制的有用方式，这对于人工智能系统的道德创造和使用至关重要。例如，Azure OpenAI 服务的透明度笔记解释了系统功能、边界、应用和最佳实践，以优化系统性能。您可以在[https://mng.bz/aVKm](https://mng.bz/aVKm)访问这些透明度笔记。
- en: B.3 HAX Toolkit
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 HAX 工具包
- en: The HAX Toolkit ([https://aka.ms/haxtoolkit](https://aka.ms/haxtoolkit)), developed
    by Microsoft Research in collaboration with Aether, Microsoft’s advisory body
    on AI ethics and effects in engineering and research, is a suite of practical
    tools designed to facilitate the creation of responsible human–AI experiences.
    It includes guidelines for human–AI interaction, a workbook, design patterns,
    a playbook, and a design library, all aimed at helping teams strategically create
    AI technologies that interact with people.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: HAX 工具包（[https://aka.ms/haxtoolkit](https://aka.ms/haxtoolkit)），由微软研究院与微软在工程和研究中的道德和影响咨询机构
    Aether 合作开发，是一套旨在促进负责任的人类-人工智能体验创建的实用工具。它包括人类-人工智能交互指南、工作簿、设计模式、剧本和设计库，所有这些都有助于团队战略性地创建与人类互动的人工智能技术。
- en: Enterprises should consider the HAX Toolkit a valuable resource when implementing
    RAI in their AI development process. It provides actionable guidance grounded
    in research and validated through practical application. The toolkit can help
    teams prioritize guidelines, plan resources, and address common design challenges.
    It also prepares for unforeseen errors, ensuring that AI systems are developed
    with a human-centered approach and aligned with responsible AI principles.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 企业在实施其人工智能开发过程中的负责任人工智能（RAI）时，应将 HAX 工具包视为一项宝贵资源。它提供了基于研究和通过实际应用验证的可操作指导。工具包可以帮助团队优先考虑指南，规划资源，并解决常见的设计挑战。它还准备应对意外错误，确保人工智能系统以以人为本的方法开发，并与负责任的人工智能原则保持一致。
- en: The HAX Toolkit addresses bias and fairness in AI systems by providing practical
    tools that translate knowledge of human–AI interaction into actionable guidance
    for AI creators. It helps teams prioritize guidelines and plan resources to address
    priorities, including bias and fairness. The toolkit includes
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: HAX 工具包通过提供将人类-人工智能交互知识转化为人工智能创造者可操作指导的实际工具，来解决人工智能系统中的偏见和公平性问题。它帮助团队优先考虑指南并规划资源以应对优先事项，包括偏见和公平性。工具包包括
- en: '*Guidelines for human–AI interaction*—Best practices for how AI applications
    should interact with people'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人类-人工智能交互指南*—人工智能应用应如何与人类互动的最佳实践'
- en: '*HAX workbook*—Helps teams prioritize guidelines and plan the time and resources
    needed to address high-priority items'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HAX 工作簿*—帮助团队优先考虑指南并规划解决高优先级项目所需的时间和资源'
- en: '*HAX design patterns*—Offer flexible solutions for common problems in designing
    human–AI systems'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HAX 设计模式*—为设计人类-人工智能系统中的常见问题提供灵活的解决方案'
- en: '*HAX playbook*—Assists teams in identifying and planning for unforeseen errors,
    such as transcription errors or false positives, which can be sources of bias'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HAX操作手册*—帮助团队识别和规划不可预见错误，例如转录错误或假阳性，这些可能是偏见来源。'
- en: '*HAX design library*—A searchable database of design patterns and implementation
    examples'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HAX设计库*—一个可搜索的设计模式和实现示例数据库'
- en: By utilizing these resources, teams can ensure that their AI systems are designed
    with a human-centered approach that considers fairness and mitigates bias throughout
    the AI application lifecycle. More details can be found at [https://mng.bz/gAxv](https://mng.bz/gAxv).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这些资源，团队可以确保他们的AI系统在设计时考虑到以人为中心的方法，在整个AI应用生命周期中考虑公平性并减轻偏见。更多详情可以在[https://mng.bz/gAxv](https://mng.bz/gAxv)找到。
- en: B.4 Responsible AI Toolbox
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 负责任AI工具箱
- en: The Responsible AI Toolbox ([https://mng.bz/5OWO](https://mng.bz/5OWO)) is a
    suite of tools provided by Microsoft to help operationalize RAI practices. It
    includes integrated tools and functionalities enabling users to assess their AI
    models and make more efficient user-facing decisions. The toolbox is designed
    to be flexible and model agnostic, which means it can be used with various AI
    models, including generative ones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任AI工具箱([https://mng.bz/5OWO](https://mng.bz/5OWO))是微软提供的一套工具，旨在帮助实施RAI实践。它包括集成工具和功能，使用户能够评估他们的AI模型并做出更有效的面向用户的决策。工具箱旨在灵活且不依赖于特定模型，这意味着它可以与各种AI模型一起使用，包括生成模型。
- en: The toolbox provides interfaces and libraries that empower AI system developers
    and stakeholders to develop and monitor AI more responsibly. Its capabilities
    can benefit enterprises looking to ensure that their use of generative AI aligns
    with RAI principles. One key area covered by it is the Responsible AI Dashboard,
    which combines various RAI capabilities to help practitioners optimize their machine
    learning (ML) models for fairness, explainability, and other desired characteristics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 工具箱提供了接口和库，使AI系统开发者和利益相关者能够更有责任地开发和监控AI。其功能可以惠及那些希望确保其生成AI的使用与RAI原则一致的企业。它涵盖的一个关键领域是负责任AI仪表板，该仪表板结合了各种RAI能力，帮助从业者优化他们的机器学习（ML）模型以实现公平性、可解释性和其他期望的特性。
- en: It’s designed to assist in assessing and debugging ML models, providing insights
    to help business decision-makers make more informed decisions. It combines several
    advanced tools in domains such as model reliability, interpretability, fairness,
    and compliance, giving a complete evaluation and troubleshooting of models for
    data-based decisions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在协助评估和调试机器学习模型，提供洞察力以帮助业务决策者做出更明智的决策。它结合了模型可靠性、可解释性、公平性和合规性等领域的多个高级工具，为基于数据的决策提供完整的评估和故障排除。
- en: 'This is mainly used for conventional ML models rather than generative models
    such as LLMs. Still, sometimes these models work together in a workflow, enhancing
    each other, and in that situation, the RAI dashboard is useful. The dashboard
    helps with the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要适用于传统的机器学习模型，而不是像LLM这样的生成模型。然而，有时这些模型在工作流程中一起工作，相互增强，在这种情况下，RAI仪表板是有用的。仪表板有助于以下方面：
- en: '*Holistic assessment*—It provides a single interface for various RAI tools,
    enabling a complete evaluation of ML models.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全面评估*—它提供了一个单一界面，用于各种RAI工具，能够对机器学习模型进行完整评估。'
- en: '*Customizable interface*—Users can tailor the dashboard to include only the
    relevant tools for their use case.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可定制界面*—用户可以根据自己的使用场景定制仪表板，只包含相关的工具。'
- en: '*Model debugging*—It supports model debugging through stages of assessment,
    understanding, and mitigation, focusing on model reliability, interpretability,
    fairness, and compliance.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型调试*—它通过评估、理解和缓解的阶段支持模型调试，重点关注模型可靠性、可解释性、公平性和合规性。'
- en: B.5 Learning Interpretability Tool (LIT)
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 学习可解释性工具（LIT）
- en: The Learning Interpretability Tool (LIT) is an open source tool ([https://pair-code.github.io/lit](https://pair-code.github.io/lit))
    designed to help us understand and interpret ML models. It supports various data
    types, including text, image, and tabular data, and can be used with different
    ML frameworks such as TensorFlow and PyTorch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 学习可解释性工具（LIT）是一个开源工具([https://pair-code.github.io/lit](https://pair-code.github.io/lit))，旨在帮助我们理解和解释机器学习模型。它支持多种数据类型，包括文本、图像和表格数据，并且可以与不同的机器学习框架如TensorFlow和PyTorch一起使用。
- en: LIT is part of the broader Responsible GenAI Toolkit ([https://ai.google.dev/responsible](https://ai.google.dev/responsible))
    from Google, designed to work on Google Cloud. LIT provides features such as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LIT是谷歌更广泛的负责任通用人工智能工具包（[https://ai.google.dev/responsible](https://ai.google.dev/responsible)）的一部分，旨在谷歌云上运行。LIT提供如下功能：
- en: '*Local explanations*—Produced through salience maps, attention visualization,
    and model predictions'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*局部解释*——通过显著性图、注意力可视化和模型预测生成'
- en: '*Aggregate analysis*—Including custom metrics, slicing, binning, and visualization
    of embedding spaces'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚合分析*——包括自定义指标、切片、分箱和嵌入空间的可视化'
- en: '*Counterfactual generation*—Used to create and evaluate new examples dynamically'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反事实生成*——用于动态创建和评估新示例'
- en: Enterprises can use LIT with generative AI applications to debug and analyze
    models, helping them understand why and how models behave the way they do. LIT
    can also help improve model outputs by using interpretability techniques such
    as sequence salience to analyze the impact of prompt designs on model outputs
    and test hypothesized improvements. By analyzing and documenting the behavior
    of generative models, enterprises can align with RAI principles.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 企业可以使用LIT与生成式人工智能应用结合，用于调试和分析模型，帮助他们理解模型为何以及如何表现出特定的行为。LIT还可以通过使用诸如序列显著性等可解释性技术来分析提示设计对模型输出的影响，并测试假设的改进。通过分析和记录生成模型的运行行为，企业可以与负责任人工智能（RAI）原则保持一致。
- en: B.6 AI Fairness 360
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.6 AI Fairness 360
- en: 'AI Fairness 360 (AIF360 Paper: [https://arxiv.org/abs/1810.01943](https://arxiv.org/abs/1810.01943))
    is an open source toolkit ([https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360))
    that helps users check, measure, and reduce discrimination and bias in ML models
    at any stage of the AI application lifecycle. It offers a full set of fairness
    metrics and bias mitigation algorithms created by the research community to deal
    with bias in AI systems. It can be part of the AI development process to monitor
    and reduce unwanted biases. With AIF360, organizations can quantify bias by using
    over 70 fairness metrics. After we measure bias, then AIF360 can help eliminate
    it by applying advanced algorithms to decrease bias in training data and models
    and ensure compliance by following ethical standards and regulations and showing
    efforts to address AI fairness.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: AI Fairness 360（AIF360论文：[https://arxiv.org/abs/1810.01943](https://arxiv.org/abs/1810.01943)）是一个开源工具包（[https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)），帮助用户在任何人工智能应用生命周期的任何阶段检查、衡量和减少机器学习模型中的歧视和偏见。它提供了一套由研究社区创建的公平性指标和偏差缓解算法，以处理人工智能系统中的偏差。它可以作为人工智能开发过程的一部分，用于监控和减少不希望的偏差。使用AIF360，组织可以通过超过70个公平性指标来量化偏差。在测量偏差之后，AIF360可以通过应用高级算法来减少训练数据和模型中的偏差，并确保合规性，遵循伦理标准和法规，并展示解决人工智能公平性的努力。
- en: Finally, AIF360 helps with red-teaming, and enterprises build trust with users
    and stakeholders by ensuring their fair and equitable AI systems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，AIF360有助于进行红队测试，企业通过确保其公平和均衡的人工智能系统，与用户和利益相关者建立信任。
- en: B.7 C2PA
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.7 C2PA
- en: The Coalition for Content Provenance and Authenticity (C2PA) is a project that
    develops technical standards to certify the source and history of media content
    online. It aims to prevent the spread of misleading information by providing a
    way to trace the origin of different types of media, such as images, videos, and
    documents. The standard is a collaboration between major tech companies, and it
    enables content creators to attach cryptographically signed metadata, C2PA manifests,
    to digital assets. This metadata can verify the content’s origin and any subsequent
    edits, increasing trust and authenticity in digital media.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 内容来源和真实性联盟（C2PA）是一个开发技术标准以认证在线媒体内容来源和历史的计划。它旨在通过提供一种追踪不同类型媒体（如图像、视频和文档）起源的方法来防止误导性信息的传播。该标准是主要科技公司之间的合作成果，它使内容创作者能够将加密签名元数据、C2PA清单附加到数字资产上。这些元数据可以验证内容的来源和任何后续编辑，从而增加数字媒体的可信度和真实性。
- en: C2PA allows the creation of content credentials for a digital media file, which
    shows the creation process, including the creator’s identity and the tools used.
    These credentials are then secured with digital signatures to prevent tampering.
    When the media is shared, the embedded C2PA metadata enables others to check the
    authenticity of the media and any changes that have been made. A few open source
    tools, such as c2patool, can help with this task ([https://github.com/contentauth/c2patool](https://github.com/contentauth/c2patool)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: C2PA 允许为数字媒体文件创建内容凭证，这些凭证显示了创作过程，包括创作者的身份和使用的工具。这些凭证随后通过数字签名进行加密，以防止篡改。当媒体被分享时，嵌入的
    C2PA 元数据使其他人能够检查媒体的真实性以及任何已做的更改。一些开源工具，如 c2patool，可以帮助完成这项任务 ([https://github.com/contentauth/c2patool](https://github.com/contentauth/c2patool)).
