- en: 8 Graph, multimodal, agentic, and other RAG variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing RAG variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graph RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agentic RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other RAG variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first part of the book introduced retrieval-augmented generation (RAG) and
    the core idea behind it. The second part dealt with building and evaluating basic
    RAG systems. Part 3 took RAG beyond the naïve approach and discussed advanced
    techniques and the technology stack that supports a RAG system. The last part
    of the book looks at more RAG patterns, and we conclude our discussion with a
    few best practices and some areas for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8 looks at some popular RAG variants. These variants adapt different
    stages of RAG (i.e., indexing, retrieval, augmentation, and generation) to specific
    use case requirements. The chapter begins by discussing the emergence of these
    variants and the purpose they serve. We then continue talking about three important
    variants that have gained prominence in applied RAG. These are knowledge-graph-enhanced,
    multimodal, and agentic RAG. We also briefly examine other RAG variants that significantly
    contribute to the evolution of RAG in practical applications. We discuss the purpose
    and motivation behind each variant. This chapter also breaks down the workflow,
    features, and technical details of the variants along with their strengths and
    weaknesses. For simplicity, the code for these variants is not included in this
    chapter but can be found in the book’s code repository.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Be familiar with the idea and motivation behind RAG variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an in-depth understanding of graph, multimodal, and agentic RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of several popular RAG variants and the use cases they solve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several limitations of a naïve approach to RAG that affect the overall
    usability of a standard RAG system. These limitations range from difficulties
    in understanding relationships across different documents to challenges in handling
    various data types, as well as concerns regarding system cost and efficiency.
    Chapter 6 discussed several pre-retrieval, retrieval, and post-retrieval techniques,
    such as index optimization, query optimization, hybrid and iterative retrieval
    strategies, compression, and re-ranking, which address different limitations and
    improve the accuracy of a RAG system. Several RAG patterns that incorporate one
    or more of these techniques have emerged over time to solve specific use challenges.
    We refer to them as RAG variants.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 What are RAG variants, and why do we need them?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The universe of applications that rely on RAG is expanding every day. Some of
    these applications process not just text, but different data modalities such as
    image, video, and audio as well. Others are being applied in domains such as healthcare
    and finance, where the effects of inaccurate results are catastrophic. The emerging
    domain of using LLMs as decision-making agents has also enabled a more adaptive
    and intelligent RAG system. Apart from factual accuracy, practical RAG applications
    demand low latency and low costs to enhance user experience and adoption. As the
    range of applications for RAG has expanded, so need specialized variations of
    RAG—known as RAG variants—designed to address unique challenges across different
    tasks and data types.
  prefs: []
  type: TYPE_NORMAL
- en: These RAG variants are adaptations of the standard RAG framework that extend
    its functionality to meet demands of diverse and complex use cases. By employing
    advanced pre-retrieval, retrieval and post-retrieval techniques, these variants
    enhance RAG with capabilities such as handling multimodal data, providing higher
    accuracy, and better relational understanding. The evolution of these RAG variants
    makes the system both flexible and domain aware.
  prefs: []
  type: TYPE_NORMAL
- en: 'While several RAG variants have emerged, the three that we are going to discuss
    in-depth in the subsequent sections have gained prominence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multimodal RA**G*—Extends capabilities of the standard RAG beyond text data
    and incorporates other data types such as images, video, and audio. This characteristic
    enables the system to fetch information from nontextual documents and provide
    additional context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Knowledge graph RA**G*—Integrates knowledge graphs into the retrieval process.
    This idea was introduced in chapter 6 as part of improving the indexing structure.
    Knowledge graphs help establish relationships between entities, providing better
    context, especially in multi-hop queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Agentic RA**G*—Incorporates LLM agents into the RAG framework. These agents
    enable autonomous decision making across the RAG value chain from indexing to
    generation. Simultaneously, all components become adaptive to the user query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these three, we also touch upon additional variants, such as
    corrective RAG, self-RAG, and more, but first, we begin by discussing multimodality.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Multimodal RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have seen that standard RAG systems are effective in managing
    and retrieving textual data to generate context-aware and grounded responses.
    However, the scope of enterprise data extends beyond text to image, audio, and
    video. Standard RAG systems fall short when attempting to interpret nontextual
    data formats. This is the core motivation behind a multimodal variant of RAG,
    which extends the capabilities to more data formats.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Data modality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multimodality can be a confusing term for the uninitiated, especially because
    “modality” varies in meaning across different fields. Grammatical modality relates
    to the expression of the speaker’s attitude, while treatment modality may refer
    to the medical approach in medicine. In RAG, and AI in general, modality refers
    to data format. Text is a modality, image is a modality, video and audio are different
    modalities, and we can also consider tables and code as distinct modalities. Figure
    8.1 shows some data modalities, including less common ones such as genomic and
    3D data.
  prefs: []
  type: TYPE_NORMAL
- en: '![A group of icons with text'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F01_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1  Examples of different data modalities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multimodal RAG is, therefore, the extended variant of standard RAG with the
    capability to process multiple data modalities. Before diving into the requirements
    and architectural details of multimodal RAG, let’s ponder over the use cases where
    multimodal RAG is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Multimodal RAG use cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several industries and functions where a multimodal variant of RAG
    is required, such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Medical diagnosi**s*—A diagnostic assistant can work with patient records
    that may include medical history (in text form), lab results (in tabular form),
    and diagnostic images (like X-rays, MRIs, etc.), along with studies and research
    papers that include graphs, charts, or microscopic images. When the patient comes
    in for a consultation, this assistant can provide a holistic analysis to the doctor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Investment analysi**s*—Working with financial reports and other filings that
    have charts showing trends, earnings, and projections along with balance sheets
    and income statements in tabular form, apart from the usual text commentary, an
    investment research assistant can provide analysts with crucial information needed
    to make investment decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Buying assistanc**e*—Through an analysis of product images, textual descriptions,
    product specifications (in tabular form), and customer reviews, a shopping assistant
    can help the shoppers on an e-commerce website with personalized recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coding assistanc**e*—Coding assistants retrieve relevant documentation, function
    usage examples, and code snippets from repositories based on the query context.
    For example, when a developer asks how to implement a certain API function. The
    RAG system retrieves precise code snippets and explanations from the documentation,
    helping the developer avoid time-consuming searches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Equipment maintenanc**e*—Using historical text reports with visual inspection
    images or video feed, sensor data, and performance tables, a maintenance assistant
    can provide maintenance recommendations and trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples. While standard text-only RAG finds acceptability
    in the initial stages of a use case, a large proportion of production-grade RAG
    systems incorporate at least one other modality of data.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Multimodal RAG pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now explore how developing a multimodal RAG pipeline differs from a standard
    text-only RAG pipeline you have learned so far. An obvious change will be in loading
    and indexing the data of nontext modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal indexing pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Developing the knowledge base for multimodal RAG requires enhancement in each
    of the four components of the indexing pipelines. Apart from loading and chunking
    files of different modalities, creating embeddings for multimodal data requires
    special attention. Let’s look at each of the components one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '*The data-loading step* is quite like the standard text-only RAG but now includes
    connectors and data loaders for nontext modalities. There are several options
    available. `Pillow`, also known as `PIL`, is a popular Python library for loading
    images. `Unstructured` is an open source library that includes components for
    ingesting a variety of data formats. `Pydub` is another Python library that allows
    the loading of audio files such as WAV and MP3\. LangChain provides an integration
    with the unstructured library. `UnstructuredImageLoader` is a class available
    in LangChain document loaders for loading images. For audio and video transcription,
    libraries such as `OpenAIWhisperParser`, `AssemblyAIAudioTranscriptLoader`, and
    `YoutubeLoader` can be used. Likewise, for tabular data `CSVLoader` and `DataFrameLoader`
    come in handy. For simplicity, sometimes data of different modalities is transcribed
    into text.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunking* for multimodal data largely follows a process similar to text chunking
    in cases where audio/video data is transcribed and stored as text. However, for
    raw audio and video data, specific chunking methods can be employed. Voice activity
    detection (VAD) chunks the data based on silences or background noise in the audio.
    Scene-detection-based chunking identifies major changes in the scene to segment
    the video. For tabular data, sometimes row/column-level chunking can be incorporated,
    and for code, the chunking can be carried out at a function, a class, or a logical
    unit level. All strategies used for chunking text data such as context enrichment,
    semantic chunking, and similar are also held here. For images, chunking is generally
    not done. `semantic_chunkers` is a multimodal chunking library for intelligent
    chunking of text, video, and audio. It makes AI and data processing more efficient
    and accurate.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Embeddings* is where nuance begins in multimodal RAG. In standard text-only
    RAG, there are several embeddings models available to vectorize the chunks. But
    how does one vectorize data of different modalities, such as an image? There are
    three approaches to deal with this complexity: shared or joint embedding models,
    modality-specific embeddings, and conversion of all non-text data into text.'
  prefs: []
  type: TYPE_NORMAL
- en: Shared or joint embeddings models map diverse data types into a unified embeddings
    space. By doing this, cross-modal retrieval is enabled, such as finding images
    based on textual descriptions or generating text from images. Google Vertex AI
    offers shared embeddings models that generate vectors for all data modalities
    in the unified embeddings space. Shared embeddings models are also called multimodal
    embeddings models. While efficient at understanding general image data, multimodal
    embeddings sometimes fall short when granular understanding is needed, as in charts
    and tables represented as images and infographics. In figure 8.2, image, text,
    audio, and video data are plotted in the same 3D vector space.
  prefs: []
  type: TYPE_NORMAL
- en: The modality-specific embeddings approach resemble multimodal embeddings, except
    that instead of a single embeddings space for all modalities, the embeddings space
    maps only two modalities. In such a scenario, we need an image–text embeddings
    model to process text, image, and audio data (e.g., Contrastive Language–Image
    Pretraining, or CLIP) and an audio-text embeddings model (e.g., Contrastive Language–Audio
    Pretraining, or CLAP). The knowledge base has text, image, and audio embeddings
    in different embeddings spaces and stored separately. Figure 8.3 is an example
    of CLIP image–text embeddings where image and text embeddings are projected onto
    a shared embeddings space.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a dog'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F02_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 Images, text, video, and audio are plotted on the same embeddings
    space. Dog, bark, and dog’s image are close to each other.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A diagram of a number of letters'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F03_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3  CLIP uses multimodal pre-training to convert classification into
    a retrieval task, which enables pre-trained models to tackle zero-shot recognition.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Conversion of all non-text data into text is employed to first convert all nontext
    (image) data into text using a multimodal LLM and then follow the standard text-only
    RAG approach. (A multimodal LLM is a large language model that processes all modalities
    of data. You will read more about multimodal LLMs later in this section.) In this
    strategy, you may notice that we may not be entirely using multimodal data as
    information loss is bound to occur when converting nontext to text data. In a
    variation of this strategy, instead of converting all multimodal data into text
    and using it as text, a two-pronged approach is employed. Here all multimodal
    data is summarized in text using a multimodal LLM. Embeddings of this text are
    used to search for during the retrieval process. However, for generation, not
    only the summary but the actual multimodal file (e.g., a .jpeg) is retrieved and
    passed to the multimodal LLM for generation. This reduces the loss of information
    when converting to text.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings, either multimodal or text, are *stored* in vector databases such
    as standard text-only RAG. In addition to vector storage, document storage is
    required to store raw files that can be retrieved and passed to the LLM for generation.
    Document stores such as Redis can be used to store raw files. When text summaries
    are used, a key mapping of the summary embeddings to the raw documents must be
    created. Figure 8.4 shows the indexing pipeline with all three options for embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F04_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4  Multimodal indexing pipeline presenting three options
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the loading, chunking, and storage components are similar, the embedding
    component presents several options in multimodal RAG. Table 8.1 compares the indexing
    pipelines of text-only RAG and multimodal RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Indexing pipelines of text-only vs. multimodal RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Indexing component | Text-only RAG | Multimodal RAG |'
  prefs: []
  type: TYPE_TB
- en: '| Loading | Standard text data loaders are used to load documents, such as
    plain text files, PDFs, and other text-based formats. | Requires connectors for
    additional data types. For images, libraries such as `Pillow` (`PIL`) and `Unstructured­ImageLoader`
    in LangChain are used; for audio, we use libraries such as `Pydub` or `OpenAIWhisperParser`,
    whereas `CSVLoader` and `DataFrameLoader` are used for tabular data. Audio and
    video transcription tools such as AssemblyAI and YoutubeLoader are also incorporated
    to preprocess audio/video content. |'
  prefs: []
  type: TYPE_TB
- en: '| Chunking | Text data is divided into segments (chunks) based on context or
    structure (e.g., sentences, paragraphs) and optionally enriched semantically.
    | Follows text chunking when data is transcribed to text (audio/video). For raw
    audio, voice activity detection (VAD) can be used to chunk by pauses. For videos,
    scene detection identifies visual transitions, and tabular data can be chunked
    row/column-wise. Image chunking is typically skipped. |'
  prefs: []
  type: TYPE_TB
- en: '| Embeddings | Text embeddings are created using a single-modality text embeddings
    model (e.g., OpenAI embeddings or BERT), which vectorizes each chunk for storage
    and retrieval. | Embeddings can be generated via multimodal embeddings models,
    which unify all data types in a shared vector space for cross-modal retrieval,
    modality-specific embeddings such as CLIP and CLAP or converting multimodal data
    to text first and use text embeddings, although this may cause information loss.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Storage | Embeddings are stored in vector databases. | Embeddings are stored
    in vector databases, but additional document storage for raw multimodal files
    may be used. |'
  prefs: []
  type: TYPE_TB
- en: Once the knowledge base is created, such as in text-only RAG, the generation
    pipeline is responsible for real-time interaction with the knowledge base. Depending
    on the embedding strategy used, the generation pipeline components adapt to incorporate
    multimodal data.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal generation pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the knowledge base is created by the indexing pipeline, the generation
    pipeline needs to search, retrieve, process, and generate multimodal data. This
    requires variations in retrieval approach and a multimodal LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieva**l*—Depending on the embeddings strategy, the retrieval technique
    varies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case a shared multimodal embeddings model is used, the retrieval process
    follows a similarity search approach, where the user query is converted into a
    vector form using the same multimodal embeddings, and the documents are retrieved
    based on their cosine similarity value irrespective of their modality.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the modality-specific embedding approach, because multiple embeddings are
    present, a multi-vector retrieval approach is employed. For a single query, documents
    are retrieved from each modality-specific embeddings space based on similarity.
    These documents may later be re-ranked before augmentation and generation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When nontext data is converted into text, the retrieval process is the same
    as the standard text-only RAG. In the variation where both text summaries and
    raw files are used, the retriever first retrieves the relevant summaries from
    the text embeddings space, and then the files from the document stores mapped
    to those summaries are also retrieved.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Augmentatio**n*—The augmentation step remains the same as text-only RAG, except
    that the augmented prompt now includes the raw multimodal file accompanying the
    text prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generatio**n*—Like multimodal embeddings, for processing and generating multimodal
    data, multimodal LLMs are used. LLMs are limited by their ability to process text
    data only. Multimodal LLMs are transformers-based models, too, but have been trained
    on data of all modalities, in addition to text data. There are nuanced differences
    in the training process of multimodal LLMs, and the readers are encouraged to
    explore them. However, for building RAG systems, we can use the available foundation
    multimodal LLMs. OpenAI’s GPT 4o and GPT 4o mini and Google’s Gemini are popular
    proprietary multimodal LLMs, while Meta’s Llama 3.2 and Mistral AI’s Pixtral are
    open source multimodal LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the augmentation step remains similar to text-only RAG, the retrieval
    step adapts based on the embeddings strategy used, and the generation step swaps
    the LLMs with multimodal LLMs. The differences in the generation pipelines are
    highlighted in
  prefs: []
  type: TYPE_NORMAL
- en: table 8.2\.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 Indexing pipelines of text-only vs. multimodal RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Generation component | Text-only RAG | Multimodal RAG |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval | Retrieves similar text embeddings to the query using similarity
    search | Varies by embedding strategy—in shared embeddings model, a similarity
    search is employed regardless of modality, converting the query into a multimodal
    vector. In modality-specific embeddings, multi-vector retrieval is used for modality-specific
    results, and in text-converted nontext data, a standard text retrieval along with
    raw files mapped to text summaries is used. |'
  prefs: []
  type: TYPE_TB
- en: '| Augmentation | Adds retrieved text to the prompt | Similar to text-only but
    includes the raw multimodal files alongside the text in the prompt. |'
  prefs: []
  type: TYPE_TB
- en: '| Generation | Uses LLMs to generate responses | Uses multimodal LLMs instead
    of text-only LLMs. |'
  prefs: []
  type: TYPE_TB
- en: By tweaking the indexing and generation pipelines, a standard text-only RAG
    system can be upgraded to a multimodal RAG system, as illustrated in figure 8.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a multilevel system'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F05_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5  For each of the three approaches, the generation pipeline also adapts.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.2.4 Challenges and best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multimodal RAG systems are gaining prominence owing to the diversity present
    in enterprise data. However, one must note that with multimodality, the complexity
    of the system increases along with higher latency and more expenditure on multimodal
    embeddings and generation. Some of the common challenges associated with multimodal
    RAG are
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring coherent alignment between different data modalities (e.g., text and
    images) can be difficult. Utilizing multimodal embeddings projecting different
    modalities into a common embedding space does create better integration, but these
    embeddings models can still lead to inaccuracies and must be evaluated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling multiple data types may increase computational requirements and processing
    time. Robust preprocessing pipelines to standardize and align data from various
    modalities are essential. Sometimes, converting multimodal data to text and following
    a text-only RAG approach may be enough to generate the desired results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all models are capable of effectively processing and integrating multimodal
    data of all modalities. Incorporate only those that add significant value to the
    task to optimize performance and resource utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have looked at a RAG variant that extends the capability of RAG to different
    data modalities. However, standard RAG is still deficient when the information
    is dispersed across different documents. Let’s now look at a pattern in which
    knowledge graphs are used to establish higher-order relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Knowledge graph RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine summarizing a large report or answering complex questions that draw
    information from diverse sources. For example, a question such as, “What are the
    main themes in this report?” or “Which products in the catalogue are endorsed
    by the same celebrities?” are questions that are difficult for standard RAG systems
    to answer.
  prefs: []
  type: TYPE_NORMAL
- en: In a summarization task such as the “main themes” in a report, there is no chunk
    of the document that can answer the question completely. Likewise, “endorsed by
    the same celebrities” is not likely to be present in the data for the retriever
    to search through.
  prefs: []
  type: TYPE_NORMAL
- en: To answer these kinds of complex questions requiring multi-hop reasoning, identifying
    contextual relationships, and addressing higher-order queries, a powerful RAG
    pattern that incorporates knowledge graphs has been widely successful.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is called knowledge graph RAG or simply *graph RAG* (not to be
    confused with Microsoft’s GraphRAG, which is a specific framework of knowledge
    graph RAG). It must be noted here that graph RAG is not necessarily a replacement
    for standard vector-based RAG, but a hybrid approach in which both vectors and
    graphs are used to retrieve context. Before moving forward, The following sections
    explain what knowledge graphs are and what benefits are inherent to them.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Knowledge graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term *knowledge graph* was popularized by Google somewhere around 2012 by
    integrating an entity-relationship structure into its search engine to deliver
    more accurate and context-aware results. The simplest way to understand knowledge
    graphs is through the node-and-edge structure. Nodes may represent entities such
    as people, organizations, products, and events, and edges represent relationships
    between the nodes, such as *is a part of*, *works at*, *is related to*, and so
    on. The nodes and edges can also have attributes such as id, timestamp, and similar.
    Knowledge graphs, therefore, rely on semantics or meaning to create a shared,
    human-like, understanding of data. Figure 8.6 illustrates a simple knowledge graph
    with nodes, edges, and attributes for customer data.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs offer several advantages over standard structured databases
    such as SQL by prioritizing relationships and context, which results in deeper
    data exploration. A standard row–column or a document storage does not allow for
    context a knowledge graph does.
  prefs: []
  type: TYPE_NORMAL
- en: The storage and data processing in knowledge graphs is unique. Specialized databases
    such as Neo4j, Amazon Neptune, and TigerGraph are used to store knowledge graph
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a product'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F06_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6  Knowledge graph representation of customer activity where nodes
    (circles) represent entities, edges (arrows) represent relationships, and attributes
    (rectangles) are the properties.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: data, and query languages such as Cypher, Gremlin, and SparkQL are used for
    graph traversal. Readers are encouraged to learn more about graph databases, but
    some key concepts to keep in mind are
  prefs: []
  type: TYPE_NORMAL
- en: '*Nodes and edge**s**—*Nodes represent entities, and edges represent relationships
    to form the graph structure and enable a visual structure to the knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attribute**s*—Attributes are properties of entities(nodes) and relationships(edges).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Triplet**s*—Knowledge is represented in triplets such as “customer A purchased
    product X” (node–edge–node). Here the two entities, “customer A” and “product
    X,” and one relationship, “purchased,” form a triplet. These triples are the building
    blocks of knowledge graphs, capturing facts and relationships in a structured
    way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ontolog**y*—An ontology defines the schema or structure of a knowledge graph,
    specifying the types of entities, relationships, and their properties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph embedding**s*—Graph embeddings are vector representations of nodes and
    edges that capture graph structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph query languag**e*—SPARQL, Cypher, and similar languages allow users
    to retrieve information from the graph, formulating complex queries to find patterns,
    connections, and insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph traversa**l*—This is the method of navigating through nodes and edges
    to discover paths, patterns, and insights, essential for algorithms such as shortest
    path or recommendation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of their inherent focus on relationships and context, knowledge graphs
    enhance standard RAG for a superior context-aware retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Knowledge graph RAG use cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knowledge graphs can be useful in a variety of use cases where the ability
    to handle multi-hop relationships, entity disambiguation, and complex networks
    is required. Standard RAG systems are limited to retrieving isolated information
    chunks, while knowledge graph RAG can dynamically connect and analyze data points
    within a network, making it ideal for applications requiring a deep understanding
    of interrelated data. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Personalized treatment plan**s*—Knowledge graph RAG can link drugs, treatments,
    and conditions in a networked format, which allows it to identify potential interactions
    and customize treatment recommendations based on multiple factors. Standard RAG
    can retrieve information about a specific drug or treatment but struggles to cross-reference
    interactions across a network of symptoms, conditions, and treatments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Personalized product recommendation**s*—Standard RAG can retrieve individual
    touchpoints or customer reviews but fails to capture the interconnected path a
    customer follows across their journey. Knowledge graph RAG allows for multi-hop
    reasoning across transactions, browsing history, and customer feedback, enabling
    a more holistic analysis of the journey and providing highly relevant recommendations
    based on relationships between customer behaviors and preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contract analysi**s*—Standard RAG can retrieve text from individual contracts
    or clauses but cannot map relationships among contracts, parties, or compliance
    requirements. Knowledge graph RAG can link contracts, clauses, and parties in
    a relational network, enabling it to identify conflicts, dependencies, and compliance
    risks across interconnected legal documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While standard RAG can solve simple queries, for processes that require analysis
    and reasoning on data from multiple sources, knowledge graph can prove to be advantageous.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3 Graph RAG approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge graph is a powerful data pattern. The approach to using knowledge
    graphs can be determined by the complexity of the use case and the diversity of
    data. This section discusses three common approaches that can be followed.
  prefs: []
  type: TYPE_NORMAL
- en: Structure awareness through graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the simplest approach to incorporating knowledge graphs. Recall that
    in the standard vector-based RAG approach, documents are chunked, and embeddings
    are created then and stored for retrieval. The problem that may arise is that
    the information in the adjacent chunks might not be retrieved, and a certain degree
    of context loss may happen. In section 6.2.1, we discussed a hierarchical indexing
    structure such as a parent–child structure. The parent document contains overarching
    themes or summaries, while child documents delve into specific details. During
    retrieval, the system can first locate the most relevant child documents and then
    refer to the parent documents for additional context if required. This approach
    enhances the precision of retrieval, while maintaining the broader context.
  prefs: []
  type: TYPE_NORMAL
- en: An efficient way to store documents in a hierarchical structure is in graphs.
    Parent and child documents can be stored in the nodes with a relationship “is
    child of.” More levels of hierarchies can be created. In figure 8.7, there are
    three levels of indexing hierarchy, and while the search happens at the lowest
    level, parent documents at a higher hierarchy level are retrieved for deeper context.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F07_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7  While search in a hierarchical index structure happens at the lowest
    level, retrieved documents are more contextually complete from a higher level
    of hierarchy.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Graph-enhanced vector search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graphs are not mandatory when implementing hierarchical indexing. The true value
    of knowledge graphs is realized when connections can be made across chunks. Standard
    vector-based search on a collection of chunks can be enhanced by traversing a
    knowledge graph to retrieve related chunks. To do this, a set of entities and
    relationships are extracted from the chunks using an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: In the retrieval stage, the first step is a usual vector search executed based
    on the user query. An initial set of chunks is identified that has a high similarity
    with the user query. In the next step, the knowledge graph is traversed to fetch-related
    entities around the entities of the chunks identified in the first step. By doing
    this, the retriever fetches not only the chunks similar to the user query but
    also related chunks, which leads to deeper context and can be quite effective
    in solving multi-hop queries. This is often coupled with hierarchical structures
    and a re-ranking of retrieved documents. Figure 8.8 shows an enhanced knowledge
    graph, where chuwnks also have the extracted entities and relationships. During
    retrieval, in addition to similar chunks, the parent chunks of related entities
    are also retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F08_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8  Entities and relationships extracted from the chunks play a crucial
    role. When chunks similar to the user query are retrieved, the chunks that have
    entities related to the entities of similar chunks are also retrieved.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Graph communities and community summaries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed before, knowledge graphs are about entities and their relationships.
    Depending on the process, there may be patterns in which certain entities interact
    more with each other. Graph communities are a subset of entities connected more
    densely. For example, communities of customers with similar demographics and buying
    patterns can be identified or clusters of product features that appear together
    can be discovered. Community detection algorithms such as the Leiden and the Louvain
    algorithm are employed to detect communities within a knowledge graph. After detecting
    these communities, an LLM is used to generate summaries of the entities and the
    relationship information in the community. The retrieval process can be similar
    to vector search, where initial nodes are identified using a similarity score
    and community summaries related to the nodes are fetched, or vector search can
    be employed directly on the community summaries since they already contain a deeper
    context of several entities. This approach is particularly useful when queries
    relate to the broader themes within the knowledge base. Figure 8.9 shows how the
    retrieval at a community level is sufficient to answer questions at a broader
    thematic level.
  prefs: []
  type: TYPE_NORMAL
- en: In any of these approaches, both the indexing and the retrieval pipeline need
    to be modified to incorporate the graph and create a hybrid retrieval system where
    both vector databases and graph databases exist.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.4 Graph RAG pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have been discussing, knowledge graph is a unique data pattern that requires
    specific processing and storage. RAG pipelines need to be customized to incorporate
    knowledge graphs. Depending on the approach used, both the indexing and the generation
    pipelines need tweaking.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graph RAG indexing pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The knowledge base in graph RAG requires a different kind of parsing and storage.
    New components are introduced in the indexing pipeline to create knowledge graphs,
    extract summaries, and store the data for generation. While the loading and chunking
    components remain similar, the remaining components change significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data loadin**g*—There is no difference in the loading of the documents from
    the standard vector-based RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data chunkin**g*—To create knowledge graphs from the documents, large documents
    are chunked in the same way as the vector RAG approach. These chunks are then
    passed to an LLM to extract entities and their relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Entity relationship attribute extraction (for graph-enhanced RAG**)*—This
    is a crucial step in graph enhancement because the quality of responses will depend
    on how well the entities and relationships have been identified. This step can
    be customized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a community'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F09_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9  Communities club entities under a consistent theme and summarize
    the information at this group level. Since the summaries are created from a high
    number of thematically related chunks, these summaries can answer broad queries.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: according to the need and complexity of the use case. The simplest approach
    can be to ask an LLM directly to do the extraction. The exact kind of entities
    and relationships can also be predetermined, say, allowed entities are “people,”
    “country,” and “organization,” and allowed relationships are “nationality,” “located
    at,” and “works at.” There can be another approach in which an LLM is used to
    identify the schema of the knowledge graph. Attributes can also be added to the
    entities and relationships. There can be multiple passes of this step to ensure
    that an exhaustive list has been created. Another step can be employed to remove
    redundancies and duplication. In LangChain, `LLMGraphTransformer` class is available
    in the `langchain_experimental` library that abstracts the entity relationship
    extraction from documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storag**e*—Once the entities, relationships, and attributes have been extracted,
    these can be stored in a graph database such as Neo4j. LangChain has integration
    with the Neo4j graph database, and the `Neo4jGraph` library from the `langchain_community`
    can be used. Since the entity relationship extraction is done at a chunk level,
    the storage is also iterative, and the graph database is updated after each pass.
    In LangChain, the `add_graph_documents()` function of the `Neo4jGraph` library
    can be used to directly update the knowledge graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Creating community summarie**s*—As discussed previously, once the knowledge
    graph is created, an algorithm is used to detect communities, and an LLM is used
    to create a summary of the community. `Graphrag`, a library developed by Microsoft,
    provides end-to-end knowledge graph and community summary creation from documents.
    Another approach is to just use the community summaries and store the summaries
    in a vector database and use the standard vector RAG on the community summaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This graph database can be used as the complete knowledge base or be treated
    as an addition to the regular vector database in the knowledge base. Figure 8.10
    illustrates the indexing pipeline with each step.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a graph'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F10_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10  Indexing pipeline for graph RAG. Chunks can directly be stored
    for simple structure-aware indexing, and community summaries can be created and
    stored with the graph.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Generation pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since the nature of the knowledge base in graph RAG is quite unlike standard
    RAG, it requires significant changes in the generation pipeline. The retrieval
    process becomes slightly more nuanced than vector retrieval because of an additional
    step of graph traversal. Graph databases such as Neo4j have introduced vector
    indexes, via the Neo4j vector search plugin, which represent nodes and attributes
    as embeddings and enable similarity search. For effective retrieval, the user
    query (in natural language) is converted into a graph query that can be used to
    traverse the knowledge graph. Neo4j uses a graph query language called Cypher.
    For using the Cypher query language, there are a couple of approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Template base**d*—Several pre-defined Cypher templates are created and based
    on the user query, an LLM selects which template to use. This is an extremely
    rigid and limiting approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM-generated quer**y*—An LLM generates the Cypher query directly based on
    the natural language user query. Prompt engineering techniques such as few-shot
    prompting are employed. This approach is more flexible than a template-based approach,
    but not 100% reliable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In LangChain, the `GraphCypherQAChain` class is from the `langchain.chains`
    library. For better querying, the schema of the knowledge graph is also provided
    to the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Augmentatio**n*—Depending on the graph query, the response received from the
    graph database is processed to extract the text that can be augmented to the original
    user query. Apart from this, the augmentation step is the same as in vector RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generatio**n*—The augmented prompt is sent to the LLM like in the standard
    vector RAG approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the final generation step and initial data loading and chunking do not
    require any special adjustment, the rest of the process changes significantly.
    Table 8.3 summarizes the differences between vector and graph RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.3 Differences between vector RAG and graph RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Step | Vector RAG | Graph RAG |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data loading | Loads documents without specialized preprocessing for relationships
    | Similar to vector RAG; documents are loaded without special graph handling.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data chunking | Divides large documents into smaller chunks for embedding
    and vector storage | Documents are chunked similarly; each chunk is then processed
    to extract entities and relationships, building a relational structure. |'
  prefs: []
  type: TYPE_TB
- en: '| Entity and relationship extraction | Not applicable; focuses on creating
    embeddings from chunks | Entities, relationships, and attributes are extracted
    from each chunk using an LLM, potentially in multiple passes to refine and de-duplicate
    entities and relationships. |'
  prefs: []
  type: TYPE_TB
- en: '| Storage | Stores embeddings in a vector database | Entities and relationships
    are stored in a graph database (e.g., Neo4j), with the option to update the graph
    iteratively. Tools such as LangChain’s Neo4jGraph can automate this process. |'
  prefs: []
  type: TYPE_TB
- en: '| Community summaries | Not applicable; primarily relies on similarity search
    on individual embeddings | Detects communities within the knowledge graph and
    uses an LLM to create summaries for each community. These summaries can be stored
    as vectors for a hybrid graph–vector RAG approach. |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval | Performs direct similarity searches on embeddings | Involves
    graph traversal using Cypher queries, generated either from pre-defined templates
    or dynamically by an LLM. Neo4j’s vector indexes can enhance similarity-based
    node searches. |'
  prefs: []
  type: TYPE_TB
- en: '| Augmentation | Uses retrieved embeddings to augment the user’s query | Retrieved
    nodes, relationships, or summaries augment the user’s query. Additional LLM processing
    might be used to refine responses based on the retrieved graph content. |'
  prefs: []
  type: TYPE_TB
- en: '| Generation | Sends the augmented prompt to an LLM for response generation
    | Like vector RAG but relies on augmented data with graph-derived insights, relationships,
    and context from the knowledge graph to enrich the response. |'
  prefs: []
  type: TYPE_TB
- en: 8.3.5 Challenges and best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite all the benefits of graph RAG, there are certain challenges that must
    be considered carefully:'
  prefs: []
  type: TYPE_NORMAL
- en: Merging diverse data sources into a cohesive knowledge graph can be intricate
    and time-consuming. Start with a focused domain and gradually expand the knowledge
    graph to manage complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the iterative LLM processing at different stages, large-scale knowledge
    graph generation and community summarization from documents are computationally
    expensive. Therefore, the data for graph RAG must be selected carefully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current similarity measurement techniques may not fully capture the nuanced
    relationships or structural dependencies in graphs, leading to potential mismatches
    in retrieved information. Careful use of case-specific evaluation is warranted
    for acceptable accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each deployment may need custom graph data construction, indexing, and retrieval
    adaptations, which makes generalization difficult. Keeping the knowledge graph
    updated with accurate and current information requires continuous effort. Consequently,
    graph RAG may not be the default RAG strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we have looked at two RAG variants that extend standard RAG capabilities
    by including multimodal data and graph structures. Next, we discuss one of the
    most significant concepts in the field of generative AI: agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Agentic RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By now, you understand that challenges exist with standard RAG systems. They
    may struggle with reasoning, answering complex questions, and multistep processes.
    One of the key aspects of comprehensive RAG systems is the ability to search through
    multiple sources of data. This can be internal company documents, the open internet,
    third-party applications, and even structured data sources like an SQL database.
    So far in this book, we have built systems that can search through a single knowledge
    base, and for any query, the entire knowledge base is searched through.
  prefs: []
  type: TYPE_NORMAL
- en: Two challenges arise with this approach. First, all information must be indexed
    and stored in a single vector store, which leads to storage problems at scale.
    Second, for any query, the entire knowledge base needs to be searched, which is
    highly inefficient for large knowledge bases. To overcome this challenge, a module
    that can understand the user’s query and route the query to a relevant source
    is needed. This is one of the limitations addressed by agentic RAG that uses one
    or more LLM agents for decision-making. Let’s first understand what is meant by
    the term *agent*.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 LLM agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of agents in AI predates the popularity of LLMs. The overarching meaning
    of an AI agent is a software system that can autonomously perceive the environment
    it is in, make decisions, and perform actions to achieve a goal. Traditionally,
    AI agents have been developed to execute specific tasks and rely on predefined
    rules or learned behaviors, like in the fields of autonomous vehicles or robotics.
    Due to the ability to process and understand language (and now even multimodal
    data), LLMs are now being seen as a general-purpose technology that can help build
    autonomous decision-making without explicitly defining rules or environment data.
    While there is no common definition of an LLM-based AI agent, there are four key
    components of the system that enable autonomous decision-making and task execution.
  prefs: []
  type: TYPE_NORMAL
- en: The*core LLM brain* is an LLM that assigned a certain role and a task. This
    component is responsible for understanding the user request and interacting with
    other components to respond to the user. For example, an AI agent built for travel
    assistance may have to deal with different types of tasks such as searching for
    information, creating itineraries, booking tickets, or managing previous bookings.
  prefs: []
  type: TYPE_NORMAL
- en: The*memory* component manages the agent’s past experiences. It can be short-term
    like the chat history of the current conversation or long-term where important
    pieces of information from previous interactions are stored. For a travel assistant
    AI agent, short-term memory will hold the current context of the user query, while
    the ticket booking history or previous travel searches can be fetched from long-term
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: The*planning* component creates a step-by-step sequence of tasks that will be
    followed to respond to the user’s request. Task decomposition or breaking down
    complex tasks into smaller, manageable subtasks. ReAct, which stands for reasoning
    and acting, or reflection, where the agent does a self-assessment of the outcomes,
    can be part of the planning component.
  prefs: []
  type: TYPE_NORMAL
- en: '*Tools* assist the agent in performing actions on resources external to it.
    This can be conducting a web search on the internet, querying an external database
    such as an SQL database, invoking a third-party API such as a weather API, and
    similar. The core LLM brain is responsible for sending the payload request to
    the tools in the accepted format. These four components and their interactions
    are shown in figure 8.11\.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F11_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11  An LLM agent’s four components break down the user’s query, recall
    the history of interaction with the user, and employ external tools to accomplish
    tasks and respond to the user.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since the definition of AI agents continues evolving, these components are not
    set in stone but are generally agreed upon. To help understand how these components
    interact, let’s take an example of an AI agent built for travel assistance, like
    the customer service agent of an online travel agency.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a customer asks a question like, “Is my flight on schedule?” The core
    LLM brain receives this input and understands that the user intent is to check
    a specific flight status. At this stage, the core LLM brain can invoke the planning
    module to decide the course of action required to answer queries of this intent.
    The planning module may respond with steps such as retrieving booking information
    from previous interactions (memory), querying the latest flight information from
    a database, comparing it with previous details from memory, and conveying the
    result to the user. Here, retrieving the information from the database will require
    a tool such as an API, which is a prebuilt module that the core LLM brain has
    access to. The planning module can also bring in conditional steps—for example,
    if the previous booking information cannot be retrieved from memory, the core
    LLM brain must prompt the user to provide this information. When the core LLM
    brain gets the plan from the planning module, it retrieves previous booking information,
    invokes the tool to retrieve flight information, compares the new information
    with the old information in memory, and crafts a response based on this analysis.
    This simple workflow of the agent is illustrated in figure 8.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a flight process'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH08_F12_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12  A simple task of responding to a user query on flight schedule
    responded to by an LLM agent by using the planning, memory, and tools modules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is an example of a simple task. Multiple agents can come together to solve
    tasks of a higher level of complexity, such as “Plan and book a holiday for me.”
    The field of LLM-based AI agents is quite promising, and readers are encouraged
    to read more about this evolving domain. For our discussion on agentic RAG in
    this section, we focus on a few aspects, specifically on tool usage and a little
    bit of planning. The use cases for agentic RAG span across industries, so it makes
    more sense to look at the capabilities of agentic RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2 Agentic RAG capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our introduction to agentic RAG, we highlighted the challenge in standard
    RAG using a single knowledge base. Agentic RAG infuses abilities in the RAG system
    that make the system more efficient and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Query understanding and routing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on the user query, an LLM agent can be tasked with deciding which knowledge
    base to search through. For example, assume a programming assistant that can not
    only search the codebase but also the product documentation, along with searching
    the web. Depending on the question that the developer asks, the agent can decide
    which database to query. For generic messages such as greetings, the agent can
    also decide not to invoke the retriever and send the message directly to the LLM
    for a response.
  prefs: []
  type: TYPE_NORMAL
- en: Tools usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous example, the system was also required to search the web. The
    internet cannot be stored in a knowledge base and is usually accessed through
    an API that returns search results. This search API is an example of a tool the
    agent can use. Similarly, other APIs, such as Notion or Google Drive, can be used
    to access information sources. One of the features of tools like APIs is that
    they have fixed query and response formats. The job of the agent is to process
    natural language information into the format structure and parse the response
    to use it for generation.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive retrieval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall adaptive retrieval discussed in chapter 6\. An LLM is enabled to determine
    the most appropriate moment and content for retrieval. This is an extension of
    query routing, where after deciding the most appropriate source to query, an agent
    can also determine whether the retrieved information is good enough to generate
    responses or whether another iteration of retrieval is required. For the next
    iteration, the agent can also form fresh queries based on the retrieved context.
    This enables the RAG system to solve complex queries.
  prefs: []
  type: TYPE_NORMAL
- en: These capabilities enable agentic RAG systems to be comprehensive and work on
    a scale. While the indexing and generation pipelines do not change in structure,
    agents can be invoked throughout the two pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.3 Agentic RAG pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The capability of LLM-based agents to understand the context and invoke tools
    can be used to elevate each stage of the RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The idea of the knowledge base in agentic RAG is no different from standard
    RAG. Agents can be used across components to enhance the indexing pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data loadin**g*—Loading data and extracting information is the first and incredibly
    crucial step of RAG system development. Accurate parsing of information is critical
    in building an accurate RAG system. Parsing complex documents such as PDF reports
    can be tough. While there are libraries and tools present for these tasks, LLM
    agents can be used for high-precision parsing. The importance of metadata in RAG
    cannot be overstated. It is useful for filtering, more contextual mapping, and
    source citation. In most scenarios, it is difficult to source rich metadata. LLM
    agents can be used to build metadata architecture and extract contextual metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunkin**g*—In agentic chunking, chunks from the text are created based on
    a goal or a task. Consider an e-commerce platform wanting to analyze customer
    reviews. The best way for the reviews to be chunked is if the reviews about a
    particular topic are put in the same chunk. Similarly, the critical and positive
    reviews may be put in different chunks. To achieve this kind of chunking, we will
    need to do sentiment analysis, entity extraction, and some kind of clustering.
    This can be achieved by a multiagent system. Agentic chunking is still an active
    area of research and improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Embedding**s*—The role of agents in embeddings can be the selection of the
    right embeddings model, depending on the context of the chunks. For example, if
    there is information from multiple domains in the loaded data, there may be a
    case for using domain-specific embeddings for different chunks. Apart from this,
    quality control agents can validate embeddings by measuring similarity or alignment
    with predefined standards or use case requirements. You may also recall from the
    discussion on graph RAG that agents can also decide to use graph structures for
    certain chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storag**e*—There is also a possibility to store chunk embeddings from the
    same document in different collections owing to the nature of the information.
    For example, the information related to the installation and troubleshooting of
    a product can be stored in one collection of a vector database, and product features
    and advantages can be stored in another. This helps in setting the retrieval up
    for higher precision. You may notice that the use of agents in chunking, embeddings,
    and storage are closely related.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 8.13 summarizes how the use of agents can embellish the indexing pipeline.
    The nature of the knowledge base itself doesn’t change, but the process of creation
    is embellished with agents.
  prefs: []
  type: TYPE_NORMAL
- en: Generation Pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The true advantage of an agentic system lies in how it transforms the entire
    generation pipeline across all three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieval*—Perhaps the most significant use of agents is in the retrieval
    stage. Query routing to the most appropriate source and the integration of tools
    to query external sources of information is a crucial feature of agentic RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F13_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13  Agentic embellishment to the indexing pipeline enhances the quality
    of the knowledge base.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Adaptive retrieval strategies also bring significant improvement in the retrieval
    stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Augmentation*—Agents can choose the correct prompting technique for augmentation,
    depending on the nature of the query and the retrieved context. Prompts can also
    be generated dynamically by an agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generation*—One of the uses of agentic RAG is also in multistep generation
    such as IterRetGen or iterative-retrieval generation. In this approach, an agent
    is used to review the response generated by the LLM in the first pass, and it
    decides if any further iteration of retrieval and generation is required to completely
    respond to the user query. This is particularly useful in multi-hop reasoning
    and fact verification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to think about agentic RAG is that wherever dynamic decision-making
    can improve the RAG system, an agent can be used to autonomously make those decisions.
    From the previous discussion, you may conclude that agentic RAG is a superior
    version of standard RAG. Table 8.4 summarizes the advantages of agentic over standard
    RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.4 Advantages of agentic RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Aspect | Standard RAG | Agentic RAG |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval process | Passive retrieval based on initial query | Adaptive retrieval
    with intelligent agents routing and reformulating queries as needed |'
  prefs: []
  type: TYPE_TB
- en: '| Handling complex queries | Struggles with multistep reasoning and complex
    queries | Can be used to break down and address complex, multifaceted queries
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tool integration | Limited integration with external tools and APIs | Seamless
    integration with various external tools and APIs for enhanced information gathering
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Challenges in scaling due to static processes | Scalable through
    modular agent-based architecture, allowing for easy expansion |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy and relevance | Dependent on initial query quality; may retrieve
    less relevant information | Higher accuracy and relevance due to agents’ ability
    to refine queries and validate information |'
  prefs: []
  type: TYPE_TB
- en: 8.4.4 Challenges and pest practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLM based agents are still evolving and are not foolproof. There are also concerns
    around the planning and reasoning abilities of LLMs. For implementing agentic
    abilities into the RAG pipelines, a few aspects should be evaluated carefully:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of tool selection diminishes when a single agent is responsible
    for invoking a high number of tools. Therefore, the number of decision choices
    for the agent needs to be controlled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No agent can be expected to be accurate all the time. Error rates in multiagent
    systems can also increase. It is important to establish a failsafe at every stage.
    The choice of the use case should also be guided by the expected accuracy levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased autonomy in decision-making can lead to unintended actions if not
    properly controlled. In other words, agents can misfire, and establishing explicit
    boundaries and guidelines for agent behavior is critical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal, graph, and agentic RAG patterns have demonstrated significant improvements
    over the standard RAG pipelines. Multimodal RAG opens the RAG systems to different
    modalities, graph RAG introduces relational understanding, and agentic RAG infuses
    RAG systems with intelligence and autonomous decision making. Apart from these
    three, ongoing research on RAG has resulted in several other frameworks and variations
    to the standard RAG systems. The next section discusses variants that show significant
    promise.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Other RAG variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have talked about the three major RAG variants in this chapter. Research
    in the field is bustling, and every week, several papers are released by researchers
    about their experiments and key findings. Out of these papers, quite a few demonstrate
    RAG variants that find relevance in practical applications. We close this chapter
    by briefly discussing four such RAG variants.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1 Corrective RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The effectiveness of a RAG system depends on the quality of retrieval. Inaccuracies
    in retrieval negate all RAG benefits. To address this, the corrective RAG (CRAG)
    approach evaluates the quality of retrieved documents. It uses a lightweight evaluator
    and triggers corrective action if the retrieved information is found to be inaccurate.
    The key CRAG components are
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieval evaluato**r*—A model that evaluates the relevance of the retrieved
    documents and assigns a relevance score to each retrieved document. In the original
    CRAG paper ([https://arxiv.org/abs/2401.15884](https://arxiv.org/abs/2401.15884)),
    the evaluator is a fine-tuned T5 model that assigns a score of being correct,
    incorrect, or ambiguous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Web search supplementatio**n*—If a retrieved document is classified as incorrect,
    the system conducts a web search to supplement the knowledge base, ensuring more
    accurate, up-to-date information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Knowledge refinemen**t*—Retrieved documents classified as correct by the evaluator
    and the content retrieved from web search are broken down further into smaller
    knowledge strips, and each strip undergoes evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 8.14 illustrates the CRAG workflow with the evaluator, knowledge refinement,
    and web search added to the standard RAG flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for its advantages and limitations, CRAG secures accurate, context-relevant
    knowledge for generation, particularly in cases where initial retrieval may be
    flawed. The corrective actions enhance the factual accuracy of the generated content.
    CRAG is a solution that can be integrated with all RAG pipelines and other RAG
    variants without causing any disruptions. There are also a couple of factors that
    need to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: The additional corrective actions and web search integration may increase response
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance of the system is closely tied to the accuracy of the evaluator
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRAG is an improvement over standard RAG, which uses the retrieved documents
    as is. The corrective approach makes it effective for accuracy-sensitive applications
    that demand data verification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH08_F14_Kimothi.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14  CRAG corrects the knowledge at the most granular level, hence
    the name corrective RAG. Source: [https://arxiv.org/abs/2401.15884](https://arxiv.org/abs/2401.15884).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.5.2 Speculative RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Latency and redundancy are ubiquitous concerns in RAG systems. Speculative RAG
    addresses these in a two-step approach. First, small language models parallelly
    generate multiple answer drafts, each based on diverse subsets of documents. Then,
    a larger LLM verifies and selects the most accurate draft. The key components
    of speculative RAG are
  prefs: []
  type: TYPE_NORMAL
- en: '*Document clusterin**g*—Retrieved documents are clustered into topic-related
    groups, each offering a unique perspective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RAG drafte**r*—A smaller LLM produces initial answer drafts based on each
    cluster subset, generating responses and rationales in parallel for efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RAG verifie**r*—A larger LLM evaluates each draft’s accuracy and coherence,
    assigning confidence scores based on self-consistency and rationale support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key advantage of speculative RAG is faster response generation by reducing
    the workload on the generator LLM and performing parallel draft generation. However,
    some of the following limitations require careful consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: Involves managing a two-model setup and document clustering, which may increase
    initial setup complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document clustering directly affects draft diversity, and poor clustering can
    lead to redundant drafts by grouping highly similar or repetitive documents into
    multiple clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The smaller LLM may require training for effective draft and rationale generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike standard RAG, which incorporates all retrieved data into a single prompt,
    speculative RAG uses parallel draft generation for efficiency and a dedicated
    verification step for accuracy, which leads to a reduction in latency, while improving
    the factual efficiency of the responses.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.3 Self-reflective (self RAG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-reflection in an LLM is the ability of the LLM to analyze its actions,
    identify potential errors or flaws in its reasoning process, and then use that
    feedback to improve its responses and decision-making. Self RAG incorporates reflection
    to dynamically decide whether to retrieve relevant information, evaluate retrieved
    content, and to critique its output. The key components of self RAG are
  prefs: []
  type: TYPE_NORMAL
- en: '*Reflection token**s*—Self RAG trains an LLM to use “reflection tokens,” which
    help it assess the relevance, support, and usefulness of retrieved passages. These
    tokens are designed to guide the model in judging the quality of both the retrieved
    content and its generated response, adding layers of control and adaptability.
    A *retrieve token* indicates whether retrieval is needed. Similarly, the *relevance
    token* determines whether a passage is relevant, the *support token* verifies
    whether the generated response is fully supported by retrieved content, and the
    *utility token* scores the usefulness of the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dynamic retrieval decisio**n*—The model uses reflection tokens to determine
    if retrieval is necessary based on each segment of the response and skips retrieval
    if it is unnecessary at any step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-critiqu**e*—The model critiques its output at each generation step, applying
    reflection tokens to guide retrieval and refine the response in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adaptive retrieval in self RAG reduces unnecessary retrievals, and self-reflection
    results in better accuracy, factual consistency, and relevance. However, some
    limitations need to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing multiple passages in parallel and self-reflection may increase computational
    demands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The additional training and use of reflection tokens require fine-tuning of
    thresholds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self RAG is one of the most cited techniques in research on RAG. Its dynamic
    adjustment of retrieval based on task needs evaluates output quality, achieving
    superior accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.4 RAPTOR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recursive abstractive processing for tree-organized retrieval, or RAPTOR, is
    a RAG variant designed to handle hierarchical relationships in data. It creates
    a multilevel, tree-based structure of recursive summaries, capturing both granular
    details and overarching themes in long documents. Like graph RAG, RAPTOR uses
    a tree structure to achieve similar objectives. Here are the key RAPTOR components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk clustering and summarizatio**n*—Chunk embeddings are clustered based
    on similarity, and an LLM is used to summarize the clusters. Soft clustering with
    Gaussian mixture models allows text segments to belong to multiple clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive tree constructio**n*—RAPTOR builds a multilayered tree by using
    chunks, clusters, and summaries in a bottom-up process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dual querying mechanism**s*—A top-down approach starts traversing down to
    select the most relevant nodes at each level based on cosine similarity to the
    query. Another single-layer search retrieves context across all tree nodes irrespective
    of the levels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Like graph RAG, RAPTOR enables better multi-hop reasoning and thematic question
    answering by incorporating both granular and high-level summaries. However, tree
    structures are complex to manage and RAPTOR comes with its set of challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: The recursive clustering and summarization steps can be computationally intensive,
    especially for very large documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective retrieval hinges on the quality of the clustering; errors in initial
    clustering can propagate up the tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike standard RAG, which may struggle with multilayered content, RAPTOR’s
    hierarchical model allows targeted retrieval, optimizing for both specificity
    and contextual relevance.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explored RAG variants that use advanced techniques to improve RAG
    systems for specific use cases. Multimodal pipelines give RAG systems access to
    previously unusable data, graph RAG provides the ability of relational analysis,
    and agentic RAG introduces autonomous decision-making for complex tasks. Each
    RAG variant addresses a certain aspect of improvement in standard RAG systems.
    Corrective RAG focuses on factual relevance, RAPTOR builds relational intelligence
    for hierarchical data, speculative RAG is built for efficiency, and self RAG makes
    the system adaptive.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we are almost at the end of our discussion on RAG. The last
    chapter discusses some of the independent considerations and best practices across
    different stages of RAG system lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing RAG variants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAG variants are adaptations of the naïve RAG framework that extend its functionality
    to specific use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These variants address challenges, such as processing nontextual data, improving
    relational understanding, enhancing accuracy, and enabling autonomous decision-making.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three major RAG variants were discussed in depth: multimodal, graph, and agentic
    RAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other promising RAG variants are corrective RAG, speculative RAG, self RAG,
    and RAPTOR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal rag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It extends RAG capabilities to handle multiple data modalities such as text,
    images, audio, and video. It can be used for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Medical diagnosi**s*—Analyzing text, images (X-rays), and tabular data (lab
    results)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Investment analysi**s*—Processing financial documents, charts, and balance
    sheets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Equipment maintenanc**e*—Combining text reports, visual inspections, and sensor
    data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As for the pipeline enhancements, multimodal RAG introduces multimodal embeddings
    (shared or modality specific), transcription tools, and specialized chunking methods
    to indexing pipeline. In the generation pipeline, it employs multimodal LLMs (e.g.,
    GPT-4o, Google Gemini).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal RAG has high computational requirements and increased latency. Information
    loss is possible during text conversion of nontext modalities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graph RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It enhances retrieval and reasoning through relationships represented in a graph
    structure. It can be used for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Personalized treatment plan**s*—Linking drugs, conditions, and symptoms for
    customized recommendations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contract analysi**s*—Identifying dependencies and compliance risks across
    interconnected legal documents'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As for the pipeline enhancements, the knowledge graph RAG extracts entities,
    relationships, and attributes from chunks to create a graph in the indexing pipeline.
    As for the generation pipeline, it incorporates graph traversal using graph query
    languages such as Cypher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and maintaining knowledge graphs is complex and computationally expensive.
    It also requires custom adaptations for each deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agentic RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It introduces LLM-based agents for autonomous decision-making and dynamic query
    routing. Agentic RAG can be used for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query understanding and routing to relevant data sources
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive retrieval and multistep generation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with tools such as web search APIs and external databases
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With regard to pipeline enhancements, agentic RAG enhances chunking, metadata
    extraction, and embeddings selection with agentic decision-making in the indexing
    pipeline. In the generation pipeline, it dynamically augments prompts and employs
    iterative retrieval-generation workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agentic RAG requires robust controls to prevent unintended actions by agents.
    High computational overhead and multiplied error rates in multiagent systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other RAG variants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Corrective RAG (CRAG) Focuses on factual accuracy by evaluating retrieved content.
    It also adds corrective steps such as web search supplementation and knowledge
    refinement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advantage**s*—Enhances accuracy and can integrate seamlessly with other RAG
    pipelines'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Challenge**s*—Increased response time and dependency on the evaluator model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Speculative RAG reduces latency by generating multiple drafts in parallel using
    smaller LLMs. A larger LLM verifies and selects the most accurate draft.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advantage**s*—Faster response generation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Challenge**s*—Requires careful document clustering and draft diversity'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Self RAG incorporates reflection tokens for adaptive retrieval and self-assessment
    of generated content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advantage**s*—Superior accuracy and factual consistency'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Challenge**s*—Computationally demanding and requires fine-tuned thresholds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RAPTOR builds hierarchical relationships through tree-structured summaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advantage**s*—Optimized for multi-hop reasoning and thematic queries'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Challenge**s*—Computationally intensive and relies on effective clustering'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
