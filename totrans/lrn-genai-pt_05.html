<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">4 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/>Image generation with generative adversarial networks</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-69"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Designing a generator by mirroring steps in the discriminator network</li>
<li class="co-summary-bullet">How a 2D convolutional operation works on an image</li>
<li class="co-summary-bullet">How a 2D transposed convolutional operation inserts gaps between the output values and generates feature maps of a higher resolution</li>
<li class="co-summary-bullet">Building and training generative adversarial networks to generate grayscale and color images</li>
</ul>
<p class="body">You have successfully generated an exponential growth curve and a sequence of integers that are all multiples of 5 in chapter 3. Now that you understand how generative adversarial networks (GANs) work, you are ready to apply the same skills to generate many other forms of content, such as high-resolution color images and realistic-sounding music. However, this may be easier said than done (you know what they say: the devil is in the details). For example, exactly how can we make the generator conjure up realistic images out of thin air? That’s the question we’re going to tackle in this chapter.<a id="idIndexMarker002"/></p>
<p class="body">A common approach for the generator to create images from scratch is to mirror steps in the discriminator network. In the first project in this chapter, your goal is to create grayscale images of clothing items such as coats, shirts, sandals, and so on. You learn to mirror the layers in the discriminator network when designing a generator network. In this project, only dense layers are used in both the generator and discriminator networks. Each neuron in a dense layer is connected to every neuron in the previous and next layer. For this reason, dense layers are also called fully connected layers.</p>
<p class="body">In the second project in this chapter, your goal is to create high-resolution color images of anime faces. Like in the first project, the generator mirrors the steps in the discriminator network to conjure up images. However, high-resolution color images in this project contain many more pixels than the low-resolution grayscale images in the first project. If we use dense layers only, the number of parameters in the model increases enormously. This, in turn, makes learning slow and ineffective. We, therefore, turn to convolutional neural networks (CNNs). In CNNs, each neuron in a layer is connected only to a small region of the input. This local connectivity reduces the number of parameters, making the network more efficient. CNNs require fewer parameters than fully connected networks of similar size, leading to faster training times and lower computational costs. CNNs are also generally more effective at capturing spatial hierarchies in image data because they treat images as multidimensional objects instead of 1D vectors.<a id="idIndexMarker003"/><a id="marker-70"/></p>
<p class="body">To prepare you for the second project, we’ll show you how convolutional operations work and how they downsample the input images and extract spatial features in them. You’ll also learn concepts such as filter size, stride, and zero-padding and how they affect the degree of downsampling in CNNs. While the discriminator network uses convolutional layers, the generator mirrors these layers by using transposed convolutional layers (also known as deconvolution or upsampling layers). You’ll learn how transposed convolutional layers are used for upsampling to generate high-resolution feature maps.</p>
<p class="body">To summarize, you’ll learn how to mirror the steps in the discriminator network to create images from scratch in this chapter. In addition, you’ll learn how convolutional layers and transposed convolutional layers work. After this chapter, you’ll use convolutional layers and transposed convolutional layers to create high-resolution images in other settings later in this book (such as in feature transfers when training a CycleGAN to convert blond hair to black hair or i<a id="idTextAnchor005"/>n a variational autoencoder [VAE] to generate high-resolution human face images).<a id="idIndexMarker004"/></p>
<h2 class="fm-head" id="heading_id_3">4.1 GANs to generate grayscale images of clothing items</h2>
<p class="body">Our goal in the first project is to train a model to generate grayscale images of clothing items such as sandals, t-shirts, coats, and bags. <a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>
<p class="body">When you use GANs to generate images, you’ll always start by obtaining training data. You’ll then create a discriminator network from scratch. You’ll mirror steps in the discriminator network when creating a generator network. Finally, you’ll train the GANs and use the trained model for image generation. Let’s see how that works with a simple project that creates grayscale images of clothing items.</p>
<h3 class="fm-head1" id="heading_id_4">4.1.1 Training samples and the discriminator</h3>
<p class="body">The steps involved with preparing the training data are similar to what we have done in chapter 2, with a few exceptions that I’ll highlight later. To save time, I’ll skip the steps you have seen before in chapter 2 and refer you to the book’s GitHub repository. Follow the steps <a id="idTextAnchor006"/>in the Jupyter Notebook for this chapter in the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>) so that you create a data iterator with batches. <a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="marker-71"/></p>
<p class="body">There are 60,000 images in the training set. In chapter 2, we split the training set further into a train set and a validation set. We used the loss in the validation set to determine whether the parameters had converged so that we could stop training. However, GANs are trained using a different approach compared to traditional supervised learning models (such as the classification models you have seen in chapter 2). Since the quality of the generated samples improves throughout training, the discriminator’s task becomes more and more difficult. The loss from the discriminator network is not a good indicator of the quality of the model. The usual way of measuring the performance of GANs is through visual inspection to assess the quality and realism of generated images. We can potentially compare the quality of generated samples with training samples and use methods such as the Inception Score to evaluate the performance of GANs (See, for example, “Pros and Cons of GAN Evaluation Measures,” by Ali Borji, 2018, for a survey on various GAN evaluation methods; <a class="url" href="https://arxiv.org/abs/1802.03446">https://arxiv.org/abs/1802.03446</a>). However, researchers have documented the weaknesses of these measures (“A Note on the Inception Score,” by Shane Barratt and Rishi Sharma, 2018, demonstrates that the inception score fails to provide useful guidance when comparing models; <a class="url" href="https://arxiv.org/abs/1801.01973">https://arxiv.org/abs/1801.01973</a>). In this chapter, we’ll use visual inspections to check the quality of generated samples periodically and determine when to stop training.</p>
<p class="body">The discriminator network is a binary classifier, which is similar to the binary classifier for clothing items we discussed in chapter 2. Here the discriminator’s job is to classify the samples into either real or fake.</p>
<p class="body">We use PyTorch to create the following discriminator neural network D:</p>
<pre class="programlisting">import torch
import torch.nn as nn
  
device="cuda" if torch.cuda.is_available() else "cpu"
D=nn.Sequential(
    nn.Linear(784, 1024),          <span class="fm-combinumeral">①</span>
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(1024, 512),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(256, 1),    
    nn.Sigmoid()).to(device)       <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The first fully connected layer has 784 inputs and 1,024 outputs.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The last fully connected layer has 256 inputs and 1 output.</p>
<p class="body">The input size is 784 because each grayscale image has a size of 28 <span class="cambria">×</span> 28 pixels in the training set. Because dense layers take only 1D inputs, we flatten the images before feeding them to the model. The output layer has just one neuron in it: the output of the discriminator D is a single value. We use the sigmoid activation function to squeeze the output to the range [0, 1] so that it can be interpreted as the probability, p, that the sample is real. With complementary proba<a id="idTextAnchor007"/>bility 1 – p, the sample is fake.<a id="idIndexMarker009"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 4.1</p>
<p class="fm-sidebar-text">Modify the discriminator D so that the numbers of outputs in the first three layers are 1,000, 500, and 200 instead of 1,024, 512, and 256. Make sure the number of outputs in a layer matches the number of inputs in the next layer. <a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
</div>
<h3 class="fm-head1" id="heading_id_5">4.1.2 A generator to create grayscale images</h3>
<p class="body"><a id="marker-72"/>While the discriminator network is fairly easy to create, how to create a generator so that it can conjure up realistic images is a different matter. A common approach is to mirror the layers used in the discriminator network to create a generator, as shown in the following listing.<a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>
<p class="fm-code-listing-caption">Listing 4.1 Designing a generator by mirroring layers in the discriminator</p>
<pre class="programlisting">G=nn.Sequential(
    nn.Linear(100, 256),         <span class="fm-combinumeral">①</span>
    nn.ReLU(),
    nn.Linear(256, 512),         <span class="fm-combinumeral">②</span>
    nn.ReLU(),
    nn.Linear(512, 1024),        <span class="fm-combinumeral">③</span>
    nn.ReLU(),
    nn.Linear(1024, 784),        <span class="fm-combinumeral">④</span>
    nn.Tanh()).to(device)        <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The first layer in the generator is symmetric to the last layer in the discriminator.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The second layer in the generator is symmetric to the second to last layer in the discriminator (numbers of inputs and outputs have switched positions).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The third layer in the generator is symmetric to the third to last layer in the discriminator.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The last layer in the generator is symmetric to the first layer in the discriminator.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Uses Tanh() activation so the output is between –1 and 1, the same as values in images</p>
<p class="body">Figure 4.1 provides a diagram of the architecture of generator and discriminator networks in the GAN to generate grayscale images of clothing items. As shown in the top right corner of figure 4.1, a flattened grayscale image from the training set, which contains 28 <span class="cambria">×</span> 28 = 784 pixels, goes through four dense layers sequentially in the discriminator network, and the output is the probability that the image is real. To create an image, the generator uses the same four dense layers but in reverse order: it obtains a 100-value random noise vector from the latent space (bottom left in figure 4.1) and feeds the vector through the four dense layers. In each layer, the numbers of <i class="fm-italics">inputs</i> and <i class="fm-italics">outputs</i> in the discriminator are reversed and used as the numbers of <i class="fm-italics">outputs</i> and <i class="fm-italics">inputs</i> in the generator. Finally, the generator comes up with a 784-value tensor, which can be reshaped into a 28 <span class="cambria">×</span> 28 grayscale image (top left). <a id="idIndexMarker015"/><a id="idIndexMarker016"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="441" src="../../OEBPS/Images/CH04_F01_Liu.png" width="439"/></p>
<p class="figurecaption">Figure 4.1 Designing a generator network to create clothing items by mirroring the layers in the discriminator network. The right side of the diagram shows the discriminator network, which contains four dense layers. To design a generator that can conjure up clothing items from thin air, we mirror the layers in the discriminator network. Specifically, as shown on the left half of the figure, the generator has four similar dense layers in it but in reverse order: the first layer in the generator mirrors the last layer in the discriminator, the second layer in the generator mirrors the second to last layer in the discriminator, and so on. Further, in each of the top three layers, the numbers of inputs and outputs in the discriminator are reversed and used as the numbers of outputs and inputs in the generator.</p>
</div>
<p class="body"><a id="marker-73"/>The left side of figure 4.1 is the generator network, while the right side is the discriminator network. If you compare the two networks, you’ll notice how the generator mirrors the layers used in the discriminator. Specifically, the generator has four similar dense layers in it but in reverse order: the first layer in the generator mirrors the last layer in the discriminator, the second layer in the generator mirrors the second to last layer in the discriminator, and so on. The number of outputs of the generator is 784, with values between -1 and 1 after the <code class="fm-code-in-text">Tanh()</code> activation, and this matches the input to the discriminator network. <a id="idIndexMarker017"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 4.2</p>
<p class="fm-sidebar-text">Modify the generator G so that the numbers of outputs in the first three layers are 1,000, 500, and 200 instead of 1,024, 512, and 256. Make sure that the modified generator mirrors the layers used in the modified discriminator in exercise 4.1.</p>
</div>
<p class="body">As in GAN models we have seen in chapter 3, the loss function is the binary cross-entropy loss since the discriminator D is performing a binary classification problem. We’ll use the Adam optimizer for both the discriminator and the generator, with a learning rate of 0.0001:</p>
<pre class="programlisting">loss_fn=nn.BCELoss()
lr=0.0001
optimD=torch.optim.Adam(D.parameters(),lr=lr)
optimG=torch.optim.Adam(G.parameters(),lr=lr)  </pre>
<p class="body">Next, we’ll train the GANs we just created by using the clothing item images in the training dataset. <a id="idIndexMarker018"/><a id="idIndexMarker019"/><a id="marker-74"/><a id="idIndexMarker020"/></p>
<h3 class="fm-head1" id="heading_id_6">4.1.3 Training GANs to generate images of clothing items</h3>
<p class="body">The training process is similar to what we have done in chapter 3 when training GANs to generate an exponential growth curve or to generate a sequence of numbers that are all multiples of 5. <a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>
<p class="body">Unlike in chapter 3, we’ll solely rely on visual inspections to determine whether the model is well-trained. For that purpose, we define a <code class="fm-code-in-text">see_output()</code> function to visualize the fake images created by the generator periodically.<a id="idIndexMarker023"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Interested readers can check this GitHub repository to learn how to implement the inception score in PyTorch to evaluate GANs: <a class="url" href="https://github.com/sbarratt/inception-score-pytorch">https://github.com/sbarratt/inception-score-pytorch</a>. However, the repository doesn’t recommend using the inception score to evaluate generative models due to its ineffectiveness.</p>
<p class="fm-code-listing-caption">Listing 4.2 Defining a function to visualize the generated clothing items</p>
<pre class="programlisting">import matplotlib.pyplot as plt
  
def see_output():
    noise=torch.randn(32,100).to(device=device)
    fake_samples=G(noise).cpu().detach()                 <span class="fm-combinumeral">①</span>
    plt.figure(dpi=100,figsize=(20,10))
    for i in range(32):
        ax=plt.subplot(4, 8, i + 1)                      <span class="fm-combinumeral">②</span>
        img=(fake_samples[i]/2+0.5).reshape(28, 28)
        plt.imshow(img)                                  <span class="fm-combinumeral">③</span>
        plt.xticks([])
        plt.yticks([])
    plt.show()
    
see_output()                                             <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates 32 fake images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Plots them in a 4 <span class="cambria">×</span> 8 grid</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Shows the ith image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calls the see_output() function to visualize the generated images before training</p>
<p class="body">If you run the preceding code cell, you’ll see 32 images that look like snowflake statics on a TV screen, as shown in figure 4.2. They don’t look like clothing items at all because we haven’t trained the generator yet.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="382" src="../../OEBPS/Images/CH04_F02_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 4.2 Output from the GAN model to generate clothing items before training. Since the model is not trained, the generated images are nothing like the images in the training set.</p>
</div>
<p class="body"><a id="marker-75"/>To train the GAN model, we define a few functions: <code class="fm-code-in-text">train_D_on_real()</code>, <code class="fm-code-in-text">train_D_on_fake()</code>, and <code class="fm-code-in-text">train_G()</code>. They are similar to those defined in chapter 3. Go to the Jupypter Notebook for this chapter in the book’s GitHub repository and see what minor modifications we have made. <a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>
<p class="body">Now we are ready to train the model. We iterate through all batches in the training dataset. For each batch of data, we first train the discriminator using the real samples. After that, the generator creates a batch of fake samples, and we use them to train the discriminator again. Finally, we let the generator create a batch of fake samples again, but this time, we use them to train the generator instead. We train the model for 50 epochs, as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 4.3 Training GANs for clothing item generation</p>
<pre class="programlisting">for i in range(50):    
    gloss=0
    dloss=0
    for n, (real_samples,_) in enumerate(train_loader):
        loss_D=train_D_on_real(real_samples)            <span class="fm-combinumeral">①</span>
        dloss+=loss_D
        loss_D=train_D_on_fake()                        <span class="fm-combinumeral">②</span>
        dloss+=loss_D
        loss_G=train_G()                                <span class="fm-combinumeral">③</span>
        gloss+=loss_G
    gloss=gloss/n
    dloss=dloss/n    
    if i % 10 == 9:
        print(f"at epoch {i+1}, dloss: {dloss}, gloss {gloss}")
        see_output()                                    <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Trains the discriminator using real samples</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Trains the discriminator using fake samples</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Trains the generator</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Visualizes generated samples after every 10 epochs</p>
<p class="body">The training takes about 10 minutes if you are using GPU training. Otherwise, it may take an hour or so, depending on the hardware configuration on your computer. Or you can download the traine<a id="idTextAnchor008"/>d model from my website: <a class="url" href="https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip">https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip</a>. Unzip it after downloading.</p>
<p class="body"><a id="marker-76"/>After every 10 epochs of training, you can visualize the generated clothing items, as shown in figure 4.3. After just 10 epochs of training, the model can already generate clothing items that clearly can pass as real: you can tell what they are. The first three items in the first row in figure 4.3 are clearly a coat, a dress, and a pair of trousers, for example. As training progresses, the quality of the generated images becomes better and better.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="410" src="../../OEBPS/Images/CH04_F03_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 4.3 Clothing items generated by an image GAN model after 10 epochs of training</p>
</div>
<p class="body">As we do in all GANs, we discard the discriminator and save the trained generator to generate samples later:</p>
<pre class="programlisting">scripted = torch.jit.script(G) 
scripted.save('files/fashion_gen.pt') </pre>
<p class="body">We have now saved the generator in the local folder. To use the generator, we load up the model:</p>
<pre class="programlisting">new_G=torch.jit.load('files/fashion_gen.pt',
                     map_location=device)
new_G.eval()</pre>
<p class="body">The generator is now loaded. We can use it to generate clothing items:</p>
<pre class="programlisting">noise=torch.randn(32,100).to(device=device)
fake_samples=new_G(noise).cpu().detach()
for i in range(32):
    ax = plt.subplot(4, 8, i + 1)
    plt.imshow((fake_samples[i]/2+0.5).reshape(28, 28))
    plt.xticks([])
    plt.yticks([])
plt.subplots_adjust(hspace=-0.6)
plt.show() </pre>
<p class="body"><a id="marker-77"/>The generated clothing items are shown in figure 4.4. As you can see, the clothing items are fairly close to those in the training set.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="410" src="../../OEBPS/Images/CH04_F04_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 4.4 Clothing items generated by a trained image GAN model (after 50 epochs)</p>
</div>
<p class="body">Now that you have learned how to create grayscale images by using GANs, you’ll learn how to generate high-resolution<a id="idTextAnchor009"/> color images by using deep convolutional GAN (DCGAN) in the remaining sections of this chapter. <a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<h2 class="fm-head" id="heading_id_7">4.2 Convolutional layers</h2>
<p class="body">To create high-resolution color images, we need more sophisticated techniques than simple fully connected neural networks. Specifically, we’ll use CNNs, which are particularly effective for processing data with a grid-like topology, such as images. They are distinct from fully connected (dense) layers in a couple of ways. First, in CNNs, each neuron in a layer is connected only to a small region of the input. This is based on the understanding that in image data, local groups of pixels are more likely to be related to each other. This local connectivity reduces the number of parameters, making the network more efficient. Second, CNNs use the concept of shared weights—the same weights are used across different regions of the input. This is akin to sliding a filter across the entire input space. This filter detects specific features (e.g., edges or textures) regardless of their position in the input, leading to the property of translation invariance. <a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="marker-78"/></p>
<p class="body">Due to their structure, CNNs are more efficient for image processing. They require fewer parameters than fully connected networks of similar size, leading to faster training times and lower computational costs. They are also generally more effective at capturing spatial hierarchies in image data.</p>
<p class="body">Convolutional layers and transposed convolutional layers are two fundamental building blocks in CNNs, commonly used in image processing and computer vision tasks. They have different purposes and characteristics: convolutional layers are used for feature extraction. They apply a set of learnable filters (also known as kernels) to the input data to detect patterns and features at different spatial scales. These layers are essential for capturing hierarchical representations of the input data. In contrast, transposed convolutional layers are used for upsampling or generating high-resolution feature maps.</p>
<p class="body">In this section, you’ll learn how convolutional operations work and how kernel size, stride, and zero-padding affe<a id="idTextAnchor010"/>ct convolutional operations.</p>
<h3 class="fm-head1" id="heading_id_8">4.2.1 How do convolutional operations work?</h3>
<p class="body">Convolutional layers use filters to extract spatial patterns on the input data. A convolutional layer is capable of automatically detecting a large number of patterns and associating them with the target label. Therefore, convolutional layers are commonly used in image classification tasks. <a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/></p>
<p class="body">Convolutional operations involve applying a filter to an input image to produce a feature map. This process involves using element-wise multiplication of the filter with the input image and summing the results. The weights in the filter are the same as the filter moves on the input image to scan different areas. Figure 4.5 shows a numerical example of how convolutional operations work. The left column is the input image, and the second column is a filter (a 2 <span class="cambria">×</span> 2 matrix). Convolutional operations (the third column) involve sliding the filter over the input image, multiplying corresponding elements, and summing them up (the last column).</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="688" src="../../OEBPS/Images/CH04_F05_Liu.png" width="630"/></p>
<p class="figurecaption">Figure 4.5 A numerical example of how convolutional operations work, with stride equal to 1 and no padding</p>
</div>
<p class="body"><a id="marker-79"/>To gain a deep understanding of exactly how convolutional operations work, let’s implement the convolutional operations in PyTorch in parallel so that you can verify the numbers as shown in figure 4.5. First, let’s create a PyTorch tensor to represent the input image in the figure:</p>
<pre class="programlisting">img = torch.Tensor([[1,1,1],
                    [0,1,2],
                    [8,7,6]]).reshape(1,1,3,3)    <span class="fm-combinumeral">①</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The four values in the shape of the image, (1, 1, 3, 3), are the number of images in the batch, number of color channels, image height, and image width, respectively.</p>
<p class="body">The image is reshaped so that it has a dimension of (1, 1, 3, 3), indicating that there is just one observation in the batch, and the image has just one color channel. The height and the width of the image are both 3 pixels.</p>
<p class="body">Let’s represent the 2 <span class="cambria">×</span> 2 filter, as shown in the second column of figure 4.5, by creating a 2D convolutional layer in PyTorch:</p>
<pre class="programlisting">conv=nn.Conv2d(in_channels=1,
            out_channels=1,
            kernel_size=2, 
            stride=1)                             <span class="fm-combinumeral">①</span>
sd=conv.state_dict()                              <span class="fm-combinumeral">②</span>
print(sd)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initiates a 2D convolutional layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts the randomly initialized weights and bias in the layer</p>
<p class="body">A 2D convolutional layer takes several arguments. The <code class="fm-code-in-text">in_channels</code> argument is the number of channels in the input image. This value is 1 for grayscale images and 3 for color images since color images have three color channels (red, green, and blue [RGB]). The <code class="fm-code-in-text">out_channels</code> is the number of channels after the convolutional layer, which can take any number based on how many features you want to extract from the image. The <code class="fm-code-in-text">kernel_size</code> argument controls the size of the kernel; for example, <code class="fm-code-in-text">kernel_size=3</code> means the filter has a shape of 3 <span class="cambria">×</span> 3, and <code class="fm-code-in-text">kernel_size=4</code> means the filter has a shape of 4 <span class="cambria">×</span> 4. We set the kernel size to 2 so the filter has a shape of 2 <span class="cambria">×</span> 2. <a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="marker-80"/><a id="idIndexMarker039"/></p>
<p class="body">A 2D convolutional layer also has several optional arguments. The <code class="fm-code-in-text">stride</code> argument specifies how many pixels to move to the right or down each time the filter moves along the input image. The <code class="fm-code-in-text">stride</code> argument has a default value of 1. A higher value of stride leads to more downsampling of the image. The <code class="fm-code-in-text">padding</code> argument means how many rows of zeros to add to four sides of the input image, with a default value of 0. The <code class="fm-code-in-text">bias</code> argument indicates whether to add a learnable bias as the parameter, with a default value of <code class="fm-code-in-text">True</code>.<a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<p class="body">The preceding 2D convolutional layer has one input channel, one output channel, with a kernel size of 2 <span class="cambria">×</span> 2, and a stride of 1. When the convolutional layer is created, the weights and the bias in it are randomly initialized. You will see the following output as the weights and the bias of this convolutional layer:</p>
<pre class="programlisting">OrderedDict([('weight', tensor([[[[ 0.3823,  0.4150],
          [-0.1171,  0.4593]]]])), ('bias', tensor([-0.1096]))])</pre>
<p class="body">To make our example easier to follow, we’ll replace the weights and the bias with whole numbers:</p>
<pre class="programlisting">weights={'weight':torch.tensor([[[[1,2],
   [3,4]]]]), 'bias':torch.tensor([0])}          <span class="fm-combinumeral">①</span>
for k in sd:
    with torch.no_grad():
        sd[k].copy_(weights[k])                  <span class="fm-combinumeral">②</span>
print(conv.state_dict())                         <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Handpicks weights and bias</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Replaces the weights and bias in the convolutional layer with our handpicked numbers</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Prints out the new weights and bias in the convolutional layer</p>
<p class="body">Since we are not learning the parameters in the convolutional layer, <code class="fm-code-in-text">torch.no_grad()</code> is used to disable gradient calculation, which reduces memory consumption and speeds up computations. Now the convolutional layer has weights and the bias that we have chosen. They also match the numbers in figure 4.5. The output from the preceding code cell is:<a id="idIndexMarker043"/></p>
<pre class="programlisting">OrderedDict([('weight', tensor([[[[1., 2.],
          [3., 4.]]]])), ('bias', tensor([0.]))])</pre>
<p class="body">If we apply the preceding convolutional layer on the 3 <span class="cambria">×</span> 3 image we mentioned, what is the output? Let’s find out:</p>
<pre class="programlisting">output = conv(img)
print(output)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor([[[[ 7., 14.],
          [54., 50.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre>
<p class="body"><a id="marker-81"/>The output has a shape of (1, 1, 2, 2), with four values in it: 7, 14, 54, and 50. These numbers match those in figure 4.5.</p>
<p class="body">But how exactly does the convolutional layer generate this output through the filter? We’ll explain in detail next.</p>
<p class="body">The input image is a 3 <span class="cambria">×</span> 3 matrix, and the filter is a 2 <span class="cambria">×</span> 2 matrix. When the filter scans over the image, it first covers the four pixels in the top left corner of the image, which have values <code class="fm-code-in-text">[[1, 1], [0, 1]]</code>, as shown in the first row in Figure 4.5. The filter has values <code class="fm-code-in-text">[[1,2],[3,4]]</code>. The convolution operation finds the sum of the element-wise multiplication of the two tensors (in this case, one tensor is the filter and the other is the covered area). In other words, the convolution operation performs element-wise multiplication in each of the four cells and then adds up the values in the four cells. Therefore, the output from scanning the top left corner is</p>
<p class="fm-equation">1 <span class="cambria">×</span> 1 <span class="cambria">×</span> 1 <span class="cambria">×</span> 2 + 0 <span class="cambria">×</span> 3 + 1 <span class="cambria">×</span> 4 = 7.</p>
<p class="body">This explains why the top left corner of the output has a value of 7. Similarly, when the filter is applied to the top right corner of the image, the covered area is <code class="fm-code-in-text">[[1,1],[1,2]]</code>. The output is therefore:</p>
<p class="fm-equation">1 <span class="cambria">×</span> 1 + 1 <span class="cambria">×</span> 2 + 1 <span class="cambria">×</span> 3 + 2 <span class="cambria">×</span> 4 = 14.</p>
<p class="body">This explains why the top right corner of the output has a value of 14. <a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 4.3</p>
<p class="fm-sidebar-text">What are the values in the covered area when the filter is applied to the bottom right corner of the image? Explain why the bottom rig<a id="idTextAnchor011"/>ht corner of the output has a value of 50.</p>
</div>
<h3 class="fm-head1" id="heading_id_9">4.2.2 How do stride and padding affect convolutional operations?</h3>
<p class="body">Stride and zero padding are two important concepts in the context of convolutional operations. They play a crucial role in determining the dimensions of the output feature map and the way the filter interacts with the input data.<a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="marker-82"/></p>
<p class="body">Stride refers to the number of pixels by which the filter moves across the input image. When the stride is 1, the filter moves 1 pixel at a time. A larger stride means the filter jumps over more pixels as it slides over the image. Increasing the stride reduces the spatial dimensions of the output feature map.</p>
<p class="body">Zero padding involves adding layers of zeros around the border of the input image before applying the convolutional operation. Zero padding allows control over the spatial dimensions of the output feature map. Without padding, the dimensions of the output will be smaller than the input. By adding padding, you can preserve the dimensions of the input.</p>
<p class="body">Let’s use an example to show how stride and padding work. The following code cell redefines the 2D convolutional layer:</p>
<pre class="programlisting">conv=nn.Conv2d(in_channels=1,
            out_channels=1,
            kernel_size=2, 
            stride=2,                           <span class="fm-combinumeral">①</span>
            padding=1)                          <span class="fm-combinumeral">②</span>
sd=conv.state_dict()
for k in sd:
    with torch.no_grad():
        sd[k].copy_(weights[k])
output = conv(img)
print(output)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Changes the stride from 1 to 2</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Changes the padding from 0 to 1</p>
<p class="body">The output is</p>
<pre class="programlisting">tensor([[[[ 4.,  7.],
          [32., 50.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre>
<p class="body">The <code class="fm-code-in-text">padding=1</code> argument adds one row of 0s around the input image, so the padded image now has a size of 5 <span class="cambria">×</span> 5 instead of 3 <span class="cambria">×</span> 3.</p>
<p class="body">When the filter scans over the padded image, it first covers the top left corner, which has values <code class="fm-code-in-text">[[0, 0], [0, 1]]</code>. The filter has values <code class="fm-code-in-text">[[1,2],[3,4]]</code>. Therefore, the output from scanning the top left corner is:</p>
<p class="fm-equation">0 <span class="cambria">×</span> 1+0 <span class="cambria">×</span> 2+0 <span class="cambria">×</span> 3+1 <span class="cambria">×</span> 4=4</p>
<p class="body">This explains why the top left corner of the output has a value of 4. Similarly, when the filter slides two pixels down to the bottom left corner of the image, the covered area is <code class="fm-code-in-text">[[0,0],[0,8]]</code>. The output is therefore:</p>
<p class="fm-equation">0 <span class="cambria">×</span> 1+0 <span class="cambria">×</span> 2+0 <span class="cambria">×</span> 3+8 <span class="cambria">×</span> 4=32</p>
<p class="body">This explains why the bottom left corner of the output has a value of 32. <a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/></p>
<h2 class="fm-head" id="heading_id_10">4.3 Transposed convolution and batch normalization</h2>
<p class="body">Transposed convolutional layers are also known as deconvolution or upsampling Layers. They are used for upsampling or generating high-resolution feature maps. They are often employed in generative models like GANs and VAEs.<a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="marker-83"/><a id="idIndexMarker060"/></p>
<p class="body">Transposed convolutional layers apply a filter to the input data, but unlike standard convolution, they increase the spatial dimensions by inserting gaps between the output values, which effectively “upscales” the feature maps. This process generates feature maps of a higher resolution. Transposed convolutional layers help increase the spatial resolution, which is useful in image generation.</p>
<p class="body">Strides can be used in transposed convolution layers to control the amount of upsampling. The greater the value of the stride, the more upsampling the transposed convolution layer has on the input data.<a id="idTextAnchor012"/></p>
<p class="body">Two-dimensional batch normalization is a technique used in neural networks, particularly CNNs, to stabilize and speed up the training process. It addresses several problems, including saturation, vanishing gradients, and exploding gradients, which are common challenges in deep learning. In this section, you’ll look at some examples so you have a deeper understanding of how it works. You’ll use it when creating GANs to generate high-resolution color images in the next section.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Vanishing and exploding gradients in deep learning</p>
<p class="fm-sidebar-text">The vanishing gradient problem occurs in deep neural networks when the gradients of the loss function with respect to the network parameters become exceedingly small during backpropagation. This results in very slow updates to the parameters, hindering the learning process, especially in the early layers of the network. Conversely, the exploding gradient problem happens when these gradients become excessively large, leading to unstable updates and causing the model parameters to oscillate or diverge to very large values. Both problems impede the effective training of deep neural networks. <a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>
</div>
<h3 class="fm-head1" id="heading_id_11">4.3.1 How do transposed convolutional layers work?</h3>
<p class="body">Contrary to convolutional layers, transposed convolutional layers upsample and fill in gaps in an image to generate features and increase resolution by using kernels (i.e., filters). The output is usually larger than the input in a transposed convolutional layer. Therefore, transposed convolutional layers are essential tools when it comes to generating high-resolution images. To show you exactly how 2D transposed convolutional operations work, let’s use a simple example and a figure. Suppose you have a very small 2 <span class="cambria">×</span> 2 input image, as shown in the left column in figure 4.6.<a id="idIndexMarker063"/><a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="idIndexMarker068"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="560" src="../../OEBPS/Images/CH04_F06_Liu.png" width="694"/></p>
<p class="figurecaption">Figure 4.6 A numerical example of how transposed convolutional operations work</p>
</div>
<p class="body"><a id="marker-84"/>The input image has the following values in it:</p>
<pre class="programlisting">img = torch.Tensor([[1,0],
                    [2,3]]).reshape(1,1,2,2)</pre>
<p class="body">You want to upsample the image so that it has a higher resolution. You can create a 2D transposed convolutional layer in PyTorch:</p>
<pre class="programlisting">transconv=nn.ConvTranspose2d(in_channels=1,
            out_channels=1,
            kernel_size=2, 
            stride=2)                            <span class="fm-combinumeral">①</span>
sd=transconv.state_dict()
weights={'weight':torch.tensor([[[[2,3],
   [4,5]]]]), 'bias':torch.tensor([0])}
for k in sd:
    with torch.no_grad():
        sd[k].copy_(weights[k])                  <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> A transposed convolutional layer with one input channel, one output channel, a kernel size of 2, and a stride of 2</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Replaces the weights and bias in the transposed convolutional layer with handpicked values</p>
<p class="body">This 2D transposed convolutional layer has one input channel, one output channel, with a kernel size of 2 <span class="cambria">×</span> 2 and a stride of 2. The 2 <span class="cambria">×</span> 2 filter is shown in the second column in figure 4.6. We replaced the randomly initialized weights and the bias in the layer with our handpicked whole numbers so it’s easy to follow the calculations. The <code class="fm-code-in-text">state_dict()</code> method in the preceding code listing returns the parameters in a deep neural network.</p>
<p class="body">When the transposed convolutional layer is applied to the 2 <span class="cambria">×</span> 2 image we mentioned earlier, what is the output? Let’s find out:</p>
<pre class="programlisting">transoutput = transconv(img)
print(transoutput)</pre>
<p class="body">The output is</p>
<pre class="programlisting">tensor([[[[ 2.,  3.,  0.,  0.],
          [ 4.,  5.,  0.,  0.],
          [ 4.,  6.,  6.,  9.],
          [ 8., 10., 12., 15.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre>
<p class="body">The output has a shape of (1, 1, 4, 4), meaning we have upsampled a 2 <span class="cambria">×</span> 2 image to a 4 <span class="cambria">×</span> 4 image. How does the transposed convolutional layer generate the preceding output through the filter? We’ll explain in detail next.</p>
<p class="body">The image is a 2 <span class="cambria">×</span> 2 matrix, and the filter is also a 2 <span class="cambria">×</span> 2 matrix. When the filter is applied to the image, each element in the image multiplies with the filter and goes to the output. The top left value in the image is 1, and we multiply it with the values in the filter, <code class="fm-code-in-text">[[2, 3], [4, 5]]</code>, and this leads to the four values in the top left block of the output matrix <code class="fm-code-in-text">transoutput</code>, with values <code class="fm-code-in-text">[[2, 3], [4, 5]]</code>, as shown at the top right corner in figure 4.6. Similarly, the bottom left value in the image is 2, and we multiply it with the values in the filter, <code class="fm-code-in-text">[[2, 3], [4, 5]]</code>, and this leads to the four values in the bottom left block of the output matrix <code class="fm-code-in-text">transoutput</code>, <code class="fm-code-in-text">[[4, 6], [8, 10]]</code>. <a id="idIndexMarker069"/><a id="marker-85"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 4.4</p>
<p class="fm-sidebar-text">If an image has values <code class="fm-code-in-text1">[[10, 10], [15, 20]]</code> in it, what is the output after you apply the 2D transposed convolutional layer <code class="fm-code-in-text1">transconv</code> to the image? Assume <code class="fm-code-in-text1">transconv</code> has values <code class="fm-code-in-text1">[[2, 3], [4, 5]]</code> in it. Assume a kernel size of 2 and a stride size of 2. <a id="idIndexMarker070"/></p>
</div>
<h3 class="fm-head1" id="heading_id_12">4.3.2 Batch normalization</h3>
<p class="body">Two-dimensional batch normalization is a standard technique in modern deep learning frameworks and has become a crucial component for effectively training deep neural networks. You’ll see it quite often later in this book.</p>
<p class="body">In 2D batch normalization, normalization is performed independently for each feature channel by adjusting and scaling val<a id="idTextAnchor013"/>ues in the channel so they have a mean of 0 and a variance of 1. A feature channel refers to one of the dimensions in a multidimensional tensor in CNNs used to represent different aspects or features of the input data. For example, they can represent color channels like red, green, or blue. The normalization ensures that the distribution of the inputs to layers deep in the network remains more stable during training. This stability arises because the normalization process reduces the internal covariate shift, which is the change in the distribution of network activations due to the update of weights in lower layers. It also helps to address the vanishing or exploding gradient problems by keeping the inputs in an appropriate range to prevent gradients from becoming too small (vanishing) or too large (exploding).<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup></p>
<p class="body">Here’s how the 2D batch normalization works: for each feature channel, we first calculate the mean and variance of all observations within the channel. We then normalize the values for each feature channel using the mean and variance obtained earlier (by subtracting the mean from each observation and then dividing the difference by the standard deviation). This ensures that the values in each channel have a mean of 0 and a standard deviation of 1 after normalization, which helps stabilize and speed up training. It also helps maintain stable gradients during backpropagation, which further aids in training deep neural networks.</p>
<p class="body">Let’s use a concrete example to show how the 2D batch normalization works.</p>
<p class="body">Suppose that you have a three-channel input with a size of 64 <span class="cambria">×</span> 64. You pass the input through a 2D convolutional layer with three output channels as follows:</p>
<pre class="programlisting">torch.manual_seed(42)                          <span class="fm-combinumeral">①</span>
img = torch.rand(1,3,64,64)                    <span class="fm-combinumeral">②</span>
conv = nn.Conv2d(in_channels=3,
            out_channels=3,
            kernel_size=3, 
            stride=1,
            padding=1)                         <span class="fm-combinumeral">③</span>
out=conv(img)                                  <span class="fm-combinumeral">④</span>
print(out.shape)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Fixes the random state so results are reproducible</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a 3-channel input</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a 2D convolutional layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Passes the input through the convolutional layer</p>
<p class="body">The output from the preceding code cell is</p>
<pre class="programlisting">torch.Size([1, 3, 64, 64])</pre>
<p class="body"><a id="marker-86"/>We have created a three-channel input and passed it through a 2D convolutional layer with three output channels. The processed input has three channels with a size of <span class="times">64 <span class="cambria">×</span> 64</span> pixels.</p>
<p class="body">Let’s look at the mean and standard deviation of the pixels in each of the three output channels:</p>
<pre class="programlisting">for i in range(3):
    print(f"mean in channel {i} is", out[:,i,:,:].mean().item())
    print(f"std in channel {i} is", out[:,i,:,:].std().item())</pre>
<p class="body">The output is</p>
<pre class="programlisting">mean in channel 0 is -0.3766776919364929
std in channel 0 is 0.17841289937496185
mean in channel 1 is -0.3910464942455292
std in channel 1 is 0.16061744093894958
mean in channel 2 is 0.39275866746902466
std in channel 2 is 0.18207983672618866</pre>
<p class="body">The average values of the pixels in each output channel are not 0; the standard deviations of pixels in each output channel are not 1. Now, we perform a 2D batch normalization:</p>
<pre class="programlisting">norm=nn.BatchNorm2d(3)
out2=norm(out)
print(out2.shape)
for i in range(3):
    print(f"mean in channel {i} is", out2[:,i,:,:].mean().item())
    print(f"std in channel {i} is", out2[:,i,:,:].std().item())</pre>
<p class="body">Then we have the following output:</p>
<pre class="programlisting">torch.Size([1, 3, 64, 64])
mean in channel 0 is 6.984919309616089e-09
std in channel 0 is 0.9999650120735168
mean in channel 1 is -5.3085386753082275e-08
std in channel 1 is 0.9999282956123352
mean in channel 2 is 9.872019290924072e-08
std in channel 2 is 0.9999712705612183</pre>
<p class="body">The average values of pixels in each output channel are now practically 0 (or a very small number that is close to 0); the standard deviations of pixels in each output channel are now a number close to 1. That’s what batch normalization does: it normalizes observations in each feature channel so that values in each feature channel have 0 mean and unit standard deviation. <a id="idIndexMarker071"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="idIndexMarker075"/><a id="marker-87"/></p>
<h2 class="fm-head" id="heading_id_13">4.4 Color images of anime faces</h2>
<p class="body">In this second project, you’ll learn how to create high-resolution color images. The training steps in this project are similar to the first project, with the exception that the training data are color images of anime faces. Further, the discriminator and generator neural networks are more sophisticated. We’ll use 2D convolutional and 2D transposed convolutional layers in the two networks.<a id="idIndexMarker076"/><a id="idIndexMarker077"/></p>
<h3 class="fm-head1" id="heading_id_14">4.4.1 Downloading anime faces</h3>
<p class="body">You can download the training data from Kaggle <a class="url" href="https://mng.bz/1a9R">https://mng.bz/1a9R</a>, which contains 63,632 color images of anime faces. You need to set up a free Kaggle account to log in first. Extract the data from the zip file and put them in a folder on your computer. For example, I placed everything in the zip file in /files/anime/ on my computer. As a result, all anime face images are in /files/anime/images/. <a id="idIndexMarker078"/><a id="idIndexMarker079"/></p>
<p class="body">Define the path name so you can use it later to load the images in Pytorch:</p>
<pre class="programlisting">anime_path = r"files/anime"</pre>
<p class="body">Change the name of the path depending on where you have saved the images on your computer. Note that the <code class="fm-code-in-text">ImageFolder()</code> class uses the directory name of the images to identify the class the images belong to. As a result, the final /images/ directory is not included in <code class="fm-code-in-text">anime_path</code> that we define earlier. <a id="idIndexMarker080"/></p>
<p class="body">Next, we use the <code class="fm-code-in-text">ImageFolder()</code> class in Torchvision <code class="fm-code-in-text">datasets</code> package to load the dataset:<a id="idIndexMarker081"/><a id="idIndexMarker082"/></p>
<pre class="programlisting">from torchvision import transforms as T
from torchvision.datasets import ImageFolder
  
transform = T.Compose([T.Resize((64, 64)),               <span class="fm-combinumeral">①</span>
    T.ToTensor(),                                        <span class="fm-combinumeral">②</span>
    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])      <span class="fm-combinumeral">③</span>
train_data = ImageFolder(root=anime_path,
                         transform=transform)            <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Changes image size to 64 <span class="cambria">×</span> 64</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts images to PyTorch tensors</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Normalizes image values to [-1, 1] in all three color channels</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Loads the data and transforms images</p>
<p class="body">We perform three different transformations when loading up the images from the local folder. First, we resize all images to 64 pixels in height and 64 pixels in width. Second, we convert the images to PyTorch tensors with values in the range [0, 1] by using the <code class="fm-code-in-text">ToTensor()</code> class. Finally, we use the <code class="fm-code-in-text">Normalize()</code> class to deduct 0.5 from the value and divide the difference by 0.5. As a result, the image data are now between –1 and 1.<a id="idIndexMarker083"/><a id="idIndexMarker084"/></p>
<p class="body">We can now put the training data in batches:</p>
<pre class="programlisting">from torch.utils.data import DataLoader
   
batch_size = 128
train_loader = DataLoader(dataset=train_data, 
               batch_size=batch_size, shuffle=True)</pre>
<p class="body">The training dataset is now in batches, with a batch size of 128.</p>
<h3 class="fm-head1" id="heading_id_15">4.4.2 Channels-first color images in PyTorch</h3>
<p class="body"><a id="marker-88"/>PyTorch uses a so-called channels-first approach when handling color images. This means the shape of images in PyTorch are (number_channels, height, width). In contrast, in other Python libraries such as TensorFlow or Matplotlib, a channels-last approach is used: a color image has a shape of (height, width, number_channels) instead. <a id="idIndexMarker085"/><a id="idIndexMarker086"/></p>
<p class="body">Let’s look at an example image in our dataset and print out the shape of the image:</p>
<pre class="programlisting">image0, _ = train_data[0]
print(image0.shape)</pre>
<p class="body">The output is</p>
<pre class="programlisting">torch.Size([3, 64, 64])</pre>
<p class="body">The shape of the first image is 3 <span class="cambria">×</span> 64 <span class="cambria">×</span> 64. This means the image has three color channels (RGB). The height and width of the image are both 64 pixels.</p>
<p class="body">When we plot the images in Matplotlib, we need to convert them to channels-last by using the <code class="fm-code-in-text">permute()</code> method in PyTorch:<a id="idIndexMarker087"/></p>
<pre class="programlisting">import matplotlib.pyplot as plt
   
plt.imshow(image0.permute(1,2,0)*0.5+0.5)
plt.show()</pre>
<p class="body">Note that we need to multiply the PyTorch tensor representing the image by 0.5 and then add 0.5 to it to convert the values from the range [–1, 1] to the range [0, 1]. You’ll see a plot of an anime face after running the preceding code cell.</p>
<p class="body">Next, we define a function <code class="fm-code-in-text">plot_images()</code> to visualize 32 images in four rows and eight columns:<a id="idIndexMarker088"/></p>
<pre class="programlisting">def plot_images(imgs):                              <span class="fm-combinumeral">①</span>
    for i in range(32):
        ax = plt.subplot(4, 8, i + 1)               <span class="fm-combinumeral">②</span>
        plt.imshow(imgs[i].permute(1,2,0)/2+0.5)
        plt.xticks([])
        plt.yticks([])
    plt.subplots_adjust(hspace=-0.6)
    plt.show()    
  
imgs, _ = next(iter(train_loader))                  <span class="fm-combinumeral">③</span>
plot_images(imgs)                                   <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a function to visualize 32 images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Places them in a 4 <span class="cambria">×</span> 8 grid</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains a batch of images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calls the function to visualize the images</p>
<p class="body"><a id="marker-89"/>You’ll see a plot of 32 anime faces <a id="idTextAnchor014"/>in a <span class="cambria">4 × 8</span> grid after running the preceding code cell, as shown in figure 4.7. <a id="idIndexMarker089"/><a id="idIndexMarker090"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="351" src="../../OEBPS/Images/CH04_F07_Liu.png" width="728"/></p>
<p class="figurecaption">Figure 4.7 Examples from the anime faces training dataset</p>
</div>
<h2 class="fm-head" id="heading_id_16">4.5 Deep convolutional GAN</h2>
<p class="body">In this section, you’ll create a DCGAN model so that we can train it to generate anime face images. As usual, the GAN model consists of a discriminator network and a generator network. However, the networks are more sophisticated than the ones we have seen before: we’ll use convolutional layers, transposed convolutional layers, and batch normalization layers in these networks. <a id="idIndexMarker091"/><a id="idIndexMarker092"/></p>
<p class="body">We’ll start with the discriminator network. After that, I’ll explain how the generator network mirrors the layers in the discriminator network to conjure up realistic color images. You’ll then train the model with the data you prepared earlier in this chapter and use the trained model to generate novel images of anime face images.</p>
<h3 class="fm-head1" id="heading_id_17">4.5.1 Building a DCGAN</h3>
<p class="body"><a id="marker-90"/>As in previous GAN models we have seen, the discriminator is a binary classifier to classify samples into real or fake. However, different from the networks we have used so far, we’ll use convolutional layers and batch normalizations. The high-resolution color images in this project have too many parameters, and if we use dense layers only, it’s difficult to train the model effectively. The structure of the discriminator neural network is shown in the following listing.<a id="idIndexMarker093"/><a id="idIndexMarker094"/></p>
<p class="fm-code-listing-caption">Listing 4.4 A discriminator in DCGAN</p>
<pre class="programlisting">import torch.nn as nn
import torch
  
device = "cuda" if torch.cuda.is_available() else "cpu"
  
D = nn.Sequential(
    nn.Conv2d(3, 64, 4, 2, 1, bias=False),           <span class="fm-combinumeral">①</span>
    nn.LeakyReLU(0.2, inplace=True),                 <span class="fm-combinumeral">②</span>
    nn.Conv2d(64, 128, 4, 2, 1, bias=False),
    nn.BatchNorm2d(128),                             <span class="fm-combinumeral">③</span>
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(128, 256, 4, 2, 1, bias=False),
    nn.BatchNorm2d(256),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(256, 512, 4, 2, 1, bias=False),
    nn.BatchNorm2d(512),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(512, 1, 4, 1, 0, bias=False),
    nn.Sigmoid(),
    nn.Flatten()).to(device)                         <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Passes the image through a 2D convolutional layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Applies the LeakyReLU activation on outputs of the first convolutional layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Performs 2D batch normalization on outputs of the second convolutional layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The output is a single value between 0 and 1, which can be interpreted as the probability that an image is real.</p>
<p class="body">The input to the discriminator network is a color image with three color channels. The first 2D convolutional layer is <code class="fm-code-in-text">Conv2d(3, 64, 4, 2, 1, bias=False)</code>: this means the input has three channels and the output has 64 channels; the kernel size is 4; the stride is 2; and the padding is 1. Each of the 2D convolutional layers in the network takes an image and applies filters to extract spatial features.</p>
<p class="body">Starting from the second 2D convolutional layer, we apply 2D batch normalization (which I explained in the last section) and LeakyReLU activation (which I’ll explain later) on the output. The LeakyReLU activation function is a modified version of ReLU. It allows the output to have a slope for values below zero. Specifically, the LeakyReLU function is defined as follows:</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="70" src="../../OEBPS/Images/CH04_F07_Liu_EQ01.png" width="357"/></p>
</div>
<p class="body">where <span class="cambria">β</span> is a constant between 0 and 1. The LeakyReLU activation function is commonly used to address the sparse gradients problem (when most gradients become zero or near-zero). Training DCGANs is one such case. When the input to a neuron is negative, the output of ReLU is zero, and the neuron becomes inactive. LeakyReLU returns a small negative value, not zero, for negative inputs. This helps keep the neurons active and learning, maintaining a better gradient flow and leading to faster convergence of model parameters.</p>
<p class="body"><a id="marker-91"/>We’ll use the same approach when building the generator for clothing item generation. We’ll mirror the layers used in the discriminator in DCGAN to create a generator, as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 4.5 Designing a generator in DCGAN</p>
<pre class="programlisting">G=nn.Sequential(
    nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),    <span class="fm-combinumeral">①</span>
    nn.BatchNorm2d(512),
    nn.ReLU(inplace=True),
    nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    <span class="fm-combinumeral">②</span>
    nn.BatchNorm2d(256),
    nn.ReLU(inplace=True),
    nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
    nn.BatchNorm2d(128),
    nn.ReLU(inplace=True),
    nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
    nn.BatchNorm2d(64),
    nn.ReLU(inplace=True),
    nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),       <span class="fm-combinumeral">③</span>
    nn.Tanh()).to(device)                                 <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The first layer in the generator is modeled after the last layer in the discriminator.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The second layer in the generator is symmetric to the second to last layer in the discriminator (numbers of inputs and outputs have switched positions).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The last layer in the generator is symmetric to the first layer in the discriminator.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Uses the Tanh() activation to squeeze values in the output layer to the range [–1, 1] because the images in the training set have values between –1 and 1</p>
<p class="body">As shown in figure 4.8, to create an image, the generator uses five 2D transposed convolutional layers: they are symmetric to the five 2D convolutional layers in the discriminator. For example, the last layer, <code class="fm-code-in-text">ConvTranspose2d(64, 3, 4, 2, 1, bias=False)</code>, is modeled after the first layer in the discriminator, <code class="fm-code-in-text">Conv2d(3, 64, 4, 2, 1, bias=False)</code>. The numbers of <i class="fm-italics">input</i> and <i class="fm-italics">output</i> channels in <code class="fm-code-in-text">Conv2d</code> are reversed and used as the numbers of <i class="fm-italics">output</i> and <i class="fm-italics">input</i> channels in <code class="fm-code-in-text">ConvTranspose2d</code>. <a id="idIndexMarker095"/><a id="idIndexMarker096"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="553" src="../../OEBPS/Images/CH04_F08_Liu.png" width="477"/></p>
<p class="figurecaption">Figure 4.8 Designing a generator network in DCGAN to create anime faces by mirroring the layers in the discriminator network. The right side of the diagram shows the discriminator network, which contains five 2D convolutional layers. To design a generator that can conjure up anime faces out of thin air, we mirror the layers in the discriminator network. Specifically, as shown on the left half of the figure, the generator has five 2D transposed convolutional layers, symmetric to the 2D convolutional layers in the discriminator. Further, in each of the top four layers, the numbers of <code class="fm-code-in-text">input</code> and <code class="fm-code-in-text">output</code> channels in the discriminator are reversed and used as the numbers of <code class="fm-code-in-text">output</code> and <code class="fm-code-in-text">input</code> channels in the generator.</p>
</div>
<p class="body"><a id="marker-92"/>The number of input channels in the first 2D transposed convolutional layer is 100. This is because the generator obtains a 100-value random noise vector from the latent space (bottom left of figure 4.8) and feeds it to the generator. The number of output channels in the last 2D transposed convolutional layer in the generator is 3 because the output is an image with three color channels (RGB). We apply the Tanh activation function to the output of the generator to squeeze all values to the range [–1, 1] because the training images all have values between –1 and 1.</p>
<p class="body">As usual, the loss function is binary cross-entropy loss. The discriminator is trying to maximize the accuracy of the binary classification: identify a real sample as real and a fake sample as fake. The generator, on the other hand, is trying to minimize the probability that the fake sample is being identified as fake.</p>
<p class="body">We’ll use the Adam optimizer for both the discriminator and the generator and set the learning rate to 0.0002:</p>
<pre class="programlisting">loss_fn=nn.BCELoss()
lr = 0.0002
optimG = torch.optim.Adam(G.parameters(), 
                         lr = lr, betas=(0.5, 0.999))
optimD = torch.optim.Adam(D.parameters(), 
                         lr = lr, betas=(0.5, 0.999))</pre>
<p class="body">You have seen the Adam optimizer in chapter 2 but with default values of betas. Here, we select betas that are different from the default values. The betas in the Adam optimizer play crucial roles in stabilizing and speeding up the convergence of the training process. They do this by controlling how much emphasis is placed on recent versus past gradient information (beta1) and by adapting the learning rate based on the certainty of the gradient information (beta2). These parameters are typically fine-tuned based on the specific characteristics of the problem being solved.<a id="idIndexMarker097"/><a id="idIndexMarker098"/></p>
<h3 class="fm-head1" id="heading_id_18">4.5.2 Training and using DCGAN</h3>
<p class="body"><a id="marker-93"/>The training process for DCGAN is similar to what we have done for other GAN models, such as those used in chapter 3 and earlier in this chapter. Since we don’t know the true distribution of anime face images, we’ll rely on visualization techniques to determine when the training is complete. Specifically, we define a <code class="fm-code-in-text">test_epoch()</code> function to visualize the anime faces created by the generator after each epoch of training:<a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/></p>
<pre class="programlisting">def test_epoch():
    noise=torch.randn(32,100,1,1).\
        to(device=device)                             <span class="fm-combinumeral">①</span>
    fake_samples=G(noise).cpu().detach()              <span class="fm-combinumeral">②</span>
    for i in range(32):                               <span class="fm-combinumeral">③</span>
        ax = plt.subplot(4, 8, i + 1)
        img=(fake_samples.cpu().detach()[i]/2+0.5).\
            permute(1,2,0)
        plt.imshow(img)
        plt.xticks([])
        plt.yticks([])
    plt.subplots_adjust(hspace=-0.6)
    plt.show()
test_epoch()                                          <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Obtains 32 random noise vectors from the latent space</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates 32 anime face images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Plots the generated images in a 4 <span class="cambria">×</span> 8 grid</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calls the function to generate images before training the model</p>
<p class="body">If you run the preceding code cell, you’ll see 32 images that look like snowflake statics on a TV screen. They don’t look like anime faces at all because we haven’t trained the generator yet.</p>
<p class="body">We define three functions, <code class="fm-code-in-text">train_D_on_real()</code>, <code class="fm-code-in-text">train_D_on_fake()</code>, and <code class="fm-code-in-text">train_G()</code>, similar to those we used to train the GANs to generate grayscale images of clothing items earlier in this chapter. Go to the Jupypter Notebook for this chapter in the book’s GitHub repository and familiarize yourself with the functions. They train the discriminator with real images. They then train the discriminator with fake images; finally, they train the generator. <a id="idIndexMarker102"/><a id="idIndexMarker103"/><a id="idIndexMarker104"/></p>
<p class="body">Next, we train the model for 20 epochs:</p>
<pre class="programlisting">for i in range(20):
    gloss=0
    dloss=0
    for n, (real_samples,_) in enumerate(train_loader):
        loss_D=train_D_on_real(real_samples)
        dloss+=loss_D
        loss_D=train_D_on_fake()
        dloss+=loss_D
        loss_G=train_G()
        gloss+=loss_G
    gloss=gloss/n
    dloss=dloss/n
    print(f"epoch {i+1}, dloss: {dloss}, gloss {gloss}")
    test_epoch()</pre>
<p class="body">The training takes about 20 minutes if you are using GPU training. Otherwise, it may take 2 to 3 hours, depending on the hardware c<a id="idTextAnchor015"/>onfiguration on your computer. Alternatively, you can download the trained model from my website: <a class="url" href="https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip">https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip</a>.<a id="marker-94"/></p>
<p class="body">After every epoch of training, you can visualize the generated anime faces. After just one epoch of training, the model can already generate color images that look like anime faces, as shown in figure 4.9. As training progresses, the quality of the generated images becomes better and better.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="410" src="../../OEBPS/Images/CH04_F09_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 4.9 Generated images in DCGAN after one epoch of training</p>
</div>
<p class="body">We’ll discard the discriminator and save the trained generator in the local folder:</p>
<pre class="programlisting">scripted = torch.jit.script(G) 
scripted.save('files/anime_gen.pt') </pre>
<p class="body">To use the trained generator, we load up the model and use it to generate 32 images:</p>
<pre class="programlisting">new_G=torch.jit.load('files/anime_gen.pt',
                     map_location=device)
new_G.eval()
noise=torch.randn(32,100,1,1).to(device)
fake_samples=new_G(noise).cpu().detach()
for i in range(32):
    ax = plt.subplot(4, 8, i + 1)
    img=(fake_samples.cpu().detach()[i]/2+0.5).permute(1,2,0)
    plt.imshow(img)
    plt.xticks([])
    plt.yticks([])
plt.subplots_adjust(hspace=-0.6)
plt.show() </pre>
<p class="body">The generated anime faces are shown in figure 4.10. The generated images bear a close resemblance to the ones in the training set shown in figure 4.7.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="351" src="../../OEBPS/Images/CH04_F10_Liu.png" width="728"/></p>
<p class="figurecaption">Figure 4.10 Generated anime face images by the trained generator in DCGAN</p>
</div>
<p class="body"><a id="marker-95"/>You may have noticed that the hair colors of the generated images are different: some are black, some are red, and some are blond. You may wonder: Can we tell the generator to create images with a certain characteristic, such as black hair or red hair? The answer is yes. You’ll learn a couple of different methods to select characteristics in generated images in GANs in chapter 5. <a id="idIndexMarker105"/><a id="idIndexMarker106"/><a id="idIndexMarker107"/><a id="idIndexMarker108"/></p>
<h2 class="fm-head" id="heading_id_19">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">To conjure up realistic-looking images out of thin air, the generator mirrors layers used in the discriminator network.</p>
</li>
<li class="fm-list-bullet">
<p class="list">While it’s feasible to generate grayscale images by using just fully connected layers, to generate high-resolution color images, we need to use CNNs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Two-dimensional convolutional layers are used for feature extraction. They apply a set of learnable filters (also known as kernels) to the input data to detect patterns and features at different spatial scales. These <a id="idTextAnchor016"/>layers are essential for capturing hierarchical representations of the input data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Two-dimensional transposed convolutional layers (also known as deconvolution or upsampling layers) are used for upsampling or generating high-resolution feature maps. They apply a filter to the input data. However, unlike standard convolution, they increase the spatial dimensions by inserting gaps between the output values, which effectively “upscales” the feature maps. This process generates feature maps of a higher resolution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Two-dimensional batch normalization is a technique commonly used in deep learning and neural networks to improve the training and performance of CNNs and other models that work with 2D data, such as images. It normalizes the values for each feature channel, so they have a mean of 0 and a standard deviation of 1, which helps stabilize and speed up training.<a id="marker-96"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Sergey Ioffe, Christian Szegedy, 2015, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” <a class="url" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>.</p>
</div></body></html>