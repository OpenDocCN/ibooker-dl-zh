<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Learning the Logic of DNA"><div class="chapter" id="learning-the-logic-of-dna">
 <h1><span class="label">Chapter 3. </span>Learning the Logic of DNA</h1>
 <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-type="indexterm" id="ch03_dna.html0"/>In this chapter, we’ll build a deep learning model to predict whether a DNA sequence is bound by a class of proteins called <em>transcription factors</em> (TFs). <a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="defined" data-type="indexterm" id="id612"/>Transcription factors play a central role in gene regulation: they bind to specific DNA sequences and influence whether nearby genes are turned on or off. By recognizing these sequence patterns, we can begin to decode the regulatory logic embedded in the genome.
 </p>
 <p>Unlike the previous chapter—where we used an off-the-shelf protein model from Hugging Face—here we’ll start defining and training our own models from scratch. This gives us more control and helps us better understand how deep learning works on biological data. We’ll explore both convolutional and transformer-based architectures and introduce interpretation techniques to help us understand how our models make predictions.</p>
 <p>We will tackle this problem in stages, gradually increasing the complexity:
 </p>
 <dl>
  <dt>1. Start simple</dt>
  <dd>
   <p>First, we’ll train a basic convolutional network to predict whether a DNA sequence binds a single transcription factor called CTCF. Its binding behavior is relatively easy to predict, making it a great first target. We’ll build the full pipeline: loading data, training the model, and checking whether it captures meaningful biological signals.
   </p>
  </dd>
  <dt>2. Increase complexity</dt>
  <dd>
   <p>Next, we’ll scale up to predicting whether a sequence binds any of 10 different TFs. We’ll introduce regularization and normalization, improve our evaluation metrics, and begin inspecting individual predictions. We’ll also use mutation experiments and input gradients to highlight which parts of the sequence the model relies on—offering a first step toward interpretability.
   </p>
  </dd>
  <dt>3. Incorporate advanced techniques</dt>
  <dd>
   <p>Finally, we’ll try adding transformer layers to explore whether they improve performance and continue dissecting model behavior to understand how different architectural choices influence learning.
   </p>
  </dd>
</dl>
 <p>This staged approach—building up from simple to more complex—is one we recommend in general. It helps keep models interpretable, makes debugging easier, and builds confidence along the way.</p> 
 <p>Before diving in, we’ll do a quick refresher on the biological and machine learning concepts that underpin this chapter.
 </p>

 <div data-type="tip"><h6>Tip</h6>
<p>To get the most out of this chapter, open the companion Colab notebook and run the code cells as you follow along. Executing the code interactively will deepen your understanding and give you space to experiment with the concepts in real time.</p>
</div>

 <section data-type="sect1" data-pdf-bookmark="Biology Primer"><div class="sect1" id="biology-primer_2111874">
  <h1>Biology Primer</h1>
  <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="biology" data-type="indexterm" id="ch03_dna.html1"/>It’s astonishing that all the instructions for building an entire human body are encoded in the DNA of a single cell. Every human starts as one tiny cell—about 100 micrometers wide—with its DNA tightly packed into a nucleus just 6 micrometers across. This DNA acts as the blueprint for processes like cell division and differentiation, eventually giving rise to the diverse tissues and cell types that make up an entire human body.
  </p>
  <div data-type="note" epub:type="note"><h6>Note</h6>
   <p><a contenteditable="false" data-primary="genetics, defined" data-type="indexterm" id="id613"/><a contenteditable="false" data-primary="genome, defined" data-type="indexterm" id="id614"/><a contenteditable="false" data-primary="genomics, defined" data-type="indexterm" id="id615"/>A few terminology clarifications: The <em>genome</em> refers to the complete set of DNA in an organism, including all of its genes and other genetic material. While <em>genetics</em> typically studies individual genes or small gene sets, <em>genomics</em> takes a broader view, often analyzing entire genomes across individuals or even species.
   </p>
  </div>
  <p>The human genome is vast—more than 3 billion base pairs long—and carries with it billions of years of evolutionary history. But what is this molecule, really, and how does it encode biological function?
  </p>
  <section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="What Exactly Is DNA?"><div class="sect2" id="what-is-dna">
   <h2>What Exactly Is DNA?</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="biology" data-tertiary="DNA basics" data-type="indexterm" id="ch03_dna.html2"/>DNA is the molecule of inheritance—the fundamental code of life. Its double-helix structure was first revealed in 1953 by Watson, Crick, and Franklin, marking a pivotal moment in the biological sciences. Nearly half a century later, the first complete draft of the human genome was published in 2001,<sup><a data-type="noteref" id="id616-marker" href="ch03.html#id616">1</a></sup> laying the foundation for modern genetics and genomics. But these milestones are relatively recent, and while we now know a great deal about <em>what</em> is in the genome, we still understand surprisingly little about <em>how</em> it actually works.
   </p>
   <p>We do know that DNA is built from four chemical letters, or nucleotide bases: <code>A</code> (adenine), <code>C</code> (cytosine), <code>G</code> (guanine), and <code>T</code> (thymine). These bases form long sequences that carry genetic instructions. The full human genome contains around 3.2 billion of these letters, packed into 23 pairs of chromosomes. To fit inside the tiny nucleus of a cell, this DNA wraps around proteins and coils into compact, highly organized structures known as <em>chromatin</em>, as shown in <a data-type="xref" href="#dna-organization">Figure 3-1</a>.
   </p>
   <p>Despite decades of research, the genome remains full of unanswered questions. Only about 2% of it directly codes for proteins—what is the rest doing? How can all the cells in your body share the same DNA, yet behave so differently? What controls when a gene is used, and how do changes in the environment or during development affect this process?</p>
   <p>These mysteries lie at the heart of gene regulation—and increasingly, deep learning is helping us explore them.</p>
   <figure><div id="dna-organization" class="figure">
    <img alt="" src="assets/dlfb_0301.png" width="600" height="514"/>
    <h6><span class="label">Figure 3-1. </span>DNA is packed into the nucleus in multiple layers of structure. Starting with the double helix, it first wraps around histone proteins to form nucleosomes (beads on a string), which fold into chromatin fibers, and ultimately into chromosomes. Source: <a href="https://oreil.ly/h8M44">National Institute of Environmental Health Sciences</a>.
    </h6>
   </div></figure>
  

  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Coding and Noncoding Regions"><div class="sect2" id="coding-and-non-coding-regions">
   <h2>Coding and Noncoding Regions</h2>
  <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="biology" data-tertiary="coding and noncoding regions" data-type="indexterm" id="id617"/>The human genome contains around 20,000 protein-coding genes. These make up the <em>coding</em> regions—stretches of DNA that are <em>transcribed</em> into RNA and then <em>translated</em> into proteins. Each protein carries out specific tasks, from building cellular structures to catalyzing chemical reactions. Together, they perform most of the cell’s essential functions.</p>
  <p class="pagebreak-before">Yet, protein-coding genes account for only about 2% of the genome. The remaining 98% is noncoding DNA. While it doesn’t produce proteins, noncoding DNA plays critical regulatory roles, helping to control when and where genes are used. In fact, most genetic variants associated with human disease fall in noncoding regions—though we still have limited understanding of how most of them exert their effects.</p>
  <p>Some noncoding DNA produces RNAs that regulate gene expression, while other regions help organize the 3D structure of the genome or serve as docking sites for regulatory proteins. One especially important category of noncoding region is transcription factor binding sites. These are short DNA sequences where <em>transcription factors</em> (TFs) attach to help regulate gene activity—and they’re the central focus of this chapter.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="How Transcription Factors Orchestrate Gene Activity"><div class="sect2" id="transcription-factors-orchestrate-gene-activity">
   <h2>How Transcription Factors Orchestrate Gene Activity</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="biology" data-tertiary="how transcription factors orchestrate gene activity" data-type="indexterm" id="ch03_dna.html3"/><a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="gene activity and" data-type="indexterm" id="ch03_dna.html4"/>TFs are proteins that control which genes are used, when, and in what context. They do this by binding to short, specific DNA sequences called <em>motifs</em>, often located near genes. By binding these motifs, TFs can activate or repress transcription. You can think of them as conductors of a genomic orchestra—directing the performance by determining which regions get “played” and when.
   </p>
   <p>TFs are involved in nearly every biological process, from guiding development to coordinating how cells respond to stress or infection. Humans have around 1,600 different TFs, each of which has evolved to recognize specific DNA motifs. These motifs are short sequence patterns—typically 6 to 15 base pairs long—that TFs preferentially bind to. For example, the well-studied transcription factor CTCF binds a core motif with the central pattern <code>CCCTC</code>. TFs don’t bind randomly across the genome—they search for these preferred sequences.
   </p>
   <p>These motifs form specific 3D shapes in the DNA helix, and the protein-binding domains of TFs are shaped to complement them—much like a key fitting into a lock. In reality, the interaction is often more flexible and dynamic than the analogy suggests, but the idea of a physical match still holds.</p>
   <p>To make this interaction more concrete, <a data-type="xref" href="#fig3-2">Figure 3-2</a> shows crystallographic structures of different TFs bound to DNA.</p>

   <figure><div id="fig3-2" class="figure">
    <img alt="" src="assets/dlfb_0302.png" width="485" height="800"/>
    <h6><span class="label">Figure 3-2. </span>Crystallographic structures showing how different types of TF binding domains interact with DNA. Each gray structure represents a different TF binding domain—zinc finger, homeodomain, helix-loop-helix, and forkhead—bound to a double-stranded DNA molecule. These protein segments recognize specific DNA motifs by the physical shape those sequences form. Like a key fitting into a lock, the structure of the protein complements the shape of the DNA at its binding site. Shown here are actual resolved structures from the Protein Data Bank (PDB: 6ML2, 6KKS, 1NKP, 3G73). Source: <a href="https://oreil.ly/Nd4Tv">Wikipedia</a>.
    </h6>
   </div></figure>
   <p>However, not every matching motif is actually bound in a living cell. In fact, the genome contains far more motif matches than actual binding events. That’s because binding depends on many additional factors:
   </p>
   <dl class="less_space pagebreak-before">
   <dt>Chromatin accessibility</dt>
    <dd>
     <p>DNA that’s tightly packed into chromatin is harder for proteins to access. TFs are more likely to bind in regions of open chromatin.
     </p>
    </dd>
    <dt>DNA methylation</dt>
    <dd>
     <p>Certain TFs, like CTCF, are sensitive to methylation (a certain chemical modification of DNA bases) at their binding sites, which can block binding even if the motif is present.
     </p>
    </dd>
    <dt>Cellular signals</dt>
    <dd>
     <p>Signals inside the cell—such as hormones or stress responses—can activate or deactivate a TF’s ability to bind.
     </p>
    </dd>
    <dt>Other proteins</dt>
    <dd>Helper or blocking proteins in the local environment can facilitate or inhibit binding.</dd>
    <dt>Cooperative binding</dt>
    <dd>Many TFs work in complexes or recruit others to stabilize binding and control gene activity.</dd>
   </dl>
   <p>And perhaps most importantly, real cells are dynamic. Molecules move, concentrations change, and TF binding events happen on short timescales. Most genomic datasets, by contrast, are static snapshots—freeze-frames of a constantly changing scene. That’s worth keeping in mind when interpreting binding data in this chapter.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html4" data-type="indexterm" id="id618"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html3" data-type="indexterm" id="id619"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Measuring Where Transcription Factors Bind"><div class="sect2" id="measuring-tf-binding">
   <h2>Measuring Where Transcription Factors Bind</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="biology" data-tertiary="measuring where transcription factors bind" data-type="indexterm" id="id620"/><a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="measuring where transcription factors bind" data-type="indexterm" id="id621"/>In deep learning, we’re incredibly reliant on experimental data—we need something to learn from. When it comes to studying TFs, that data typically comes from laboratory experiments that measure where in the genome a particular TF binds.</p>
<p>The cornerstone wet lab method is <a href="https://oreil.ly/IovSO">ChIP-seq</a>, or chromatin immunoprecipitation followed by sequencing. TFs don’t permanently stick to DNA—they bind and unbind constantly. ChIP-seq captures a snapshot of this dynamic process by chemically cross-linking (gluing) proteins to DNA, essentially freezing them in place. The DNA fragments bound by a specific TF can then be isolated, sequenced, and mapped back to the genome to determine where the protein was bound.</p>
<p>ChIP-seq data is typically visualized as peaks over the DNA sequence—regions of the genome where TF binding was enriched. The height of a peak reflects how strong or frequent the binding was at that site, while a flat zero signal means no detectable binding occurred there.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To simplify the data for modeling, ChIP-seq peaks can be binarized: instead of retaining the full quantitative signal, we apply a threshold and record only whether the TF was bound in a given region. This reduces the task to a binary classification problem—does the TF bind to this DNA sequence or not—which is the setup we’ll use throughout this chapter.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html1" data-type="indexterm" id="id622"/></p></div>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Machine Learning Primer"><div class="sect1" id="machine-learning-primer_57866338">
  <h1>Machine Learning Primer</h1>

    <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-type="indexterm" id="ch03_dna.html5"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-type="indexterm" id="ch03_dna.html6"/>With the biology background knowledge in place, we now review some foundational machine learning concepts that we’ll use in this chapter. If you’re already familiar with deep learning, feel free to skim this section as a refresher. We’ll briefly cover <em>convolutional</em> and <em>transformer</em>-based architectures, how they can be applied to biological sequence data like DNA, and what kinds of insights they can provide.</p>
  <section data-type="sect2" data-pdf-bookmark="Convolutional Neural Networks"><div class="sect2" id="convolutional-neural-networks">
   <h2>Convolutional Neural Networks</h2>
   <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="ch03_dna.html7"/><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="convolutional neural networks" data-type="indexterm" id="ch03_dna.html8"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="convolutional neural networks" data-type="indexterm" id="ch03_dna.html9"/><em>Convolutional neural networks</em> (CNNs) are one of the most widely used deep learning architectures. Their core strength lies in their ability to automatically learn useful patterns from raw, grid-like input data—whether that’s pixels in an image or bases in a DNA sequence—without the need for hand-engineered features.
   </p>
   <p>CNNs were originally developed for image recognition tasks. In images, low-level features like edges and textures appear in small local patches, while higher-level concepts like shapes or objects are formed by combining these local features. CNNs mirror this structure by using small, learnable <em>filters</em> that slide across the input, extracting local patterns at each position. As we stack more layers, the model combines local features into more abstract and global representations.</p>
   <p>This ability to model hierarchical structure turns out to be useful far beyond images. Whenever there are meaningful local patterns in data—like substructures in molecules, motifs in DNA, or phonemes in speech—CNNs often perform well.</p>
   <div data-type="tip"><h6>Tip</h6>
    <p>We cover CNNs in more detail in Chapter 5, where we build a skin cancer classifier and explore common model design patterns. For now, we’ll provide a short overview of the key components you’ll need for this chapter.</p>
   </div>
   <p>
    Let’s briefly walk through the key components of a typical CNN:
   </p>
   <dl>
   <dt>Convolutional layers</dt>
    <dd>
     <p><a contenteditable="false" data-primary="convolutional layers" data-type="indexterm" id="id623"/>These are the heart of the CNN. A convolutional layer contains multiple filters (also called <em>kernels</em>)—small weight matrices that slide across the input and compute dot products. Each filter acts like a pattern detector, lighting up when it finds a good match in the input. The result of applying a convolution is a <em>feature map</em> showing where the pattern occurred.
     </p>
    </dd>
    <dt>Pooling layers</dt>
    <dd>
     <p><a contenteditable="false" data-primary="pooling layers" data-type="indexterm" id="id624"/>These downsample the feature maps to reduce dimensionality and computation. <em>Max pooling</em>, for example, keeps only the strongest signal in each region, helping the model focus on the most salient features.
     </p>
    </dd>
    <dt>Normalization</dt>
    <dd>
     <p><a contenteditable="false" data-primary="normalization, defined" data-type="indexterm" id="id625"/>Layers like <em>batch normalization</em> rescale activations to make training more stable and efficient. They reduce internal <em>covariate shift</em>—the tendency for activations to drift during training—and often speed up convergence.
     </p>
    </dd>
    <dt>Fully connected layers</dt>
    <dd>
     <p><a contenteditable="false" data-primary="fully connected layers, defined" data-type="indexterm" id="id626"/>These sit at the end of the network and use the features extracted by earlier layers to make final predictions—such as whether a DNA sequence is bound by a transcription factor.
     </p>
    </dd>
   </dl>
   <p>One key property of CNNs is that they are <em>translation equivariant</em>, meaning that a pattern can be recognized regardless of where it appears in the input. This is especially useful for DNA: a binding motif is still a binding motif whether it’s at position 10 or position 90 of the sequence.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html9" data-type="indexterm" id="id627"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html8" data-type="indexterm" id="id628"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html7" data-type="indexterm" id="id629"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Convolutions for DNA Sequences"><div class="sect2" id="convolutions-for-dna-sequences">
   <h2>Convolutions for DNA Sequences</h2>
   <p>
    <a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutions for DNA sequences" data-type="indexterm" id="id630"/><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="convolutions for DNA sequences" data-type="indexterm" id="id631"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="convolutions for DNA sequences" data-type="indexterm" id="id632"/>Although CNNs were originally developed for images (which are 2D grids of pixels), the architecture can easily be adapted to 1D data like DNA sequences. In genomics, DNA is commonly represented as a one-hot encoded matrix, where each base (<code>A</code>, <code>C</code>, <code>G</code>, <code>T</code>) is turned into a binary vector. A sequence of 100 bases would thus become a 100×4 matrix.
   </p>
   <p><a contenteditable="false" data-primary="1D convolutions" data-primary-sortas="oneD" data-type="indexterm" id="id633"/>We then apply <em>1D convolutions</em>—filters that slide across the sequence in one dimension, looking for patterns in short windows of DNA bases. These filters often end up learning to identify the presence of DNA motifs: the short sequence patterns that have biological meaning we mentioned before, such as transcription factor binding sites. For example:</p>
   <ul>
    <li>Shallow layers might learn to detect low-level DNA features such as simple GC-rich or AT-rich regions in DNA.</li>
    <li>Mid-level filters may identify known TF motifs.</li>
    <li>Deeper layers might learn higher-order combinations—such as co-occurring motifs or long-range dependencies.</li>
   </ul>
   <p>Importantly, the model learns all of this automatically from labeled data. It doesn’t need to be told what motif to look for—it discovers useful patterns by optimizing for the task at hand, such as predicting TF binding.</p>
   <div data-type="note" epub:type="note"><h6>Note</h6><p>CNNs have become a standard architecture in sequence-based biology tasks because they offer a good balance between power, speed, and interpretability. Compared to other deep learning architectures, CNNs are relatively lightweight, easy to train, and often easier to interpret: you can visualize which motifs a filter has learned and where they appear in a sequence.</p></div>
   <p>Their main limitation, however, is that they operate on fixed windows of sequence and struggle to model interactions between distant bases. For problems involving long-range dependencies—relationships between elements far apart in a sequence—we often turn to a different class of models.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Transformers"><div class="sect2" id="transformers">
   <h2>Transformers</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="transformers" data-type="indexterm" id="id634"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="transformers" data-type="indexterm" id="id635"/><a contenteditable="false" data-primary="transformers" data-type="indexterm" id="id636"/>While CNNs are excellent at detecting local patterns, transformers are particularly powerful for modeling relationships across long distances in a sequence. <a contenteditable="false" data-primary="self-attention" data-secondary="transformers and" data-type="indexterm" id="id637"/>Their core mechanism—<em>self-attention</em>—allows the model to dynamically determine which parts of the input are relevant for predicting any given output, regardless of how far apart those parts are.
   </p>
   <p>Transformer-based models have revolutionized deep learning since their debut in 2017.<sup><a data-type="noteref" id="id638-marker" href="ch03.html#id638">2</a></sup> Originally designed for natural language processing tasks like translation and summarization, transformers have since become the state of the art in a wide range of domains, including genomics and protein modeling. Earlier architectures like RNNs and CNNs can also handle sequences, but transformers have largely surpassed them for tasks that require understanding global sequence context.
   </p>
   <p>The key innovation is <em>self-attention</em>: a mechanism that lets each token in the sequence “attend to” every other token, computing how much influence they should have. This is useful for:</p>
   <ul>
    <li><p>Language: Where a word’s meaning depends on faraway context</p></li>
    <li><p>Genomics: Where a regulatory DNA motif located thousands of bases away might affect gene activity</p></li>
   </ul>
   <p>This flexibility allows transformers to learn arbitrary and complex dependencies—something CNNs struggle with. The main drawback of transformers is scalability: self-attention requires computations that grow quadratically with sequence length. For very long inputs like whole-genome sequences, this can become a bottleneck. However, many efficient transformer variants (e.g., Linformer, Longformer, Performer) have been developed to partially address this weakness.</p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>If you’d like to dive deeper into transformers, we recommend the excellent blog post <a href="https://oreil.ly/WxzGl">“The Illustrated Transformer”</a> by Jay Alammar or the <a href="https://oreil.ly/dLW40">3Blue1Brown video</a> on attention. We’ll only touch on the basics here.
    </p>
   </div>
   <p>The foundation of transformers lies in their ability to assign <em>attention</em>—a mechanism that lets each position in a sequence dynamically focus on other positions. This is what enables them to model long-range dependencies.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Attention"><div class="sect2" id="attention">
   <h2>Attention</h2>
   <p><a contenteditable="false" data-primary="attention (machine learning process)" data-type="indexterm" id="ch03_dna.html10"/><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="attention" data-type="indexterm" id="ch03_dna.html11"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="attention" data-type="indexterm" id="ch03_dna.html12"/>At a high level, attention is a process that enriches the embedding of each token by incorporating information from every other token in the sequence. This makes each token more context aware, as illustrated in <a data-type="xref" href="#attention-mechanism">Figure 3-3</a>.</p>
   <figure><div id="attention-mechanism" class="figure">
    <img alt="" src="assets/dlfb_0303.png" width="535" height="800"/>
    <h6><span class="label">Figure 3-3. </span>High-level visualization of the attention mechanism transforming input token embeddings into context-aware output embeddings. Each of the four tokens in the sequence (“the cat is black”) attends to every other token, allowing the model to capture relationships and dependencies across the entire sequence. The output is also four tokens, each enriched with contextual information from the others.</h6>
   </div></figure>
<p>Briefly, here’s how it works. The model first creates three versions of the input embeddings—queries (Q), keys (K), and values (V)—via learned linear transformations. Each query is compared to all keys using a dot product, producing a score that reflects how relevant one token is to another. These scores are then normalized via a softmax function, producing the attention weights. Finally, each token’s new embedding is computed as a weighted sum of the value vectors, with the attention weights determining how much each value contributes.</p>

<p>Let’s walk through a simple example. Suppose we have the input sequence <code>the</code>, <code>cat</code>, <code>is</code>, <code>black</code>. In a transformer model, each token doesn’t just pass through the network on its own; it decides how much attention to pay to every other token. For example, when processing the word <code>cat</code>, the model might assign a high attention weight to <code>the</code>, recognizing that articles and nouns are often linked. This helps the model understand grammatical relationships and contextual meaning.
</p>
<p>In genomics, attention can serve a similar purpose. Imagine a model processing a DNA sequence to predict TF binding. An attention mechanism allows the model to ask: how relevant is this motif to another element upstream or downstream—perhaps thousands of bases away? Just as a word’s meaning is shaped by the words around it, the function of a sequence motif may depend on other elements scattered across the genome.</p>
<p><a contenteditable="false" data-primary="feedforward network" data-type="indexterm" id="id639"/>Once attention has enriched the token embeddings with contextual information, the result is passed through a <em>feedforward network</em>—typically a small, multilayer perceptron applied independently to each token. This network introduces nonlinearity and helps the model capture more complex patterns. The output is then passed through <em>residual connections</em> (which help with gradient flow) and <em>layer normalization</em> (which stabilizes training).</p>
<p>All together, this sequence—attention, feedforward layers, residual connections, and normalization—forms one transformer block. A full transformer model is typically built by stacking many of these blocks, allowing information to flow and be refined across layers. As tokens pass through successive layers, their representations become increasingly rich, capturing everything from local patterns to global structure.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html12" data-type="indexterm" id="id640"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html11" data-type="indexterm" id="id641"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html10" data-type="indexterm" id="id642"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Query, Key, and Value Intuition"><div class="sect2" id="query-key-and-value-intuition">
 <h2>Query, Key, and Value Intuition</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="query/key/value intuition" data-type="indexterm" id="id643"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="query/key/value intuition" data-type="indexterm" id="id644"/><a contenteditable="false" data-primary="tokens, query/key/value intuition" data-type="indexterm" id="id645"/>You might wonder: why bother transforming the input into queries, keys, and values at all? The key idea is that Q, K, and V aren’t just redundant copies of the original token embeddings. They’re learned projections that allow the model to look at the same sequence from different perspectives:
   </p>
   <ul class="simple">
    <li><a contenteditable="false" data-primary="query, defined" data-type="indexterm" id="id646"/>The <em>query</em> is like a lens that each token uses to express what it wants to pay attention to. In our sentence example, the word <code>black</code> might use its query to find the noun it modifies—<code>cat</code>. In DNA, a regulatory region might “look for” compatible regulatory motifs elsewhere in the sequence. The query says, “I’m looking for X.”</li>
    <li><a contenteditable="false" data-primary="key, defined" data-type="indexterm" id="id647"/>The <em>key</em> is how each token presents itself to others: it encodes what kind of information it offers. Continuing the analogy, each word or DNA element “advertises” what kind of content it has, saying: “I contain Y.”</li>
    <li><a contenteditable="false" data-primary="value, defined" data-type="indexterm" id="id648"/>The <em>value</em> is the actual content that gets passed along if the query decides the key is relevant. In other words, the query compares itself to all keys to compute attention weights and then uses those weights to pull a mixture of values from across the sequence.</li>
   </ul>
   <p>This separation of roles allows the model to reason more flexibly. Instead of treating all parts of the sequence as equally relevant, each token decides what matters to it right now, based on its query and the other tokens’ keys. The values then supply the useful content.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6><p>This design makes attention work like a smart lookup system: tokens advertise what they contain (keys), queries look for matches, and then the actual content (values) is pulled in weighted proportion to how well the key matched the query.</p></div>
   <p>Importantly, the Q, K, and V projections are all learned from data. They start out random, and the model figures out—through training—how best to shape these representations for the task at hand, whether that’s learning grammar, predicting regulatory activity, or modeling protein interactions.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Multiheaded Attention"><div class="sect2" id="multi-headed-attention">
   <h2>Multiheaded Attention</h2>
   <p><em><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="multiheaded attention" data-type="indexterm" id="id649"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="multiheaded attention" data-type="indexterm" id="id650"/><a contenteditable="false" data-primary="MHA (multiheaded attention)" data-type="indexterm" id="id651"/><a contenteditable="false" data-primary="multiheaded attention (MHA)" data-type="indexterm" id="id652"/>Multiheaded attention</em> (MHA) enhances the attention mechanism by running several attention operations—called <em>heads</em>—in parallel. Each head learns to focus on different parts or patterns in the input sequence. For instance, one head might focus on local motifs, while another might detect longer-range interactions or subtle contextual cues.
   </p>
   <p>By combining these multiple perspectives, MHA allows the model to capture a richer and more diverse set of relationships within the data, beyond what a single attention operation could learn.</p>
   <div data-type="tip"><h6>Tip</h6><p>While often the different heads learn somewhat redundant patterns, this parallel structure increases the model’s expressive power and flexibility. The outputs from all heads are concatenated and linearly transformed to produce the final representation, which then feeds into subsequent model layers.</p></div>
   <p>In essence, MHA lets transformers attend to multiple types of interactions simultaneously, which is particularly useful for complex biological sequences where various signals and dependencies exist at different scales.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Representing Positional Information"><div class="sect2" id="representing-positional-information">
       <h2>Representing Positional Information</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="representing positional information" data-type="indexterm" id="id653"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="representing positional information" data-type="indexterm" id="id654"/>One final point: basic self-attention is <em>position invariant</em>, meaning it does not inherently capture the order or position of tokens in a sequence. To address this, transformer models include positional encodings or other mechanisms that inject information about the relative or absolute positions of tokens, enabling the model to understand sequence order. In the original transformer paper, sinusoidal functions of the token index were added to the token embeddings to provide this positional information.</p>
   <p>With this brief overview of transformers complete, let’s move on to some model interpretation techniques we’ll apply in this chapter.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Model Interpretation"><div class="sect2" id="model-interpretation">
   <h2>Model Interpretation</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="model interpretation" data-type="indexterm" id="ch03_dna.html13"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="model interpretation" data-type="indexterm" id="ch03_dna.html14"/><a contenteditable="false" data-primary="model interpretation" data-type="indexterm" id="ch03_dna.html15"/>A common criticism of deep learning models is that they act as a <em>black box</em>—they may produce accurate predictions, but it’s often unclear how exactly those predictions are made or what internal reasoning the model uses. While deep learning models are generally less interpretable than simpler methods like linear models or decision trees, there are several techniques to probe their inner workings. These techniques fall under the umbrella of <em>model interpretation</em>.
   </p>
   <p>Model interpretation for deep learning is a vast and active research area, so here we provide a brief overview of the most commonly used techniques in the DNA modeling space:
   </p>
   <dl>
   <dt>Mutagenesis</dt>
    <dd><p><a contenteditable="false" data-primary="mutagenesis" data-seealso="in silico saturation mutagenesis" data-type="indexterm" id="id655"/>To understand which input features the model relies on, we systematically alter (or <em>mutate</em>) parts of the input and observe how the model’s predictions change. For example, when predicting gene expression from DNA sequence, we can shuffle, replace, or delete certain bases and see if the prediction shifts significantly. Large changes indicate that the mutated region is important for the model.</p>
     <ul>
      <li>
       <p>Pro: This is direct and intuitive. It provides rich, localized insights.
       </p>
      </li>
      <li>
       <p>Con: It is computationally expensive, since each mutation requires a separate forward pass through the model.
       </p>
      </li>
    </ul></dd>

    <dt class="less_space pagebreak-before">Input gradients</dt>
      <dd><p><a contenteditable="false" data-primary="input gradients" data-secondary="for model interpretation" data-type="indexterm" id="id656"/>A faster but approximate method involves computing the gradient of the model’s output with respect to each input feature. This gradient shows how sensitive the prediction is to small changes in each input element.
       </p>
       <ul>
        <li>
         <p>Pro: It is efficient, as it requires only one backward pass to generate an importance map.
         </p>
        </li>
        <li>
         <p>Con: It can be noisy and less precise, making it harder to distinguish signal from noise.
         </p>
        </li>
      </ul>
      </dd>
      <dt>Attention mechanisms</dt>
        <dd><p><a contenteditable="false" data-primary="attention mechanisms, for model interpretation" data-type="indexterm" id="id657"/>For models that include attention, we can inspect the attention weights to see where the model is focusing when making predictions.
         </p>
         <ul>
          <li>
           <p>Pro: This provides a naturally interpretable visualization of model focus and interactions.
           </p>
          </li>
          <li>
           <p>Con: Attention scales quadratically with input sequence length, meaning that in practice, we can’t use attention to model very long DNA strings without first condensing them down in some way.
           </p>
          </li>
        </ul>
        </dd></dl>

   <p>There are many extensions and refinements of these methods, and model interpretation is increasingly standard in deep learning biology research papers.</p>
<p>Next, we’ll dive deeper into the two interpretation approaches that we will implement in this chapter: <em>in silico</em> mutagenesis and input gradients.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html15" data-type="indexterm" id="id658"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html14" data-type="indexterm" id="id659"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html13" data-type="indexterm" id="id660"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="In Silico Saturation Mutagenesis"><div class="sect2" id="in-silico-saturation-mutagenesis">
   <h2>In Silico Saturation Mutagenesis</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="in silico saturation mutagenesis" data-type="indexterm" id="id661"/><a contenteditable="false" data-primary="in silico saturation mutagenesis (ISM)" data-type="indexterm" id="id662"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="in silico saturation mutagenesis" data-type="indexterm" id="id663"/><em>In silico saturation mutagenesis</em> (ISM) may sound complex, but it essentially involves systematically making every possible alteration (or mutation) to a biological sequence—such as DNA or protein—and generating a separate model prediction for each mutated sequence. Because this requires many forward passes through the model, it is computationally expensive. However, the detailed insights it provides into how each possible variation affects the output often justify the cost.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>Terminology breakdown: it’s called <em>mutagenesis</em> because we induce mutations, <em>saturation</em> because every possible mutation is tested, and <em>in silico</em> since these predictions are done computationally rather than experimentally in the lab.
    </p>
   </div>
   <p><a data-type="xref" href="#saturation-mutagenesis">Figure 3-4</a> shows an example plot we will generate later in this chapter.
   </p>
   <figure><div id="saturation-mutagenesis" class="figure">
    <img alt="" src="assets/dlfb_0304.png" width="600" height="165"/>
    <h6><span class="label">Figure 3-4. </span>Example of an in silico saturation mutagenesis plot showing predicted probabilities of transcription factor binding across a 200-base DNA sequence. The x-axis represents the DNA sequence positions, while the y-axis shows the three possible mutations at each position (including the mutation to the original base, which has no effect and is set to zero). In this example, most mutation effects are negative (darker color), indicating that changing bases generally reduces the predicted binding probability. This approach is computationally expensive due to the large number of forward passes required to generate this output matrix (here, 3 mutations × 200 positions = 600 model predictions).
    </h6>
   </div></figure>
   <p>ISM plots help to quickly highlight which parts of the sequence are most important for the model’s predictions. Because they visualize the most salient or impactful input regions, they are often referred to as <em>saliency maps</em>. In this example, the plot suggests that mutating any of the central bases likely disrupts a motif that the protein binds to, as these mutations generally lead to lower predicted binding probabilities (indicated by the negative values).
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Input Gradients"><div class="sect2" id="input-gradients">
   <h2>Input Gradients</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="machine learning primer" data-tertiary="input gradients" data-type="indexterm" id="id664"/><a contenteditable="false" data-primary="input gradients" data-type="indexterm" id="id665"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning the logic of DNA" data-tertiary="input gradients" data-type="indexterm" id="id666"/><a contenteditable="false" data-primary="saliency maps" data-type="indexterm" id="id667"/>Input gradients provide a faster way to generate a saliency map, summarizing which parts of the sequence most influence the model’s predictions. Conceptually, they are the derivatives of the model’s output with respect to its input features. If you’ve trained neural networks before, you’ve already encountered gradients—typically computed with respect to model parameters to guide weight updates.</p>
<p>Input gradients follow the same principle, but they shift the focus from parameters to the input itself. By calculating how the output changes in response to tiny perturbations at each input position, we can assess the model’s sensitivity. For DNA sequences, this means identifying which bases have the greatest influence on predictions like TF binding. A large gradient at a given base suggests that altering it would significantly impact the model’s output—signaling that the base is important.
   </p>
   <p>Concretely, for a TF binding prediction:
   </p>
   <ul class="simple">
    <li><p>A <em>large negative gradient</em> at a base suggests that changing it would significantly lower the binding probability, perhaps disrupting the motif the TF needs to fit properly.
     </p>
    </li>
    <li><p>A <em>large positive gradient</em> suggests that changing it would significantly increase the binding probability, maybe by strengthening an existing motif or creating a new one, thus improving the physical binding affinity between the DNA and the TF.
     </p>
    </li>
   </ul>
<div data-type="note" epub:type="note"><h6>Note</h6>   
  <p>What does “making a small change to the input” mean when the input is a one-hot encoded DNA sequence, rather than a continuous scalar value? Each DNA sequence, each base (A, T, C, G) is generally represented as a binary vector (e.g., <code>[1, 0, 0, 0]</code> for <code>A</code>). During gradient calculation, however, we treat these vectors as if they could vary continuously—allowing fractional values like <code>[0.9, 0.1, 0, 0]</code>. While such fractional bases aren’t biologically meaningful, this mathematical abstraction lets us compute gradients and gain insights into which positions are influential for <span class="keep-together">predictions</span>.
   </p></div>
   <p>You can think of input gradients as a faster but more approximate alternative to in silico saturation mutagenesis. Input gradients provide a general idea of important regions, whereas saturation mutagenesis directly tests every possible mutation’s effect but is computationally expensive.
   </p>
   <p>This concludes the biology and machine learning primers. Now let’s dive into exploring and modeling the data to predict transcription factor binding in DNA sequences.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html6" data-type="indexterm" id="id668"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html5" data-type="indexterm" id="id669"/></p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Building a Simple Prototype"><div class="sect1" id="building-a-simple-prototype">
  <h1>Building a Simple Prototype</h1>
  <p><a contenteditable="false" data-primary="binary classification" data-secondary="simple prototype for solving" data-type="indexterm" id="ch03_dna.html16"/><a contenteditable="false" data-primary="CTCF" data-type="indexterm" id="ch03_dna.html17"/><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="building a simple prototype for binary classification" data-type="indexterm" id="ch03_dna.html18"/>The modeling task we’ll tackle in this chapter is a <em>binary classification problem</em>: given a 200-base DNA sequence, can we predict whether it binds to a specific TF called CTCF? Among the 1,500+ TFs in humans, CTCF stands out for its role in organizing the genome’s 3D architecture—folding DNA into loops and domains that regulate gene activity.
  </p>
  <p>CTCF is also a great first target because its binding behavior is relatively easy to model. It recognizes a well-characterized, highly conserved motif with strong sequence specificity, meaning its binding sites are more predictable than those of many other TFs. That makes CTCF an ideal entry point for building and interpreting sequence-based models of TF binding. Later, we will expand our scope to predict the binding of 10 different TFs.</p>
  <p>As with other chapters in this book, we’ll begin by exploring the dataset, building a simple prototype model, and then iteratively extending and improving it.</p>
   <section data-type="sect2" data-pdf-bookmark="Building a Dataset"><div class="sect2" id="building-a-dataset">
    <h2>Building a Dataset</h2>
   <p>The dataset we will use looks like <a data-type="xref" href="#dna-data-task">Figure 3-5</a>.
   </p>
   <figure><div id="dna-data-task" class="figure">
    <img alt="" src="assets/dlfb_0305.png" width="600" height="275"/>
    <h6><span class="label">Figure 3-5. </span>The input dataset consists of DNA sequences, each 200 bases long, with an associated binary label indicating whether the protein CTCF binds to it.
    </h6>
   </div></figure>
   <p>This task is inspired by one of the evaluation challenges presented in a recent 2024 paper preprint,<sup><a data-type="noteref" id="id670-marker" href="ch03.html#id670">3</a></sup> which sourced the dataset from a 2023 genomics interpretation study.<sup><a data-type="noteref" id="id671-marker" href="ch03.html#id671">4</a></sup>
   </p>

<section data-type="sect3" data-pdf-bookmark="Loading the Labeled Sequences"><div class="sect3" id="loading-the-labeled-sequences">
    <h3>Loading the Labeled Sequences</h3>

    <p><a contenteditable="false" data-primary="dataset" data-secondary="for binary classification problem" data-tertiary="loading the labeled sequences" data-type="indexterm" id="ch03_dna.html19"/>We start by examining the training dataset:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.context</code> <code class="kn">import</code> <code class="n">assets</code>

<code class="n">train_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">assets</code><code class="p">(</code><code class="s2">"dna/datasets/CTCF_train_sequences.csv"</code><code class="p">))</code>
<code class="n">train_df</code>
</pre>

<p>Output:</p>
        <pre data-type="programlisting">                  sequence  label transcription_factor subset
0      TACCACATGAGTTCTC...      1                 CTCF  train
1      CATCAACACTCGTGCG...      0                 CTCF  train
2      GCACACAGCGCAGGAA...      1                 CTCF  train
...                    ...    ...                  ...    ...
61080  CCTCCCTCCCATCCCC...      1                 CTCF  train
61081  CAGGAATGCACCGGAA...      0                 CTCF  train
61082  AAAACAGAAACTGAAA...      0                 CTCF  train

[61083 rows x 4 columns]
</pre>
       

        <p>The two classes appear fairly balanced (equally represented) in the training dataset, so we won’t have to do any rebalancing by downsampling the majority class here:
    </p>

    <pre data-type="programlisting" data-code-language="python"><code class="n">train_df</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>
</pre>
   
<p>Output:</p>
<pre data-type="programlisting">label
1    30545
0    30538
Name: count, dtype: int64
</pre>
    
<p>To use the DNA sequences numerically, we need to convert them into one-hot format. The function <code>dna_to_one_hot</code> performs this mapping:</p>
    
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="k">def</code> <code class="nf">dna_to_one_hot</code><code class="p">(</code><code class="n">dna_sequence</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>
  <code class="sd">"""Convert DNA into a one-hot encoded format with channel ordering ACGT."""</code>
  <code class="n">base_to_one_hot</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"A"</code><code class="p">:</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code>
    <code class="s2">"C"</code><code class="p">:</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code>
    <code class="s2">"G"</code><code class="p">:</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code>
    <code class="s2">"T"</code><code class="p">:</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
    <code class="s2">"N"</code><code class="p">:</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>  <code class="c1"># N represents any unknown or ambiguous base.</code>
  <code class="p">}</code>
  <code class="n">one_hot_encoded</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">base_to_one_hot</code><code class="p">[</code><code class="n">base</code><code class="p">]</code> <code class="k">for</code> <code class="n">base</code> <code class="ow">in</code> <code class="n">dna_sequence</code><code class="p">])</code>
  <code class="k">return</code> <code class="n">one_hot_encoded</code>
</pre>
       
 <p>Let’s see what the one-hot encoding looks like on a sample DNA sequence, “AAACGT”:</p>

 <pre data-type="programlisting" data-code-language="python"><code class="n">dna_to_one_hot</code><code class="p">(</code><code class="s2">"AAACGT"</code><code class="p">)</code></pre>
 
 <p>Output:</p>
<pre data-type="programlisting">array([[1, 0, 0, 0],
       [1, 0, 0, 0],
       [1, 0, 0, 0],
       [0, 1, 0, 0],
       [0, 0, 1, 0],
       [0, 0, 0, 1]])
</pre>

   <p>We can apply this converter to the entire training dataset to generate the numerical training data <code>x_train</code> as follows:</p>
  
        <pre data-type="programlisting" data-code-language="python"><code class="n">x_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">dna_to_one_hot</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">train_df</code><code class="p">[</code><code class="s2">"sequence"</code><code class="p">]])</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">train_df</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">]</code>
</pre>
       
<p>Here, <code>y_train</code> contains the binary target labels: <code>0</code> means the sequence does not bind CTCF, and <code>1</code> means the sequence does bind CTCF.</p>

<p>The dataset loading code for this problem is fairly straightforward, but we can wrap it into a convenient function called <code>load_dataset</code> for cleaner use:<a contenteditable="false" data-primary="" data-startref="ch03_dna.html19" data-type="indexterm" id="id672"/></p> 
  
          <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">load_dataset</code><code class="p">(</code><code class="n">sequence_db</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">]:</code>
  <code class="sd">"""Load sequences and labels from a CSV into numpy arrays."""</code>
  <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">sequence_db</code><code class="p">)</code>
  <code class="k">return</code> <code class="p">{</code>
    <code class="s2">"labels"</code><code class="p">:</code> <code class="n">df</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">()[:,</code> <code class="kc">None</code><code class="p">],</code>
    <code class="s2">"sequences"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">dna_to_one_hot</code><code class="p">(</code><code class="n">seq</code><code class="p">)</code> <code class="k">for</code> <code class="n">seq</code> <code class="ow">in</code> <code class="n">df</code><code class="p">[</code><code class="s2">"sequence"</code><code class="p">]]),</code>
  <code class="p">}</code>
</pre>
     
    
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Converting the Data to a TensorFlow Dataset"><div class="sect3" id="convert-the-data-to-a-tensorflow-dataset">
    <h3>Converting the Data to a TensorFlow Dataset</h3>
    <p><a contenteditable="false" data-primary="dataset" data-secondary="for binary classification problem" data-tertiary="converting data to TensorFlow dataset" data-type="indexterm" id="ch03_dna.html20"/>Next, we convert the training data into a TensorFlow dataset. This format makes it easy to efficiently iterate over batches during model training, especially when shuffling and repeating the data for multiple epochs:
    </p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">convert_to_tfds</code><code class="p">(</code>
  <code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code>
<code class="p">):</code>
  <code class="sd">"""Convert DNA sequences and labels to a TensorFlow dataset."""</code>
  <code class="n">ds</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">dataset</code><code class="p">)</code>
  <code class="k">if</code> <code class="n">is_training</code><code class="p">:</code>
    <code class="n">ds</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">buffer_size</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">]))</code>
    <code class="n">ds</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">repeat</code><code class="p">()</code>
  <code class="n">batch_size</code> <code class="o">=</code> <code class="n">batch_size</code> <code class="ow">or</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">])</code>
  <code class="n">ds</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code>
  <code class="n">ds</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">AUTOTUNE</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">ds</code>
</pre>
       
      
     
    
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>

<code class="n">train_ds</code> <code class="o">=</code> <code class="n">convert_to_tfds</code><code class="p">(</code>
  <code class="n">load_dataset</code><code class="p">(</code><code class="n">assets</code><code class="p">(</code><code class="s2">"dna/datasets/CTCF_train_sequences.csv"</code><code class="p">)),</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
  <code class="n">is_training</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>
</pre>
       
<p>In the preceding code, we created the training dataset <code>train_ds</code> with batching, shuffling, and repetition enabled. Let’s sanity-check by pulling one batch from the dataset and inspecting its shape and contents:</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_ds</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">())</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Batch sequence shape: </code><code class="si">{</code><code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Batch sequence instances: </code><code class="si">{</code><code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">][:</code><code class="mi">3</code><code class="p">,:</code><code class="mi">3</code><code class="p">,]</code><code class="si">}</code><code class="s1">...'</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Batch labels shape: </code><code class="si">{</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Batch labels instances: </code><code class="si">{</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">][:</code><code class="mi">3</code><code class="p">,]</code><code class="si">}</code><code class="s1">...'</code><code class="p">)</code>
</pre>

<p class="pagebreak-before">Output:</p>
<pre data-type="programlisting">Batch sequence shape: (32, 200, 4)
Batch sequence instances: [[[0 1 0 0]
  [0 0 1 0]
  [0 0 1 0]]

 [[1 0 0 0]
  [1 0 0 0]
  [0 1 0 0]]

 [[1 0 0 0]
  [0 1 0 0]
  [0 1 0 0]]]...
Batch labels shape: (32, 1)
Batch labels instances: [[0]
 [1]
 [0]]...
</pre>
       
<p>This looks sensible. We see the data has shape (<code>32</code>, <code>200</code>, <code>4</code>), indicating a batch size of <code>32</code>, sequence length of 200, and 4 channels per DNA base as expected. The labels have shape (<code>32</code>, <code>1</code>) since each label is a simple binary <code>0</code> or <code>1</code>.</p>
<p>For validation, since the dataset is smaller, we can structure it as one single big batch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">valid_ds</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="n">assets</code><code class="p">(</code><code class="s2">"dna/datasets/CTCF_valid_sequences.csv"</code><code class="p">))</code>
</pre>

<p>With our data now in the correct format, we are ready to build and train our first simple model<a contenteditable="false" data-primary="" data-startref="ch03_dna.html20" data-type="indexterm" id="id673"/>.</p>
     
    </div></section>
   </div></section>
   <section data-type="sect2" data-pdf-bookmark="Defining a Simple Convolutional Model"><div class="sect2" id="defining-a-simple-convolutional-model">
    <h2>Defining a Simple Convolutional Model</h2>
    <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-type="indexterm" id="ch03_dna.html21"/><a contenteditable="false" data-primary="dataset" data-secondary="for binary classification problem" data-tertiary="defining a simple convolutional model" data-type="indexterm" id="ch03_dna.html22"/>Next, we define a simple CNN composed of two 1D convolutional layers, followed by flattening and fully connected (dense) layers:
    </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ConvModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Basic CNN model for binary sequence classification."""</code>

  <code class="n">conv_filters</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code>  <code class="c1"># Number of filters for conv layers.</code>
  <code class="n">kernel_size</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">10</code><code class="p">,)</code>  <code class="c1"># Kernel size for 1D conv layers.</code>
  <code class="n">dense_units</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">128</code>  <code class="c1"># Units in first dense fully-connected layer.</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="c1"># First convolutional layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">conv_filters</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,))</code>

    <code class="c1"># Second convolutional layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">conv_filters</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,))</code>

    <code class="c1"># Flatten the values before passing them to the dense layers.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="o">-</code><code class="mi">1</code><code class="p">))</code>

    <code class="c1"># First dense layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>

    <code class="c1"># Second dense layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code> <code class="o">//</code> <code class="mi">2</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>

    <code class="c1"># Output layer (single unit for binary classification).</code>
    <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
</pre>
       
<p>This model architecture is fairly “no frills” but should already be able to capture local patterns in DNA sequences. We can instantiate our model like this:</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">ConvModel</code><code class="p">()</code>
</pre>
       
<p>To initialize model parameters in JAX, we need to provide a dummy input tensor that matches the expected shape of the model’s input. Although we could use an actual batch from our dataset, it is common and simpler to use a tensor of ones with batch size 1 and the same shape as a single-encoded DNA sequence. Importantly, the batch size used for this dummy input does not affect the model initialization—JAX initializes parameters based on the shape of an individual input sample (excluding the batch dimension). This means the model can later be trained or used for inference with any batch size.</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">jax</code>
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>

<code class="n">dummy_input</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ones</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="o">*</code><code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">][</code><code class="mi">1</code><code class="p">,]</code><code class="o">.</code><code class="n">shape</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">dummy_input</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>

<code class="n">rng_init</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">variables</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">)</code>
<code class="n">params</code> <code class="o">=</code> <code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">]</code>
</pre>
  
<p>Output:</p>
        <pre data-type="programlisting">(1, 200, 4)
</pre>

<p>This dummy input allows JAX to infer the shapes of all model parameters, and the random key <code>rng_init</code> seeds any stochastic initialization, ensuring reproducibility.</p>

<section data-type="sect3" data-pdf-bookmark="Examining Model Tensor Shapes"><div class="sect3" id="examining-model-tensor-shapes">
     <h3>Examining Model Tensor Shapes</h3>
     <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="examining model tensor shapes" data-type="indexterm" id="ch03_dna.html23"/><a contenteditable="false" data-primary="tensor shapes, examining" data-type="indexterm" id="ch03_dna.html24"/>Understanding and keeping track of tensor shapes is a crucial part of machine learning. Always make it a habit to verify the shapes of your data as it flows through the model.
     </p>
     <div data-type="tip"><h6>Tip</h6><p>As a practical exercise, try adding <code>print(x.shape)</code> statements at various points inside the model’s <code>__call__</code> method and then rerun the <code>model.init</code> step to observe how the shapes change as data flows through the model layers.
     </p></div>
     <p>Here are some key operations in the model that change tensor shapes:</p>
<ul> 
  <li><p>Convolutional layers (<code>nn.Conv</code>): These layers can modify the channel dimension. For example, our input starts with four channels (DNA bases), but the first convolution increases this to 64 channels, effectively learning 64 different sequence features or motifs. Additionally, the convolution’s <code>padding</code> option affects the sequence length dimension: </p>
    <ul> 
      <li><code>adding='SAME'</code> preserves the sequence length.</li> 
      <li><code>padding='VALID'</code> reduces it depending on the kernel size. Try switching between these to see the impact on the sequence length.</li> 
    </ul> 
  </li> 
  <li><p>Max pooling layers (<code>nn.max_pool</code>): These reduce the sequence length by downsampling. In our model, each max pooling layer halves the length, from 200 → 100 → 50 bases. To reduce the spatial axis more aggressively (e.g., by a factor of 5 each time), adjust the <code>window_shape</code> and <code>strides</code> arguments accordingly (typically, these are set to the same value to avoid overlapping windows and simplify downsampling).</p></li> 
  <li><p>Flattening (<code>reshape</code>): Before passing data to dense layers, the tensor is reshaped from <code>(batch_size, sequence_length, channels)</code> to <code>(batch_size, flattened_features)</code>. This collapses the spatial and channel dimensions into one long vector per example, preparing it for fully connected layers. </p>
  </li> 
</ul>
     <p>Once the model parameters are initialized, you can inspect them to confirm the layer structure and shapes. Check the parameter keys (layer names) with:
     </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="n">params</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code>
</pre>
   

<p>Output:</p>
<pre data-type="programlisting">dict_keys(['Conv_0', 'Conv_1', 'Dense_0', 'Dense_1', 'Dense_2'])
</pre>
        
<p class="pagebreak-before">Then, inspect each layer’s kernel shape to verify the expected parameter dimensions:</p>
    
<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">layer_name</code> <code class="ow">in</code> <code class="n">params</code><code class="o">.</code><code class="n">keys</code><code class="p">():</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Layer </code><code class="si">{</code><code class="n">layer_name</code><code class="si">}</code><code class="s1"> param shape: </code><code class="si">{</code><code class="n">params</code><code class="p">[</code><code class="n">layer_name</code><code class="p">][</code><code class="s2">"kernel"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
</pre>
   
<p>Output:</p>
<pre data-type="programlisting">Layer Conv_0 param shape: (10, 4, 64)
Layer Conv_1 param shape: (10, 64, 64)
Layer Dense_0 param shape: (3200, 128)
Layer Dense_1 param shape: (128, 64)
Layer Dense_2 param shape: (64, 1)
</pre>
        
       
      
     <p>Some notes on these shapes:</p>
     <ul>
      <li>For convolutional layers, the parameter shape is <code>(kernel_size, input_channels, output_channels)</code>. For example, <code>Conv_0</code> has kernel size 10, input channels 4 (DNA bases), and outputs 64 feature maps.</li>
      <li>Dense layers have shapes (<code>input_features</code>, <code>output_units</code>). For instance, <code>Dense_0</code> maps 3200 flattened features (50 sequence length * 64) to 128 units.</li>
     </ul>
     <p>With the model initialized and the parameter shapes explored, let’s start setting up our training loop.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html24" data-type="indexterm" id="id674"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html23" data-type="indexterm" id="id675"/></p>
    
    </div></section> 
 <section data-type="sect3" data-pdf-bookmark="Making predictions with the model"><div class="sect3" id="making-predictions-with-the-model">
 <h3>Making predictions with the model</h3>
     
     <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="making predictions with the model" data-type="indexterm" id="id676"/>We can actually already obtain predictions from the model using the randomly initialized parameters, though the predictions will be random. We make predictions by calling <code>model.apply</code> on a batch of sequences:
     </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="n">logits</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">])</code>

<code class="c1"># Apply sigmoid to convert logits to probabilities.</code>
<code class="n">probs</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">logits</code><code class="p">)</code>

<code class="c1"># Print just the first few predictions.</code>
<code class="nb">print</code><code class="p">(</code><code class="n">probs</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5</code><code class="p">])</code>
</pre>
  
<p>Output:</p>
<pre data-type="programlisting">[[0.48703438]
 [0.49615338]
 [0.48638064]
 [0.4973824 ]
 [0.48106888]]
</pre>

<p>Since the model is untrained, predicted probabilities will be around around 0.5, reflecting pure guesswork from our randomly-parameterized model.</p>
     </div></section>
<section data-type="sect3" data-pdf-bookmark="Defining a loss function"><div class="sect3" id="defining-a-loss-function">
<h3>Defining a loss function</h3>
     <p><a contenteditable="false" data-primary="binary cross-entropy loss function" data-type="indexterm" id="id677"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="defining a loss function" data-type="indexterm" id="id678"/><a contenteditable="false" data-primary="loss function" data-secondary="defining binary cross-entropy loss function" data-type="indexterm" id="id679"/>Let’s now define a  binary cross-entropy loss function using the <code>optax</code> library that we can use to train our model parameters:
     </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">optax</code>

<code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
  <code class="sd">"""Make predictions on batch and compute binary cross entropy loss."""</code>
  <code class="n">logits</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">])</code>
  <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">sigmoid_binary_cross_entropy</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
  <code class="k">return</code> <code class="n">loss</code>
</pre>
        
       
      
     
     <p>We can use it to compute the loss for the initial batch:
     </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="n">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
</pre>
 
<p>Output:</p>
<pre data-type="programlisting">Array(0.69774264, dtype=float32)
</pre>
        
<p>This gives a baseline loss value before training. As the model learns, this loss should decrease substantially.</p>
     <div data-type="note" epub:type="note"><h6>Note</h6>
      <p>Why use binary cross-entropy loss? In this chapter, we’re predicting whether a TF binds to a given DNA sequence—a classic binary classification task. Binary cross-entropy is the standard loss function for this setting: it measures how well the model’s predicted probabilities align with the true binary labels.</p>
<p>It penalizes confident but incorrect predictions more heavily, encouraging well-calibrated outputs near 0 or 1. You can also think of it as a signal reconstruction problem: the model tries to approximate a hidden binary signal, and cross-entropy imposes a sharp cost for noisy or off-target estimates.</p>
     </div>
     </div></section>
     <section data-type="sect3" data-pdf-bookmark="Defining the TrainState"><div class="sect3" id="defining-the-trainState">
     <h3>Defining the TrainState</h3>
     
     <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="defining the TrainState" data-type="indexterm" id="id680"/><a contenteditable="false" data-primary="TrainState class (Flax)" data-type="indexterm" id="id681"/>To start training the model, we first need to define the optimizer. We’ll use Adam with a learning rate of 1e-3, which is a common default value that tends to work well across diverse problems:
     </p>

<pre data-type="programlisting" data-code-language="python"><code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.001</code>

<code class="n">tx</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">)</code>
</pre>
    
      <p>With this, we now have all the components to initialize the training state. For this, we’ll use Flax’s <code>TrainState</code> class, which is a container that bundles all the important objects for training (the model, parameters, and optimizer):</p>
       
<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">flax.training.train_state</code> <code class="kn">import</code> <code class="n">TrainState</code>

<code class="n">state</code> <code class="o">=</code> <code class="n">TrainState</code><code class="o">.</code><code class="n">create</code><code class="p">(</code><code class="n">apply_fn</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">,</code> <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">)</code>
</pre>

<p>For convenience, let’s define a function to create the train state:</p>
        
<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">rng</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">TrainState</code><code class="p">:</code>
  <code class="n">variables</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">TrainState</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
    <code class="n">apply_fn</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code> <code class="n">tx</code><code class="o">=</code><code class="n">tx</code>
  <code class="p">)</code>
  <code class="k">return</code> <code class="n">state</code>
</pre>
      </div></section>
         <section data-type="sect3" data-pdf-bookmark="Defining a single training step"><div class="sect3" id="defining-a-single-training-step">
     <h3>Defining a single training step</h3>
     <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="defining a single training step" data-type="indexterm" id="id682"/><a contenteditable="false" data-primary="training" data-secondary="for learning the logic of DNA" data-tertiary="defining a single training step" data-type="indexterm" id="id683"/>Finally, putting everything together, we can write a function to run one training iteration, which performs these steps:
     </p>
     <dl>
     <dt>1. Forward pass</dt>
      <dd>
       <p>Takes a batch of data, makes model predictions, and computes loss based on the current parameters
       </p>
      </dd>
      <dt>2. Backwards pass:</dt>
      <dd>
       <p>Computes gradients of the loss with respect to the parameters
       </p>
      </dd>
      <dt>3. Update</dt>
      <dd>
       <p>Using the computed gradients, updates the parameters to minimize the model’s loss
       </p>
      </dd>
     </dl>
     <p>
      These steps happen in the <code>train_step</code> function:
     </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
  <code class="sd">"""Run single training step to compute gradients and update model params."""</code>
  <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
  <code class="n">loss</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">loss</code>
</pre>
        
       
      
     
     <p>
      Let’s run one training step:
     </p>
    
      
       
        
         <pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">,</code> <code class="n">loss</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
</pre>
        
<p>If our setup is working well, the training loss should already be lower on this batch. Let’s check:
     </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="n">calculate_loss</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
</pre>
     

<p>Output:</p>
<pre data-type="programlisting">Array(0.63649833, dtype=float32)
</pre>
        
<p>And indeed, the loss is lower than it was before a <code>train_step</code> was run. With the model and training loop ready, let’s set up a full training run.
     </p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Training the Simple Model"><div class="sect3" id="training-the-simple-model">
    <h3>Training the Simple Model</h3>
    <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="training the simple model" data-type="indexterm" id="ch03_dna.html25"/><a contenteditable="false" data-primary="training" data-secondary="for learning the logic of DNA" data-tertiary="training the simple model" data-type="indexterm" id="ch03_dna.html26"/>Let’s now run the preceding <code>train_step</code> many times in order to optimize the model. Here, we train for 500 steps and periodically evaluate on the validation set:
    </p>
    
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tqdm</code>

<code class="c1"># Reinitialize the model state to ensure we start fresh each time cell is run.</code>
<code class="n">rng_init</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">state</code> <code class="o">=</code> <code class="n">create_train_state</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code><code class="p">)</code>

<code class="c1"># Keep track of both the training and validation set losses.</code>
<code class="n">train_losses</code><code class="p">,</code> <code class="n">valid_losses</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>
<code class="n">train_batches</code> <code class="o">=</code> <code class="n">train_ds</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">()</code>

<code class="c1"># We use tqdm, which is a progress bar.</code>
<code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">500</code><code class="p">)):</code>
  <code class="n">batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_batches</code><code class="p">)</code>
  <code class="n">state</code><code class="p">,</code> <code class="n">loss</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
  <code class="n">train_losses</code><code class="o">.</code><code class="n">append</code><code class="p">({</code><code class="s2">"step"</code><code class="p">:</code> <code class="n">step</code><code class="p">,</code> <code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()})</code>

  <code class="c1"># Compute loss on the entire validation set occasionally (every 100 steps).</code>
  <code class="k">if</code> <code class="n">step</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
    <code class="n">valid_loss</code> <code class="o">=</code> <code class="n">calculate_loss</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="n">valid_ds</code><code class="p">)</code>
    <code class="n">valid_losses</code><code class="o">.</code><code class="n">append</code><code class="p">({</code><code class="s2">"step"</code><code class="p">:</code> <code class="n">step</code><code class="p">,</code> <code class="s2">"loss"</code><code class="p">:</code> <code class="n">valid_loss</code><code class="o">.</code><code class="n">item</code><code class="p">()})</code>

<code class="n">losses</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">(</code>
  <code class="p">[</code>
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">train_losses</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">),</code>
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">valid_losses</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">),</code>
  <code class="p">]</code>
<code class="p">)</code>
</pre>
       
<p>In <a data-type="xref" href="#training-simplest-model">Figure 3-6</a>, we can plot the training and validation loss curves resulting from this run:
    </p>
   
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.metric_plots</code> <code class="kn">import</code> <code class="n">DEFAULT_SPLIT_COLORS</code>

<code class="n">sns</code><code class="o">.</code><code class="n">lineplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">losses</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"step"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"loss"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">palette</code><code class="o">=</code><code class="n">DEFAULT_SPLIT_COLORS</code>
<code class="p">);</code>
</pre>
       
<figure><div id="training-simplest-model" class="figure">
       <img alt="" src="assets/dlfb_0306.png" width="600" height="453"/>
       <h6><span class="label">Figure 3-6. </span>Training and validation loss over learning steps. Both decrease, indicating the model is learning signal in the data over time. Note that training loss curve is comparatively noisier due to it being computed on a small batch of the data at each step (rather than the full training set), which introduces variability.
       </h6>
      </div></figure>
     
    <p>Reaching this stage—where you have a working model, a dataset, and training with decreasing loss—is a major milestone. The rest of this chapter focuses on improving and extending this foundation.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html26" data-type="indexterm" id="id684"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html25" data-type="indexterm" id="id685"/></p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Sanity-checking the model"><div class="sect3" id="sanity-checking-the-model">
    <h3>Sanity-checking the model</h3>
    <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defining a simple convolutional model" data-tertiary="sanity-checking the model" data-type="indexterm" id="ch03_dna.html27"/>Before we do anything more complicated, we should first check that the model has learned something sensible. One simple check is verifying that the trained model behaves as expected on known DNA motifs. For example, from an online search, we can see that the CTCF transcription factor is known to prefer binding DNA sequences containing motifs similar <code>CCACCAGGGGGCGC</code>. Let’s construct the 200-base-long DNA string containing repeats of this motif and convert it to the one-hot encoded format that our model expects:
    </p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">ctcf_motif_dna</code> <code class="o">=</code> <code class="s2">"CCACCAGGGGGCGC"</code> <code class="o">*</code> <code class="mi">14</code> <code class="o">+</code> <code class="s2">"AAAA"</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Length of CTCF motif-filled DNA string:"</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">ctcf_motif_dna</code><code class="p">))</code>

<code class="c1"># We add the None here as a batch axis, since our model expects batched input.</code>
<code class="n">ctcf_input</code> <code class="o">=</code> <code class="n">dna_to_one_hot</code><code class="p">(</code><code class="n">ctcf_motif_dna</code><code class="p">)[</code><code class="kc">None</code><code class="p">,</code> <code class="p">:]</code>
<code class="n">ctcf_input</code><code class="o">.</code><code class="n">shape</code>
</pre>
    

<p>Output:</p>
<pre data-type="programlisting">Length of CTCF motif-filled DNA string: 200
(1, 200, 4)
</pre>
       
<p>We expect that the model will predict a very high probability of CTCF binding this sequence, since it’s packed with the relevant motif. Let’s check:</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">ctcf_input</code><code class="p">))</code>
</pre>
  

<p>Output:</p>
<pre data-type="programlisting">Array([[0.9994091]], dtype=float32)
</pre>
       
<p>And this is indeed the case—the predicted probability of binding is very close to 1. This means the model has learned to identify this motif in the DNA sequence and associate it with CTCF binding.
    </p>
    <p>Conversely, if we construct some pseudorandom DNA strings, the model should predict a relatively low probability of CTCF binding them:
    </p>

    <pre data-type="programlisting" data-code-language="python"><code class="n">random_dna_strings</code> <code class="o">=</code> <code class="p">[</code>
  <code class="s2">"A"</code> <code class="o">*</code> <code class="mi">200</code><code class="p">,</code>
  <code class="s2">"C"</code> <code class="o">*</code> <code class="mi">200</code><code class="p">,</code>
  <code class="s2">"G"</code> <code class="o">*</code> <code class="mi">200</code><code class="p">,</code>
  <code class="s2">"T"</code> <code class="o">*</code> <code class="mi">200</code><code class="p">,</code>
  <code class="s2">"ACGTACGT"</code> <code class="o">*</code> <code class="mi">25</code><code class="p">,</code>
  <code class="s2">"TCGATCGT"</code> <code class="o">*</code> <code class="mi">25</code><code class="p">,</code>
  <code class="s2">"TATACGCG"</code> <code class="o">*</code> <code class="mi">25</code><code class="p">,</code>
  <code class="s2">"CAGGCAGG"</code> <code class="o">*</code> <code class="mi">25</code><code class="p">,</code>
<code class="p">]</code>

<code class="n">probabilities</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">random_dna_string</code> <code class="ow">in</code> <code class="n">random_dna_strings</code><code class="p">:</code>
  <code class="n">random_dna_input</code> <code class="o">=</code> <code class="n">dna_to_one_hot</code><code class="p">(</code><code class="n">random_dna_string</code><code class="p">)[</code><code class="kc">None</code><code class="p">,</code> <code class="p">:]</code>

  <code class="n">probabilities</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
    <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">random_dna_input</code><code class="p">))[</code><code class="mi">0</code><code class="p">]</code>
  <code class="p">)</code>

<code class="n">probabilities</code>
</pre>
 
<p>Output:</p>
<pre data-type="programlisting">[Array([0.00025924], dtype=float32),
 Array([6.9913156e-05], dtype=float32),
 Array([6.1404e-05], dtype=float32),
 Array([2.6038255e-05], dtype=float32),
 Array([0.10472302], dtype=float32),
 Array([0.00381694], dtype=float32),
 Array([0.01843039], dtype=float32),
 Array([0.00171606], dtype=float32)]
</pre>
       
      
     
    
    <p>As expected, these probabilities look relatively low, meaning that the CTCF protein is not likely to bind these sequences of random DNA. This completes a basic sanity check of our approach, but let’s dig a bit deeper into what this model has already learned about<a contenteditable="false" data-primary="" data-startref="ch03_dna.html27" data-type="indexterm" id="id686"/> DNA sequences<a contenteditable="false" data-primary="" data-startref="ch03_dna.html22" data-type="indexterm" id="id687"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html21" data-type="indexterm" id="id688"/>.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html18" data-type="indexterm" id="id689"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html17" data-type="indexterm" id="id690"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html16" data-type="indexterm" id="id691"/>
    </p>
   </div></section>
 </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Increasing Complexity"><div class="sect1" id="increasing-model-complexity">
  <h1>Increasing Complexity</h1>
  <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="increasing the complexity of the model" data-type="indexterm" id="ch03_dna.html28"/>In this section, we introduce two important extensions to our modeling approach.
  </p>
  <p>First, we focus on <em>model interpretation</em>. We’ll apply two techniques from the earlier machine learning primer—in silico mutagenesis (ISM) and input gradients—to better understand what the model has learned. These methods produce <em>contribution scores</em> (or <em>saliency maps</em>) that assign an importance value to each base in the DNA sequence, indicating how much that base influences the model’s prediction of TF binding.</p>
  <p>Second, we’ll expand the scope of our prediction task. Instead of predicting binding for just one transcription factor (CTCF), we train models for <em>all 10 transcription factors</em> in the dataset. This allows us to explore how model performance varies across TFs and how motif preferences differ between them.</p>
  <p>Together, these steps deepen both our understanding of the model’s behavior and the complexity of the biological task it’s modeling. After this, we’ll turn our attention to improving the model architecture itself.</p>
  <section data-type="sect2" data-pdf-bookmark="Conducting In Silico Mutagenesis"><div class="sect2" id="in-silico-mutagenesis">
   <h2>Conducting In Silico Mutagenesis</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="increasing the complexity of the model" data-tertiary="conducting in silico mutagenesis" data-type="indexterm" id="ch03_dna.html29"/><a contenteditable="false" data-primary="in silico saturation mutagenesis (ISM)" data-secondary="conducting" data-type="indexterm" id="ch03_dna.html30"/>Recall from the introduction that ISM is a technique in which each base in a DNA sequence is systematically mutated to all possible alternative bases one at a time, and the effect of each mutation on a given output (in this example, CTCF binding probability) is quantified. This allows us to identify which regions are important to the output—unimportant regions can be freely mutated without impacting predictions, whereas important regions significantly affect the probability of TF binding.
   </p>
   <p>Before making all possible mutations, let’s first check the effect of making just a single mutation. We’ll start by identifying a DNA sequence in the validation set that binds the CTCF protein (i.e., has a label of <code>1</code>):
   </p>
  
<pre data-type="programlisting" data-code-language="python"><code class="c1"># The first positive example of a sequence that binds the transcription factor.</code>
<code class="n">first_positive_index</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">valid_ds</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code>

<code class="n">original_sequence</code> <code class="o">=</code> <code class="n">valid_ds</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">][</code><code class="n">first_positive_index</code><code class="p">]</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'This sequence has label: </code><code class="si">{</code><code class="n">valid_ds</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">][</code><code class="mi">4</code><code class="p">]</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
</pre>
      
<p class="pagebreak-before">Output:</p>
<pre data-type="programlisting">This sequence has label: [1]
</pre>
      
<p>Next, let’s examine what the model predicts for this unmodified sequence:
   </p>
  
<pre data-type="programlisting" data-code-language="python"><code class="n">pred</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code>
  <code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">original_sequence</code><code class="p">[</code><code class="kc">None</code><code class="p">,</code> <code class="p">:])</code>
<code class="p">)</code>
<code class="n">pred</code>
</pre>
      
<p>Output:</p>
<pre data-type="programlisting">Array([[0.9581409]], dtype=float32)
</pre>
      
     
    
   
   <p>The original sequence has a predicted binding probability of 95.8%.
   </p>
  <p>Now let’s create a single mutation at position 100. The original base is a <code>G</code> (encoded as <code>[0, 0, 1, 0]</code>), and we change it to a <code>C</code> (encoded as <code>[0, 1, 0, 0]</code>):</p>
    
<pre data-type="programlisting" data-code-language="python"><code class="n">sequence</code> <code class="o">=</code> <code class="n">original_sequence</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Original base at index 100: </code><code class="si">{</code><code class="n">sequence</code><code class="p">[</code><code class="mi">100</code><code class="p">]</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="n">sequence</code><code class="p">[</code><code class="mi">100</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Mutated base at index 100: </code><code class="si">{</code><code class="n">sequence</code><code class="p">[</code><code class="mi">100</code><code class="p">]</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>
      
<p>Output:</p>
<pre data-type="programlisting">Original base at index 100: [0 0 1 0]
Mutated base at index 100: [0 1 0 0]
</pre>
      
<p>
    We’ll now run the model again to see if this mutation has a measurable effect on the prediction:
   </p>
  
<pre data-type="programlisting" data-code-language="python"><code class="n">pred_with_mutation</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code>
  <code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">sequence</code><code class="p">[</code><code class="kc">None</code><code class="p">,</code> <code class="p">:])</code>
<code class="p">)</code>
<code class="n">pred_with_mutation</code>
</pre>
      

<p>Output:</p>
<pre data-type="programlisting">Array([[0.93434644]], dtype=float32)
</pre>
      
<p>After the mutation, the model’s predicted binding probability drops to 93.4%, a decrease of over 2.4%. This shows that even a single-base change at a sensitive position can substantially impact the model’s output.
   </p>
   <p>Now that we’ve observed how a single mutation can influence the prediction, let’s extend this approach to systematically mutate every position in the sequence. This will give us a more global view of which bases matter most for the model’s prediction.
   </p>
   <section data-type="sect3" class="less_space pagebreak-before" data-pdf-bookmark="Implementing in silico saturation mutagenesis"><div class="sect3" id="implementing-in-silico-saturation-mutagenesis">
    <h3><a contenteditable="false" data-primary="in silico saturation mutagenesis (ISM)" data-secondary="conducting" data-tertiary="implementing ISM" data-type="indexterm" id="ch03_dna.html31"/>Implementing in silico saturation mutagenesis</h3>
    <p>Here’s the basic plan for our implementation:
    </p>
    <dl>
    <dt>Mutate</dt>
     <dd>
      <p>Start with the original sequence. At each position, change the base to each of the other three possible DNA bases.
      </p>
     </dd>
     <dt>Predict</dt>
     <dd>
      <p>Run the model on each mutated sequence and record the predicted binding probability.
      </p>
     </dd>
     <dt>Aggregate</dt>
     <dd>
      <p>Collect all results to identify which mutations cause meaningful changes.
      </p>
             
      </dd>
    </dl>
<p>We begin by generating all possible single-base mutations:</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">generate_all_mutations</code><code class="p">(</code><code class="n">sequence</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>
  <code class="sd">"""Generate all possible single base mutations of a one-hot DNA sequence."""</code>
  <code class="n">mutated_sequences</code> <code class="o">=</code> <code class="p">[]</code>
  <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">sequence</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]):</code>
    <code class="c1"># At each position, one the four 'mutations' is the original base (no-op).</code>
    <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">):</code>
      <code class="n">mutated_sequence</code> <code class="o">=</code> <code class="n">sequence</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
      <code class="n">mutated_sequence</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code>
      <code class="n">mutated_sequence</code><code class="p">[</code><code class="n">i</code><code class="p">][</code><code class="n">j</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>
      <code class="n">mutated_sequences</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">mutated_sequence</code><code class="p">)</code>

  <code class="n">sequences</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">mutated_sequences</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">sequences</code>


<code class="n">mutated_sequences</code> <code class="o">=</code> <code class="n">generate_all_mutations</code><code class="p">(</code><code class="n">sequence</code><code class="o">=</code><code class="n">original_sequence</code><code class="o">.</code><code class="n">copy</code><code class="p">())</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Shape of mutated sequences: </code><code class="si">{</code><code class="n">mutated_sequences</code><code class="o">.</code><code class="n">shape</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>
      
<p>Output:</p>
<pre data-type="programlisting">Shape of mutated sequences: (800, 200, 4)
</pre>     
     
    
    <p>We now have 800 sequences: four variants for each of the 200 positions. Although only three of those per position are true mutations (since mutating, say, <code>A</code> to an <code>A</code> is a no-op), we include all four to simplify downstream logic.
    </p>
    <p>We can now run predictions on all these mutated sequences in a single batch:
    </p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">preds</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">mutated_sequences</code><code class="p">))</code>

<code class="c1"># Reshape to get the shape (sequence_length, dna_bases).</code>
<code class="n">preds</code> <code class="o">=</code> <code class="n">preds</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">200</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
</pre>
       
      
     
    
    <p class="pagebreak-before">Let’s visualize the predicted binding probabilities for every mutated sequence in <a data-type="xref" href="#mutation-plot-saturated">Figure 3-7</a>:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">preds</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"RdBu_r"</code><code class="p">,</code> <code class="n">yticklabels</code><code class="o">=</code><code class="p">[</code><code class="s2">"A"</code><code class="p">,</code> <code class="s2">"C"</code><code class="p">,</code> <code class="s2">"G"</code><code class="p">,</code> <code class="s2">"T"</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Position in DNA sequence"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"DNA Base"</code><code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="mutation-plot-saturated" class="figure">
       <img alt="" src="assets/dlfb_0307.png" width="600" height="167"/>
       <h6><span class="label">Figure 3-7. </span>All possible variations of a 200-base pair DNA region and their corresponding probabilities of TF binding, represented as a heatmap. The x-axis indicates a position in the DNA sequence, and the y-axis represents each possible DNA base at that position. The value represents the predicted probability of CTCF binding.
       </h6>
      </div></figure>
     
    
    <p>This shows us that:
    </p>
    <ul class="simple">
     <li><p>Most mutations don’t change the prediction much. The model predicts close to the original 95.8% probability of CTCF binding for sequences containing most mutations.
      </p>
     </li>
     <li><p>A central region does appear to significantly affect the binding prediction.
      </p>
     </li>
    </ul>
    <p>But what we really care about is how much each mutation changes the prediction. Let’s subtract the original (unmutated) predicted probability and center the color map at zero in <a data-type="xref" href="#mutation-plot-difference">Figure 3-8</a>:
    </p>
    
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">baseline_pred</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code>
  <code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">original_sequence</code><code class="p">[</code><code class="kc">None</code><code class="p">,</code> <code class="p">:])</code>
<code class="p">)</code>
<code class="n">deltas</code> <code class="o">=</code> <code class="n">preds</code> <code class="o">-</code> <code class="n">baseline_pred</code>

<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">deltas</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">center</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"RdBu_r"</code><code class="p">,</code> <code class="n">yticklabels</code><code class="o">=</code><code class="p">[</code><code class="s2">"A"</code><code class="p">,</code> <code class="s2">"C"</code><code class="p">,</code> <code class="s2">"G"</code><code class="p">,</code> <code class="s2">"T"</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Position in DNA sequence"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"DNA Base"</code><code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="mutation-plot-difference" class="figure">
       <img alt="" src="assets/dlfb_0308.png" width="600" height="165"/>
       <h6><span class="label">Figure 3-8. </span>In silico mutagenesis results showing the change in predicted CTCF binding probability for each possible mutation. Light and dark shades indicate a deviation from the original prediction, with lighter colors showing mutations that increase predicted binding and darker colors showing those that decrease binding.
       </h6>
      </div></figure>

      <p>Now the heatmap is clearer:</p>
      <ul>
        <li><p>Most positions are lighter, meaning their mutations have little effect.</p></li>
        <li><p>A few bases in the center are dark (decreased binding) or light (increased binding), showing meaningful influence.</p></li>
      </ul>
<p>We can quantify mutation effects using a helper function, <code>describe_change</code>.</p>

      <p>It allows us to have a look at the impact of mutating position 100 for all bases:</p>
    
      <pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">):</code>
  <code class="nb">print</code><code class="p">(</code><code class="n">describe_change</code><code class="p">((</code><code class="mi">100</code><code class="p">,</code> <code class="n">i</code><code class="p">),</code> <code class="n">deltas</code><code class="p">,</code> <code class="n">original_sequence</code><code class="p">))</code>
  </pre>

  <p>Output:</p>
<pre data-type="programlisting">
position 100 with G→A (-4.20% decrease)
position 100 with G→C (-2.38% decrease)
position 100 with G→G (0.00% increase)
position 100 with G→T (0.24% increase)
</pre>

    

    <p>Let’s also summarize the overall importance of each position in the DNA sequence by summing the absolute changes across all possible base mutations at each position. The result is visualized in <a data-type="xref" href="#mutation-plot-stacked">Figure 3-9</a>, highlighting which regions the model considers most influential for its prediction:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.dna.inspect</code> <code class="kn">import</code> <code class="n">plot_binding_site</code>

<code class="n">importance</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">abs</code><code class="p">(</code><code class="n">deltas</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">plot_binding_site</code><code class="p">(</code>
  <code class="n">panels</code><code class="o">=</code><code class="p">{</code>
    <code class="s2">"tiles"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="s2">"Deltas"</code><code class="p">,</code> <code class="s2">"values"</code><code class="p">:</code> <code class="n">deltas</code><code class="p">},</code>
    <code class="s2">"line"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="s2">"Importance"</code><code class="p">,</code> <code class="s2">"values"</code><code class="p">:</code> <code class="n">importance</code><code class="p">},</code>
  <code class="p">}</code>
<code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="mutation-plot-stacked" class="figure">
       <img alt="" src="assets/dlfb_0309.png" width="600" height="261"/>
       <h6><span class="label">Figure 3-9. </span>Positional importance of the TF binding motif. The bottom panel is the same as in <a data-type="xref" href="#mutation-plot-difference">Figure 3-8</a>
        with a linegraph superimposed.
       </h6>
      </div></figure>
     
    
    <p>We can identify the most impactful mutations by ranking the values. In this case, the largest <em>increase</em> comes from mutating the base at position 92 with G→C (3.11% increase), and the biggest decrease comes from mutating the base at position 102 with G→T (-20.24% decrease).
    </p>
    <div data-type="note" epub:type="note"><h6>Note</h6><p>This is a great moment to reflect on the biological meaning of these plots. The central region of the sequence likely contains the CTCF binding motif—mutations here have a strong impact on the model’s prediction. In contrast, flanking regions show little effect, indicating they contribute less to binding. Interestingly, most impactful mutations in the core motif tend to reduce predicted binding, suggesting the original sequence already contains a fairly strong CTCF motif that’s difficult to strengthen with single-base changes.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html31" data-type="indexterm" id="id692"/></p></div>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Verifying motif presence"><div class="sect3" id="verifying-motif-presence">
    <h3>Verifying motif presence</h3>
    <p><a contenteditable="false" data-primary="in silico saturation mutagenesis (ISM)" data-secondary="conducting" data-tertiary="verifying motif presence" data-type="indexterm" id="ch03_dna.html32"/>To validate whether the region identified by our model corresponds to a known CTCF binding motif, we can use an external bioinformatics tool. But first, we need to convert the one-hot encoded sequence back into the standard DNA string format. This is done using the <code>one_hot_to_dna</code> function:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">one_hot_to_dna</code><code class="p">(</code><code class="n">one_hot_encoded</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
  <code class="sd">"""Convert one-hot encoded array back to DNA sequence."""</code>
  <code class="n">one_hot_to_base</code> <code class="o">=</code> <code class="p">{</code>
    <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">):</code> <code class="s2">"A"</code><code class="p">,</code>
    <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">):</code> <code class="s2">"C"</code><code class="p">,</code>
    <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">):</code> <code class="s2">"G"</code><code class="p">,</code>
    <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">):</code> <code class="s2">"T"</code><code class="p">,</code>
    <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">):</code> <code class="s2">"N"</code><code class="p">,</code>  <code class="c1"># N represents any unknown or ambiguous base.</code>
  <code class="p">}</code>

  <code class="n">dna_sequence</code> <code class="o">=</code> <code class="s2">""</code><code class="o">.</code><code class="n">join</code><code class="p">(</code>
    <code class="n">one_hot_to_base</code><code class="p">[</code><code class="nb">tuple</code><code class="p">(</code><code class="n">base</code><code class="p">)]</code> <code class="k">for</code> <code class="n">base</code> <code class="ow">in</code> <code class="n">one_hot_encoded</code>
  <code class="p">)</code>
  <code class="k">return</code> <code class="n">dna_sequence</code>
</pre>
       
<p>which we can use on our sequence:</p>

<pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">one_hot_to_dna</code><code class="p">(</code><code class="n">original_sequence</code><code class="p">)[</code><code class="mi">0</code><code class="p">:</code><code class="mi">25</code><code class="p">],</code> <code class="s2">"..."</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">ACCCCAGGGTAGGGCCTATTGTATG ...
</pre>

<p><a contenteditable="false" data-primary="FIMO (Find Individual Motif Occurrences)" data-type="indexterm" id="ch03_dna.html33"/>After converting the sequence, we paste it into the FIMO (Find Individual Motif Occurrences) tool, which is part of the MEME suite of motif discovery and search tools. FIMO allows for fuzzy matching of motifs—meaning the match doesn’t need to be exact—which better reflects biological reality, as transcription factors often tolerate some variability in their binding sites.</p>

<p>For CTCF, the known binding motif we used was CCACCAGGGGGCGC. When we submitted our sequence, FIMO reported a match to the CTCF motif (specifically, the subsequence GCCTCTGGGGGCGC, spanning positions 93 to 106) with a highly significant p-value of 2.6e-05, as shown in <a data-type="xref" href="#dlfb_0310">Figure 3-10</a>.</p>

      <figure><div id="dlfb_0310" class="figure">
       <img alt="" src="assets/dlfb_0310.png" width="600" height="240"/>
       <h6><span class="label">Figure 3-10. </span>How we used FIMO to search for the known CTCF binding motif within a 200 bp DNA sequence labeled as positive. Although transcription factor motifs are usually represented more flexibly using position weight matrices (PWMs), a simple string-based search for the canonical motif <code>CCACCAGGGGGCGC</code> offers a quick sanity check to confirm that the expected pattern is present in the sequence.
       </h6>
      </div></figure>
      
     
    
    <p>To further verify this, we can overlay the motif region found by FIMO on our earlier saliency map of mutation impact. This helps us visually compare where mutations have the strongest effect on the model’s prediction versus where the detected motif actually occurs (see <a data-type="xref" href="#mutation-plot-stacked-highlighted">Figure 3-11</a>):
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">plot_binding_site</code><code class="p">(</code>
  <code class="n">panels</code><code class="o">=</code><code class="p">{</code>
    <code class="s2">"tiles"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="s2">"Deltas"</code><code class="p">,</code> <code class="s2">"values"</code><code class="p">:</code> <code class="n">deltas</code><code class="p">},</code>
    <code class="s2">"line"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="s2">"Importance"</code><code class="p">,</code> <code class="s2">"values"</code><code class="p">:</code> <code class="n">importance</code><code class="p">},</code>
  <code class="p">},</code>
  <code class="n">highlight</code><code class="o">=</code><code class="p">(</code><code class="mi">92</code><code class="p">,</code> <code class="mi">106</code><code class="p">),</code>
<code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="mutation-plot-stacked-highlighted" class="figure">
       <img alt="" src="assets/dlfb_0311.png" width="600" height="261"/>
       <h6><span class="label">Figure 3-11. </span>Highlight of the TF binding site overlapping neatly with the region where mutations have the biggest influence.
       </h6>
      </div></figure>
     
    
    <p>As you can see, the motif returned by FIMO lines up almost exactly with the most mutation-sensitive region identified by our model through in silico mutagenesis. This alignment gives us confidence that the model has learned to recognize meaningful biological signal—in this case, a known CTCF binding motif—directly from sequence data<a contenteditable="false" data-primary="" data-startref="ch03_dna.html33" data-type="indexterm" id="id693"/>.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html32" data-type="indexterm" id="id694"/>
    </p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Implementing input gradients"><div class="sect3" id="implementing-input-gradients">
    <h3>Implementing input gradients</h3>
    <p><a contenteditable="false" data-primary="in silico saturation mutagenesis (ISM)" data-secondary="conducting" data-tertiary="implementing input gradients" data-type="indexterm" id="ch03_dna.html34"/><a contenteditable="false" data-primary="input gradients" data-secondary="implementing" data-type="indexterm" id="ch03_dna.html35"/>While <em>in silico</em> mutagenesis provides highly interpretable insights, it can be computationally expensive—requiring multiple forward passes through the model for each position in the input sequence. An alternative approach that is much cheaper to compute is <em>input gradients</em>. This technique, introduced earlier in the chapter, relies on a simple idea: how much does the model output change if we nudge each input base slightly?
    </p>
    <p>Mathematically, this corresponds to computing the gradient of the model’s prediction with respect to its input sequence. If a small change in a particular base results in a large change in the output, that base must be important.
    </p>
<p>The implementation is straightforward. We use <code>jax.grad</code> to take the derivative of the model’s predicted binding probability with respect to the one-hot input sequence:</p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">compute_input_gradient</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">sequence</code><code class="p">):</code>
  <code class="sd">"""Compute input gradient for a one-hot DNA sequence."""</code>
  <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">sequence</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code> <code class="o">!=</code> <code class="mi">2</code><code class="p">:</code>
    <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="s2">"Input must be a single one-hot encoded DNA sequence."</code><code class="p">)</code>

  <code class="n">sequence</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">sequence</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">jnp</code><code class="o">.</code><code class="n">float32</code><code class="p">)[</code><code class="kc">None</code><code class="p">,</code> <code class="p">:]</code>

  <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">sequence</code><code class="p">):</code>
    <code class="c1"># We take the mean to ensure we have a single scalar to take the grad of.</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">sequence</code><code class="p">))</code>

  <code class="n">gradient</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">grad</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">predict</code><code class="p">(</code><code class="n">x</code><code class="p">))(</code><code class="n">sequence</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>
</pre>
       
<p>Let’s apply it to the same sequence we used for ISM and inspect the output:</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">input_gradient</code> <code class="o">=</code> <code class="n">compute_input_gradient</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">original_sequence</code><code class="p">)</code>
<code class="n">input_gradient</code><code class="o">.</code><code class="n">shape</code>
</pre>
    
<p>Output:</p>
 <pre data-type="programlisting">(200, 4)
</pre>     
     
    
    <p>The result is a matrix of the same shape as the input sequence, where each value indicates how sensitive the model’s prediction is to a small change in a particular base at a particular position.</p>

<p>We can visualize this as a heatmap in <a data-type="xref" href="#mutation-plot-input-gradients">Figure 3-12</a>:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">importance</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">abs</code><code class="p">(</code><code class="n">input_gradient</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">plot_binding_site</code><code class="p">(</code>
  <code class="n">panels</code><code class="o">=</code><code class="p">{</code>
    <code class="s2">"tiles"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="s2">"Gradients"</code><code class="p">,</code> <code class="s2">"values"</code><code class="p">:</code> <code class="n">input_gradient</code><code class="p">},</code>
    <code class="s2">"line"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"label"</code><code class="p">:</code> <code class="s2">"Importance"</code><code class="p">,</code> <code class="s2">"values"</code><code class="p">:</code> <code class="n">importance</code><code class="p">},</code>
  <code class="p">},</code>
<code class="p">);</code>
</pre>
       
          <p>Just like with ISM, we see that most positions have little influence on the prediction, while a central region—roughly positions 90 to 110—exhibits strong gradients. This reflects the bases that the model is most sensitive to for making its prediction. <a contenteditable="false" data-primary="in silico saturation mutagenesis (ISM)" data-secondary="input gradients versus" data-type="indexterm" id="ch03_dna.html36"/>However, input gradients differ from ISM in key ways: the values are not bounded to represent discrete mutations, and even the base currently present at each position can have a nonzero gradient. This is because the gradient describes how the model’s output would change with an infinitesimal increase in that base’s activation—not an actual mutation. This makes gradients a bit more abstract but also more flexible and computationally efficient.
    </p>
     
     
      <figure><div id="mutation-plot-input-gradients" class="figure">
       <img alt="" src="assets/dlfb_0312.png" width="600" height="262"/>
       <h6><span class="label">Figure 3-12. </span>Input gradient saliency map for a CTCF-binding sequence, indicating the contribution of each DNA base to the model’s prediction of CTCF binding. The bottom heatmap shows input gradients per base (A/C/G/T), while the top line shows aggregated importance across positions. A clear central region stands out as most influential for the model’s decision.
       </h6>
      </div></figure>
     
    

    <p>To examine this region more closely, we can zoom in and label the actual bases in the 90:110 base region in
     <a data-type="xref" href="#mutation-plot-input-gradients-zoomed">Figure 3-13</a>:
    </p>
   
<pre data-type="programlisting" data-code-language="python"><code class="n">important_sequence</code> <code class="o">=</code> <code class="n">one_hot_to_dna</code><code class="p">(</code><code class="n">original_sequence</code><code class="p">)[</code><code class="mi">90</code><code class="p">:</code><code class="mi">110</code><code class="p">]</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Central DNA sequence with high importance: "</code><code class="p">,</code> <code class="n">important_sequence</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code>
  <code class="n">input_gradient</code><code class="p">[</code><code class="mi">90</code><code class="p">:</code><code class="mi">110</code><code class="p">]</code><code class="o">.</code><code class="n">T</code><code class="p">,</code>
  <code class="n">cmap</code><code class="o">=</code><code class="s2">"RdBu_r"</code><code class="p">,</code>
  <code class="n">center</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
  <code class="n">xticklabels</code><code class="o">=</code><code class="n">important_sequence</code><code class="p">,</code>
  <code class="n">yticklabels</code><code class="o">=</code><code class="p">[</code><code class="s2">"A"</code><code class="p">,</code> <code class="s2">"C"</code><code class="p">,</code> <code class="s2">"G"</code><code class="p">,</code> <code class="s2">"T"</code><code class="p">],</code>
<code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">();</code>
</pre>
 
<p>Output:</p>
<pre data-type="programlisting">Central DNA sequence with high importance:  TGGCCTCTGGGGGCGCTCTG
</pre>      
     
    <p>This plot confirms what we saw using ISM and the FIMO tool: the model is placing high importance on the central region of the sequence, which corresponds to a canonical CTCF binding motif.</p>
<p>While input gradients are computationally much cheaper than ISM, they can be somewhat noisier or harder to interpret due to model nonlinearities or saturation effects. Still, they offer a practical way to rapidly inspect what a model is attending to—especially when analyzing many sequences at once.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html36" data-type="indexterm" id="id695"/></p>
     
      <figure><div id="mutation-plot-input-gradients-zoomed" class="figure">
       <img alt="" src="assets/dlfb_0313.png" width="600" height="114"/>
       <h6><span class="label">Figure 3-13. </span>Zoomed input gradients over the CTCF motif. In this close-up view of the region identified as important by input gradients, each cell shows how the model’s predicted CTCF binding probability would change in response to a small increase in a specific base at a given position. Unlike in silico mutagenesis, even the base that is already present (e.g., G→G) can have a high gradient—indicating that the model is highly sensitive to that base and relies on it for its prediction. Strongly positive or negative values reflect positions where the model has learned a motif and is using it to assess CTCF binding.
       </h6>
      </div></figure>      
     
    
   
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id696">
  <h5>Beware How You Interpret Input Gradients</h5>
  <p><a contenteditable="false" data-primary="input gradients" data-secondary="mutation effects versus" data-type="indexterm" id="id697"/>Interpreting input gradients can be unintuitive, and a common source of confusion is mistaking input gradients for mutation effects. Unlike in silico mutagenesis, input gradients do not simulate base changes. Instead, they show how the model’s output would change if you slightly nudged the input toward each of the four bases—even the one that’s already there.</p>
<p>So a high gradient at a G→G position (i.e., where the base is already G), as in <a data-type="xref" href="#mutation-plot-input-gradients-zoomed">Figure 3-13</a>, doesn’t mean “mutating G to G is important” (which would be meaningless). Instead, it means:</p>
<ul>
  <li><p>The model is <em>sensitive</em> to the presence of that G.</p></li>
  <li><p>A tiny increase in the “G-ness” at that position would meaningfully affect the prediction.</p></li>
  <li><p>This G is contributing strongly to the model’s decision.</p></li>
</ul>
<p>In other words, if this G were somehow made even more G—despite already being fully G—the model’s output would shift. That’s not biologically plausible, but it’s a side effect of how gradients work mathematically.</p>
<p>In short, input gradients reflect directional sensitivity, not discrete substitutions. That’s why even unchanged bases can show strong signal in the heatmap.</p>
</div></aside>     
      
    <p class="pagebreak-before">So far, we have been analyzing a single sequence example (the first validation example). In <a data-type="xref" href="#mutation-plot-ten-input-gratient-panels">Figure 3-14</a> we extend this to the first 10 sequences in the validation set that are labeled 1 (i.e., sequences that are known to bind CTCF), and visualize their input gradients:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.dna.inspect</code> <code class="kn">import</code> <code class="n">plot_10_gradients</code>

<code class="n">plot_10_gradients</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">valid_ds</code><code class="p">,</code> <code class="n">target_label</code><code class="o">=</code><code class="mi">1</code><code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="mutation-plot-ten-input-gratient-panels" class="figure">
       <img alt="" src="assets/dlfb_0314.png" width="600" height="292"/>
       <h6><span class="label">Figure 3-14. </span>Input gradients for 10 CTCF-bound sequences show consistent central regions of high importance, suggesting a shared motif-like structure driving model predictions. Each cell shows the gradient value for mutating a specific base at a given position. Strongly positive or negative gradients indicate high sensitivity to mutations.
       </h6>
      </div></figure>
     
    
    <p>These gradients show a consistent central pattern across the examples, indicating that the model has likely learned a motif centered within the sequence. While the width of the important region varies slightly, the signal is strong and localized.
    </p>

    <p>By contrast, <a data-type="xref" href="#dlfb_0315">Figure 3-15</a> shows the input gradients for 10 negative examples—sequences that the model predicts do not bind CTCF. Here, the picture is far more <span class="keep-together">heterogeneous</span>:</p>

     <pre data-type="programlisting" data-code-language="python"><code class="n">plot_10_gradients</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">valid_ds</code><code class="p">,</code> <code class="n">target_label</code><code class="o">=</code><code class="mi">0</code><code class="p">);</code>
</pre>

      <figure><div id="dlfb_0315" class="figure">
       <img alt="" src="assets/dlfb_0315.png" width="600" height="292"/>
       <h6><span class="label">Figure 3-15. </span>Input gradients for 10 nonbinding sequences reveal more diffuse or noisy importance patterns, but often retain a weak central focus—likely reflecting the peak-centered sampling strategy used during dataset construction.
       </h6>
      </div></figure>
      <p>In this plot, some sequences show a weaker but still present centered signal. Others display no clear motif or have diffuse scores spread across the entire sequence.</p>

<p>This variation makes sense given the way the dataset was constructed. While these sequences are labeled negative, they were carefully matched to positive examples in terms of chromatin accessibility. That means they also come from open chromatin regions and may still contain weak or partial motifs—or even motifs for other TFs. As a result, the model might still “attend” to the center of the sequence, even if it ultimately predicts no binding.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>These findings serve as a good reminder that contribution scores like input gradients don’t just reflect the presence or absence of strong motifs. They also reveal where the model is looking for evidence and can surface subtle or confounding biological signals introduced by dataset design choices. In this case, the weak central patterns likely reflect a bias introduced by sampling from peak-centered open chromatin, even in negative (class 0) examples.
</p>
</div>

    <p>Now that we’ve explored two complementary interpretation tools—ISM and input gradients—we’re in a better position to trust (and debug) what the model is learning. Let’s return to the modeling task and increase both the problem complexity and model capacity<a contenteditable="false" data-primary="" data-startref="ch03_dna.html35" data-type="indexterm" id="id698"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html34" data-type="indexterm" id="id699"/>.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html30" data-type="indexterm" id="id700"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html29" data-type="indexterm" id="id701"/>
    </p>
   </div></section>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Modeling Multiple Transcription Factors"><div class="sect2" id="modelling-multiple-transcription-factors">
   <h2>Modeling Multiple Transcription Factors</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="increasing the complexity of the model" data-tertiary="modeling multiple transcription factors" data-type="indexterm" id="ch03_dna.html37"/><a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="modeling multiple TFs" data-type="indexterm" id="ch03_dna.html38"/>So far, we’ve focused on a single transcription factor: CTCF. Let’s now increase the biological scope of our modeling by predicting binding for all 10 TFs in the dataset.
   </p>
   <section data-type="sect3" data-pdf-bookmark="Preparing a multi-TF dataset"><div class="sect3" id="preparing-a-multi-tf-dataset">
    <h3>Preparing a multi-TF dataset</h3>
    <p><a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="modeling multiple TFs" data-tertiary="preparing a dataset" data-type="indexterm" id="id702"/>The dataset includes binding labels for the following 10 transcription factors:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">transcription_factors</code> <code class="o">=</code> <code class="p">[</code>
  <code class="s2">"ARID3"</code><code class="p">,</code>
  <code class="s2">"ATF2"</code><code class="p">,</code>
  <code class="s2">"BACH1"</code><code class="p">,</code>
  <code class="s2">"CTCF"</code><code class="p">,</code>
  <code class="s2">"ELK1"</code><code class="p">,</code>
  <code class="s2">"GABPA"</code><code class="p">,</code>
  <code class="s2">"MAX"</code><code class="p">,</code>
  <code class="s2">"REST"</code><code class="p">,</code>
  <code class="s2">"SRF"</code><code class="p">,</code>
  <code class="s2">"ZNF24"</code><code class="p">,</code>
<code class="p">]</code>
</pre>
       
      
     
    
    <p>Each of these TFs has its own distinct DNA-binding preference. For instance:</p>
    <ul>
     <li>CTCF, as we’ve seen, binds motifs like <code>CCACCAGGGGGCGC</code>.</li>
     <li>MAX prefers the E-box motif <code>CACGTG</code>, important in regulating cell proliferation.</li>
     <li>SRF binds to <code>CCW6GG</code> (where W = A or T), a motif involved in muscle-specific gene expression.</li>
    </ul>
    <p>These preferences are often conserved across species, and the role of each TF can be deeply rooted in specific cell type identity or developmental processes. If you’re curious, it’s worth looking up some of these proteins—for example, REST is a key repressor in neurons, while MAX has a known role in oncogenesis through interaction with MYC.</p>
    <p>Our task now is to train models that can automatically discover these motif patterns and accurately associate them with TF binding.</p>
    <p>There are two common strategies for tackling this problem:</p>
    <dl>
     <dt>Multitask learning</dt>
     <dd>Train a single model to output one prediction per TF, potentially learning shared representations across tasks.</dd>
     <dt>Single-task learning</dt>
     <dd>Train separate binary classification models for each TF independently.</dd>
    </dl>
    <p class="pagebreak-before">In our case, the 10 TFs are not particularly closely related, and their binding preferences are quite distinct—ranging from insulator proteins like CTCF to general transcriptional regulators like MAX and neuron-specific factors like REST. Additionally, the dataset provides separate training sets for each TF, and the original paper trained them independently.</p>
<p>Given these factors, we’ll follow a single-task learning approach: train 10 individual binary classification models, one per TF. This also contrasts nicely with the multitask approach you saw in <a data-type="xref" href="ch02.html#learning-the-language-of-proteins">Chapter 2</a>.</p>
    
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Defining a more complex model"><div class="sect3" id="defining-a-more-complex-model">
    <h3>Defining a more complex model</h3>
    <p>
     <a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="modeling multiple TFs" data-tertiary="developing a more complex model" data-type="indexterm" id="ch03_dna.html39"/>Now that we’re training on 10 separate TF binding tasks, it’s a good opportunity to improve the training stability and generalization of our model.</p>
<p>We’ll retain the same core convolutional architecture from earlier, but introduce three standard deep learning techniques—batch normalization, dropout, and learning rate scheduling—that are widely used in CNNs and often help even for relatively shallow models and simple datasets.
    </p>
    <dl>
      <dt>Batch normalization</dt>
      <dd><p><a contenteditable="false" data-primary="batch normalization" data-type="indexterm" id="id703"/>We add batch normalization after each convolutional layer to improve training stability:</p>
       
       <ul>
        <li>
         <p>
          Batch norm normalizes the activations across the batch, which helps smooth the optimization landscape and accelerates convergence.
         </p>
        </li>
        <li>
         <p>
          Even though our network is relatively shallow, batch norm can still improve performance and robustness.
         </p>
        </li>
        </ul>

<p>A few implementation details:</p>
<ul>
 <li>Batch norm behaves differently during training and inference. During training, it computes mean and variance from the current batch and updates running averages; at inference time, it uses the learned averages.</li>
 <li>In Flax, you control this behavior with the <code>is_training</code> flag using the <code>use_running_average=not is_training</code> pattern.</li>
</ul>
</dd>

          <dt class="pagebreak-before">Dropout regularization</dt>
          <dd><p><a contenteditable="false" data-primary="dropout regularization" data-type="indexterm" id="id704"/>To reduce overfitting, we add dropout after the dense (fully connected) layers:
           </p>
           <ul>
            <li><p>Dropout randomly sets a portion of activations to zero during training, encouraging the model to learn redundant and robust features.</p>
            </li>
            <li><p>It is typically used after dense layers, rather than convolutions, because spatially shared convolutional filters already generalize well.</p>
            </li>
            <li><p>In Flax, dropout requires passing a PRNG (pseudorandom number generator) key to the forward pass.</p>
            </li>
            <li><p>We use a moderate dropout rate of 0.2, which adds some regularization without significantly reducing model capacity.</p></li>
           </ul>
</dd>


            <dt>Learning rate scheduling</dt>
            <dd><p><a contenteditable="false" data-primary="learning rate scheduling" data-type="indexterm" id="id705"/>Rather than using a fixed learning rate, we adopt a learning rate schedule to guide training:</p>
              <ul>
                <li><p>Dynamic learning rates usually start high (encouraging fast exploration) and decrease over time to help convergence.</p></li>
                <li><p>Popular options include exponential decay, step decay, and cosine annealing.</p></li>
                <li><p>Implemented via <code>optax.cosine_decay_schedule</code>, we use a cosine decay schedule which gradually reduces the learning rate over the course of training. The shape of this learning rate schedule is visualized in <a data-type="xref" href="#dlfb_0316">Figure 3-16</a>:</p>

       
  <pre data-type="programlisting" data-code-language="python"><code class="n">num_steps</code> <code class="o">=</code> <code class="mi">1000</code>

<code class="n">scheduler</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">cosine_decay_schedule</code><code class="p">(</code>
  <code class="n">init_value</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code>
  <code class="n">decay_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>  <code class="c1"># How long to decay over.</code>
<code class="p">)</code>
<code class="n">learning_rates</code> <code class="o">=</code> <code class="p">[</code><code class="n">scheduler</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">)]</code>

<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">),</code> <code class="n">learning_rates</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Learning Rate over Steps"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Learning Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Step"</code><code class="p">);</code>
</pre>     

<figure><div id="dlfb_0316" class="figure">
       <img alt="" src="assets/dlfb_0316.png" width="600" height="452"/>
       <h6><span class="label">Figure 3-16. </span>Cosine decay learning rate schedule used during training. The learning rate starts at 0.001 and gradually decreases over 1,000 steps, helping the model explore early on and converge smoothly toward the end.
       </h6>
      </div></figure>

                    </li></ul>

            </dd>
</dl>
      <p>Even though our dataset and architecture are fairly simple, these additions should help improve training stability and model generalization across the expanded set of transcription factor tasks.</p>

      <p>Let’s now implement the updated model definition:</p>
       
<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ConvModelV2</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""CNN with batch norm and dropout for binary classification."""</code>

  <code class="n">conv_filters</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code>  <code class="c1"># Number of filters for conv layers.</code>
  <code class="n">kernel_size</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">10</code><code class="p">,)</code>  <code class="c1"># Kernel size for 1D conv layers.</code>
  <code class="n">dense_units</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">128</code>  <code class="c1"># Units in first dense fully-connected layer.</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.2</code>  <code class="c1"># Proportion of dense neurons to randomly drop out.</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">True</code><code class="p">):</code>
    <code class="c1"># First convolutional layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">conv_filters</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm</code><code class="p">(</code><code class="n">use_running_average</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,))</code>

    <code class="c1"># Second convolutional layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">conv_filters</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm</code><code class="p">(</code><code class="n">use_running_average</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,))</code>

    <code class="c1"># Flatten the values before passing them to the dense layers.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="o">-</code><code class="mi">1</code><code class="p">))</code>

    <code class="c1"># First dense layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="c1"># Second dense layer.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code> <code class="o">//</code> <code class="mi">2</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="c1"># Output layer (single unit for binary classification).</code>
    <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

  <code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code><code class="p">):</code>
    <code class="sd">"""Initializes model parameters and returns a train state for training."""</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">)</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">TrainStateWithBatchNorm</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
      <code class="n">apply_fn</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code>
      <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">,</code>
      <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code>
      <code class="n">batch_stats</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">],</code>
      <code class="n">key</code><code class="o">=</code><code class="n">rng_dropout</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">state</code>
</pre>
     
    
    <p>
     Our <code>ConvModelV2</code> implementation is starting to get a bit long and repetitive. Later in the chapter, we’ll address this by refactoring out the repeated logic into <code>ConvBlock</code> and <code>DenseBlock</code> components to make the model definition more concise and <span class="keep-together">modular</span>.</p>
<p>For now, we have all the pieces needed to create the training state:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">state</code> <code class="o">=</code> <code class="n">ConvModelV2</code><code class="p">()</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">],</code> <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="n">scheduler</code><code class="p">)</code>
<code class="p">)</code>
</pre>
       
      
     
    
    <p>The training step looks fairly similar to what we’ve seen before, but now it needs to handle both dropout and batch normalization.
    </p>
   <p>One small additional change: the loss function is defined directly inside <code>train_step</code>. This is often done for readability and encapsulation, especially when the loss depends on extra mutable model state (like <code>batch_stats</code>) or dropout, both of which are now required in the forward pass:</p>
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">,</code> <code class="n">rng_dropout</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
  <code class="sd">"""Run a training step and update parameters."""</code>

  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
    <code class="sd">"""Make predictions on batch and compute binary cross-entropy loss."""</code>
    <code class="n">logits</code><code class="p">,</code> <code class="n">updates</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
      <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">,</code> <code class="s2">"batch_stats"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code><code class="p">},</code>
      <code class="n">x</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">],</code>
      <code class="n">is_training</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
      <code class="n">rngs</code><code class="o">=</code><code class="p">{</code><code class="s2">"dropout"</code><code class="p">:</code> <code class="n">rng_dropout</code><code class="p">},</code>
      <code class="n">mutable</code><code class="o">=</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">],</code>
    <code class="p">)</code>

    <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">sigmoid_binary_cross_entropy</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

    <code class="k">return</code> <code class="n">loss</code><code class="p">,</code> <code class="n">updates</code>

  <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
  <code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">updates</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">batch_stats</code><code class="o">=</code><code class="n">updates</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">])</code>

  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="p">}</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code>
</pre>
       
      
     
    
    <p>To confirm that everything is wired up correctly, we can overfit to a single batch—that is, run a few steps on the same batch and check that the loss decreases:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="c1"># Overfit on one batch.</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">):</code>
  <code class="n">rng</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
  <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">,</code> <code class="n">rng_dropout</code><code class="p">)</code>
  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Step </code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s2"> loss: </code><code class="si">{</code><code class="n">metrics</code><code class="p">[</code><code class="s1">'loss'</code><code class="p">]</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>
  

<p>Output:</p>
<pre data-type="programlisting">Step 0 loss: 0.6932974457740784
Step 1 loss: 0.25415313243865967
Step 2 loss: 0.08513301610946655
Step 3 loss: 0.02221144177019596
Step 4 loss: 0.019464049488306046
</pre>
       
      
     
    
    <p>Looks good—the loss decreases quickly. This indicates the model is capable of fitting the data.
    </p>

    <p>However, loss alone isn’t the best way to evaluate classification models. We want to monitor additional metrics like accuracy and the area under the ROC curve (auROC). We compute these in an evaluation step:</p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
  <code class="sd">"""Evaluate model on a single batch."""</code>
  <code class="n">logits</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
    <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="s2">"batch_stats"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code><code class="p">},</code>
    <code class="n">x</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"sequences"</code><code class="p">],</code>
    <code class="n">is_training</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
    <code class="n">mutable</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
  <code class="p">)</code>
  <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">sigmoid_binary_cross_entropy</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
    <code class="o">**</code><code class="n">compute_metrics</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">],</code> <code class="n">logits</code><code class="p">),</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="n">metrics</code>


<code class="k">def</code> <code class="nf">compute_metrics</code><code class="p">(</code><code class="n">y_true</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">logits</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">):</code>
  <code class="sd">"""Compute accuracy and auROC for model predictions."""</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"accuracy"</code><code class="p">:</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_true</code><code class="p">,</code> <code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">logits</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="mf">0.5</code><code class="p">),</code>
    <code class="s2">"auc"</code><code class="p">:</code> <code class="n">roc_auc_score</code><code class="p">(</code><code class="n">y_true</code><code class="p">,</code> <code class="n">logits</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="n">metrics</code>
</pre>

<p>Note that since <code>scikit-learn</code> functions are not JAX compatible, the evaluation step is not decorated with <code>@jax.jit</code>.</p>
       
 <p>Let’s see the output of running <code>eval_step</code>:</p>  

  <pre data-type="programlisting" data-code-language="python"><code class="c1"># Evaluate the batch.</code>
<code class="n">metrics</code> <code class="o">=</code> <code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code>
</pre>   

<p>Output:</p>
  <pre data-type="programlisting">{'loss': 0.45877009630203247, 'accuracy': 0.9375, 'auc': 1.0}
</pre> 

<p>Now that we’ve implemented the training and evaluation steps, let’s define a full training loop. The train function takes an initialized model state, training and validation datasets, and a few configuration parameters:</p>

  <pre data-type="programlisting" data-code-language="python"><code class="nd">@restorable</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainStateWithBatchNorm</code><code class="p">,</code>
  <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="p">],</code>
  <code class="n">num_steps</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainStateWithBatchNorm</code><code class="p">,</code> <code class="n">Any</code><code class="p">]:</code>
  <code class="sd">"""Train a model and log metrics over steps."""</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="n">MetricsLogger</code><code class="p">()</code>
  <code class="n">train_batches</code> <code class="o">=</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">()</code>

  <code class="n">steps</code> <code class="o">=</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">))</code>  <code class="c1"># Steps with progress bar.</code>
  <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="n">steps</code><code class="p">:</code>
    <code class="n">steps</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Step </code><code class="si">{</code><code class="n">step</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
    <code class="n">train_batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_batches</code><code class="p">)</code>
    <code class="n">state</code><code class="p">,</code> <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">train_batch</code><code class="p">,</code> <code class="n">rng_dropout</code><code class="p">)</code>
    <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>

    <code class="k">if</code> <code class="n">step</code> <code class="o">%</code> <code class="n">eval_every</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
      <code class="k">for</code> <code class="n">eval_batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">]</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">():</code>
        <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">eval_batch</code><code class="p">)</code>
        <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>
      <code class="n">metrics</code><code class="o">.</code><code class="n">flush</code><code class="p">(</code><code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code>

    <code class="n">steps</code><code class="o">.</code><code class="n">set_postfix_str</code><code class="p">(</code><code class="n">metrics</code><code class="o">.</code><code class="n">latest</code><code class="p">([</code><code class="s2">"loss"</code><code class="p">]))</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">export</code><code class="p">()</code>
</pre> 

<p>The training loop is quite similar to before, but as a quick recap, here is what it does:</p>

<ul>
  <li><p>Iterates through training steps using a progress bar</p></li>
  <li><p>Runs <code>train_step</code> to update model parameters</p></li>
  <li><p>Periodically evaluates on the validation set using <code>eval_step</code></p></li>
  <li><p>Logs and stores metrics at each step using a custom MetricsLogger</p></li>
</ul>

<p>Since we’re training one model per TF, we’ll use a utility function to load datasets split by TF name:</p>

  <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">load_dataset_splits</code><code class="p">(</code>
  <code class="n">path</code><code class="p">,</code> <code class="n">transcription_factor</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code>
<code class="p">):</code>
  <code class="sd">"""Load TF dataset splits (train, valid, test) as TensorFlow datasets."""</code>
  <code class="n">dataset_splits</code> <code class="o">=</code> <code class="p">{}</code>
  <code class="k">for</code> <code class="n">split</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"train"</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">]:</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code>
      <code class="n">sequence_db</code><code class="o">=</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">path</code><code class="si">}</code><code class="s2">/</code><code class="si">{</code><code class="n">transcription_factor</code><code class="si">}</code><code class="s2">_</code><code class="si">{</code><code class="n">split</code><code class="si">}</code><code class="s2">_sequences.csv"</code>
    <code class="p">)</code>
    <code class="n">ds</code> <code class="o">=</code> <code class="n">convert_to_tfds</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="p">(</code><code class="n">split</code> <code class="o">==</code> <code class="s2">"train"</code><code class="p">))</code>
    <code class="n">dataset_splits</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="n">split</code><code class="p">:</code> <code class="n">ds</code><code class="p">})</code>
  <code class="k">return</code> <code class="n">dataset_splits</code>
</pre> 

<p class="pagebreak-before">This function loads training, validation, and test splits for the given <code>transcription_factor</code> and converts them into TensorFlow datasets ready for batching.</p>
    
    <p>We now have everything in place to train one model per TF:
    </p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">prefix</code> <code class="o">=</code> <code class="n">assets</code><code class="p">(</code><code class="s2">"dna/datasets"</code><code class="p">)</code>
<code class="n">tf_metrics</code> <code class="o">=</code> <code class="p">{}</code>

<code class="c1"># Train one model per transcription factor.</code>
<code class="k">for</code> <code class="n">transcription_factor</code> <code class="ow">in</code> <code class="n">transcription_factors</code><code class="p">:</code>
  <code class="c1"># Load data for this TF.</code>
  <code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">load_dataset_splits</code><code class="p">(</code>
    <code class="n">assets</code><code class="p">(</code><code class="s2">"dna/datasets"</code><code class="p">),</code> <code class="n">transcription_factor</code><code class="p">,</code> <code class="n">batch_size</code>
  <code class="p">)</code>
  <code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
  <code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
  <code class="n">dummy_batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">())[</code><code class="s2">"sequences"</code><code class="p">]</code>

  <code class="c1"># Create train state.</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">ConvModelV2</code><code class="p">()</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dummy_batch</code><code class="p">,</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="n">scheduler</code><code class="p">),</code>
  <code class="p">)</code>

  <code class="c1"># Train the model.</code>
  <code class="n">_</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
    <code class="n">state</code><code class="o">=</code><code class="n">state</code><code class="p">,</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
    <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
    <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
    <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
    <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="sa">f</code><code class="s2">"dna/models/</code><code class="si">{</code><code class="n">transcription_factor</code><code class="si">}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">)</code>

  <code class="c1"># Store metrics.</code>
  <code class="n">tf_metrics</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="n">transcription_factor</code><code class="p">:</code> <code class="n">metrics</code><code class="p">})</code>
</pre>
  
<p>With training complete, let’s take a look at just CTCF transcription factor performance in <a data-type="xref" href="#dlfb_0317">Figure 3-17</a>:</p>
   
<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.dna.inspect</code> <code class="kn">import</code> <code class="n">plot_learning</code>

<code class="n">tf</code> <code class="o">=</code> <code class="s2">"CTCF"</code>
<code class="n">plot_learning</code><code class="p">(</code><code class="n">tf_metrics</code><code class="p">[</code><code class="n">tf</code><code class="p">],</code> <code class="n">tf</code><code class="p">);</code>
</pre>

      <figure><div id="dlfb_0317" class="figure">
       <img alt="" src="assets/dlfb_0317.png" width="600" height="288"/>
       <h6><span class="label">Figure 3-17. </span>Learning curves for the CTCF transcription factor. The left panel shows the training and validation loss over time. The right panel tracks the model’s classification performance using validation set accuracy and auROC. Performance improves steadily during training and converges smoothly.</h6>
      </div></figure>

      <p>Our model performs well on predicting whether CTCF binds a given DNA sequence, with training and validation losses decreasing and validation auROC approaching a perfect score of 1.0—consistent with the fact that CTCF is relatively easy to predict compared to some of the other TFs.</p>

<p>Let’s now visualize the training curves across all 10 TFs. We first process the logged metrics:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.utils.metric_plots</code> <code class="kn">import</code> <code class="n">to_df</code>

<code class="c1"># Extract metrics logged per transcription factor.</code>
<code class="n">tf_df</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">tf</code><code class="p">,</code> <code class="n">metrics</code> <code class="ow">in</code> <code class="n">tf_metrics</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">tf_df</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">to_df</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">TF</code><code class="o">=</code><code class="n">tf</code><code class="p">))</code>
<code class="n">tf_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">(</code><code class="n">tf_df</code><code class="p">)</code>

<code class="c1"># Determine order of best performance.</code>
<code class="n">auc_df</code> <code class="o">=</code> <code class="n">tf_df</code><code class="p">[(</code><code class="n">tf_df</code><code class="p">[</code><code class="s2">"metric"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"auc"</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">tf_df</code><code class="p">[</code><code class="s2">"split"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"valid"</code><code class="p">)]</code>
<code class="n">max_auc_by_tf</code> <code class="o">=</code> <code class="n">auc_df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"TF"</code><code class="p">)[</code><code class="s2">"mean"</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
<code class="n">tf_order</code> <code class="o">=</code> <code class="n">max_auc_by_tf</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>
<code class="n">tf_df</code><code class="p">[</code><code class="s2">"TF"</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Categorical</code><code class="p">(</code><code class="n">tf_df</code><code class="p">[</code><code class="s2">"TF"</code><code class="p">],</code> <code class="n">categories</code><code class="o">=</code><code class="n">tf_order</code><code class="p">,</code> <code class="n">ordered</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
</pre>
     
    
    <p>We can then visualize their learning curves in <a data-type="xref" href="#learning-curves-10-tfs">Figure 3-18</a> sorted by auROC performance (best-performing TFs are first):
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">sns</code><code class="o">.</code><code class="n">set_context</code><code class="p">(</code><code class="s2">"notebook"</code><code class="p">,</code> <code class="n">font_scale</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">rc</code><code class="o">=</code><code class="p">{</code><code class="s2">"lines.linewidth"</code><code class="p">:</code> <code class="mf">2.5</code><code class="p">})</code>
<code class="n">sns</code><code class="o">.</code><code class="n">set_style</code><code class="p">(</code><code class="s2">"ticks"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"axes.grid"</code><code class="p">:</code> <code class="kc">True</code><code class="p">})</code>
<code class="n">g</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">relplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">tf_df</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"round"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"mean"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"metric"</code><code class="p">,</code>
  <code class="n">kind</code><code class="o">=</code><code class="s2">"line"</code><code class="p">,</code>
  <code class="n">col</code><code class="o">=</code><code class="s2">"TF"</code><code class="p">,</code>
  <code class="n">col_order</code><code class="o">=</code><code class="n">tf_order</code><code class="p">,</code>
  <code class="n">col_wrap</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
  <code class="n">alpha</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code>
  <code class="n">palette</code><code class="o">=</code><code class="n">DEFAULT_SPLIT_COLORS</code><code class="p">,</code>
  <code class="n">dashes</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">g</code><code class="o">.</code><code class="n">set_axis_labels</code><code class="p">(</code><code class="s2">"Step"</code><code class="p">,</code> <code class="s2">"Value"</code><code class="p">)</code>
<code class="n">g</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">));</code>
</pre>
       
      
     
     
      <figure><div id="learning-curves-10-tfs" class="figure">
       <img alt="" src="assets/dlfb_0318.png" width="600" height="374"/>
       <h6><span class="label">Figure 3-18. </span>Learning curves for all 10 TFs. Each panel shows the training and validation loss (solid lines), validation accuracy (dashed lines), and validation auROC (dotted lines) over training steps. TFs are ordered by peak auROC performance. While some TFs, like CTCF and ATF2, reach near-perfect performance quickly, others, such as ZNF24 and BACH1, prove more challenging to model.
       </h6>
      </div></figure>
     
    <p>Are these different TF binding models doing a good job? It can be hard to know what constitutes a “good enough” auROC—especially in biological settings where label noise and dataset complexity can vary widely. Fortunately, since our dataset comes from a published benchmark, we can directly compare our model’s results to those reported in the original paper.</p>
    <p>In <a href="https://oreil.ly/QciKO">Figure 3 of the source paper</a>, we see that auROC scores vary considerably across TFs, and some are challenging to predict well—even when using information from more advanced architectures like genomic language models (gLMs) trained with large-scale pretraining. The paper’s figure helps establish a performance ceiling for baseline CNNs trained on one-hot encoded DNA. Interestingly, the paper notes that pretrained gLM representations do not consistently outperform conventional approaches using one-hot inputs, suggesting that simple models can still be competitive on these TF binding tasks.
    </p>
    <p>The following are key takeaways from the paper’s figure:</p>
    <ul class="simple">
     <li><p>CTCF and ATF2 are the most predictable TFs, with both one-hot and pretrained models achieving auROC scores above 0.95. These TFs have strong, conserved binding motifs that are easy for models to learn.
      </p>
     </li>
     <li><p>REST, MAX, and ELK1 show intermediate difficulty, with auROCs around 0.83 to 0.85.
      </p>
     </li>
     <li><p>ZNF24, BACH1, ARID3, and GABPA tend to be more difficult, with auROCs hovering in the 0.75 to 0.80 range.</p></li>
    </ul>
    <p>Let’s now see how our own models compare. The following prints the peak validation auROC achieved by our CNNs trained independently for each TF:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">max_auc_by_tf</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">))</code>
</pre>
    
<p>Output:</p>
        <pre data-type="programlisting">TF
ATF2     0.985182
CTCF     0.980479
SRF      0.849944
           ...   
ARID3    0.769149
BACH1    0.767122
ZNF24    0.761506
Name: mean, Length: 10, dtype: float64
</pre>

    <p>Our results closely mirror the rankings and scores from the paper—especially in terms of which TFs are easier or harder to predict. This consistency offers reassuring external validation that our training setup is functioning correctly and our models are learning something meaningful.
    </p>
      
 <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id706">
  <h5>Why Are Some TFs Harder to Model Than Others?</h5>
  <p><a contenteditable="false" data-primary="transcription factors (TFs)" data-secondary="modeling multiple TFs" data-tertiary="variation in modeling difficulty" data-type="indexterm" id="id707"/>Not all TFs are equally predictable. Some—like CTCF or ATF2—bind to well-defined, highly conserved motifs that produce strong, consistent signals in the DNA. These clear patterns make it easier for neural networks to accurately predict their binding.</p>
<p>Others—such as ZNF24, ARID3, or BACH1—are more complex. They may bind weakly, recognize flexible or degenerate motifs, or depend on subtle contextual cues such as cofactor presence, motif spacing, or chromatin accessibility. These factors introduce biological variability and weaken the sequence-to-binding signal, making the task inherently noisier and more difficult.</p>
<p>In other words, variation in model performance across TFs often reflects true biological complexity—not just machine learning limitations.
It’s also worth noting that our benchmark is simplified: we’re predicting TF binding from just 200 base pairs of sequence. In reality, binding depends on broader genomic and epigenetic context—including long-range interactions and chromatin state. Still, many of these influences are at least partially encoded in local DNA patterns, and modern deep learning models like Enformer, Borzoi, and AlphaGenome can achieve high correlation with ChIP-seq binding data using only sequence<a contenteditable="false" data-primary="" data-startref="ch03_dna.html39" data-type="indexterm" id="id708"/> input<a contenteditable="false" data-primary="" data-startref="ch03_dna.html38" data-type="indexterm" id="id709"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html37" data-type="indexterm" id="id710"/>.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html28" data-type="indexterm" id="id711"/></p>
</div></aside>    

<p>Let’s now explore how we might push our TF-binding prediction results further through more expressive architectures.</p>

    

   </div></section>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Advanced Techniques"><div class="sect1" id="advanced-techniques">
  <h1>Advanced Techniques</h1>
  <p>
   <a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="advanced modeling techniques" data-type="indexterm" id="ch03_dna.html40"/>Before we introduce more complex model components, let’s first improve the clarity and modularity of our model architecture by refactoring it into reusable building blocks. This makes our code easier to read, extend, and maintain.
  </p>
 <p>We will modularize our convolutional and MLP layers by creating two helper modules, <code>ConvBlock</code> and <code>MLPBlock</code>:</p>
   
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ConvBlock</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Convolutional block with batch norm, GELU and max pooling."""</code>

  <code class="n">conv_filters</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">kernel_size</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code>
  <code class="n">pool_size</code><code class="p">:</code> <code class="nb">int</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">True</code><code class="p">):</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">conv_filters</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm</code><code class="p">(</code><code class="n">use_running_average</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code>
      <code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">pool_size</code><code class="p">,),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">pool_size</code><code class="p">,)</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>


<code class="k">class</code> <code class="nc">MLPBlock</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Dense + GELU + dropout block."""</code>

  <code class="n">dense_units</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.0</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">True</code><code class="p">):</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>
</pre>
    
   
  
  <p>With these reusable blocks in place, we can now define a more compact and configurable model architecture:
  </p>
 
   
   <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ConvTransformerModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Model combining CNN, transformer, and MLP blocks."""</code>

  <code class="n">num_conv_blocks</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">2</code>
  <code class="n">conv_filters</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code>
  <code class="n">kernel_size</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">10</code><code class="p">,)</code>
  <code class="n">num_mlp_blocks</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">2</code>
  <code class="n">dense_units</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">128</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.2</code>  <code class="c1"># Global.</code>
  <code class="n">num_transformer_blocks</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">0</code>
  <code class="n">num_transformer_heads</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">8</code>
  <code class="n">transformer_dense_units</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">True</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_conv_blocks</code><code class="p">):</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">ConvBlock</code><code class="p">(</code>
        <code class="n">conv_filters</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">conv_filters</code><code class="p">,</code>
        <code class="n">kernel_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code>
        <code class="n">pool_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
      <code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_transformer_blocks</code><code class="p">):</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">TransformerBlock</code><code class="p">(</code>
        <code class="n">num_heads</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">num_transformer_heads</code><code class="p">,</code>
        <code class="n">dense_units</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">transformer_dense_units</code><code class="p">,</code>
        <code class="n">dropout_rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">,</code>
      <code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="o">-</code><code class="mi">1</code><code class="p">))</code>

    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_mlp_blocks</code><code class="p">):</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">MLPBlock</code><code class="p">(</code>
        <code class="n">dense_units</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code> <code class="o">//</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">),</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code>
      <code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
</pre> 
     

     
    
   <p>This architecture supports optional transformer blocks inserted between the convolutional feature extractors and the MLP classifier. You’ll also notice the introduction of <code>TransformerBlock</code>, which we’ll explore next.</p>
  
  <section data-type="sect2" data-pdf-bookmark="Adding Self-attention and Transformer Blocks"><div class="sect2" id="adding-self-attention-and-transformer-blocks">
   <h2>Adding Self-attention and Transformer Blocks</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="advanced modeling techniques" data-tertiary="adding self-attention/transformer blocks" data-type="indexterm" id="ch03_dna.html41"/><a contenteditable="false" data-primary="self-attention" data-secondary="adding to transformer blocks" data-type="indexterm" id="ch03_dna.html42"/><a contenteditable="false" data-primary="transformer blocks" data-type="indexterm" id="ch03_dna.html43"/>Our final architecture combines convolutional layers with transformer blocks. While convolutional layers are excellent at extracting local motif-like patterns from DNA, transformer blocks can capture long-range dependencies—patterns that span larger regions of the sequence. These two types of layers are highly complementary.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6><p>In our case, the sequences are relatively short (just 200 base pairs), so the benefits of long-range attention may be modest. However, attention mechanisms become increasingly useful as input length increases. For longer genomic contexts—such as full gene bodies, or interactions between potentially distant regulatory elements such as promoter and enhancer sequences—transformers can integrate signals across a broader range than is possible with fixed-size convolutional kernels.</p></div>
   <p><a contenteditable="false" data-primary="SelfAttention module (Flax)" data-type="indexterm" id="id712"/>Flax includes a built-in <code>SelfAttention</code> module, which we can use as the foundation for our <code>TransformerBlock</code>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">TransformerBlock</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Transformer block with self-attention and MLP."""</code>

  <code class="n">num_heads</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">8</code>
  <code class="n">dense_units</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.2</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">True</code><code class="p">):</code>
    <code class="c1"># Self-attention with layer norm.</code>
    <code class="n">residual</code> <code class="o">=</code> <code class="n">x</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">()(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">SelfAttention</code><code class="p">(</code><code class="n">num_heads</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">num_heads</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">+=</code> <code class="n">residual</code>

    <code class="c1"># Feedforward block.</code>
    <code class="n">residual</code> <code class="o">=</code> <code class="n">x</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">()(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">gelu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dense_units</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>  <code class="c1"># No GELU after this Dense.</code>
    <code class="n">x</code> <code class="o">+=</code> <code class="n">residual</code>
    <code class="k">return</code> <code class="n">x</code>
</pre>
      
     
    
   
   <p>
    A few notes on this design:
   </p>
  
<dl>
 <dt>Self-attention</dt>
 <dd>The core operation is <code>nn.SelfAttention</code>, which enables each position in the sequence to attend to every other position, capturing dependencies across the input.</dd>
 <dt>Residual connections</dt>
 <dd>The skip connections help stabilize training by allowing gradients to flow more easily through deep architectures.</dd>
 <dt>Layer normalization</dt>
 <dd>Transformers typically use layer norm rather than batch norm, as it tends to be more stable for sequence-based models and is invariant to batch size.</dd>
 <dt>Position information</dt>
 <dd>In our current setup, we omit explicit positional encodings—this is a simplification that may be acceptable for short sequences (e.g., 200 bp), where the relative arrangement of motifs can still be learned through the convolutional layers preceding attention. For longer inputs or if attention is applied very early, consider adding learned or sinusoidal positional encodings.</dd>
</dl>
<p>This modular <code>TransformerBlock</code> can now be optionally included between convolutional and MLP layers in our <code>ConvTransformerModel</code>.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html43" data-type="indexterm" id="id713"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html42" data-type="indexterm" id="id714"/><a contenteditable="false" data-primary="" data-startref="ch03_dna.html41" data-type="indexterm" id="id715"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Defining Various Model Architectures"><div class="sect2" id="defining-the-final-model-architecture">
   <h2>Defining Various Model Architectures</h2>
   <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="advanced modeling techniques" data-tertiary="defining various model architectures" data-type="indexterm" id="id716"/>Now that our model is modular, we can experiment with different architectural settings to better understand their impact on performance. This kind of architectural exploration is common when tuning deep learning models—there’s rarely a single obvious “best” model, and trying multiple variants can reveal helpful insights.
   </p>
   <p>In the following code, we define several models with different settings:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">models</code> <code class="o">=</code> <code class="p">{</code>
  <code class="c1"># Our standard 2-layer CNN with dropout and MLP.</code>
  <code class="s2">"baseline"</code><code class="p">:</code> <code class="n">ConvTransformerModel</code><code class="p">(),</code>
  <code class="c1"># Ablations: Remove or reduce certain components.</code>
  <code class="c1"># Only a single convolutional block.</code>
  <code class="s2">"single_conv_only"</code><code class="p">:</code> <code class="n">ConvTransformerModel</code><code class="p">(</code>
    <code class="n">num_conv_blocks</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">num_transformer_blocks</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">num_mlp_blocks</code><code class="o">=</code><code class="mi">0</code>
  <code class="p">),</code>
  <code class="c1"># Reduced capacity by lowering conv filters.</code>
  <code class="s2">"fewer_conv_channels"</code><code class="p">:</code> <code class="n">ConvTransformerModel</code><code class="p">(</code><code class="n">conv_filters</code><code class="o">=</code><code class="mi">8</code><code class="p">),</code>
  <code class="c1"># Drop the MLP layers to test if they help.</code>
  <code class="s2">"remove_MLP"</code><code class="p">:</code> <code class="n">ConvTransformerModel</code><code class="p">(</code><code class="n">num_mlp_blocks</code><code class="o">=</code><code class="mi">0</code><code class="p">),</code>
  <code class="c1"># Potential improvements: Add more expressive capacity.</code>
  <code class="c1"># Add a transformer block after convolutions.</code>
  <code class="s2">"add_one_transformer_block"</code><code class="p">:</code> <code class="n">ConvTransformerModel</code><code class="p">(</code><code class="n">num_transformer_blocks</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
  <code class="c1"># Stack two transformer blocks.</code>
  <code class="s2">"add_two_transformer_block"</code><code class="p">:</code> <code class="n">ConvTransformerModel</code><code class="p">(</code><code class="n">num_transformer_blocks</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
<code class="p">}</code>
</pre>
      
     
    
   

   <p>To test these variants, we’ll focus on the most difficult TF from our earlier results: ZNF24, which achieved a peak validation auROC of 0.76 in our initial model. This makes it a great candidate to explore whether architectural improvements can lead to better predictive performance.
   </p>
  </div></section>

  <section data-type="sect2" data-pdf-bookmark="Sweeping Over the Different Models"><div class="sect2" id="sweeping-over-different-models">
   <h2>Sweeping Over the Different Models</h2>
   <p>
    <a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="advanced modeling techniques" data-tertiary="comparing architectural choices" data-type="indexterm" id="ch03_dna.html44"/>With all our model components now modularized, we can easily run systematic experiments to compare architectural choices.
   </p>
   <p>We’ll train several model variants—ranging from simpler ablations to transformer-enhanced architectures—on the most challenging TF in our benchmark: ZNF24. This allows us to explore which components help or hurt performance on a relatively difficult task.</p>
   <p>With all of this in place, we can train the different models in the <code>model</code> dict:</p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="c1"># Train and evaluate multiple model architectures on the ZNF24 dataset.</code>
<code class="n">transcription_factor</code> <code class="o">=</code> <code class="s2">"ZNF24"</code>
<code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">load_dataset_splits</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="s2">"dna/datasets"</code><code class="p">),</code> <code class="n">transcription_factor</code><code class="p">,</code> <code class="n">batch_size</code>
<code class="p">)</code>

<code class="c1"># Prepare a dummy input for model initialization.</code>
<code class="n">dummy_input</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">())[</code><code class="s2">"sequences"</code><code class="p">]</code>

<code class="c1"># Initialize PRNGs.</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>

<code class="c1"># Dictionary to store metrics for each model variant.</code>
<code class="n">model_metrics</code> <code class="o">=</code> <code class="p">{}</code>

<code class="c1"># Train each model variant and store its metrics.</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dummy_input</code><code class="p">,</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code>
      <code class="n">optax</code><code class="o">.</code><code class="n">cosine_decay_schedule</code><code class="p">(</code>
        <code class="n">init_value</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">,</code>
        <code class="n">decay_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
      <code class="p">)</code>
    <code class="p">),</code>
  <code class="p">)</code>
  <code class="n">_</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
    <code class="n">state</code><code class="o">=</code><code class="n">state</code><code class="p">,</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
    <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
    <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
    <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
    <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="sa">f</code><code class="s2">"dna/models/</code><code class="si">{</code><code class="n">name</code><code class="si">}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">)</code>
  <code class="n">model_metrics</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="n">name</code><code class="p">:</code> <code class="n">metrics</code><code class="p">})</code>
</pre>
      
<p>After training each model, we extract logged metrics over training time and visualize the results:
</p>   

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Extract metrics logged per transcription factor.</code>
<code class="n">model_df</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">model</code><code class="p">,</code> <code class="n">metrics</code> <code class="ow">in</code> <code class="n">model_metrics</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="n">model_df</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">to_df</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">))</code>
<code class="n">model_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">(</code><code class="n">model_df</code><code class="p">)</code>

<code class="c1"># Determine order of best performance.</code>
<code class="n">auc_df</code> <code class="o">=</code> <code class="n">model_df</code><code class="p">[</code>
  <code class="p">(</code><code class="n">model_df</code><code class="p">[</code><code class="s2">"metric"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"auc"</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">model_df</code><code class="p">[</code><code class="s2">"split"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"valid"</code><code class="p">)</code>
<code class="p">]</code>
<code class="n">max_auc_by_model</code> <code class="o">=</code> <code class="n">auc_df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"model"</code><code class="p">)[</code><code class="s2">"mean"</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
<code class="n">model_order</code> <code class="o">=</code> <code class="n">max_auc_by_model</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>
<code class="n">model_df</code><code class="p">[</code><code class="s2">"model"</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Categorical</code><code class="p">(</code>
  <code class="n">model_df</code><code class="p">[</code><code class="s2">"model"</code><code class="p">],</code> <code class="n">categories</code><code class="o">=</code><code class="n">model_order</code><code class="p">,</code> <code class="n">ordered</code><code class="o">=</code><code class="kc">True</code>
<code class="p">)</code>
</pre>
    
   
   <p>
    With this code, we can plot our learning curves and metrics over time in <a data-type="xref" href="#learning-curve-panels-model-variants-znf24">Figure 3-19</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">sns</code><code class="o">.</code><code class="n">set_context</code><code class="p">(</code><code class="s2">"notebook"</code><code class="p">,</code> <code class="n">font_scale</code><code class="o">=</code><code class="mf">1.2</code><code class="p">,</code> <code class="n">rc</code><code class="o">=</code><code class="p">{</code><code class="s2">"lines.linewidth"</code><code class="p">:</code> <code class="mf">2.5</code><code class="p">})</code>
<code class="n">sns</code><code class="o">.</code><code class="n">set_style</code><code class="p">(</code><code class="s2">"ticks"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"axes.grid"</code><code class="p">:</code> <code class="kc">True</code><code class="p">})</code>
<code class="n">g</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">relplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">model_df</code><code class="p">,</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"round"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"mean"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"split"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"metric"</code><code class="p">,</code>
  <code class="n">kind</code><code class="o">=</code><code class="s2">"line"</code><code class="p">,</code>
  <code class="n">col</code><code class="o">=</code><code class="s2">"model"</code><code class="p">,</code>
  <code class="n">col_order</code><code class="o">=</code><code class="n">model_order</code><code class="p">,</code>
  <code class="n">col_wrap</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
  <code class="n">alpha</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code>
  <code class="n">palette</code><code class="o">=</code><code class="n">DEFAULT_SPLIT_COLORS</code><code class="p">,</code>
  <code class="n">dashes</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">g</code><code class="o">.</code><code class="n">set_axis_labels</code><code class="p">(</code><code class="s2">"Step"</code><code class="p">,</code> <code class="s2">"Value"</code><code class="p">)</code>
<code class="n">g</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mf">0.4</code><code class="p">,</code> <code class="mf">0.9</code><code class="p">));</code>
</pre>
      
     
    
    
     <figure><div id="learning-curve-panels-model-variants-znf24" class="figure">
      <img alt="" src="assets/dlfb_0319.png" width="600" height="795"/>
      <h6><span class="label">Figure 3-19. </span>Learning curves for different model architectures on ZNF24 binding prediction. Each panel shows training and validation loss, accuracy, and auROC. We see that adding transformer blocks slightly improves performance, while removing capacity (e.g., fewer conv filters) hurts it.
      </h6>
     </div></figure>
    
   
   <p>To better isolate model differences, we can plot just the validation auROC over time in <a data-type="xref" href="#comparison-of-model-variants-znf24">Figure 3-20</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">g</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">lineplot</code><code class="p">(</code>
  <code class="n">data</code><code class="o">=</code><code class="n">model_df</code><code class="p">[(</code><code class="n">model_df</code><code class="p">[</code><code class="s2">"metric"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"auc"</code><code class="p">)],</code>
  <code class="n">x</code><code class="o">=</code><code class="s2">"round"</code><code class="p">,</code>
  <code class="n">y</code><code class="o">=</code><code class="s2">"mean"</code><code class="p">,</code>
  <code class="n">hue</code><code class="o">=</code><code class="s2">"model"</code><code class="p">,</code>
  <code class="n">style</code><code class="o">=</code><code class="s2">"model"</code><code class="p">,</code>
  <code class="n">alpha</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">g</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Step"</code><code class="p">)</code>
<code class="n">g</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"auROC"</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="comparison-of-model-variants-znf24" class="figure">
      <img alt="" src="assets/dlfb_0320.png" width="600" height="445"/>
      <h6><span class="label">Figure 3-20. </span>Validation auROC across training steps for each model variant. Models with transformer blocks outperform simpler baselines, while reducing convolutional capacity or removing MLPs tends to hurt performance.
      </h6>
     </div></figure>
    
   
   <p>And here are the max auROC values for each model (by step 1,000):
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="nb">print</code><code class="p">(</code><code class="n">max_auc_by_model</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">))</code>
</pre>
 
     
   <p>Output:</p>   
       <pre data-type="programlisting">model
add_two_transformer_block    0.770343
add_one_transformer_block    0.769196
baseline                     0.756317
remove_MLP                   0.752024
single_conv_only             0.719931
fewer_conv_channels          0.697637
Name: mean, dtype: float64
</pre>

<p class="pagebreak-before">Some observations and hypotheses based on these results:
   </p>
   <dl>
   <dt>Convolutions are critical</dt>
    <dd>
     <p>The <code>single_conv_only</code> model underperforms relative to the baseline, and reducing the number of convolutional filters degrades performance further. This suggests that having multiple convolutional layers with sufficient capacity is important for learning meaningful sequence features. It may be worth exploring deeper convolutional stacks or wider kernels.
     </p>
    </dd>
    <dt>Transformer blocks help</dt>
    <dd>
     <p>Adding one or two self-attention blocks gives a modest but consistent improvement in auROC, despite the input sequences being only 200 bp long. This supports the idea that even short DNA windows can benefit from modeling long-range dependencies.
     </p>
    </dd>
    <dt>MLPs may be unnecessary</dt>
    <dd>
     <p>Surprisingly, removing the MLP blocks doesn’t drastically hurt performance. This suggests that most of the representational power is coming from earlier convolutional layers, and the additional MLP layers may be redundant.
     </p>
    </dd>
    <dt>Training dynamics</dt>
    <dd>Models with transformer blocks exhibit noisier validation loss during early training. This instability might be reduced with a smaller initial learning rate.
</dd>
   </dl>
   <p>While we won’t exhaustively optimize hyperparameters here, these results demonstrate the value of modular model exploration and raise intriguing questions for further study.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html44" data-type="indexterm" id="id717"/>
   </p>
</div></section>
  <section data-type="sect2" data-pdf-bookmark="Evaluating on the Test Split"><div class="sect2" id="evaluating_on_the_test_split">
   <h2>Evaluating on the Test Split</h2>
<p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="advanced modeling techniques" data-tertiary="evaluating on the test split" data-type="indexterm" id="id718"/>The final step is to evaluate our best-performing model on the held-out test set. This ensures that the model’s performance generalizes to completely unseen data and was not overfit to the training or validation distributions.</p>
<p>We reload the checkpoint for the top model—selected based on the highest validation auROC—and evaluate it on the test split:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="c1"># Identify best-performing model variant based on validation auROC.</code>
<code class="n">top_model</code> <code class="o">=</code> <code class="n">model_order</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

<code class="c1"># Restore the trained model state from disk.</code>
<code class="n">state</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">restore</code><code class="p">(</code>
  <code class="n">assets</code><code class="p">(</code><code class="sa">f</code><code class="s2">"dna/models/</code><code class="si">{</code><code class="n">top_model</code><code class="si">}</code><code class="s2">"</code><code class="p">),</code>
  <code class="n">models</code><code class="p">[</code><code class="n">top_model</code><code class="p">]</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="nb">next</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">())[</code><code class="s2">"sequences"</code><code class="p">],</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code>
      <code class="n">optax</code><code class="o">.</code><code class="n">cosine_decay_schedule</code><code class="p">(</code>
        <code class="n">init_value</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">,</code>
        <code class="n">decay_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
      <code class="p">)</code>
    <code class="p">),</code>
  <code class="p">),</code>
<code class="p">)</code>

<code class="c1"># Evaluate on the held-out test set.</code>
<code class="n">test_batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"test"</code><code class="p">]</code><code class="o">.</code><code class="n">as_numpy_iterator</code><code class="p">())</code>
<code class="n">metrics</code> <code class="o">=</code> <code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">test_batch</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">{'loss': 0.5123618841171265, 'accuracy': 0.75, 'auc': 0.8196078431372549}
</pre>

<p>Success! Our best model achieves a comparable AUC on the test set to what we saw on the validation set, demonstrating good generalization to unseen data.</p>

    </div></section>
 <section data-type="sect2" data-pdf-bookmark="Extensions and Improvements"><div class="sect2" id="extensions-and-improvements">
  <h2>Extensions and Improvements</h2>
  
  <p><a contenteditable="false" data-primary="DNA, learning the logic of" data-secondary="advanced modeling techniques" data-tertiary="extensions and improvements" data-type="indexterm" id="ch03_dna.html45"/>There are many ways to extend this work—in terms of both analyzing model behavior and exploring new modeling directions.</p>
  <p>A few analysis ideas:</p>
  <dl>
   <dt>Failure analysis</dt>
   <dd>Inspect misclassified sequences. Do they correspond to originally noisy or lower-magnitude ChIP-seq peaks? Are there weak motif matches within them? Trace these sequences back to the raw data to understand the source of prediction errors.</dd>
   <dt>Motif discovery</dt>
   <dd>Use saliency maps to extract high-signal regions and align them to known motifs from databases like JASPAR or HOCOMOCO. You can also explore tools like TF-MoDISco for automatic motif discovery based on contribution scores.</dd>
   <dt>Attribution comparisons</dt>
   <dd>Compare in silico mutagenesis and input gradients across TFs. Do easier tasks yield more consistent or interpretable saliency patterns?</dd>
   <dt>Cross-TF generalization</dt>
   <dd>Evaluate a model trained on one TF against the validation data for another. Which TFs generalize well to others? This could reveal shared motif features or similarities in binding logic.</dd>
   <dt>Saliency reproducibility</dt>
   <dd>Compare saliency maps across different training runs or architectures to assess how reliably motif patterns are captured.</dd>
  </dl>
  <p>And some potential modeling extensions:</p>
  <dl>
   <dt>Model optimization</dt>
   <dd>Tune architectural hyperparameters such as the number of convolutional filters, kernel width, dropout rate, or transformer head count and MLP size.</dd>
   <dt>Positional encodings</dt>
   <dd>Add sinusoidal or learned positional embeddings to improve transformer modeling of sequence order—especially valuable for longer DNA sequences.</dd>
   <dt>Multitask setup</dt>
   <dd>Build a multitask model that predicts binding for all TFs simultaneously. This allows shared representations across tasks and may improve performance on lower-data TFs with less data.</dd>
   <dt>Quantitative binding prediction</dt>
   <dd>Instead of binary classification, predict continuous binding intensity (e.g., ChIP-seq signal). This would require adapting the model for sequence-to-sequence or dense regression output.</dd>
   <dt>Pretraining</dt>
   <dd>Fine-tune pretrained DNA models such as DNABERT or Nucleotide Transformer to leverage prior genomic knowledge and improve performance with less data.</dd>
   <dt>Data augmentation</dt>
   <dd>Improve generalization with reverse complements, shifted windows (jittering), or synthetic motif insertions.</dd>
  </dl>
  <p>These directions offer exciting possibilities for improving accuracy, interpretability, and generalizability in TF binding prediction<a contenteditable="false" data-primary="" data-startref="ch03_dna.html45" data-type="indexterm" id="id719"/>.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html40" data-type="indexterm" id="id720"/></p>
  
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="summary_122286992">
  <h1>Summary</h1>
  <p>In this chapter, we explored the fascinating world of gene regulation through transcription factor binding. Starting with simple convolutional models, we built a foundation for understanding how neural networks can learn to recognize sequence motifs in raw DNA sequences. From there, we incrementally increased model complexity—adding batch normalization, dropout, and even transformer blocks—to investigate how architectural changes impact performance.
  </p>
  <p>This gradual, modular approach to model development not only helped clarify the strengths of each component, but also made debugging and evaluation more manageable. Along the way, we touched on model interpretability, performance benchmarking, and ideas for deeper analysis and extensions.
  </p>
  <p>Up to this point, we’ve focused on biological data with an inherent sequential structure—first with protein sequences in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#learning-the-language-of-proteins">Chapter 2, “Learning the Language of Proteins”</a> and now with DNA. In the next chapter, we shift gears to a very different kind of data: <em>graphs</em>. We’ll explore how graph neural networks can help us reason about relationships between entities—in particular, interactions between different drugs.<a contenteditable="false" data-primary="" data-startref="ch03_dna.html0" data-type="indexterm" id="id721"/></p>
 </div></section>
<div data-type="footnotes"><p data-type="footnote" id="id616"><sup><a href="ch03.html#id616-marker">1</a></sup> International Human Genome Sequencing Consortium, “Initial Sequencing and Analysis of the Human Genome.” Nature 409 (2001): 860–921.</p><p data-type="footnote" id="id638"><sup><a href="ch03.html#id638-marker">2</a></sup> Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <a href="https://oreil.ly/-Qa6G">“Attention Is All You Need”</a>. <em>arXiv (Cornell University)</em> 30 (June): 5998–6008.</p><p data-type="footnote" id="id670"><sup><a href="ch03.html#id670-marker">3</a></sup> Tang, Z., Somia, N., Yu, Y., &amp; Koo, P. K. (2024). <a href="https://doi.org/10.1101/2024.02.29.582810">Evaluating the representational power of pre-trained DNA language models for regulatory genomics</a>. bioRxiv (Cold Spring Harbor Laboratory).</p><p data-type="footnote" id="id671"><sup><a href="ch03.html#id671-marker">4</a></sup> Majdandzic, A., Rajesh, C., &amp; Koo, P. K. (2023). <a href="https://doi.org/10.1186/s13059-023-02956-3">Correcting gradient-based interpretations of deep neural networks for genomics</a>. Genome Biology, 24(1).</p></div></div></section></div></div></body></html>