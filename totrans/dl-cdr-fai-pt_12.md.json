["```py\n!pip install kaggle\n```", "```py\ncreds = ''\n```", "```py\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write(creds)\n    cred_path.chmod(0o600)\n```", "```py\npath = URLs.path('bluebook')\npath\n```", "```py\nPath('/home/sgugger/.fastai/archive/bluebook')\n```", "```py\nif not path.exists():\n    path.mkdir()\n    api.competition_download_cli('bluebook-for-bulldozers', path=path)\n    file_extract(path/'bluebook-for-bulldozers.zip')\n\npath.ls(file_type='text')\n```", "```py\n(#7) [Path('Valid.csv'),Path('Machine_Appendix.csv'),Path('ValidSolution.csv'),P\n > ath('TrainAndValid.csv'),Path('random_forest_benchmark_test.csv'),Path('Test.\n > csv'),Path('median_benchmark.csv')]\n```", "```py\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\n```", "```py\ndf.columns\n```", "```py\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n```", "```py\ndf['ProductSize'].unique()\n```", "```py\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'],\n > dtype=object)\n```", "```py\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\n```", "```py\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n```", "```py\ndep_var = 'SalePrice'\n```", "```py\ndf[dep_var] = np.log(df[dep_var])\n```", "```py\ndf = add_datepart(df, 'saledate')\n```", "```py\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n```", "```py\n' '.join(o for o in df.columns if o.startswith('sale'))\n```", "```py\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear\n > saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start\n > saleIs_year_end saleIs_year_start saleElapsed'\n```", "```py\nprocs = [Categorify, FillMissing]\n```", "```py\ncond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n```", "```py\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n```", "```py\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n```", "```py\nlen(to.train),len(to.valid)\n```", "```py\n(404710, 7988)\n```", "```py\nto.show(3)\n```", "```py\nto.items.head(3)\n```", "```py\nto.classes['ProductSize']\n```", "```py\n(#7) ['#na#','Large','Large / Medium','Medium','Small','Mini','Compact']\n```", "```py\n(path/'to.pkl').save(to)\n```", "```py\nto = (path/'to.pkl').load()\n```", "```py\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n```", "```py\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n```", "```py\ndraw_tree(m, xs, size=7, leaves_parallel=True, precision=2)\n```", "```py\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n```", "```py\nxs.loc[xs['YearMade']<1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950\n```", "```py\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n```", "```py\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n```", "```py\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n```", "```py\nm_rmse(m, xs, y)\n```", "```py\n0.0\n```", "```py\nm_rmse(m, valid_xs, valid_y)\n```", "```py\n0.337727\n```", "```py\nm.get_n_leaves(), len(xs)\n```", "```py\n(340909, 404710)\n```", "```py\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n```", "```py\n(0.248562, 0.32368)\n```", "```py\nm.get_n_leaves()\n```", "```py\n12397\n```", "```py\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n```", "```py\nm = rf(xs, y);\n```", "```py\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n```", "```py\n(0.170896, 0.233502)\n```", "```py\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n```", "```py\nr_mse(preds.mean(0), valid_y)\n```", "```py\n0.233502\n```", "```py\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n```", "```py\nr_mse(m.oob_prediction_, y)\n```", "```py\n0.210686\n```", "```py\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n```", "```py\npreds.shape\n```", "```py\n(40, 7988)\n```", "```py\npreds_std = preds.std(0)\n```", "```py\npreds_std[:5]\n```", "```py\narray([0.21529149, 0.10351274, 0.08901878, 0.28374773, 0.11977206])\n```", "```py\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n```", "```py\nfi = rf_feat_importance(m, xs)\nfi[:10]\n```", "```py\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n```", "```py\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\n```", "```py\n21\n```", "```py\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n```", "```py\nm = rf(xs_imp, y)\n```", "```py\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n```", "```py\n(0.181208, 0.232323)\n```", "```py\nlen(xs.columns), len(xs_imp.columns)\n```", "```py\n(78, 21)\n```", "```py\nplot_fi(rf_feat_importance(m, xs_imp));\n```", "```py\ncluster_columns(xs_imp)\n```", "```py\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n```", "```py\nget_oob(xs_imp)\n```", "```py\n0.8771039618198545\n```", "```py\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n```", "```py\n{'saleYear': 0.8759666979317242,\n 'saleElapsed': 0.8728423449081594,\n 'ProductGroupDesc': 0.877877012281002,\n 'ProductGroup': 0.8772503407182847,\n 'fiModelDesc': 0.8756415073829513,\n 'fiBaseModel': 0.8765165299438019,\n 'Hydraulics_Flow': 0.8778545895742573,\n 'Grouser_Tracks': 0.8773718142788077,\n 'Coupler_System': 0.8778016988955392}\n```", "```py\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n```", "```py\n0.8739605718147015\n```", "```py\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n```", "```py\n(path/'xs_final.pkl').save(xs_final)\n(path/'valid_xs_final.pkl').save(valid_xs_final)\n```", "```py\nxs_final = (path/'xs_final.pkl').load()\nvalid_xs_final = (path/'valid_xs_final.pkl').load()\n```", "```py\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n```", "```py\n(0.183263, 0.233846)\n```", "```py\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n```", "```py\nax = valid_xs_final['YearMade'].hist()\n```", "```py\nfrom sklearn.inspection import plot_partial_dependence\n\nfig,ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, ['YearMade','ProductSize'],\n                        grid_resolution=20, ax=ax);\n```", "```py\n!pip install treeinterpreter\n!pip install waterfallcharts\n```", "```py\nrow = valid_xs_final.iloc[:5]\n```", "```py\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n```", "```py\nprediction[0], bias[0], contributions[0].sum()\n```", "```py\n(array([9.98234598]), 10.104309759725059, -0.12196378442186026)\n```", "```py\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08,\n          rotation_value=45,formatting='{:,.3f}');\n```", "```py\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n```", "```py\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n```", "```py\n(torch.Size([40]), torch.Size([40, 1]))\n```", "```py\nx_lin[:,None].shape\n```", "```py\ntorch.Size([40, 1])\n```", "```py\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n```", "```py\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n```", "```py\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n```", "```py\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n```", "```py\norig 0.232795\nSalesID 0.23109\nsaleElapsed 0.236221\nMachineID 0.233492\n```", "```py\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n```", "```py\n0.231307\n```", "```py\nxs['saleYear'].hist();\n```", "```py\nfilt = xs['saleYear']>2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n```", "```py\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n```", "```py\n(0.17768, 0.230631)\n```", "```py\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n```", "```py\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n```", "```py\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n```", "```py\ncont_nn.append('saleElapsed')\ncat_nn.remove('saleElapsed')\n```", "```py\ndf_nn_final[cat_nn].nunique()\n```", "```py\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nModelID               5281\nHydraulics_Flow          3\nfiSecondaryDesc        177\nfiModelDesc           5059\nProductGroup             6\nEnclosure                6\nfiModelDescriptor      140\nDrive_System             4\nHydraulics              12\nTire_Size               17\ndtype: int64\n```", "```py\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n```", "```py\n(0.176706, 0.230642)\n```", "```py\ncat_nn.remove('fiModelDescriptor')\n```", "```py\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n```", "```py\ndls = to_nn.dataloaders(1024)\n```", "```py\ny = to_nn.train.y\ny.min(),y.max()\n```", "```py\n(8.465899897028686, 11.863582336583399)\n```", "```py\nfrom fastai.tabular.all import *\n```", "```py\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n```", "```py\nlearn.lr_find()\n```", "```py\n(0.005754399299621582, 0.0002754228771664202)\n```", "```py\nlearn.fit_one_cycle(5, 1e-2)\n```", "```py\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n```", "```py\n0.2258\n```", "```py\nlearn.save('nn')\n```", "```py\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n```", "```py\nr_mse(ens_preds,valid_y)\n```", "```py\n0.22291\n```"]