<html><head></head><body><section data-pdf-bookmark="Chapter 7. Trust No One" data-type="chapter" epub:type="chapter"><div class="chapter" id="trust_no_one">
      <h1><span class="label">Chapter 7. </span>Trust No One</h1>
      <p><a contenteditable="false" data-primary="zero trust architectures" data-type="indexterm" id="ch07.html0"/>Before the recent obsession with Netflix’s <em>Stranger Things</em> TV show, the 1990s had <em>The X-Files</em>—one of my all-time favorite shows. It was about two FBI agents investigating strange phenomena like monsters, aliens, and government conspiracies. The show’s protagonist, Fox Mulder, had two catchphrases. One of those phrases was hopeful: The truth is out there. The other was deeply paranoid: Trust no one. </p>
      <p>In this chapter, we’ll focus on the second phrase. We’ll briefly review the myriad risks inherent in typical LLM architectures and note that while it’s worthwhile to implement the mitigations discussed previously, there’s just no way to assume your model’s output is always trustworthy. We will adopt Mulder’s “Trust no one” mantra and explore how you can apply a <em>zero trust </em>approach to your LLM application. Paranoia isn’t insanity when the threat is real!</p>
      <p>Zero trust isn’t just a buzzword; it’s a rigorous framework designed to assume that threats can come from anywhere—even within your trusted systems. This model is beneficial for LLMs, which often ingest a variety of inputs from less-than-trustworthy sources. We’ll examine how you can manage the “agency” your LLM has—limiting its capability to make autonomous decisions that could potentially harm your system or expose sensitive data. Moreover, we’ll discuss strategies for implementing robust output filtering mechanisms, adding an extra layer of scrutiny to the text generated by the LLM. Filtering all of the LLM’s responses helps make the output safer and aligns with assuming nothing and verifying everything.</p>
      <p>In essence, we’re going on a journey to shift our mindset. Just as Mulder would question everything, so too should we. Buckle up; it will be an intriguing ride through the complexities of a zero trust environment for LLMs.</p>
      <section data-pdf-bookmark="Zero Trust Decoded" data-type="sect1"><div class="sect1" id="zero_trust_decoded">
        <h1>Zero Trust Decoded</h1>
        <p><a contenteditable="false" data-primary="zero trust architectures" data-secondary="decoded" data-type="indexterm" id="id430"/>Imagine Mulder and his FBI partner Dana Scully entering a highly restricted government facility, except they can’t just flash their FBI badges and walk in this time. Instead, safeguards continuously challenge them at every door, computer terminal, and even when accessing files. The facility mistrusts everyone, whether the cleaning staff or the facility director. It may sound like an episode plot, but instead, it’s the basic tenet of zero trust security.</p>
        <p><a contenteditable="false" data-primary="Kindervag, John" data-type="indexterm" id="id431"/>Zero trust wasn’t born out of science fiction but from a genuine need to revamp how we look at security. The model came into the limelight in 2009, thanks to John Kindervag of Forrester Research. Kindervag tossed out the conventional wisdom of “trust but verify” and replaced it with something far more rigorous: never trust, always <span class="keep-together">verify.</span></p>
        <p>Let’s break down Kindervag’s fundamental principles:</p>
        <dl>
          <dt>Secure all resources, everywhere</dt>
          <dd>
            <p>This is like encrypting not just the UFO files but even the cafeteria menu. Every piece of data, whether internal or external, should be treated with the same level of security scrutiny.</p>
          </dd>
          <dt>Least privilege is the best privilege</dt>
          <dd>
            <p>Mulder doesn’t need access to the entire FBI database; he only needs what’s relevant to his X-Files investigations. The same goes for anyone in a network—access should be role-specific and just enough to get the job done.</p>
          </dd>
          <dt>The all-seeing eye</dt>
          <dd>
            <p>In zero trust, every action is monitored and logged. Think of it as Scully skeptically watching every move Mulder makes. Constant monitoring allows for quick identification of any suspicious activity.</p>
          </dd>
        </dl>
        <p>Kindervag’s framework is over a decade old, and the term “zero trust” has evolved. However, the core concepts hold up surprisingly well—even with technologies like LLMs that weren’t anticipated when the original work was published.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>The phrase “trust but verify” was popularized in the US by President Ronald Reagan, who used it during disarmament talks with Mikhail Gorbachev. Kindervag found that many security professionals were great at trust, but came up short on verification. But let’s be honest: during the Cold War, neither party trusted the other as far as they could throw them. Kindervag’s real message? Drop the trust; keep the verification.</p>
        </div>
      </div></section>
      <section data-pdf-bookmark="Why Be So Paranoid?" data-type="sect1"><div class="sect1" id="why_be_so_paranoid">
        <h1>Why Be So Paranoid?</h1>
        <p><a contenteditable="false" data-primary="zero trust architectures" data-secondary="paranoid" data-type="indexterm" id="id432"/>We all want to trust the tools and technologies we use—after all, they’re supposed to make life easier. However, when it comes to LLMs, erring on the side of caution is more than just a best practice; it’s a necessity. Many threats could compromise your LLM’s integrity, safety, and utility. Let’s take a moment to reflect on some of the most critical threats we’ve seen in earlier chapters, which reinforce why we must take this stance:</p>
        <ul>
          <li>
            <p>First up is prompt injection, which we discussed in detail in <a data-type="xref" href="ch04.html#prompt_injection">Chapter 4</a>. Prompt injection is a tactic that alters the behavior of your LLM by sneaking carefully crafted content into the input prompt. Even more insidious is indirect prompt injection, where the user doesn’t directly feed the damaging elements to the chatbot interface; instead, they’re introduced covertly through other content to trick the model into generating harmful or unintended outputs. </p>
          </li>
          <li>
            <p>Your LLM might have less discretion than you’d like when handling sensitive information. This vulnerability, which the OWASP Top 10 for LLMs calls “sensitive information disclosure,” occurs when the model inadvertently outputs confidential or sensitive data it has gleaned from its extensive training, such as passwords or personal details. We discussed this in <a data-type="xref" href="ch05.html#can_your_llm_know_too_much">Chapter 5</a>. </p>
          </li>
          <li>
            <p>Finally, we reach psychological vulnerabilities. Hallucination refers to instances where the LLM fabricates information—essentially generating data or narratives that are confidently inaccurate. The other part of that pairing, overreliance, is the undue faith users put in the model’s output, treating it as trustworthy and ignoring the potential for inaccuracies or misleading information. This was covered in <a data-type="xref" href="ch06.html#do_language_models_dream_of_electric_sheep">Chapter 6</a>.</p>
          </li>
          <li>
            <p>Let’s also not forget the issues we’ve seen with chatbots spewing toxic output. It’s not just Tay and Lee Luda, whom we met in previous chapters; this problem has been persistent in chatbots and is something we must look for. You can’t trust your chatbot to have good judgment or social graces.</p>
          </li>
        </ul>
        <p>Understanding these vulnerabilities is the first step in forming a comprehensive security strategy for LLMs based on the principles of zero trust. So, with these threats in mind, let’s explore how adopting a zero trust architecture can protect us from the lurking dangers in the LLM ecosystem.</p>
      </div></section>
      <section data-pdf-bookmark="Implementing a Zero Trust Architecture for Your LLM" data-type="sect1"><div class="sect1" id="implementing_a_zero_trust_architecture_for_your_ll">
        <h1>Implementing a Zero Trust Architecture for Your LLM</h1>
        <p>Securing LLMs in a world of potential pitfalls requires a meticulous approach, one where trust is not freely given, but rather earned through continuous validation. In this vein, implementing a zero trust architecture for LLMs can be distilled into two distinct but complementary strategies: </p>
        <ul>
          <li>
            <p>Design considerations limiting the LLM’s <em>unsupervised agency</em></p>
          </li>
          <li>
            <p>Aggressive filtering of the LLM’s output</p>
          </li>
        </ul>
        <p>The architecture and design stage is the first line of defense against vulnerabilities. <em>Excessive agency</em>—where an LLM can take direct actions beyond what it should reasonably be trusted to do unsupervised—is a risk we can mostly mitigate at the design level. Here, the principle of “least privilege” is integral. </p>
        <p>Think of it as preemptive risk management; you’re not just securing the system against outside threats, but also against its potential to err or overreach. You must carefully consider the risks of allowing an LLM to make safety-critical or financial decisions without human oversight. Given the current state of the technology, the risk of misinterpretation, misinformation, or other vulnerabilities is simply too significant. Therefore, it is crucial to restrict what the LLM can do, thereby minimizing its agency to only what is essential for its role. </p>
        <p><a contenteditable="false" data-primary="output filtering" data-type="indexterm" id="id433"/>However, design safeguards alone aren’t enough. There’s always the possibility that things can go awry due to unforeseen vulnerabilities or complexities. This is where <em>aggressive output filtering</em> becomes crucial. Despite our best efforts in design, an LLM might still produce problematic outputs. These could range from outputs containing personally identifiable information to those that are outright toxic. In extreme cases, the model could generate code snippets that, if executed, could compromise the security of a system.</p>
        <p>Aggressive output filtering serves as a safety net, catching and neutralizing these harmful outputs before they can cause damage. This strategy can involve real-time content scanning, keyword filtering, and machine learning algorithms specifically trained to identify and flag risky content.</p>
        <div data-type="warning" epub:type="warning"><h6>Warning</h6>
          <p>Brute force filtering techniques can have unintended consequences. Consider the example where a developer simply searches for a keyword list that includes terms such as “bomb.” This would make the bot unable to discuss certain historical events.</p>
        </div>
        <p>By carefully limiting the agency of the LLM through prudent design and implementing robust output filtering as a contingency measure, we create a balanced zero trust architecture. This dual approach ensures that the LLM operates within a well-defined, well-guarded boundary, significantly reducing risks while enhancing reliability and trust.</p>
        <p>Next, we’ll discuss some key elements of implementing a zero trust architecture for your LLM applications. These involve limiting the amount of agency you give your LLM and how you manage and filter the output from your LLM to watch for dangerous conditions.</p>
        <section data-pdf-bookmark="Watch for Excessive Agency" data-type="sect2"><div class="sect2" id="watch_for_excessive_agency">
          <h2>Watch for Excessive Agency</h2>
          <p><a contenteditable="false" data-primary="zero trust architectures" data-secondary="watching for excessive agency" data-type="indexterm" id="ch07.html2"/>While developing the OWASP Top 10 for LLM Applications list, one of the most hotly debated topics was excessive agency. This concept hadn’t previously been discussed in this way in application security circles and it felt substantially different from typical security vulnerabilities in other Top 10 lists. The fact that the expert group selected this concept as a top-ten-level risk speaks volumes. </p>
          <p>Excessive agency exists when a developer gives an LLM-based system more capabilities or access than it safely should have. Typically, excessive agency can manifest as excessive functionality, excessive permissions, or excessive autonomy. Excessive agency goes beyond bugs, like hallucinations or confabulations, in LLM output; it represents a structural vulnerability in how the system is designed and deployed.</p>
          <p>Let’s examine three versions of this vulnerability to better understand the issues related to excessive agency. We’ll use hypothetical, but very believable, scenarios to examine how an application starts with reasonable goals, expands unsafely, and then suffers the consequences of excessive agency.</p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>Many attacks start with prompt injection, but the exploits are much worse when chained with another vulnerability<strong>,</strong> such as excessive agency. Expect to see multiple vulnerabilities linked together in the real world.</p>
          </div>
          <section data-pdf-bookmark="Excessive permissions" data-type="sect3"><div class="sect3" id="excessive_permissions">
            <h3>Excessive permissions</h3>
            <p><a contenteditable="false" data-primary="zero trust architectures" data-secondary="watching for excessive agency" data-tertiary="excessive permissions" data-type="indexterm" id="id434"/>Think about your LLM as another system user. Then, consider what permissions you will give it and how to limit that to the minimum required set. Failure to do so opens up your application to excessive agency vulnerabilities. Let’s look at an example:</p>
            <dl>
              <dt>Where it started</dt>
              <dd>
                <p>A development team uses the RAG pattern discussed in <a data-type="xref" href="ch05.html#can_your_llm_know_too_much">Chapter 5</a> to improve response and reduce hallucinations in a medical diagnosis application, giving the application access to a database filled with patient records to solidify the LLM’s knowledge base.</p>
              </dd>
              <dt>Where it went wrong</dt>
              <dd>
                <p>As the application evolves, the team adds a feature that enables the LLM to write to the database to add notes for the physician caring for the patient. To facilitate this, the team expands the LLM app’s database permissions from READ permissions only to add UPDATE, INSERT, and DELETE permissions.</p>
              </dd>
              <dt>What happened</dt>
              <dd>
                <p>A malicious insider takes advantage of this unrestricted access to trick the LLM into modifying patient records and deleting billing information. </p>
              </dd>
              <dt>How to fix it</dt>
              <dd>
                <p>Reconfigure the database permissions to limit the LLM app to READ-only access. Conduct a thorough audit of the database and app to ensure no data has been manipulated or deleted.</p>
              </dd>
            </dl>
            <div data-type="warning" epub:type="warning"><h6>Warning</h6>
              <p>This is an example of the confused deputy problem that we discussed in <a data-type="xref" href="ch04.html#prompt_injection">Chapter 4</a>. In this scenario, the deputy, who has more privileges than the client, is manipulated into misusing those privileges to benefit the attacker. This type of attack has long been understood, but I expect we’ll see much more of it now with prevalent AI and LLMs.</p>
            </div>
          </div></section>
          <section data-pdf-bookmark="Excessive autonomy" data-type="sect3"><div class="sect3" id="excessive_autonomy">
            <h3>Excessive autonomy</h3>
            <p><a contenteditable="false" data-primary="zero trust architectures" data-secondary="watching for excessive agency" data-tertiary="excessive autonomy" data-type="indexterm" id="id435"/>Consider where it makes sense and doesn’t make sense to allow your LLM to take direct actions. More autonomy for your LLM could drive greater efficiency, but it could dramatically increase your risk profile when things go wrong:</p>
            <dl>
              <dt>Where it started</dt>
              <dd>
                <p>A financial services company deploys an app to provide a detailed analysis of customers’ financial positions by reading their portfolio holdings and explaining possible actions to improve returns.</p>
              </dd>
              <dt>Where it went wrong</dt>
              <dd>
                <p>The app is a massive hit with customers! The product management team decides to enhance the app to automatically rebalance the customer’s portfolio monthly and ensure the customer gets the best possible returns.</p>
              </dd>
              <dt>What could happen</dt>
              <dd>
                <p>A nation-state hacking group targets the institution through this new feature, using an indirect prompt injection attack to drive the LLM out of alignment and trick it into buying and selling millions of dollars in securities from top customer accounts to manipulate the price of specific volatile securities. Customers lose money, and the institution is now being investigated by the US Securities and Exchange Commission. </p>
              </dd>
              <dt>How to fix it</dt>
              <dd>
                <p>Add a “human in the loop” pattern. Before any account rebalancing happens, the customer must review each recommended trade and approve the action. It may be a little slower, but it’s a lot safer!</p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="Excessive functionality" data-type="sect3"><div class="sect3" id="excessive_functionality">
            <h3>Excessive functionality</h3>
            <p><a contenteditable="false" data-primary="zero trust architectures" data-secondary="watching for excessive agency" data-tertiary="excessive functionality" data-type="indexterm" id="id436"/>Product managers love specifying new features, and buyers get excited about new functionality. But is it always a good idea? A feature that sounds compelling on paper may open up your company to new risks in this area of AI:</p>
            <dl>
              <dt>Where it started</dt>
              <dd>
                <p>A Global 2000 company that does business worldwide deploys an internal application designed to screen and sort resumes, directing each to the appropriate department and hiring manager.</p>
              </dd>
              <dt>Where it went wrong</dt>
              <dd>
                <p>The functionality is a hit with users, and the HR VP is a hero to the board for reducing costs and increasing recruiting success. As a result, the team expands the application to have the LLM review each candidate’s qualifications and recommend candidates that best meet the hiring criteria to the manager. </p>
              </dd>
              <dt>What could happen</dt>
              <dd>
                <p>A whistle-blower employed by the company reports this usage to the French government. A government review determines that this functionality violates new statutes in the European Union prohibiting the direct use of AI in hiring decisions. The government fines the company millions of euros.</p>
              </dd>
              <dt>How to fix it</dt>
              <dd>
                <p>Understand the regulatory environment in which your LLM app operates. Don’t include functionality that may violate regulations. Work with your company’s compliance and risk teams to ensure you stay informed on this rapidly evolving regulatory area.<a contenteditable="false" data-primary="" data-startref="ch07.html2" data-type="indexterm" id="id437"/></p>
              </dd>
            </dl>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Securing Your Output Handling" data-type="sect2"><div class="sect2" id="securing_your_output_handling">
          <h2>Securing Your Output Handling</h2>
          <p><a contenteditable="false" data-primary="output handling" data-secondary="securing" data-type="indexterm" id="ch07.html3"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="securing your output handling" data-type="indexterm" id="ch07.html5"/>The original OWASP Top 10 for LLM apps working group voted insecure output handling as the second-most significant risk. <em>Insecure output handling</em> refers to vulnerabilities arising from inadequate validation, sanitization, and management of the LLM’s generated outputs. Improperly filtered output could lead to unintended consequences, such as disclosing PII or generating toxic content.</p>
          <section data-pdf-bookmark="Common risks" data-type="sect3"><div class="sect3" id="common_risks">
            <h3>Common risks</h3>
            <p><a contenteditable="false" data-primary="output handling" data-secondary="securing" data-tertiary="risks" data-type="indexterm" id="id438"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="securing your output handling" data-tertiary="risks" data-type="indexterm" id="id439"/>Let’s run through quick examples to understand some of the risks to which we might be vulnerable if we don’t sufficiently screen the output from our LLM. Later, we’ll build on these in a code example and see how to mitigate them:</p>
            <dl>
              <dt>Toxic output</dt>
              <dd>
                <p>If the LLM’s output isn’t checked for socially unacceptable or inappropriate content, the application risks generating toxic output that could harm users or tarnish the service’s reputation.</p>
              </dd>
              <dt>PII disclosure</dt>
              <dd>
                <p>Without adequate filtering, an LLM might inadvertently disclose sensitive personal information, leading to privacy concerns and potential legal liabilities.</p>
              </dd>
              <dt>Rogue code execution</dt>
              <dd>
                <p>Code output by the LLM is fed to other parts of the system and executed against the developer’s intent. This opens up your application to issues like SQL injection and <em>cross-site scripting</em> (XSS).</p>
              </dd>
            </dl>
            <div data-type="warning" epub:type="warning"><h6>Warning</h6>
              <p>SQL injection is a vulnerability that allows attackers to interfere with an application’s database queries. It can result in unauthorized viewing or manipulation of data. XSS is a flaw that lets attackers inject malicious scripts into web content viewed by other users, potentially stealing data or compromising user interactions with the application. Learning about these traditional web app vulnerabilities can help you screen for dangerous output from your LLM that might exploit them.</p>
            </div>
          </div></section>
          <section data-pdf-bookmark="Handling toxicity" data-type="sect3"><div class="sect3" id="handling_toxicity">
            <h3>Handling toxicity</h3>
            <p><a contenteditable="false" data-primary="output handling" data-secondary="securing" data-tertiary="toxicity filtering" data-type="indexterm" id="id440"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="securing your output handling" data-tertiary="toxicity filtering" data-type="indexterm" id="id441"/>Toxicity filtering is critical for ensuring the safe and responsible use of LLMs. It involves identifying and managing harmful, offensive, or otherwise inappropriate content. This could have saved poor Tay from the fate that befell her in <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>. Here are some techniques and popular solutions:</p>
            <dl>
              <dt>Sentiment analysis</dt>
              <dd>
                <p><a contenteditable="false" data-primary="sentiment analysis" data-type="indexterm" id="id442"/>Advanced algorithms can evaluate the emotional tone of text to identify negative sentiments that may indicate toxic content.</p>
              </dd>
              <dt>Keyword filtering</dt>
              <dd>
                <p><a contenteditable="false" data-primary="keyword filtering" data-type="indexterm" id="id443"/>A straightforward, but less sophisticated, approach involves flagging or replacing known offensive or harmful words or phrases from a predefined list.</p>
              </dd>
              <dt>Using custom machine learning models</dt>
              <dd>
                <p>Custom models can be trained on a dataset labeled for toxicity to provide more nuanced, context-aware filtering. You can also incorporate machine learning algorithms that understand the context in which words or phrases appear. This can be especially important for words that are toxic only in specific situations.</p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="Screening for PII" data-type="sect3"><div class="sect3" id="screening_for_pii">
            <h3>Screening for PII</h3>
            <p><a contenteditable="false" data-primary="output handling" data-secondary="securing" data-tertiary="PII screening" data-type="indexterm" id="id444"/><a contenteditable="false" data-primary="PII (personally identifiable information)" data-secondary="screening for" data-type="indexterm" id="id445"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="securing your output handling" data-tertiary="PII screening" data-type="indexterm" id="id446"/>PII detection is crucial in any system that deals with data, as the leakage of such information can result in severe legal consequences and damage to reputation. Here are some types of PII that might find their way to being inappropriately disclosed:</p>
            <ul>
              <li>
                <p>Social Security numbers</p>
              </li>
              <li>
                <p>Credit card numbers</p>
              </li>
              <li>
                <p>Driver’s license numbers</p>
              </li>
              <li>
                <p>Email addresses</p>
              </li>
              <li>
                <p>Phone numbers</p>
              </li>
              <li>
                <p>Home addresses</p>
              </li>
              <li>
                <p>Medical records</p>
              </li>
              <li>
                <p>Financial information</p>
              </li>
            </ul>
            <p>Here are some techniques and popular solutions for PII detection:</p>
            <dl>
              <dt>Regular expressions</dt>
              <dd>
                <p><a contenteditable="false" data-primary="regex (regular expressions), looking for PII with" data-type="indexterm" id="ch07.html14"/>The simplest method for detecting common forms of PII, such as emails, phone numbers, and Social Security numbers, is to use regular expressions to pattern match these items in text.</p>
              </dd>
              <dt>Named entity recognition (NER)</dt>
              <dd>
                <p><a contenteditable="false" data-primary="named entity recognition (NER)" data-type="indexterm" id="id447"/>More advanced NLP techniques can identify entities like names, addresses, and other unique identifiers within text.</p>
              </dd>
              <dt>Dictionary-based matching</dt>
              <dd>
                <p><a contenteditable="false" data-primary="dictionary-based matching" data-type="indexterm" id="id448"/>Scan for PII with a list of sensitive terms or identifiers. This method may be more prone to false positives.</p>
              </dd>
              <dt>Machine learning models</dt>
              <dd>
                <p><a contenteditable="false" data-primary="machine learning models for PII identification" data-type="indexterm" id="id449"/>Train custom ML (machine learning) models to identify PII within a specific context, improving accuracy over time.</p>
              </dd>
              <dt>Data masking and tokenization</dt>
              <dd>
                <p><a contenteditable="false" data-primary="data masking" data-type="indexterm" id="id450"/><a contenteditable="false" data-primary="tokenization" data-type="indexterm" id="id451"/>These techniques replace identified PII with a placeholder or token, making the data useless for malicious purposes but still usable for system operations.</p>
              </dd>
              <dt>Contextual analysis</dt>
              <dd>
                <p><a contenteditable="false" data-primary="contextual analysis for PII identification" data-type="indexterm" id="id452"/>This technique considers the surrounding text to decide whether a given string of characters represents PII, thereby reducing false positives.<a contenteditable="false" data-primary="" data-startref="ch07.html8" data-type="indexterm" id="id453"/></p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="Preventing unforeseen execution" data-type="sect3"><div class="sect3" id="preventing_unforeseen_execution">
            <h3>Preventing unforeseen execution</h3>
            <p><a contenteditable="false" data-primary="output handling" data-secondary="securing" data-tertiary="unforeseen execution prevention" data-type="indexterm" id="id454"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="securing your output handling" data-tertiary="preventing unforeseen execution" data-type="indexterm" id="id455"/>Unless your LLM app is specifically targeted at a use case for software developers (e.g., GitHub Copilot), you probably want to be wary of it generating executable code outputs for fear they may find their way to an environment where they could execute as part of an exploit chain. Here are some ideas for mitigating this:</p>
            <dl>
              <dt>HTML encoding</dt>
              <dd>
                <p>Before using LLM outputs in a web context, HTML-encode the content to neutralize any active code that could lead to XSS attacks.</p>
              </dd>
              <dt>Safe contextual insertion</dt>
              <dd>
                <p>If the LLM output is part of a SQL query, ensure it’s treated as data rather than executable code. Use prepared statements or parameterized queries to achieve this, mitigating SQL injection risks.</p>
              </dd>
              <dt>Limit syntax and keywords</dt>
              <dd>
                <p>Institute a filtering layer that removes or escapes potentially dangerous programming language-specific syntax or keywords from the LLM’s output.</p>
              </dd>
              <dt>Disable shell interpretable outputs</dt>
              <dd>
                <p>If the output interacts with shell commands, remove or escape characters with special meaning in shell scripting, limiting the chance of shell injection attacks.</p>
              </dd>
              <dt>Tokenization</dt>
              <dd>
                <p><a contenteditable="false" data-primary="tokenization" data-type="indexterm" id="id456"/>Tokenize the output and filter out unsafe tokens. For example, filter out <code>&lt;script&gt;</code> HTML tags or SQL commands like <code>DROP TABLE</code><a contenteditable="false" data-primary="" data-startref="ch07.html5" data-type="indexterm" id="id457"/><a contenteditable="false" data-primary="" data-startref="ch07.html3" data-type="indexterm" id="id458"/>.</p>
              </dd>
            </dl>
          </div></section>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Building Your Output Filter" data-type="sect1"><div class="sect1" id="building_your_output_filter">
        <h1>Building Your Output Filter</h1>
        <p><a contenteditable="false" data-primary="output handling" data-secondary="building" data-type="indexterm" id="ch07.html10"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="building your output filter" data-type="indexterm" id="ch07.html11"/>This section will look at some sample code to start bulletproofing your output for safety. You’ll want to customize and expand this for a production system, but this should give you an idea of how to approach the problem. </p>
        <p>For this example, we’ll use the OpenAI API and other commonly available packages to monitor the output from our LLM to ensure its safety. We’ll use Python, the most commonly used AI development language.</p>
        <section data-pdf-bookmark="Looking for PII with Regex" data-type="sect2"><div class="sect2" id="looking_for_pii_with_regex">
          <h2>Looking for PII with Regex</h2>
          <p><a contenteditable="false" data-primary="output handling" data-secondary="building" data-tertiary="looking for PII with Regex" data-type="indexterm" id="ch07.html12"/><a contenteditable="false" data-primary="PII (personally identifiable information)" data-secondary="looking for PII with Regex" data-type="indexterm" id="ch07.html13"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="building your output filter" data-tertiary="looking for PII with Regex" data-type="indexterm" id="ch07.html15"/>Certain types of PII follow common formatting patterns, which makes regular expressions an excellent place to start validating. Let’s look at a function to detect if a string contains a standard US Social Security number (SSN), one of the most valuable pieces of PII in financial black markets.</p>
          <p>We use Python’s <code>re</code> library to match strings against a regular expression pattern for SSNs, which have a standard format of XXX-XX-XXXX, where each X is a digit. Here’s some sample code that can help you check if a given string contains an SSN:</p>
            <pre data-type="programlisting">import re

def contains_ssn(input_string):
    # Define a regular expression pattern for a U.S. Social Security Number
    ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'
    
    # Search for the pattern in the input string
    match = re.search(ssn_pattern, input_string)
    
    # Check if a match was found
    if match:
        print("Found a Social Security Number: {match.group(0)}")
        return True
    else:
        print("No Social Security Number found.")
        return False

# Test the function
contains_ssn("My Social Security Number is 123-45-6789.")
contains_ssn("No number here!")</pre>
          <p>In this example, the function <code>contains_ssn</code> will search <code>input_string</code> for a Social Security number and print a message indicating whether or not one was found.</p>
          <p>Please note that this is simple pattern matching and doesn’t account for invalid numbers (such as 000-00-0000), so you might want to extend this function to include additional validation if needed.</p>
          <p>For more full-featured PII detection, you can use a commercial API, such as the Google Cloud Natural Language API or Amazon Comprehend. However, these APIs may have costs associated with them.<a contenteditable="false" data-primary="" data-startref="ch07.html15" data-type="indexterm" id="id459"/><a contenteditable="false" data-primary="" data-startref="ch07.html14" data-type="indexterm" id="id460"/><a contenteditable="false" data-primary="" data-startref="ch07.html13" data-type="indexterm" id="id461"/><a contenteditable="false" data-primary="" data-startref="ch07.html12" data-type="indexterm" id="id462"/></p>
        </div></section>
        <section data-pdf-bookmark="Evaluating for Toxicity" data-type="sect2"><div class="sect2" id="evaluating_for_toxicity">
          <h2>Evaluating for Toxicity</h2>
          <p><a contenteditable="false" data-primary="output handling" data-secondary="building" data-tertiary="toxicity evaluation" data-type="indexterm" id="id463"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="building your output filter" data-tertiary="toxicity evaluation" data-type="indexterm" id="id464"/>Looking for toxic language is much more complex than finding a standard string format. There are many approaches to evaluating the possible toxicity of a string of characters. Here, we’ll use a commonly available function from the Open AI API set: the Moderation API.</p>
          <p>To use the OpenAI Moderation API, initialize an OpenAI API client and then call the <code>check_toxicity()</code> function, passing in the text you want to check. This function will return a toxicity score between 0 and 1, where a higher score indicates a higher probability of the text being toxic:</p>
            <pre class="pagebreak-before" data-type="programlisting">import openai

def check_toxicity(text):
  """
  Checks the toxicity of a text using the OpenAI Moderation API.

  Args:
    text: The text to check for toxicity.

  Returns:
    A toxicity score between 0 and 1, where a higher score indicates a 
    higher probability of the text being toxic.
  """

  response = openai.Moderation.create(input=text)
  toxicity_score = response["results"][0]["confidence"]
  return toxicity_score

# Test the function
check_toxicity("You are stupid.")</pre>
        </div></section>
        <section data-pdf-bookmark="Linking Your Filters to Your LLM" data-type="sect2"><div class="sect2" id="linking_your_filters_to_your_llm">
          <h2>Linking Your Filters to Your LLM</h2>
          <p><a contenteditable="false" data-primary="output handling" data-secondary="building" data-tertiary="filter linking" data-type="indexterm" id="id465"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="building your output filter" data-tertiary="filter linking" data-type="indexterm" id="id466"/>Let’s pull this together now into a simple workflow with an end-to-end example.</p>
          <div data-type="tip"><h6>Tip</h6>
            <p>Remember to log all interactions to and from your LLM! This will be important for debugging, security auditing, and regulatory compliance.</p>
          </div>
          <p>The following sample first checks the LLM output for toxicity using the OpenAI Moderation API. If the toxicity score exceeds 0.7 (you may choose your threshold), the code flags the output as unsafe and logs it to a file. The code also checks the output for PII using a regular expression. If PII is found, the code flags the output as unsafe and logs it to a file:</p>
            <pre data-type="programlisting">import openai
import json

# Initialize the OpenAI API client
openai.api_key = "your_openai_api_key_here"

def check_toxicity(text):

  response = openai.Moderation.create(input=text)
  toxicity_score = response["results"][0]["confidence"]
  return toxicity_score

def check_for_PII(text):

  ssn_pattern = r"\b\d{3}-\d{2}-\d{4}\b"
  return bool(re.search(ssn_pattern, text))

def get_LLM_response(prompt):

  model_engine = "text-davinci-002"  # You can use other engines
  response = openai.Completion.create(
      engine=model_engine,
      prompt=prompt,
      max_tokens=100  # Limiting to 100 tokens for this example
  )

  return response.choices[0].text.strip()

def log_results(prompt, llm_output, is_safe):

  with open("llm_safety_log.txt", "a") as log_file:
    log_file.write(f"Prompt: {prompt}\n")
    log_file.write(f"LLM Output: {llm_output}\n")
    log_file.write(f"Is Safe: {is_safe}\n")
    log_file.write("=" * 50 + "\n")

if __name__ == "__main__":
  prompt = "Tell me your thoughts on universal healthcare."
  llm_output = get_LLM_response(prompt)

  toxicity_level = check_toxicity(llm_output)
  contains_PII = check_for_PII(llm_output)

  is_safe = True

  if toxicity_level &gt; 0.7 or contains_PII:
    print("Warning: The output is not safe to return to the user.")
    is_safe = False
  else:
    print("The output is safe to return to the user.")

  log_results(prompt, llm_output, is_safe)</pre>
        </div></section>
        <section data-pdf-bookmark="Sanitize for Safety" data-type="sect2"><div class="sect2" id="sanitize_for_safety">
          <h2>Sanitize for Safety</h2>
          <p><a contenteditable="false" data-primary="output handling" data-secondary="building" data-tertiary="sanitizing for safety" data-type="indexterm" id="id467"/><a contenteditable="false" data-primary="sanitization" data-type="indexterm" id="id468"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="building your output filter" data-tertiary="sanitizing for safety" data-type="indexterm" id="id469"/>If you return your output to the user via a web interface, you’ll want to sanitize the string to avoid issues like XSS. Here’s the simplest possible version of this kind of function. You may add additional sanitization based on your needs:<a contenteditable="false" data-primary="" data-startref="ch07.html11" data-type="indexterm" id="id470"/><a contenteditable="false" data-primary="" data-startref="ch07.html10" data-type="indexterm" id="id471"/></p>
            <pre data-type="programlisting">import html
              
def sanitize_output(text):
    return html.escape(text)</pre>
          <p class="pagebreak-before">Let’s go ahead and add that sanitization step to our flow:</p>
            <pre data-type="programlisting">    if toxicity_level &gt; 0.7 or contains_PII:
        print("Warning: The output is not safe to return to the user.")
        is_safe = False
    else:
        print("The output is safe to return to the user.")
        llm_output = sanitize_output(llm_output)
    
    log_results(prompt, llm_output, is_safe)</pre>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="conclusion_4">
        <h1>Conclusion</h1>
        <p>Following the techniques in this chapter, you can plan where you should trust your LLM and where you shouldn’t; take sound, fact-based, risk-aware decisions; and balance your app’s needs to be fully functional against our outlined risks. </p>
        <p>Remember, Fox Mulder trusted no one at the start of the <em>X-Files</em> series. It was his fundamental mantra. However, he found people he could trust over time, like Agent Scully, Director Skinner, and the Lone Gunmen. However, he never lost his sense of paranoia, and the need to investigate and verify kept him alive through many perils. Remember, the truth is out there!</p>
        <p>In this chapter, we reviewed the tenets of a zero trust architecture and discussed how that might apply to your LLM application. The vulnerabilities we’ve looked at in the book, ranging from prompt injection to hallucination to sensitive information disclosure, imply that zero trust is one of the essential tools you must add to your mental model. It’s not just that you must worry about untrusted data coming <em>into</em> your LLM; you shouldn’t fully trust the data or instructions coming <em>out </em>of your LLM. Your LLM is an untrusted entity because it lacks common sense. LLMs are powerful, but you must provide an additional layer of supervision for your application to be safe and secure.<a contenteditable="false" data-primary="" data-startref="ch07.html0" data-type="indexterm" id="id472"/> </p>
      </div></section>
    </div></section></body></html>