["```py\ndef create_adjacency_list(data_dict, suffix=''):\n   list_of_nodes = []\n\n   for source_node in list(data_dict.keys()):  #1\n\n       if source_node not in list_of_nodes:\n           list_of_nodes.append(source_node)\n\n       for y in data_dict[source_node]:               #2\n           if y not in list_of_nodes:                 #2\n               list_of_nodes.append(y)                #2\n           if y not in data_dict.keys():              #2\n               data_dict[y]=[source_node]             #2\n           Else:                                      #2\n               if source_node not in data_dict[y]:    #2\n                   data_dict[y].append(source_node)   #2\n               else: continue                         #2\n\n   g= open(\"adjacency_list_{}.txt\".format(suffix),\"w+\")  #3\n   for source_node in list(data_dict.keys()):  #4\n       dt = ' '.join(data_dict[source_node])   #5\n       print(\"{} {}\".format(source_node, dt))  #6\n       g.write(\"{} {} \\n\".format(source_node, dt))   #7\n\n   g.close\n   return list_of_nodes\n```", "```py\ndef create_edge_list(data_dict, suffix=''):\n    edge_list_file = open(\"edge_list_{}.txt\".format(suffix),\"w+\")\n    edges = []    \n    nodes_all = []\n\n    for source in list(data_dict.keys()):\n        if source not in list_of_nodes_all:\n            nodes_all.append(source)\n        connections = data_dict[source]\n\n        for destination in connections:   #1\n            if destination not in nodes_all:\n                nodes_all.append(destination)\n\n           if {source, destination} not in edges:   #2\n               print(f\"{source} {destination}\")\n               out_string =  f\"{source} {destination}\\nâ€\n               edge_list_file.write(out_string)   #3\n               edges.append({source, destination })\n\n           else: continue\n\n       edge_list_file.close\n       return list_of_edges, list_of_nodes_all\n```", "```py\nsocial_graph = nx.read_adjlist('adjacency_list_candidates.txt')\nnx.set_node_attributes(social_graph, attribute_dict)\nprint(social_graph.number_of_nodes(), social_graph.number_of_edges())\n>> 1933 12239\n```", "```py\nlen(list((c for c in nx.connected_components(social_graph))))\n>>> 219\n```", "```py\nconnected_component = nx.connected_components(social_graph\nGcc = social_graph.subgraph(sorted(connected ), \n                            key=len, \n                            reverse=True)[0]\n                            )\n```", "```py\npos = nx.spring_layout(Gcc, seed=10396953)\n```", "```py\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected component of Social Graph\")\nax0.set_axis_off()\n```", "```py\ndegree_sequence = sorted([d for n, d in social_graph.degree()], reverse=True)\n\nax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")\nax1.set_title(\"Degree Rank Plot\")\nax1.set_ylabel(\"Degree\")\nax1.set_xlabel(\"Rank\")\n\nax2 = fig.add_subplot(axgrid[3:, 2:])\nax2.bar(*np.unique(degree_sequence, return_counts=True))\nax2.set_title(\"Degree histogram\")\nax2.set_xlabel(\"Degree\")\nax2.set_ylabel(\"# of Nodes\")\n```", "```py\nplt.imshow(nx.to_numpy_matrix(social_graph), aspect='equal',cmap='twilight')\n```", "```py\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.data import InMemoryDataset\nfrom torch_geometric import utils\n```", "```py\ndata = utils.from_networkx(social_graph)\n```", "```py\nsocial_graph = nx.read_edgelist('edge_list2.txt')   #1\n\nlist_of_nodes = list(set(list(social_graph)))   #2\nindices_of_nodes = [list_of_nodes.index(x)\\\n for x in list_of_nodes]    #3\n\nnode_to_index = dict(zip(list_of_nodes, indices_of_nodes))   #4\nindex_to_node = dict(zip(indices_of_nodes, list_of_nodes))\n```", "```py\nlist_edges = nx.convert.to_edgelist(social_graph)   #1\nlist_edges = list(list_edges)\nnamed_edge_list_0 = [x[0] for x in list_edges]   #2\nnamed_edge_list_1 = [x[1] for x in list_edges]\n\nindexed_edge_list_0 = [node_to_index[x]\\\n for x in named_edge_list_0]   #3\nindexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]\n\nx = torch.FloatTensor([[1] for x in\\ \nrange(len(list_of_nodes))])  #4\ny = torch.FloatTensor([1]*974 + [0]*973)   #5\ny = y.long()\n```", "```py\nedge_index = torch.tensor([indexed_edge_list_0,\\\n indexed_edge_list_1])    #1\n\ntrain_mask = torch.zeros(len(list_of_nodes),\\\n dtype=torch.uint8)   #2\ntrain_mask[:int(0.8 * len(list_of_nodes))] = 1 #train only on the 80% nodes\ntest_mask = torch.zeros(len(list_of_nodes),\\\n dtype=torch.uint8) #test on 20 % nodes \ntest_mask[- int(0.2 * len(list_of_nodes)):] = 1\ntrain_mask = train_mask.bool()\ntest_mask = test_mask.bool()\n\ndata = Data(x=x, y=y, edge_index=edge_index,\\\n train_mask=train_mask, test_mask=test_mask)    #3\n```", "```py\nclass MyOwnDataset(InMemoryDataset):\n    def __init__(self, root, \\\n    transform=None, pre_transform=None):\\    #1\n        super(MyOwnDataset, self).__init__(root,\n    @property transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    def raw_file_names(self):  #2\n        return []\n    @property\n    def processed_file_names(self):   #3\n        return ['../test.dataset']\n```", "```py\n    def download(self):   #1\n        # Download to `self.raw_dir`.\n        pass\n\n    def process(self):   #2\n        # Read data into `Data` list.\n        data_list = []\n\n        eg = nx.read_edgelist('edge_list2.txt') \n\n        list_of_nodes = list(set(list(eg)))\n        indices_of_nodes = [list_of_nodes.index(x) for x in list_of_nodes]\n\n        node_to_index = dict(zip(list_of_nodes, indices_of_nodes))\n        index_to_node = dict(zip(indices_of_nodes, list_of_nodes))\n\n        list_edges = nx.convert.to_edgelist(eg)\n        list_edges = list(list_edges)\n        named_edge_list_0 = [x[0] for x in list_edges]\n        named_edge_list_1 = [x[1] for x in list_edges]\n\n        indexed_edge_list_0 = [node_to_index[x] for x in named_edge_list_0]\n        indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]\n```", "```py\n        x = torch.FloatTensor([[1] for x in range(len(list_of_nodes))])#\n  [[] for x in xrange(n)]\n        y = torch.FloatTensor([1]*974 + [0]*973)\n        y = y.long()\n\n        edge_index = torch.tensor([indexed_edge_list_0, indexed_edge_list_1])\n\n        train_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8)\n        train_mask[:int(0.8 * len(list_of_nodes))]\\\n = 1 #train only on the 80% nodes\n        test_mask = torch.zeros(len(list_of_nodes), \\\ndtype=torch.uint8) #test on 20 % nodes \n        test_mask[- int(0.2 * len(list_of_nodes)):] = 1\n\n        train_mask = train_mask.bool()\n        test_mask = test_mask.bool()\n\n        data_example = Data(x=x, y=y, edge_index=edge_index, \\\ntrain_mask=train_mask, test_mask=test_mask)\n\n        data_list.append(data_example)           #1\n\n        data, slices = self.collate(data_list)  \n        torch.save((data, slices),\\\n self.processed_paths[0])    #2\n```", "```py\nfrom torch_geometric.data import Data, DataLoader\n\ndata_list = [Data(...), ..., Data(...)]\nloader = DataLoader(data_list, batch_size=32)\n```"]