["```py\nfrom networkx import is_d_separator      #1\nfrom pgmpy.base import DAG     #2\ndag = DAG([     #2\n    ('I', 'U'),     #2\n    ('I', 'M'),     #2\n    ('M', 'U'),  #2\n    ('J', 'V'),    #2\n    ('J', 'M'),     #2\n    ('M', 'V')     #2\n])     #2\nprint(is_d_separator(dag, {\"U\"}, {\"V\"}, {\"M\"}))     #3\nprint(is_d_separator(dag, {\"U\"}, {\"V\"}, {\"M\", \"I\", \"J\"}))    #4\nprint(is_d_separator(dag, {\"U\"}, {\"V\"}, {\"M\", \"I\"}))     #5\nprint(is_d_separator(dag, {\"U\"}, {\"V\"}, {\"M\", \"J\"}))    #5\n```", "```py\nfrom pgmpy.base import DAG\ndag = DAG([\n    ('I', 'U'),\n    ('I', 'M'),\n    ('M', 'U'),\n    ('J', 'V'),\n    ('J', 'M'),\n    ('M', 'V')\n])\ndag.get_independencies()    #1\n```", "```py\n(I ⊥ J)\n(I ⊥ V | J, M)\n(I ⊥ V | J, U, M)\n(V ⊥ I, U | J, M)\n(V ⊥ U | I, M)\n(V ⊥ I | J, U, M)\n(V ⊥ U | J, M, I)\n(J ⊥ I)\n(J ⊥ U | I, M)\n(J ⊥ U | I, M, V)\n(U ⊥ V | J, M)\n(U ⊥ J, V | I, M)\n(U ⊥ V | J, M, I)\n(U ⊥ J | I, M, V)\n```", "```py\nimport pandas as pd\nsurvey_url = \"https://raw.githubusercontent.com/altdeep/causalML/master\n[CA] /datasets/transportation_survey.csv\"\nfulldata = pd.read_csv(survey_url)\n\ndata = fulldata[0:30]     #1\nprint(data[0:5])\n```", "```py\n       A  S     E    O      R      T\n0  adult  F  high  emp  small  train\n1  young  M  high  emp    big    car\n2  adult  M   uni  emp    big  other\n3    old  F   uni  emp    big    car\n4  young  F   uni  emp    big    car\n```", "```py\nfrom pgmpy.estimators.CITests import chi_square     #1\nsignificance = .05     #2\n\nresult = chi_square(    #3\n    X=\"E\", Y=\"T\", Z=[\"O\", \"R\"],     #3\n    data=data,     #3\n    boolean=False,   #3\n    significance_level=significance     #3\n)     #3\nprint(result)\n```", "```py\nfrom pgmpy.estimators.CITests import chi_square     #1\nsignificance = .05     #2\nresult = chi_square(     #3\n    X=\"E\", Y=\"T\", Z=[\"O\", \"R\"],   #3\n    data=data,    #3\n    boolean=True,    #3\n    significance_level=significance  #3   \n)    #3\nprint(result)\n```", "```py\nfrom pprint import pprint\nfrom pgmpy.base import DAG\nfrom pgmpy.independencies import IndependenceAssertion\n\ndag = DAG([\n    ('A', 'E'),\n    ('S', 'E'),\n    ('E', 'O'),\n    ('E', 'R'),\n    ('O', 'T'),\n    ('R', 'T')\n])\ndseps = dag.get_independencies()    \n\ndef test_dsep(dsep):\n    test_outputs = []\n    for X in list(dsep.get_assertion()[0]):\n        for Y in list(dsep.get_assertion()[1]):\n            Z = list(dsep.get_assertion()[2])\n            test_result = chi_square(\n                X=X, Y=Y, Z=Z,\n                data=data,\n                boolean=True,\n                significance_level=significance\n            )\n            assertion = IndependenceAssertion(X, Y, Z)\n            test_outputs.append((assertion, test_result))\n    return test_outputs\n\nresults = [test_dsep(dsep) for dsep in dseps.get_assertions()]\nresults = dict([item for sublist in results for item in sublist])\npprint(results)\n```", "```py\n{(O ⊥ A | R, E, T, S): True,\n (S ⊥ R | E, T, A): True,\n (S ⊥ O | E, T, A): True,\n (T ⊥ S | R, O, A): True,\n (S ⊥ O | R, E): True,\n (R ⊥ O | E): False,\n (S ⊥ O | E, A): True,\n (S ⊥ R | E, A): True,\n (S ⊥ R | E, T, O, A): True,\n (S ⊥ R | E, O, A): True,\n (O ⊥ A | E, T): True,\n (S ⊥ O | R, E, T): True,\n (R ⊥ O | E, S): False, \n …\n (T ⊥ A | E, S): True}\n```", "```py\nnum_pass = sum(results.values())\nnum_dseps = len(dseps.independencies)\nnum_fail = num_dseps - num_pass\nprint(num_fail / num_dseps)\n```", "```py\nfrom numpy import mean, quantile\n\ndef sample_p_val(data_size, data, alpha):     #1\n    bootstrap_data = data.sample(n=data_size, replace=True)    #1\n    result = chi_square(    #1\n        X=\"E\", Y=\"T\", Z=[\"O\", \"R\"],    #1\n        data=bootstrap_data,     #1\n        boolean=False,     #1\n        significance_level = alpha     #1\n    )    #1\n    p_val = result[1]    #1\n    return p_val     #1\n\ndef estimate_p_val(data_size, data=fulldata, boot_size=1000, α=.05):    #2\n    samples = [  #2\n        sample_p_val(data_size, data=fulldata, alpha=α)  #2\n        for _ in range(boot_size)     #2\n    ]    #2\n    positive_tests = [p_val > significance for p_val in samples]    #3\n    prob_conclude = mean(positive_tests)    #4\n    p_estimate = mean(samples)    #4\n    quantile_05, quantile_95 = quantile(samples, [.05, .95])    #5\n    lower_error = p_estimate - quantile_05     #5\n    higher_error = quantile_95 - p_estimate    #5\n    return p_estimate, lower_error, higher_error, prob_conclude\n\ndata_size = range(30, 1000, 20)     #6\nresult = list(zip(*[estimate_p_val(size) for size in data_size]))  #6\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np_vals, lower_bars, higher_bars, probs_conclude_indep = result    #1\nplt.title('Data size vs. p-value (Ind. of E & T | O & R)')     #2\nplt.xlabel(\"Number of examples in data\")     #2\nplt.ylabel(\"Expected p-value\")   #2\nerror_bars = np.array([lower_bars, higher_bars])    #2\nplt.errorbar(   #2\n    data_size,    #2\n    p_vals,     #2\n    yerr=error_bars,    #2\n    ecolor=\"grey\",    #2\n    elinewidth=.5   #2\n)     #2\nplt.hlines(significance, 0, 1000, linestyles=\"dashed\")     #2\nplt.show()\nplt.title('Probability of favoring independence given data size')    #3\nplt.xlabel(\"Number of examples in data\")     #3\nplt.ylabel(\"Probability of test favoring conditional independence\")    #3\nplt.plot(data_size, probs_conclude_indep)  #3\n```", "```py\nfrom functools import partial\nimport numpy as np\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/altdeep/causalML/master\n[CA] /datasets/cigs_and_cancer.csv\"\ndata = pd.read_csv(data_url)     #1\ncost_lower = np.quantile(data[\"C\"], 1/3)    #2\ncost_upper = np.quantile(data[\"C\"], 2/3)  #2\ndef discretize_three(val, lower, upper):  #2\n    if val < lower:   #2\n        return \"Low\"    #2\n    if val < upper:    #2\n        return \"Med\"   #2\n    return \"High\"    #2\n    #2\ndata_disc = data.assign(    #2\n    C = lambda df: df['C'].map(    #2\n            partial(   #2\n                discretize_three,   #2\n                lower=cost_lower,   #2\n                upper=cost_upper    #2\n            )   #2\n        )    #2\n)    #2\ndata_disc = data_disc.assign(     #3\n    L = lambda df: df['L'].map(str),    #3\n)    #3\nprint(data_disc)\n```", "```py\n       C     S     T      L\n0   High   Med   Low   True\n1    Med  High  High  False\n2    Med  High  High   True\n3    Med  High  High   True\n4    Med  High  High   True\n.. ... ... ...  ...\n95   Low  High  High   True\n96  High  High  High  False\n97   Low   Low   Low  False\n98  High   Low   Low  False\n99   Low  High  High   True\n\n[100 rows x 4 columns]\n```", "```py\nfrom pgmpy.inference import VariableElimination\nfrom pgmpy.models import NaiveBayes\n\nmodel_L_given_CST = NaiveBayes()    #1\nmodel_L_given_CST.fit(data_disc, 'L')   #1\ninfer_L_given_CST = VariableElimination(model_L_given_CST)   #1\n#1\ndef p_L_given_CST(L_val, C_val, S_val, T_val):#1\n    result_out = infer_L_given_CST.query(   #1\n        variables=[\"L\"],    #1\n        evidence={'C': C_val, 'S': S_val, 'T': T_val},    #1\n        show_progress=False    #1\n    )    #1\n    var_outcomes = result_out.state_names[\"L\"]    #1\n    var_values = result_out.values    #1\n    prob = dict(zip(var_outcomes, var_values))    #1\n    return prob[L_val]  #1\n```", "```py\nmodel_S_given_C = NaiveBayes()    \nmodel_S_given_C.fit(data_disc, 'S')    \ninfer_S_given_C = VariableElimination(model_S_given_C)    \ndef p_S_given_C(S_val, C_val):    \n    result_out = infer_S_given_C.query(    \n        variables=['S'],    \n        evidence={'C': C_val},    \n        show_progress=False    \n    )    \n    var_names = result_out.state_names[\"S\"]    \n    var_values = result_out.values    \n    prob = dict(zip(var_names, var_values))    \n    return prob[S_val]\n```", "```py\ndef h_function(L, C, T):    #1\n    summ = 0     #2\n    for s in [\"Low\", \"Med\", \"High\"]:   #2\n        summ += p_L_given_CST(L, C, s, T) * p_S_given_C(s, C)    #2\n    return summ\n```", "```py\nctl_outcomes = pd.DataFrame(\n    [     #1\n        (C, T, L)     #1\n        for C in [\"Low\", \"Med\", \"High\"]    #1\n        for T in [\"Low\", \"High\"]    #1\n        for L in [\"False\", \"True\"]     #1\n    ],    #1\n    columns = ['C', 'T', 'L']     #1\n)\n```", "```py\n       C     T      L\n0    Low   Low  False\n1    Low   Low   True\n2    Low  High  False\n3    Low  High   True\n4    Med   Low  False\n5    Med   Low   True\n6    Med  High  False\n7    Med  High   True\n8   High   Low  False\n9   High   Low   True\n10  High  High  False\n11  High  High   True\n```", "```py\nh_dist = ctl_outcomes.assign(    \n    h_func = ctl_outcomes.apply(    \n        lambda row: h_function(    \n            row['L'], row['C'], row['T']), axis = 1    \n    )    \n)    \nprint(h_dist)\n```", "```py\n       C     T      L    h_func\n0    Low   Low  False  0.392395\n1    Low   Low   True  0.607605\n2    Low  High  False  0.255435\n3    Low  High   True  0.744565\n4    Med   Low  False  0.522868\n5    Med   Low   True  0.477132\n6    Med  High  False  0.369767\n7    Med  High   True  0.630233\n8   High   Low  False  0.495525\n9   High   Low   True  0.504475\n10  High  High  False  0.344616\n11  High  High   True  0.655384\n```", "```py\ndf_mod = data_disc.merge(h_dist, on=['C', 'T', 'L'], how='left')    #1\nprint(df_mod)\n```", "```py\n       C     S     T      L    h_func\n0   High   Med   Low   True  0.504475\n1    Med  High  High  False  0.369767\n2    Med  High  High   True  0.630233\n3    Med  High  High   True  0.630233\n4    Med  High  High   True  0.630233\n..   ...   ...   ...    ...       ...\n95   Low  High  High   True  0.744565\n96  High  High  High  False  0.344616\n97   Low   Low   Low  False  0.392395\n98  High   Low   Low  False  0.495525\n99   Low  High  High   True  0.744565\n\n[100 rows x 5 columns]\n```", "```py\ndf_mod.boxplot(\"h_func\", \"C\")\n```", "```py\nfrom statsmodels.formula.api import ols\nimport statsmodels.api as sm\n\nmodel = ols('h_func ~ C', data=df_mod).fit()    #1\naov_table = sm.stats.anova_lm(model, typ=2)   #1\nprint(aov_table[\"PR(>F)\"][\"C\"])   #1\n\nmodel = ols('h_func ~ T', data=df_mod).fit()    #2\naov_table = sm.stats.anova_lm(model, typ=2)    #2\nprint(aov_table[\"PR(>F)\"][\"T\"])    #2\n\nmodel = ols('h_func ~ L', data=df_mod).fit()     #3\naov_table = sm.stats.anova_lm(model, typ=2)  #3\nprint(aov_table[\"PR(>F)\"][\"L\"])   #3\n```"]