<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Unsupervised Learning Techniques"><div class="chapter" id="unsupervised_learning_chapter">
<h1><span class="label">Chapter 8. </span>Unsupervised Learning Techniques</h1>


<p>Yann<a data-type="indexterm" data-primary="unsupervised learning" id="xi_unsupervisedlearning835_1"/> LeCun, Turing Award winner and Meta‚Äôs Chief AI Scientist, famously said that ‚Äúif intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake‚Äù (NeurIPS 2016). In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth into. Indeed, the vast majority of the available data is unlabeled: we have the input features <strong>X</strong>, but we do not have the labels <strong>y</strong>.</p>

<p>Say you want to create a system that will take a few pictures of each item on a manufacturing production line and detect which items are defective. You can fairly easily create a system that will take pictures automatically, and this might give you thousands of pictures every day. You can then build a reasonably large dataset in just a few weeks. But wait, there are no labels! If you want to train a regular binary classifier that will predict whether an item is defective or not, you will need to label every single picture as ‚Äúdefective‚Äù or ‚Äúnormal‚Äù. This will generally require human experts to sit down and manually go through all the pictures. This is a long, costly, and tedious task, so it will usually only be done on a small subset of the available pictures. As a result, the labeled dataset will be quite small, and the classifier‚Äôs performance will be disappointing. Moreover, every time the company makes any change to its products, the whole process will need to be started over from scratch. Wouldn‚Äôt it be great if the algorithm could just exploit the unlabeled data without needing humans to label every picture? Enter unsupervised learning.</p>

<p>In <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter¬†7</a> we looked at the most common unsupervised learning task: dimensionality reduction. In this chapter we will look at a few more unsupervised tasks:</p>
<dl>
<dt>Clustering</dt>
<dd>
<p>The goal is to group similar instances together into <em>clusters</em>. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.</p>
</dd>
<dt>Anomaly detection (also called <em>outlier detection</em>)</dt>
<dd>
<p>The objective is to learn what ‚Äúnormal‚Äù data looks like, and then use that to detect abnormal instances. These instances are called <em>anomalies</em>, or <em>outliers</em>,<a data-type="indexterm" data-primary="outlier detection" data-seealso="anomaly detection" id="id1953"/> while the normal instances are called <em>inliers</em>.<a data-type="indexterm" data-primary="inliers" id="id1954"/> Anomaly detection is useful in a wide variety of applications, such as fraud detection, detecting defective products in manufacturing, identifying new trends in time series, or removing outliers from a dataset before training another model, which can significantly improve the performance of the resulting model.</p>
</dd>
<dt>Density estimation</dt>
<dd>
<p>This<a data-type="indexterm" data-primary="density estimation" id="id1955"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="density estimation" id="id1956"/> is the task of estimating the <em>probability density function</em> (PDF)<a data-type="indexterm" data-primary="PDF (probability density function)" id="id1957"/><a data-type="indexterm" data-primary="probability density function (PDF)" id="id1958"/> of the random process that generated the dataset.‚Å†<sup><a data-type="noteref" id="id1959-marker" href="ch08.html#id1959">1</a></sup> Density estimation is commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis and visualization.</p>
</dd>
</dl>

<p>Ready for some cake? We will start with two clustering algorithms, <em>k</em>-means and DBSCAN, then we‚Äôll discuss Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly detection.</p>






<section data-type="sect1" data-pdf-bookmark="Clustering Algorithms: k-means and DBSCAN"><div class="sect1" id="id130">
<h1>Clustering Algorithms: k-means and DBSCAN</h1>

<p>As<a data-type="indexterm" data-primary="clustering algorithms" id="xi_clusteringalgorithms8203_1"/> you enjoy a hike in the mountains, you stumble upon a plant you have never seen before. You look around and you notice a few more. They are not identical, yet they are sufficiently similar for you to know that they most likely belong to the same species (or at least the same genus). You may need a botanist to tell you what species that is, but you certainly don‚Äôt need an expert to identify groups of similar-looking objects. This is called <em>clustering</em>:<a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="clustering" id="id1960"/> it is the task of identifying similar instances and assigning them to <em>clusters</em>, or groups of similar instances.</p>

<p>Just like in classification<a data-type="indexterm" data-primary="classification" data-secondary="clustering compared to" id="id1961"/>, each instance gets assigned to a group. However, unlike classification, clustering is an unsupervised task, there are no labels, so the algorithm needs to figure out on its own how to group instances. Consider <a data-type="xref" href="#classification_vs_clustering_plot">Figure¬†8-1</a>: on the left is the iris dataset (introduced in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>), where each instance‚Äôs species (i.e., its class) is represented with a different marker. It is a labeled dataset, for which classification algorithms such as logistic regression, SVMs, or random forest classifiers are well suited. On the right is the same dataset, but without the labels, so you cannot use a classification algorithm anymore. This is where clustering algorithms step in: many of them can easily detect the lower-left cluster. It is also quite easy to see with our own eyes, but it is not so obvious that the upper-right cluster is composed of two distinct subclusters. That said, the dataset has two additional features (sepal length and width) that are not represented here, and clustering algorithms can make good use of all features, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model, only 5 instances out of 150 are assigned to the wrong cluster).</p>

<figure><div id="classification_vs_clustering_plot" class="figure">
<img src="assets/hmls_0801.png" alt="Diagram comparing classification (left) with labeled data and clustering (right) with unlabeled data on the iris dataset, highlighting how clustering identifies groups without prior labels." width="2546" height="924"/>
<h6><span class="label">Figure 8-1. </span>Classification (left) versus clustering (right): in clustering, the dataset is unlabeled so the algorithm must identify the clusters without guidance</h6>
</div></figure>

<p>Clustering<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="applications for" id="xi_clusteringalgorithmsapplicationsfor82811_1"/> is used in a wide variety of applications, including:</p>
<dl>
<dt>Customer segmentation</dt>
<dd>
<p>You<a data-type="indexterm" data-primary="customer segmentation" id="id1962"/> can cluster your customers based on their purchases and their activity on your website. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, customer segmentation can be useful in <em>recommender systems</em> to suggest content that other users in the same cluster enjoyed.</p>
</dd>
<dt>Data analysis</dt>
<dd>
<p>When<a data-type="indexterm" data-primary="data analysis, clustering for" id="id1963"/> you analyze a new dataset, it can be helpful to run a clustering algorithm, and then analyze each cluster separately.</p>
</dd>
<dt>Dimensionality reduction</dt>
<dd>
<p>Once a dataset has been clustered, it is usually possible to measure each instance‚Äôs <em>affinity</em><a data-type="indexterm" data-primary="affinity function" id="id1964"/> with each cluster; affinity is any measure of how well an instance fits into a cluster. Each instance‚Äôs feature vector <strong>x</strong> can then be replaced with the vector of its cluster affinities. If there are <em>k</em> clusters, then this vector is <em>k</em>-dimensional. The new vector is typically much lower-dimensional than the original feature vector, but it can preserve enough information for further processing.</p>
</dd>
<dt>Feature engineering</dt>
<dd>
<p>The<a data-type="indexterm" data-primary="feature engineering" id="id1965"/> cluster affinities can often be useful as extra features. For example, we used <em>k</em>-means in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a> to add geographic cluster affinity features to the California housing dataset, and they helped us get better performance.</p>
</dd>
<dt><em>Anomaly detection</em> (also called <em>outlier detection</em>)</dt>
<dd>
<p>Any<a data-type="indexterm" data-primary="anomaly detection" data-secondary="clustering for" id="id1966"/> instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second.</p>
</dd>
<dt>Semi-supervised learning</dt>
<dd>
<p>If<a data-type="indexterm" data-primary="semi-supervised learning" id="id1967"/> you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance.</p>
</dd>
<dt>Search engines</dt>
<dd>
<p>Some search engines<a data-type="indexterm" data-primary="search engines, clustering for" id="id1968"/> let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database; similar images would end up in the same cluster. Then when a user provides a reference image, all you‚Äôd need to do is use the trained clustering model to find this image‚Äôs cluster, and you could then simply return all the images from this cluster.</p>
</dd>
<dt>Image segmentation</dt>
<dd>
<p>By<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="image segmentation" id="id1969"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="segmentation" id="id1970"/> clustering pixels according to their color, then replacing each pixel‚Äôs color with the mean color of its cluster, it is possible to considerably reduce the number of different colors in an image. Image segmentation is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object.</p>
</dd>
</dl>

<p>There<a data-type="indexterm" data-startref="xi_clusteringalgorithmsapplicationsfor82811_1" id="id1971"/> is no universal definition of what a cluster is: it really depends on the context, and different algorithms will capture different kinds of clusters. Some algorithms look for instances centered around a particular point, called a <em>centroid</em>.<a data-type="indexterm" data-primary="centroid, cluster" id="id1972"/> Others look for continuous regions of densely packed instances: these clusters can take on any shape. Some algorithms are hierarchical, looking for clusters of clusters. And the list goes on.</p>

<p>In this section, we will look at two popular clustering algorithms, <em>k</em>-means and DBSCAN, and explore some of their applications, such as nonlinear dimensionality reduction, semi-supervised learning, and anomaly detection.</p>








<section data-type="sect2" data-pdf-bookmark="k-Means Clustering"><div class="sect2" id="id131">
<h2>k-Means Clustering</h2>

<p>Consider<a data-type="indexterm" data-primary="k-means algorithm" id="xi_kmeansalgorithm8519_1"/> the unlabeled dataset represented in <a data-type="xref" href="#blobs_plot">Figure¬†8-2</a>: you can clearly see five blobs of instances. The <em>k</em>-means algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957 as a technique for pulse-code 
<span class="keep-together">modulation,</span> but it was only <a href="https://homl.info/36">published</a> outside of the company in 1982.‚Å†<sup><a data-type="noteref" id="id1973-marker" href="ch08.html#id1973">2</a></sup> In 1965, Edward W. Forgy had published virtually the same algorithm, so <em>k</em>-means is sometimes referred to as the Lloyd‚ÄìForgy algorithm.</p>

<figure class="smallerseventy"><div id="blobs_plot" class="figure">
<img src="assets/hmls_0802.png" alt="Scatterplot displaying five distinct clusters of data points, illustrating an unlabeled dataset suitable for k-means clustering." width="2275" height="1071"/>
<h6><span class="label">Figure 8-2. </span>An unlabeled dataset composed of five blobs of instances</h6>
</div></figure>

<p>Let‚Äôs train a <em>k</em>-means clusterer on this dataset. It will try to find each blob‚Äôs center and assign each instance to the closest blob:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>  <code class="c1"># make the blobs: y contains the cluster IDs, but we</code>
                          <code class="c1"># will not use them; that's what we want to predict</code>
<code class="n">k</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>Note that you have to specify the number of clusters <em>k</em> that the algorithm must find. In this example, it is pretty obvious from looking at the data that <em>k</em> should be set to 5, but in general it is not that easy. We will discuss this shortly.</p>

<p>Each instance will be assigned to one of the five clusters. In the context of clustering, an instance‚Äôs <em>label</em><a data-type="indexterm" data-primary="labels" data-secondary="in clustering" id="id1974"/> is the index of the cluster to which the algorithm assigns this instance; this is not to be confused with the class labels in classification, which are used as targets (remember that clustering is an unsupervised learning task). The <code translate="no">KMeans</code> instance<a data-type="indexterm" data-primary="sklearn" data-secondary="cluster.KMeans" id="id1975"/> preserves the predicted labels of the instances it was trained on, available via the <code translate="no">labels_</code> instance variable:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="w"/>
<code class="go">array([4, 0, 1, ..., 2, 1, 0], dtype=int32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="ow">is</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="w"/>
<code class="go">True</code></pre>

<p>We can also take a look at the five centroids that the algorithm found:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="w"/>
<code class="go">array([[-2.80389616,  1.80117999],</code>
<code class="go">       [ 0.20876306,  2.25551336],</code>
<code class="go">       [-2.79290307,  2.79641063],</code>
<code class="go">       [-1.46679593,  2.28585348],</code>
<code class="go">       [-2.80037642,  1.30082566]])</code></pre>

<p>You can easily assign new instances to the cluster whose centroid<a data-type="indexterm" data-primary="centroid, cluster" id="id1976"/> is closest:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code><code class="w"> </code><code class="nn">numpy</code><code class="w"> </code><code class="k">as</code><code class="w"> </code><code class="nn">np</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mf">2.5</code><code class="p">]])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code><code class="w"/>
<code class="go">array([1, 1, 2, 2], dtype=int32)</code></pre>

<p>If you plot the cluster‚Äôs decision boundaries<a data-type="indexterm" data-primary="decision boundaries" id="id1977"/>, you get a Voronoi tessellation: see <a data-type="xref" href="#voronoi_plot">Figure¬†8-3</a>, where each centroid is represented with an ‚ìß.</p>

<figure class="smallerseventy"><div id="voronoi_plot" class="figure">
<img src="assets/hmls_0803.png" alt="Diagram showing Voronoi tessellation with k-means centroids marked, highlighting decision boundaries between clusters." width="2275" height="1071"/>
<h6><span class="label">Figure 8-3. </span>k-means decision boundaries (Voronoi tessellation)</h6>
</div></figure>

<p>The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled, especially near the boundary between the top-left cluster and the central cluster. Indeed, the <em>k</em>-means algorithm does not behave very well when the blobs have very different diameters because all it cares about when assigning an instance to a cluster is the distance to the centroid.</p>

<p>Instead of assigning each instance to a single cluster, which is called <em>hard clustering</em>,<a data-type="indexterm" data-primary="hard clustering" id="id1978"/> it can be useful to give each instance a score per cluster, which is called <em>soft clustering</em>.<a data-type="indexterm" data-primary="soft clustering" id="id1979"/> The score can be the distance between the instance and the centroid or a similarity score (or affinity), such as the Gaussian radial basis function we used in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>. In the <code translate="no">KMeans</code> class, the <code translate="no">transform()</code> method measures the distance from each instance to every centroid:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([[2.81, 0.33, 2.9 , 1.49, 2.89],</code>
<code class="go">       [5.81, 2.8 , 5.85, 4.48, 5.84],</code>
<code class="go">       [1.21, 3.29, 0.29, 1.69, 1.71],</code>
<code class="go">       [0.73, 3.22, 0.36, 1.55, 1.22]])</code></pre>

<p>In this example, the first instance in <code translate="no">X_new</code> is located at a distance of about 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid. If you have a high-dimensional dataset and you transform it this way, you end up with a <em>k</em>-dimensional dataset: this transformation can be a very efficient nonlinear dimensionality reduction technique. Alternatively, you can use these distances as extra features to train another model, as in <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>.</p>










<section data-type="sect3" data-pdf-bookmark="The k-means algorithm"><div class="sect3" id="id132">
<h3>The k-means algorithm</h3>

<p>So,<a data-type="indexterm" data-primary="k-means algorithm" data-secondary="workings of" id="xi_kmeansalgorithmworkingsof81294_1"/> how does the algorithm work? Well, suppose you were given the centroids. You could easily label all the instances in the dataset by assigning each of them to the cluster whose centroid is closest. Conversely, if you were given all the instance labels, you could easily locate each cluster‚Äôs centroid<a data-type="indexterm" data-primary="centroid, cluster" id="xi_centroidcluster8129304_1"/> by computing the mean of the instances in that cluster. But you are given neither the labels nor the centroids, so how can you proceed? Start by placing the centroids randomly (e.g., by picking <em>k</em> instances at random from the dataset and using their locations as centroids). Then label the instances, update the centroids, label the instances, update the centroids, and so on until the centroids stop moving. The algorithm is guaranteed to converge in a finite number of steps (usually quite small). That‚Äôs because the mean squared distance between the instances and their closest centroids can only go down at each step, and since it cannot be negative, it‚Äôs guaranteed to converge.</p>

<p>You can see the algorithm in action in <a data-type="xref" href="#kmeans_algorithm_plot">Figure¬†8-4</a>: the centroids are initialized randomly (top left), then the instances are labeled (top right), then the centroids are updated (center left), the instances are relabeled (center right), and so on. As you can see, in just three iterations the algorithm has reached a clustering that seems close to optimal.<a data-type="indexterm" data-primary="computational complexity" data-secondary="k-means algorithm" id="id1980"/></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The computational complexity of the algorithm is generally linear with regard to the number of instances <em>m</em>, the number of clusters <em>k</em>, and the number of dimensions <em>n</em>. However, this is only true when the data has a clustering structure. If it does not, then in the worst-case scenario the complexity can increase exponentially with the number of instances. In practice, this rarely happens, and <em>k</em>-means is generally one of the fastest clustering algorithms.</p>
</div>

<figure><div id="kmeans_algorithm_plot" class="figure">
<img src="assets/hmls_0804.png" alt="Diagram illustrating the k-means algorithm's process with random centroid initialization and subsequent instance labeling, showing potential clustering outcomes over iterations." width="2875" height="2269"/>
<h6><span class="label">Figure 8-4. </span>The <em>k</em>-means algorithm</h6>
</div></figure>

<p>Although the algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum): whether it does or not depends on the centroid initialization. <a data-type="xref" href="#kmeans_variability_plot">Figure¬†8-5</a> shows two suboptimal solutions that the algorithm can converge to if you are not lucky with the random initialization step.<a data-type="indexterm" data-startref="xi_kmeansalgorithmworkingsof81294_1" id="id1981"/></p>

<figure><div id="kmeans_variability_plot" class="figure">
<img src="assets/hmls_0805.png" alt="Diagram showing two suboptimal k-means clustering solutions caused by different random centroid initializations." width="2875" height="829"/>
<h6><span class="label">Figure 8-5. </span>Suboptimal solutions due to unlucky centroid initializations</h6>
</div></figure>

<p>Let‚Äôs take a look at a few ways you can mitigate this risk by improving the centroid <span class="keep-together">initialization</span>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Centroid initialization methods"><div class="sect3" id="id133">
<h3>Centroid initialization methods</h3>

<p>If<a data-type="indexterm" data-primary="k-means algorithm" data-secondary="centroid initialization methods" id="xi_kmeansalgorithmcentroidinitializationmethods81483_1"/> you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the <code translate="no">init</code> hyperparameter to a NumPy array containing the list of centroids:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">good_init</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">]])</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="n">good_init</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>Another solution is to run the algorithm multiple times with different random initializations and keep the best solution. The number of random initializations is controlled by the <code translate="no">n_init</code> hyperparameter: by default it is equal to <code translate="no">10</code> when using <code translate="no">init="random"</code>, which means that the whole algorithm described earlier runs 10 times when you call <code translate="no">fit()</code>, and Scikit-Learn keeps the best solution. But how exactly does it know which solution is the best? It uses a performance metric! That metric is called the model‚Äôs <em>inertia</em><a data-type="indexterm" data-primary="inertia, model" id="id1982"/>, which is defined in <a data-type="xref" href="#inertia_equation">Equation 8-1</a>.</p>
<div id="inertia_equation" data-type="equation">
<h5><span class="label">Equation 8-1. </span>A model‚Äôs inertia is the sum of all squared distances between each instance <strong>x</strong><sup>(<em>i</em>)</sup> and the closest centroid <strong>c</strong><sup>(<em>i</em>)</sup> predicted by the model</h5>
<math display="block">
  <mrow>
    <mtext>inertia</mtext>
    <mo>=</mo>
    <munder><mo>‚àë</mo> <mi>i</mi> </munder>
    <msup>
      <mrow><mo>‚à•</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msup><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo rspace="0.1em">‚à•</mo></mrow>
      <mn>2</mn>
    </msup>
  </mrow>
</math>
</div>

<p>The inertia is roughly equal to 219.6 for the model on the left in <a data-type="xref" href="#kmeans_variability_plot">Figure¬†8-5</a>, 600.4 for the model on the right in <a data-type="xref" href="#kmeans_variability_plot">Figure¬†8-5</a>, and only 211.6 for the model in <a data-type="xref" href="#voronoi_plot">Figure¬†8-3</a>. The <code translate="no">KMeans</code> class runs the initialization algorithm <code translate="no">n_init</code> times and keeps the model with the lowest inertia. In this example, the model in <a data-type="xref" href="#voronoi_plot">Figure¬†8-3</a> will be selected (unless we are very unlucky with <code translate="no">n_init</code> consecutive random initializations). If you are curious, a model‚Äôs inertia is accessible via the <code translate="no">inertia_</code> instance variable:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code><code class="w"/>
<code class="go">211.59853725816828</code></pre>

<p>The <code translate="no">score()</code> method returns the negative inertia (it‚Äôs negative because a predictor‚Äôs <code translate="no">score()</code> method must always respect Scikit-Learn‚Äôs ‚Äúgreater is better‚Äù rule‚Äîif a predictor is better than another, its <code translate="no">score()</code> method should return a greater score):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="w"/>
<code class="go">-211.59853725816828</code></pre>

<p>An important improvement to the <em>k</em>-means algorithm, <em>k-means++</em>, was proposed in a <a href="https://homl.info/37">2006 paper</a> by David Arthur and Sergei Vassilvitskii.‚Å†<sup><a data-type="noteref" id="id1983-marker" href="ch08.html#id1983">3</a></sup> They introduced a smarter initialization step that tends to select centroids that are distant from one another. This change makes the <em>k</em>-means algorithm much more likely to locate all important clusters, and less likely to converge to a suboptimal solution (just like spreading out fishing boats increases the chance of locating more schools of fish). The paper showed that the additional computation required for the smarter initialization step is well worth it because it makes it possible to drastically reduce the number of times the algorithm needs to be run to find the optimal solution. The <em>k</em>-means++ initialization algorithm<a data-type="indexterm" data-primary="k-means++" id="id1984"/> works like this:</p>
<ol>
<li>
<p>Take one centroid <strong>c</strong><sup>(1)</sup>, chosen uniformly at random from the dataset.</p>
</li>
<li>
<p>Take a new centroid <strong>c</strong><sup>(<em>i</em>)</sup>, choosing an instance <strong>x</strong><sup>(<em>i</em>)</sup> with probability <math alttext="upper D left-parenthesis bold x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis squared">
  <mrow>
    <mi>D</mi>
    <msup><mrow><mfenced separators="" open="(" close=")"><msup><mrow><mi>ùê±</mi></mrow> <mfenced open="(" close=")"><mi>i</mi></mfenced> </msup></mfenced></mrow> <mn>2</mn> </msup>
  </mrow>
</math>  / <math alttext="sigma-summation Underscript j equals 1 Overscript m Endscripts upper D left-parenthesis bold x Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis squared">
  <mrow>
    <msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
    <mi>D</mi>
    <msup><mrow><mfenced separators="" open="(" close=")"><msup><mrow><mi>ùê±</mi></mrow> <mfenced open="(" close=")"><mi>j</mi></mfenced> </msup></mfenced></mrow> <mn>2</mn> </msup>
  </mrow>
</math>, where D(<strong>x</strong><sup>(<em>i</em>)</sup>) is the distance between the instance <strong>x</strong><sup>(<em>i</em>)</sup> and the closest centroid that was already chosen. This probability distribution ensures that instances farther away from already chosen centroids are much more likely to be selected as centroids.</p>
</li>
<li>
<p>Repeat the previous step until all <em>k</em> centroids have been chosen.</p>
</li>

</ol>

<p>When you set <code translate="no">init="k-means++"</code> (which is the default), the <code translate="no">KMeans</code> class actually uses a variant of <em>k</em>-means++ called <em>greedy k-means++</em>: instead of sampling a single centroid at each iteration, it samples multiple and picks the best one. When using this algorithm, <code translate="no">n_init</code> defaults to 1.<a data-type="indexterm" data-startref="xi_centroidcluster8129304_1" id="id1985"/><a data-type="indexterm" data-startref="xi_kmeansalgorithmcentroidinitializationmethods81483_1" id="id1986"/></p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Accelerated k-means and mini-batch k-means"><div class="sect3" id="id134">
<h3>Accelerated k-means and mini-batch k-means</h3>

<p>Another<a data-type="indexterm" data-primary="accelerated k-means" id="id1987"/><a data-type="indexterm" data-primary="k-means algorithm" data-secondary="accelerated k-means" id="id1988"/> improvement to the <em>k</em>-means algorithm was proposed in a <a href="https://homl.info/38">2003 paper</a> by Charles Elkan.‚Å†<sup><a data-type="noteref" id="id1989-marker" href="ch08.html#id1989">4</a></sup> On some large datasets with many clusters, the algorithm can be accelerated by avoiding many unnecessary distance calculations. Elkan achieved this by exploiting the triangle inequality (i.e., that a straight line is always the shortest distance between two points‚Å†<sup><a data-type="noteref" id="id1990-marker" href="ch08.html#id1990">5</a></sup>) and by keeping track of lower and upper bounds for distances between instances and centroids. However, Elkan‚Äôs algorithm does not always accelerate training, and sometimes it can even slow down training significantly; it depends on the dataset. Still, if you want to give it a try, set <code translate="no">algorithm="elkan"</code>.</p>

<p>Yet<a data-type="indexterm" data-primary="k-means algorithm" data-secondary="mini-batch k-means" id="id1991"/><a data-type="indexterm" data-primary="mini-batch k-means" id="id1992"/> another important variant of the <em>k</em>-means algorithm was proposed in a <a href="https://homl.info/39">2010 paper</a> by David Sculley.‚Å†<sup><a data-type="noteref" id="id1993-marker" href="ch08.html#id1993">6</a></sup> Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the <code translate="no">MiniBatchKMeans</code><a data-type="indexterm" data-primary="sklearn" data-secondary="cluster.MiniBatchKMeans" id="id1994"/> class, which you can use just like the <code translate="no">KMeans</code> class:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">MiniBatchKMeans</code>

<code class="n">minibatch_kmeans</code> <code class="o">=</code> <code class="n">MiniBatchKMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">minibatch_kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>If the dataset does not fit in memory, the simplest option is to use the <code translate="no">memmap</code> class, as we did for incremental PCA in <a data-type="xref" href="ch07.html#dimensionality_chapter">Chapter¬†7</a>. Alternatively, you can pass one mini-batch at a time to the <code translate="no">partial_fit()</code> method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Finding the optimal number of clusters"><div class="sect3" id="id135">
<h3>Finding the optimal number of clusters</h3>

<p>So<a data-type="indexterm" data-primary="k-means algorithm" data-secondary="finding optimal number of clusters" id="xi_kmeansalgorithmfindingoptimalnumberofclusters82103_1"/> far, we‚Äôve set the number of clusters <em>k</em> to 5 because it was obvious by looking at the data that this was the correct number of clusters. But in general, it won‚Äôt be so easy to know how to set <em>k</em>, and the result might be quite bad if you set it to the wrong value. As you can see in <a data-type="xref" href="#bad_n_clusters_plot">Figure¬†8-6</a>, for this dataset setting <em>k</em> to 3 or 8 results in fairly bad models.</p>

<p>You might be thinking that you could just pick the model with the lowest inertia. Unfortunately, it is not that simple. The inertia<a data-type="indexterm" data-primary="inertia, model" id="id1995"/> for <em>k</em> = 3 is about 653.2, which is much higher than for <em>k</em> = 5 (211.7). But with <em>k</em> = 8, the inertia is just 127.1. The inertia is not a good performance metric when trying to choose <em>k</em> because it keeps getting lower as we increase <em>k</em>. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. Let‚Äôs plot the inertia as a function of <em>k</em>. When we do this, the curve often contains an inflexion point called the <em>elbow</em> (see <a data-type="xref" href="#inertia_vs_k_plot">Figure¬†8-7</a>).</p>

<figure><div id="bad_n_clusters_plot" class="figure">
<img src="assets/hmls_0806.png" alt="Diagram showing clustering outcomes; with _k_ = 3, distinct clusters merge, and with _k_ = 8, some clusters split into multiple parts." width="2875" height="829"/>
<h6><span class="label">Figure 8-6. </span>Bad choices for the number of clusters: when k is too small, separate clusters get merged (left), and when k is too large, some clusters get chopped into multiple pieces (right)</h6>
</div></figure>

<figure class="width-85"><div id="inertia_vs_k_plot" class="figure">
<img src="assets/hmls_0807.png" alt="Line graph showing inertia decreasing sharply with increasing clusters \(k\), with an elbow at \(k = 4\)." width="2272" height="912"/>
<h6><span class="label">Figure 8-7. </span>Plotting the inertia as a function of the number of clusters <em>k</em></h6>
</div></figure>

<p>As you can see, the inertia drops very quickly as we increase <em>k</em> up to 4, but then it decreases much more slowly as we keep increasing <em>k</em>. This curve has roughly the shape of an arm, and there is an elbow at <em>k</em> = 4. So, if we did not know better, we might think 4 was a good choice: any lower value would be dramatic, while any higher value would not help much, and we might just be splitting perfectly good clusters in half for no good reason.</p>

<p>This technique for choosing the best value for the number of clusters is rather coarse. A more precise (but also more computationally expensive) approach is to use the <span class="keep-together"><em>silhouette score</em></span>, which is the mean <em>silhouette coefficient</em><a data-type="indexterm" data-primary="silhouette coefficient" id="id1996"/> over all the instances. An <span class="keep-together">instance‚Äôs</span> silhouette coefficient is equal to  (<em>b</em> ‚Äì <em>a</em>) / max(<em>a</em>, <em>b</em>), where <em>a</em> is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and <em>b</em> is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes <em>b</em>, excluding the instance‚Äôs own cluster). The silhouette coefficient can vary between ‚Äì1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to ‚Äì1 means that the instance may have been assigned to the wrong cluster.</p>

<p>To compute the silhouette score, you can use Scikit-Learn‚Äôs <code translate="no">silhouette_score()</code> function<a data-type="indexterm" data-primary="sklearn" data-secondary="metrics.silhouette_score()" id="id1997"/><a data-type="indexterm" data-primary="silhouette_score()" id="id1998"/>, giving it all the instances in the dataset and the labels they were assigned:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code><code class="w"> </code><code class="nn">sklearn.metrics</code><code class="w"> </code><code class="kn">import</code> <code class="n">silhouette_score</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">silhouette_score</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code><code class="w"/>
<code class="go">np.float64(0.655517642572828)</code></pre>

<p>Let‚Äôs compare the silhouette scores for different numbers of clusters (see <a data-type="xref" href="#silhouette_score_vs_k_plot">Figure¬†8-8</a>).</p>

<figure class="width-75"><div id="silhouette_score_vs_k_plot" class="figure">
<img src="assets/hmls_0808.png" alt="Line plot showing silhouette scores for various cluster counts, with peaks at _k_ = 4 and 5, indicating optimal clustering choices." width="2304" height="759"/>
<h6><span class="label">Figure 8-8. </span>Selecting the number of clusters <em>k</em> using the silhouette score</h6>
</div></figure>

<p>As you can see, this visualization is much richer than the previous one: although it confirms that <em>k</em> = 4 is a very good choice, it also highlights the fact that <em>k</em> = 5 is quite good as well, and much better than <em>k</em> = 6 or 7. This was not visible when comparing inertias.</p>

<p>An even more informative visualization is obtained when we plot every instance‚Äôs silhouette coefficient, sorted by the clusters they are assigned to and by the value of the coefficient. This is called a <em>silhouette diagram</em><a data-type="indexterm" data-primary="silhouette diagram" id="id1999"/> (see <a data-type="xref" href="#silhouette_analysis_plot">Figure¬†8-9</a>). Each diagram contains one knife shape per cluster. The shape‚Äôs height indicates the number of instances in the cluster, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better).</p>

<p>The vertical dashed lines represent the mean silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score (i.e., if many of the instances stop short of the dashed line, ending to the left of it), then the cluster is rather bad since this means its instances are much too close to other clusters. Here we can see that when <em>k</em> = 3 or 6, we get bad clusters. But when <em>k</em> = 4 or 5, the clusters look pretty good: most instances extend beyond the dashed line, to the right and closer to 1.0. When <em>k</em> = 4, the cluster at index 0 (at the bottom) is rather big. When <em>k</em> = 5, all clusters have similar sizes. So, even though the overall silhouette score from <em>k</em> = 4 is slightly greater than for <em>k</em> = 5, it seems like a good idea to use <em>k</em> = 5 to get clusters of similar sizes.<a data-type="indexterm" data-startref="xi_kmeansalgorithmfindingoptimalnumberofclusters82103_1" id="id2000"/></p>

<figure><div id="silhouette_analysis_plot" class="figure">
<img src="assets/hmls_0809.png" alt="Silhouette diagrams comparing cluster quality for different values of _k_, showing that clusters are more balanced when _k_ = 5, despite slightly higher overall scores for _k_ = 4." width="3139" height="2561"/>
<h6><span class="label">Figure 8-9. </span>Analyzing the silhouette diagrams for various values of <em>k</em></h6>
</div></figure>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Limits of k-Means"><div class="sect2" id="id401">
<h2>Limits of k-Means</h2>

<p>Despite<a data-type="indexterm" data-primary="k-means algorithm" data-secondary="limitations of" id="id2001"/> its many merits, most notably being fast and scalable, <em>k</em>-means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, <em>k</em>-means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes. For example, <a data-type="xref" href="#bad_kmeans_plot">Figure¬†8-10</a> shows how <em>k</em>-means clusters a dataset containing three ellipsoidal clusters of different dimensions, densities, and orientations.</p>

<p>As you can see, neither of these solutions is any good. The solution on the left is better, but it still chops off 25% of the middle cluster and assigns it to the cluster on the right. The solution on the right is just terrible, even though its inertia is lower. So, depending on the data, different clustering algorithms may perform better. On these types of elliptical clusters, Gaussian mixture models work great.</p>

<figure><div id="bad_kmeans_plot" class="figure">
<img src="assets/hmls_0810.png" alt="Diagram comparing two k-means clustering attempts on ellipsoidal data, highlighting poor performance in clustering accuracy." width="2855" height="829"/>
<h6><span class="label">Figure 8-10. </span>k-means fails to cluster these ellipsoidal blobs properly</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>It is important to scale the input features (see <a data-type="xref" href="ch02.html#project_chapter">Chapter¬†2</a>) before you run <em>k</em>-means, or the clusters may be very stretched and <em>k</em>-means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally helps <em>k</em>-means.</p>
</div>

<p>Now let‚Äôs look at a few ways we can benefit from clustering. We will use <em>k</em>-means, but feel free to experiment with other clustering algorithms.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Using Clustering for Image Segmentation"><div class="sect2" id="id136">
<h2>Using Clustering for Image Segmentation</h2>

<p><em>Image segmentation</em><a data-type="indexterm" data-primary="clustering algorithms" data-secondary="image segmentation" id="xi_clusteringalgorithmsimagesegmentation826721_1"/><a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="segmentation" id="xi_imagesclassifyingandgeneratingsegmentation826721_1"/> is the task of partitioning an image into multiple segments. There are several variants:</p>

<ul>
<li>
<p>In <em>color segmentation</em>,<a data-type="indexterm" data-primary="color segmentation, images" id="id2002"/> pixels with a similar color get assigned to the same segment. This is sufficient in many applications. For example, if you want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine.</p>
</li>
<li>
<p>In <em>semantic segmentation</em>,<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="semantic segmentation" id="id2003"/><a data-type="indexterm" data-primary="semantic segmentation" id="id2004"/><a data-type="indexterm" data-primary="dense prediction, transformers for" data-secondary="semantic segmentation" id="id2005"/> all pixels that are part of the same object type get assigned to the same segment. For example, in a self-driving car‚Äôs vision system, all pixels that are part of a pedestrian‚Äôs image might be assigned to the ‚Äúpedestrian‚Äù segment (there would be one segment containing all the pedestrians).</p>
</li>
<li>
<p>In <em>instance segmentation</em>,<a data-type="indexterm" data-primary="instance segmentation" id="id2006"/> all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a different segment for each pedestrian.</p>
</li>
</ul>

<p>The state of the art in semantic or instance segmentation today is achieved using complex architectures based on convolutional neural networks (see <a data-type="xref" href="ch12.html#cnn_chapter">Chapter¬†12</a>) or vision transformers (see <a data-type="xref" href="ch16.html#vit_chapter">Chapter¬†16</a>). In this chapter we are going to focus on the (much simpler) color segmentation task, using <em>k</em>-means.</p>

<p>We‚Äôll start by importing the Pillow package (successor to the Python Imaging Library, PIL), which we‚Äôll then use to load the <em>ladybug.png</em> image (see the upper-left image in <a data-type="xref" href="#image_segmentation_plot">Figure¬†8-11</a>), assuming it‚Äôs located at <code translate="no">filepath</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code><code class="w"> </code><code class="nn">PIL</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">image</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">PIL</code><code class="o">.</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">))</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">image</code><code class="o">.</code><code class="n">shape</code><code class="w"/>
<code class="go">(533, 800, 3)</code></pre>

<p>The image is represented as a 3D array. The first dimension‚Äôs size is the height; the second is the width; and the third is the number of color channels, in this case red, green, and blue (RGB). In other words, for each pixel there is a 3D vector containing the intensities of red, green, and blue as unsigned 8-bit integers between 0 and 255. Some images may have fewer channels (such as grayscale images, which only have one), and some images may have more channels (such as images with an additional <em>alpha channel</em> for transparency, or satellite images, which often contain channels for additional light frequencies, like infrared).</p>

<p>The following code reshapes the array to get a long list of RGB colors, then it clusters these colors using <em>k</em>-means with eight clusters. It creates a <code translate="no">segmented_img</code> array containing the nearest cluster center for each pixel (i.e., the mean color of each pixel‚Äôs cluster), and lastly it reshapes this array to the original image shape. The third line uses advanced NumPy indexing; for example, if the first 10 labels in <code translate="no">kmeans_.labels_</code> are equal to 1, then the first 10 colors in <code translate="no">segmented_img</code> are equal to <code translate="no">kmeans.cluster_centers_[1]</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">segmented_img</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">]</code>
<code class="n">segmented_img</code> <code class="o">=</code> <code class="n">segmented_img</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">image</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p>This outputs the image shown in the upper right of <a data-type="xref" href="#image_segmentation_plot">Figure¬†8-11</a>. You can experiment with various numbers of clusters, as shown in the figure. When you use fewer than eight clusters, notice that the ladybug‚Äôs flashy red color fails to get a cluster of its own: it gets merged with colors from the environment. This is because <em>k</em>-means prefers clusters of similar sizes. The ladybug is small‚Äîmuch smaller than the rest of the image‚Äîso even though its color is flashy, <em>k</em>-means fails to dedicate a cluster to it.</p>

<figure><div id="image_segmentation_plot" class="figure">
<img src="assets/hmls_0811.png" alt="Image showing the effect of _k_-means clustering with 10, 8, 6, 4, and 2 color clusters on an original image of a ladybug on a dandelion, illustrating how fewer clusters merge colors and lose detail." width="2325" height="1175"/>
<h6><span class="label">Figure 8-11. </span>Image segmentation using <em>k</em>-means with various numbers of color clusters</h6>
</div></figure>

<p>That wasn‚Äôt too hard, was it? Now let‚Äôs look at another application of clustering.<a data-type="indexterm" data-startref="xi_clusteringalgorithmsimagesegmentation826721_1" id="id2007"/><a data-type="indexterm" data-startref="xi_imagesclassifyingandgeneratingsegmentation826721_1" id="id2008"/><a data-type="indexterm" data-startref="xi_kmeansalgorithm8519_1" id="id2009"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Using Clustering for Semi-Supervised Learning"><div class="sect2" id="id137">
<h2>Using Clustering for Semi-Supervised Learning</h2>

<p>Another<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="semi-supervised learning with" id="xi_clusteringalgorithmssemisupervisedlearningwith83078_1"/><a data-type="indexterm" data-primary="semi-supervised learning" id="xi_semisupervisedlearning83078_1"/> use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances. For example, clustering can help choose which additional instances to label (e.g., near the cluster centroids). It can also be used to propagate the most common label in each cluster to the unlabeled instances in that cluster. Let‚Äôs try these ideas on the digits dataset, which is a simple MNIST-like dataset containing 1,797 grayscale 8 √ó 8 images representing the digits 0 to 9. First, let‚Äôs load and split the dataset (it‚Äôs already shuffled):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>

<code class="n">X_digits</code><code class="p">,</code> <code class="n">y_digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">(</code><code class="n">return_X_y</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">X_digits</code><code class="p">[:</code><code class="mi">1400</code><code class="p">],</code> <code class="n">y_digits</code><code class="p">[:</code><code class="mi">1400</code><code class="p">]</code>
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">X_digits</code><code class="p">[</code><code class="mi">1400</code><code class="p">:],</code> <code class="n">y_digits</code><code class="p">[</code><code class="mi">1400</code><code class="p">:]</code></pre>

<p>We will pretend we only have labels for 50 instances. To get a baseline performance, let‚Äôs train a logistic regression model on these 50 labeled instances:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>

<code class="n">n_labeled</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">10_000</code><code class="p">)</code>
<code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="n">n_labeled</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="n">n_labeled</code><code class="p">])</code></pre>

<p class="pagebreak-before">We can then measure the accuracy of this model on the test set (note that the test set must be labeled):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.7581863979848866</code></pre>

<p>The model‚Äôs accuracy is just 75.8%. That‚Äôs not great: indeed, if you try training the model on the full training set, you will find that it will reach about 90.9% accuracy. Let‚Äôs see how we can do better. First, let‚Äôs cluster the training set into 50 clusters. Then, for each cluster, we‚Äôll find the image closest to the centroid. We‚Äôll call these images the <em>representative images</em>:<a data-type="indexterm" data-primary="images, classifying and generating" data-secondary="representative images" id="id2010"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">k</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">X_digits_dist</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">representative_digit_idx</code> <code class="o">=</code> <code class="n">X_digits_dist</code><code class="o">.</code><code class="n">argmin</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">X_representative_digits</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">representative_digit_idx</code><code class="p">]</code></pre>

<p><a data-type="xref" href="#representative_images_plot">Figure¬†8-12</a> shows the 50 representative images.</p>

<figure class="width-75"><div id="representative_images_plot" class="figure">
<img src="assets/hmls_0812.png" alt="Fifty handwritten digit images, each representing a distinct cluster for analysis." width="1777" height="462"/>
<h6><span class="label">Figure 8-12. </span>Fifty representative digit images (one per cluster)</h6>
</div></figure>

<p>Let‚Äôs look at each image and manually label them:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_representative_digits</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">8</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">])</code></pre>

<p>Now we have a dataset with just 50 labeled instances, but instead of being random instances, each of them is a representative image of its cluster. Let‚Äôs see if the performance is any better:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">10_000</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_representative_digits</code><code class="p">,</code> <code class="n">y_representative_digits</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.8337531486146096</code></pre>

<p>Wow! We jumped from 75.8% accuracy to 83.4%, although we are still only training the model on 50 instances. Since it is often costly and painful to label instances, especially when it has to be done manually by experts, it is a good idea to label representative instances rather than just random instances.</p>

<p>But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster? This is called <em>label propagation</em>:<a data-type="indexterm" data-primary="label propagation" id="xi_labelpropagation8376157_1"/></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">y_train_propagated</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">empty</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int64</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">):</code>
    <code class="n">y_train_propagated</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code> <code class="o">==</code> <code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">y_representative_digits</code><code class="p">[</code><code class="n">i</code><code class="p">]</code></pre>

<p>Now let‚Äôs train the model again and look at its performance:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_propagated</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.8690176322418136</code></pre>

<p>We got another significant accuracy boost! Let‚Äôs see if we can do even better by ignoring the 50% of instances that are farthest from their cluster center: this should eliminate some outliers. The following code first computes the distance from each instance to its closest cluster center, then for each cluster it sets the 50% largest distances to ‚Äì1. Lastly, it creates a set without these instances marked with a ‚Äì1 distance:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">percentile_closest</code> <code class="o">=</code> <code class="mi">50</code>

<code class="n">X_cluster_dist</code> <code class="o">=</code> <code class="n">X_digits_dist</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)),</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">]</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">):</code>
    <code class="n">in_cluster</code> <code class="o">=</code> <code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code> <code class="o">==</code> <code class="n">i</code><code class="p">)</code>
    <code class="n">cluster_dist</code> <code class="o">=</code> <code class="n">X_cluster_dist</code><code class="p">[</code><code class="n">in_cluster</code><code class="p">]</code>
    <code class="n">cutoff_distance</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">cluster_dist</code><code class="p">,</code> <code class="n">percentile_closest</code><code class="p">)</code>
    <code class="n">above_cutoff</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_cluster_dist</code> <code class="o">&gt;</code> <code class="n">cutoff_distance</code><code class="p">)</code>
    <code class="n">X_cluster_dist</code><code class="p">[</code><code class="n">in_cluster</code> <code class="o">&amp;</code> <code class="n">above_cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>

<code class="n">partially_propagated</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_cluster_dist</code> <code class="o">!=</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">X_train_partially_propagated</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">]</code>
<code class="n">y_train_partially_propagated</code> <code class="o">=</code> <code class="n">y_train_propagated</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">]</code></pre>

<p>Now let‚Äôs train the model again on this partially propagated dataset and see what accuracy we get:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">10_000</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_partially_propagated</code><code class="p">,</code> <code class="n">y_train_partially_propagated</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code><code class="w"/>
<code class="go">0.8841309823677582</code></pre>

<p>Nice! With just 50 labeled instances (only 5 examples per class on average!) we got 88.4% accuracy, pretty close to the performance we got on the fully labeled digits dataset. This is partly thanks to the fact that we dropped some outliers, and partly because the propagated labels are actually pretty good‚Äîtheir accuracy is about 98.9%, as the following code shows:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="p">(</code><code class="n">y_train_partially_propagated</code> <code class="o">==</code> <code class="n">y_train</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">])</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="w"/>
<code class="go">np.float64(0.9887798036465638)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Scikit-Learn also offers two classes that can propagate labels automatically: <code translate="no">LabelSpreading</code><a data-type="indexterm" data-primary="sklearn" data-secondary="semi_supervised.LabelPropagation" id="id2011"/> and <code translate="no">LabelPropagation</code><a data-type="indexterm" data-primary="LabelSpreading" id="id2012"/><a data-type="indexterm" data-primary="sklearn" data-secondary="semi_supervised.LabelSpreading" id="id2013"/> in the <code>sklearn.‚Äãsemi_supervised</code> package. Both classes construct a similarity matrix between all the instances, and iteratively propagate labels from labeled instances to similar unlabeled instances. There‚Äôs also a different class called <code translate="no">SelfTrainingClassifier</code><a data-type="indexterm" data-primary="sklearn" data-secondary="semi_supervised.SelfTrainingClassifier" id="id2014"/><a data-type="indexterm" data-primary="SelfTrainingClassifier" id="id2015"/> in the same package: you give it a base classifier (e.g., <code>RandomForestClassifier</code>) and it trains it on the labeled instances, then uses it to predict labels for the unlabeled samples. It then updates the training set with the labels it is most confident about, and repeats this process of training and labeling until it cannot add labels anymore. These techniques are not magic bullets, but they can occasionally give your model a little boost.<a data-type="indexterm" data-primary="active learning" id="id2016"/><a data-type="indexterm" data-startref="xi_labelpropagation8376157_1" id="id2017"/><a data-type="indexterm" data-primary="uncertainty sampling" id="id2018"/></p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="activelearning_sb">
<h1>Active Learning</h1>
<p>To continue improving your model and your training set, the next step could be to do a few rounds of <em>active learning</em>, which is when a human expert interacts with the learning algorithm, providing labels for specific instances when the algorithm requests them. There are many different strategies for active learning, but one of the most common ones is called <em>uncertainty sampling</em>. Here is how it works:</p>
<ol>
<li>
<p>The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances.</p>
</li>
<li>
<p>The instances for which the model is most uncertain (i.e., where its estimated probability is lowest) are given to the expert for labeling.</p>
</li>
<li>
<p>You iterate this process until the performance improvement stops being worth the labeling effort.</p>
</li>

</ol>

<p>Other active learning strategies include labeling the instances that would result in the largest model change or the largest drop in the model‚Äôs validation error, or the instances that different models disagree on (e.g., an SVM and a random forest).</p>
</div></aside>

<p>Before we move on to Gaussian mixture models, let‚Äôs take a look at DBSCAN, another popular clustering algorithm that illustrates a very different approach based on local density estimation. This approach allows the algorithm to identify clusters of arbitrary shapes.<a data-type="indexterm" data-startref="xi_clusteringalgorithmssemisupervisedlearningwith83078_1" id="id2019"/><a data-type="indexterm" data-startref="xi_semisupervisedlearning83078_1" id="id2020"/></p>
</div></section>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="DBSCAN"><div class="sect2" id="id138">
<h2>DBSCAN</h2>

<p>The <em>density-based spatial clustering of applications with noise</em> (DBSCAN)<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="DBSCAN" id="xi_clusteringalgorithmsDBSCAN845075_1"/><a data-type="indexterm" data-primary="DBSCAN" id="xi_DBSCAN845075_1"/><a data-type="indexterm" data-primary="density estimation" id="xi_densityestimation845075_1"/> algorithm defines clusters as continuous regions of high density. Here is how it works:</p>

<ul>
<li>
<p>For each instance, the algorithm counts how many instances are located within a small distance Œµ (epsilon) from it. This region is called the instance‚Äôs<a data-type="indexterm" data-primary="Œµ-neighborhood" id="id2021"/> 
<span class="keep-together"><em>Œµ-neighborhood</em>.</span></p>
</li>
<li>
<p>If an instance has at least <code translate="no">min_samples</code> instances in its Œµ-neighborhood (including itself), then it is considered a <em>core instance</em>. In other words, core instances are those that are located in dense regions.</p>
</li>
<li>
<p>All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances<a data-type="indexterm" data-primary="core instance" id="id2022"/>; therefore, a long sequence of neighboring core instances forms a single cluster.</p>
</li>
<li>
<p>Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.</p>
</li>
</ul>

<p>This algorithm works well if all the clusters are well separated by low-density regions. The <code translate="no">DBSCAN</code> class<a data-type="indexterm" data-primary="sklearn" data-secondary="cluster.DBSCAN" id="id2023"/> in Scikit-Learn is as simple to use as you might expect. Let‚Äôs test it on the moons dataset, introduced in <a data-type="xref" href="ch05.html#trees_chapter">Chapter¬†5</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">DBSCAN</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">dbscan</code> <code class="o">=</code> <code class="n">DBSCAN</code><code class="p">(</code><code class="n">eps</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">min_samples</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">dbscan</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>The labels of all the instances are now available in the <code translate="no">labels_</code> instance variable:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="w"/>
<code class="go">array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5, [...], 3,  3,  4,  2,  6,  3])</code></pre>

<p>Notice that some instances have a cluster index equal to ‚Äì1, which means that they are considered as anomalies by the algorithm. The indices of the core instances are available in the <code translate="no">core_sample_indices_</code> instance variable, and the core instances themselves are available in the <code translate="no">components_</code> instance variable:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="w"/>
<code class="go">array([  0,   4,   5,   6,   7,   8,  10,  11, [...], 993, 995, 997, 998, 999])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">components_</code><code class="w"/>
<code class="go">array([[-0.02137124,  0.40618608],</code>
<code class="go">       [-0.84192557,  0.53058695],</code>
<code class="go">       [...],</code>
<code class="go">       [ 0.79419406,  0.60777171]])</code></pre>

<p class="pagebreak-before">This clustering is represented in the lefthand plot of <a data-type="xref" href="#dbscan_plot">Figure¬†8-13</a>. As you can see, it identified quite a lot of anomalies, plus seven different clusters. How disappointing! Fortunately, if we widen each instance‚Äôs neighborhood by increasing <code translate="no">eps</code> to 0.2, we get the clustering on the right, which looks perfect. Let‚Äôs continue with this model.</p>

<figure><div id="dbscan_plot" class="figure">
<img src="assets/hmls_0813.png" alt="DBSCAN clustering results with `eps` values of 0.05 and 0.20, showing improved clustering with fewer anomalies in the right plot." width="2548" height="829"/>
<h6><span class="label">Figure 8-13. </span>DBSCAN clustering using two different neighborhood radiuses</h6>
</div></figure>

<p>Surprisingly, the <code translate="no">DBSCAN</code> class does not have a <code translate="no">predict()</code> method, although it has a <code translate="no">fit_predict()</code> method. In other words, it cannot predict which cluster a new instance belongs to. This decision was made because different classification algorithms can be better for different tasks, so the authors decided to let the user choose which one to use. Moreover, it‚Äôs not hard to implement. For example, let‚Äôs train a <code translate="no">KNeighborsClassifier</code><a data-type="indexterm" data-primary="KNeighborsClassifier" id="id2024"/><a data-type="indexterm" data-primary="sklearn" data-secondary="neighbors.KNeighborsClassifier" id="id2025"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">dbscan</code><code class="o">.</code><code class="n">components_</code><code class="p">,</code> <code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="p">[</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">])</code></pre>

<p>Now, given a few new instances, we can predict which clusters they most likely belong to and even estimate a probability for each cluster:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">0.5</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">]])</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">knn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code><code class="w"/>
<code class="go">array([1, 0, 1, 0])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">knn</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code><code class="w"/>
<code class="go">array([[0.18, 0.82],</code>
<code class="go">       [1.  , 0.  ],</code>
<code class="go">       [0.12, 0.88],</code>
<code class="go">       [1.  , 0.  ]])</code></pre>

<p>Note that we only trained the classifier on the core instances, but we could also have chosen to train it on all the instances, or all but the anomalies: this choice depends on the final task.</p>

<p>The decision boundary is represented in <a data-type="xref" href="#cluster_classification_plot">Figure¬†8-14</a> (the crosses represent the four instances in <code translate="no">X_new</code>). Notice that since there is no anomaly in the training set, the classifier always chooses a cluster, even when that cluster is far away. It is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies. To do this, use the <code translate="no">kneighbors()</code> method of the <code translate="no">KNeighborsClassifier</code>. Given a set of instances, it returns the distances and the indices of the <em>k</em>-nearest neighbors in the training set (two matrices, each with <em>k</em> columns):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_dist</code><code class="p">,</code> <code class="n">y_pred_idx</code> <code class="o">=</code> <code class="n">knn</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="p">[</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">][</code><code class="n">y_pred_idx</code><code class="p">]</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="p">[</code><code class="n">y_dist</code> <code class="o">&gt;</code> <code class="mf">0.2</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="o">.</code><code class="n">ravel</code><code class="p">()</code><code class="w"/>
<code class="go">array([-1,  0,  1, -1])</code></pre>

<figure class="width-80"><div id="cluster_classification_plot" class="figure">
<img src="assets/hmls_0814.png" alt="Diagram showing a decision boundary between two clusters, illustrating how DBSCAN identifies clusters of varying shapes." width="1648" height="771"/>
<h6><span class="label">Figure 8-14. </span>Decision boundary between two clusters</h6>
</div></figure>

<p>In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape. It is robust to outliers, and it has just two hyperparameters (<code translate="no">eps</code> and <code translate="no">min_samples</code>). If the density varies significantly across the clusters, however, or if there‚Äôs no sufficiently low-density region around some clusters, DBSCAN can struggle to capture all the clusters properly. Moreover, its computational complexity<a data-type="indexterm" data-primary="computational complexity" data-secondary="DBSCAN" id="id2026"/> is roughly <em>O</em>(<em>m</em><sup>2</sup><em>n</em>), so it does not scale well to <a data-type="indexterm" data-primary="HDBSCAN (hierarchical DBSCAN)" id="id2027"/><a data-type="indexterm" data-primary="hierarchical DBSCAN (HDBSCAN)" id="id2028"/>large<a data-type="indexterm" data-startref="xi_clusteringalgorithmsDBSCAN845075_1" id="id2029"/><a data-type="indexterm" data-startref="xi_DBSCAN845075_1" id="id2030"/><a data-type="indexterm" data-startref="xi_densityestimation845075_1" id="id2031"/> 
<span class="keep-together">datasets.</span></p>
<div data-type="tip"><h6>Tip</h6>
<p>You may also want to try <em>hierarchical DBSCAN</em> (HDBSCAN), using <code translate="no">sklearn.cluster.HDBSCAN</code>: it is often better than DBSCAN at finding clusters of varying densities.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Other Clustering Algorithms"><div class="sect2" id="id139">
<h2>Other Clustering Algorithms</h2>

<p>Scikit-Learn implements several more clustering algorithms that you should take a look at. I cannot cover them all in detail here, but here is a brief overview:</p>
<dl>
<dt>Agglomerative clustering</dt>
<dd>
<p><a data-type="indexterm" data-primary="agglomerative clustering" id="id2032"/><a data-type="indexterm" data-primary="clustering algorithms" data-secondary="agglomerative clustering" id="id2033"/>A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles floating on water and gradually attaching to each other until there‚Äôs one big group of bubbles. Similarly, at each iteration, agglomerative clustering connects the nearest pair of clusters (starting with individual instances). If you drew a tree with a branch for every pair of clusters that merged, you would get a binary tree<a data-type="indexterm" data-primary="binary trees" id="id2034"/> of clusters, where the leaves are the individual instances. This approach can capture clusters of various shapes; it also produces a flexible and informative cluster tree instead of forcing you to choose a particular cluster scale, and it can be used with any pairwise distance. It can scale nicely to large numbers of instances if you provide a connectivity matrix, which is a sparse <em>m</em> √ó <em>m</em> matrix that indicates which pairs of instances are neighbors (e.g., returned by <code translate="no">sklearn.neighbors.kneighbors_graph()</code>). Without a connectivity matrix, the algorithm does not scale well to large datasets.</p>
</dd>
<dt>BIRCH</dt>
<dd>
<p><a data-type="indexterm" data-primary="Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)" id="id2035"/><a data-type="indexterm" data-primary="BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)" id="id2036"/><a data-type="indexterm" data-primary="clustering algorithms" data-secondary="BIRCH" id="id2037"/>The balanced iterative reducing and clustering using hierarchies (BIRCH) algorithm was designed specifically for very large datasets, and it can be faster than batch <em>k</em>-means, with similar results, as long as the number of features is not too large (&lt;20). During training, it builds a tree structure containing just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree: this approach allows it to use limited memory while handling huge datasets.</p>
</dd>
<dt>Mean-shift</dt>
<dd>
<p><a data-type="indexterm" data-primary="clustering algorithms" data-secondary="mean-shift" id="id2038"/>T<a data-type="indexterm" data-primary="mean-shift, clustering algorithms" id="id2039"/>his algorithm starts by placing a circle centered on each instance; then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting step until all the circles stop moving (i.e., until each of them is centered on the mean of the instances it contains). Mean-shift shifts the circles in the direction of higher density, until each of them has found a local density maximum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. Mean-shift has some of the same features as DBSCAN, like how it can find any number of clusters of any shape, it has very few hyperparameters (just one‚Äîthe radius of the circles, called the <em>bandwidth</em>), and it relies on local density estimation. But unlike DBSCAN, mean-shift tends to chop clusters into pieces when they have internal density variations. Unfortunately, its computational complexity is <em>O</em>(<em>m</em><sup>2</sup><em>n</em>), so it is not suited for large datasets.</p>
</dd>
<dt>Affinity propagation</dt>
<dd>
<p><a data-type="indexterm" data-primary="affinity propagation" id="id2040"/><a data-type="indexterm" data-primary="clustering algorithms" data-secondary="affinity propagation" id="id2041"/>In this algorithm, instances repeatedly exchange messages between one another until every instance has elected another instance (or itself) to represent it. These elected instances are called <em>exemplars</em>.<a data-type="indexterm" data-primary="exemplars, affinity propagation" id="id2042"/> Each exemplar and all the instances that elected it form one cluster. In real-life politics, you typically want to vote for a candidate whose opinions are similar to yours, but you also want them to win the election, so you might choose a candidate you don‚Äôt fully agree with, but who is more popular. You typically evaluate popularity through polls. Affinity propagation works in a similar way, and it tends to choose exemplars located near the center of clusters, similar to <em>k</em>-means. But unlike with <em>k</em>-means, you don‚Äôt have to pick a number of clusters ahead of time: it is determined during training. Moreover, affinity propagation can deal nicely with clusters of different sizes. Sadly, this algorithm has a computational complexity of <em>O</em>(<em>m</em><sup>2</sup>), so it is not suited for large datasets.</p>
</dd>
<dt>Spectral clustering</dt>
<dd>
<p>T<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="spectral clustering" id="id2043"/><a data-type="indexterm" data-primary="spectral clustering" id="id2044"/>his algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., it reduces the matrix‚Äôs dimensionality), then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn‚Äôs implementation uses <em>k</em>-means). Spectral clustering can capture complex cluster structures, and it can also be used to cut graphs (e.g., to identify clusters of friends on a social network). It does not scale well to large numbers of instances, and it does not behave well when the clusters have very different sizes.</p>
</dd>
</dl>

<p>Now let‚Äôs dive into Gaussian mixture models, which can be used for density estimation, clustering, and anomaly detection.<a data-type="indexterm" data-startref="xi_clusteringalgorithms8203_1" id="id2045"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Gaussian Mixtures"><div class="sect1" id="id140">
<h1>Gaussian Mixtures</h1>

<p>A<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="GMM" id="xi_clusteringalgorithmsGMM85622_1"/><a data-type="indexterm" data-primary="Gaussian mixture model (GMM)" id="xi_GaussianmixturemodelGMM85622_1"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="GMM" id="xi_unsupervisedlearningGMM85622_1"/> <em>Gaussian mixture model</em> (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density, and orientation, just like in <a data-type="xref" href="#bad_kmeans_plot">Figure¬†8-10</a>.‚Å†<sup><a data-type="noteref" id="id2046-marker" href="ch08.html#id2046">7</a></sup> When you observe an instance<a data-type="indexterm" data-primary="clustering algorithms" data-secondary="responsibilities of clusters for instances" id="id2047"/>, you know it was generated from one of the Gaussian distributions, but you are not told which one, and you do not know what the parameters of these distributions are.</p>

<p>There are several GMM variants. In the simplest variant, implemented in the 
<span class="keep-together"><code translate="no">GaussianMixture</code></span> class<a data-type="indexterm" data-primary="GaussianMixture" id="id2048"/><a data-type="indexterm" data-primary="sklearn" data-secondary="mixture.GaussianMixture" id="id2049"/>, you must know in advance the number <em>k</em> of Gaussian distributions. The dataset <strong>X</strong> is assumed to have been generated through the following probabilistic process:</p>

<ul>
<li>
<p>For each instance, a cluster is picked randomly from among <em>k</em> clusters. The probability of choosing the <em>j</em><sup>th</sup> cluster is the cluster‚Äôs weight <em>œï</em><sup>(<em>j</em>)</sup>.‚Å†<sup><a data-type="noteref" id="id2050-marker" href="ch08.html#id2050">8</a></sup> The index of the cluster chosen for the <em>i</em><sup>th</sup> instance is denoted <em>z</em><sup>(<em>i</em>)</sup>.</p>
</li>
<li>
<p>If the <em>i</em><sup>th</sup> instance was assigned to the <em>j</em><sup>th</sup> cluster (i.e., <em>z</em><sup>(<em>i</em>)</sup> = <em>j</em>), then the location <strong>x</strong><sup>(<em>i</em>)</sup> of this instance is sampled randomly from the Gaussian distribution with mean <strong>Œº</strong><sup>(<em>j</em>)</sup> and covariance matrix <strong>Œ£</strong><sup>(<em>j</em>)</sup>. This is denoted <strong>x</strong><sup>(<em>i</em>)</sup> ~ ùí©(Œº*<sup>(<em>j</em>)</sup>, <strong>Œ£</strong><sup>(<em>j</em>)</sup>).</p>
</li>
</ul>

<p>So what can you do with such a model? Well, given the dataset <strong>X</strong>, you typically want to start by estimating the weights <strong>œï</strong> and all the distribution parameters <strong>Œº</strong><sup>(1)</sup> to <strong>Œº</strong><sup>(<em>k</em>)</sup> and <strong>Œ£</strong><sup>(1)</sup> to <strong>Œ£</strong><sup>(<em>k</em>)</sup>. Scikit-Learn‚Äôs <code translate="no">GaussianMixture</code> class makes this super easy:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">GaussianMixture</code>

<code class="n">gm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">gm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>Let‚Äôs look at the parameters that the algorithm estimated:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">weights_</code><code class="w"/>
<code class="go">array([0.40005972, 0.20961444, 0.39032584])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">means_</code><code class="w"/>
<code class="go">array([[-1.40764129,  1.42712848],</code>
<code class="go">       [ 3.39947665,  1.05931088],</code>
<code class="go">       [ 0.05145113,  0.07534576]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">covariances_</code><code class="w"/>
<code class="go">array([[[ 0.63478217,  0.72970097],</code>
<code class="go">        [ 0.72970097,  1.16094925]],</code>

<code class="go">       [[ 1.14740131, -0.03271106],</code>
<code class="go">        [-0.03271106,  0.95498333]],</code>

<code class="go">       [[ 0.68825143,  0.79617956],</code>
<code class="go">        [ 0.79617956,  1.21242183]]])</code></pre>

<p>Great, it worked fine! Indeed, two of the three clusters were generated with 500 instances each, while the third cluster only contains 250 instances. So the true cluster weights are 0.4, 0.4, and 0.2, respectively, and that‚Äôs roughly what the algorithm found (in a different order). Similarly, the true means and covariance matrices are quite close to those found by the algorithm. But how? This class relies on the <em>expectation-maximization</em> (EM) algorithm<a data-type="indexterm" data-primary="EM (expectation-maximization)" id="id2051"/><a data-type="indexterm" data-primary="expectation-maximization (EM)" id="id2052"/>, which has many similarities with the <em>k</em>-means algorithm: it also initializes the cluster parameters randomly, then it repeats two steps until convergence, first assigning instances to clusters (this is called the <em>expectation step</em>) and then updating the clusters (this is called the <em>maximization step</em>)<a data-type="indexterm" data-primary="maximization step, Gaussian mixtures" id="id2053"/>. Sounds familiar, right? In the context of clustering, you can think of EM as a generalization of <em>k</em>-means that not only finds the cluster centers (<strong>Œº</strong><sup>(1)</sup> to <strong>Œº</strong><sup>(<em>k</em>)</sup>), but also their size, shape, and orientation (<strong>Œ£</strong><sup>(1)</sup> to <strong>Œ£</strong><sup>(<em>k</em>)</sup>), as well as their relative weights (<em>œï</em><sup>(1)</sup> to <em>œï</em><sup>(<em>k</em>)</sup>). Unlike <em>k</em>-means, though, EM uses soft cluster assignments, not hard assignments. For each instance, during the expectation step, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters). Then, during the maximization step, each cluster is updated using <em>all</em> the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster. These probabilities are called the <em>responsibilities</em> of the clusters for the instances. <span class="keep-together">During</span> the maximization step, each cluster‚Äôs update will mostly be impacted by the instances it is most responsible for.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Unfortunately, just like <em>k</em>-means, EM can end up converging to poor solutions, so it needs to be run several times, keeping only the best solution. This is why we set <code translate="no">n_init</code> to 10. Be careful: by default <code translate="no">n_init</code> is set to 1.</p>
</div>

<p>You can check whether the algorithm converged and how many iterations it took:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">converged_</code><code class="w"/>
<code class="go">True</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">n_iter_</code><code class="w"/>
<code class="go">4</code></pre>

<p>Now that you have an estimate of the location, size, shape, orientation, and relative weight of each cluster, the model can easily assign each instance to the most likely cluster (hard clustering) or estimate the probability that it belongs to a particular cluster (soft clustering). Just use the <code translate="no">predict()</code> method for hard clustering, or the <code translate="no">predict_proba()</code> method for soft clustering:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="w"/>
<code class="go">array([2, 2, 0, ..., 1, 1, 1])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="w"/>
<code class="go">array([[0.   , 0.023, 0.977],</code>
<code class="go">       [0.001, 0.016, 0.983],</code>
<code class="go">       [1.   , 0.   , 0.   ],</code>
<code class="go">       ...,</code>
<code class="go">       [0.   , 1.   , 0.   ],</code>
<code class="go">       [0.   , 1.   , 0.   ],</code>
<code class="go">       [0.   , 1.   , 0.   ]])</code></pre>

<p>A Gaussian mixture model is a <em>generative model</em>, meaning you can sample new instances from it (note that they are ordered by cluster index):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code><code class="p">,</code> <code class="n">y_new</code> <code class="o">=</code> <code class="n">gm</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">6</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code><code class="w"/>
<code class="go">array([[-2.32491052,  1.04752548],</code>
<code class="go">       [-1.16654983,  1.62795173],</code>
<code class="go">       [ 1.84860618,  2.07374016],</code>
<code class="go">       [ 3.98304484,  1.49869936],</code>
<code class="go">       [ 3.8163406 ,  0.53038367],</code>
<code class="go">       [ 0.38079484, -0.56239369]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_new</code><code class="w"/>
<code class="go">array([0, 0, 1, 1, 1, 2])</code></pre>

<p>It is also possible to estimate the density<a data-type="indexterm" data-primary="density estimation" id="id2054"/> of the model at any given location. This is achieved using the <code translate="no">score_samples()</code> method: for each instance it is given, this method estimates the log of the <em>probability density function</em> (PDF) at that location. The greater the score, the higher the density:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([-2.61, -3.57, -3.33, ..., -3.51, -4.4 , -3.81])</code></pre>

<p>If you compute the exponential of these scores, you get the value of the PDF at the location of the given instances. These are not probabilities, but probability <em>densities</em>: they can take on any positive value, not just a value between 0 and 1. To estimate the probability that an instance will fall within a particular region, you would have to integrate the PDF over that region (if you do so over the entire space of possible instance locations, the result will be 1).</p>

<p><a data-type="xref" href="#gaussian_mixtures_plot">Figure¬†8-15</a> shows the cluster means, the decision boundaries (dashed lines), and the density contours of this model.</p>

<figure><div id="gaussian_mixtures_plot" class="figure">
<img src="assets/hmls_0815.png" alt="Diagram of a Gaussian mixture model showing three cluster means, decision boundaries as dashed lines, and density contours indicating distribution." width="2275" height="1066"/>
<h6><span class="label">Figure 8-15. </span>Cluster means, decision boundaries, and density contours of a trained Gaussian mixture model</h6>
</div></figure>

<p>Nice! The algorithm clearly found an excellent solution. Of course, we made its task easy by generating the data using a set of 2D Gaussian distributions (unfortunately, real-life data is not always so Gaussian and low-dimensional). We also gave the algorithm the correct number of clusters. When there are many dimensions, or many clusters, or few instances, EM can struggle to converge to the optimal solution. You might need to reduce the difficulty of the task by limiting the number of parameters that the algorithm has to learn. One way to do this is to limit the range of shapes and orientations that the clusters can have. This can be achieved by imposing constraints on the covariance matrices. To do this, set the <code translate="no">covariance_type</code> hyperparameter to one of the following values:</p>
<dl>
<dt><code translate="no">"spherical"</code></dt>
<dd>
<p>All clusters must be spherical, but they can have different diameters (i.e., different variances).</p>
</dd>
<dt><code translate="no">"diag"</code></dt>
<dd>
<p>Clusters can take on any ellipsoidal shape of any size, but the ellipsoid‚Äôs axes must be parallel to the coordinate axes (i.e., the covariance matrices must be diagonal).</p>
</dd>
<dt><code translate="no">"tied"</code></dt>
<dd>
<p>All clusters must have the same ellipsoidal shape, size, and orientation (i.e., all clusters share the same covariance matrix).</p>
</dd>
</dl>

<p>By default, <code translate="no">covariance_type</code> is equal to <code translate="no">"full"</code>, which means that each cluster can take on any shape, size, and orientation (it has its own unconstrained covariance matrix). <a data-type="xref" href="#covariance_type_plot">Figure¬†8-16</a> plots the solutions found by the EM algorithm when <code translate="no">covariance_type</code> is set to <code translate="no">"tied"</code> or <code translate="no">"spherical"</code>.</p>

<figure><div id="covariance_type_plot" class="figure">
<img src="assets/hmls_0816.png" alt="Diagram showing Gaussian mixture models for tied covariance (left) with oval clusters and spherical covariance (right) with circular clusters." width="2575" height="1069"/>
<h6><span class="label">Figure 8-16. </span>Gaussian mixtures for tied clusters (left) and spherical clusters (right)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The computational complexity<a data-type="indexterm" data-primary="computational complexity" data-secondary="Gaussian mixture model" id="id2055"/> of training a <code translate="no">GaussianMixture</code> model depends on the number of instances <em>m</em>, the number of dimensions <em>n</em>, the number of clusters <em>k</em>, and the constraints on the covariance matrices. If <code translate="no">covariance_type</code> is <code translate="no">"spherical"</code> or <code translate="no">"diag"</code>, it is <em>O</em>(<em>kmn</em>), assuming the data has a clustering structure. If <code translate="no">covariance_type</code> is <code translate="no">"tied"</code> or <code translate="no">"full"</code>, it is <em>O</em>(<em>kmn</em><sup>2</sup> + <em>kn</em><sup>3</sup>), so it will not scale to large numbers of features.</p>
</div>

<p>Gaussian mixture models can also be used for anomaly detection. We‚Äôll see how in the next section.</p>








<section data-type="sect2" data-pdf-bookmark="Using Gaussian Mixtures for Anomaly Detection"><div class="sect2" id="id141">
<h2>Using Gaussian Mixtures for Anomaly Detection</h2>

<p>Using<a data-type="indexterm" data-primary="anomaly detection" data-secondary="GMM" id="xi_anomalydetectionGMM86836_1"/><a data-type="indexterm" data-primary="Gaussian mixture model (GMM)" data-secondary="anomaly detection" id="xi_GaussianmixturemodelGMManomalydetection86836_1"/> a Gaussian mixture model for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. You must define what density threshold<a data-type="indexterm" data-primary="density threshold" id="id2056"/> you want to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well known. Say it is equal to 2%. You then set the density threshold to be the value that results in having 2% of the instances located in areas below that threshold density. If you notice that you get too many false positives (i.e., perfectly good products that are flagged as defective), you can lower the threshold. Conversely, if you have too many false negatives (i.e., defective products that the system does not flag as defective), you can increase the threshold. This is the usual precision/recall trade-off (see <a data-type="xref" href="ch03.html#classification_chapter">Chapter¬†3</a>). Here is how you would identify the outliers using the second percentile lowest density as the threshold (i.e., approximately 2% of the instances will be flagged as anomalies):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">densities</code> <code class="o">=</code> <code class="n">gm</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">density_threshold</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">densities</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">anomalies</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">densities</code> <code class="o">&lt;</code> <code class="n">density_threshold</code><code class="p">]</code></pre>

<p><a data-type="xref" href="#mixture_anomaly_detection_plot">Figure¬†8-17</a> represents these anomalies as stars.</p>

<figure><div id="mixture_anomaly_detection_plot" class="figure">
<img src="assets/hmls_0817.png" alt="Contour plot illustrating anomaly detection with a Gaussian mixture model, where anomalies are marked with red stars." width="2275" height="1066"/>
<h6><span class="label">Figure 8-17. </span>Anomaly detection using a Gaussian mixture model</h6>
</div></figure>

<p>A closely related task is <em>novelty detection</em>:<a data-type="indexterm" data-primary="novelty detection" id="id2057"/> it differs from anomaly detection in that the algorithm is assumed to be trained on a ‚Äúclean‚Äù dataset, uncontaminated by outliers, whereas anomaly detection does not make this assumption. Indeed, outlier detection is often used to clean up a dataset.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Gaussian mixture models try to fit all the data, including the outliers; if you have too many of them this will bias the model‚Äôs view of ‚Äúnormality‚Äù, and some outliers may wrongly be considered as normal. If this happens, you can try to fit the model once, use it to detect and remove the most extreme outliers, then fit the model again on the cleaned-up dataset. Another approach is to use robust covariance estimation methods (see the <code translate="no">EllipticEnvelope</code> class).</p>
</div>

<p>Just like <em>k</em>-means, the <code translate="no">GaussianMixture</code> algorithm requires you to specify the number of clusters. So how can you find that number?<a data-type="indexterm" data-startref="xi_anomalydetectionGMM86836_1" id="id2058"/><a data-type="indexterm" data-startref="xi_GaussianmixturemodelGMManomalydetection86836_1" id="id2059"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Selecting the Number of Clusters"><div class="sect2" id="id142">
<h2>Selecting the Number of Clusters</h2>

<p>With<a data-type="indexterm" data-primary="Gaussian mixture model (GMM)" data-secondary="selecting number of clusters" id="xi_GaussianmixturemodelGMMselectingnumberofclusters87075_1"/> <em>k</em>-means, you can use the inertia or the silhouette score to select the appropriate number of clusters. But with Gaussian mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes. Instead, you can try to find the model that minimizes a <em>theoretical information criterion</em>,<a data-type="indexterm" data-primary="theoretical information criterion" id="id2060"/> such as the <em>Bayesian information criterion</em> (BIC)<a data-type="indexterm" data-primary="Bayesian information criterion (BIC)" id="id2061"/><a data-type="indexterm" data-primary="BIC (Bayesian information criterion)" id="id2062"/> or the <em>Akaike information criterion</em> (AIC),<a data-type="indexterm" data-primary="Akaike information criterion (AIC)" id="id2063"/> defined in <a data-type="xref" href="#information_criteria_equation">Equation 8-2</a>.</p>
<div id="information_criteria_equation" data-type="equation">
<h5><span class="label">Equation 8-2. </span>Bayesian information criterion (BIC) and Akaike information criterion (AIC)</h5>
<math alttext="StartLayout 1st Row 1st Column Blank 2nd Column upper B upper I upper C equals normal l normal o normal g left-parenthesis m right-parenthesis p minus 2 normal l normal o normal g left-parenthesis ModifyingAbove script upper L With caret right-parenthesis 2nd Row 1st Column Blank 2nd Column upper A upper I upper C equals 2 p minus 2 normal l normal o normal g left-parenthesis ModifyingAbove script upper L With caret right-parenthesis EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mi>B</mi>
          <mi>I</mi>
          <mi>C</mi>
          <mo>=</mo>
          <mi> log </mi>
          <mrow>
            <mo>(</mo>
            <mi>m</mi>
            <mo>)</mo>
          </mrow>
          <mi>p</mi>
          <mo>-</mo>
          <mn>2</mn>
          <mspace width="0.166667em"/>
          <mi> log </mi>
          <mrow>
            <mo>(</mo>
            <mover accent="true"><mi>‚Ñí</mi> <mo>^</mo></mover>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mi>A</mi>
          <mi>I</mi>
          <mi>C</mi>
          <mo>=</mo>
          <mn>2</mn>
          <mi>p</mi>
          <mo>-</mo>
          <mn>2</mn>
          <mspace width="0.166667em"/>
          <mi> log </mi>
          <mo>(</mo>
          <mover accent="true"><mi>‚Ñí</mi> <mo>^</mo></mover>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>In these equations:</p>

<ul>
<li>
<p><em>m</em> is the number of instances, as always.</p>
</li>
<li>
<p><em>p</em> is the number of parameters learned by the model.</p>
</li>
<li>
<p><math alttext="ModifyingAbove script upper L With caret">
  <mover accent="true"><mi>‚Ñí</mi> <mo>^</mo></mover>
</math> is the maximized value of the <em>likelihood function</em> of the model.</p>
</li>
</ul>

<p>Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters) and reward models that fit the data well. They often end up selecting the same model. When they differ, the model selected by the BIC tends to be simpler (fewer parameters) than the one selected by the AIC, but tends to not fit the data quite as well (this is especially true for larger datasets).<a data-type="indexterm" data-primary="likelihood function" id="xi_likelihoodfunction8728396_1"/><a data-type="indexterm" data-primary="probability versus likelihood" id="xi_probabilityversuslikelihood8728396_1"/></p>
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id2064">
<h1>Likelihood Function</h1>
<p>The terms ‚Äúprobability‚Äù and ‚Äúlikelihood‚Äù are often used interchangeably in everyday language, but they have very different meanings in statistics. Given a statistical model with some parameters <strong>Œ∏</strong>, the word ‚Äúprobability‚Äù is used to describe how plausible a future outcome <strong>x</strong> is (knowing the parameter values <strong>Œ∏</strong>), while the word ‚Äúlikelihood‚Äù is used to describe how plausible a particular set of parameter values <strong>Œ∏</strong> are, after the outcome <strong>x</strong> is known.</p>

<p>Consider a 1D mixture model of two Gaussian distributions centered at ‚Äì4 and +1. For simplicity, this toy model has a single parameter <em>Œ∏</em> that controls the standard deviations of both distributions. The top-left contour plot in <a data-type="xref" href="#likelihood_function_plot">Figure¬†8-18</a> shows the entire model <em>f</em>(<em>x</em>; <em>Œ∏</em>) as a function of both <em>x</em> and <em>Œ∏</em>. To estimate the probability distribution of a future outcome <em>x</em>, you need to set the model parameter <em>Œ∏</em>. For example, if you set <em>Œ∏</em> to 1.3 (the horizontal line), you get the probability density function<a data-type="indexterm" data-primary="PDF (probability density function)" id="id2065"/><a data-type="indexterm" data-primary="probability density function (PDF)" id="id2066"/> <em>f</em>(<em>x</em>; <em>Œ∏</em> = 1.3) shown in the lower-left plot. Say you want to estimate the probability that <em>x</em> will fall between ‚Äì2 and +2. You must calculate the integral of the PDF on this range (i.e., the surface of the shaded region). But what if you don‚Äôt know <em>Œ∏</em>, and if instead you have observed a single instance <em>x</em> = 2.5 (the vertical line in the upper-left plot)? In this case, you get the likelihood function ‚Ñí(<em>Œ∏</em>|<em>x</em> = 2.5) = f(<em>x</em> = 2.5; <em>Œ∏</em>), represented in the upper-right plot.</p>

<figure><div id="likelihood_function_plot" class="figure">
<img src="assets/hmls_0818.png" alt="Diagram showing a model's parametric function, probability density function, likelihood function, and log likelihood function, illustrating the relationships between _x_ and _Œ∏_ in a Gaussian mixture model context." width="2255" height="1207"/>
<h6><span class="label">Figure 8-18. </span>A model‚Äôs parametric function (top left), and some derived functions: a PDF (lower left), a likelihood function (top right), and a log likelihood function (lower right)</h6>
</div></figure>

<p>In short, the PDF is a function of <em>x</em> (with <em>Œ∏</em> fixed), while the likelihood function is a function of <em>Œ∏</em> (with <em>x</em> fixed). It is important to understand that the likelihood function is <em>not</em> a probability distribution: if you integrate a probability distribution over all possible values of <em>x</em>, you always get 1, but if you integrate the likelihood function over all possible values of <em>Œ∏</em>, the result can be any positive value.</p>

<p>Given a dataset <strong>X</strong>, a common task is to try to estimate the most likely values for the model parameters. To do this, you must find the values that maximize the likelihood function, given <strong>X</strong>. In this example, if you have observed a single instance <em>x</em> = 2.5, the <em>maximum likelihood estimate</em> (MLE)<a data-type="indexterm" data-primary="maximum likelihood estimate (MLE)" id="id2067"/><a data-type="indexterm" data-primary="MLE (maximum likelihood estimate)" id="id2068"/> of <em>Œ∏</em> is <math><mover><mi mathvariant="bold">Œ∏</mi><mo>^</mo></mover></math> ‚âà 1.66. If a prior probability distribution <em>g</em> over <em>Œ∏</em> exists, it is possible to take it into account by maximizing ‚Ñí(<em>Œ∏</em>|<em>x</em>)g(<em>Œ∏</em>) rather than just maximizing ‚Ñí(<em>Œ∏</em>|<em>x</em>). This is called <em>maximum a-posteriori</em> (MAP) estimation<a data-type="indexterm" data-primary="MAP (maximum a-posteriori) estimation" id="id2069"/><a data-type="indexterm" data-primary="maximum a-posteriori (MAP) estimation" id="id2070"/>. Since MAP constrains the parameter values, you can think of it as a regularized version of MLE.</p>

<p>Notice that maximizing the likelihood function is equivalent to maximizing its logarithm (represented in the lower-right plot in <a data-type="xref" href="#likelihood_function_plot">Figure¬†8-18</a>). Indeed, the logarithm is a strictly increasing function, so if <em>Œ∏</em> maximizes the log likelihood, it also maximizes the likelihood. It turns out that it is generally easier to maximize the log likelihood. For example, if you observed several independent instances <em>x</em><sup>(1)</sup> to <em>x</em><sup>(<em>m</em>)</sup>, you would need to find the value of <em>Œ∏</em> that maximizes the product of the individual likelihood functions. But it is equivalent, and much simpler, to maximize the sum (not the product) of the log likelihood functions, thanks to the magic of the logarithm which converts products into sums: log(<em>ab</em>) = log(<em>a</em>) + log(<em>b</em>).</p>

<p>Once you have estimated <math><mover><mi mathvariant="bold">Œ∏</mi><mo>^</mo></mover></math>, the value of <em>Œ∏</em> that maximizes the likelihood function, then you are ready to compute <math><mover><mi mathvariant="script">L</mi><mo>^</mo></mover><mo>=</mo><mo mathvariant="script">L</mo><mo>(</mo><mover><mi mathvariant="bold">Œ∏</mi><mo>^</mo></mover><mo lspace="0%" rspace="0%">,</mo><mi mathvariant="bold">X</mi><mo>)</mo></math>, which is the value used to compute the AIC and BIC; you can think of it as a measure of how well the model fits the data.</p>
</div></aside>

<p>To<a data-type="indexterm" data-startref="xi_likelihoodfunction8728396_1" id="id2071"/><a data-type="indexterm" data-startref="xi_probabilityversuslikelihood8728396_1" id="id2072"/> compute the BIC and AIC, call the <code translate="no">bic()</code> and <code translate="no">aic()</code> methods:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">bic</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="w"/>
<code class="go">np.float64(8189.733705221636)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">aic</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="w"/>
<code class="go">np.float64(8102.508425106598)</code></pre>

<p><a data-type="xref" href="#aic_bic_vs_k_plot">Figure¬†8-19</a> shows the BIC for different numbers of clusters <em>k</em>. As you can see, both the BIC and the AIC are lowest when <em>k</em> = 3, so it is most likely the best choice.<a data-type="indexterm" data-startref="xi_GaussianmixturemodelGMMselectingnumberofclusters87075_1" id="id2073"/></p>

<figure class="width-85"><div id="aic_bic_vs_k_plot" class="figure">
<img src="assets/hmls_0819.png" alt="Plot showing AIC and BIC values for different numbers of clusters _k_, with both criteria reaching their minimum at _k_ = 3." width="2277" height="762"/>
<h6><span class="label">Figure 8-19. </span>AIC and BIC for different numbers of clusters <em>k</em></h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Bayesian Gaussian Mixture Models"><div class="sect2" id="id143">
<h2>Bayesian Gaussian Mixture Models</h2>

<p>Rather<a data-type="indexterm" data-primary="Bayesian Gaussian mixtures" id="id2074"/><a data-type="indexterm" data-primary="Gaussian mixture model (GMM)" data-secondary="Bayesian Gaussian mixtures" id="id2075"/> than manually searching for the optimal number of clusters, you can use the <code translate="no">BayesianGaussianMixture</code> class, which is capable of giving weights equal (or close) to zero to unnecessary clusters. Set the number of clusters <code translate="no">n_components</code> to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically. For example, let‚Äôs set the number of clusters to 10 and see what happens<a data-type="indexterm" data-primary="sklearn" data-secondary="mixture.BayesianGaussianMixture" id="id2076"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code><code class="w"> </code><code class="nn">sklearn.mixture</code><code class="w"> </code><code class="kn">import</code> <code class="n">BayesianGaussianMixture</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code> <code class="o">=</code> <code class="n">BayesianGaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code><code class="w"/>
<code class="gp">... </code>                              <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code><code class="w"/>
<code class="gp">...</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="w"/>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code><code class="o">.</code><code class="n">weights_</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="w"/>
<code class="go">array([0.4 , 0.21, 0.39, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])</code></pre>

<p>Perfect: the algorithm automatically detected that only three clusters are needed, and the resulting clusters are almost identical to the ones in <a data-type="xref" href="#gaussian_mixtures_plot">Figure¬†8-15</a>.</p>

<p>A final note about Gaussian mixture models: although they work great on clusters with ellipsoidal shapes, they don‚Äôt do so well with clusters of very different shapes. For example, let‚Äôs see what happens if we use a Bayesian Gaussian mixture model to cluster the moons dataset (see <a data-type="xref" href="#moons_vs_bgm_plot">Figure¬†8-20</a>).</p>

<p>Oops! The algorithm desperately searched for ellipsoids, so it found eight different clusters instead of two. The density estimation is not too bad, so this model could perhaps be used for anomaly detection, but it failed to identify the two moons. To conclude this chapter, let‚Äôs take a quick look at a few algorithms capable of dealing with arbitrarily shaped clusters.<a data-type="indexterm" data-startref="xi_clusteringalgorithmsGMM85622_1" id="id2077"/><a data-type="indexterm" data-startref="xi_GaussianmixturemodelGMM85622_1" id="id2078"/><a data-type="indexterm" data-startref="xi_unsupervisedlearningGMM85622_1" id="id2079"/></p>

<figure><div id="moons_vs_bgm_plot" class="figure">
<img src="assets/hmls_0820.png" alt="Diagram showing a Gaussian mixture model's attempt to fit nonellipsoidal clusters, illustrating its limitation in detecting two moon-shaped clusters by identifying eight separate sections instead." width="2575" height="831"/>
<h6><span class="label">Figure 8-20. </span>Fitting a Gaussian mixture to nonellipsoidal clusters</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Other Algorithms for Anomaly and Novelty Detection"><div class="sect2" id="id144">
<h2>Other Algorithms for Anomaly and Novelty Detection</h2>

<p>Scikit-Learn implements other algorithms dedicated to anomaly detection or novelty detection:</p>
<dl>
<dt>Fast-MCD (minimum covariance determinant)</dt>
<dd>
<p>Implemented<a data-type="indexterm" data-primary="fast-MCD" id="id2080"/><a data-type="indexterm" data-primary="anomaly detection" data-secondary="fast-MCD" id="id2081"/> by the <code translate="no">EllipticEnvelope</code> class<a data-type="indexterm" data-primary="EllipticEnvelope" id="id2082"/>, this algorithm is useful for outlier detection, in particular to clean up a dataset. It assumes that the normal instances (inliers) are generated from a single Gaussian distribution (not a mixture). It also assumes that the dataset is contaminated with outliers that were not generated from this Gaussian distribution. When the algorithm estimates the parameters of the Gaussian distribution (i.e., the shape of the elliptic envelope around the inliers), it is careful to ignore the instances that are most likely outliers. This technique gives a better estimation of the elliptic envelope and thus makes the algorithm better at identifying the outliers.</p>
</dd>
<dt>Isolation forest</dt>
<dd>
<p>T<a data-type="indexterm" data-primary="anomaly detection" data-secondary="isolation forest" id="id2083"/><a data-type="indexterm" data-primary="isolation forest" id="id2084"/>his is an efficient algorithm for outlier detection, especially in high-dimensional datasets. The algorithm builds a random forest in which each decision tree is grown randomly: at each node, it picks a feature randomly, then it picks a random threshold value (between the min and max values) to split the dataset in two. The dataset gradually gets chopped into pieces this way, until all instances end up isolated from the other instances. Anomalies are usually far from other instances, so on average (across all the decision trees) they tend to get isolated in fewer steps than normal instances.</p>
</dd>
<dt>Local outlier factor (LOF)</dt>
<dd>
<p>T<a data-type="indexterm" data-primary="anomaly detection" data-secondary="local outlier factor" id="id2085"/><a data-type="indexterm" data-primary="local outlier factor (LOF)" id="id2086"/><a data-type="indexterm" data-primary="LOF (local outlier factor)" id="id2087"/>his algorithm is also good for outlier detection. It compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its <em>k</em>-nearest neighbors.</p>
</dd>
<dt>One-class SVM</dt>
<dd>
<p>T<a data-type="indexterm" data-primary="novelty detection" id="id2088"/><a data-type="indexterm" data-primary="one-class SVM" id="id2089"/><a data-type="indexterm" data-primary="support vector machines (SVMs)" id="id2090"/>his algorithm is better suited for novelty detection. Recall that a kernelized SVM classifier separates two classes by first (implicitly) mapping all the instances to a high-dimensional space, then separating the two classes using a linear SVM classifier within this high-dimensional space (see the online chapter on SVMs at <a href="https://homl.info" class="bare"><em class="hyperlink">https://homl.info</em></a>). Since we just have one class of instances, the one-class SVM algorithm instead tries to separate the instances in high-dimensional space from the origin. In the original space, this will correspond to finding a small region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly. There are a few hyperparameters to tweak: the usual ones for a kernelized SVM, plus a margin hyperparameter that corresponds to the probability of a new instance being mistakenly considered as novel when it is in fact normal. It works great, especially with high-dimensional datasets, but like all SVMs it does not scale to large <span class="keep-together">datasets</span>.</p>
</dd>
<dt>PCA and other dimensionality reduction techniques with an <code translate="no">inverse_transform()</code> method</dt>
<dd>
<p>I<a data-type="indexterm" data-primary="inverse_transform()" id="id2091"/>f you compare the reconstruction error of a normal instance with the reconstruction error of an anomaly, the latter will usually be much larger. This is a simple and often quite efficient anomaly detection approach (see this chapter‚Äôs exercises for an example).<a data-type="indexterm" data-startref="xi_unsupervisedlearning835_1" id="id2092"/></p>
</dd>
</dl>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="exercises-ch9">
<h1>Exercises</h1>
<ol>
<li>
<p>How would you define clustering? Can you name a few clustering algorithms?</p>
</li>
<li>
<p>What are some of the main applications of clustering algorithms?</p>
</li>
<li>
<p>Describe two techniques to select the right number of clusters when using <span class="keep-together"><em>k</em>-means</span>.</p>
</li>
<li>
<p>What is label propagation? Why would you implement it, and how?</p>
</li>
<li>
<p>Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?</p>
</li>
<li>
<p>Can you think of a use case where active learning would be useful? How would you implement it?</p>
</li>
<li>
<p>What is the difference between anomaly detection and novelty detection?</p>
</li>
<li>
<p>What is a Gaussian mixture? What tasks can you use it for?</p>
</li>
<li>
<p>Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?</p>
</li>
<li>
<p>The classic Olivetti faces dataset contains 400 grayscale 64 √ó 64‚Äìpixel images of faces. Each image is flattened to a 1D vector of size 4,096. Forty different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the <code translate="no">sklearn.datasets.fetch_olivetti_faces()</code> function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you will probably want to use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using <em>k</em>-means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?</p>
</li>
<li>
<p>Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. Next, use <em>k</em>-means as a dimensionality reduction tool, and train a classifier on the reduced set. Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?</p>
</li>
<li>
<p>Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset‚Äôs dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the <code translate="no">sample()</code> method), and visualize them (if you used PCA, you will need to use its <code translate="no">inverse_transform()</code> method). Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the <code translate="no">score_samples()</code> method for normal images and for anomalies).</p>
</li>
<li>
<p>Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise and look at their reconstruction error: notice how much larger it is. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face.</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter‚Äôs notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id1959"><sup><a href="ch08.html#id1959-marker">1</a></sup> If you are not familiar with probability theory, I highly recommend the free online classes by Khan Academy.</p><p data-type="footnote" id="id1973"><sup><a href="ch08.html#id1973-marker">2</a></sup> Stuart P. Lloyd, ‚ÄúLeast Squares Quantization in PCM‚Äù, <em>IEEE Transactions on Information Theory</em> 28, no. 2 (1982): 129‚Äì137.</p><p data-type="footnote" id="id1983"><sup><a href="ch08.html#id1983-marker">3</a></sup> David Arthur and Sergei Vassilvitskii, ‚Äú<span>k-Means++</span>: The Advantages of Careful Seeding‚Äù, <em>Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em> (2007): 1027‚Äì1035.</p><p data-type="footnote" id="id1989"><sup><a href="ch08.html#id1989-marker">4</a></sup> Charles Elkan, ‚ÄúUsing the Triangle Inequality to Accelerate k-Means‚Äù, <em>Proceedings of the 20th International Conference on Machine Learning</em> (2003): 147‚Äì153.</p><p data-type="footnote" id="id1990"><sup><a href="ch08.html#id1990-marker">5</a></sup> The triangle inequality is AC ‚â§ AB + BC, where A, B, and C are three points and AB, AC, and BC are the distances between these points.</p><p data-type="footnote" id="id1993"><sup><a href="ch08.html#id1993-marker">6</a></sup> David Sculley, ‚ÄúWeb-Scale K-Means Clustering‚Äù, <em>Proceedings of the 19th International Conference on World Wide Web</em> (2010): 1177‚Äì1178.</p><p data-type="footnote" id="id2046"><sup><a href="ch08.html#id2046-marker">7</a></sup> In contrast, as we saw earlier, <em>k</em>-means implicitly assumes that clusters all have a similar size and density, and are all roughly round.</p><p data-type="footnote" id="id2050"><sup><a href="ch08.html#id2050-marker">8</a></sup> Phi (<em>œï</em> or <em>œÜ</em>) is the 21st letter of the Greek alphabet.</p></div></div></section></div></div></body></html>