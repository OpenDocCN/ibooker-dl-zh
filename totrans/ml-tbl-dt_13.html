<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">11 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/><a id="idTextAnchor007"/>Building a machine learning pipeline</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-397"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">An overview of machine learning pipelines</li>

    <li class="co-summary-bullet">Prerequisites for running a machine learning pipeline in Vertex AI</li>

    <li class="co-summary-bullet">Model training and deployment: local implementation vs. machine learning pipeline implementation</li>

    <li class="co-summary-bullet">Defining a machine learning pipeline to train and deploy a model</li>

    <li class="co-summary-bullet">Updating the model training code to work with a machine learning pipeline</li>

    <li class="co-summary-bullet">Using generative AI to help create the machine learning pipeline</li>
  </ul>

  <p class="body">In chapter 10, we went through the steps to deploy a deep learning model trained on tabular data. We deployed the model in a web application, first with the model running entirely on our local system and then having the model deployed to a Vertex AI endpoint. In this chapter, we will go through the further steps to automate the training and deployment process by using a machine learning (ML) pipeline in Vertex AI. We will start by going over the setup steps necessary for a ML pipeline, including defining a Vertex AI dataset. Next, we will contrast the local model training and deployment we have seen from chapter 10 with model training and deployment using an ML pipeline. We will proceed to review the code specifically for the ML pipeline itself, along with the updates to the existing code required for the model training code to work in the context of an ML pipeline. Finally, we will examine some of the ways that we can apply generative AI and get useful help from its outputs in the workflow for creating a ML pipeline. The code described in this chapter is available at<a id="idTextAnchor008"/> <a class="url" href="https://mng.bz/DM4n">https://mng.bz/DM4n</a>.<a id="idTextAnchor009"/></p>

  <h2 class="fm-head" id="heading_id_3">11.1 Introduction to ML pipelines</h2>

  <p class="body">Consider the steps that we have covered so far in this book to prepare a deep learning model trained on tabular data:<a id="idIndexMarker001"/><a id="marker-398"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Process the data to deal with problems such as missing values, columns containing two distinct kinds of data, and numeric data expressed as strings</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Train the model using the processed data</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Deploy the trained model so that it can be used by an application</p>
    </li>
  </ul>

  <p class="body">Suppose we needed to go through this process repeatedly for the Kuala Lumpur real estate problem. This is a reasonable expectation because the real estate market will keep changing as prices develop, interest rates change, and macroeconomic factors affect the demand for real estate. Rather than running the various notebooks and deployment steps manually for each end-to-end cycle from raw data to deployed model, it would be better to have a coded solution that we could run as a unit repeatedly and consistently. An ML pipeline gives us this exactly, and in this section, we will go through an example illustrating how to set up a simple, end-to-end pipeline for the Kuala Lumpur real estate problem<a id="idTextAnchor010"/>.</p>

  <h3 class="fm-head1" id="heading_id_4">11.1.1 Three kinds of pipelines</h3>

  <p class="body">Before getting into the details of an ML pipeline, it is worth noting that the term <i class="fm-italics">pipeline</i> has been overloaded with different meanings over time. At the moment, there are at least three distinct meanings for the term <i class="fm-italics">pipeline</i> that are predominant in the world of ML/data science:<a id="idIndexMarker002"/><a id="idIndexMarker003"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Training/inference pipe<a id="idTextAnchor011"/>line</i>—This pipeline ensures that data transformations, such as assigning text to tokens or assigning values in a categorical column to numeric identifiers, are done consistently in the training and inference steps. The preprocessing Keras layers in the Kuala Lumpur model constitute this kind of pipeline because they ensure, for example, that the transformations done on the processed data prior to training exactly match the transformations done on the data points entered in <code class="fm-code-in-text">home.html</code> in the web deployment.<a id="idIndexMarker004"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Data pipeline</i>—This pipeline deals with anomalies in the input training data, such as missing values or schema problems. It can overlap the pipeline described in the previous point, but it performs a distinct task. In the context of Google Cloud, Dataflow and Cloud Data Fusion are examples of products that can perform data pipeline tasks. You don’t need to know about Dataflow or Cloud Data Fusion for the purposes of this chapter, but if you are curious, you can check out the documentation: <a class="url" href="https://cloud.google.com/dataflow/docs">https://cloud.google.com/dataflow/docs</a> and <a class="url" href="https://cloud.google.com/data-fusion/docs">https://cloud.google.com/data-fusion/docs</a>.<a id="idIndexMarker005"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">ML pipeline</i>—This is a pipeline that automates various steps such as training, deploying, and monitoring the model. TFX and KubeFlow are the two approaches that are available in Vertex AI for implementing ML pipelines.<a id="idIndexMarker006"/></p>
    </li>
  </ul>

  <p class="body"><a id="marker-399"/>Figure 11.1 shows how each of these three kinds of pipelines fits into the end-to-end ML workfl<a id="idTextAnchor012"/>ow.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.1 Three kinds of pipelines and how they relate</p>
  </div>

  <p class="body">Figure 11.1 illustrates the following characteristics of the pipelines:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The ML pipeline can encompass the entire workflow, from raw data to monitoring the deployed model. The rationale for this is that the ML pipeline is intended to automate the complete process when the model needs to be retrained and redeployed.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The distinction between a data pipeline and a training/inference pipeline is that the training/inference pipeline handles transformations that need to be applied to new data points to which we want to apply the trained model to get predictions, such as replacing categorical values with numeric identifiers. The same transformations need to be applied to the prepared data prior to training the model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">As we saw in the Keras custom layers solution to the Airbnb NYC price prediction problem in chapter 3, the training/inference pipeline can be distinct from the model training process. In the Keras customer layers solution, the training/inference pipeline was implemented using Scikit-learn pipeline structures and custom classes, both of which need to be applied to data prior to model training and prior to applying new data points to the trained model to get predictions. In chapter 9, on the other hand, we saw how the same processing could be incorporated directly into the Keras model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Data pipelines can exist outside the context of the ML workflow. The same data pipeline tools, such as Dataflow and Cloud Data Fusion, that can be used in ML workflows in Google Cloud can be part of applications that don’t include ML.</p>
    </li>
  </ul>

  <p class="body">Now that we have described three different kinds of pipelines, in the next section, we will start to explore how to create an ML pipeline for the Kuala Lumpur real estate price prediction problem in Google Cloud using Kube<a id="idTextAnchor013"/>flow.</p>

  <h3 class="fm-head1" id="heading_id_5">11.1.2 Overview of Vertex AI ML pipelines</h3>

  <p class="body">In chapter 10, we went through the process of deploying the Kuala Lumpur real estate price prediction model to a Vertex AI endpoint. <a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="marker-400"/></p>

  <p class="body">To create an ML pipeline for the Kuala Lumpur price prediction model, we are going to start with the steps described in the Vertex AI docume<a id="idTextAnchor014"/>ntation: <a class="url" href="https://mng.bz/lYW6">https://mng.bz/lYW6</a>.</p>

  <p class="body">The following is an overview of the steps:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Set up a <i class="fm-italics">service account</i>. A service account is an account used by an application to take actions in Google Cloud. When we imported the Keras model into Google Cloud and deployed it to an endpoint, we used our own ID to perform these actions. Since the ML pipeline will be an automated script, we need a service account to allow the script to perform actions without depending directly on manual intervention from any individual. See the Google Cloud documentation for more details on service accounts: <a class="url" href="https://mng.bz/BXA0">https://mng.bz/BXA0</a>.<a id="idIndexMarker010"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Get a service account key for the service account and provide the service account with the required access to run the ML pipeline.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Create a pipeline script to invoke the Vertex AI SDK.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Adapt the model training notebook to be a standalone Python script that can be run in a prebuilt Vertex AI container.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Run the pipeline script to run the training script inside a container and generate a trained model.</p>
    </li>
  </ul>

  <p class="body">In the subsequent sections, we will go through these steps to create an ML pipeline for the Kuala Lumpur real estate predicti<a id="idTextAnchor015"/>on model.<a id="idIndexMarker011"/></p>

  <h2 class="fm-head" id="heading_id_6">11.2 ML pipeline preparation steps</h2>

  <p class="body">Before we can run the ML pipeline to train and deploy a model, we need to set up the Google Cloud objects the pipeline needs. In this section, we will set up a service account and introduce the Cloud Shell, an instance that is available directly in Google Cloud that we can use to enter commands. We will also upload our dataset to Google Cloud Storage and use the uploaded dataset to create a Vertex A<a id="idTextAnchor016"/>I dataset.<a id="idIndexMarker012"/><a id="marker-401"/></p>

  <h3 class="fm-head1" id="heading_id_7">11.2.1 Creating a service account for the ML pipeline</h3>

  <p class="body">Since we want to be able to run the ML pipeline automatically without manual intervention, we need to set up a service account to run the pipeline.<a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>

  <p class="body">To create a service account, follow these steps:</p>

  <p class="body-dialog">   1.  Select IAM &amp; Admin -&gt; Service Accounts from the overall Google Cloud Console menu, as shown in fi<a id="idTextAnchor017"/>gure 11.2.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F02_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.2 Selecting Service Accounts in the Google Cloud Console</p>
  </div>

  <p class="body-dialog">   2.  In the Service Accounts page, select Create Service Account, as shown in fig<a id="idTextAnchor018"/>ure 11.3.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F03_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.3 Creating a service account</p>
  </div>

  <p class="body-dialog">   3.  In the Create service account page, enter a name for the service account and click Create and Continue, as shown in figure 11.4. Note that the service account ID gets filled in automatically and that an email ID for the service account is shown in the form <code class="fm-code-in-text">service-account-id@project-id.iam.gserviceaccount.com</code>—in this case: <code class="fm-code-in-text">ml-tabular-pipeline@first-project-ml-tabular.iam.gserviceacc<a class="calibre" id="idTextAnchor019"/>ount.com</code> </p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F04_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.4 Setting a service account name</p>
  </div>

  <p class="body-dialog">   4.  Select Vertex AI User in the Role field and click Done, as shown in figure 11.5.<a id="marker-402"/><a id="idTextAnchor020"/></p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F05_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.5 Giving the service account Vertex AI user role</p>
  </div>

  <p class="body">Now that we have created a service account and given it access to Vertex AI, in the next section we can create a service acc<a id="idTextAnchor021"/>ount key.</p>

  <h3 class="fm-head1" id="heading_id_8">11.2.2 Creating a service account key</h3>

  <p class="body">The ML pipeline uses a service account key to authenticate the service account used to run the ML pipeline. <a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="marker-403"/></p>

  <p class="body">To create a service account key, follow these steps:</p>

  <p class="body-dialog">   1.  In the Service accounts page, click on the email address for the service account that you just created, as shown in figu<a id="idTextAnchor022"/>re 11.6.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F06_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.6 Selecting the service account</p>
  </div>

  <p class="body-dialog">   2.  Select the Keys tab and click Add key -&gt; Create new key, as shown in figur<a id="idTextAnchor023"/>e 11.7.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F07_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.7 Creating a service account key</p>
  </div>

  <p class="body-dialog">   3.  Select JSON and click Create, as shown in figure<a id="idTextAnchor024"/> 11.8.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F08_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.8 Downloading the service account key</p>
  </div>

  <p class="body">A JSON file containing the service account key is created and downloaded to your local system with a name that looks like: <code class="fm-code-in-text">first-project-ml-tabular-039ff1f820<a class="calibre" id="idTextAnchor025"/>a8.json</code>.</p>

  <h3 class="fm-head1" id="heading_id_9">11.2.3 Granting the service account access to the Compute Engine default service account</h3>

  <p class="body"><a id="marker-404"/>When you set up your project in Google Cloud, a Compute Engine default service account was created. This account has an email address like <code class="fm-code-in-text">PROJECT_NUMBER-compute@developer.gserviceaccount.com</code>. For more details on the Compute Engine default service account, see the docume<a id="idTextAnchor026"/>ntation (<a class="url" href="https://mng.bz/dXdN">https://mng.bz/dXdN</a>).<a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>

  <p class="body">We need to give the service account that we set up in the preceding sections access to the Compute Engine default service account to run the ML pipeline. Follow these steps to set up this access to the Compute Engine default service account:</p>

  <p class="body-dialog">   1.  In the Service accounts page, click the copy icon beside the email address for the service account you just created (you will need this in the next step) and then click the email address of the Compute Engine default service account, as shown in figur<a id="idTextAnchor027"/>e 11.9.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F09_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.9 Compute Engine default service account</p>
  </div>

  <p class="body-dialog">   2.  Click the Permissions tab and click Grant Access, as shown in figure <a id="marker-405"/><a id="idTextAnchor028"/>11.10.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F10_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.10 Granting access to the Compute Engine default service account</p>
  </div>

  <p class="body-dialog">   3.  In the Grant access page, paste the email ID of the service account that you created in the New Principals field, select Service Account User in the Role field, and click Save, as shown in figure 1<a id="idTextAnchor029"/>1.11.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F11_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.11 Specifying access to the Compute Engine default service account</p>
  </div>

  <p class="body">Now that we have completed the steps to set up the service account for the ML pipeline, we can continue with the setup of the pi<a id="idTextAnchor030"/>peline.<a id="marker-406"/></p>

  <h3 class="fm-head1" id="heading_id_10">11.2.4 Introduction to Cloud Shell</h3>

  <p class="body">So far, all the actions that we have taken in Google Cloud have been in the Console UI. Google Cloud also includes the Cloud Shell, which is a self-contained instance that lets you run command line commands to interact with Google Cloud. In addition to the command line interface, you can use the Cloud Shell Editor to edit files in the Cloud Shell filesystem. With Cloud Shell, you get the function of a local Linux instance combined with the convenience of a web-based environment that is integrated with Google Cloud resources. Cloud Shell is particularly well-suited for prototyping and working through tutorials. We will use the Cloud Shell in the next few steps of setting up the ML pipeline. For additional details about Cloud Shell, see the documentation: <a class="url" href="https://cloud.google.com/shell">https://cloud.google.com/shell</a>.<a id="idIndexMarker019"/></p>

  <p class="body">To start the Cloud Shell, click on the Activate Cloud Shell icon at the top of the Google Cloud Console, as shown in figure 1<a id="idTextAnchor031"/>1.12.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F12_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.12 Activating Cloud Shell icon</p>
  </div>

  <p class="body">When you click on the Activate Cloud Shell icon, the Cloud Shell terminal opens at the bottom of the console with your home directory as the current directory, as shown in figure 11.13.<a id="marker-407"/><a id="idTextAnchor032"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F13_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.13 Cloud Console with Cloud Shell activated</p>
  </div>

  <p class="body">You can run commands directly in the Cloud Shell Terminal, including standard Linux commands and Google Cloud-specific commands. You can click Open Editor to edit files in the Cloud Shell file system, as shown in figure 11.14. To get back to the Cloud Shell Terminal, click on Open Termina<a id="idTextAnchor033"/>l.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F14_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.14 Cloud Shell Editor</p>
  </div>

  <p class="body">Now that we have taken a brief tour of the Cloud Shell, we can continue with the next step of setting up the ML pipeline: making the service account key available to the pipe<a id="idTextAnchor034"/>line.</p>

  <h3 class="fm-head1" id="heading_id_11">11.2.5 Uploading the service account key</h3>

  <p class="body">In this section, we will use the Cloud Shell to upload the service account key JSON file and then set an environment variable to point to the location of the service account key:<a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="marker-408"/></p>

  <p class="body-dialog">   1.  In Cloud Shell, set your home directory as the current directory, create a new directory called <code class="fm-code-in-text">ml_pipeline</code>, and then set that new directory as your current directory:</p>
  <pre class="programlistinge">cd ~  
mkdir ml_pipeline
cd ml_pipeline</pre>

  <p class="body-dialog">   2.  To upload the service account key, select the three dots in the Cloud Shell toolbar and select Upload, as shown in figure 11.15.<a id="idTextAnchor035"/></p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F15_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.15 Uploading a file in Cloud Shell</p>
  </div>

  <p class="body-dialog">   3.  In the Upload page, update Destination Directory to be the <code class="fm-code-in-text">ml_pipeline</code> directory in your home directory, click Choose Files, and select the service account key JSON file that you downloaded in section 11.2.2 and click Upload, as shown in figure 11.1<a id="idTextAnchor036"/>6.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F16_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.16 Setting upload parameters</p>
  </div>

  <p class="body-dialog">   4.  Validate the upload by making <code class="fm-code-in-text">~/ml_pipeline</code> your current directory and using the <code class="fm-code-in-text">ls</code> command to ensure that the JSON service account key is now in this directory:<a id="marker-409"/></p>
  <pre class="programlistinge">cd ~/ml_pipeline  
ls</pre>

  <p class="body-dialog">   5.  Set the environment variable <code class="fm-code-in-text">GOOGLE_APPLICATION_CREDENTIALS</code> to the fully qualified filename of the service account key JSON file. In the following example, replace the fully qualified filename with that for your own service account key JSON file:</p>
  <pre class="programlistinge">export \
GOOGLE_APPLICATION_CREDENTIALS=\
'/home/ryanmark2023/ml_pipeline/\
first-project-ml-tabular-039ff1f820a8.json'</pre>

  <p class="body-dialog">   6.  Confirm the value of the <code class="fm-code-in-text">GOOGLE_APPLICATION_CREDENTIALS</code> environment variable with the following command and validate that it is set to the fully qualified path of your service account key file:</p>
  <pre class="programlistinge">$ echo $GOOGLE_APPLICATION_CREDENTIALS</pre>

  <p class="body">Now that we have uploaded the service account key and set the environment variable to point to the location of the service account key, we are ready to get into the key step of defining the ML pipe<a id="idTextAnchor037"/>line.</p>

  <h3 class="fm-head1" id="heading_id_12">11.2.6 Uploading the cleaned-up dataset to a Google Cloud Storage bucket</h3>

  <p class="body">To simplify the pipeline, we will upload the processed dataset generated by the data preparation not<a id="idTextAnchor038"/>ebook (<a class="url" href="https://mng.bz/rKjB">https://mng.bz/rKjB</a>) to a Cloud Storage bucket so that it is accessible to the rest of the ML pipeline. In a real-world application, we would incorporate the data cleanup steps into the ML pipeline, but for the sake of simplicity, we will start the pipeline with the data already cleaned up. Follow the steps in this section to upload the cleaned-up dataset to Google Cloud Storage:<a id="marker-410"/><a id="idIndexMarker022"/></p>

  <p class="body-dialog">   1.  Upload the CSV version of the cleaned-up dataset to the same bucket that you created to upload the model.</p>

  <p class="body-dialog">   2.  From the Google Cloud Console main menu, select Cloud Storage -&gt; Buckets, as shown in figure 11.<a id="idTextAnchor039"/>17.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F17_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.17 Setting upload parameters</p>
  </div>

  <p class="body-dialog">   3.  In the Buckets page, select the bucket you created in chapter 10 to contain the trained model. In the Bucket details page, select Create Folder, as shown in figure 11.1<a id="idTextAnchor040"/>8.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F18_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.18 Creating a folder</p>
  </div>

  <p class="body-dialog">   4.  Enter <code class="fm-code-in-text">processed_dataset</code> in the name field and click Create.</p>

  <p class="body-dialog">   5.  Select the new folder that you just created, as shown in figure 11.19<a id="idTextAnchor041"/>.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F19_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.19 Selecting the folder</p>
  </div>

  <p class="body-dialog">   6.  Click Upload Files and select the CSV file containing the processed version of the Kuala Lumpur dataset (output of the data preparation notebook).<a id="marker-411"/></p>

  <p class="body-dialog">   7.  You will see the file in the Bucket details page when the upload is complete. Click the three dots, then Copy gsutil URI, as shown in figure 11.20.<a id="idTextAnchor042"/></p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F20_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.20 Copying gsutil URI</p>
  </div>

  <p class="body">The gsutil Uniform Resource Identifer (URI) value will look like this: <code class="fm-code-in-text">gs://first-project-ml-tabular-bucket/processed_dataset/kl_real_estate_output.csv</code> </p>

  <p class="body">Now that we have uploaded the cleaned-up dataset to a Google Cloud Storage bucket, we can use it to create a Vertex AI datase<a id="idTextAnchor043"/>t.</p>

  <h3 class="fm-head1" id="heading_id_13">11.2.7 Creating a Vertex AI managed dataset</h3>

  <p class="body">The ML pipeline invokes the Vertex AI SDK to train the model; it identifies the dataset used to train the model as a Vertex AI managed dataset. To learn more about Vertex AI managed datasets, see the documentatio<a id="idTextAnchor044"/>n: <a class="url" href="https://mng.bz/VVRP">https://mng.bz/VVRP</a>. <a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="marker-412"/></p>

  <p class="body">The Vertex AI SDK automatically does the following to make the managed dataset available to the training script:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Copies the content of the dataset to Cloud Storage.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Divides the dataset into training, validation, and testing subsets. The proportion of the dataset for each subset is set in the pipeline config file <code class="fm-code-in-text">pipeline_config.yml</code>, as shown in figure 11.2<a id="idTextAnchor045"/>.</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F21_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.21 Proportions for train, validation, and test in the pipeline configuration</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Divides each of the subsets into multiple CSV files. Figure 11.22 shows an example of what the CSV files for the dataset look like in Cloud Storage.<a id="idTextAnchor046"/></p>
    </li>
  </ul>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F22_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.22 Processed dataset in Google Cloud Storage</p>
  </div>

  <p class="body">Now that we have seen how the dataset gets set up in Cloud Storage, let’s go through the steps to create a Vertex AI dataset for the training data.</p>

  <p class="body-dialog">   1.  In Vertex AI, select Datasets. In the Datasets page, click Create as shown in figure 11.23.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F23_Ryan2.png"/><a id="idTextAnchor047"/><a id="marker-413"/></p>

    <p class="figurecaptione">Figure 11.23 Creating a dataset</p>
  </div>

  <p class="body-dialog">   2.  In the Create dataset page, set <code class="fm-code-in-text">kuala-lumpur-real-estate</code> as the dataset name, select the Tabular tab, select Regression/Classification, and click Create, as shown in figure 11.24.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F24_Ryan2.png"/><a id="idTextAnchor048"/></p>

    <p class="figurecaptione">Figure 11.24 Specifying dataset details</p>
  </div>

  <p class="body-dialog">   3.  In the Source tab, select Select CSV file from Cloud Storage. In Import file path, click Browse, select the Cloud Storage bucket location where you uploaded the processed training file in the previous section, and click Continue, as shown in figure 11.25.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F25_Ryan2.png"/></p>

    <p class="figurecaptione"><a id="idTextAnchor049"/>Figure 11.25 Specifying the source for the dataset</p>
  </div>

  <p class="body-dialog">   4.  Note the ID value of the dataset that you just created, as shown in figure 11.26.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F26_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.26 Dataset ID in Google Cloud Console<a id="marker-414"/><a id="idTextAnchor050"/></p>
  </div>

  <p class="body">This is the value that needs to be set for <code class="fm-code-in-text">dataset_id</code> in the pipeline config file <code class="fm-code-in-text">pipeline_config.yml</code>, as shown in figure 11.27.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F27_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.27 <code class="fm-code-in-text">dataset_id</code> in the pipeline config file<a id="idTextAnchor051"/></p>
  </div>

  <p class="body">Congratulations! You have set up a Vertex AI managed dataset for the dataset that the model training portion of the ML pipeline will use to train the model.<a id="idIndexMarker025"/></p>

  <h2 class="fm-head" id="heading_id_14">11.3 D<a id="idTextAnchor052"/>efining the ML pipeline</h2>

  <p class="body"><a id="marker-415"/>So far in this chapter, we have completed the following preparation steps for the ML pipeline:<a id="idIndexMarker026"/></p>

  <p class="body-dialog">   1.  Created a service account and a service account key</p>

  <p class="body-dialog">   2.  Uploaded the service account key to the directory where we will run the pipeline script</p>

  <p class="body-dialog">   3.  Uploaded the cleaned-up dataset to Cloud Storage</p>

  <p class="body-dialog">   4.  Created a Vertex AI-managed dataset from the cleaned-up dataset</p>

  <p class="body">In this section, we will take the elements we prepared in the preceding section and use them to create an ML pipeline that takes in a preprocessed dataset at one end and produces a trained model deployed with a Vertex AI endpoint at the other end.</p>

  <h3 class="fm-head1" id="heading_id_15">11.3.1 <a id="idTextAnchor053"/>Local implementation vs. ML pipeline</h3>

  <p class="body">Before we continue with defining the ML pipeline, let’s contrast the ML pipeline with the local setup to train the Kuala Lumpur real estate price prediction model that we implemented in chapter 10. Figure 11.28 shows this contrast and highlights some of the differences between the two implementations.<a id="idIndexMarker027"/><a id="idIndexMarker028"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F28_Ryan2.png"/><a id="idTextAnchor054"/></p>

    <p class="figurecaption">Figure 11.28 Training on a local system vs. training with an ML pipeline</p>
  </div>

  <p class="body"><a id="marker-416"/>Figure 11.28 contrasts the structure of the training process for an entirely local implementation compared to the training process using an ML pipeline. The key ways that the ML pipeline implementation differs from the local system implementation of the solution are</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The data cleanup process is identical. In a real-world production pipeline, we would move this data processing step into the Vertex AI environment and make it part of the ML pipeline, but to make the ML pipeline as simple as possible, we skip that step for our ML pipeline implementation and start the pipeline with the cleaned-up dataset.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">In the local implementation, the data cleanup process output is a pickle file. To avoid compatibility problems, we switched to a CSV file for the ML pipeline. The ML pipeline takes the contents of this CSV file and splits them into train, validation, and test subsets, each of which is segmented into multiple CSV files in Cloud Storage.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The training code in the ML pipeline implementation is in a Python <code class="fm-code-in-text">.py</code> file (the <i class="fm-italics">model training script</i>) rather than a notebook. Significant updates to the training code to make it work in a container environment are described in the following section.<a id="idIndexMarker029"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">In the ML pipeline implementation, the model training config file is in Cloud Storage so that its location can be shared by the pipeline script as a parameter for the model training script.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The pipeline script is a new component in the ML pipeline. This script sets up the input necessary for the model training script, uses the Vertex AI SDK to create a container for the model training script, and invokes the script to do the model training.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The pipeline config file is a new component in the ML pipeline. This config file contains parameters for the pipeline script, including the built-in Vertex AI containers to use for the ML pipeline; the proportion of the cleaned-up dataset for each of the training, validation, and testing subsets; the dataset ID; and the location of the code for the training script.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The trained model is automatically put in the model registry in the ML pipeline implementation and deployed to a Vertex AI endpoint. In the local system implementation, we manually uploaded the model to Cloud Storage and then deployed it to an endpoint.</p>
    </li>
  </ul>

  <p class="body"><a id="marker-417"/>The endpoint that is the result of both the local system implementation and the ML pipeline implementation can be plugged into our web deployment simply by updating the <code class="fm-code-in-text">endpoint_id</code> parameter in the Flask server config file, as shown in figure 11.29.<a id="idIndexMarker030"/></p>

  <div class="figure">
    <p class="figure1"><a id="idTextAnchor055"/><img alt="" class="calibre4" src="../Images/CH11_F29_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.29 Web deployment with endpoint from local or ML pipeline</p>
  </div>

  <p class="body">For more details on the workflow for training a custom model on Vertex AI, see the documentation<a id="idTextAnchor056"/>: <a class="url" href="https://mng.bz/xKjW">https://mng.bz/xKjW</a><a id="idTextAnchor057"/>.</p>

  <h3 class="fm-head1" id="heading_id_16">11.3.2 Introduction to containers</h3>

  <p class="body">One key point of an ML pipeline in Vertex AI is using containers to make the model training process easy to automate and flexible. In this section, we will briefly introduce containers and their benefits to the ML pipeline. If you are already familiar with the concept of containers and Docker, you can skip this section.<a id="idIndexMarker031"/><a id="idIndexMarker032"/></p>

  <p class="body">A container is a software construct that allows you to package an application with its dependencies so that you can run the application predictably and efficiently across a range of environments. Google Cloud uses Docker containers. A detailed description of containers is beyond the scope of this book, but we need to spend some time looking at them to understand why they are used for ML pipelines and what constraints they place on our code. For more details on containers, see the Docker si<a id="idTextAnchor058"/>te: <a class="url" href="https://www.docker.com/resources/what-container/">https://www.docker.com/resources/what-containe<span id="idTextAnchor059">r/</span></a>.</p>

  <h3 class="fm-head1" id="heading_id_17">11.3.3 Benefits of using containers in an ML pipeline</h3>

  <p class="body"><a id="marker-418"/>Using containers to package the training code means that we don’t have to worry about the Python libraries that are required for the training because the container comes with all the required Python libraries already set up. Also, the code is easy to reproduce anywhere. Vertex AI provides a range of prebuilt container images for the most popular machine learning frameworks, including PyTorch, TensorFlow, and XGBoost. We use TensorFlow prebuilt containers for our ML pipeline. See the Vertex AI documentation for details on prebuilt containers:<a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Prebuilt containers for training custom <a id="idTextAnchor060"/>models</i>—<a class="url" href="https://mng.bz/AQ8z">https://mng.bz/AQ8z</a></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Prebuilt containers for pred<a id="idTextAnchor061"/>iction</i>—<a class="url" href="https://mng.bz/ZlRP">https://mng.bz/ZlRP</a></p>
    </li>
  </ul>

  <p class="body">If our training ends up becoming more demanding (either in terms of how quickly a training cycle needs to be completed or the compute resources needed to complete a training cycle of a given duration), we can take advantage of the containerized nature of the training to distribute training across multiple compute engines. For a simple problem like the Kuala Lumpur real estate price prediction problem, a single node is more than sufficient to do the training, but bigger applications can really benefit from distributed training. A detailed explanation of all the options that are available for distributed training with Vertex AI is beyond the scope of this book. Check out the documentation if you are interested in more det<a id="idTextAnchor062"/>ails: <a class="url" href="https://mng.bz/RVmK">https://mng.bz/<span id="idTextAnchor063">RVmK</span></a>.</p>

  <h3 class="fm-head1" id="heading_id_18">11.3.4 Introduction to adapting code to run in a container</h3>

  <p class="body">Now that we have reviewed some of the benefits of using containers for the training process, we can look at changes that are required to run the training code in a container. To understand the difference between running code in a nonvirtualized environment and in a container, it helps to think of the container as its own self-contained machine where the code runs. In particular, code running in a container will not, by default, have access to the file system of the environment from which the container is managed. Figure 11.30 shows how the model training notebook interacts with files in the filesyste<a id="idTextAnchor064"/>m.<a id="idIndexMarker035"/><a id="idIndexMarker036"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F30_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.30 Training code interactions with external files</p>
  </div>

  <p class="body"><a id="marker-419"/>When the training code runs in a container, it can’t get access to files on an external local filesystem. Instead, the artifacts that the model training script uses are stored in Cloud Storage, and the locations for these artifacts in Cloud Storage are passed to the model training script as URIs. Figure 11.31 gives an example of how to interpret a Google Cloud Storage U<a id="idTextAnchor065"/>RI.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F31_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.31 Interpreting a Google Cloud Storage URI</p>
  </div>

  <p class="body">In the ML pipeline, we use two methods to pass URIs to the training script running in a container: via the environment variables set in the container by the Vertex AI SDK and via the argument list of the <code class="fm-code-in-text">job.run</code> call in the pipeline script, as shown in figure 11.3<a id="idTextAnchor066"/>2.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F32_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.32 Training code interactions with content in Cloud Storage</p>
  </div>

  <p class="body">The location of the training data (split into training, validation, and test subsets) is automatically assigned to environment variables that get set in the container when it is set up by the pipeline script. This is standard for all Vertex AI containers; see the documentation at <a class="url" href="https://mng.bz/2y70">https://mng.bz/2y70</a>.</p>

  <p class="body">The way that the URI for the config file is passed to the model training script is not the default for Vertex AI. If we had a training script that had a small number of arguments, we could create an <code class="fm-code-in-text">argparser</code> list that contains the argument values and pass that list to the model training script. The config file for our application is too complex for this to be efficient, so instead of passing each argument individually, we pass a single argument: the URI for the Cloud Storage location where we have saved a copy of the config file. With that, all the model training script needs to do is get the Cloud Storage location from the argument list and ingest the YAML file from there. Once the arguments have been pulled into the config dictionary in the model training script, the rest of the code that uses them can work unchanged. This is a major bene<a id="idTextAnchor067"/>fit.<a id="idIndexMarker037"/></p>

  <h3 class="fm-head1" id="heading_id_19">11.3.5 Updating the training code to work in a container</h3>

  <p class="body"><a id="marker-420"/>In this section, we will review how we changed the model training notebook (<a class="url" href="https://mng.bz/1XJj">https://mng.bz/1XJj</a>) that we ran in Colab in chapter 9 to get a model to predict Kuala Lumpur property prices. By making these changes, we convert the model training notebook into a training script that can run in a Vertex AI built-in container.<a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>

  <p class="body">The following are the key changes we made to the training notebook to create the training script:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Removed extraneous library imports and associated code. For example, we don’t need to generate a diagram of the model when we run the training script, so we removed the code associated with <code class="fm-code-in-text">plot_model</code>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Removed code that splits the dataset into training, validation, and testing subsets. In the ML pipeline, the Vertex AI SDK takes care of splitting the dataset prior to the testing script being started.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Added code to interpret the <code class="fm-code-in-text">job.run</code> argument list, as shown in the following l<a id="idTextAnchor068"/>isting.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.1 Loading the saved Keras model</p>
  <pre class="programlistinge">parser = argparse.ArgumentParser()                           <span class="fm-combinumeral">①</span>
parser.add_argument(
        '--config_bucket',                                   <span class="fm-combinumeral">②</span>
        help='Config details',
        required=True
    )
args = parser.parse_args().__dict__                          <span class="fm-combinumeral">③</span>
config_bucket = args['config_bucket']                        <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Defines an argparser object for the arguments passed by the Vertex AI SDK</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Adds the config_bucket argument to the argparser object</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Ingests the arguments passed by the Vertex AI SDK as a dictionary</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">④</span> Gets the config file URI from the argument dictionary</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Updated the code that ingests the training config file so that it ingests the contents of the config file from the Cloud Storage URI passed by the pipeline script (<code class="fm-code-in-text">config_bucket</code> from listing 11.1) rather than from the local file system. As shown in the following listing, the URI for the config file in Cloud Storage (<code class="fm-code-in-text">config_bucket</code>) is used to copy the config file from Cloud Storage to a file in the container, and then the contents of that file are copied into the dictionary <a id="idTextAnchor069"/><code class="fm-code-in-text">config</code>.<a id="marker-421"/></p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.2 Ingesting the training config file via the URI argument</p>
  <pre class="programlistinge">bucket_name = config_bucket.split("/")[2]                    <span class="fm-combinumeral">①</span>
object_name = "/".join(config_bucket.split("/")[3:])         <span class="fm-combinumeral">②</span>
storage_client2 = storage.Client()                           <span class="fm-combinumeral">③</span>
bucket = storage_client2.bucket(bucket_name)                 <span class="fm-combinumeral">④</span>
blob_out = bucket.blob(object_name)                          <span class="fm-combinumeral">⑤</span>
destination_file_name = 'config.yml'                         <span class="fm-combinumeral">⑥</span>
blob_out.\
download_to_filename(destination_file_name)                  <span class="fm-combinumeral">⑦</span>
try:
    with open (destination_file_name, 'r') as c_file:
        config = yaml.safe_load(c_file)                      <span class="fm-combinumeral">⑧</span>
except Exception as e:
    print('Error reading the config file')</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Gets the bucket prefix of config_bucket</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Gets the file path suffix of config_buckett</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Defines a storage.Client object</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">④</span> Creates a storage object for the bucket</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">⑤</span> Creates a storage object for the file</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">⑥</span> Sets the name of the config file copy in the container</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">⑦</span> Downloads the file from Cloud Storage to the container</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">⑧</span> Reads the contents of the container version of the config file into a dictionary</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Copied the values of the AIP environment variables that the Vertex AI SDK sets in the container. These environment variables contain URI patterns for the CSV files that the SDK creates in Google Storage that contain the train, validation, and test subsets of the d<a id="idTextAnchor070"/>ataset.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.3 Copying AIP environment variable values</p>
  <pre class="programlistinge">def assign_container_env_variables():
    OUTPUT_MODEL_DIR = os.getenv("AIP_MODEL_DIR")             <span class="fm-combinumeral">①</span>
    TRAIN_DATA_PATTERN = \
os.getenv("AIP_TRAINING_DATA_URI")                            <span class="fm-combinumeral">②</span>
    EVAL_DATA_PATTERN = \
os.getenv("AIP_VALIDATION_DATA_URI")                          <span class="fm-combinumeral">③</span>
    TEST_DATA_PATTERN = \
os.getenv("AIP_TEST_DATA_URI")                                <span class="fm-combinumeral">④</span>
    return OUTPUT_MODEL_DIR, TRAIN_DATA_PATTERN, \
EVAL_DATA_PATTERN, TEST_DATA_PATTERN</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Gets the URI for the location to save the trained model</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Gets the URI for the training dataset CSVs</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Gets the URI for the validation dataset CSVs</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">④</span> Gets the URI for the testing dataset CSVs</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Created dataframes for each of the patterns from the AIP environment variables. For each of these environment variables, we parsed the URI, got the list of matching files CSV blobs in Cloud Storage, and reassembled them into a single dat<a id="idTextAnchor071"/>aframe.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.4 Creating a dataframe for subsets of the dataset</p>
  <pre class="programlistinge">bucket_pattern = tracer_pattern.split("/")[2]                  <span class="fm-combinumeral">①</span>
pattern = "/".join(tracer_pattern.split("/")[3:])              <span class="fm-combinumeral">②</span>
pattern_client = storage.Client()                              <span class="fm-combinumeral">③</span>
bucket = pattern_client.get_bucket(bucket_pattern)
blobs = bucket.list_blobs()                                    <span class="fm-combinumeral">④</span>
matching_files = [f"gs://{bucket_pattern}/{blob.name}" \
for blob in blobs if fnmatch.fnmatch(blob.name, pattern)]      <span class="fm-combinumeral">⑤</span>
merged_data = \
pd.concat([pd.read_csv(f) for f in matching_files], 
ignore_index=True)                                             <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> For each file pattern, gets the bucket prefix</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Gets the CSV file pattern</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Defines a storage.Client object</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">④</span> Gets the list of CSVs in the bucket that match the pattern</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">⑤</span> Gets a list of the fully qualified URIs for the CSVs that match the pattern</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">⑥</span> Creates a dataframe containing the contents of all the CSVs that match the pattern</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Saved the trained model to a location specified by <code class="fm-code-in-text">OUTPUT_MODEL_DIR</code>, the URI set by the Vertex AI SDK as the location for saving the model:</p>
    </li>
  </ul>
  <pre class="programlistinge">tf.saved_model.save(model, OUTPUT_MODEL_DIR)</pre>

  <p class="body">With these changes, the rest of the training code works running in a container. Now that we have gone through the updates required to create the training script, in the next section we will go through the key parts of the pipeline script that sets up the container that the training script r<a id="idTextAnchor072"/>uns in.<a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>

  <h3 class="fm-head1" id="heading_id_20">11.3.6 The pipeline script</h3>

  <p class="body"><a id="marker-422"/>Now that we have gone through the training script, we can examine the code that makes up the pipeline script. You can see the complete pipeline script code at <a class="url" href="https://mng.bz/PdRn">https://mng.bz/PdRn</a>.<a id="idIndexMarker042"/><a id="idIndexMarker043"/></p>

  <p class="body">The key parts of the pipeline script are</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Ingest the pipeline c<a id="idTextAnchor073"/>onfig file: <a class="url" href="https://mng.bz/JYdV">https://mng.bz/JYdV</a>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Set the arguments for the training script:</p>
    </li>
  </ul>
  <pre class="programlistinge">model_args = ['--config_bucket', config['config_bucket_path']]</pre>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Create a <code class="fm-code-in-text">CustomTrainingJob</code> object that specifies the location of the training script <code class="fm-code-in-text">script_path</code>, the prebuilt image to use for training <code class="fm-code-in-text">container_uri</code>, and any additional Python libraries that need to be installed in the training container requirements, as shown in the followi<a id="idTextAnchor074"/>ng listing.<a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.5 Creating a <code class="fm-code-in-text">CustomTrainingJob</code> object</p>
  <pre class="programlistinge">def create_job(config):
    model_display_name = '{}-{}'.format(config['ENDPOINT_NAME'], TIMESTAMP)
    job = aiplatform.CustomTrainingJob(
            display_name='train-{}'.format(model_display_name),
            script_path = config['script_path'],
            container_uri=config['train_image'],            <span class="fm-combinumeral">①</span>
            staging_bucket = config['staging_path'],
            requirements=['gcsfs'],                         <span class="fm-combinumeral">②</span>
            model_serving_container_image_uri= \
config['deploy_image']                                      <span class="fm-combinumeral">③</span>
    ) 
    return job</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Sets the prebuilt Vertex AI container image to run the training script</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Defines the list of any additional requirements to be installed in the container</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Sets the prebuilt Vertex AI container image to use for prediction</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Define the path for the managed dataset used for training (using the dataset ID for the managed dataset that you created in section 11.2.7) and create a <code class="fm-code-in-text">TabularDataset</code> object using that path:<a id="marker-423"/><a id="idIndexMarker047"/></p>
    </li>
  </ul>
  <pre class="programlistinge">dataset_path = \
'projects/'+config['project_id']+\
'/locations/'+config['region']+\
'/datasets/'+config['dataset_id']
 ds = aiplatform.TabularDataset(dataset_path)</pre>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Run the job defined previously, specifying the dataset created here; the proportion of the dataset to use for training, validation, and test; and the <code class="fm-code-in-text">machine_type</code> to use fo<a id="idTextAnchor075"/>r the training.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.6 Running the job</p>
  <pre class="programlistinge">def run_job(job, ds, model_args,config):
    model_display_name = \
'{}-{}'.format(config['ENDPOINT_NAME'], TIMESTAMP)
    model = job.run(
        dataset=ds,                                           <span class="fm-combinumeral">①</span>
        training_fraction_split = \
config['training_fraction_split'],                            <span class="fm-combinumeral">②</span>
        validation_fraction_split = config['validation_fraction_split'],
        test_fraction_split = config['test_fraction_split'],
        model_display_name=model_display_name,
        args=model_args,                                      <span class="fm-combinumeral">③</span>
        machine_type= config['machine_type'] 
    )
    return model</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Associates the job with the managed dataset</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Sets proportions of the dataset to use for training, validation, and testing</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Sets the argument list (which contains the URI for the testing script config file)</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Create an endpoint and deploy the model trained in the training script to<a id="idTextAnchor076"/> that endpoint.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.7 Deploying the trained model to an endpoint</p>
  <pre class="programlistinge">def deploy_model(model,config):
    endpoints = aiplatform.Endpoint.list(                      <span class="fm-combinumeral">①</span>
        filter='display_name="{}"'.format(config['ENDPOINT_NAME']),
        order_by='create_time desc',
        project=config['project_id'], 
        location=config['region']
    )
    endpoint = aiplatform.Endpoint.create(                     <span class="fm-combinumeral">②</span>
         display_name=config['ENDPOINT_NAME'], 
         project=config['project_id'], 
         location=config['region']
        )
    model.deploy(                                              <span class="fm-combinumeral">③</span>
        endpoint=endpoint,
        traffic_split={"0": 100},
        machine_type=config['machine_type_deploy'],
        min_replica_count=1,
        max_replica_count=1,
    )</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Sets characteristics of the endpoint</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Creates the endpoint</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Deploys the model to the endpoint</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><a id="marker-424"/>The following listing is the main function of the pipeline script that invokes the functions to ru<a id="idTextAnchor077"/>n the pipeline.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.8 Main function of the pipeline script</p>
  <pre class="programlistinge">    start_time = time.time()
    config = get_pipeline_config('pipeline_config.yml')          <span class="fm-combinumeral">①</span>
    model_args = ['--config_bucket', config['config_bucket_path']]
    job = create_job(config)
    dataset_path = \
'projects/'+config['project_id']+\
'/locations/'+config['region']+\
'/datasets/'+config['dataset_id']
    ds = aiplatform.TabularDataset(dataset_path)
    model = run_job(job, ds, model_args,config)                  <span class="fm-combinumeral">②</span>
    if config['deploy_model']:
        deploy_model(model,config)                               <span class="fm-combinumeral">③</span>
    print("pipeline completed")</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Sets characteristics of the endpoint</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Creates the endpoint</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">③</span> Deploys the model to the endpoint</p>

  <p class="body">To run the pipeline script, do the following:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Clone <a class="url" href="https://github.com/lmassaron/ml_on_tabular_data">https://github.com/lmassaron/ml_on_tabular_data</a> in a new directory in Cloud Shell and make <code class="fm-code-in-text">chapter_11</code> the current directory.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Update the pipeline config file to ensure that <code class="fm-code-in-text">project_id</code> and <code class="fm-code-in-text">region</code> match the settings for your project, <code class="fm-code-in-text">dataset_id</code> matches the ID for your managed dataset, <code class="fm-code-in-text">staging_path</code> matches your staging path, and <code class="fm-code-in-text">config_bucket_path</code> matches the location in Cloud Storage, where you copied the training script config file, as shown in f<a id="idTextAnchor078"/>igure 11.33.</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F33_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.33 Training code interactions with content in Cloud Storage</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">In the root directory where you cloned the repo, enter the following command:</p>
    </li>
  </ul>
  <pre class="programlistinge">python pipeline_script.py</pre>

  <p class="body">Note that running the entire pipeline script can take 10 minutes or more. If the script fails, you will get a message that includes a link to the log file containing diagnostic messages about the training run. If the script succeeds, the output will end with the pipeline completed and the time ta<a id="idTextAnchor079"/>ken to run it.<a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>

  <h3 class="fm-head1" id="heading_id_21">11.3.7 Testing the model trained in the pipeline</h3>

  <p class="body"><a id="marker-425"/>Once you have run the pipeline script to run the ML pipeline to train and deploy a model, you can use the resulting Vertex AI endpoint to exercise the model in the same web deployment framework that we used in chapter 10. Note that testing the endpoint using this simple web deployment does not match what you would do in a production environment. However, using the same web deployment that we used in chapter 10 simplifies the testing process for this exercise.<a id="idIndexMarker050"/><a id="idIndexMarker051"/></p>

  <p class="body">The steps to test the model trained in the pipeline are</p>

  <p class="body-dialog">   1.  In the Google Cloud Console, go to the Vertex AI Endpoints. Copy the ID for the deployment that was created by the ML pipeline, as shown i<a id="idTextAnchor080"/>n figure 11.34.</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F34_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.34 Endpoint ID for the model generated by the pipeline</p>
  </div>

  <p class="body-dialog">   2.  In the same local system where you tested the initial endpoint deployment with Flask in chapter 10, paste the endpoint ID that you just copied into the value of the <code class="fm-code-in-text">endpoint_id parameter</code> in the <code class="fm-code-in-text">flask_web_deploy_config.yml</code> config file and save the file: <a id="idIndexMarker052"/><a id="idIndexMarker053"/></p>
  <pre class="programlistinge">endpoint:
   project: "1028332300603"
   endpoint_id: "1447850105594970112"
   location: "us-central1"</pre>

  <p class="body-dialog">   3.  On your local system, start the Flask server module:</p>
  <pre class="programlistinge">python flask_endpoint_deploy.py</pre>

  <p class="body-dialog">   4.  Once the Flask server module is running, go to <code class="fm-code-in-text">localhost:5000</code> in a browser. <code class="fm-code-in-text">home.html</code> will be rendered as shown in figure 11.35. When you click on Get prediction, the model trained and deployed at a Vertex AI endpoint by the ML pipeline will be invoked (se<a id="idTextAnchor081"/><a id="marker-426"/>e figure 11.35).</p>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F35_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.35 Home.html</p>
  </div>

  <p class="body">Note that the TensorFlow level used in the prebuilt container that you used for the model training has to match the TensorFlow level in the environment where you run the web application to test the endpoint. For example, if we want to exercise the endpoint deployment in an environment that has TensorFlow 2.9, then in the pipeline config file, we need to specify a value for <code class="fm-code-in-text">train_image</code> (the prebuilt training container) that is consistent with that level of TensorFlow level, such as <code class="fm-code-in-text">us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest</code>.</p>

  <p class="body">If you get a protobuf error when you run the pipeline script in Cloud Shell, try running the following command to specify the protobuf level:</p>
  <pre class="programlisting">pip install protobuf==3.20.*</pre>

  <p class="body">If you want to experiment with different training configurations, you can update the training config file, upload it to Cloud Storage (ensuring that the value of <code class="fm-code-in-text">config_bucket_path</code> in the pipeline config file matches the URI for the training config file), and rerun the pipeline script. You can use the web application to exercise the new model by updating the value of <code class="fm-code-in-text">endpoint_id</code> in the pipeline config file to match the endpoint ID of the new endpoint and repeating the steps in this section. By encapsulating multiple steps in the ML workflow in an ML pipeline, we make it easy to get repeatable results and experiment <a id="idTextAnchor082"/>with new settings.<a id="idIndexMarker054"/></p>

  <h2 class="fm-head" id="heading_id_22">11.4 Using generative AI to help create the ML pipeline</h2>

  <p class="body">So far in this chapter, we have seen how we use a combination of actions in Google Cloud and manual scripting to set up a basic ML pipeline to train and deploy a model trained on tabular data. In this section, we’ll explore how we can use the generative AI capabilities in Gemini for Google Cloud, introduced in chapter 10, to simplify or automate some of these actions. As we saw in chapter 10, there are four ways that Gemini for Google Cloud can help us:<a id="idIndexMarker055"/><a id="marker-427"/><a id="idIndexMarker056"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Answer questions about Google Cloud.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Generate code from text.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Interpret code. That is, given a piece of code, generate text that explains what the code does. We can use this capability to help us understand the code that we are adapting from other places. We can use this capability to document the code that we are writing ourselves.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Summarize log entries to<a id="idTextAnchor083"/> help debug problems.</p>
    </li>
  </ul>

  <h3 class="fm-head1" id="heading_id_23">11.4.1 Using Gemini for Google Cloud to answer questions about the ML pipeline</h3>

  <p class="body">As we saw in chapter 10, we can use the generative AI capabilities in Gemini for Google Cloud to get answers to questions about Google Cloud. The following are some examples of questions about creating an ML pipeline that Gemini for Google Cloud could help us to answer:<a id="idIndexMarker057"/><a id="idIndexMarker058"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">What is an ML pipeline? While Gemini for Google Cloud is trained specifically for Google Cloud, it is able to answer broad questions about technology, such as this one. Note that the answer shown in figure 11.36 is generally applicable and not limited to just Google Cloud. The citations come from a variety of credible sources, including the documentation for TensorFlow and Scikit-learn:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list">What is an ML pipeline? (<a class="url" href="https://mng.bz/wJjP">https://mng.bz/wJjP</a>)</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">Building a data pipeline (<a class="url" href="https://cs230.stanford.edu/blog/datapipeline/">https://cs230.stanford.edu/blog/datapipeline/</a>)</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list">ML pipelines with Scikit-learn (<a id="idTextAnchor084"/><a class="url" href="https://mng.bz/qxjr">https://mng.bz/qxjr</a>)</p>
        </li>
      </ul>
    </li>
  </ul>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F36_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.36 Gemini for Google Cloud answers the question “what is an ML pipeline?”</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">What is a Vertex AI pipeline? When we take the same question and qualify it, as shown in figure 11.37, Gemini for Google Cloud gives us an answer that is specific to the ML pipeline implementati<a id="idTextAnchor085"/><a id="marker-428"/>on in Google Cloud.</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F37_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.37 Gemini for Google Cloud answers the question “what is a Vertex AI pipeline?”</p>
  </div>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">What are Vertex AI prebuilt containers for training custom models? Finally, let’s try asking a question related to a specific task we tackled in this chapter. As you can see in figure 11.38, the answer provided by Gemini for Google Cloud describes both what prebuilt containers for training custom models are as well as the po<a id="idTextAnchor086"/>int of using them.</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figuree"><img alt="" class="calibre4" src="../Images/CH11_F38_Ryan2.png"/></p>

    <p class="figurecaptione">Figure 11.38 Gemini for Google Cloud answers the question “what are Vertex AI prebuilt containers for training custom models?”</p>
  </div>

  <p class="body">In this section, we have seen how we can use Gemini for Google Cloud to answer questions, both general and specific, about building an ML pipeline. In the next section, we’ll look at how we use Gemini for Google Cloud to generate the code required <a id="idTextAnchor087"/>for the ML pipeline.</p>

  <h3 class="fm-head1" id="heading_id_24">11.4.2 Using Gemini for Google Cloud to generate code for the ML pipeline</h3>

  <p class="body">Now that we have seen how Gemini for Google Cloud can answer questions about creating an ML pipeline, let’s explore how the generative AI capabilities in Gemini for Google Cloud can help us to create the code related to the ML pipeline.<a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="marker-429"/></p>

  <p class="body">Gemini for Google Cloud is enabled in several IDEs supported by Google Cloud, including VS Code, Cloud Workstations, and Cloud Shell Editor. In this section, we will use Gemini for Google Cloud in the context of Cloud Shell Editor. If you need a refresher on Cloud Shell Editor, see the overview documentation: <a class="url" href="https://mng.bz/7pvv">https://mng.bz/7pvv</a>.</p>

  <p class="body">We will see how Gemini for Google Cloud is able to generate the code for functions i<a id="idTextAnchor088"/>n the pipeline script: <a class="url" href="https://mng.bz/PdRn">https://mng.bz/PdRn</a> Using the function signatures and introductory comments from this script, we will see what Gemini for Google Cloud generates.</p>

  <p class="body">To begin with, if you have not done so already, follow the documentation to enable Gemini Code Assist <a id="idTextAnchor089"/>in Cloud Shell Editor: <a class="url" href="https://mng.bz/mGja">https://mng.bz/mGja</a>.</p>

  <p class="body">Once you have enabled Gemini Code Assist in Cloud Shell Editor, open a new Python file in Cloud Shell Editor and enter the signature and introductory comment for the <code class="fm-code-in-text">get_pipeline_config</code> function, as shown in <a id="idTextAnchor090"/>the following listing.<a id="idIndexMarker061"/></p>

  <p class="fm-code-listing-caption">Listing 11.9 Signature for <code class="fm-code-in-text">get_pipeline_config</code></p>
  <pre class="programlisting">def get_pipeline_config(path_to_yaml):
    '''ingest the config yaml file
    Args:
        path_to_yaml: yaml file containing parameters for the pipeline script
    
    Returns:
        config: dictionary containing parameters read from the config file
    '''</pre>

  <p class="body">Note that this code snippet does not include the logic of the function.</p>

  <p class="body">To get Gemini for Google Cloud to generate code to complete this function, simply press Enter. Gemini for Google Cloud generates provisional code in italics, as shown <a id="marker-430"/><a id="idTextAnchor091"/>in the figure 11.39.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F39_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.39 First set of provisional code generated by Gemini for Google Cloud</p>
  </div>

  <p class="body">Press Tab to accept this provisional code and then press Enter again to get the next set of code generated, as sho<a id="idTextAnchor092"/>wn in figure 11.40.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F40_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.40 Second set of provisional code generated by Gemini for Google Cloud</p>
  </div>

  <p class="body">Press Tab again to accept this second set of provisional code. The resulting function is shown in t<a id="idTextAnchor093"/>he following listing.</p>

  <p class="fm-code-listing-caption">Listing 11.10 <code class="fm-code-in-text">get_pipeline_config</code> function</p>
  <pre class="programlisting">def get_pipeline_config(path_to_yaml):
    '''ingest the config yaml file
    Args:
        path_to_yaml: yaml file containing parameters for the pipeline script
    
    Returns:
        config: dictionary containing parameters read from the config file
    '''
    with open(path_to_yaml) as file:                     <span class="fm-combinumeral">①</span>
        config = yaml.safe_load(file)
                    
    return config                                        <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> First set of code generated by Gemini for Google Cloud</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Second set of code generated by Gemini for Google Cloud</p>

  <p class="body">The code in listing 11.10 is not identical to the hand-written code for the <code class="fm-code-in-text">get_pipeline_config</code> function, as shown in t<a id="idTextAnchor094"/>he following listing.<a id="idIndexMarker062"/><a id="marker-431"/></p>

  <p class="fm-code-listing-caption">Listing 11.11 get_<code class="fm-code-in-text">pipeline</code>_<code class="fm-code-in-text">config</code> function: handwritten version</p>
  <pre class="programlisting">def get_pipeline_config(path_to_yaml):
    '''ingest the config yaml file
    Args:
        path_to_yaml: yaml file containing parameters for the pipeline script
    
    Returns:
        config: dictionary containing parameters read from the config file
    '''
    print("path_to_yaml "+path_to_yaml)
    try:                                                  <span class="fm-combinumeral">①</span>
        with open (path_to_yaml, 'r') as c_file:          <span class="fm-combinumeral">②</span>
            config = yaml.safe_load(c_file)
    except Exception as e:
        print('Error reading the config file')
    return config</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Handwritten code includes exception handling for the file opening operation</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Handwritten code includes 'r' parameter with the file open</p>

  <p class="body">Comparing the code generated by Gemini for Google Cloud in listing 11.10 with the hand-written code in listing 11.11, we can see two differences:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The handwritten code includes exception handling to deal with problems opening the config file.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The handwritten code includes the <code class="fm-code-in-text">'r'</code> parameter in the file open operation.</p>
    </li>
  </ul>

  <p class="body">The <code class="fm-code-in-text">get_pipeline_config</code> function is trivial, but, nevertheless, Gemini for Google Cloud was able to generate working code for the function. <a id="idIndexMarker063"/></p>

  <p class="body">Some additional considerations for Gemini for Google Cloud code generations are</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">You don’t have to accept the provisional code generations from Gemini for Google Cloud all at once. To accept the provisional code token-by-token, press CTRL + Right Arrow to accept a single token.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">To reject the entire provisional code generation and start again, press ESC, and the entire set of provisional code will be erased.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">When you ask Gemini for Google Cloud to generate code multiple times with the exact same input, you aren’t guaranteed to get identical code generated. For instance, in the <code class="fm-code-in-text">get_pipeline_config</code> example, sometimes Gemini for Google Cloud generated the function in two steps, as shown in this section, and sometimes it generated the entire function, including the <code class="fm-code-in-text">return</code> statement, in a single step.</p>
    </li>
  </ul>

  <p class="body">Now that we have used generative AI to generate code, we’ll see how we can use it to explain co<a id="idTextAnchor095"/>de in the next section.<a id="idIndexMarker064"/><a id="idIndexMarker065"/></p>

  <h3 class="fm-head1" id="heading_id_25">11.4.3 Using Gemini for Google Cloud to explain code for the ML pipeline</h3>

  <p class="body"><a id="marker-432"/>Now that we have seen an example of how Gemini for Google Cloud can generate code, let’s exercise its ability to interpret code. <a id="idIndexMarker066"/><a id="idIndexMarker067"/></p>

  <p class="body">To get Gemini for Google Cloud to interpret a code snippet, copy the code in the following listing (the <code class="fm-code-in-text">main</code> function from the pipeline script) into a new <a id="idTextAnchor096"/>file in Cloud Shell Editor.<a id="idIndexMarker068"/></p>

  <p class="fm-code-listing-caption">Listing 11.12 <code class="fm-code-in-text">get_pipeline_config</code> function</p>
  <pre class="programlisting">if __name__ == '__main__':
    start_time = time.time()
    # load pipeline config parameters
    config = get_pipeline_config('pipeline_config.yml')
    # all the arguments sent to the training 
    #script run in the container are sent via
    # a yaml file in Cloud Storage whose URI is the single argument sent
    model_args = ['--config_bucket', config['config_bucket_path']]
    print("model_args: ",model_args)
    # create a CustomTrainingJob object
    job = create_job(config)
    # define TabularDataset object to use in running CustomTrainingJob
    dataset_path = \
'projects/'+config['project_id']+\
'/locations/'+config['region']+\
'/datasets/'+config['dataset_id']
    ds = aiplatform.TabularDataset(dataset_path)
    # run the CustomTrainingJob object to get a trained model
    model = run_job(job, ds, model_args,config)
    print("deployment starting")
    # deploy model to a Vertex AI endpoint
    if config['deploy_model']:
        deploy_model(model,config)
    print("pipeline completed")
    # show time taken by script
    print("--- %s seconds ---" % (time.time() - start_time))</pre>

  <p class="body">Once you have pasted the code in listing 11.13 into a file, select it and then select the Gemini for Google Cloud Smart Actions icon from the Cloud Shell Editor tool<a id="idTextAnchor097"/>bar (see figure 11.41).</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F41_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.41 Gemini for Google Cloud Smart Actions icon</p>
  </div>

  <p class="body">In the menu that appears, select Explain, a<a id="idTextAnchor098"/>s shown in figure 11.42.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F42_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.42 Gemini for Google Cloud Smart Actions icon</p>
  </div>

  <p class="body">When you select this, the explanation for the code appears in the left pane of Cloud Shell Editor, as<a id="idTextAnchor099"/> shown in figure 11.43.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F43_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.43 Code explanation</p>
  </div>

  <p class="body">The code explanation capacity of Gemini for Google Cloud can be applied to a wide variety of code, including Python, Java, and JavaScript code. You can use the code explanations to understand code that you aren’t familiar with and to recommend documentation for your own code.</p>

  <p class="body">So far in this section, we have seen how we can use the generative AI capabilities in Gemini for Google Cloud to answer questions, generate code, and explain code. In the next subsection, we’ll see how we can use Gemini for Google Cloud to help <a id="idTextAnchor100"/>to summarize log entries.</p>

  <h3 class="fm-head1" id="heading_id_26">11.4.4 Using Gemini for Google Cloud to summarize log entries</h3>

  <p class="body"><a id="marker-433"/>Google Cloud includes a log that you can use to track the behavior of your environment and debug problems. Sometimes, however, the log entries can be hard to interpret. Gemini for Google Cloud can help you understand the point of a log entry by summarizing it. In this subsection, we’ll go through how you can use Gemini for Google Cloud to get the most out of Google Cloud logs.<a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>

  <p class="body">To exercise this capability of Gemini for Google Cloud, we will try to use the foundation model tuning in Vertex AI. Foundation model tuning lets us take a pretrained model and tune it with a dataset in the JSONL (JSON Lines: <a class="url" href="https://jsonlines.org/">https://jsonlines.org/</a>) dataset. See the documentation for more details on tuning text models in Vertex AI: <a class="url" href="https://mng.bz/5goO">https://mng.bz/5goO</a>.<a id="idIndexMarker071"/></p>

  <p class="body">To prepare for the example in this section, create a new folder called <code class="fm-code-in-text">staging</code> in the Cloud Storage bucket that you created in chapter 10.</p>

  <p class="body">In Vertex AI in the Google Cloud Console, select Vertex AI Studio -&gt; Language. In the Language page, select Tune and Distill and then select Create Tuned Model, <a id="idTextAnchor101"/>as shown in figure 11.44.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F44_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.44 Vertex AI Studio language page</p>
  </div>

  <p class="body"><a id="marker-434"/>In the Tuning Method pane of the Create Tuned Model page:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Specify a name for your model in the Tuned model name field.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Specify the URI for the staging folder that you created at the beginning of this section in the Output Directory field.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Click Con<a id="idTextAnchor102"/>tinue.</p>
    </li>
  </ul>

  <p class="body">See figure 11.45.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F45_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.45 Tuning method pane of the Create a tuned model page</p>
  </div>

  <p class="body">In the Tuning dataset pane of the Create Tuned Model page, do the following:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Select Existing File on Cloud Storage.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Enter the URI for this sample JSONL file <code class="fm-code-in-text">cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl</code> in the Cloud storage file path field. See the documentation for de<a id="idTextAnchor103"/>tails about JSONL samples: <a class="url" href="https://mng.bz/6ene">https://mng.bz/6ene</a>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Click Start Tuning.</p>
    </li>
  </ul>

  <p class="body">See figure 11.46.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F46_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.46 Tuning dataset pane of the Create a tuned model page</p>
  </div>

  <p class="body"><a id="marker-435"/>Once you click Start tuning, you will see a list of tuned models with the status of your model showing as Running, as <a id="idTextAnchor104"/>shown in figure 11.47.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F47_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.47 Tuning job status</p>
  </div>

  <p class="body">When the tuning job is complete, the status will change to Succeeded, as s<a id="idTextAnchor105"/>hown in figure 11.48.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F48_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.48 Tuning job status showing a completed tuning job</p>
  </div>

  <p class="body">If the tuning job does not succeed, that’s fine. The goal of this particular exercise is to examine an error, so it’s okay if the operation fails for some reason.</p>

  <p class="body">Once the tuning job is complete, put “logs explorer” in the search field at the top of the Console to get to the Logs Explorer page. This page provides many options for inspecting the logs generated by Google Cloud. For now, we just want to look at one of the errors. To view the errors, select Error in the bottom left of the Logs Explorer page, as sho<a id="idTextAnchor106"/>wn in figure 11.49.<a id="marker-436"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F49_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.49 Tuning job status showing a completed tuning job</p>
  </div>

  <p class="body">The Query Results pane at the bottom of the page shows the errors, as sho<a id="idTextAnchor107"/>wn in figure 11.50.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F50_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.50 Query results pane showing errors</p>
  </div>

  <p class="body"><a id="marker-437"/>Select one of these error entries to expand it and click on Explain This Log Entry as show<a id="idTextAnchor108"/>n in figure 11.51.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F51_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.51 Expanded error entry</p>
  </div>

  <p class="body">On the right, Gemini for Google Cloud shows an explanation of the error, as shown<a id="idTextAnchor109"/> in figure 11.52.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F52_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.52 Error explanation</p>
  </div>

  <p class="body">The explanation provided by Gemini for Google Cloud summarizes the nested entries in the log and makes it easier to read and interpret. Note that the explanation that you will see will depend on the error that you s<a id="idTextAnchor110"/>elect from the log.<a id="idIndexMarker072"/><a id="idIndexMarker073"/></p>

  <h3 class="fm-head1" id="heading_id_27">11.4.5 Tuning a foundation model in Vertex AI</h3>

  <p class="body">In the previous subsection, we saw how to use the generative AI capabilities in Gemini for Google Cloud to interpret error logs. It’s worth taking a closer look at the action that we triggered to generate logs that we could examine with Gemini for Google Cloud. Here is a summary of what we did:<a id="idIndexMarker074"/><a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="marker-438"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">We started with one of the foundation models available in Vertex AI, <code class="fm-code-in-text">text-bison</code>. This model is designed for various natural language tasks like content creation and classification. See the documentation for m<a id="idTextAnchor111"/>ore details on <code class="fm-code-in-text">text-bison</code>: <a class="url" href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text">https://mng.bz/oKrZ</a>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">We applied supervised tuning to adapt the <code class="fm-code-in-text">text-bison</code> foundation model to a particular use case—our case classifying medical transcripts. To learn more about supervised tuning of foundation models in Vertex AI, see the documentation: <a class="url" href="https://mng.bz/nR15">https://mng.bz/nR15</a>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The dataset that we used for tuning contained medical diagnosis transcripts paired with the classification for the transcript, as shown i<a id="idTextAnchor112"/>n the following listing.</p>
    </li>
  </ul>

  <p class="fm-code-listing-captione">Listing 11.13 Example record from tuning dataset</p>
  <pre class="programlistinge">{
"input_text":"TRANSCRIPT: \nREASON FOR CONSULTATION: , 
Loculated left effusion, 
multilobar pneumonia.\n\n LABEL:",                      <span class="fm-combinumeral">①</span>
"output_text":"Consult - History and Phy."              <span class="fm-combinumeral">②</span>
}</pre>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">①</span> Medical transcript</p>

  <p class="fm-code-annotatione"><span class="fm-combinumeral">②</span> Classification</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The<a id="idTextAnchor113"/> URI for this dataset is <code class="fm-code-in-text">gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl</code>.</p>
    </li>
  </ul>

  <p class="body">Once the tuning process is complete, you can exercise the tuned model in Vertex AI Studio by selecting Language -&gt; Tune and Distill and then selecting Test in the row for the model you tuned in the previous subsection, as s<a id="idTextAnchor114"/>hown in figure 11.53.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F53_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.53 Selecting the tuned model in Vertex AI Studio</p>
  </div>

  <p class="body"><a id="marker-439"/>The prompt editor opens with your tuned model selected as the model, as sh<a id="idTextAnchor115"/>own in figure 11.54.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F54_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.54 Prompt editor with the tuned model selected</p>
  </div>

  <p class="body">Exercise the tuned model by entering the following text in the Prompt field and clicking Submit:</p>
  <pre class="programlisting">TRANSCRIPT: \nIMPRESSION:  ,EEG during wakefulness, 
drowsiness, and sleep with synchronous 
video monitoring demonstrated no evidence 
of focal or epileptogenic activity.\n\n LABEL:</pre>

  <p class="body">Note the response, as sho<a id="idTextAnchor116"/>wn in figure 11.55.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F55_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.55 Sleep medicine response</p>
  </div>

  <p class="body">Now change the model back to the foundation model <code class="fm-code-in-text">text-bison@001</code>, as shown in figure 11.56, and c<a id="idTextAnchor117"/>lick Submit again.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH11_F56_Ryan2.png"/></p>

    <p class="figurecaption">Figure 11.56 Changing the model back to text-bison@001</p>
  </div>

  <p class="body">What’s the difference between the response you get from the prompt with the tuned model and the untuned foundation model? With the tuned model, you get all the capability of the foundation model along with appropriate responses for the medical transcript classification use case.</p>

  <p class="body">If you examine the dataset that we used to tune the foundation model (with the URI <code class="fm-code-in-text">gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl</code>) you will notice that it is, in fact, a tabular dataset with two columns: one containing medical transcription notes and the other containing a category for the notes, such as “cardiovascular / pulmonary,” “chiropractic,” or “pain management.” So far in this book, we have examined how generative AI can be applied to the workflow for machine learning on tabular data. The example we used in the log interpretation exercise demonstrates a different kind of relationship between tabular data and generative AI: tabular data being part of the workflow for generative AI. A detailed examination of this subject is beyond the scope of this book, but we think that the role of tabular data in generative AI workflows is an underresearched area that could yield significant benefits in getting the most out of generative AI.</p>

  <p class="body"><a id="marker-440"/>In this section, we have seen how we can use Gemini for Google Cloud’s generative AI capabilities to answer questions about ML pipelines, generate some of the code required to create one, explain the code that makes up an ML pipeline, and e<a id="idTextAnchor118"/>xplain log messages.<a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/></p>

  <h2 class="fm-head" id="heading_id_28">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Several set-up tasks need to be completed before setting up an ML pipeline in Vertex AI.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">A service account needs to be created, and the service account key needs to be uploaded to the directory where you run the pipeline script.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The dataset that will be used to train the model in the pipeline needs to be uploaded to a Google Cloud Storage bucket. This bucket location then needs to be used to define a Vertex AI dataset that will be used as an argument to the Vertex AI SDK in the pipeline script.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The training script running in a Vertex AI prebuilt container does not have access to the file system outside of the container, so the training dataset and the training config file for the ML pipeline implementation are in Cloud Storage, and their locations are passed to the training script as URIs.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The training code from the training notebook that we ran in Colab in chapter 9 needs to be adapted to run in a container. For example, the training script needs to be updated to use the Cloud Storage locations for the config file, the training data, and the location where the trained model should be saved.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The pipeline script invokes a series of functions from the Vertex AI SDK to create the container the training script runs in, to run the training script, and to deploy the trained model to a Vertex AI endpoint.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">You can use the same web application that you used to exercise the local deployment in chapter 10 to exercise the endpoint deployment generated by the ML pipeline.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">You can use Gemini for Google Cloud (the generative AI toolkit incorporated in Google Cloud) at various steps of the ML pipeline creation process to answer questions, generate code from text, interpret code, and explain log messages.<a id="marker-441"/></p>
    </li>
  </ul>
</body></html>