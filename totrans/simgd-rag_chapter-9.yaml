- en: 9 RAG development framework and further exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A recap of the concepts covered in this book using a six-stage RAG development
    framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Areas for further exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The previous eight chapters covered a wide breadth of retrieval-augmented generation
    (RAG), including a conceptual foundation, critical components, evaluation methods,
    advanced techniques, the operations stack, and essential variants of RAG. By now,
    you should be equipped with the necessary information required to develop RAG
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: This concluding chapter summarizes the discussion and recaps all the previously
    discussed concepts. To accomplish this, we put all the different aspects of developing
    RAG systems together and came up with a RAG development framework. Across the
    six stages of this RAG development framework, we recap the concepts covered in
    this book along with some best practices. This framework not only covers the technical
    aspects but also looks at the development process holistically.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is a rapidly evolving technique. At the end of this chapter, we also discuss
    some of the ideas that you can explore further. Some of these approaches to incorporating
    context may compete with the RAG technique, while others may be complementary.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Have reviewed and consolidated your understanding of key RAG concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a solid understanding of the RAG development framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be ready to build and deploy RAG systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, the problem statements that the developer of a RAG system is presented
    with will be open ended. For example, an e-commerce platform wants to develop
    a buying assistant, or the marketing function wants a research agent to track
    and summarize competitive information. So, how does one navigate from an open-ended
    problem statement to a fully developed RAG system? It becomes very important that
    this journey is guided by a thought process. For this purpose, let’s define and
    discuss a framework for developing RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 RAG development framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of developing RAG systems is not very different from developing
    an application that uses a machine learning model. We have seen that a RAG system
    can be complex and include several components. It goes beyond the elements such
    as models, data, and retrievers. It requires a service infrastructure to make
    the system available to users. Evaluation, monitoring, and maintaining the systems
    becomes as important as developing and deploying them. It all begins with an understanding
    of requirements and a conceptual design. To address all these aspects, a RAG development
    framework that will assist us in building RAG systems is proposed here. This framework
    involves the following six stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Initiatio**n*—This stage involves understanding the problem statement, aligning
    the stakeholders, gathering system requirements, and analyzing these requirements
    to draft a high-level system architecture.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Desig**n*—At this stage, design choices for RAG pipelines are made, and the
    suite of tools to develop the system is developed. In addition, different layers
    of the RAG operations stack are conceptualized.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Developmen**t*—This stage involves developing a working prototype of the desired
    RAG system. All required models are trained, and the required APIs are developed.
    This stage leads to the creation of the knowledge base and the development of
    the application orchestration layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Evaluation*—During this stage, the retrieval and generation components are
    evaluated, along with testing the end-to-end system performance. At the end of
    this stage, the system is ready for deployment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Deploymen**t*—During this stage, the system is made available to end users.
    The deployment strategy is also decided at this stage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Maintenanc**e*—This final stage is an ongoing one that involves system monitoring,
    incorporating user feedback, and keeping abreast of technological enhancements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bear in mind that the RAG development framework is not a linear process, but
    flexible, iterative, and cyclic. Figure 9.1 illustrates the cyclic nature of the
    six stages of the RAG development framework, showing the key artifacts of each
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F01_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1  The six stages of the RAG development framework are iterative and
    cyclic. At each stage, specific artifacts can be created.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each of the stages involves certain activities. We look at these activities
    one by one and discuss the best practices associated with them. We begin with
    the initiation stage.
  prefs: []
  type: TYPE_NORMAL
- en: '9.1.1 Initiation stage: Defining and scoping the RAG system'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The journey toward a successful RAG system begins with the initial interactions
    with the stakeholders. This is an opportunity to gain an in-depth understanding
    of the problem statement and the user requirements. It is an exploratory stage
    and sets the direction of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Use case identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A lot of the choices a developer will make in the development process of a
    RAG system depend heavily on the use case being addressed. Even a basic understanding
    of the industry domain/function and a simple definition of the use case is enough
    to answer crucial starting questions about the system. The requirement of a RAG
    system needs to be assessed here. Recall from chapter 1 the challenges that RAG
    solves: RAG overcomes training data limitations, knowledge cut-off date, and LLM
    hallucinations to bring factual accuracy, reliability, and trust to the system.
    It is important to assess whether these RAG benefits are pivotal to the use case.
    There can be LLM applications that may not even require RAG. Here are some questions
    you may need to ask at this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the system require data that may not be present in the training set of
    an available LLM?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the system require data that is current or updates frequently?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the system need to quote or generate facts? How crucial is the accuracy
    of the generated facts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the users benefit if the sources are cited?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A use case evaluation card as the one shown in figure 9.2 can help in assessing
    whether a RAG system is required to solve the use case. Use cases such as creative
    writing, language translation, sentiment analysis, grammar correction, and so
    forth do not generally require a RAG system unless some nuance of the use case
    warrants it.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, the industry domain and function can also give an early indication
    of the system requirements. For example, use cases from the healthcare and finance
    domain may require more security and compliance measures, while a use case from
    sports may require processing of quickly updating information.
  prefs: []
  type: TYPE_NORMAL
- en: This initial assessment of the use case may provide early insights, but a detailed
    understanding and analysis of the requirements is necessary before proceeding
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering of requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Developing the right RAG system means meeting the stakeholders’ needs and wants.
    Understanding these needs and wants is a crucial step. Gaining this understanding
    is an interactive and investigative process. Most stakeholders and end users may
    have limited knowledge about technology and how a RAG system is built. It is therefore
    important to know what a successful application would mean to them. These requirements
    can range from the features needed in the system to the expected scale and the
  prefs: []
  type: TYPE_NORMAL
- en: '![A chart of a system'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F02_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2  A use case evaluation card with the evaluating questions can help
    in assessing whether a RAG system is required to address the use case.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: desired performance of the system. A good way to gather requirements may be
    to look at them through different lenses, such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Business objective**s*—These requirements relate to the main business reasons
    for building these systems, such as increasing click-through rates, saving process
    costs, improving customer satisfaction, and so forth. Technical developers may
    not directly be responsible for business metrics, but these business metrics can
    act as the leading light in the development process of the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User need**s*—These are the core requirements of the users for whom the system
    is being developed. Expressing these needs helps in determining the inputs and
    outputs of the system along with other functionalities such as multilingual support
    and source citation. These needs are also key in determining the types of user
    queries that the RAG system can expect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Functional requirement**s*—These are the core functionalities of the system,
    such as the supported data types, number of documents to be retrieved and length/tone/style
    of generation, and similar. Functional requirements are influenced by user needs
    and business objectives. They are also the main influencers of the development
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Non-functional requirement**s*—These are requirements about the performance,
    scalability, reliability, security, and privacy of the system. There may be additional
    requirements such as legal and compliance, especially for regulated industries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Constraint**s*—One should also focus on any constraints that the system should
    be cognizant of, such as access to the internet, availability of data, cost, and
    integration with existing systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A customer service system, for example, may be envisioned to reduce customer
    query resolution time, requiring quick response time and a constraint of integrating
    with existing customer support platforms. An illustrative requirement document
    for the above can look like the one shown in figure 9.3, detailing out different
    types of requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a customer support system'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F03_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3  An illustrative requirements document for a customer support system
    requiring RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Requirements analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eliciting requirements from the stakeholders is a major activity in the initiation
    stage. These raw requirements then need to be analyzed. The requirements should
    be clear, precise, and quantifiable so that they can lead to specific development
    steps. For example, a non-functional need for a quick response may be too vague.
    Instead, a better requirement is that 90% of queries should be responded to within
    2 seconds. Similarly, a constraint of limited internet connectivity can lead the
    developer to believe that a completely offline system is required. Such vagueness
    in the requirements needs to be addressed in further interactions with the stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, it is also important to define the success criteria on which
    the system will be evaluated. A few success metrics need to be defined and agreed
    on. For developers, these success metrics should be different from the business
    objectives since business outcomes may depend on factors beyond their control.
    Latency, throughput, percentage of queries resolved, and similar, are good criteria
    for success metrics. Figure 9.4 presents an illustrative requirements document
    after an analysis of the success metrics. It is an improvement on the previous
    requirement document shown in figure 9.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a customer support system'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F04_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4  Illustrative requirements document with success metrics defined
    and requirements analyzed for clarity and precision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: High-level architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the requirements are understood well, the initiation stage can be deemed
    complete. It is good practice to close the initiation stage with a high-level
    architecture diagram that can be used as a starting point for the design stage.
    This architecture can be used to bring alignment among stakeholders and discuss
    the requirements further. The focus of this high-level architecture is to illustrate
    the system inputs and outputs. Since data plays such a crucial role in a RAG system,
    this high-level architecture should also include the data component. As illustrated
    in figure 9.5, for a multichannel customer support system, the system must allow
    inputs and outputs from and to different channels.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a product portal'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F05_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5  High-level architecture of a proposed customer support bot highlighting
    inputs and outputs, along with the data, human-in-the-loop, and cache layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A first go/no-go decision or the going forward strategic call can be taken on
    the completion of the initiation stage. Once the stakeholders are aligned, all
    the RAG operations layers for the system can be designed in the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: '9.2 Design stage: Layering the RAGOps stack'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a clear understanding of the use case and the requirements, developers
    can start planning for the development. In the design stage, the high-level architecture
    is refined to map out RAGOps stack, and the choices around tools and technology
    are made. At this stage, we design the indexing and generation pipelines along
    with other components such as caching, guardrails, and the like.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Indexing pipeline design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the requirement-gathering step, we identify the data sources. During the
    design stage, we double-click on these data sources to identify the nature of
    the source systems, file types, and nature of the data itself to determine the
    development steps for the knowledge base. Recall from chapter 3 that the knowledge
    base is created for a RAG system via the indexing pipeline. Components such as
    data loading, chunking, embeddings, and storage form the indexing pipeline. In
    chapter 7, we also discussed that the data layer of the RAGOps stack enables this
    by extracting, transforming, and loading the data. Figure 9.6 summarizes the indexing
    pipeline components and the data layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of data processing'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F06_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6  The indexing pipeline of the RAG system is executed using the data
    layer in the RAGOps stack.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now let’s look at some important points of consideration that will help us when
    making the choices for the indexing pipeline design.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you’re working with less data, like a few PDF files or a couple of websites,
    *data ingestion* is a relatively simple step. However, in production-grade systems,
    the complexity increases with the scale of the data. Special attention needs to
    be given to the source systems and the file formats. Here are a few questions
    about connecting to source systems that will help in designing the data ingestion
    component:'
  prefs: []
  type: TYPE_NORMAL
- en: Which source systems will the data layer need to connect to?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are the connectors readily available? If yes, which tools or services are required
    to establish these connections?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which connectors will need to be developed? Which technology will these connectors
    be developed on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is access to open internet required? How will the system connect to the internet?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following group of questions is about parsing files:'
  prefs: []
  type: TYPE_NORMAL
- en: Which file formats will be ingested?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will the web pages be scraped, if required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we have the necessary parsers for the different file types?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is some special parsing technique required to be developed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can there be more than one modality of data in a single file?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answers to these questions will determine the tools you will need to use
    for ingesting data and the parts that will need to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the data is ingested, the *transformation step* converts the data into
    a suitable format for the knowledge base. In the data transformation step, the
    data will first be cleaned and pre-processed. A good practice is also to extract
    metadata information. Sometimes, other preprocessing steps such as PII data redaction
    or resolving conflicting information are required.
  prefs: []
  type: TYPE_NORMAL
- en: After pre-processing, the data will be chunked using a suitable chunking technique.
    Chunk size, overlap size, and the chunking strategy should be decided at this
    stage. Chunking can be fixed size, structure driven, semantic chunking, or agentic
    chunking.
  prefs: []
  type: TYPE_NORMAL
- en: Once the chunks are created, they need to be transformed for retrieval. We have
    discussed approaches such as embeddings and knowledge graphs. For use cases that
    require relational understanding between chunks, knowledge graphs should be explored.
    The creation of vector embeddings is almost mandatory in all RAG systems. To create
    vector embeddings, pre-trained embeddings models can be used. However, sometimes,
    due to the peculiarity of the domain, embedding models may need to be fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now look at some of the questions that should be considered at this stage.
    The first group of questions is about pre-processing:'
  prefs: []
  type: TYPE_NORMAL
- en: How noisy is the data? What algorithms and techniques can be used to clean up
    the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is structured data like tables or JSON present?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is metadata readily available, or should it be extracted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What algorithms or models should be used for metadata extraction? (Note: All
    models sit in the model library of the model layer of the RAGOps stack.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the data contain sensitive information that needs to be masked or redacted?
    What techniques will be used to execute this?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any other data protocols or guidelines that need to be followed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When it comes to chunking, consider asking the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the chunk size pre-determined? If not, what chunk sizes should be experimented
    with?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data in a format that will warrant structured chunking?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What techniques and models will be employed for semantic chunking, if required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a chunking agent readily available, or will it need to be built? Which models,
    algorithms, and tools will be used by the chunking agent?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following group of questions covers graphRAG:'
  prefs: []
  type: TYPE_NORMAL
- en: Is a hierarchical indexing structure required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to extract entities and relationships for relational context? Do
    we have the necessary budget?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What approaches are we going to take for entity-relationship extraction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we using any frameworks for graph extraction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which models are going to be used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for embeddings, ask the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Which embeddings model will we use? Are there any domain-specific embeddings
    models available that will be more useful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are multimodal embeddings required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to fine-tune embeddings for our use case? Do we have the training
    data for fine-tuning? How will the training data be sourced?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation steps require significant thought and effort. This is also
    where significant costs can be incurred, especially in using agents and employing
    graphRAG.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final component of the data layer is the storage. Depending on the choices
    made during the data transformation, the storage will comprise vector stores,
    graph databases, and document stores (if necessary). At this stage, we should
    also keep in mind that a cache store may be required in the application that can
    be a part of the data layer. We will discuss caching separately. Some of the questions
    pertinent to data storage are
  prefs: []
  type: TYPE_NORMAL
- en: Can all data be stored in a single collection, or are multiple collections required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we manage the vector database or do we require a managed service?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the current scale of data and how is it likely to grow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which vector database will we use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need a graph database? Which graph database will we use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to store raw documents or images? Which document store will we use
    for this purpose?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the storage in place, the creation of the knowledge base can be executed.
    It is important to note that the choices at this stage should be flexible. You
    should also keep options available for tools, services and libraries that can
    be experimented with during development. You’ll also have to estimate the costs
    associated with different steps of this stage and ensure that the stakeholders
    are aligned with these costs.
  prefs: []
  type: TYPE_NORMAL
- en: With the data layer of the RAGOps stack, the design of the indexing pipeline
    is complete. You may also note that the indexing pipeline also interacts with
    the model layer where embeddings models and LLMs along with other task specific
    algorithms sit.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Generation pipeline design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have discussed that the real-time interaction of the user with the knowledge
    base is facilitated by the generation pipeline. In chapter 4, we developed the
    three main components of the generation pipeline—the retrievers, augmentation
    via prompts, and generation using LLMs. Apart from these three components, query
    optimization in the pre-retrieval stage and context optimization in the post-retrieval
    stage are advanced components of the generation pipeline. Sometimes, even post-generation,
    response optimization is conducted to better align the responses. The generation
    pipeline is powered by the model layer of the RAGOps stage, which has the LLMs,
    the retrievers, embeddings models, and other task-specific models. The generation
    pipeline is brought alive by the app orchestration layer of the RAGOps stack.
    Let’s discuss the design of the generation pipeline in the following six steps:
    query optimization (pre-retrieval), retrieval, context optimization (post-retrieval),
    augmentation, generation, and response optimization (post-generation).'
  prefs: []
  type: TYPE_NORMAL
- en: Query optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Query optimization techniques are employed to help retrieval better align with
    the query. Several techniques are employed for transforming and rewriting queries.
    For agentic RAG, query routing is an important aspect of this step. Some of the
    questions to help finalize the nature of query optimization are
  prefs: []
  type: TYPE_NORMAL
- en: How many types of queries can the user ask? Do each of these query types require
    different downstream processes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there multiple collections in the knowledge base that need to be selected
    before the search?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are user queries expected to be short or generic?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are users looking for precise responses?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much processing time can be afforded to query optimization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which models and techniques will be used for query optimization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query optimization is optional but may be unavoidable when the data in the knowledge
    base is voluminous. It must also be noted that query optimization can add to the
    latency of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Retrieval is a pivotal component of RAG systems. There are many retrieval techniques
    and strategies discussed in this book. The quality of the RAG system hinges on
    the accuracy of the retrieval component. You may use a dense embeddings similarity
    match for simple RAG systems. In more complex systems, you will need to use hybrid,
    iterative, or adaptive retrieval strategies. The questions to ask at this stage
    are
  prefs: []
  type: TYPE_NORMAL
- en: Does our retrieval component need high precision, high recall, or both?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can the queries be resolved with a simple similarity match?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need graph retrieval?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will searching through the entire data be prohibitively long? Do we need filtering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will a single pass retrieve all necessary documents?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the information from the retrieved documents lead to more questions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which models and techniques will we use for adaptive, recursive, or iterative
    retrieval?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which retrieval algorithms should we try?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any providers or libraries that we will leverage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will we estimate the cost of retrieval?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many documents should be retrieved for acceptable levels of coverage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does ranking in retrieved results matter?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval, especially in large knowledge bases, can lead to significant latency
    and should be optimized for speed and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Context optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the results are retrieved from the knowledge base, they need to be sent
    to the LLM for generation along with the original user query. However, once the
    results are retrieved to sharpen the context, certain optimization techniques
    such as re-ranking and compression can be applied. These techniques filter, compress,
    and optimize the retrieved information to reduce noise and increase the precision
    of the context. To validate the need for context optimization, a few questions
    can be asked:'
  prefs: []
  type: TYPE_NORMAL
- en: Will the amount of information retrieved overwhelm the LLM?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the retrieved information fit the context window of the LLM?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a possibility of the retrieved information being noisy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a lot of documents been retrieved? Do we need to discard a few?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which techniques can be used to sharpen the retrieve context to the query?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any services or libraries that we can use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we afford the time taken for this optimization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizations like this are very helpful in making the context precise and improving
    the overall quality of the RAG system, but they do add to the processing time
    and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Augmentation is the process of adding the retrieved context to the original
    query in a prompt that can be sent to the LLM for generation. While it may seem
    a simple step, there can be many nuances to it. All the use case context along
    with the retrieved context also needs to be passed. Sometimes, you may need to
    pass examples of desired responses or the thought process. In cases where you
    need to use the LLMs internal parametric knowledge, this can also be specified
    in the prompt. Key questions to ask at this stage are
  prefs: []
  type: TYPE_NORMAL
- en: What is the system prompt or the overall persona that we need the LLM to take?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the response require nuanced analysis? Can that be passed as a chain of
    thought?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we want to restrict the responses to the context only?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of examples should be given?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will different query types need different prompting techniques?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmentation is done through prompts, and prompts can be managed by the prompt
    layer of the RAGOps stack. Prompting affects the cost and latency since the LLM-s
    processing depends on the number of tokens passed in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generation is a core component of all generative AI apps and contains an LLM
    that takes a prompt as input and generates a response. The nature of the LLM determines
    the efficacy and efficiency of the RAG system to a large extent. There are several
    choices that you will need to make:'
  prefs: []
  type: TYPE_NORMAL
- en: Should an open source model be used? Do we have the skills and resources to
    use them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should a proprietary managed LLM be used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will we need to fine-tune an LLM for our use case?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large a model do we need? What capabilities do we need to address?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we estimate the cost of the generation component?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any deployment constraints to be considered?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the models need optimization for deployment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any security implications to be considered?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any ethical or legal implications to be considered?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The selected LLMs will sit in the model library. All training fine-tuning activities
    and optimization are carried out in the model layer of the RAGOps stack. LLMs
    can be costly to train and use. Using the right LLM is key to the success of the
    RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: Response optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, the response from the generation component may be further processed
    before presenting the results to the user. This can range from evaluating the
    response for relevance to checking the format and appending the responses with
    the retrieved sources. Some questions that can help with the assessment at this
    stage are
  prefs: []
  type: TYPE_NORMAL
- en: Does the response from the LLM be presented to the user as is?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there any kind of verification that the responses need to go through?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the impact of a sub-optimal result?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any workflows that need to be triggered based on the responses?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response optimizations are highly subjective and closely coupled to the use
    case, but it is a consideration that should not be overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: With these seven steps, the generation pipeline design is complete. The model
    library and the training/fine-tuning components of the RAGOps stack can be covered
    with the necessary tools, platforms, and algorithms. The orchestration of the
    generation pipeline can also be finalized depending on the choices made during
    this stage. The prompt layer can also be addressed after finalizing the augmentation
    techniques. Figure 9.7 shows the generation pipeline design with the overarching
    question of each step.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a system'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F07_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7  Key questions need to be answered to make the choices for the generation
    pipeline.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This completes the design choices of the core RAG pipelines. The model, prompt,
    and the orchestration layers are largely complete by this stage. But there are
    more design considerations regarding security, guardrails, caching, and other
    use case requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Other design considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While well-designed core RAG pipelines complete the critical layers of the
    RAG system, other system considerations and business requirements also need to
    be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of guardrails are required in the system? Should the user queries
    be restricted? Is there any kind of information that should not be output?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it possible and useful to cache certain kinds of responses?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need human supervision or action at any stage in the system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will the models be protected from adverse attacks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there any approval workflow required in the system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are users looking for explainability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These questions will help address the essential and enhancement layers of the
    RAGOps stack. You should be able to have a complete view of the necessary components,
    tools, platforms, and libraries for the development of the RAG system. The last
    choice to be made is on deployment options.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose between a managed deployment on the cloud, a self-hosted deployment
    on a private cloud, a bare metal server, or local/edge machines. The choice will
    largely be driven by the business constraints but can have an effect on the design
    choices of the pipelines. Fully managed deployment favors managed services for
    storage and compute to reduce development complexity and ensure scalability, self-hosted
    solutions need a special focus on a design with modularity and optimization techniques
    to handle limited infrastructure, and in edge deployment, you should emphasize
    lightweight components and efficient retrieval strategies due to resource constraints.
  prefs: []
  type: TYPE_NORMAL
- en: With all these design elements finalized, experimentation can begin for the
    development of the RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: '9.2.4 Development stage: Building modular RAG pipelines'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The development stage of the RAG development framework focuses on implementing
    the design choices into a functional RAG system. The ideal way would be to build
    the RAG pipelines in a modular fashion, which involves decomposing the system
    into distinct, interchangeable components, each responsible for a specific function.
    This approach enhances flexibility, scalability, and maintainability, allowing
    for tailored configurations to meet diverse application requirements. A few activities
    in the development stage involve training and fine-tuning models; creating APIs
    or microservices for different components; and creating an orchestration layer
    using different tools, services, and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and fine-tuning LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For most systems, a pre-trained foundation LLM and embeddings models will meet
    the requirement. There may be instances where you may need to fine-tune models
    for domain adaptation. In rare cases, you may choose to train language models
    from scratch. In such cases, the development of RAG systems may take a back seat,
    and training the models will be the core of the development effort. You can follow
    a progressive approach when deciding whether to fine-tune embeddings models and
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: When creating embeddings using a pre-trained model, you will need to assess
    if a similarity search yields relevant results. To do this, you can also create
    ground truth data. The ground truth data can be a set of manually curated search
    queries and their matching documents. If the embeddings model can retrieve the
    documents accurately, you may use the pre-trained model. If not, you can either
    look for another embeddings model more suited for the use case domain or fine-tune
    the pre-trained embeddings model for the use case domain.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if a pre-trained LLM generates desired results by prompting alone,
    you can use the model as is. In cases where you desire a specific style, vocabulary,
    or tonality, you can choose to fine-tune a model.
  prefs: []
  type: TYPE_NORMAL
- en: If the system warrants other models such as query classification, harmful content
    detection, usefulness, and similar, they will also need to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Module development
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different RAG pipeline components should be developed as independent modules
    in the form of packages, APIs, or other modular frameworks. Some of the modules
    can be
  prefs: []
  type: TYPE_NORMAL
- en: '*Data loading and parsin**g*—Responsible for connecting to the source system
    and parsing file formations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metadata extractio**n*—Responsible for extracting and tagging metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunkin**g*—Responsible for creating chunks from documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Embedding**s*—Responsible for converting chunks into vector embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Storag**e*—Responsible for storing embeddings into vector databases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Query optimizatio**n*—Responsible for aligning user query with retrievers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieva**l*—Responsible for efficient retrieval of documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Augmentatio**n*—Responsible for maintaining and invoking the prompt library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generatio**n*—Responsible for using the LLMs to generate responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memor**y*—Responsible for storing conversations, user preferences, and similar'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only a few examples. Modularity will be dependent on the complexity
    of the components. For example, if you are convinced that fixed-size chunking
    is sufficient for your use case, you may not develop an independent chunking module.
    Conversely, if you assume that LLMs may need to be changed as the system evolves
    with the technology, you can create the generation module that allows for quick
    and easy replacement of models. Figure 9.8 recalls the modular RAG design discussed
    in chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, you will develop the orchestration layer that will manage the interaction
    among the different modules that you have developed. This enables the workflow
    of your RAGsystem. This workflow should be flexible enough to adapt with feedback
    for different query types.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F08_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8  Modular structure allows for flexibility and scalability of individual
    components.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You will also have access to various managed services, frameworks, libraries,
    and tools that you can integrate with any of the modules. For example, LangChain
    is a framework that provides libraries for most components of a RAG framework.
    You can use these libraries for quick and easy development. However, for components
    that you desire more control over, you may need to build the functionality from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Development is an experimentation-driven iterative process. To finalize the
    different components of the RAG system, you will need to evaluate them and benchmark
    them against the goals you had set in the initiation stage.
  prefs: []
  type: TYPE_NORMAL
- en: '9.2.5 Evaluation stage: Validating and optimizing the RAG system'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluation of the RAG system is a key component of its development process.
    All the different strategies, tools, and frameworks must be evaluated against
    some set of benchmarks. The actual business effect can only be measured post-deployment,
    but some metrics can be evaluated at the development stage. We can look at these
    metrics in two broad categories.
  prefs: []
  type: TYPE_NORMAL
- en: RAG components
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The purpose of evaluating the RAG system is to assess the performance of different
    RAG components. To this end, there can be retriever-specific, generation-specific,
    and overall RAG evaluation metrics. Here is a summary of these metrics discussed
    in chapter 5\. We begin with retriever-specific metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy* is typically defined as the proportion of correct predictions (both
    true positives and true negatives) among the total number of cases examined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precision* focuses on the quality of the retrieved results. It measures the
    proportion of retrieved documents relevant to the user query. It answers the question,
    “Of all the documents that were retrieved, how many were relevant?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precision@k* is a variation of precision that measures the proportion of relevant
    documents among the top ‘k’ retrieved results. It is particularly important because
    it focuses on the top results rather than all the retrieved documents. For RAG,
    it is important because only the top results are most likely to be used for augmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recall* focuses on the coverage that the retriever provides. It measures the
    proportion of the relevant documents retrieved from all the relevant documents
    in the corpus. It answers the question, “Of all the relevant documents, how many
    were retrieved?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1-score* is the harmonic mean of precision and recall. It provides a single
    metric that balances both the quality and coverage of the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mean reciprocal rank, or MRR*, is particularly useful in evaluating the rank
    of the relevant document. It measures the reciprocal of the ranks of the first
    relevant document in the list of results. MRR is calculated over a set of queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mean average precision, or MAP,* is a metric that combines precision and recall
    at different cut-off levels of ‘k’ (i.e. the cut-off number for the top results).
    It calculates a measure called average precision and then averages it across all
    queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*nDCG* evaluates the ranking quality by considering the position of relevant
    documents in the result list and assigning higher scores to relevant documents
    appearing earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the summary of generation specific metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Coherence* assesses the logical flow and clarity of the response, ensuring
    that the information is presented in an understandable and organized manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conciseness* evaluates whether the response is succinct and to the point,
    avoiding unnecessary verbosity, while still conveying complete information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We conclude with a summary of overall RAG metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Context relevance* assesses the proportion of retrieved information relevant
    to the user query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Faithfulness* or *groundedness* assesses the proportion of the claims in the
    response that are backed by the retrieved context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hallucination rate*calculates the proportion of generated claims in the response
    that are not present in the retrieved context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coverage* measures the number of relevant claims in the context and calculates
    the proportion of relevant claims present in the generated response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Answer relevance* assesses the overall effectiveness of the system by calculating
    the relevance of the final response to the original question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall the triad of RAG evaluation from chapter 5\. Figure 9.9 shows the pairwise
    interaction between the user query, retrieved context, and the generated response,
    which calculates the RAG specific metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a customer relationship management'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F09_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9  The triad of RAG evaluation proposed by TruEra
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To calculate some of these metrics, a ground truth dataset is required. Ground
    truth is information known to be real or true. In RAG, and the generative AI domain
    in general, ground truth is a prepared set of prompt–context–response or question–context–response
    examples, akin to labeled data in supervised machine learning parlance. Ground
    truth data created for your knowledge base can be used for the evaluation of your
    RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: You can measure these metrics for different components. For example, you can
    check if context relevance increases by replacing a hybrid retrieval strategy
    with an adaptive one. You can also check the effectiveness of query and context
    optimization. You can also compare two service providers for a particular component.
  prefs: []
  type: TYPE_NORMAL
- en: System performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: System performance metrics relate to the non-functional requirements of the
    system, which affect the usability of the system more than the accuracy of the
    system. Some of these metrics are
  prefs: []
  type: TYPE_NORMAL
- en: '*Latenc**y*—Measures the time taken from receiving a query to delivering a
    response. Low latency is crucial for user satisfaction, especially in real-time
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Throughpu**t*—Indicates the number of queries the system can handle within
    a specific time frame. Higher throughput reflects the system’s ability to manage
    large volumes of requests efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource utilizatio**n*—Assesses the efficiency of CPU and GPU usage during
    operations. Optimal utilization ensures cost-effectiveness and prevents resource
    bottlenecks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost per query* calculates the average expense incurred for processing each
    query, encompassing infrastructure, energy, and maintenance costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency and cost get special attention in LLM-based systems. This is because
    of the inherent nature of the LLM architecture. RAG adds to both latency and cost.
    Therefore, the impact of additional components like filtering during retrieval,
    optimizations, and retrieval strategies should be evaluated from this lens. Sometimes
    the stakeholders may also ask you to evaluate some use case-specific metrics,
    and that should also be a part of this evaluation stage.
  prefs: []
  type: TYPE_NORMAL
- en: When your system is thoroughly evaluated and improved to meet all the benchmarks,
    it is ready to go. You can now deploy it to make it available to the intended
    users.
  prefs: []
  type: TYPE_NORMAL
- en: '9.2.6 Deployment stage: Launching and scaling the RAG system'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the system is ready to ship, it needs to be deployed into a production
    server accessible by the intended users. There are a few deployment techniques
    that are popular for software systems, which can also be used for RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Blue–green deployment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Blue–green deployment maintains two separate environments named blue and green.
    The existing system is in the blue environment, and the new RAG system is put
    in the green. Once the green environment is tested and verified, all traffic is
    directed to the green environment, and the blue environment is deactivated. The
    advantage of this blue–green deployment is that it is possible to test the production
    environment without affecting the live traffic. Consequently, there is zero downtime
    and an easy option for a rollback if any problem is encountered. However, it is
    a costly option since the entire production environment is duplicated. Indexing
    pipelines can be updated in the green environment without affecting the live system.
    Changes to retrieval strategies or embeddings models can be safely validated before
    production use.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Canary deployment gradually releases the new RAG system to a small number of
    users. If it performs well with these users, it is expanded to all users. Canary
    deployment allows for real-time user feedback that enables early detection of
    problems. However, it adds feedback and monitoring complexity and multiple versions
    to manage. It can test changes in retrieval algorithms, embeddings, or generation
    models on limited queries or specific regions.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling deployment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rolling deployment is used when there are multiple production servers. The new
    RAG system is deployed to one server incrementally at a time before moving to
    the next. So, there is no complete downtime and only a part of the system is offline
    at one time. It may become complex if problems arise mid-deployment. The rollback
    can become tedious when some servers are updated, while others are not.
  prefs: []
  type: TYPE_NORMAL
- en: Shadow deployment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shadow deployment mirrors live traffic to a new version of the system running
    alongside the old one, without exposing the new RAG system’s responses to users.
    By doing this, the system can be tested without affecting the users. However,
    it requires duplication of the infrastructure much like the blue–green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A/B testing involves deploying two versions of the RAG system (A and B) to separate
    subsets of users and comparing their performance to determine the better option.
    This can also be done for new systems. It enables direct comparison and provides
    clear insights into performance. However, it requires robust mechanisms to split
    traffic and collect performance metrics. It allows for experimenting with different
    LLMs or retrieval strategies and variations in prompting and augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Interleaving experiments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Interleaving experiments* compare two RAG systems by blending their outputs
    into a single result set shown to users. Results from both systems are interleaved,
    and user interactions are attributed to the originating system to determine which
    performs better. This approach provides fast feedback and reduces bias by comparing
    systems under identical conditions. However, the attribution of user engagement
    to the correct system can be complex.'
  prefs: []
  type: TYPE_NORMAL
- en: The choices for the deployment strategy can depend on factors like such as tolerance,
    and using strategies such as shadow, canary, and blue–green can mitigate risks
    in mission-critical systems. It also depends on the scale, and rolling deployments
    make sense for large-scale systems. Small new RAG systems can be also deployed
    all at once.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the system is available to the users, you will start getting real-time
    feedback, and the success and failure of the system will also depend on how you
    react to the feedback. To measure and improve the system, continuous monitoring
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: '9.2.7 Maintenance stage: Ensuring reliability and adaptability'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deploying a RAG system into production is only the first milestone in the journey
    toward an evolved contextual AI system. Explicit user feedback, evolving technology,
    and changing user behavior present previously unexplored challenges that the system
    may encounter. It is therefore essential to be continually vigilant and monitor
    the system performance. There are several reasons why a RAG system may fail in
    production. There are operational reasons such as compute resource constraints,
    sudden spikes in load, and malicious attacks. The reason can also be a shift in
    the type of data in the knowledge base or a change in user queries. It is therefore
    essential to measure a few metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: RAG component metrics that were evaluated before deployment need to be continuously
    monitored for degradation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in user behavior can be tracked by analyzing the nature of user queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System performance metrics such as latency, throughput, and similar should also
    be continuously monitored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional metrics such as error rates, system downtime, malicious attacks,
    and similar should also be tracked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User engagement metrics such as customer satisfaction scores or repeat engagement
    can indicate the usability of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business metrics such as revenue effects and cost savings should also be tracked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This development framework completed its cycle with maintenance. However, it
    is not a linear process. New requirements and business objectives will emerge.
    This will re-initiate the development cycle for an improved RAG system. This development
    framework will prove to be a good reference resource while building RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: We conclude this book and end the discussion on RAG in the next section with
    some additional considerations to keep in mind as the generative AI domain evolves.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Ideas for further exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like any technology, even with RAG, there are some complementary and some competing
    ideas that coexist. You may hear about these techniques and sometimes be challenged
    to defend the use of RAG. There are also common points of failure for RAG systems
    that need attention.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Fine-tuning within RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Supervised fine-tuning (SFT) of LLMs has become a popular method to customize
    and adapt foundation models for specific objectives. There has been a growing
    debate in the applied AI community around the application of fine-tuning or RAG
    to accomplish tasks. While RAG enhances the non-parametric memory of a foundation
    model without changing the parameters, SFT changes the parameters of a foundation
    model and therefore influences the parametric memory. RAG and SFT should be considered
    as complementary, rather than competing, techniques because both address different
    parts of a generative AI system. You may prefer fine-tuning over RAG if there
    is a change required in the writing style, tonality, and vocabulary of the LLM
    responses. In their paper “Retrieval-Augmented Generation for Large Language Models:
    A Survey” ([https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997)),
    Gao and colleagues plot the evolution of prompt engineering to RAG and fine-tuning.
    This is illustrated in figure 9.10, demonstrating the need for fine-tuning with
    the increase in the need for model adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F10_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.10  Prompt engineering requires low modifications to the model and
    external knowledge, focusing on harnessing the capabilities of LLMs themselves.
    Fine-tuning, however, involves further training the model. Source: [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997).'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fine-tuning methods for both retrievers and generators hold immense potential
    for significantly improving RAG performance. Retriever fine-tuning enhances the
    ability of retrieval models to accurately capture semantic nuances relevant to
    specific domains, using methods such as contrastive learning, supervised embedding
    fine-tuning, LM-
  prefs: []
  type: TYPE_NORMAL
- en: supervised retrieval, or reward-based fine-tuning. Generator fine-tuning complements
    this by adapting language models through methods such as fusion-in-decoder (FiD),
    prompt tuning, latent fusion techniques, and parameter-efficient fine-tuning (PEFT).
    Combining these approaches within a hybrid fine-tuning framework can align the
    retrieval and generation components more effectively, leading to higher accuracy,
    reduced hallucinations, and improved adaptability to domain-specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Long-context windows in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Context windows in LLMs keep growing significantly with iteration. As of this
    writing, Claude 3.5 sonnet supports a window of up to 200,000 tokens, while GPT-4o,
    O1, and variants can process 128,000 tokens. Google Gemini 1.5 leads with a massive
    1-million-token context window. It is possible that when you read this book, there
    may be models with even longer context windows. So, in a lot of cases, we can
    just pass the entire context such as a long document to the model as part of the
    prompt. This would eliminate the need for chunking, indexing, and retrieval in
    cases where the knowledge base is not too large. In their paper, “Retrieval Augmented
    Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach” ([https://arxiv.org/abs/2407.16833](https://arxiv.org/abs/2407.16833)),
    Li and colleagues systematically compare RAG and LLMs with long-context windows.
    They demonstrate that long-context LLMs outperform RAG with a few exceptions.
    However, processing long contexts directly with LLMs can be computationally expensive.
    RAG is significantly more cost-efficient owing to processing shorter inputs. A
    hybrid approach such as SELF-ROUTE proposed in the same paper uses model self-reflection
    to decide whether a query can be answered with retrieved chunks or if it needs
    the full context. Figure 9.11 illustrates the SELF-ROUTE approach, in which the
    model receives the query with the retrieved chunks and determines whether the
    query can be answered based on this information. If yes, it generates the answer.
    If no, the full context is provided to the model, and the model generates the
    final answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH09_F11_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11  A hybrid approach utilizing RAG and long context in LLMs can lead
    to better performance without adversely increasing the costs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 9.3.3 Managed solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the growing popularity of RAG and its significance in generative AI applications,
    many service providers offer managed RAG pipelines in which several RAG components
    can be configured without the need for custom development. For example, knowledge
    bases are an Amazon Bedrock capability that facilitates implementation of the
    entire RAG workflow. Azure AI Search provides indexing and query capabilities,
    with the infrastructure of the Azure cloud, and Vertex AI RAG Engine is a component
    of Google’s Vertex AI platform that facilitates RAG. There are also independent
    service providers such as CustomGPT, Needle AI, Ragie, and so forth that provide
    managed RAG pipelines. As with managed solutions across technologies, the factors
    to consider are cost, applicability to the use case, flexibility, and control
    over components.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.4 Difficult queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some key reasons for failures in RAG systems are related to the types of queries.
    As RAG developers, it is important to keep focusing on these query types so that
    the technique can be improved. Some of these are
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-step reasonin**g*—RAG struggles with queries needing multi-hop retrieval
    (e.g., “What nationality is the performer of song XXX?”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*General querie**s*—Vague or broad questions are hard to retrieve relevant
    chunks for (e.g., “What does the group think about XXX?”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Complex or long querie**s*—Complex queries challenge the retriever’s understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implicit querie**s*—Questions requiring comprehensive context understanding
    can’t be addressed by RAG alone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have come a long way in our discussion on RAG. This chapter provided an
    exhaustive summary of the contents of this book, from the benefit of RAG to the
    best practices in building RAG systems. At the risk of repetition, RAG is an important
    and evolving technique in the field of generative AI. I hope you had a good time
    reading this book. I’ll leave you with the following closing thoughts:'
  prefs: []
  type: TYPE_NORMAL
- en: Remember to remain familiar with the principles of contextual AI powered by
    RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have faith in your ability to build complex RAG systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always bear in mind the development challenges and strategies to overcome them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the ethical and legal concerns around generative AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be on top of the rapidly changing trends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG development framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The RAG development framework provides a structured approach to building, deploying,
    and maintaining retrieval-augmented generation systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It addresses the complexity of RAG systems by incorporating six iterative and
    cyclic stages: initiation, design, development, evaluation, deployment, and maintenance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The framework emphasizes both the technical and operational aspects of RAG system
    development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG development framework stages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Initiation stage**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focuses on understanding the problem statement, aligning stakeholders, and gathering
    requirements.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Emphasizes use case identification and assessing the need for RAG, using tools
    like use case evaluation cards.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Involves requirements gathering across business, functional, and non-functional
    needs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Concludes with drafting a high-level architecture diagram for alignment and
    strategic decision-making.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design stage**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforms high-level architecture into detailed pipeline designs for indexing
    and generation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporates choices around chunking, embeddings, and retrieval strategies.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Addresses additional considerations such as guardrails, caching, security, and
    deployment strategies.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development stage**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements modular RAG pipelines, enabling flexibility, scalability, and maintainability.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Activities include training/fine-tuning models, creating independent modules
    (e.g., chunking, retrieval, generation), and building orchestration layers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation stage**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validates RAG system components and overall performance using metrics such as
    context relevance, faithfulness, precision, recall, latency, and cost per query.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Employs ground truth datasets for benchmarking and optimization.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment stage**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Includes deployment strategies like blue-green, canary, rolling, and A/B testing
    to ensure smooth transitions and minimal disruption.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Emphasizes real-time user feedback and system scalability.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintenance stage**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensures system reliability through continuous monitoring of component metrics,
    user behavior, and performance metrics.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapts to evolving use cases, technological advancements, and user feedback.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in RAG development
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modular design improves adaptability and ease of updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ground truth datasets are essential for accurate evaluation and fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment strategies should align with system criticality, scale, and risk
    tolerance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularly monitor for changes in user behavior, data, and performance to maintain
    reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideas for further exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**RAG vs. fine-tuning**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG complements fine-tuning by enhancing non-parametric memory, while fine-tuning
    adapts parametric memory for style, tonality, and vocabulary.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases may benefit from hybrid approaches, depending on specific needs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-context windows in LLMs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advances in LLMs (e.g., 200k+ token contexts) can reduce reliance on chunking
    and retrieval for smaller knowledge bases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid models such as SELF-ROUTE combine RAG with long-context processing to
    optimize cost and accuracy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managed solutions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services such as Amazon Bedrock, Azure AI Search, and Google Vertex AI RAG Engine
    offer prebuilt RAG pipelines, simplifying deployment and reducing development
    effort.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling difficult queries**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-step reasoning, general queries, and implicit questions remain challenges
    for RAG systems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
