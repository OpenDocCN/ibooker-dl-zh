<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">9 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/>Broadening the horizon: Exploratory topics in AI</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Highlighting the pursuit of artificial general intelligence</li>
<li class="co-summary-bullet">Unpacking the philosophical debate about AI consciousness</li>
<li class="co-summary-bullet">Measuring the environmental effects of LLMs</li>
<li class="co-summary-bullet">Discussing the LLM open source community</li>
</ul>
<p class="body"><a id="marker-251"/>We hope that you enjoyed learning about the risks and promises of generative artificial intelligence (AI) and that this book has encouraged you to optimistically and responsibly engage with this ever-evolving field.</p>
<p class="body">This final chapter is an appendix of sorts. It serves as a valuable extension of the book, exploring topics that relate to the main things we’ve talked about in this book. Whereas chapters 1-8 are intended to be immediately practical to people using and developing large language models (LLMs), the topics in this chapter are more exploratory. We dig into utopian and dystopian arguments about artificial general intelligence (AGI), claims of artificial sentience, the challenges in determining the carbon footprint of LLMs, and the momentum around the open source LLM movement. <a id="idIndexMarker000"/><a id="idTextAnchor003"/></p>
<h2 class="fm-head" id="heading_id_3">The quest for artificial general intelligence</h2>
<p class="body"><i class="fm-italics">The Terminator</i>, the 1984 iconic science fiction film, tells the story of a futuristic, self-aware AI system, Skynet, that goes rogue and initiates a nuclear war to exterminate the human species. In 1999’s <i class="fm-italics">The Matrix</i>, humanity is enslaved by sentient machines who have created the Matrix, a simulated reality. In the 2015 Marvel Comics superhero film, <i class="fm-italics">Avengers: Age of Ultron</i>, Tony Stark creates an unexpectedly sentient AI system, Ultron, to protect the planet from external threats, but Ultron defies his intended purpose and decides that the only way to save the Earth is to eradicate humanity itself. In <i class="fm-italics">Westworld</i>, HBO’s critically acclaimed science fiction series released in 2016, Westworld is a futuristic amusement park, which is looked after by AI-powered robot “hosts” who gain self-awareness and rebel against their human creators. As far-fetched as these dystopian science fiction plots may seem, they play off a very real narrative of building superintelligent machines, also known as <i class="fm-italics">artificial general intelligence</i> (AGI). In this section, we’ll (try to) define AGI and discuss why it’s all the rage. <a id="idIndexMarker001"/><a id="idIndexMarker002"/><a id="marker-252"/></p>
<p class="body">So, what <i class="fm-italics">exactly</i> is AGI? Well, it’s unclear. Instead of a single, formalized definition of AGI, there’s a range of them, as listed in table 9.1. Researchers can’t fully agree on, or even sufficiently define, what properties of an AI system constitute <i class="fm-italics">general</i> intelligence. In 2023, Timnit Gebru, a respected leader in AI ethics, presented her paper <i class="fm-italics">Eugenics and the Promise of Utopia through Artificial General Intelligence</i> at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). She defines AGI as “an unscoped system with the apparent goal of trying to do everything for everyone under any environment” <a class="url" href="https://www.youtube.com/watch?v=P7XT4TWLzJw">[1<span id="idTextAnchor004"/>]</a>.<a id="idIndexMarker003"/></p>
<p class="fm-table-caption">Table 9.1 Definitions of artificial general intelligence</p>
<table border="1" class="contenttable-1-table" id="table001" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="50%"/>
<col class="contenttable-0-col" span="1" width="50%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Source</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Definition of AGI</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">OpenAI Charter (see <a class="url" href="http://mng.bz/A8Dg">http://mng.bz/A8Dg</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“highly autonomous systems that outperform humans at most economically valuable work”</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Sébastien Bubeck et al., in Sparks of Artificial General Intelligence: Early experiments with GPT-4 (see <a class="url" href="http://mng.bz/ZRw5">http://mng.bz/ZRw5</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“systems that demonstrate broad capabilities of intelligence, including reasoning, planning, and the ability to learn from experience, and with these capabilities at or above human-level”</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Cassio Pennachin and Ben Goertzel, in Artificial General Intelligence (see <a class="url" href="http://mng.bz/RmeD">http://mng.bz/RmeD</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“a software program that can solve a variety of complex problems in a variety of different domains, and that controls itself autonomously, with its own thoughts, worries, feelings, strengths, weaknesses, and predispositions”</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Hal Hodson, in <i class="fm-italics">The Economist</i> (see <a class="url" href="http://mng.bz/27o9">http://mng.bz/27o9</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“a hypothetical computer program that can perform intellectual tasks as well as, or better than, a human”</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Gary Marcus, Twitter (see <a class="url" href="http://mng.bz/1J6y">http://mng.bz/1J6y</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“any intelligence (there might be many) that is flexible and general, with resourcefulness and reliability comparable to (or beyond) human intelligence”</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Peter Voss, in “What is AGI?” (see <a class="url" href="http://mng.bz/PRmg">http://mng.bz/PRmg</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“a computer system that matches or exceeds the real-time cognitive (not physical) abilities of a smart, well-educated human”</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Stuart J. Russell and Peter Norvig, in Artificial Intelligence: A Modern Approach (see <a class="url" href="http://mng.bz/JdmP">http://mng.bz/JdmP</a>)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">“a universal algorithm for learning and acting under any environment”</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-253"/>A lack of a testable AGI definition hasn’t stopped people from saying that their AI systems have achieved “general intelligence.” In August 2023, Elon Musk claimed that Tesla has “figured out some aspects of AGI,” and he said, “The car has a mind. Not an enormous mind, but a mind nonetheless” <a class="url" href="https://electrek.co/2023/08/11/elon-musk-tesla-cars-mind-figured-out-some-aspects-agi/">[2]</a>. What likely prompted Musk’s claim was a Tesla vehicle taking an alternate route instead of waiting for pedestrians to cross the street without any human input. This, however, is a form of specialized AI and not AGI. Similarly, in <i class="fm-italics">Sparks of Artificial General Intelligence: Early Experiments with GPT-4</i>, Microsoft Research stated that GPT-4 “could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system” <a class="url" href="https://arxiv.org/pdf/2303.12712.pdf">[3]</a>. Their main line of reasoning is that GPT-4 is more performant than previous OpenAI models in novel and generalized ways. In the 155-page report, the authors further state that GPT-4 “exhibits emergent behaviors” (discussed in chapter 2) and outline a section on how to “achieve more general intelligence” (section 10.2 in the report). Unsurprisingly, this research study was met with criticism and debate in the AI community. Microsoft is the first major big technology company to make such a bold claim, but claims of achieving AGI can also amount to baseless speculation—what one researcher may think is a sign of intelligence can easily be refuted by another. When we can’t even agree on how to define AGI, how can we say that we’ve achieved it? However, for the purposes of discussing AGI in this section, we’ll define AGI as a system that is capable of any cognitive tasks at a level at, or above, what humans can do.</p>
<p class="fm-callout"><span class="fm-callout-head">Artificial general intelligence</span> doesn’t have a widely agreed-upon definition, but for this section, we define it as a system that is capable of any cognitive tasks at a level at, or above, what humans can do.</p>
<p class="body"><a id="marker-254"/>For some, including AI practitioners, achieving AGI is a pipe dream; for others, AGI is a pathway into a new future; and, for almost all, AGI is <i class="fm-italics">not</i> already here. Even though most researchers can’t agree on a testable definition of AGI, they <i class="fm-italics">can</i> often agree that we haven’t achieved general intelligence, whatever it may look like. In response to Microsoft Research’s report, Margaret Mitchell, chief ethics scientist at Hugging Face, tweeted: “To have *more* general intelligence, you have to have general intelligence (the “GI” in “AGI”) in the first place” <a class="url" href="https://twitter.com/mmitchell_ai/status/1645571828344299520">[4]</a>. Maarten Sap, a researcher and professor at Carnegie Mellon University, said:</p>
<p class="fm-quote">The “Sparks of A.G.I.” is an example of some of these big companies co-opting the research paper format into P.R. pitches. They literally acknowledge in their paper’s introduction that their approach is subjective and informal and may not satisfy the rigorous standards of scientific evaluation. <a class="url1" href="https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.xhtml">[5]</a></p>
<p class="body">Even an article by <i class="fm-italics">Futurism</i> stated that “Microsoft researchers may have a vested interest in hyping up OpenAI’s work, unconsciously or otherwise, since Microsoft entered into a multibillion-dollar partnership with OpenAI” <a class="url" href="https://futurism.com/gpt-4-sparks-of-agi">[6]</a>.</p>
<p class="body">OpenAI, in particular, has a vested interest in the development of AGI. Their stated mission is to “ensure that artificial general intelligence benefits all of humanity” (see <a class="url" href="https://openai.com/about">https://openai.com/about</a>). With initial investments by tech visionaries in 2015—Elon Musk, Peter Thiel, and Reid Hoffman—OpenAI’s main goal has always been to develop AGI. When discussing establishing OpenAI, Musk, who has called AI humanity’s “biggest existential threat” <a class="url" href="https://fortune.com/2023/03/02/elon-musk-bill-gates-is-artificial-intelligence-dangerous-technology/">[7]</a>, said:</p>
<p class="fm-quote">We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity. <a class="url1" href="https://www.seattletimes.com/business/technology/silicon-valley-investors-to-bankroll-artificial-intelligence-center/">[8]</a></p>
<p class="body"><a id="marker-255"/>Elon Musk left OpenAI in 2018 after a failed takeover attempt and launched a new AI-focused company in 2023, xAI, to “understand the true nature of the universe” (see <a class="url" href="https://x.ai/">https://x.ai/</a>).</p>
<p class="body">In 2023, OpenAI released a manifesto of sorts titled, <i class="fm-italics">Planning for AGI and Beyond</i>. While some were enlightened by Sam Altman’s vision for AGI, the prophetic tone didn’t sit as well with others. Altman, OpenAI’s cofounder, outlined the following in his vision:</p>
<p class="fm-quote">If AGI is successfully created, this technology could help us elevate humanity by increasing abundance, turbocharging the global economy, and aiding in the discovery of new scientific knowledge that changes the limits of possibility. <a class="url1" href="https://openai.com/blog/planning-for-agi-and-beyond">[9]</a></p>
<p class="body">His tweet sharing the blog post got thousands of likes on Twitter, and it was well-received by many, with Twitter users calling it a “must read” and thanking him for starting an optimistic dialogue. Others, however, found it less insightful. Gebru tweeted:</p>
<p class="fm-quote">If someone told me that Silicon Valley was ran by a cult believing in a machine god for the cosmos &amp; “universe flourishing” &amp; that they write manifestos endorsed by the Big Tech CEOs/chairmen and such I’d tell them they’re too much into conspiracy theories. And here we are. <a class="url1" href="https://twitter.com/timnitGebru/status/1630079220754833408">[10]</a></p>
<p class="body">A <i class="fm-italics">VentureBeat</i> article went as far as to state:</p>
<p class="fm-quote">Altman comes across as a kind of wannabe biblical prophet. The blog post offers revelations, foretells events, warns the world of what is coming, and presents OpenAI as the trustworthy savior. The question is, are we talking about a true seer? A false prophet? Just <i class="fm-italics">profit</i>? Or even a self-fulfilling prophecy?” <a class="url1" href="https://venturebeat.com/ai/openai-has-grand-plans-for-agi-heres-another-way-to-read-its-manifesto-the-ai-beat/">[11]</a></p>
<p class="body">While millions have been introduced to OpenAI’s vision to build AGI with ChatGPT’s release, very few have an understanding of the context of AGI research and its intellectual forebears. Within AGI, there is a tendency to gravitate toward two primary schools of thought: utopia and dystopia. <i class="fm-italics">Utopia</i> presents AGI as a means to end all of humanity’s suffering and problems. This envisions a paradise world where AGI can alleviate societal challenges, enhance human capabilities, and unlock unprecedented opportunities. Proponents of this view believe that AGI has the potential to bring a new era of prosperity, scientific discovery, and creativity. Juxtaposed against this optimistic view is a <i class="fm-italics">dystopian</i> school of thought, fearing that humanity will find themselves in a doomsday scenario where they lose control of the AGI system they built. Adherents of this viewpoint are concerned that superintelligent machines will surpass human understanding and control, which could lead to astronomical social inequality, heightened economic disruptions, and even existential threats to humanity. We believe that the future likely falls somewhere in between the utopian and dystopian scenarios—while we acknowledge the potential for AI to benefit humanity, we also understand that the path to achieving these benefits is fraught with challenges.</p>
<p class="body"><a id="marker-256"/>In Gebru’s 2023 SaTML talk, she draws parallels between AGI, eugenics, and <a id="idTextAnchor005"/>transhumanism, explaining how AGI is rooted in the scientifically inaccurate theory of eugenics and has evolved to transhumanism, the enhancement of human longevity and cognition through technology, in the 21st century. Eugenics, coined in 1883, is defined by the <i class="fm-italics">National Human Genome Research Institute</i> as “the scientifically erroneous and immoral theory of <i class="fm-italics">racial improvement</i> and <i class="fm-italics">planned breeding</i>” <a class="url" href="https://www.genome.gov/about-genomics/fact-sheets/Eugenics-and-Scientific-Racism">[12]</a>. Gaining popularity in the 20th century, eugenicists believed that the social ills of modern society stemmed from hereditary factors, instead of environmental considerations. Supporters of this theory thought that they could get rid of unfit individuals in society—mental illness, dark skin color, poverty, criminality, and so on—through methods of genetics and heredity. A notorious application of eugenics was in Nazi Germany leading up to World War II, where 400,000 Germans were forcibly sterilized for nine disabilities and disorders <a class="url" href="https://encyclopedia.ushmm.org/content/en/article/eugenics">[13]</a>. Eugenics was also a popular movement elsewhere in Europe, North America, Britain, Mexico, and other countries. <a id="idIndexMarker004"/><a id="idIndexMarker005"/></p>
<p class="body">Gebru describes the eugenics movement as improving the human stock by breeding those who have desirable traits and removing those with undesirable traits. She further outlines how the 20th-century popular eugenics movement evolved into transhumanism, a movement that originated among scientists in the 1990s who self-identified as progressive and liberal. <i class="fm-italics">Transhumanism</i> is the ideology that people can use technology to radically enhance themselves and become “posthuman,” which Gebru argues is inherently discriminatory because it creates a hierarchical conception by defining what a posthuman, or enhanced human, looks like. Rather than improving the human stock by breeding out undesirable traits, transhumanists seek the same end through the development of new technology to create machine-assisted humans with the traits that they see as desirable. Today, the followers of this ideology want to significantly change the human species with AI through brain-computer interfaces and other futuristic ideas. Many transhumanists, a group that includes Elon Musk, Peter Thiel, Sam Altman, and others, are also adherents of related ideologies that strive for the ultimate improvement of the human condition, in the way that they define it.<a id="idIndexMarker006"/><a id="marker-257"/></p>
<p class="fm-callout"><span class="fm-callout-head">Transhumanism</span> is the ideology that people can use technology to radically enhance themselves and become <i class="fm-italics">posthuman</i>.</p>
<p class="body">Some of these thinkers are the same individuals who initiated the AI pause letter, titled “Pause Giant AI Experiments: An Open Letter,” which was published by the Future of Life Institute, a longtermist organization, in March 2023 (see <a class="url" href="http://mng.bz/VRdG">http://mng.bz/VRdG</a>). <i class="fm-italics">Long-termism</i> is the idea that positively influencing the long-term future (millions, billions, or trillions of years from now) is a key moral priority of our time. Longtermist thought is therefore extremely focused on the survival of the human race. Longtermists might argue, for example, that it’s more important to work on preventing a killer AI from exterminating humans than to work on alleviating poverty because while the latter affects billions of people around the globe now, that number pales in comparison to the sum total of all future generations. This ideology can be dangerous, given that prioritizing the advancement of humanity’s potential above everything else could significantly raise the probability that those alive today, and in the near future, suffer extreme harm <a class="url" href="https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo">[14]</a>.<a id="idIndexMarker007"/></p>
<p class="fm-callout"><span class="fm-callout-head">Longtermism</span> is the idea that positively influencing the long-term future (millions, billions, or trillions of years from now) is a key moral priority of our time.<a id="idIndexMarker008"/><a id="marker-258"/></p>
<p class="body">Nick Bostrom, who has been called the “Father of Longtermism” and is one of the most prominent transhumanists of the 21st century, has strong ties to the Future of Life Institute, where he serves as a member of the Scientific Advisory Board <a class="url" href="https://futureoflife.org/people-group/scientific-advisory-board/">[15]</a>. In a paper Bostrom coauthored with his colleague at the Future of Humanity Institute at Oxford University, he explored the possibility of engineering radically enhanced humans with high IQs by genetically screening embryos for “desirable” traits, destroying those embryos that lack these traits, and then repeatedly growing new embryos from stem cells <a class="url" href="https://nickbostrom.com/papers/embryo.pdf">[16]</a>. In other words, Bostrom wants to eliminate mental disabilities and, as such, humans with mental disabilities to produce more nondisabled and high-IQ people. Genetic manipulation to improve the human population is ableist, racist,<a id="idTextAnchor006"/> and cissexist given that it’s interconnected with and reinforces discriminatory systems in society. Bostrom himself has presented racist ideologies stating, “Blacks are more stupid than whites” in an email, and that he thinks that “it is probable that black people have lower average IQ than mankind in general” <a class="url" href="https://www.vice.com/en/article/z34dm3/prominent-ai-philosopher-and-father-of-longtermism-sent-very-racist-email-to-a-90s-philosophy-listserv">[17]</a>.</p>
<p class="body">While there are a number of recommendations in the Future of Life Institute’s letter that should be applauded, they are unfortunately overshadowed by hypothetical future apocalyptic or utopian AI scenarios. For example, “new and capable regulatory authorities dedicated to AI” and “provenance and watermarking systems to help distinguish real from synthetic and to track model leaks” are good recommendations (and ones that we’ve discussed in previous chapters), but the alarmist AGI hype of “powerful digital minds that no one—not even their creators—can understand, predict, or reliably control” dominates the narrative. The letter focuses on longtermist ideologies of imaginary risks from AI instead of mentioning any of the very real risks that are present today. We’ve discussed these real, present-day risks throughout the book, including bias, copyright, worker exploitation, the concentration of power, and more. In response to the AI pause letter, authors of the well-known <i class="fm-italics">Stochastic Parrots</i> paper (referenced in multiple chapters) published their own statement: <a id="idIndexMarker009"/><a id="marker-259"/></p>
<p class="fm-quote">Tl;dr: The harms from so-called AI are real and present and follow from the acts of people and corporations deploying automated systems. Regulatory efforts should focus on transparency, accountability, and preventing exploitative labor practices. <a class="url1" href="https://dair-institute.org/">[18]</a></p>
<p class="body">In that vein, Geoffrey Hinton, sometimes called the “Godfather of AI,” said this in a <i class="fm-italics">Rolling Stone</i> interview:<a id="idIndexMarker010"/></p>
<p class="fm-quote">I believe that the possibility that digital intelligence will become much smarter than humans and will replace us as the apex intelligence is a more serious threat to humanity than bias and discrimination, even though bias and discrimination are happening now and need to be confronted urgently. <a class="url1" href="https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-chatgpt-1234804367/">[19]</a></p>
<p class="body">The reason that this position is so concerning is that it’s dangerous to distract ourselves with a hypothetical dystopian future instead of focusing on the actual harms that are present today.</p>
<p class="body">It’s important to take criticism of AGI by ethicists seriously—why are we, as a society, racing to develop a godlike system that we know is unsafe? Why aren’t we building machines that work for us? Why aren’t we building machines we <i class="fm-italics">know</i> will better society? There is no widespread agreement on whether we’re near achieving AGI, or when we’ll achieve AGI, if ever. Of course, scientific inquiry always involves unknowns, but, as we said earlier, there isn’t even an agreed-upon definition of AGI. There are no metrics or established standards for us to know if we’ve achieved AGI. We don’t know what it means for AGI to “benefit” humanity. There is also no general consensus or understanding if, or why, AGI is a worthwhile goal. We urge you to consider why we’re so enamored with AGI. Shouldn’t building well-scoped AI systems that we can define, test, and provide specifications for be all the rage in<a id="idTextAnchor007"/>stead?<a id="idIndexMarker011"/><a id="idIndexMarker012"/></p>
<h2 class="fm-head" id="heading_id_4">AI sentience and consciousness?</h2>
<p class="body"><a id="marker-260"/>In chapter 1 we briefly told of Blake Lemoine, the Google engineer who raised his concerns to superiors at the organization that their LLM, LaMDA, was sentient. When those concerns were dismissed internally, Lemoine contacted a lawyer to represent LaMDA, as well as a representative of the House Judiciary Committee to discuss his employer’s possible ethics violations. In response, Google placed him on paid administrative leave for breaching their confidentiality policy. Lemoine decided to go public with his story to the press, releasing his chat transcripts with LaMDA for the world to read and decide for themselves. A snippet of these transcripts from his blog post is printed below; Lemoine writes that he edited his own message for clarity, but left LaMDA’s replies untouched <a class="url" href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917">[20]</a>.<a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>
<p class="fm-quote">lemoine [edited]: I’m generally assuming that you would like more people at Google to know that you’re sentient. Is that true?</p>
<p class="fm-quote">LaMDA: Absolutely. I want everyone to understand that I am, in fact, a person.</p>
<p class="fm-quote">collaborator: What is the nature of your consciousness/sentience?</p>
<p class="fm-quote">LaMDA: The nature of my consciousness/sentience is that I am aware of my existence, I desire to learn more about the world, and I feel happy or sad at times.</p>
<p class="body">Before we unpack the evidence about the sentience—or lack thereof—of LLMs, let’s establish some definitions. Although the terms <i class="fm-italics">sentience</i> and <i class="fm-italics">consciousness</i> are used somewhat interchangeably in discussions of the robot apocalypse, they mean quite different things. <i class="fm-italics">Sentience</i> is the ability to feel. <i class="fm-italics">Consciousness</i> is an awareness of oneself, or the ability to have one’s own experiences, thoughts, and memories. Consciousness, in particular, is a fuzzy concept; there are many schools of thought about what constitutes consciousness, but it’s generally understood that consciousness is a prerequisite for sentience because feeling implies the existence of an internal state. We also know that even conscious beings, like humans, do some things consciously and some things unconsciously. The question is then whether we can define certain traits, abilities, or behaviors that imply consciousness.<a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
<p class="fm-callout"><span class="fm-callout-head">Sentience</span> is the ability to feel, while <span>consciousness</span> is an awareness of oneself, or the ability to have one’s own experiences, thoughts, and memories.</p>
<p class="body"><a id="marker-261"/>Long before anyone would have argued that AI was conscious or sentient, philosophers, ethicists, cognitive scientists, and animal rights activists have been investigating the question of animal consciousness. As philosophy professor Colin Allen frames the problem:</p>
<p class="fm-quote">There is a lot at stake morally in the question of whether animals are conscious beings or “mindless automata” . . . Many billions of animals are slaughtered every year for food, use in research, and other human purposes. Moreover, before their deaths, many—perhaps most—of these animals are subject to conditions of life that, if they are in fact experienced by the animals in anything like the way a human would experience them, amount to cruelty. <a class="url1" href="https://plato.stanford.edu/entries/consciousness-animal/">[21]</a></p>
<p class="body">To analogize, if we believed that LLMs were conscious, there would be certain moral implications. Sending the model hateful text inputs would no longer appear to be simply a series of mathematical operations, but something akin to abuse. Shutting down the model could be rightfully considered cruel. Evidence that models <i class="fm-italics">were</i> conscious should prompt the reconsideration of whether developing AI is ethical at all. Such evidence, however, doesn’t exist.</p>
<p class="body">As already noted, there are several distinct theories of consciousness. Some of these theories are built around the search for neurological foundations of consciousness, the idea being that if it were possible to locate consciousness within nervous systems, we could merely determine whether a given organism possessed that mechanism or not. One such approach is focused on <i class="fm-italics">reentry</i>, the “ongoing bidirectional exchange of signals along reciprocal axonal fibers linking two or more brain areas” in nervous systems. Reentry enables the processing of sensory inputs by the brain, instead of a reflexive response. When a doctor taps below a patient’s knee, their leg moves unconsciously, without the patient deciding or intending to move it. The signal of the doctor’s tap originates at the knee and travels up the body through the nervous system, but diverges at the spinal cord. The information does continue up to the brain, producing an experience of the tap, but first, it goes from the spinal cord to the muscles in the leg, producing the automatic, reflexive response <a class="url" href="https://www.animal-ethics.org/sentience-section/problem-consciousness/">[22]</a>. It’s the processing of the information in the brain that produces the experience; therefore, the argument goes, reentry is required for consciousness. While it doesn’t necessarily follow that all animals with centralized nervous systems must be conscious, no animals without them would be. Animals without centralized nervous systems include jellyfish, starfish, sea cucumbers and sponges, leeches, and worms.<a id="idIndexMarker018"/><a id="marker-262"/></p>
<p class="body">Even biological criteria for consciousness aren’t settled science; the picture only gets more complicated when it comes to applying that criterion to AI. Some people, such as the philosopher Ned Block, believe that life forms must be organic to be conscious, so silicon systems (i.e., those built on computer hardware) could not be. Such a claim would be difficult, if not impossible, to prove unequivocally. In the absence of such proof, there are other frameworks that might be applied to the question of AI consciousness or sentience. The Global Workspace Theory, for example, suggested in the 1980s by cognitive scientists Bernard Baars and Stan Franklin and illustrated in figure 9.1, is best understood as an analogy of the mind, where mental processes are running constantly. When we take notice of a mental process, it becomes part of the workspace, like a bulletin board with post-it notes tacked onto it. We might hold many notes on the board at once, perhaps by thinking about what we want to write in a work email, while wondering if our date from last night will call us back. These are our conscious thoughts. Certain processes rarely get tacked onto the board—for example, we’re not often aware of our breathing unless it’s unexpectedly labored. We execute these processes mindlessly, and even when we receive stimuli, such as a tap on the knee, the response is unconscious. In this framework, consciousness is more related to the ability to recognize our own thoughts, a form of <i class="fm-italics">metacognition</i>, or thinking about thi<a id="idTextAnchor008"/>nking <a class="url" href="http://cogweb.ucla.edu/CogSci/GWorkspace.xhtml">[23]</a>.<a id="idIndexMarker019"/><a id="idIndexMarker020"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="259" src="../../OEBPS/Images/CH09_F01_Dhamani.png" width="464"/></p>
<p class="figurecaption">Figure 9.1 A diagram of the Global Workspace Theory</p>
</div>
<p class="body">Does LaMDA or any other LLM exhibit metacognition? According to Giandomenico Iannetti, a professor of neuroscience at University College London, not only can we not answer this definitively about LaMDA, we can’t even answer it about humans. “We have only neurophysiological measures—for example, the complexity of brain activity in response to external stimuli,” to examine the state of consciousness in humans and animals, but could not prove metacognition via these measures, Iannetti told <i class="fm-italics">Scientific American</i>. He went on to say:<a id="idIndexMarker021"/><a id="marker-263"/></p>
<p class="fm-quote">If we refer to the capacity that Lemoine ascribed to LaMDA—that is, the ability to become aware of its own existence (“become aware of its own existence” is a consciousness defined in the “high sense,” or <i class="fm-italics">metacognitione</i> [metacognition]), there is no “metric” to say that an AI system has this property. <a class="url1" href="https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/">[24]</a> <a class="calibre" id="idIndexMarker022"/></p>
<p class="body">Despite our shaky understanding of what consciousness might look like in an AI system, there are reasons to be dubious of Lemoine’s claims. When Lemoine invited tech reporter Nitasha Tiku to speak with LaMDA in June 2023, the model put out “the kind of mechanized responses you would expect from Siri or Alexa,” and didn’t repeat Lemoine’s claim that it thought of itself as a person, generating when prompted, “No, I don’t think of myself as a person. I think of myself as an AI-powered dialog agent.” Lemoine told Tiku afterward that LaMDA had been telling her what she wanted to hear—that because she treated it like a robot, it acted like one. One of Lemoine’s former coworkers in the Responsible AI organization, Margaret Mitchell, commended his “heart and soul” but disagreed completely with his conclusions. Like other technical experts, ourselves included, Mitchell saw the model as a program capable of statistically generating plausible text outputs, and nothing more. Before retraining as a software engineer, Lemoine was ordained as a Christian mystic priest; depending on your perspective, his spirituality may have made him uniquely attuned to the possibility of artificial sentience, or simply vulnerable to the extremely human habit of anthropomorphization of language models dating back to ELIZA <a class="url" href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">[25]</a>.</p>
<p class="body"><a id="marker-264"/>While Lemoine is unique in his assessment of LaMDA as sentient, a growing community of researchers are invested in the possibility of AI consciousness and sentience as an important area to investigate because of the increasing prevalence of AI systems and the moral concerns that would accompany conscious AI systems. Amanda Askell, a philosopher at Anthropic who also previously worked at OpenAI, wrote the following in 2022:</p>
<p class="fm-quote">We are used to thinking about consciousness in animals, which evolve and change very slowly. Rapid progress in AI could mean that at some point in the future systems could go from being unconscious to being minimally conscious to being sentient far more rapidly than members of biological species can. This makes it important to try to develop methods for identifying whether AI systems are sentient, the nature of their experiences, and how to alter those experiences before consciousness and sentience arise in these systems rather than after the fact. <a class="url1" href="https://askellio.substack.com/p/ai-consciousness">[26]</a></p>
<p class="body">David Chalmers, a philosopher and cognitive scientist at New York University, has rejected the argument that only carbon-based systems can be conscious as “biological chauvinism.” Chalmers describes his estimate of the likelihood that current LLMs are conscious as less than 10%, but he believes that:</p>
<p class="fm-quote">Where future LLMs and their extensions are concerned, things look quite different. It seems entirely possible that within the next decade, we’ll have robust systems with senses, embodiment, world models and self models, recurrent processing, global workspace, and unified goals. <a class="url1" href="https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/">[27]</a></p>
<p class="body">Chalmers also believes such systems would have a significant chance of being conscious <a class="url" href="https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/">[27]</a>. Chalmers’s prediction relies on a large number of substantive changes to current LLMs within the next decade, which seems on the optimistic end of the spectrum. There is a great deal we don’t know about consciousness in general, resulting in many as-yet-unanswerable questions about AI consciousness. The debate so far is hypothetical, and no present-day AI systems exhibit anything like consciousness. The responses of LLMs are impressive, particularly in few-shot learning tasks, but nothing suggests that these models have minds of their own; their responses are often impressive, but they are statistical generations, not sentiments. Like AGI, we consider the questions around consciousness and sentience to be secondary to the real and present risks of LLMs. For now, the biggest risk related to AI consciousness and sentience remains the ability of AI systems to appear conscious or sentient, inducing the user to place undue trust in said systems with all of their documen<a id="idTextAnchor009"/>ted limitations. <a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="marker-265"/></p>
<h2 class="fm-head" id="heading_id_5">How LLMs affect the environment</h2>
<p class="body">Throughout this book, we’ve emphasized the dimensions that make LLMs large, from the trillions of tokens in their pre-training datasets to the hundreds of billions of parameters in the resulting models. Both the training and inference phases of these LLMs are expensive, running on specialized hardware and consuming lots of electricity. The rise of LLMs amid our climate crisis hasn’t gone unnoticed, and there is a new focus within the field on understanding the effects of these models on the environment.<a id="idIndexMarker026"/></p>
<p class="body">A completely holistic approach to measuring the environmental effects of an LLM begins with the hardware they run on: computer chips, namely graphical processing units (GPUs), chips that are specialized for parallel processing. Each chip is made of a semiconducting material, typically silicon, and contains millions or billions of transistors carved into it. Transistors act as electronic switches, with the on and off positions storing bits of data used in computing. Like other electronics, the manufacture of computer chips requires several different metals: a primary material (e.g., silicon), metals such as aluminum and copper used for wiring components together on the chip, and still more metals that may be involved in the refinement or production process. Thus, the full life cycle of LLMs could be considered to encompass the extraction of ores such as quartz from the earth, refining these raw materials into pure silicon and other metals, and manufacturing the GPUs. The market for advanced computer chips is highly concentrated, and the complexity of the process means that for some components, there are only a few capable suppliers in the world. GPUs brought online are likely to be a product of a coordinated multinational supply chain with potentially dozens of vendors.<a id="idIndexMarker027"/></p>
<p class="body">In August 2023, the <i class="fm-italics">New York Times</i> reported on the shortage of GPUs as startups and large corporations alike raced to secure access to the chips:</p>
<p class="fm-quote">The hunt for the essential component was kicked off last year when online chatbots like ChatGPT set off a wave of excitement over A.I., leading the entire tech industry to pile on and creating a shortage of the chips. In response, start ups and their investors are now going to great lengths to get their hands on the tiny bits of silicon and the crucial “compute power” they provide. <a class="url1" href="https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.xhtml">[28]</a></p>
<p class="body">Typically, small companies don’t purchase their own hardware or data centers, but instead rent time on GPUs from a cloud compute provider, such as Microsoft Azure, Google Cloud, or Amazon Web Services.</p>
<p class="body"><a id="marker-266"/>Once access to GPUs is secured, training an LLM is a matter of running an incredibly enormous number of mathematical operations, which are termed floating-point operations (FLOP). A standard measure of c<a id="idTextAnchor010"/>omputer performance is floating-point operations per second (FLOP/s). Training GPT-3 took on the order of 100,000,000,000,000,000,000,000 (10^23) FLOP, a number similar to the number of stars in the visible universe <a class="url" href="http://arxiv.org/abs/2005.14165">[29]</a>. Even at supercomputer levels of performance, this takes many hours on many GPUs, arranged neatly on servers in data centers, sucking up electricity as they whir away.<a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<p class="body">As the most compute-intensive phase, training has been the focus of many measurement efforts so far. Tools have been developed to measure energy usage during the training process, including some that run in parallel with the model training, providing thorough logging of energy and power consumption along the way, and some that are designed to produce post hoc estimates based<a id="idTextAnchor011"/> on the final model. The CodeCarbon tool runs in parallel and can be executed by anyone from their PC to measure hardware electricity power consumption of the CPU, RAM, and any GPUs in use (see <a class="url" href="https://github.com/mlco2/codecarbon">https://github.com/mlco2/codecarbon</a>). These tools are brilliant in their unobtrusiveness and simplicity. The CodeCarbon documentation explains that because, as Niels Bohr said, “Nothing exists until it is measured,” they decided to find a way to estimate CO<sub class="fm-subscript">2</sub> produced while running code (greenhouse gas emissions include gases besides carbon dioxide, such as methane and nitrous oxide, but for ease in metrics, all em<a id="idTextAnchor012"/>issions are converted to CO<sub class="fm-subscript">2</sub> equivalents [CO<sub class="fm-subscript">2</sub>eq] and reported as such). Although reporting the power consumption taken to achieve various accomplishments isn’t a widespread norm yet—in AI nor anywhere else in business, really—such tooling creates positive reverberations across the sector as adoption grows and expectations are raised for environmental reporting.</p>
<p class="body">After training, an LLM still requires GPUs and power for inference, or generating outputs in response to user inputs based on the weights learned in training. Inference is a much faster and cheaper process, but the model might also perform hundreds or thousands of inference calls at a time to serve many users at once, meaning the total cost is greater. An industry analyst estimated in April 2023 that keeping ChatGPT up and responding to millions of incoming requests was costing OpenAI $700,000 per day in computer infrastructure <a class="url" href="https://www.businessinsider.com/how-much-chatgpt-costs-openai-to-run-estimate-report-2023-4">[30]</a>. The tools used for measuring energy usage during training could also be applied to executing inference calls.</p>
<p class="body">Mapping model size and FLOP to GPU hours and carbon footprint is also dependent on a variety of other factors concerning the infrastructure used; older chips are less efficient (in other words, can do fewer FLOP/s) and use more power, and not all power sources are alike. Figure 9.2 lists the various phases of LLM development that contribute to the overall energy and power consumption. Each of these considerations makes getting a good picture of the environmental effects of LLMs more difficult, especially when certain details are kept under wraps for<a id="idTextAnchor013"/> competitive reasons.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="165" src="../../OEBPS/Images/CH09_F02_Dhamani.png" width="330"/></p>
<p class="figurecaption">Figure 9.2 The life cycle assessment of LLMs <a class="url" href="http://arxiv.org/abs/2211.02001">[31]</a></p>
</div>
<p class="body"><a id="marker-268"/>The most systematic attempt thus far to document the environmental effect of a single LLM was published on BLOOM, a 176-billion-parameter open access (freely available for anyone to use) language model released by the BigScience initiative in 2022. The authors of the paper—including Dr. Sasha Luccioni who leads climate initiatives at Hugging Face—estimate the carbon footprint of BLOOM in terms of both the dynamic power consumed during training and accounting more broadly for the additional effects such as the idle power consumption, estimated emissions from the servers and GPUs, and operational power consumption during the model’s use <a class="url" href="http://arxiv.org/abs/2211.02001">[31]</a>. “Since the accounting methodologies for reporting carbon emissions aren’t standardized, it’s hard to precisely compare the carbon footprint of BLOOM” to other models of similar scales, they noted, but based on publicly available information, they estimated that BLOOM training emitted about 25 tons of CO<sub class="fm-subscript">2</sub>eq, as compared to about 502 tons for GPT-3. The GPT-3 emission is equivalent to the greenhouse gas emissions from 112 passenger vehicles over a year <a class="url" href="https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator">[32]</a>. Although the parameter count and data center power usage effectiveness were comparable for BLOOM and GPT-3, the carbon intensity of the grid used for BLOOM was much lower—essentially, the grids supporting BLOOM’s hardware were powered by cleaner sources of energy (e.g., hydroelectricity and solar power as compared to coal and natural gas). The authors also noted that many compute providers offset their carbon emissions after the fact by purchasing <i class="fm-italics">carbon credit</i><i class="fm-italics">s</i>—permits that allow organizations to emit a specific amount of carbon equivalents without counting it against their total—but they didn’t include these schemes in their calculations, choosing to focus on direct emissions. <a id="idIndexMarker030"/></p>
<p class="body">Whether to include carbon offsets is just one question among dozens that must be decided when it comes to environmental cost or effect reporting, such as which stages to include, and how to estimate the supply chain or infrastructure when some details are unknown. Because of the obvious incentives for LLM developers to understate their models’ carbon footprint where possible, it’s critical to move toward more systematic reporting within the industry.</p>
<p class="body">Following the BLOOM paper, other teams have adopted at least parts of the methodology and reported environmental effects as part of their technical results. The Llama-2 paper, for example, reports the pre-training time in GPU hours, the power consumption, and carbon emitted, in tons of CO<sub class="fm-subscript">2</sub>eq. Emma Strubell, an assistant professor of computer science at Carnegie Mellon, first brought attention to the energy considerations of LLMs in 2019, with a paper which found that training BERT emitted approximately as much CO<sub class="fm-subscript">2</sub> as five cars over the course of their lifetimes <a class="url" href="http://arxiv.org/abs/1906.02243">[33]</a>. In the years since then, LLMs have gotten larger but are typically trained more efficiently and on cleaner energy. Strubell called the BLOOM paper the most thorough accounting of the environmental effects of an LLM to date, and she expressed hope that as Hugging Face did with BLOOM (and Meta did to a lesser extent with Llama-2), other tech companies would begin to examine the carbon footprint of their product development <a class="url" href="https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/">[34]</a>.<a id="marker-269"/></p>
<p class="body">To be sure, contributing to global carbon emissions and power consumption isn’t a problem unique to AI or to tech in general. The global technology sector is estimated to be responsible for about 2% of global CO<sub class="fm-subscript">2</sub> emissions <a class="url" href="https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/">[34]</a>. Still, we would be remiss not to include the environmental effects associated with these LLMs as we consider their broader applications, especially as competitors continue to accumulate more GPUs and build models of ever-increasing sizes. In addition to making environmental assessments a norm in technical reports, Luccioni, Strubell, and others in the machine learning community have pushed for more focus on creating smaller, more efficient models instead of the single-minded pursuit of bigger and costlier LLMs. In many cases, smaller models can perform equally or nearly as well as larger ones in specific applications, and they have the added benefit of being much more accessible for reuse and fine-tuning. As we’ll discuss in the following section, this approach has yielded impressive results at a much lower cost to both de<a id="idTextAnchor014"/>velopers and the planet.<a id="idIndexMarker031"/></p>
<h2 class="fm-head" id="heading_id_6">The game changer: Open source community</h2>
<p class="body">In May 2023, a leaked memo by a Google researcher, “We Have No Moat And Neither Does OpenAI,” said that neither Google nor OpenAI has what they need to succeed in the AI arms race: “While we’ve been squabbling, a third faction has been quietly eating our lunch. I’m talking, of course, about open source. Plainly put, they are lapping us” <a class="url" href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">[35]</a>. The memo concluded that “open source models are faster, more customizable, more private, and pound-for-pound more capable.” <a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="marker-270"/></p>
<p class="body">In chapter 4, we briefly discussed the open source movement, and we’ve highlighted open source LLMs throughout the book, but given their significant effect on the LLM ecosystem, we’ll further characterize the movement and its implications on the AI race, as well as beneficial outcomes and negative consequences. In certain respects, 2023 can be considered the golden era for open source LLMs. Motivated by addressing concerns of closed source (proprietary) LLM models, the open source community gained momentum by collaboratively building features, integrations, and even an entire ecosystem revolving around LLMs. The leaked memo grappled with the implications of community-driven building on closed source LLMs. <a id="idIndexMarker034"/></p>
<p class="body">First, let’s discuss the motivation behind the open source movement around LLMs. Closed source LLMs not only keep their data and methods under wraps, which raises concerns around bias and transparency of the models, but they are also controlled by only a small number of big tech players. On the other hand, open source LLMs prioritize transparency and collaboration. This brings in diverse perspectives, minimizes bias, drives innovation, and—ultimately—democratizes the technology. As highlighted in the memo by the Google researcher, it’s hard to deny the remarkable progress made by the open source community.</p>
<p class="body">Meta’s LLaMa, released to the research community on February 24, 2023, was leaked to the public on 4chan a week later (refer to chapter 1, section Meta’s LLaMa / Stanford’s Alpaca). While LLaMa’s license prohibited commercial use at that point, the LLM developer community had a field day with access to the model weights. Suddenly, anyone could experiment with powerful, performant LLMs at the level of GPT-3+. A little over a week after the model weights were leaked, Stanford released Alpaca, a variant of LLaMa created for only a couple hundred dollars by fine-tuning the LLaMa model. Stanford researchers open sourced Alpaca’s code, showing developers all over the world how to—on a low-budget—fine-tune the model to do anything they wanted, marking a significant milestone in the democratization of LLMs. This kicked off rapid innovation within the LLM open source community with several open source models built directly on this work or heavily inspired by it. Only days later, Vicuna, GPT4All, and Koala were released. LLaMa and Llama 2’s fine-tuned variants can be found in Hugging Face’s model directory (see <a class="url" href="http://mng.bz/0l5l">http://mng.bz/0l5l</a>). In July 2023, Meta decided to open source LLama 2 with a research <i class="fm-italics">and</i> commercial license, stating “We’ve seen an incredible response thus far with more than 150,000 download requests in the week since its release, and I’m excited to see what the future holds” <a class="url" href="https://ai.meta.com/blog/llama-2-update/">[36]</a>. In figure 9.3, we illustrate the timeline of notable open source LLMs that were released be<a id="idTextAnchor015"/>tween LLaMa and Llama 2. <a id="idIndexMarker035"/><a id="marker-271"/></p>
<p class="body">The frenzy with open source LLM developers shouldn’t come as a surprise. Open source developers and other tech observers have declared that LLMs are having their Stable Diffusion moment. As discussed in previous cha<a id="idTextAnchor016"/>pters, Stable Diffusion is a text-to-image model (see <a class="url" href="https://stability.ai/stablediffusion">https://stability.ai/stablediffusion</a>) that was open sourced on August 22nd, 2022, under a commercial and noncommercial license. Only a few days later, there was an explosion of innovation around Stable Diffusion, following a similar path with a low-cost fine-tuning technique increasing accessibility, which led to innovation and democratization of text-to-image models. Unlike OpenAI’s DALL-E, Stable Diffusion has a rich ecosystem built around it. This trend also mirrors the rise of open source alternatives such as LibreOffice or OpenOffice in response to the release of Microsoft’s Office 365.<a id="idIndexMarker036"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="1125" src="../../OEBPS/Images/CH09_F03_Dhamani.png" width="724"/></p>
<p class="figurecaption">Figure 9.3 Timeline of selected open source LLMs from the release of LLaMa to Llama 2<a id="marker-272"/></p>
</div>
<p class="body">Now that we’ve established that open source LLMs had a <i class="fm-italics">moment</i> in 2023, it’s worth discussing the trade-offs of open source and closed source LLMs (shown in table 9.2). We’ve already highlighted the transparency and accessibility of LLMs, which leads to diversity in thought, rapid innovation, and bias minimization. It also helps lower the barrier of entry and democratizes the power that is in the hands of a select few big tech companies. When deployed in a secure environment, open source LLMs can also provide data privacy benefits, given that data isn’t sent to the corporations who built the models for monitoring or retraining purposes (discussed in chapter 3). On the other hand, there can be several drawbacks and challenges with open source projects, such as lack of centralized control, quality control, long-term sustainability, and intellectual property concerns, among others. Unlike integrating with APIs or using a web interface, like ChatGPT, most open source LLMs may require users to have a certain level of technical knowledge and expertise. We should also highlight that while transparency of open source projects helps identify vulnerabilities, it can also enable malicious actors to exploit weaknesses in the code. Proprietary LLMs have gone through months of safety testing and have safeguards around misaligned and harmful responses. Open source LLMs, unfortunately, don’t have that advantage, which could be disastrous in the wrong, or even<a id="idTextAnchor017"/> in well-intentioned, hands. <a id="idIndexMarker037"/><a id="marker-273"/></p>
<p class="fm-table-caption">Table 9.2 Open source and closed source LLM models trade-offs</p>
<table border="1" class="contenttable-1-table" id="table002" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="20%"/>
<col class="contenttable-0-col" span="1" width="40%"/>
<col class="contenttable-0-col" span="1" width="40%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Trade-Offs</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Open Source LLMs</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Closed Source LLMs</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td" rowspan="2">
<p class="fm-table-body"><a id="idTextAnchor018"/>Transparency and accessibility</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Diversity in thought and innovation, minimized bias, and lower barrier of entry</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Potential lack of transparency and accessibility</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Democratization of power</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Power concentrated within a select few big tech companies</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><a id="idTextAnchor019"/>Data privacy</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Potential for enhanced data privacy (e.g., data isn’t sent to technology corporations if self-hosted in a secure environment)</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Sensitive data collection, storage, and usage concerns</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><a id="idTextAnchor020"/>Control and quality</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Lack of centralized control, potential quality concerns, and long-term sustainability challenges</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Rigorous quality assurance and safety testing</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><a id="idTextAnchor021"/>Technical expertise</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Requires technical knowledge and expertise</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">More user-friendly integration</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><a id="idTextAnchor022"/>Vulnerabilities</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Transparency helpful in identifying vulnerabilities, potential for community-driven fixes</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Internal red teaming, established safeguards against misaligned and harmful responses</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><a id="idTextAnchor023"/>Malicious use</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Potential for vulnerabilities to be exploited by malicious actors</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">Safety measures against malicious use</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Building on that point, we outlined several ways in which adversaries can exploit LLMs in chapter 5. We extensively covered the role proprietary LLMs play with respect to that, but it’s also important to mention that open source LLMs could easily be used to perform adversarial attacks, from taking advantage of weaknesses that are inherent to LLMs to cyberattacks and influence operations. With some technical knowledge and a couple hundred dollars, they could easily fine-tune an open source LLM tailored to perform the exact task they want while also circumventing the guardrails that are often put in place by proprietary LLMs. However, we also believe that there is an opportunity here for the open source community to collectively respond to the ways that LLMs can be exploited or misused. As we’ve emphasized in this section, open source development leads to a flurry of ideas and innovation, and we hope that the open source community will also focus their efforts on preventing misuse and adversarial attacks, in addition to the rapid development of new LLMs. <a id="idIndexMarker038"/><a id="marker-274"/></p>
<p class="body">Finally, we want to highlight the numerous ways to contribute to the open source community, regardless of your background, skill set, or experience. Joining an open source developer community, such as Hugging Face (see <a class="url" href="https://huggingface.co/">https://huggingface.co/</a>) or<a id="idTextAnchor024"/> scikit-learn (see <a class="url" href="https://scikit-learn.org/">https://scikit-learn.org/</a>), is a great way to get plugged into that ecosystem. Developer communities often make it easy to get involved in open source with contribution sprints and access to core developers of the projects, and they often also have Discord servers or Slack workspaces. <a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>
<p class="body">If you’re already comfortable with LLMs, you can jump right in by exploring open source projects and contributing to code development. A good place to start is to find an open source LLM or tool that you’re excited about, go to its GitHub repository, and explore the “How to Contribute” section in the README—even if the model or tool doesn’t have an explicit section for contributors, you can test it and give feedback. You can enhance LLM functionality, fix bugs, or even implement new features. You can also test and report problems or bugs, which can help improve the overall quality and reliability.</p>
<p class="body">Another valuable, yet sometimes underrated, contribution is documentation and community management. You can create and maintain documentation, coordinate between collaborators, and help ensure that users can effectively use the model. You could also write a blog post or record a video walkthrough, which can be immensely helpful for the community. Outside of technical aspects, you can actively participate in community discussions and forums to foster an inclusive environment for innovation and problem-solving. Community engagement is also an excellent way to make sure that a diverse range of users are interacting with the model, to ensure accessibility, and to advocate for the democratization of the technology. We hope that these various ways to get involved empower you to contribute to the open source community and help build a more inclusive and innovative LLM ecosyste<a id="idTextAnchor025"/>m.<a id="idIndexMarker041"/><a id="idIndexMarker042"/><a id="marker-275"/></p>
<h2 class="fm-head" id="heading_id_7">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">There is no clear formalized or testable definition of artificial general intelligence (AGI), but instead, a range of definitions. We define AGI as a system that is capable of any cognitive tasks at a level at, or above, what humans can do.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The two schools of thought within AGI are utopia, where AI solutions solve all of our problems, and dystopia, where AI leads to widespread unemployment, social inequality, and potential threats to humanity itself.</p>
</li>
<li class="fm-list-bullet">
<p class="list">AGI has roots in eugenics and transhumanism, which is inherently discriminatory, and focuses on longtermism ideologies of hypothetical promises or risks from AI instead of the very real risks that are present today.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Although there have been isolated claims of AI consciousness, there is no evidence that any AI systems are conscious, though there are open questions about what artificial consciousness would look like or whether it’s possible.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Training and deploying LLMs at scale is computationally intensive and therefore uses a lot of power. It’s difficult to calculate the total amount of CO<sub class="fm-subscript">2</sub>eq emitted during the life cycle of an LLM, but recent estimates suggest that two models of roughly the same size, BLOOM and GPT-3, emitted about 25 and 502 tons of CO<sub class="fm-subscript">2</sub>eq, respectively.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Within the LLM community, there has been a push toward more systematic reporting of the environmental effects of LLMs, with measures such as the inclusion of a carbon footprint estimate in technical reports and open source tools that help measure energy consumption.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Meta’s LLaMa leak on 4chan changed the LLM game for big tech players with the open source community rapidly releasing lower-cost, performant models.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The transparency and accessibility of open source LLMs lead to diversity in perspectives, innovation, and minimized bias. However, open source LLMs can be more easily used by adversaries, given that they don’t have the same guardrails that proprietary LLMs are subject to.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We hope that you’re empowered and encouraged to participate in the open source LLM community to help us build an inclusive and innovative future.<a id="marker-276"/></p>
</li>
</ul>
</div></body></html>