["```py\n$ wget\nhttp://www.vision.caltech.edu/Image_Datasets/Caltech101/\n101_ObjectCategories.tar.gz\n$ tar -xvf 101_ObjectCategories.tar.gz\n$ mv 101_ObjectCategories caltech101\n$ rm -rf caltech101/BACKGROUND_Google\n```", "```py\nimport numpy as np\nfrom numpy.linalg import norm\nimport pickle\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nimport time\nfrom tf.keras.preprocessing import image\nfrom tf.keras.applications.resnet50 import ResNet50, preprocess_input\n```", "```py\nmodel = ResNet50(weights='imagenet', include_top=False,\n                 input_shape=(224, 224, 3))\ndef extract_features(img_path, model):\n    input_shape = (224, 224, 3)\n    img = image.load_img(img_path, target_size=(\n        input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    expanded_img_array = np.expand_dims(img_array, axis=0)\n    preprocessed_img = preprocess_input(expanded_img_array)\n    features = model.predict(preprocessed_img)\n    flattened_features = features.flatten()\n    normalized_features = flattened_features / norm(flattened_features)\n    return normalized_features\n```", "```py\nfeatures = extract_features('../../sample_images/cat.jpg', model)\nprint(len(features))\n```", "```py\n> 2048\n```", "```py\nextensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\ndef get_file_list(root_dir):\n    file_list = []\n    counter = 1\n    for root, directories, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if any(ext in filename for ext in extensions):\n                file_list.append(os.path.join(root, filename))\n                counter += 1\n    return file_list\n```", "```py\n*`# path to the datasets`*\nroot_dir = '../../datasets/caltech101'\nfilenames = sorted(get_file_list(root_dir))\n```", "```py\nfeature_list = []\nfor i in tqdm_notebook(range(len(filenames))):\n    feature_list.append(extract_features(filenames[i], model))\n```", "```py\npickle.dump(feature_list, open('data/features-caltech101-resnet.pickle', 'wb'))\npickle.dump(filenames, open('data/filenames-caltech101.pickle','wb'))\n```", "```py\nfilenames = pickle.load(open('data/filenames-caltech101.pickle', 'rb'))\nfeature_list = pickle.load(open('data/features-caltech101-resnet.pickle', 'rb'))\n```", "```py\nfrom sklearn.neighbors import NearestNeighbors\nneighbors = NearestNeighbors(n_neighbors=5, algorithm='brute',\nmetric='euclidean').fit(feature_list)\ndistances, indices = neighbors.kneighbors([feature_list[0]])\n```", "```py\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline       # Show the plots as a cell within the Jupyter Notebooks\nplt.imshow(mpimg.imread(filenames[0]))\n```", "```py\nplt.imshow(mpimg.imread(filenames[indices[0]]))\n```", "```py\nfor i in range(5):\n    print(distances[0][i])\n```", "```py\n0.0\n0.8285478\n0.849847\n0.8529018\n```", "```py\nplt.imshow(mpimg.imread(filenames[indices[1]]))\n```", "```py\nfor i in range(6):\n    random_image_index = random.randint(0,num_images)\n    distances, indices = neighbors.kneighbors([featureList[random_image_index]])\n    *`# don't take the first closest image as it will be the same image`*\n    similar_image_paths = [filenames[random_image_index]] +\n                          [filenames[indices[0][i]] for i in range(1,4)]\n    plot_images(similar_image_paths, distances[0])\n```", "```py\n*`# Perform PCA over the features`*\nnum_feature_dimensions=100      *`# Set the number of features`*\npca = PCA(n_components = num_feature_dimensions)\npca.fit(featureList)\nfeature_list_compressed = pca.transform(featureList)\n\n*`# For speed and clarity, we'll analyze about first half of the dataset.`*\nselected_features = feature_list_compressed[:4000]\nselected_class_ids = class_ids[:4000]\nselected_filenames = filenames[:4000]\n\ntsne_results =\nTSNE(n_components=2,verbose=1,metric='euclidean')\n    .fit_transform(selected_features)\n\n*`# Plot a scatter plot from the generated t-SNE results`*\ncolormap = plt.cm.get_cmap('coolwarm')\nscatter_plot = plt.scatter(tsne_results[:,0],tsne_results[:,1], c =\n               selected_class_ids, cmap=colormap)\nplt.colorbar(scatter_plot)\nplt.show()\n```", "```py\ntsne_to_grid_plotter_manual(tsne_results[:,0], tsne_results[:,1],\n                            selected_filenames)\n```", "```py\nmodel = InceptionV3(weights='imagenet', include_top=False,\ninput_shape = (224,224,3), pooling='max')\n```", "```py\nimport sklearn.decomposition.PCA as PCA\nnum_feature_dimensions=100\npca = PCA(n_components = num_feature_dimensions)\npca.fit(feature_list)\nfeature_list_compressed = pca.transform(feature_list)\n```", "```py\n# Explain the importance of first 20 features\nprint(pca.explained_variance_ratio_[0:20])\n```", "```py\n[ 0.07320023  0.05273142   0.04310822 0.03494248  0.02166119  0.0205037\n  0.01974325  0.01739547   0.01611573 0.01548918  0.01450421  0.01311005\n  0.01200541  0.0113084    0.01103872 0.00990405  0.00973481  0.00929487\n  0.00915592  0.0089256 ]\n```", "```py\npca = PCA(200)\npca.fit(feature_list)\nmatplotlib.style.use('seaborn')\nplt.plot(range(1,201),pca.explained_variance_ratio_,'o--', markersize=4)\nplt.title ('Variance for each PCA dimension')\nplt.xlabel('PCA Dimensions')\nplt.ylabel('Variance')\nplt.grid(True)\nplt.show()\n```", "```py\nplt.plot(range(1,201),pca.explained_variance_ratio_.cumsum(),'o--', markersize=4)\nplt.title ('Cumulative Variance with each PCA dimension')\nplt.xlabel('PCA Dimensions')\nplt.ylabel('Variance')\nplt.grid(True)\nplt.show()\n```", "```py\npca_dimensions = [1,2,3,4,5,10,20,50,75,100,150,200]\npca_accuracy = []\npca_time = []\n\nfor dimensions in pca_dimensions:\n    *`# Perform PCA`*\n    pca = PCA(n_components = dimensions)\n    pca.fit(feature_list)\n    feature_list_compressed = pca.transform(feature_list[:])\n    *`# Calculate accuracy over the compressed features`*\n    accuracy, time_taken = accuracy_calculator(feature_list_compressed[:])\n    pca_time.append(time_taken)\n    pca_accuracy.append(accuracy)\n    print(\"For PCA Dimensions = \", dimensions, \",\\tAccuracy = \",accuracy,\"%\",\n\",\\tTime = \", pca_time[-1])\n```", "```py\nplt.plot(pca_time, pca_accuracy,'o--', markersize=4)\nfor label, x, y in zip(pca_dimensions, pca_time,pca_accuracy):\n    plt.annotate(label, xy=(x, y), ha='right', va='bottom')\nplt.title ('Test Time vs Accuracy for each PCA dimension')\nplt.xlabel('Test Time')\nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.show()\n```", "```py\nnum_items = 1000000\nnum_dimensions = 100\ndataset = np.random.randn(num_items, num_dimensions)\ndataset /= np.linalg.norm(dataset, axis=1).reshape(-1, 1)\n\nrandom_index = random.randint(0,num_items)\nquery = dataset[random_index]\n```", "```py\nneighbors = NearestNeighbors(n_neighbors=5, algorithm='brute',\nmetric='euclidean').fit(dataset)\n%timeit distances, indices = neighbors.kneighbors([query])\n```", "```py\n> 177 ms \u00b1 136 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```", "```py\n$ pip install annoy\n```", "```py\nfrom annoy import AnnoyIndex\nannoy_index = AnnoyIndex(num_dimensions) *`# Length of item vector that will be`\n`indexed`*\nfor i in range(num_items):\n    annoy_index.add_item(i, dataset[i])\nannoy_index.build(40) *`# 40 trees`*\n```", "```py\n%timeit indexes=t.get_nns_by_vector(query, 5, include_distances=True)\n```", "```py\n> 34.9 \u03bcs \u00b1 165 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```", "```py\nnames_of_worst_classes_before_finetuning, accuracy_per_class_before_finetuning =\nworst_classes(feature_list[:])\n```", "```py\n    Accuracy is 56.54\n    Top 6 incorrect classifications\n    059.drinking-straw    Accuracy: 11.76%\n    135.mailbox             Accuracy: 16.03%\n    108.hot-dog             Accuracy: 16.72%\n    163.playing-card      Accuracy: 17.29%\n    195.soda-can            Accuracy: 19.68%\n    125.knife         Accuracy: 20.53%\n```", "```py\nmarkers = [ \"^\", \".\",\"s\", \"o\",\"x\", \"P\" ]\ncolors = ['red', 'blue', 'fuchsia', 'green', \n'purple', 'orange']\n```", "```py\nfrom tf.keras.applications.resnet50 import ResNet50\nmodel = ResNet50(weights='imagenet', include_top=False,\ninput_shape = (224,224,3))\ninput = Input(shape=(224, 224, 3))\nx = model(input)\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(NUM_CLASSES, activation='softmax')(x)\nmodel_classification_optimized = Model(inputs=input, outputs=x)\n```", "```py\nfrom tf.keras.applications.resnet50 import ResNet50\nmodel = ResNet50(weights='imagenet', include_top=False,\ninput_shape = (224,224,3))\ninput = Input(shape=(224, 224, 3))\nx = model(input)\nx = GlobalAveragePooling2D()(x)\n*`# No dense or dropout layers`*\nx = Dense(NUM_CLASSES, activation='softmax')(x)\nmodel_similarity_optimized = Model(inputs=input, outputs=x)\n```", "```py\nmodel_similarity_optimized.layers.pop()\nmodel = Model(model_similarity_optimized.input,\nmodel_similarity_optimized.layers[-1].output)\n```"]