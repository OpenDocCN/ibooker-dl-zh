["```py\nimport pandas as pd\n\ndf=pd.read_csv(\"files/en2fr.csv\")                                ①\nnum_examples=len(df)                                             ②\nprint(f\"there are {num_examples} examples in the training data\")\nprint(df.iloc[30856][\"en\"])                                      ③\nprint(df.iloc[30856][\"fr\"])                                      ④\n```", "```py\nthere are 47173 examples in the training data\nHow are you?\nComment êtes-vous?\n```", "```py\n!pip install transformers\n```", "```py\nfrom transformers import XLMTokenizer                           ①\n\ntokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\n\ntokenized_en=tokenizer.tokenize(\"I don't speak French.\")        ②\nprint(tokenized_en)\ntokenized_fr=tokenizer.tokenize(\"Je ne parle pas français.\")    ③\nprint(tokenized_fr)\nprint(tokenizer.tokenize(\"How are you?\"))\nprint(tokenizer.tokenize(\"Comment êtes-vous?\"))\n```", "```py\n['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'fr', 'ench</w>', '.</w>']\n['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\n['how</w>', 'are</w>', 'you</w>', '?</w>']\n['comment</w>', 'et', 'es-vous</w>', '?</w>']\n```", "```py\nfrom collections import Counter\n\nen=df[\"en\"].tolist()                                           ①\n\nen_tokens=[[\"BOS\"]+tokenizer.tokenize(x)+[\"EOS\"] for x in en]  ②\nPAD=0\nUNK=1\nword_count=Counter()\nfor sentence in en_tokens:\n    for word in sentence:\n        word_count[word]+=1\nfrequency=word_count.most_common(50000)                        ③\ntotal_en_words=len(frequency)+2\nen_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}    ④\nen_word_dict[\"PAD\"]=PAD\nen_word_dict[\"UNK\"]=UNK\nen_idx_dict={v:k for k,v in en_word_dict.items()}              ⑤\n```", "```py\nenidx=[en_word_dict.get(i,UNK) for i in tokenized_en]   \nprint(enidx)\n```", "```py\n[15, 100, 38, 377, 476, 574, 5]\n```", "```py\nentokens=[en_idx_dict.get(i,\"UNK\") for i in enidx]          ①\nprint(entokens)\nen_phrase=\"\".join(entokens)                                 ②\nen_phrase=en_phrase.replace(\"</w>\",\" \")                     ③\nfor x in '''?:;.,'(\"-!&)%''':\n    en_phrase=en_phrase.replace(f\" {x}\",f\"{x}\")             ④\nprint(en_phrase)\n```", "```py\n['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'fr', 'ench</w>', '.</w>']\ni don't speak french. \n```", "```py\nfr=df[\"fr\"].tolist()       \nfr_tokens=[[\"BOS\"]+tokenizer.tokenize(x)+[\"EOS\"] for x in fr]  ①\nword_count=Counter()\nfor sentence in fr_tokens:\n    for word in sentence:\n        word_count[word]+=1\nfrequency=word_count.most_common(50000)                        ②\ntotal_fr_words=len(frequency)+2\nfr_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}    ③\nfr_word_dict[\"PAD\"]=PAD\nfr_word_dict[\"UNK\"]=UNK\nfr_idx_dict={v:k for k,v in fr_word_dict.items()}              ④\n```", "```py\nfridx=[fr_word_dict.get(i,UNK) for i in tokenized_fr]   \nprint(fridx)\n```", "```py\n[28, 40, 231, 32, 726, 370, 4]\n```", "```py\nfrtokens=[fr_idx_dict.get(i,\"UNK\") for i in fridx] \nprint(frtokens)\nfr_phrase=\"\".join(frtokens)\nfr_phrase=fr_phrase.replace(\"</w>\",\" \")\nfor x in '''?:;.,'(\"-!&)%''':\n    fr_phrase=fr_phrase.replace(f\" {x}\",f\"{x}\")  \nprint(fr_phrase)\n```", "```py\n['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\nje ne parle pas francais. \n```", "```py\nimport pickle\n\nwith open(\"files/dict.p\",\"wb\") as fb:\n    pickle.dump((en_word_dict,en_idx_dict,\n                 fr_word_dict,fr_idx_dict),fb)\n```", "```py\nout_en_ids=[[en_word_dict.get(w,UNK) for w in s] for s in en_tokens]\nout_fr_ids=[[fr_word_dict.get(w,UNK) for w in s] for s in fr_tokens]\nsorted_ids=sorted(range(len(out_en_ids)),\n                  key=lambda x:len(out_en_ids[x]))\nout_en_ids=[out_en_ids[x] for x in sorted_ids]\nout_fr_ids=[out_fr_ids[x] for x in sorted_ids]\n```", "```py\nimport numpy as np\n\nbatch_size=128\nidx_list=np.arange(0,len(en_tokens),batch_size)\nnp.random.shuffle(idx_list)\n\nbatch_indexs=[]\nfor idx in idx_list:\n    batch_indexs.append(np.arange(idx,min(len(en_tokens),\n                                          idx+batch_size)))\n```", "```py\ndef seq_padding(X, padding=0):\n    L = [len(x) for x in X]\n    ML = max(L)                                                   ①\n    padded_seq = np.array([np.concatenate([x, [padding] * (ML - len(x))])\n        if len(x) < ML else x for x in X])                        ②\n    return padded_seq\n```", "```py\nimport torch\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclass Batch:\n    def __init__(self, src, trg=None, pad=0):\n        src = torch.from_numpy(src).to(DEVICE).long()\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)              ①\n        if trg is not None:\n            trg = torch.from_numpy(trg).to(DEVICE).long()\n            self.trg = trg[:, :-1]                              ②\n            self.trg_y = trg[:, 1:]                             ③\n            self.trg_mask = make_std_mask(self.trg, pad)        ④\n            self.ntokens = (self.trg_y != pad).data.sum()\n```", "```py\nimport numpy as np\ndef subsequent_mask(size):\n    attn_shape = (1, size, size)\n    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n    output = torch.from_numpy(subsequent_mask) == 0\n    return output\ndef make_std_mask(tgt, pad):\n    tgt_mask=(tgt != pad).unsqueeze(-2)\n    output=tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n    return output \n```", "```py\nfrom utils.ch09util import Batch\n\nclass BatchLoader():\n    def __init__(self):\n        self.idx=0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        self.idx += 1\n        if self.idx<=len(batch_indexs):\n            b=batch_indexs[self.idx-1]\n            batch_en=[out_en_ids[x] for x in b]\n            batch_fr=[out_fr_ids[x] for x in b]\n            batch_en=seq_padding(batch_en)\n            batch_fr=seq_padding(batch_fr)\n            return Batch(batch_en,batch_fr)\n        raise StopIteration\n```", "```py\nsrc_vocab = len(en_word_dict)\ntgt_vocab = len(fr_word_dict)\nprint(f\"there are {src_vocab} distinct English tokens\")\nprint(f\"there are {tgt_vocab} distinct French tokens\")\n```", "```py\nthere are 11055 distinct English tokens\nthere are 11239 distinct French tokens\n```", "```py\nimport math\n\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        out = self.lut(x) * math.sqrt(self.d_model)\n        return out\n```", "```py\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):       ①\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model, device=DEVICE)\n        position = torch.arange(0., max_len, \n                                device=DEVICE).unsqueeze(1)\n        div_term = torch.exp(torch.arange(\n            0., d_model, 2, device=DEVICE)\n            * -(math.log(10000.0) / d_model))\n        pe_pos = torch.mul(position, div_term)\n        pe[:, 0::2] = torch.sin(pe_pos)                        ②\n        pe[:, 1::2] = torch.cos(pe_pos)                        ③\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)  \n\n    def forward(self, x):\n        x=x+self.pe[:,:x.size(1)].requires_grad_(False)        ④\n        out=self.dropout(x)\n        return out\n```", "```py\nfrom utils.ch09util import PositionalEncoding\nimport torch\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npe = PositionalEncoding(256, 0.1)                              ①\nx = torch.zeros(1, 8, 256).to(DEVICE)                          ②\ny = pe.forward(x)                                              ③\nprint(f\"the shape of positional encoding is {y.shape}\")\nprint(y)                                                       ④\n```", "```py\nthe shape of positional encoding is torch.Size([1, 8, 256])\ntensor([[[ 0.0000e+00,  1.1111e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  1.1111e+00],\n         [ 9.3497e-01,  6.0034e-01,  8.9107e-01,  ...,  1.1111e+00,\n           1.1940e-04,  1.1111e+00],\n         [ 0.0000e+00, -4.6239e-01,  1.0646e+00,  ...,  1.1111e+00,\n           2.3880e-04,  1.1111e+00],\n         ...,\n         [-1.0655e+00,  3.1518e-01, -1.1091e+00,  ...,  1.1111e+00,\n           5.9700e-04,  1.1111e+00],\n         [-3.1046e-01,  1.0669e+00, -0.0000e+00,  ...,  0.0000e+00,\n           7.1640e-04,  1.1111e+00],\n         [ 7.2999e-01,  8.3767e-01,  2.5419e-01,  ...,  1.1111e+00,\n           8.3581e-04,  1.1111e+00]]], device='cuda:0')\n```", "```py\nfrom utils.ch09util import create_model\n\nmodel = create_model(src_vocab, tgt_vocab, N=6,\n    d_model=256, d_ff=1024, h=8, dropout=0.1)\n```", "```py\nclass LabelSmoothing(nn.Module):\n    def __init__(self, size, padding_idx, smoothing=0.1):\n        super().__init__()\n        self.criterion = nn.KLDivLoss(reduction='sum')  \n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()                              ①\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, \n               target.data.unsqueeze(1), self.confidence)       ②\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        output = self.criterion(x, true_dist.clone().detach())  ③\n        return output\n```", "```py\nclass NoamOpt:\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup                                  ①\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n    def step(self):                                           ②\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n    def rate(self, step=None):\n        if step is None:\n            step = self._step\n        output = self.factor * (self.model_size ** (-0.5) *\n        min(step ** (-0.5), step * self.warmup ** (-1.5)))    ③\n        return output\n```", "```py\nfrom utils.ch09util import NoamOpt\n\noptimizer = NoamOpt(256, 1, 2000, torch.optim.Adam(\n    model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n```", "```py\nclass SimpleLossCompute:\n    def __init__(self, generator, criterion, opt=None):\n        self.generator = generator\n        self.criterion = criterion\n        self.opt = opt\n    def __call__(self, x, y, norm):\n        x = self.generator(x)                                    ①\n        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n                              y.contiguous().view(-1)) / norm    ②\n        loss.backward()                                          ③\n        if self.opt is not None:\n            self.opt.step()                                      ④\n            self.opt.optimizer.zero_grad()\n        return loss.data.item() * norm.float()\n```", "```py\nfrom utils.ch09util import (LabelSmoothing,\n       SimpleLossCompute)\n\ncriterion = LabelSmoothing(tgt_vocab, \n                           padding_idx=0, smoothing=0.1)\nloss_func = SimpleLossCompute(\n            model.generator, criterion, optimizer)\n```", "```py\nfor epoch in range(100):\n    model.train()\n    tloss=0\n    tokens=0\n    for batch in BatchLoader():\n        out = model(batch.src, batch.trg, \n                    batch.src_mask, batch.trg_mask)            ①\n        loss = loss_func(out, batch.trg_y, batch.ntokens)      ②\n        tloss += loss\n        tokens += batch.ntokens                                ③\n    print(f\"Epoch {epoch}, average loss: {tloss/tokens}\")\ntorch.save(model.state_dict(),\"files/en2fr.pth\")               ④\n```", "```py\ndef translate(eng):\n    tokenized_en=tokenizer.tokenize(eng)\n    tokenized_en=[\"BOS\"]+tokenized_en+[\"EOS\"]\n    enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]  \n    src=torch.tensor(enidx).long().to(DEVICE).unsqueeze(0)    \n    src_mask=(src!=0).unsqueeze(-2)\n    memory=model.encode(src,src_mask)                           ①\n    start_symbol=fr_word_dict[\"BOS\"]\n    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n    translation=[]\n    for i in range(100):\n        out = model.decode(memory,src_mask,ys,\n        subsequent_mask(ys.size(1)).type_as(src.data))          ②\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]    \n        ys = torch.cat([ys, torch.ones(1, 1).type_as(\n            src.data).fill_(next_word)], dim=1)\n        sym = fr_idx_dict[ys[0, -1].item()]\n        if sym != 'EOS':\n            translation.append(sym)\n        else:\n            break                                               ③\n    trans=\"\".join(translation)\n    trans=trans.replace(\"</w>\",\" \") \n    for x in '''?:;.,'(\"-!&)%''':\n        trans=trans.replace(f\" {x}\",f\"{x}\")                     ④\n    print(trans) \n    return trans\n```", "```py\nfrom utils.ch09util import subsequent_mask\n\nwith open(\"files/dict.p\",\"rb\") as fb:\n    en_word_dict,en_idx_dict,\\\n    fr_word_dict,fr_idx_dict=pickle.load(fb)\ntrained_weights=torch.load(\"files/en2fr.pth\",\n                           map_location=DEVICE)\nmodel.load_state_dict(trained_weights)\nmodel.eval()\neng = \"Today is a beautiful day!\"\ntranslated_fr = translate(eng)\n```", "```py\naujourd'hui est une belle journee!\n```", "```py\neng = \"A little boy in jeans climbs a small tree while another child looks on.\"\ntranslated_fr = translate(eng)\n```", "```py\nun petit garcon en jeans grimpe un petit arbre tandis qu'un autre enfant regarde. \n```", "```py\neng = \"I don't speak French.\"\ntranslated_fr = translate(eng)\n```", "```py\nje ne parle pas francais. \n```", "```py\neng = \"I do not speak French.\"\ntranslated_fr = translate(eng)\n```", "```py\nje ne parle pas francais. \n```"]