- en: Chapter 6\. Agent Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the architectures described in [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774),
    this chapter will cover what is perhaps the most important of all current LLM
    architectures, the agent architecture. First, we introduce what makes LLM agents
    unique, then we show how to build them and how to extend them for common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the artificial intelligence field, there is a long history of creating (intelligent)
    agents, which can be most simply defined as “something that acts,” in the words
    of Stuart Russell and Peter Norvig in their *Artificial Intelligence* (Pearson,
    2020) textbook. The word *acts* actually carries a little more meaning than meets
    the eye:'
  prefs: []
  type: TYPE_NORMAL
- en: Acting requires some capacity for deciding what to do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding what to do implies having access to more than one possible course of
    action. After all, a decision without options is no decision at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to decide, the agent also needs access to information about the external
    environment (anything outside of the agent itself).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So an *agentic* LLM application must be one that uses an LLM to pick from one
    or more possible courses of action, given some context about the current state
    of the world or some desired next state. These attributes are usually implemented
    by mixing two prompting techniques we first met in the [Preface](preface01.html#pr01_preface_1736545679069216):'
  prefs: []
  type: TYPE_NORMAL
- en: Tool calling
  prefs: []
  type: TYPE_NORMAL
- en: Include a list of external functions that the LLM can make use of in your prompt
    (that is, the actions it can decide to take) and provide instructions on how to
    format its choice in the output it generates. You’ll see in a moment what this
    looks like in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have found that LLMs “make better decisions” when given instructions
    to reason about complex problems by breaking them down into granular steps to
    be taken in sequence. This is usually done either by adding instructions along
    the lines of “think step by step” or including examples of questions and their
    decomposition into several steps/actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example prompt using both tool calling and chain-of-thought:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output, when run against `gpt-3.5-turbo` at temperature 0 (to ensure
    the LLM follows the desired output format, CSV) and newline as the stop sequence
    (which instructs the LLM to stop producing output when it reaches this character).
    This makes the LLM produce a single action (as expected, given the prompt asked
    for this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The most recent LLMs and chat models have been fine-tuned to improve their
    performance for tool-calling and chain-of-thought applications, removing the need
    for adding specific instructions to the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Plan-Do Loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What makes the agent architecture different from the architectures discussed
    in [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    is a concept we haven’t covered yet: the LLM-driven loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Every programmer has encountered loops in their code before. By *loop*, we mean
    running the same code multiple times until a stop condition is hit. The key to
    the agent architecture is to have an LLM control the stop condition—that is, decide
    when to stop looping.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we’ll run in this loop will be some variation of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning an action or actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing said action(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Picking up on the example in the previous section, we’ll next run the `search`
    tool with the input `30th president of the United States`, which produces this
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we’ll rerun the prompt, with a small addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice we added two things:'
  prefs: []
  type: TYPE_NORMAL
- en: An “output” tool—which the LLM should use when it has found the final answer,
    and which we’d use as the signal to stop the loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of the tool call from the preceding iteration, simply with the name
    of the tool and its (text) output. This is included in order to allow the LLM
    to move on to the next step in the interaction. In other words, we’re telling
    the LLM, “Hey, we got the results you asked for, what do you want to do next?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s continue with a third iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With the result from the `calculator` tool, the LLM now has enough information
    to provide the final answer, so it picked the `output` tool and chose “61” as
    the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: This is what makes the agent architecture so useful—the LLM is given the agency
    to decide. The next step is to arrive at an answer and decide how many steps to
    take—that is, when to stop.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture, called [ReAct](https://oreil.ly/M7hF-), was first proposed
    by Shunyu Yao et al. The rest of this chapter explores how to improve the performance
    of the agent architecture, motivated by the email assistant example from [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774).
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s see what it looks like to implement the basic agent architecture
    using a chat model and LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: Building a LangGraph Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we need to install additional dependencies for the search
    tool we chose to use, DuckDuckGo. To install it for Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And for JS, we also need to install a dependency for the calculator tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With that complete, let’s get into the actual code to implement the agent architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The visual representation is shown in [Figure 6-1](#ch06_figure_1_1736545671744632).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model  Description automatically generated](assets/lelc_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The agent architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A few things to notice here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re using two tools in this example: a search tool and a calculator tool,
    but you could easily add more or replace the ones we used. In the Python example,
    you also see an example of creating a custom tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve used two convenience functions that ship with LangGraph. `ToolNode` serves
    as a node in our graph; it executes the tool calls requested in the latest AI
    message found in the state and returns a `ToolMessage` with the results of each.
    `ToolNode` also handles exceptions raised by tools—using the error message to
    build a `ToolMessage` that is then passed to the LLM—which may decide what to
    do with the error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tools_condition` serves as a conditional edge function that looks at the latest
    AI message in the state and routes to the `tools` node if there are any tools
    to execute. Otherwise, it ends the graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, notice that this graph loops between the model and tools nodes. That
    is, the model itself is in charge of deciding when to end the computation, which
    is a key attribute of the agent architecture. Whenever we code a loop in LangGraph,
    we’ll likely want to use a conditional edge, as that allows you to define the
    *stop condition* when the graph should exit the loop and stop executing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s see how it does in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Walking through this output:'
  prefs: []
  type: TYPE_NORMAL
- en: First the `model` node executed and decided to call the `duckduckgo_search`
    tool, which led the conditional edge to route us to the `tools` node after.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `ToolNode` executed the search tool and got the search results printed above,
    which actually contain the answer “Age and Year of Death . January 5, 1933 (aged
    60)”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `model` tool was called again, this time with the search results as the
    latest message, and produced the final answer (with no more tool calls); therefore,
    the conditional edge ended the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let’s look at a few useful extensions to this basic agent architecture,
    customizing both planning and tool calling.
  prefs: []
  type: TYPE_NORMAL
- en: Always Calling a Tool First
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the standard agent architecture, the LLM is always called upon to decide
    what tool to call next. This arrangement has a clear advantage: it gives the LLM
    ultimate flexibility to adapt the behavior of the application to each user query
    that comes in. But this flexibility comes at a cost: unpredictability. If, for
    instance, you, the developer of the application, know that the search tool should
    always be called first, that can actually be beneficial to your application:'
  prefs: []
  type: TYPE_NORMAL
- en: It will reduce overall latency, as it will skip the first LLM call that would
    generate that request to call the search tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will prevent the LLM from erroneously deciding it doesn’t need to call the
    search tool for some user queries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the other hand, if your application doesn’t have a clear rule of the kind
    “you should always call this tool first,” introducing such a constraint would
    actually make your application worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what it looks like to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The visual representation is shown in [Figure 6-2](#ch06_figure_2_1736545671744669).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model  Description automatically generated](assets/lelc_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Modifying the agent architecture to always call a specific tool
    first
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice the differences compared to the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we start all invocations by calling `first_model`, which doesn’t call an
    LLM at all. It just creates a tool call for the search tool, using the user’s
    message verbatim as the search query. The previous architecture would have the
    LLM generate this tool call (or some other response it deemed better).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, we proceed to `tools`, which is identical to the previous example,
    and from there we proceed to the `agent` node as before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s see some example output, for the same query as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This time, we skipped the initial LLM call. We first went to `first_model` node,
    which directly returned a tool call for the search tool. From there we went to
    the previous flow—that is, we executed the search tool and finally went back to
    the `model` node to generate the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: Next let’s go over what you can do when you have many tools you want to make
    available to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Many Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are far from perfect, and they currently struggle more when given multiple
    choices or excessive information in a prompt. These limitations also extend to
    the planning of the next action to take. When given many tools (say, more than
    10) the planning performance (that is, the LLM’s ability to choose the right tool)
    starts to suffer. The solution to this problem is to reduce the number of tools
    the LLM can choose from. But what if you do have many tools you want to see used
    for different user queries?
  prefs: []
  type: TYPE_NORMAL
- en: One elegant solution is to use a RAG step to preselect the most relevant tools
    for the current query and then feed the LLM only that subset of tools instead
    of the entire arsenal. This can also help to reduce the cost of calling the LLM
    (commercial LLMs usually charge based on the length of the prompt and outputs).
    On the other hand, this RAG step introduces additional latency to your application,
    so should only be taken when you see performance decreasing after adding more
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can see the visual representation in [Figure 6-3](#ch06_figure_3_1736545671744693).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software development process  Description automatically generated](assets/lelc_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Modifying the agent architecture to deal with many tools
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is very similar to the regular agent architecture. The only difference
    is that we stop by the `select_tools` node before entering the actual agent loop.
    After that, it works just as the regular agent architecture we’ve seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see some example output for the same query as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the first thing we did was query the retriever to get the most relevant
    tools for the current user query. Then, we proceeded to the regular agent architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduced the concept of *agency* and discussed what it takes
    to make an LLM application *agentic*: giving the LLM the ability to decide between
    multiple options by using external information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We walked through the standard agent architecture built with LangGraph and
    looked at two useful extensions: how to always call a specific tool first and
    how to deal with many tools.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html#ch07_agents_ii_1736545673023633) looks at additional
    extensions to the agent architecture.'
  prefs: []
  type: TYPE_NORMAL
