- en: Chapter 9\. Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 变压器
- en: We saw in [Chapter 5](ch05.xhtml#chapter_autoregressive) how we can build generative
    models on text data using recurrent neural networks (RNNs), such as LSTMs and
    GRUs. These autoregressive models process sequential data one token at a time,
    constantly updating a hidden vector that captures the current latent representation
    of the input. The RNN can be designed to predict the next word in a sequence by
    applying a dense layer and softmax activation over the hidden vector. This was
    considered the most sophisticated way to generatively produce text until 2017,
    when one paper changed the landscape of text generation forever.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](ch05.xhtml#chapter_autoregressive)中看到，我们可以使用循环神经网络（RNNs）（如LSTM和GRU）在文本数据上构建生成模型。这些自回归模型一次处理一个令牌的顺序数据，不断更新一个捕获输入当前潜在表示的隐藏向量。可以设计RNN以通过在隐藏向量上应用密集层和softmax激活来预测序列中的下一个单词。直到2017年，这被认为是生成文本的最复杂方式，当一篇论文永久改变了文本生成的格局。
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The Google Brain paper, confidently entitled “Attention Is All You Need,”^([1](ch09.xhtml#idm45387006840576))
    is famous for popularizing the concept of *attention*—a mechanism that now powers
    most state-of-the-art text generation models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌Brain的论文，自信地命名为“注意力就是一切”^([1](ch09.xhtml#idm45387006840576))，因推广*注意力*的概念而闻名，这个概念现在驱动着大多数最先进的文本生成模型。
- en: The authors show how it is possible to create powerful neural networks called
    *Transformers* for sequential modeling that do not require complex recurrent or
    convolutional architectures but instead only rely on attention mechanisms. This
    approach overcomes a key downside to the RNN approach, which is that it is challenging
    to parallelize, as it must process sequences one token as a time. Transformers
    are highly paralellizable, allowing them to be trained on massive datasets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了如何创建称为*变压器*的强大神经网络，用于顺序建模，而不需要复杂的循环或卷积架构，而只依赖于注意机制。这种方法克服了RNN方法的一个关键缺点，即难以并行化，因为它必须一次处理一个令牌的序列。变压器是高度可并行化的，使它们能够在大规模数据集上进行训练。
- en: In this chapter, we are going to delve into how modern text generation models
    make use of the Transformer architecture to reach state-of-the-art performance
    on text generation challenges. In particular, we will explore a type of autoregressive
    model known as the *generative pre-trained transformer* (GPT), which powers OpenAI’s
    GPT-4 model, widely considered to be the current state of the art for text generation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨现代文本生成模型如何利用Transformer架构在文本生成挑战中达到最先进的性能。特别是，我们将探索一种称为*生成式预训练变压器*（GPT）的自回归模型，它驱动着OpenAI的GPT-4模型，被广泛认为是当前文本生成领域的最先进技术。
- en: GPT
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT
- en: OpenAI introduced GPT in June 2018, in the paper “Improving Language Understanding
    by Generative Pre-Training,”^([2](ch09.xhtml#idm45387006828736)) almost exactly
    a year after the appearance of the original Transformer paper.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2018年6月推出了GPT，在论文“通过生成式预训练改进语言理解”中^([2](ch09.xhtml#idm45387006828736))，几乎与原始Transformer论文出现一年后完全一致。
- en: In this paper, the authors show how a Transformer architecture can be trained
    on a huge amount of text data to predict the next word in a sequence and then
    subsequently fine-tuned to specific downstream tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，作者展示了如何训练Transformer架构以预测序列中的下一个单词，然后随后对特定下游任务进行微调。
- en: The pre-training process of GPT involves training the model on a large corpus
    of text called BookCorpus (4.5 GB of text from 7,000 unpublished books of different
    genres). During pre-training, the model is trained to predict the next word in
    a sequence given the previous words. This process is known as *language modeling*
    and is used to teach the model to understand the structure and patterns of natural
    language.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GPT的预训练过程涉及在名为BookCorpus的大型文本语料库上训练模型（来自不同流派的7,000本未发表书籍的4.5 GB文本）。在预训练期间，模型被训练以预测给定前面单词的序列中的下一个单词。这个过程被称为*语言建模*，用于教导模型理解自然语言的结构和模式。
- en: After pre-training, the GPT model can be fine-tuned for a specific task by providing
    it with a smaller, task-specific dataset. Fine-tuning involves adjusting the parameters
    of the model to better fit the task at hand. For example, the model can be fine-tuned
    for tasks such as classification, similarity scoring, or question answering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练之后，GPT模型可以通过提供较小的、特定于任务的数据集来进行微调以适应特定任务。微调涉及调整模型的参数以更好地适应手头的任务。例如，模型可以针对分类、相似性评分或问题回答等任务进行微调。
- en: The GPT architecture has since been improved and extended by OpenAI with the
    release of subsequent models such as GPT-2, GPT-3, GPT-3.5, and GPT-4\. These
    models are trained on larger datasets and have larger capacities, so they can
    generate more complex and coherent text. The GPT models have been widely adopted
    by researchers and industry practitioners and have contributed to significant
    advancements in natural language processing tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自GPT架构推出以来，OpenAI通过发布后续模型如GPT-2、GPT-3、GPT-3.5和GPT-4对其进行了改进和扩展。这些模型在更大的数据集上进行训练，并具有更大的容量，因此可以生成更复杂和连贯的文本。研究人员和行业从业者广泛采用了GPT模型，并为自然语言处理任务的重大进展做出了贡献。
- en: In this chapter, we will build our own variation of the original GPT model,
    trained on less data, but still utilizing the same components and underlying principles.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建我们自己的变体GPT模型，该模型在较少数据上进行训练，但仍利用相同的组件和基本原则。
- en: Running the Code for This Example
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/09_transformer/01_gpt/gpt.ipynb*
    in the book repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书籍存储库中的Jupyter笔记本中找到，位置为*notebooks/09_transformer/01_gpt/gpt.ipynb*。
- en: The code is adapted from the excellent [GPT tutorial](https://oreil.ly/J86pg)
    created by Apoorv Nandan available on the Keras website.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码改编自由Apoorv Nandan创建的优秀[GPT教程](https://oreil.ly/J86pg)，该教程可在Keras网站上找到。
- en: The Wine Reviews Dataset
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 葡萄酒评论数据集
- en: We’ll be using the [Wine Reviews dataset](https://oreil.ly/DC9EG) that is available
    through Kaggle. This is a set of over 130,000 reviews of wines, with accompanying
    metadata such as description and price.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用通过Kaggle提供的[Wine Reviews数据集](https://oreil.ly/DC9EG)。这是一个包含超过130,000条葡萄酒评论的数据集，附带元数据，如描述和价格。
- en: You can download the dataset by running the Kaggle dataset downloader script
    in the book repository, as shown in [Example 9-1](#downloading-wine-dataset).
    This will save the wine reviews and accompanying metadata locally to the */data*
    folder.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在书库中运行Kaggle数据集下载脚本来下载数据集，如[示例9-1](#downloading-wine-dataset)所示。这将把葡萄酒评论和相关元数据保存在本地的*/data*文件夹中。
- en: Example 9-1\. Downloading the Wine Reviews dataset
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-1\. 下载葡萄酒评论数据集
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`The data preparation steps are identical to the steps used in [Chapter 5](ch05.xhtml#chapter_autoregressive)
    for preparing data for input into an LSTM, so we will not repeat them in detail
    here. The steps, as shown in [Figure 9-1](#transformer_data_prep), are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`数据准备步骤与[第5章](ch05.xhtml#chapter_autoregressive)中用于准备输入到LSTM的数据的步骤是相同的，因此我们不会在这里详细重复它们。如[图9-1](#transformer_data_prep)所示，步骤如下：'
- en: Load the data and create a list of text string descriptions of each wine.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据并创建每种葡萄酒的文本字符串描述列表。
- en: Pad punctuation with spaces, so that each punctuation mark is treated as a separate
    word.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用空格填充标点符号，以便每个标点符号被视为一个单独的单词。
- en: Pass the strings through a `TextVectorization` layer that tokenizes the data
    and pads/clips each string to a fixed length.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`TextVectorization`层将字符串传递，对数据进行标记化，并将每个字符串填充/裁剪到固定长度。
- en: Create a training set where the inputs are the tokenized text strings and the
    outputs to predict are the same strings shifted by one token.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个训练集，其中输入是标记化的文本字符串，输出是预测的相同字符串向后移动一个标记。
- en: '![](Images/gdl2_0901.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0901.png)'
- en: Figure 9-1\. Data processing for the Transformer`  `## Attention
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. Transformer的数据处理`  `## 注意力
- en: The first step to understanding how GPT works is to understand how the *attention
    mechanism* works. This mechanism is what makes the Transformer architecture unique
    and distinct from recurrent approaches to language modeling. When we have developed
    a solid understanding of attention, we will then see how it is used within Transformer
    architectures such as GPT.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 了解GPT如何工作的第一步是了解*注意力机制*的工作原理。这个机制是使Transformer架构与循环方法在语言建模方面独特和不同的地方。当我们对注意力有了扎实的理解后，我们将看到它如何在GPT等Transformer架构中使用。
- en: 'When you write, the choice that you make for the next word in the sentence
    is influenced by other words that you have already written. For example, suppose
    you start a sentence as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当您写作时，句子中下一个词的选择受到您已经写过的其他单词的影响。例如，假设您开始一个句子如下：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Clearly, the next word should be something synonymous with *big*. How do we
    know this?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，下一个词应该是与*big*同义的。我们怎么知道这一点？
- en: Certain other words in the sentence are important for helping us to make our
    decision. For example, the fact that it is an elephant, rather than a sloth, means
    that we prefer *big* rather than *slow*. If it were a swimming pool, rather than
    a car, we might choose *scared* as a possible alternative to *big*. Lastly, the
    action of *getting into* the car implies that size is the problem—if the elephant
    was trying to *squash* the car instead, we might choose *fast* as the final word,
    with *it* now referring to the car.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的某些其他单词对帮助我们做出决定很重要。例如，它是大象而不是树懒，意味着我们更喜欢*big*而不是*slow*。如果它是游泳池而不是汽车，我们可能会选择*scared*作为*big*的一个可能替代。最后，*getting
    into*汽车的行为意味着大小是问题所在——如果大象试图*压扁*汽车，我们可能会选择*fast*作为最后一个词，现在*it*指的是汽车。
- en: Other words in the sentence are not important at all. For example, the fact
    that the elephant is pink has no influence on our choice of final word. Equally,
    the minor words in the sentence (*the*, *but*, *it*, etc.) give the sentence grammatical
    form, but here aren’t important to determine the required adjective.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的其他单词一点都不重要。例如，大象是粉红色这个事实对我们选择最终词汇没有影响。同样，句子中的次要单词（*the*、*but*、*it*等）给句子以语法形式，但在这里并不重要，以确定所需形容词。
- en: In other words, we are *paying attention* to certain words in the sentence and
    largely ignoring others. Wouldn’t it be great if our model could do the same thing?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们正在*关注*句子中的某些单词，而基本上忽略其他单词。如果我们的模型也能做同样的事情，那不是很好吗？
- en: An attention mechanism (also know as an *attention head*) in a Transformer is
    designed to do exactly this. It is able to decide where in the input it wants
    to pull information from, in order to efficiently extract useful information without
    being clouded by irrelevant details. This makes it highly adaptable to a range
    of circumstances, as it can decide where it wants to look for information at inference
    time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer中的注意力机制（也称为*注意力头*）旨在做到这一点。它能够决定从输入的哪个位置提取信息，以有效地提取有用信息而不被无关细节混淆。这使得它非常适应各种情况，因为它可以在推断时决定在哪里寻找信息。
- en: In contrast, a recurrent layer tries to build up a generic hidden state that
    captures an overall representation of the input at each timestep. A weakness of
    this approach is that many of the words that have already been incorporated into
    the hidden vector will not be directly relevant to the immediate task at hand
    (e.g., predicting the next word), as we have just seen. Attention heads do not
    suffer from this problem, because they can pick and choose how to combine information
    from nearby words, depending on the context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，循环层试图建立一个捕捉每个时间步输入的整体表示的通用隐藏状态。这种方法的一个弱点是，已经合并到隐藏向量中的许多单词对当前任务（例如，预测下一个单词）并不直接相关，正如我们刚刚看到的。注意力头不会遇到这个问题，因为它们可以选择如何从附近的单词中组合信息，具体取决于上下文。
- en: Queries, Keys, and Values
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询、键和值
- en: So how does an attention head decide where it wants to look for information?
    Before we get into the details, let’s explore how it works at a high level, using
    our *pink elephant* example.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，注意力头如何决定在哪里查找信息呢？在深入细节之前，让我们以高层次的方式探讨它是如何工作的，使用我们的*粉色大象*示例。
- en: Imagine that we want to predict what follows the word *too*. To help with this
    task, other preceding words chime in with their opinions, but their contributions
    are weighted by how confident they are in their own expertise in predicting words
    that follow *too*. For example, the word *elephant* might confidently contribute
    that it is more likely to be a word related to size or loudness, whereas the word
    *was* doesn’t have much to offer to narrow down the possibilities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想预测跟在单词*too*后面的是什么。为了帮助完成这个任务，其他前面的单词发表意见，但他们的贡献受到他们对自己预测跟在*too*后面的单词的信心程度的加权。例如，单词*elephant*可能自信地贡献说，它更有可能是与大小或响度相关的单词，而单词*was*没有太多可以提供来缩小可能性。
- en: In other words, we can think of an attention head as a kind of information retrieval
    system, where a *query* (“What word follows *too*?”) is made into a *key/value*
    store (other words in the sentence) and the resulting output is a sum of the values,
    weighted by the *resonance* between the query and each key.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将注意力头视为一种信息检索系统，其中一个“查询”（“后面跟着什么词？”）被转换为一个*键/值*存储（句子中的其他单词），输出结果是值的加权和，权重由查询和每个键之间的*共鸣*决定。
- en: We will now walk through the process in detail ([Figure 9-2](#attention_head)),
    again with reference to our *pink elephant* sentence.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将详细介绍这个过程（[图9-2](#attention_head)），再次参考我们的*粉色大象*句子。
- en: '![](Images/gdl2_0902.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0902.png)'
- en: Figure 9-2\. The mechanics of an attention head
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。注意力头的机制
- en: The *query* ( <math alttext="upper Q"><mi>Q</mi></math> ) can be thought of
    as a representation of the current task at hand (e.g., “What word follows *too*?”).
    In this example, it is derived from the embedding of the word *too*, by passing
    it through a weights matrix <math alttext="upper W Subscript upper Q"><msub><mi>W</mi>
    <mi>Q</mi></msub></math> to change the dimensionality of the vector from <math
    alttext="d Subscript e"><msub><mi>d</mi> <mi>e</mi></msub></math> to <math alttext="d
    Subscript k"><msub><mi>d</mi> <mi>k</mi></msub></math> .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*查询*（<math alttext="upper Q"><mi>Q</mi></math>）可以被视为当前任务的表示（例如，“后面跟着什么词？”）。在这个例子中，它是从单词*too*的嵌入中导出的，通过将其通过权重矩阵<math
    alttext="upper W Subscript upper Q"><msub><mi>W</mi> <mi>Q</mi></msub></math>传递来将向量的维度从<math
    alttext="d Subscript e"><msub><mi>d</mi> <mi>e</mi></msub></math>更改为<math alttext="d
    Subscript k"><msub><mi>d</mi> <mi>k</mi></msub></math>。'
- en: The *key* vectors ( <math alttext="upper K"><mi>K</mi></math> ) are representations
    of each word in the sentence—you can think of these as descriptions of the kinds
    of prediction tasks that each word can help with. They are derived in a similar
    fashion to the query, by passing each embedding through a weights matrix <math
    alttext="upper W Subscript upper K"><msub><mi>W</mi> <mi>K</mi></msub></math>
    to change the dimensionality of each vector from <math alttext="d Subscript e"><msub><mi>d</mi>
    <mi>e</mi></msub></math> to <math alttext="d Subscript k"><msub><mi>d</mi> <mi>k</mi></msub></math>
    . Notice that the keys and the query are the same length ( <math alttext="d Subscript
    k"><msub><mi>d</mi> <mi>k</mi></msub></math> ).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*键*向量（<math alttext="upper K"><mi>K</mi></math>）是句子中每个单词的表示——您可以将这些视为每个单词可以帮助的预测任务的描述。它们以类似的方式导出查询，通过将每个嵌入通过权重矩阵<math
    alttext="upper W Subscript upper K"><msub><mi>W</mi> <mi>K</mi></msub></math>传递来将每个向量的维度从<math
    alttext="d Subscript e"><msub><mi>d</mi> <mi>e</mi></msub></math>更改为<math alttext="d
    Subscript k"><msub><mi>d</mi> <mi>k</mi></msub></math>。请注意，键和查询具有相同的长度（<math alttext="d
    Subscript k"><msub><mi>d</mi> <mi>k</mi></msub></math>）。'
- en: Inside the attention head, each key is compared to the query using a dot product
    between each pair of vectors ( <math alttext="upper Q upper K Superscript upper
    T"><mrow><mi>Q</mi> <msup><mi>K</mi> <mi>T</mi></msup></mrow></math> ). This is
    why the keys and the query have to be the same length. The higher this number
    is for a particular key/query pair, the more the key resonates with the query,
    so it is allowed to make more of a contribution to the output of the attention
    head. The resulting vector is scaled by <math alttext="StartRoot d Subscript k
    Baseline EndRoot"><msqrt><msub><mi>d</mi> <mi>k</mi></msub></msqrt></math> to
    keep the variance of the vector sum stable (approximately equal to 1), and a softmax
    is applied to ensure the contributions sum to 1\. This is a vector of *attention
    weights*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力头内部，每个键与查询之间的向量对之间使用点积进行比较（<math alttext="upper Q upper K Superscript upper
    T"><mrow><mi>Q</mi> <msup><mi>K</mi> <mi>T</mi></msup></mrow></math>）。这就是为什么键和查询必须具有相同的长度。对于特定的键/查询对，这个数字越高，键与查询的共鸣就越强，因此它可以更多地对注意力头的输出做出贡献。结果向量被缩放为<math
    alttext="StartRoot d Subscript k Baseline EndRoot"><msqrt><msub><mi>d</mi> <mi>k</mi></msub></msqrt></math>，以保持向量和的方差稳定（大约等于1），并且应用softmax以确保贡献总和为1。这是一个*注意力权重*向量。
- en: The *value* vectors ( <math alttext="upper V"><mi>V</mi></math> ) are also representations
    of the words in the sentence—you can think of these as the unweighted contributions
    of each word. They are derived by passing each embedding through a weights matrix
    <math alttext="upper W Subscript upper V"><msub><mi>W</mi> <mi>V</mi></msub></math>
    to change the dimensionality of each vector from <math alttext="d Subscript e"><msub><mi>d</mi>
    <mi>e</mi></msub></math> to <math alttext="d Subscript v"><msub><mi>d</mi> <mi>v</mi></msub></math>
    . Notice that the value vectors do not necessarily have to have the same length
    as the keys and query (but often do, for simplicity).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*值*向量（<math alttext="upper V"><mi>V</mi></math>）也是句子中单词的表示——您可以将这些视为每个单词的未加权贡献。它们通过将每个嵌入通过权重矩阵<math
    alttext="upper W Subscript upper V"><msub><mi>W</mi> <mi>V</mi></msub></math>传递来导出，以将每个向量的维度从<math
    alttext="d Subscript e"><msub><mi>d</mi> <mi>e</mi></msub></math>更改为<math alttext="d
    Subscript v"><msub><mi>d</mi> <mi>v</mi></msub></math>。请注意，值向量不一定要与键和查询具有相同的长度（但通常为了简单起见）。'
- en: The value vectors are multiplied by the attention weights to give the *attention*
    for a given <math alttext="upper Q"><mi>Q</mi></math> , <math alttext="upper K"><mi>K</mi></math>
    , and <math alttext="upper V"><mi>V</mi></math> , as shown in [Equation 9-1](#attention_equation).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 值向量乘以注意力权重，给出给定<math alttext="upper Q"><mi>Q</mi></math>，<math alttext="upper
    K"><mi>K</mi></math>和<math alttext="upper V"><mi>V</mi></math>的*注意力*，如[方程9-1](#attention_equation)所示。
- en: Equation 9-1\. Attention equation
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程9-1。注意力方程
- en: <math alttext="StartLayout 1st Row  upper A t t e n t i o n left-parenthesis
    upper Q comma upper K comma upper V right-parenthesis equals s o f t m a x left-parenthesis
    StartFraction upper Q upper K Superscript upper T Baseline Over StartRoot d Subscript
    k Baseline EndRoot EndFraction right-parenthesis upper V EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>A</mi> <mi>t</mi>
    <mi>t</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mrow><mo>(</mo>
    <mi>Q</mi> <mo>,</mo> <mi>K</mi> <mo>,</mo> <mi>V</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>s</mi> <mi>o</mi> <mi>f</mi> <mi>t</mi> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mrow><mo>(</mo>
    <mfrac><mrow><mi>Q</mi><msup><mi>K</mi> <mi>T</mi></msup></mrow> <msqrt><msub><mi>d</mi>
    <mi>k</mi></msub></msqrt></mfrac> <mo>)</mo></mrow> <mi>V</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  upper A t t e n t i o n left-parenthesis
    upper Q comma upper K comma upper V right-parenthesis equals s o f t m a x left-parenthesis
    StartFraction upper Q upper K Superscript upper T Baseline Over StartRoot d Subscript
    k Baseline EndRoot EndFraction right-parenthesis upper V EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>A</mi> <mi>t</mi>
    <mi>t</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mrow><mo>(</mo>
    <mi>Q</mi> <mo>,</mo> <mi>K</mi> <mo>,</mo> <mi>V</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>s</mi> <mi>o</mi> <mi>f</mi> <mi>t</mi> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mrow><mo>(</mo>
    <mfrac><mrow><mi>Q</mi><msup><mi>K</mi> <mi>T</mi></msup></mrow> <msqrt><msub><mi>d</mi>
    <mi>k</mi></msub></msqrt></mfrac> <mo>)</mo></mrow> <mi>V</mi></mrow></mtd></mtr></mtable></math>
- en: To obtain the final output vector from the attention head, the attention is
    summed to give a vector of length <math alttext="d Subscript v"><msub><mi>d</mi>
    <mi>v</mi></msub></math> . This *context vector* captures a blended opinion from
    words in the sentence on the task of predicting what word follows *too*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从注意力头中获取最终输出向量，将注意力求和得到长度为<math alttext="d Subscript v"><msub><mi>d</mi> <mi>v</mi></msub></math>的向量。这个*上下文向量*捕捉了句子中单词对于预测接下来的单词是什么的任务的混合意见。
- en: Multihead Attention
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: There’s no reason to stop at just one attention head! In Keras, we can build
    a `MultiHeadAttention` layer that concatenates the output from multiple attention
    heads, allowing each to learn a distinct attention mechanism so that the layer
    as a whole can learn more complex relationships.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由只停留在一个注意力头上！在Keras中，我们可以构建一个`MultiHeadAttention`层，将多个注意力头的输出连接起来，使每个头学习不同的注意力机制，从而使整个层能够学习更复杂的关系。
- en: The concatenated outputs are passed through one final weights matrix <math alttext="upper
    W Subscript upper O"><msub><mi>W</mi> <mi>O</mi></msub></math> to project the
    vector into the desired output dimension, which in our case is the same as the
    input dimension of the query ( <math alttext="d Subscript e"><msub><mi>d</mi>
    <mi>e</mi></msub></math> ), so that the layers can be stacked sequentially on
    top of each other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 连接的输出通过一个最终的权重矩阵<math alttext="upper W Subscript upper O"><msub><mi>W</mi> <mi>O</mi></msub></math>传递，将向量投影到所需的输出维度，这在我们的情况下与查询的输入维度相同（<math
    alttext="d Subscript e"><msub><mi>d</mi> <mi>e</mi></msub></math>），以便层可以顺序堆叠在一起。
- en: '[Figure 9-3](#multi_attention_layer) shows how the output from a `MultiHeadAttention`
    layer is constructed. In Keras we can simply write the line shown in [Example 9-2](#multihead_attention_keras)
    to create such a layer.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](#multi_attention_layer)展示了一个`MultiHeadAttention`层的输出是如何构建的。在Keras中，我们可以简单地写下[示例9-2](#multihead_attention_keras)中显示的代码来创建这样一个层。'
- en: Example 9-2\. Creating a `MultiHeadAttention` layer in Keras
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-2。在Keras中创建一个`MultiHeadAttention`层
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_transformers_CO1-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformers_CO1-1)'
- en: This multihead attention layer has four heads.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多头注意力层有四个头。
- en: '[![2](Images/2.png)](#co_transformers_CO1-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformers_CO1-2)'
- en: The keys (and query) are vectors of length 128.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 键（和查询）是长度为128的向量。
- en: '[![3](Images/3.png)](#co_transformers_CO1-3)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformers_CO1-3)'
- en: The values (and therefore also the output from each head) are vectors of length
    64.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 值（因此也是每个头的输出）是长度为64的向量。
- en: '[![4](Images/4.png)](#co_transformers_CO1-4)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_transformers_CO1-4)'
- en: The output vector has length 256.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出向量的长度为256。
- en: '![](Images/gdl2_0903.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0903.png)'
- en: Figure 9-3\. A multihead attention layer with four heads
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3。一个具有四个头的多头注意力层
- en: Causal Masking
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因果掩码
- en: So far, we have assumed that the query input to our attention head is a single
    vector. However, for efficiency during training, we would ideally like the attention
    layer to be able to operate on every word in the input at once, predicting for
    each what the subsequent word will be. In other words, we want our GPT model to
    be able to handle a group of query vectors in parallel (i.e., a matrix).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设我们的注意力头的查询输入是一个单一的向量。然而，在训练期间为了效率，我们理想情况下希望注意力层能够一次操作输入中的每个单词，为每个单词预测接下来的单词。换句话说，我们希望我们的GPT模型能够并行处理一组查询向量（即一个矩阵）。
- en: You might think that we can just batch the vectors together into a matrix and
    let linear algebra handle the rest. This is true, but we need one extra step—we
    need to apply a mask to the query/key dot product, to avoid information from future
    words leaking through. This is known as *causal masking* and is shown in [Figure 9-4](#causal_mask).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会认为我们可以将向量批量处理成一个矩阵，让线性代数处理剩下的部分。这是正确的，但我们需要一个额外的步骤——我们需要对查询/键的点积应用一个掩码，以避免未来单词的信息泄漏。这被称为*因果掩码*，在[图9-4](#causal_mask)中显示。
- en: '![](Images/gdl2_0904.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0904.png)'
- en: Figure 9-4\. Matrix calculation of the attention scores for a batch of input
    queries, using a causal attention mask to hide keys that are not available to
    the query (because they come later in the sentence)
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。对一批输入查询计算注意力分数的矩阵，使用因果注意力掩码隐藏对查询不可用的键（因为它们在句子中后面）
- en: Without this mask, our GPT model would be able to perfectly guess the next word
    in the sentence, because it would be using the key from the word itself as a feature!
    The code for creating a causal mask is shown in [Example 9-3](#causal_mask_code),
    and the resulting `numpy` array (transposed to match the diagram) is shown in
    [Figure 9-5](#causal_mask_numpy).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这个掩码，我们的GPT模型将能够完美地猜测句子中的下一个单词，因为它将使用单词本身的键作为特征！创建因果掩码的代码显示在[示例9-3](#causal_mask_code)中，结果的`numpy`数组（转置以匹配图表）显示在[图9-5](#causal_mask_numpy)中。
- en: Example 9-3\. The causal mask function
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-3。因果掩码函数
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](Images/gdl2_0905.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0905.png)'
- en: Figure 9-5\. The causal mask as a `numpy` array—1 means unmasked and 0 means
    masked
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5。作为`numpy`数组的因果掩码——1表示未掩码，0表示掩码
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Causal masking is only required in *decoder Transformers* such as GPT, where
    the task is to sequentially generate tokens given previous tokens. Masking out
    future tokens during training is therefore essential.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因果掩码仅在*解码器Transformer*（如GPT）中需要，其中任务是根据先前的标记顺序生成标记。在训练期间屏蔽未来标记因此至关重要。
- en: Other flavors of Transformer (e.g., *encoder Transformers*) do not need causal
    masking, because they are not trained to predict the next token. For example Google’s
    BERT predicts masked words within a given sentence, so it can use context from
    both before and after the word in question.^([3](ch09.xhtml#idm45387006370384))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的Transformer（例如*编码器Transformer*）不需要因果掩码，因为它们不是训练来预测下一个标记。例如，Google的BERT预测给定句子中的掩码单词，因此它可以使用单词之前和之后的上下文。^([3](ch09.xhtml#idm45387006370384))
- en: We will explore the different types of Transformers in more detail at the end
    of the chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章末尾更详细地探讨不同类型的Transformer。
- en: This concludes our explanation of the multihead attention mechanism that is
    present in all Transformers. It is remarkable that the learnable parameters of
    such an influential layer consist of nothing more than three densely connected
    weights matrices for each attention head ( <math alttext="upper W Subscript upper
    Q"><msub><mi>W</mi> <mi>Q</mi></msub></math> , <math alttext="upper W Subscript
    upper K"><msub><mi>W</mi> <mi>K</mi></msub></math> , <math alttext="upper W Subscript
    upper V"><msub><mi>W</mi> <mi>V</mi></msub></math> ) and one further weights matrix
    to reshape the output ( <math alttext="upper W Subscript upper O"><msub><mi>W</mi>
    <mi>O</mi></msub></math> ). There are no convolutions or recurrent mechanisms
    at all in a multihead attention layer!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对存在于所有Transformer中的多头注意力机制的解释。令人惊讶的是，这样一个有影响力的层的可学习参数仅由每个注意力头的三个密集连接权重矩阵（<math
    alttext="upper W Subscript upper Q"><msub><mi>W</mi> <mi>Q</mi></msub></math>，<math
    alttext="upper W Subscript upper K"><msub><mi>W</mi> <mi>K</mi></msub></math>，<math
    alttext="upper W Subscript upper V"><msub><mi>W</mi> <mi>V</mi></msub></math>）和一个进一步的权重矩阵来重塑输出（<math
    alttext="upper W Subscript upper O"><msub><mi>W</mi> <mi>O</mi></msub></math>）。在多头注意力层中完全没有卷积或循环机制！
- en: Next, we shall take a step back and see how the multihead attention layer forms
    just one part of a larger component known as a *Transformer block*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将退一步，看看多头注意力层如何形成更大组件的一部分，这个组件被称为*Transformer块*。
- en: The Transformer Block
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer块
- en: A *Transformer block* is a single component within a Transformer that applies
    some skip connections, feed-forward (dense) layers, and normalization around the
    multihead attention layer. A diagram of a Transformer block is shown in [Figure 9-6](#transformer_block).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformer块*是Transformer中的一个单一组件，它应用一些跳跃连接、前馈（密集）层和在多头注意力层周围的归一化。Transformer块的示意图显示在[图9-6](#transformer_block)中。'
- en: '![](Images/gdl2_0906.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0906.png)'
- en: Figure 9-6\. A Transformer block
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6。一个Transformer块
- en: Firstly, notice how the query is passed around the multihead attention layer
    to be added to the output—this is a skip connection and is common in modern deep
    learning architectures. It means we can build very deep neural networks that do
    not suffer as much from the vanishing gradient problem, because the skip connection
    provides a gradient-free *highway* that allows the network to transfer information
    forward uninterrupted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到查询是如何在多头注意力层周围传递并添加到输出中的——这是一个跳跃连接，在现代深度学习架构中很常见。这意味着我们可以构建非常深的神经网络，不会受到梯度消失问题的困扰，因为跳跃连接提供了一个无梯度的*高速公路*，允许网络将信息向前传递而不中断。
- en: Secondly, *layer normalization* is used in the Transformer block to provide
    stability to the training process. We have already seen the batch normalization
    layer in action throughout this book, where the output from each channel is normalized
    to have a mean of 0 and standard deviation of 1\. The normalization statistics
    are calculated across the batch and spatial dimensions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在Transformer块中使用*层归一化*来提供训练过程的稳定性。我们已经在本书中看到了批归一化层的作用，其中每个通道的输出被归一化为均值为0，标准差为1。归一化统计量是跨批次和空间维度计算的。
- en: In contrast, layer normalization in a Transformer block normalizes each position
    of each sequence in the batch by calculating the normalizing statistics across
    the channels. It is the complete opposite of batch normalization, in terms of
    how the normalization statistics are calculated. A diagram showing the difference
    between batch normalization and layer normalization is shown in [Figure 9-7](#layer_norm).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在Transformer块中，层归一化通过计算跨通道的归一化统计量来归一化批次中每个序列的每个位置。就归一化统计量的计算方式而言，它与批归一化完全相反。显示批归一化和层归一化之间差异的示意图显示在[图9-7](#layer_norm)中。
- en: '![](Images/gdl2_0907.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0907.png)'
- en: 'Figure 9-7\. Layer normalization versus batch normalization—the normalization
    statistics are calculated across the blue cells (source: [Sheng et al., 2020](https://arxiv.org/pdf/2003.07845.pdf))^([4](ch09.xhtml#idm45387006340992))'
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7。层归一化与批归一化——归一化统计量是跨蓝色单元计算的（来源：[Sheng等人，2020](https://arxiv.org/pdf/2003.07845.pdf))^([4](ch09.xhtml#idm45387006340992))
- en: Layer Normalization Versus Batch Normalization
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层归一化与批归一化
- en: Layer normalization was used in the original GPT paper and is commonly used
    for text-based tasks to avoid creating normalization dependencies across sequences
    in the batch. However, recent work such as Shen et al.*s* challenges this assumption,
    showing that with some tweaks a form of batch normalization can still be used
    within Transformers, outperforming more traditional layer normalization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化在原始GPT论文中使用，并且通常用于基于文本的任务，以避免在批次中的序列之间创建归一化依赖关系。然而，最近的工作，如Shen等人的挑战了这一假设，显示通过一些调整，一种形式的批归一化仍然可以在Transformer中使用，胜过更传统的层归一化。
- en: Lastly, a set of feed-forward (i.e., densely connected) layers is included in
    the Transformer block, to allow the component to extract higher-level features
    as we go deeper into the network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在Transformer块中包含了一组前馈（即密集连接）层，以允许组件在网络深入时提取更高级别的特征。
- en: A Keras implementation of a Transformer block is shown in [Example 9-4](#transformer_block_code2).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中展示了一个Transformer块的实现，详见[示例9-4](#transformer_block_code2)。
- en: Example 9-4\. A `TransformerBlock` layer in Keras
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-4。Keras中的`TransformerBlock`层
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_transformers_CO2-1)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformers_CO2-1)'
- en: The sublayers that make up the `TransformerBlock` layer are defined within the
    initialization function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 构成`TransformerBlock`层的子层在初始化函数中定义。
- en: '[![2](Images/2.png)](#co_transformers_CO2-2)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformers_CO2-2)'
- en: The causal mask is created to hide future keys from the query.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因果掩码被创建用来隐藏查询中的未来键。
- en: '[![3](Images/3.png)](#co_transformers_CO2-3)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformers_CO2-3)'
- en: The multihead attention layer is created, with the attention masks specified.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了多头注意力层，并指定了注意力掩码。
- en: '[![4](Images/4.png)](#co_transformers_CO2-4)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_transformers_CO2-4)'
- en: The first *add and normalization* layer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个*加和归一化*层。
- en: '[![5](Images/5.png)](#co_transformers_CO2-5)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_transformers_CO2-5)'
- en: The feed-forward layers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈层。
- en: '[![6](Images/6.png)](#co_transformers_CO2-6)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_transformers_CO2-6)'
- en: The second *add and normalization* layer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个*加和归一化*层。
- en: Positional Encoding
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'There is one final step to cover before we can put everything together to train
    our GPT model. You may have noticed that in the multihead attention layer, there
    is nothing that cares about the ordering of the keys. The dot product between
    each key and the query is calculated in parallel, not sequentially, like in a
    recurrent neural network. This is a strength (because of the parallelization efficiency
    gains) but also a problem, because we clearly need the attention layer to be able
    to predict different outputs for the following two sentences:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够将所有内容整合在一起训练我们的GPT模型之前，还有一个最后的步骤要解决。您可能已经注意到，在多头注意力层中，没有任何关心键的顺序的内容。每个键和查询之间的点积是并行计算的，而不是像递归神经网络那样顺序计算。这是一种优势（因为并行化效率提高），但也是一个问题，因为我们显然需要注意力层能够预测以下两个句子的不同输出：
- en: The dog looked at the boy and …​ (barked?)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗看着男孩然后…（叫？）
- en: The boy looked at the dog and …​ (smiled?)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 男孩看着狗然后…（微笑？）
- en: To solve this problem, we use a technique called *positional encoding* when
    creating the inputs to the initial Transformer block. Instead of only encoding
    each token using a *token embedding*, we also encode the position of the token,
    using a *position embedding*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们在创建初始Transformer块的输入时使用一种称为*位置编码*的技术。我们不仅使用*标记嵌入*对每个标记进行编码，还使用*位置嵌入*对标记的位置进行编码。
- en: The *token embedding* is created using a standard `Embedding` layer to convert
    each token into a learned vector. We can create the *positional embedding* in
    the same way, using a standard `Embedding` layer to convert each integer position
    into a learned vector.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*标记嵌入*是使用标准的`Embedding`层创建的，将每个标记转换为一个学习到的向量。我们可以以相同的方式创建*位置嵌入*，使用标准的`Embedding`层将每个整数位置转换为一个学习到的向量。'
- en: Tip
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: While GPT uses an `Embedding` layer to embed the position, the original Transformer
    paper used trigonometric functions—we’ll cover this alternative in [Chapter 11](ch11.xhtml#chapter_music),
    when we explore music generation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPT使用`Embedding`层来嵌入位置，但原始Transformer论文使用三角函数——我们将在[第11章](ch11.xhtml#chapter_music)中介绍这种替代方法，当我们探索音乐生成时。
- en: To construct the joint token–position encoding, the token embedding is added
    to the positional embedding, as shown in [Figure 9-8](#positional_enc). This way,
    the meaning and position of each word in the sequence are captured in a single
    vector.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为构建联合标记-位置编码，将标记嵌入加到位置嵌入中，如[图9-8](#positional_enc)所示。这样，序列中每个单词的含义和位置都被捕捉在一个向量中。
- en: '![](Images/gdl2_0908.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0908.png)'
- en: Figure 9-8\. The token embeddings are added to the positional embeddings to
    give the token position encoding
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 将标记嵌入添加到位置嵌入以给出标记位置编码
- en: The code that defines our `TokenAndPositionEmbedding` layer is shown in [Example 9-5](#positional_embedding_code).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们的`TokenAndPositionEmbedding`层的代码显示在[示例9-5](#positional_embedding_code)中。
- en: Example 9-5\. The `TokenAndPositionEmbedding` layer
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-5\. `TokenAndPositionEmbedding`层
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_transformers_CO3-1)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformers_CO3-1)'
- en: The tokens are embedded using an `Embedding` layer.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 标记使用`Embedding`层进行嵌入。
- en: '[![2](Images/2.png)](#co_transformers_CO3-2)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformers_CO3-2)'
- en: The positions of the tokens are also embedded using an `Embedding` layer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的位置也使用`Embedding`层进行嵌入。
- en: '[![3](Images/3.png)](#co_transformers_CO3-3)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformers_CO3-3)'
- en: The output from the layer is the sum of the token and position embeddings.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的输出是标记和位置嵌入的总和。
- en: Training GPT
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练GPT
- en: Now we are ready to build and train our GPT model! To put everything together,
    we need to pass our input text through the token and position embedding layer,
    then through our Transformer block. The final output of the network is a simple
    `Dense` layer with softmax activation over the number of words in the vocabulary.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建和训练我们的GPT模型！为了将所有内容整合在一起，我们需要将输入文本通过标记和位置嵌入层，然后通过我们的Transformer块。网络的最终输出是一个简单的具有softmax激活函数的`Dense`层，覆盖词汇表中的单词数量。
- en: Tip
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For simplicity, we will use just one Transformer block, rather than the 12 in
    the paper.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们将只使用一个Transformer块，而不是论文中的12个。
- en: The overall architecture is shown in [Figure 9-9](#transformer) and the equivalent
    code is provided in [Example 9-6](#transformer_code).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 整体架构显示在[图9-9](#transformer)中，相应的代码在[示例9-6](#transformer_code)中提供。
- en: '![](Images/gdl2_0909.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0909.png)'
- en: Figure 9-9\. The simplified GPT model architecture
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9\. 简化的GPT模型架构
- en: Example 9-6\. A GPT model in Keras
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-6\. 在Keras中的GPT模型
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_transformers_CO4-1)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformers_CO4-1)'
- en: The input is padded (with zeros).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输入被填充（用零填充）。
- en: '[![2](Images/2.png)](#co_transformers_CO4-2)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformers_CO4-2)'
- en: The text is encoded using a `TokenAndPositionEmbedding` layer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 文本使用`TokenAndPositionEmbedding`层进行编码。
- en: '[![3](Images/3.png)](#co_transformers_CO4-3)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformers_CO4-3)'
- en: The encoding is passed through a `TransformerBlock`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 编码通过`TransformerBlock`传递。
- en: '[![4](Images/4.png)](#co_transformers_CO4-4)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_transformers_CO4-4)'
- en: The transformed output is passed through a `Dense` layer with softmax activation
    to predict a distribution over the subsequent word.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的输出通过具有softmax激活的`Dense`层传递，以预测后续单词的分布。
- en: '[![5](Images/5.png)](#co_transformers_CO4-5)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_transformers_CO4-5)'
- en: The `Model` takes a sequence of word tokens as input and outputs the predicted
    subsequent word distribution. The output from the Transformer block is also returned
    so that we can inspect how the model is directing its attention.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Model`以单词标记序列作为输入，并输出预测的后续单词分布。还返回了Transformer块的输出，以便我们可以检查模型如何引导其注意力。'
- en: '[![6](Images/6.png)](#co_transformers_CO4-6)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_transformers_CO4-6)'
- en: The model is compiled with `SparseCategoricalCrossentropy` loss over the predicted
    word distribution.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用预测的单词分布上的`SparseCategoricalCrossentropy`损失进行编译。
- en: Analysis of GPT
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT的分析
- en: Now that we have compiled and trained our GPT model, we can start to use it
    to generate long strings of text. We can also interrogate the attention weights
    that are output from the `TransformerBlock`, to understand where the Transformer
    is looking for information at different points in the generation process.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编译并训练了我们的GPT模型，我们可以开始使用它生成长文本字符串。我们还可以询问从`TransformerBlock`输出的注意权重，以了解Transformer在生成过程中不同点处寻找信息的位置。
- en: Generating text
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成文本
- en: 'We can generate new text by applying the following process:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下过程生成新文本：
- en: Feed the network with an existing sequence of words and ask it to predict the
    following word.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将现有单词序列馈送到网络中，并要求它预测接下来的单词。
- en: Append this word to the existing sequence and repeat.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此单词附加到现有序列并重复。
- en: The network will output a set of probabilities for each word that we can sample
    from, so we can make the text generation stochastic, rather than deterministic.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将为每个单词输出一组概率，我们可以从中进行抽样，因此我们可以使文本生成具有随机性，而不是确定性。
- en: We will use the same `TextGenerator` class introduced in [Chapter 5](ch05.xhtml#chapter_autoregressive)
    for LSTM text generation, including the `temperature` parameter that specifies
    how deterministic we would like the sampling process to be. Let’s take a look
    at this in action, at two different temperature values ([Figure 9-10](#transformer_examples)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在[第5章](ch05.xhtml#chapter_autoregressive)中引入的相同`TextGenerator`类进行LSTM文本生成，包括指定采样过程的确定性程度的`temperature`参数。让我们看看这在两个不同的温度值（[图9-10](#transformer_examples)）下是如何运作的。
- en: '![](Images/gdl2_0910.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0910.png)'
- en: Figure 9-10\. Generated outputs at `temperature = 1.0` and `temperature = 0.5`.
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10。在`temperature = 1.0`和`temperature = 0.5`时生成的输出。
- en: There are a few things to note about these two passages. First, both are stylistically
    similar to a wine review from the original training set. They both open with the
    region and type of wine, and the wine type stays consistent throughout the passage
    (for example, it doesn’t switch color halfway through). As we saw in [Chapter 5](ch05.xhtml#chapter_autoregressive),
    the generated text with temperature 1.0 is more adventurous and therefore less
    accurate than the example with temperature 0.5\. Generating multiple samples with
    temperature 1.0 will therefore lead to more variety as the model is sampling from
    a probability distribution with greater variance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这两段文字有几点需要注意。首先，两者在风格上与原始训练集中的葡萄酒评论相似。它们都以葡萄酒的产地和类型开头，而葡萄酒类型在整个段落中保持一致（例如，它不会在中途更换颜色）。正如我们在[第5章](ch05.xhtml#chapter_autoregressive)中看到的，使用温度为1.0生成的文本更加冒险，因此比温度为0.5的示例不够准确。因此，使用温度为1.0生成多个样本将导致更多的变化，因为模型正在从具有更大方差的概率分布中进行抽样。
- en: Viewing the attention scores
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看注意力分数
- en: We can also ask the model to tell us how much attention is being placed on each
    word, when deciding on the next word in the sentence. The `TransformerBlock` outputs
    the attention weights for each head, which are a softmax distribution over the
    preceding words in the sentence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以要求模型告诉我们在决定句子中的下一个单词时，每个单词放置了多少注意力。`TransformerBlock`输出每个头的注意权重，这是对句子中前面单词的softmax分布。
- en: To demonstrate this, [Figure 9-11](#attention_probs) shows the top five tokens
    with the highest probabilities for three different input prompts, as well as the
    average attention across both heads, against each preceding word. The preceding
    words are colored according to their attention score, averaged across the two
    attention heads. Darker blue indicates more attention is being placed on the word.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，[图9-11](#attention_probs)显示了三个不同输入提示的前五个具有最高概率的标记，以及两个注意力头的平均注意力，针对每个前面的单词。根据其注意力分数对前面的单词进行着色，两个注意力头的平均值。深蓝色表示对该单词放置更多的注意力。
- en: '![](Images/gdl2_0911.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0911.png)'
- en: Figure 9-11\. Distribution of word probabilities following various sequences
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11。各种序列后单词概率分布
- en: In the first example, the model attends closely to the country (*germany*) in
    order to decide on the word that relates to the region. This makes sense! To pick
    a region, it needs to take lots of information from the words that relate to the
    country, to ensure they match. It doesn’t need to pay as much attention to the
    first two tokens (*wine review*) because they don’t hold any useful information
    regarding the region.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，模型密切关注国家（*德国*），以决定与地区相关的单词。这是有道理的！为了选择一个地区，它需要从与国家相关的单词中获取大量信息，以确保它们匹配。它不需要太关注前两个标记（*葡萄酒评论*），因为它们不包含有关地区的任何有用信息。
- en: In the second example, it needs to refer back to the grape (*riesling*), so
    it pays attention to the first time that it was mentioned. It can pull this information
    by directly attending to the word, no matter how far back it is in the sentence
    (within the upper limit of 80 words). Notice that this is very different from
    a recurrent neural network, which relies on a hidden state to maintain all interesting
    information over the length of the sequence so that it can be drawn upon if required—a
    much less efficient approach.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个例子中，它需要参考葡萄（*雷司令*），因此它关注第一次提到它的时间。它可以通过直接关注这个词来提取这个信息，无论这个词在句子中有多远（在80个单词的上限内）。请注意，这与递归神经网络非常不同，后者依赖于隐藏状态来维护整个序列的所有有趣信息，以便在需要时可以利用——这是一种效率低下得多的方法。
- en: The final sequence shows an example of how our GPT model can choose an appropriate
    adjective based on a combination of information. Here the attention is again on
    the grape (*riesling*), but also on the fact that it contains *residual sugar*.
    As Riesling is typically a sweet wine, and sugar is already mentioned, it makes
    sense that it should be described as *slightly sweet* rather than *slightly earthy*,
    for example.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的序列展示了我们的GPT模型如何基于信息的组合选择适当的形容词的例子。这里的注意力再次集中在葡萄（*雷司令*）上，但也集中在它含有*残留糖*的事实上。由于雷司令通常是一种甜酒，而且已经提到了糖，因此将其描述为*略带甜味*而不是*略带泥土味*是有道理的。
- en: It is incredibly informative to be able to interrogate the network in this way,
    to understand exactly where it is pulling information from in order to make accurate
    decisions about each subsequent word. I highly recommend playing around with the
    input prompts to see if you can get the model to attend to words really far back
    in the sentence, to convince yourself of the power of attention-based models over
    more traditional recurrent models!`  `# Other Transformers
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式询问网络非常有启发性，可以准确了解它从哪里提取信息，以便对每个后续单词做出准确的决策。我强烈建议尝试玩弄输入提示，看看是否可以让模型关注句子中非常遥远的单词，以说服自己关注模型的注意力模型比传统的递归模型更具有力量！`  `#
    其他Transformer
- en: Our GPT model is a *decoder Transformer*—it generates a text string one token
    at a time and uses causal masking to only attend to previous words in the input
    string. There are also *encoder Transformers*, which do not use causal masking—instead,
    they attend to the entire input string in order to extract a meaningful contextual
    representation of the input. For other tasks, such as language translation, there
    are also *encoder-decoder Transformers* that can translate from one text string
    to another; this type of model contains both encoder Transformer blocks and decoder
    Transformer blocks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPT模型是一个*解码器Transformer*——它一次生成一个标记的文本字符串，并使用因果屏蔽只关注输入字符串中的先前单词。还有*编码器Transformer*，它不使用因果屏蔽——相反，它关注整个输入字符串以提取输入的有意义的上下文表示。对于其他任务，比如语言翻译，还有*编码器-解码器Transformer*，可以将一个文本字符串翻译成另一个；这种模型包含编码器Transformer块和解码器Transformer块。
- en: '[Table 9-1](#transformer_types) summarizes the three types of Transformers,
    with the best examples of each architecture and typical use cases.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9-1](#transformer_types)总结了三种Transformer的类型，以及每种架构的最佳示例和典型用例。'
- en: Table 9-1\. The three Transformer architectures
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1。三种Transformer架构
- en: '| Type | Examples | Use cases |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 示例 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Encoder | BERT (Google) | Sentence classification, named entity recognition,
    extractive question answering |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 编码器 | BERT（谷歌） | 句子分类、命名实体识别、抽取式问答 |'
- en: '| Encoder-decoder | T5 (Google) | Summarization, translation, question answering
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 编码器-解码器 | T5（谷歌） | 摘要、翻译、问答 |'
- en: '| Decoder | GPT-3 (OpenAI) | Text generation |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 解码器 | GPT-3（OpenAI） | 文本生成 |'
- en: A well-known example of an encoder Transformer is the *Bidirectional Encoder
    Representations from Transformers* (BERT) model, developed by Google (Devlin et
    al., 2018) that predicts missing words from a sentence, given context from both
    before and after the missing word in all layers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一个众所周知的编码器Transformer的例子是谷歌开发的*双向编码器表示来自Transformer*（BERT）模型，它可以根据缺失单词的上下文预测句子中的缺失单词（Devlin等，2018）。
- en: Encoder Transformers
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器Transformer
- en: Encoder Transformers are typically used for tasks that require an understanding
    of the input as a whole, such as sentence classification, named entity recognition,
    and extractive question answering. They are not used for text generation tasks,
    so we will not explore them in detail in this book—see Lewis Tunstall et al.’s
    [*Natural Language Processing with Transformers*](https://www.oreilly.com/library/view/natural-language-processing/9781098136789)
    (O’Reilly) for more information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器Transformer通常用于需要全面理解输入的任务，比如句子分类、命名实体识别和抽取式问答。它们不用于文本生成任务，因此我们不会在本书中详细探讨它们——有关更多信息，请参阅Lewis
    Tunstall等人的[*使用Transformer进行自然语言处理*](https://www.oreilly.com/library/view/natural-language-processing/9781098136789)（O'Reilly）。
- en: In the following sections we will explore how encoder-decoder transformers work
    and discuss extensions of the original GPT model architecture released by OpenAI,
    including ChatGPT, which has been specifically designed for conversational applications.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨编码器-解码器Transformer的工作原理，并讨论OpenAI发布的原始GPT模型架构的扩展，包括专门为对话应用设计的ChatGPT。
- en: T5
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T5
- en: An example of a modern Transformer that uses the encoder-decoder structure is
    the T5 model from Google.^([5](ch09.xhtml#idm45387005361120)) This model reframes
    a range of tasks into a text-to-text framework, including translation, linguistic
    acceptability, sentence similarity, and document summarization, as shown in [Figure 9-12](#t5).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用编码器-解码器结构的现代Transformer的例子是谷歌的T5模型。这个模型将一系列任务重新构建为文本到文本的框架，包括翻译、语言可接受性、句子相似性和文档摘要，如[图9-12](#t5)所示。
- en: '![](Images/gdl2_0912.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0912.png)'
- en: 'Figure 9-12\. Examples of how T5 reframes a range of tasks into a text-to-text
    framework, including translation, linguistic acceptability, sentence similarity,
    and document summarization (source: [Raffel et al., 2019](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html))'
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12。T5如何将一系列任务重新构建为文本到文本框架的示例，包括翻译、语言可接受性、句子相似性和文档摘要（来源：[Raffel et al., 2019](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)）
- en: The T5 model architecture closely matches the encoder-decoder architecture used
    in the original Transformer paper, shown in [Figure 9-13](#transformer2). The
    key difference is that T5 is trained on an enormous 750 GB corpus of text (the
    Colossal Clean Crawled Corpus, or C4), whereas the original Transformer paper
    was focused only on language translation, so it was trained on 1.4 GB of English–German
    sentence pairs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: T5模型架构与原始Transformer论文中使用的编码器-解码器架构非常相似，如[图9-13](#transformer2)所示。关键区别在于T5是在一个庞大的750GB文本语料库（Colossal
    Clean Crawled Corpus，或C4）上进行训练的，而原始Transformer论文仅关注语言翻译，因此它是在1.4GB的英德句对上进行训练的。
- en: '![](Images/gdl2_0913.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0913.png)'
- en: 'Figure 9-13\. An encoder-decoder Transformer model: each gray box is a Transformer
    block (source: [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))'
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13。编码器-解码器Transformer模型：每个灰色框是一个Transformer块（来源：[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)）
- en: 'Much of this diagram is already familiar to us—we can see the Transformer blocks
    being repeated and positional embedding being used to capture the ordering of
    the input sequences. The two key differences between this model and the GPT model
    that we built earlier in the chapter are as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表中的大部分内容对我们来说已经很熟悉了——我们可以看到Transformer块被重复，并且使用位置嵌入来捕捉输入序列的顺序。这个模型与我们在本章前面构建的GPT模型之间的两个关键区别如下：
- en: On the lefthand side, a set of *encoder* Transformer blocks encode the sequence
    to be translated. Notice that there is no causal masking on the attention layer.
    This is because we are not generating further text to extend the sequence to be
    translated; we just want to learn a good representation of the sequence as a whole
    that can be fed to the decoder. Therefore, the attention layers in the encoder
    can be completely unmasked to capture all the cross-dependencies between words,
    no matter the order.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在左侧，一组*编码器*Transformer块对待翻译的序列进行编码。请注意，注意力层上没有因果屏蔽。这是因为我们不生成更多文本来扩展要翻译的序列；我们只想学习一个可以提供给解码器的整个序列的良好表示。因此，编码器中的注意力层可以完全不加屏蔽，以捕捉单词之间的所有交叉依赖关系，无论顺序如何。
- en: On the righthand side, a set of *decoder* Transformer blocks generate the translated
    text. The initial attention layer is *self-referential* (i.e., the key, value,
    and query come from the same input) and causal masking is used to ensure information
    from future tokens is not leaked to the current word to be predicted. However,
    we can then see that the subsequent attention layer pulls the key and value from
    the encoder, leaving only the query passed through from the decoder itself. This
    is called *cross-referential* attention and means that the decoder can attend
    to the encoder representation of the input sequence to be translated. This is
    how the decoder knows what meaning the translation needs to convey!
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在右侧，一组*解码器*Transformer块生成翻译文本。初始注意力层是*自指*的（即，键、值和查询来自相同的输入），并且使用因果屏蔽确保来自未来标记的信息不会泄漏到当前要预测的单词。然而，我们可以看到随后的注意力层从编码器中提取键和值，只留下查询从解码器本身传递。这被称为*交叉引用*注意力，意味着解码器可以关注输入序列的编码器表示。这就是解码器知道翻译需要传达什么含义的方式！
- en: '[Figure 9-14](#attention_example) shows an example of cross-referential attention.
    Two attention heads of the decoder layer are able to work together to provide
    the correct German translation for the word *the*, when used in the context of
    *the street*. In German, there are three definite articles (*der, die, das*) depending
    on the gender of the noun, but the Transformer knows to choose *die* because one
    attention head is able to attend to the word *street* (a feminine word in German),
    while another attends to the word to translate (*the*).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-14](#attention_example)展示了一个交叉引用注意力的示例。解码器层的两个注意力头能够共同提供单词*the*的正确德语翻译，当它在*the
    street*的上下文中使用时。在德语中，根据名词的性别有三个定冠词（*der, die, das*），但Transformer知道选择*die*，因为一个注意力头能够关注单词*street*（德语中的一个女性词），而另一个关注要翻译的单词（*the*）。'
- en: '![](Images/gdl2_0914.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0914.png)'
- en: Figure 9-14\. An example of how one attention head attends to the word “the”
    and another attends to the word “street” in order to correctly translate the word
    “the” to the German word “die” as the feminine definite article of “Straße”
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-14。一个示例，展示一个注意力头关注单词“the”，另一个关注单词“street”，以便正确将单词“the”翻译为德语单词“die”，作为“Straße”的女性定冠词
- en: Tip
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This example is from the [Tensor2Tensor GitHub repository](https://oreil.ly/84lIA),
    which contains a Colab notebook that allows you to play around with a trained
    encoder-decoder Transformer model and see how the attention mechanisms of the
    encoder and decoder impact the translation of a given sentence into German.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子来自[Tensor2Tensor GitHub存储库](https://oreil.ly/84lIA)，其中包含一个Colab笔记本，让您可以玩转一个经过训练的编码器-解码器Transformer模型，并查看编码器和解码器的注意力机制如何影响将给定句子翻译成德语。
- en: GPT-3 and GPT-4
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3和GPT-4
- en: Since the original 2018 publication of GPT, OpenAI has released multiple updated
    versions that improve upon the original model, as shown in [Table 9-2](#gpt_releases).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年GPT的原始出版以来，OpenAI已发布了多个更新版本，改进了原始模型，如[表9-2](#gpt_releases)所示。
- en: Table 9-2\. The evolution of OpenAI’s GPT collection of models
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-2。OpenAI的GPT系列模型的演变
- en: '| Model | Date | Layers | Attention heads | Word embedding size | Context window
    | # parameters | Training data |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 日期 | 层 | 注意力头 | 词嵌入大小 | 上下文窗口 | 参数数量 | 训练数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    | Jun 2018 | 12 | 12 | 768 | 512 | 120,000,000 | BookCorpus: 4.5 GB of text from
    unpublished books |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    | 2018年6月 | 12 | 12 | 768 | 512 | 120,000,000 | BookCorpus：来自未发表书籍的4.5 GB文本 |'
- en: '| [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    | Feb 2019 | 48 | 48 | 1,600 | 1,024 | 1,500,000,000 | WebText: 40 GB of text
    from outbound Reddit links |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    | 2019年2月 | 48 | 48 | 1,600 | 1,024 | 1,500,000,000 | WebText：来自Reddit外链的40 GB文本
    |'
- en: '| [GPT-3](https://arxiv.org/abs/2005.14165) | May 2020 | 96 | 96 | 12,888 |
    2,048 | 175,000,000,000 | CommonCrawl, WebText, English Wikipedia, book corpora
    and others: 570 GB |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| [GPT-3](https://arxiv.org/abs/2005.14165) | 2020年5月 | 96 | 96 | 12,888 |
    2,048 | 175,000,000,000 | CommonCrawl，WebText，英文维基百科，书籍语料库等：570 GB |'
- en: '| [GPT-4](https://arxiv.org/abs/2303.08774) | Mar 2023 | - | - | - | - | -
    | - |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [GPT-4](https://arxiv.org/abs/2303.08774) | 2023年3月 | - | - | - | - | - |
    - |'
- en: The model architecture of GPT-3 is fairly similar to the original GPT model,
    except it is much larger and trained on much more data. At the time of writing,
    GPT-4 is in limited beta—OpenAI has not publicly released details of the model’s
    structure and size, though we do know that it is able to accept images as input,
    so crosses over into being a multimodal model for the first time. The model weights
    of GPT-3 and GPT-4 are not open source, though the models are available through
    a [commercial tool and API](https://platform.openai.com).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的模型架构与原始GPT模型非常相似，只是规模更大，训练数据更多。在撰写本文时，GPT-4处于有限的测试阶段——OpenAI尚未公开发布模型的结构和规模的详细信息，尽管我们知道它能够接受图像作为输入，因此首次跨越成为多模态模型。GPT-3和GPT-4的模型权重不是开源的，尽管这些模型可以通过[商业工具和API](https://platform.openai.com)获得。
- en: GPT-3 can also be [fine-tuned to your own training data](https://oreil.ly/B-Koo)—this
    allows you to provide multiple examples of how it should react to a given style
    of prompt by physically updating the weights of the network. In many cases this
    may not be necessary, as GPT-3 can be told how to react to a given style of prompt
    simply by providing a few examples in the prompt itself (this is known as *few-shot
    learning*). The benefit of fine-tuning is that you do not need to provide these
    examples as part of every single input prompt, saving costs in the long run.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3也可以[根据您自己的训练数据进行微调](https://oreil.ly/B-Koo)——这使您可以提供多个示例，说明它应该如何对特定风格的提示做出反应，通过物理更新网络的权重。在许多情况下，这可能是不必要的，因为GPT-3可以通过在提示本身提供几个示例来告诉它如何对特定风格的提示做出反应（这被称为*few-shot
    learning*）。微调的好处在于，您不需要在每个单独的输入提示中提供这些示例，从长远来看可以节省成本。
- en: An example of the output from GPT-3, given a system prompt sentence, is shown
    in [Figure 9-15](#gpt3_story).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 给定系统提示句子的GPT-3输出示例显示在[图9-15](#gpt3_story)中。
- en: '![](Images/gdl2_0915.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0915.png)'
- en: Figure 9-15\. An example of how GPT-3 can extend a given system prompt
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-15。GPT-3如何扩展给定系统提示的示例
- en: Language models such as GPT benefit hugely from scaling—both in terms of number
    of model weights and dataset size. The ceiling of large language model capability
    has yet to be reached, with researchers continuing to push the boundaries of what
    is possible with increasingly larger models and datasets.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如GPT之类的语言模型在规模上受益巨大——无论是模型权重的数量还是数据集的大小。大型语言模型能力的上限尚未达到，研究人员继续推动着使用越来越大的模型和数据集所能实现的边界。
- en: ChatGPT
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT
- en: A few months before the beta release of GPT-4, OpenAI announced [*ChatGPT*](https://chat.openai.com)—a
    tool that allows users to interact with their suite of large language models through
    a conversational interface. The original release in November 2022 was powered
    by *GPT-3.5*, a version of the model that was more powerful that GPT-3 and was
    fine-tuned to conversational responses.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-4的测试版发布几个月前，OpenAI宣布了[*ChatGPT*](https://chat.openai.com)——这是一个允许用户通过对话界面与其一系列大型语言模型进行交互的工具。2022年11月的原始版本由*GPT-3.5*提供支持，这个版本比GPT-3更强大，经过微调以进行对话回应。
- en: Example dialogue is shown in [Figure 9-16](#chatgpt_example). Notice how the
    agent is able to maintain state between inputs, understanding that the *attention*
    mentioned in the second question refers to attention in the context of Transformers,
    rather than a person’s ability to focus.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 示例对话显示在[图9-16](#chatgpt_example)中。请注意，代理能够在输入之间保持状态，理解第二个问题中提到的*attention*指的是Transformer上下文中的注意力，而不是一个人的专注能力。
- en: '![](Images/gdl2_0916.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0916.png)'
- en: Figure 9-16\. An example of ChatGPT answering questions about Transformers
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-16。ChatGPT回答有关Transformer的问题的示例
- en: At the time of writing, there is no official paper that describes how ChatGPT
    works in detail, but from the official [blog post](https://openai.com/blog/chatgpt)
    we know that it uses a technique called *reinforcement learning from human feedback*
    (RLHF) to fine-tune the GPT-3.5 model. This technique was also used in the ChatGPT
    group’s earlier paper^([6](ch09.xhtml#idm45387005277024)) that introduced the
    *InstructGPT* model, a fine-tuned GPT-3 model that is specifically designed to
    more accurately follow written instructions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，尚无描述ChatGPT工作详细信息的官方论文，但根据官方[博客文章](https://openai.com/blog/chatgpt)，我们知道它使用一种称为*reinforcement
    learning from human feedback*（RLHF）的技术来微调GPT-3.5模型。这种技术也在ChatGPT小组早期的论文^([6](ch09.xhtml#idm45387005277024))中使用，该论文介绍了*InstructGPT*模型，这是一个经过微调的GPT-3模型，专门设计用于更准确地遵循书面说明。
- en: 'The training process for ChatGPT is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的训练过程如下：
- en: '*Supervised fine-tuning*: Collect a demonstration dataset of conversational
    inputs (prompts) and desired outputs that have been written by humans. This is
    used to fine-tune the underlying language model (GPT-3.5) using supervised learning.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*监督微调*：收集人类编写的对话输入（提示）和期望输出的演示数据集。这用于使用监督学习微调基础语言模型（GPT-3.5）。'
- en: '*Reward modeling*: Present a human labeler with examples of prompts and several
    sampled model outputs and ask them to rank the outputs from best to worst. Train
    a reward model that predicts the score given to each output, given the conversation
    history.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*奖励建模*：向人类标记者展示提示的示例和几个抽样的模型输出，并要求他们将输出从最好到最差进行排名。训练一个奖励模型，预测给定对话历史的每个输出的得分。'
- en: '*Reinforcement learning*: Treat the conversation as a reinforcement learning
    environment where the *policy* is the underlying language model, initialized to
    the fine-tuned model from step 1\. Given the current *state* (the conversation
    history) the policy outputs an *action* (a sequence of tokens), which is scored
    by the reward model trained in step 2\. A reinforcement learning algorithm—proximal
    policy optimization (PPO)—can then be trained to maximize the reward, by adjusting
    the weights of the language model.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*强化学习*：将对话视为一个强化学习环境，其中*策略*是基础语言模型，初始化为从步骤1中微调的模型。给定当前的*状态*（对话历史），策略输出一个*动作*（一系列标记），由在步骤2中训练的奖励模型评分。然后可以训练一个强化学习算法——近端策略优化（PPO），通过调整语言模型的权重来最大化奖励。'
- en: Reinforcement Learning
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: For an introduction to reinforcement learning see [Chapter 12](ch12.xhtml#chapter_world_models),
    where we explore how generative models can be used in a reinforcement learning
    setting.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有关强化学习的介绍，请参阅[第12章](ch12.xhtml#chapter_world_models)，在那里我们探讨了生成模型如何在强化学习环境中使用。
- en: The RLHF process is shown in [Figure 9-17](#rlhf).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF过程如[图9-17](#rlhf)所示。
- en: '![](Images/gdl2_0917.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0917.png)'
- en: 'Figure 9-17\. The reinforcement learning from human feedback fine-tuning process
    used in ChatGPT (source: [OpenAI](https://openai.com/blog/chatgpt))'
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-17。ChatGPT中使用的强化学习来自人类反馈微调过程的示意图（来源：[OpenAI](https://openai.com/blog/chatgpt)）
- en: While ChatGPT still has many limitations (such as sometimes “hallucinating”
    factually incorrect information), it is a powerful example of how Transformers
    can be used to build generative models that can produce complex, long-ranging,
    and novel output that is often indistinguishable from human-generated text. The
    progress made thus far by models like ChatGPT serves as a testament to the potential
    of AI and its transformative impact on the world.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ChatGPT仍然存在许多限制（例如有时“产生”事实不正确的信息），但它是一个强大的示例，展示了Transformers如何用于构建生成模型，可以产生复杂、长期和新颖的输出，往往难以区分是否为人类生成的文本。像ChatGPT这样的模型迄今取得的进展证明了人工智能的潜力及其对世界的变革性影响。
- en: Moreover, it is evident that AI-driven communication and interaction will continue
    to rapidly evolve in the future. Projects like *Visual ChatGPT*^([7](ch09.xhtml#idm45387005252672))
    are now combining the linguistic power of ChatGPT with visual foundation models
    such as Stable Diffusion, enabling users to interact with ChatGPT not only through
    text, but also images. The fusion of linguistic and visual capabilities in projects
    like Visual ChatGPT and GPT-4 have the potential to herald a new era in human–computer
    interaction.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，显而易见的是，基于人工智能的沟通和互动将继续在未来快速发展。像*Visual ChatGPT*^([7](ch09.xhtml#idm45387005252672))这样的项目现在正在将ChatGPT的语言能力与Stable
    Diffusion等视觉基础模型相结合，使用户不仅可以通过文本与ChatGPT互动，还可以通过图像。在像Visual ChatGPT和GPT-4这样的项目中融合语言和视觉能力，有望开启人机交互的新时代。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the Transformer model architecture and built a
    version of GPT—a model for state-of-the-art text generation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了Transformer模型架构，并构建了一个GPT的版本——用于最先进文本生成的模型。
- en: GPT makes use of a mechanism known as attention, which removes the need for
    recurrent layers (e.g., LSTMs). It works like an information retrieval system,
    utilizing queries, keys, and values to decide how much information it wants to
    extract from each input token.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: GPT利用一种称为注意力的机制，消除了循环层（例如LSTM）的需求。它类似于信息检索系统，利用查询、键和值来决定它想要从每个输入标记中提取多少信息。
- en: Attention heads can be grouped together to form what is known as a multihead
    attention layer. These are then wrapped up inside a Transformer block, which includes
    layer normalization and skip connections around the attention layer. Transformer
    blocks can be stacked to create very deep neural networks.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力头可以组合在一起形成所谓的多头注意力层。然后将它们包装在一个Transformer块中，其中包括围绕注意力层的层归一化和跳过连接。Transformer块可以堆叠以创建非常深的神经网络。
- en: Causal masking is used to ensure that GPT cannot leak information from downstream
    tokens into the current prediction. Also, a technique known as positional encoding
    is used to ensure that the ordering of the input sequence is not lost, but instead
    is baked into the input alongside the traditional word embedding.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因果屏蔽用于确保GPT不能从下游标记泄漏信息到当前预测中。此外，还使用一种称为位置编码的技术，以确保输入序列的顺序不会丢失，而是与传统的词嵌入一起嵌入到输入中。
- en: When analyzing the output from GPT, we saw it was possible not only to generate
    new text passages, but also to interrogate the attention layer of the network
    to understand where in the sentence it is looking to gather information to improve
    its prediction. GPT can access information at a distance without loss of signal,
    because the attention scores are calculated in parallel and do not rely on a hidden
    state that is carried through the network sequentially, as is the case with recurrent
    neural networks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析GPT的输出时，我们看到不仅可以生成新的文本段落，还可以审查网络的注意力层，以了解它在句子中查找信息以改善预测的位置。GPT可以在不丢失信号的情况下访问远处的信息，因为注意力分数是并行计算的，不依赖于通过网络顺序传递的隐藏状态，这与循环神经网络的情况不同。
- en: We saw how there are three families of Transformers (encoder, decoder, and encoder-decoder)
    and the different tasks that can be accomplished with each. Finally, we explored
    the structure and training process of other large language models such as Google’s
    T5 and OpenAI’s ChatGPT.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了Transformer有三个系列（编码器、解码器和编码器-解码器）以及每个系列可以完成的不同任务。最后，我们探讨了其他大型语言模型的结构和训练过程，如谷歌的T5和OpenAI的ChatGPT。
- en: ^([1](ch09.xhtml#idm45387006840576-marker)) Ashish Vaswani et al., “Attention
    Is All You Need,” June 12, 2017, [*https://arxiv.org/abs/1706.03762*](https://arxiv.org/abs/1706.03762).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#idm45387006840576-marker)) Ashish Vaswani等人，“注意力就是一切”，2017年6月12日，[*https://arxiv.org/abs/1706.03762*](https://arxiv.org/abs/1706.03762)。
- en: ^([2](ch09.xhtml#idm45387006828736-marker)) Alec Radford et al., “Improving
    Language Understanding by Generative Pre-Training,” June 11, 2018, [*https://openai.com/research/language-unsupervised*](https://openai.com/research/language-unsupervised).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm45387006828736-marker)) Alec Radford等人，“通过生成式预训练改进语言理解”，2018年6月11日，[*https://openai.com/research/language-unsupervised*](https://openai.com/research/language-unsupervised)。
- en: '^([3](ch09.xhtml#idm45387006370384-marker)) Jacob Devlin et al., “BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding,” October 11, 2018,
    [*https://arxiv.org/abs/1810.04805*](https://arxiv.org/abs/1810.04805).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch09.xhtml#idm45387006370384-marker)) Jacob Devlin等人，“BERT: 深度双向Transformer的语言理解预训练”，2018年10月11日，[*https://arxiv.org/abs/1810.04805*](https://arxiv.org/abs/1810.04805)。'
- en: '^([4](ch09.xhtml#idm45387006340992-marker)) Sheng Shen et al., “PowerNorm:
    Rethinking Batch Normalization in Transformers,” June 28, 2020, [*https://arxiv.org/abs/2003.07845*](https://arxiv.org/abs/2003.07845).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch09.xhtml#idm45387006340992-marker)) Sheng Shen等人，“PowerNorm: 重新思考Transformer中的批归一化”，2020年6月28日，[*https://arxiv.org/abs/2003.07845*](https://arxiv.org/abs/2003.07845)。'
- en: ^([5](ch09.xhtml#idm45387005361120-marker)) Colin Raffel et al., “Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer,” October
    23, 2019, [*https://arxiv.org/abs/1910.10683*](https://arxiv.org/abs/1910.10683).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.xhtml#idm45387005361120-marker)) Colin Raffel等人，“探索统一文本到文本Transformer的迁移学习极限”，2019年10月23日，[*https://arxiv.org/abs/1910.10683*](https://arxiv.org/abs/1910.10683)。
- en: ^([6](ch09.xhtml#idm45387005277024-marker)) Long Ouyang et al., “Training Language
    Models to Follow Instructions with Human Feedback,” March 4, 2022, [*https://arxiv.org/abs/2203.02155*](https://arxiv.org/abs/2203.02155).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.xhtml#idm45387005277024-marker)) Long Ouyang等人，“使用人类反馈训练语言模型遵循指令”，2022年3月4日，[*https://arxiv.org/abs/2203.02155*](https://arxiv.org/abs/2203.02155)。
- en: '^([7](ch09.xhtml#idm45387005252672-marker)) Chenfei Wu et al., “Visual ChatGPT:
    Talking, Drawing and Editing with Visual Foundation Models,” March 8, 2023, [*https://arxiv.org/abs/2303.04671*](https://arxiv.org/abs/2303.04671).`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '^([7](ch09.xhtml#idm45387005252672-marker)) Chenfei Wu等人，“Visual ChatGPT: 使用视觉基础模型进行对话、绘画和编辑”，2023年3月8日，[*https://arxiv.org/abs/2303.04671*](https://arxiv.org/abs/2303.04671)。'
