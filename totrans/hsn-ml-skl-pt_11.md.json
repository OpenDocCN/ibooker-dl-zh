["```py\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron\n\niris = load_iris(as_frame=True)\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = (iris.target == 0)  # Iris setosa\n\nper_clf = Perceptron(random_state=42)\nper_clf.fit(X, y)\n\nX_new = [[2, 0.5], [3, 1]]\ny_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers\n```", "```py\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n```", "```py\nhousing = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    housing.data, housing.target, random_state=42)\n```", "```py\nmlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], early_stopping=True,\n                       verbose=True, random_state=42)\n```", "```py\n>>> pipeline = make_pipeline(StandardScaler(), mlp_reg) `>>>` `pipeline``.``fit``(``X_train``,` `y_train``)` `` `Iteration 1, loss = 0.85190332` `Validation score: 0.534299` `Iteration 2, loss = 0.28288639` `Validation score: 0.651094` `[...]` `Iteration 45, loss = 0.12960481` `Validation score: 0.788517` `Validation score did not improve more than tol=0.000100 for 10 consecutive` `epochs. Stopping.` ``\n```", "```py```", "```py```", "``` >>> mlp_reg.best_validation_score_ `0.791536125425778` ```", "````` Let’s evaluate the RMSE on the test set:    ```py >>> y_pred = pipeline.predict(X_test) `>>>` `rmse` `=` `root_mean_squared_error``(``y_test``,` `y_pred``)` ``` `>>>` `rmse` `` `0.5327699946812925` `` ```py ```   ```py` ``` ``We get a test RMSE of about 0.53, which is comparable to what you would get with a random forest classifier. Not too bad for a first try! [Figure 9-9](#predictions_vs_targets_plot) plots the model’s predictions versus the targets (on the test set). The dashed red line represents the ideal predictions (i.e., equal to the targets): most of the predictions are close to the targets, but there are still quite a few errors, especially for larger targets.  ![A scatter plot shows the MLP regressor's predictions versus the targets, with most points clustering near the dashed red line indicating ideal predictions.](assets/hmls_0909.png)  ###### Figure 9-9\\. MLP regressor’s predictions versus the targets    Note that this MLP does not use any activation function for the output layer, so it’s free to output any value it wants. This is generally fine, but if you want to guarantee that the output is always positive, then you should use the ReLU activation function on the output layer, or the *softplus* activation function, which is a smooth variant of ReLU: softplus(*z*) = log(1 + exp(*z*)). Softplus is close to 0 when *z* is negative, and close to *z* when *z* is positive. Finally, if you want to guarantee that the predictions always fall within a given range of values, then you should use the sigmoid function or the hyperbolic tangent, and scale the targets to the appropriate range: 0 to 1 for sigmoid and –1 to 1 for tanh. Sadly, the `MLPRegressor` class does not support activation functions in the output layer.    ###### Warning    Scikit-Learn does not offer GPU acceleration, and its neural net features are fairly limited. This is why we will switch to PyTorch starting in [Chapter 10](ch10.html#pytorch_chapter). That said, it is quite convenient to be able to build and train a standard MLP in just a few lines of code using Scikit-Learn: it lets you tackle many complex tasks very quickly.    In general, the mean squared error is the right loss to use for a regression tasks, but if you have a lot of outliers in the training set, you may sometimes prefer to use the mean absolute error instead, or preferably the *Huber loss*, which is a combination of both: it is quadratic when the error is smaller than a threshold *δ* (typically 1), but linear when the error is larger than *δ*. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error. Unfortunately, `MLPRegressor` only supports the MSE loss.    [Table 9-1](#regression_mlp_architecture) summarizes the typical architecture of a regression MLP.      Table 9-1\\. Typical regression MLP architecture   | Hyperparameter | Typical value | | --- | --- | | # hidden layers | Depends on the problem, but typically 1 to 5 | | # neurons per hidden layer | Depends on the problem, but typically 10 to 100 | | # output neurons | 1 per target dimension | | Hidden activation | ReLU | | Output activation | None, or ReLU/softplus (if positive outputs) or sigmoid/tanh (if bounded outputs) | | Loss function | MSE, or Huber if outliers |    All right, MLPs can tackle regression tasks. What else can they do?`` ```py ```` ```py`` `````", "``````py`  ``````", "``` ```", "```py`` ```", "```py` ## Classification MLPs    MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the sigmoid activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number.    MLPs can also easily handle multilabel binary classification tasks (see [Chapter 3](ch03.html#classification_chapter)). For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the sigmoid activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to 1\\. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).    If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer (see [Figure 9-10](#fnn_for_classification_diagram)). The softmax function (introduced in [Chapter 4](ch04.html#linear_models_chapter)) will ensure that all the estimated probabilities are between 0 and 1, and that they add up to 1, since the classes are exclusive. As we saw in [Chapter 3](ch03.html#classification_chapter), this is called multiclass classification.    Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (or *x-entropy* or log loss for short, see [Chapter 4](ch04.html#linear_models_chapter)) is generally a good choice.  ![Diagram illustrating a modern Multi-Layer Perceptron (MLP) architecture for classification, featuring input, hidden, and output layers with ReLU and softmax functions.](assets/hmls_0910.png)  ###### Figure 9-10\\. A modern MLP (including ReLU and softmax) for classification    [Table 9-2](#classification_mlp_architecture) summarizes the typical architecture of a classification MLP.      Table 9-2\\. Typical classification MLP architecture   | Hyperparameter | Binary classification | Multilabel binary classification | Multiclass classification | | --- | --- | --- | --- | | # hidden layers | Typically 1 to 5 layers, depending on the task | | # output neurons | 1 | 1 per binary label | 1 per class | | Output layer activation | Sigmoid | Sigmoid | Softmax | | Loss function | X-entropy | X-entropy | X-entropy |    As you might expect, Scikit-Learn offers an `MLPClassifier` class in the `sklearn.neural_network` package, which you can use for binary or multiclass classification. It is almost identical to the `MLPRegressor` class, except that its output layer uses the softmax activation function, and it minimizes the cross-entropy loss rather than the MSE. Moreover, the `score()` method returns the model’s accuracy rather than the R² score. Let’s try it out.    We could tackle the iris dataset, but that task is too simple for a neural net: a linear model would do just as well and wouldn’t risk overfitting. So let’s instead tackle a more complex task: Fashion MNIST. This is a drop-in replacement of MNIST (introduced in [Chapter 3](ch03.html#classification_chapter)). It has the exact same format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is much more diverse, and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST. Let’s see if we can do better with an MLP.    First, let’s load the dataset using the `fetch_openml()` function, very much like we did for MNIST in [Chapter 3](ch03.html#classification_chapter). Note that the targets are represented as strings `'0'`, `'1'`, …​, `'9'`, so we convert them to integers:    ```", "```py    The data is already shuffled, so we just take the first 60,000 images for training, and the last 10,000 for testing:    ```", "```py    Each image is represented as a 1D integer array containing 784 pixel intensities ranging from 0 to 255\\. You can use the `plt.imshow()` function to plot an image, but first you need to reshape it to `[28, 28]`:    ```", "```py    If you run this code, you should see the ankle boot represented in the top-right corner of [Figure 9-11](#fashion_mnist_plot).  ![Grid of the first four samples from each class in the Fashion MNIST dataset, showing various clothing and footwear items labeled by category.](assets/hmls_0911.png)  ###### Figure 9-11\\. First four samples from each class in Fashion MNIST    With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5\\. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with. Scikit-Learn does not provide it, so let’s create it:    ```", "```py    We can now confirm that the first image in the training set represents an ankle boot:    ```", "```py   ```", "```py We’re ready to build the classification MLP:    ```", "```py    This code is very similar to the regression code we used earlier, but there are a few differences:    *   Of course, it’s a classification task so we use an `MLPClassifier` rather than an `MLPRegressor`.           *   We use just two hidden layers with 300 and 100 neurons, respectively. You can try a different number of hidden layers, and change the number of neurons as well if you want.           *   We also use a `MinMaxScaler` instead of a `StandardScaler`. We need it to shrink the pixel intensities down to the 0–1 range rather than 0–255: having features in this range usually works better with the default hyperparameters used by `MLPClassifier`, such as its default learning rate and weight initialization scale. You might wonder why we didn’t use a `StandardScaler`? Well some pixels don’t vary much across images; for example, the pixels around the edges are almost always white. If we used the `StandardScaler`, these pixels would get scaled up to have the same variance as every other pixel: as a result, we would give more importance to these pixels than they probably deserve. Using the `MinMaxScaler` often works better than the `StandardScaler` for images (but your mileage may vary).           *   Lastly, the `score()` function returns the model’s accuracy.              If you run this code, you will find that the model reaches about 89.7% accuracy on the validation set during training (the exact value is given by `mlp_clf.best_validation_score_`), but it starts overfitting a bit toward the end, so it ends up at just 89.2% accuracy. When we evaluate the model on the test set, we get 87.1%, which is not bad for this task, although we can do better with other neural net architectures such as convolutional neural networks ([Chapter 12](ch12.html#cnn_chapter)).    You probably noticed that training was quite slow. That’s because the hidden layers have a *lot* of parameters, so there are many computations to run at each iteration. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! All these parameters give the model quite a lot of flexibility to fit the training data, but it also means that there’s a high risk of overfitting, especially when you do not have a lot of training data. In this case, you may want to use regularization techniques such as early stopping and ℓ[2] regularization.    Once the model is trained, you can use it to classify new images:    ```", "```py   ```", "```py` All these predictions are correct, except for the one at index 12, which should be a 7 (sneaker) instead of a 8 (bag). You might want to know how confident the model was about these predictions, especially the bad one. For this, you can use `model.predict_proba()` instead of `model.predict()`, like we did in [Chapter 3](ch03.html#classification_chapter):    ```", "```py   ```", "```py ```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "```` ```py ``# Hyperparameter Tuning Guidelines    The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a basic MLP you can change the number of layers, the number of neurons and the type of activation function to use in each layer, the weight initialization logic, the type of optimizer to use, its learning rate, the batch size, and more. What are some good values for these hyperparameters?    ## Number of Hidden Layers    For many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But deep networks have a much higher *parameter efficiency* than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. This is because their layered structure enables them to reuse and compose features across multiple levels: for example, the first layer in a face classifier may learn to recognize low-level features such as dots, arcs, or straight lines; while the second layer may learn to combine these low-level features into higher-level features such as squares or circles; and the third layer may learn to combine these higher-level features into a mouth, an eye, or a nose; and the top layer would then be able to use these top-level features to classify faces.    Not only does this hierarchical architecture help DNNs converge faster to a good solution, but it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called *transfer learning*.    In summary, for many problems you can start with just one or two hidden layers, and the neural network will work pretty well. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as you will see in [Chapter 12](ch12.html#cnn_chapter)), and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data.    ## Number of Neurons per Hidden Layer    The number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 × 28 = 784 inputs and 10 output neurons.    As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100\\. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer a bit larger than the others.    Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. Alternatively, you can try building a model with slightly more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting too much. Vincent Vanhoucke, a Waymo researcher and former Googler, has dubbed this the “stretch pants” approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid bottleneck layers that could ruin your model. Indeed, if a layer has too few neurons, it will lack the computational capacity to model complex relationships, and it may not even have enough representational power to preserve all the useful information from the inputs. For example, if you apply PCA (introduced in [Chapter 7](ch07.html#dimensionality_chapter)) to the Fashion MNIST training set, you will find that you need 187 dimensions to preserve 95% of the variance in the data. So if you set the number of neurons in the first hidden layer to some greater number, say 200, you can be confident that this layer will not be a bottleneck. However, you don’t want to add too many neurons, or else the model will have too many parameters to optimize, and it will take more time and data to train.    ###### Tip    In general, you will get more bang for your buck by increasing the number of layers rather than the number of neurons per layer.    That said, bottleneck layers are not always a bad thing. For example, limiting the dimensionality of the first hidden layers forces the neural net to keep only the most important dimensions, which can eliminate some of the noise in the data (but don’t go too far!). Also, having a bottleneck layer near the output layer can force the neural net to learn good representations of the data in the previous layers (i.e., packing more useful information in less space), which can help the neural net generalize, and can also be useful in and of itself for *representation learning*. We will get back to that in [Chapter 18](ch18.html#autoencoders_chapter).    ## Learning Rate    The learning rate is a hugely important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges, as we saw in [Chapter 4](ch04.html#linear_models_chapter)). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., 10^(–5)) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by (10 / 10^(-5))^(1 / 500) to go from 10^(–5) to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate is often a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate.    ###### Tip    To change the learning rate during training when using Scikit-Learn, you must set the MLP’s `warm_start` hyperparameter to `True`, and fit the model one batch at a time using `partial_fit()`, much like we did with the `SGDRegressor` in [Chapter 4](ch04.html#linear_models_chapter). Simply update the learning rate at each iteration.    ## Batch Size    The batch size can have a significant impact on your model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently (as we will see in [Chapter 10](ch10.html#pytorch_chapter)), so the training algorithm will see more instances per second. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in *VRAM* (video RAM, i.e., the GPU’s memory). There’s a catch, though: large batch sizes can sometimes lead to training instabilities, especially with smaller models and at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. Yann LeCun once tweeted “Friends don’t let friends use mini-batches larger than 32”, citing a [2018 paper](https://homl.info/smallbatch)⁠^([15](ch09.html#id2213)) by Dominic Masters and Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time.    However, other research points in the opposite direction. For example, in 2017, papers by [Elad Hoffer et al.](https://homl.info/largebatch)⁠^([16](ch09.html#id2214)) and [Priya Goyal et al.](https://homl.info/largebatch2)⁠^([17](ch09.html#id2215)) showed that it is possible to use very large batch sizes (up to 8,192), along with various techniques such as warming up the learning rate (i.e., starting training with a small learning rate, then ramping it up), to obtain very short training times, without any generalization gap.    So one strategy is to use a large batch size, possibly with learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a smaller batch size instead.    ## Other Hyperparameters    Here are two more hyperparameters you can tune if you have the computation budget and the time:    Optimizer      Choosing a better optimizer than plain old mini-batch gradient descent (and tuning its hyperparameters) can help speed up training and sometimes reach better performance.      Activation function      We discussed how to choose the activation function earlier in this chapter: in general, the ReLU activation function is a good default for all hidden layers. In some cases, replacing ReLU with another function can help.      ###### Tip    The optimal learning rate depends on the other hyperparameters—especially the batch size—so if you modify any hyperparameter, make sure to tune the learning rate again.    For more best practices regarding tuning neural network hyperparameters, check out the excellent [2018 paper](https://homl.info/1cycle)⁠^([18](ch09.html#id2220)) by Leslie Smith. The [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook) by Google researchers is also well worth reading. The free e-book [*Machine Learning Yearning* by Andrew Ng](https://homl.info/ngbook) also contains a wealth of practical advice.    Lastly, I highly recommend you go through exercise 1 at the end of this chapter. You will use a nice web interface to play with various neural network architectures and visualize their outputs. This will be very useful to better understand MLPs and grow a good intuition for the effects of each hyperparameter (number of layers and neurons, activation functions, and more).    This concludes our introduction to artificial neural networks and their implementation with Scikit-Learn. In the next chapter, we will switch to PyTorch, the leading open source library for neural networks, and we will use it to train and run MLPs much faster by exploiting the power of graphical processing units (GPUs). We will also start building more complex models, with multiple inputs and outputs.    # Exercises    1.  This [neural network playground](https://playground.tensorflow.org) is a great tool to build your intuitions without writing any code (it was built by the TensorFlow team, but there’s nothing TensorFlow-specific about it; in fact, it doesn’t even use TensorFlow). In this exercise, you will train several binary classifiers in just a few clicks, and tweak the model’s architecture and its hyperparameters to gain some intuition on how neural networks work and what their hyperparameters do. Take some time to explore the following:               1.  The patterns learned by a neural net. Try training the default neural network by clicking the Run button (top left). Notice how it quickly finds a good solution for the classification task. The neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns. In general, the more layers there are, the more complex the patterns can be.                       2.  Activation functions. Try replacing the tanh activation function with a ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.                       3.  The risk of local minima. Modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, click the Reset button next to the Play button). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.                       4.  What happens when neural nets are too small. Remove one neuron to keep just two. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and systematically underfits the training set.                       5.  What happens when neural nets are large enough. Set the number of neurons to eight, and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks rarely get stuck in local minima, and even when they do, these local optima are often almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.                       6.  The risk of vanishing gradients in deep networks. Select the spiral dataset (the bottom-right dataset under “DATA”), and change the network architecture to have four hidden layers with eight neurons each. Notice that training takes much longer and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (on the right) tend to evolve faster than the neurons in the lowest layers (on the left). This problem, called the *vanishing gradients* problem, can be alleviated with better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or batch normalization (discussed in [Chapter 11](ch11.html#deep_chapter)).                       7.  Go further. Take an hour or so to play around with other parameters and get a feel for what they do to build an intuitive understanding about neural networks.                   2.  Draw an ANN using the original artificial neurons (like the ones in [Figure 9-3](#nn_propositional_logic_diagram)) that computes *A* ⊕ *B* (where ⊕ represents the XOR operation). Hint: *A* ⊕ *B* = (*A* ∧ ¬ *B*) ∨ (¬ *A* ∧ *B*).           3.  Why is it generally preferable to use a logistic regression classifier rather than a classic perceptron (i.e., a single layer of threshold logic units trained using the perceptron training algorithm)? How can you tweak a perceptron to make it equivalent to a logistic regression classifier?           4.  Why was the sigmoid activation function a key ingredient in training the first MLPs?           5.  Name three popular activation functions. Can you draw them?           6.  Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.               1.  What is the shape of the input matrix **X**?                       2.  What are the shapes of the hidden layer’s weight matrix **W**[*h*] and bias vector **b**[*h*]?                       3.  What are the shapes of the output layer’s weight matrix **W**[*o*] and bias vector **b**[*o*]?                       4.  What is the shape of the network’s output matrix **Y**?                       5.  Write the equation that computes the network’s output matrix **Y** as a function of **X**, **W**[*h*], **b**[*h*], **W**[*o*], and **b**[*o*].                   7.  How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in [Chapter 2](ch02.html#project_chapter)?           8.  What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?           9.  Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?           10.  Train a deep MLP on the CoverType dataset. You can load it using `sklearn.datasets.fetch_covtype()`. See if you can get over 93% accuracy on the test set by fine-tuning the hyperparameters manually and/or using `RandomizedSearchCV`.              Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch09.html#id2093-marker)) You can get the best of both worlds by being open to biological inspirations without being afraid to create biologically unrealistic models, as long as they work well.    ^([2](ch09.html#id2094-marker)) Warren S. McCulloch and Walter Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity”, *The Bulletin of Mathematical Biology* 5, no. 4 (1943): 115–113.    ^([3](ch09.html#id2106-marker)) They are not actually attached, just so close that they can very quickly exchange chemical signals.    ^([4](ch09.html#id2110-marker)) Image by Bruce Blaus ([Creative Commons 3.0](https://oreil.ly/pMbrK)). Reproduced from [*https://en.wikipedia.org/wiki/Neuron*](https://en.wikipedia.org/wiki/Neuron).    ^([5](ch09.html#id2111-marker)) In the context of machine learning, the phrase “neural networks” generally refers to ANNs, not BNNs.    ^([6](ch09.html#id2115-marker)) Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from [*https://en.wikipedia.org/wiki/Cerebral_cortex*](https://en.wikipedia.org/wiki/Cerebral_cortex).    ^([7](ch09.html#id2122-marker)) Logistic regression and the logistic function were introduced in [Chapter 4](ch04.html#linear_models_chapter), along with several other concepts that we will heavily rely on in this chapter, including softmax, cross-entropy, gradient descent, early stopping, and more, so please make sure to read it first.    ^([8](ch09.html#id2128-marker)) In some libraries, such as PyTorch, the weight matrix is transposed, so there’s one row per neuron, and one column per input feature.    ^([9](ch09.html#id2132-marker)) Note that this solution is not unique: when data points are linearly separable, there is an infinity of hyperplanes that can separate them.    ^([10](ch09.html#id2139-marker)) For example, when the inputs are (0, 1) the lower-left neuron computes 0 × 1 + 1 × 1 – 3 / 2 = –1 / 2, which is negative, so it outputs 0\\. The lower-right neuron computes 0 × 1 + 1 × 1 – 1 / 2 = 1 / 2, which is positive, so it outputs 1\\. The output neuron receives the outputs of the first two neurons as its inputs, so it computes 0 × (–1) + 1 × 1 - 1 / 2 = 1 / 2. This is positive, so it outputs 1.    ^([11](ch09.html#id2146-marker)) In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.    ^([12](ch09.html#id2152-marker)) David Rumelhart et al., “Learning Internal Representations by Error Propagation” (Defense Technical Information Center technical report, September 1985).    ^([13](ch09.html#id2162-marker)) Biological neurons seem to implement a roughly sigmoid (*S*-shaped) activation function, so researchers stuck to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is one of the cases where the biological analogy was perhaps misleading.    ^([14](ch09.html#id2192-marker)) C. Szegedy et al., “Rethinking the Inception Architecture for Computer Vision”, CVPR 2016: 2818–2826.    ^([15](ch09.html#id2213-marker)) Dominic Masters and Carlo Luschi, “Revisiting Small Batch Training for Deep Neural Networks”, arXiv preprint arXiv:1804.07612 (2018).    ^([16](ch09.html#id2214-marker)) Elad Hoffer et al., “Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks”, *Proceedings of the 31st International Conference on Neural Information Processing Systems* (2017): 1729–1739.    ^([17](ch09.html#id2215-marker)) Priya Goyal et al., “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour”, arXiv preprint arXiv:1706.02677 (2017).    ^([18](ch09.html#id2220-marker)) Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size, Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).`` ``` ````"]