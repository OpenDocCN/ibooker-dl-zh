- en: Chapter 6\. Building with Amazon Bedrock and Amazon Q
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon cofounder Jeff Bezos once [said](https://oreil.ly/kxvps), “We’re not
    competitor obsessed, we’re customer obsessed. We start with what the customer
    needs and we work backwards.” This guiding principle has helped make his company
    into a powerhouse of the digital age.
  prefs: []
  type: TYPE_NORMAL
- en: 'When generative AI surged in popularity, AWS first [talked extensively](https://oreil.ly/_n5bS)
    with its customers and there were some consistent themes:'
  prefs: []
  type: TYPE_NORMAL
- en: Customers wanted an easy way to find and use FMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They needed seamless integration of FMs into their applications without managing
    complex infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costs had to remain low.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They wanted a simple way to customize FMs using their own data to fit unique
    business needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data privacy and security were essential throughout the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customers did not want their data to be used to train FMs for other companies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this feedback, AWS initiated an ambitious program to create a sophisticated
    platform to build generative AI applications—which became known as Amazon Bedrock.
    It was announced as a preview edition in April 2023 and was made generally available
    in September 2023\. For AWS, this was a strategically important effort, and speed
    was critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will get an overview of this powerful system. We’ll also
    look at another important generative AI technology platform: Amazon Q, which is
    a virtual assistant for businesses and software developers. For the purposes of
    the exam, you will need to know the capabilities of each of these systems and
    how they are used.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use Bedrock, you will need to log into AWS and then enter “Bedrock” in the
    search box at the top of the screen. When you click this, you will be taken to
    the dashboard, as shown in [Figure 6-1](#figure_six_onedot_bedrock_dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Bedrock dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the middle of the screen, you will see descriptions of the various generative
    AI services and announcements. You can also find the services on the left sidebar.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use this to navigate to “Foundation models,” which has various options.
    Select “View Model catalog.” [Figure 6-2](#figure_six_twodot_bedrock_model_catalog)
    shows the screen for this.
  prefs: []
  type: TYPE_NORMAL
- en: There are 187 generative AI models available. Of these, 51 are severless—meaning
    they automatically manage the underlying infrastructure, allowing users to run
    models without needing to provision or maintain servers. They are also seamlessly
    integrated into Bedrock. The rest of the models are in the Bedrock Marketplace.
    To use these, you will need to go through a process to subscribe to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left sidebar, there are filters to search for all the models. You can
    do this based on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Model provider
  prefs: []
  type: TYPE_NORMAL
- en: This is the company that created the model, such as Amazon, Anthropic, DeepSeek,
    Hugging Face, Meta, Mistral, or NVIDIA.
  prefs: []
  type: TYPE_NORMAL
- en: Modality
  prefs: []
  type: TYPE_NORMAL
- en: This describes the type of input and output for the model. The modalities include
    audio, image, text, vision, video, and multimodal. There are also embedding models,
    which create the vectors for using generative AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Bedrock Model catalog
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you use the filter options, you will see the results on the right side
    of the screen. Each model will have a brief description.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s select Claude 3.7 Sonnet. [Figure 6-3](#figure_six_threedot_the_model_profile_f)
    shows the profile for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The model profile for Claude 3.7 Sonnet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are details like the version, modalities, release date, model ID, language,
    software license, and deployment type. There is also sample code to make an API
    request. You can copy this and put it into the code of your program to make a
    connection to the model.
  prefs: []
  type: TYPE_NORMAL
- en: To use these models, go to the bottom of the left sidebar and click “Model access.”
    You can select either “Enable all models” or “Enable specific models.” Whichever
    you select, Bedrock will ask you to fill out a form. You will enter basic details
    like your company name and website URL, industry, the intended users (either internal
    employees or external users), and a description of your use case. Then you will
    click Submit. Depending on the model, it can take a few minutes to be activated.
    [Figure 6-4](#figure_six_fourdot_the_models_activated) shows an example of what
    you might see when the process is completed.
  prefs: []
  type: TYPE_NORMAL
- en: You are not charged for registering for the models. But you will pay a fee when
    you use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test your models by using the playground, which you can find on the
    left sidebar. There are two options: Chat/Text and Image/Video. We’ll cover these
    options in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. The models activated in Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Chat/Text Playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Chat/Text playground is for using text-based models (see [Figure 6-5](#figure_six_fivedot_the_dashboard_for_th)).
    There are various settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs: []
  type: TYPE_NORMAL
- en: There is Chat, which allows for an ongoing conversation with the FM. The other
    option is a single prompt, which will generate one response.
  prefs: []
  type: TYPE_NORMAL
- en: Select model
  prefs: []
  type: TYPE_NORMAL
- en: You will click this to get a screen to choose a model. At the top, you can search
    for the model. You can also use the three-step process. First, you will select
    the model provider and then the model. Finally, there are options for inference,
    which is how the model generates responses. Select “On-demand,” which is the most
    basic approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. The dashboard for the text-based models in Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-6](#figure_six_sixdot_selecting_a_model_in) shows the screen you
    will see when selecting a model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Selecting a model in Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After you select the model, the screen will have a configurations section on
    the sidebar, as shown in [Figure 6-7](#figure_six_sevendot_the_configurations).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. The configurations section for a model in Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some of the common configuration options include Temperature, Top P, Top K,
    Response length, Stop sequences, and Guardrails. Let’s dive into these configuration
    options in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The temperature sets the randomness or creativity of the responses from the
    FM. The range is from 0 to 1, which represents a probability distribution. The
    closer it is to 0, the more deterministic and predictable the responses will be.
    This is typically best when you want content that is more fact-based, such as
    FAQs, summarizations, or instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the temperature, the higher the probability that the responses will
    be more random or creative. This can be useful for brainstorming and creative
    writing.
  prefs: []
  type: TYPE_NORMAL
- en: Temperatures will vary based on the model. For example, the value of 0.6 will
    likely have different types of responses for an OpenAI model versus one from Meta.
  prefs: []
  type: TYPE_NORMAL
- en: Top P
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Top P is also called *nucleus sampling* or *top probability sampling*. It selects
    the next word in an FM response that is based on a cumulative probability threshold.
    To understand this, let’s look at an example. Suppose an FM is generating this
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: The dog wagged its
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The model will calculate a probability distribution for the next word. This
    includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '“tail”: 50%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“paw”: 20%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“ears”: 15%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“tongue”: 10%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“whiskers”: 5%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Top P is set to 0.5, then the only response will be “tail.” The reason is
    that the model will look for the smallest set of words whose cumulative probability
    is greater than or equal to 0.50.
  prefs: []
  type: TYPE_NORMAL
- en: But suppose we set Top P to 0.9\. We would include “tail,” “paw,” and “ears,”
    which would total 0.85\. But “tongue” will be excluded because the cumulative
    probability would be 0.95.
  prefs: []
  type: TYPE_NORMAL
- en: From the pool of available words, the FM will randomly make a choice. In other
    words, the higher the Top P, the more diverse or creative the responses will be.
  prefs: []
  type: TYPE_NORMAL
- en: Top K
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Top K limits the number of token choices the model will consider when generating
    the next word in a response. This is another way to control the randomness or
    diversity of the content. With Top K, the FM will consider the K most likely words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example. Suppose the FM is generating this response:'
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn programming is
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is the probability distribution for the next word:'
  prefs: []
  type: TYPE_NORMAL
- en: '“practice”: 35%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“by”: 20%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“through”: 15%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“with”: 10%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“using”: 8%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“via”: 7%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“reading”: 5%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have the Top K set to 2, then the words selected would be “practice” and
    “by.” The reason is that they are the two most likely words.
  prefs: []
  type: TYPE_NORMAL
- en: So yes, if Top K is set to 4, then we would have “practice,” “by,” “through,”
    and “with.” As the value increases, so will the randomness and diversity of the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: When working with Temperature, Top P, and Top K, you will need to experiment
    with the values. The process is mostly trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you set values for Temperature, Top P and Top Q for an FM, they
    will interact with each other. Often, this can lead to unexpected results. This
    is why it is better to set only one or two parameters. Generally, it’s recommended
    to use either Temperature or Top P, but not both. As for Top K, it can best be
    used with either Temperature or Top P.
  prefs: []
  type: TYPE_NORMAL
- en: Response length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The “Response length” is a way to control the size of the generated response
    from the FM. Depending on a model, it can be a minimum or maximum value. The values
    may vary depending on the model.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, the response length can be a good way to provide more conciseness
    to a response. This can be useful for chat responses and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: But of course, if the response length is small, important content may be truncated.
  prefs: []
  type: TYPE_NORMAL
- en: Stop sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Stop sequences” allow you to control when an FM stops generating a response.
    These are specific words, phrases, or punctuation marks that signal the model
    to end its output.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you’re building a system that generates JSON objects. You
    only want the model to output up to the end of the JSON structure. You could use
    a stop sequence like “}” to make sure the response ends once the closing brace
    is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Using stop sequences alongside a maximum token limit can provide greater control
    over both the content and length of the response.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With guardrails in AWS Bedrock, you can create safeguards that help enforce
    responsible AI practices and ensure your generative AI applications align with
    your organization’s values and compliance requirements. These guardrails act as
    content moderation layers, analyzing both the prompts sent to the FM and the responses
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: They allow you to define use-case-specific protections—whether you’re building
    a customer service chatbot, a document summarization tool, or any other generative
    AI application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are key capabilities for guardrails:'
  prefs: []
  type: TYPE_NORMAL
- en: Content filtering
  prefs: []
  type: TYPE_NORMAL
- en: You can filter out toxic, adult, or hostile content to maintain brand safety
    and user trust. This helps prevent inappropriate or offensive language from being
    included in either the user input or the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive information protection
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails can identify and block outputs that include personal data, financial
    identifiers, or other forms of confidential information. This is important for
    industries like healthcare and finance.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual support
  prefs: []
  type: TYPE_NORMAL
- en: Currently, Bedrock guardrails support English, French, and Spanish, allowing
    broader coverage for global applications.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and response protection
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails apply to both user prompts and model responses, ensuring two-way
    protection.
  prefs: []
  type: TYPE_NORMAL
- en: When you create a guardrail, there’s no limit to the number of filters you can
    apply. This enables organizations to tailor protections based on specific regulatory,
    ethical, or operational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also provides a testing environment, so you can simulate different inputs
    and see how your guardrail behaves before deploying it into production. This helps
    you fine-tune your filters and avoid unintended model blocking or leakage of sensitive
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take an example of the use of guardrails. Suppose you’re building a legal
    advice assistant using an FM on Bedrock. You can configure a guardrail that blocks
    prompts or responses mentioning personal identifiers like Social Security numbers
    or legal threats. You could also add filters to ensure the assistant doesn’t engage
    in offensive or overly aggressive language, even if prompted by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the FM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have configured the FM for the Chat/Text playground, you can then
    start interacting with it. Suppose you enter the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: What is generative AI?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can press Enter or Run. You can also select the icon—which has two arrows—that
    will expand the input box. If you want to add a space, you can press Shift+Enter.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-8](#figure_six_eightdot_the_response_of_an) shows part of the response.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. The response of an LLM in Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is a paragraph description of generative AI. At the top, there are metrics
    on the response. The Input has 9 tokens, the Output has 96 tokens, and the Latency—the
    time it takes for the model to generate a response—is 4,583 milliseconds. There
    are two icons alongside these. One is to clear the chat and the other is to copy
    the content.
  prefs: []
  type: TYPE_NORMAL
- en: Image/Video Playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the Image/Video playground, you can use natural language to create images
    and videos. [Figure 6-9](#figure_six_ninedot_the_dashboard_for_th) shows the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. The dashboard for the Image/Video playground
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the top left, you can select the model: Amazon, Luma AI, or Stability AI.
    You can also select for the inference, whether for on-demand or provisioned throughput.
    These options work the same way we saw when using the Chat/Text playground.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you select a model, the configuration section will appear on the left
    side of the screen. One option is called Action. This includes a variety of ways
    for generating the image or video:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate image
  prefs: []
  type: TYPE_NORMAL
- en: Create a new image based on your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Generate variations
  prefs: []
  type: TYPE_NORMAL
- en: Create a new image similar to a reference image you uploaded. The file formats
    supported include *.png* and *.jpeg*. The maximum image size is 24 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Remove object
  prefs: []
  type: TYPE_NORMAL
- en: Remove a specific object from an image you uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: Replace background
  prefs: []
  type: TYPE_NORMAL
- en: You can change the background of an image while keeping the main theme intact.
  prefs: []
  type: TYPE_NORMAL
- en: Replace object
  prefs: []
  type: TYPE_NORMAL
- en: Swap out one object in an image for another.
  prefs: []
  type: TYPE_NORMAL
- en: Generate video
  prefs: []
  type: TYPE_NORMAL
- en: Create a short video clip based on your prompt and starting image, which is
    optional.
  prefs: []
  type: TYPE_NORMAL
- en: Negative prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next configuration option is “Negative prompt.” This allows you to indicate
    certain elements of an image or video you do not want to appear. [Table 6-1](#table_six_onedot_examples_of_negative_p)
    shows some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Examples of negative prompts
  prefs: []
  type: TYPE_NORMAL
- en: '| Hide faces | Distorted faces blurry |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Style control | Cartoon |'
  prefs: []
  type: TYPE_TB
- en: '| Colors | Sepia, neon colors |'
  prefs: []
  type: TYPE_TB
- en: '| Unwanted composition elements | Busy background |'
  prefs: []
  type: TYPE_TB
- en: '| Texture | Shiny surfaces, rough textures |'
  prefs: []
  type: TYPE_TB
- en: '| Lighting atmosphere | Foggy |'
  prefs: []
  type: TYPE_TB
- en: 'You can also use multiple negative prompts. An example is: blurry, cartoon,
    cluttered background.'
  prefs: []
  type: TYPE_NORMAL
- en: Response image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Then there is the “Response image” configuration. You can set the dimensions
    of the image, as well as the number of images to be generated. You can have up
    to 5.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, there are two advanced configuration options: one for “Prompt strength”
    and one for Seed.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt strength
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prompt strength indicates how the generated image will adhere to your prompt.
    It’s a value between 1 and 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the ranges:'
  prefs: []
  type: TYPE_NORMAL
- en: '1–3: allows for more creativity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4–7: a balance between creativity and adhering closely to the prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '8–0: keeps the image mostly focused on the requirements of the prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seed
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Seed is the value for the random number generator. This is the starting value
    used to set the initial state. What this means is that when you use the same seed,
    you will get the same output. This means you can create variations of the image
    using different seeds while keeping the other configurations the same. This helps
    to evaluate different images without them being widely different from the core
    idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s select the Stable Image Core 1.0 model and use this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: A serene mountain lake at sunset, with snow-capped peaks reflected in the calm
    water. Warm golden light, wispy clouds in the sky.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 6-10](#figure_six_onezerodot_the_image_created) shows the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The image created by Stable Image Core 1.0
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the top left of the screen, there is an icon—which has three vertical dots—where
    you can export the image as a zip file. There is also an icon, which is a down
    arrow, that downloads the image as a *.jpg* file format.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an FM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using Bedrock, the selection of an FM is a critical step, which involves
    evaluating a variety of factors. There is also no right answer, as the process
    will involve experimentation. When selecting a model, you should first consider
    the use case. You can use the “Model catalog” for this, which we learned about
    earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Then you can use Filters and focus on the modality that you will need for your
    application. This will help to narrow your search.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you want to select an image model. For this, there are
    nine options. Some of these will have the “Legacy” tag, which means they will
    be phased out at some point. Of course, you can ignore these. Then you can search
    the other models by reviewing the profiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the factors to consider include:'
  prefs: []
  type: TYPE_NORMAL
- en: Categories
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the capabilities of the FM. Some can be limited, such as Titan Image
    Generator G1\. It can do the following: text-to-image, image-to-image, background
    removal, and image conditioning. Stable Diffusion 3.5 Large, on the other hand,
    is much more robust, such as allowing for better scene layout, anime, and cartoons.'
  prefs: []
  type: TYPE_NORMAL
- en: Last version
  prefs: []
  type: TYPE_NORMAL
- en: You should focus on the latest.
  prefs: []
  type: TYPE_NORMAL
- en: Language
  prefs: []
  type: TYPE_NORMAL
- en: This is the language you can write your prompts in.
  prefs: []
  type: TYPE_NORMAL
- en: Max tokens
  prefs: []
  type: TYPE_NORMAL
- en: This is the maximum number of tokens that the model will generate for a response,
    and these can vary widely. [Table 6-2](#table_six_twodot_maximum_tokens_for_dif)
    shows some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Maximum tokens for different models
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Maximum tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3.7 Sonnet | 200k |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSeek-R1 | 128k |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3.3 70B Instruct | 128k |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral Large 2 (24.07) | 128k |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon Nova Micro | 128k |'
  prefs: []
  type: TYPE_TB
- en: The software license can also be important. In the model profile, you will see
    a link to it at the bottom of the screen, labeled End User License Agreement (EULA).
    The EULA will either be open source or proprietary. This will determine whether
    and how you can use the models for your particular project. For example, an open
    source license may allow you to freely modify and distribute the model, while
    a proprietary license may restrict usage to internal development or require a
    commercial agreement. Understanding these terms helps ensure legal and compliant
    use of the model.
  prefs: []
  type: TYPE_NORMAL
- en: License Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The three main types of open source licenses include:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache 2.0
  prefs: []
  type: TYPE_NORMAL
- en: This is a permissive license that allows you to use, modify, and distribute
    software freely. There is also protection against patent litigation, but you need
    to provide the copyright notices in your application.
  prefs: []
  type: TYPE_NORMAL
- en: MIT license
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to the Apache 2.0 license, with nearly unrestricted use of the
    software. However, you need to provide the copyright notices and include the license
    in the software.
  prefs: []
  type: TYPE_NORMAL
- en: GNU General Public License (GPL)
  prefs: []
  type: TYPE_NORMAL
- en: This is a copyleft license, which means that derivative works based on the software
    must abide by the terms of the license. This helps to maintain that the software
    will remain free. But some companies may not want to use this license because
    they may lose the intellectual property rights for their own code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of these types of licenses has allowed for wide distribution of AI
    models. But there are other advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs: []
  type: TYPE_NORMAL
- en: The developer can understand how a model generates responses, which can improve
    trust. It can also make it easier for an organization to evaluate the model for
    compliance, ethics, and regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Innovation
  prefs: []
  type: TYPE_NORMAL
- en: Some models have thriving global communities of developers and data scientists.
    This means that the systems can benefit from their innovative modifications and
    enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: Customization
  prefs: []
  type: TYPE_NORMAL
- en: By having access to the code, you can build your own version of the AI model
    for particular needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some AI model developers will have their own licenses. An example is Meta.
    For its Llama models, it uses the Llama Community License Agreement, which is
    more restrictive than those licenses mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: If your application has more than 700 million monthly active users, you must
    obtain a separate license from Meta.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot use Llama to create competing LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derivative works created from Llama must adhere to the same Llama Community
    License.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingly enough, there is considerable debate about these types of licenses.
    Are they true open source or somewhat open source? According to the [Free Software
    Foundation (FSF)](https://oreil.ly/zdBtp), the answer is no. The organization
    considers the license to be too restrictive.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Meta, FSF also says that the company has not provided enough
    transparency. The reason is that it does not disclose the parameters of the model
    as well as the training data.
  prefs: []
  type: TYPE_NORMAL
- en: FM Response Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When selecting a model for building an application, a helpful approach is to
    come up with a list of expected prompts from users, which are based on your use
    case. For example, suppose you are building a chatbot for human resources (HR).
    You will have subject matter experts (SMEs) in your organization who will come
    up with typical scenarios and write prompts for them. Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HR policies: “I want to take a vacation for the December holiday. What are
    the steps I need to take for this leave and who do I inform?”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Benefits: “What are the health insurance options available? What are the eligibility
    requirements? How do I enroll in a plan?”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Job description creation: “Create a detailed job description for a data engineer.
    Include key responsibilities, required qualifications, and preferred skills.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Employee reviews: “What are the best practices for employee reviews?”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Onboarding: “Create an onboarding checklist.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: No doubt, this process can be time-consuming. Yet it is critical to make an
    effective generative AI application that will have a positive impact.
  prefs: []
  type: TYPE_NORMAL
- en: The prompts in our example are also in two main categories. The first two are
    based on corporate data, while the last three are generic. This means you will
    need to customize the FM. You can do this with fine-tuning or RAG. We learned
    about these topics in [Chapter 2](ch02.html#chapter_two_aws_fundamentals_for_the_ai),
    and we will discuss this more later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the generic prompts, you can use the Bedrock playground for testing
    different models. Let’s see how you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Bedrock console, select Chat/Text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the Mode to Chat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Turn on “Compare mode.” [Figure 6-11](#figure_six_oneonedot_the_quotation_mark)
    shows the updated screen, which allows you to select two models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![The “Compare mode” option for Bedrock](assets/awsc_0611.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6-11\. The “Compare mode” option for Bedrock
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: For the first model, select Amazon and then choose Titan Text G1 - Premier v1\.
    Then click Apply.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the next model, select Meta and choose Llama 3 8B Instruct. Then click Apply.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter this in the input box: “Create an onboarding checklist”.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will see a response for both of them, which you can compare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Measuring Success: Business Goals and Metrics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While generative AI is a powerful technology, it can be difficult to implement.
    One reason is the lack of clear-cut business goals and metrics. This is the case
    for two-thirds of C-suite executives, according to a survey from the Boston Consulting
    Group.^([1](ch06.html#ch01fn25)) A study from the Everest Group had a similar
    finding.
  prefs: []
  type: TYPE_NORMAL
- en: 'True, it can be difficult to come up with goals and metrics for a dynamic and
    complex technology. But there are some general approaches to consider, which are
    recommended by the AWS certification:'
  prefs: []
  type: TYPE_NORMAL
- en: User satisfaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average revenue per user (ARPU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s unpack these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: User Satisfaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customer Satisfaction Score (CSAT)
  prefs: []
  type: TYPE_NORMAL
- en: CSAT measures user satisfaction based on a scale, such as 1 to 5.
  prefs: []
  type: TYPE_NORMAL
- en: Net Promoter Score (NPS)
  prefs: []
  type: TYPE_NORMAL
- en: NPS evaluates whether a user is likely to recommend a product or service to
    users. This is on a scale of 0 to 10\. The higher the score, the higher the user
    satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to measuring user satisfaction is to use an AI service, like
    Amazon Comprehend. This will use NLP to extract insights from user feedback, such
    as from online forms or thumbs-up/thumbs-down icons in an application. The system
    can categorize the information in terms of positive, negative, neutral, and mixed
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: With these user satisfaction metrics, you can create baseline scores to measure
    against. However, they should be realistic and reviewed periodically to understand
    the impact of the generative AI application.
  prefs: []
  type: TYPE_NORMAL
- en: Average Revenue per User
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Average revenue per user (ARPU) measures revenue per user for a period of time,
    say monthly or annually. This metric is common for businesses that charge subscriptions,
    such as for SaaS software or telecommunications services. It can also be useful
    for ecommerce.
  prefs: []
  type: TYPE_NORMAL
- en: Even a small increase in ARPU can have a notable impact on a company’s bottom
    line. Suppose an online service has 10,000 customers and the current ARPU is $50
    per month ($500,000) in monthly revenue. Let’s say the company implements generative
    AI features that greatly improve the service. For this, the price is increased
    by 10% to $55\. Even without adding any new members, the company will add $600,000
    in annual revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The conversion rate measures the rate at which a user takes an action online,
    such as to make a purchase, fill out a form, or sign up for a newsletter. However,
    this is generally low. For an ecommerce website, the average conversion rate is
    [2.8% on desktops and mobile devices](https://oreil.ly/VJa7t). It’s even worse
    with social media, which is at [about 1%](https://oreil.ly/_oUTg).
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the costs of online advertising have been rising. During certain
    periods of times—like Black Friday—they can be exorbitant, as the bidding on keywords
    is intense. This is why the conversion rate is so important. A minimal increase
    can mean the difference of sustaining a loss and generating a profit.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various ways generative AI can help out:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimized content
  prefs: []
  type: TYPE_NORMAL
- en: The generative AI can create ads that are engaging and based on customer preferences,
    segments, and purchase history. They can also be improved for search engine optimization
    (SEO). This means the ads will have a higher likelihood of showing up as top results.
  prefs: []
  type: TYPE_NORMAL
- en: Search
  prefs: []
  type: TYPE_NORMAL
- en: This can go beyond the typical keyword methods. Generative AI can analyze the
    natural language for intent, which will provide more relevant results. An example
    is the search system on [*Walmart.com*](http://walmart.com). You can ask questions
    like “Help me plan a football watch party” or “What supplies do I need for a newborn?”
    The generative AI will generate recommendations that are highly pertinent, which
    helps to increase the conversion rate.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic pricing
  prefs: []
  type: TYPE_NORMAL
- en: In real time, generative AI can process the demand, competition, and customer
    behavior. Based on this, it can adjust prices to optimize the conversion rates.
  prefs: []
  type: TYPE_NORMAL
- en: Automated A/B testing
  prefs: []
  type: TYPE_NORMAL
- en: This is where you compare the conversion rates for two versions of a website,
    where there will be one element changed. This could be the layout, call to action,
    or the content. Generative AI can manage the A/B testing in real time, processing
    substantial amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use efficiency metrics when evaluating the underlying model for a generative
    AI application. These metrics help measure how well the system performs in terms
    of speed, responsiveness, and resource consumption. For example, latency is a
    key metric. High latency can lead to a poor user experience, especially in use
    cases like chatbots, gaming, and live streaming, where users expect real-time
    or near-instant feedback. Other metrics might include throughput (how many requests
    the system can handle per second) and memory usage, which affect scalability and
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another important consideration is resource allocation, which refers to how
    computational resources such as GPUs, CPUs, memory, and storage are used. Generative
    AI models can be highly resource-intensive, particularly during inference and
    training. As an application scales to serve more users, the cost of these resources
    can increase significantly. Efficient use of infrastructure—such as autoscaling,
    choosing the right model size, and optimizing inference—can help control these
    costs while maintaining performance. Monitoring and optimizing these aspects is
    critical for both user satisfaction and cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Model Customization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bedrock allows you to customize a generative AI model. While this system streamlines
    the process, it is still complicated. You will need to have a strong background
    in data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'But for the exam, you need to know about the key features of model customization.
    There are several ways to do this: distillation, fine-tuning, and continued pretraining.'
  prefs: []
  type: TYPE_NORMAL
- en: Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distillation is where you transfer knowledge from a more sophisticated model—called
    a teacher—to a smaller one, which is usually faster and lower cost. This is called
    the student.
  prefs: []
  type: TYPE_NORMAL
- en: When using Bedrock for distillation, you will select the models and then provide
    relevant prompts for the input data. This can be either unstructured information
    or labeled data. Bedrock will then generate responses using the teacher model,
    which will then be fine-tuned by the student model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the advantages of distillation:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs: []
  type: TYPE_NORMAL
- en: The models tend to use less compute power, which can translate into lower costs
    and faster response times.
  prefs: []
  type: TYPE_NORMAL
- en: Edge
  prefs: []
  type: TYPE_NORMAL
- en: Since distilled models are smaller, they can be used in constrained environments,
    like with mobile devices and Internet of Things (IoT) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning with Bedrock allows for pretraining an FM for specific, labeled
    data. This means that the model’s parameters can be adjusted, which will allow
    for more accurate and relevant responses. For example, you can fine-tune an FM
    for information about customer support interactions, such as tickets, feedback,
    Slacks, and call transcripts.
  prefs: []
  type: TYPE_NORMAL
- en: To implement this in Bedrock, you will need to prepare the dataset by creating
    input-output pairs. For customer support, you could have the types of data in
    JSON format, shown in [Figure 6-12](#figure_six_onetwodot_the_preparation_of).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. The preparation of a dataset for Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each training example consists of a customer query as the input prompt (such
    as “How do I reset my password?” or “What is the return policy?”) paired with
    an appropriate support agent response as the completion output. By providing this
    type of structured data, you are essentially allowing the FM to learn the patterns
    and appropriate responses for common customer support scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the dataset is completed, you will upload it to Amazon S3, and then you
    will specify the hyperparameters in Bedrock. These are external configuration
    settings set before the training begins. These are examples of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate
  prefs: []
  type: TYPE_NORMAL
- en: Controls the step size for each iteration in the optimization process
  prefs: []
  type: TYPE_NORMAL
- en: Epoch
  prefs: []
  type: TYPE_NORMAL
- en: One pass through the entire dataset
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs: []
  type: TYPE_NORMAL
- en: The number of training examples used in one iteration
  prefs: []
  type: TYPE_NORMAL
- en: After the model is trained, Bedrock will evaluate it by using a validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Continued Pretraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continued pretraining uses large volumes of raw text or unlabeled data to help
    the model improve its understanding of language and context. This can be particularly
    useful for adapting the model to specific domains or industries where relevant
    public data exists but labeled data is scarce.
  prefs: []
  type: TYPE_NORMAL
- en: Like fine-tuning, you can configure various hyperparameters—such as batch size,
    learning rate, and number of training epochs—to control the training process and
    optimize for your specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: As of now, AWS Bedrock supports continued pretraining for select Amazon Titan
    models, including Titan Text G1-Lite and Titan Text G1-Express. These models are
    well-suited for a range of NLP tasks and can benefit significantly from additional
    domain-specific training using your own data. This helps enhance the model’s performance
    without requiring extensive annotation or labeling efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Agents in Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Amazon Bedrock, agents are AI-powered systems that automate complex, multistep
    business tasks by orchestrating interactions between FMs, APIs, and data sources.
    They interpret user inputs, break down tasks into manageable actions, and execute
    them by invoking APIs or querying knowledge bases, all while maintaining conversational
    context.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider an automotive parts retailer aiming to enhance its customer
    support experience. By deploying an Amazon Bedrock agent, the retailer can automate
    responses to customer inquiries about part compatibility and availability. When
    a customer asks, “What wiper blades fit a 2021 Honda CR-V?,” the agent interprets
    the query, retrieves relevant information from the company’s inventory and compatibility
    databases via integrated APIs, and provides a precise, context-aware response
    (see [Figure 6-13](#figure_six_onethreedot_workflow_of_an_a)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/awsc_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. Workflow of an agent in Bedrock
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The process begins with model selection, where you choose an appropriate FM
    that suits your use case requirements. Next, you define instructions using natural
    language to describe what your agent should do. Here’s an example, “You are a
    customer support chatbot for a website that will help answer questions about the
    company’s products and provide order status.”
  prefs: []
  type: TYPE_NORMAL
- en: The third step involves configuring actions, where Bedrock’s flexibility shines
    through its support for API calls using OpenAPI schemas (allowing you to specify
    endpoints, methods, parameters, and expected responses like invoking a weather
    service), Lambda functions that provide business logic, and interactive actions
    where the agent requests follow-up information from users.
  prefs: []
  type: TYPE_NORMAL
- en: The workflow then moves to knowledge base integration, where you can connect
    your proprietary data sources such as FAQs, blogs, and documentation to enhance
    your agent’s capabilities with domain-specific information.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the testing phase allows you to validate your agent’s performance within
    Bedrock’s environment, including access to trace functionality that reveals the
    agent’s decision-making steps and reasoning process. Once you’re satisfied with
    the agent’s performance, the deployment step enables you to launch your agent
    either directly on AWS infrastructure or expose it as an API endpoint for broader
    integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this works, let’s take an example. Suppose you want to create an
    agent for a restaurant’s website. First, you will want to collect relevant data:'
  prefs: []
  type: TYPE_NORMAL
- en: Restaurant details
  prefs: []
  type: TYPE_NORMAL
- en: Name, location, hours, and contact information
  prefs: []
  type: TYPE_NORMAL
- en: Menu
  prefs: []
  type: TYPE_NORMAL
- en: Have an up-to-date version
  prefs: []
  type: TYPE_NORMAL
- en: Reservation information
  prefs: []
  type: TYPE_NORMAL
- en: Seating capacity and booking policies
  prefs: []
  type: TYPE_NORMAL
- en: Customer reviews
  prefs: []
  type: TYPE_NORMAL
- en: Online feedback and ratings
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will need to consider the actions. This can take some time, as you
    will need to brainstorm the various scenarios. But here are some actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Making a reservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canceling a reservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating a reservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting details about a reservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the menu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting restaurant information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this information, you can then have Bedrock create the agent. A customer
    can then use it for questions like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Reserve a table for three at 6:30 on Saturday and let me know what gluten-free
    options you have.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cancel my dinner reservation for tomorrow at 6 P.M.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are the most popular dishes?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are your hours this weekend?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can I get a table with a view?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Multiagent Collaboration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bedrock also allows for the creation of multiagent collaboration. This is where
    more than one agent works collaboratively to solve problems. Each agent will focus
    on a particular task, and there will be a system to provide collaboration for
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: Another key feature is memory. This is where the system will retain information
    about all the interactions, which will help improve the accuracy and the reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through an example of multiagent collaboration for a customer support
    chatbot. When the customer support chatbot receives a question, the Intent Agent
    will process it and evaluate the customer needs, say to get product information,
    order status, or process a return. The agent will use the Retrieval Agent to collect
    the data. From here, the Sentiment Analysis Agent will determine the tone of the
    customer query. If it is negative, then there may be escalation to a human agent.
    Otherwise, the Response Agent will be activated. This will generate a response
    to the customer’s question. For the whole process, the Interaction Agent will
    log the interactions, which can be used to help improve future customer questions.
  prefs: []
  type: TYPE_NORMAL
- en: With multiagent collaboration, there is leveraging of specialization for each
    of the agents. This leads to better responses and actions. But there should also
    be guardrails in place. You do not want to give full control to the agentic system,
    especially for high-stakes matters.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s true that many open source models are free. But this does not mean you
    will not be charged for their use on Bedrock. The reason is that there are still
    the costs for the infrastructure to set up and operate the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches for the pricing:'
  prefs: []
  type: TYPE_NORMAL
- en: On demand
  prefs: []
  type: TYPE_NORMAL
- en: You will be charged for the number of tokens processed and the output tokens
    generated. This is usually when an application has unpredictable or varied workloads.
    However, for a lower cost—generally at a 50% discount—you can use batch processing.
    You will submit a group of prompts in a file, which is stored in an Amazon S3
    bucket. The responses will not be in real time, but provided as a group when processed.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioned throughput
  prefs: []
  type: TYPE_NORMAL
- en: This is when you need guaranteed performance. You can set this for a duration,
    like for one or six months. This will reserve the capacity for your needs. For
    the most part, provisioned throughput is when you have large workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Besides these two approaches, there are other costs as well for using customization
    features, like fine-tuning, RAG, and distillation. There are also fees for guardrails
    and knowledge bases.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Launched in April 2024, Amazon Q is a generative AI virtual assistant for business.
    It’s built on Amazon Bedrock and uses several FMs. There are two versions: Amazon
    Q Business and Amazon Q Developer. Let’s discuss each in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q Business
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Q Business is a virtual assistant for employees. You can embed this into
    a web app or into various systems like Slack, Word, Outlook, Teams, and Microsoft
    365.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the capabilities of Amazon Q:'
  prefs: []
  type: TYPE_NORMAL
- en: Unified search
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q will seamlessly index the data in your organization, such as documents,
    images, and videos. This allows for more accurate and useful responses. All the
    responses from Amazon Q have citations and references, which helps bolster transparency.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q Apps
  prefs: []
  type: TYPE_NORMAL
- en: This allows you to create small apps on the Amazon Q Business system. They can
    be restricted to certain users or made available to the whole organization through
    an app library. With Amazon Q Apps, you can create programs that generate content—like
    writing emails or blogs—and carry out workflows, such as for triggering notifications.
  prefs: []
  type: TYPE_NORMAL
- en: Application tasks
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q Apps has a library of more than 50 actions for many business applications,
    which include ServiceNow, Zendesk, and Salesforce. For example, in Microsoft Exchange,
    you can receive events from your calendar and retrieve emails.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q Business has several plans, which start at $3 per user per month.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q Developer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Q Developer is a tool that leverages generative AI for software development.
    It helps to generate and debug code. The tool can also optimize and explain the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Various studies have shown the notable benefits of Amazon Q Developer. For certain
    organizations, it has increased the speed of development tasks by [up to 80% and
    improved developer productivity by up to 40%](https://oreil.ly/Pb--R).
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of Amazon Q Developer is that it is embedded into the common
    workflows for developers. The tool is available as a plug-in for IDEs like VS
    Code, Visual Studio, and JetBrains, IntelliJ IDEA, and Eclipse. You can also use
    it as a command-line interface (CLI)—such as in a terminal—for creating scripts.
    The same goes for the AWS Console. Then there is an integration with GitLab, which
    is a version control system, as well as Microsoft Teams and Slack.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q Developer has enterprise-grade capabilities too. You can use the tool
    for workload transformations, such as porting .NET applications from Windows to
    Linux or migrating mainframe systems.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this is with Amazon’s own project to migrate tens of thousands
    of applications from Java 8 or 11 to Java 17\. Ordinarily, this process would
    take a staggering 4,500 years of development work. But with Amazon Q Developer,
    the process took a fraction of the time. It also resulted in about [$260 million
    in annual cost savings](https://oreil.ly/YCgw8).
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Q Developer is also tightly integrated with AWS. These are the kinds
    of prompts you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon EC2, and how does it work?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are the best practices for securing my AWS environment?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: List all my running EC2 instances in the us-east-1 region.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What were my EC2 costs by instance type last month?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why can’t I SSH into my EC2 instance?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In terms of the pricing for Amazon Q Developer, there is a free trier. This
    allows up to 50 chat interactions per month. Then there is a premium edition,
    which has a subscription of $19 per month per user.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Bedrock and Amazon Q
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the launch of ChatGPT in late 2022, many companies have invested heavily
    in generative AI projects. But the results have often been disappointing, as shown
    by research from Gartner. It finds that—for 2025—at least [30% of generative AI
    projects](https://oreil.ly/KrkSP) will be abandoned after the proof–of-concept
    stage. Some of the reasons include poor data quality, high costs, and inadequate
    risk guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: This is why platforms like Amazon Bedrock and Amazon Q are so important. As
    we’ve seen in this chapter, they help to greatly streamline the process for creating
    generative AI applications. In fact, you do not have to be a data scientist to
    create an effective system.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that AWS is committed to these AI platforms and continues to
    invest substantial resources in them. This will help to reduce the risks of application
    development and improve the overall quality of the systems.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we took a deep dive into two key generative AI platforms for
    AWS: Amazon Bedrock and Amazon Q. Amazon Bedrock is a powerful application development
    environment, where you can test and integrate models, as well as use techniques
    like RAG, fine-tuning, and AI agents. Meanwhile, Amazon Q is a virtual assistant
    for business and software development. It allows for customization for your proprietary
    data, which allows for more relevant and accurate responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Together, Amazon Bedrock and Amazon Q form a comprehensive AI ecosystem that
    embodies AWS’s customer-obsessed philosophy. They allow organizations to move
    beyond experimentation and toward real, measurable value from generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 6 Answer Key”](app02.html#answers_ch_6).
  prefs: []
  type: TYPE_NORMAL
- en: For an AI model in Amazon Bedrock, what might happen if you adjust the temperature
    and Top P?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using both will guarantee more accurate responses.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a combination of these may cause unpredictable or unexpected responses.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using both will disable the model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using both will make the responses more deterministic.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When configuring the image/video playground in Amazon Bedrock, what is a “negative
    prompt”?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This increases the model’s temperature.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It generates black-and-white images.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It specifies elements you want to exclude from the image or video.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This decreases the size of the image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of the modality filter in Amazon Bedrock’s model catalog?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The filter lets users sort models by license type.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It filters models based on language support.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The filter identifies models that support serverless deployment.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It categorizes models by input and output types like text, image, audio, and
    multimodal.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For a model in Amazon Bedrock, why would you increase the setting for temperature?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate deterministic content
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To produce videos
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate creative content
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To summarize information
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For a model profile in Amazon Bedrock, what type of information would you usually
    find?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type of GPU used
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Version, release date, deployment type, modalities, and model ID
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Only the license information
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The datasets
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What do you need to do to gain access to foundation models (FMs) in Amazon Bedrock
    that are not enabled by default?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable “On-demand” and restart the session.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Submit a model access request form with use case details.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete an AWS certification exam.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the model on your own computer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([1](ch06.html#ch01fn25-marker)) Lindsey Wilkinson, [“Why Generative AI Experiments
    Fail”](https://oreil.ly/G_zFU), CIO Dive, February 14, 2024.
  prefs: []
  type: TYPE_NORMAL
