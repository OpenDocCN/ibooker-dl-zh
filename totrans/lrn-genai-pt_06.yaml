- en: 5 Selecting characteristics in generated images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Building a conditional generative adversarial network to generate images with
    certain attributes (human faces with or without eyeglasses, for example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Wasserstein distance and gradient penalty to improve image quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting vectors associated with different features so that the trained GAN
    model generates images with certain characteristics (male or female faces, for
    example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining conditional GAN with vector selection to specify two attributes simultaneously
    (female faces without glasses or male faces with glasses, for example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The anime faces we generated with deep convolutional GAN (DCGAN) in chapter
    4 look realistic. However, you may have noticed that each generated image has
    different attributes such as hair color, eye color, and whether the head tilts
    toward the left or right. You may be wondering if there is a way to tweak the
    model so that the generated images have certain characteristics (such as with
    black hair and tilting toward the left). It turns out you can.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn two different ways of selecting characteristics
    in the generated images and their respective advantages and disadvantages. The
    first method involves selecting specific vectors in the latent space. Different
    vectors correspond to different characteristics—for example, one vector might
    result in a male face and another in a female face. The second method uses a conditional
    GAN (cGAN), which involves training the model on labeled data. This allows us
    to prompt the model to generate images with a specified label, each representing
    a distinct characteristic—like faces with or without eyeglasses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, you’ll learn to combine the two methods so that you can select
    two independent attributes of the images at the same time. As a result, you can
    generate four different groups of images: males with glasses, males without glasses,
    females with glasses, and females without glasses. To make things more interesting,
    you can use a weighted average of the labels or a weighted average of the input
    vectors to generate images that transition from one attribute to another. For
    example, you can generate a series of images so that the eyeglasses gradually
    fade out on the same person’s face (label arithmetic). Or you can generate a series
    of images so that the male features gradually fade out and a male face changes
    to a female face (vector arithmetic).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Being able to conduct either vector arithmetic or label arithmetic alone feels
    like science fiction, let alone performing the two simultaneously. The whole experience
    reminds us of the quote by Arthur C. Clarke (author of *2001: A Space Odyssey*),
    “Any sufficiently advanced technology is indistinguishable from magic.”'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the realism of the anime faces generated in chapter 4, they were limited
    by low resolution. Training GAN models can be tricky and is often hampered by
    problems like small sample sizes or low-quality images. These challenges can prevent
    models from converging, resulting in poor image quality. To address this, we’ll
    discuss and implement an improved training technique using the Wasserstein distance
    with gradient penalty in our cGAN. This enhancement results in more realistic
    human faces and noticeably better image quality compared to the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 The eyeglasses dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use the eyeglasses dataset in this chapter to train a cGAN model. In
    the next chapter, we’ll also use this dataset to train a CycleGAN model in one
    of the exercises: to convert an image with eyeglasses to an image without eyeglasses
    and vice versa. In this section, you’ll learn to download the dataset and preprocess
    images in it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python programs in this chapter and the next are adapted from two excellent
    online open-source projects: the Kaggle project by Yashika Jain [https://mng.bz/JNVQ](https://mng.bz/JNVQ)
    and a GitHub repository by Aladdin Persson [https://mng.bz/w5yg](https://mng.bz/w5yg).
    I encourage you to look into these two projects while going through this chapter
    and the next.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Downloading the eyeglasses dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The eyeglasses dataset we use is from Kaggle. Log into Kaggle and go to the
    link [https://mng.bz/q0oz](https://mng.bz/q0oz) to download the image folder and
    the two CSV files on the right: `train.csv` and `test.csv`. There are 5,000 images
    in the folder /faces-spring-2020/. Once you have the data, place both the image
    folder and the two CSV files inside the folder /files/ on your computer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll sort the photos into two subfolders: one containing only images
    with eyeglasses and another one with images without eyeglasses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s look at the file train.csv:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads the data in the file train.csv as a pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the values in the id column as the indexes of observations
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous code cell imports the file `train.csv` and sets the variable `id`
    as the index of each observation. The column `glasses` in the file has two values:
    0 or 1, indicating whether the image has eyeglasses in it or not (0 means no glasses;
    1 means with glasses).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we separate the images into two different folders: one containing images
    with eyeglasses and one containing images without eyeglasses.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Sorting images with and without eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a subfolder /files/glasses/G/ to contain images with eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a subfolder /files/glasses/NoG/ to contain images without eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: ③ Moves images labeled 0 to folder NoG
  prefs: []
  type: TYPE_NORMAL
- en: ④ Moves images labeled 1 to folder G
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code cell, we first use the `os` library to create two subfolders
    /glasses/G/ and /glasses/NoG/ inside the folder /files/ on your computer. We then
    use the `shutil` library to move images to the two folders based on the label
    `glasses` in the file `train.csv`. Those labeled 1 are moved to folder G and those
    labeled 0 to folder NoG.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Visualizing images in the eyeglasses dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The classification column `glasses` in the file `train.csv` is not perfect.
    If you go to the subfolder G on your computer, for example, you’ll see that most
    images have glasses, but about 10% have no glasses. Similarly, if you go to the
    subfolder NoG, you’ll see that about 10% actually have glasses. You need to manually
    correct this by moving images from one folder to the other. This is important
    for our training later so you should manually move images in the two folders so
    that one contains only images with glasses and the other images without glasses.
    Welcome to the life of a data scientist: fixing data problems is part of daily
    routine! Let’s first visualize some examples of images with eyeglasses.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Visualizing images with eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Randomly selects 16 images from folder G
  prefs: []
  type: TYPE_NORMAL
- en: ② Displays the 16 images in a 2 × 8 grid
  prefs: []
  type: TYPE_NORMAL
- en: If you have manually corrected the mislabeling of images in folder G, you’ll
    see 16 images with eyeglasses after running the code in listing 5.2\. The output
    is shown in figure 5.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Sample images with eyeglasses in the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: You can change G to NoG in listing 5.2 to visualize 16 sample images without
    eyeglasses in the dataset. The complete code is in the book’s GitHub repository
    [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI). The output
    is shown in figure 5.2\.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 Sample images without eyeglasses in the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 cGAN and Wasserstein distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A cGAN is similar to the GAN models you have seen in chapters 3 and 4, with
    the exception that you attach a label to the input data. The labels correspond
    to different characteristics in the input data. Once the trained GAN model “learns”
    to associate a certain label with a characteristic, you can feed a random noise
    vector with a label to the model to generate output with the desired characteristic.^([1](#footnote-002))
  prefs: []
  type: TYPE_NORMAL
- en: 'GAN models often suffer from problems like mode collapse (the generator finds
    a certain type of output that is good at fooling the discriminator and then collapses
    its outputs to these few modes, ignoring other variations), vanishing gradients,
    and slow convergence. Wasserstein GAN (WGAN) introduces the Earth Mover’s (or
    Wasserstein-1) distance as the loss function, offering a smoother gradient flow
    and more stable training. It mitigates problems like mode collapse.^([2](#footnote-001))
    We’ll implement it in cGAN training in this chapter. Note that WGAN is a concept
    independent of cGAN: It uses the Wasserstein distance to improve the training
    process and can be applied to any GAN model (such as the ones we created in chapters
    3 and 4). We’ll combine both concepts in one setting to save space.'
  prefs: []
  type: TYPE_NORMAL
- en: Other ways to stabilize GAN training
  prefs: []
  type: TYPE_NORMAL
- en: The problems with training GAN models are most common when generating high-resolution
    images. The model architecture is usually complex, with many neural layers. Other
    than WGAN, progressive GAN is another way to stabilize training. Progressive GANs
    enhance the stability of GAN training by breaking down the complex task of high-resolution
    image generation into manageable steps, allowing for more controlled and effective
    learning. For details, see “Progressive Growing of GANs for Improved Quality,
    Stability, and Variation.” by Karas et al., [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 WGAN with gradient penalty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: WGAN is a technique used to improve the training stability and performance of
    GAN models. Regular GANs (such as the ones you have seen in Chapters 3 and 4)
    have two components—a generator and a discriminator. The generator creates fake
    data, while the discriminator evaluates whether the data is real or fake. Training
    involves a competitive zero-sum game in which the generator tries to fool the
    discriminator, and the discriminator tries to accurately classify real and fake
    data instances.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have proposed to use Wasserstein distance (a measure of dissimilarity
    between two distributions) instead of the binary cross-entropy as the loss function
    to stabilize training with a gradient penalty term.^([3](#footnote-000)) The technique
    offers a smoother gradient flow and mitigates problems like mode collapse. Figure
    5.3 provides a diagram of WGAN. As you can see on the right side of the figure,
    the losses associated with the real and fake images are Wasserstein loss instead
    of the regular binary cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3 WGAN with gradient penalty. The discriminator network in WGAN (which
    we call the critic) rates input images: it tries to assign a score of –∞ to a
    fake image (bottom left) and a score of ∞ to the real image (top middle). Further,
    an interpolated image of the real and fake images (top left) is presented to the
    critic, and the gradient penalty with respect to the critic’s rating on the interpolated
    image is added to the total loss in the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: Further, for the Wasserstein distance to work correctly, the discriminator (called
    the critic in WGANs) must be 1-Lipschitz continuous, meaning the gradient norms
    of the critic’s function must be at most 1 everywhere. The original WGAN paper
    proposed weight clipping to enforce the Lipschitz constraint.
  prefs: []
  type: TYPE_NORMAL
- en: To address weight clipping problems, the gradient penalty is added to the loss
    function to enforce the Lipschitz constraint more effectively. To implement WGAN
    with gradient penalty, we first randomly sample points along the straight line
    between real and generated data points (as indicated by the interpolated image
    in the top left of figure 5.3). Since both real and fake images have labels attached
    to them, the interpolated image also has a label attached to it, which is the
    interpolated value of the two original labels. We then compute the gradient of
    the critic’s output with respect to these sampled points. Finally, we add a penalty
    to the loss function proportional to the deviation of these gradient norms from
    1 (the penalty term is called gradient penalty). That is, gradient penalty in
    WGANs is a technique to improve training stability and sample quality by enforcing
    the Lipschitz constraint more effectively, addressing the limitations of the original
    WGAN model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 cGANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'cGAN is an extension of the basic GAN framework. In a cGAN, both the generator
    and the discriminator (or the critic since we are implementing WGAN and cGAN in
    the same setting) are conditioned on some additional information. This could be
    anything, such as class labels, data from other modalities, or even textual descriptions.
    This conditioning is typically achieved by feeding this additional information
    into both the generator and discriminator. In our setting, we’ll add class labels
    to the inputs to both the generator and the critic: we attach one label to images
    with eyeglasses and another label to images without eyeglasses. Figure 5.4 provides
    a diagram of the training process for cGANs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 The training process for cGANs
  prefs: []
  type: TYPE_NORMAL
- en: As you can see at the top left of figure 5.4, in a cGAN, the generator receives
    both a random noise vector and the conditional information (a label indicating
    whether the image has eyeglasses or not) as input. It uses this information to
    generate data that not only looks real but also aligns with the conditional input.
  prefs: []
  type: TYPE_NORMAL
- en: The critic receives either real data from the training set or fake data generated
    by the generator, along with the conditional information (a label indicating whether
    the image has eyeglasses or not in our setting). Its task is to determine whether
    the given data is real or fake, taking the conditional information into account
    (does the generated image have eyeglasses in it?). In figure 5.4, we use the critic
    network instead of the discriminator network since we implement both cGAN and
    WGAN simultaneously, but the concept of cGAN applies to traditional GANs as well.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of cGANs is their ability to select aspects of the generated
    data, making them more versatile and applicable in scenarios where the output
    needs to be directed or conditioned on certain input parameters. In our setting,
    we’ll train the cGAN so that we have the ability to select whether the generated
    images have eyeglasses or not.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, cGANs are a powerful extension of the basic GAN architecture, enabling
    targeted generation of synthetic data based on conditional inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Create a cGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you’ll learn to create a cGAN to generate human faces with
    or without eyeglasses. You’ll also learn to implement the WGAN with gradient penalty
    to stabilize training.
  prefs: []
  type: TYPE_NORMAL
- en: The generator in cGANs uses not only random noise vectors but also conditional
    information such as labels as inputs to create images either with or without eyeglasses.
    Further, a critic network in WGANs is different from the discriminator network
    in traditional GANs. You’ll also learn how to calculate the Wasserstein distance
    and the gradient penalty in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 A critic in cGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In cGANs, the discriminator is a binary classifier to identify the input as
    either real or fake, conditional on the label. In WGAN, we call the discriminator
    network the critic. The critic evaluates the input and gives a score between −∞
    and ∞. The higher the score, the more likely that the input is from the training
    set (that is, real).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 creates the critic network. The architecture is somewhat similar
    to the discriminator network we used in chapter 4 when generating color images
    of anime faces. In particular, we use seven `Conv2d` layers in PyTorch to gradually
    downsample the input so that the output is a single value between −∞ and ∞.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 A critic network in cGAN with Wasserstein distance
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① The critic network has two Conv2d layers plus five blocks.
  prefs: []
  type: TYPE_NORMAL
- en: ② The output has one feature, without activation.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Each block contains a Conv2d layer, an InstanceNorm2d layer, with LeakyReLU
    activation.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the critic network is a color image with a shape of 5 × 256 × 256.
    The first three channels are the color channels (colors red, green, and blue).
    The last two channels (the fourth and fifth channels) are label channels to tell
    the critic whether the image is with glasses or without glasses. We’ll discuss
    the exact mechanism to accomplish this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The critic network consists of seven `Conv2d` layers. In chapter 4, we discussed
    in depth how these layers work. They are used for feature extraction by applying
    a set of learnable filters on the input images to detect patterns and features
    at different spatial scales, effectively capturing hierarchical representations
    of the input data. The critic then evaluates the input images based on these representations.
    The five `Conv2d` layers in the middle are all followed by an `InstanceNorm2d`
    layer and a `LeakyReLU` activation; hence, we define a `block()` method to streamline
    the critic network. The `InstanceNorm2d` layer is similar to the `BatchNorm2d`
    layer we discussed in chapter 4, except that we normalize each individual instance
    in the batch independently.
  prefs: []
  type: TYPE_NORMAL
- en: Another key point is that the output is no longer a value between 0 and 1 since
    we don’t use the sigmoid activation in the last layer in the critic network. Instead,
    the output is a value between −∞ and ∞ since we use the Wasserstein distance with
    gradient penalty in our cGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 A generator in cGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In WGANs, the generator’s job is to create data instances so that they can be
    evaluated at a high score by the critic. In cGANs, the generator must generate
    data instances with conditional information (with or without eyeglasses in our
    setting). Since we are implementing a cGAN with Wasserstein distance, we’ll tell
    the generator what type of images we want to generate by attaching a label to
    the random noise vector. We’ll discuss the exact mechanism in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: We create the neural network shown in the following listing to represent the
    generator.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 A generator in cGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① The generator consists of seven ConvTranspose2d layers.
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses Tanh activation to squeeze values to the range [–1, 1], the same as images
    in the training set
  prefs: []
  type: TYPE_NORMAL
- en: ③ Each block consists of a ConvTranspose2d layer, a BatchNorm2d layer, and ReLU
    activation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll feed a random noise vector from a 100-dimensional latent space to the
    generator as input. We’ll also feed a 2-value one-hot encoded image label to the
    generator to tell it to generate an image either with or without eyeglasses. We’ll
    concatenate the two pieces of information together to form a 102-dimensional input
    variable to the generator. The generator then generates a color image based on
    the input from the latent space and the labeling information.
  prefs: []
  type: TYPE_NORMAL
- en: The generator network consists of seven `ConvTranspose2d` layers, and the idea
    is to mirror the steps in the critic network to conjure up images, as we discussed
    in chapter 4\. The first six `ConvTranspose2d` layers are all followed by a `BatchNorm2d`
    layer and a `ReLU` activation; hence, we define a `block()` method in the generator
    network to simplify the architecture. As we have done in chapter 4, we use the
    Tanh activation function at the output layer so the output pixels are all in the
    range of –1 and 1, the same as the images in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Weight initialization and the gradient penalty function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In deep learning, the weights in neural networks are randomly initialized. When
    the network architecture is complicated, and there are many hidden layers (which
    is the case in our setting), how weights are initialized is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'We, therefore, define the following `weights_init()` function to initialize
    weights in both the generator and the critic networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The function initializes weights in `Conv2d` and `ConvTranspose2d` layers with
    values drawn from a normal distribution with a mean of 0 and a standard deviation
    of 0.02\. It also initializes weights in `BatchNorm2d` layers with values drawn
    from a normal distribution with a mean of 1 and a standard deviation of 0.02\.
    We choose a small standard deviation in weight initializations to avoid exploding
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a generator and a critic based on the `Generator()` and `Critic()`
    classes we defined in the last subsection. We then initialize the weights in them
    based on the `weights_init()` function defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we’ll use the Adam optimizer for both the critic and the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The generator tries to create images that are indistinguishable from those
    in the training set with the given label. It presents the images to the critic
    to obtain high ratings on the generated images. The critic, on the other hand,
    tries to assign high ratings to real images and low ratings to fake images, conditional
    on the given label. Specifically, the loss function for the critic has three components:'
  prefs: []
  type: TYPE_NORMAL
- en: critic_value(fake) − critic_value(real) + weight × GradientPenalty
  prefs: []
  type: TYPE_NORMAL
- en: The first term, *critic_value(fake)*, says that if an image is fake, the critic’s
    objective is to identify it as fake and give it a low evaluation. The second term,
    *− critic_value(real)*, indicates that if the image is real, the critic’s objective
    is to identify it as real and give it a high evaluation. Further, the critic wants
    to minimize the gradient penalty term, *weight* *× GradientPenalty*, where *weight*
    is a constant to determine how much penalty we want to assign to deviations of
    the gradient norms from the value 1\. The gradient penalty is calculated as shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Calculating gradient penalty
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates an interpolated image of the real and the fake
  prefs: []
  type: TYPE_NORMAL
- en: ② Obtains the critic value with respect to the interpolated image
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calculates the gradient of the critic value
  prefs: []
  type: TYPE_NORMAL
- en: ④ Gradient penalty is the squared deviation of the gradient norm from value
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the function `GP()`, we first create interpolated images of real ones and
    fake ones. This is done by randomly sampling points along the straight line between
    real and generated images. Imagine a slider: at one end is the real image, and
    at the other is the fake image. As you move the slider, you see a continuous blend
    from the real to the fake, with the interpolated images representing the stages
    in between.'
  prefs: []
  type: TYPE_NORMAL
- en: We then present interpolated images to the critic network to obtain ratings
    on them and calculate the gradient of the critic’s output with respect to the
    interpolated images. Finally, the gradient penalty is calculated as the squared
    deviation of the gradient norms from the target value of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Training the cGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned in the last section, we need to find a way to tell both the
    critic and the generator what the image label is so they know if the image has
    eyeglasses or not.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll first learn how to add labels to the inputs to the critic
    network and the inputs to the generator network so the generator knows what type
    of images to create while the critic can evaluate the images conditional on the
    labels. After that, you’ll learn how to train the cGAN with Wasserstein distance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Adding labels to inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first preprocess the data and convert the images to torch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We set the batch size to 16 and the image size to 256 by 256 pixels. The pixel
    values are chosen so the generated images have higher resolutions than those in
    the last chapter (64 by 64 pixels). We choose a batch size of 16, smaller than
    the batch size in chapter 3, due to the larger image size. If the batch size is
    too large, your GPU (or even CPU) will run out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: tip If you are using GPU training and your GPU has a small memory (say, 6GB),
    consider reducing the batch size to a smaller number than 16, such as 10 or 8,
    so that your GPU doesn’t run out of memory. Alternatively, you can keep the batch
    size at 16 but switch to CPU training to address the GPU memory problem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll add labels to the training data. Since there are two types of images—images
    with eyeglasses and images without glasses—we’ll create two one-hot image labels.
    Images with glasses will have a one-hot label of [1, 0], and images without glasses
    will have a one-hot label of [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: 'The input to the generator is a 100-value random noise vector. We concatenate
    the one-hot label with the random noise vector and feed the 102-value input to
    the generator. The input to the critic network is a three-channel color image
    with a shape of 3 by 256 by 256 (PyTorch uses channel-first tensors to represent
    images). How do we attach a label with a shape of 1 by 2 to an image with a shape
    of 3 by 256 by 256? The solution is to add two channels to the input image so
    that the image shape changes from (3, 256, 256) to (5, 256, 256): the two additional
    channels are the one-hot labels. Specifically, if an image has eyeglasses in it,
    the fourth channel is filled with 1s and the fifth channel 0s; if the image has
    no eyeglasses in it, the fourth channel is filled with 0s and the fifth channel
    1s.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating labels if there are more than two values in a characteristic
  prefs: []
  type: TYPE_NORMAL
- en: You can easily extend the cGAN model to characteristics with more than two values.
    For example, if you create a model to generate images with the different hair
    colors black, blond, and white, the image labels you feed to the generator can
    have values [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively. You can attach
    three channels to the input image before you feed it to the discriminator or critic.
    For example, if an image has black hair, the fourth channel is filled with 1s
    and the fifth and sixth channels 0s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, in the eyeglasses example, since there are only two values in
    the label, you can potentially use values 0 and 1 to indicate images with and
    without glasses when you feed the label to the generator. You can attach one channel
    to the input image before you feed it to the critic: if an image has eyeglasses,
    the fourth channel is filled with 1s; if the image has no eyeglasses, the fourth
    channel is filled with 0s. I’ll leave that as an exercise for you. The solution
    is provided in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: We implement this change as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Attaching labels to input images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates two extra channels filled with 0s, each channel with a shape of 256
    by 256, the same as the dimension of each channel in the input image
  prefs: []
  type: TYPE_NORMAL
- en: ② If the original image label is 0, fills the fourth channel with 1s
  prefs: []
  type: TYPE_NORMAL
- en: ③ If the original image label is 1, fills the fifth channel with 1s
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds the fourth and fifth channels to the original image to form a five-channel
    labeled image
  prefs: []
  type: TYPE_NORMAL
- en: tip Earlier when we load the images by using the `torchvision.datasets.ImageFolder()`
    method from the folder /files/glasses, PyTorch assigns labels to images in each
    subfolder in alphabetical order. Therefore, images in /files/glasses/G/ are assigned
    a label of 0, and those in /files/glasses/NoG/, a label of 1.
  prefs: []
  type: TYPE_NORMAL
- en: We first create an empty list `newdata` to hold images with labels. We create
    a PyTorch tensor with a shape (2, 256, 256) to be attached to the original input
    image to form a new image with a shape of (5, 256, 256). If the original image
    label is 0 (this means images are from the folder /files/glasses/G/), we fill
    the fourth channel with 1s and the fifth channel with 0s so that the critic knows
    it’s an image with glasses. On the other hand, if the original image label is
    1 (this means images are from the folder /files/glasses/NoG/), we fill the fourth
    channel with 0s and the fifth channel with 1s so that the critic knows it’s an
    image without glasses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a data iterator with batches (to improve computational efficiency,
    memory usage, and optimization dynamics in the training process) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 5.4.2 Training the cGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have the training data and two networks, we’ll train the cGAN. We’ll
    use visual inspections to determine when the training should stop.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we’ll discard the critic network and use the generator
    to create images with a certain characteristic (with or without glasses, in our
    case).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll create a function to test periodically what the generated images look
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Inspecting generated images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a one-hot label for images with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ② Feeds the concatenated noise vector and label to the generator to create images
    with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ③ Plots the generated images with glasses
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a one-hot label for images without glasses
  prefs: []
  type: TYPE_NORMAL
- en: 'After each epoch of training, we’ll ask the generator to create a set of images
    with glasses and a set of images without glasses. We then plot the images so that
    we can inspect them visually. To create images with glasses, we first create one-hot
    labels [1, 0] and attach them to the random noise vectors before feeding the concatenated
    vector to the generator network. The generator creates images with glasses since
    the label is [1, 0] instead of [0, 1]. We then plot the generated images in four
    rows and eight columns and save the subplots on your computer. The process of
    creating images without glasses is similar, except that we use the one-hot label
    [0, 1] instead of [1, 0]. I skipped part of the code in listing 5.7, but you can
    find it in the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: We define a `train_batch()` function to train the model with a batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Training the model with a batch of data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① A batch of real images with labels
  prefs: []
  type: TYPE_NORMAL
- en: ② A batch of generated images with labels
  prefs: []
  type: TYPE_NORMAL
- en: '③ The total loss for the critic has three components: loss from evaluating
    real images, loss from evaluating fake images, and the gradient penalty loss.'
  prefs: []
  type: TYPE_NORMAL
- en: ④ Trains the generator with the Wasserstein loss
  prefs: []
  type: TYPE_NORMAL
- en: In the `train_batch()` function, we first train the critic with real images.
    We also ask the generator to create a batch of fake data with the given label.
    We then train the critic with fake images. In the `train_batch()` function, we
    also train the generator with a batch of fake data.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE The loss for the critic has three components: loss from evaluating real
    images, loss from evaluating fake images, and the gradient penalty loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now train the model for 100 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches in the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Trains the model with a batch of data
  prefs: []
  type: TYPE_NORMAL
- en: ③ Saves the weights in the trained generator
  prefs: []
  type: TYPE_NORMAL
- en: After each epoch of training, we print out the critic loss and the generator
    loss to ensure that the losses are in a reasonable range. We also generate 32
    images of faces with glasses as well as 32 images without glasses by using the
    `plot_epoch()` function we defined earlier. We save the weights in the trained
    generator in the local folder after training is done so that later we can generate
    images using the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This training takes about 30 minutes if you are using GPU training. Otherwise,
    it may take several hours, depending on the hardware configuration on your computer.
    Alternatively, you can download the trained model from my website: [https://gattonweb.uky.edu/faculty/lium/gai/cgan.zip](https://gattonweb.uky.edu/faculty/lium/gai/cgan.zip).
    Unzip the file after downloading.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Selecting characteristics in generated images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are at least two ways to generate images with a certain characteristic.
    The first is to attach a label to a random noise vector before feeding it to the
    trained cGAN model. Different labels lead to different characteristics in the
    generated image (in our case, whether the image has eyeglasses). The second way
    is to select the noise vector you feed to the trained model: while one vector
    leads to an image with a male face, another leads to an image with a female face.
    Note that the second way works even in a traditional GAN such as the ones we trained
    in chapter 4\. It works in a cGAN as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Better yet, in this section, you’ll learn to combine these two methods so you
    can select two characteristics simultaneously: an image of a male face with eyeglasses
    or a female face without eyeglasses, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: There are pros and cons for each one of these two methods in selecting a certain
    characteristic in generated images. The first way, the cGAN, requires labeled
    data to train the model. Sometimes, labeled data is costly to curate. However,
    once you have successfully trained a cGAN, you can generate a wide range of images
    with a certain characteristic. In our case, you can generate many different images
    with eyeglasses (or without eyeglasses); each one is different from the other.
    The second way, handpicking a noise vector, doesn’t need labeled data to train
    the model. However, each handpicked noise vector can only generate one image.
    If you want to generate many different images with the same characteristic as
    the cGAN, you’ll need to handpick many different noise vectors ex ante.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Selecting images with or without eyeglasses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By attaching a label of either [1, 0] or [0, 1] to a random noise vector before
    you feed it to the trained cGAN model, you can select whether the generated image
    has eyeglasses.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll use the trained model to generate 32 images with glasses and plot
    them in a 4 × 8 grid. To make results reproducible, we’ll fix the random state
    in PyTorch. Further, we’ll use the same set of random noise vectors so that we
    look at the same set of faces.
  prefs: []
  type: TYPE_NORMAL
- en: We fix the random state at seed 0 and generate 32 images of faces with eyeglasses.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9 Generating images of human faces with eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Fixes the random state so results are reproducible
  prefs: []
  type: TYPE_NORMAL
- en: ② Loads up the trained weights
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generates a set of random noise vectors and saves it so we can select certain
    vectors from it to perform vector arithmetic
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a label to generate images with eyeglasses
  prefs: []
  type: TYPE_NORMAL
- en: 'We create another instance of the `Generator()` class and name it `generator`.
    We then load up the trained weights that we saved in the local folder in the last
    section (or you can download the weights from my website: [https://mng.bz/75Z4](https://mng.bz/75Z4)).
    To generate 32 images of human faces with eyeglasses; we first draw 32 random
    noise vectors in the latent space. We’ll also create a set of labels and name
    them `labels_g`, and they tell the generator to produce 32 images with eyeglasses.'
  prefs: []
  type: TYPE_NORMAL
- en: If you run the program in listing 5.9, you’ll see 32 images as shown in figure
    5.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Images of human faces with eyeglasses that are generated by the trained
    cGAN model
  prefs: []
  type: TYPE_NORMAL
- en: 'First, all 32 images do have eyeglasses in them. This indicates that the trained
    cGAN model is able to generate images conditional on the provided labels. You
    may have noticed that some images have male features while others have female
    features. To prepare us for vector arithmetic in the next subsection, we’ll select
    one random noise vector that leads to an image with male features and one that
    leads to female features. After inspecting the 32 images in figure 5.5, we select
    images with index values 0 and 14, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate 32 images without eyeglasses, we first produce another set of random
    noise vectors and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The new set of random noise vectors is named `noise_ng`, and the new set of
    labels `labels_ng`. Feed them to the generator and you should see 32 images without
    eyeglasses, as shown in figure 5.6.
  prefs: []
  type: TYPE_NORMAL
- en: 'None of the 32 faces in figure 5.6 has eyeglasses in it: the trained cGAN model
    can generate images contingent upon the given label. We select images with indexes
    8 (male) and 31 (female) to prepare for vector arithmetic in the next subsection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../../OEBPS/Images/CH05_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Images of human faces without eyeglasses that are generated by the
    trained cGAN model
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use label interpolation to perform label arithmetic. Recall that
    the two labels, `noise_g` and `noise_ng`, instruct the trained cGAN model to create
    images with and without eyeglasses, respectively. What if we feed an interpolated
    label (a weighted average of the two labels [1, 0] and [0, 1]) to the model? What
    type of images will the trained generator produce? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 Label arithmetic in cGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates five weights
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a weighted average of the two labels
  prefs: []
  type: TYPE_NORMAL
- en: ③ Gives the new label to the trained model to create an image
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create five weights (w): 0, 0.25, 0.5, 0.75, and 1, equally spaced
    between 0 and 1\. Each of these five values of w is the weight we put on the no
    eyeglasses label `labels_ng`. The complementary weight is put on the eyeglasses
    label `labels_g`. The interpolated label therefore has a value of `w*labels_ng+(1-w)*labels_g`.
    We then feed the interpolated label to the trained model, along with the random
    noise vector `z_female_g` that we saved earlier. The five generated images, based
    on the five values of w, are plotted in a 1 × 5 grid, as shown in figure 5.7\.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7 Label arithmetic in cGAN. We first create two labels: the no eyeglasses
    label `labels_ng` and the eyeglasses label `labels_g`. These two labels instruct
    the trained generator to produce images with and without eyeglasses, respectively.
    We then create five interpolated labels, each as a weighted average of the original
    two labels: `w*labels_ng+(1-w)*labels_g`, where the weight `w` takes five different
    values, 0, 0.25, 0.5, 0.75, and 1\. The five generated images based on the five
    interpolated labels are shown in the figure. The image on the far left has eyeglasses.
    As we move from the left to the right, the eyeglasses gradually fade away, until
    the image on the far right has no eyeglasses in it.'
  prefs: []
  type: TYPE_NORMAL
- en: When you look at the five generated images in figure 5.7 from the left to the
    right, you’ll notice that the eyeglasses gradually fade away. The image on the
    left has eyeglasses while the image on the right has no eyeglasses. The three
    images in the middle show some signs of eyeglasses, but the eyeglasses are not
    as conspicuous as those in the first image.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.1
  prefs: []
  type: TYPE_NORMAL
- en: Since we used the random noise vector `z_female_g` in listing 5.10, the images
    in figure 5.7 have a female face. Change the noise vector to `z_male_g` in listing
    5.10 and rerun the program; see what the images look like.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Vector arithmetic in latent space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have noticed that some generated human face images have male features
    while others have female features. You may wonder: Can we select male or female
    features in generated images? The answer is yes. We can achieve this by selecting
    noise vectors in the latent space.'
  prefs: []
  type: TYPE_NORMAL
- en: In the last subsection, we have saved two random noise vectors, `z_male_ng`
    and `z_female_ng`, that lead to images of a male face and a female face, respectively.
    Next, we feed a weighted average of the two vectors (i.e., an interpolated vector)
    to the trained model and see what the generated images look like.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Vector arithmetic to select image characteristics
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates five weights
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a weighted average of the two random noise vectors
  prefs: []
  type: TYPE_NORMAL
- en: ③ Feeds the new random noise vector to the trained model to create an image
  prefs: []
  type: TYPE_NORMAL
- en: We have created five weights, 0, 0.25, 0.5, 0.75, and 1\. We iterate through
    the five weights and create five weighted averages of the two random noise vectors,
    `w*z_female_ng+(1-w)*z_male_ng`. We then feed the five vectors, along with the
    label, `labels_ng`, to the trained model to obtain five images, as shown in figure
    5.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F08_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8 Vector arithmetic in GAN. We first save two random noise vectors
    `z_female_ng` and `z_male_ng`. The two vectors lead to images of female and male
    faces, respectively. We then create five interpolated vectors, each as a weighted
    average of the original two vectors: `w*z_female_ng+(1-w)*z_male_ng`, where the
    weight `w` takes five different values, 0, 0.25, 0.5, 0.75, and 1\. The five generated
    images based on the five interpolated vectors are shown in the figure. The image
    on the far left has male features. As we move from the left to the right, the
    male features gradually fade away and the female features gradually appear, until
    the image on the far right shows a female face.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector arithmetic can transition from one instance of an image to another instance.
    Since we happen to have selected a male and a female image, when you look at the
    five generated images in figure 5.8 from the left to the right, you’ll notice
    that male features gradually fade away and female features gradually appear. The
    first image shows an image with a male face while the last image shows an image
    with a female face.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.2
  prefs: []
  type: TYPE_NORMAL
- en: Since we used the label `labels_ng` in listing 5.11, the images in figure 5.8
    have no eyeglasses in them. Change the label to `labels_g` in listing 5.11 and
    rerun the program to see what the images look like.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Selecting two characteristics simultaneously
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have selected one characteristic at a time. By selecting the label,
    you have learned how to generate images with or without eyeglasses. By selecting
    a specific noise vector, you have learned how to select a specific instance of
    the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you want to select two characteristics (glasses and gender, for example)
    at the same time? There are four possible combinations of the two independent
    characteristics: male faces with glasses, male faces without glasses, female faces
    with glasses, and female faces without glasses. Next we’ll generate an image of
    each type.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 Selecting two characteristics simultaneously
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through 0 to 3
  prefs: []
  type: TYPE_NORMAL
- en: ② The value of p, which can be either 0 or 1, selects the random noise vector
    to generate a male or female face.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The value of q, which can be either 0 or 1, selects the label to determine
    whether the generated image has eyeglasses in it or not.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Combines the random noise vector with the label to select two characteristics
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate four images to cover the four different cases, we need to use one
    of the noise vectors as the input: `z_female_g` or `z_male_g`. We also need to
    attach to the input a label, which can be either `labels_ng` or `labels_g.` To
    use one single program to cover all four cases, we iterate through four values
    of i, 0 to 3, and create two values, p and q, which are the integer quotient and
    the remainder of the value i divided by 2\. Therefore, the values of p and q can
    be either 0 or 1\. By setting the value of the random noise vector to `z_female_g*p+z_male_g*(1-p)`,
    we can select a random noise vector to generate either a male or female face.
    Similarly, by setting the value of the label to `labels_ng[0]*q+labels_g[0]*(1-q)`,
    we can select a label to determine whether the generated image has eyeglasses
    in it or not. Once we combine the random noise vector with the label and feed
    them to the trained model, we can select two characteristics simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: If you run the program in listing 5.12, you’ll see four images as shown in figure
    5.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F09_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9 Selecting two characteristics simultaneously in the generated image.
    We select a noise vector from the following two choices: `z_female_ng` and `z_male_ng`.
    We also select a label from the following two choices: `labels_ng` and `labels_g`.
    We then feed the noise vector and the label to the trained generator to create
    an image. Based on the values of the noise vector and the label, the trained model
    can create four types of images. By doing this, we effectively select two independent
    characteristics in the generated image: a male or a female face and whether the
    image has eyeglasses in it or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The four generated images in figure 5.9 have two independent characteristics:
    a male or a female face and whether the image has eyeglasses in it or not. The
    first image shows an image of a male face with glasses; the second image is a
    male face without glasses. The third image is a female face with glasses, while
    the last image shows a female face without glasses.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.3
  prefs: []
  type: TYPE_NORMAL
- en: We used the two random noise vectors `z_female_g` and `z_male_g` in listing
    5.12\. Change the two random noise vectors to `z_female_ng` and `z_male_ng` instead
    and rerun the program to see what the images look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can conduct label arithmetic and vector arithmetic simultaneously.
    That is, we can feed an interpolated noise vector and an interpolated label to
    the trained cGAN model and see what the generated image looks like. You can achieve
    that by running the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is similar to that in listing 5.12, except that p and q each can take
    six different values: 0, 1, 2, 3, 4, and 5\. The random noise vector, `z_female_ng*p/5+z_male_ng*(1-p/5)`,
    takes six different values based on the value of p. The label, `labels_ng[0]*q/5+labels_g[0]*(1-q/5)`,
    takes six different values based on the value of q. We therefore have 36 different
    combinations of images based on the interpolated noise vector and the interpolated
    label. If you run the previous program, you’ll see 36 images as shown in figure
    5.10.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH05_F10_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10 Conducting vector arithmetic and label arithmetic simultaneously.
    The value of i changes from 0 to 35; p and q are the integer quotient and remainder,
    respectively, when i is divided by 6\. Therefore, p and q each can take six different
    values: 0, 1, 2, 3, 4, and 5\. The interpolated noise vector, `z_female_ng*p/5+z_male_ng*(1-p/5)`,
    and the interpolated label, `labels_ng[0]*q/5+labels_g[0]*(1-q/5)`, can each take
    six different values. In each row, when you go from left to right, the eyeglasses
    gradually fade away. In each column, when you go from top to bottom, the image
    changes gradually from a male face to a female face.'
  prefs: []
  type: TYPE_NORMAL
- en: The are 36 images in figure 5.10\. The interpolated noise vector is a weighted
    average of the two random noise vectors, `z_female_ng` and `z_male_ng`, which
    generate a female face and a male face, respectively. The label is a weighted
    average of the two labels, `labels_ng` and `labels_g`, which determine whether
    the generated image has eyeglasses in it or not. The trained model generates 36
    different images based on the interpolated noise vector and the interpolated label.
    In each row, when you go from the left to the right, the eyeglasses gradually
    fade away. That is, we conduct label arithmetic in each row. In each column, when
    you go from the top to the bottom, the image changes gradually from a male face
    to a female face. That is, we conduct vector arithmetic in each column.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this project, there are two values in the label: one indicates eyeglasses
    and one indicates no eyeglasses. Therefore, we can use a binary value instead
    of one-hot variables as labels. Change the programs in this chapter and use values
    1 and 0 (instead of [1, 0] and [0, 1]) to represent images with and without glasses.
    Attach 1 or 0 to the random noise vector so that you feed a 101-value vector to
    the generator. Attach one channel to the input image before you feed it to the
    critic: if an image has eyeglasses in it, the fourth channel is filled with 0s;
    if the image has no eyeglasses in it, the fourth channel is filled with 1s. Then
    create a generator and a critic; use the training dataset to train them. The solution
    is provided in the book’s GitHub repository, along with solutions to the other
    three exercises in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have witnessed what GAN models are capable of, you’ll explore
    deeper in the next chapter by conducting style transfers with GANs. For example,
    you’ll learn how to build a CycleGAN model and train it using celebrity face images
    so that you can convert blond hair to black hair or black hair to blond hair in
    these images. The exact same model can be trained on other datasets: for example,
    you can train it on the human face dataset you used in this chapter so that you
    can add or remove eyeglasses in human face images.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By selecting a certain noise vector in the latent space and feeding it to the
    trained GAN model, we can select a certain characteristic in the generated image,
    such as whether the image has a male or female face in it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cGAN is different from a traditional GAN. We train the model on labeled data
    and ask the trained model to generate data with a specific attribute. For example,
    one label tells the model to generate images of human faces with eyeglasses while
    another tells the model to create human faces without eyeglasses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After a cGAN is trained, we can use a series of weighted averages of the labels
    to generate images that transition from an image represented by one label to an
    image represented by another label—for example, a series of images in which the
    eyeglasses gradually fade away on the same person’s face. We call this label arithmetic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also use a series of weighted averages of two different noise vectors
    to create images that transition from one attribute to another—for example, a
    series of images in which the male features gradually fade away, and female features
    gradually appear. We call this vector arithmetic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wasserstein GAN (WGAN) is a technique used to improve the training stability
    and performance of GAN models by using Wasserstein distance instead of the binary
    cross-entropy as the loss function. Further, for the Wasserstein distance to work
    correctly, the critic in WGANs must be 1-Lipschitz continuous, meaning the gradient
    norms of the critic’s function must be at most 1 everywhere. The gradient penalty
    in WGANs adds a regularization term to the loss function to enforce the Lipschitz
    constraint more effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-002-backlink))  Mehdi Mirza, Simon Osindero, 2014, “Conditional
    Generative Adversarial Nets.” [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](#footnote-001-backlink))  Martin Arjovsky, Soumith Chintala, and Léon
    Bottou, 2017, “Wasserstein GAN.” [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](#footnote-000-backlink))  Martin Arjovsky, Soumith Chintala, and Leon
    Bottou, 2017, “Wasserstein GAN.” [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875);
    and Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron
    Courville, 2017, “Improved Training of Wasserstein GANs.” [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028).
  prefs: []
  type: TYPE_NORMAL
