["```py\n07.1 Text Classification\n07.2 Emoji Suggestions\n07.3 Tweet Embeddings\n```", "```py\nimport pandas as pd\nfrom keras.utils.data_utils import get_file\nimport nb_utils\n\nemotion_csv = get_file('text_emotion.csv',\n                       'https://www.crowdflower.com/wp-content/'\n                       'uploads/2016/07/text_emotion.csv')\nemotion_df = pd.read_csv(emotion_csv)\n\nemotion_df.head()\n```", "```py\nemotion_df['sentiment'].value_counts()\n```", "```py\nneutral       8638\nworry         8459\nhappiness     5209\nsadness       5165\nlove          3842\nsurprise      2187\n```", "```py\ntfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\nlabel_encoder = LabelEncoder()\nlinear_x = tfidf_vec.fit_transform(emotion_df['content'])\nlinear_y = label_encoder.fit_transform(emotion_df['sentiment'])\n```", "```py\nbayes = MultinomialNB()\nbayes.fit(linear_x, linear_y)\npred = bayes.predict(linear_x)\nprecision_score(pred, linear_y, average='micro')\n```", "```py\n0.28022727272727271\n```", "```py\nclassifiers = {'sgd': SGDClassifier(loss='hinge'),\n               'svm': SVC(),\n               'random_forrest': RandomForestClassifier()}\n\nfor lbl, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    print(lbl, precision_score(predictions, y_test, average='micro'))\n```", "```py\nrandom_forrest 0.283939393939\nsvm 0.218636363636\nsgd 0.325454545455\n```", "```py\nd = eye(len(tfidf_vec.vocabulary_))\nword_pred = bayes.predict_proba(d)\n```", "```py\nby_cls = defaultdict(Counter)\nfor word_idx, pred in enumerate(word_pred):\n    for class_idx, score in enumerate(pred):\n        cls = label_encoder.classes_[class_idx]\n        by_cls[cls][inverse_vocab[word_idx]] = score\n```", "```py\nfor k in by_cls:\n    words = [x[0] for x in by_cls[k].most_common(5)]\n    print(k, ':', ' '.join(words))\n```", "```py\nhappiness : excited woohoo excellent yay wars\nhate : hate hates suck fucking zomberellamcfox\nboredom : squeaking ouuut cleanin sooooooo candyland3\nenthusiasm : lena_distractia foolproofdiva attending krisswouldhowse tatt\nfun : xbox bamboozle sanctuary oldies toodaayy\nlove : love mothers mommies moms loved\nsurprise : surprise wow surprised wtf surprisingly\nempty : makinitrite conversating less_than_3 shakeyourjunk kimbermuffin\nanger : confuzzled fridaaaayyyyy aaaaaaaaaaa transtelecom filthy\nworry : worried poor throat hurts sick\nrelief : finally relax mastered relief inspiration\nsadness : sad sadly cry cried miss\nneutral : www painting souljaboytellem link frenchieb\n```", "```py\nchar_input = Input(shape=(max_sequence_len, num_chars), name='input')\n\nconv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\nmax_pool_1x = MaxPooling1D(6)(conv_1x)\nconv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\nmax_pool_2x = MaxPooling1D(6)(conv_2x)\n\nflatten = Flatten()(max_pool_2x)\ndense = Dense(128, activation='relu')(flatten)\npreds = Dense(num_labels, activation='softmax')(dense)\n\nmodel = Model(char_input, preds)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n```", "```py\nchars = list(sorted(set(chain(*emotion_df['content']))))\nchar_to_idx = {ch: idx for idx, ch in enumerate(chars)}\nmax_sequence_len = max(len(x) for x in emotion_df['content'])\n\nchar_vectors = []\nfor txt in emotion_df['content']:\n    vec = np.zeros((max_sequence_len, len(char_to_idx)))\n    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1\n    char_vectors.append(vec)\nchar_vectors = np.asarray(char_vectors)\nchar_vectors = pad_sequences(char_vectors)\nlabels = label_encoder.transform(emotion_df['sentiment'])\n```", "```py\ndef split(lst):\n    training_count = int(0.9 * len(char_vectors))\n    return lst[:training_count], lst[training_count:]\n\ntraining_char_vectors, test_char_vectors = split(char_vectors)\ntraining_labels, test_labels = split(labels)\n```", "```py\nchar_cnn_model.fit(training_char_vectors, training_labels,\n                   epochs=20, batch_size=1024)\nchar_cnn_model.evaluate(test_char_vectors, test_labels)\n```", "```py\nCONSUMER_KEY = '<*`your value`*>'\nCONSUMER_SECRET = '<*`your value`*>'\nACCESS_TOKEN = '<*`your value`*>'\nACCESS_SECRET = '<*`your value`*>'\n```", "```py\nauth=twitter.OAuth(\n    consumer_key=CONSUMER_KEY,\n    consumer_secret=CONSUMER_SECRET,\n    token=ACCESS_TOKEN,\n    token_secret=ACCESS_SECRET,\n)\n```", "```py\nstatus_stream = twitter.TwitterStream(auth=auth).statuses\n```", "```py\n[x['text'] for x in itertools.islice(stream.sample(), 0, 5) if x.get('text')]\n```", "```py\ndef english_has_emoji(tweet):\n    if tweet.get('lang') != 'en':\n        return False\n    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n```", "```py\ntweets = list(itertools.islice(\n    filter(english_has_emoji, status_stream.sample()), 0, 100))\n```", "```py\nstripped = []\nfor tweet in tweets:\n    text = tweet['text']\n    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n    if len(emojis) == 1:\n        emoiji = emojis.pop()\n        text = ''.join(ch for ch in text if ch != emoiji)\n        stripped.append((text, emoiji))\n```", "```py\nall_tweets = pd.read_csv('data/emojis.txt',\n        sep='\\t', header=None, names=['text', 'emoji'])\ntweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\ntweets['emoji'].value_counts()\n```", "```py\ndef data_generator(tweets, batch_size):\n    while True:\n        batch = tweets.sample(batch_size)\n        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n        y = np.zeros((batch_size,))\n        for row_idx, (_, row) in enumerate(batch.iterrows()):\n            y[row_idx] = emoji_to_idx[row['emoji']]\n            for ch_idx, ch in enumerate(row['text']):\n                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n        yield X, y\n```", "```py\ntrain_tweets, test_tweets = train_test_split(tweets, test_size=0.1)\nBATCH_SIZE = 512\nchar_cnn_model.fit_generator(\n    data_generator(train_tweets, batch_size=BATCH_SIZE),\n    epochs=20,\n    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n    verbose=2\n)\n```", "```py\nchar_cnn_model.evaluate_generator(\n    data_generator(test_tweets, batch_size=BATCH_SIZE),\n    steps=len(test_tweets) / BATCH_SIZE\n)\n```", "```py\n[3.0898117224375405, 0.35545459692028986]\n```", "```py\nlayers = []\nfor window in (4, 5, 6):\n    conv_1x = Conv1D(128, window, activation='relu',\n                     padding='valid')(char_input)\n    max_pool_1x = MaxPooling1D(4)(conv_1x)\n    conv_2x = Conv1D(256, window, activation='relu',\n                     padding='valid')(max_pool_1x)\n    max_pool_2x = MaxPooling1D(4)(conv_2x)\n    layers.append(max_pool_2x)\n\nmerged = Concatenate(axis=1)(layers)\n```", "```py\n    for window in (4, 5, 6):\n        conv_1x = Conv1D(128, window,\n                         activation='relu', padding='valid')(char_input)\n        max_pool_1x = MaxPooling1D(4)(conv_1x)\n        dropout_1x = Dropout(drop_out)(max_pool_1x)\n        conv_2x = Conv1D(256, window,\n                         activation='relu', padding='valid')(dropout_1x)\n        max_pool_2x = MaxPooling1D(4)(conv_2x)\n        dropout_2x = Dropout(drop_out)(max_pool_2x)\n        layers.append(dropout_2x)\n\n    merged = Concatenate(axis=1)(layers)\n\n    dropout = Dropout(drop_out)(merged)\n```", "```py\nVOCAB_SIZE = 50000\ntokenizer = Tokenizer(num_words=VOCAB_SIZE)\ntokenizer.fit_on_texts(tweets['text'])\ntraining_tokens = tokenizer.texts_to_sequences(train_tweets['text'])\ntest_tokens = tokenizer.texts_to_sequences(test_tweets['text'])\nmax_num_tokens = max(len(x) for x in chain(training_tokens, test_tokens))\ntraining_tokens = pad_sequences(training_tokens, maxlen=max_num_tokens)\ntest_tokens = pad_sequences(test_tokens, maxlen=max_num_tokens)\n```", "```py\ndef load_w2v(tokenizer=None):\n    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\n        word2vec_vectors, binary=True)\n\n    total_count = sum(tokenizer.word_counts.values())\n    idf_dict = {k: np.log(total_count/v)\n                for (k,v) in tokenizer.word_counts.items()}\n\n    w2v = np.zeros((tokenizer.num_words, w2v_model.syn0.shape[1]))\n    idf = np.zeros((tokenizer.num_words, 1))\n\n    for k, v in tokenizer.word_index.items():\n        if < tokenizer.num_words and k in w2v_model:\n            w2v[v] = w2v_model[k]\n            idf[v] = idf_dict[k]\n\n    return w2v, idf\n```", "```py\n    message = Input(shape=(max_num_tokens,), dtype='int32', name='title')\n    embedding = Embedding(mask_zero=False, input_dim=vocab_size,\n                          output_dim=embedding_weights.shape[1],\n                          weights=[embedding_weights],\n                          trainable=False,\n                          name='cnn_embedding')(message)\n```", "```py\nclass TokensYielder(object):\n    def __init__(self, tweet_count, stream):\n        self.tweet_count = tweet_count\n        self.stream = stream\n\n    def __iter__(self):\n        print('!')\n        count = self.tweet_count\n        for tweet in self.stream:\n            if tweet.get('lang') != 'en':\n                continue\n            text = tweet['text']\n            text = html.unescape(text)\n            text = RE_WHITESPACE.sub(' ', text)\n            text = RE_URL.sub(' ', text)\n            text = strip_accents(text)\n            text = ''.join(ch for ch in text if ord(ch) < 128)\n            if text.startswith('RT '):\n                text = text[3:]\n            text = text.strip()\n            if text:\n                yield text_to_word_sequence(text)\n                count -= 1\n                if count <= 0:\n                    break\n```", "```py\ntweets = list(TokensYielder(100000,\n              twitter.TwitterStream(auth=auth).statuses.sample()))\n```", "```py\nmodel = gensim.models.Word2Vec(tweets, min_count=2)\n```", "```py\nmodel.wv.most_similar(positive=['love'], topn=5)\n```", "```py\n[('hate', 0.7243724465370178),\n ('loved', 0.7227891087532043),\n ('453', 0.707709789276123),\n ('melanin', 0.7069753408432007),\n ('appreciate', 0.696381688117981)]\n```", "```py\ndef create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n    message = layers.Input(shape=(None,), dtype='int32', name='title')\n    embedding = Embedding(mask_zero=False, input_dim=vocab_size,\n                          output_dim=embedding_weights.shape[1],\n                          weights=[embedding_weights],\n                          trainable=True,\n                          name='lstm_embedding')(message)\n\n    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n    category = layers.Dense(units=len(emojis), activation='softmax')(lstm_1)\n\n    model = Model(\n        inputs=[message],\n        outputs=[category],\n    )\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='rmsprop', metrics=['accuracy'])\n    return model\n```", "```py\ntest_char_vectors, _ = next(data_generator(test_tweets, None))\n```", "```py\npredictions = {\n    label: [emojis[np.argmax(x)] for x in pred]\n    for label, pred in (\n        ('lstm', lstm_model.predict(test_tokens[:100])),\n        ('char_cnn', char_cnn_model.predict(test_char_vectors[:100])),\n        ('cnn', cnn_model.predict(test_tokens[:100])),\n    )\n}\n```", "```py\npd.options.display.max_colwidth = 128\ntest_df = test_tweets[:100].reset_index()\neval_df = pd.DataFrame({\n    'content': test_df['text'],\n    'true': test_df['emoji'],\n    **predictions\n})\neval_df[['content', 'true', 'char_cnn', 'cnn', 'lstm']].head(25)\n```", "```py\ndef prediction_layer(model):\n    layers = [layer for layer in model.layers\n              if layer.name.endswith('_predictions')]\n    return layers[0].output\n\ndef create_ensemble(*models):\n    inputs = [model.input for model in models]\n    predictions = [prediction_layer(model) for model in models]\n    merged = Average()(predictions)\n    model = Model(\n        inputs=inputs,\n        outputs=[merged],\n    )\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    return model\n```", "```py\ndef combined_data_generator(tweets, tokens, batch_size):\n    tweets = tweets.reset_index()\n    while True:\n        batch_idx = random.sample(range(len(tweets)), batch_size)\n        tweet_batch = tweets.iloc[batch_idx]\n        token_batch = tokens[batch_idx]\n        char_vec = np.zeros((batch_size, max_sequence_len, len(chars)))\n        token_vec = np.zeros((batch_size, max_num_tokens))\n        y = np.zeros((batch_size,))\n        it = enumerate(zip(token_batch, tweet_batch.iterrows()))\n        for row_idx, (token_row, (_, tweet_row)) in it:\n            y[row_idx] = emoji_to_idx[tweet_row['emoji']]\n            for ch_idx, ch in enumerate(tweet_row['text']):\n                char_vec[row_idx, ch_idx, char_to_idx[ch]] = 1\n            token_vec[row_idx, :] = token_row\n        yield {'char_cnn_input': char_vec,\n               'cnn_input': token_vec,\n               'lstm_input': token_vec}, y\n```", "```py\nBATCH_SIZE = 512\nensemble.fit_generator(\n    combined_data_generator(train_tweets, training_tokens, BATCH_SIZE),\n    epochs=20,\n    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n    verbose=2,\n    callbacks=[early]\n)\n```"]