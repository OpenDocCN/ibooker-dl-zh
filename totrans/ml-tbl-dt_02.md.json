["```py\nimport pandas as pd\ndata = {'gender': ['male', 'female', 'male', 'female'],\n        'age': [25, 30, 27, 29],\n        'education_level': ['Bachelor', 'Master', 'Bachelor', 'PhD'],\n        'income': [50000, 60000, 55000, 70000]}              ①\nindex = [ 'Bob ', 'Alice', 'Charlie', 'Emily']               ②\ndf = pd.DataFrame(data, index=index)                         ③\nprint(df)\nprint(df.iloc[1])                                            ④\nprint(df.loc['Alice'])                                       ⑤\n```", "```py\n         gender  age education_level  income\nBob        male   25        Bachelor   50000\nAlice    female   30          Master   60000\nCharlie    male   27        Bachelor   55000\nEmily    female   29             PhD   70000\n```", "```py\ngender             female\nage                    30\neducation_level    Master\nincome              60000\nName: Alice, type: object\n```", "```py\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import VarianceThreshold\n\ndata = pd.DataFrame({\"feature_1\":['A' for i in range(15)],\n                     \"feature_2\":['B' if i%2==0 else 'C' \n                                  for i in range(15)],\n                     \"feature_3\":[i**2 for i in range(15)]})\nord_enc = OrdinalEncoder()                                  ①\ndata[data.columns] = ord_enc.fit_transform(data)\nvar_threshold = VarianceThreshold(threshold=0)              ②\n\nclean_data = var_threshold.fit_transform(data)\nprint(var_threshold.variances_)                             ③\nprint(clean_data.shape)\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nnp.random.seed(0)                                                   ①\nX, _ = make_classification(n_redundant=0,\n                           n_repeated=0,\n                           random_state=0)                          ②\nX = np.hstack([X, X[:,:5] + np.random.random((X.shape[0],5))])      ③\n\nvif = [variance_inflation_factor(X, i) for i in range(X.shape[1])]  ④\nprint(np.round(vif,2))\n\nfor a in range(X.shape[1]):                                         ⑤\n    for b in range(X.shape[1]):\n        if a < b:\n            r = np.corrcoef(X[:, a], X[:, b])[0][1]                 ⑥\n            if np.abs(r) > 0.90:                                    ⑦\n                print(f\"feature {a} and {b} have r={r:0.3f}\")\n```", "```py\n[14.98 13.64 11.85 13.12 15.75  1.2   1.41  1.19  1.46  1.31  1.38  1.3  1.19  1.24  1.63  1.28  1.45  1.23  1.15  1.16 18.86 16.79 15.39 16.45 17.82]\n```", "```py\nfeature 0 and 20 have r=0.966\nfeature 1 and 21 have r=0.963\nfeature 2 and 22 have r=0.947\nfeature 3 and 23 have r=0.958\nfeature 4 and 24 have r=0.964\n```", "```py\nSELECT df1.product_id, df1.product_name, df1.price, \ndf2.product_description, df2.category, df3.manufacturer, df3.weight\nFROM df1\nJOIN df2 ON df1.product_id = df2.product_id\nJOIN df3 ON df1.product_id = df3.product_id;\n```", "```py\nimport pandas as pd\ndf1 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n                    'product_name': ['Product A', \n                                     'Product B', \n                                     'Product C', \n                                     'Product D'],\n                    'price': [10.99, 20.99, 15.99, 8.99]})           ①\ndf2 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n                 'product_description': ['A great product', \n                                         'A high-quality product', \n                                         'A reliable product', \n                                         'An affordable product'],\n                 'category': ['Category A', 'Category B', \n                              'Category C', 'Category D']})          ②\ndf3 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n                    'manufacturer': ['Manufacturer A', 'Manufacturer B', \n                                     'Manufacturer C', 'Manufacturer D'],\n                    'weight': [1.5, 2.0, 1.8, 1.2]})                 ③\nmerged_df = pd.merge(df1, df2, on='product_id')                      ④\nmerged_df = pd.merge(merged_df, df3, on='product_id')                ⑤\nprint(merged_df)\n```", "```py\nkaggle datasets download -d uciml/german-credit\n```", "```py\npip install sdv\n```", "```py\nimport pandas as pd\nfrom sdv.metadata import SingleTableMetadata                       ①\nfrom sdv.single_table import TVAESynthesizer                       ②\n\ndata = pd.read_csv(\"./german_credit_data.csv\", index_col=0)        ③\ndata = data.reset_index()\nprint(data.shape)\nprint(data.head)\n\nmetadata = SingleTableMetadata()                                   ④\nmetadata.detect_from_dataframe(data)                               ⑤\nprint(metadata.to_dict())                                          ⑥\nmetadata.update_column(column_name=\"Saving accounts\", \n                       sdtype=\"categorical\")                       ⑦\nsynthesizer = TVAESynthesizer(metadata, epochs=10_000)             ⑧\nsynthesizer.fit(data)                                              ⑨\nsynthetic_data = synthesizer.sample(num_rows=10_000)               ⑩\nsynthetic_data.head()\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_auc_score\n\nX = pd.concat([synthetic_data, data])                               ①\ncategorical_columns = [\n    \"Sex\", \"Housing\", \"Saving accounts\",\n    \"Checking account\", \"Purpose\"]\nX_encoded = pd.get_dummies(X, columns=categorical_columns)          ②\nX= X.drop(\"index\", axis=\"columns\") \ny = [0] * len(synthetic_data) + [1] * len(data)\n\nmodel = RandomForestClassifier()\ncv_preds = cross_val_predict(\n    model,                                                          ③\n    X_encoded,                                                      ③\n    y,                                                              ③\n    cv=5,                                                           ③\n    n_jobs=-1,                                                      ③\n    method=\"predict_proba\"                                          ③\n)                                                                   ③\nroc_adv_score = roc_auc_score(y_true=y, y_score=cv_preds[:, 1])     ④\nprint(f\"roc auc adv score: {roc_adv_score:0.3f}\")\n```", "```py\nfrom io import StringIO                                             ①\nimport requests                                                     ②\nimport pandas as pd\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/\"\ndata = \"auto-mpg.data-original\"\ncolumns = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \n           \"acceleration\", \"model_year\", \"origin\", \"car_name\"]      ③\n\ncolspecs = [(0, 4), (6, 9), (12, 17), (23, 28), (34, 39), \n            (45, 49), (52, 55), (57, 59), (61, -2)]                 ④\n\ndata_ingestion = StringIO\n(requests.get(url + data).text)                                     ⑤\n\ndata = pd.read_fwf(data_ingestion, \ncolspecs=colspecs, \nnames=columns)                                                      ⑥\n```", "```py\n18.0  8\\.  307.0  130.0  3504\\.  12.0  70\\.  1\\.  \"chevrolet chevelle malibu\"\n15.0  8\\.  350.0  165.0  3693\\.  11.5  70\\.  1\\.  \"buick skylark 320\"\n18.0  8\\.  318.0  150.0  3436\\.  11.0  70\\.  1\\.  \"plymouth satellite\"\n…\n31.0  4\\.  119.0  82.00  2720\\.  19.4  82\\.  1\\.  \"chevy s-10\"\n```", "```py\n\"mpg\",\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\n\"model_year\",\"origin\",\"car_name\"\n18.0,8.0,307.0,130.0,3504.0,12.0,70.0,1.,\"chevrolet chevelle malibu\"\n15.0,8.0,350.0,165.0,3693.0,11.5,70.0,1.,\"buick skylark 320\"\n18.0,8.0,318.0,150.0,3436.0,11.0,70.0,1.,\"plymouth satellite\"\n…\n31.0,4.0,119.0,82.0,2720.0,19.4,82.0,1.,\"chevy s-10\"\n```", "```py\nnumeric_feats = [\"mpg\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"]\nordinal_feats = [\"cylinders\", \"model_year\"]\ncategorical_feats = [\"origin\", \"car_name\"]\n```", "```py\ndata.head(5)\n```", "```py\ndata.nunique()\n```", "```py\nmpg             129\ncylinders         5\ndisplacement     84\nhorsepower       92\nweight          356\nacceleration     96\nmodel_year       14\norigin            4\ncar_name        312\ndtype: int64\n```", "```py\ndata[numeric_feats].std()\n```", "```py\nmpg               7.815984\ndisplacement    105.207362\nhorsepower       38.522063\nweight          849.827166\nacceleration      2.820984\ndtype: float64\n```", "```py\n(data.isna()\n     .sum(axis=0)         \n)                   \n```", "```py\nmpg             8\ncylinders       0\ndisplacement    0\nhorsepower      6\nweight          0\nacceleration    0\nmodel_year      0\norigin          0\ncar_name        0\ndtype: int64\n```", "```py\n(data[numeric_feats]==-999).sum(axis=0)\n```", "```py\ndata.describe()\n```", "```py\nwords = (data.car_name\n             .apply(lambda x: x.split()) \n             .explode()\n             .value_counts()  \n        )\nwords.head(15)\n```", "```py\nford          53\nchevrolet     44\nplymouth      32\n(sw)          32\namc           29\ndodge         28\ntoyota        25\ndatsun        23\ncustom        18\nbuick         17\npontiac       16\nvolkswagen    16\nhonda         13\nmercury       11\nbrougham      10\nName: car_name, dtype: int64\n```", "```py\n(words.index\n      .sort_values()\n)[-50:]\n```", "```py\nIndex(['seville', 'sj', 'skyhawk', 'skylark', 'special', 'spirit', 'sport',\n'sportabout', 'squire', 'sst', 'st.', 'stanza', 'starfire', 'starlet',\n'strada', 'subaru', 'suburb', 'sunbird', 'super', 'supreme', 'sx', 'tc',\n'tc3', 'tercel', 'thunderbird', 'torino', 'town', 'toyota', 'toyouta',\n'tr7', 'triumph', 'turbo', 'type', 'v6', 'v8', 'valiant', 'vega',\n'ventura', 'vista', 'vokswagen', 'volare', 'volkswagen', 'volvo', 'vw',\n'wagon', 'woody', 'x1.9', 'xe', 'yorker', 'zephyr'], dtype='object')\n```", "```py\n(data.origin\n     .value_counts()\n)\n```", "```py\n1\\.    253\n3\\.     79\n2\\.     73\n.       1\nName: origin, dtype: int64\n```", "```py\n(data.model_year\n     .value_counts()\n     .reset_index()\n     .rename(columns={'index':'model_year', \n                      'model_year':'counts'})\n     .sort_values(by=\"model_year\")\n)\n```", "```py\nstandardized = ((data[numeric_feats] - data[numeric_feats].mean()) \n                / data[numeric_feats].std())\nstandardized.boxplot(column=numeric_feats, figsize= (12, 4))\n```", "```py\ndata.horsepower.round().hist(bins=64)\n```", "```py\ndata.acceleration.hist(bins=24)\n```", "```py\n(data.cylinders\n     .value_counts()\n     .reset_index()\n     .rename(columns={'index':'counts'})\n     .sort_values(by=\"counts\")\n     .plot.bar(x=\"counts\")\n)\n```", "```py\nimport seaborn as sns\ncorr = data[numeric_feats].corr()\nsns.heatmap(corr, cmap=\"Blues\",annot=True)\n```", "```py\nfrom scipy.stats import chi2_contingency                       ①\n\ndef cramerV(chi2, table):                                      ②\n\n    n = table.values.sum()                                     ③\n    minimum_dimension = min(table.shape)-1                     ④\n\n    result = ((chi2 / n) / minimum_dimension)**0.5             ⑤\n    return result\n\ndeciles = pd.qcut(data.mpg, q=10, labels=False)                ⑥\n\ntable = pd.crosstab(data.model_year, deciles)                  ⑦\n\nchi2, p, dof, expected = chi2_contingency(observed=table)      ⑧\nprint(cramerV(chi2, table))                                    ⑨\n```", "```py\nfrom sklearn.manifold import TSNE                           ①\nimport matplotlib.pyplot as plt                             ②\n\ntsne = TSNE(n_components=2, \n            perplexity=30., \n            init=\"random\", \n            learning_rate=\"auto\",\n            random_state=42)                                ③\n\nX = data[numeric_feats + ordinal_feats].fillna(\n            data[numeric_feats + ordinal_feats].mean())     ④\nprojection_2D = tsne.fit_transform(X)                       ⑤\n\nplt.figure(figsize=(15, 15))\nplt.scatter(projection_2D[:, 0], projection_2D[:, 1],\n            edgecolor='none', \n            alpha=0.80, \n            s=10) \nplt.show()                                                  ⑥\n```"]