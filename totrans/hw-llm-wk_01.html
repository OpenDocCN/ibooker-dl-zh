<html><head></head><body>

  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__tokenization"> <span class="chapter-title-numbering"><span class="num-string">2</span></span> <span class="title-text"> Tokenizers: How large language models see the world</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Creating tokens from sentences</li> 
    <li class="readable-text" id="p3"> Controlling vocabulary size with normalization</li> 
    <li class="readable-text" id="p4">Avoiding risks in tokenization</li> 
    <li class="readable-text" id="p5"> Tokenization strategies to remove ambiguity</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>As discussed in chapter 1, in the world of artificial intelligence, it is often helpful to find analogies to human learning to explain how machines “learn.” How you read and understand sentences is a complex process that changes as you get older and involves multiple sequential and concurrent cognitive processes [1]. Large language models (LLMs), however, use simpler processes than human cognitive processes. They employ algorithms based on neural networks to capture the relationships between words in large amounts of data and then use this information about relationships to interpret and generate sentences.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Our discussion of how these algorithms work will begin with their input: sentences of text. In this chapter, we explore how the LLM processes these sentences to become inputs for the model. Just as language is critical for how you think and process information, the inputs to an LLM are crucial in influencing what kinds of concepts and tasks LLMs can perform.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class=" readable-text-h2" id="tokens-as-numeric-representations"><span class="num-string browsable-reference-id">2.1</span> Tokens as numeric representations</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>It may seem obvious that LLMs should process sentences, but to fully understand, we must be more specific. As we talk about how LLMs work, you will see that textual sentences are unnatural for the neural network algorithms that power LLMs because neural networks fundamentally employ numbers to do their work. As shown in figure <a href="#fig__tokenizationExample">2.1</a>, the algorithms employed by LLMs must convert human text into a numeric representation before working with it. <em>Tokens</em> are the representations that LLMs use to break text into pieces that can be encoded as numbers.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p10">  
   <img alt="figure" src="../Images/CH02_F01_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__tokenizationExample"><span class="num-string">Figure <span class="browsable-reference-id">2.1</span></span> To understand text, LLMs must break text into tokens. Each unique token has a numeric identifier associated with it.</h5>
  </div> 
  <div class="readable-text" id="p11"> 
   <p>You can think of tokens as the smallest unit of text an LLM processes—an “atom,” if you will, the smallest part from which all other things are built. So what are the atoms of text? Consider this: As you read this book, what are the smallest building blocks that your brain uses to process meaning? Two natural answers are <em>letters</em> and <em>words</em>. It is very tempting to define letters as the atom since words are made of letters, but do you consciously read every letter in every word? For most people, the answer is “no.” (If you are dyslexic like one of the co-authors of this book, this is a bizarre question. But cognitive processing is complex and not fully understood; please bear with us on the analogies!) You look at the more prominent words and word parts. In fbct, yoy cn probbly unrestand ths sentnce ever through we diddt sue th ryght cpellng or l3ttrs. People unconsciously use parts of words to process text, and LLMs are built using the same principle. </p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>In this chapter, you will learn how the process of converting text to tokens works. First, we will discuss tokens in more detail; then, we will discuss the procedures used to decide how sentences are turned into tokens.</p> 
  </div> 
  <div class="readable-text" id="p13"> 
   <h2 class=" readable-text-h2" id="language-models-see-only-tokens"><span class="num-string browsable-reference-id">2.2</span> Language models see only tokens</h2> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>By adulthood, most English-speaking people know around 30,000 words [2]. GPT-3, the LLM that initially powered ChatGPT, has a vocabulary of 50,257 tokens [3]. These tokens are not words but parts of words referred to as <em>subwords</em>, a representation that is somewhere between words and letters. Intuitively, a token captures language’s <em>minimum meaningful semantic unit</em>. For example, the word <code>schoolhouse</code> will often get broken into two tokens, <code>school</code> and <code>house</code>, and the word <code>thoughtful</code> as <code>thought</code> and <code>ful</code>. This is useful for recognizing frequent words and having the subwords to interpret new words we have never seen before. People often use a similar technique, called semantic decomposition, to understand words they’ve never seen before. We intuitively break new words into constituent parts to grasp their meaning based on words we already understand. </p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p><em>Feature engineering</em> is the process of converting your data to a form that is more convenient to your algorithm and the task you want to solve. To build an algorithm that can detect the language of a given text, you could write code that takes text as input and outputs the percentage of times each character occurs. For example, if <code>é</code> appears a lot in a document, you have a good feature to indicate that the document is more likely to be Spanish or French than Russian or Chinese. Sound feature engineering is concerned with thinking through how your model works, what you want to achieve, and how to prepare your data for the combination of model and goal. </p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Tokenization is the feature engineering of LLMs; it is critically essential because tokens are the only information a model interacts with. Tokens are seen as individual, abstract <em>things</em> that are not inherently connected. The relationships are learned through observation of data.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Looking back at figure <a href="#fig__tokenizationExample">2.1</a>, it is evident that the tokens for <code>Dis</code> and <code>dis</code> are related, the only difference being that one starts with a capital <code>D</code>. However, you can seethat the model assigns the identifier <span><img alt="equation image" src="../Images/eq-chapter-2-17-1.png"/></span> to <code>Dis</code> and the identifier <span><img alt="equation image" src="../Images/eq-chapter-2-17-2.png"/></span> to <code>dis</code>. That is, the model doesn’t inherently see any connection between the tokens representing <code>Dis</code> and <code>dis</code>, even if we, as humans, see an obvious connection. The model doesn’t even <em>see</em> <code>Dis</code> or <code>dis</code>. For an LLM to process tokens, we must convert those tokens into numbers so that the model will see the numbers <span><img alt="equation image" src="../Images/eq-chapter-2-17-3.png"/></span> and <span><img alt="equation image" src="../Images/eq-chapter-2-17-4.png"/></span>. Importantly, the model doesn’t have any direct way to know that these tokens are related. </p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>A token is a mapping from a subword to a unique numeric representation. In turn, <em>tokenization</em> is the process of converting a full-text string into a sequence of tokens. If you have used machine learning libraries before (especially any natural language processing [NLP] tools), you are probably familiar with some of the simpler forms of tokenization. For example, a simple tokenization process breaks a text into tokens by splitting a text based on spaces. However, this approach limits our abilities to create subwords or process languages that don’t use whitespace to delimit words, such as Chinese.</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h3 class=" readable-text-h3" id="the-tokenization-process"><span class="num-string browsable-reference-id">2.2.1</span> The tokenization process</h3> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>The generic process that tokenization follows is shown in figure <a href="#fig__tokenizationProcess">2.2</a> with four key steps:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p21"><em>Receiving the text to process</em>—This means obtaining text input as a <code>string</code> data type (a collection of letters, digits, or symbols) from a user, the internet, or whatever source that has the text you want.</li> 
   <li class="readable-text" id="p22"><em>Transforming the string</em>—This often involves changing the string in some useful way, such as converting uppercase characters into lowercase. This could also be done for security reasons (e.g., the text came from a user, and we need to remove anything that might look like some malicious input) or to eliminate irrelevant variations in the text to help the algorithm learn better. This process is known as <em>normalization</em>. </li> 
   <li class="readable-text" id="p23"><em>Breaking the string into tokens</em>—Once a string is available, it needs to be separated into a sequence of discrete substrings; these are the tokens found in the larger string. This is referred to as <em>segmentation</em>.</li> 
   <li class="readable-text" id="p24"><em>Mapping each token to a unique identifier</em>—The unique identifier is usually an integer number, which produces output that the LLM can understand.</li> 
  </ol> 
  <div class="browsable-container figure-container" id="p25">  
   <img alt="figure" src="../Images/CH02_F02_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__tokenizationProcess"><span class="num-string">Figure <span class="browsable-reference-id">2.2</span></span> Generically, tokenization involves processing input to produce numeric identifiers for tokens.</h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <p>The first and last parts of this process have little room for choice or different behavior. First, you need input to process; last, you need a numeric identifier for each token to store and retrieve the information you will associate with that token. The two middle steps, normalization and segmentation, are where you can choose what happens.</p> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>The last step of the tokenization process is where the vocabulary is built. The <em>vocabulary</em> of a model is the total number of unique tokens that are seen during training when we give the algorithm data to learn from. It almost always takes a large amount of data to build a rich vocabulary with many unique tokens.</p> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Choosing the vocabulary for a model involves a series of trade-offs: the larger the vocabulary, the more information your model can process successfully. Consider a one-year-old child with a vocabulary of maybe a few dozen words. This child will not be a very effective communicator (but that’s okay; they have lots of time to learn). So a more extensive vocabulary not only helps the model understand more things, but it also makes the model larger. If you have a vocabulary that’s too large, you may make the model slower due to the number of computations required to use it, or the model may consume an excessive amount of memory or disk storage, which makes it more difficult to transfer or share to other machines—for example, when deploying it as a part of a software application.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>You build the model’s vocabulary by processing the training data and identifying tokens. Each time you see a new token, you give it a unique identifier based on the number of unique tokens you’ve seen. This process is often as simple as storing a counter set to <code>0</code> and incrementing it every time a new token is found. Once the process is complete, you have a tokenizer that is effectively an <em>encoder</em>. The tokenizer can receive text as input and return a numeric encoding of that text that the LLM algorithms can use as its output.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <h3 class=" readable-text-h3" id="controlling-vocabulary-size-in-tokenization"><span class="num-string browsable-reference-id">2.2.2</span> Controlling vocabulary size in tokenization</h3> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>GPT-NeoX, a publicly available LLM, takes about 10 GB to store its vocabulary on disk. That is a lot of data, already large enough to make many real-world use cases challenging from the perspective of data storage and computation. It is so large that storing it on a micro-SD card would be prohibitively slow, making use on a mobile phone or some game consoles a significant challenge. It is big enough that it can’t be streamed in real time and must be downloaded and loaded into the processor’s RAM to perform tokenization. However, a vocabulary must be sufficiently large to represent all words and subwords the model will encounter during training and use. Suppose a model encounters a word that is not in its vocabulary and cannot be represented by combining subwords in its vocabulary. In that case, the model cannot capture information about that piece of text. As a result, it is essential to weigh concerns about vocabulary size against the need for models to interpret a wide variety of content. In NLP, this is often called the out-of-vocabulary problem, when we encounter words we can’t represent using the tokens available to the model.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>Vocabulary size is one factor contributing to an LLM’s size, so discussing methods and tradeoffs for controlling vocabulary size is vital. In this section, we will describe how changing the tokenization process’s behavior can influence vocabulary size and affect model capabilities and accuracy.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p33">  
   <img alt="figure" src="../Images/CH02_F03_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__normalization"><span class="num-string">Figure <span class="browsable-reference-id">2.3</span></span> The normalization process commonly involves changing text to remove uppercase characters and punctuation.</h5>
  </div> 
  <div class="readable-text" id="p34"> 
   <p>In figure <a href="#fig__normalization">2.3</a>, we focus on the second transformation step, normalization, which converts the uppercase characters “H” and “W” to lowercase and removes punctuation. These common normalization steps originate from classical NLP pipelines and are still sometimes done in modern deep learning approaches today. They have the immediately desirable effect of reducing the size of the vocabulary. Instead of needing to represent “Hello” and “hello” as two separate tokens, they get mapped to one unique token. This mapping makes an enormous difference because every word that starts a sentence and gets capitalized would potentially duplicate a word in the vocabulary with a capitalized version. Such normalization can also help with various typos and misspellings.</p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>For example, while writing this book, we typed “LLMs,” “LLms,” and “llms,” and made various other mixed-case typos. Converting each character to lowercase in each variation resolves all these typos into a single, simple form, so we get a smaller vocabulary and decrease ambiguity.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>However, converting text to lowercase doesn’t always decrease ambiguity. Consider “Bill” and “bill.” In the first situation, capitalization is vital for understanding that “Bill” is probably someone’s name, and “bill” is more likely a unit of money (or one of the other definitions of “bill”). Capitalization is crucial not only for understanding the meaning of the text but also for understanding the errors in the text. Consider again all the various ways we miscapitalized “LLMs” in this book. A high-quality AI algorithm would be able to recognize that we made a typo and correct it! ChatGPT is capable of this and thus requires capitalization in the model. So there is an important tradeoff between vocabulary size and potential model accuracy to consider.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>In classical NLP and even not-that-old deep learning models like BERT (a predecessor to the LLMs that power ChatGPT), the ability of an algorithm to recognize typos and fix them was extremely limited outside of solutions designed explicitly for that purpose. For this reason, much of the work that used to go into engineering a robust normalization step has been discarded for LLMs today. A more extensive vocabulary is desirable to produce more capable models that can learn to understand mistakes.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h3 class=" readable-text-h3" id="tokenization-in-detail"><span class="num-string browsable-reference-id">2.2.3</span> Tokenization in detail</h3> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>The normalization and segmentation steps in the tokenization process largelydetermine the vocabulary size. In figure <a href="#fig__segmentation">2.4</a>, we show one of the most straightforward strategies for tokenization. This strategy follows a simple rule: any time a space is seen in the text, split the larger string into those tokens. In the case of “hello world,” it is as easy as calling <code>hello world.split( )</code> in Python. This is a reasonable approach to take; it is how we, as humans, read sentences. But it also adds some subtle complexity.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p40">  
   <img alt="figure" src="../Images/CH02_F04_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__segmentation"><span class="num-string">Figure <span class="browsable-reference-id">2.4</span></span> The segmentation process breaks normalized text into words or tokens so that each can be processed independently.</h5>
  </div> 
  <div class="readable-text" id="p41"> 
   <p>What happens when you have punctuation in your text? If we use our white space rule to convert the string “hello, world” into <code>[hello,, world]</code>, we run into a similar problem as we do with capitalization. We end up with two distinct tokens for the same concept: <code>hello</code> and <code>hello,</code>. The old-school approach often addressed this by removing and developing more complex rules for splitting strings into tokens. While this is a step in the right direction toward reducing vocabulary size, manually specifying tokenization rules does not address other concerns. For example, rule-based tokenization strategies are a significant struggle for languages like Chinese that do not use spaces to separate words. </p> 
  </div> 
  <div class="readable-text" id="p42"> 
   <h4 class=" readable-text-h4" id="identifying-subwords-with-byte-pair-encoding"> Identifying subwords with byte-pair encoding</h4> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>The general theme of LLMs is to do less feature engineering by hand and let algorithms do the heavy lifting instead. For this reason, an algorithm known as <em>byte pair encoding</em> (BPE) is typically used to break strings into tokens. Byte pair encoding is an algorithm for breaking words into common subword sequences of characters. BPE today is usually done with a custom segmenter and almost no normalization.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p44"> 
   <p> <span class="print-book-callout-head">Note</span> By experimentation, we see many ChatGPT-like products will remove some Unicode characters that do not print (Unicode is weird), but otherwise mostly take your text as-is. Most prior language models do use various flavors of normalization, and how to normalize text for LLMs better is, we think, a good and open question. </p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>Since finding the most efficient set of subwords is a computationally expensive task, BPE uses a heuristic to take a shortcut. It starts by looking at individual letters as tokens and then finds pairs of adjacent letters that occur most frequently and combines them into subword tokens. The algorithm repeats this process many times, continuing with subword tokens, until some threshold is met and the vocabulary is “small enough.” For example, in the first pass, the BPE algorithm examines the frequency of the individual letters used in English and encounters the letters “i,” “n,” and “g” near each other frequently. In the first pass, BPE might observe that “n” and “g” occur together more frequently than “i” and “n,” so it will produce the tokens <code>i</code> and <code>ng</code>. In a subsequent pass, it may combine those tokens into <code>ing</code> based on the frequency of that combination of letters versus how often “ng” occurs with other letters or subwords. Once BPE has reached its stopping point, it will have identified individual words such as “eating” and “drinking” as frequently occurring combinations. It may also capture “ing” as a suffix so that other words ending with that subword can also be represented as tokens. When the algorithm is complete, we end up with tokens that capture complete words and others that capture subwords. This process is shown at a high level in figure <a href="#fig__bpe">2.5</a>. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p46">  
   <img alt="figure" src="../Images/CH02_F05_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__bpe"><span class="num-string">Figure <span class="browsable-reference-id">2.5</span></span> A simplified byte pair encoding algorithm for creating tokens: first, find the most frequent pair of characters “ng.” Next, replace all instances of “ng” with a placeholder token “T,” and add “ng” to the vocabulary. Repeat the process until no common byte pairs remain.</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p47"> 
   <p> <span class="print-book-callout-head">Note</span> Running the BPE algorithm to create a vocabulary is surprisinglyexpensive because it must read the input data many times to calculate the most frequent combinations of letters. While LLMs are trained on over 500 million or even 1 billion pages of text, their tokenizers are usually created using a tiny subset of that data. Often, a tokenizer is trained using a much smaller collection of text the size of a novel. </p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>The BPE process may seem odd at first, but you can think of it as a way of identifying common strings in a corpus. For example, BPE will almost always learn to represent <code>New York</code> as one token, which is useful since the state and city of New York are frequent occurrences in the text. Representing the whole concept as a single token makes it easier to use that kind of information. Indeed, most common words will become unique tokens, while rare words are hopefully captured as a combination of subwords. For example, <code>loquacious</code> will be tokenized by GPT-4 as <code>lo</code>, <code>qu</code>, and <code>acious</code>. This method is a success because “acious” is a Latin postfix for inclination/propensity, making it easier for the model to handle an unusual word correctly. It is also a failure case because the Latin prefix “<code>loqu</code>” got broken up into two tokens instead of one, making learning harder. </p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>After BPE is used to make a vocabulary, model authors manually add additional tokens for various reasons, such as words that are important to a specific knowledge domain. As we will discuss in the next section, in some domains, having the correct tokens has a significant effect by capturing nuanced meaning. So often, the authors will make sure the necessary tokens are included. Model authors will also add special tokens that don’t directly represent word parts but provide auxiliary information to the model. Some common examples of this are the “unknown” token (typically represented as <code>[UNK]</code>), which is used if the tokenizer fails to process a symbol correctly, and the system token <code>[SYSTM]</code>, which is used to distinguish between a model’s built-in prompt and user-entered data, as well as other kinds of stylistic markers. Multimodal models that accept text and image inputs use unique tokens to tell the model when the input stream switches between bytes that represent text data and bytes that represent image data. </p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Open AI decided to use BPE to encode text into tokens when they developed ChatGPT and have released their tokenizer as the open source package <code>tiktoken</code> (<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>). Still, several other algorithms and implementations for automatically generating tokens are available, including the WordPiece and SentencePiece algorithms developed at Google [4]. Each of these have different tradeoffs. For example, WordPiece uses a different technique for counting the frequency of the candidate subwords when building the tokenizer’s vocabulary. One of the algorithms implemented in SentencePiece processes entire sentences, preserving white space when calculating tokens, which may improve output when building models that handle multiple languages. However, BPE is the most broadly used algorithm. For example, it is now used exclusively in Google’s recent LLMs.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Regardless of the algorithm chosen, the size of a tokenizer’s vocabulary is a critical model parameter determined by the data scientist or engineer in charge of training and augmenting the tokenizer. The following sections dive deep into some of the considerations on vocabulary size and other decisions made throughout the tokenizer development process.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class=" readable-text-h3" id="the-risks-of-tokenization"><span class="num-string browsable-reference-id">2.2.4</span> The risks of tokenization</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>As mentioned in chapter 1, we won’t go much into coding in this book. The goal is to give you a reasonable understanding of how LLMs work and remove some of the magic and mystery so you can focus instead on how LLMs may be used for your job. Tokenization is the first piece of the puzzle. It is a simple but effective strategy to produce the inputs to LLMs. You have learned how the size of the vocabulary plays a significant role in a model’s deployability, the tradeoff in recognizing nuance versus the unnecessary redundancy associated with making a vocabulary, how the tokenization process influences the size of the vocabulary, and how the token selection process can be automated with BPE.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>The choices made at tokenization time affect what LLMs can do today and will affect them in the future. These choices involve a few big-picture challenges to be aware of. To explore this topic further, two salient yet nuanced details of BPE are worth sharing some concerns about: the relationship between sentence length and token counts and the potential for LLMs to be confused by characters, known as homoglyphs, that appear identical yet have different binary encodings.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <h4 class=" readable-text-h4" id="longer-sentences-do-not-mean-more-tokens"> Longer sentences do not mean more tokens</h4> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>An unintuitive aspect of BPE is that longer sentences do not mean more tokens. To see why, look at figure <a href="#fig__whatIsTokenization">2.6</a>, where we show a real tokenization of two different strings by GPT-3. The string “I’m running” is longer by one character than the string “I’m runnin,” but it is one token shorter! If you don’t believe it, you can try tokenizing different strings at <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p57">  
   <img alt="figure" src="../Images/CH02_F06_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__whatIsTokenization"><span class="num-string">Figure <span class="browsable-reference-id">2.6</span></span> Tokenizing two different sentences</h5>
  </div> 
  <div class="readable-text" id="p58"> 
   <p>This discrepancy occurs because BPE is greedily looking for the smallest set of tokens for any piece of input. In this specific case, the string “running” occurs frequently enough in our training data that it gets its own token. In the case where the “g” is missing, there is no token for “runnin” in our vocabulary because that variation may have appeared rarely in our training data. Thus, “runnin” needs to be broken into at least two tokens, giving us <code>run</code> and <code>nin</code>. </p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>This nuance of tokenizer implementation is fertile ground for software bugs. Different tokenizers may provide different answers on how to tokenize the same string. When designing unit tests and infrastructure, this factor is important to keep in mind to avoid getting lost or confused when upgrading or converting between tokenizer implementations that may cause new differences in token generation. It can also affect evaluations of LLMs, as many models are highly sensitive to added white space, and inconsistent tokenization may inadvertently lead to comparisons not being apples to apples.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <h4 class=" readable-text-h4" id="homoglyphs-create-confusion"> Homoglyphs create confusion</h4> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Homoglyphs are a problem developers may encounter when working with multiple human languages or considering the security implications of processing externally provided data. When input comes from arbitrary users, sometimes it may be nefarious and want to trick your model into bad behavior. One way that could be done against an LLM is with a <em>homoglyph</em> attack. </p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>A homoglyph is when two or more characters have different byte encodings but appear identical when rendered on the screen. One example is the Latin letter “H” used in most Western European languages and the Cyrillic “H” used throughout Eastern Europe and Central Asia.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>BPE will encode homoglyphs that use different byte encodings into different tokens. As a result, homoglyphs can inflate the number of tokens in a text, change how an LLM parses the information, and run up your compute costs. An amusing example of a homoglyph is the Unicode character U+200B, also known as the “zero width space.” This character is used in typesetting and takes up space, but it does not print anything, show anything, or change anything about how a document is rendered.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>The zero width space is one of many strange and interesting things that exist within the Unicode specification and could be used to cause you pain. Many services thus employ normalization steps that remove such strange characters and replace homoglyphs with a canonical representation (i.e., anything that looks like an “a” must be encoded as an <code>a</code>). For example, OpenAI’s current tokenizer interface will remove homoglyphs. You must consider homoglyphs if you want to deploy an LLM on your hardware or a user’s device.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h2 class=" readable-text-h2" id="tokenization-and-llm-capabilities"><span class="num-string browsable-reference-id">2.3</span> Tokenization and LLM capabilities</h2> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>If we are only concerned with the ability of an LLM to produce high-quality human-like text, the specific details of how you tokenize your text do not matter as much as the data and compute used to build these models. If you put enough computational power and scale into your models, they will eventually figure out useful representations regardless of the building blocks. But sometimes, tokenization dramatically affects what an LLM is capable of. In this section, we cover some examples.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>It may be the case that the examples that follow are not directly relevant to your job or what you would like to do with an LLM. That is perfectly fine; the point of these examples is not to dissuade you from using an LLM. Instead, the goal is to help you understand that the scope of what LLMs learn is limited by the representation chosen, and there may not be a way around these concerns without major engineering work. If you start building an application with LLMs and find significant difficulty, think about how tokenization could be a factor in your goal. If tokenization is indeed the problem, there is little you can do to solve it, so it may be best to look at other approaches, such as manually augmenting the vocabulary with tokens that are important for your application.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h3 class=" readable-text-h3" id="llms-are-bad-at-word-games"><span class="num-string browsable-reference-id">2.3.1</span> LLMs are bad at word games</h3> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>Users frequently enjoy asking LLMs to solve word puzzles or perform tasks that involve word games. For example, figure <a href="#fig__badAtWordGames">2.7</a> shows a word game where the correct answer depends on the exact letter sequence and the number of letters in a word.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p70">  
   <img alt="figure" src="../Images/CH02_F07_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__badAtWordGames"><span class="num-string">Figure <span class="browsable-reference-id">2.7</span></span> The tokenization approach means that ChatGPT cannot really “see” single characters or word lengths. If you ask questions that require subcharacter identification and change them in a unique and unusual way, ChatGPT starts to fail. The correct middle character is “a,” but ChatGPT insists that the letter is “e.” What ChatGPT sees is three tokens, representing <code>P</code>, <code>ine</code>, and <code>apple</code>, respectively. </h5>
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Playing word games may not be something you care about for your application, but the reason word games fail may be highly salient to your problem. Although many examples like this are toy problems in that they aren’t particularly scientifically or commercially important, they reveal notable breakdowns in how these models operate. They may come into play in more practical uses, such as when models struggle to write poetry containing rhymes or assonance.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Consider, for example, that you want to build an application that answers questions about a user’s prescription drugs. Drugs often have longer, confusing names that people fail to remember or spell incorrectly, and because an LLM does not understand letters, it may confuse one drug’s name with a different drug’s long and strange name.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Because drug names are uncommon, they will tokenize differently, even with minor misspellings. For example, in GPT-3, “Amoxicillin” and the easy misspelling “Amoxicillan” share no common tokens! This creates a much greater risk of the LLM responding incorrectly, where the risk is intrinsically higher, making an LLM application all the more important to thoroughly test, engineer around with extreme care, or potentially avoid altogether.</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h3 class=" readable-text-h3" id="llms-are-challenged-by-mathematics"><span class="num-string browsable-reference-id">2.3.2</span> LLMs are challenged by mathematics</h3> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>Tokenization significantly affects tasks involving formal symbolic reasoning, including mathematics and playing board games. Both math and board games are implemented by LLMs as symbolic reasoning problems where individual tokens have specific rules governing their interactions and meaning when observed in conjunction with other tokens. For example, models containing individual tokens for each digit tend to perform better at arithmetic than models that don’t. This is because the number 123456 will become two tokens in GPT-3, <code>[, ]</code>, based on the frequency of those tokens in the tokenizer’s original training data. This makes it harder for the model to deal with the individual digits in that number. Some system developers have solved this problem by normalizing numbers by inserting spaces between all digits, such as 1 2 3 4 5 6, which creates a new output with six tokens, one for each digit.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>This difference in math capability is well-illustrated in figure <a href="#fig__math-eval">2.8</a>, which shows performance on arithmetic computations throughout training. The top curve is a typical BPE tokenizer, while the bottom curve, which shows better performance, is the same tokenizer modified to have digit-level tokenization of numbers.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p77">  
   <img alt="figure" src="../Images/CH02_F08_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__math-eval"><span class="num-string">Figure <span class="browsable-reference-id">2.8</span></span> A comparison of how two LLMs learn to perform arithmetic computations over time. Time is shown on the x-axis. The upper curve is a typical BPE tokenizer, while the lower curve is the same tokenizer modified to use tokens that represent individual digits. The y-axis describes the ability of the LLM to perform accurately, where a smaller number means fewer errors. The bottom line is that LLMs that use digit-level tokenization can learn how to do math better and faster.</h5>
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3" id="llms-and-language-equity"><span class="num-string browsable-reference-id">2.3.3</span> LLMs and language equity</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Most LLM tokenizers can represent any symbol covered by Unicode, which includes the characters from most of the world’s alphabets. However, how efficiently those tokenizers represent text in a given language varies massively, especially as thetokenizers are typically trained on smaller collections of text resources for different languages. This can cause substantial inequity in commercial services based on LLMs [5] because tokenization of words in languages that are rare in the training set defaults to a more granular set of subwords, resulting in increased token usage. Commercial LLM providers like OpenAI and Anthropic typically charge customers on a per-token basis, usually a fraction of a cent for every token input into the LLM and produced as output by the LLM. These costs add up when you consider that a high-use commercial application may process tens of millions of tokens daily.</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>The time it takes for an LLM to complete a request and the amount a user is charged per token depends directly on the tokenizer. Therefore, languages that are more efficiently represented using a tokenizer are economically incentivized over those that are not represented efficiently. Using English as a baseline, researchers have found that the cost to answer a user query in German or Italian is about 50% more when using ChatGPT and GPT-4. Languages that differ even more substantially from English can incur much larger charges: Tumbuka and Bulgarian are more than twice the cost, and Dzongkha, Odia, Santali, and Shan cost over 12 times as much as English to process.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <h2 class=" readable-text-h2" id="check-your-understanding"><span class="num-string browsable-reference-id">2.4</span> Check your understanding</h2> 
  </div> 
  <ol> 
   <li class="readable-text" id="p82">How would you expect the following words or phrases to be tokenized? Try breaking them out yourself and then running them through an actual LLM tokenizer, such as the one at <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>: 
    <ul> 
     <li class="readable-text">backstopped</li> 
     <li class="readable-text">large language models</li> 
     <li class="readable-text">Schoolhouse</li> 
     <li class="readable-text">How you process sentences to understand them is a complex process that changes as you get older and involves multiple sequential and concurrent cognitive processes</li> 
    </ul> </li> 
   <li class="readable-text" id="p83">How much do you think uppercase versus lowercase letters matter for each of the previous examples? Try submitting them again with various casings.</li> 
   <li class="readable-text" id="p84">Let’s simulate how LLMs think about math using a cipher where each English letter corresponds to a number. For example, <span><img alt="equation image" src="../Images/eq-chapter-2-84-1.png"/></span>, <span><img alt="equation image" src="../Images/eq-chapter-2-84-2.png"/></span>, <span><img alt="equation image" src="../Images/eq-chapter-2-84-3.png"/></span>, and <span><img alt="equation image" src="../Images/eq-chapter-2-84-4.png"/></span>, so we would write <em>WAIT</em> to mean <span><img alt="equation image" src="../Images/eq-chapter-2-84-5.png"/></span>. Knowing this fact and that <span><img alt="equation image" src="../Images/eq-chapter-2-84-6.png"/></span>, can you figure out what <span><img alt="equation image" src="../Images/eq-chapter-2-84-7.png"/></span> represents?</li> 
   <li class="readable-text" id="p85">Since a token is the basic unit an LLM operates on, why does it make sense (technologically) that languages less efficiently represented by a tokenizer would cost more?</li> 
   <li class="readable-text" id="p86">Is it an ethical problem that LLMs charge different amounts to people for the same service based on what language they speak? Would you consider this discrimination?</li> 
  </ol> 
  <div class="readable-text" id="p87"> 
   <h2 class=" readable-text-h2" id="tokenization-in-context"><span class="num-string browsable-reference-id">2.5</span> Tokenization in context</h2> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>The details of tokenization we discuss in this chapter are the foundational building blocks of LLMs that govern the input they can represent effectively and the output they produce. Tokenization is a critical component of LLMs like ChatGPT in develop-ing effective representations of text so that they can be used to learn relationships between tokens when presented with vast amounts of information in the training process, interpreting user input and producing the high-quality responses we’ve become accustomed to. An LLM’s potential is limited or enabled by the tokenization strategy and vocabulary it employs, in conjunction with all of the other characteristics we explore in the following chapters.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p90">Tokenization is the fundamental process that LLMs use to understand text by converting sentences into tokens.</li> 
   <li class="readable-text" id="p91">Tokens are the smallest units of information in text that represent content. Sometimes, they correspond to full words, but often, they represent pieces of words or sub-words.</li> 
   <li class="readable-text" id="p92">Tokenization involves <em>normalizing</em> text into a standard representation, which may involve converting characters to lowercase or translating the byte encoding of Unicode characters so that visibly identical characters employ the same encoding.</li> 
   <li class="readable-text" id="p93">Tokenization also involves <em>segmentation</em>, which is breaking up text into words or subwords. Algorithms like byte pair encoding (BPE) provide a mechanism to automatically learn how to efficiently segment text based on the statistical occurrence of combinations of letters in a training data set.</li> 
   <li class="readable-text" id="p94">The result of building a tokenizer is known as a <em>vocabulary</em>, which is the unique collection of word and subword tokens that a tokenizer can use to represent text it has processed. </li> 
   <li class="readable-text" id="p95">The size of a tokenizer’s vocabulary affects the LLM’s ability to accurately represent data and the storage and computational resources required to understand and predict text.</li> 
   <li class="readable-text" id="p96">Internally to the LLM, tokens are represented using numbers. As a result, there is no understanding of relationships between tokens, such as prefixes and suffixes, or the fact that two tokens share a similar set of letters.</li> 
   <li class="readable-text" id="p97">To support specific domains of knowledge, tokenizers trained automatically may be augmented to provide tokens that are important to their application.</li> 
   <li class="readable-text" id="p98">Tokenizers that do not understand individual letters or digits will have problems with arithmetic operations or simple word games.</li> 
  </ul>
 </body></html>