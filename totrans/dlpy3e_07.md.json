["```py\nimport keras\nfrom keras import layers\n\nmodel = keras.Sequential(\n    [\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ]\n) \n```", "```py\nmodel = keras.Sequential()\nmodel.add(layers.Dense(64, activation=\"relu\"))\nmodel.add(layers.Dense(10, activation=\"softmax\")) \n```", "```py\n>>> # At that point, the model isn't built yet.\n>>> model.weights\n[]\n```", "```py\n>>> # Builds the model. Now the model will expect samples of shape\n>>> # (3,). The None in the input shape signals that the batch size\n>>> # could be anything.\n>>> model.build(input_shape=(None, 3))\n>>> # Now you can retrieve the model's weights.\n>>> model.weights\n[<Variable shape=(3, 64), dtype=float32, path=sequential/dense_2/kernel ...>,\n <Variable shape=(64,), dtype=float32, path=sequential/dense_2/bias ...>,\n <Variable shape=(64, 10), dtype=float32, path=sequential/dense_3/kernel ...>,\n <Variable shape=(10,), dtype=float32, path=sequential/dense_3/bias ...>>]\n```", "```py\n>>> model.summary()\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_2 (Dense)                   │ (None, 64)               │           256 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ (None, 10)               │           650 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 906 (3.54 KB)\n Trainable params: 906 (3.54 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\n>>> model = keras.Sequential(name=\"my_example_model\")\n>>> model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n>>> model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n>>> model.build((None, 3))\n>>> model.summary()\nModel: \"my_example_model\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ my_first_layer (Dense)            │ (None, 64)               │           256 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ my_last_layer (Dense)             │ (None, 10)               │           650 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 906 (3.54 KB)\n Trainable params: 906 (3.54 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nmodel = keras.Sequential()\n# Use an Input to declare the shape of the inputs. Note that the shape\n# argument must be the shape of each sample, not the shape of one\n# batch.\nmodel.add(keras.Input(shape=(3,)))\nmodel.add(layers.Dense(64, activation=\"relu\")) \n```", "```py\n>>> model.summary()\nModel: \"sequential_2\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_4 (Dense)                   │ (None, 64)               │           256 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 256 (1.00 KB)\n Trainable params: 256 (1.00 KB)\n Non-trainable params: 0 (0.00 B) \n>>> model.add(layers.Dense(10, activation=\"softmax\"))\n>>> model.summary()\nModel: \"sequential_2\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_4 (Dense)                   │ (None, 64)               │           256 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_5 (Dense)                   │ (None, 10)               │           650 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 906 (3.54 KB)\n Trainable params: 906 (3.54 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\ninputs = keras.Input(shape=(3,), name=\"my_input\")\nfeatures = layers.Dense(64, activation=\"relu\")(inputs)\noutputs = layers.Dense(10, activation=\"softmax\")(features)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"my_functional_model\") \n```", "```py\ninputs = keras.Input(shape=(3,), name=\"my_input\") \n```", "```py\n>>> # The model will process batches where each sample has shape (3,).\n>>> # The number of samples per batch is variable (indicated by the\n>>> # None batch size).\n>>> inputs.shape\n(None, 3)\n>>> # These batches will have dtype float32.\n>>> inputs.dtype\n\"float32\"\n```", "```py\nfeatures = layers.Dense(64, activation=\"relu\")(inputs) \n```", "```py\n>>> features.shape\n(None, 64)\n```", "```py\noutputs = layers.Dense(10, activation=\"softmax\")(features)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"my_functional_model\") \n```", "```py\n>>> model.summary()\nModel: \"my_functional_model\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ my_input (InputLayer)             │ (None, 3)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_8 (Dense)                   │ (None, 64)               │           256 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_9 (Dense)                   │ (None, 10)               │           650 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 906 (3.54 KB)\n Trainable params: 906 (3.54 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nvocabulary_size = 10000\nnum_tags = 100\nnum_departments = 4\n\n# Defines model inputs\ntitle = keras.Input(shape=(vocabulary_size,), name=\"title\")\ntext_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\ntags = keras.Input(shape=(num_tags,), name=\"tags\")\n\n# Combines input features into a single tensor, features, by\n# concatenating them\nfeatures = layers.Concatenate()([title, text_body, tags])\n# Applies intermediate layer to recombine input features into richer\n# representations\nfeatures = layers.Dense(64, activation=\"relu\", name=\"dense_features\")(features)\n\n# Defines model outputs\npriority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\ndepartment = layers.Dense(\n    num_departments, activation=\"softmax\", name=\"department\"\n)(features)\n\n# Creates the model by specifying its inputs and outputs\nmodel = keras.Model(\n    inputs=[title, text_body, tags],\n    outputs=[priority, department],\n) \n```", "```py\nimport numpy as np\n\nnum_samples = 1280\n\n# Dummy input data\ntitle_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\ntext_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\ntags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n\n# Dummy target data\npriority_data = np.random.random(size=(num_samples, 1))\ndepartment_data = np.random.randint(0, num_departments, size=(num_samples, 1))\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=[\"mean_squared_error\", \"sparse_categorical_crossentropy\"],\n    metrics=[[\"mean_absolute_error\"], [\"accuracy\"]],\n)\nmodel.fit(\n    [title_data, text_body_data, tags_data],\n    [priority_data, department_data],\n    epochs=1,\n)\nmodel.evaluate(\n    [title_data, text_body_data, tags_data], [priority_data, department_data]\n)\npriority_preds, department_preds = model.predict(\n    [title_data, text_body_data, tags_data]\n) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    loss={\n        \"priority\": \"mean_squared_error\",\n        \"department\": \"sparse_categorical_crossentropy\",\n    },\n    metrics={\n        \"priority\": [\"mean_absolute_error\"],\n        \"department\": [\"accuracy\"],\n    },\n)\nmodel.fit(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n    {\"priority\": priority_data, \"department\": department_data},\n    epochs=1,\n)\nmodel.evaluate(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n    {\"priority\": priority_data, \"department\": department_data},\n)\npriority_preds, department_preds = model.predict(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}\n) \n```", "```py\nkeras.utils.plot_model(model, \"ticket_classifier.png\") \n```", "```py\nkeras.utils.plot_model(\n    model,\n    \"ticket_classifier_with_shape_info.png\",\n    show_shapes=True,\n    show_layer_names=True,\n) \n```", "```py\n>>> model.layers\n[<InputLayer name=title, built=True>,\n <InputLayer name=text_body, built=True>,\n <InputLayer name=tags, built=True>,\n <Concatenate name=concatenate, built=True>,\n <Dense name=dense_10, built=True>,\n <Dense name=priority, built=True>,\n <Dense name=department, built=True>] \n>>> model.layers[3].input\n[<KerasTensor shape=(None, 10000), dtype=float32, sparse=None, name=title>,\n <KerasTensor shape=(None, 10000), dtype=float32, sparse=None, name=text_body>,\n <KerasTensor shape=(None, 100), dtype=float32, sparse=None, name=tags>] \n>>> model.layers[3].output\n<KerasTensor shape=(None, 20100), dtype=float32, sparse=False>\n```", "```py\n# layers[4] is our intermediate Dense layer.\nfeatures = model.layers[4].output\ndifficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n\nnew_model = keras.Model(\n    inputs=[title, text_body, tags], outputs=[priority, department, difficulty]\n) \n```", "```py\nkeras.utils.plot_model(\n    new_model,\n    \"updated_ticket_classifier.png\",\n    show_shapes=True,\n    show_layer_names=True,\n) \n```", "```py\nclass CustomerTicketModel(keras.Model):\n    def __init__(self, num_departments):\n        # Don't forget to call the super constructor!\n        super().__init__()\n        # Defines sublayers in the constructor\n        self.concat_layer = layers.Concatenate()\n        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n        self.department_classifier = layers.Dense(\n            num_departments, activation=\"softmax\"\n        )\n\n    # Defines the forward pass in the call() method\n    def call(self, inputs):\n        title = inputs[\"title\"]\n        text_body = inputs[\"text_body\"]\n        tags = inputs[\"tags\"]\n\n        features = self.concat_layer([title, text_body, tags])\n        features = self.mixing_layer(features)\n        priority = self.priority_scorer(features)\n        department = self.department_classifier(features)\n        return priority, department \n```", "```py\nmodel = CustomerTicketModel(num_departments=4)\n\npriority, department = model(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}\n) \n```", "```py\nmodel.compile(\n    optimizer=\"adam\",\n    # The structure of what you pass as the loss and metrics must match\n    # exactly what gets returned by call() — since we returned a list\n    # of two elements, so should loss and metrics be lists of two\n    # elements.\n    loss=[\"mean_squared_error\", \"sparse_categorical_crossentropy\"],\n    metrics=[[\"mean_absolute_error\"], [\"accuracy\"]],\n)\nmodel.fit(\n    # The structure of the input data must match exactly what is\n    # expected by the call() method, and the structure of the target\n    # data must match exactly what gets returned by the call() method.\n    # Here, the input data must be a dict with three keys (title,\n    # text_body, and tags) and the target data must be a list of two\n    # elements.\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n    [priority_data, department_data],\n    epochs=1,\n)\nmodel.evaluate(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n    [priority_data, department_data],\n)\npriority_preds, department_preds = model.predict(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}\n) \n```", "```py\nclass Classifier(keras.Model):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        if num_classes == 2:\n            num_units = 1\n            activation = \"sigmoid\"\n        else:\n            num_units = num_classes\n            activation = \"softmax\"\n        self.dense = layers.Dense(num_units, activation=activation)\n\n    def call(self, inputs):\n        return self.dense(inputs)\n\ninputs = keras.Input(shape=(3,))\nfeatures = layers.Dense(64, activation=\"relu\")(inputs)\noutputs = Classifier(num_classes=10)(features)\nmodel = keras.Model(inputs=inputs, outputs=outputs) \n```", "```py\ninputs = keras.Input(shape=(64,))\noutputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\nbinary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n\nclass MyModel(keras.Model):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.dense = layers.Dense(64, activation=\"relu\")\n        self.classifier = binary_classifier\n\n    def call(self, inputs):\n        features = self.dense(inputs)\n        return self.classifier(features)\n\nmodel = MyModel() \n```", "```py\nfrom keras.datasets import mnist\n\n# Creates a model. (We factor this into a separate function so as to\n# reuse it later.)\ndef get_mnist_model():\n    inputs = keras.Input(shape=(28 * 28,))\n    features = layers.Dense(512, activation=\"relu\")(inputs)\n    features = layers.Dropout(0.5)(features)\n    outputs = layers.Dense(10, activation=\"softmax\")(features)\n    model = keras.Model(inputs, outputs)\n    return model\n\n# Loads your data, reserving some for validation\n(images, labels), (test_images, test_labels) = mnist.load_data()\nimages = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\ntest_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\ntrain_images, val_images = images[10000:], images[:10000]\ntrain_labels, val_labels = labels[10000:], labels[:10000]\n\nmodel = get_mnist_model()\n# Compiles the model by specifying its optimizer, the loss function to\n# minimize, and metrics to monitor\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n# Uses `fit()` to train the model, optionally providing validation data\n# to monitor performance on unseen data\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=3,\n    validation_data=(val_images, val_labels),\n)\n# Uses `evaluate()` to compute the loss and metrics on new data\ntest_metrics = model.evaluate(test_images, test_labels)\n# Uses `predict()` to compute classification probabilities on new data\npredictions = model.predict(test_images) \n```", "```py\nfrom keras import ops\n\n# Subclasses the Metric class\nclass RootMeanSquaredError(keras.metrics.Metric):\n    # Defines the state variables in the constructor. Like for layers,\n    # you have access to the add_weight() method.\n    def __init__(self, name=\"rmse\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n        self.total_samples = self.add_weight(\n            name=\"total_samples\", initializer=\"zeros\"\n        )\n\n    # Implements the state update logic in update_state(). The y_true\n    # argument is the targets (or labels) for one batch, while y_pred\n    # represents the corresponding predictions from the model. To match\n    # our MNIST model, we expect categorical predictions and integer\n    # labels. You can ignore the sample_weight argument; we won't use\n    # it here.\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = ops.one_hot(y_true, num_classes=ops.shape(y_pred)[1])\n        mse = ops.sum(ops.square(y_true - y_pred))\n        self.mse_sum.assign_add(mse)\n        num_samples = ops.shape(y_pred)[0]\n        self.total_samples.assign_add(num_samples) \n```", "```py\n def result(self):\n        return ops.sqrt(self.mse_sum / self.total_samples) \n```", "```py\n def reset_state(self):\n        self.mse_sum.assign(0.)\n        self.total_samples.assign(0.) \n```", "```py\nmodel = get_mnist_model()\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\", RootMeanSquaredError()],\n)\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=3,\n    validation_data=(val_images, val_labels),\n)\ntest_metrics = model.evaluate(test_images, test_labels) \n```", "```py\nkeras.callbacks.ModelCheckpoint\nkeras.callbacks.EarlyStopping\nkeras.callbacks.LearningRateScheduler\nkeras.callbacks.ReduceLROnPlateau\nkeras.callbacks.CSVLogger \n```", "```py\n# Callbacks are passed to the model via the callbacks argument in\n# fit(), which takes a list of callbacks. You can pass any number of\n# callbacks.\ncallbacks_list = [\n    # Interrupts training when improvement stops\n    keras.callbacks.EarlyStopping(\n        # Monitors the model's validation accuracy\n        monitor=\"accuracy\",\n        # Interrupts training when accuracy has stopped improving for\n        # more than one epoch (that is, two epochs)\n        patience=1,\n    ),\n    # Saves the current weights after every epoch\n    keras.callbacks.ModelCheckpoint(\n        # Path to the destination model file\n        filepath=\"checkpoint_path.keras\",\n        # These two arguments mean you won't overwrite the model file\n        # unless val_loss has improved, which allows you to keep the\n        # best model seen during training.\n        monitor=\"val_loss\",\n        save_best_only=True,\n    ),\n]\nmodel = get_mnist_model()\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    # You monitor accuracy, so it should be part of the model's\n    # metrics.\n    metrics=[\"accuracy\"],\n)\n# Because the callback will monitor validation loss and validation\n# accuracy, you need to pass validation_data to the call to fit().\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=10,\n    callbacks=callbacks_list,\n    validation_data=(val_images, val_labels),\n) \n```", "```py\nmodel = keras.models.load_model(\"checkpoint_path.keras\") \n```", "```py\n# Called at the start of every epoch\non_epoch_begin(epoch, logs)\n# Called at the end of every epoch\non_epoch_end(epoch, logs)\n# Called right before processing each batch\non_batch_begin(batch, logs)\n# Called right after processing each batch\non_batch_end(batch, logs)\n# Called at the start of training\non_train_begin(logs)\n# Called at the end of training\non_train_end(logs) \n```", "```py\nfrom matplotlib import pyplot as plt\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs):\n        self.per_batch_losses = []\n\n    def on_batch_end(self, batch, logs):\n        self.per_batch_losses.append(logs.get(\"loss\"))\n\n    def on_epoch_end(self, epoch, logs):\n        plt.clf()\n        plt.plot(\n            range(len(self.per_batch_losses)),\n            self.per_batch_losses,\n            label=\"Training loss for each batch\",\n        )\n        plt.xlabel(f\"Batch (epoch {epoch})\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(f\"plot_at_epoch_{epoch}\", dpi=300)\n        self.per_batch_losses = [] \n```", "```py\nmodel = get_mnist_model()\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=10,\n    callbacks=[LossHistory()],\n    validation_data=(val_images, val_labels),\n) \n```", "```py\nmodel = get_mnist_model()\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\ntensorboard = keras.callbacks.TensorBoard(\n    log_dir=\"/full_path_to_your_log_dir\",\n)\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=10,\n    validation_data=(val_images, val_labels),\n    callbacks=[tensorboard],\n) \n```", "```py\ntensorboard --logdir /full_path_to_your_log_dir \n```", "```py\n%load_ext tensorboard\n%tensorboard --logdir /full_path_to_your_log_dir \n```", "```py\ndef train_step(inputs, targets):\n    # Runs the forward pass\n    predictions = model(inputs, training=True)\n    # Computes the loss for the current batch\n    loss = loss_fn(targets, predictions)\n    # Retrieves the gradients of the loss with regard to the model's\n    # trainable weights This function doesn't actually exist!\n    gradients = get_gradients_of(loss, wrt=model.trainable_weights)\n    # Updates the model's trainable weights based on the gradients\n    optimizer.apply(gradients, model.trainable_weights) \n```", "```py\nimport tensorflow as tf\n\nmodel = get_mnist_model()\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\noptimizer = keras.optimizers.Adam()\n\ndef train_step(inputs, targets):\n    # Opens a GradientTape\n    with tf.GradientTape() as tape:\n        # Runs the forward pass\n        predictions = model(inputs, training=True)\n        loss = loss_fn(targets, predictions)\n    # Retrieves the gradients from the tape\n    gradients = tape.gradient(loss, model.trainable_weights)\n    # Updates the model's trainable weights based on the gradients\n    optimizer.apply(gradients, model.trainable_weights)\n    return loss \n```", "```py\nbatch_size = 32\ninputs = train_images[:batch_size]\ntargets = train_labels[:batch_size]\nloss = train_step(inputs, targets) \n```", "```py\nimport torch\n\nmodel = get_mnist_model()\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\noptimizer = keras.optimizers.Adam()\n\ndef train_step(inputs, targets):\n    # Runs the forward pass\n    predictions = model(inputs, training=True)\n    loss = loss_fn(targets, predictions)\n    # Runs the backward pass, populating gradient values\n    loss.backward()\n    # Recovers the gradient associated with each trainable variable.\n    # That weight.value is the PyTorch tensor that contains the\n    # variable's value.\n    gradients = [weight.value.grad for weight in model.trainable_weights]\n    # Updates the model's trainable weights based on the gradients.\n    # This must be done in a no_grad() scope.\n    with torch.no_grad():\n        optimizer.apply(gradients, model.trainable_weights)\n    # Don't forget to clear the gradients!\n    model.zero_grad()\n    return loss \n```", "```py\nbatch_size = 32\ninputs = train_images[:batch_size]\ntargets = train_labels[:batch_size]\nloss = train_step(inputs, targets) \n```", "```py\noutputs, non_trainable_weights = model.stateless_call(\n    trainable_weights, non_trainable_weights, inputs\n) \n```", "```py\nmodel = get_mnist_model()\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\n\n# Gradients are computed for the entries in the first argument\n# (trainable_variables here)\ndef compute_loss_and_updates(\n    trainable_variables, non_trainable_variables, inputs, targets\n):\n    # Calls stateless_call\n    outputs, non_trainable_variables = model.stateless_call(\n        trainable_variables, non_trainable_variables, inputs, training=True\n    )\n    loss = loss_fn(targets, outputs)\n    # Returns the scalar loss value and the updated non-trainable\n    # weights\n    return loss, non_trainable_variables \n```", "```py\nimport jax\n\ngrad_fn = jax.value_and_grad(fn)\nloss, gradients = grad_fn(...) \n```", "```py\nimport jax\n\ngrad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True) \n```", "```py\n(loss, non_trainable_weights), gradients = grad_fn(\n    trainable_variables, non_trainable_variables, inputs, targets\n) \n```", "```py\nlearning_rate = 1e-3\n\ndef update_weights(gradients, weights):\n    for g, w in zip(gradients, weights):\n        w.assign(w - g * learning_rate) \n```", "```py\ntrainable_variables, optimizer_variables = optimizer.stateless_apply(\n    optimizer_variables, grads, trainable_variables\n) \n```", "```py\noptimizer = keras.optimizers.Adam()\noptimizer.build(model.trainable_variables)\n\n# The state is part of the function arguments.\ndef train_step(state, inputs, targets):\n    # Unpacks the state\n    (trainable_variables, non_trainable_variables, optimizer_variables) = state\n    # Computes gradients and updates to non-trainable variables\n    (loss, non_trainable_variables), grads = grad_fn(\n        trainable_variables, non_trainable_variables, inputs, targets\n    )\n    # Updates trainable variables and optimizer variables\n    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n        optimizer_variables, grads, trainable_variables\n    )\n    return loss, (\n        # Returns the updated state alongside the loss\n        trainable_variables,\n        non_trainable_variables,\n        optimizer_variables,\n    ) \n```", "```py\nbatch_size = 32\ninputs = train_images[:batch_size]\ntargets = train_labels[:batch_size]\n\ntrainable_variables = [v.value for v in model.trainable_variables]\nnon_trainable_variables = [v.value for v in model.non_trainable_variables]\noptimizer_variables = [v.value for v in optimizer.variables]\n\nstate = (trainable_variables, non_trainable_variables, optimizer_variables)\nloss, state = train_step(state, inputs, targets) \n```", "```py\nfrom keras import ops\n\nmetric = keras.metrics.SparseCategoricalAccuracy()\ntargets = ops.array([0, 1, 2])\npredictions = ops.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nmetric.update_state(targets, predictions)\ncurrent_result = metric.result()\nprint(f\"result: {current_result:.2f}\") \n```", "```py\nvalues = ops.array([0, 1, 2, 3, 4])\nmean_tracker = keras.metrics.Mean()\nfor value in values:\n    mean_tracker.update_state(value)\nprint(f\"Mean of values: {mean_tracker.result():.2f}\") \n```", "```py\nmetric = keras.metrics.SparseCategoricalAccuracy()\ntargets = ops.array([0, 1, 2])\npredictions = ops.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n# Gets the metric's state variables\nmetric_variables = metric.variables\n# Gets updated values for the metric's state\nmetric_variables = metric.stateless_update_state(\n    metric_variables, targets, predictions\n)\n# Computes the metric value corresponding to the current state\ncurrent_result = metric.stateless_result(metric_variables)\nprint(f\"result: {current_result:.2f}\")\n\n# Gets blank variable values for the metric\nmetric_variables = metric.stateless_reset_state() \n```", "```py\nimport keras\nfrom keras import layers\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\n# This metric object will be used to track the average of per-batch\n# losses during training and evaluation.\nloss_tracker = keras.metrics.Mean(name=\"loss\")\n\nclass CustomModel(keras.Model):\n    # Overrides the train_step() method\n    def train_step(self, data):\n        inputs, targets = data\n        with tf.GradientTape() as tape:\n            # We use self(inputs, training=True) instead of\n            # model(inputs, training=True) since our model is the class\n            # itself.\n            predictions = self(inputs, training=True)\n            loss = loss_fn(targets, predictions)\n        gradients = tape.gradient(loss, self.trainable_weights)\n        self.optimizer.apply(gradients, self.trainable_weights)\n\n        # Updates the loss tracker metric that tracks the average of\n        # the loss\n        loss_tracker.update_state(loss)\n        # Returns the average loss so far by querying the loss tracker\n        # metric\n        return {\"loss\": loss_tracker.result()}\n\n    # Listing the loss tracker metric in the model.metrics property\n    # enables the model to automatically call reset_state() on it at\n    # the start of each epoch and at the start of a call to evaluate()\n    # — so you don't have to do it by hand. Any metric you would like\n    # to reset across epochs should be listed here.\n    @property\n    def metrics(self):\n        return [loss_tracker] \n```", "```py\ndef get_custom_model():\n    inputs = keras.Input(shape=(28 * 28,))\n    features = layers.Dense(512, activation=\"relu\")(inputs)\n    features = layers.Dropout(0.5)(features)\n    outputs = layers.Dense(10, activation=\"softmax\")(features)\n    model = CustomModel(inputs, outputs)\n    model.compile(optimizer=keras.optimizers.Adam())\n    return model \n```", "```py\nmodel = get_custom_model()\nmodel.fit(train_images, train_labels, epochs=3) \n```", "```py\nimport keras\nfrom keras import layers\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\nloss_tracker = keras.metrics.Mean(name=\"loss\")\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        inputs, targets = data\n        # Runs the forward pass\n        predictions = self(inputs, training=True)\n        loss = loss_fn(targets, predictions)\n\n        # Retrieves the gradients\n        loss.backward()\n        trainable_weights = [v for v in self.trainable_weights]\n        gradients = [v.value.grad for v in trainable_weights]\n\n        with torch.no_grad():\n            # Updates weights\n            self.optimizer.apply(gradients, trainable_weights)\n\n        # Updates loss tracker metric\n        loss_tracker.update_state(loss)\n        # Returns the average loss so far by querying the loss tracker\n        # metric\n        return {\"loss\": loss_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [loss_tracker] \n```", "```py\nmodel = get_custom_model()\nmodel.fit(train_images, train_labels, epochs=3) \n```", "```py\nimport keras\nfrom keras import layers\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\n\nclass CustomModel(keras.Model):\n    def compute_loss_and_updates(\n        self,\n        trainable_variables,\n        non_trainable_variables,\n        inputs,\n        targets,\n        training=False,\n    ):\n        predictions, non_trainable_variables = self.stateless_call(\n            trainable_variables,\n            non_trainable_variables,\n            inputs,\n            training=training,\n        )\n        loss = loss_fn(targets, predictions)\n        # Returns both the loss and the updated non-trainable variables\n        return loss, non_trainable_variables \n```", "```py\n def train_step(self, state, data):\n        # Unpacks the state. metrics_variables are part of it, although\n        # we won't use them here.\n        (\n            trainable_variables,\n            non_trainable_variables,\n            optimizer_variables,\n            metrics_variables,\n        ) = state\n        inputs, targets = data\n\n        # Gets the gradient function\n        grad_fn = jax.value_and_grad(\n            self.compute_loss_and_updates, has_aux=True\n        )\n\n        # Computes gradients and updates to non-trainable variables\n        (loss, non_trainable_variables), grads = grad_fn(\n            trainable_variables,\n            non_trainable_variables,\n            inputs,\n            targets,\n            training=True,\n        )\n\n        # Updates trainable variables and optimizer variables\n        (\n            trainable_variables,\n            optimizer_variables,\n        ) = self.optimizer.stateless_apply(\n            optimizer_variables, grads, trainable_variables\n        )\n\n        # We aren't computing a moving average of the loss, instead\n        # returning the per-batch value.\n        logs = {\"loss\": loss}\n        state = (\n            trainable_variables,\n            non_trainable_variables,\n            optimizer_variables,\n            metrics_variables,\n        )\n        # Returns metric logs and updated state variables\n        return logs, state \n```", "```py\nmodel = get_custom_model()\nmodel.fit(train_images, train_labels, epochs=3) \n```", "```py\nimport keras\nfrom keras import layers\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        inputs, targets = data\n        with tf.GradientTape() as tape:\n            predictions = self(inputs, training=True)\n            # Computes the loss via self.compute_loss\n            loss = self.compute_loss(y=targets, y_pred=predictions)\n\n        gradients = tape.gradient(loss, self.trainable_weights)\n        self.optimizer.apply(gradients, self.trainable_weights)\n\n        # Updates the model's metrics, including the one that tracks\n        # the loss\n        for metric in self.metrics:\n            if metric.name == \"loss\":\n                metric.update_state(loss)\n            else:\n                metric.update_state(targets, predictions)\n\n        # Returns a dict mapping metric names to their current value\n        return {m.name: m.result() for m in self.metrics} \n```", "```py\ndef get_custom_model():\n    inputs = keras.Input(shape=(28 * 28,))\n    features = layers.Dense(512, activation=\"relu\")(inputs)\n    features = layers.Dropout(0.5)(features)\n    outputs = layers.Dense(10, activation=\"softmax\")(features)\n    model = CustomModel(inputs, outputs)\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    )\n    return model\n\nmodel = get_custom_model()\nmodel.fit(train_images, train_labels, epochs=3) \n```", "```py\nimport keras\nfrom keras import layers\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        inputs, targets = data\n        predictions = self(inputs, training=True)\n        loss = self.compute_loss(y=targets, y_pred=predictions)\n\n        loss.backward()\n        trainable_weights = [v for v in self.trainable_weights]\n        gradients = [v.value.grad for v in trainable_weights]\n\n        with torch.no_grad():\n            self.optimizer.apply(gradients, trainable_weights)\n\n        for metric in self.metrics:\n            if metric.name == \"loss\":\n                metric.update_state(loss)\n            else:\n                metric.update_state(targets, predictions)\n\n        return {m.name: m.result() for m in self.metrics} \n```", "```py\ndef get_custom_model():\n    inputs = keras.Input(shape=(28 * 28,))\n    features = layers.Dense(512, activation=\"relu\")(inputs)\n    features = layers.Dropout(0.5)(features)\n    outputs = layers.Dense(10, activation=\"softmax\")(features)\n    model = CustomModel(inputs, outputs)\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    )\n    return model\n\nmodel = get_custom_model()\nmodel.fit(train_images, train_labels, epochs=3) \n```", "```py\nimport keras\nfrom keras import layers\n\nclass CustomModel(keras.Model):\n    def compute_loss_and_updates(\n        self,\n        trainable_variables,\n        non_trainable_variables,\n        inputs,\n        targets,\n        training=False,\n    ):\n        predictions, non_trainable_variables = self.stateless_call(\n            trainable_variables,\n            non_trainable_variables,\n            inputs,\n            training=training,\n        )\n        loss = self.compute_loss(y=targets, y_pred=predictions)\n        return loss, (predictions, non_trainable_variables) \n```", "```py\n def train_step(self, state, data):\n        (\n            trainable_variables,\n            non_trainable_variables,\n            optimizer_variables,\n            # Metric variables are part of the state.\n            metrics_variables,\n        ) = state\n        inputs, targets = data\n\n        grad_fn = jax.value_and_grad(\n            self.compute_loss_and_updates, has_aux=True\n        )\n\n        (loss, (predictions, non_trainable_variables)), grads = grad_fn(\n            trainable_variables,\n            non_trainable_variables,\n            inputs,\n            targets,\n            training=True,\n        )\n        (\n            trainable_variables,\n            optimizer_variables,\n        ) = self.optimizer.stateless_apply(\n            optimizer_variables, grads, trainable_variables\n        )\n\n        new_metrics_vars = []\n        logs = {}\n        # Iterates over metrics\n        for metric in self.metrics:\n            num_prev = len(new_metrics_vars)\n            num_current = len(metric.variables)\n            # Grabs the variables of the current metrics\n            current_vars = metrics_variables[num_prev : num_prev + num_current]\n            # Updates the metric's state\n            if metric.name == \"loss\":\n                current_vars = metric.stateless_update_state(current_vars, loss)\n            else:\n                current_vars = metric.stateless_update_state(\n                    current_vars, targets, predictions\n                )\n            # Stores the results in the logs dict\n            logs[metric.name] = metric.stateless_result(current_vars)\n            new_metrics_vars += current_vars\n\n        state = (\n            trainable_variables,\n            non_trainable_variables,\n            optimizer_variables,\n            # Returns the new metrics variables as part of the state\n            new_metrics_vars,\n        )\n        return logs, state \n```"]