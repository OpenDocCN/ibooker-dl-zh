<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 13. Deploying a Machine Learning API"><div class="chapter" id="chapter_13">
<h1><span class="label">Chapter 13. </span>Deploying a Machine Learning API</h1>

<blockquote data-type="epigraph" epub:type="epigraph">
  <p>Always in motion is the future.</p>
  <p data-type="attribution">Yoda, <em>The Empire Strikes Back</em></p>
</blockquote>

<p>Fantasy football managers spend<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="about" id="id2296"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="about" id="id2297"/> most of their time attempting to predict the future and plotting strategies based on those predictions. Before the season begins, managers want to know how NFL players will perform in the upcoming season so that they can build the best team. During their fantasy drafts, managers want to know where a player would be picked by other managers so that they can outmaneuver their competition. Each week, managers want to know which of their players are going to score the most so that they can set their lineups accordingly.</p>

<p>Many fantasy websites and platforms provide predictions to these managers. <a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="about" id="id2298"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="machine learning" data-seealso="machine learning (ML)" id="id2299"/>One of the tools available to the platforms is a <em>machine learning</em> (ML) model, which you learned about in <a data-type="xref" href="ch12.html#chapter_12">Chapter 12</a>. <a data-type="indexterm" data-primary="inferences from ML models" id="id2300"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="inferences from ML models" id="id2301"/>The platforms train various models and use them to make predictions, or <em>inferences</em>, to managers. <a data-type="indexterm" data-primary="inferences from ML models" data-secondary="batch inferences" id="id2302"/><a data-type="indexterm" data-primary="batch inferences from ML models" id="id2303"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="inferences from ML models" data-tertiary="batch inferences" id="id2304"/>If a model processes an entire group of predictions at once, it is called <em>batch inference</em>. Some fantasy questions are appropriate for batch inference, such as making a week’s worth of player predictions all at once. Batch inference may be done by a scheduled script or job. <a data-type="indexterm" data-primary="inferences from ML models" data-secondary="real-time inference" id="id2305"/><a data-type="indexterm" data-primary="real-time inference from ML models" id="id2306"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="inferences from ML models" data-tertiary="real-time inference" id="id2307"/>But if the predictions are changing minute by minute—like in the case of a live score prediction for a game—then real-time inference is needed. <em>Real-time inference</em> is calling a model to get a single prediction immediately. This is where deploying the model as an API is most <span class="keep-together">valuable.</span></p>

<p class="less_space pagebreak-before">In this chapter, you will create<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="real-time inference model" id="id2308"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="real-time inference model" id="id2309"/> an ML model and deploy it with an API to make real-time inference. As you proceed through the chapter, here are a few terms that you will come across:<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="about" data-tertiary="terminology" id="id2310"/></p>
<dl>
<dt>Classification</dt>
<dd>
<p>A type of model that predicts<a data-type="indexterm" data-primary="classification ML models" id="id2311"/> what category a value will fall into. For example, a classification model might predict if a player will be drafted or undrafted. Models that perform classification are called <em>classifiers</em>.</p>
</dd>
<dt>Decision trees</dt>
<dd>
<p>A type of ML algorithm that creates a recursive tree structure to perform classification or regression.<a data-type="indexterm" data-primary="decision trees" id="id2312"/></p>
</dd>
<dt>Evaluating a model</dt>
<dd>
<p>Comparing the model’s predictions to test data to see how well it would have predicted past events.<a data-type="indexterm" data-primary="evaluating a machine learning model" data-secondary="about" id="id2313"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="evaluating a model" data-tertiary="about" id="id2314"/></p>
</dd>
<dt>Gradient boosting</dt>
<dd>
<p>An ML technique<a data-type="indexterm" data-primary="gradient boosting" id="id2315"/> that combines multiple models to create a model that is more effective than the individual models.</p>
</dd>
<dt>Regression</dt>
<dd>
<p>A type of model that predicts<a data-type="indexterm" data-primary="regression ML models" id="id2316"/> a continuous numeric value. For example, a regression model might predict how many points a player will score. Models that perform regression are called <em>regressors</em>.</p>
</dd>
<dt>Training a model</dt>
<dd>
<p>Using the training portion of historical data to create<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="training models" id="id2317"/><a data-type="indexterm" data-primary="training ML models" id="id2318"/> a model that can make inferences based on new data.</p>
</dd>
</dl>






<section data-type="sect1" data-pdf-bookmark="Training Machine Learning Models"><div class="sect1" id="id131">
<h1>Training Machine Learning Models</h1>

<p><em>Supervised learning</em> is a method<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="training models" data-tertiary="supervised learning" id="id2319"/><a data-type="indexterm" data-primary="training ML models" data-secondary="supervised learning" id="id2320"/><a data-type="indexterm" data-primary="supervised learning" id="id2321"/> of creating models by processing existing data where the expected values are known. For example, a financial fraud detection model might be trained by processing a large number of bank transactions<a data-type="indexterm" data-primary="supervised learning" data-secondary="labeled data" id="id2322"/> that have been <em>labeled</em> or categorized as either fraud or nonfraud. Through this process, the model recognizes future records that are potentially fraudulent. Through this type of supervised learning, ML models can be created that create predictions on various data formats including tabular data, images, audio files, and others.</p>

<p><a data-type="xref" href="#machine_learning_diagram_ch12">Figure 13-1</a> shows this type of training.</p>

<figure><div id="machine_learning_diagram_ch12" class="figure">
<img src="assets/haad_1301.png" alt="Machine learning training" width="694" height="691"/>
<h6><span class="label">Figure 13-1. </span>ML training model</h6>
</div></figure>

<p>The diagram shows a set of historical rows of data. The goal of the ML model in this case would be to predict future values of the output column. The training process would involve using software to read the input columns from historical data and look for patterns in how they are related to the output column.</p>

<p>When the model has been trained,<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="inferences from ML models" data-tertiary="example" id="id2323"/><a data-type="indexterm" data-primary="inferences from ML models" data-secondary="example" id="id2324"/> it can be used to read the input columns from new rows of data and predict what the values will be for the output columns. This is the <em>inference</em> process, shown in <a data-type="xref" href="#machine_learning_diagram_2_ch12">Figure 13-2</a>.</p>

<figure><div id="machine_learning_diagram_2_ch12" class="figure">
<img src="assets/haad_1302.png" alt="Machine learning inference model" width="729" height="691"/>
<h6><span class="label">Figure 13-2. </span>ML inference model</h6>
</div></figure>

<p>The model you will create in this chapter is a supervised ML model.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="New Software Used in This Chapter"><div class="sect1" id="id132">
<h1>New Software Used in This Chapter</h1>

<p><a data-type="xref" href="#tools_table_chapter_13">Table 13-1</a> lists a few of the new software components you will begin using in this chapter.<a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" data-secondary="ONNX runtime" id="id2325"/><a data-type="indexterm" data-primary="scikit-learn" id="id2326"/><a data-type="indexterm" data-primary="sklearn-onnx library" id="id2327"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="software used" id="id2328"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="software used" id="id2329"/><a data-type="indexterm" data-primary="regression ML models" data-secondary="GradientBoostingRegressor algorithm" id="id2330"/><a data-type="indexterm" data-primary="GradientBoostingRegressor algorithm" id="id2331"/></p>
<table id="tools_table_chapter_13">
<caption><span class="label">Table 13-1. </span>Software used in this chapter</caption>
<thead>
<tr>
<th>Software name</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>ONNX Runtime</p></td>
<td><p>A cross-platform tool for using models from a variety of different frameworks.</p></td>
</tr>
<tr>
<td><p>scikit-learn</p></td>
<td><p>An ML framework for training models. You will use the <code>GradientBoostingRegressor</code> from this library.</p></td>
</tr>
<tr>
<td><p>sklearn-onnx</p></td>
<td><p>A library that converts scikit-learn models to ONNX format.</p></td>
</tr>
</tbody>
</table>








<section data-type="sect2" data-pdf-bookmark="ONNX Runtime"><div class="sect2" id="id133">
<h2>ONNX Runtime</h2>

<p>The Open Neural Network Exchange (ONNX) is an<a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" id="id2332"/> open standard for ML models. Because such a variety of programming languages and libraries are used to make ML models, it can be complicated to deploy and run multiple different models. ONNX is a standard format that models from different programming languages and different frameworks can be converted to and run in a standard way.</p>

<p>This allows greater interoperability, because when models from different programming languages and frameworks are converted to ONNX format, they can be more easily deployed using the standard ONNX Runtime. The ONNX Runtime also includes acceleration that can improve model inference performance.</p>

<p>After you have developed your model in scikit-learn, you will convert it to ONNX format, and then use the <a href="https://oreil.ly/IGEBD">ONNX Runtime</a> in your API to make predictions (inferences).</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="scikit-learn"><div class="sect2" id="id394">
<h2>scikit-learn</h2>

<p>The scikit-learn library is a Python framework that allows you to create models for classification, regression, clustering, and a variety of other tasks. This is one of the more popular ML libraries in Python, along with PyTorch, TensorFlow, and XGBoost.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="sklearn-onnx"><div class="sect2" id="id395">
<h2>sklearn-onnx</h2>

<p>Since you are using scikit-learn to create your model, you will use the sklearn-onnx library to convert your model into ONNX format. This will be the final step of the model training process.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Installing the New Libraries in Your Codespace"><div class="sect1" id="id213">
<h1>Installing the New Libraries in Your Codespace</h1>

<p>Open the Part III GitHub Codespace<a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="installing new libraries" id="id2333"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="installing new libraries" id="id2334"/><a data-type="indexterm" data-primary="requirements.txt file for pip" data-secondary="installing ML API development libraries" id="id2335"/> that you created in <a data-type="xref" href="ch12.html#chapter_12">Chapter 12</a>. To install the libraries you need for this chapter, create a file named <em>chapter13/requirements.txt</em>:</p>

<pre data-type="programlisting" data-code-language="shell">.../ai-project (main) $ cd chapter13
.../chapter13 (main) $ touch requirements.txt</pre>

<p>Update <em>chapter13/requirements.txt</em> with the following contents:</p>

<pre data-type="programlisting" data-code-language="shell">#model training
scikit-learn <a class="co" id="co_deploying_a_machine_learning_api_CO1-1" href="#callout_deploying_a_machine_learning_api_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
numpy <a class="co" id="co_deploying_a_machine_learning_api_CO1-2" href="#callout_deploying_a_machine_learning_api_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
pandas <a class="co" id="co_deploying_a_machine_learning_api_CO1-3" href="#callout_deploying_a_machine_learning_api_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
skl2onnx <a class="co" id="co_deploying_a_machine_learning_api_CO1-4" href="#callout_deploying_a_machine_learning_api_CO1-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a>
pydantic&gt;=2.4.0
fastapi[standard]&gt;=0.115.0
uvicorn&gt;=0.23.0 <a class="co" id="co_deploying_a_machine_learning_api_CO1-5" href="#callout_deploying_a_machine_learning_api_CO1-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a>
onnxruntime <a class="co" id="co_deploying_a_machine_learning_api_CO1-6" href="#callout_deploying_a_machine_learning_api_CO1-6"><img src="assets/6.png" alt="6" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO1-1" href="#co_deploying_a_machine_learning_api_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The scikit-learn library will be used to create the ML model.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO1-2" href="#co_deploying_a_machine_learning_api_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The numpy library will be used to format numbers.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO1-3" href="#co_deploying_a_machine_learning_api_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The pandas library will be used to process the input data file.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO1-4" href="#co_deploying_a_machine_learning_api_CO1-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>The skl2onnx library will be used to save the scikit-learn model into ONNX <span class="keep-together">format.</span></p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO1-5" href="#co_deploying_a_machine_learning_api_CO1-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Uvicorn is the web server used to host FastAPI.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO1-6" href="#co_deploying_a_machine_learning_api_CO1-6"><img src="assets/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>The onnxruntime library is used to perform inference with a saved ONNX model file.</p></dd>
</dl>

<p>Execute the following command to install the new libraries in your Codespace:</p>

<pre data-type="programlisting" data-code-language="shell">.../chapter13 (main) $ pip3 install -r requirements.txt</pre>

<p>You should see a message that states that these libraries were successfully installed.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Using the CRISP-DM Process"><div class="sect1" id="id134">
<h1>Using the CRISP-DM Process</h1>

<p>ML projects have many steps requiring<a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="CRISP-DM process" id="ch13crisp"/><a data-type="indexterm" data-primary="data scientists" data-secondary="CRISP-DM process" id="ch13crisp2"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="CRISP-DM process" id="ch13crisp3"/><a data-type="indexterm" data-primary="CRISP-DM process" id="ch13crisp4"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="about" id="id2336"/> people with a lot of specialized skills. A useful method of organizing an ML modeling project is the Cross-Industry Standard Process for Data Mining (Shearer, 2000). This model is widely used in the data science community.</p>

<p class="less_space pagebreak-before">The following are definitions of the stages in CRISP-DM:</p>
<dl>
<dt>Business understanding</dt>
<dd>
<p>During the this stage,<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="business understanding" id="id2337"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="business understanding" id="id2338"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="business understanding" id="id2339"/> the team identifies business objectives and assesses tools and techniques available.</p>
</dd>
<dt>Data understanding</dt>
<dd>
<p>Collecting data that is available<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="data understanding" id="id2340"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="data understanding" id="id2341"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="data understanding" id="id2342"/> to solve the problem, explore it, and verify the data quality.</p>
</dd>
<dt>Data preparation</dt>
<dd>
<p>During this stage, data scientists<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="data preparation" id="id2343"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="data preparation" id="id2344"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="data preparation" id="id2345"/> select specific data elements to be used, format them, and merge with any additional sources needed.</p>
</dd>
<dt>Modeling</dt>
<dd>
<p>Selecting a modeling technique<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="modeling" id="id2346"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="modeling" id="id2347"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="modeling" id="id2348"/><a data-type="indexterm" data-primary="modeling per CRISP-DM process" id="id2349"/> and building a model that answers your business question.</p>
</dd>
<dt>Evaluation</dt>
<dd>
<p>Review the model for its ability to solve the question and its readiness for production.<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="evaluation of model" id="id2350"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="evaluation of model" id="id2351"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="evaluation of model" id="id2352"/></p>
</dd>
<dt>Deployment</dt>
<dd>
<p>Models are deployed in an<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="deployment" id="id2353"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="deployment" id="id2354"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="deployment" id="id2355"/> environment where they can be consumed by the customer. Monitor and maintain the model.</p>
</dd>
</dl>

<p>You will follow this process as you proceed with the chapter. The primary focus is on the deployment stage, so I will only touch lightly on some of the other stages.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Business Understanding"><div class="sect1" id="id135">
<h1>Business Understanding</h1>

<p>The first stage of the process<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="business needs and resources" id="id2356"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="business needs and resources" id="id2357"/> is to establish a business understanding of the problem you are trying to solve. You are creating a model to serve fantasy football managers who are running their own team in a league with other owners. The question they need to answer each week of the season is “How much will it cost to acquire this player on waivers?”</p>

<p>Fantasy managers can add<a data-type="indexterm" data-primary="waiver requests" id="id2358"/> new players to their rosters through a <em>waiver request</em>. In many leagues, a blind bidding auction is performed to decide who gets the best available players. Managers decide which players they want to bid for and put in the dollar amount they want to spend, which is hidden from other managers. When the bidding is processed on Tuesday or Wednesday of each week, the highest bidder gets the player at full price. Lower bidders miss out (but also don’t lose their money).</p>

<p>Each manager has a set amount of money they can use for the season, such as $100. (These aren’t real-world dollars, these are fantasy dollars.) <a data-type="indexterm" data-primary="free agent acquisition budget (FAAB)" id="id2359"/><a data-type="indexterm" data-primary="FAAB (free agent acquisition budget)" id="id2360"/>This is sometimes called the <em>free agent acquisition budget</em> (FAAB). A manager wants to bid high enough to win the bid, but not overspend. The best-case scenario would be to win the bid at a lower dollar amount—getting a bargain.</p>

<p>To help the manager bid enough to win the player they want without overspending, you will give the manager a range of predictions: the low-end cost (10th percentile), the median cost (50th percentile), and the high-end cost (90th percentile).</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Data Understanding"><div class="sect1" id="id136">
<h1>Data Understanding</h1>

<p>In this stage, you will collect and explore<a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="data understanding" id="id2361"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="data understanding" id="id2362"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="data understanding" id="id2363"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="player_training_data_full.csv" id="id2364"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="player_training_data_full.csv" id="id2365"/><a data-type="indexterm" data-primary="player_training_data_full.csv file" id="id2366"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="data understanding" data-tertiary="player_training_data_full.csv" id="id2367"/> the data that is available for your project. In the project repository, you’ll find the file <em>player_training_data_full.csv</em>. It contains historical fantasy football transaction data with the following columns:</p>
<dl>
<dt>Fantasy regular season weeks remaining</dt>
<dd>
<p>How many weeks are left in the regular season. For example, in week 2 of a season with 14 weeks, this would be 12.</p>
</dd>
<dt>League budget percentage remaining</dt>
<dd>
<p>The percent of total dollars available in the league. For example, if $900 remain in the league’s original $1,200, this would be 75.</p>
</dd>
<dt>Player season number</dt>
<dd>
<p>The number of seasons this player has been in the league. Rookies have a value <span class="keep-together">of  1.</span></p>
</dd>
<dt>Position</dt>
<dd>
<p>The fantasy football position of the players that was acquired.</p>
</dd>
<dt>Waiver value tier</dt>
<dd>
<p>A qualitative measure of how valuable an individual player is. Each week, some players are “top tier” pickup targets, and they would get a 1. Players who are nothing special would get a 5. This is a categorical feature because putting players into the tiers is a qualitative judgment. (You may get these from a fantasy website or assign them yourself.)</p>
</dd>
</dl>

<p>To begin reviewing the data and selecting fields<a data-type="indexterm" data-primary="Jupyter Notebooks" data-secondary="ML API development" id="id2368"/><a data-type="indexterm" data-primary="Jupyter Notebooks" data-secondary="ML API development" data-tertiary="player_acquisition_model.ipynb" id="id2369"/> you want to use in your model, create a Jupyter Notebook by running the following commands in the Terminal window:<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="player_acquisition_model.ipynb" id="id2370"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="player_acquisition_model.ipynb" id="id2371"/><a data-type="indexterm" data-primary="player_acquisition_model.ipynb file" id="id2372"/><a data-type="indexterm" data-primary=".ipynb file extension" data-secondary="player_acquisition_model.ipynb" data-primary-sortas="ipynb" id="id2373"/></p>

<pre data-type="programlisting" data-code-language="shell">.../ai-project (main) $ cd chapter13
.../chapter13 (main) $ touch player_acquisition_model.ipynb</pre>

<p>Open the <em>player_acquisition_model.ipynb</em> file. As you did in <a data-type="xref" href="ch09.html#chapter_9">Chapter 9</a>, select the Python kernel and enable the Python and Jupyter extensions, then select the recommended Python environment.</p>

<p class="less_space pagebreak-before">Enter the following title in the Markdown cell and run it:</p>

<pre data-type="programlisting" data-code-language="markdown"># Player Acquisition Models
*This notebook is used to train a machine learning model using scikit-learn
and save it in ONNX format.*</pre>

<p>Now you will import the Python libraries you need. Create and run the following Markdown cell:</p>

<pre data-type="programlisting" data-code-language="markdown">## Library imports</pre>

<p>Add and run a new Python cell with the following code:</p>

<pre data-type="programlisting" data-code-language="python">import logging
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split <a class="co" id="co_deploying_a_machine_learning_api_CO2-1" href="#callout_deploying_a_machine_learning_api_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
from sklearn.ensemble import GradientBoostingRegressor <a class="co" id="co_deploying_a_machine_learning_api_CO2-2" href="#callout_deploying_a_machine_learning_api_CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>

from skl2onnx import to_onnx
import onnxruntime as rt</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO2-1" href="#co_deploying_a_machine_learning_api_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This function is used to split data files into train and test sets.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO2-2" href="#co_deploying_a_machine_learning_api_CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Your model will use the <code>GradientBoostingRegressor</code> algorithm.<a data-type="indexterm" data-primary="regression ML models" data-secondary="GradientBoostingRegressor algorithm" id="id2374"/><a data-type="indexterm" data-primary="GradientBoostingRegressor algorithm" id="id2375"/></p></dd>
</dl>

<p>Add another Markdown cell with the following text:<a data-type="indexterm" data-primary="logging" data-secondary="Jupyter Notebook" data-tertiary="CRISP-DM process" id="id2376"/><a data-type="indexterm" data-primary="Jupyter Notebooks" data-secondary="logging" data-tertiary="CRISP-DM process" id="id2377"/><a data-type="indexterm" data-primary="logging" data-secondary="ML API development" id="id2378"/></p>

<pre data-type="programlisting" data-code-language="markdown">## Configure logging</pre>

<p>Add and run a Python code cell with the following:</p>

<pre data-type="programlisting" data-code-language="python">for handler in logging.root.handlers[:]: <a class="co" id="co_deploying_a_machine_learning_api_CO3-1" href="#callout_deploying_a_machine_learning_api_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
    logging.root.removeHandler(handler)

logging.basicConfig(
    filename='player_acquisition_notebook.log',
    level=logging.INFO,  <a class="co" id="co_deploying_a_machine_learning_api_CO3-2" href="#callout_deploying_a_machine_learning_api_CO3-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
)</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO3-1" href="#co_deploying_a_machine_learning_api_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This statement removes any existing logging handlers configured by CodeSpaces.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO3-2" href="#co_deploying_a_machine_learning_api_CO3-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This sets the logging level to record in the log.</p></dd>
</dl>

<p>To begin loading your training data, add another Markdown cell with the following text:<a data-type="indexterm" data-primary="player_training_data_full.csv file" id="id2379"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="player_training_data_full.csv" id="id2380"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="data understanding" data-tertiary="player_training_data_full.csv" id="id2381"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="player_training_data_full.csv" id="id2382"/></p>

<pre data-type="programlisting" data-code-language="markdown">## Load data</pre>

<p>Add and run a Python code cell with the following:</p>

<pre data-type="programlisting" data-code-language="python">dataset=pd.read_csv("player_training_data_full.csv")</pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Data Preparation"><div class="sect1" id="id214">
<h1>Data Preparation</h1>

<p>Next you will select the data to be included in the model. <a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="data preparation" id="id2383"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="data preparation" id="id2384"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="data preparation" id="id2385"/> Rather than simply trying out all possible variables, you should consider the reason or theory that each would make a contribution to your model. Here are the three columns you’ll include, and the theory behind each:</p>
<dl>
<dt>League budget percentage remaining</dt>
<dd>
<p>Your intuition is that a higher budget remaining leads to higher bids. This would make this a <em>linear</em> feature, in which the output variable goes up or down at a consistent rate as this value changes.</p>
</dd>
<dt>Fantasy regular season weeks remaining</dt>
<dd>
<p>The theory here is that players cost more at different points of the season. This probably isn’t a strictly linear value. History suggests some of the highest bids come in at the beginning of the season when the starting lineups are revealed, but other peak bids occur from injured players during the season and when managers have “use it or lose it” at the end of the season.</p>
</dd>
<dt>Waiver value tier</dt>
<dd>
<p>At a high level this is straightforward: higher-value players will cost more. But how much more? And how is each tier affected? These are more nuanced questions that you hope the model will be able to detect in the training data.</p>
</dd>
</dl>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Modeling"><div class="sect1" id="id137">
<h1>Modeling</h1>

<p>Now you will begin the modeling stage, first by selecting<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="modeling" id="ch13train2"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="modeling" id="ch13train3"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="modeling" id="ch13train4"/><a data-type="indexterm" data-primary="modeling per CRISP-DM process" id="ch13train5"/> the algorithm and ML framework to use for your model. This decision is a combination of technical limitations and modeling factors.</p>

<p>Your technical limitations are that you want to use a Python framework and you want to convert the model to the ONNX format for inference. You also want to make predictions for the 10th, 50th, and 90th percentiles, so you need to use an algorithm that meets these technical criteria.</p>

<p>Two modeling factors to consider are the type of output and the features you’ve selected. Your output will be numerical dollar values, so you will use a regression model (regressor). If your input features were all linear (as your input goes up or down, your prediction goes up or down), you could use a linear regressor. But your selected features are budget remaining (a linear feature), value tier (a categorical feature), and weeks remaining (a slightly more complicated feature). Because of the complexity of these features, some type of decision tree regressor is more appropriate.</p>

<p class="less_space pagebreak-before">Based on these technical and modeling factors,<a data-type="indexterm" data-primary="regression ML models" data-secondary="GradientBoostingRegressor algorithm" id="id2386"/><a data-type="indexterm" data-primary="GradientBoostingRegressor algorithm" id="id2387"/><a data-type="indexterm" data-primary="resources online" data-secondary="GradientBoostingRegressor algorithm" id="id2388"/><a data-type="indexterm" data-primary="scikit-learn" data-secondary="GradientBoostingRegressor algorithm" id="id2389"/> you will use the <a href="https://oreil.ly/_gqSO"><code>GradientBoostingRegressor</code> algorithm</a> from scikit-learn. The gradient boosting algorithm is way of combining multiple decision trees into an ensemble model that is more predictive than using the individual decision trees by themselves. It also supports the multiple predictions by percentile that you want to use. This algorithm is also supported by the ONNX format you will be saving the model in.</p>

<p>To get started with the modeling process, you will first split your data into multiple variables for the training (80% of the rows) and testing (20% of the rows). There are conventions for naming of variables, and you will follow those so that your code is understandable by other data scientists. <a data-type="xref" href="#training_variables_ch13">Table 13-2</a> explains the purpose of these variable names.</p>
<table id="training_variables_ch13">
<caption><span class="label">Table 13-2. </span>Conventional variable names for training models</caption>
<thead>
<tr>
<th>Variable name</th>
<th>Purpose</th>
<th>Columns included</th>
<th>Data included</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>X (uppercase)</p></td>
<td><p>Input columns for full data</p></td>
<td><p>Input</p></td>
<td><p>All</p></td>
</tr>
<tr>
<td><p>y (lowercase)</p></td>
<td><p>Output columns for full data</p></td>
<td><p>Output</p></td>
<td><p>All</p></td>
</tr>
<tr>
<td><p>X_train</p></td>
<td><p>Input columns of training data</p></td>
<td><p>Input</p></td>
<td><p>Training data (80%)</p></td>
</tr>
<tr>
<td><p>y_train</p></td>
<td><p>Output columns of training data</p></td>
<td><p>Output</p></td>
<td><p>Training data (80%)</p></td>
</tr>
<tr>
<td><p>X_test</p></td>
<td><p>Input columns of test data</p></td>
<td><p>Input</p></td>
<td><p>Test data (20%)</p></td>
</tr>
<tr>
<td><p>y_test</p></td>
<td><p>Output columns of test data</p></td>
<td><p>Output</p></td>
<td><p>Test data (20%)</p></td>
</tr>
</tbody>
</table>

<p>Add another Markdown cell with the following text:</p>

<pre data-type="programlisting" data-code-language="markdown">## Prepare and split data</pre>

<p>Add and run a Python code cell with the following:</p>

<pre data-type="programlisting" data-code-language="python">X = dataset[['waiver_value_tier','fantasy_regular_season_weeks_remaining',
            'league_budget_pct_remaining']] <a class="co" id="co_deploying_a_machine_learning_api_CO4-1" href="#callout_deploying_a_machine_learning_api_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
y = dataset['winning_bid_dollars'] <a class="co" id="co_deploying_a_machine_learning_api_CO4-2" href="#callout_deploying_a_machine_learning_api_CO4-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
X_train, X_test, y_train, y_test = train_test_split(X, <a class="co" id="co_deploying_a_machine_learning_api_CO4-3" href="#callout_deploying_a_machine_learning_api_CO4-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
                                                    y,
                                                    test_size = 0.20, <a class="co" id="co_deploying_a_machine_learning_api_CO4-4" href="#callout_deploying_a_machine_learning_api_CO4-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a>
                                                    random_state = 0) <a class="co" id="co_deploying_a_machine_learning_api_CO4-5" href="#callout_deploying_a_machine_learning_api_CO4-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO4-1" href="#co_deploying_a_machine_learning_api_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>You are selecting three of the input columns for X.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO4-2" href="#co_deploying_a_machine_learning_api_CO4-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>You only include the output column when creating y.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO4-3" href="#co_deploying_a_machine_learning_api_CO4-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The <code>train_test_split</code> function reads the X and y variables and then outputs the variables explained in <a data-type="xref" href="#training_variables_ch13">Table 13-2</a>.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO4-4" href="#co_deploying_a_machine_learning_api_CO4-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>This parameter determines that 20% of the data will be in the test set and 80% will be in the training set.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO4-5" href="#co_deploying_a_machine_learning_api_CO4-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>If you use the same <code>random_state</code> value each time you call this method, you will get the same rows in the train and test variables.</p></dd>
</dl>

<p>Now that you split the data, you are ready to build a model. Because you want to give a range of predictions, you will create three separate models. When you create the API, you will combine the results of these models into a single API call.</p>

<p>The process of training your model<a data-type="indexterm" data-primary="training ML models" data-secondary="ML API development" id="id2390"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="model training" id="id2391"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="model training" id="id2392"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="modeling" data-tertiary="training the model" id="id2393"/><a data-type="indexterm" data-primary="fitting a model" id="id2394"/><a data-type="indexterm" data-primary="modeling per CRISP-DM process" data-secondary="training the model" id="id2395"/> is called <em>fitting</em>, where the library takes a general algorithm and <em>fits</em> it or applies it to your training data to make a specialized model.</p>

<p>Add another Markdown cell with the following text:</p>

<pre data-type="programlisting" data-code-language="markdown">## Creating and fitting models</pre>

<p>Add and run a Python code cell with the following:</p>

<pre data-type="programlisting" data-code-language="python">model_10th_percentile = GradientBoostingRegressor(loss="quantile", alpha=0.1) <a class="co" id="co_deploying_a_machine_learning_api_CO5-1" href="#callout_deploying_a_machine_learning_api_CO5-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
model_50th_percentile = GradientBoostingRegressor(loss="quantile", alpha=0.5)
model_90th_percentile = GradientBoostingRegressor(loss="quantile", alpha=0.9)

model_10th_percentile.fit(X_train, y_train) <a class="co" id="co_deploying_a_machine_learning_api_CO5-2" href="#callout_deploying_a_machine_learning_api_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
model_50th_percentile.fit(X_train, y_train)
model_90th_percentile.fit(X_train, y_train)</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO5-1" href="#co_deploying_a_machine_learning_api_CO5-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This command creates a <code>GradientBoostingRegressor</code> model that will try to predict the 10th percentile values. In this case, this means a dollar amount that will be less than 90% of the bids. The next two statements are similar except with different percentiles.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO5-2" href="#co_deploying_a_machine_learning_api_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This statement uses<a data-type="indexterm" data-primary="fitting a model" data-secondary="fit() method" id="id2396"/> the <code>fit()</code> method to prepare this model to make predictions based on the training data you provided. The next two lines do the same for the other two models.</p></dd>
</dl>

<p>At this point, your models are in scikit_learn format<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="modeling" data-tertiary="saving model to ONNX format" id="id2397"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="model saved to ONNX" id="id2398"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="model saved to ONNX" id="id2399"/><a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" data-secondary="saving model to ONNX format" id="id2400"/><a data-type="indexterm" data-primary="modeling per CRISP-DM process" data-secondary="saving model to ONNX format" id="id2401"/> and are only available in this Jupyter Notebook. To prepare these models for deployment and make them more cross-platform compatible, you will save your models in the ONNX format. Before doing this, you’ll need to combine the features from the X variable into the two-dimensional array format required by the converter. Add another Markdown cell with the following text:</p>

<pre data-type="programlisting" data-code-language="markdown">## Convert and save these models in ONNX format</pre>

<p>Add and run a Python code cell with the following:</p>

<pre data-type="programlisting" data-code-language="python">X_array = np.column_stack((X['waiver_value_tier'],  <a class="co" id="co_deploying_a_machine_learning_api_CO6-1" href="#callout_deploying_a_machine_learning_api_CO6-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
                           X['fantasy_regular_season_weeks_remaining'],
                           X['league_budget_pct_remaining']))

onx = to_onnx(model_10th_percentile, X_array[:1]) <a class="co" id="co_deploying_a_machine_learning_api_CO6-2" href="#callout_deploying_a_machine_learning_api_CO6-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
with open("acquisition_model_10.onnx", "wb") as f:<a class="co" id="co_deploying_a_machine_learning_api_CO6-3" href="#callout_deploying_a_machine_learning_api_CO6-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
    f.write(onx.SerializeToString())

onx = to_onnx(model_50th_percentile, X_array[:1])
with open("acquisition_model_50.onnx", "wb") as f:
    f.write(onx.SerializeToString())

onx = to_onnx(model_90th_percentile, X_array[:1])
with open("acquisition_model_90.onnx", "wb") as f:
    f.write(onx.SerializeToString())</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO6-1" href="#co_deploying_a_machine_learning_api_CO6-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This statement combines the features from the X variable into the two-dimensional array format required by the convertor.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO6-2" href="#co_deploying_a_machine_learning_api_CO6-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This statement converts the first model to ONNX format. It sets the names of the input and output attributes in the model by reading the first row of the <code>X_array</code>, which contains the element names.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO6-3" href="#co_deploying_a_machine_learning_api_CO6-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This statement creates a file in the local filesystem and saves the model in ONNX format.<a data-type="indexterm" data-startref="ch13train2" id="id2402"/><a data-type="indexterm" data-startref="ch13train3" id="id2403"/><a data-type="indexterm" data-startref="ch13train4" id="id2404"/><a data-type="indexterm" data-startref="ch13train5" id="id2405"/></p></dd>
</dl>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Evaluation"><div class="sect1" id="id138">
<h1>Evaluation</h1>

<p>Planning and training models<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="evaluation of model" id="id2406"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="evaluation of model" id="id2407"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="evaluation of model" id="id2408"/><a data-type="indexterm" data-primary="evaluating a machine learning model" id="id2409"/> is an iterative process, so the model at this point likely needs improving. In a full project, you would iteratively evaluate the model with formal metrics for accuracy, fairness, and other qualities that make a model appropriate for production. At that point, you might decide to try a different combination of features and tune your model in different ways.</p>

<p>Since this chapter is focused on deploying models, you will not be performing those steps. <a data-type="indexterm" data-primary="evaluating a machine learning model" data-secondary="information online" id="id2410"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="evaluating a model" data-tertiary="information online" id="id2411"/><a data-type="indexterm" data-primary="resources online" data-secondary="model evaluation information" id="id2412"/>For more information about model evaluation, read <em>Designing Machine Learning Sytems</em> by Chip Huyen (O’Reilly, 2022).</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Deployment"><div class="sect1" id="id139">
<h1>Deployment</h1>

<p>You are are ready to deploy<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="deployment" id="ch13depl"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="deployment" id="ch13depl2"/><a data-type="indexterm" data-primary="deploying to the cloud" data-secondary="AI models" id="ch13depl4"/><a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="deployment" id="ch13depl5"/> the model for real-time inference, with one API call returning a prediction that combines all three models.</p>

<p><a data-type="xref" href="#model_serving_api_ch13">Figure 13-3</a> demonstrates the components used to create this API. If you compare this to the components of the API created in Part I of this book, you will see many similarities. <a data-type="indexterm" data-primary="FastAPI" data-secondary="ML API development" id="id2413"/>FastAPI is still used as the controller, and <a data-type="indexterm" data-primary="Pydantic" data-secondary="ML API development" id="id2414"/>Pydantic is still used for data transfer and data validation. <a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" data-secondary="ONNX runtime" data-tertiary="ML API development" id="id2415"/>However, instead of retrieving data from a database like the Part I API did, this API will use the ONNX Runtime to perform inference from the models that you trained and saved.</p>

<figure><div id="model_serving_api_ch13" class="figure">
<img src="assets/haad_1303.png" alt="Model serving API components" width="1441" height="286"/>
<h6><span class="label">Figure 13-3. </span>Model-serving API components</h6>
</div></figure>

<p>Begin by creating a Pydantic <a data-type="indexterm" data-primary="Pydantic" data-secondary="schemas" data-tertiary="ML API development" id="id2416"/><a data-type="indexterm" data-primary="building APIs" data-secondary="FastAPI" data-tertiary="schemas.py for ML API" id="id2417"/><a data-type="indexterm" data-primary="FastAPI" data-secondary="Python files" data-tertiary="schemas.py for ML API" id="id2418"/><a data-type="indexterm" data-primary="Python" data-secondary="API files" data-tertiary="schemas.py for ML API" id="id2419"/><a data-type="indexterm" data-primary="schemas.py" data-secondary="schemas.py for ML API" id="id2420"/>file named <em>schemas.py</em> to define the inputs and outputs to the API. FastAPI will use these schemas to generate the OAS file:</p>

<pre data-type="programlisting" data-code-language="shell">.../chapter13 (main) $ touch schemas.py</pre>

<p>Add the following to this file:</p>

<pre data-type="programlisting" data-code-language="python">"""Pydantic schemas"""
from pydantic import BaseModel <a class="co" id="co_deploying_a_machine_learning_api_CO7-1" href="#callout_deploying_a_machine_learning_api_CO7-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>

class FantasyAcquisitionFeatures(BaseModel): <a class="co" id="co_deploying_a_machine_learning_api_CO7-2" href="#callout_deploying_a_machine_learning_api_CO7-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
    waiver_value_tier: int
    fantasy_regular_season_weeks_remaining: int
    league_budget_pct_remaining: int

class PredictionOutput(BaseModel): <a class="co" id="co_deploying_a_machine_learning_api_CO7-3" href="#callout_deploying_a_machine_learning_api_CO7-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
    winning_bid_10th_percentile: float
    winning_bid_50th_percentile: float
    winning_bid_90th_percentile: float</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO7-1" href="#co_deploying_a_machine_learning_api_CO7-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The Pydantic library includes a <code>BaseModel</code> object that contains the validation logic.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO7-2" href="#co_deploying_a_machine_learning_api_CO7-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This class defines the input values that users will send to get a prediction from the model.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO7-3" href="#co_deploying_a_machine_learning_api_CO7-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This class defines the output that will be returned from the model. It contains three values—one from each model that you trained in the previous section.</p></dd>
</dl>

<p>Next, create the <em>main.py</em> file, which will contain the rest of the code for your API:<a data-type="indexterm" data-primary="building APIs" data-secondary="FastAPI" data-tertiary="main.py for ML API" id="ch13piem"/><a data-type="indexterm" data-primary="FastAPI" data-secondary="Python files" data-tertiary="main.py for ML API" id="ch13piem2"/><a data-type="indexterm" data-primary="main.py" data-secondary="main.py for ML API" id="ch13piem3"/><a data-type="indexterm" data-primary="Python" data-secondary="API files" data-tertiary="main.py for ML API" id="ch13piem4"/></p>

<pre data-type="programlisting" data-code-language="shell">.../chapter13 (main) $ touch main.py</pre>

<p>At the top of this file, add the imports and an API description. These will be used in the OAS file and then displayed on the Swagger UI documentation that FastAPI produces. Add this Python code:</p>

<pre data-type="programlisting" data-code-language="python">"""Fantasy acquisition API"""

from fastapi import FastAPI
import onnxruntime as rt <a class="co" id="co_deploying_a_machine_learning_api_CO8-1" href="#callout_deploying_a_machine_learning_api_CO8-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
import numpy as np
from schemas import FantasyAcquisitionFeatures, PredictionOutput  <a class="co" id="co_deploying_a_machine_learning_api_CO8-2" href="#callout_deploying_a_machine_learning_api_CO8-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>

api_description = """
This API predicts the range of costs to acquire a player in fantasy football

The endpoints are grouped into the following categories:

## Analytics
Get information about health of the API.

## Prediction
Get predictions of player acquisition cost.
"""</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO8-1" href="#co_deploying_a_machine_learning_api_CO8-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This library is used to load the models from their files and serve inferences in the API.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO8-2" href="#co_deploying_a_machine_learning_api_CO8-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This imports the Pydantic schemas, which will be used to define the inputs and outputs of the API.</p></dd>
</dl>

<p>Next, you will add the code that<a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" data-secondary="ONNX runtime" data-tertiary="main.py" id="id2421"/> uses the ONNX Runtime to load an inference session object for each of the three models. Then, these sessions are used to get labels for the input and output expected for this model. You defined the three inputs expected and the one output when you created the model in scikit-learn and then converted it to ONNX format.</p>

<p>Add the following code to the bottom of <em>main.py</em>:</p>

<pre data-type="programlisting" data-code-language="python"># Load the ONNX model
sess_10 = rt.InferenceSession("acquisition_model_10.onnx",
                             providers=["CPUExecutionProvider"]) <a class="co" id="co_deploying_a_machine_learning_api_CO9-1" href="#callout_deploying_a_machine_learning_api_CO9-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
sess_50 = rt.InferenceSession("acquisition_model_50.onnx",
                             providers=["CPUExecutionProvider"])
sess_90 = rt.InferenceSession("acquisition_model_90.onnx",
                             providers=["CPUExecutionProvider"])

# Get the input and output names of the model
input_name_10 = sess_10.get_inputs()[0].name <a class="co" id="co_deploying_a_machine_learning_api_CO9-2" href="#callout_deploying_a_machine_learning_api_CO9-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
label_name_10 = sess_10.get_outputs()[0].name <a class="co" id="co_deploying_a_machine_learning_api_CO9-3" href="#callout_deploying_a_machine_learning_api_CO9-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
input_name_50 = sess_50.get_inputs()[0].name
label_name_50 = sess_50.get_outputs()[0].name
input_name_90 = sess_90.get_inputs()[0].name
label_name_90 = sess_90.get_outputs()[0].name</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO9-1" href="#co_deploying_a_machine_learning_api_CO9-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This loads the first ONNX model from the file and creates a session object that can be used to make inferences. The next two lines do the same for the other model files.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO9-2" href="#co_deploying_a_machine_learning_api_CO9-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This statement gets the name of the input features from the session object. These will be used when making inferences.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO9-3" href="#co_deploying_a_machine_learning_api_CO9-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This statement gets the name of the output features from the session object. These will be used when making inferences.</p></dd>
</dl>

<p>Because you have placed this code outside any function definitions, it will run once at startup of the API.</p>

<p>The next section of FastAPI code<a data-type="indexterm" data-primary="FastAPI" data-secondary="ML API development" id="id2422"/> will be familiar to you if you created the API in Part I. The first statement creates the FastAPI <code>app</code> object using the API description you added previously. Then, the <code>@app.get()</code> method creates the health check. This is a useful best practice that allows users to check the status of the API before making other API calls.</p>

<p>Add the following code to the bottom of <em>main.py</em>:</p>

<pre data-type="programlisting" data-code-language="python">app = FastAPI( <a class="co" id="co_deploying_a_machine_learning_api_CO10-1" href="#callout_deploying_a_machine_learning_api_CO10-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
    description=api_description,
    title="Fantasy acquisition API",
    version="0.1",
)

@app.get( <a class="co" id="co_deploying_a_machine_learning_api_CO10-2" href="#callout_deploying_a_machine_learning_api_CO10-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
    "/",
    summary="Check to see if the Fantasy acquisition API is running",
    description="""Use this endpoint to check if the API is running. You can 
    also check it first before making other calls to be sure it's running.""",
    response_description="A JSON record with a message in it. If the API is
    running the message will say successful.",
    operation_id="v0_health_check",
    tags=["analytics"],
)
def root(): <a class="co" id="co_deploying_a_machine_learning_api_CO10-3" href="#callout_deploying_a_machine_learning_api_CO10-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
    return {"message": "API health check successful"} </pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO10-1" href="#co_deploying_a_machine_learning_api_CO10-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This creates the main FastAPI <code>app</code> object using the <code>api_description</code> defined previously.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO10-2" href="#co_deploying_a_machine_learning_api_CO10-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This is the FastAPI decorator that defines a <code>GET</code> endpoint at the root address.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO10-3" href="#co_deploying_a_machine_learning_api_CO10-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This is the function that will be excecuted when this endpoint is called. It returns a single Python statement to show that the API is running.</p></dd>
</dl>

<p>The remaining code defines the API endpoint that provides the prediction capabilities for users. It begins with a Python decorator that provides information that will end up in the OAS file (and documentation). <a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" data-secondary="ONNX runtime" data-tertiary="predict() method using" id="id2423"/>Then, it has the <code>predict()</code> method that uses the ONNX Runtime to call each model and put their outputs in the API response.</p>

<p>Add the following code to the bottom of <em>main.py</em>:</p>

<pre data-type="programlisting" data-code-language="python"># Define the prediction route
@app.post("/predict/",  <a class="co" id="co_deploying_a_machine_learning_api_CO11-1" href="#callout_deploying_a_machine_learning_api_CO11-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
          response_model=PredictionOutput,<a class="co" id="co_deploying_a_machine_learning_api_CO11-2" href="#callout_deploying_a_machine_learning_api_CO11-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
          summary="Predict the cost of acquiring a player",
          description="""Use this endpoint to predict the range of cost to 
          acquire a player in fantasy football.""",
          response_description="""A JSON record three predicted amounts. 
          Together they give a possible range of acquisition costs for a 
          player.""",
          operation_id="v0_predict",
          tags=["prediction"],
)
def predict(features: FantasyAcquisitionFeatures): <a class="co" id="co_deploying_a_machine_learning_api_CO11-3" href="#callout_deploying_a_machine_learning_api_CO11-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>
   # Convert Pydantic model to NumPy array
   input_data = np.array([[features.waiver_value_tier, <a class="co" id="co_deploying_a_machine_learning_api_CO11-4" href="#callout_deploying_a_machine_learning_api_CO11-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a>
                           features.fantasy_regular_season_weeks_remaining,
                           features.league_budget_pct_remaining]],
                           dtype=np.int64)

    pred_onx_10 = sess_10.run([label_name_10], {input_name_10: input_data})[0] <a class="co" id="co_deploying_a_machine_learning_api_CO11-5" href="#callout_deploying_a_machine_learning_api_CO11-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a>
    pred_onx_50 = sess_50.run([label_name_50], {input_name_50: input_data})[0]
    pred_onx_90 = sess_90.run([label_name_90], {input_name_90: input_data})[0]


    # Return prediction as a Pydantic response model
   return PredictionOutput(winning_bid_10th_percentile=round(
                               float(pred_onx_10[0]),2), <a class="co" id="co_deploying_a_machine_learning_api_CO11-6" href="#callout_deploying_a_machine_learning_api_CO11-6"><img src="assets/6.png" alt="6" width="12" height="12"/></a>
                           winning_bid_50th_percentile=round(
                               float(pred_onx_50[0]),2),
                           winning_bid_90th_percentile=round(
                               float(pred_onx_90[0]), 2))</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO11-1" href="#co_deploying_a_machine_learning_api_CO11-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This decorator creates a <code>POST</code> endpoint at the <em>/predict</em> address. It will be used to perform inferences.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO11-2" href="#co_deploying_a_machine_learning_api_CO11-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The <code>ResponseModel</code> statement is used by FastAPI to define the return type of this endpoint. This will be used to generate the OAS file.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO11-3" href="#co_deploying_a_machine_learning_api_CO11-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This is the function that will be called at this endpoint.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO11-4" href="#co_deploying_a_machine_learning_api_CO11-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>This statement reformats the input variables<a data-type="indexterm" data-primary="Open Neural Network Exchange (ONNX)" data-secondary="ONNX runtime" data-tertiary="main.py" id="id2424"/> into a NumPy array, which is expected by the ONNX Runtime to call the model.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO11-5" href="#co_deploying_a_machine_learning_api_CO11-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>This statement calls the ONNX Runtime and gets an inference for the 10th percentile model using the input from the API call. The next two statements use the same input to call the other two models.</p></dd>
<dt><a class="co" id="callout_deploying_a_machine_learning_api_CO11-6" href="#co_deploying_a_machine_learning_api_CO11-6"><img src="assets/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>This statement creates a <code>PredictionOutput</code> object with the inference values, <span class="keep-together">and returns</span> it in the API call. It rounds the values to two decimal places for <span class="keep-together">presentation.</span></p></dd>
</dl>

<p>With all of the API code completed,<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="deployment" data-tertiary="launching API" id="ch13laun"/><a data-type="indexterm" data-primary="launching your API" data-secondary="ML API development" id="ch13laun2"/><a data-type="indexterm" data-primary="building APIs" data-secondary="launching your API" data-tertiary="ML API development" id="ch13laun3"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="launching API" id="ch13laun4"/><a data-type="indexterm" data-primary="main.py" data-secondary="launching your API" data-tertiary="ML API" id="ch13laun5"/><a data-type="indexterm" data-primary="main.py" data-secondary="main.py for ML API" data-tertiary="launching" id="ch13laun6"/> you are ready to run the API and test out the ML model. Enter the following command from the command line:<a data-type="indexterm" data-primary="FastAPI" data-secondary="CLI" data-tertiary="fastapi run" id="id2425"/></p>

<pre data-type="programlisting" data-code-language="shell">.../chapter13 (main) $ fastapi run main.py</pre>

<p>You will see the application startup occur as shown in <a data-type="xref" href="#fast_api_run_ch13">Figure 13-4</a>.<a data-type="indexterm" data-startref="ch13piem" id="id2426"/><a data-type="indexterm" data-startref="ch13piem2" id="id2427"/><a data-type="indexterm" data-startref="ch13piem3" id="id2428"/><a data-type="indexterm" data-startref="ch13piem4" id="id2429"/></p>

<figure><div id="fast_api_run_ch13" class="figure">
<img src="assets/haad_1304.png" alt="ML model running" width="744" height="306"/>
<h6><span class="label">Figure 13-4. </span>ML Model API running</h6>
</div></figure>

<p>In Codespaces, you will also see a pop-up as shown in <a data-type="xref" href="#codespaces_api_open_browser">Figure 13-5</a>.<a data-type="indexterm" data-primary="building APIs" data-secondary="health check" data-tertiary="main.py for ML API" id="id2430"/><a data-type="indexterm" data-primary="FastAPI" data-secondary="health check" data-tertiary="main.py for ML API" id="id2431"/><a data-type="indexterm" data-primary="health check" data-secondary="main.py for ML API" id="id2432"/><a data-type="indexterm" data-primary="main.py" data-secondary="health check" data-tertiary="ML API" id="id2433"/></p>

<figure><div id="codespaces_api_open_browser" class="figure">
<img src="assets/haad_1305.png" alt="Codespaces browser window popup" width="766" height="162"/>
<h6><span class="label">Figure 13-5. </span>Codespaces browser window pop-up</h6>
</div></figure>

<p>Click “Open in Browser” to open a browser tab outside your Codespaces. This browser will show a base URL ending in <em>app.github.dev</em> that contains the response from your API running on Codespaces. You should see the following health check message in your web browser:</p>

<pre data-type="programlisting">{"message":"API health check successful"}</pre>

<p class="less_space pagebreak-before">This confirms your API is running. <a data-type="indexterm" data-primary="documentation" data-secondary="interactive documentation" data-tertiary="ML API" id="id2434"/><a data-type="indexterm" data-primary="interactive documentation" data-secondary="ML API" id="id2435"/><a data-type="indexterm" data-primary="URL of API" data-secondary="ML API" id="id2436"/>To view the interactive API documentation for your API, copy and paste the following onto the end of the base URL in your browser: <strong><code>/docs</code></strong>. For example, the full URL might be <em><a href="https://happy-pine-tree-1234-8000.app.github.dev/docs" class="bare"><em class="hyperlink">https://happy-pine-tree-1234-8000.app.github.dev/docs</em></a></em> in the browser. You should see documentation, as shown in <a data-type="xref" href="#swagger_docs_1_ch13">Figure 13-6</a>.</p>

<figure><div id="swagger_docs_1_ch13" class="figure">
<img src="assets/haad_1306.png" alt="Documentation for the ML API" width="1095" height="935"/>
<h6><span class="label">Figure 13-6. </span>Documentation for the ML API</h6>
</div></figure>

<p>To call the ML API, select<a data-type="indexterm" data-primary="CRISP-DM process" data-secondary="calling API via POST" id="id2437"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="calling API via POST" id="id2438"/><a data-type="indexterm" data-primary="building APIs" data-secondary="ML API development" data-tertiary="calling API via POST" id="id2439"/><a data-type="indexterm" data-primary="POST HTTP verb" data-secondary="calling ML API" id="id2440"/> the <code>POST</code> <em>/predict/</em> endpoint to open up that section, and then select “Try it out,” after which you should see a display as shown in <a data-type="xref" href="#swagger_docs_2_ch13">Figure 13-7</a>.</p>

<figure><div id="swagger_docs_2_ch13" class="figure">
<img src="assets/haad_1307.png" alt="Trying it out for /predict" width="1112" height="857"/>
<h6><span class="label">Figure 13-7. </span>Trying it out for /predict</h6>
</div></figure>

<p>Here is one big difference from the API you created in Part I: you are using a <code>POST</code> endpoint instead of <code>GET</code>. To make a call to a <code>POST</code> endpoint, you provide an HTTP request body in JSON format. This is where users will provide the input values to send to the API. These were automatically generated based on the <code>FantasyAcquisitionFeatures</code> Pydantic class you defined in the previous section. Update the request body with the following values:</p>

<pre data-type="programlisting" data-code-language="json">{
  "waiver_value_tier": 1,
  "fantasy_regular_season_weeks_remaining": 12,
  "league_budget_pct_remaining": 88
}</pre>

<p class="less_space pagebreak-before">Click Execute to send these values to the API. Scroll down and you should see a server response with a Code value of 200, which indicates success. The predicted values will be returned in the HTTP response body, which matches the definition of the <code>PredictionOutput</code> Pydantic class. You should see an output similar to <a data-type="xref" href="#swagger_docs_3_ch13">Figure 13-8</a>, although the predicted values may differ.<a data-type="indexterm" data-startref="ch13depl" id="id2441"/><a data-type="indexterm" data-startref="ch13depl2" id="id2442"/><a data-type="indexterm" data-startref="ch13depl4" id="id2443"/></p>

<figure><div id="swagger_docs_3_ch13" class="figure">
<img src="assets/haad_1308.png" alt="API response with prediction" width="878" height="248"/>
<h6><span class="label">Figure 13-8. </span>API response with prediction</h6>
</div></figure>

<p>To see the final structure of your project, execute the <code>tree</code> command as follows:</p>

<pre data-type="programlisting" data-code-language="shell">.../chapter13 (main) $ tree --prune -I 'build|*.egg-info|__pycache__'
.
├── acquisition_model_10.onnx
├── acquisition_model_50.onnx
├── acquisition_model_90.onnx
├── main.py
├── player_acquisition_model.ipynb
├── player_acquisition_notebook.log
├── player_training_data_full.csv
├── requirements.txt
└── schemas.py

0 directories, 9 files</pre>

<p>Congratulations! You have created the first draft of an ML model and served it with a REST API.<a data-type="indexterm" data-primary="REST (Representational State Transfer) APIs" data-secondary="AI model deployment" data-tertiary="ML AI" id="id2444"/><a data-type="indexterm" data-startref="ch13crisp" id="id2445"/><a data-type="indexterm" data-startref="ch13crisp2" id="id2446"/><a data-type="indexterm" data-startref="ch13crisp3" id="id2447"/><a data-type="indexterm" data-startref="ch13crisp4" id="id2448"/><a data-type="indexterm" data-startref="ch13depl5" id="id2449"/> <a data-type="indexterm" data-startref="ch13laun" id="id2450"/><a data-type="indexterm" data-startref="ch13laun2" id="id2451"/><a data-type="indexterm" data-startref="ch13laun3" id="id2452"/><a data-type="indexterm" data-startref="ch13laun4" id="id2453"/><a data-type="indexterm" data-startref="ch13laun5" id="id2454"/><a data-type="indexterm" data-startref="ch13laun6" id="id2455"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2456">
<h1>Documenting Machine Learning Models</h1>
<p>Because ML models<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="documentation" id="id2457"/><a data-type="indexterm" data-primary="documentation" data-secondary="machine learning models" id="id2458"/><a data-type="indexterm" data-primary="documentation" data-secondary="API documentation" data-tertiary="machine learning models" id="id2459"/> are not explicitly programmed, the full details of how they operate are not always understood, even by the people who train them. Due to the way the models operate, it is not usually possible to determine why a model makes an individual prediction or generates specific content.</p>

<p>Model providers typically provide documentation on the model to explain its intended purpose, along with known issues and limitations. The techniques for documenting ML models and applications are quickly evolving. Two methods I have come across are model cards and system cards.</p>

<p><em>Model cards</em> were proposed by Google<a data-type="indexterm" data-primary="documentation" data-secondary="machine learning models" data-tertiary="model cards" id="id2460"/><a data-type="indexterm" data-primary="model cards for documentation" id="id2461"/><a data-type="indexterm" data-primary="model cards for documentation" data-secondary="information online" id="id2462"/><a data-type="indexterm" data-primary="resources online" data-secondary="model cards information" id="id2463"/> to be transparent about an ML model’s operations, risks and biases. Google’s <a href="https://oreil.ly/PDB6C">proposal for model cards</a> gives more information about this method, and a few examples.</p>

<p>Meta proposed <em>system cards</em> to document<a data-type="indexterm" data-primary="documentation" data-secondary="machine learning models" data-tertiary="system cards" id="id2464"/><a data-type="indexterm" data-primary="system cards for documentation" id="id2465"/><a data-type="indexterm" data-primary="system cards for documentation" data-secondary="Meta description of online" id="id2466"/><a data-type="indexterm" data-primary="resources online" data-secondary="system cards description" id="id2467"/> a broader AI system that may contain multiple models. <a href="https://oreil.ly/Lx2md">Meta’s description</a> says system cards look “holistically across an AI system, versus one-off models.”</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Additional Resources"><div class="sect1" id="id140">
<h1>Additional Resources</h1>

<p>To learn more about data science projects, read <em>Practical Data Science with Python</em> by Nathan George (Packt Publishing, 2021).<a data-type="indexterm" data-primary="Practical Data Science with Python (George)" id="id2468"/><a data-type="indexterm" data-primary="George, Nathan" id="id2469"/></p>

<p>To get more experience using scikit-learn and other ML libraries, read <em>Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow</em> by Aurélien Géron (O’Reilly, 2022).<a data-type="indexterm" data-primary="Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow (Géron)" id="id2470"/><a data-type="indexterm" data-primary="Géron, Aurélien" id="id2471"/><a data-type="indexterm" data-primary="scikit-learn" data-secondary="book for hands-on learning" id="id2472"/></p>

<p>To learn more about deploying models for prediction, read <em>Designing Machine Learning Systems</em> by Chip Huyen (O’Reilly, 2022).<a data-type="indexterm" data-primary="Designing Machine Learning Systems (Huyen)" id="id2473"/><a data-type="indexterm" data-primary="Huyen, Chip" id="id2474"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2475">
<h1>Extending Your Portfolio Project</h1>
<p>Now that you have an API template<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="API development" data-tertiary="extending portfolio project" id="id2476"/><a data-type="indexterm" data-primary="portfolio projects" data-secondary="AI project introduction" data-tertiary="extending portfolio project" id="id2477"/> for serving ML models, there are many ways you can extend this project:</p>

<ul>
<li>
<p>Perform evaluation of this model to determine how effective it is at predictions. Explore tuning parameters and change the features to see if you can improve its effectiveness.</p>
</li>
<li>
<p>Create other types of models using scikit-learn and convert them to ONNX format. Use the API to deploy these new models.</p>
</li>
<li>
<p>ONNX format also supports libraries other than scikit-learn. Explore creating models with libraries such as PyTorch or XGBoost, convert them to ONNX format, and use the API to deploy these models.</p>
</li>
</ul>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id396">
<h1>Summary</h1>

<p>In this chapter, you learned about the ML lifecycle and created an ML model using scikit-learn. Then, you converted the model to ONNX format to make it compatible with more frameworks. Finally, you deployed your model using FastAPI and used it for real-time inference.</p>

<p>In <a data-type="xref" href="ch14.html#chapter_14">Chapter 14</a>, you will use generative AI to call an API using LangChain.</p>
</div></section>
</div></section></div>
</div>
</body></html>