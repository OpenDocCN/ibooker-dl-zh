- en: Chapter 19\. Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章。强化学习
- en: '*Reinforcement learning* (RL) is one of the most exciting fields of machine
    learning today, and also one of the oldest. It has been around since the 1950s,
    producing many interesting applications over the years,⁠^([1](ch19.html#id4170))
    particularly in games (e.g., *TD-Gammon*, a backgammon-playing program) and in
    machine control, but seldom making the headline news. However, a revolution took
    place in 2013, when researchers from a British startup called DeepMind⁠^([2](ch19.html#id4171))
    demonstrated a system that could learn to play just about any Atari game from
    scratch,⁠^([3](ch19.html#id4172)) eventually outperforming humans⁠^([4](ch19.html#id4173))
    in most of them, using only raw pixels as inputs and without any prior knowledge
    of the rules of the games.⁠^([5](ch19.html#id4174)) This was the first of a series
    of amazing feats:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*（RL）是当今机器学习中最激动人心的领域之一，同时也是最古老的领域之一。它自20世纪50年代以来一直存在，多年来产生了许多有趣的应用，尤其是在游戏（例如，*TD-Gammon*，一种国际象棋程序）和机器控制方面，但很少成为头条新闻。然而，在2013年发生了一场革命，当时来自一家名为DeepMind的英国初创公司的研究人员展示了一个可以从零开始学习几乎任何Atari游戏的系统，最终在大多数游戏中超过了人类，仅使用原始像素作为输入，并且没有任何关于游戏规则的先验知识。这是系列惊人壮举中的第一个：'
- en: In 2016, DeepMind’s AlphaGo beat Lee Sedol, a legendary professional player
    of the game of Go; and in 2017, it beat Ke Jie, the world champion. No program
    had ever come close to beating a master of this game, let alone the very best.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2016年，DeepMind的AlphaGo击败了传奇围棋职业选手李世石；在2017年，它又击败了世界冠军柯洁。没有任何程序曾接近击败这个游戏的宗师，更不用说最顶尖的选手了。
- en: In 2020, DeepMind released AlphaFold, which can predict the 3D shape of proteins
    with unprecedented accuracy. This is a game changer in biology, chemistry, and
    medicine. In fact, Demis Hassabis (founder and CEO) and John Jumper (director)
    were awarded the Nobel Prize in Chemistry for AlphaFold.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2020年，DeepMind发布了AlphaFold，它可以以前所未有的准确性预测蛋白质的3D形状。这在生物学、化学和医学上是一个变革性的成果。事实上，Demis
    Hassabis（创始人兼首席执行官）和John Jumper（总监）因AlphaFold获得了诺贝尔化学奖。
- en: In 2022, DeepMind released AlphaCode, which can generate code at a competitive
    programming level.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2022年，DeepMind发布了AlphaCode，它可以生成具有竞技编程水平的代码。
- en: In 2023, DeepMind released GNoME which can predict new crystal structures, including
    hundreds of thousands of predicted stable materials.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2023年，DeepMind发布了GNoME，它可以预测新的晶体结构，包括数十万种预测的稳定材料。
- en: 'So how did DeepMind researchers achieve all of this? Well, they applied the
    power of deep learning to the field of reinforcement learning, and it worked beyond
    their wildest dreams: *deep reinforcement learning* was born. Today, although
    DeepMind continues to lead the way, many other organizations have joined in, and
    the whole field is boiling with new ideas, with a wide range of applications.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，DeepMind的研究人员是如何实现这一切的呢？嗯，他们将深度学习的力量应用于强化学习领域，结果超出了他们的预期：*深度强化学习*诞生了。今天，尽管DeepMind继续引领潮流，但许多其他组织也加入了进来，整个领域充满了新的想法，应用范围广泛。
- en: 'In this chapter I will first explain what reinforcement learning is and what
    it’s good at, then present three of the most important families of techniques
    in deep reinforcement learning: policy gradients, deep Q-networks (including a
    discussion of Markov decision processes), and lastly, actor-critic methods, including
    the popular PPO, which we will use to beat an Atari game. So let’s get started!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将首先解释什么是强化学习以及它的优势，然后介绍深度强化学习中最重要的一些技术家族：策略梯度、深度Q网络（包括马尔可夫决策过程的讨论），最后是演员-评论家方法，包括流行的PPO，我们将用它来击败Atari游戏。那么，让我们开始吧！
- en: What Is Reinforcement Learning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: In reinforcement learning, a software *agent* makes *observations* and takes
    *actions* within an *environment*, and in return it receives *rewards* from the
    environment. Its objective is to learn to act in a way that will maximize its
    expected rewards over time. If you don’t mind a bit of anthropomorphism, you can
    think of positive rewards as pleasure, and negative rewards as pain (the term
    “reward” is a bit misleading in this case). In short, the agent acts in the environment
    and learns by trial and error to maximize its pleasure and minimize its pain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个软件*代理*在*环境*中做出*观察*并采取*行动*，作为回报，它从环境中获得*奖励*。其目标是学习以最大化其预期奖励的方式行事。如果你不介意有一点拟人化，你可以把正奖励看作是快乐，负奖励看作是痛苦（在这种情况下，“奖励”这个词有点误导）。简而言之，代理在环境中行动，并通过试错来学习最大化快乐和最小化痛苦。
- en: 'This is quite a broad setting that can apply to a wide variety of tasks. Here
    are a few examples (see [Figure 19-1](#rl_examples_diagram)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当广泛的设置，可以应用于各种任务。以下是一些例子（参见[图19-1](#rl_examples_diagram))：
- en: The agent can be the program controlling a robot. In this case, the environment
    is the real world, the agent observes the environment through a set of *sensors*,
    such as cameras and touch sensors, and its actions consist of sending signals
    to activate motors. It may be programmed to get positive rewards whenever it approaches
    the target destination, and negative rewards whenever it wastes time or goes in
    the wrong direction.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以是控制机器人的程序。在这种情况下，环境是现实世界，代理通过一组*传感器*（如摄像头和触觉传感器）观察环境，其动作包括发送信号以激活电机。它可以编程为在接近目标目的地时获得正奖励，而在浪费时间或走错方向时获得负奖励。
- en: The agent can be the program controlling *Ms. Pac-Man*. In this case, the environment
    is a simulation of the Atari game, the actions are the nine possible joystick
    positions (upper left, down, center, and so on), the observations are screenshots,
    and the rewards are just the game points.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以是控制*Ms. Pac-Man*的程序。在这种情况下，环境是Atari游戏的模拟，动作是九个可能的摇杆位置（左上角、向下、中心等），观察是屏幕截图，奖励只是游戏分数。
- en: Similarly, the agent can be the program playing a board game such as Go. It
    only gets a reward if it wins.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，代理可以是玩围棋等棋类游戏的程序。只有当它获胜时才会获得奖励。
- en: The agent does not have to control a physically (or virtually) moving thing.
    For example, it can be a smart thermostat, getting positive rewards whenever it
    is close to the target temperature and saves energy, and negative rewards when
    humans need to tweak the temperature, so the agent must learn to anticipate human
    needs.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理不必控制物理上（或虚拟上）移动的东西。例如，它可以是智能恒温器，在接近目标温度并节省能源时获得正奖励，当人类需要调整温度时获得负奖励，因此代理必须学会预测人类的需求。
- en: The agent can observe stock market prices and decide how much to buy or sell
    every second. Rewards are obviously the monetary gains and losses.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以观察股票市场价格，并决定每秒买卖多少。奖励显然是货币的盈亏。
- en: Note that there may not be any positive rewards at all; for example, the agent
    may move around in a maze, getting a negative reward at every time step, so it
    had better find the exit as quickly as possible! There are many other examples
    of tasks to which reinforcement learning is well suited, such as self-driving
    cars, recommender systems, placing ads on a web page, or controlling where an
    image classification system should focus its attention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可能根本没有任何正奖励；例如，代理可能在迷宫中移动，每一步都获得负奖励，所以它最好尽快找到出口！强化学习非常适合许多其他任务，例如自动驾驶汽车、推荐系统、在网页上放置广告，或控制图像分类系统应该关注的焦点。
- en: '![Reinforcement learning examples include a Mars rover, the _Ms. Pac-Man_ game,
    a Go board, a smart thermostat, and a digital stock trading interface.](assets/hmls_1901.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习示例包括火星车、Ms. Pac-Man 游戏、围棋盘、智能恒温器和数字股票交易界面。](assets/hmls_1901.png)'
- en: 'Figure 19-1\. Reinforcement learning examples: (a) robotics, (b) *Ms. Pac-Man*,
    (c) Go player, (d) thermostat, (e) automatic trader⁠^([6](ch19.html#id4189))'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-1\. 强化学习示例：（a）机器人，（b）*Ms. Pac-Man*，（c）围棋选手，（d）恒温器，（e）自动交易员⁠^([6](ch19.html#id4189))
- en: 'Let’s now turn to one large family of RL algorithms: *policy gradients*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向强化学习算法的一个大型家族：*策略梯度*。
- en: Policy Gradients
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: The algorithm a software agent uses to determine its actions is called its *policy*.
    The policy can be any algorithm you can think of, such as a neural network taking
    observations as inputs and outputting the action to take (see [Figure 19-2](#rl_with_nn_policy_diagram)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 软件代理用来确定其行为的算法被称为其*策略*。策略可以是任何你能想到的算法，例如一个以观察为输入并输出要采取的行动的神经网络（参见[图19-2](#rl_with_nn_policy_diagram)）。
- en: '![Diagram illustrating reinforcement learning with a robot agent using a neural
    network policy to interact with an environment, showing the flow of actions, rewards,
    and observations.](assets/hmls_1902.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![说明使用神经网络策略的机器人代理与环境交互的流程图，展示了动作、奖励和观察的流动。](assets/hmls_1902.png)'
- en: Figure 19-2\. Reinforcement learning using a neural network policy
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-2\. 使用神经网络策略的强化学习
- en: The policy does not even have to be deterministic. In fact, in some cases it
    does not even have to observe the environment, as long as it can get rewards!
    For example, consider a blind robotic vacuum cleaner whose reward is the amount
    of dust it picks up in 30 minutes. Its policy could be to move forward with some
    probability *p* every second, or randomly rotate left or right with probability
    1 – *p*. The rotation angle would be a random angle between –*r* and +*r*. Since
    this policy involves some randomness, it is called a *stochastic policy*. The
    robot will have an erratic trajectory, which guarantees that it will eventually
    get to any place it can reach and pick up all the dust. The question is, how much
    dust will it pick up in 30 minutes?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 策略甚至不必是确定性的。实际上，在某些情况下，它甚至不必观察环境，只要它能获得奖励！例如，考虑一个盲目的机器人吸尘器，其奖励是在30分钟内收集的灰尘量。其策略可以是每秒以概率*p*向前移动，或者以概率1
    – *p*随机左转或右转。旋转角度将是介于 –*r* 和 +*r* 之间的随机角度。由于这个策略涉及一些随机性，它被称为*随机策略*。机器人将会有一个不规则的轨迹，这保证了它最终会到达它能到达的任何地方并收集所有灰尘。问题是，它在30分钟内能收集多少灰尘？
- en: 'How would you train such a robot? There are just two *policy parameters* you
    can tweak: the probability *p* and the angle range *r*. One possible learning
    algorithm could be to try out many different values for these parameters, and
    pick the combination that performs best (see [Figure 19-3](#policy_search_diagram)).
    This is an example of *policy search*, in this case using a brute-force approach.
    When the *policy space* is too large (which is generally the case), finding a
    good set of parameters this way is like searching for a needle in a gigantic haystack.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何训练这样的机器人？你只能调整两个*策略参数*：概率*p*和角度范围*r*。一个可能的学习算法是尝试这些参数的许多不同值，并选择表现最好的组合（参见[图19-3](#policy_search_diagram)）。这是一个*策略搜索*的例子，在这种情况下使用的是穷举法。当*策略空间*太大时（这通常是情况），以这种方式找到一组好的参数就像在大堆稻草中寻找一根针。
- en: Another way to explore the policy space is to use *genetic algorithms*. For
    example, you could randomly create a first generation of 100 policies and try
    them out, then “kill” the 80 worst policies⁠^([7](ch19.html#id4198)) and make
    the 20 survivors produce 4 offspring each. An offspring is a copy of its parent⁠^([8](ch19.html#id4199))
    plus some random variation. The surviving policies plus their offspring together
    constitute the second generation. You can continue to iterate through generations
    this way until you find a good policy.⁠^([9](ch19.html#id4200))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种探索策略空间的方法是使用*遗传算法*。例如，你可以随机创建第一代100个策略并尝试它们，然后“淘汰”80个最差的策略⁠^([7](ch19.html#id4198))，让20个幸存者各自产生4个后代。后代是其父母的副本⁠^([8](ch19.html#id4199))加上一些随机变异。幸存策略及其后代共同构成了第二代。你可以继续这样迭代通过几代，直到找到好的策略。⁠^([9](ch19.html#id4200))
- en: '![Diagram illustrating four points in the policy space on the left and their
    corresponding agent behaviors on the right, demonstrating the exploration of policy
    space in reinforcement learning.](assets/hmls_1903.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![说明强化学习中策略空间左边的四个点及其右边的对应代理行为，展示了策略空间的探索。](assets/hmls_1903.png)'
- en: Figure 19-3\. Four points in the policy space (left) and the agent’s corresponding
    behavior (right)
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-3\. 策略空间中的四个点（左）和代理的对应行为（右）
- en: Yet another approach is to use optimization techniques by evaluating the gradients
    of the rewards with regard to the policy parameters, then tweaking these parameters
    by following the gradients toward higher rewards.⁠^([10](ch19.html#id4201)) Algorithms
    that follow this strategy are known as *policy gradient* (PG) algorithms. But
    before we can implement them, we first need to create an environment for the agent
    to live in⁠—so it’s time to introduce the Gymnasium library.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用优化技术，通过评估奖励相对于策略参数的梯度来调整这些参数，然后沿着梯度方向调整这些参数以获得更高的奖励。遵循这种策略的算法被称为*策略梯度*（PG）算法。但在我们能够实现它们之前，我们首先需要为智能体创建一个生存的环境——因此，现在是介绍Gymnasium库的时候了。
- en: Introduction to the Gymnasium Library
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gymnasium库简介
- en: 'One of the challenges of reinforcement learning is that in order to train an
    agent, you first need to have a working environment. If you want to program an
    agent that will learn to play an Atari game, you will need an Atari game simulator.
    If you want to program a walking robot, then the environment is the real world,
    and you can directly train your robot in that environment. However, this has its
    limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed
    up time either—adding more computing power won’t make the robot move any faster—and
    it’s generally too expensive to train 1,000 robots in parallel. In short, training
    is hard and slow in the real world, so you generally need a *simulated environment*
    at least for bootstrap training. For example, you might use a library like [PyBullet](https://pybullet.org)
    or [MuJoCo](https://mujoco.org) for 3D physics simulation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个挑战是，为了训练一个智能体，您首先需要一个工作的环境。如果您想编写一个学习玩Atari游戏的智能体，您需要一个Atari游戏模拟器。如果您想编写一个行走机器人，那么环境就是现实世界，您可以直接在那个环境中训练您的机器人。然而，这有其局限性：如果机器人从悬崖上掉下来，您不能只是点击撤销。您也不能加快时间——增加更多的计算能力不会让机器人移动得更快——并且并行训练1000个机器人通常太昂贵了。简而言之，在现实世界中训练既困难又缓慢，因此您至少需要一个*模拟环境*来进行启动训练。例如，您可能使用像[PyBullet](https://pybullet.org)或[MuJoCo](https://mujoco.org)这样的库来进行3D物理模拟。
- en: The [Gymnasium library](https://gymnasium.farama.org) is an open source toolkit
    that provides a wide variety of simulated environments (Atari games, board games,
    2D and 3D physics simulations, and so on), that you can use to train agents, compare
    them, or develop new RL algorithms. It’s the successor of OpenAI Gym, and is now
    maintained by a community of researchers and developers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[体育馆图书馆](https://gymnasium.farama.org)是一个开源工具包，它提供了各种模拟环境（如Atari游戏、棋盘游戏、2D和3D物理模拟等），您可以使用这些环境来训练智能体、比较它们或开发新的强化学习算法。它是OpenAI
    Gym的继任者，现在由一群研究人员和开发者维护。'
- en: Gymnasium is preinstalled on Colab, along with the Arcade Learning Environment
    (ALE) library `ale_py`, which is an emulator for Atari 2600 games and is required
    for all the Atari environments, as well as the Box2D library, required for several
    environments with 2D physics. If you are coding on your own machine instead of
    Colab, and you followed the installation instructions at [*https://homl.info/install-p*](https://homl.info/install-p),
    then you should be good to go.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium在Colab上预先安装，包括Arcade Learning Environment（ALE）库`ale_py`，这是一个Atari 2600游戏的模拟器，对于所有Atari环境都是必需的，以及Box2D库，它是用于几个具有2D物理的环境。如果您在自己的机器上编码而不是在Colab上，并且您遵循了[*https://homl.info/install-p*](https://homl.info/install-p)上的安装说明，那么您应该可以开始了。
- en: 'Let’s start by importing Gymnasium and making an environment:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入Gymnasium并创建一个环境开始：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we’ve created a CartPole environment (version 1). This is a 2D simulation
    in which a cart can be accelerated left or right in order to balance a pole placed
    on top of it (see [Figure 19-4](#cart_pole_diagram))—a classic control task. I’ll
    explain `render_mode` and `max_episode_steps` shortly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个CartPole环境（版本1）。这是一个2D模拟，其中一辆小车可以加速向左或向右，以平衡放在它顶部的一根杆子（参见[图19-4](#cart_pole_diagram)）——这是一个经典的控制任务。我很快会解释`render_mode`和`max_episode_steps`。
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `gym.envs.registry` dictionary contains the names and specifications of
    all the available environments. You can print a nice list with `gym.pprint_registry()`.
    The Atari environments will only be available once we start the ALE emulator.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym.envs.registry`字典包含了所有可用环境的名称和规范。您可以使用`gym.pprint_registry()`打印出一个漂亮的列表。Atari环境将只有在启动ALE模拟器后才能使用。'
- en: '![Diagram illustrating the CartPole environment with labeled vectors showing
    the cart''s position, velocity, pole angle, and angular velocity.](assets/hmls_1904.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![说明CartPole环境并带有标注向量的图，显示小车的位置、速度、杆角度和角速度。](assets/hmls_1904.png)'
- en: Figure 19-4\. The CartPole environment
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-4\. 小车杆环境
- en: 'After the environment is created, you must initialize it using the `reset()`
    method, optionally specifying a random seed. This returns the first observation.
    Observations depend on the type of environment. For the CartPole environment,
    each observation is a NumPy array containing four floats representing the cart’s
    horizontal position (`0.0` = center), its velocity (positive means right), the
    angle of the pole (`0.0` = vertical), and its angular velocity (positive means
    clockwise). The `reset()` method also returns a dictionary that may contain extra
    environment-specific information. This can be useful for debugging and sometimes
    for training. For example, in many Atari environments, it contains the number
    of lives left. However, in the CartPole environment, this dictionary is empty:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 创建环境后，您必须使用 `reset()` 方法对其进行初始化，可选地指定一个随机种子。这将返回第一个观察结果。观察结果取决于环境类型。对于CartPole环境，每个观察结果都是一个包含四个浮点数的NumPy数组，分别表示小车的水平位置（`0.0`
    = 中心）、速度（正值表示向右）、杆的角度（`0.0` = 垂直）和角速度（正值表示顺时针）。`reset()` 方法还返回一个可能包含额外环境特定信息的字典。这可以用于调试，有时也用于训练。例如，在许多Atari环境中，它包含剩余的生命数。然而，在CartPole环境中，此字典为空：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s call the `render()` method to render this environment as an image. Since
    we set `render_mode="rgb_array"` when creating the environment, the image will
    be returned as a NumPy array (you can then use Matplotlib’s `imshow()` function
    to display this image):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用 `render()` 方法将此环境渲染为图像。由于我们在创建环境时设置了 `render_mode="rgb_array"`，因此图像将以NumPy数组的形式返回（然后您可以使用Matplotlib的
    `imshow()` 函数来显示此图像）：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let’s ask the environment what actions are possible:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们询问环境可能有哪些动作：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Discrete(2)` means that the possible actions are integers 0 and 1, which represent
    accelerating left or right. Other environments may have additional discrete actions,
    or other kinds of actions (e.g., continuous). Since the pole is leaning toward
    the right (`obs[2] > 0`), let’s accelerate the cart toward the right:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discrete(2)` 表示可能的动作是整数0和1，分别代表向左或向右加速。其他环境可能有额外的离散动作，或其他类型的动作（例如连续动作）。由于杆向右倾斜
    (`obs[2] > 0`)，让我们加速小车向右：'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `step()` method executes the desired action and returns five values:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()` 方法执行所需的动作并返回五个值：'
- en: '`obs`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`obs`'
- en: This is the new observation. The cart is now moving toward the right (`obs[1]
    > 0`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular
    velocity is now negative (`obs[3] < 0`), so it will likely be tilted toward the
    left after the next step.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一条新的观察。小车现在正向右移动 (`obs[1] > 0`)。杆仍然向右倾斜 (`obs[2] > 0`)，但其角速度现在是负的 (`obs[3]
    < 0`)，因此它很可能会在下一步后向左倾斜。
- en: '`reward`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`reward`'
- en: In this environment, you get a reward of 1.0 at every step, no matter what you
    do, so the goal is to keep the episode running for as long as possible. An *episode*
    is one run of the environment until the game is over or interrupted.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，无论您做什么，每一步都会获得1.0的奖励，因此目标是尽可能长时间地保持剧集运行。一个 *剧集* 是环境的一次运行，直到游戏结束或中断。
- en: '`done`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`done`'
- en: This value will be `True` when the episode is over. This will happen when the
    pole tilts too much, or goes off the screen. After that, the environment must
    be reset before it can be used again.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当剧集结束时，此值将为 `True`。这将在杆倾斜过多或超出屏幕时发生。之后，必须重置环境才能再次使用。
- en: '`truncated`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`truncated`'
- en: This value will be `True` when an episode is interrupted early, typically by
    an environment wrapper that imposes a maximum number of steps per episode (see
    Gymnasium’s documentation for more details on environment wrappers). By default,
    the environment specification for CartPole sets the maximum number of steps to
    500, but we changed this to 1,000 when we created the environment. Some RL algorithms
    treat truncated episodes differently from episodes finished normally (i.e., when
    `done` is `True`), but in this chapter we will treat them identically.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个剧集提前中断时，此值将为 `True`，通常是由施加每个剧集最大步数的环境包装器引起的（有关环境包装器的更多详细信息，请参阅Gymnasium的文档）。默认情况下，CartPole的环境规范将最大步数设置为500，但我们在创建环境时将其更改为1000。一些强化学习算法将截断剧集与正常完成的剧集（即
    `done` 为 `True`）区别对待，但在本章中我们将它们视为相同。
- en: '`info`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`info`'
- en: This environment-specific dictionary may provide extra information, just like
    the one returned by the `reset()` method.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定环境的字典可能提供额外信息，就像`reset()`方法返回的信息一样。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Once you have finished using an environment—possibly after many episodes—you
    should call its `close()` method to free resources.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了一个环境的使用——可能是在许多回合之后——你应该调用它的`close()`方法来释放资源。
- en: 'Let’s hardcode a simple policy that accelerates left when the pole is leaning
    toward the left and accelerates right when the pole is leaning toward the right.
    We will run this policy to see the average rewards it gets over 500 episodes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们硬编码一个简单的策略，当杆子向左倾斜时加速向左，当杆子向右倾斜时加速向右。我们将运行这个策略，看看它在500个回合中获得的平均奖励：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code is self-explanatory. Let’s look at the result:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是自我解释的。让我们看看结果：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Even with 500 tries, this policy never managed to keep the pole upright for
    more than 63 consecutive steps. Not great. If you look at the simulation in this
    chapter’s notebook, you will see that the cart oscillates left and right more
    and more strongly until the pole tilts too much. A neural network can do better!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 即使尝试了500次，这项政策也从未能将杆子保持直立超过63个连续步骤。这并不理想。如果你查看本章笔记本中的模拟，你会看到小车左右摆动越来越剧烈，直到杆子倾斜过度。神经网络可以做得更好！
- en: Neural Network Policies
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络策略
- en: Let’s create a neural network policy. This neural network will take an observation
    as input, and it will output the action to be executed, just like the policy we
    hardcoded earlier. More precisely, it will estimate a probability for each action,
    then it will select an action randomly, according to the estimated probabilities
    (see [Figure 19-5](#neural_network_policy_diagram)). In the case of the CartPole
    environment, there are just two possible actions (left or right), so we only need
    one output neuron. It will output the probability *p* of action 1 (right), and
    of course the probability of action 0 (left) will be 1 – *p*. For example, if
    it outputs 0.7, then we will pick action 1 with 70% probability, or action 0 with
    30% probability (this is a *Bernoulli distribution* with *p* = 0.7).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个神经网络策略。这个神经网络将观察作为输入，并将输出要执行的操作，就像我们之前硬编码的策略一样。更确切地说，它将为每个操作估计一个概率，然后根据估计的概率随机选择一个操作（参见[图19-5](#neural_network_policy_diagram)）。在CartPole环境中，只有两个可能的操作（左或右），所以我们只需要一个输出神经元。它将输出操作1（右）的概率*p*，当然操作0（左）的概率将是1
    – *p*。例如，如果它输出0.7，那么我们将以70%的概率选择操作1，或者以30%的概率选择操作0（这是一个*p* = 0.7的伯努利分布）。
- en: '![Diagram illustrating a neural network policy where observations are processed
    through hidden layers to generate a probability for action, leading to random
    action sampling.](assets/hmls_1905.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![说明神经网络策略的图，其中观察通过隐藏层处理以生成动作的概率，从而进行随机动作采样。](assets/hmls_1905.png)'
- en: Figure 19-5\. Neural network policy
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-5\. 神经网络策略
- en: 'You may wonder why we are picking a random action based on the probabilities
    given by the neural network, rather than just picking the action with the highest
    score. This approach lets the agent find the right balance between *exploring*
    new actions and *exploiting* the actions that are known to work well. Here’s an
    analogy: suppose you go to a restaurant for the first time, and all the dishes
    look equally appealing, so you randomly pick one. If it turns out to be good,
    you can increase the probability that you’ll order it next time, but you shouldn’t
    increase that probability up to 100%, or you will never try the other dishes,
    some of which may be even better than the one you tried. This *exploration*/*exploitation
    dilemma* is central in reinforcement learning.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，为什么我们根据神经网络给出的概率选择随机动作，而不是只选择得分最高的动作。这种方法让智能体在探索新动作和利用已知效果良好的动作之间找到正确的平衡。这里有一个类比：假设你第一次去餐厅，所有的菜看起来都同样吸引人，所以你随机选择一个。如果它结果很好，你可以增加下次点这个菜的概率，但你不应该将这个概率增加到100%，否则你将永远无法尝试其他菜肴，其中一些可能比你所尝试的更好。这种*探索*/*利用*的困境是强化学习中的核心问题。
- en: Also note that in this particular environment, the past actions and observations
    can safely be ignored, since each observation contains the environment’s full
    state. If there were some hidden state, then you might need to consider past actions
    and observations as well. For example, if the environment only revealed the position
    of the cart but not its velocity, you would have to consider not only the current
    observation but also the previous observation in order to estimate the current
    velocity. Another example is when the observations are noisy; in that case, you
    generally want to use the past few observations to estimate the most likely current
    state. The CartPole problem is thus as simple as can be; the observations are
    noise-free, and they contain the environment’s full state.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在这个特定的环境中，过去的行为和观察可以安全地忽略，因为每个观察都包含了环境的完整状态。如果有某些隐藏状态，那么你可能需要考虑过去的行为和观察。例如，如果环境只揭示了滑车的位置但没有其速度，那么你必须考虑当前观察以及之前的观察，以便估计当前速度。另一个例子是当观察有噪声时；在这种情况下，你通常想使用过去几项观察来估计最可能的状态。因此，CartPole问题是最简单的；观察是无噪声的，并且它们包含了环境的完整状态。
- en: 'Let’s use PyTorch to implement a basic neural network policy for CartPole:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用PyTorch来实现一个基本的神经网路策略，用于CartPole：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Our policy network is a tiny MLP, since it’s a fairly simple task. The number
    of inputs is the size of the environment’s state: in the case of CartPole, it
    is just the size of a single observation, which is four. We have just one hidden
    layer with five units (no need for more in this case). Finally, we want to output
    a single probability, so we have a single output neuron. If there were more than
    two possible actions, there would be one output neuron per action instead. For
    performance and numerical stability, we don’t add a sigmoid function at the end,
    so the network will actually output logits rather than probabilities.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的政策网络是一个小型MLP，因为这个任务相对简单。输入的数量是环境状态的尺寸：在CartPole的情况下，它只是单个观察的尺寸，即四个。我们只有一个包含五个单元的隐藏层（在这种情况下不需要更多）。最后，我们希望输出一个概率，所以我们有一个输出神经元。如果有超过两个可能的行为，那么每个行为将有一个输出神经元。为了性能和数值稳定性，我们不在最后添加sigmoid函数，因此网络实际上会输出logits而不是概率。
- en: 'Next let’s define a function that will use this policy network to choose an
    action:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们定义一个函数，它将使用这个策略网络来选择一个动作：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The function takes a single observation, converts it to a tensor, and passes
    it to the policy network to get the logit for action 1 (right). It then creates
    a `Bernoulli` probability distribution with this logit, and it samples an action
    from it: this distribution will output 1 (right) with probability *p* = exp(logit)
    / (1 + exp(logit)), and 0 (left) with probability 1 – *p*. If there were more
    than two possible actions, you would use a `Categorical` distribution instead.
    Lastly, we compute the log probability of the sampled action (i.e., either log(*p*)
    or log(1 – *p*)): this log probability will be needed later for training.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受单个观察，将其转换为张量，并将其传递给策略网络以获取动作1（向右）的logit。然后它使用这个logit创建一个`Bernoulli`概率分布，并从中采样一个动作：这个分布将以概率
    *p* = exp(logit) / (1 + exp(logit)) 输出1（向右），以概率 1 – *p* 输出0（向左）。如果有超过两个可能的行为，你将使用一个`Categorical`分布代替。最后，我们计算采样动作的对数概率（即log(*p*)或log(1
    – *p*)）：这个对数概率将在训练时被需要。
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the action space is continuous, you can use a Gaussian distribution instead
    of a Bernoulli or categorical distribution. Instead of predicting logits, the
    policy network must predict the mean and standard deviation (or the log of the
    standard deviation) of the distribution. The log of the standard deviation is
    often clipped to ensure the distribution is neither too wide nor too narrow.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作空间是连续的，你可以使用高斯分布而不是Bernoulli或Categorical分布。策略网络必须预测分布的均值和标准差（或标准差的对数）。标准差的对数通常会被截断，以确保分布既不太宽也不太窄。
- en: OK, we now have a neural network policy that can take an environment state (in
    this case, a single observation) and choose an action. But how do we train it?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在有一个神经网络策略，它可以接受环境状态（在这种情况下，单个观察）并选择一个动作。但我们是怎样训练它的呢？
- en: 'Evaluating Actions: The Credit Assignment Problem'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估动作：信用分配问题
- en: 'If we knew what the best action was at each step, we could train the neural
    network as usual by minimizing the cross-entropy between the estimated probability
    distribution and the target probability distribution. It would just be regular
    supervised learning. However, in reinforcement learning the only guidance the
    agent gets is through rewards, and rewards are typically sparse and delayed. For
    example, if the agent manages to balance the pole for a total of 100 steps, how
    can it know which of the 100 actions it took were good, and which of them were
    bad? All it knows is that the pole fell after the last action, but surely this
    last action is not entirely responsible. This is called the *credit assignment
    problem*: when the agent gets a reward (or a penalty), it is hard for it to know
    which actions should get credited (or blamed) for it. Think of a dog that gets
    rewarded hours after it behaved well; will it understand what it is being rewarded
    for?'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道在每一步的最佳动作是什么，我们可以通过最小化估计概率分布与目标概率分布之间的交叉熵来像往常一样训练神经网络。这将是常规的监督学习。然而，在强化学习中，智能体得到的唯一指导是通过奖励，而奖励通常是稀疏和延迟的。例如，如果一个智能体成功平衡杆100步，它如何知道它所采取的100个动作中哪些是好的，哪些是坏的？它所知道的就是在最后一步后杆子倒了，但显然这一步并不完全负责。这被称为**信用分配问题**：当智能体获得奖励（或惩罚）时，它很难知道哪些动作应该得到认可（或责备）。想象一下一只狗在表现良好数小时后得到奖励；它会明白自己为什么被奖励吗？
- en: To simplify credit assignment, a common strategy is to evaluate an action based
    on the sum of all the rewards that come after it, applying a *discount factor,
    _γ* (gamma), at each step. This sum of discounted rewards is called the action’s
    *return*. Consider the example in [Figure 19-6](#discounted_rewards_diagram).
    If an agent decides to go right three times in a row and gets +10 reward after
    the first step, 0 after the second step, and finally –50 after the third step,
    then assuming we use a discount factor *γ* = 0.8, the first action will have a
    return of 10 + *γ* × 0 + *γ*² × (–50) = –22.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化信用分配，一个常见的策略是评估一个动作基于其之后所有奖励的总和，并在每一步应用一个**折现因子**，_γ*（伽马）。这个折现奖励的总和被称为动作的**回报**。考虑[图19-6](#discounted_rewards_diagram)中的例子。如果一个智能体连续三次选择向右移动，并在第一步后获得+10的奖励，第二步后获得0，最后在第三步后获得-50，那么如果我们使用折现因子
    *γ* = 0.8，第一个动作的回报将是 10 + *γ* × 0 + *γ*² × (–50) = –22。
- en: '![Diagram showing a robot earning sequential rewards of +10, 0, and -50 as
    it moves right, with calculated discounted returns of -22 and -40, highlighting
    the effect of an 80% discount factor.](assets/hmls_1906.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![展示机器人向右移动获得+10、0和-50的连续奖励，并计算折现回报为-22和-40的图表，突出80%折现因子的影响。](assets/hmls_1906.png)'
- en: 'Figure 19-6\. Computing an action’s return: the sum of discounted future rewards'
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-6\. 计算动作的回报：折现未来奖励的总和
- en: 'The following function computes the returns, given the rewards and the discount
    factor:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数计算回报，给定奖励和折现因子：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This function produces the expected result:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数产生预期的结果：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If the discount factor is close to 0, then future rewards won’t count for much
    compared to immediate rewards. Conversely, if the discount factor is close to
    1, then rewards far into the future will count almost as much as immediate rewards.
    Typical discount factors vary from 0.9 to 0.99\. With a discount factor of 0.95,
    rewards 13 steps into the future count roughly for half as much as immediate rewards
    (since 0.95^(13) ≈ 0.5), while with a discount factor of 0.99, rewards 69 steps
    into the future count for half as much as immediate rewards. In the CartPole environment,
    actions have fairly short-term effects, so choosing a low discount factor of 0.95
    seems reasonable, and it will help with credit assignment, making training faster
    and more stable. However, if the discount factor is set too low, then the agent
    will learn a suboptimal strategy, focusing too much on short-term gains.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果折现因子接近0，那么与即时奖励相比，未来的奖励不会太多。相反，如果折现因子接近1，那么远期奖励将几乎与即时奖励一样重要。典型的折现因子范围从0.9到0.99。以0.95的折现因子为例，13步后的奖励大约相当于即时奖励的一半（因为0.95^(13)
    ≈ 0.5），而以0.99的折现因子为例，69步后的奖励相当于即时奖励的一半。在CartPole环境中，动作有相当短期的效果，因此选择一个较低的折现因子0.95似乎是合理的，并且它将有助于信用分配，使训练更快更稳定。然而，如果折现因子设置得太低，那么智能体将学会次优策略，过分关注短期收益。
- en: Now that we have a way to evaluate each action, we are ready to train our first
    agent using policy gradients. Let’s see how.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了评估每个动作的方法，我们就可以使用策略梯度来训练我们的第一个智能体了。让我们看看怎么做。
- en: Solving the CartPole Using Policy Gradients
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用策略梯度解决CartPole问题
- en: 'As discussed earlier, policy gradient algorithms optimize the parameters of
    a policy by following the gradients toward higher rewards. One popular PG algorithm,
    called *REINFORCE* (or *Monte Carlo PG*), was [introduced back in 1992 by Ronald
    Williams](https://homl.info/132).⁠^([11](ch19.html#id4223)) It has many variants,
    with various tweaks, but the general principle is this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，策略梯度算法通过跟随梯度指向更高的奖励来优化策略的参数。一个流行的PG算法，称为*REINFORCE*（或*蒙特卡洛PG*），是由Ronald
    Williams在1992年[首次提出](https://homl.info/132)。⁠^([11](ch19.html#id4223)) 它有许多变体，各种调整，但基本原则是这样的：
- en: First, let the neural network policy play the game for an episode, and record
    the rewards and estimated log probabilities.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让神经网络策略玩一个游戏，并记录奖励和估计的对数概率。
- en: Then compute each action’s return, using the function defined in the previous
    section.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用上一节中定义的函数计算每个动作的回报。
- en: 'If an action’s return is positive, it means that the action was probably good,
    and you want to make this action even more likely to be chosen in the future.
    Conversely, if an action’s return is negative, you want to make this action *less*
    likely. To achieve this, you can minimize the REINFORCE loss defined in [Equation
    19-1](#reinforce_equation): this will maximize the expected discounted rewards.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个动作的回报是正的，这意味着这个动作可能是好的，你希望这个动作在未来更有可能被选择。相反，如果一个动作的回报是负的，你希望这个动作*更少*可能。为了实现这一点，你可以最小化[方程19-1](#reinforce_equation)中定义的REINFORCE损失：这将最大化期望的折现回报。
- en: Equation 19-1\. REINFORCE loss
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程19-1\. REINFORCE损失
- en: <mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>t</mi></munder> <mrow><mo form="prefix">log</mo>
    <msub><mi>π</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>·</mo> <msub><mi>r</mi> <mi>t</mi></msub></mrow></mrow>
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi mathvariant="bold">θ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <munder><mo>∑</mo> <mi>t</mi></munder> <mrow><mo form="prefix">log</mo>
    <msub><mi>π</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo> <msub><mi>a</mi>
    <mi>t</mi></msub> <mo>|</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>·</mo> <msub><mi>r</mi> <mi>t</mi></msub></mrow></mrow>
- en: In this equation, *π*[**θ**](*a*[*t*]|*s*[*t*]) is the policy network’s estimated
    probability for action *a*[*t*], given state *s*[*t*] (where *t* is the time step),
    and *r*[*t*] is the observed return of this action; **θ** represents the model
    parameters.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*π*[**θ**](*a*[*t*]|*s*[*t*])是策略网络的估计概率，表示在状态*s*[*t*]（其中*t*是时间步）下动作*a*[*t*]的概率，*r*[*t*]是观察到的这个动作的回报；**θ**代表模型参数。
- en: 'Let’s use PyTorch to implement this algorithm. First, we need a function to
    let the policy network play an episode, and record the rewards and log probabilities:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用PyTorch来实现这个算法。首先，我们需要一个函数来让策略网络玩一个游戏，并记录奖励和对数概率：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The function first resets the environment to start a new episode. For reproducibility,
    we pass a seed to the `reset()` method. Then comes the game loop: at each iteration,
    we pass the current environment state (i.e., the last observation) to the `choose_action()`
    method we defined earlier. It returns the chosen action and its log probability.
    We then call the environment’s `step()` method to execute the action. This returns
    a new observation (a NumPy array), a reward, two booleans indicating whether the
    game is over or truncated, and an info dict (which we can safely ignore in the
    case of CartPole). We record the log probabilities and rewards in two lists, which
    we return when the episode is over.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先将环境重置以开始一个新的游戏。为了可重复性，我们向`reset()`方法传递一个种子。然后是游戏循环：在每次迭代中，我们将当前环境状态（即最后的观察）传递给之前定义的`choose_action()`方法。它返回所选动作及其对数概率。然后我们调用环境的`step()`方法来执行动作。这返回一个新的观察（NumPy数组）、一个奖励、两个布尔值表示游戏是否结束或截断，以及一个info字典（在CartPole的情况下我们可以安全地忽略它）。我们记录对数概率和奖励在两个列表中，当游戏结束时返回这些列表。
- en: 'We can finally write the training function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终可以编写训练函数：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That’s nice and short, isn’t it? At each training iteration, the function runs
    an episode and gets the log probabilities and rewards.⁠^([12](ch19.html#id4225))
    Then it computes the return for each action. Next, it standardizes the returns
    (i.e., it subtracts the mean return and divides by the standard deviation, plus
    a small value to avoid division by zero). This standardization step is optional
    but it’s a common and recommended tweak to the REINFORCE algorithm, as it stabilizes
    training. Next, the function computes the REINFORCE loss using [Equation 19-1](#reinforce_equation),
    and it performs an optimizer step to minimize the loss.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，也很简洁，不是吗？在每次训练迭代中，函数运行一个回合并获取对数概率和奖励。⁠^([12](ch19.html#id4225)) 然后，它计算每个动作的回报。接下来，它标准化回报（即，它减去平均回报并除以标准差，再加上一个小的值以避免除以零）。这个标准化步骤是可选的，但它是REINFORCE算法的一个常见且推荐的调整，因为它可以稳定训练。接下来，函数使用[方程式19-1](#reinforce_equation)计算REINFORCE损失，并执行优化器步骤以最小化损失。
- en: That’s it, we’re ready to build and train a policy network!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们已经准备好构建和训练策略网络了！
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training will take less than a minute. If you run an episode using this policy
    network, you will see that it perfectly balances the pole. Success!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练将不到一分钟。如果你使用这个策略网络运行一个回合，你会看到它完美地平衡了杆。成功！
- en: The simple policy gradients algorithm we just trained solved the CartPole task,
    but it would not scale well to larger and more complex tasks. Indeed, it is highly
    *sample inefficient*, meaning it needs to explore the game for a very long time
    before it can make significant progress. This is because its return estimates
    are extremely noisy, especially when good actions are mixed with bad ones. However,
    it is the foundation of more powerful algorithms, such as *actor-critic* algorithms
    (which we will discuss at the end of this chapter).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚训练的简单策略梯度算法解决了CartPole任务，但它不会很好地扩展到更大和更复杂的任务。事实上，它非常*样本低效*，这意味着它需要探索游戏很长时间才能取得显著的进步。这是因为它的回报估计非常嘈杂，尤其是在好的动作和坏的动作混合在一起时。然而，它是更强大算法的基础，例如*演员-评论家*算法（我们将在本章末尾讨论）。
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Researchers try to find algorithms that work well even when the agent initially
    knows nothing about the environment. However, unless you are writing a paper,
    you should not hesitate to inject prior knowledge into the agent, as it will speed
    up training dramatically. For example, since you know that the pole should be
    as vertical as possible, you could add negative rewards proportional to the pole’s
    angle. This will make the rewards much less sparse and speed up training. Also,
    if you already have a reasonably good policy (e.g., hardcoded), you may want to
    train the neural network to imitate it before using policy gradients to improve
    it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 研究者们试图寻找即使在智能体最初对环境一无所知的情况下也能良好工作的算法。然而，除非你正在撰写论文，否则你不应该犹豫将先验知识注入智能体中，因为这会显著加快训练速度。例如，既然你知道杆应该尽可能垂直，你可以添加与杆角度成比例的负奖励。这将使奖励更加稀疏，并加快训练速度。此外，如果你已经有一个相当不错的策略（例如，硬编码），在使用策略梯度改进它之前，你可能想要训练神经网络来模仿它。
- en: 'Moreover, the REINFORCE algorithm is quite unstable: the agent may improve
    for a while during training, then forget everything catastrophically, learn again,
    forget, learn, etc. It’s a roller coaster. This is in large part because the training
    samples are not independent and identically distributed (IID); indeed, the training
    samples consist of whatever states the agent is capable of reaching right now.
    As the agent progresses, it explores different parts of the environment, and it
    can forget everything about other parts. For example, once it learns to properly
    hold the pole upright, it will no longer see nonvertical poles, and it will totally
    forget how to handle them. And this issue gets much worse with more complex environments.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，REINFORCE算法相当不稳定：智能体在训练过程中可能会进步一段时间，然后突然忘记一切，再次学习，忘记，学习，等等。这是一段过山车般的经历。这在很大程度上是因为训练样本不是独立同分布的（IID）；事实上，训练样本包括智能体现在能够达到的任何状态。随着智能体的进步，它会探索环境的不同部分，并且它可能会忘记其他部分的一切。例如，一旦它学会了正确地保持杆竖直，它就再也不会看到非垂直的杆，并且它将完全忘记如何处理它们。而且这个问题在更复杂的环境中会变得更加严重。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Reinforcement learning is notoriously difficult, largely because of the training
    instabilities and the huge sensitivity to the choice of hyperparameter values
    and random seeds.⁠^([13](ch19.html#id4234)) As the researcher Andrej Karpathy
    put it, “[Supervised learning] wants to work. […​] RL must be forced to work”.
    You will need time, patience, perseverance, and perhaps a bit of luck, too. This
    is a major reason RL is not as widely adopted as regular deep learning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习因其训练的不稳定性和对超参数值和随机种子选择的巨大敏感性而闻名，这是一个非常困难的领域。⁠^([13](ch19.html#id4234)) 如研究者安德烈·卡帕西所说，“[监督学习]希望工作。[……]
    RL必须被迫工作”。你需要时间、耐心、毅力，也许还需要一点运气。这也是强化学习不像常规深度学习那样被广泛采用的主要原因。
- en: 'We will now look at another popular family of algorithms: *value-based methods*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨另一组流行的算法：*基于价值的算法*。
- en: Value-Based Methods
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于价值的算法
- en: 'Whereas PG algorithms directly try to optimize the policy to increase rewards,
    value-based methods are less direct: the agent learns to estimate the value of
    each state (i.e., the expected return), or the value of each action in a given
    state, then it uses this knowledge to decide how to act. To understand these algorithms,
    we must first discuss *Markov decision processes* (MDPs).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与PG算法直接尝试优化策略以增加奖励不同，基于价值的算法更为间接：智能体学习估计每个状态的价值（即预期的回报），或者给定状态下每个动作的价值，然后它使用这些知识来决定如何行动。为了理解这些算法，我们首先必须讨论*马尔可夫决策过程*（MDP）。
- en: Markov Decision Processes
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In the early 20th century, the mathematician Andrey Markov studied stochastic
    processes with no memory, called *Markov chains*. Such a process has a fixed number
    of states, and it randomly evolves from one state to another at each step. The
    probability for it to evolve from a state *s* to a state *s*′ is fixed, and it
    depends only on the pair (*s*, *s*′), not on past states. This is why we say that
    the system has no memory.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪初，数学家安德烈·马尔可夫研究了没有记忆的随机过程，称为*马尔可夫链*。这样的过程具有固定数量的状态，并且它在每一步随机地从一种状态演变到另一种状态。从状态
    *s* 到状态 *s*′ 的演变概率是固定的，并且它只依赖于对 (*s*, *s*′)，而不依赖于过去的状态。这就是为什么我们说系统没有记忆。
- en: '[Figure 19-7](#markov_chain_diagram) shows an example of a Markov chain with
    four states.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图19-7](#markov_chain_diagram) 展示了一个具有四个状态的马尔可夫链的示例。'
- en: '![Diagram of a Markov chain with four states, illustrating transitions and
    probabilities between states including a terminal state.](assets/hmls_1907.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![具有四个状态的马尔可夫链图，展示了状态之间的转移和概率，包括一个终止状态。](assets/hmls_1907.png)'
- en: Figure 19-7\. Example of a Markov chain
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-7\. 马尔可夫链的示例
- en: 'Suppose that the process starts in state *s*[0], and there is a 70% chance
    that it will remain in that state at the next step. Eventually it is bound to
    leave that state and never come back, because no other state points back to *s*[0].
    If it goes to state *s*[1], it will then most likely go to state *s*[2] (90% probability),
    then immediately back to state *s*[1] (with 100% probability). It may alternate
    a number of times between these two states, but eventually it will fall into state
    *s*[3] and remain there forever, since there’s no way out: this is called a *terminal
    state*. Markov chains can have very different dynamics, and they are frequently
    used in thermodynamics, chemistry, statistics, and much more.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设过程从状态 *s*[0] 开始，有70%的几率在下一步仍然处于该状态。最终它必然会离开该状态并且永远不再回来，因为没有其他状态指向 *s*[0]。如果它进入状态
    *s*[1]，那么它接下来最有可能进入状态 *s*[2]（90%的概率），然后立即回到状态 *s*[1]（100%的概率）。它可能在这两个状态之间交替多次，但最终它会陷入状态
    *s*[3] 并永远停留在这里，因为没有出路：这被称为*终止状态*。马尔可夫链可以具有非常不同的动态，并且它们在热力学、化学、统计学等领域被广泛使用。
- en: 'Markov decision processes were first described in the 1950s by [Richard Bellman](https://homl.info/133).⁠^([14](ch19.html#id4239))
    They resemble Markov chains, but with a twist: at each step, an agent can choose
    one of several possible actions, and the transition probabilities depend on the
    chosen action. Moreover, some state transitions return some reward (positive or
    negative), and the agent’s goal is to find a policy that will maximize its cumulative
    reward over time.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程最早在20世纪50年代由[理查德·贝尔曼](https://homl.info/133)描述。⁠^([14](ch19.html#id4239))
    它们类似于马尔可夫链，但有一个转折点：在每一步，一个智能体可以选择几种可能的行为之一，并且转移概率取决于所选择的行为。此外，某些状态转移会返回一些奖励（正面或负面），智能体的目标是找到一个策略，使其在时间上的累积奖励最大化。
- en: For example, the MDP represented in [Figure 19-8](#mdp_diagram) has three states
    (represented by circles) and up to three possible discrete actions at each step
    (represented by diamonds).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图19-8](#mdp_diagram) 所示的MDP有三个状态（用圆圈表示）和每个步骤最多三个可能的离散行动（用菱形表示）。
- en: '![Diagram of a Markov decision process with three states (circles) and discrete
    actions (diamonds), showing transitions and associated rewards or penalties.](assets/hmls_1908.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![具有三个状态（圆圈）和离散行动（菱形）的马尔可夫决策过程图，显示转换和相关的奖励或惩罚。](assets/hmls_1908.png)'
- en: Figure 19-8\. Example of a Markov decision process
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-8. 马尔可夫决策过程的示例
- en: 'If it starts in state *s*[0], the agent can choose among actions *a*[0], *a*[1],
    or *a*[2]. If it chooses action *a*[1], it just remains in state *s*[0] with certainty
    and without any reward. It can thus decide to stay there forever if it wants to.
    But if it chooses action *a*[0], it has a 70% probability of gaining a reward
    of +10 and remaining in state *s*[0]. It can then try again and again to gain
    as much reward as possible, but at one point it is going to end up instead in
    state *s*[1]. In state *s*[1] it has only two possible actions: *a*[0] or *a*[2].
    It can choose to stay put by repeatedly choosing action *a*[0], or it can choose
    to move on to state *s*[2] and get a negative reward of –50 (ouch). In state *s*[2]
    it has no choice but to take action *a*[1], which will most likely lead it back
    to state *s*[0], gaining a reward of +40 on the way. You get the picture. By looking
    at this MDP, can you guess which strategy will gain the most reward over time?
    In state *s*[0] it is clear that action *a*[0] is the best option, and in state
    *s*[2] the agent has no choice but to take action *a*[1], but in state *s*[1]
    it is not obvious whether the agent should stay put (*a*[0]) or go through the
    fire (*a*[2]).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它从状态 *s*[0] 开始，代理可以选择行动 *a*[0]、*a*[1] 或 *a*[2]。如果它选择行动 *a*[1]，它将肯定地停留在状态 *s*[0]
    而没有任何奖励。因此，如果它想的话，它可以永远留在那里。但是，如果它选择行动 *a*[0]，它有70%的概率获得+10的奖励并停留在状态 *s*[0]。然后它可以一次又一次地尝试以获得尽可能多的奖励，但最终它可能会结束在状态
    *s*[1]。在状态 *s*[1] 中，它只有两种可能的行为：*a*[0] 或 *a*[2]。它可以反复选择行动 *a*[0] 以保持原位，或者它可以选择移动到状态
    *s*[2] 并获得-50的负面奖励（ouch）。在状态 *s*[2] 中，它别无选择，只能采取行动 *a*[1]，这很可能会将它带回到状态 *s*[0]，并在路上获得+40的奖励。你明白这个意思了。通过观察这个MDP，你能猜出哪种策略在长时间内能获得最多的奖励吗？在状态
    *s*[0] 中，显然行动 *a*[0] 是最佳选择，在状态 *s*[2] 中，代理别无选择，只能采取行动 *a*[1]，但在状态 *s*[1] 中，并不明显代理应该留在原位
    (*a*[0]) 还是穿过火海 (*a*[2])。
- en: Bellman found a way to estimate the *optimal state value* of any state *s*,
    denoted *V**(*s*), which is the sum of all discounted future rewards the agent
    can expect on average starting from state *s*, assuming it acts optimally. He
    showed that if the agent acts optimally, then the *Bellman optimality equation*
    applies (see [Equation 19-2](#bellman_optimality_equation)). This recursive equation
    says that if the agent acts optimally, then the optimal value of the current state
    is equal to the reward it will get on average after taking one optimal action,
    plus the expected optimal value of all possible next states that this action can
    lead to.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼找到了一种方法来估计任何状态 *s* 的 *最优状态值*，表示为 *V**(*s*)，这是代理从状态 *s* 开始，平均期望获得的折扣未来奖励的总和，假设它采取最优行动。他证明了如果代理采取最优行动，那么
    *贝尔曼最优方程* 适用（见 [方程19-2](#bellman_optimality_equation)）。这个递归方程表明，如果代理采取最优行动，那么当前状态的最优值等于采取一个最优行动后平均获得的奖励，加上所有可能后续状态期望的最优值的总和。
- en: Equation 19-2\. Bellman optimality equation
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程19-2. 贝尔曼最优方程
- en: $upper V Superscript asterisk Baseline left-parenthesis s right-parenthesis
    equals max Underscript a Endscripts sigma-summation Underscript s Superscript
    prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript
    prime Baseline right-parenthesis left-bracket upper R left-parenthesis s comma
    a comma s Superscript prime Baseline right-parenthesis plus gamma dot upper V
    Superscript asterisk Baseline left-parenthesis s Superscript prime Baseline right-parenthesis
    right-bracket for all s$
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: $upper V Superscript asterisk Baseline left-parenthesis s right-parenthesis
    equals max Underscript a Endscripts sigma-summation Underscript s Superscript
    prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript
    prime Baseline right-parenthesis left-bracket upper R left-parenthesis s comma
    a comma s Superscript prime Baseline right-parenthesis plus gamma dot upper V
    Superscript asterisk Baseline left-parenthesis s Superscript prime Baseline right-parenthesis
    right-bracket for all s$
- en: 'In this equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*T*(*s*, *a*, *s*′) is the transition probability from state *s* to state *s*′,
    given that the agent chose action *a*. For example, in [Figure 19-8](#mdp_diagram),
    *T*(*s*[2], *a*[1], *s*[0]) = 0.8\. Note that $sigma-summation Underscript s Superscript
    prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript
    prime Baseline right-parenthesis equals 1$ .'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*(*s*, *a*, *s*′) 是在智能体选择了动作 *a* 的情况下，从状态 *s* 到状态 *s*′ 的转移概率。例如，在[图 19-8](#mdp_diagram)中，*T*(*s*[2],
    *a*[1], *s*[0]) = 0.8。注意，$sigma-summation Underscript s Superscript prime Baseline
    Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline
    right-parenthesis equals 1$ 。'
- en: '*R*(*s*, *a*, *s*′) is the reward that the agent gets when it goes from state
    *s* to state *s*′, given that the agent chose action *a*. For example, in [Figure 19-8](#mdp_diagram),
    *R*(*s*[2], *a*[1], *s*[0]) = +40.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R*(*s*, *a*, *s*′) 是当智能体从状态 *s* 转移到状态 *s*′ 时获得的奖励，前提是智能体选择了动作 *a*。例如，在[图 19-8](#mdp_diagram)中，*R*(*s*[2],
    *a*[1], *s*[0]) = +40。'
- en: '*γ* is the discount factor.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ* 是折扣因子。'
- en: Note
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In the Bellman equation and the rest of this chapter, an optimal policy is
    one that maximizes the expected sum of *discounted* future rewards: this means
    that it depends on the discount factor *γ*. However, in real-world tasks we’re
    generally more interested in the expected sum of rewards per episode, without
    any discount (in fact, that’s usually how we evaluate agents). To approach this
    goal, we usually choose a discount factor close to 1 (but not too close or else
    training becomes slow and unstable).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝尔曼方程和本章的其余部分中，最优策略是指最大化期望的*折扣*未来奖励总和的策略：这意味着它依赖于折扣因子 *γ*。然而，在现实世界的任务中，我们通常对每轮期望的奖励总和更感兴趣，没有任何折扣（实际上，我们通常就是这样评估智能体的）。为了达到这个目标，我们通常选择一个接近
    1 的折扣因子（但不要太接近，否则训练会变得缓慢且不稳定）。
- en: 'This equation leads directly to an algorithm that can precisely estimate the
    optimal state value of every possible state: first initialize all the state value
    estimates to zero, and then iteratively update them using the *value iteration*
    algorithm (see [Equation 19-3](#value_iteration_equation)). A remarkable result
    is that, given enough time, these estimates are guaranteed to converge to the
    optimal state values, corresponding to the optimal policy.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程直接导出了一个可以精确估计每个可能状态的最优状态值的算法：首先将所有状态值估计初始化为零，然后使用*值迭代算法*（见[方程 19-3](#value_iteration_equation)）迭代更新它们。一个显著的结果是，给定足够的时间，这些估计将保证收敛到最优状态值，对应于最优策略。
- en: Equation 19-3\. Value iteration algorithm
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 19-3\. 值迭代算法
- en: $upper V Subscript k plus 1 Baseline left-parenthesis s right-parenthesis left-arrow
    max Underscript a Endscripts sigma-summation Underscript s Superscript prime Baseline
    Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline
    right-parenthesis left-bracket upper R left-parenthesis s comma a comma s Superscript
    prime Baseline right-parenthesis plus gamma dot upper V Subscript k Baseline left-parenthesis
    s Superscript prime Baseline right-parenthesis right-bracket for all s$
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: $upper V Subscript k plus 1 Baseline left-parenthesis s right-parenthesis left-arrow
    max Underscript a Endscripts sigma-summation Underscript s Superscript prime Baseline
    Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline
    right-parenthesis left-bracket upper R left-parenthesis s comma a comma s Superscript
    prime Baseline right-parenthesis plus gamma dot upper V Subscript k Baseline left-parenthesis
    s Superscript prime Baseline right-parenthesis right-bracket for all s$
- en: In this equation, *V*[*k*](*s*) is the estimated value of state *s* at the *k*^(th)
    iteration of the algorithm.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*V*[*k*](*s*) 是算法第 *k* 次迭代时状态 *s* 的估计值。
- en: Note
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This algorithm is an example of *dynamic programming*, which breaks down a complex
    problem into tractable subproblems that can be tackled iteratively.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法是**动态规划**的一个例子，它将复杂问题分解为可处理的子问题，这些子问题可以迭代解决。
- en: Knowing the optimal state values can be useful, in particular to evaluate a
    policy, but it does not give us the optimal policy for the agent. Luckily, Bellman
    found a very similar algorithm to estimate the optimal *state-action values*,
    generally called *Q-values* (quality values). The optimal Q-value of the state-action
    pair (*s*, *a*), denoted *Q**(*s*, *a*), is the sum of discounted future rewards
    the agent can expect on average starting from state *s* if it chooses action *a*,
    but before it sees the outcome of this action, assuming it acts optimally after
    that action.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 了解最优状态值可能很有用，特别是为了评估策略，但它并不给我们提供智能体的最优策略。幸运的是，贝尔曼找到了一个非常相似的算法来估计最优的*状态-动作值*，通常称为*Q值*（质量值）。状态-动作对
    (*s*, *a*) 的最优 Q 值，表示为 *Q**(*s*, *a*)，是智能体从状态 *s* 出发，如果选择动作 *a*，在看到该动作的结果之前，平均期望的折扣未来奖励的总和，假设它在该动作之后采取最优行动。
- en: Let’s look at how it works. Once again, you start by initializing all the Q-value
    estimates to zero, then you update them using the *Q-value iteration* algorithm
    (see [Equation 19-4](#q_value_iteration_equation)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的。再次，你首先将所有 Q 值估计初始化为零，然后使用 *Q 值迭代* 算法（见 [方程式 19-4](#q_value_iteration_equation)）来更新它们。
- en: Equation 19-4\. Q-value iteration algorithm
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19-4\. Q 值迭代算法
- en: $upper Q Subscript k plus 1 Baseline left-parenthesis s comma a right-parenthesis
    left-arrow sigma-summation Underscript s Superscript prime Baseline Endscripts
    upper T left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis
    left-bracket upper R left-parenthesis s comma a comma s Superscript prime Baseline
    right-parenthesis plus gamma dot max Underscript a Superscript prime Baseline
    Endscripts upper Q Subscript k Baseline left-parenthesis s prime comma a Superscript
    prime Baseline right-parenthesis right-bracket for all left-parenthesis s comma
    a right-parenthesis$
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: $upper Q Subscript k plus 1 Baseline left-parenthesis s comma a right-parenthesis
    left-arrow sigma-summation Underscript s Superscript prime Baseline Endscripts
    upper T left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis
    left-bracket upper R left-parenthesis s comma a comma s Superscript prime Baseline
    right-parenthesis plus gamma dot max Underscript a Superscript prime Baseline
    Endscripts upper Q Subscript k Baseline left-parenthesis s prime comma a Superscript
    prime Baseline right-parenthesis right-bracket for all left-parenthesis s comma
    a right-parenthesis$
- en: 'Once you have the optimal Q-values, defining the optimal policy, denoted *π*^*(*s*),
    is trivial: when the agent is in state *s*, it should choose the action with the
    highest Q-value for that state. The fancy math notation for this is $pi Superscript
    asterisk Baseline left-parenthesis s right-parenthesis equals argmax Underscript
    a Endscripts upper Q Superscript asterisk Baseline left-parenthesis s comma a
    right-parenthesis$ .'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了最优 Q 值，定义最优策略，记为 *π*^*(*s*), 就变得简单了：当智能体处于状态 *s* 时，它应该选择该状态具有最高 Q 值的动作。这个复杂的数学符号表示为
    $pi Superscript asterisk Baseline left-parenthesis s right-parenthesis equals
    argmax Underscript a Endscripts upper Q Superscript asterisk Baseline left-parenthesis
    s comma a right-parenthesis$ .
- en: 'Let’s apply this algorithm to the MDP represented in [Figure 19-8](#mdp_diagram).
    First, we need to define the MDP:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此算法应用于 [图 19-8](#mdp_diagram) 中表示的 MDP。首先，我们需要定义 MDP：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For example, to know the transition probability of going from *s*[2] to *s*[0]
    after playing action *a*[1], we will look up `transition_probabilities[2][1][0]`
    (which is 0.8). Similarly, to get the corresponding reward, we will look up `rewards[2][1][0]`
    (which is +40). And to get the list of possible actions in *s*[2], we will look
    up `possible_actions[2]` (in this case, only action *a*[1] is possible). Next,
    we must initialize all the Q-values to zero (except for the impossible actions,
    for which we set the Q-values to –∞):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要知道在执行动作 *a*[1] 后从 *s*[2] 转移到 *s*[0] 的转移概率，我们将查找 `transition_probabilities[2][1][0]`（这是
    0.8）。同样，为了得到相应的奖励，我们将查找 `rewards[2][1][0]`（这是 +40）。为了得到 *s*[2] 中可能动作的列表，我们将查找
    `possible_actions[2]`（在这种情况下，只有动作 *a*[1] 是可能的）。接下来，我们必须将所有 Q 值初始化为零（除了不可能的动作，我们将
    Q 值设置为 –∞）：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now let’s run the Q-value iteration algorithm. It applies [Equation 19-4](#q_value_iteration_equation)
    repeatedly, to all Q-values, for every state and every possible action:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来运行 Q 值迭代算法。它反复应用 [方程式 19-4](#q_value_iteration_equation)，针对所有 Q 值，对每个状态和每个可能动作进行迭代：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'That’s it! The resulting Q-values look like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！得到的 Q 值看起来是这样的：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For example, when the agent is in state *s*[0] and it chooses action *a*[1],
    the expected sum of discounted future rewards is approximately 17.0.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当智能体处于状态 *s*[0] 并选择动作 *a*[1] 时，期望的折现未来奖励总和大约为 17.0。
- en: 'For each state, we can find the action that has the highest Q-value:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态，我们可以找到具有最高 Q 值的动作：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This gives us the optimal policy for this MDP when using a discount factor
    of 0.90: in state *s*[0] choose action *a*[0], in state *s*[1] choose action *a*[0]
    (i.e., stay put), and in state *s*[2] choose action *a*[1] (the only possible
    action). Interestingly, if we increase the discount factor to 0.95, the optimal
    policy changes: in state *s*[1] the best action becomes *a*[2] (go through the
    fire!). This makes sense because the more you value future rewards, the more you
    are willing to put up with some pain now for the promise of future bliss.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了使用折现因子 0.90 的 MDP 的最优策略：在状态 *s*[0] 选择动作 *a*[0]，在状态 *s*[1] 选择动作 *a*[0]（即保持原位），在状态
    *s*[2] 选择动作 *a*[1]（唯一可能动作）。有趣的是，如果我们把折现因子提高到 0.95，最优策略就会改变：在状态 *s*[1] 最好的动作变为
    *a*[2]（穿过火焰！）。这很有道理，因为你对未来奖励的重视程度越高，你现在就越愿意为了未来的幸福忍受一些痛苦。
- en: Temporal Difference Learning
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间差分学习
- en: Reinforcement learning problems with discrete actions can often be modeled as
    Markov decision processes, but the agent initially has no idea what the transition
    probabilities are (it does not know *T*(*s*, *a*, *s*′)), and it does not know
    what the rewards are going to be either (it does not know *R*(*s*, *a*, *s*′)).
    It must experience each state and each transition at least once to know the rewards,
    and it must experience them multiple times if it is to have a reasonable estimate
    of the transition probabilities.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 具有离散动作的强化学习问题通常可以建模为马尔可夫决策过程，但代理最初并不知道状态转移概率是什么（它不知道 *T*(*s*, *a*, *s*′))，也不知道奖励将会是什么（它不知道
    *R*(*s*, *a*, *s*′))。它必须至少体验每个状态和每个转移一次才能知道奖励，如果它想要对转移概率有一个合理的估计，它必须多次体验它们。
- en: The *temporal difference (TD) learning* algorithm is very similar to the Q-value
    iteration algorithm, but tweaked to take into account the fact that the agent
    has only partial knowledge of the MDP. In general we assume that the agent initially
    knows only the possible states and actions, and nothing more. The agent uses an
    *exploration policy*—for example, a purely random policy—to explore the MDP, and
    as it progresses, the TD learning algorithm updates the estimates of the state
    values based on the transitions and rewards that are actually observed (see [Equation
    19-5](#td_learning_equation)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间差分 (TD) 学习* 算法与 Q 值迭代算法非常相似，但经过调整以考虑代理对 MDP 只有部分知识的事实。一般来说，我们假设代理最初只知道可能的状态和动作，没有更多。代理使用
    *探索策略*——例如，一个完全随机的策略——来探索 MDP，随着它的进展，TD 学习算法根据实际观察到的转移和奖励来更新状态值的估计（参见 [方程 19-5](#td_learning_equation)）。'
- en: Equation 19-5\. TD learning algorithm
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 19-5\. TD 学习算法
- en: $StartLayout 1st Row 1st Column upper V Subscript k plus 1 Baseline left-parenthesis
    s right-parenthesis 2nd Column left-arrow left-parenthesis 1 minus alpha right-parenthesis
    upper V Subscript k Baseline left-parenthesis s right-parenthesis plus alpha left-parenthesis
    r plus gamma dot upper V Subscript k Baseline left-parenthesis s prime right-parenthesis
    right-parenthesis 2nd Row 1st Column or comma 2nd Column equivalently colon 3rd
    Row 1st Column upper V Subscript k plus 1 Baseline left-parenthesis s right-parenthesis
    2nd Column left-arrow upper V Subscript k Baseline left-parenthesis s right-parenthesis
    plus alpha dot delta Subscript k Baseline left-parenthesis s comma r comma s Superscript
    prime Baseline right-parenthesis 4th Row 1st Column with 2nd Column delta Subscript
    k Baseline left-parenthesis s comma r comma s Superscript prime Baseline right-parenthesis
    equals r plus gamma dot upper V Subscript k Baseline left-parenthesis s prime
    right-parenthesis minus upper V Subscript k Baseline left-parenthesis s right-parenthesis
    EndLayout$
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row 1st Column 上标 k 加 1 基线 左括号 s 右括号 2nd Column 左箭头 左括号 1 减去
    alpha 右括号 上标 k 基线 左括号 s 右括号 加上 alpha 左括号 r 加 gamma 点 上标 k 基线 左括号 s' 右括号 右括号 2nd
    Row 1st Column 或逗号 2nd Column 相当于冒号 3rd Row 1st Column 上标 k 加 1 基线 左括号 s 右括号 2nd
    Column 左箭头 上标 k 基线 左括号 s 右括号 加上 alpha 点 delta 上标 k 基线 左括号 s ，r ，s 上标 prime 基线
    右括号 4th Row 1st Column 与 2nd Column delta 上标 k 基线 左括号 s ，r ，s 上标 prime 基线 右括号
    等于 r 加 gamma 点 上标 k 基线 左括号 s' 右括号 减去 上标 k 基线 左括号 s 右括号 EndLayout$
- en: 'In this equation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*α* is the learning rate (e.g., 0.01).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α* 是学习率（例如，0.01）。'
- en: '*r* + *γ* · *V*[*k*](*s*′) is called the *TD target*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r* + *γ* · *V*[*k*](*s*′) 被称为 *TD 目标*。'
- en: '*δ*[*k*](*s*, *r*, *s*′) is called the *TD error*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*δ*[*k*](*s*, *r*, *s*′) 被称为 *TD 错误*。'
- en: 'A more concise way of writing the first form of this equation is to use the
    notation $a left-arrow Underscript alpha Endscripts b$ , which means *a*[*k*+1]
    ← (1 – *α*) · *a*[*k*] + *α* ·*b*[*k*]. So the first line of [Equation 19-5](#td_learning_equation)
    can be rewritten like this: $upper V left-parenthesis s right-parenthesis left-arrow
    Underscript alpha Endscripts r plus gamma dot upper V left-parenthesis s prime
    right-parenthesis$ .'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 写这个方程的第一种形式的一个更简洁的方法是使用符号 $a left-arrow Underscript alpha Endscripts b$ ，它意味着
    *a*[*k*+1] ← (1 – *α*) · *a*[*k*] + *α* ·*b*[*k*]。因此，[方程 19-5](#td_learning_equation)
    的第一行可以重写如下：$upper V left-parenthesis s right-parenthesis left-arrow Underscript
    alpha Endscripts r plus gamma dot upper V left-parenthesis s prime right-parenthesis$
    .
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: TD learning has many similarities with stochastic gradient descent, including
    the fact that it handles one sample at a time. Moreover, just like SGD, it can
    only truly converge if you gradually reduce the learning rate; otherwise, it will
    keep bouncing around the optimum Q-values.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: TD 学习与随机梯度下降有很多相似之处，包括它一次处理一个样本的事实。此外，就像 SGD 一样，只有当你逐渐降低学习率时，它才能真正收敛；否则，它将围绕最优
    Q 值不断震荡。
- en: For each state *s*, this algorithm keeps track of a running average of the immediate
    rewards the agent gets upon leaving that state, plus the rewards it expects to
    get later, assuming it acts optimally.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态 *s*，此算法会跟踪智能体离开该状态后获得的即时奖励的运行平均值，以及它期望在以后获得的奖励，假设它采取最优行动。
- en: Q-Learning
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q-Learning
- en: Similarly, the Q-learning algorithm is an adaptation of the Q-value iteration
    algorithm to the situation where the transition probabilities and the rewards
    are initially unknown (see [Equation 19-6](#q_learning_equation)). Q-learning
    works by watching an agent play (e.g., randomly) and gradually improving its estimates
    of the Q-values. Once it has accurate Q-value estimates (or close enough), then
    the optimal policy is to choose the action that has the highest Q-value (i.e.,
    the greedy policy).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Q 学习算法是 Q 值迭代算法对初始时过渡概率和奖励未知的情况的改编（见 [方程 19-6](#q_learning_equation)）。Q
    学习通过观察智能体（例如，随机）玩游戏并逐渐改进其对 Q 值的估计来工作。一旦它有了准确的 Q 值估计（或者足够接近），那么最优策略就是选择具有最高 Q 值的动作（即贪婪策略）。
- en: Equation 19-6\. Q-learning algorithm
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 19-6\. Q 学习算法
- en: $upper Q left-parenthesis s comma a right-parenthesis left-arrow Underscript
    alpha Endscripts r plus gamma dot max Underscript a Superscript prime Baseline
    Endscripts upper Q left-parenthesis s prime comma a prime right-parenthesis$
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: $upper Q left-parenthesis s comma a right-parenthesis left-arrow Underscript
    alpha Endscripts r plus gamma dot max Underscript a Superscript prime Baseline
    Endscripts upper Q left-parenthesis s prime comma a prime right-parenthesis$
- en: For each state-action pair (*s*, *a*), this algorithm keeps track of a running
    average of the rewards *r* the agent gets upon leaving the state *s* with action
    *a*, plus the sum of discounted future rewards it expects to get. To estimate
    this sum, we take the maximum of the Q-value estimates for the next state *s*′,
    since we assume that the target policy will act optimally from then on.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态-动作对 (*s*, *a*)，此算法会跟踪一个运行平均奖励 *r*，这是智能体在执行动作 *a* 离开状态 *s* 后获得的奖励，以及它期望获得的未来折扣奖励的总和。为了估计这个总和，我们取下一个状态
    *s*′ 的 Q 值估计的最大值，因为我们假设从那时起目标策略将采取最优行动。
- en: 'Let’s implement the Q-learning algorithm. First, we will need to make an agent
    explore the environment. For this, we need a step function so that the agent can
    execute one action and get the resulting state and reward:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现 Q 学习算法。首先，我们需要让智能体探索环境。为此，我们需要一个步进函数，以便智能体可以执行一个动作并获得相应的状态和奖励：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now let’s implement the agent’s exploration policy. Since the state space is
    pretty small, a simple random policy will be sufficient. If we run the algorithm
    for long enough, the agent will visit every state many times, and it will also
    try every possible action many times:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现智能体的探索策略。由于状态空间相当小，一个简单的随机策略就足够了。如果我们运行足够长的时间，智能体将多次访问每个状态，并且也会多次尝试每个可能的动作：
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, after we initialize the Q-values just like earlier, we are ready to run
    the Q-learning algorithm with learning rate decay (using power scheduling, introduced
    in [Chapter 11](ch11.html#deep_chapter)):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在初始化 Q 值就像之前一样之后，我们就可以运行带有学习率衰减的 Q 学习算法（使用在第 11 章中引入的幂调度）：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This algorithm will converge to the optimal Q-values, but it will take many
    iterations, and possibly quite a lot of hyperparameter tuning. As you can see
    in [Figure 19-9](#q_value_plot), the Q-value iteration algorithm (left) converges
    very quickly, in fewer than 20 iterations, while the Q-learning algorithm (right)
    takes about 8,000 iterations to converge. Obviously, not knowing the transition
    probabilities or the rewards makes finding the optimal policy significantly harder!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法将收敛到最优 Q 值，但需要许多迭代，并且可能需要进行大量的超参数调整。正如你在 [图 19-9](#q_value_plot) 中可以看到，Q
    值迭代算法（左）在不到 20 次迭代内就非常快地收敛了，而 Q 学习算法（右）则需要大约 8,000 次迭代才能收敛。显然，不知道过渡概率或奖励会使找到最优策略变得显著更难！
- en: '![Comparison of learning curves showing Q-value iteration converging in under
    20 iterations and Q-learning taking around 8,000 iterations.](assets/hmls_1909.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![展示 Q 值迭代在不到 20 次迭代内收敛和 Q 学习大约需要 8,000 次迭代的学习曲线比较](assets/hmls_1909.png)'
- en: Figure 19-9\. Learning curve of the Q-value iteration algorithm versus the Q-learning
    algorithm
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-9\. Q值迭代算法与Q学习算法的学习曲线
- en: 'The Q-learning algorithm is called an *off-policy* algorithm because the policy
    being trained is not necessarily the one used during training. For example, in
    the code we just ran, the policy being executed (the exploration policy) was completely
    random, while the policy being trained was never used. After training, the optimal
    policy corresponds to systematically choosing the action with the highest Q-value.
    Conversely, the REINFORCE algorithm is *on-policy*: it explores the world using
    the policy being trained. It is somewhat surprising that Q-learning is capable
    of learning the optimal policy by just watching an agent act randomly. Imagine
    learning to play golf when your teacher is a blindfolded monkey. Can we do better?'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法被称为 *离线策略*算法，因为正在训练的策略不一定是训练期间使用的策略。例如，在我们刚刚运行的代码中，正在执行的策略（探索策略）是完全随机的，而正在训练的策略从未被使用过。训练后，最佳策略对应于系统地选择具有最高Q值的动作。相反，REINFORCE算法是
    *在线策略*：它使用正在训练的策略来探索世界。令人惊讶的是，Q学习仅通过观察代理随机行动就能学会最佳策略。想象一下，当你的老师是一个蒙上眼睛的猴子时，你学习打高尔夫球。我们能做得更好吗？
- en: Exploration Policies
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索策略
- en: 'Of course, Q-learning can work only if the exploration policy explores the
    MDP thoroughly enough. Although a purely random policy is guaranteed to eventually
    visit every state and every transition many times, it may take an extremely long
    time to do so. Therefore, a better option is to use the *ε-greedy policy* (ε is
    epsilon): at each step it acts randomly with probability *ε*, or greedily with
    probability 1–*ε* (i.e., choosing the action with the highest Q-value). The advantage
    of the *ε*-greedy policy (compared to a completely random policy) is that it will
    spend more and more time exploring the interesting parts of the environment, as
    the Q-value estimates get better and better, while still spending some time visiting
    unknown regions of the MDP. It is quite common to start with a high value for
    *ε* (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Q学习只有在探索策略足够彻底地探索MDP时才能工作。尽管纯随机策略保证最终会多次访问每个状态和每个转换，但这可能需要非常长的时间。因此，更好的选择是使用
    *ε-greedy* 策略（ε是epsilon）：在每一步，它以概率 *ε* 随机行动，或者以概率 1–*ε*（即选择具有最高Q值的动作）贪婪行动。与完全随机的策略相比，*ε*-greedy策略的优势在于，随着Q值估计越来越好，它将越来越多地花时间探索环境的有趣部分，同时仍然花一些时间访问MDP中的未知区域。通常，开始时将
    *ε* 的值设得较高（例如，1.0），然后逐渐降低它（例如，降至0.05）。
- en: Alternatively, rather than relying only on chance for exploration, another approach
    is to encourage the exploration policy to try actions that it has not tried much
    before. This can be implemented as a bonus added to the Q-value estimates, as
    shown in [Equation 19-7](#exploration_function_equation).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，而不是仅仅依赖机会进行探索，另一种方法是通过鼓励探索策略尝试之前很少尝试过的动作。这可以通过向Q值估计中添加一个奖励来实现，如[方程式 19-7](#exploration_function_equation)所示。
- en: Equation 19-7\. Q-learning using an exploration function
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19-7\. 使用探索函数的Q学习
- en: $upper Q left-parenthesis s comma a right-parenthesis left-arrow Underscript
    alpha Endscripts r plus gamma dot max Underscript a Superscript prime Baseline
    Endscripts f left-parenthesis upper Q left-parenthesis s prime comma a Superscript
    prime Baseline right-parenthesis comma upper N left-parenthesis s prime comma
    a prime right-parenthesis right-parenthesis$
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: $upper Q left-parenthesis s comma a right-parenthesis left-arrow Underscript
    alpha Endscripts r plus gamma dot max Underscript a Superscript prime Baseline
    Endscripts f left-parenthesis upper Q left-parenthesis s prime comma a Superscript
    prime Baseline right-parenthesis comma upper N left-parenthesis s prime comma
    a prime right-parenthesis right-parenthesis$
- en: 'In this equation:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*N*(*s*′, *a*′) counts the number of times the action *a*′ was chosen in state
    *s*′.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*(*s*′, *a*′) 计算在状态 *s*′ 下动作 *a*′ 被选择的次数。'
- en: '*f*(*Q*, *N*) is an *exploration function*, such as *f*(*Q*, *N*) = *Q* + *κ*/(1
    + *N*), where *κ* (kappa) is a curiosity hyperparameter that measures how much
    the agent is attracted to the unknown.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*(*Q*, *N*) 是一个 *探索函数*，例如 *f*(*Q*, *N*) = *Q* + *κ*/(1 + *N*)，其中 *κ*（kappa）是一个好奇心超参数，它衡量代理对未知事物的吸引力。'
- en: Approximate Q-Learning and Deep Q-Learning
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近似Q学习和深度Q学习
- en: The main problem with Q-learning is that it does not scale well to large (or
    even medium) MDPs with many states and actions. For example, suppose you wanted
    to use Q-learning to train an agent to play *Ms. Pac-Man* (see [Figure 19-1](#rl_examples_diagram)).
    There are about 240 pellets that Ms. Pac-Man can eat, each of which can be present
    or absent (i.e., already eaten). So, the number of possible pellet states is about
    2^(240) ≈ 10^(73). And if you add all the possible combinations of positions for
    all the ghosts and Ms. Pac-Man, the number of possible states becomes larger than
    the number of atoms in our galaxy, so there’s absolutely no way you can keep track
    of an estimate for every single Q-value.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习的主要问题是它不能很好地扩展到具有许多状态和动作的大（甚至中等）MDP。例如，假设你想使用 Q 学习来训练一个智能体来玩 *Ms. Pac-Man*（见
    [图 19-1](#rl_examples_diagram)）。Ms. Pac-Man 可以吃大约 240 个豆子，每个豆子可以是存在或不存在（即，已经被吃掉）。因此，可能的豆子状态数约为
    2^(240) ≈ 10^(73)。如果你加上所有幽灵和 Ms. Pac-Man 的所有可能位置组合，可能的状态数将大于我们银河系中的原子数，因此你绝对无法跟踪每个
    Q 值的估计。
- en: The solution is to find a function *Q*[**θ**](*s*, *a*) that approximates the
    Q-value of any state-action pair (*s*, *a*), where the vector **θ** parameterizes
    the function. This is called *approximate Q-learning*. For years it was recommended
    to use linear combinations of handcrafted features extracted from the state (e.g.,
    the distances of the closest ghosts, their directions, and so on) to estimate
    Q-values, but in 2013, [DeepMind](https://homl.info/dqn) showed that using deep
    neural networks can work much better, especially for complex problems, and it
    does not require any feature engineering. A DNN that is used to estimate Q-values
    is called a *deep Q-network* (DQN), and using a DQN for approximate Q-learning
    is called *deep Q-learning*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是找到一个函数 *Q*[**θ**](*s*, *a*)，它近似于任何状态-动作对 (*s*, *a*) 的 Q 值，其中向量 **θ** 参数化该函数。这被称为
    *近似 Q 学习*。多年来，推荐使用从状态中提取的手工特征（例如，最近幽灵的距离、他们的方向等）的线性组合来估计 Q 值，但 2013 年，[DeepMind](https://homl.info/dqn)
    展示了使用深度神经网络可以工作得更好，特别是对于复杂问题，而且不需要任何特征工程。用于估计 Q 值的 DNN 被称为 *深度 Q 网络* (DQN)，使用
    DQN 进行近似 Q 学习被称为 *深度 Q 学习*。
- en: Now, how can we train a DQN? Well, consider the approximate Q-value computed
    by the DQN for a given state-action pair (*s*, *a*). Thanks to Bellman, we know
    we want this approximate Q-value to be as close as possible to the reward *r*
    that we actually observe after playing action *a* in state *s*, plus the discounted
    value of playing optimally from then on. To estimate this sum of future discounted
    rewards, we can just execute the DQN on the next state *s*′, for all possible
    actions *a*′. We get an approximate future Q-value for each possible action. We
    then pick the highest (since we assume we will be playing optimally) and discount
    it, and this gives us an estimate of the sum of future discounted rewards. By
    summing the reward *r* and the future discounted value estimate, we get a target
    Q-value *y*(*s*, *a*) for the state-action pair (*s*, *a*), as shown in [Equation
    19-8](#target_q_value_equation).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何训练一个 DQN 呢？好吧，考虑 DQN 为给定的状态-动作对 (*s*, *a*) 计算的近似 Q 值。多亏了 Bellman，我们知道我们希望这个近似
    Q 值尽可能接近我们在状态 *s* 中执行动作 *a* 后实际观察到的奖励 *r*，以及从那时起最优地玩下去的折现值。为了估计这个未来折现奖励的总和，我们只需在下一个状态
    *s*′ 上对所有可能动作 *a*′ 执行 DQN。我们得到每个可能动作的近似未来 Q 值。然后我们选择最高的（因为我们假设我们将进行最优游戏），并对其进行折现，这给我们提供了未来折现奖励总和的估计。通过将奖励
    *r* 和未来折现价值估计相加，我们得到状态-动作对 (*s*, *a*) 的目标 Q 值 *y*(*s*, *a*)，如 [方程式 19-8](#target_q_value_equation)
    所示。
- en: Equation 19-8\. Target Q-value
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 19-8\. 目标 Q 值
- en: <mrow><mi>y</mi> <mrow><mo>(</mo> <mi>s</mi> <mo lspace="0%" rspace="0%">,</mo>
    <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>r</mi> <mo>+</mo> <mi>γ</mi> <mo>·</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>'</mo></msup></munder>
    <msub><mi>Q</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo> <msup><mi>a</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow></mrow>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>y</mi> <mrow><mo>(</mo> <mi>s</mi> <mo lspace="0%" rspace="0%">,</mo>
    <mi>a</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>r</mi> <mo>+</mo> <mi>γ</mi> <mo>·</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>'</mo></msup></munder>
    <msub><mi>Q</mi> <mi mathvariant="bold">θ</mi></msub> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo> <msup><mi>a</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow></mrow>
- en: With this target Q-value, we can run a training step using any gradient descent
    algorithm. In general, we try to minimize the squared error between the estimated
    Q-value *Q*[**θ**](*s*, *a*) and the target Q-value *y*(*s*, *a*), or the Huber
    loss to reduce the algorithm’s sensitivity to large errors. And that’s the deep
    Q-learning algorithm! Let’s see how to implement it to solve the CartPole environment.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个目标Q值，我们可以使用任何梯度下降算法运行一个训练步骤。通常，我们试图最小化估计Q值*Q*[**θ**](*s*, *a*)和目标Q值*y*(*s*,
    *a*)之间的平方误差，或者使用Huber损失来减少算法对大错误的敏感性。这就是深度Q学习算法！让我们看看如何实现它来解决CartPole环境。
- en: Implementing Deep Q-Learning
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现深度Q学习
- en: 'The first thing we need is a deep Q-network. In theory, we need a neural net
    that takes a state-action pair as input, and outputs an approximate Q-value. However,
    in practice it’s much more efficient to use a neural net that takes only a state
    as input, and outputs one approximate Q-value for each possible action. To solve
    the CartPole environment, we do not need a very complicated neural net; a couple
    of hidden layers will do:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要一个深度Q网络。从理论上讲，我们需要一个神经网络，它以状态-动作对作为输入，并输出一个近似的Q值。然而，在实践中，使用一个仅以状态作为输入，并为每个可能的动作输出一个近似Q值的神经网络要高效得多。为了解决CartPole环境，我们不需要一个非常复杂的神经网络；几个隐藏层就足够了：
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Our DQN is very similar to our earlier policy network, except it outputs a
    Q-value for each action instead of logits. Now let’s define a function to choose
    an action based on this DQN:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的DQN与我们早期的策略网络非常相似，只是它为每个动作输出一个Q值而不是logits。现在让我们定义一个基于这个DQN选择动作的函数：
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function takes an environment state (a single observation) and passes it
    to the neural net to predict the Q-values, then it simply returns the action with
    the largest predicted Q-value (`argmax()`). To ensure that the agent explores
    the environment, we use an *ε*-greedy policy, meaning we choose a random action
    with probability *ε*.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受一个环境状态（一个单独的观察值）并将其传递给神经网络以预测Q值，然后它简单地返回具有最大预测Q值的动作（`argmax()`）。为了确保代理探索环境，我们使用ε-贪婪策略，这意味着我们以概率ε选择一个随机动作。
- en: Warning
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: DQNs generally don’t work with continuous action spaces, unless you can discretize
    the space (which only works if it’s tiny) or combine them with policy gradients.
    This is because the DQN agent must find the action with the highest Q-value at
    each step. In a continuous action space, this requires running an optimization
    algorithm on the Q-value function at each step, which is not practical.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: DQN通常不适用于连续动作空间，除非你能将空间离散化（这仅适用于非常小的空间）或者将它们与策略梯度结合。这是因为DQN代理必须在每一步找到具有最高Q值的动作。在连续动作空间中，这需要在每一步对Q值函数运行优化算法，这在实际中是不可行的。
- en: 'Instead of training the DQN based only on the latest experiences, we will store
    all experiences in a *replay buffer* (or *replay memory*), and we will sample
    a random training batch from it at each training iteration. This helps reduce
    the correlations between the experiences in a training batch, which stabilizes
    training by making the data distribution more consistent. Each experience will
    be represented as a tuple with six elements: a state *s*, the action *a* that
    the agent took, the resulting reward *r*, the next state *s′* it reached, a boolean
    indicating whether the episode ended at that point (`done`), and finally another
    boolean indicating whether the episode was truncated at that point. We will also
    need a function to sample a random batch of experiences from the replay buffer.
    It will return a tuple containing six tensors, one for each field:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会仅基于最新的经验来训练DQN，而是将所有经验存储在重放缓冲区（或重放记忆）中，并在每次训练迭代中从中随机抽取一个训练批次。这有助于减少训练批次中经验之间的相关性，通过使数据分布更加一致来稳定训练。每个经验将表示为一个包含六个元素的元组：一个状态`s*`，代理采取的动作`a*`，得到的奖励`r*`，代理达到的下一个状态`s′*`，一个布尔值表示在该点是否结束剧集（`done`），以及最后另一个布尔值表示在该点是否截断剧集。我们还需要一个函数来从重放缓冲区中随机抽取一个经验批次。它将返回一个包含六个张量的元组，每个字段一个：
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `sample_experiences()` function takes a replay buffer and a batch size,
    and it randomly samples the desired number of experience tuples from the buffer.
    Then, for each of the six fields in the experience tuples, it extracts that field
    from each experience in the batch, and converts that list to a tensor using the
    `to_tensor()` function. Lastly, it returns the list of six tensors. The tensors
    all have shape [`batch size`] except for the observation tensors, which have shape
    [`batch size`, 4].
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_experiences()` 函数接受一个重放缓冲区和批次大小，并从缓冲区中随机采样所需数量的经验元组。然后，对于经验元组中的六个字段，它从批次中的每个经验中提取该字段，并使用
    `to_tensor()` 函数将该列表转换为张量。最后，它返回六个张量的列表。所有张量都具有 `batch size` 的形状，除了观察张量，它们的形状为
    `batch size`, 4。'
- en: The `to_tensor()` function takes a Python list containing observations (i.e.,
    64-bit NumPy arrays of shape [`4`]), or actions (integers), or rewards (floats),
    or booleans (done or truncated), and it returns a tensor of the appropriate PyTorch
    type. Note that the 64-bit NumPy arrays containing the observations are converted
    to 32-bit tensors.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_tensor()` 函数接受一个包含观察（即形状为 `4` 的64位NumPy数组）的Python列表（即动作（整数）、奖励（浮点数）或布尔值（完成或截断）），并返回适当PyTorch类型的张量。请注意，包含观察的64位NumPy数组被转换为32位张量。'
- en: The replay buffer can be any data structure that supports appending and indexing,
    and can limit the size to avoid blowing up memory during training. For simplicity,
    we will use a Python *deque*, from the standard `collections` package. This is
    a double-ended queue, in which elements can be efficiently appended or popped
    (i.e., removed) on both ends. If you set a size limit, and that limit is reached,
    appending an element to one end of the queue automatically pops an item from the
    other side. This means that each new experience replaces the oldest experience,
    which is exactly what we want.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区可以是任何支持追加和索引的数据结构，并且可以限制大小以避免在训练过程中内存爆炸。为了简单起见，我们将使用来自标准 `collections`
    包的Python *deque*。这是一个双端队列，其中元素可以在两端高效地追加或弹出（即移除）。如果您设置了一个大小限制，并且该限制被达到，则将元素追加到队列的一端会自动从另一端弹出。这意味着每个新的经验会替换最老的经验，这正是我们想要的。
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Appending and popping items on the ends of a deque is very fast, but random
    access can be slow when the queue gets very long (e.g., 100,000 items or more).
    If you need a very large replay buffer, you should use a circular buffer instead
    (see the notebook for an implementation), or check out [DeepMind’s Reverb library](https://homl.info/reverb).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在deque的两端追加和弹出项非常快，但当队列非常长时（例如，100,000个项或更多），随机访问可能会很慢。如果您需要一个非常大的重放缓冲区，您应该使用循环缓冲区（请参阅笔记本中的实现），或者查看
    [DeepMind的Reverb库](https://homl.info/reverb)。
- en: 'Let’s also create a function that will play a full episode using our DQN, and
    store the resulting experiences in the replay buffer. We’ll run in eval mode with
    `torch.no_grad()` since we don’t need gradients for now. For logging purposes,
    we’ll also make the function sum up all the rewards in the episode and return
    the result:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个函数，该函数将使用我们的DQN播放一个完整剧集，并将生成的经验存储在重放缓冲区中。我们将以评估模式运行并使用 `torch.no_grad()`，因为我们目前不需要梯度。为了日志记录的目的，我们还将使该函数汇总剧集中的所有奖励并返回结果：
- en: '[PRE25]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, let’s create a function that will sample a batch of experiences from
    the replay buffer and train the DQN by performing a single gradient descent step
    on this batch:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个函数，该函数将从重放缓冲区中采样一批经验，并通过对这个批次执行单次梯度下降步骤来训练DQN：
- en: '[PRE26]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `torch.inference_mode()` context is like `torch.no_grad()`, plus models
    run in eval mode within the context, and new tensors cannot be used in backpropagation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.inference_mode()` 上下文类似于 `torch.no_grad()`，在上下文中，模型以评估模式运行，并且不能在反向传播中使用新张量。'
- en: 'Here’s what’s happening in this code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这段代码中发生的事情：
- en: The function starts by sampling a batch of experiences from the replay buffer.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数首先从重放缓冲区中采样一批经验。
- en: 'Then it uses the DQN to compute the target Q-value for each experience in the
    batch. For this, the code implements [Equation 19-8](#target_q_value_equation):
    the DQN is used in inference mode to evaluate all the Q-values for the next state
    *s’*, then we keep only the max Q-value since we assume that the agent will play
    optimally from now on, and we multiply this max Q-value with the discount factor.
    If the episode was over (done or truncated), then the discounted max Q-value is
    multiplied by zero since we cannot expect any more rewards. Otherwise, it’s multiplied
    by 1 (i.e., unchanged). Lastly, we add the experience’s reward. All of this is
    performed simultaneously for all experiences in the batch.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它使用DQN来计算批次中每个经验的预期Q值。为此，代码实现了[方程19-8](#target_q_value_equation)：DQN以推理模式评估下一个状态
    *s’* 的所有Q值，然后我们只保留最大Q值，因为我们假设从现在开始代理将最优地玩游戏，并将这个最大Q值乘以折扣因子。如果剧集已经结束（完成或截断），则折扣后的最大Q值乘以零，因为我们不能期望获得更多奖励。否则，它乘以1（即不变）。最后，我们添加经验的奖励。所有这些操作都是同时对批次中的所有经验执行的。
- en: Next, the function uses the model again (in training mode this time) to compute
    all the Q-values for the current state *s*, and it uses the `gather()` method
    to extract just the Q-value that corresponds to the action that was actually chosen.
    Again, this is done simultaneously for all experiences in the batch.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，该函数再次使用模型（这次是训练模式）来计算当前状态 *s* 的所有Q值，并使用 `gather()` 方法提取实际选择的动作对应的Q值。同样，这是对批次中的所有经验同时进行的。
- en: Lastly, we compute the loss, which is typically the MSE between the target Q-values
    and the predicted Q-values, and we perform an optimizer step to minimize the loss.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们计算损失，这通常是目标Q值和预测Q值之间的均方误差，并执行优化器步骤以最小化损失。
- en: 'Phew! That was the hardest part. Now we can write the main training function
    and run it:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 呼！那是最难的部分。现在我们可以编写主训练函数并运行它：
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The training algorithm runs for 800 episodes. At each training iteration, we
    make the DQN play one full episode using the `play_and_record_episode()` function,
    then we run one training step using the `dqn_training_step()` function. Note that
    we only start training after several warmup episodes to ensure that the replay
    buffer contains plenty of experiences. We also linearly decrease the epsilon value
    for the ε-greedy policy from 1.0 down to 0.01 after 500 episodes (then it remains
    at 0.01). This way, the agent’s behavior will gradually become less random, focusing
    more on exploitation and less on exploration. The function also records the total
    rewards for each episode, and returns these totals; they are plotted in [Figure 19-10](#dqn_rewards_plot).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法运行800个剧集。在每个训练迭代中，我们使用 `play_and_record_episode()` 函数让DQN玩一个完整的剧集，然后使用 `dqn_training_step()`
    函数运行一个训练步骤。请注意，我们只在几个预热剧集之后开始训练，以确保重放缓冲区包含足够多的经验。我们还从500个剧集后线性减少ε-greedy策略的epsilon值，从1.0下降到0.01（然后保持在0.01）。这样，代理的行为将逐渐变得不那么随机，更多地关注利用而不是探索。该函数还记录每个剧集的总奖励，并返回这些总数；它们在[图19-10](#dqn_rewards_plot)中绘制。
- en: '![Graph showing the sum of rewards per episode during deep Q-learning training,
    illustrating increasing performance with noticeable improvement after 500 episodes.](assets/hmls_1910.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![展示深度Q学习训练过程中每集奖励总和的图表，说明在500集后性能有所提高。](assets/hmls_1910.png)'
- en: Figure 19-10\. Learning curve of the deep Q-learning algorithm
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-10\. 深度Q学习算法的学习曲线
- en: Note
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not plot the loss? Well, it’s a poor indicator of the model’s performance,
    so it’s preferable to plot the total rewards for each episode. Indeed, the loss
    might go down while the agent performs worse (e.g., if the agent gets stuck in
    one small region of the environment and the DQN starts overfitting it). Conversely,
    the loss could go up while the agent performs better (e.g., if the DQN was underestimating
    the target Q-values and it starts correctly increasing them).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么不绘制损失？嗯，它是一个很差的模型性能指标，因此绘制每集的总奖励更可取。确实，损失可能会下降，而代理的表现可能会变差（例如，如果代理卡在环境的一个小区域，DQN开始过度拟合它）。相反，损失可能会上升，而代理的表现可能会变好（例如，如果DQN低估了目标Q值，它开始正确地增加它们）。 '
- en: 'The good news is that the algorithm worked: the trained agent perfectly balances
    the pole on the cart and reaches the maximum total reward of 1,000\. The bad news
    is that the training is completely unstable. In fact, it’s even less stable than
    REINFORCE. I had to tweak the hyperparameters quite a bit before stumbling upon
    this successful training run. As you can see, the agent managed to reach a reward
    of 200 points after roughly 200 episodes, which isn’t bad, but soon after it forgot
    everything and performed terribly until episode ~550, when it quickly cracked
    the problem.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'So why is this DQN implementation unstable? Could it be the data distribution?
    Well, the replay buffer is quite large, so the data distribution is certainly
    much more stable than with the REINFORCE algorithm. So what’s happening? Well,
    in this basic deep Q-learning implementation, the model is used both to make predictions
    and to set its own targets. This can lead to a situation analogous to a dog chasing
    its own tail. This feedback loop can make the network unstable: it can diverge,
    oscillate, freeze, and so on. Luckily, there are ways to improve this; let’s see
    how.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: DQN Improvements
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In their 2013 paper, DeepMind researchers proposed a way to stabilize DQN training
    by using two DQNs instead of one: the first is the *online model*, which learns
    at each step and is used to move the agent around, and the other is the *target
    model* used only to define the targets. The target model is just a clone of the
    online model, and its weights are copied from the online model at regular intervals
    (e.g., every 10,000 steps in their Atari models). This makes the Q-value targets
    much more stable, so the feedback loop is damped, and its effects are much less
    severe. They combined this major improvement with several other tweaks: a very
    large replay buffer, a tiny learning rate, a very long training time (50 million
    steps), a very slowly decreasing epsilon (over 1 million steps), and a powerful
    neural net (a CNN).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in a [2015 paper](https://homl.info/doubledqn),⁠^([15](ch19.html#id4266))
    DeepMind researchers tweaked their DQN algorithm again, increasing its performance
    and somewhat stabilizing training. They called this variant *double DQN*. The
    update was based on the observation that the target network is prone to overestimating
    Q-values. Indeed, suppose all actions are equally good: the Q-values estimated
    by the target model should be identical, but since they are approximations, some
    may be slightly greater than others by pure chance. The target model will always
    select the largest Q-value, which will be slightly greater than the mean Q-value,
    most likely overestimating the true Q-value (a bit like counting the height of
    the tallest random wave when measuring the depth of a pool). To fix this, the
    researchers proposed using the online model instead of the target model when selecting
    the best action for the next state, and using the target model only to estimate
    the Q-value of this best action.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2015年的一篇论文[2015 paper](https://homl.info/doubledqn)⁠^([15](ch19.html#id4266))中，DeepMind的研究人员再次调整了他们的DQN算法，提高了其性能并使训练更加稳定。他们将这个变体称为*double
    DQN*。这次更新基于这样一个观察：目标网络容易高估Q值。确实，假设所有动作都是同样好的：目标模型估计的Q值应该是相同的，但由于它们是近似值，一些值可能会因为纯粹的机会而略大于其他值。目标模型将始终选择最大的Q值，这个值将略大于平均Q值，很可能会高估真实Q值（有点像在测量泳池深度时计算最高随机波浪的高度）。为了解决这个问题，研究人员提出在为下一个状态选择最佳动作时使用在线模型而不是目标模型，并且仅使用目标模型来估计这个最佳动作的Q值。
- en: Another important improvement was the introduction of *prioritized experience
    replay* (PER), which was proposed in a [2015 paper](https://homl.info/prioreplay)⁠^([16](ch19.html#id4270))
    by DeepMind researchers (once again!). Instead of sampling experiences *uniformly*
    from the replay buffer, why not sample important experiences more frequently?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的改进是引入了*优先经验回放*（PER），这是DeepMind研究人员在2015年的一篇论文[2015 paper](https://homl.info/prioreplay)⁠^([16](ch19.html#id4270))中提出的（又一次！）。不是从重放缓冲区中*均匀*地采样经验，为什么不更多地采样重要的经验呢？
- en: 'More specifically, experiences are considered “important” if they are likely
    to lead to fast learning progress. But how can we estimate this? One reasonable
    approach is to measure the magnitude of the TD error *δ* = *r* + *γ*·*V*(*s*′)
    – *V*(*s*). A large TD error indicates that a transition (*s*, *a*, *s*′) is very
    surprising and thus probably worth learning from.⁠^([17](ch19.html#id4272)) When
    an experience is recorded in the replay buffer, its priority is set to a very
    large value to ensure that it gets sampled at least once. However, once it is
    sampled (and every time it is sampled), the TD error *δ* is computed, and this
    experience’s priority is set to *p* = |*δ*| (plus a small constant to ensure that
    every experience has a nonzero probability of being sampled). The probability
    *P* of sampling an experience with priority *p* is proportional to *p*^(*ζ*),
    where *ζ* (zeta) is a hyperparameter that controls how greedy we want importance
    sampling to be: when *ζ* = 0, we just get uniform sampling, and when *ζ* = 1,
    we get full-blown importance sampling. In the paper, the authors used *ζ* = 0.6,
    but the optimal value will depend on the task.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，如果经验可能导致快速的学习进步，则认为这些经验是“重要的”。但我们如何估计这一点呢？一个合理的方法是测量TD误差*δ* = *r* + *γ*·*V*(*s*′)
    – *V*(*s*)的大小。大的TD误差表明一个转换(*s*, *a*, *s*′)非常令人惊讶，因此可能值得学习。⁠^([17](ch19.html#id4272))
    当一个经验被记录在重放缓冲区中时，其优先级被设置为一个非常大的值，以确保它至少被采样一次。然而，一旦它被采样（并且每次被采样时），就会计算TD误差*δ*，并将这个经验的优先级设置为*p*
    = |*δ*|（加上一个小的常数，以确保每个经验都有非零的采样概率）。具有优先级*p*的经验采样的概率*P*与*p*^(*ζ*)成正比，其中*ζ*（zeta）是一个超参数，它控制我们希望重要性采样有多贪婪：当*ζ*
    = 0时，我们只得到均匀采样，当*ζ* = 1时，我们得到完整的重要性采样。在论文中，作者使用了*ζ* = 0.6，但最佳值将取决于任务。
- en: 'There’s one catch though: since the samples will be biased toward important
    experiences, we must compensate for this bias during training by downweighting
    the experiences according to their importance, or the model will just overfit
    the important experiences. To be clear, we want important experiences to be sampled
    more often, but this also means we must give them a lower weight during training.
    To do this, we define each experience’s training weight as *w* = (*n* *P*)^(–*β*),
    where *n* is the number of experiences in the replay buffer, and *β* is a hyperparameter
    that controls how much we want to compensate for the importance sampling bias
    (0 means not at all, while 1 means entirely). In the paper, the authors used *β*
    = 0.4 at the beginning of training and linearly increased it to *β* = 1 by the
    end of training. Again, the optimal value will depend on the task, but if you
    increase one, you will usually want to increase the other as well.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有一个问题：由于样本将偏向于重要的经验，我们必须在训练过程中通过根据其重要性降低经验权重来补偿这种偏差，否则模型将只是过度拟合重要的经验。为了清楚起见，我们希望重要的经验被更频繁地采样，但这同时也意味着我们必须在训练期间给予它们更低的权重。为此，我们定义每个经验的训练权重为
    *w* = (*n* *P*)^(–*β*)，其中 *n* 是重放缓冲区中的经验数量，而 *β* 是一个超参数，它控制我们想要补偿的重要性采样偏差的程度（0表示完全不补偿，而1表示完全补偿）。在论文中，作者在训练开始时使用了
    *β* = 0.4，并在训练结束时将其线性增加到 *β* = 1。再次强调，最佳值将取决于任务，但如果你增加一个，你通常也会想增加另一个。
- en: One last noteworthy DQN variant is the *dueling DQN* algorithm (DDQN, not to
    be confused with double DQN, although both techniques can easily be combined).
    It was introduced in yet another [2015 paper](https://homl.info/ddqn)⁠^([18](ch19.html#id4281))
    by DeepMind researchers. To understand how it works, we must first note that the
    Q-value of a state-action pair (*s*, *a*) can be expressed as *Q*(*s*, *a*) =
    *V*(*s*) + *A*(*s*, *a*), where *V*(*s*) is the value of state *s*, and *A*(*s*,
    *a*) is the *advantage* of taking the action *a* in state *s*, compared to all
    other possible actions in that state. Moreover, the value of a state is equal
    to the Q-value of the best action *a*^* for that state (since we assume the optimal
    policy will pick the best action), so *V*(*s*) = *Q*(*s*, *a*^*), which implies
    that *A*(*s*, *a*^*) = 0\. In a dueling DQN, the model estimates both the value
    of the state and the advantage of each possible action. Since the best action
    should have an advantage of 0, the model subtracts the maximum predicted advantage
    from all predicted advantages. The rest of the algorithm is just the same as earlier.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个值得注意的DQN变体是*对抗DQN*算法（DDQN，不要与double DQN混淆，尽管这两种技术可以很容易地结合）。它是由DeepMind研究人员在另一篇[2015年的论文](https://homl.info/ddqn)⁠^([18](ch19.html#id4281))中引入的。为了理解它是如何工作的，我们必须首先注意，状态-动作对(*s*,
    *a*)的Q值可以表示为 *Q*(*s*, *a*) = *V*(*s*) + *A*(*s*, *a*)，其中 *V*(*s*) 是状态 *s* 的值，而
    *A*(*s*, *a*) 是在状态 *s* 中采取动作 *a* 相对于该状态下所有其他可能动作的*优势*。此外，状态的值等于该状态下最佳动作 *a*^*
    的Q值（因为我们假设最优策略会选择最佳动作），所以 *V*(*s*) = *Q*(*s*, *a*^*)，这意味着 *A*(*s*, *a*^*) = 0。在对抗DQN中，模型估计每个可能动作的价值和优势。由于最佳动作应该具有0的优势，模型从所有预测的优势中减去最大预测的优势。算法的其余部分与之前相同。
- en: These techniques can be combined in various ways, as DeepMind demonstrated in
    a [2017 paper](https://homl.info/rainbow):⁠^([19](ch19.html#id4283)) the paper’s
    authors combined six different techniques into an agent called *Rainbow*, which
    largely outperformed the state of the art.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术可以以各种方式组合，正如DeepMind在2017年的一篇[论文](https://homl.info/rainbow)中展示的那样：⁠^([19](ch19.html#id4283))该论文的作者将六种不同的技术组合成一个名为*Rainbow*的代理，它在很大程度上超越了当时的最佳水平。
- en: Speaking of combining different methods, why not combine policy gradients with
    value-based methods to get the best of both worlds? This is the core idea behind
    actor-critic algorithms. Let’s discuss them now.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到组合不同的方法，为什么不将策略梯度与基于价值的方法结合起来，以获得两者的最佳效果呢？这就是actor-critic算法背后的核心思想。现在让我们来讨论它们。
- en: Actor-Critic Algorithms
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Actor-Critic 算法
- en: Actor-critics are a family of RL algorithms that combine policy gradients with
    value-based methods. An actor-critic is composed of a policy (the actor) and a
    value network (the critic), which are trained simultaneously. The actor relies
    on the critic to estimate the value (or advantage) of actions or states, guiding
    its policy updates. Since the critic can use a large replay buffer, it stabilizes
    training and increases data efficiency. It’s a bit like an athlete (the actor)
    learning with the help of a coach (the critic).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 演员评论家是一系列结合策略梯度与基于价值方法的强化学习算法。一个演员评论家由一个策略（演员）和一个价值网络（评论家）组成，它们同时训练。演员依赖于评论家来估计动作或状态的价值（或优势），以指导其策略更新。由于评论家可以使用大的重放缓冲区，它稳定了训练并提高了数据效率。这有点像一名运动员（演员）在教练（评论家）的帮助下学习。
- en: Moreover, actor-critic methods support stochastic policies and continuous action
    spaces, just like policy gradients. So we do get the best of both worlds.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，演员-评论家方法支持随机策略和连续动作空间，就像策略梯度一样。所以我们确实得到了两者的最佳结合。
- en: 'Let’s implement a basic actor-critic:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个基本的演员-评论家：
- en: '[PRE28]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the constructor, we build the actor and critic networks. In this implementation,
    they share the same lower layers (called the *body*). This is common practice,
    as it reduces the total number of parameters and thereby increases data efficiency,
    but it also makes training a bit less stable since it couples the actor and critic
    more closely (another dog chasing its tail situation). The actor network takes
    a batch of environment states and outputs an action logit for each state (that’s
    the logit for action 1, just like for REINFORCE). The critic network estimates
    the value of each given state. The `forward()` method takes a batch of states
    and runs them through both networks (with a shared body), and returns the action
    logits and state values.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们构建演员和评论家网络。在这个实现中，它们共享相同的底层（称为 *body*）。这是一种常见的做法，因为它减少了总参数数量，从而提高了数据效率，但它也使得训练稍微不稳定，因为演员和评论家耦合得更紧密（另一种追逐自己的尾巴的情况）。演员网络接受一批环境状态，并为每个状态输出一个动作logit（这就是动作1的logit，就像REINFORCE一样）。评论家网络估计每个给定状态的价值。`forward()`
    方法接受一批状态，并将它们通过两个网络（共享身体）运行，并返回动作logit和状态值。
- en: 'Now let’s write a function to choose an action. It’s identical to the `choose_action()`
    function we wrote earlier for the REINFORCE policy network, except that it also
    returns the state value estimated by the critic network. This will be needed for
    training:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个函数来选择一个动作。它与我们之前为REINFORCE策略网络编写的 `choose_action()` 函数相同，除了它还返回评论家网络估计的状态值。这将在训练中需要：
- en: '[PRE29]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Great! Now let’s see how to train our actor-critic. We’ll start by defining
    a function that will perform one training step:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在让我们看看如何训练我们的演员-评论家。我们将首先定义一个函数，该函数将执行一个训练步骤：
- en: '[PRE30]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: First, we compute the TD error, which is the difference between the target value
    *y* = *r* + *γ*V(*s’*) and the state value V(*s*). The actor’s loss is the same
    as in REINFORCE, except that we multiply the log probability by the TD error instead
    of the (standardized) return. In other words, we encourage actions that performed
    better than the value network expected. As for the critic’s loss, it encourages
    the critic’s value estimates V(*s*) to match the target values *y* (e.g., using
    the MSE). Lastly, the overall loss is a weighted sum of the actor’s loss and the
    critic’s loss. To stabilize training, it’s generally a good idea to give less
    weight to the critic’s loss. Then we perform an optimizer step to minimize the
    loss. Oh, and note that we call `td_error.detach()` because we don’t want gradient
    descent to affect the critic network via the actor’s loss.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算TD误差，这是目标值 *y* = *r* + *γ*V(*s’*) 与状态值 V(*s*) 之间的差异。演员的损失与REINFORCE相同，只是我们将对数概率乘以TD误差而不是（标准化的）回报。换句话说，我们鼓励那些比价值网络预期表现更好的动作。至于评论家的损失，它鼓励评论家的价值估计
    V(*s*) 与目标值 *y* 匹配（例如，使用MSE）。最后，整体损失是演员损失和评论家损失的加权总和。为了稳定训练，通常给评论家损失更少的权重。然后我们执行一个优化器步骤来最小化损失。哦，注意，我们调用
    `td_error.detach()` 是因为我们不希望梯度下降通过演员的损失影响评论家网络。
- en: 'We’ll also need a function to compute the target value:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个函数来计算目标值：
- en: '[PRE31]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This code first evaluates V(*s’*) using the `choose_action_and_evaluate()`
    function (we ignore the chosen action and its log probability). We run this in
    inference mode because we are computing the target: we don’t want gradient descent
    to affect it. Next, we simply evaluate the target *y* = *r* + *γ*V(*s’*). If the
    episode is over, then *y* = *r*.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码首先使用`choose_action_and_evaluate()`函数评估V(*s’*)（我们忽略所选动作及其对数概率）。我们在推理模式下运行此操作，因为我们正在计算目标：我们不希望梯度下降影响它。接下来，我们简单地评估目标*y*
    = *r* + *γ*V(*s’*)。如果回合结束，则*y* = *r*。
- en: 'With that, we have all we need to write a function that will run a whole episode
    and train the actor-critic at each step (we also compute the total rewards and
    return it when the episode is over):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就有了编写一个函数所需的所有内容，该函数将运行整个回合并在每个步骤中训练演员-评论家（我们还在回合结束时计算总奖励并返回它）：
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And lastly, we can write our main training function, which just calls the `run_​epi⁠sode_and_train()`
    function many times and returns the total rewards for each episode:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以编写我们的主要训练函数，该函数只是多次调用`run_​epi⁠sode_and_train()`函数，并返回每个回合的总奖励：
- en: '[PRE33]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Let’s run it!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行它！
- en: '[PRE34]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And it works! We get a very stable CartPole that collects the maximum rewards.
    That said, this implementation is still very sensitive to the choice of hyperparameters
    and random seeds, and training is still very unstable. Luckily, researchers have
    come up with various techniques that can stabilize the actor-critic. Here are
    some of the most popular:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实有效！我们得到了一个非常稳定的CartPole，能够收集最大奖励。尽管如此，此实现仍然非常敏感于超参数和随机种子的选择，训练仍然非常不稳定。幸运的是，研究人员已经提出了各种技术，可以稳定演员-评论家。以下是一些最受欢迎的技术：
- en: '[*Asynchronous advantage actor-critic (A3C)*](https://homl.info/a3c)⁠^([20](ch19.html#id4290))'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[*异步优势演员-评论家（A3C）*](https://homl.info/a3c)⁠^([20](ch19.html#id4290))'
- en: This is an important actor-critic variant introduced by DeepMind researchers
    in 2016 where multiple agents learn in parallel, exploring different copies of
    the environment. At regular intervals, but asynchronously (hence the name), each
    agent pushes some weight updates to a master network, then it pulls the latest
    weights from that network. Each agent thus contributes to improving the master
    network and benefits from what the other agents have learned. Moreover, instead
    of estimating the state values, or even the Q-values, the critic estimates the
    *advantage* of each action (hence the second A in the name), just like in the
    Dueling DQN.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是DeepMind研究人员在2016年引入的一个重要演员-评论家变体，其中多个智能体并行学习，探索环境的不同副本。在固定间隔内，但异步（因此得名），每个智能体将一些权重更新推送到主网络，然后从该网络拉取最新权重。因此，每个智能体都为改进主网络做出贡献，并从其他智能体学到的内容中受益。此外，评论家不仅估计状态值，甚至Q值，而是估计每个动作的*优势*（因此名称中的第二个A），就像在Dueling
    DQN中一样。
- en: '[*Advantage actor-critic (A2C)*](https://homl.info/a2c)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[*优势演员-评论家（A2C）*](https://homl.info/a2c)'
- en: A2C is a variant of the A3C algorithm that removes the asynchronicity. All model
    updates are synchronous, so gradient updates are performed over larger batches,
    which allows the model to better utilize the power of the GPU.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: A2C是A3C算法的一个变体，它去除了异步性。所有模型更新都是同步的，因此梯度更新是在更大的批次上进行的，这使得模型能够更好地利用GPU的强大功能。
- en: '[*Soft actor-critic (SAC)*](https://homl.info/sac)⁠^([21](ch19.html#id4296))'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[*软演员-评论家（SAC）*](https://homl.info/sac)⁠^([21](ch19.html#id4296))'
- en: SAC is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other
    UC Berkeley researchers. It learns not only rewards, but also how to maximize
    the entropy of its actions. In other words, it tries to be as unpredictable as
    possible while still getting as many rewards as possible. This encourages the
    agent to explore the environment, which speeds up training and makes it less likely
    to repeatedly execute the same action when the critic produces imperfect estimates.
    This algorithm has demonstrated an amazing sample efficiency (contrary to all
    the previous algorithms, which learn very slowly).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: SAC是2018年由Tuomas Haarnoja和其他加州大学伯克利分校研究人员提出的一种演员-评论家变体。它不仅学习奖励，还学习如何最大化其动作的熵。换句话说，它试图尽可能不可预测，同时获得尽可能多的奖励。这鼓励智能体探索环境，从而加快训练速度，并减少评论家产生不完美估计时重复执行相同动作的可能性。该算法已经展示了惊人的样本效率（与所有之前的算法相反，这些算法学习速度非常慢）。
- en: '[*Proximal policy optimization (PPO)*](https://homl.info/ppo)⁠^([22](ch19.html#id4299))'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[*近端策略优化（PPO）*](https://homl.info/ppo)⁠^([22](ch19.html#id4299))'
- en: This algorithm by John Schulman and other OpenAI researchers is based on A2C,
    but it clips the loss function to avoid excessively large weight updates (which
    often lead to training instabilities). PPO is a simplification of the previous
    [*trust region policy optimization* (TRPO) algorithm](https://homl.info/trpo),⁠^([23](ch19.html#id4300))
    also by OpenAI. OpenAI made the news in April 2019 with its AI called OpenAI Five,
    based on the PPO algorithm, which defeated the world champions at the multiplayer
    game *Dota 2*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰·舒尔曼和其他 OpenAI 研究人员提出的这个算法基于 A2C，但它剪辑损失函数以避免过大的权重更新（这通常会导致训练不稳定）。PPO 是 OpenAI
    另一个[*信任域策略优化* (TRPO) 算法](https://homl.info/trpo)的简化，同样也是 OpenAI 提出的。2019 年 4 月，OpenAI
    因其基于 PPO 算法的 AI 产品 OpenAI Five 而闻名，该产品在多人游戏 *Dota 2* 中击败了世界冠军。
- en: The last two algorithms, SAC and PPO, are among the most widely used RL algorithms
    today, and several libraries provide easy to use and highly optimized implementations.
    For example, let’s use the popular Stable-Baselines3 library to train a PPO agent
    on the *Breakout* Atari game.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个算法，SAC 和 PPO，是目前最广泛使用的强化学习算法之一，几个库提供了易于使用且高度优化的实现。例如，让我们使用流行的 Stable-Baselines3
    库在 *Breakout* Atari 游戏上训练一个 PPO 代理。
- en: Tip
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Which RL algorithm should you use? PPO is a great general-purpose RL algorithm—a
    good bet if you’re not sure. SAC is the most sample efficient for continuous action
    tasks, making it ideal for robotics. DQN remains strong for discrete tasks such
    as Atari games or board games.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该使用哪种强化学习算法？PPO 是一个优秀的通用强化学习算法——如果你不确定，这是一个不错的选择。SAC 对于连续动作任务来说是最高效的采样算法，使其非常适合机器人技术。DQN
    在诸如 Atari 游戏或棋类游戏等离散任务上仍然表现强劲。
- en: Mastering Atari Breakout Using the Stable-Baselines3 PPO Implementation
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Stable-Baselines3 PPO 实现掌握 Atari Breakout
- en: Since Stable-Baselines3 (SB3) is not installed by default on Colab, we must
    first run `%pip install -q stable_baselines3`, which will take a couple of minutes.
    However, if you are running the code on your own machine and you followed the
    [installation instructions](https://homl.info/install-p), then it’s already installed.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Stable-Baselines3 (SB3) 在 Colab 上默认未安装，我们必须首先运行 `%pip install -q stable_baselines3`，这将花费几分钟时间。然而，如果你在自己的机器上运行代码，并且遵循了[安装说明](https://homl.info/install-p)，那么它已经安装了。
- en: 'Next, we must create an ALE interface: it will run the Atari 2600 emulator
    and allow Gymnasium to interface with it (the Atari games will appear in the list
    of available environments):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须创建一个 ALE 接口：它将运行 Atari 2600 模拟器，并允许 Gymnasium 与其接口（Atari 游戏将出现在可用环境列表中）：
- en: '[PRE35]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Atari games were stored on read-only memory (ROM) cartridges. These ROMs can
    now be downloaded freely and used for research and educational purposes. On Colab,
    they are preinstalled, and if you followed the installation instructions to run
    the code locally, then they are also preinstalled.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 游戏存储在只读存储器（ROM）卡上。现在，这些 ROM 可以免费下载并用于研究和教育目的。在 Colab 上，它们是预安装的，如果你按照安装说明在本地运行代码，那么它们也是预安装的。
- en: 'Now that we have SB3, the ALE interface, and the ROMs, we are ready to create
    the *Breakout* environment. But instead of creating it using Gymnasium directly,
    we will use SB3’s `make_atari_env()` function: it creates a wrapper environment
    containing multiple *Breakout* environments that will run in parallel. Each observation
    from the wrapper environment will contain one observation for each *Breakout*
    environment. Similarly, the wrapper environment’s `step()` function will take
    an array containing one action for each *Breakout* environment. Lastly, the wrapper
    environment will take care of preprocessing the images, converting them from 210
    × 160 RGB images to 84 × 84 grayscale images. Very convenient! So let’s create
    an SB3 environment containing four *Breakout* environments, and reset it to get
    an observation:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 SB3、ALE 接口和 ROM，我们准备创建 *Breakout* 环境。但不是直接使用 Gymnasium 创建它，我们将使用 SB3
    的 `make_atari_env()` 函数：它创建了一个包含多个 *Breakout* 环境的包装环境，这些环境将并行运行。包装环境中的每个观察结果将包含每个
    *Breakout* 环境的一个观察结果。同样，包装环境的 `step()` 函数将接受一个包含每个 *Breakout* 环境一个动作的数组。最后，包装环境将负责预处理图像，将它们从
    210 × 160 RGB 图像转换为 84 × 84 灰度图像。非常方便！所以让我们创建一个包含四个 *Breakout* 环境的 SB3 环境，并将其重置以获取一个观察结果：
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `env.get_images()` method returns the original images, before preprocessing
    (see [Figure 19-11](#atari_preprocessing_plot)).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.get_images()` 方法返回预处理前的原始图像（见[图 19-11](#atari_preprocessing_plot)）。'
- en: '![Comparison of an original Breakout game frame in color with a preprocessed
    grayscale version, illustrating the change in dimensions and color reduction for
    efficiency.](assets/hmls_1911.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![原始 Breakout 游戏帧与预处理后的灰度版本的对比，展示了为了效率而进行的尺寸变化和颜色减少。](assets/hmls_1911.png)'
- en: Figure 19-11\. A *Breakout* frame before (left) and after (right) preprocessing
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-11\. 预处理前后的 *Breakout* 帧对比（左）和（右）
- en: 'The ALE interface runs at 60 frames per second, which is quite fast, so consecutive
    frames look very similar, which wastes computation. To avoid this, the default
    *Breakout* environment repeats each action four times and returns only the final
    observation; this is called *frame skipping*. However, instead of skipping the
    frames, it’s preferable to stack them into a single four-channel image and use
    that as the observation. For this, we must first avoid frame skipping: this is
    why we used the `BreakoutNoFrameskip-v4` environment rather than `Breakout-v4`.⁠^([24](ch19.html#id4305))
    Then, we must wrap the environment in a `VecFrameStack`; this wrapper environment
    will repeat each action several times (four in our case) and stack the resulting
    frames along the channel dimension (i.e., the last one):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ALE 接口以每秒 60 帧的速度运行，这相当快，因此连续帧看起来非常相似，这浪费了计算资源。为了避免这种情况，默认的 *Breakout* 环境将每个动作重复四次，并只返回最终观察结果；这被称为
    *帧跳过*。然而，与其跳过帧，不如将它们堆叠成一个单通道四通道图像，并使用该图像作为观察。为此，我们首先必须避免帧跳过：这就是为什么我们使用了 `BreakoutNoFrameskip-v4`
    环境而不是 `Breakout-v4`。⁠^([24](ch19.html#id4305)) 然后，我们必须将环境包裹在 `VecFrameStack` 中；这个包装环境将重复每个动作多次（在我们的例子中是四次）并将结果帧沿通道维度（即最后一个）堆叠：
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now let’s create a PPO model with some good hyperparameters:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个具有一些良好超参数的 PPO 模型：
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'That’s a lot of arguments! Let’s see what they do:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这有很多参数！让我们看看它们的作用：
- en: 'The first argument is the policy network. Since we specified `CnnPolicy`, SB3
    will build a good CNN for us, based on the chosen algorithm (PPO in this case)
    and the observation space. If you’re curious, take a look at `ppo_model.policy`
    to see the CNN’s architecture: it’s a deep CNN with an actor head (for the action
    logits) and a critic head (for the state values). There are four possible actions:
    left, right, fire (to launch the ball), and no-op (do nothing). If you prefer
    to use a custom neural net, you must create a subclass of the `ActorCriticPolicy`
    class, located in the `stable_baselines3.common.policies` module. See [SB3’s documentation](https://homl.info/sb3)
    for more details.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数是策略网络。由于我们指定了 `CnnPolicy`，SB3 将根据选择的算法（本例中为 PPO）和观察空间为我们构建一个良好的 CNN。如果您好奇，可以查看
    `ppo_model.policy` 以了解 CNN 的架构：它是一个具有演员头（用于动作对数）和评论家头（用于状态值）的深度 CNN。有四种可能的行为：左、右、开火（发射球）和空操作（不执行任何操作）。如果您想使用自定义神经网络，您必须创建
    `ActorCriticPolicy` 类的子类，该类位于 `stable_baselines3.common.policies` 模块中。有关更多详细信息，请参阅
    [SB3 的文档](https://homl.info/sb3)。
- en: '`env`, `device`, `learning_rate`, and `batch_size` are self-explanatory.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env`、`device`、`learning_rate` 和 `batch_size` 是不言自明的。'
- en: '`n_steps` is the number of environment steps to run (per environment) before
    each policy update.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_steps` 是在每个策略更新之前（每个环境）运行的环环境步骤数。'
- en: '`n_epochs` is the number of training steps to run on each batch during optimization.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_epochs` 是在优化过程中每个批次上运行的训练步骤数。'
- en: '`clip_range` limits the magnitude of the policy updates to avoid large changes
    that might cause catastrophic forgetting.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_range` 限制了策略更新的幅度，以避免可能引起灾难性遗忘的大幅变化。'
- en: '`vf_coef` is the weight of the value function loss in the total loss (similar
    to our actor-critic’s `critic_weight` hyperparameter).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vf_coef` 是价值函数损失在总损失中的权重（类似于我们演员-评论家的 `critic_weight` 超参数）。'
- en: '`ent_coef` is the weight of the entropy term that encourages exploration.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ent_coef` 是鼓励探索的熵项的权重。'
- en: '`gamma` is the discount rate.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma` 是折扣率。'
- en: '`verbose` is the logging verbosity (0 = silent, 1 = info, 2 = debug).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` 是日志详细程度（0 = 静默，1 = 信息，2 = 调试）。'
- en: Tip
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: For new tasks, the default `PPO` hyperparameters are a good place to start.
    If learning is too slow, try more parallel environments first; then consider using
    a higher learning rate or a larger clip range. You can also shrink `n_steps` or
    `batch_size`, but this risks noisier gradients. If learning is unstable, try lowering
    the learning rate or clip range, and use larger rollouts (i.e., `n_steps`) or
    batch sizes. Use `gamma` near 0.95 for short-horizon tasks, and 0.995 to 0.999
    for long-horizon ones. Lastly, increase `ent_coef` if you want to encourage more
    exploration.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新任务，默认的`PPO`超参数是一个良好的起点。如果学习过程太慢，首先尝试使用更多的并行环境；然后考虑使用更高的学习率或更大的clip范围。你也可以减小`n_steps`或`batch_size`，但这可能会增加梯度噪声。如果学习不稳定，尝试降低学习率或clip范围，并使用更大的rollouts（即，`n_steps`）或批量大小。对于短期任务，使用`gamma`接近0.95，对于长期任务，使用0.995到0.999。最后，如果你想鼓励更多的探索，增加`ent_coef`。
- en: 'And now let’s start training. The following code will train the model for 30
    million steps. This will take many hours and will probably be too long for a Colab
    session (unless you get a paid subscription), so the notebook also includes code
    to download the trained model if you prefer. Whenever you train a model for a
    long time, it’s important to save checkpoints at regular intervals (e.g., every
    100,000 calls to the `step()` method) to avoid having to start from scratch in
    case of a crash or a power outage. For this, we can create a checkpoint callback
    and pass it to the `learn()` method:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始训练。以下代码将对模型进行3000万步的训练。这将需要很多小时，可能对于Colab会话来说时间太长（除非你有付费订阅），因此笔记本还包括了如果你更喜欢的话可以下载训练好的模型的代码。每次你长时间训练一个模型时，定期保存检查点（例如，每100,000次`step()`方法的调用）都很重要，以避免在发生崩溃或断电时需要从头开始。为此，我们可以创建一个检查点回调并将其传递给`learn()`方法：
- en: '[PRE39]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `save_freq` argument counts calls to the `step()` method. Since there are
    4 environments running in parallel, 50,000 calls correspond to 200,000 total time
    steps.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`save_freq`参数计算`step()`方法的调用次数。由于有4个环境并行运行，50,000次调用对应于200,000个总时间步。'
- en: 'To see the progress during training, one option is to load the latest checkpoint
    in another notebook, and try it out. A simpler option is to use TensorBoard to
    visualize the learning curves, especially the mean reward per episode. For this,
    you must first activate the TensorBoard extension in Colab or Jupyter by running
    `%load_ext` `tensorboard` (this is done at the start of this chapter’s notebook).
    Next, you must start the TensorBoard server, point it to a log directory, and
    choose the TCP port it will listen on. The following “magic” command (i.e., starting
    with a %) will do that and also open up the TensorBoard client interface directly
    inside Colab or Jupyter:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 要在训练过程中查看进度，一个选项是在另一个笔记本中加载最新的检查点，并尝试它。一个更简单的选项是使用TensorBoard来可视化学习曲线，特别是每集的平均奖励。为此，你必须在Colab或Jupyter中首先激活TensorBoard扩展，通过运行`%load_ext
    tensorboard`（这是在本章笔记本的开始处完成的）。接下来，你必须启动TensorBoard服务器，将其指向日志目录，并选择它将监听的TCP端口。以下“魔法”命令（即以%开头）将执行此操作，并直接在Colab或Jupyter中打开TensorBoard客户端界面：
- en: '[PRE40]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, you must tell the PPO model where to save its TensorBoard logs. This
    is done when creating the model:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你必须告诉PPO模型在哪里保存其TensorBoard日志。这是在创建模型时完成的：
- en: '[PRE41]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And that’s it. Once you start training, you will see the learning curves change
    every 30 seconds or so in the TensorBoard interface (or click the refresh button).
    The most important metric to track is the `rollout/ep_rew_mean`, which is the
    mean reward per episode: it should slowly ramp up, even though it will sometimes
    go down a bit. After 1 million total steps it will typically reach around 20;
    that’s not a very good agent. But if you let training run for 10 million steps,
    it should reach human level. And after 50 million steps, it will generally be
    superhuman.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。一旦开始训练，你将在TensorBoard界面（或点击刷新按钮）中看到学习曲线大约每30秒变化一次。最重要的指标是`rollout/ep_rew_mean`，即每集的平均奖励：它应该缓慢上升，尽管有时会略有下降。在总共1百万步之后，它通常会达到大约20；这不是一个非常好的智能体。但如果你让训练运行到1000万步，它应该达到人类水平。在5000万步之后，它通常将是超人类的。
- en: 'Congratulations, you know how to train a superhuman AI! You can try it out
    like this:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你已经知道如何训练一个超人类AI了！你可以这样尝试：
- en: '[PRE42]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will capture all the frames during one episode. You can render them as
    an animation using Matplotlib (see the notebook for an example). If you trained
    the agent for long enough (or used the pretrained model), you will see that the
    agent plays pretty well, and even found the strategy of digging tunnels on the
    sides and sending the ball through them: that’s one of the best strategies in
    this game!'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这将捕捉到一集中所有的帧。你可以使用Matplotlib将它们渲染成动画（参见笔记本中的示例）。如果你训练代理足够长的时间（或者使用了预训练模型），你会发现代理玩得相当不错，甚至找到了在侧面挖隧道并将球通过它们的策略：这是这款游戏中最好的策略之一！
- en: Overview of Some Popular RL Algorithms
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些流行强化学习算法概述
- en: 'Before we close this chapter, let’s take a brief look at a few other popular
    algorithms:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这一章之前，让我们简要地看看一些其他流行的算法：
- en: '[*AlphaGo*](https://homl.info/alphago)⁠^([25](ch19.html#id4314))'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[*AlphaGo*](https://homl.info/alphago)⁠^([25](ch19.html#id4314))'
- en: AlphaGo uses a variant of *Monte Carlo tree search* (MCTS) based on deep neural
    networks to beat human champions at the game of Go. MCTS was invented in 1949
    by Nicholas Metropolis and Stanislaw Ulam. It selects the best move after running
    many simulations, repeatedly exploring the search tree starting from the current
    position, and spending more time on the most promising branches. When it reaches
    a node that it hasn’t visited before, it plays randomly until the game ends, and
    updates its estimates for each visited node (excluding the random moves), increasing
    or decreasing each estimate, depending on the final outcome.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo使用基于深度神经网络的变体*蒙特卡洛树搜索*（MCTS）来击败围棋的人类冠军。MCTS是由尼古拉斯·梅特罗波利斯和斯坦尼斯瓦夫·乌拉姆在1949年发明的。它在运行了许多模拟后选择最佳移动，反复从当前位置开始探索搜索树，并在最有希望的分支上花费更多时间。当它到达一个之前未访问过的节点时，它会随机移动直到游戏结束，并更新每个访问节点的估计（不包括随机移动），根据最终结果增加或减少每个估计。
- en: AlphaGo is based on the same principle, but it uses a policy network to select
    moves, rather than playing randomly. This policy net is trained using policy gradients.
    The original algorithm involved three additional neural networks, and was more
    complicated, but it was simplified in the [AlphaGo Zero paper](https://homl.info/alphagozero),⁠^([26](ch19.html#id4317))
    which uses a single neural network to both select moves and evaluate game states.
    The [AlphaZero paper](https://homl.info/alphazero)⁠^([27](ch19.html#id4318)) generalized
    this algorithm, making it capable of tackling not only the game of Go, but also
    chess and shogi (Japanese chess). Lastly, the [MuZero paper](https://homl.info/muzero)⁠^([28](ch19.html#id4319))
    continued to improve upon this algorithm, outperforming the previous iterations
    even though the agent starts out without even knowing the rules of the game!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo基于同样的原理，但它使用策略网络来选择移动，而不是随机移动。这个策略网络使用策略梯度进行训练。原始算法涉及三个额外的神经网络，更复杂，但在[AlphaGo
    Zero论文](https://homl.info/alphagozero)⁠^([26](ch19.html#id4317))中得到了简化，该论文使用单个神经网络来选择移动和评估游戏状态。[AlphaZero论文](https://homl.info/alphazero)⁠^([27](ch19.html#id4318))将这个算法推广，使其能够处理不仅仅是围棋，还有象棋和国际象棋。最后，[MuZero论文](https://homl.info/muzero)⁠^([28](ch19.html#id4319))继续改进这个算法，即使代理一开始甚至不知道游戏规则，也优于之前的迭代！
- en: Note
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The rules of the game of Go were hardcoded into AlphaGo. In contrast, MuZero
    gradually learns a model of the environment: given a state *s* and an action *a*,
    it learns to predict the reward *r* and the probability of reaching state *s’*.
    Having a model of the environment (hardcoded or learned) allows these algorithms
    to plan ahead (in this case using MCTS). For this reason, both of these algorithms
    belong to the broad class of *model-based RL* algorithms. In contrast, policy
    gradients, value-based methods, and actor-critic methods are all *model-free RL*
    algorithms: they have a policy model and/or a value model, but not an *environment*
    model.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋的规则被硬编码到AlphaGo中。相比之下，MuZero逐渐学习环境的模型：给定状态*s*和动作*a*，它学习预测奖励*r*和达到状态*s’*的概率。拥有环境模型（硬编码或学习）允许这些算法进行前瞻性规划（在这种情况下使用MCTS）。因此，这两个算法都属于广泛的*基于模型的强化学习*算法类别。相比之下，策略梯度、基于值的方法和演员-评论员方法都是*无模型强化学习*算法：它们有一个策略模型和/或一个值模型，但没有*环境*模型。
- en: '[*Curiosity-based exploration*](https://homl.info/curiosity)⁠^([29](ch19.html#id4321))'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[*基于好奇的探索*](https://homl.info/curiosity)⁠^([29](ch19.html#id4321))'
- en: 'A recurring problem in RL is the sparsity of the rewards, which makes learning
    very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have
    proposed an exciting way to tackle this issue: why not ignore the rewards and
    just make the agent extremely curious to explore the environment? The rewards
    thus become intrinsic to the agent, rather than coming from the environment. Similarly,
    stimulating curiosity in a child is more likely to give good results than purely
    rewarding the child for getting good grades.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，一个反复出现的问题是奖励的稀疏性，这使得学习变得非常缓慢且效率低下。Deepak Pathak和其他加州大学伯克利分校的研究人员提出了一种解决这个问题的激动人心的方法：为什么不忽略奖励，而是让智能体对探索环境极度好奇呢？因此，奖励对智能体来说是内在的，而不是来自环境。同样，激发孩子的求知欲比仅仅因为孩子取得好成绩而奖励孩子更有可能取得好的结果。
- en: 'How does this work? The agent continuously tries to predict the outcome of
    its actions, and it seeks situations where the outcome does not match its predictions.
    In other words, it wants to be surprised. If the outcome is predictable (boring),
    it goes elsewhere. However, if the outcome is unpredictable but the agent notices
    that it has no control over it, it also gets bored after a while. With only curiosity,
    the authors succeeded in training an agent at many video games: even though the
    agent gets no penalty for losing, it finds it boring to lose because the game
    starts over, so it learns to avoid it.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？智能体持续尝试预测其行动的结果，并寻找结果与其预测不符的情况。换句话说，它想要感到惊讶。如果结果是可预测的（无聊的），它会去别处。然而，如果结果是不可预测的，但智能体注意到它无法控制它，那么它过一段时间后也会感到无聊。仅凭好奇心，作者成功地在许多视频游戏中训练了智能体：尽管智能体在失败时不会受到惩罚，但它发现失败很无聊，因为游戏会重新开始，所以它学会了避免失败。
- en: Open-ended learning (OEL)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 开放式学习（OEL）
- en: The objective of OEL is to train agents capable of endlessly learning new and
    interesting tasks, typically generated procedurally. We’re not there yet, but
    there has been some amazing progress over the last few years. For example, a [2019
    paper](https://homl.info/poet)⁠^([30](ch19.html#id4325)) by a team of researchers
    from Uber AI introduced the *POET algorithm*, which generates multiple simulated
    2D environments with bumps and holes, and trains one agent per environment. The
    agent’s goal is to walk as fast as possible while avoiding the obstacles.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: OEL的目标是训练能够无限学习新且有趣任务的智能体，这些任务通常是通过程序生成的。我们还没有达到那里，但过去几年已经取得了一些惊人的进展。例如，来自Uber
    AI的研究团队在2019年发表的一篇论文[2019 paper](https://homl.info/poet)⁠^([30](ch19.html#id4325))中介绍了*POET算法*，该算法生成多个带有凹凸和孔洞的模拟2D环境，并为每个环境训练一个智能体。智能体的目标是尽可能快地行走，同时避开障碍物。
- en: 'The algorithm starts out with simple environments, but they gradually get harder
    over time: this is called *curriculum learning*. Moreover, although each agent
    is only trained within one environment, it must regularly compete against other
    agents, across all environments. In each environment, the winner is copied over
    and replaces the agent that was there before. This way, knowledge is regularly
    transferred across environments, and the most adaptable agents are selected.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从简单的环境开始，但随着时间的推移，它们会逐渐变得更具挑战性：这被称为*课程学习*。此外，尽管每个智能体只在一个环境中进行训练，但它必须定期与其他环境中的所有智能体竞争。在每个环境中，获胜者会复制并取代之前的智能体。这样，知识就会定期在不同环境之间转移，并且选择最适应的智能体。
- en: In the end, the agents are much better walkers than agents trained on a single
    task, and they can tackle much harder environments. Of course, this principle
    can be applied to other environments and tasks as well. If you’re interested in
    OEL, make sure to check out the [Enhanced POET paper](https://homl.info/epoet),⁠^([31](ch19.html#id4328))
    as well as DeepMind’s [2021 paper](https://homl.info/oel2021)⁠^([32](ch19.html#id4329))
    on this topic.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些智能体比仅训练单一任务的智能体走得更好，并且它们可以应对更困难的环境。当然，这个原则也可以应用于其他环境和任务。如果你对OEL感兴趣，务必查看[Enhanced
    POET论文](https://homl.info/epoet)，⁠^([31](ch19.html#id4328))以及DeepMind关于这个主题的2021年论文[2021
    paper](https://homl.info/oel2021)⁠^([32](ch19.html#id4329))。
- en: Tip
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you’d like to learn more about reinforcement learning, check out the book
    [*Reinforcement Learning*](https://homl.info/rlbook) by Phil Winder (O’Reilly).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于强化学习的信息，可以阅读Phil Winder（O’Reilly）所著的[*Reinforcement Learning*](https://homl.info/rlbook)一书。
- en: We covered many topics in this chapter. We learned about policy gradient methods;
    we implemented the REINFORCE algorithm to solve the CartPole problem using Gymnasium;
    we explored Markov chains and Markov decision processes, which led us to value-based
    methods; and we implemented a deep Q-Learning model. Then we discussed actor-critic
    methods, and we used the Stable-Baselines3 library to implement a PPO model that
    beat the Atari game *Breakout*. Lastly, we took a peek at some of the other areas
    of RL, including model-based RL and more. Reinforcement learning is a huge and
    exciting field, with new ideas and algorithms popping out every day, so I hope
    this chapter sparked your curiosity. There is a whole world to explore!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们涵盖了众多主题。我们学习了策略梯度方法；我们使用 Gymnasium 实现了 REINFORCE 算法来解决 CartPole 问题；我们探讨了马尔可夫链和马尔可夫决策过程，这引导我们到基于价值的方法；然后我们实现了一个深度
    Q-Learning 模型。然后我们讨论了演员-评论家方法，并使用 Stable-Baselines3 库实现了一个击败 Atari 游戏 *Breakout*
    的 PPO 模型。最后，我们瞥了一眼强化学习的其他一些领域，包括基于模型的方法等。强化学习是一个庞大且令人兴奋的领域，每天都有新的想法和算法涌现，所以我希望这一章激起了你的好奇心。有一个广阔的世界等待你去探索！
- en: Exercises
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: How would you define reinforcement learning? How is it different from regular
    supervised or unsupervised learning?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何定义强化学习？它与常规的监督学习或无监督学习有何不同？
- en: Can you think of three possible applications of RL that were not mentioned in
    this chapter? For each of them, what is the environment? What is the agent? What
    are some possible actions? What are the rewards?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到本章未提及的三个可能的强化学习应用吗？对于每个应用，环境是什么？智能体是什么？一些可能的行为有哪些？奖励有哪些？
- en: What is the discount factor? Can the optimal policy change if you modify the
    discount factor?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 折扣因子是什么？如果你修改折扣因子，最优策略会改变吗？
- en: How do you measure the performance of a reinforcement learning agent?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何衡量强化学习智能体的性能？
- en: What is the credit assignment problem? When does it occur? How can you alleviate
    it?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信用分配问题是什么？它何时发生？你如何减轻它？
- en: What is the point of using a replay buffer?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用重放缓冲区有什么意义？
- en: What is an off-policy RL algorithm? What are the benefits?
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是离策略强化学习算法？它的好处是什么？
- en: What is a model-based RL algorithm? Can you give some examples?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于模型强化学习算法是什么？你能给出一些例子吗？
- en: Use policy gradients to solve Gymnasium’s LunarLander-v2 environment.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略梯度解决 Gymnasium 的 LunarLander-v2 环境。
- en: Solve the BipedalWalker-v3 environment using the RL algorithm of your choice.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你选择的强化学习算法解决 BipedalWalker-v3 环境。
- en: If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some
    cheap robotics components, install PyTorch on the Pi, and go wild! Start with
    simple goals, like making the robot turn around to find the brightest angle (if
    it has a light sensor) or the closest object (if it has a sonar sensor), and move
    in that direction. Then you can start using deep learning. For example, if the
    robot has a camera, you can try to implement an object detection algorithm so
    it detects people and moves toward them. You can also try to use RL to make the
    agent learn on its own how to use the motors to achieve that goal. Have fun!
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有多余的约 $100，你可以购买一个 Raspberry Pi 3 和一些便宜的机器人组件，在 Pi 上安装 PyTorch，然后尽情探索！从简单的目标开始，比如让机器人转圈找到最亮的角度（如果它有光传感器）或最近的对象（如果它有声纳传感器），然后朝那个方向移动。然后你可以开始使用深度学习。例如，如果机器人有摄像头，你可以尝试实现一个目标检测算法，使其能够检测到人并朝他们移动。你也可以尝试使用强化学习让智能体自己学习如何使用电机来实现那个目标。祝你好玩！
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: Thank You!
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谢谢！
- en: Before we close the last chapter of this book, I would like to thank you for
    reading it up to the last paragraph. I truly hope that you had as much pleasure
    reading this book as I had writing it, and that it will be useful for your projects,
    big or small.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关闭这本书的最后一章之前，我想感谢你阅读到最后一句话。我真心希望你在阅读这本书时和我写作时一样感到快乐，并且它对你的项目（无论大小）都很有用。
- en: If you find errors, please send feedback. More generally, I would love to know
    what you think, so please don’t hesitate to contact me via O’Reilly, or through
    the *ageron/handson-mlp* GitHub project.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现错误，请发送反馈。更普遍地说，我很乐意知道你的想法，所以请不要犹豫，通过 O’Reilly 或通过 *ageron/handson-mlp*
    GitHub 项目联系我。
- en: 'Going forward, my best advice to you is to practice and practice: try going
    through all the exercises (if you have not done so already), play with the Jupyter
    notebooks, join Kaggle.com or some other ML community, watch ML courses, read
    papers, attend conferences, and meet experts. It also helps tremendously to have
    a concrete project to work on, whether it is for work or fun (ideally for both),
    so if there’s anything you have always dreamt of building, give it a shot! Work
    incrementally; don’t shoot for the moon right away, but stay focused on your project
    and build it piece by piece. It will require patience and perseverance, but when
    you have a walking robot, or a working chatbot, or whatever else you fancy to
    build, it will be immensely rewarding.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的最佳建议是：多加练习：尝试完成所有练习（如果你还没有这样做的话），玩玩Jupyter笔记本，加入Kaggle.com或其他机器学习社区，观看机器学习课程，阅读论文，参加会议，并结识专家。有一个具体的项目来做也会非常有帮助，无论是为了工作还是为了乐趣（理想情况下两者都有），所以如果你一直梦想着构建某个东西，那就试试吧！逐步工作；不要一开始就瞄准月亮，但要专注于你的项目，逐步构建。这需要耐心和毅力，但当你拥有一个行走的机器人、一个工作的聊天机器人或你想要构建的任何其他东西时，这将是非常有回报的。
- en: My greatest hope is that this book will inspire you to build a wonderful ML
    application that will benefit all of us! What will it be?
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我最大的希望是这本书能激励你构建一个能让我们所有人受益的精彩机器学习应用！那会是什么呢？
- en: —*Aurélien Geron*
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: —*奥雷利安·热隆*
- en: '*October 22, 2025*'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '*2025年10月22日*'
- en: '^([1](ch19.html#id4170-marker)) For more details, be sure to check out Richard
    Sutton and Andrew Barto’s book on RL, *Reinforcement Learning: An Introduction*
    (MIT Press).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch19.html#id4170-marker)) 想要了解更多细节，请务必查看理查德·萨顿和安德鲁·巴特罗关于强化学习的书籍，《强化学习：入门》（麻省理工学院出版社）。
- en: ^([2](ch19.html#id4171-marker)) DeepMind was bought by Google for over $500
    million in 2014.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch19.html#id4171-marker)) DeepMind在2014年被谷歌以超过5亿美元的价格收购。
- en: ^([3](ch19.html#id4172-marker)) Volodymyr Mnih et al., “Playing Atari with Deep
    Reinforcement Learning”, arXiv preprint arXiv:1312.5602 (2013), [*https://homl.info/dqn*](https://homl.info/dqn).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch19.html#id4172-marker)) Volodymyr Mnih等人，“使用深度强化学习玩Atari游戏”，arXiv预印本arXiv:1312.5602（2013），[*https://homl.info/dqn*](https://homl.info/dqn)。
- en: '^([4](ch19.html#id4173-marker)) Volodymyr Mnih et al., “Human-Level Control
    Through Deep Reinforcement Learning”, *Nature* 518 (2015): 529–533, [*https://homl.info/dqn2*](https://homl.info/dqn2).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch19.html#id4173-marker)) Volodymyr Mnih等人，“通过深度强化学习实现人类水平控制”，*自然* 518（2015）：529–533，[*https://homl.info/dqn2*](https://homl.info/dqn2)。
- en: ^([5](ch19.html#id4174-marker)) Check out the videos of DeepMind’s system learning
    to play *Space Invaders*, *Breakout*, and other video games at [*https://homl.info/dqn3*](https://homl.info/dqn3).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch19.html#id4174-marker)) 查看DeepMind的系统学习玩《太空侵略者》、《Breakout》和其他电子游戏的视频，请访问[*https://homl.info/dqn3*](https://homl.info/dqn3)。
- en: ^([6](ch19.html#id4189-marker)) Images (a), (d), and (e) are in the public domain.
    Image (b) is a screenshot from the *Ms. Pac-Man* game, copyright Atari (fair use
    in this chapter). Image (c) is reproduced from Wikipedia; it was created by user
    Stevertigo and released under [Creative Commons BY-SA 2.0](https://oreil.ly/O2fAq).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch19.html#id4189-marker)) 图像（a）、（d）和（e）属于公有领域。图像（b）是《Ms. Pac-Man》游戏的截图，版权属于Atari（本章中为合理使用）。图像（c）来自维基百科；它是由用户Stevertigo创建并发布在[Creative
    Commons BY-SA 2.0](https://oreil.ly/O2fAq)下。
- en: ^([7](ch19.html#id4198-marker)) It is often better to give the poor performers
    a slight chance of survival, to preserve some diversity in the “gene pool”.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch19.html#id4198-marker)) 通常给表现不佳的人一点生存的机会，以保留“基因池”中的多样性。
- en: ^([8](ch19.html#id4199-marker)) If there is a single parent, this is called
    *asexual reproduction*. With two (or more) parents, it is called *sexual reproduction*.
    An offspring’s genome (in this case a set of policy parameters) is randomly composed
    of parts of its parents’ genomes.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch19.html#id4199-marker)) 如果只有一个父母，这被称为*无性繁殖*。如果有两个（或更多）父母，则称为*有性繁殖*。后代（在这种情况下是一组策略参数）的基因组是随机由其父母基因组的部分组成的。
- en: ^([9](ch19.html#id4200-marker)) One interesting example of a genetic algorithm
    used for reinforcement learning is the [*NeuroEvolution of Augmenting Topologies*](https://homl.info/neat)
    (NEAT) algorithm. Also check out [*evolutionary policy optimization*](https://homl.info/epo)
    (EPO), proposed in 2025, where a master agent learns stably and efficiently from
    the experiences of a population of agents.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch19.html#id4200-marker)) 强化学习中使用的遗传算法的一个有趣例子是[*增强拓扑结构的神经进化*](https://homl.info/neat)（NEAT）算法。还可以查看2025年提出的[*进化策略优化*](https://homl.info/epo)（EPO），其中主代理从一群代理的经验中稳定且高效地学习。
- en: '^([10](ch19.html#id4201-marker)) This is called *gradient ascent*. It’s just
    like gradient descent, but in the opposite direction: maximizing instead of minimizing.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch19.html#id4201-marker)) 这被称为*梯度上升*。它就像梯度下降，但方向相反：最大化而不是最小化。
- en: '^([11](ch19.html#id4223-marker)) Ronald J. Williams, “Simple Statistical Gradient-Following
    Algorithms for Connectionist Reinforcement Leaning”, *Machine Learning* 8 (1992):
    229–256.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch19.html#id4223-marker)) 罗纳德·J·威廉姆斯，“用于连接主义强化学习的简单统计梯度跟随算法”，《机器学习》8期（1992年）：229–256。
- en: ^([12](ch19.html#id4225-marker)) We generate a new random seed for each episode
    using `torch.randint()`. This ensures that each episode is different, yet the
    whole training process is reproducible if we set PyTorch’s random seed before
    calling `train_reinforce()`.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch19.html#id4225-marker)) 我们为每个回合生成一个新的随机种子，使用`torch.randint()`。这确保了每个回合都是不同的，但如果我们在调用`train_reinforce()`之前设置PyTorch的随机种子，整个训练过程是可重现的。
- en: ^([13](ch19.html#id4234-marker)) A great [2018 post](https://homl.info/rlhard)
    by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch19.html#id4234-marker)) 亚历克斯·伊尔潘在2018年发表的一篇优秀的[博客文章](https://homl.info/rlhard)很好地概述了强化学习最大的困难和局限性。
- en: '^([14](ch19.html#id4239-marker)) Richard Bellman, “A Markovian Decision Process”,
    *Journal of Mathematics and Mechanics* 6, no. 5 (1957): 679–684.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch19.html#id4239-marker)) 理查德·贝尔曼，“马尔可夫决策过程”，《数学与力学杂志》6卷第5期（1957年）：679–684。
- en: '^([15](ch19.html#id4266-marker)) Hado van Hasselt et al., “Deep Reinforcement
    Learning with Double Q-Learning”, *Proceedings of the 30th AAAI Conference on
    Artificial Intelligence* (2015): 2094–2100.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch19.html#id4266-marker)) 霍多·范·哈塞尔特等人，“使用双重Q学习的深度强化学习”，《第30届AAAI人工智能会议论文集》（2015年）：2094–2100。
- en: ^([16](ch19.html#id4270-marker)) Tom Schaul et al., “Prioritized Experience
    Replay”, arXiv preprint arXiv:1511.05952 (2015).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch19.html#id4270-marker)) 托马斯·沙乌等人，“优先经验回放”，arXiv预印本arXiv:1511.05952（2015年）。
- en: ^([17](ch19.html#id4272-marker)) It could also just be that the rewards are
    noisy, in which case there are better methods for estimating an experience’s importance
    (see “Prioritized Experience Replay” for some examples).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch19.html#id4272-marker)) 也可能只是奖励是噪声的，在这种情况下，有更好的方法来估计经验的重要性（例如，参见“优先经验回放”中的某些示例）。
- en: ^([18](ch19.html#id4281-marker)) Ziyu Wang et al., “Dueling Network Architectures
    for Deep Reinforcement Learning”, arXiv preprint arXiv:1511.06581 (2015).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch19.html#id4281-marker)) 王子瑜等人，“用于深度强化学习的对抗网络架构”，arXiv预印本arXiv:1511.06581（2015年）。
- en: '^([19](ch19.html#id4283-marker)) Matteo Hessel et al., “Rainbow: Combining
    Improvements in Deep Reinforcement Learning”, arXiv preprint arXiv:1710.02298
    (2017): 3215–3222.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch19.html#id4283-marker)) 马泰奥·赫塞尔等人，“Rainbow：结合深度强化学习的改进”，arXiv预印本arXiv:1710.02298（2017年）：3215–3222。
- en: '^([20](ch19.html#id4290-marker)) Volodymyr Mnih et al., “Asynchronous Methods
    for Deep Reinforcement Learning”, *Proceedings of the 33rd International Conference
    on Machine Learning* (2016): 1928–1937.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch19.html#id4290-marker)) 瓦洛迪米尔·米欣等人，“深度强化学习的异步方法”，《第33届国际机器学习会议论文集》（2016年）：1928–1937。
- en: '^([21](ch19.html#id4296-marker)) Tuomas Haarnoja et al., “Soft Actor-Critic:
    Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor”,
    *Proceedings of the 35th International Conference on Machine Learning* (2018):
    1856–1865.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch19.html#id4296-marker)) 托马斯·哈诺亚等人，“软演员-评论家：具有随机演员的离策略最大熵深度强化学习”，《第35届国际机器学习会议论文集》（2018年）：1856–1865。
- en: ^([22](ch19.html#id4299-marker)) John Schulman et al., “Proximal Policy Optimization
    Algorithms”, arXiv preprint arXiv:1707.06347 (2017).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch19.html#id4299-marker)) 约翰·舒尔曼等人，“近端策略优化算法”，arXiv预印本arXiv:1707.06347（2017年）。
- en: '^([23](ch19.html#id4300-marker)) John Schulman et al., “Trust Region Policy
    Optimization”, *Proceedings of the 32nd International Conference on Machine Learning*
    (2015): 1889–1897.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch19.html#id4300-marker)) 约翰·舒尔曼等人，“信任域策略优化”，《第32届国际机器学习会议论文集》（2015年）：1889–1897。
- en: ^([24](ch19.html#id4305-marker)) The “v4” suffix is the version number; it’s
    unrelated to frame skipping or the number of parallel environments.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch19.html#id4305-marker)) “v4”后缀是版本号；它与帧跳过或并行环境数量无关。
- en: '^([25](ch19.html#id4314-marker)) David Silver et al., “Mastering the Game of
    Go with Deep Neural Networks and Tree Search”, *Nature* 529 (2016): 484–489.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch19.html#id4314-marker)) 大卫·西尔弗等人，“使用深度神经网络和树搜索掌握围棋”，《自然》529期（2016年）：484–489。
- en: '^([26](ch19.html#id4317-marker)) David Silver et al., “Mastering the Game of
    Go Without Human Knowledge”, *Nature* 550 (2017): 354–359.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch19.html#id4317-marker)) 大卫·西尔弗等人，“无需人类知识的围棋掌握”，《自然》550期（2017年）：354–359。
- en: ^([27](ch19.html#id4318-marker)) David Silver et al., “Mastering Chess and Shogi
    by Self-Play with a General Reinforcement Learning Algorithm”, arXiv preprint
    arXiv:1712.01815.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch19.html#id4318-marker)) 大卫·西尔弗等，“通过通用强化学习算法自我对弈掌握国际象棋和将棋”，arXiv预印本
    arXiv:1712.01815.
- en: ^([28](ch19.html#id4319-marker)) Julian Schrittwieser et al., “Mastering Atari,
    Go, Chess and Shogi by Planning with a Learned Model”, arXiv preprint arXiv:1911.08265
    (2019).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch19.html#id4319-marker)) 朱利安·施里特维瑟等，“通过学习模型进行规划以掌握Atari、围棋、国际象棋和将棋”，arXiv预印本
    arXiv:1911.08265 (2019).
- en: '^([29](ch19.html#id4321-marker)) Deepak Pathak et al., “Curiosity-Driven Exploration
    by Self-Supervised Prediction”, *Proceedings of the 34th International Conference
    on Machine Learning* (2017): 2778–2787.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '^([29](ch19.html#id4321-marker)) Deepak Pathak 等人，“通过自监督预测驱动的探索”，*第34届国际机器学习会议论文集*
    (2017): 2778–2787.'
- en: '^([30](ch19.html#id4325-marker)) Rui Wang et al., “Paired Open-Ended Trailblazer
    (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments
    and Their Solutions”, arXiv preprint arXiv:1901.01753 (2019).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch19.html#id4325-marker)) 王瑞等，“成对开放式开拓者（POET）：无限生成越来越复杂和多样化的学习环境及其解决方案”，arXiv预印本
    arXiv:1901.01753 (2019).
- en: '^([31](ch19.html#id4328-marker)) Rui Wang et al., “Enhanced POET: Open-Ended
    Reinforcement Learning Through Unbounded Invention of Learning Challenges and
    Their Solutions”, arXiv preprint arXiv:2003.08536 (2020).'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch19.html#id4328-marker)) 王瑞等，“通过无界创造学习挑战及其解决方案增强POET：开放式强化学习”，arXiv预印本
    arXiv:2003.08536 (2020).
- en: ^([32](ch19.html#id4329-marker)) Open-Ended Learning Team et al., “Open-Ended
    Learning Leads to Generally Capable Agents”, arXiv preprint arXiv:2107.12808 (2021).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch19.html#id4329-marker)) 开放式学习团队等，“开放式学习导致通用智能体”，arXiv预印本 arXiv:2107.12808
    (2021).
