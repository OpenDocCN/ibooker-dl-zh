- en: Chapter 19\. Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章\. 强化学习
- en: '*Reinforcement learning* (RL) is one of the most exciting fields of machine
    learning today, and also one of the oldest. It has been around since the 1950s,
    producing many interesting applications over the years,⁠^([1](ch19.html#id4170))
    particularly in games (e.g., *TD-Gammon*, a backgammon-playing program) and in
    machine control, but seldom making the headline news. However, a revolution took
    place in 2013, when researchers from a British startup called DeepMind⁠^([2](ch19.html#id4171))
    demonstrated a system that could learn to play just about any Atari game from
    scratch,⁠^([3](ch19.html#id4172)) eventually outperforming humans⁠^([4](ch19.html#id4173))
    in most of them, using only raw pixels as inputs and without any prior knowledge
    of the rules of the games.⁠^([5](ch19.html#id4174)) This was the first of a series
    of amazing feats:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*（RL）是当今最激动人心的机器学习领域之一，同时也是最古老的领域之一。它自20世纪50年代以来一直存在，多年来产生了许多有趣的应用，尤其是在游戏（例如，国际跳棋程序TD-Gammon）和机器控制方面，但很少成为头条新闻。然而，2013年发生了一场革命，当时来自一家名为DeepMind的英国初创公司的研究人员展示了一个可以从零开始学习几乎任何Atari游戏的系统，最终在大多数游戏中超过了人类，仅使用原始像素作为输入，并且没有任何关于游戏规则的先验知识。这是一系列惊人壮举中的第一个：'
- en: In 2016, DeepMind’s AlphaGo beat Lee Sedol, a legendary professional player
    of the game of Go; and in 2017, it beat Ke Jie, the world champion. No program
    had ever come close to beating a master of this game, let alone the very best.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2016年，DeepMind的AlphaGo击败了围棋传奇职业选手李世石；2017年，它又击败了世界冠军柯洁。没有任何程序曾接近击败这个游戏的宗师，更不用说最顶尖的选手了。
- en: In 2020, DeepMind released AlphaFold, which can predict the 3D shape of proteins
    with unprecedented accuracy. This is a game changer in biology, chemistry, and
    medicine. In fact, Demis Hassabis (founder and CEO) and John Jumper (director)
    were awarded the Nobel Prize in Chemistry for AlphaFold.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2020年，DeepMind发布了AlphaFold，它可以以前所未有的精度预测蛋白质的3D形状。这在生物学、化学和医学领域是一次变革。实际上，Demis
    Hassabis（创始人兼首席执行官）和John Jumper（总监）因AlphaFold获得了诺贝尔化学奖。
- en: In 2022, DeepMind released AlphaCode, which can generate code at a competitive
    programming level.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2022年，DeepMind发布了AlphaCode，它可以生成具有竞技编程水平的代码。
- en: In 2023, DeepMind released GNoME which can predict new crystal structures, including
    hundreds of thousands of predicted stable materials.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2023年，DeepMind发布了GNoME，它可以预测新的晶体结构，包括数十万个预测的稳定材料。
- en: 'So how did DeepMind researchers achieve all of this? Well, they applied the
    power of deep learning to the field of reinforcement learning, and it worked beyond
    their wildest dreams: *deep reinforcement learning* was born. Today, although
    DeepMind continues to lead the way, many other organizations have joined in, and
    the whole field is boiling with new ideas, with a wide range of applications.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，DeepMind的研究人员是如何实现这一切的呢？嗯，他们将深度学习的力量应用于强化学习领域，结果超出了他们的预期：*深度强化学习*诞生了。今天，尽管DeepMind继续引领潮流，但许多其他组织也加入了进来，整个领域充满了新的想法，应用范围广泛。
- en: 'In this chapter I will first explain what reinforcement learning is and what
    it’s good at, then present three of the most important families of techniques
    in deep reinforcement learning: policy gradients, deep Q-networks (including a
    discussion of Markov decision processes), and lastly, actor-critic methods, including
    the popular PPO, which we will use to beat an Atari game. So let’s get started!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将首先解释什么是强化学习以及它的优势，然后介绍深度强化学习中最重要的一些技术家族：策略梯度、深度Q网络（包括马尔可夫决策过程的讨论），最后是演员-评论家方法，包括流行的PPO，我们将用它来击败Atari游戏。那么，让我们开始吧！
- en: What Is Reinforcement Learning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: In reinforcement learning, a software *agent* makes *observations* and takes
    *actions* within an *environment*, and in return it receives *rewards* from the
    environment. Its objective is to learn to act in a way that will maximize its
    expected rewards over time. If you don’t mind a bit of anthropomorphism, you can
    think of positive rewards as pleasure, and negative rewards as pain (the term
    “reward” is a bit misleading in this case). In short, the agent acts in the environment
    and learns by trial and error to maximize its pleasure and minimize its pain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个软件*代理*在*环境*中做出*观察*和*动作*，并从环境中获得*奖励*。它的目标是学习以最大化其预期奖励的方式行事。如果你不介意一点拟人化，你可以把正奖励看作是快乐，负奖励看作是痛苦（在这种情况下，“奖励”这个词有点误导）。简而言之，代理在环境中行动，并通过试错来学习最大化快乐和最小化痛苦。
- en: 'This is quite a broad setting that can apply to a wide variety of tasks. Here
    are a few examples (see [Figure 19-1](#rl_examples_diagram)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当广泛的设置，可以应用于各种任务。以下是一些例子（见图19-1）：
- en: The agent can be the program controlling a robot. In this case, the environment
    is the real world, the agent observes the environment through a set of *sensors*,
    such as cameras and touch sensors, and its actions consist of sending signals
    to activate motors. It may be programmed to get positive rewards whenever it approaches
    the target destination, and negative rewards whenever it wastes time or goes in
    the wrong direction.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以是控制机器人的程序。在这种情况下，环境是现实世界，代理通过一组*传感器*（如摄像头和触觉传感器）观察环境，其动作包括发送信号以激活电机。它可能被编程为每当它接近目标目的地时获得正奖励，而每当它浪费时间或走错方向时获得负奖励。
- en: The agent can be the program controlling *Ms. Pac-Man*. In this case, the environment
    is a simulation of the Atari game, the actions are the nine possible joystick
    positions (upper left, down, center, and so on), the observations are screenshots,
    and the rewards are just the game points.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以是控制*Ms. Pac-Man*的程序。在这种情况下，环境是Atari游戏的模拟，动作是九个可能的摇杆位置（左上角、向下、中心等），观察是屏幕截图，奖励只是游戏分数。
- en: Similarly, the agent can be the program playing a board game such as Go. It
    only gets a reward if it wins.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，代理可以是玩围棋等棋类游戏的程序。只有当它获胜时才会获得奖励。
- en: The agent does not have to control a physically (or virtually) moving thing.
    For example, it can be a smart thermostat, getting positive rewards whenever it
    is close to the target temperature and saves energy, and negative rewards when
    humans need to tweak the temperature, so the agent must learn to anticipate human
    needs.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理不需要控制一个物理上（或虚拟上）移动的物体。例如，它可以是智能恒温器，每当它接近目标温度并节省能源时，就会获得正奖励，而当人类需要调整温度时，就会获得负奖励，因此代理必须学会预测人类的需求。
- en: The agent can observe stock market prices and decide how much to buy or sell
    every second. Rewards are obviously the monetary gains and losses.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可以观察股票市场价格，并决定每秒买卖多少。奖励显然是货币的盈亏。
- en: Note that there may not be any positive rewards at all; for example, the agent
    may move around in a maze, getting a negative reward at every time step, so it
    had better find the exit as quickly as possible! There are many other examples
    of tasks to which reinforcement learning is well suited, such as self-driving
    cars, recommender systems, placing ads on a web page, or controlling where an
    image classification system should focus its attention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可能根本没有任何正奖励；例如，代理可能在迷宫中移动，每一步都获得负奖励，因此它最好尽快找到出口！强化学习非常适合许多其他任务，例如自动驾驶汽车、推荐系统、在网页上放置广告，或者控制图像分类系统应该关注的地方。
- en: '![Reinforcement learning examples include a Mars rover, the _Ms. Pac-Man_ game,
    a Go board, a smart thermostat, and a digital stock trading interface.](assets/hmls_1901.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习示例包括火星车、Ms. Pac-Man 游戏、围棋棋盘、智能恒温器和数字股票交易界面。](assets/hmls_1901.png)'
- en: 'Figure 19-1\. Reinforcement learning examples: (a) robotics, (b) *Ms. Pac-Man*,
    (c) Go player, (d) thermostat, (e) automatic trader⁠^([6](ch19.html#id4189))'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-1\. 强化学习示例：（a）机器人，（b）*Ms. Pac-Man*，（c）围棋选手，（d）恒温器，（e）自动交易员⁠^([6](ch19.html#id4189))
- en: 'Let’s now turn to one large family of RL algorithms: *policy gradients*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向强化学习算法的一个大型家族：*策略梯度*。
- en: Policy Gradients
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度
- en: The algorithm a software agent uses to determine its actions is called its *policy*.
    The policy can be any algorithm you can think of, such as a neural network taking
    observations as inputs and outputting the action to take (see [Figure 19-2](#rl_with_nn_policy_diagram)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 软件代理用来确定其动作的算法被称为其*策略*。策略可以是任何你能想到的算法，例如一个神经网络，它以观察作为输入，并输出要采取的动作（参见[图19-2](#rl_with_nn_policy_diagram))）。
- en: '![Diagram illustrating reinforcement learning with a robot agent using a neural
    network policy to interact with an environment, showing the flow of actions, rewards,
    and observations.](assets/hmls_1902.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用神经网络策略与环境交互的机器人代理的强化学习图，显示了动作、奖励和观察的流程。](assets/hmls_1902.png)'
- en: Figure 19-2\. Reinforcement learning using a neural network policy
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-2. 使用神经网络策略的强化学习
- en: The policy does not even have to be deterministic. In fact, in some cases it
    does not even have to observe the environment, as long as it can get rewards!
    For example, consider a blind robotic vacuum cleaner whose reward is the amount
    of dust it picks up in 30 minutes. Its policy could be to move forward with some
    probability *p* every second, or randomly rotate left or right with probability
    1 – *p*. The rotation angle would be a random angle between –*r* and +*r*. Since
    this policy involves some randomness, it is called a *stochastic policy*. The
    robot will have an erratic trajectory, which guarantees that it will eventually
    get to any place it can reach and pick up all the dust. The question is, how much
    dust will it pick up in 30 minutes?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 策略甚至不必是确定性的。实际上，在某些情况下，它甚至不必观察环境，只要它能获得奖励！例如，考虑一个盲目的机器人吸尘器，其奖励是在30分钟内收集的灰尘量。其策略可以是每秒钟以概率*p*向前移动，或者以概率1
    – *p*随机左转或右转。旋转角度将是介于- *r*和+ *r*之间的随机角度。由于这种策略涉及一些随机性，它被称为*随机策略*。机器人将会有一个不规则的轨迹，这保证了它最终会到达它能到达的任何地方并收集所有灰尘。问题是，它在30分钟内能收集多少灰尘？
- en: 'How would you train such a robot? There are just two *policy parameters* you
    can tweak: the probability *p* and the angle range *r*. One possible learning
    algorithm could be to try out many different values for these parameters, and
    pick the combination that performs best (see [Figure 19-3](#policy_search_diagram)).
    This is an example of *policy search*, in this case using a brute-force approach.
    When the *policy space* is too large (which is generally the case), finding a
    good set of parameters this way is like searching for a needle in a gigantic haystack.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何训练这样的机器人？你可以调整的只有两个*策略参数*：概率*p*和角度范围*r*。一个可能的学习算法是尝试这些参数的许多不同值，并选择表现最好的组合（参见[图19-3](#policy_search_diagram))）。这是一个*策略搜索*的例子，在这种情况下使用的是穷举法。当*策略空间*太大（这通常是情况）时，以这种方式找到一组好的参数就像在大堆稻草中寻找一根针一样。
- en: Another way to explore the policy space is to use *genetic algorithms*. For
    example, you could randomly create a first generation of 100 policies and try
    them out, then “kill” the 80 worst policies⁠^([7](ch19.html#id4198)) and make
    the 20 survivors produce 4 offspring each. An offspring is a copy of its parent⁠^([8](ch19.html#id4199))
    plus some random variation. The surviving policies plus their offspring together
    constitute the second generation. You can continue to iterate through generations
    this way until you find a good policy.⁠^([9](ch19.html#id4200))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 探索政策空间的另一种方法是使用*遗传算法*。例如，你可以随机创建第一代100个策略并尝试它们，然后“淘汰”80个最差的策略⁠^([7](ch19.html#id4198))，让20个幸存者各自产生4个后代。后代是其父母的副本⁠^([8](ch19.html#id4199))加上一些随机变异。幸存策略及其后代共同构成了第二代。你可以继续这样迭代通过代数，直到找到好的策略。⁠^([9](ch19.html#id4200))
- en: '![Diagram illustrating four points in the policy space on the left and their
    corresponding agent behaviors on the right, demonstrating the exploration of policy
    space in reinforcement learning.](assets/hmls_1903.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![说明左侧政策空间中的四个点及其右侧对应的代理行为的图，展示了强化学习中政策空间的探索。](assets/hmls_1903.png)'
- en: Figure 19-3\. Four points in the policy space (left) and the agent’s corresponding
    behavior (right)
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-3. 政策空间中的四个点（左侧）及其对应的代理行为（右侧）
- en: Yet another approach is to use optimization techniques by evaluating the gradients
    of the rewards with regard to the policy parameters, then tweaking these parameters
    by following the gradients toward higher rewards.⁠^([10](ch19.html#id4201)) Algorithms
    that follow this strategy are known as *policy gradient* (PG) algorithms. But
    before we can implement them, we first need to create an environment for the agent
    to live in⁠—so it’s time to introduce the Gymnasium library.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用优化技术，通过评估奖励相对于策略参数的梯度，然后通过跟随梯度向更高奖励的方向调整这些参数。遵循这种策略的算法被称为 *策略梯度*（PG）算法。但在我们能够实现它们之前，我们首先需要为智能体创建一个生存的环境——因此，现在是介绍
    Gymnasium 库的时候了。
- en: Introduction to the Gymnasium Library
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gymnasium 库简介
- en: 'One of the challenges of reinforcement learning is that in order to train an
    agent, you first need to have a working environment. If you want to program an
    agent that will learn to play an Atari game, you will need an Atari game simulator.
    If you want to program a walking robot, then the environment is the real world,
    and you can directly train your robot in that environment. However, this has its
    limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed
    up time either—adding more computing power won’t make the robot move any faster—and
    it’s generally too expensive to train 1,000 robots in parallel. In short, training
    is hard and slow in the real world, so you generally need a *simulated environment*
    at least for bootstrap training. For example, you might use a library like [PyBullet](https://pybullet.org)
    or [MuJoCo](https://mujoco.org) for 3D physics simulation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个挑战是，为了训练一个智能体，您首先需要一个工作环境。如果您想编写一个学习玩 Atari 游戏的智能体，您将需要一个 Atari 游戏模拟器。如果您想编写一个行走机器人，那么环境就是现实世界，您可以直接在那个环境中训练您的机器人。然而，这有其局限性：如果机器人从悬崖上掉下来，您不能只是点击撤销。您也不能加快时间——增加计算能力不会让机器人移动得更快——并且并行训练
    1,000 个机器人通常成本太高。简而言之，在现实世界中训练既困难又缓慢，因此您至少需要一个 *模拟环境* 来进行启动训练。例如，您可能使用 [PyBullet](https://pybullet.org)
    或 [MuJoCo](https://mujoco.org) 这样的库来进行 3D 物理模拟。
- en: The [Gymnasium library](https://gymnasium.farama.org) is an open source toolkit
    that provides a wide variety of simulated environments (Atari games, board games,
    2D and 3D physics simulations, and so on), that you can use to train agents, compare
    them, or develop new RL algorithms. It’s the successor of OpenAI Gym, and is now
    maintained by a community of researchers and developers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Gymnasium 库](https://gymnasium.farama.org)是一个开源工具包，它提供了各种模拟环境（Atari 游戏、棋盘游戏、2D
    和 3D 物理模拟等），您可以使用这些环境来训练智能体、比较它们或开发新的强化学习算法。它是 OpenAI Gym 的继任者，现在由一群研究人员和开发者维护。'
- en: Gymnasium is preinstalled on Colab, along with the Arcade Learning Environment
    (ALE) library `ale_py`, which is an emulator for Atari 2600 games and is required
    for all the Atari environments, as well as the Box2D library, required for several
    environments with 2D physics. If you are coding on your own machine instead of
    Colab, and you followed the installation instructions at [*https://homl.info/install-p*](https://homl.info/install-p),
    then you should be good to go.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium 预装在 Colab 上，包括 Arcade Learning Environment（ALE）库 `ale_py`，这是一个 Atari
    2600 游戏的模拟器，对于所有 Atari 环境都是必需的，以及 Box2D 库，它是用于几个具有 2D 物理的环境所必需的。如果您在自己的机器上编码而不是在
    Colab 上，并且您遵循了 [*https://homl.info/install-p*](https://homl.info/install-p) 上的安装说明，那么您应该可以开始了。
- en: 'Let’s start by importing Gymnasium and making an environment:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入 Gymnasium 并创建一个环境：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we’ve created a CartPole environment (version 1). This is a 2D simulation
    in which a cart can be accelerated left or right in order to balance a pole placed
    on top of it (see [Figure 19-4](#cart_pole_diagram))—a classic control task. I’ll
    explain `render_mode` and `max_episode_steps` shortly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个 CartPole 环境（版本 1）。这是一个 2D 模拟，其中一辆小车可以向左或向右加速，以平衡放置在其顶部的一根杆子（参见[图
    19-4](#cart_pole_diagram)）——这是一个经典的控制任务。我很快会解释 `render_mode` 和 `max_episode_steps`。
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `gym.envs.registry` dictionary contains the names and specifications of
    all the available environments. You can print a nice list with `gym.pprint_registry()`.
    The Atari environments will only be available once we start the ALE emulator.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym.envs.registry` 字典包含了所有可用环境的名称和规范。您可以使用 `gym.pprint_registry()` 打印出一个漂亮的列表。Atari
    环境只有在启动 ALE 模拟器后才会可用。'
- en: '![Diagram illustrating the CartPole environment with labeled vectors showing
    the cart''s position, velocity, pole angle, and angular velocity.](assets/hmls_1904.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![说明 CartPole 环境的图解，其中标有向量的标签显示小车的位置、速度、杆的角度和角速度。](assets/hmls_1904.png)'
- en: Figure 19-4\. The CartPole environment
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-4\. CartPole 环境
- en: 'After the environment is created, you must initialize it using the `reset()`
    method, optionally specifying a random seed. This returns the first observation.
    Observations depend on the type of environment. For the CartPole environment,
    each observation is a NumPy array containing four floats representing the cart’s
    horizontal position (`0.0` = center), its velocity (positive means right), the
    angle of the pole (`0.0` = vertical), and its angular velocity (positive means
    clockwise). The `reset()` method also returns a dictionary that may contain extra
    environment-specific information. This can be useful for debugging and sometimes
    for training. For example, in many Atari environments, it contains the number
    of lives left. However, in the CartPole environment, this dictionary is empty:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 环境创建完成后，您必须使用 `reset()` 方法对其进行初始化，可选地指定一个随机种子。这将返回第一个观察结果。观察结果取决于环境的类型。对于 CartPole
    环境，每个观察结果都是一个包含四个浮点数的 NumPy 数组，分别代表小车水平位置（`0.0` = 中间），其速度（正值表示向右），杆的角度（`0.0` =
    垂直），以及其角速度（正值表示顺时针）。`reset()` 方法还返回一个可能包含额外环境特定信息的字典。这可以用于调试，有时也用于训练。例如，在许多 Atari
    环境中，它包含剩余生命值。然而，在 CartPole 环境中，这个字典是空的：
- en: '[PRE1] `array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)`
    `>>>` `info` `` `{}` `` [PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] `array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)`
    `>>>` `info` `` `{}` `` [PRE2]'
- en: '[PRE3][PRE4]``py[PRE5]``py Let’s call the `render()` method to render this
    environment as an image. Since we set `render_mode="rgb_array"` when creating
    the environment, the image will be returned as a NumPy array (you can then use
    Matplotlib’s `imshow()` function to display this image):    [PRE6]py   [PRE7]py``
    Now let’s ask the environment what actions are possible:    [PRE8]py   [PRE9]py
    [PRE10]`py [PRE11]py`` [PRE12]py[PRE13][PRE14][PRE15][PRE16]py[PRE17]` ## Neural
    Network Policies    Let’s create a neural network policy. This neural network
    will take an observation as input, and it will output the action to be executed,
    just like the policy we hardcoded earlier. More precisely, it will estimate a
    probability for each action, then it will select an action randomly, according
    to the estimated probabilities (see [Figure 19-5](#neural_network_policy_diagram)).
    In the case of the CartPole environment, there are just two possible actions (left
    or right), so we only need one output neuron. It will output the probability *p*
    of action 1 (right), and of course the probability of action 0 (left) will be
    1 – *p*. For example, if it outputs 0.7, then we will pick action 1 with 70% probability,
    or action 0 with 30% probability (this is a *Bernoulli distribution* with *p*
    = 0.7).  ![Diagram illustrating a neural network policy where observations are
    processed through hidden layers to generate a probability for action, leading
    to random action sampling.](assets/hmls_1905.png)  ###### Figure 19-5\. Neural
    network policy    You may wonder why we are picking a random action based on the
    probabilities given by the neural network, rather than just picking the action
    with the highest score. This approach lets the agent find the right balance between
    *exploring* new actions and *exploiting* the actions that are known to work well.
    Here’s an analogy: suppose you go to a restaurant for the first time, and all
    the dishes look equally appealing, so you randomly pick one. If it turns out to
    be good, you can increase the probability that you’ll order it next time, but
    you shouldn’t increase that probability up to 100%, or you will never try the
    other dishes, some of which may be even better than the one you tried. This *exploration*/*exploitation
    dilemma* is central in reinforcement learning.    Also note that in this particular
    environment, the past actions and observations can safely be ignored, since each
    observation contains the environment’s full state. If there were some hidden state,
    then you might need to consider past actions and observations as well. For example,
    if the environment only revealed the position of the cart but not its velocity,
    you would have to consider not only the current observation but also the previous
    observation in order to estimate the current velocity. Another example is when
    the observations are noisy; in that case, you generally want to use the past few
    observations to estimate the most likely current state. The CartPole problem is
    thus as simple as can be; the observations are noise-free, and they contain the
    environment’s full state.    Let’s use PyTorch to implement a basic neural network
    policy for CartPole:    [PRE18]    Our policy network is a tiny MLP, since it’s
    a fairly simple task. The number of inputs is the size of the environment’s state:
    in the case of CartPole, it is just the size of a single observation, which is
    four. We have just one hidden layer with five units (no need for more in this
    case). Finally, we want to output a single probability, so we have a single output
    neuron. If there were more than two possible actions, there would be one output
    neuron per action instead. For performance and numerical stability, we don’t add
    a sigmoid function at the end, so the network will actually output logits rather
    than probabilities.    Next let’s define a function that will use this policy
    network to choose an action:    [PRE19]    The function takes a single observation,
    converts it to a tensor, and passes it to the policy network to get the logit
    for action 1 (right). It then creates a `Bernoulli` probability distribution with
    this logit, and it samples an action from it: this distribution will output 1
    (right) with probability *p* = exp(logit) / (1 + exp(logit)), and 0 (left) with
    probability 1 – *p*. If there were more than two possible actions, you would use
    a `Categorical` distribution instead. Lastly, we compute the log probability of
    the sampled action (i.e., either log(*p*) or log(1 – *p*)): this log probability
    will be needed later for training.    ###### Tip    If the action space is continuous,
    you can use a Gaussian distribution instead of a Bernoulli or categorical distribution.
    Instead of predicting logits, the policy network must predict the mean and standard
    deviation (or the log of the standard deviation) of the distribution. The log
    of the standard deviation is often clipped to ensure the distribution is neither
    too wide nor too narrow.    OK, we now have a neural network policy that can take
    an environment state (in this case, a single observation) and choose an action.
    But how do we train it?    ## Evaluating Actions: The Credit Assignment Problem    If
    we knew what the best action was at each step, we could train the neural network
    as usual by minimizing the cross-entropy between the estimated probability distribution
    and the target probability distribution. It would just be regular supervised learning.
    However, in reinforcement learning the only guidance the agent gets is through
    rewards, and rewards are typically sparse and delayed. For example, if the agent
    manages to balance the pole for a total of 100 steps, how can it know which of
    the 100 actions it took were good, and which of them were bad? All it knows is
    that the pole fell after the last action, but surely this last action is not entirely
    responsible. This is called the *credit assignment problem*: when the agent gets
    a reward (or a penalty), it is hard for it to know which actions should get credited
    (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well;
    will it understand what it is being rewarded for?    To simplify credit assignment,
    a common strategy is to evaluate an action based on the sum of all the rewards
    that come after it, applying a *discount factor, _γ* (gamma), at each step. This
    sum of discounted rewards is called the action’s *return*. Consider the example
    in [Figure 19-6](#discounted_rewards_diagram). If an agent decides to go right
    three times in a row and gets +10 reward after the first step, 0 after the second
    step, and finally –50 after the third step, then assuming we use a discount factor
    *γ* = 0.8, the first action will have a return of 10 + *γ* × 0 + *γ*² × (–50)
    = –22.  ![Diagram showing a robot earning sequential rewards of +10, 0, and -50
    as it moves right, with calculated discounted returns of -22 and -40, highlighting
    the effect of an 80% discount factor.](assets/hmls_1906.png)  ###### Figure 19-6\.
    Computing an action’s return: the sum of discounted future rewards    The following
    function computes the returns, given the rewards and the discount factor:    [PRE20]    This
    function produces the expected result:    [PRE21]   `If the discount factor is
    close to 0, then future rewards won’t count for much compared to immediate rewards.
    Conversely, if the discount factor is close to 1, then rewards far into the future
    will count almost as much as immediate rewards. Typical discount factors vary
    from 0.9 to 0.99\. With a discount factor of 0.95, rewards 13 steps into the future
    count roughly for half as much as immediate rewards (since 0.95^(13) ≈ 0.5), while
    with a discount factor of 0.99, rewards 69 steps into the future count for half
    as much as immediate rewards. In the CartPole environment, actions have fairly
    short-term effects, so choosing a low discount factor of 0.95 seems reasonable,
    and it will help with credit assignment, making training faster and more stable.
    However, if the discount factor is set too low, then the agent will learn a suboptimal
    strategy, focusing too much on short-term gains.    Now that we have a way to
    evaluate each action, we are ready to train our first agent using policy gradients.
    Let’s see how.`  [PRE22] def run_episode(model, env, seed=None):     log_probs,
    rewards = [], []     obs, info = env.reset(seed=seed)     while True:  # the environment
    will truncate the episode if it is too long         action, log_prob = choose_action(model,
    obs)         obs, reward, done, truncated, _info = env.step(action)         log_probs.append(log_prob)         rewards.append(reward)         if
    done or truncated:             return log_probs, rewards [PRE23] def train_reinforce(model,
    optimizer, env, n_episodes, discount_factor):     for episode in range(n_episodes):         seed
    = torch.randint(0, 2**32, size=()).item()         log_probs, rewards = run_episode(model,
    env, seed=seed)         returns = compute_returns(rewards, discount_factor)         std_returns
    = (returns - returns.mean()) / (returns.std() + 1e-7)         losses = [-logp
    * rt for logp, rt in zip(log_probs, std_returns)]         loss = torch.cat(losses).sum()         optimizer.zero_grad()         loss.backward()         optimizer.step()         print(f"\rEpisode
    {episode + 1}, Reward: {sum(rewards):.2f}", end=" ") [PRE24] torch.manual_seed(42)
    model = PolicyNetwork() optimizer = torch.optim.NAdam(model.parameters(), lr=0.06)
    train_reinforce(model, optimizer, env, n_episodes=200, discount_factor=0.95) [PRE25]`
    [PRE26][PRE27][PRE28][PRE29][PRE30][PRE31] transition_probabilities = [  # shape=[s,
    a, s'']     [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],     [[0.0, 1.0,
    0.0], None, [0.0, 0.0, 1.0]],     [None, [0.8, 0.1, 0.1], None] ] rewards = [  #
    shape=[s, a, s'']     [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],     [[0, 0, 0], [0,
    0, 0], [0, 0, -50]],     [[0, 0, 0], [+40, 0, 0], [0, 0, 0]] ] possible_actions
    = [[0, 1, 2], [0, 2], [1]] [PRE32] Q_values = np.full((3, 3), -np.inf)  # -np.inf
    for impossible actions for state, actions in enumerate(possible_actions):     Q_values[state,
    actions] = 0.0  # for all possible actions [PRE33] gamma = 0.90  # the discount
    factor  for iteration in range(50):     Q_prev = Q_values.copy()     for s in
    range(3):         for a in possible_actions[s]:             Q_values[s, a] = np.sum([                     transition_probabilities[s][a][sp]                     *
    (rewards[s][a][sp] + gamma * Q_prev[sp].max())                 for sp in range(3)])
    [PRE34] >>> Q_values `array([[18.91891892, 17.02702702, 13.62162162],`  `[ 0\.        ,        -inf,
    -4.87971488],`  `[       -inf, 50.13365013,        -inf]])` [PRE35]` For example,
    when the agent is in state *s*[0] and it chooses action *a*[1], the expected sum
    of discounted future rewards is approximately 17.0.    For each state, we can
    find the action that has the highest Q-value:    [PRE36]   `This gives us the
    optimal policy for this MDP when using a discount factor of 0.90: in state *s*[0]
    choose action *a*[0], in state *s*[1] choose action *a*[0] (i.e., stay put), and
    in state *s*[2] choose action *a*[1] (the only possible action). Interestingly,
    if we increase the discount factor to 0.95, the optimal policy changes: in state
    *s*[1] the best action becomes *a*[2] (go through the fire!). This makes sense
    because the more you value future rewards, the more you are willing to put up
    with some pain now for the promise of future bliss.` [PRE37]`` [PRE38] def step(state,
    action):     probas = transition_probabilities[state][action]     next_state =
    np.random.choice([0, 1, 2], p=probas)     reward = rewards[state][action][next_state]     return
    next_state, reward [PRE39] def exploration_policy(state):     return np.random.choice(possible_actions[state])
    [PRE40] alpha0 = 0.05  # initial learning rate decay = 0.005  # learning rate
    decay gamma = 0.90  # discount factor state = 0  # initial state  for iteration
    in range(10_000):     action = exploration_policy(state)     next_state, reward
    = step(state, action)     next_value = Q_values[next_state].max()  # greedy policy
    at the next step     alpha = alpha0 / (1 + iteration * decay)     Q_values[state,
    action] *= 1 - alpha     Q_values[state, action] += alpha * (reward + gamma *
    next_value)     state = next_state [PRE41] class DQN(nn.Module):     def __init__(self):         super().__init__()         self.net
    = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),                                  nn.Linear(32,
    32), nn.ReLU(),                                  nn.Linear(32, 2))      def forward(self,
    state):         return self.net(state) [PRE42] def choose_dqn_action(model, obs,
    epsilon=0.0):         if torch.rand(()) < epsilon:  # epsilon greedy policy             return
    torch.randint(2, size=()).item()         else:             state = torch.as_tensor(obs)             Q_values
    = model(state)             return Q_values.argmax().item()  # optimal according
    to the DQN [PRE43] def sample_experiences(replay_buffer, batch_size):     indices
    = torch.randint(len(replay_buffer), size=[batch_size])     batch = [replay_buffer[index]
    for index in indices.tolist()]     return [to_tensor([exp[index] for exp in batch])
    for index in range(6)]  def to_tensor(data):     array = np.stack(data)     dtype
    = torch.float32 if array.dtype == np.float64 else None     return torch.as_tensor(array,
    dtype=dtype) [PRE44] def play_and_record_episode(model, env, replay_buffer, epsilon,
    seed=None):     obs, _info = env.reset(seed=seed)     total_rewards = 0     model.eval()     with
    torch.no_grad():         while True:             action = choose_dqn_action(model,
    obs, epsilon)             next_obs, reward, done, truncated, _info = env.step(action)             experience
    = (obs, action, reward, next_obs, done, truncated)             replay_buffer.append(experience)             total_rewards
    += reward             if done or truncated:                 return total_rewards             obs
    = next_obs [PRE45] def dqn_training_step(model, optimizer, criterion, replay_buffer,
    batch_size,                       discount_factor):     experiences = sample_experiences(replay_buffer,
    batch_size)     state, action, reward, next_state, done, truncated = experiences     with
    torch.inference_mode():         next_Q_value = model(next_state)      max_next_Q_value,
    _ = next_Q_value.max(dim=1)     running = (~(done | truncated)).float()  # 0 if
    s'' is over, 1 if running     target_Q_value = reward + running * discount_factor
    * max_next_Q_value     all_Q_values = model(state)     Q_value = all_Q_values.gather(dim=1,
    index=action.unsqueeze(1))     loss = criterion(Q_value, target_Q_value.unsqueeze(1))     optimizer.zero_grad()     loss.backward()     optimizer.step()
    [PRE46] from collections import deque  def train_dqn(model, env, replay_buffer,
    optimizer, criterion, n_episodes=800,               warmup=30, batch_size=32,
    discount_factor=0.95):     totals = []     for episode in range(n_episodes):         epsilon
    = max(1 - episode / 500, 0.01)         seed = torch.randint(0, 2**32, size=()).item()         total_rewards
    = play_and_record_episode(model, env, replay_buffer,                                                 epsilon,
    seed=seed)         print(f"\rEpisode: {episode + 1}, Rewards: {total_rewards}",
    end=" ")         totals.append(total_rewards)         if episode >= warmup:             dqn_training_step(model,
    optimizer, criterion, replay_buffer,                               batch_size,
    discount_factor)     return totals  torch.manual_seed(42) dqn = DQN() optimizer
    = torch.optim.NAdam(dqn.parameters(), lr=0.03) mse = nn.MSELoss() replay_buffer
    = deque(maxlen=100_000) totals = train_dqn(dqn, env, replay_buffer, optimizer,
    mse) [PRE47]` [PRE48]`` [PRE49] class ActorCritic(nn.Module):     def __init__(self):         super().__init__()         self.body
    = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),                                   nn.Linear(32,
    32), nn.ReLU())         self.actor_head = nn.Linear(32, 1)  # outputs action logits         self.critic_head
    = nn.Linear(32, 1)  # outputs state values      def forward(self, state):         features
    = self.body(state)         return self.actor_head(features), self.critic_head(features)
    [PRE50] def choose_action_and_evaluate(model, obs):     state = torch.as_tensor(obs)     logit,
    state_value = model(state)     dist = torch.distributions.Bernoulli(logits=logit)     action
    = dist.sample()     log_prob = dist.log_prob(action)     return int(action.item()),
    log_prob, state_value [PRE51] def ac_training_step(optimizer, criterion, state_value,
    target_value, log_prob,                      critic_weight):     td_error = target_value
    - state_value     actor_loss = -log_prob * td_error.detach()     critic_loss =
    criterion(state_value, target_value)     loss = actor_loss + critic_weight * critic_loss     optimizer.zero_grad()     loss.backward()     optimizer.step()
    [PRE52] def get_target_value(model, next_obs, reward, done, truncated, discount_factor):     with
    torch.inference_mode():         _, _, next_state_value = choose_action_and_evaluate(model,
    next_obs)      running = 0.0 if (done or truncated) else 1.0     target_value
    = reward + running * discount_factor * next_state_value     return target_value
    [PRE53] def run_episode_and_train(model, optimizer, criterion, env, discount_factor,                           critic_weight,
    seed=None):     obs, _info = env.reset(seed=seed)     total_rewards = 0     while
    True:         action, log_prob, state_value = choose_action_and_evaluate(model,
    obs)         next_obs, reward, done, truncated, _info = env.step(action)         target_value
    = get_target_value(model, next_obs, reward, done,                                         truncated,
    discount_factor)         ac_training_step(optimizer, criterion, state_value, target_value,                          log_prob,
    critic_weight)         total_rewards += reward         if done or truncated:             return
    total_rewards         obs = next_obs [PRE54] def train_actor_critic(model, optimizer,
    criterion, env, n_episodes=400,                        discount_factor=0.95, critic_weight=0.3):     totals
    = []     model.train()     for episode in range(n_episodes):         seed = torch.randint(0,
    2**32, size=()).item()         total_rewards = run_episode_and_train(model, optimizer,
    criterion, env,                                               discount_factor,
    critic_weight,                                               seed=seed)         totals.append(total_rewards)         print(f"\rEpisode:
    {episode + 1}, Rewards: {total_rewards}", end=" ")      return totals [PRE55]
    torch.manual_seed(42) ac_model = ActorCritic() optimizer = torch.optim.NAdam(ac_model.parameters(),
    lr=1.1e-3) criterion = nn.MSELoss() totals = train_actor_critic(ac_model, optimizer,
    criterion, env) [PRE56] import ale_py  ale = ale_py.ALEInterface() [PRE57] from
    stable_baselines3.common.env_util import make_atari_env  envs = make_atari_env("BreakoutNoFrameskip-v4",
    n_envs=4) obs = envs.reset()  # a 4 × 84 × 84 × 1 NumPy array (note: no info dict)
    [PRE58] from stable_baselines3.common.vec_env import VecFrameStack  envs_stacked
    = VecFrameStack(envs, n_stack=4) obs = envs_stacked.reset()  # returns a 4 × 84
    × 84 × 4 NumPy array [PRE59] from stable_baselines3 import PPO  ppo_model = PPO("CnnPolicy",
    envs_stacked, device=device, learning_rate=2.5e-4,                 batch_size=256,
    n_steps=256, n_epochs=4, clip_range=0.1,                 vf_coef=0.5, ent_coef=0.01,
    gamma=0.99, verbose=0) [PRE60] from stable_baselines3.common.callbacks import
    CheckpointCallback  cb = CheckpointCallback(save_freq=100_000, save_path="my_ppo_breakout.ckpt")
    ppo_model.learn(total_timesteps=30_000_000, progress_bar=True, callback=cb) ppo_model.save("my_ppo_breakout")  #
    save the final model [PRE61] tensorboard_logdir = "my_ppo_breakout_tensorboard"  #
    path to the log directory %tensorboard --logdir={tensorboard_logdir} --port 6006
    [PRE62] ppo_model = PPO("CnnPolicy", [...], tensorboard_log=tensorboard_logdir)
    [PRE63] ppo_model = PPO.load("my_ppo_agent_breakout")  # or load the best checkpoint
    eval_env = make_atari_env("BreakoutNoFrameskip-v4", n_envs=1, seed=42) eval_stacked
    = VecFrameStack(eval_env, n_stack=4) frames = [] obs = eval_stacked.reset() for
    _ in range(5000):  # some limit in case the agent never loses     frames.append(eval_stacked.render())     action,
    _ = ppo_model.predict(obs, deterministic=True) # for reproducibility     obs,
    reward, done, info = eval_stacked.step(action)     if done[0]:  # note: there''s
    no `truncated`         break  eval_stacked.close() [PRE64]` [PRE65][PRE66][PRE67]`````'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
