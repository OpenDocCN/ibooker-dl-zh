- en: Chapter 19\. Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Reinforcement learning* (RL) is one of the most exciting fields of machine
    learning today, and also one of the oldest. It has been around since the 1950s,
    producing many interesting applications over the years,⁠^([1](ch19.html#id4170))
    particularly in games (e.g., *TD-Gammon*, a backgammon-playing program) and in
    machine control, but seldom making the headline news. However, a revolution took
    place in 2013, when researchers from a British startup called DeepMind⁠^([2](ch19.html#id4171))
    demonstrated a system that could learn to play just about any Atari game from
    scratch,⁠^([3](ch19.html#id4172)) eventually outperforming humans⁠^([4](ch19.html#id4173))
    in most of them, using only raw pixels as inputs and without any prior knowledge
    of the rules of the games.⁠^([5](ch19.html#id4174)) This was the first of a series
    of amazing feats:'
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, DeepMind’s AlphaGo beat Lee Sedol, a legendary professional player
    of the game of Go; and in 2017, it beat Ke Jie, the world champion. No program
    had ever come close to beating a master of this game, let alone the very best.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2020, DeepMind released AlphaFold, which can predict the 3D shape of proteins
    with unprecedented accuracy. This is a game changer in biology, chemistry, and
    medicine. In fact, Demis Hassabis (founder and CEO) and John Jumper (director)
    were awarded the Nobel Prize in Chemistry for AlphaFold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2022, DeepMind released AlphaCode, which can generate code at a competitive
    programming level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2023, DeepMind released GNoME which can predict new crystal structures, including
    hundreds of thousands of predicted stable materials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So how did DeepMind researchers achieve all of this? Well, they applied the
    power of deep learning to the field of reinforcement learning, and it worked beyond
    their wildest dreams: *deep reinforcement learning* was born. Today, although
    DeepMind continues to lead the way, many other organizations have joined in, and
    the whole field is boiling with new ideas, with a wide range of applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter I will first explain what reinforcement learning is and what
    it’s good at, then present three of the most important families of techniques
    in deep reinforcement learning: policy gradients, deep Q-networks (including a
    discussion of Markov decision processes), and lastly, actor-critic methods, including
    the popular PPO, which we will use to beat an Atari game. So let’s get started!'
  prefs: []
  type: TYPE_NORMAL
- en: What Is Reinforcement Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reinforcement learning, a software *agent* makes *observations* and takes
    *actions* within an *environment*, and in return it receives *rewards* from the
    environment. Its objective is to learn to act in a way that will maximize its
    expected rewards over time. If you don’t mind a bit of anthropomorphism, you can
    think of positive rewards as pleasure, and negative rewards as pain (the term
    “reward” is a bit misleading in this case). In short, the agent acts in the environment
    and learns by trial and error to maximize its pleasure and minimize its pain.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is quite a broad setting that can apply to a wide variety of tasks. Here
    are a few examples (see [Figure 19-1](#rl_examples_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: The agent can be the program controlling a robot. In this case, the environment
    is the real world, the agent observes the environment through a set of *sensors*,
    such as cameras and touch sensors, and its actions consist of sending signals
    to activate motors. It may be programmed to get positive rewards whenever it approaches
    the target destination, and negative rewards whenever it wastes time or goes in
    the wrong direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent can be the program controlling *Ms. Pac-Man*. In this case, the environment
    is a simulation of the Atari game, the actions are the nine possible joystick
    positions (upper left, down, center, and so on), the observations are screenshots,
    and the rewards are just the game points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the agent can be the program playing a board game such as Go. It
    only gets a reward if it wins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent does not have to control a physically (or virtually) moving thing.
    For example, it can be a smart thermostat, getting positive rewards whenever it
    is close to the target temperature and saves energy, and negative rewards when
    humans need to tweak the temperature, so the agent must learn to anticipate human
    needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent can observe stock market prices and decide how much to buy or sell
    every second. Rewards are obviously the monetary gains and losses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that there may not be any positive rewards at all; for example, the agent
    may move around in a maze, getting a negative reward at every time step, so it
    had better find the exit as quickly as possible! There are many other examples
    of tasks to which reinforcement learning is well suited, such as self-driving
    cars, recommender systems, placing ads on a web page, or controlling where an
    image classification system should focus its attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement learning examples include a Mars rover, the _Ms. Pac-Man_ game,
    a Go board, a smart thermostat, and a digital stock trading interface.](assets/hmls_1901.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19-1\. Reinforcement learning examples: (a) robotics, (b) *Ms. Pac-Man*,
    (c) Go player, (d) thermostat, (e) automatic trader⁠^([6](ch19.html#id4189))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s now turn to one large family of RL algorithms: *policy gradients*.'
  prefs: []
  type: TYPE_NORMAL
- en: Policy Gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The algorithm a software agent uses to determine its actions is called its *policy*.
    The policy can be any algorithm you can think of, such as a neural network taking
    observations as inputs and outputting the action to take (see [Figure 19-2](#rl_with_nn_policy_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating reinforcement learning with a robot agent using a neural
    network policy to interact with an environment, showing the flow of actions, rewards,
    and observations.](assets/hmls_1902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19-2\. Reinforcement learning using a neural network policy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The policy does not even have to be deterministic. In fact, in some cases it
    does not even have to observe the environment, as long as it can get rewards!
    For example, consider a blind robotic vacuum cleaner whose reward is the amount
    of dust it picks up in 30 minutes. Its policy could be to move forward with some
    probability *p* every second, or randomly rotate left or right with probability
    1 – *p*. The rotation angle would be a random angle between –*r* and +*r*. Since
    this policy involves some randomness, it is called a *stochastic policy*. The
    robot will have an erratic trajectory, which guarantees that it will eventually
    get to any place it can reach and pick up all the dust. The question is, how much
    dust will it pick up in 30 minutes?
  prefs: []
  type: TYPE_NORMAL
- en: 'How would you train such a robot? There are just two *policy parameters* you
    can tweak: the probability *p* and the angle range *r*. One possible learning
    algorithm could be to try out many different values for these parameters, and
    pick the combination that performs best (see [Figure 19-3](#policy_search_diagram)).
    This is an example of *policy search*, in this case using a brute-force approach.
    When the *policy space* is too large (which is generally the case), finding a
    good set of parameters this way is like searching for a needle in a gigantic haystack.'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to explore the policy space is to use *genetic algorithms*. For
    example, you could randomly create a first generation of 100 policies and try
    them out, then “kill” the 80 worst policies⁠^([7](ch19.html#id4198)) and make
    the 20 survivors produce 4 offspring each. An offspring is a copy of its parent⁠^([8](ch19.html#id4199))
    plus some random variation. The surviving policies plus their offspring together
    constitute the second generation. You can continue to iterate through generations
    this way until you find a good policy.⁠^([9](ch19.html#id4200))
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating four points in the policy space on the left and their
    corresponding agent behaviors on the right, demonstrating the exploration of policy
    space in reinforcement learning.](assets/hmls_1903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19-3\. Four points in the policy space (left) and the agent’s corresponding
    behavior (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Yet another approach is to use optimization techniques by evaluating the gradients
    of the rewards with regard to the policy parameters, then tweaking these parameters
    by following the gradients toward higher rewards.⁠^([10](ch19.html#id4201)) Algorithms
    that follow this strategy are known as *policy gradient* (PG) algorithms. But
    before we can implement them, we first need to create an environment for the agent
    to live in⁠—so it’s time to introduce the Gymnasium library.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Gymnasium Library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the challenges of reinforcement learning is that in order to train an
    agent, you first need to have a working environment. If you want to program an
    agent that will learn to play an Atari game, you will need an Atari game simulator.
    If you want to program a walking robot, then the environment is the real world,
    and you can directly train your robot in that environment. However, this has its
    limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed
    up time either—adding more computing power won’t make the robot move any faster—and
    it’s generally too expensive to train 1,000 robots in parallel. In short, training
    is hard and slow in the real world, so you generally need a *simulated environment*
    at least for bootstrap training. For example, you might use a library like [PyBullet](https://pybullet.org)
    or [MuJoCo](https://mujoco.org) for 3D physics simulation.'
  prefs: []
  type: TYPE_NORMAL
- en: The [Gymnasium library](https://gymnasium.farama.org) is an open source toolkit
    that provides a wide variety of simulated environments (Atari games, board games,
    2D and 3D physics simulations, and so on), that you can use to train agents, compare
    them, or develop new RL algorithms. It’s the successor of OpenAI Gym, and is now
    maintained by a community of researchers and developers.
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium is preinstalled on Colab, along with the Arcade Learning Environment
    (ALE) library `ale_py`, which is an emulator for Atari 2600 games and is required
    for all the Atari environments, as well as the Box2D library, required for several
    environments with 2D physics. If you are coding on your own machine instead of
    Colab, and you followed the installation instructions at [*https://homl.info/install-p*](https://homl.info/install-p),
    then you should be good to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing Gymnasium and making an environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’ve created a CartPole environment (version 1). This is a 2D simulation
    in which a cart can be accelerated left or right in order to balance a pole placed
    on top of it (see [Figure 19-4](#cart_pole_diagram))—a classic control task. I’ll
    explain `render_mode` and `max_episode_steps` shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `gym.envs.registry` dictionary contains the names and specifications of
    all the available environments. You can print a nice list with `gym.pprint_registry()`.
    The Atari environments will only be available once we start the ALE emulator.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the CartPole environment with labeled vectors showing
    the cart''s position, velocity, pole angle, and angular velocity.](assets/hmls_1904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19-4\. The CartPole environment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After the environment is created, you must initialize it using the `reset()`
    method, optionally specifying a random seed. This returns the first observation.
    Observations depend on the type of environment. For the CartPole environment,
    each observation is a NumPy array containing four floats representing the cart’s
    horizontal position (`0.0` = center), its velocity (positive means right), the
    angle of the pole (`0.0` = vertical), and its angular velocity (positive means
    clockwise). The `reset()` method also returns a dictionary that may contain extra
    environment-specific information. This can be useful for debugging and sometimes
    for training. For example, in many Atari environments, it contains the number
    of lives left. However, in the CartPole environment, this dictionary is empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1] `array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)`
    `>>>` `info` `` `{}` `` [PRE2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3][PRE4]``py[PRE5]``py Let’s call the `render()` method to render this
    environment as an image. Since we set `render_mode="rgb_array"` when creating
    the environment, the image will be returned as a NumPy array (you can then use
    Matplotlib’s `imshow()` function to display this image):    [PRE6]py   [PRE7]py``
    Now let’s ask the environment what actions are possible:    [PRE8]py   [PRE9]py
    [PRE10]`py [PRE11]py`` [PRE12]py[PRE13][PRE14][PRE15][PRE16]py[PRE17]` ## Neural
    Network Policies    Let’s create a neural network policy. This neural network
    will take an observation as input, and it will output the action to be executed,
    just like the policy we hardcoded earlier. More precisely, it will estimate a
    probability for each action, then it will select an action randomly, according
    to the estimated probabilities (see [Figure 19-5](#neural_network_policy_diagram)).
    In the case of the CartPole environment, there are just two possible actions (left
    or right), so we only need one output neuron. It will output the probability *p*
    of action 1 (right), and of course the probability of action 0 (left) will be
    1 – *p*. For example, if it outputs 0.7, then we will pick action 1 with 70% probability,
    or action 0 with 30% probability (this is a *Bernoulli distribution* with *p*
    = 0.7).  ![Diagram illustrating a neural network policy where observations are
    processed through hidden layers to generate a probability for action, leading
    to random action sampling.](assets/hmls_1905.png)  ###### Figure 19-5\. Neural
    network policy    You may wonder why we are picking a random action based on the
    probabilities given by the neural network, rather than just picking the action
    with the highest score. This approach lets the agent find the right balance between
    *exploring* new actions and *exploiting* the actions that are known to work well.
    Here’s an analogy: suppose you go to a restaurant for the first time, and all
    the dishes look equally appealing, so you randomly pick one. If it turns out to
    be good, you can increase the probability that you’ll order it next time, but
    you shouldn’t increase that probability up to 100%, or you will never try the
    other dishes, some of which may be even better than the one you tried. This *exploration*/*exploitation
    dilemma* is central in reinforcement learning.    Also note that in this particular
    environment, the past actions and observations can safely be ignored, since each
    observation contains the environment’s full state. If there were some hidden state,
    then you might need to consider past actions and observations as well. For example,
    if the environment only revealed the position of the cart but not its velocity,
    you would have to consider not only the current observation but also the previous
    observation in order to estimate the current velocity. Another example is when
    the observations are noisy; in that case, you generally want to use the past few
    observations to estimate the most likely current state. The CartPole problem is
    thus as simple as can be; the observations are noise-free, and they contain the
    environment’s full state.    Let’s use PyTorch to implement a basic neural network
    policy for CartPole:    [PRE18]    Our policy network is a tiny MLP, since it’s
    a fairly simple task. The number of inputs is the size of the environment’s state:
    in the case of CartPole, it is just the size of a single observation, which is
    four. We have just one hidden layer with five units (no need for more in this
    case). Finally, we want to output a single probability, so we have a single output
    neuron. If there were more than two possible actions, there would be one output
    neuron per action instead. For performance and numerical stability, we don’t add
    a sigmoid function at the end, so the network will actually output logits rather
    than probabilities.    Next let’s define a function that will use this policy
    network to choose an action:    [PRE19]    The function takes a single observation,
    converts it to a tensor, and passes it to the policy network to get the logit
    for action 1 (right). It then creates a `Bernoulli` probability distribution with
    this logit, and it samples an action from it: this distribution will output 1
    (right) with probability *p* = exp(logit) / (1 + exp(logit)), and 0 (left) with
    probability 1 – *p*. If there were more than two possible actions, you would use
    a `Categorical` distribution instead. Lastly, we compute the log probability of
    the sampled action (i.e., either log(*p*) or log(1 – *p*)): this log probability
    will be needed later for training.    ###### Tip    If the action space is continuous,
    you can use a Gaussian distribution instead of a Bernoulli or categorical distribution.
    Instead of predicting logits, the policy network must predict the mean and standard
    deviation (or the log of the standard deviation) of the distribution. The log
    of the standard deviation is often clipped to ensure the distribution is neither
    too wide nor too narrow.    OK, we now have a neural network policy that can take
    an environment state (in this case, a single observation) and choose an action.
    But how do we train it?    ## Evaluating Actions: The Credit Assignment Problem    If
    we knew what the best action was at each step, we could train the neural network
    as usual by minimizing the cross-entropy between the estimated probability distribution
    and the target probability distribution. It would just be regular supervised learning.
    However, in reinforcement learning the only guidance the agent gets is through
    rewards, and rewards are typically sparse and delayed. For example, if the agent
    manages to balance the pole for a total of 100 steps, how can it know which of
    the 100 actions it took were good, and which of them were bad? All it knows is
    that the pole fell after the last action, but surely this last action is not entirely
    responsible. This is called the *credit assignment problem*: when the agent gets
    a reward (or a penalty), it is hard for it to know which actions should get credited
    (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well;
    will it understand what it is being rewarded for?    To simplify credit assignment,
    a common strategy is to evaluate an action based on the sum of all the rewards
    that come after it, applying a *discount factor, _γ* (gamma), at each step. This
    sum of discounted rewards is called the action’s *return*. Consider the example
    in [Figure 19-6](#discounted_rewards_diagram). If an agent decides to go right
    three times in a row and gets +10 reward after the first step, 0 after the second
    step, and finally –50 after the third step, then assuming we use a discount factor
    *γ* = 0.8, the first action will have a return of 10 + *γ* × 0 + *γ*² × (–50)
    = –22.  ![Diagram showing a robot earning sequential rewards of +10, 0, and -50
    as it moves right, with calculated discounted returns of -22 and -40, highlighting
    the effect of an 80% discount factor.](assets/hmls_1906.png)  ###### Figure 19-6\.
    Computing an action’s return: the sum of discounted future rewards    The following
    function computes the returns, given the rewards and the discount factor:    [PRE20]    This
    function produces the expected result:    [PRE21]   `If the discount factor is
    close to 0, then future rewards won’t count for much compared to immediate rewards.
    Conversely, if the discount factor is close to 1, then rewards far into the future
    will count almost as much as immediate rewards. Typical discount factors vary
    from 0.9 to 0.99\. With a discount factor of 0.95, rewards 13 steps into the future
    count roughly for half as much as immediate rewards (since 0.95^(13) ≈ 0.5), while
    with a discount factor of 0.99, rewards 69 steps into the future count for half
    as much as immediate rewards. In the CartPole environment, actions have fairly
    short-term effects, so choosing a low discount factor of 0.95 seems reasonable,
    and it will help with credit assignment, making training faster and more stable.
    However, if the discount factor is set too low, then the agent will learn a suboptimal
    strategy, focusing too much on short-term gains.    Now that we have a way to
    evaluate each action, we are ready to train our first agent using policy gradients.
    Let’s see how.`  [PRE22] def run_episode(model, env, seed=None):     log_probs,
    rewards = [], []     obs, info = env.reset(seed=seed)     while True:  # the environment
    will truncate the episode if it is too long         action, log_prob = choose_action(model,
    obs)         obs, reward, done, truncated, _info = env.step(action)         log_probs.append(log_prob)         rewards.append(reward)         if
    done or truncated:             return log_probs, rewards [PRE23] def train_reinforce(model,
    optimizer, env, n_episodes, discount_factor):     for episode in range(n_episodes):         seed
    = torch.randint(0, 2**32, size=()).item()         log_probs, rewards = run_episode(model,
    env, seed=seed)         returns = compute_returns(rewards, discount_factor)         std_returns
    = (returns - returns.mean()) / (returns.std() + 1e-7)         losses = [-logp
    * rt for logp, rt in zip(log_probs, std_returns)]         loss = torch.cat(losses).sum()         optimizer.zero_grad()         loss.backward()         optimizer.step()         print(f"\rEpisode
    {episode + 1}, Reward: {sum(rewards):.2f}", end=" ") [PRE24] torch.manual_seed(42)
    model = PolicyNetwork() optimizer = torch.optim.NAdam(model.parameters(), lr=0.06)
    train_reinforce(model, optimizer, env, n_episodes=200, discount_factor=0.95) [PRE25]`
    [PRE26][PRE27][PRE28][PRE29][PRE30][PRE31] transition_probabilities = [  # shape=[s,
    a, s'']     [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],     [[0.0, 1.0,
    0.0], None, [0.0, 0.0, 1.0]],     [None, [0.8, 0.1, 0.1], None] ] rewards = [  #
    shape=[s, a, s'']     [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],     [[0, 0, 0], [0,
    0, 0], [0, 0, -50]],     [[0, 0, 0], [+40, 0, 0], [0, 0, 0]] ] possible_actions
    = [[0, 1, 2], [0, 2], [1]] [PRE32] Q_values = np.full((3, 3), -np.inf)  # -np.inf
    for impossible actions for state, actions in enumerate(possible_actions):     Q_values[state,
    actions] = 0.0  # for all possible actions [PRE33] gamma = 0.90  # the discount
    factor  for iteration in range(50):     Q_prev = Q_values.copy()     for s in
    range(3):         for a in possible_actions[s]:             Q_values[s, a] = np.sum([                     transition_probabilities[s][a][sp]                     *
    (rewards[s][a][sp] + gamma * Q_prev[sp].max())                 for sp in range(3)])
    [PRE34] >>> Q_values `array([[18.91891892, 17.02702702, 13.62162162],`  `[ 0\.        ,        -inf,
    -4.87971488],`  `[       -inf, 50.13365013,        -inf]])` [PRE35]` For example,
    when the agent is in state *s*[0] and it chooses action *a*[1], the expected sum
    of discounted future rewards is approximately 17.0.    For each state, we can
    find the action that has the highest Q-value:    [PRE36]   `This gives us the
    optimal policy for this MDP when using a discount factor of 0.90: in state *s*[0]
    choose action *a*[0], in state *s*[1] choose action *a*[0] (i.e., stay put), and
    in state *s*[2] choose action *a*[1] (the only possible action). Interestingly,
    if we increase the discount factor to 0.95, the optimal policy changes: in state
    *s*[1] the best action becomes *a*[2] (go through the fire!). This makes sense
    because the more you value future rewards, the more you are willing to put up
    with some pain now for the promise of future bliss.` [PRE37]`` [PRE38] def step(state,
    action):     probas = transition_probabilities[state][action]     next_state =
    np.random.choice([0, 1, 2], p=probas)     reward = rewards[state][action][next_state]     return
    next_state, reward [PRE39] def exploration_policy(state):     return np.random.choice(possible_actions[state])
    [PRE40] alpha0 = 0.05  # initial learning rate decay = 0.005  # learning rate
    decay gamma = 0.90  # discount factor state = 0  # initial state  for iteration
    in range(10_000):     action = exploration_policy(state)     next_state, reward
    = step(state, action)     next_value = Q_values[next_state].max()  # greedy policy
    at the next step     alpha = alpha0 / (1 + iteration * decay)     Q_values[state,
    action] *= 1 - alpha     Q_values[state, action] += alpha * (reward + gamma *
    next_value)     state = next_state [PRE41] class DQN(nn.Module):     def __init__(self):         super().__init__()         self.net
    = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),                                  nn.Linear(32,
    32), nn.ReLU(),                                  nn.Linear(32, 2))      def forward(self,
    state):         return self.net(state) [PRE42] def choose_dqn_action(model, obs,
    epsilon=0.0):         if torch.rand(()) < epsilon:  # epsilon greedy policy             return
    torch.randint(2, size=()).item()         else:             state = torch.as_tensor(obs)             Q_values
    = model(state)             return Q_values.argmax().item()  # optimal according
    to the DQN [PRE43] def sample_experiences(replay_buffer, batch_size):     indices
    = torch.randint(len(replay_buffer), size=[batch_size])     batch = [replay_buffer[index]
    for index in indices.tolist()]     return [to_tensor([exp[index] for exp in batch])
    for index in range(6)]  def to_tensor(data):     array = np.stack(data)     dtype
    = torch.float32 if array.dtype == np.float64 else None     return torch.as_tensor(array,
    dtype=dtype) [PRE44] def play_and_record_episode(model, env, replay_buffer, epsilon,
    seed=None):     obs, _info = env.reset(seed=seed)     total_rewards = 0     model.eval()     with
    torch.no_grad():         while True:             action = choose_dqn_action(model,
    obs, epsilon)             next_obs, reward, done, truncated, _info = env.step(action)             experience
    = (obs, action, reward, next_obs, done, truncated)             replay_buffer.append(experience)             total_rewards
    += reward             if done or truncated:                 return total_rewards             obs
    = next_obs [PRE45] def dqn_training_step(model, optimizer, criterion, replay_buffer,
    batch_size,                       discount_factor):     experiences = sample_experiences(replay_buffer,
    batch_size)     state, action, reward, next_state, done, truncated = experiences     with
    torch.inference_mode():         next_Q_value = model(next_state)      max_next_Q_value,
    _ = next_Q_value.max(dim=1)     running = (~(done | truncated)).float()  # 0 if
    s'' is over, 1 if running     target_Q_value = reward + running * discount_factor
    * max_next_Q_value     all_Q_values = model(state)     Q_value = all_Q_values.gather(dim=1,
    index=action.unsqueeze(1))     loss = criterion(Q_value, target_Q_value.unsqueeze(1))     optimizer.zero_grad()     loss.backward()     optimizer.step()
    [PRE46] from collections import deque  def train_dqn(model, env, replay_buffer,
    optimizer, criterion, n_episodes=800,               warmup=30, batch_size=32,
    discount_factor=0.95):     totals = []     for episode in range(n_episodes):         epsilon
    = max(1 - episode / 500, 0.01)         seed = torch.randint(0, 2**32, size=()).item()         total_rewards
    = play_and_record_episode(model, env, replay_buffer,                                                 epsilon,
    seed=seed)         print(f"\rEpisode: {episode + 1}, Rewards: {total_rewards}",
    end=" ")         totals.append(total_rewards)         if episode >= warmup:             dqn_training_step(model,
    optimizer, criterion, replay_buffer,                               batch_size,
    discount_factor)     return totals  torch.manual_seed(42) dqn = DQN() optimizer
    = torch.optim.NAdam(dqn.parameters(), lr=0.03) mse = nn.MSELoss() replay_buffer
    = deque(maxlen=100_000) totals = train_dqn(dqn, env, replay_buffer, optimizer,
    mse) [PRE47]` [PRE48]`` [PRE49] class ActorCritic(nn.Module):     def __init__(self):         super().__init__()         self.body
    = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),                                   nn.Linear(32,
    32), nn.ReLU())         self.actor_head = nn.Linear(32, 1)  # outputs action logits         self.critic_head
    = nn.Linear(32, 1)  # outputs state values      def forward(self, state):         features
    = self.body(state)         return self.actor_head(features), self.critic_head(features)
    [PRE50] def choose_action_and_evaluate(model, obs):     state = torch.as_tensor(obs)     logit,
    state_value = model(state)     dist = torch.distributions.Bernoulli(logits=logit)     action
    = dist.sample()     log_prob = dist.log_prob(action)     return int(action.item()),
    log_prob, state_value [PRE51] def ac_training_step(optimizer, criterion, state_value,
    target_value, log_prob,                      critic_weight):     td_error = target_value
    - state_value     actor_loss = -log_prob * td_error.detach()     critic_loss =
    criterion(state_value, target_value)     loss = actor_loss + critic_weight * critic_loss     optimizer.zero_grad()     loss.backward()     optimizer.step()
    [PRE52] def get_target_value(model, next_obs, reward, done, truncated, discount_factor):     with
    torch.inference_mode():         _, _, next_state_value = choose_action_and_evaluate(model,
    next_obs)      running = 0.0 if (done or truncated) else 1.0     target_value
    = reward + running * discount_factor * next_state_value     return target_value
    [PRE53] def run_episode_and_train(model, optimizer, criterion, env, discount_factor,                           critic_weight,
    seed=None):     obs, _info = env.reset(seed=seed)     total_rewards = 0     while
    True:         action, log_prob, state_value = choose_action_and_evaluate(model,
    obs)         next_obs, reward, done, truncated, _info = env.step(action)         target_value
    = get_target_value(model, next_obs, reward, done,                                         truncated,
    discount_factor)         ac_training_step(optimizer, criterion, state_value, target_value,                          log_prob,
    critic_weight)         total_rewards += reward         if done or truncated:             return
    total_rewards         obs = next_obs [PRE54] def train_actor_critic(model, optimizer,
    criterion, env, n_episodes=400,                        discount_factor=0.95, critic_weight=0.3):     totals
    = []     model.train()     for episode in range(n_episodes):         seed = torch.randint(0,
    2**32, size=()).item()         total_rewards = run_episode_and_train(model, optimizer,
    criterion, env,                                               discount_factor,
    critic_weight,                                               seed=seed)         totals.append(total_rewards)         print(f"\rEpisode:
    {episode + 1}, Rewards: {total_rewards}", end=" ")      return totals [PRE55]
    torch.manual_seed(42) ac_model = ActorCritic() optimizer = torch.optim.NAdam(ac_model.parameters(),
    lr=1.1e-3) criterion = nn.MSELoss() totals = train_actor_critic(ac_model, optimizer,
    criterion, env) [PRE56] import ale_py  ale = ale_py.ALEInterface() [PRE57] from
    stable_baselines3.common.env_util import make_atari_env  envs = make_atari_env("BreakoutNoFrameskip-v4",
    n_envs=4) obs = envs.reset()  # a 4 × 84 × 84 × 1 NumPy array (note: no info dict)
    [PRE58] from stable_baselines3.common.vec_env import VecFrameStack  envs_stacked
    = VecFrameStack(envs, n_stack=4) obs = envs_stacked.reset()  # returns a 4 × 84
    × 84 × 4 NumPy array [PRE59] from stable_baselines3 import PPO  ppo_model = PPO("CnnPolicy",
    envs_stacked, device=device, learning_rate=2.5e-4,                 batch_size=256,
    n_steps=256, n_epochs=4, clip_range=0.1,                 vf_coef=0.5, ent_coef=0.01,
    gamma=0.99, verbose=0) [PRE60] from stable_baselines3.common.callbacks import
    CheckpointCallback  cb = CheckpointCallback(save_freq=100_000, save_path="my_ppo_breakout.ckpt")
    ppo_model.learn(total_timesteps=30_000_000, progress_bar=True, callback=cb) ppo_model.save("my_ppo_breakout")  #
    save the final model [PRE61] tensorboard_logdir = "my_ppo_breakout_tensorboard"  #
    path to the log directory %tensorboard --logdir={tensorboard_logdir} --port 6006
    [PRE62] ppo_model = PPO("CnnPolicy", [...], tensorboard_log=tensorboard_logdir)
    [PRE63] ppo_model = PPO.load("my_ppo_agent_breakout")  # or load the best checkpoint
    eval_env = make_atari_env("BreakoutNoFrameskip-v4", n_envs=1, seed=42) eval_stacked
    = VecFrameStack(eval_env, n_stack=4) frames = [] obs = eval_stacked.reset() for
    _ in range(5000):  # some limit in case the agent never loses     frames.append(eval_stacked.render())     action,
    _ = ppo_model.predict(obs, deterministic=True) # for reproducibility     obs,
    reward, done, info = eval_stacked.step(action)     if done[0]:  # note: there''s
    no `truncated`         break  eval_stacked.close() [PRE64]` [PRE65][PRE66][PRE67]`````'
  prefs: []
  type: TYPE_NORMAL
