["```py\nfrom sklearn.datasets import make_moons\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(random_state=42))\n    ]\n)\nvoting_clf.fit(X_train, y_train)\n```", "```py\n>>> for name, clf in voting_clf.named_estimators_.items(): `... `    `print``(``name``,` `\"=\"``,` `clf``.``score``(``X_test``,` `y_test``))` ```", "```py\n```", "```py```", "````py ````", "``` ```", "```py`` When you call the voting classifier’s `predict()` method, it performs hard voting. For example, the voting classifier predicts class 1 for the first instance of the test set, because two out of three classifiers predict that class:    ```", "```py   ```", "```py` ```", "```py Now let’s look at the performance of the voting classifier on the test set:    ```", "```py   ```", "```py >>> voting_clf.voting = \"soft\" `>>>` `voting_clf``.``named_estimators``[``\"svc\"``]``.``probability` `=` `True` ```", "```py `>>>` `voting_clf``.``score``(``X_test``,` `y_test``)` `` `0.92` `` ```", "```py` ```", "```py` ```", "```py ```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py````  ```py``````", "````` ```py`````", "```` ```py````", "```py```", "``` from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier  bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,                             max_samples=100, n_jobs=-1, random_state=42) bag_clf.fit(X_train, y_train) ```", "``` >>> bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, `... `                            `oob_score``=``True``,` `n_jobs``=-``1``,` `random_state``=``42``)` ```", "```` `>>>` `bag_clf``.``fit``(``X_train``,` `y_train``)` ```py `>>>` `bag_clf``.``oob_score_` `` `0.896` `` ``` ```py` ````", "```py   ```", "```py```", "``````py`` ``````", "``` >>> from sklearn.metrics import accuracy_score `>>>` `y_pred` `=` `bag_clf``.``predict``(``X_test``)` ```", "``` ```", "``````py ````` ```py` We get 92.0% accuracy on the test. The OOB evaluation was a bit too pessimistic.    The OOB decision function for each training instance is also available through the `oob_decision_function_` attribute. Since the base estimator has a `predict_proba()` method, the decision function returns the class probabilities for each training instance. For example, the OOB evaluation estimates that the first training instance has a 67.6% probability of belonging to the positive class and a 32.4% probability of belonging to the negative class:    ``` >>> bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances `array([[0.32352941, 0.67647059],`  `[0.3375    , 0.6625    ],`  `[1\\.        , 0\\.        ]])` ```py ```` ```py`` ``````", "``````py` ``````", "``````py```  ```` ```py ``## Random Patches and Random Subspaces    The `BaggingClassifier` class supports sampling the features as well. Sampling is controlled by two hyperparameters: `max_features` and `bootstrap_features`. They work the same way as `max_samples` and `bootstrap`, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.    This technique is particularly useful when you are dealing with high-dimensional inputs (such as images), as it can considerably speed up training. Sampling both training instances and features is called the [*random patches* method](https://homl.info/22).⁠^([8](ch06.html#id1765)) Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0`) but sampling features (by setting `bootstrap_features` to `True` and/or `max_features` to a value smaller than `1.0`) is called the [*random subspaces* method](https://homl.info/23).⁠^([9](ch06.html#id1767))    Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.`` ``` ```py`  ``````", "`````` ```py``````", "``````py```` # Random Forests    As we have discussed, a [random forest](https://homl.info/24)⁠^([10](ch06.html#id1771)) is an ensemble of decision trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples` set to the size of the training set. Instead of building a `BaggingClassifier` and passing it a `DecisionTreeClassifier`, you can use the `RandomForestClassifier` class, which is more convenient and optimized for decision trees⁠^([11](ch06.html#id1772)) (similarly, there is a `RandomForestRegressor` class for regression tasks). The following code trains a random forest classifier with 500 trees, each limited to maximum 16 leaf nodes, using all available CPU cores:    ```py from sklearn.ensemble import RandomForestClassifier  rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,                                  n_jobs=-1, random_state=42) rnd_clf.fit(X_train, y_train)  y_pred_rf = rnd_clf.predict(X_test) ```    With a few exceptions, a `RandomForestClassifier` has all the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters of a `BaggingClassifier` to control the ensemble itself.    The `RandomForestClassifier` class introduces extra randomness when growing trees: instead of searching for the very best feature when splitting a node (see [Chapter 5](ch05.html#trees_chapter)), it searches for the best feature among a random subset of features. By default, it samples $StartRoot n EndRoot$ features (where *n* is the total number of features). The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, generally yielding an overall better model. So, the following `BaggingClassifier` is equivalent to the previous `RandomForestClassifier`:    ```py bag_clf = BaggingClassifier(     DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),     n_estimators=500, n_jobs=-1, random_state=42) ```    ## Extra-Trees    When you are growing a tree in a random forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular decision trees do). For this, simply set `splitter=\"random\"` when creating a `DecisionTreeClassifier`.    A forest of such extremely random trees is called an [*extremely randomized trees*](https://homl.info/25)⁠^([12](ch06.html#id1782)) (or *extra-trees* for short) ensemble. Once again, this technique trades more bias for a lower variance, so they may perform better than regular random forests if you encounter overfitting, especially with noisy and/or high-dimensional datasets. Extra-trees classifiers are also much faster to train than regular random forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree in a random forest.    You can create an extra-trees classifier using Scikit-Learn’s `ExtraTreesClassifier` class. Its API is identical to the `RandomForestClassifier` class, except `bootstrap` defaults to `False`. Similarly, the `ExtraTreesRegressor` class has the same API as the `RandomForestRegressor` class, except `bootstrap` defaults to `False`.    ###### Tip    Just like decision tree classes, random forest classes and extra-trees classes in recent Scikit-Learn versions support missing values natively, no need for an imputer.    ## Feature Importance    Yet another great quality of random forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average, across all trees in the forest. More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see [Chapter 5](ch05.html#trees_chapter)).    Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1\\. You can access the result using the `feature_importances_` variable. For example, the following code trains a `RandomForestClassifier` on the iris dataset (introduced in [Chapter 4](ch04.html#linear_models_chapter)) and outputs each feature’s importance. It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively):    ```py >>> from sklearn.datasets import load_iris `>>>` `iris` `=` `load_iris``(``as_frame``=``True``)` ``````", "``````py `>>>` `rnd_clf``.``fit``(``iris``.``data``,` `iris``.``target``)` ````` `>>>` `for` `score``,` `name` `in` `zip``(``rnd_clf``.``feature_importances_``,` `iris``.``data``.``columns``):` ```py` `... `    `print``(``round``(``score``,` `2``),` `name``)` ``` `...` `` `0.11 sepal length (cm)` `0.02 sepal width (cm)` `0.44 petal length (cm)` `0.42 petal width (cm)` `` ```py ```` ```py`` ``````", "``````py` ```   ```py `` `Similarly, if you train a random forest classifier on the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter)) and plot each pixel’s importance, you get the image represented in [Figure 6-6](#mnist_feature_importance_plot).  ![Heatmap showing MNIST pixel importance with colors indicating feature significance according to a random forest classifier, where yellow denotes high importance and red denotes low importance.](assets/hmls_0606.png)  ###### Figure 6-6\\. MNIST pixel importance (according to a random forest classifier)    Random forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection.` `` ```  ```py``````", "```py```", "```py```", "```py from sklearn.ensemble import AdaBoostClassifier  ada_clf = AdaBoostClassifier(     DecisionTreeClassifier(max_depth=1), n_estimators=30,     learning_rate=0.5, random_state=42, algorithm=\"SAMME\") ada_clf.fit(X_train, y_train) ```", "```py import numpy as np from sklearn.tree import DecisionTreeRegressor  m = 100  # number of instances rng = np.random.default_rng(seed=42) X = rng.random((m, 1)) - 0.5 noise = 0.05 * rng.standard_normal(m) y = 3 * X[:, 0] ** 2 + noise  # y = 3x² + Gaussian noise  tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42) tree_reg1.fit(X, y) ```", "```py y2 = y - tree_reg1.predict(X) tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43) tree_reg2.fit(X, y2) ```", "```py y3 = y2 - tree_reg2.predict(X) tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44) tree_reg3.fit(X, y3) ```", "```py >>> X_new = np.array([[-0.4], [0.], [0.5]]) `>>>` `sum``(``tree``.``predict``(``X_new``)` `for` `tree` `in` `(``tree_reg1``,` `tree_reg2``,` `tree_reg3``))` `` `array([0.57356534, 0.0405142 , 0.66914249])` `` ```", "```py`` ```", "```py from sklearn.ensemble import GradientBoostingRegressor  gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,                                  learning_rate=1.0, random_state=42) gbrt.fit(X, y) ```", "```py gbrt_best = GradientBoostingRegressor(     max_depth=2, learning_rate=0.05, n_estimators=500,     n_iter_no_change=10, random_state=42) gbrt_best.fit(X, y) ```", "```py >>> gbrt_best.n_estimators_ `53` ```", "```py` ```", "```py```", "````` ```py`## Histogram-Based Gradient Boosting    Scikit-Learn also provides another GBRT implementation, optimized for large datasets: *histogram-based gradient boosting* (HGB). It works by binning the input features, replacing them with integers. The number of bins is controlled by the `max_bins` hyperparameter, which defaults to 255 and cannot be set any higher than this. Binning can greatly reduce the number of possible thresholds that the training algorithm needs to evaluate. Moreover, working with integers makes it possible to use faster and more memory-efficient data structures. And the way the bins are built removes the need for sorting the features when training each tree.    As a result, this implementation has a computational complexity of *O*(*b*×*m*) instead of *O*(*n*×*m*×log(*m*)), where *b* is the number of bins, *m* is the number of training instances, and *n* is the number of features. In practice, this means that HGB can train hundreds of times faster than regular GBRT on large datasets. However, binning causes a precision loss, which acts as a regularizer: depending on the dataset, this may help reduce overfitting, or it may cause underfitting.    Scikit-Learn provides two classes for HGB: `HistGradientBoostingRegressor` and `HistGradientBoostingClassifier`. They’re similar to `GradientBoostingRegressor` and `GradientBoostingClassifier`, with a few notable differences:    *   Early stopping is automatically activated if the number of instances is greater than 10,000\\. You can turn early stopping always on or always off by setting the `early_stopping` hyperparameter to `True` or `False`.           *   Subsampling is not supported.           *   `n_estimators` is renamed to `max_iter`.           *   The only decision tree hyperparameters that can be tweaked are `max_leaf_nodes`, `min_samples_leaf`, `max_depth`, and `max_features`.              The HGB classes support missing values natively, as well as categorical features. This simplifies preprocessing quite a bit. However, the categorical features must be represented as integers ranging from 0 to a number lower than `max_bins`. You can use an `OrdinalEncoder` for this. For example, here’s how to build and train a complete pipeline for the California housing dataset introduced in [Chapter 2](ch02.html#project_chapter):    ``` from sklearn.pipeline import make_pipeline from sklearn.compose import make_column_transformer from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.preprocessing import OrdinalEncoder  hgb_reg = make_pipeline(     make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),                             remainder=\"passthrough\",                             force_int_remainder_cols=False),     HistGradientBoostingRegressor(categorical_features=[0], random_state=42) ) hgb_reg.fit(housing, housing_labels) ```py    The whole pipeline is almost as short as the imports! No need for an imputer, scaler, or a one-hot encoder, it’s really convenient. Note that `categorical_features` must be set to the categorical column indices (or a Boolean array). Without any hyperparameter tuning, this model yields an RMSE of about 47,600, which is not too bad.    In short, HGB is a great choice when you have a fairly large dataset, especially when it contains categorical features and missing values: it performs well, doesn’t require much preprocessing work, and training is fast. However, it can be a bit less accurate than GBRT, due to the binning, so you might want to try both.    ###### Tip    Several other optimized implementations of gradient boosting are available in the Python ML ecosystem: in particular, [XGBoost](https://github.com/dmlc/xgboost), [CatBoost](https://catboost.ai), and [LightGBM](https://lightgbm.readthedocs.io). These libraries have been around for several years. They are all specialized for gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide many additional features, including hardware acceleration using GPUs; you should definitely check them out! Moreover, [Yggdrasil Decision Forests (YDF)](https://ydf.readthedocs.io) provides optimized implementations of a variety of random forest algorithms.```` ```py`` `````", "``````py ````` ```py`# Stacking    The last ensemble method we will discuss in this chapter is called *stacking* (short for [*stacked generalization*](https://homl.info/29)).⁠^([18](ch06.html#id1836)) It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? [Figure 6-11](#blending_prediction_diagram) shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a *blender*, or a *meta learner*) takes these predictions as inputs and makes the final prediction (3.0).  ![Diagram illustrating how blending combines predictions from multiple predictors to produce a singular prediction for a new instance.](assets/hmls_0611.png)  ###### Figure 6-11\\. Aggregating predictions using a blending predictor    To train the blender, you first need to build the blending training set. You can use `cross_val_predict()` on every predictor in the ensemble to get out-of-sample predictions for each instance in the original training set ([Figure 6-12](#blending_layer_training_diagram)), and use these as the input features to train the blender; the targets can simply be copied from the original training set. Note that regardless of the number of features in the original training set (just one in this example), the blending training set will contain one input feature per predictor (three in this example). Once the blender is trained, the base predictors must be retrained one last time on the full original training set (since `cross_val_predict()` does not give access to the trained estimators).    It is actually possible to train several different blenders this way (e.g., one using linear regression, another using random forest regression) to get a whole layer of blenders, and then add another blender on top of that to produce the final prediction, as shown in [Figure 6-13](#multi_layer_blending_diagram). You may be able to squeeze out a few more drops of performance by doing this, but it will cost you in both training time and system complexity.  ![Diagram illustrating the process of training a blender in a stacking ensemble, showing how predictions from multiple models form a blending training set for the final blending step.](assets/hmls_0612.png)  ###### Figure 6-12\\. Training the blender in a stacking ensemble  ![Diagram illustrating a multilayer stacking ensemble with three layers, showing how predictions are processed through interconnected blenders from a new instance input to produce a final prediction.](assets/hmls_0613.png)  ###### Figure 6-13\\. Predictions in a multilayer stacking ensemble    Scikit-Learn provides two classes for stacking ensembles: `StackingClassifier` and `StackingRegressor`. For example, we can replace the `VotingClassifier` we used at the beginning of this chapter on the moons dataset with a `StackingClassifier`:    ``` from sklearn.ensemble import StackingClassifier  stacking_clf = StackingClassifier(     estimators=[         ('lr', LogisticRegression(random_state=42)),         ('rf', RandomForestClassifier(random_state=42)),         ('svc', SVC(probability=True, random_state=42))     ],     final_estimator=RandomForestClassifier(random_state=43),     cv=5  # number of cross-validation folds ) stacking_clf.fit(X_train, y_train) ```py    For each predictor, the stacking classifier will call `predict_proba()` if available; if not it will fall back to `decision_function()` or, as a last resort, call `predict()`. If you don’t provide a final estimator, `StackingClassifier` will use `LogisticRegression` and `StackingRegressor` will use `RidgeCV`.    If you evaluate this stacking model on the test set, you will find 92.8% accuracy, which is a bit better than the voting classifier using soft voting, which got 92%. Depending on your use case, this may or may not be worth the extra complexity and computational cost (since there’s an extra model to run after all the others).    In conclusion, ensemble methods are versatile, powerful, and fairly simple to use. They can overfit if you’re not careful, but that’s true of every powerful model. [Table 6-1](#ensemble_summary_table) summarizes all the techniques we discussed in this chapter, and when to use each one.      Table 6-1\\. When to use each ensemble learning method   | Ensemble method | When to use it | Example use cases | | --- | --- | --- | | Hard voting | Balanced classification dataset with multiple strong but diverse classifiers. | Spam detection, sentiment analysis, disease classification | | Soft voting | Classification dataset with probabilistic models, where confidence scores matter. | Medical diagnosis, credit risk analysis, fake news detection | | Bagging | Structured or semi-structured dataset with high variance and overfitting-prone models. | Financial risk modeling, ecommerce recommendation | | Pasting | Structured or semi-structured dataset where more independent models are needed. | Customer segmentation, protein classification | | Random forest | High-dimensional structured datasets with potentially noisy features. | Customer churn prediction, genetic data analysis, fraud detection | | Extra-trees | Large structured datasets with many features, where speed is critical and reducing variance is important. | Real-time fraud detection, sensor data analysis | | AdaBoost | Small to medium-sized, low-noise, structured datasets with weak learners (e.g., decision stumps), where interpretability is helpful. | Credit scoring, anomaly detection, predictive maintenance | | Gradient boosting | Medium to large structured datasets where high predictive power is required, even at the cost of extra tuning. | Housing price prediction, risk assessment, demand forecasting | | Histogram-based gradient boosting (HGB) | Large structured datasets where training speed and scalability are key. | Click-through rate prediction, ranking algorithms, real-time bidding in advertising | | Stacking | Complex, high-dimensional datasets where combining multiple diverse models can maximize accuracy. | Recommendation engines, autonomous vehicle decision-making, Kaggle competitions |    ###### Tip    Random forests, AdaBoost, GBRT, and HGB are among the first models you should test for most machine learning tasks, particularly with heterogeneous tabular data. Moreover, as they require very little preprocessing, they’re great for getting a prototype up and running quickly.    So far, we have looked only at supervised learning techniques. In the next chapter, we will turn to the most common unsupervised learning task: dimensionality reduction.    # Exercises    1.  If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?           2.  What is the difference between hard and soft voting classifiers?           3.  Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?           4.  What is the benefit of out-of-bag evaluation?           5.  What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees classifiers slower or faster than regular random forests?           6.  If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak, and how?           7.  If your gradient boosting ensemble overfits the training set, should you increase or decrease the learning rate?           8.  Load the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter)), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a random forest classifier, an extra-trees classifier, and an SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?           9.  Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Train a classifier on this new training set. Congratulations—you have just trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier? Now try again using a `StackingClassifier` instead. Do you get better performance? If so, why?              Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch06.html#id1744-marker)) Imagine picking a card randomly from a deck of cards, writing it down, then placing it back in the deck before picking the next card: the same card could be sampled multiple times.    ^([2](ch06.html#id1745-marker)) Leo Breiman, “Bagging Predictors”, *Machine Learning* 24, no. 2 (1996): 123–140.    ^([3](ch06.html#id1746-marker)) In statistics, resampling with replacement is called *bootstrapping*.    ^([4](ch06.html#id1747-marker)) Leo Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line”, *Machine Learning* 36, no. 1–2 (1999): 85–103.    ^([5](ch06.html#id1749-marker)) Bias and variance were introduced in [Chapter 4](ch04.html#linear_models_chapter). Recall that a high bias means that the average prediction is far off target, while a high variance means that the predictions are very scattered. We want both to be low.    ^([6](ch06.html#id1752-marker)) `max_samples` can alternatively be set to a float between 0.0 and 1.0, in which case the max number of sampled instances is equal to the size of the training set times `max_samples`.    ^([7](ch06.html#id1757-marker)) As *m* grows, this ratio approaches 1 – exp(–1) ≈ 63%.    ^([8](ch06.html#id1765-marker)) Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches”, *Lecture Notes in Computer Science* 7523 (2012): 346–361.    ^([9](ch06.html#id1767-marker)) Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests”, *IEEE Transactions on Pattern Analysis and Machine Intelligence* 20, no. 8 (1998): 832–844.    ^([10](ch06.html#id1771-marker)) Tin Kam Ho, “Random Decision Forests”, *Proceedings of the Third International Conference on Document Analysis and Recognition* 1 (1995): 278.    ^([11](ch06.html#id1772-marker)) The `BaggingClassifier` class remains useful if you want a bag of something other than decision trees.    ^([12](ch06.html#id1782-marker)) Pierre Geurts et al., “Extremely Randomized Trees”, *Machine Learning* 63, no. 1 (2006): 3–42.    ^([13](ch06.html#id1792-marker)) Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”, *Journal of Computer and System Sciences* 55, no. 1 (1997): 119–139.    ^([14](ch06.html#id1793-marker)) This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost; they are slow and tend to be unstable with it.    ^([15](ch06.html#id1795-marker)) The original AdaBoost algorithm does not use a learning rate hyperparameter.    ^([16](ch06.html#id1797-marker)) For more details, see Ji Zhu et al., “Multi-Class AdaBoost”, *Statistics and Its Interface* 2, no. 3 (2009): 349–360.    ^([17](ch06.html#id1803-marker)) Gradient boosting was first introduced in Leo Breiman’s [1997 paper](https://homl.info/arcing) “Arcing the Edge” and was further developed in the [1999 paper](https://homl.info/gradboost) “Greedy Function Approximation: A Gradient Boosting Machine” by Jerome H. Friedman.    ^([18](ch06.html#id1836-marker)) David H. Wolpert, “Stacked Generalization”, *Neural Networks* 5, no. 2 (1992): 241–259.```` ```py`` ``````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py``` ``````py```````", "``````py``````"]