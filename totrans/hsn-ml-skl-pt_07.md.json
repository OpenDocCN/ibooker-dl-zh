["```py\nfrom sklearn.datasets import make_moons\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(random_state=42))\n    ]\n)\nvoting_clf.fit(X_train, y_train)\n```", "```py\n>>> for name, clf in voting_clf.named_estimators_.items():\n...     print(name, \"=\", clf.score(X_test, y_test))\n...\nlr = 0.864\nrf = 0.896\nsvc = 0.896\n```", "```py\n>>> voting_clf.predict(X_test[:1])\narray([1])\n>>> [clf.predict(X_test[:1]) for clf in voting_clf.estimators_]\n[array([1]), array([1]), array([0])]\n```", "```py\n>>> voting_clf.score(X_test, y_test)\n0.912\n```", "```py\n>>> voting_clf.voting = \"soft\"\n>>> voting_clf.named_estimators[\"svc\"].probability = True\n>>> voting_clf.fit(X_train, y_train)\n>>> voting_clf.score(X_test, y_test)\n0.92\n```", "```py\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n                            max_samples=100, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\n```", "```py\n>>> bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n...                             oob_score=True, n_jobs=-1, random_state=42)\n...\n>>> bag_clf.fit(X_train, y_train)\n>>> bag_clf.oob_score_\n0.896\n```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred = bag_clf.predict(X_test)\n>>> accuracy_score(y_test, y_pred)\n0.92\n```", "```py\n>>> bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances\narray([[0.32352941, 0.67647059],\n [0.3375    , 0.6625    ],\n [1\\.        , 0\\.        ]])\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\n\ny_pred_rf = rnd_clf.predict(X_test)\n```", "```py\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n    n_estimators=500, n_jobs=-1, random_state=42)\n```", "```py\n>>> from sklearn.datasets import load_iris\n>>> iris = load_iris(as_frame=True)\n>>> rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n>>> rnd_clf.fit(iris.data, iris.target)\n>>> for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n...     print(round(score, 2), name)\n...\n0.11 sepal length (cm)\n0.02 sepal width (cm)\n0.44 petal length (cm)\n0.42 petal width (cm)\n```", "```py\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n    learning_rate=0.5, random_state=42, algorithm=\"SAMME\")\nada_clf.fit(X_train, y_train)\n```", "```py\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nm = 100  # number of instances\nrng = np.random.default_rng(seed=42)\nX = rng.random((m, 1)) - 0.5\nnoise = 0.05 * rng.standard_normal(m)\ny = 3 * X[:, 0] ** 2 + noise  # y = 3xÂ² + Gaussian noise\n\ntree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg1.fit(X, y)\n```", "```py\ny2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\ntree_reg2.fit(X, y2)\n```", "```py\ny3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\ntree_reg3.fit(X, y3)\n```", "```py\n>>> X_new = np.array([[-0.4], [0.], [0.5]])\n>>> sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\narray([0.57356534, 0.0405142 , 0.66914249])\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n                                 learning_rate=1.0, random_state=42)\ngbrt.fit(X, y)\n```", "```py\ngbrt_best = GradientBoostingRegressor(\n    max_depth=2, learning_rate=0.05, n_estimators=500,\n    n_iter_no_change=10, random_state=42)\ngbrt_best.fit(X, y)\n```", "```py\n>>> gbrt_best.n_estimators_\n53\n```", "```py\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\n\nhgb_reg = make_pipeline(\n    make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\n                            remainder=\"passthrough\",\n                            force_int_remainder_cols=False),\n    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n)\nhgb_reg.fit(housing, housing_labels)\n```", "```py\nfrom sklearn.ensemble import StackingClassifier\n\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(probability=True, random_state=42))\n    ],\n    final_estimator=RandomForestClassifier(random_state=43),\n    cv=5  # number of cross-validation folds\n)\nstacking_clf.fit(X_train, y_train)\n```"]