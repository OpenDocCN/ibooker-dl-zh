- en: 5 Knowledge graph learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 知识图谱学习
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Building and working with knowledge graphs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和使用知识图谱
- en: Implementing open information extraction to generate knowledge graphs from text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现开放信息提取以从文本生成知识图谱
- en: Discovering arbitrary semantic relationships with semantic knowledge graphs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语义知识图谱发现任意语义关系
- en: Query expansion and rewriting using knowledge graphs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用知识图谱进行查询扩展和重写
- en: Interpreting documents with knowledge graphs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用知识图谱解释文档
- en: In the last chapter, we primarily focused on learning the similarity between
    queries and documents based on users’ behavioral signals. In chapter 2, we also
    discussed how textual document content, instead of being “unstructured data”,
    is more like a giant graph of hyper-structured data containing a rich graph of
    semantic relationships connecting the many character sequences, terms, and phrases
    that exist across our collections of documents.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们主要关注基于用户行为信号学习查询和文档之间的相似性。在第2章中，我们也讨论了文本文档内容，而不是“非结构化数据”，更像是一个包含丰富语义关系的巨大超结构化数据图，这些关系连接了我们文档集合中的许多字符序列、术语和短语。
- en: In this chapter, we’ll demonstrate how to use this giant graph of semantic relationships
    within our content to better interpret domain-specific terminology. We’ll accomplish
    this by using both traditional knowledge graphs, which enable explicit modeling
    of relationships within a domain, and semantic knowledge graphs, which enable
    real-time inference of nuanced semantic relationships within a domain.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示如何利用我们内容中的这个巨大的语义关系图来更好地解释特定领域的术语。我们将通过使用传统的知识图谱和语义知识图谱来实现这一目标，前者能够在一个领域内显式地建模关系，而后者能够在一个领域内实时推断细微的语义关系。
- en: A semantic knowledge graph is a simple kind of *language model* (a language
    model represents a probability distribution over sequences of words). We’ll use
    a semantic knowledge graph as a stepping stone to understanding large language
    models (LLMs) in later chapters. LLMs are deep neural networks typically trained
    on billions of parameters and massive amounts of data (often much of the known
    internet) to model a general representation of human knowledge. Semantic knowledge
    graphs, however, are queryable language models representing only the relationships
    that are actually within your search index. While semantic knowledge graphs don’t
    contain the ability to reason in general about language, they can be very powerful
    for domain-specific contextual inference, as we’ll see.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 语义知识图谱是一种简单的*语言模型*（语言模型表示一系列单词的概率分布）。我们将使用语义知识图谱作为理解后续章节中大型语言模型（LLMs）的垫脚石。LLMs通常是经过数十亿参数和大量数据（通常包括大部分互联网）训练的深度神经网络，以模拟人类知识的通用表示。然而，语义知识图谱是可查询的语言模型，仅代表您搜索索引中实际存在的那些关系。虽然语义知识图谱不具备在语言上进行一般推理的能力，但它们在特定领域的上下文推理方面可以非常强大，正如我们将看到的。
- en: We’ll also play with several fun datasets in this chapter, to show some variety
    in how knowledge graphs can be built and applied to improve query understanding
    across different domains.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将在这个章节中玩转几个有趣的数据集，以展示知识图谱如何构建和应用，以及如何在不同领域改善查询理解。
- en: 5.1 Working with knowledge graphs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 与知识图谱一起工作
- en: In section 2.4, we introduced the idea of *knowledge graphs* and discussed how
    they relate to other types of knowledge models, such as ontologies, taxonomies,
    synonyms, and alternative labels. Knowledge graphs, if you recall, integrate each
    of those other types of knowledge models, so we’ll be referring to them all collectively
    as “knowledge graphs” as we build throughout this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.4节中，我们介绍了*知识图谱*的概念，并讨论了它们与其他类型知识模型（如本体、分类法、同义词和备选标签）之间的关系。如果您还记得，知识图谱整合了所有这些其他类型的知识模型，因此在我们本章的构建过程中，我们将把它们统称为“知识图谱”。
- en: A knowledge graph (or any graph, for that matter) is represented through the
    concept of nodes (also known as *vertices*) and edges. A *node* is an entity represented
    in the knowledge graph (such as a term, person, place, thing, or concept), whereas
    an *edge* represents a relationship between two nodes. Figure 5.1 shows an example
    of a graph displaying nodes and edges.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（或任何图）是通过节点（也称为*顶点*）和边来表示的。*节点*是知识图谱中代表的一个实体（如术语、人、地点、事物或概念），而*边*则代表两个节点之间的关系。图5.1显示了显示节点和边的图的一个示例。
- en: '![figure](../Images/CH05_F01_Grainger.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F01_Grainger.png)'
- en: Figure 5.1 A graph structure. Graphs are composed of nodes (also known as “vertices”)
    that represent entities and of edges that represent a node’s relationship with
    another node. Graphs provide a way to model knowledge and infer new insights by
    traversing (or “following”) the edges between nodes.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 图结构。图由表示实体的节点（也称为“顶点”）和表示节点之间关系的边组成。图提供了一种通过遍历（或“跟随”）节点之间的边来建模知识和推断新见解的方法。
- en: In this figure, you can see four nodes representing authors, one node representing
    a research paper they wrote together, one node representing the academic conference
    at which the paper was presented and published, and then nodes representing the
    city, province, country, and dates during which the conference was held. By traversing
    (or “following”) the independent edges between nodes, you might infer that one
    of the authors was in Montreal, Canada in October 2016\. While any structure with
    nodes and edges like this is considered a graph, this particular graph represents
    factual knowledge and is therefore also considered a knowledge graph.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，您可以看到四个表示作者的节点，一个表示他们共同撰写的研究论文的节点，一个表示论文所展示和发表的学术会议，然后是表示会议举行的城市、省份、国家和日期的节点。通过遍历（或“跟随”）节点之间的独立边，您可能会推断出其中一位作者在2016年10月的蒙特利尔，加拿大。虽然任何具有节点和边的结构都被称为图，但这个特定的图代表了事实知识，因此也被称为知识图谱。
- en: There are numerous ways to build and represent knowledge graphs, both through
    explicitly modeling data as nodes and edges and through dynamically materializing
    (discovering) nodes and edges from your data in real time. The latter is what’s
    known as a *semantic knowledge graph*. In this chapter, we’ll walk through various
    examples including building an explicit knowledge graph by hand, autogenerating
    an explicit knowledge graph, and using a semantic knowledge graph that is already
    present within your search index.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 建立和表示知识图谱的方法有很多，既可以通过显式地将数据建模为节点和边，也可以通过实时动态地（发现）从您的数据中生成节点和边。后者被称为*语义知识图谱*。在本章中，我们将通过各种示例进行讲解，包括手动构建显式知识图谱、自动生成显式知识图谱以及使用已存在于您的搜索索引中的语义知识图谱。
- en: 'To get started with knowledge graphs, you have essentially three options:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用知识图谱，您实际上有三个选择：
- en: Build a knowledge graph from scratch using a graph database (Neo4j, Apache TinkerPop,
    ArangoDB, etc.)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图数据库（Neo4j、Apache TinkerPop、ArangoDB等）从头开始构建知识图谱
- en: Plug in a preexisting knowledge graph (ConceptNet, DBpedia, a large language
    model, etc.)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入一个现有的知识图谱（ConceptNet、DBpedia、大型语言模型等）
- en: Autogenerate a knowledge graph from your data, using your content directly to
    extract knowledge
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从您的数据中自动生成知识图谱，直接使用您的内容来提取知识
- en: Each approach has its strengths and weaknesses, though the approaches are not
    necessarily mutually exclusive. If you are building a general-knowledge search
    engine (such as a web search engine), utilizing a preexisting knowledge graph
    or large language model is a great place to start. If your search engine is more
    domain-specific, however, your domain-specific entities and terminology may not
    be present in a preexisting graph, requiring you to create a custom knowledge
    graph.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都有其优势和劣势，尽管这些方法并不一定是相互排斥的。如果您正在构建一个通用知识搜索引擎（如网络搜索引擎），利用现有的知识图谱或大型语言模型是一个很好的起点。然而，如果您的搜索引擎更具有领域特定性，那么您的特定领域实体和术语可能不会出现在现有的图谱中，这就需要您创建一个定制的知识图谱。
- en: 'In this chapter, we will focus primarily on the third option: autogenerating
    a knowledge graph from your content. The other two techniques are already covered
    well in external materials, using technologies like SPARQL, RDF Triples, and Apache
    Jena or preexisting knowledge graphs like DBpedia and Yago. You will still need
    to be able to override your knowledge graph and add custom content, so we will
    include examples of how you can integrate both explicitly defined knowledge graphs
    (built with a specific list of predefined relationships) and implicitly defined
    knowledge graphs (autogenerated relationships discovered dynamically from the
    data) into your search platform.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要关注第三个选项：从您的内容中自动生成知识图谱。其他两种技术已在外部材料中得到很好的介绍，使用了像SPARQL、RDF三元组、Apache
    Jena或现有的知识图谱如DBpedia和Yago等技术。您仍然需要能够覆盖您的知识图谱并添加自定义内容，因此我们将包括如何将显式定义的知识图谱（使用特定预定义关系列表构建）和隐式定义的知识图谱（从数据中动态发现的自动生成关系）集成到您的搜索平台中的示例。
- en: 5.2 Using our search engine as a knowledge graph
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 将我们的搜索引擎作为知识图谱使用
- en: Many organizations spend considerable resources building out knowledge graphs
    for their organizations but have trouble integrating them within their search
    engines. We have fortunately chosen a default search engine implementation (Apache
    Solr) for our examples that has explicit graph traversal capabilities built in,
    so there is no need to pull in a new, external system to implement or traverse
    our knowledge graphs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织投入大量资源为其组织构建知识图谱，但难以将其集成到其搜索引擎中。幸运的是，我们为示例选择了默认的搜索引擎实现（Apache Solr），它内置了显式的图遍历功能，因此无需引入新的外部系统来实现或遍历我们的知识图谱。
- en: While there may be some advantages to using an external graph database, such
    as Neo4J or ArangoDB, that supports more sophisticated graph traversal semantics,
    using an external system like this makes coordinating requests, keeping data in
    sync, and infrastructure management more complex. Additionally, because some kinds
    of graph operations can only be done effectively in the search engine (like the
    semantic knowledge graph traversals using an inverted index, which we’ll encounter
    shortly), using the search engine as a unified platform for both search and knowledge
    graph capabilities reduces the number of systems we’ll need to manage.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用支持更复杂图遍历语义的外部图数据库（如Neo4J或ArangoDB）可能有一些优势，但使用这样的外部系统会使协调请求、保持数据同步和基础设施管理变得更加复杂。此外，由于某些类型的图操作只能在搜索引擎中有效地执行（例如，使用倒排索引进行的语义知识图谱遍历，我们将在稍后遇到），将搜索引擎作为统一平台，用于搜索和知识图谱功能，可以减少我们需要管理的系统数量。
- en: We will focus extensively on implementing a semantic search system in chapter
    7, including semantic query parsing, phrase extraction, misspelling detection,
    synonym expansion, and query rewriting, all of which will be modeled into an explicitly
    built knowledge graph. Since the purpose of the current chapter is to focus on
    knowledge graph *learning*, we’ll save most of the discussion of query-time integration
    pattens until chapter 7 when we can tie everything from this chapter and chapter
    6 together into the appropriate knowledge graph structure.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第7章中详细讨论实现语义搜索系统，包括语义查询解析、短语提取、拼写检测、同义词扩展和查询重写，所有这些都将建模为显式构建的知识图谱。由于当前章节的目的是专注于知识图谱*学习*，我们将把关于查询时集成模式的讨论大部分留到第7章，那时我们可以将本章和第6章的内容结合起来，构建适当的知识图谱结构。
- en: 5.3 Automatically extracting knowledge graphs from content
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 从内容中自动提取知识图谱
- en: While you’ll need to be able to modify nodes and edges in your knowledge graphs,
    manually maintaining a large-scale knowledge graph is very challenging. Manually
    maintained knowledge graphs require substantial subject matter expertise, must
    be actively kept up to date with changing information, and are subject to the
    biases and errors of those maintaining them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您需要能够修改您知识图谱中的节点和边，但手动维护大规模知识图谱是非常具有挑战性的。手动维护的知识图谱需要大量的专业知识，必须积极更新以适应信息的变化，并且容易受到维护者偏见和错误的干扰。
- en: '*Open information extraction* is an evolving area of natural language processing
    (NLP) research. Open information extraction aims to extract facts directly from
    your text content. This is often done using NLP libraries and language models
    to parse sentences and assess the dependency graph between them. A *dependency
    graph* is a breakdown of the parts of speech for each word and phrase in a sentence,
    along with an indication of which words refer to which other words.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*开放信息提取* 是自然语言处理（NLP）研究的一个发展中的领域。开放信息提取旨在直接从您的文本内容中提取事实。这通常是通过使用NLP库和语言模型来解析句子并评估它们之间的依赖图来完成的。*依赖图*
    是对句子中每个单词和短语的词性分解，以及指示哪些单词指代哪些其他单词的指示。'
- en: More recent approaches to knowledge graph extraction tend to use LLMs specifically
    trained for entity extraction, such as UniRel (unified representation and interaction
    for joint relational triple extraction) and REBEL (relation extraction by end-to-end
    language generation). LLM-based approaches are likely to become the standard for
    knowledge graph extraction over time due to their ability to represent and extract
    more nuanced relationships between entities than traditional dependency graph–based
    approaches. For the sake of learning in this chapter, however, we’ll focus on
    the dependency graph–based approach, as it will provide a better foundation for
    understanding the mechanics of knowledge graph extraction from text and the ability
    to craft custom relationship extraction patterns. You can always switch to a more
    advanced LLM-driven approach later if it better suits your needs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱提取的较新方法往往使用专门针对实体提取训练的LLM（大型语言模型），例如UniRel（统一表示和交互用于联合关系三元组提取）和REBEL（通过端到端语言生成进行关系提取）。由于LLM方法能够比传统的基于依存图的方法更细致地表示和提取实体之间的关系，它们很可能会随着时间的推移成为知识图谱提取的标准。然而，为了本章的学习，我们将专注于基于依存图的方法，因为它将为理解从文本中提取知识图谱的机制以及定制关系提取模式提供更好的基础。如果需要，你总是可以在以后切换到一个更先进的LLM驱动的方法，如果它更适合你的需求。
- en: 'In this section, we’ll use a language model and dependency graphs to extract
    two different types of relationships: arbitrary relationships and hyponym relationships.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用语言模型和依存图来提取两种不同类型的关系：任意关系和上下位关系。
- en: 5.3.1 Extracting arbitrary relationships from text
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 从文本中提取任意关系
- en: Given the hyper-structured nature of text and the rich relationships expressed
    within typical sentences and paragraphs, it stands to reason that we should be
    able to identify the subjects and objects of sentences and how they are related.
    In this section, we’ll focus on extracting arbitrary relationships between the
    entities described within the sentences of our text content.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到文本的超结构性质以及典型句子和段落中表达出的丰富关系，我们可以合理地认为我们应该能够识别句子的主语和宾语以及它们之间的关系。在本节中，我们将专注于从文本内容中的句子提取实体之间的任意关系。
- en: By analyzing the nouns and verbs within a sentence, it is often possible to
    infer a fact that is present in the sentence and map that fact into an RDF triple
    (also known as a semantic triple). The Resource Description Framework (RDF) is
    a data model used to represent graphs and relationships. An *RDF triple* is a
    three-part data structure representing a subject (starting node), relationship
    (edge), and object (ending node). For example, in the sentence “Colin attends
    Riverside High School”, the verb “attends” can be extracted as a relationship
    type connecting the subject (“Colin”) with the object (“Riverside High School”).
    The RDF triple is therefore `("Colin", "attends",` `"Riverside High School")`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析句子中的名词和动词，通常可以推断出句子中存在的事实，并将该事实映射到一个RDF三元组（也称为语义三元组）。资源描述框架（RDF）是一种用于表示图和关系的数据模型。一个*RDF三元组*是一个包含三个部分的数据结构，代表一个主题（起始节点）、关系（边）和对象（结束节点）。例如，在句子“Colin
    attends Riverside High School”中，动词“attends”可以被提取为一个关系类型，将主题（“Colin”）与对象（“Riverside
    High School”）连接起来。因此，RDF三元组是`("Colin", "attends", "Riverside High School")`。
- en: Listing 5.1 walks through an example of using the Python-based spaCy library
    to extract facts from text content. SpaCy is a popular natural language processing
    library that ships with state-of-the-art statistical neural network models for
    part-of-speech tagging, dependency parsing, text categorization, and named-entity
    recognition.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1展示了如何使用基于Python的spaCy库从文本内容中提取事实的示例。SpaCy是一个流行的自然语言处理库，它包含了用于词性标注、依存句法分析、文本分类和命名实体识别的最先进的统计神经网络模型。
- en: Listing 5.1 Extracting relationships and resolving co-references
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1 提取关系和解决共指
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Resolves entities, such as replacing pronouns with nouns'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 解析实体，例如用名词替换代词'
- en: '#2 Classifies parts of speech for the text'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对文本进行词性分类'
- en: '#3 Generates RDF triples'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 生成RDF三元组'
- en: '#4 The spaCy-experimental model used for co-reference resolution'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 用于共指消解的spaCy实验模型'
- en: 'Output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, the example code has taken the text content, parsed it into
    sentences, and then determined the subjects, relationships, and objects within
    those sentences. Those RDF triples can then be saved into an explicitly built
    knowledge graph and traversed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，示例代码已经提取了文本内容，将其解析为句子，并确定了这些句子中的主语、关系和宾语。然后可以将这些RDF三元组保存到显式构建的知识图中进行遍历。
- en: Figure 5.2 provides a visualization of this extracted graph. Though this example
    is basic, advanced algorithms can extract facts from more sophisticated linguistic
    patterns. We are using the spaCy library in the code example, which uses a deep-learning-based
    neural language model to detect parts of speech, phrases, dependencies, and co-references
    within the input text. The mechanism we then employ to parse those linguistic
    outputs into RDF triples is more rules-based, following known semantic patterns
    within the English language.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2展示了从该图中提取的图形的可视化。虽然这个例子很简单，但高级算法可以从更复杂的语言模式中提取事实。在代码示例中，我们使用了spaCy库，该库使用基于深度学习的神经语言模型来检测输入文本中的词性、短语、依存关系和共指。然后我们采用的机制是将这些语言输出解析为RDF三元组，这个机制更依赖于规则，遵循英语语言中的已知语义模式。
- en: Unfortunately, when parsing arbitrary verbs into relationships this way, the
    extracted relationships can become quite noisy. Since verbs conjugate differently,
    have synonyms, and have overlapping meanings, it is often necessary to prune,
    merge, and otherwise clean up any list of arbitrary extracted relationships.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，以这种方式将任意动词解析为关系时，提取的关系可能会变得相当嘈杂。由于动词的变形不同，有同义词，并且有重叠的意义，通常需要修剪、合并以及其他方式清理任何任意提取的关系列表。
- en: In contrast, some relationship types are much simpler, such as statistical relationships
    (“is related to”) and hyponyms (“is a”). We’ll spend the rest of the chapter focusing
    primarily on using these two special types, starting with hyponyms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，某些关系类型要简单得多，例如统计关系（“is related to”）和下位词（“is a”）。我们将在本章的剩余部分主要关注使用这两种特殊类型，从下位词开始。
- en: '![figure](../Images/CH05_F02_Grainger.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F02_Grainger.png)'
- en: Figure 5.2 Extracted knowledge graph. The nodes and edges in this graph were
    automatically extracted from textual content based upon part-of-speech patterns.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2提取的知识图。该图中的节点和边是基于词性模式从文本内容中自动提取的。
- en: 5.3.2 Extracting hyponyms and hypernyms from text
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 从文本中提取下位词和上位词
- en: While it can be challenging mapping arbitrary verbs to clean lists of relationships
    within a knowledge graph, extracting hyponyms and hypernyms can be much easier.
    *Hyponyms* are entities that maintain an “is a” or “is instance of” relationship
    with a more general form of the entities, with the more general form being called
    a *hypernym*. For example, for the relationships between the terms “phillips head”,
    “screwdriver”, and “tool”, we would say that “phillips head” is a hyponym of “screwdriver”,
    that “tool” is a hypernym of “screwdriver”, and that “screwdriver” is both a hypernym
    of “phillips head” and a hyponym of “tool”.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在知识图中将任意动词映射到干净的关系列表可能具有挑战性，但提取下位词和上位词可能要容易得多。*下位词*是与更一般形式的实体保持“是”或“是实例”关系的实体，更一般的形式被称为*上位词*。例如，对于“phillips
    head”（ Phillips 头）、“screwdriver”（螺丝刀）和“tool”（工具）之间的关系，我们会说“phillips head”是“screwdriver”的下位词，“tool”是“screwdriver”的上位词，而“screwdriver”既是“phillips
    head”的上位词也是“tool”的下位词。
- en: 'One common and fairly accurate way to extract hyponym/hypernym relationships
    from text is through the use of Hearst patterns, described by Marti Hearst in
    “Automatic Acquisition of Hyponyms from Large Text Corpora” (in *COLING 1992 Volume
    2: The 14th International Conference on Computational Linguistics*, 1992). These
    patterns describe common linguistic templates that reliably indicate the presence
    of hyponyms within sentences. The following listing demonstrates a few examples
    of such patterns.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '从文本中提取下位词/上位词关系的一种常见且相当准确的方法是通过使用Hearst模式，由Marti Hearst在“从大型文本语料库中自动获取下位词”（在*COLING
    1992 Volume 2: 第14届国际计算语言学会议*，1992年）中描述。这些模式描述了常见的语言模板，这些模板可靠地指示句子中存在下位词。以下列表演示了此类模式的一些示例。'
- en: Listing 5.2 Hearst patterns that identify semantic relationships
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2 Hearst模式识别语义关系
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each of these five simple patterns is represented as a Python tuple, with the
    first entry being a *regular expression* and the second being a position within
    the pattern match (i.e., `first` or `last`). If you are unfamiliar with regular
    expressions, they provide a common and powerful syntax for pattern matching within
    strings. Anywhere you see the *NP* characters, this stands for the existence of
    a *noun phrase* within a sentence. The position specified in the second element
    of the tuple (`first` or `last`) indicates which noun phrase in the sentence represents
    the hypernym, with all other noun phrases matching the pattern considered the
    hyponyms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个简单的模式分别用 Python 元组表示，第一个元素是一个 *正则表达式*，第二个元素是在模式匹配中的位置（即 `first` 或 `last`）。如果你不熟悉正则表达式，它们提供了一种在字符串中进行模式匹配的通用且强大的语法。任何你看到
    *NP* 字符的地方，这表示句子中存在一个 *名词短语*。元组的第二个元素（`first` 或 `last`）指定的位置表示句子中哪个名词短语代表上位词，所有其他与模式匹配的名词短语被认为是下位词。
- en: In the following listing, we run through almost 50 of these Hearst patterns
    to match many combinations of “is a” relationships within our content.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们运行了几乎 50 个这样的 Hearst 模式，以匹配我们内容中许多“是”关系的组合。
- en: Listing 5.3 Extracting hyponym relationships using Hearst patterns
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3 使用 Hearst 模式提取下位词关系
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see from this listing, by focusing on extracting a fixed type of
    relationship (and the most prevalent one—the “is a” relationship), we can generate
    a nice, clean list of taxonomical facts with the more specific term (the hyponym)
    pointing to the more general term (the hypernym) with an `is_a` edge. Figure 5.3
    demonstrates this generated graph visually.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如此列表所示，通过专注于提取固定类型的关系（以及最普遍的一种——“是”关系），我们可以生成一个整洁的、清晰的分类事实列表，其中更具体的术语（下位词）通过
    `is_a` 边指向更一般的术语（上位词）。图 5.3 以视觉方式展示了这个生成的图。
- en: '![figure](../Images/CH05_F03_Grainger.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH05_F03_Grainger.png)'
- en: Figure 5.3 Knowledge graph derived from Hearst patterns. We can see that all
    nodes are connected to other nodes through an `is_a` edge.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3 由 Hearst 模式派生的知识图谱。我们可以看到所有节点都通过 `is_a` 边与其他节点相连。
- en: The inconsistency and noise that exists with arbitrary relationship extraction
    is significantly reduced by utilizing Hearst patterns. We could still have ambiguity
    about the relationship between similar terms (for example, misspellings, alternative
    spellings, known phrases, or synonyms), but those are much easier to resolve.
    In fact, we’ll spend the entire next chapter discussing how to learn this kind
    of domain-specific language from your signals and content to use it when interpreting
    incoming user queries.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 Hearst 模式，可以显著减少任意关系提取中存在的不一致性和噪声。我们仍然可能对相似术语之间的关系存在歧义（例如，拼写错误、替代拼写、已知短语或同义词），但这些更容易解决。实际上，我们将在下一章中讨论如何从你的信号和内容中学习这种特定领域的语言，以便在解释传入的用户查询时使用。
- en: Although it can be useful to extract information from our text into an explicit
    knowledge graph for later traversal, the reality is that this kind of extraction
    is a lossy process, as the representation of the items gets disconnected from
    the originating context of those items within our content (the surrounding text
    and documents containing the text). In the next section, we’ll introduce an entirely
    different kind of knowledge graph—a semantic knowledge graph—that is optimized
    to enable real-time traversal and ranking of the relationships between terms and
    phrases within our content without having to be explicitly built and without separating
    terms from their original textual context.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将信息从我们的文本中提取到显式的知识图谱中以便后续遍历可能很有用，但现实是这种提取是一个有损过程，因为项目的表示与我们的内容中这些项目的原始上下文（周围的文本和包含文本的文档）断开连接。在下一节中，我们将介绍一种完全不同的知识图谱——语义知识图谱，它优化了实时遍历和排序我们内容中术语和短语之间关系的能力，而无需显式构建，也不会将术语与其原始文本上下文分开。
- en: 5.4 Learning intent by traversing semantic knowledge graphs
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 通过遍历语义知识图谱学习意图
- en: In chapter 2, sections 2.1 and 2.2, we discussed the myth of text content being
    “unstructured data” and how, in reality, text documents represent hyper-structured
    data. We discussed the distributional hypothesis (“a word shall be known by the
    company it keeps”) and walked through how character sequences, terms, phrases,
    and other arbitrary term sequences can be thought of as fuzzy foreign keys relating
    similar concepts between documents. We also discussed how these links between
    documents can be thought of as edges in a giant graph of relationships, enabling
    us to learn the contextual meaning of the terms and entities present within our
    corpus of documents.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章的第2.1节和第2.2节中，我们讨论了文本内容是“非结构化数据”的神话，以及实际上文本文档如何代表超结构化数据。我们讨论了分布假设（“一词应当通过其伴随的词群来认识”）并探讨了如何将字符序列、术语、短语以及其他任意术语序列视为模糊的外键，这些外键在文档之间关联相似的概念。我们还讨论了这些文档之间的链接可以被视为一个巨大关系图中的边，使我们能够学习文档语料库中术语和实体的上下文意义。
- en: In this section, we’ll introduce a semantic knowledge graph, a tool and technique
    that will enable us to traverse that giant graph of semantic relationships present
    within our documents.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一个语义知识图，这是一个工具和技术，将使我们能够遍历文档中存在的巨大语义关系图。
- en: 5.4.1 What is a semantic knowledge graph?
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 什么是语义知识图？
- en: A *semantic knowledge graph* (SKG) is a “compact, auto-generated model for real-time
    traversal and ranking of any relationship within a domain”.[¹](#footnote-113)
    We can think of an SKG as a search engine that, instead of matching and ranking
    documents, finds and ranks *terms* that best match a query.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**语义知识图**（SKG）是一个“紧凑、自动生成的模型，用于实时遍历和排序领域内任何关系”。[¹](#footnote-113)我们可以将SKG视为一个搜索引擎，它不是匹配和排序文档，而是找到并排序与查询最佳匹配的**术语**。
- en: 'For example, if we indexed a collection of documents about health topics and
    searched for `advil`, instead of returning documents that contain the term for
    the pain reliever “advil”, an SKG would automatically (with no manual list creation
    or data modeling required) return values like these:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们索引了一个关于健康主题的文档集合，并搜索`advil`，那么一个SKG会自动（无需手动创建列表或数据建模）返回如下值：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Such results can be thought of as “dynamic synonyms”, but instead of the terms
    having the same meaning, they are more like conceptually related terms. You could
    expand a lexical search engine query for `advil` to include these other terms
    to improve the recall of your search results or to boost documents that conceptually
    match the meaning of “advil”, instead of just the string containing the five characters
    `a`, `d`, `v`, `i`, `l`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的结果可以被视为“动态同义词”，但与具有相同意义的术语不同，它们更像是概念上相关的术语。你可以将针对`advil`的词汇搜索查询扩展到包括这些其他术语，以提高搜索结果的召回率或提升与“advil”概念匹配的文档，而不仅仅是包含五个字符`a`、`d`、`v`、`i`、`l`的字符串。
- en: In addition to finding related terms, an SKG can traverse between fields in
    your inverted index (“find the most-related skills to this job title”), traverse
    multiple levels deep (“find the most-related job titles to this query and then
    find the most-related skills for this query and each of those job titles”), and
    use any arbitrary query you can send to the search engine as a node in the graph
    traversal to find semantically related terms in any field.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了找到相关术语之外，一个语义知识图（SKG）可以在你的倒排索引字段之间进行遍历（“找到与这个职位名称最相关的技能”），深入多个层级（“找到与这个查询最相关的职位名称，然后找到与这个查询和每个职位名称最相关的技能”），并且可以使用你发送给搜索引擎的任何任意查询作为图遍历的节点来找到任何字段中的语义相关术语。
- en: The use cases for SKGs are diverse. They can be used for query expansion, generating
    content-based recommendations, query classification, query disambiguation, anomaly
    detection, data cleansing, and predictive analytics. We’ll explore several of
    these in the remainder of this chapter, but let’s first set up some datasets for
    testing our SKG.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 语义知识图的使用案例多种多样。它们可用于查询扩展、生成基于内容推荐、查询分类、查询消歧、异常检测、数据清洗和预测分析。我们将在本章剩余部分探讨其中的一些，但首先让我们为测试我们的SKG设置一些数据集。
- en: 5.4.2 Indexing the datasets
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 索引数据集
- en: An SKG works best on datasets where there is more overlap of terms being used
    together across documents. The more often two words tend to appear within documents,
    the better we can determine whether those terms appear statistically more often
    than we would expect.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SKG在那些文档中使用的术语重叠度更高的数据集上工作得最好。两个词在文档中出现的频率越高，我们就能越好地确定这些术语在统计上出现的频率是否比预期的高。
- en: Although Wikipedia is often a good starting dataset for many use cases, it usually
    has a single page about a major topic that is supposed to be authoritative, so
    there isn’t significant overlap across most documents, making Wikipedia a poor
    dataset for this use case. In contrast, most other websites where users submit
    the content (questions, forum posts, job postings, social media posts, reviews)
    tend to have excellent datasets for an SKG use case.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然维基百科对于许多用例来说通常是一个好的起点数据集，但它通常只有一个关于主要主题的权威页面，因此大多数文档之间没有显著的重叠，这使得维基百科对于这个用例来说是一个较差的数据集。相比之下，大多数其他用户提交内容的网站（问题、论坛帖子、职位发布、社交媒体帖子、评论）对于SKG用例来说往往拥有优秀的数据集。
- en: 'For this chapter, we have selected two primary datasets: a jobs dataset (job
    board postings) and a series of Stack Exchange data dumps including posts from
    the following forums:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们选择了两个主要数据集：一个职位数据集（职位板发布）以及一系列包含以下论坛帖子在内的Stack Exchange数据存档：
- en: health
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康
- en: scifi
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科幻
- en: devops
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发运维
- en: travel
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旅行
- en: cooking
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 烹饪
- en: 5.4.3 Structure of an SKG
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 SKG的结构
- en: To best utilize an SKG, it is useful to understand how the graph works based
    on its underlying structure.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最好地利用SKG，了解其底层结构如何工作是有用的。
- en: Unlike a traditional knowledge graph, which must be explicitly modeled into
    nodes and edges, an SKG is *materialized* from the underlying inverted index of
    your search engine. This means that all you need to do to produce an SKG is to
    index documents into a search engine. No extra data modeling is required.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与必须显式建模到节点和边中的传统知识图谱不同，SKG是从您的搜索引擎的底层倒排索引中*实例化*的。这意味着您要产生一个SKG所需要做的只是将文档索引到搜索引擎中。不需要额外的数据建模。
- en: The inverted index and a corresponding forward index then serve as the underlying
    data structure that enables real-time traversal and ranking of any arbitrary semantic
    relationships present within your collection of documents.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 倒排索引和相应的正向索引随后作为底层数据结构，使得在您的文档集合中实时遍历和排名任何任意语义关系成为可能。
- en: Figure 5.4 demonstrates how documents get added into both the forward index
    and the inverted index. On the left of the figure, you can see three documents,
    each of which has a `job_title` field, a `desc` field, and a `skills` field. The
    right side of the figure shows how these documents are mapped into your search
    engine. We see that the inverted index maps each field to a list of terms, and
    then maps each term to a postings list containing a list of documents (along with
    positions in the documents, as well as some other data not included in the figure).
    This makes it quick and efficient to look up any term in any field and find the
    set of all documents containing that term.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4展示了文档如何被添加到正向索引和倒排索引中。在图的左侧，你可以看到三个文档，每个文档都有一个`job_title`字段、一个`desc`字段和一个`skills`字段。图的右侧显示了这些文档如何映射到您的搜索引擎中。我们看到倒排索引将每个字段映射到一个术语列表，然后将每个术语映射到一个包含文档列表的帖子列表（包括文档中的位置以及图中未包含的一些其他数据）。这使得查找任何字段中的任何术语并找到包含该术语的所有文档变得快速且高效。
- en: '![figure](../Images/CH05_F04_Grainger.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F04_Grainger.png)'
- en: Figure 5.4 Inverted index and forward index. Documents get added to an inverted
    index, which maps documents to lists of terms, and to a forward index, which maps
    terms back to lists of documents. Having the ability to map both directions will
    prove important for graph traversal and relationship discovery.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4 倒排索引和正向索引。文档被添加到倒排索引中，该索引将文档映射到术语列表，并映射到正向索引，该索引将术语映射回文档列表。能够映射两个方向的能力对于图遍历和关系发现将证明非常重要。
- en: 'In addition to the well-known inverted index, you can also see the less-well-known
    *forward index* in the center of figure 5.4\. A forward index can be thought of
    as an *uninverted index*: for each field, it maps each document to a list of terms
    contained within that document. A forward index is what search engines use to
    generate *facets* (also called *aggregations*) on search results, which show the
    top values per field from a set of documents. In Lucene-based search engines like
    Solr, OpenSearch, and Elasticsearch, a forward index is usually generated at index
    time for a field by enabling a feature called *doc values* on the field. Alternatively,
    Apache Solr also allows you to generate the same forward index by “uninverting”
    the inverted index in memory at query time, enabling faceting even on fields for
    which doc values weren’t added to the index.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了众所周知的倒排索引之外，你还可以在图5.4的中心看到不太为人所知的**正向索引**。正向索引可以被视为一个**非倒排索引**：对于每个字段，它将每个文档映射到包含在该文档中的术语列表。正向索引是搜索引擎用来生成搜索结果上的**分面**（也称为**聚合**）的工具，它显示了来自一组文档的每个字段的最高值。在基于Lucene的搜索引擎（如Solr、OpenSearch和Elasticsearch）中，正向索引通常通过在字段上启用一个称为**doc
    values**的功能来在索引时间生成一个字段。或者，Apache Solr还允许你在查询时间通过“非倒排”内存中的倒排索引来生成相同的正向索引，即使在那些未将doc
    values添加到索引的字段上也能启用分面功能。
- en: If you have the ability to search for arbitrary queries and find sets of documents
    through an inverted index (traversing from terms to documents), and you also have
    the ability to take arbitrary sets of documents and look up terms in those documents
    (traversing from documents to terms), this means that by doing two traversals
    (terms to documents to terms) you can find all of the related terms that appear
    across any documents matching the query. Figure 5.5 demonstrates how such a traversal
    can occur, including a data structure view, a set-theory view, and a graph view.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你具备通过倒排索引（从术语到文档的遍历）搜索任意查询并找到文档集合的能力，同时你也具备从任意文档集合中查找术语的能力（从文档到术语的遍历），这意味着通过进行两次遍历（术语到文档到术语），你可以找到出现在任何匹配查询的文档中的所有相关术语。图5.5展示了这种遍历如何发生，包括数据结构视图、集合论视图和图形视图。
- en: '![figure](../Images/CH05_F05_Grainger.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F05_Grainger.png)'
- en: Figure 5.5 Three representations of an SKG. The data structure view shows terms
    mapped to sets of documents, the set-theory view shows how the intersection of
    sets of documents forms the relationship between them, and the graph view shows
    the nodes and edges.
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 SKG的三个表示。数据结构视图显示了术语映射到文档集合，集合论视图显示了文档集合的交集如何形成它们之间的关系，图形视图显示了节点和边。
- en: In the data structure view, which represents our inverted and forward indices,
    we see how terms are related to documents based on whether they appear within
    them. Those relationship links are only present if there is an intersection between
    the docs that any two nodes (terms, in this case) appear within in the set-theory
    view. The graph view, finally, demonstrates a third view into the same underlying
    data structures, but in this case we see nodes (instead of document sets) and
    edges (instead of intersecting document sets). Essentially, the SKG exists as
    an abstraction on top of the inverted index that is already built and updated
    anytime the search engine indexes content.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据结构视图中，它代表我们的倒排和正向索引，我们可以看到术语如何根据它们是否出现在其中与文档相关联。这些关系链接仅在集合论视图中任何两个节点（在这种情况下是术语）出现的文档集合之间存在交集时才存在。最后，图形视图展示了同一底层数据的第三种视图，在这种情况下，我们看到的是节点（而不是文档集合）和边（而不是相交的文档集合）。本质上，SKG存在于已构建和更新的倒排索引之上，这发生在搜索引擎索引内容时。
- en: We typically consider the primary function of search engines to be accepting
    a query, finding matching documents, and returning those documents in a relevance-ranked
    order. We devoted all of chapter 3 to discussing this process, walking through
    matching (sections 3.2.4–3.2.6), TF-IDF ranking (section 3.1), and the commonly
    used BM25 ranking function (section 3.2.1). However, with an SKG, we focus on
    matching and ranking related *terms*, as opposed to related documents.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常认为搜索引擎的主要功能是接受查询，找到匹配的文档，并按相关性排序返回这些文档。我们在第3章中专门讨论了这一过程，包括匹配（3.2.4-3.2.6节）、TF-IDF排序（3.1节）和常用的BM25排序函数（3.2.1节）。然而，在SKG中，我们关注的是匹配和排序相关**术语**，而不是相关文档。
- en: Any arbitrary query (anything you can resolve to a document set) can be a node
    in your graph, and you can traverse from that node to any other term (or arbitrary
    query) in any document field. Additionally, since each traversal of an edge between
    two nodes uses both an inverted index (terms to docs) and a forward index (docs
    to terms), it is trivial to chain these traversals together into a multilevel
    graph traversal, as shown in figure 5.6.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 任何任意的查询（你可以解析为文档集的任何内容）都可以成为你图中的一个节点，你可以从该节点遍历到任何文档字段中的任何其他术语（或任意查询）。此外，由于两个节点之间的边遍历每次都使用倒排索引（术语到文档）和正向索引（文档到术语），因此将这些遍历串联成多级图遍历是微不足道的，如图5.6所示。
- en: In the figure, the data structure view shows a traversal from a skill node (`Java`)
    to a layer of other skills nodes (`Java`, `Oncology`, `Hibernate`, and `Scala`),
    to a layer of job title nodes (`Software Engineer`, `Data Scientist`, and `Java
    Developer`). You can see that not all nodes are connected—the node for `Oncology`,
    for example, does not appear in the graph view because none of the original nodes
    can connect to it through any edges—there are no overlapping documents.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，数据结构视图显示了从技能节点（`Java`）到其他技能节点层（`Java`、`肿瘤学`、`Hibernate`和`Scala`），再到职位名称节点层（`软件工程师`、`数据科学家`和`Java开发者`）的遍历。你可以看到并非所有节点都是连接的——例如，`肿瘤学`节点在图形视图中没有出现，因为没有任何原始节点可以通过任何边连接到它——没有重叠的文档。
- en: Given that not all possible nodes are going to be relevant for any given traversal,
    it is also important that SKGs be able to score and assign a weight to the relationships
    between nodes so that those edges can be prioritized during any graph traversal.
    We will cover the scoring and assignment of weights to edges in the next section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于并非所有可能的节点都会对任何给定的遍历相关，因此SKG能够对节点之间的关系进行评分和分配权重，以便在图遍历过程中优先考虑这些边，这也是非常重要的。我们将在下一节中介绍边的评分和权重分配。
- en: 5.4.4 Calculating edge weights to measure the relatedness of nodes
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 计算边权重以衡量节点之间的相关性
- en: Given that the primary function of an SKG is to discover relevant semantic relationships
    between nodes, the ability to calculate *semantic similarity* is critical. But
    what exactly is semantic similarity?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SKG的主要功能是发现节点之间的相关语义关系，因此计算*语义相似度*的能力是至关重要的。但语义相似度究竟是什么？
- en: If you recall, the distributional hypothesis, introduced in section 2.3, says
    that words appearing together in the same contexts and with similar distributions
    tend to share similar meanings. Intuitively, this makes sense—the terms “pain”
    or “swelling” will be more likely to occur in documents that also mention “advil”,
    “ibuprofen”, or “ice pack” than in some random documents. Interestingly, though,
    “ice pack” may also occur in documents containing terms like “cooler”, “road trip”,
    or “cold”, whereas “advil” and “ibuprofen” likely would not.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，第2.3节中引入的分布假设指出，在相同语境和相似分布中出现的词语往往具有相似的意义。直观上，这很有道理——术语“疼痛”或“肿胀”在提及“艾德维尔”、“布洛芬”或“冰袋”的文档中出现的可能性，要比在随机文档中高。有趣的是，然而，“冰袋”也可能出现在包含“冷却器”、“长途旅行”或“寒冷”等术语的文档中，而“艾德维尔”和“布洛芬”可能就不会。
- en: '![figure](../Images/CH05_F06_Grainger.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F06_Grainger.png)'
- en: 'Figure 5.6 Multilevel graph traversal. In the data structure view, we see two
    traversals: through the inverted index and then the forward index each time. In
    the graph structure view, we see the corresponding two-level traversal: from skills
    to skills to job titles.'
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 多级图遍历。在数据结构视图中，我们看到两次遍历：每次都是通过倒排索引然后是正向索引。在图形结构视图中，我们看到相应的两级遍历：从技能到技能再到职位名称。
- en: These examples show words (and their contexts) with similar meanings, but let’s
    also consider words like “a”, “the”, “of”, “and”, “if”, “they”, and countless
    other very common stop words. These words will also appear heavily within the
    same contexts of “pain”, “swelling”, “advil”, “ibuprofen”, or any of the other
    words we examined. This points to the second part of the distributional hypothesis—that
    the words must also occur with similar distributions. In essence, this means that
    given some number of documents containing a first term, any second term tends
    to be semantically similar to the first term if it co-occurs in the same documents
    as the first term more often than it co-occurs in documents with other random
    terms.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例显示了具有相似意义的词语（及其上下文），但让我们也考虑像“a”、“the”、“of”、“and”、“if”、“they”以及无数其他非常常见的停用词。这些词也会在“pain”、“swelling”、“advil”、“ibuprofen”或其他我们检查过的词语的相同上下文中大量出现。这指向分布假设的第二部分——即词语也必须以相似的方式出现。本质上，这意味着给定包含第一个术语的一些文档，任何第二个术语如果它比在包含其他随机术语的文档中更频繁地与第一个术语共现，那么它就倾向于与第一个术语在语义上相似。
- en: Practically, since “the” or “a” tend to co-occur commonly with almost all other
    terms, they are not considered semantically similar to those terms even though
    their level of co-occurrence is high. Terms like “pain” and “ibuprofen”, however,
    occur together statistically way more often than either term appears with random
    other terms, so they are considered semantically similar.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，由于“the”或“a”通常与几乎所有其他术语共同出现，尽管它们的共现程度很高，但它们在语义上并不被认为与这些术语相似。然而，像“pain”和“ibuprofen”这样的术语，其出现频率在统计上比任一术语与随机其他术语一起出现的频率要高得多，因此它们被认为是语义相似的。
- en: 'The following equation demonstrates one way to calculate the semantic relatedness
    of a term to a set of documents:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程展示了计算术语与一组文档语义相关性的方法：
- en: '![figure](../Images/grainger-ch5-eqs-0x.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/grainger-ch5-eqs-0x.png)'
- en: where
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '`x` is a query (usually a term or term sequence) for which the relatedness
    is calculated relative to another query, the foreground query `fg`. `D[x]` is
    the set of documents matching the query `x`.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`是一个查询（通常是一个术语或术语序列），其相关性是相对于另一个查询，前景查询`fg`来计算的。`D[x]`是匹配查询`x`的文档集。'
- en: '`D[fg]` is the set of documents matching the foreground query `fg`. The relatedness
    of `x` is calculated relative to this foreground set.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`D[fg]`是匹配前景查询`fg`的文档集。`x`的相关性是相对于这个前景集来计算的。'
- en: '`D[bg]` is the set of documents matching the background query `bg`. This `bg`
    query should be uncorrelated with `x` and `fg` and is usually set to match the
    entire collection of documents `D` or a random sample from `D`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`D[bg]`是匹配背景查询`bg`的文档集。这个`bg`查询应该与`x`和`fg`无关，通常设置为匹配整个文档集合`D`或`D`的随机样本。'
- en: '`P[x]` is the probability of finding `x` in a random document in the background
    set, calculated as ![equation image](../Images/eq-chapter-5-102-1.png)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P[x]`是在背景集中随机文档中找到`x`的概率，计算公式为![equation image](../Images/eq-chapter-5-102-1.png)'
- en: This `relatedness` calculation (conceptually similar to a z-score in a normal
    distribution) relies on the concept of a “foreground” set of documents and a “background”
    set of documents and enables the distribution of the term `x` to be statistically
    compared between the two sets. For example, if the foreground set was all documents
    matching the query `pain`, and the background set was all documents, then the
    relatedness of the term “advil” would be a measure of how much more often “advil”
    occurs in documents also containing the word “pain” (foreground set) versus in
    any random document (background set). It’s most common to normalize the relatedness
    score using a sigmoid function to map values between –1.0 and 1.0, with 0.0 indicating
    no relationship between the terms. For simplicity, we’ll rely on this normalized
    range of values in the code and all subsequent examples.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“相关性”计算（在概念上类似于正态分布中的z分数）依赖于“前景”文档集和“背景”文档集的概念，并使得可以在两个集合之间对术语`x`的分布进行统计比较。例如，如果前景集是所有匹配查询`pain`的文档，而背景集是所有文档，那么术语“advil”的相关性就是一个衡量“advil”在包含单词“pain”的文档（前景集）中出现的频率与在任意随机文档（背景集）中出现的频率之间的差异的度量。最常见的方法是使用Sigmoid函数来归一化相关性分数，将值映射到-1.0和1.0之间，其中0.0表示术语之间没有关系。为了简单起见，我们将在代码和所有后续示例中依赖这个归一化值域。
- en: If two terms are highly related, their relatedness will be a positive number
    approaching 1.0\. If the terms are highly unrelated (meaning they tend to occur
    in divergent domains only), the score would be closer to –1.0\. Finally, terms
    that aren’t semantically related at all—like stop words—will tend to have a relatedness
    score close to zero.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个术语高度相关，它们的相似度将是一个接近1.0的正数。如果术语高度不相关（意味着它们倾向于只在不同的领域出现），分数将更接近-1.0。最后，完全不相关的术语——如停用词——的相似度分数通常会接近零。
- en: Apache Solr has SKG capabilities built directly into its faceting API. Faceting
    provides the ability to traverse from terms to sets of documents to terms, and
    a relatedness aggregation function (`RelatednessAgg`) implements the semantic
    similarity calculation we just described. The following listing demonstrates searching
    for semantically related terms to “advil” within the Stack Exchange health dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Solr直接在其分面API中构建了SKG功能。分面提供了从术语遍历到文档集合再到术语的能力，一个相关度聚合函数（`RelatednessAgg`）实现了我们刚刚描述的语义相似性计算。以下列表演示了在Stack
    Exchange健康数据集中搜索与“advil”语义相关的术语。
- en: Listing 5.4 Discovering semantically related terms to `advil`
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.4 发现与`advil`语义相关的术语
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 The field in which to find values for the starting node'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 查找起始节点的值所在的字段'
- en: '#2 Our starting node is the query “advil”.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们的起始节点是查询“advil”。'
- en: '#3 Reduces noise by excluding terms not found at least this many times'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 通过排除至少出现这么多次的术语来减少噪声'
- en: '#4 How many nodes (terms) will be returned'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 返回多少个节点（术语）'
- en: '#5 Performs the graph traversal'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 执行图遍历'
- en: '#6 Prints the results of the SKG traversal'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 打印SKG遍历的结果'
- en: 'Output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, of all the terms within the forum posts in the Stack Exchange
    health dataset, the ranked order of the most semantically related terms to `advil`
    was a list of similar painkillers. This is the magic of using the distributional
    hypothesis to discover and rank terms by semantic similarity—it provides us with
    the ability to automatically discover relationships in real time that can be used
    to further improve our understanding of incoming queries.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，在Stack Exchange健康数据集中的论坛帖子中，与`advil`最语义相关的术语的排名顺序是一系列类似的止痛药。这是使用分布假设通过语义相似性发现和排名术语的魔力——它为我们提供了实时自动发现关系的能力，这些关系可以进一步改进我们对传入查询的理解。
- en: The following is a Solr SKG request, which uses Solr’s JSON faceting API and
    ability to sort on a function—the `relatedness` calculation we just discussed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个Solr SKG请求，它使用了Solr的JSON分面API和基于函数排序的能力——我们刚刚讨论的`relatedness`计算。
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `skg.traverse(*nodes_to_traverse)` function in listing 5.4 abstracts away
    this engine-specific syntax, but if you’re trying to understand the nuances of
    how your specific search engine or vector database internally handles these kinds
    of knowledge graph traversals, you can inspect the function in the notebooks.
    We’ll mostly show the `skg.traverse` abstraction going forward, but you can always
    call the `skg.transform_ request(*nodes_to_traverse)` function directly to see
    and debug the internal, engine-specific request.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4中的`skg.traverse(*nodes_to_traverse)`函数抽象了特定引擎的语法，但如果你试图理解你的特定搜索引擎或向量数据库内部如何处理这类知识图谱遍历的细微差别，你可以检查笔记本中的函数。我们将主要展示`skg.traverse`的抽象，但你可以始终直接调用`skg.transform_request(*nodes_to_traverse)`函数来查看和调试内部、特定引擎的请求。
- en: In the next section, we’ll discuss how we can apply the related terms returned
    from this SKG traversal to improve query relevance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何将此SKG遍历返回的相关术语应用于提高查询的相关性。
- en: 5.4.5 Using SKGs for query expansion
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.5 使用SKGs进行查询扩展
- en: Matching and ranking solely on the keywords entered during a search doesn’t
    always provide sufficient context to find and rank the best results. In these
    cases, you can significantly improve the quality of search results by dynamically
    expanding or otherwise augmenting queries to include conceptually related terms.
    In this section, we’ll walk through how you can generate these related terms,
    and we’ll demonstrate several strategies for applying the terms to enhance the
    quality of your search results.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 仅根据搜索过程中输入的关键词进行匹配和排名并不总是提供足够的信息来找到和排名最佳结果。在这些情况下，通过动态扩展或以其他方式增强查询以包括概念上相关的术语，你可以显著提高搜索结果的质量。在本节中，我们将介绍如何生成这些相关术语，并展示几种将术语应用于提高搜索结果质量的方法。
- en: Given its ability to start with any keyword or query and to find other highly
    related terms in any field, one obvious use case for an SKG is to dynamically
    expand queries to include related terms. This kind of expansion is sometimes referred
    to as *sparse lexical expansion*, as it operates on sparse vectors of query tokens
    made of term-based (lexical) features. One well-known technique for implementing
    this kind of query expansion is SPLADE (the Sparse Lexical and Expansion model),
    which we’ll cover in section 7.4.3\. Semantic knowledge graphs also provide a
    great way to generate contextual, sparse lexical expansions and have the benefit
    that they require no additional fine-tuning to your dataset. This enables documents
    to match even if they don’t necessarily contain the exact keywords entered by
    the user, but they do contain other terms that carry a very similar meaning. For
    example, instead of a user’s query for `advil`, an expanded query with boosted
    terms generated by the SKG might look something like `advil` `OR` `motrin^0.59897`
    `OR` `aleve^0.4662` `OR` `ibuprofen^0.3824` `OR` `.` `.` `.`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其能够从任何关键词或查询开始，并在任何领域找到其他高度相关的术语，一个明显的用例是动态扩展查询以包含相关术语。这种扩展有时被称为*稀疏词汇扩展*，因为它在由基于（词汇）特征的查询标记的稀疏向量上操作。实现这种查询扩展的一个著名技术是SPLADE（稀疏词汇和扩展模型），我们将在第7.4.3节中介绍。语义知识图谱也提供了一种生成上下文稀疏词汇扩展的绝佳方式，并且它们的好处是无需对您的数据集进行额外的微调。这使得即使文档不包含用户输入的确切关键词，也能匹配文档，但它们确实包含其他具有非常相似意义的术语。例如，对于一个用户对`advil`的查询，由SKG生成的增强术语的扩展查询可能看起来像这样：`advil`
    `OR` `motrin^0.59897` `OR` `aleve^0.4662` `OR` `ibuprofen^0.3824` `OR` `.` `.`
    `.`。
- en: 'Let’s walk through the steps for implementing this kind of query expansion,
    using a dataset from a different domain this time (the Stack Exchange scifi dataset).
    The following listing shows the first step in this process: searching for an obscure
    term (as a node in the SKG) and finding related other terms (as related nodes
    in the SKG). In this case, we’ll use the query for `vibranium` as our starting
    node.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用来自不同领域的数据集（Stack Exchange scifi数据集）来实现这种查询扩展的步骤进行说明。以下列表显示了此过程的第一个步骤：在SKG中搜索一个晦涩的术语（作为SKG中的一个节点）并找到相关的其他术语（作为SKG中的相关节点）。在这种情况下，我们将使用对`vibranium`的查询作为我们的起始节点。
- en: Listing 5.5 Discovering context for the unknown term “vibranium”
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.5 发现未知术语“vibranium”的上下文
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Response:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 响应：
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For anyone unfamiliar with the term “vibranium”, it is a strong, fictional metal
    that exists in Marvel comic books and movies (best popularized through the 2018
    Hollywood hit *Black Panther*). The most related terms that came back were related
    to “Wakandan” and “Wakanda”, the fictional culture and country from which vibranium
    originates, “adamantium”, another strong (fictional) metal from Marvel comics,
    and the names “Maclain” and “Klaw”, characters in the Marvel comic books that
    are heavily associated with the metal vibranium. Maclain created the vibranium
    “alloy” used to make Captain “America’s” shield, hence the relatedness of those
    words.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何不熟悉术语“vibranium”的人来说，它是一种强大、虚构的金属，存在于漫威漫画和电影中（通过2018年好莱坞热门电影《黑豹》广受欢迎）。返回的最相关术语是与“Wakandan”和“Wakanda”相关的，这是vibranium起源的虚构文化和国家，“adamantium”，另一种来自漫威漫画的强大（虚构）金属，以及“Maclain”和“Klaw”的名字，这些是漫威漫画中与金属vibranium高度相关的角色。Maclain创造了用于制造“美国队长”盾牌的vibranium“合金”，因此这些词的相关性。
- en: An autogenerated knowledge graph is very effective at identifying related pieces
    of information. By using an SKG and expanding your query to include additional
    related context, you can drastically improve the recall of your search requests.
    By boosting results that best match your query conceptually (as opposed to just
    the text), you may also be able to improve the precision of your top-ranked search
    results.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的知识图谱在识别相关信息方面非常有效。通过使用SKG并将您的查询扩展到包括额外的相关上下文，您可以极大地提高搜索请求的召回率。通过增强与您的查询概念上最佳匹配的结果（而不是仅仅匹配文本），您也可能能够提高您顶级搜索结果的精确度。
- en: The following listing demonstrates an example of translating this original query,
    along with the SKG output, into an expanded query.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表演示了将原始查询以及SKG输出翻译成扩展查询的示例。
- en: Listing 5.6 Expanding a query with nodes in an SKG
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.6 使用SKG中的节点扩展查询
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Expanded query:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展查询：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, we are doing a simple Boolean OR search for any of the keywords
    related to the original query `vibranium`, boosting the original query term’s
    weight by a factor of 5x and weighting each subsequent term’s effect on the relevance
    score based upon its semantic similarity score. The choice to boost the original
    term by 5x is arbitrary—you can choose any value here to assign a relative relevance
    boost compared to the other (expanded) terms.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们正在进行简单的布尔 OR 搜索，以查找与原始查询 `vibranium` 相关的任何关键词，将原始查询项的权重提高 5 倍，并根据其语义相似度分数对后续术语对相关性分数的影响进行加权。选择将原始术语提高
    5 倍是任意的——你可以选择任何值来分配与其他（扩展）术语相比的相对相关性提升。
- en: You might also notice that the term “vibranium” appears twice—first as the original
    term, and then again as an expanded term (since the term is *also* the most semantically
    similar to itself). This will almost always be the case if you are searching for
    individual keywords, but since your query might have phrases or other constructs
    that make the original query different from the returned terms (if any), it is
    usually a good idea to include the original query as part of the expanded (rewritten)
    query, so the user’s actual query is always represented in the results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到，“vibranium”这个术语出现了两次——第一次作为原始术语，然后再次作为扩展术语（因为术语*也是*与其自身最语义相似的）。如果你正在搜索单个关键词，这几乎总是这种情况，但由于你的查询可能包含短语或其他结构，使得原始查询与返回的术语（如果有）不同，通常将原始查询作为扩展（重新编写）查询的一部分是一个好主意，这样用户的实际查询总是会在结果中表示出来。
- en: Although the prior expanded query should rank the results reasonably well (prioritizing
    documents matching multiple related terms), it is also heavily focused on *recall*
    (expanding to include anything relevant) as opposed to *precision* (ensuring everything
    included is relevant). An augmented query can be constructed in many ways, depending
    on your primary goals.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然先前扩展的查询应该能够合理地排序结果（优先考虑匹配多个相关术语的文档），但它也高度关注*召回率*（扩展以包含任何相关内容），而不是*精度*（确保包含的内容都是相关的）。增强查询可以根据你的主要目标以多种方式构建。
- en: Rewritten queries can perform a simple expansion, require a minimum percentage
    or number of terms to match, require specific terms like the original query to
    match, or even just change the ranking of the same initial results set. The following
    listing demonstrates several examples, using minimum match thresholds and percentages,
    which can tilt the scale between precision and recall as needed.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 重新编写的查询可以执行简单的扩展，需要匹配最小百分比或数量的术语，需要匹配原始查询中的特定术语，或者甚至只需更改相同的初始结果集的排名。以下列表展示了几个示例，使用最小匹配阈值和百分比，可以根据需要调整精度和召回率之间的平衡。
- en: Listing 5.7 Different query augmentation strategies
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.7 不同的查询增强策略
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s look at the final search queries for each of the preceding query expansion
    techniques.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看前面提到的查询增强技术的最终搜索查询。
- en: 'Simple query expansion: `simple_expansion`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 简单查询扩展：`simple_expansion`
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This simple query expansion is the same as previously described, matching any
    documents containing either the original query or any of the semantically related
    terms.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的查询扩展与之前描述的相同，匹配包含原始查询或任何语义相关术语的任何文档。
- en: 'Increased-precision, reduced-recall query: `increased_conceptual_precision`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 提高精度、降低召回率的查询：`increased_conceptual_precision`
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This increased-precision, reduced-recall example specifies a “minimum match”
    threshold of 30%, meaning that for a document to match, it must contain at least
    30% (rounded down) of the terms in the query.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提高精度、降低召回率的示例指定了一个“最小匹配”阈值为 30%，这意味着为了匹配，文档必须包含至少 30%（向下取整）的查询术语。
- en: 'Increased precision of top results, no reduction in recall: `increased_precision_
    same_recall`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 提高顶部结果精度，不减少召回率：`increased_precision_ same_recall`
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This increased-precision, same-recall query requires the term “vibranium” to
    match, and it will rank documents higher when other expansion terms match, leading
    to an increase in precision for the top results.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提高精度、相同召回率的查询要求“vibranium”这个术语匹配，并且当其他扩展术语匹配时，将提高文档的排名，从而提高顶部结果的精度。
- en: 'Slightly increased-recall query: `slightly_increased_recall`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 略微提高召回率的查询：`slightly_increased_recall`
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This slightly increased recall query requires two terms to match, but it does
    not explicitly require the original query, so it can expand to other documents
    that are conceptually similar but don’t necessarily contain the original query
    term. Since the term “vibranium” is repeated twice, any documents containing just
    “vibranium” will also match.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个略微增加召回率的查询需要两个术语匹配，但它并不明确要求原始查询，因此它可以扩展到其他概念上相似但不必包含原始查询术语的文档。由于术语“vibranium”重复两次，任何仅包含“vibranium”的文档也将匹配。
- en: 'Same results, better conceptual ranking: `same_results_better_ranking`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的结果，更好的概念排名：`same_results_better_ranking`
- en: '[PRE18]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This final query returns the same documents as the original query for `vibranium`,
    but it ranks them differently according to how well they match the semantically
    similar terms from the knowledge graph. This ensures the keyword exists in all
    matched documents and that all documents containing the user’s query are returned,
    while also greatly improving ranking by boosting more contextually relevant documents.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终的查询返回与原始查询`vibranium`相同的文档，但根据它们与知识图谱中语义相似术语的匹配程度进行不同的排名。这确保了关键词存在于所有匹配的文档中，并且返回包含用户查询的所有文档，同时通过提升更多上下文相关的文档来大大提高排名。
- en: Of course, there are an unlimited number of possible query permutations you
    can explore when rewriting your query to include enhanced semantic context, but
    the preceding examples should provide a good sense of the kinds of options available
    and the tradeoffs you’ll want to consider.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在重写查询以包括增强的语义上下文时，你可以探索无限多种可能的查询排列，但前面的例子应该能提供对可用的选项和需要考虑的权衡的良好感觉。
- en: 5.4.6 Using SKGs for content-based recommendations
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.6 使用SKGs进行基于内容的推荐
- en: In the last section, we explored how we could augment queries by discovering
    and using related nodes from the SKG, including multiple ways of structuring rewritten
    queries to optimize for precision, recall, or even improved conceptual ranking
    over the same results. In addition to expanding queries with semantically related
    terms, it’s also possible to use the SKG to generate content-based recommendations
    by translating documents into queries based on the semantic similarity of the
    terms within the documents.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们探讨了如何通过发现和使用SKG中的相关节点来增强查询，包括多种结构重写查询的方法以优化精确度、召回率，甚至是在相同结果上改进概念排名。除了使用语义相关术语扩展查询外，还可以使用SKG通过根据文档中术语的语义相似性将文档翻译成查询来生成基于内容的推荐。
- en: Since nodes in the SKG can represent any arbitrary query, we can take terms
    from documents and model them as arbitrary nodes to be scored relative to some
    known context about the document. This means we can take dozens or hundreds of
    terms from a document, score them all relative to the topic of the document, and
    then use the most semantically similar terms to generate a query best representing
    the nuanced, contextual meaning of the document.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SKG中的节点可以代表任何任意查询，我们可以从文档中提取术语并将它们建模为任意节点，相对于文档的已知上下文进行评分。这意味着我们可以从文档中提取数十或数百个术语，将它们全部相对于文档的主题进行评分，然后使用最语义相似的术语生成一个最能代表文档细微、语境意义的查询。
- en: The following listing walks through an example of translating a document classified
    as “star wars” and ranking all the terms in the document relative to that topic.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何将一个被分类为“星球大战”的文档进行翻译，并按该主题对所有文档中的术语进行排名。
- en: Listing 5.8 Calculating how related document terms are to “star wars”
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.8 计算文档术语与“星球大战”的相关性
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Scored nodes:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 评分节点：
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In these results, you can see a list of terms from the document that is nicely
    ordered based upon semantic similarity to the topic of “star wars”. Terms with
    lower scores will have no relatedness or negative relatedness with the specified
    topic. The following listing filters terms with at least a relatedness above `0.25`
    to get a very clean list of relevant terms from the document.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些结果中，你可以看到一份根据与“星球大战”主题的语义相似性很好地排序的文档术语列表。得分较低的术语将与指定主题没有相关性或负相关性。下面的列表通过过滤至少具有`0.25`以上相关性的术语来获取文档中非常干净的、相关的术语列表。
- en: Listing 5.9 Generating a recommendation query from scored phrases
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.9 从评分短语生成推荐查询
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Expanded query:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展查询：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The next listing demonstrates the last step in this process—running the search
    to return the top documents most semantically similar to the original document.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表展示了此过程的最后一步——运行搜索以返回与原始文档最语义相似的顶级文档。
- en: Listing 5.10 Running the content-based recommendations query
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.10 运行基于内容的推荐查询
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: What we have just created is a content-based recommendations algorithm. When
    there’s an insufficient amount of user behavioral signals for signal-based recommendations
    like collaborative filtering (see section 4.2.3), a content-based approach can
    generate recommendations that are still context- and domain-aware.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的是一个基于内容的推荐算法。当用户行为信号不足，无法进行基于信号的推荐（如协同过滤，见第4.2.3节）时，基于内容的方法可以生成仍然具有上下文和领域意识的推荐。
- en: The example in this section generated a content-based recommendations query
    based on terms found in the starting document, but it is worth keeping in mind
    that the SKG is not restricted to using the terms passed in. You could add an
    extra level to the traversal to find additional terms that are semantically related
    to the terms in the original document, but not actually contained within it. This
    can be particularly useful for niche topics where not enough documents match the
    recommendations query—traversing further will open new possibilities for exploration.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例生成了一个基于起始文档中找到的术语的基于内容的推荐查询，但值得记住的是，SKG并不限于使用传入的术语。你可以添加一个额外的遍历级别，以找到与原始文档中的术语语义相关的其他术语，但实际上并不包含在文档中。这对于主题狭窄且推荐查询匹配的文档不足的情况尤其有用——进一步的遍历将打开新的探索可能性。
- en: In the next section, we’ll take a quick step beyond the “is related to” graph
    relationships and see if we can use the SKG to also generate and traverse some
    more interesting edges.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将快速超越“相关”图关系，看看我们是否可以使用SKG生成和遍历一些更有趣的边。
- en: 5.4.7 Using SKGs to model arbitrary relationships
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.7 使用SKGs来建模任意关系
- en: Thus far, all our SKG traversals have used an “is related to” relationship.
    That is to say, we’ve been finding the strength of the semantic relationship between
    two words or phrases using the `relatedness` function, but we have only measured
    that the nodes are “related”, not *how* they are related. What if we could find
    other kinds of edges between nodes instead of just “is related to” type edges?
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的SKG遍历都使用了“相关”关系。也就是说，我们一直在使用`relatedness`函数找到两个单词或短语之间的语义关系的强度，但我们只测量了节点是“相关的”，而没有测量它们是如何相关的。如果我们能找到节点之间除了“相关”类型边之外的其他类型的边会怎样呢？
- en: If you recall, the nodes in an SKG are materialized on the fly by executing
    a query that matches a set of documents. If the node you start with is `engineer`,
    that node is internally represented as the set of all documents containing the
    word “engineer”. If the node is labeled as `software engineer`, that node is internally
    represented as the set of all documents containing the term “software” intersected
    with all documents containing the term “engineer”. If the search is for `"software
    engineer" OR java` then it is internally represented as the union of the set of
    all documents containing the term “software” one position before the term “engineer”
    (a phrase) with the set of all documents containing the term “java”. All queries,
    regardless of their complexity, are internally represented as a set of documents.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，SKG中的节点是通过执行一个匹配一组文档的查询而即时物化的。如果你从“engineer”节点开始，该节点在内部表示为包含单词“engineer”的所有文档的集合。如果节点被标记为“software
    engineer”，那么该节点在内部表示为包含术语“software”的所有文档与包含术语“engineer”的所有文档的交集。如果搜索是“`software
    engineer` OR java”，那么它在内部表示为包含术语“software”且位于“engineer”（短语）之前一个位置的文档集合与包含术语“java”的所有文档集合的并集。所有查询，无论其复杂程度如何，在内部都表示为文档集合。
- en: You may also recall that an edge is formed by finding the set of documents containing
    both nodes. This means that *both* nodes and edges are internally represented
    using the same mechanism—a set of documents. Practically speaking, this means
    that if we can construct a node using a query that approximates an interesting
    relationship (as opposed to an entity), we can relate two nodes together through
    the “relationship node” in a similar way to how an edge would be used to relate
    the nodes together in a traditional graph structure.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也记得，边是通过找到包含两个节点的文档集合来形成的。这意味着**节点和边**都是使用相同的机制——文档集合——在内部表示。从实际的角度来看，这意味着如果我们可以使用查询构建一个节点，该查询近似于一个有趣的关系（而不是实体），那么我们可以通过“关系节点”以类似的方式将两个节点联系起来，就像在传统的图结构中使用边将节点联系起来一样。
- en: Let’s work through an example. Revisiting our scifi dataset, let’s say we wanted
    to ask a question about Jean Grey, one of the popular characters from Marvel Comics’
    X-Men franchise. Specifically, let’s say that we wanted to figure out who was
    in love with Jean Grey.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来分析。回顾我们的科幻数据集，假设我们想要问一个关于Jean Grey的问题，她是漫威漫画X战警系列中的一位受欢迎的角色。具体来说，假设我们想要弄清楚谁爱上了Jean
    Grey。
- en: We can accomplish this by using a starting node of `jean grey`, traversing to
    the node `in love with`, and then requesting the top related terms associated
    with `in love with` within the context of `jean grey`. Listing 5.11 demonstrates
    this query. By traversing through a node designed to capture an explicit linguistic
    relationship (`in love with`, in this case), we can use the intermediate node
    to model an edge between the starting and terminating nodes.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用起始节点`jean grey`，遍历到节点`in love with`，然后在`jean grey`的上下文中请求与`in love with`相关的顶级相关术语来实现这一点。列表5.11展示了这个查询。通过遍历一个旨在捕获显式语言关系的节点（在这种情况下是`in
    love with`），我们可以使用中间节点来表示起始节点和终止节点之间的边。
- en: Listing 5.11 Materializing an edge through a “relationship node”
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.11 通过“关系节点”实例化一条边
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In case you’re unfamiliar with these characters, here’s the relevant background
    on Jean Grey: she has recurring relationships with two mutants, one named Cyclops
    (real name: Scott Summers) and one named Wolverine. Additionally, and unknown
    to most fans, two of Jean Grey’s mentors, Professor Charles Xavier and Magneto,
    were known to have a love interest in Jean Grey at points throughout the comic
    books.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些角色不熟悉，以下是关于Jean Grey的相关背景：她与两个变种人有着持续的关系，一个名叫Cyclops（真名：Scott Summers），另一个名叫Wolverine。此外，大多数粉丝不知道，Jean
    Grey的两个导师，查尔斯·泽维尔教授和磁力王，在漫画书中的一些时候被知道对Jean Grey有爱慕之情。
- en: If we examine the results from listing 5.11, we’ll see all of these expected
    names listed. The first two terms, “jean” and “grey”, are the most related, since
    we are searching for `in love with` relative to `jean grey`. Her name is going
    to be highly semantically related to itself. The next two terms, “summers” and
    “cyclops”, both refer to the same person, Jean’s most prominent love interest.
    Then we see “xavier” and “wolverine”, and the last result in the list is “magneto”.
    Figure 5.7 illustrates some of the underlying graph relationships for this traversal.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查列表5.11的结果，我们会看到所有这些预期的名字都被列出。前两个术语，“jean”和“grey”，是最相关的，因为我们正在搜索与“jean
    grey”相关的“in love with”。她的名字将与她自己高度语义相关。接下来的两个术语，“summers”和“cyclops”，都指的是同一个人，Jean的最突出的爱情对象。然后我们看到“xavier”和“wolverine”，列表中的最后一个结果是“magneto”。图5.7展示了这次遍历的一些底层图关系。
- en: '![figure](../Images/CH05_F07_Grainger.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F07_Grainger.png)'
- en: Figure 5.7 Traversing arbitrarily defined edge types. By materializing a new
    node with the combined context of both the originating node (`"jean grey"`) and
    a new node (`"in love with"`), we can traverse from that combined node (`"jean
    grey" + "in love with"`) to other nodes. This is equivalent to saying we are traversing
    from `"jean grey"` through an edge of `"in love with"` to the other nodes.
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7 遍历任意定义的边类型。通过实例化一个结合了起始节点（`"jean grey"`）和新的节点（`"in love with"`）的上下文的节点，我们可以从这个组合节点（`"jean
    grey" + "in love with"`）遍历到其他节点。这相当于说我们正在通过`"in love with"`边从`"jean grey"`遍历到其他节点。
- en: By using an intermediate node (i.e., `in love with`) to model a relationship
    between other nodes, we can form any arbitrarily typed edge between nodes, as
    long as we can express that edge as a search query.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用一个中间节点（即`in love with`）来表示其他节点之间的关系，我们可以形成节点之间的任意类型边，只要我们能将这条边表达为一个搜索查询。
- en: While the results of our graph traversal in listing 5.11 were pretty good, we
    do see the terms “x” (presumably from “x-men”) and “mutant” also showing up. Jean
    Grey and all the other listed people are mutants in the X-Men comics, which is
    why these terms are so semantically related. However, these terms are not great
    answers to the question “Who is in love with Jean Grey?”
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然列表5.11中我们的图遍历结果相当不错，但我们确实看到了术语“x”（可能是来自“x-men”）和“mutant”也出现了。Jean Grey和列出的其他所有人都是X战警漫画中的变种人，这就是为什么这些术语在语义上如此相关。然而，这些术语并不是对“谁爱上了Jean
    Grey？”这个问题的好答案。
- en: 'This brings up an important point: the SKG is a statistical knowledge graph.
    The existence of the `in love with` relationship is purely based upon statistical
    correlations of terms within our collection, so just as with any ontology learning
    approach, there is going to be noise. That said, for an autogenerated graph with
    no explicit modeling of entities, these results are quite good.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这提出了一个重要观点：SKG 是一个统计知识图谱。`in love with` 这种关系的存在纯粹是基于我们收集的术语之间的统计相关性，因此就像任何本体学习方法一样，总会存在一些噪声。尽管如此，对于一个没有显式建模实体的自动生成的图谱来说，这些结果相当不错。
- en: If we wanted to improve the quality of these results, one of the easiest things
    to do would be to run preprocessing on the content to identify entities (people,
    places, and things) and index those instead of just single-term keywords. This
    would cause actual people’s names (e.g., “Scott Summers”, “Charles Xavier”, “Jean
    Grey”) to be returned instead of just individual keywords (“summers”, “xavier”,
    “jean”, “grey”).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要提高这些结果的质量，最简单的事情之一就是对内容进行预处理以识别实体（人物、地点和事物）并索引这些实体，而不是仅仅索引单个关键词。这将导致返回实际人物的名字（例如，“斯科特·萨默斯”、“查尔斯·泽维尔”、“珍妮·格雷”），而不是仅仅返回单个关键词（“summers”、“xavier”、“jean”、“grey”）。
- en: It is also worth pointing out that the traversal of relationships depends entirely
    on whether those relationships were discussed in the underlying corpus of documents.
    In this case, plenty of forum posts discuss each of these peoples’ relationships
    with Jean Grey. Had insufficient documents existed, the results returned may have
    been poor or nonexistent. To avoid noise in our results, we set a `min_occurrences`
    threshold of `25`, indicating that at least 25 documents must exist discussing
    `jean grey`, `in love with`, and the other nodes found and scored. We recommend
    setting a `min_occurrences` to some number greater than 1 to avoid false positives.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，关系的遍历完全取决于这些关系是否在底层文档的语料库中讨论过。在这种情况下，大量的论坛帖子讨论了这些人与珍妮·格雷的关系。如果存在不足的文档，返回的结果可能很差或不存在。为了避免结果中的噪声，我们设置了
    `min_occurrences` 阈值为 `25`，这意味着至少必须有 25 篇文档讨论 `jean grey`、`in love with` 以及找到和评分的其他节点。我们建议将
    `min_occurrences` 设置为一个大于 1 的数字，以避免假阳性。
- en: While traversing arbitrary linguistic relationships like “in love with” can
    be useful from an exploratory standpoint, it is usually sufficient from a query
    understanding standpoint to stick with the default “is related to” relationship
    and use the relatedness scores between terms for most semantic search use cases.
    It can still be useful to traverse through multiple levels of relationships to
    generate better context, however. Specifically, it can be useful to traverse from
    a term to a classification field to provide some additional context, and then
    to related meanings of the term within that category. We’ll cover this strategy
    in more detail in chapter 6, where we’ll focus on disambiguating terms with multiple
    meanings.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从探索的角度来看，遍历像“in love with”这样的任意语言关系可能是有用的，但从查询理解的角度来看，通常只需要坚持默认的“is related
    to”关系，并使用术语之间的相关性分数来处理大多数语义搜索用例。然而，遍历多个层次的关系以生成更好的上下文仍然是有用的。具体来说，从术语遍历到分类字段以提供一些额外的上下文，然后到该类别中术语的相关含义，这可能是有用的。我们将在第6章中更详细地介绍这种策略，其中我们将关注具有多个含义的术语的消歧。
- en: 5.5 Using knowledge graphs for semantic search
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 使用知识图谱进行语义搜索
- en: By providing the ability to accept arbitrary queries and dynamically discover
    related terms in a context-sensitive way, SKGs become a key tool for query interpretation
    and relevance ranking. We’ve seen that not only can SKGs help interpret and expand
    queries, they can also provide the abilities to classify queries and keywords
    in real time and to disambiguate multiple meanings of the terms in each query.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供接受任意查询并动态发现上下文中相关术语的能力，SKGs 成为了查询解释和相关性排名的关键工具。我们已经看到，SKGs 不仅可以帮助解释和扩展查询，还可以提供实时分类查询和关键词以及消歧每个查询中术语的多种含义的能力。
- en: Early in the chapter, we also explored how to build explicit knowledge graphs
    through open information extraction techniques. What may not be obvious yet is
    how to parse arbitrary incoming queries and look up the appropriate context and
    entities in the knowledge graph. We’ll spend the majority of chapter 7 covering
    how to build an end-to-end semantic search system that can parse queries and integrate
    these knowledge graph capabilities.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期，我们也探讨了如何通过开放信息提取技术构建显式知识图谱。可能还不明显的是如何解析任意传入的查询并在知识图谱中查找适当上下文和实体。我们将用第7章的大部分内容来介绍如何构建一个端到端的语义搜索系统，该系统能够解析查询并整合这些知识图谱功能。
- en: There are still some critical kinds of relationships we need to add to our knowledge
    graph that are important for search engines, such as misspellings, synonyms, and
    domain-specific phrases. We’ll cover how to automatically learn each of these
    sources of domain-specific terminology from user signals or content in the next
    chapter, which focuses on learning domain-specific language.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍需要添加一些对搜索引擎至关重要的关键关系到我们的知识图谱中，例如拼写错误、同义词和领域特定短语。我们将在下一章中介绍如何从用户信号或内容中自动学习这些领域特定术语的每个来源，该章节专注于学习领域特定语言。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Knowledge graphs model the relationships between entities within your domain
    and can be built explicitly with known relationships or can be extracted dynamically
    from your content.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱建模了您领域内实体之间的关系，可以通过已知关系显式构建，也可以从您的内容中动态提取。
- en: Open information extraction, the process of extracting facts from your content
    (subject, relationship, object triples) can be used to learn arbitrary relationships
    (which typically results in noisy data) or to extract hyponym and hypernym relationships
    (less noisy) from text into an explicit knowledge graph.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放信息提取，即从您的内容中提取事实（主题、关系、对象三元组）的过程，可以用来学习任意关系（通常会产生噪声数据）或从文本中提取下位词和上位词关系（噪声较少）到显式知识图谱中。
- en: Semantic knowledge graphs (SKGs) enable the traversal and ranking of arbitrary
    semantic relationships between any content within your search index. This allows
    you to use your indexed content directly as a knowledge graph and language model
    without any additional data modeling required.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义知识图谱（SKGs）能够遍历和排名您搜索索引中任何内容之间的任意语义关系。这使得您可以直接使用索引内容作为知识图谱和语言模型，而无需进行任何额外的数据建模。
- en: Content-based recommendations that don’t rely on user signals can be generated
    by ranking the most semantically interesting terms and phrases from documents
    and using them as a query to find and rank other related documents.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不依赖于用户信号的基于内容的推荐可以通过对文档中最具语义意义的术语和短语进行排名，并使用它们作为查询来查找和排名其他相关文档来生成。
- en: SKGs enable a better understanding of user intent by powering domain-sensitive
    and context-sensitive relationship discovery and query expansion.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SKGs通过提供领域敏感和上下文敏感的关系发现和查询扩展，能够更好地理解用户意图。
- en: '[[1]](#ftnote-113) Grainger, et al., “The Semantic Knowledge Graph: A compact,
    auto-generated model for real-time traversal and ranking of any relationship within
    a domain.” In *2016 IEEE International Conference on Data Science and Advanced
    Analytics (DSAA)*, pp. 420–429\. IEEE, 2016.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#ftnote-113) Grainger等人，“语义知识图谱：一个紧凑的、自动生成的模型，用于实时遍历和排名领域内的任何关系。”在*2016年IEEE国际数据科学和高级分析会议（DSAA）*，第420-429页。IEEE，2016。'
