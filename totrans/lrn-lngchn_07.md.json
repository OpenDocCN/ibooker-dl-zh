["```py\nfrom typing import Annotated, TypedDict\n\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import add_messages\n\nmodel = ChatOpenAI()\n\nclass State(TypedDict):\n    messages: Annotated[list[BaseMessage], add_messages]\n\ngenerate_prompt = SystemMessage(\n    \"\"\"You are an essay assistant tasked with writing excellent 3-paragraph \n essays.\"\"\"\n    \"Generate the best essay possible for the user's request.\"\n    \"\"\"If the user provides critique, respond with a revised version of your \n previous attempts.\"\"\"\n)\n\ndef generate(state: State) -> State:\n    answer = model.invoke([generate_prompt] + state[\"messages\"])\n    return {\"messages\": [answer]}\n\nreflection_prompt = SystemMessage(\n    \"\"\"You are a teacher grading an essay submission. Generate critique and \n recommendations for the user's submission.\"\"\"\n    \"\"\"Provide detailed recommendations, including requests for length, depth, \n style, etc.\"\"\"\n)\n\ndef reflect(state: State) -> State:\n    # Invert the messages to get the LLM to reflect on its own output\n    cls_map = {AIMessage: HumanMessage, HumanMessage: AIMessage}\n    # First message is the original user request. \n    # We hold it the same for all nodes\n    translated = [reflection_prompt, state[\"messages\"][0]] + [\n        cls_map[msg.__class__](content=msg.content) \n            for msg in state[\"messages\"][1:]\n    ]\n    answer = model.invoke(translated)\n    # We treat the output of this as human feedback for the generator\n    return {\"messages\": [HumanMessage(content=answer.content)]}\n\ndef should_continue(state: State):\n    if len(state[\"messages\"]) > 6:\n        # End after 3 iterations, each with 2 messages\n        return END\n    else:\n        return \"reflect\"\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"generate\", generate)\nbuilder.add_node(\"reflect\", reflect)\nbuilder.add_edge(START, \"generate\")\nbuilder.add_conditional_edges(\"generate\", should_continue)\nbuilder.add_edge(\"reflect\", \"generate\")\n\ngraph = builder.compile()\n```", "```py\nimport {\n  AIMessage,\n  BaseMessage,\n  SystemMessage,\n  HumanMessage,\n} from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport {\n  StateGraph,\n  Annotation,\n  messagesStateReducer,\n  START,\n  END,\n} from \"@langchain/langgraph\";\n\nconst model = new ChatOpenAI();\n\nconst annotation = Annotation.Root({\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\n});\n\n// fix multiline string\nconst generatePrompt = new SystemMessage(\n  `You are an essay assistant tasked with writing excellent 3-paragraph essays.\n Generate the best essay possible for the user's request.\n If the user provides critique, respond with a revised version of your \n previous attempts.`\n);\n\nasync function generate(state) {\n  const answer = await model.invoke([generatePrompt, ...state.messages]);\n  return { messages: [answer] };\n}\n\nconst reflectionPrompt = new SystemMessage(\n  `You are a teacher grading an essay submission. Generate critique and \n recommendations for the user's submission.\n Provide detailed recommendations, including requests for length, depth, \n style, etc.`\n);\n\nasync function reflect(state) {\n  // Invert the messages to get the LLM to reflect on its own output\n  const clsMap: { [key: string]: new (content: string) => BaseMessage } = {\n    ai: HumanMessage,\n    human: AIMessage,\n  };\n  // First message is the original user request. \n  // We hold it the same for all nodes\n  const translated = [\n    reflectionPrompt,\n    state.messages[0],\n    ...state.messages\n      .slice(1)\n      .map((msg) => new clsMap[msg._getType()](msg.content as string)),\n  ];\n  const answer = await model.invoke(translated);\n  // We treat the output of this as human feedback for the generator\n  return { messages: [new HumanMessage({ content: answer.content })] };\n}\n\nfunction shouldContinue(state) {\n  if (state.messages.length > 6) {\n    // End after 3 iterations, each with 2 messages\n    return END;\n  } else {\n    return \"reflect\";\n  }\n}\n\nconst builder = new StateGraph(annotation)\n  .addNode(\"generate\", generate)\n  .addNode(\"reflect\", reflect)\n  .addEdge(START, \"generate\")\n  .addConditionalEdges(\"generate\", shouldContinue)\n  .addEdge(\"reflect\", \"generate\");\n\nconst graph = builder.compile();\n```", "```py\n{\n    'messages': [\n        HumanMessage(content='Your essay on the topicality of \"The Little Prince\" \n            and its message in modern life is well-written and insightful. You \n            have effectively highlighted the enduring relevance of the book\\'s \n            themes and its importance in today\\'s society. However, there are a \n            few areas where you could enhance your essay:\\n\\n1\\. **Depth**: \n            While you touch upon the themes of cherishing simple joys, \n            nurturing connections, and understanding human relationships, \n            consider delving deeper into each of these themes. Provide specific \n            examples from the book to support your points and explore how these \n            themes manifest in contemporary life.\\n\\n2\\. **Analysis**: Consider \n            analyzing how the book\\'s messages can be applied to current \n            societal issues or personal experiences. For instance, you could \n            discuss how the Little Prince\\'s perspective on materialism relates \n            to consumer culture or explore how his approach to relationships \n            can inform interpersonal dynamics in the digital age.\\n\\n3\\. \n            **Length**: Expand on your ideas by adding more examples, \n            discussing counterarguments, or exploring the cultural impact of \n            \"The Little Prince\" in different parts of the world. This will \n            enrich the depth of your analysis and provide a more comprehensive \n            understanding of the book\\'s relevance.\\n\\n4\\. **Style**: Your essay \n            is clear and well-structured. To enhance the engagement of your \n            readers, consider incorporating quotes from the book to illustrate \n            key points or including anecdotes to personalize your analysis.\n            \\n\\n5\\. **Conclusion**: Conclude your essay by summarizing the \n            enduring significance of \"The Little Prince\" and how its messages \n            can inspire positive change in modern society. Reflect on the \n            broader implications of the book\\'s themes and leave the reader \n            with a lasting impression.\\n\\nBy expanding on your analysis, \n            incorporating more examples, and deepening your exploration of the \n            book\\'s messages, you can create a more comprehensive and \n            compelling essay on the topicality of \"The Little Prince\" in modern \n            life. Well done on your thoughtful analysis, and keep up the good \n            work!', id='70c22b1d-ec96-4dc3-9fd0-d2c6463f9e2c'),\n    ],\n}\n```", "```py\n{\n    'messages': [\n        AIMessage(content='\"The Little Prince\" by Antoine de Saint-ExupÃ©ry \n            stands as a timeless masterpiece that continues to offer profound \n            insights into human relationships and values, resonating with \n            readers across generations. The narrative of the Little Prince\\'s \n            travels and encounters with a myriad of characters serves as a rich \n            tapestry of allegorical representations, ....', response_metadata=\n            {'token_usage': {'completion_tokens': 420, 'prompt_tokens': 2501, \n            'total_tokens': 2921}, 'model_name': 'gpt-3.5-turbo', \n            'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': \n            None}, id='run-2e8f9f13-f625-4820-9c8b-b64e1c23daa2-0', \n            usage_metadata={'input_tokens': 2501, 'output_tokens': 420, \n            'total_tokens': 2921}),\n    ],\n}\n```", "```py\nfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str # this key is shared with the subgraph\n\nclass SubgraphState(TypedDict):\n    foo: str # this key is shared with the parent graph\n    bar: str\n\n# Define subgraph\ndef subgraph_node(state: SubgraphState):\n    # note that this subgraph node can communicate with the parent graph \n    # via the shared \"foo\" key\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node)\n...\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"subgraph\", subgraph)\n...\ngraph = builder.compile()\n```", "```py\nimport { StateGraph, Annotation, START } from \"@langchain/langgraph\";\n\nconst StateAnnotation = Annotation.Root({\n  foo: Annotation(),\n});\n\nconst SubgraphStateAnnotation = Annotation.Root({\n  // note that this key is shared with the parent graph state\n  foo: Annotation(), \n  bar: Annotation(),\n});\n\n// Define subgraph\nconst subgraphNode = async (state) => {\n  // note that this subgraph node can communicate with\n  // the parent graph via the shared \"foo\" key\n  return { foo: state.foo + \"bar\" };\n};\n\nconst subgraph = new StateGraph(SubgraphStateAnnotation)\n  .addNode(\"subgraph\", subgraphNode);\n  ...\n  .compile();\n\n// Define parent graph\nconst parentGraph = new StateGraph(StateAnnotation)\n  .addNode(\"subgraph\", subgraph)\n  .addEdge(START, \"subgraph\")\n  // Additional parent graph setup would go here\n  .compile();\n```", "```py\nclass State(TypedDict):\n    foo: str\n\nclass SubgraphState(TypedDict):\n    # none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\n# Define subgraph\ndef subgraph_node(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + \"baz\"}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node)\n...\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\ndef node(state: State):\n    # transform the state to the subgraph state\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n    # transform response back to the parent state\n    return {\"foo\": response[\"bar\"]}\n\nbuilder = StateGraph(State)\n# note that we are using `node` function instead of a compiled subgraph\nbuilder.add_node(node)\n...\ngraph = builder.compile()\n```", "```py\nimport { StateGraph, START, Annotation } from \"@langchain/langgraph\";\n\nconst StateAnnotation = Annotation.Root({\n  foo: Annotation(),\n});\n\nconst SubgraphStateAnnotation = Annotation.Root({\n  // note that none of these keys are shared with the parent graph state\n  bar: Annotation(),\n  baz: Annotation(),\n});\n\n// Define subgraph\nconst subgraphNode = async (state) => {\n  return { bar: state.bar + \"baz\" };\n};\n\nconst subgraph = new StateGraph(SubgraphStateAnnotation)\n  .addNode(\"subgraph\", subgraphNode);\n  ...\n  .compile();\n\n// Define parent graph\nconst subgraphWrapperNode = async (state) => {\n  // transform the state to the subgraph state\n  const response = await subgraph.invoke({\n    bar: state.foo,\n  });\n  // transform response back to the parent state\n  return {\n    foo: response.bar,\n  };\n}\n\nconst parentGraph = new StateGraph(StateAnnotation)\n  .addNode(\"subgraph\", subgraphWrapperNode)\n  .addEdge(START, \"subgraph\")\n  // Additional parent graph setup would go here\n  .compile();\n```", "```py\nfrom typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\n\nclass SupervisorDecision(BaseModel):\n    next: Literal[\"researcher\", \"coder\", \"FINISH\"]\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\nmodel = model.with_structured_output(SupervisorDecision)\n\nagents = [\"researcher\", \"coder\"]\n\nsystem_prompt_part_1 = f\"\"\"You are a supervisor tasked with managing a \nconversation between the following workers: {agents}. Given the following user \nrequest, respond with the worker to act next. Each worker will perform a\ntask and respond with their results and status. When finished,\nrespond with FINISH.\"\"\"\n\nsystem_prompt_part_2 = f\"\"\"Given the conversation above, who should act next? Or \n should we FINISH? Select one of: {', '.join(agents)}, FINISH\"\"\"\n\ndef supervisor(state):\n    messages = [\n        (\"system\", system_prompt_part_1),\n        *state[\"messages\"],\n        (\"system\", \tsystem_prompt_part_2)\n    ]\n    return model.invoke(messages)\n```", "```py\nimport { ChatOpenAI } from 'langchain-openai';\nimport { z } from 'zod';\n\nconst SupervisorDecision = z.object({\n  next: z.enum(['researcher', 'coder', 'FINISH']),\n});\n\nconst model = new ChatOpenAI({ model: 'gpt-4o', temperature: 0 });\nconst modelWithStructuredOutput = model.withStructuredOutput(SupervisorDecision);\n\nconst agents = ['researcher', 'coder'];\n\nconst systemPromptPart1 = `You are a supervisor tasked with managing a \n conversation between the following workers: ${agents.join(', ')}. Given the \n following user request, respond with the worker to act next. Each worker \n will perform a task and respond with their results and status. When \n finished, respond with FINISH.`;\n\nconst systemPromptPart2 = `Given the conversation above, who should act next? Or \n should we FINISH? Select one of: ${agents.join(', ')}, FINISH`;\n\nconst supervisor = async (state) => {\n  const messages = [\n    { role: 'system', content: systemPromptPart1 },\n    ...state.messages,\n    { role: 'system', content: systemPromptPart2 }\n  ];\n\n  return await modelWithStructuredOutput.invoke({ messages });\n};\n```", "```py\nfrom typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START\n\nmodel = ChatOpenAI()\n\nclass AgentState(BaseModel):\n    next: Literal[\"researcher\", \"coder\", \"FINISH\"]\n\ndef researcher(state: AgentState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\ndef coder(state: AgentState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(AgentState)\nbuilder.add_node(supervisor)\nbuilder.add_node(researcher)\nbuilder.add_node(coder)\n\nbuilder.add_edge(START, \"supervisor\")\n# route to one of the agents or exit based on the supervisor's decision\nbuilder.add_conditional_edges(\"supervisor\", lambda state: state[\"next\"])\nbuilder.add_edge(\"researcher\", \"supervisor\")\nbuilder.add_edge(\"coder\", \"supervisor\")\n\nsupervisor = builder.compile()\n```", "```py\nimport {\n  StateGraph,\n  Annotation,\n  MessagesAnnotation,\n  START,\n  END,\n} from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n});\n\nconst StateAnnotation = Annotation.Root({\n  ...MessagesAnnotation.spec,\n  next: Annotation(),\n});\n\nconst researcher = async (state) => {\n  const response = await model.invoke(...);\n  return { messages: [response] };\n};\n\nconst coder = async (state) => {\n  const response = await model.invoke(...);\n  return { messages: [response] };\n};\n\nconst graph = new StateGraph(StateAnnotation)\n  .addNode(\"supervisor\", supervisor)\n  .addNode(\"researcher\", researcher)\n  .addNode(\"coder\", coder)\n  .addEdge(START, \"supervisor\")\n  // route to one of the agents or exit based on the supervisor's decision\n  .addConditionalEdges(\"supervisor\", async (state) => \n    state.next === 'FINISH' ? END : state.next)\n  .addEdge(\"researcher\", \"supervisor\")\n  .addEdge(\"coder\", \"supervisor\")\n  .compile();\n```"]