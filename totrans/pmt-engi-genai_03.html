<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Standard Practices for Text Generation with ChatGPT"><div class="chapter" id="standard_practices_03">
<h1><span class="label">Chapter 3. </span>Standard Practices for <span class="keep-together">Text Generation with ChatGPT</span></h1>


<p>Simple prompting techniques will help you to maximize the output and formats from LLMs. You’ll start by tailoring the prompts to explore all of the common practices used for text generation.</p>






<section data-type="sect1" data-pdf-bookmark="Generating Lists"><div class="sect1" id="id22">
<h1>Generating Lists</h1>

<p>Automatically generating lists is <a data-type="indexterm" data-primary="generating lists" id="gntglst"/><a data-type="indexterm" data-primary="lists, generating" id="lstgnr"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="list generation" id="gpgnr"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="list generation" id="cppgn"/>incredibly powerful and enables you to focus on higher-level tasks while GPT can automatically generate, refine, rerank, and de-duplicate lists on your behalf.</p>

<p>Input:</p>

<pre data-type="programlisting">Generate a list of Disney characters.</pre>

<p>Output:</p>

<pre data-type="programlisting">Sure, here is a list of some popular Disney characters:

1. Mickey Mouse
2. Minnie Mouse
...
30. Bagheera (The Jungle Book)</pre>

<p class="pagebreak-before">GPT-4 is perfectly capable of providing a list of characters. However, there are some pitfalls with this approach:</p>

<ul>
<li>
<p>GPT has decided to provide 30 examples as a numbered list, separated by <code>\n</code> characters. However, if your downstream Python code was expecting to split on bullet points, then you’ll likely end up with undesirable results or a runtime error.</p>
</li>
<li>
<p>GPT has provided preceding commentary; removing any preceding/succeeding commentary would make parsing the output easier.</p>
</li>
<li>
<p>The list size wasn’t controlled and was left to the language model.</p>
</li>
<li>
<p>Some of the characters have the name of their corresponding film within <span class="keep-together">brackets—for</span> example, <em>Bagheera (The Jungle Book)</em>—and others don’t. This makes names harder to extract because you would need to remove the movie titles.</p>
</li>
<li>
<p>No filtering or selection has been applied to the LLM generation based on our desired result.</p>
</li>
</ul>

<p>Following you’ll find an optimized prompt.</p>

<p>Input:</p>

<pre data-type="programlisting">Generate a bullet-point list of 5 male Disney characters.
Only include the name of the character for each line.
Never include the film for each Disney character.
Only return the Disney characters, never include any commentary.

Below is an example list:

* Aladdin
* Simba
* Beast
* Hercules
* Tarzan</pre>

<p>Output:</p>

<pre data-type="programlisting">* Woody
* Buzz Lightyear
* Stitch
* Jack Sparrow
* Prince Charming</pre>
<div data-type="tip"><h1>Provide Examples</h1>
<p>Simply rephrasing your prompt <a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="list generation" id="id544"/> to include examples <em>(few-shot prompting)</em> can greatly impact the desired output.</p>
</div>

<p class="pagebreak-before">By optimizing the prompt, you’ve achieved the following:</p>

<ul>
<li>
<p>Restricted the list to a fixed size of five</p>
</li>
<li>
<p>Generated only male characters</p>
</li>
<li>
<p>Correctly formatted the list with bullet points</p>
</li>
<li>
<p>Removed any preceding commentary</p>
</li>
</ul>

<p>Simple lists are fine for most tasks; however, they are less structured, and for some tasks it’s beneficial to obtain nested data <a data-type="indexterm" data-primary="generating lists" data-startref="gntglst" id="id545"/><a data-type="indexterm" data-primary="lists, generating" data-startref="lstgnr" id="id546"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="list generation" data-startref="gpgnr" id="id547"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="list generation" data-startref="cppgn" id="id548"/>structures from a GPT-4 output.</p>

<p>Three typical data structures include:</p>

<ul>
<li>
<p>Nested text data (hierarchical lists)</p>
</li>
<li>
<p>JSON</p>
</li>
<li>
<p>YAML</p>
</li>
</ul>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Hierarchical List Generation"><div class="sect1" id="id23">
<h1>Hierarchical List Generation</h1>

<p>Hierarchical lists are useful for when <a data-type="indexterm" data-primary="generating lists" data-secondary="hierarchical lists" id="gntlsg"/><a data-type="indexterm" data-primary="lists, generating" data-secondary="hierarchical" id="lsgrghrr"/><a data-type="indexterm" data-primary="hierarchical lists" id="hrrcsts"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="list generation" data-tertiary="hierarchical lists" id="gpgrhch"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="list generation" data-tertiary="hierarchical lists" id="cppghch"/>your desired output is nested. A good example of this would be a detailed article structure.</p>

<p>Input:</p>

<pre data-type="programlisting">Generate a hierarchical and incredibly detailed article outline on:

What are the benefits of data engineering.

See an example of the hierarchical structure below:

Article Title: What are the benefits of digital marketing?

* Introduction
    a. Explanation of digital marketing
    b. Importance of digital marketing in today's business world
* Increased Brand Awareness
    a. Definition of brand awareness
    b. How digital marketing helps in increasing brand awareness</pre>

<p>Output:</p>

<pre data-type="programlisting">Article Title: What are the benefits of data engineering?

* Introduction
    a. Explanation of data engineering
    b. Importance of data engineering in today’s data-driven world

...(10 sections later)...


* Conclusion
    a. Importance of data engineering in the modern business world
    b. Future of data engineering and its impact on the data ecosystem</pre>

<p>To generate an effective article outline in the preceding output, you’ve included two key phrases:</p>
<dl>
<dt>Hierarchical</dt>
<dd>
<p>To suggest that the article <a data-type="indexterm" data-primary="phrases" data-secondary="hierarchical lists" id="id549"/><a data-type="indexterm" data-primary="phrases" data-secondary="incredibly detailed" id="id550"/><a data-type="indexterm" data-primary="number of items" id="id551"/><a data-type="indexterm" data-primary="fixed number of items" id="id552"/>outline needs to produce a nested structure.</p>
</dd>
<dt>Incredibly detailed</dt>
<dd>
<p>To guide the language model towards producing a larger output. Other words that you could include that have the same effect would be <em>very long</em> or by specifying a large number of subheadings, <em>include at least 10 top-level headings</em>.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Asking a language model for a fixed number of items doesn’t guarantee the language model will produce the same length. For example, if you ask for 10 headings, you might receive only 8. Therefore, your code should either validate that 10 headings exist or be flexible to handle varying lengths from the LLM.</p>
</div>

<p>So you’ve successfully produced a hierarchical article outline, but how could you parse the string <a data-type="indexterm" data-primary="parsing strings to structured data" id="id553"/><a data-type="indexterm" data-primary="strings" data-secondary="parsing to structured data" id="id554"/><a data-type="indexterm" data-primary="structured data" data-secondary="parsing strings to" id="id555"/>into structured data?</p>

<p>Let’s explore <a data-type="xref" href="#parsing_a_hierarchical_list_one">Example 3-1</a> using Python, where you’ve previously made a successful API call against OpenAI’s GPT-4. Two regular expressions are used to <a data-type="indexterm" data-primary="regular expressions" data-secondary="re module (Python)" id="id556"/><a data-type="indexterm" data-primary="re module (Python), regular expressions and" id="id557"/><a data-type="indexterm" data-primary="Python" data-secondary="re module, regular expressions and" id="id558"/><a data-type="indexterm" data-primary="expressions, regular" id="id559"/>extract the headings and subheadings from <code>openai_result</code>. The <code>re</code> module in Python is used for working with regular expressions.</p>
<div id="parsing_a_hierarchical_list_one" data-type="example">
<h5><span class="label">Example 3-1. </span><a href="https://oreil.ly/A0otS">Parsing a hierarchical list</a></h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">re</code>

<code class="c1"># openai_result = generate_article_outline(prompt)</code>
<code class="c1"># Commented out to focus on a fake LLM response, see below:</code>

<code class="n">openai_result</code> <code class="o">=</code> <code class="s1">'''</code>
<code class="s1">* Introduction</code>
<code class="s1">    a. Explanation of data engineering</code>
<code class="s1">    b. Importance of data engineering in today’s data-driven world</code>
<code class="s1">* Efficient Data Management</code>
<code class="s1">    a. Definition of data management</code>
<code class="s1">    b. How data engineering helps in efficient data management</code>
<code class="s1">* Conclusion</code>
<code class="s1">    a. Importance of data engineering in the modern business world</code>
<code class="s1">    b. Future of data engineering and its impact on the data ecosystem</code>
<code class="s1">'''</code>

<code class="c1"># Regular expression patterns</code>
<code class="n">heading_pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s1">'\* (.+)'</code>
<code class="n">subheading_pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s1">'\s+[a-z]\. (.+)'</code>

<code class="c1"># Extract headings and subheadings</code>
<code class="n">headings</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">heading_pattern</code><code class="p">,</code> <code class="n">openai_result</code><code class="p">)</code>
<code class="n">subheadings</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">subheading_pattern</code><code class="p">,</code> <code class="n">openai_result</code><code class="p">)</code>

<code class="c1"># Print results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Headings:</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
<code class="k">for</code> <code class="n">heading</code> <code class="ow">in</code> <code class="n">headings</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"* </code><code class="si">{</code><code class="n">heading</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Subheadings:</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
<code class="k">for</code> <code class="n">subheading</code> <code class="ow">in</code> <code class="n">subheadings</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"* </code><code class="si">{</code><code class="n">subheading</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>

<p>This code will output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Headings</code><code class="p">:</code>
<code class="o">-</code> <code class="n">Introduction</code>
<code class="o">-</code> <code class="n">Efficient</code> <code class="n">Data</code> <code class="n">Management</code>
<code class="o">-</code> <code class="n">Conclusion</code>

<code class="n">Subheadings</code><code class="p">:</code>
<code class="o">-</code> <code class="n">Explanation</code> <code class="n">of</code> <code class="n">data</code> <code class="n">engineering</code>
<code class="o">-</code> <code class="n">Importance</code> <code class="n">of</code> <code class="n">data</code> <code class="n">engineering</code> <code class="ow">in</code> <code class="n">today</code><code class="err">’</code><code class="n">s</code> <code class="n">data</code><code class="o">-</code><code class="n">driven</code> <code class="n">world</code>
<code class="o">-</code> <code class="n">Definition</code> <code class="n">of</code> <code class="n">data</code> <code class="n">management</code>
<code class="o">-</code> <code class="n">How</code> <code class="n">data</code> <code class="n">engineering</code> <code class="n">helps</code> <code class="ow">in</code> <code class="n">efficient</code> <code class="n">data</code> <code class="n">management</code>
<code class="o">-</code> <code class="n">Importance</code> <code class="n">of</code> <code class="n">data</code> <code class="n">engineering</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">modern</code> <code class="n">business</code> <code class="n">world</code>
<code class="o">-</code> <code class="n">Future</code> <code class="n">of</code> <code class="n">data</code> <code class="n">engineering</code> <code class="ow">and</code> <code class="n">its</code> <code class="n">impact</code> <code class="n">on</code> <code class="n">the</code> <code class="n">data</code> <code class="n">ecosystem</code></pre></div>

<p>The use of regular expressions allows for efficient <a data-type="indexterm" data-primary="expressions, regular" data-secondary="patterns" id="xpgptt"/>pattern matching, making it possible to handle variations in the input text, such as the presence or absence of leading spaces or tabs. Let’s explore how <a data-type="indexterm" data-primary="regular expressions" data-secondary="patterns" data-tertiary="heading_pattern" id="id560"/><a data-type="indexterm" data-primary="patterns, regular expressions" data-secondary="heading_pattern" id="id561"/>these patterns work:</p>
<ul class="simplelist">
<li><code>
heading_pattern = r'\* (.+)'
</code>
</li>
</ul>

<p>This pattern is designed to extract the main headings and consists of:</p>

<ul>
<li>
<p><code>\*</code> matches the asterisk <code>(*)</code> symbol at the beginning of a heading. The backslash is used to escape the asterisk, as the asterisk has a special meaning in regular expressions (zero or more occurrences of the preceding character).</p>
</li>
<li>
<p>A space character will match after the asterisk.</p>
</li>
<li>
<p><code>(.+)</code>: matches one or more characters, and the parentheses create a capturing group. The <code>.</code> is a wildcard that matches any character except a newline, and the <code>+</code> is a quantifier that means <em>one or more</em> occurrences of the preceding element (the dot, in this case).</p>
</li>
</ul>

<p>By applying this pattern you can easily extract all <a data-type="indexterm" data-primary="regular expressions" data-secondary="patterns" data-tertiary="subheading_pattern" id="id562"/><a data-type="indexterm" data-primary="patterns, regular expressions" data-secondary="subheading_pattern" id="id563"/><a data-type="indexterm" data-primary="subheading_pattern" id="id564"/>of the main headings into a list without the asterisk.</p>
<ul class="simplelist">
<li><code>
subheading_pattern = r'\s+[a-z]\. (.+)
</code>
</li>
</ul>

<p>The <code>subheading pattern</code> will match all of the subheadings within the <code>openai_result</code> string:</p>

<ul>
<li>
<p><code>\s+</code> matches one or more whitespace characters (spaces, tabs, and so on). The <code>+</code> means <em>one or more</em> occurrences of the preceding element (the <code>\s</code>, in this case).</p>
</li>
<li>
<p><code>[a-z]</code> matches a single lowercase letter from <em>a</em> to <em>z</em>.</p>
</li>
<li>
<p><code>\.</code> matches a period character. The backslash is used to escape the period, as it has a special meaning in regular expressions (matches any character except a newline).</p>
</li>
<li>
<p><em>A space character will match after the period.</em></p>
</li>
<li>
<p><code>(.+)</code> matches one or more characters, and the parentheses create a capturing group. The <code>.</code> is a wildcard that matches any character except a newline, and the <code>+</code> is a quantifier that means <em>one or more</em> occurrences of the preceding element (the dot, in this case).</p>
</li>
</ul>

<p>Additionally the <code>re.findall()</code> function is <a data-type="indexterm" data-primary="regular expressions" data-secondary="patterns" data-tertiary="re.findall() function" id="id565"/><a data-type="indexterm" data-primary="patterns, regular expressions" data-secondary="re.findall() function" id="id566"/><a data-type="indexterm" data-primary="re.findall() function" id="id567"/>used to find all non-overlapping matches of the patterns in the input string and return them as a list. The extracted headings and subheadings are then printed.</p>

<p>So now you’re able to extract headings and subheadings from hierarchical article outlines; however, you can further refine the regular expressions so that each heading is associated with corresponding <code>subheadings</code>.</p>

<p>In <a data-type="xref" href="#parsing_a_hierarchical_list_two">Example 3-2</a>, the regex has been slightly modified so that each subheading is attached directly with its appropriate subheading.</p>
<div id="parsing_a_hierarchical_list_two" data-type="example">
<h5><span class="label">Example 3-2. </span><a href="https://oreil.ly/LcMtv">Parsing a hierarchical list into a Python dictionary</a></h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">re</code>

<code class="n">openai_result</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">* Introduction</code>
<code class="s2">  a. Explanation of data engineering</code>
<code class="s2">  b. Importance of data engineering in today’s data-driven world</code>
<code class="s2">* Efficient Data Management</code>
<code class="s2">    a. Definition of data management</code>
<code class="s2">    b. How data engineering helps in efficient data management</code>
<code class="s2">    c. Why data engineering is important for data management</code>
<code class="s2">* Conclusion</code>
<code class="s2">    a. Importance of data engineering in the modern business world</code>
<code class="s2">    b. Future of data engineering and its impact on the data ecosystem</code>
<code class="s2">"""</code>

<code class="n">section_regex</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="sa">r</code><code class="s2">"\* (.+)"</code><code class="p">)</code>
<code class="n">subsection_regex</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="sa">r</code><code class="s2">"\s*([a-z]\..+)"</code><code class="p">)</code>

<code class="n">result_dict</code> <code class="o">=</code> <code class="p">{}</code>
<code class="n">current_section</code> <code class="o">=</code> <code class="kc">None</code>

<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">openai_result</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">):</code>
    <code class="n">section_match</code> <code class="o">=</code> <code class="n">section_regex</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="n">line</code><code class="p">)</code>
    <code class="n">subsection_match</code> <code class="o">=</code> <code class="n">subsection_regex</code><code class="o">.</code><code class="n">match</code><code class="p">(</code><code class="n">line</code><code class="p">)</code>

    <code class="k">if</code> <code class="n">section_match</code><code class="p">:</code>
        <code class="n">current_section</code> <code class="o">=</code> <code class="n">section_match</code><code class="o">.</code><code class="n">group</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">result_dict</code><code class="p">[</code><code class="n">current_section</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">elif</code> <code class="n">subsection_match</code> <code class="ow">and</code> <code class="n">current_section</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">result_dict</code><code class="p">[</code><code class="n">current_section</code><code class="p">]</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">subsection_match</code><code class="o">.</code><code class="n">group</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>

<code class="nb">print</code><code class="p">(</code><code class="n">result_dict</code><code class="p">)</code></pre>

<p>This will output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">{</code>
    <code class="s2">"Introduction"</code><code class="p">:</code> <code class="p">[</code>
        <code class="s2">"a. Explanation of data engineering"</code><code class="p">,</code>
        <code class="s2">"b. Importance of data engineering in today’s data-driven world"</code>
    <code class="p">],</code>
    <code class="s2">"Efficient Data Management"</code><code class="p">:</code> <code class="p">[</code>
        <code class="s2">"a. Definition of data management"</code><code class="p">,</code>
        <code class="s2">"b. How data engineering helps in efficient data management"</code>
    <code class="p">],</code>
    <code class="s2">"Conclusion"</code><code class="p">:</code> <code class="p">[</code>
        <code class="s2">"a. Importance of data engineering in the modern business world"</code><code class="p">,</code>
        <code class="s2">"b. Future of data engineering and its impact on the data ecosystem"</code>
    <code class="p">]</code>
<code class="p">}</code></pre></div>

<p>The section title regex, <code>r'\* (.+)'</code>, matches an <a data-type="indexterm" data-primary="regular expressions" data-secondary="section title" id="id568"/><a data-type="indexterm" data-primary="patterns, regular expressions" data-secondary="section title" id="id569"/><a data-type="indexterm" data-primary="section title regex" id="id570"/>asterisk followed by a space and then one or more characters. The parentheses capture the text following the asterisk and space to be used later in the code.</p>

<p>The subsection regex, <code>r'\s*([a-z]\..+)'</code>, starts <a data-type="indexterm" data-primary="regular expressions" data-secondary="subsection" id="id571"/><a data-type="indexterm" data-primary="patterns, regular expressions" data-secondary="subsection" id="id572"/><a data-type="indexterm" data-primary="subsection regex" id="id573"/>with <code>\s*</code>, which matches zero or more whitespace characters (spaces or tabs). This allows the regex to match subsections with or without leading spaces or tabs. The following part, <code>([a-z]\..+)</code>, matches a lowercase letter followed by a period and then one or more characters. The parentheses capture the entire matched subsection text for later use in the code.</p>

<p>The <code>for</code> loop iterates over each <a data-type="indexterm" data-primary="loops" data-secondary="for loop" id="id574"/>line in the input string, <code>openai_result</code>. Upon encountering a line that matches the section title regex, the loop sets the matched title as the current section and assigns an empty list as its value in the <code>result_dict</code> dictionary. When a line matches the subsection regex, the matched subsection text is appended to the list corresponding to the current section.</p>

<p>Consequently, the loop processes the <em>input string line by line</em>, categorizes lines as section titles or subsections, and <a data-type="indexterm" data-primary="generating lists" data-secondary="hierarchical lists" data-startref="gntlsg" id="id575"/><a data-type="indexterm" data-primary="lists, generating" data-secondary="hierarchical" data-startref="lsgrghrr" id="id576"/><a data-type="indexterm" data-primary="hierarchical lists" data-startref="hrrcsts" id="id577"/><a data-type="indexterm" data-primary="expressions, regular" data-secondary="patterns" data-startref="xpgptt" id="id578"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="list generation" data-tertiary="hierarchical lists" data-startref="gpgrhch" id="id579"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="list generation" data-tertiary="hierarchical lists" data-startref="cppghch" id="id580"/>constructs the intended dictionary structure.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="When to Avoid Using Regular Expressions"><div class="sect1" id="id141">
<h1>When to Avoid Using Regular Expressions</h1>

<p>As you work to extract more structured <a data-type="indexterm" data-primary="regular expressions" data-secondary="avoiding" id="id581"/><a data-type="indexterm" data-primary="expressions, regular" data-secondary="avoiding" id="id582"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="regular expressions" id="id583"/>data from LLM responses, relying solely on regular expressions can make the control flow <em>become increasingly complicated.</em> However, there are other formats that can facilitate the parsing of structured data from LLM responses with ease. Two common formats are <em>.json</em> and <em>.yml</em> files.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Generating JSON"><div class="sect1" id="id142">
<h1>Generating JSON</h1>

<p>Let’s start by experimenting with some prompt <a data-type="indexterm" data-primary="JSON (JavaScript Object Notation)" data-secondary="responses" id="jsvjrp"/>design that will direct an LLM to return a JSON response.</p>

<p id="json_extraction_prompt">Input:</p>

<pre data-type="programlisting">Compose a very detailed article outline on "The benefits of learning code" with a
JSON payload structure that highlights key points.

Only return valid JSON.

Here is an example of the JSON structure:
{
    "Introduction": [
        "a. Explanation of data engineering",
        "b. Importance of data engineering in today’s data-driven world"],
    ...
    "Conclusion": [
        "a. Importance of data engineering in the modern business world",
        "b. Future of data engineering and its impact on the data ecosystem"]
}</pre>

<p>Output:</p>

<pre data-type="programlisting">{
    "Introduction": [
        "a. Overview of coding and programming languages",
        "b. Importance of coding in today's technology-driven world"],
    ...
    "Conclusion": [
        "a. Recap of the benefits of learning code",
        "b. The ongoing importance of coding skills in the modern world"]
}</pre>
<div data-type="tip"><h1>Give Direction and Provide Examples</h1>
<p>Notice that in the preceding prompt, you’ve <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="JSON generation" id="id584"/><a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="JSON generation" id="id585"/>provided direction on the type of task, the format, and an example JSON output.</p>
</div>

<p>Common errors that you’ll encounter when working with JSON involve invalid payloads, or the JSON being wrapped within triple backticks (```) , such as:</p>

<p>Output:</p>

<pre data-type="programlisting">Sure here's the JSON:
```json
{"Name": "John Smith"} # valid payload
{"Name": "John Smith", "some_key":} # invalid payload
```</pre>

<p>Ideally you would like the model to respond like so:</p>

<p>Output:</p>

<pre data-type="programlisting">{"Name": "John Smith"}</pre>

<p>This is important because with the first output, you’d have to split after <code>json</code> and then parse the exact part of the string that contained valid JSON. There are several points that are worth adding to your prompts to improve JSON parsing:</p>

<pre data-type="programlisting">You must follow the following principles:
* Only return valid JSON
* Never include backtick symbols such as: `
* The response will be parsed with json.loads(), therefore it must be valid JSON.</pre>

<p>Now let’s examine how you can parse a <a href="https://oreil.ly/MoJHn">JSON output with Python</a>:</p>

<pre id="json_extraction_code_example" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">json</code>

<code class="c1"># openai_json_result = generate_article_outline(prompt)</code>

<code class="n">openai_json_result</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">{</code>
<code class="s2">    "Introduction": [</code>
<code class="s2">        "a. Overview of coding and programming languages",</code>
<code class="s2">        "b. Importance of coding in today's technology-driven world"],</code>
<code class="s2">    "Conclusion": [</code>
<code class="s2">        "a. Recap of the benefits of learning code",</code>
<code class="s2">        "b. The ongoing importance of coding skills in the modern world"]</code>
<code class="s2">}</code>
<code class="s2">"""</code>
<code class="n">parsed_json_payload</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">openai_json_result</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">parsed_json_payload</code><code class="p">)</code>

<code class="sd">'''{'Introduction': ['a. Overview of coding and programming languages',</code>
<code class="sd">"b. Importance of coding in today's technology-driven world"],</code>
<code class="sd">'Conclusion': ['a. Recap of the benefits of learning code',</code>
<code class="sd">'b. The ongoing importance of coding skills in the modern world']}'''</code></pre>

<p>Well done, you’ve successfully parsed some JSON.</p>

<p>As showcased, structuring data from an LLM response is streamlined when <span class="keep-together">requesting</span> the response in valid JSON format. Compared to the previously demonstrated regular expression parsing, this method is less cumbersome and more <span class="keep-together">straightforward.</span></p>

<p>So what could go wrong?</p>

<ul>
<li>
<p>The language model accidentally adds extra text to the response such as <code>json output:</code> and your application logic only handles for valid JSON.</p>
</li>
<li>
<p>The JSON produced isn’t valid and fails upon parsing (either due to the size or simply for not escaping certain characters).</p>
</li>
</ul>

<p>Later on you will examine strategies to gracefully handle for such edge cases.</p>








<section data-type="sect2" data-pdf-bookmark="YAML"><div class="sect2" id="id24">
<h2>YAML</h2>

<p><em>.yml</em> files are a structured data format that offer different benefits over <em>.json</em>:</p>
<dl>
<dt>No need to escape characters</dt>
<dd>
<p>YAML’s indentation <a data-type="indexterm" data-primary="JSON (JavaScript Object Notation)" data-secondary="responses" data-tertiary="YAML" id="id586"/><a data-type="indexterm" data-primary="YAML" id="id587"/>pattern eliminates the need for braces, brackets, and commas to denote structure. This can lead to cleaner and less error-prone files, as there’s less risk of mismatched or misplaced punctuation.</p>
</dd>
<dt>Readability</dt>
<dd>
<p>YAML is designed to be human-readable, with a simpler syntax and structure compared to JSON. This makes it easier for you to create, read, and edit prompts, especially when dealing with complex or nested structures.</p>
</dd>
<dt>Comments</dt>
<dd>
<p>Unlike JSON, YAML supports comments, allowing you to add annotations or explanations to the prompts directly in the file. This can be extremely helpful when working in a team or when revisiting the prompts after some time, as it allows for better understanding and collaboration.</p>
</dd>
</dl>

<p>Input:</p>

<pre data-type="programlisting">- Below you'll find the current yaml schema.
- You can update the quantities based on a User Query.
- Filter the User Query based on the schema below, if it doesn't match and
there are no items left then return `"No Items"`.
- If there is a partial match, then return only the items that are
within the schema below:

# schema:
- item: Apple Slices
  quantity: 5
  unit: pieces
- item: Milk
  quantity: 1
  unit: gallon
- item: Bread
  quantity: 2
  unit: loaves
- item: Eggs
  quantity: 1
  unit: dozen

User Query: "5 apple slices, and 2 dozen eggs."

Given the schema below, please return only a valid .yml based on the User
Query.If there's no match, return `"No Items"`. Do not provide any
commentary or explanations.</pre>

<p>Output:</p>

<pre data-type="programlisting">- item: Apple Slices
  quantity: 5
  unit: pieces
- item: Eggs
  quantity: 2
  unit: dozen</pre>

<p>Notice with the preceding example how an LLM <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary=".yml format" data-secondary-sortas="yml format" id="id588"/>is able to infer the correct <em>.yml</em> format from the <code>User Query</code> string.</p>

<p>Additionally, you’ve given the LLM an opportunity <a data-type="indexterm" data-primary="JSON (JavaScript Object Notation)" data-secondary="responses" data-startref="jsvjrp" id="id589"/>to either:</p>

<ul>
<li>
<p>Return a valid <em>.yml</em> response</p>
</li>
<li>
<p>Return a filtered <em>.yml</em> response</p>
</li>
</ul>

<p>If after filtering, there are no <em>.yml</em> items left, then return <em>No Items</em>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Filtering YAML Payloads"><div class="sect1" id="id143">
<h1>Filtering YAML Payloads</h1>

<p>You might decide to use this same <a data-type="indexterm" data-primary="YAML" data-secondary="payloads" data-tertiary="filtering" id="ylmpyf"/><a data-type="indexterm" data-primary="payloads, YAML" data-secondary="filtering" id="pylylt"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="YAML payloads" id="cpgpyy"/>prompt for cleaning/filtering a <em>.yml</em> payload.</p>

<p>First, let’s focus on a payload that contains both valid and invalid <code>schema</code> in reference to our desired <code>schema</code>. <code>Apple slices</code> fit the criteria; however, <code>Bananas</code> doesn’t exist, and you should expect for the <code>User Query</code> to be appropriately filtered.</p>

<p class="pagebreak-before">Input:</p>

<pre data-type="programlisting"># User Query:
- item: Apple Slices
  quantity: 5
  unit: pieces
- item: Bananas
  quantity: 3
  unit: pieces</pre>

<p>Output:</p>

<pre data-type="programlisting"># Updated yaml list
- item: Apple Slices
  quantity: 5
  unit: pieces</pre>

<p>In the preceding example, you’ve successfully filtered the user’s payload against a set criteria and have used the language model as a <em>reasoning engine</em>.</p>

<p>By providing the LLM with a set of instructions within the prompt, the response is closely related to what a human might do if they were manually cleaning the data.</p>

<p>The input prompt facilitates the <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="control flow" id="id590"/>delegation of more control flow tasks to a language learning model (LLM), tasks that would typically require coding in a programming language like Python or JavaScript.</p>

<p><a data-type="xref" href="#figure-3-1">Figure 3-1</a> provides a detailed overview of the <a data-type="indexterm" data-primary="YAML" data-secondary="payloads" data-tertiary="filtering" data-startref="ylmpyf" id="id591"/><a data-type="indexterm" data-primary="payloads, YAML" data-secondary="filtering" data-startref="pylylt" id="id592"/>logic applied when processing user queries by an LLM.</p>

<figure><div id="figure-3-1" class="figure">
<img src="assets/pega_0301.png" alt="Using an LLM to determine the control flow of an application instead of directly using code." width="600" height="325"/>
<h6><span class="label">Figure 3-1. </span>Using an LLM to determine the control flow of an application instead of code</h6>
</div></figure>
</div></section>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Handling Invalid Payloads in YAML"><div class="sect1" id="id25">
<h1>Handling Invalid Payloads in YAML</h1>

<p>A completely invalid payload <a data-type="indexterm" data-primary="YAML" data-secondary="payloads" data-tertiary="invalid" id="ympylv"/><a data-type="indexterm" data-primary="payloads, YAML" data-secondary="invalid" id="pyymv"/>might look like this:</p>

<p>Input:</p>

<pre data-type="programlisting"># User Query:
- item: Bananas
  quantity: 3
  unit: pieces</pre>

<p>Output:</p>

<pre data-type="programlisting">No Items</pre>

<p>As expected, the LLM returned <code>No Items</code> as none of the <code>User Query</code> items matched against the previously defined <code>schema</code>.</p>

<p>Let’s create a Python script that gracefully accommodates for the various types of LLM results returned.  The core parts of the script will focus on:</p>

<ul>
<li>
<p>Creating custom exceptions for each type of error that might occur due to the three LLM response scenarios</p>
</li>
<li>
<p>Parsing the proposed schema</p>
</li>
<li>
<p>Running a serious of custom checks against the response so you can be sure that the YML response can be safely passed to downstream software <span class="keep-together">applications/microservices</span></p>
</li>
</ul>

<p>You could define six specific errors that would handle for all of the edge cases:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">InvalidResponse</code><code class="p">(</code><code class="ne">Exception</code><code class="p">):</code>
    <code class="k">pass</code>

<code class="k">class</code> <code class="nc">InvalidItemType</code><code class="p">(</code><code class="ne">Exception</code><code class="p">):</code>
    <code class="k">pass</code>

<code class="k">class</code> <code class="nc">InvalidItemKeys</code><code class="p">(</code><code class="ne">Exception</code><code class="p">):</code>
    <code class="k">pass</code>

<code class="k">class</code> <code class="nc">InvalidItemName</code><code class="p">(</code><code class="ne">Exception</code><code class="p">):</code>
    <code class="k">pass</code>

<code class="k">class</code> <code class="nc">InvalidItemQuantity</code><code class="p">(</code><code class="ne">Exception</code><code class="p">):</code>
    <code class="k">pass</code>

<code class="k">class</code> <code class="nc">InvalidItemUnit</code><code class="p">(</code><code class="ne">Exception</code><code class="p">):</code>
    <code class="k">pass</code></pre>

<p class="pagebreak-before">Then provide the previously proposed <code>YML schema</code> as a string:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Provided schema</code>
<code class="n">schema</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">- item: Apple Slices</code>
<code class="s2">  quantity: 5</code>
<code class="s2">  unit: pieces</code>
<code class="s2">- item: Milk</code>
<code class="s2">  quantity: 1</code>
<code class="s2">  unit: gallon</code>
<code class="s2">- item: Bread</code>
<code class="s2">  quantity: 2</code>
<code class="s2">  unit: loaves</code>
<code class="s2">- item: Eggs</code>
<code class="s2">  quantity: 1</code>
<code class="s2">  unit: dozen</code>
<code class="s2">"""</code></pre>

<p>Import the <code>yaml</code> module and create a custom <a data-type="indexterm" data-primary="yaml module parser function" id="id593"/>parser function called <span class="keep-together"><code>validate_</code></span><code>response</code> that allows you to easily determine whether an LLM output is valid:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">yaml</code>

<code class="k">def</code> <code class="nf">validate_response</code><code class="p">(</code><code class="n">response</code><code class="p">,</code> <code class="n">schema</code><code class="p">):</code>
    <code class="c1"># Parse the schema</code>
    <code class="n">schema_parsed</code> <code class="o">=</code> <code class="n">yaml</code><code class="o">.</code><code class="n">safe_load</code><code class="p">(</code><code class="n">schema</code><code class="p">)</code>
    <code class="n">maximum_quantity</code> <code class="o">=</code> <code class="mi">10</code>

    <code class="c1"># Check if the response is a list</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">response</code><code class="p">,</code> <code class="nb">list</code><code class="p">):</code>
        <code class="k">raise</code> <code class="n">InvalidResponse</code><code class="p">(</code><code class="s2">"Response is not a list"</code><code class="p">)</code>

    <code class="c1"># Check if each item in the list is a dictionary</code>
    <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">response</code><code class="p">:</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">item</code><code class="p">,</code> <code class="nb">dict</code><code class="p">):</code>
            <code class="k">raise</code> <code class="n">InvalidItemType</code><code class="p">(</code><code class="s1">'''Item is not a dictionary'''</code><code class="p">)</code>

        <code class="c1"># Check if each dictionary has the keys "item", "quantity", and "unit"</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="nb">all</code><code class="p">(</code><code class="n">key</code> <code class="ow">in</code> <code class="n">item</code> <code class="k">for</code> <code class="n">key</code> <code class="ow">in</code> <code class="p">(</code><code class="s2">"item"</code><code class="p">,</code> <code class="s2">"quantity"</code><code class="p">,</code> <code class="s2">"unit"</code><code class="p">)):</code>
            <code class="k">raise</code> <code class="n">InvalidItemKeys</code><code class="p">(</code><code class="s2">"Item does not have the correct keys"</code><code class="p">)</code>

        <code class="c1"># Check if the values associated with each key are the correct type</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">item</code><code class="p">[</code><code class="s2">"item"</code><code class="p">],</code> <code class="nb">str</code><code class="p">):</code>
            <code class="k">raise</code> <code class="n">InvalidItemName</code><code class="p">(</code><code class="s2">"Item name is not a string"</code><code class="p">)</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">item</code><code class="p">[</code><code class="s2">"quantity"</code><code class="p">],</code> <code class="nb">int</code><code class="p">):</code>
            <code class="k">raise</code> <code class="n">InvalidItemQuantity</code><code class="p">(</code><code class="s2">"Item quantity is not an integer"</code><code class="p">)</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">item</code><code class="p">[</code><code class="s2">"unit"</code><code class="p">],</code> <code class="nb">str</code><code class="p">):</code>
            <code class="k">raise</code> <code class="n">InvalidItemUnit</code><code class="p">(</code><code class="s2">"Item unit is not a string"</code><code class="p">)</code>

        <code class="c1"># Check if the values associated with each key are the correct value</code>
        <code class="k">if</code> <code class="n">item</code><code class="p">[</code><code class="s2">"item"</code><code class="p">]</code> <code class="ow">not</code> <code class="ow">in</code> <code class="p">[</code><code class="n">x</code><code class="p">[</code><code class="s2">"item"</code><code class="p">]</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">schema_parsed</code><code class="p">]:</code>
            <code class="k">raise</code> <code class="n">InvalidItemName</code><code class="p">(</code><code class="s2">"Item name is not in schema"</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">item</code><code class="p">[</code><code class="s2">"quantity"</code><code class="p">]</code> <code class="o">&gt;</code>  <code class="n">maximum_quantity</code><code class="p">:</code>
            <code class="k">raise</code> <code class="n">InvalidItemQuantity</code><code class="p">(</code><code class="sa">f</code><code class="s1">'''Item quantity is greater than</code>
<code class="s1">            </code><code class="si">{</code><code class="n">maximum_quantity</code><code class="si">}</code><code class="s1">'''</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">item</code><code class="p">[</code><code class="s2">"unit"</code><code class="p">]</code> <code class="ow">not</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"pieces"</code><code class="p">,</code> <code class="s2">"dozen"</code><code class="p">]:</code>
            <code class="k">raise</code> <code class="n">InvalidItemUnit</code><code class="p">(</code><code class="s2">"Item unit is not pieces or dozen"</code><code class="p">)</code></pre>

<p>To test these edge cases, following you’ll find several <a data-type="indexterm" data-primary="edge case testing" id="id594"/>mocked LLM responses:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Fake responses</code>
<code class="n">fake_response_1</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">- item: Apple Slices</code>
<code class="s2">  quantity: 5</code>
<code class="s2">  unit: pieces</code>
<code class="s2">- item: Eggs</code>
<code class="s2">  quantity: 2</code>
<code class="s2">  unit: dozen</code>
<code class="s2">"""</code>

<code class="n">fake_response_2</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2"># Updated yaml list</code>
<code class="s2">- item: Apple Slices</code>
<code class="s2">  quantity: 5</code>
<code class="s2">  unit: pieces</code>
<code class="s2">"""</code>

<code class="n">fake_response_3</code> <code class="o">=</code> <code class="s2">"""Unmatched"""</code></pre>

<p>Finally, now you can:</p>

<ul>
<li>
<p>Use <code>yaml.safe_load(response)</code> to safely parse the <em>.yml</em> schema</p>
</li>
<li>
<p>Call the <code>validate_response</code> function for each LLM response <a data-type="indexterm" data-primary="YAML" data-secondary="payloads" data-tertiary="invalid" data-startref="ympylv" id="id595"/><a data-type="indexterm" data-primary="payloads, YAML" data-secondary="invalid" data-startref="pyymv" id="id596"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="YAML payloads" data-startref="cpgpyy" id="id597"/>to test it against custom <em>.yml</em> validation logic</p>
</li>
</ul>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Parse the fake responses</code>
<code class="n">response_1_parsed</code> <code class="o">=</code> <code class="n">yaml</code><code class="o">.</code><code class="n">safe_load</code><code class="p">(</code><code class="n">fake_response_1</code><code class="p">)</code>
<code class="n">response_2_parsed</code> <code class="o">=</code> <code class="n">yaml</code><code class="o">.</code><code class="n">safe_load</code><code class="p">(</code><code class="n">fake_response_2</code><code class="p">)</code>
<code class="n">response_3_parsed</code> <code class="o">=</code> <code class="n">yaml</code><code class="o">.</code><code class="n">safe_load</code><code class="p">(</code><code class="n">fake_response_3</code><code class="p">)</code>


<code class="c1"># Validate the responses against the schema</code>
<code class="k">try</code><code class="p">:</code>
    <code class="n">validate_response</code><code class="p">(</code><code class="n">response_1_parsed</code><code class="p">,</code> <code class="n">schema</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Response 1 is valid"</code><code class="p">)</code>
<code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Response 1 is invalid:"</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">))</code>

<code class="k">try</code><code class="p">:</code>
    <code class="n">validate_response</code><code class="p">(</code><code class="n">response_2_parsed</code><code class="p">,</code> <code class="n">schema</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Response 2 is valid"</code><code class="p">)</code>
<code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Response 2 is invalid:"</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">))</code>

<code class="k">try</code><code class="p">:</code>
    <code class="n">validate_response</code><code class="p">(</code><code class="n">response_3_parsed</code><code class="p">,</code> <code class="n">schema</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Response 3 is valid"</code><code class="p">)</code>
<code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Response 3 is invalid:"</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">))</code>

<code class="n">Response</code> <code class="mi">1</code> <code class="ow">is</code> <code class="n">valid</code>
<code class="n">Response</code> <code class="mi">2</code> <code class="ow">is</code> <code class="n">valid</code>
<code class="n">Response</code> <code class="mi">3</code> <code class="ow">is</code> <code class="n">invalid</code><code class="p">:</code> <code class="n">Response</code> <code class="ow">is</code> <code class="ow">not</code> <code class="n">a</code> <code class="nb">list</code></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Diverse Format Generation with ChatGPT"><div class="sect1" id="id26">
<h1>Diverse Format Generation with ChatGPT</h1>

<p>ChatGPT is not only capable of generating <em>.json</em> or <em>.yml</em> files but can also support many other data <a data-type="indexterm" data-primary="ChatGPT" data-secondary="formatted output" data-tertiary="mermaid" id="id598"/><a data-type="indexterm" data-primary="mermaid syntax" id="id599"/><a data-type="indexterm" data-primary="formatted output" data-secondary="mermaid" id="id600"/><a data-type="indexterm" data-primary="diagrams, mermaid syntax" id="id601"/><a data-type="indexterm" data-primary="visualizations, mermaid syntax" id="id602"/><a data-type="indexterm" data-primary="flowcharts, mermaid syntax" id="id603"/>formats. For example, it can aid you in creating a mermaid diagram or generating a mock <em>.csv</em> document. Its versatility stretches to even generating code, conversations, and scripts.</p>

<p>Let’s explore in detail how ChatGPT can produce diverse formatted content. Mermaid is a markdown-like scripting language for generating charts, diagrams, and other visualizations.</p>

<p>Input:</p>

<pre data-type="programlisting">Create a flowchart diagram in mermaid syntax showing a simple food ordering
process that starts with "Choose food" and ends with "Pay for meal."
Include steps for "Add food to cart" and "Confirm cart."</pre>

<p>Output:</p>

<pre data-type="programlisting">graph TD
    ChooseFood[Choose Food] --&gt; AddToCart[Add to Cart]
    AddToCart --&gt; ConfirmCart[Confirm Cart]
    ConfirmCart --&gt; PayForMeal[Pay for Meal]</pre>

<p>This output, when input to a mermaid diagram renderer, will create a precise flowchart reflecting the food ordering process (<a data-type="xref" href="#figure-3-2">Figure 3-2</a>).</p>

<figure><div id="figure-3-2" class="figure">
<img src="assets/pega_0302.png" alt="A simple mermaid diagram output from an LLM." width="484" height="680"/>
<h6><span class="label">Figure 3-2. </span>A streamlined flow diagram created using mermaid syntax via an LLM</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Mock CSV Data"><div class="sect2" id="id27">
<h2>Mock CSV Data</h2>

<p>ChatGPT can also generate mock <a data-type="indexterm" data-primary="ChatGPT" data-secondary="formatted output" data-tertiary="mock CSV data" id="id604"/><a data-type="indexterm" data-primary="CSV data" id="id605"/><a data-type="indexterm" data-primary="formatted output" data-secondary="mock CSV data" id="id606"/>CSV data that can be utilized for testing purposes or other uses.</p>

<p>Input:</p>

<pre data-type="programlisting">Generate a sample CSV data for five students with the following fields:
name, age, grade.</pre>

<p>Output:</p>

<pre data-type="programlisting">name,age,grade
John,16,A
Alex,17,B
Emma,16,A
Sophia,16,B
Liam,17,A</pre>

<p>The example generated is a well-formatted CSV file, which can be loaded directly into many tools that read CSV data.</p>

<p>Remember, GPT models rely heavily on the format you specify. For instance, specifying CSV implies the AI should use commas as separators. Be clear about the desired format for optimal results.</p>

<p>You’ve explored different ways of extracting structured data from language models including regular expressions, JSON, YML, and other formats. You can also now use LLMs to direct parts of <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="control flow" id="id607"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="formatted output" data-tertiary="mock CSV data" data-startref="cgomkv" id="id608"/><a data-type="indexterm" data-primary="CSV data" data-startref="cvsdt" id="id609"/><a data-type="indexterm" data-primary="formatted output" data-secondary="mock CSV data" data-startref="fmtptkv" id="id610"/>your application’s control flow.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Explain It like I’m Five"><div class="sect1" id="id28">
<h1>Explain It like I’m Five</h1>

<p>The <em>Explain It like I’m Five</em> prompt focuses on explaining a given section of text <em>as if you were talking to a five-year-old child.</em></p>

<p>This style of prompting is very <a data-type="indexterm" data-primary="Explain It like I'm Five prompt" id="id611"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="Explain It like I'm Five" id="id612"/>effective and is a great way to convert technical documents into simple summaries that anyone can understand.</p>

<p>Let’s apply this prompt to an abstract of <a href="https://oreil.ly/MDOsz">“Tumor Microenvironment: Recent Advances in Various Cancer Treatments”</a>.</p>

<p>Input:</p>

<pre data-type="programlisting">Explain the following text as if I was 5 years old. Explain it like I'm 5.

Text: This is a review regarding different types of cancer treatments.
We aimed at analyzing the tumor microenvironment and the recent trends
for the therapeutic applications and effectiveness for several kinds
of cancers. Traditionally the cancer treatment was based on the
neoplastic cells.

...(shortened for brevity)...</pre>

<p>Output:</p>

<pre data-type="programlisting">Cancer is a sickness that makes your body's cells grow and divide in a way
that is not normal. Doctors have different ways to treat cancer, like
cutting it out, giving you medicine, or using special rays to kill the
cancer cells...</pre>

<p>The ChatGPT output from the preceding <a data-type="indexterm" data-primary="Explain It like I'm Five prompt" data-startref="xplkvf" id="id613"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="Explain It like I'm Five" data-startref="cpgxpv" id="id614"/>example is much more digestible than the abstract.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Universal Translation Through LLMs"><div class="sect1" id="id29">
<h1>Universal Translation Through LLMs</h1>

<p>Additionally, language models can <a data-type="indexterm" data-primary="universal translators" id="uvstrslt"/><a data-type="indexterm" data-primary="translating languages" id="trsltgg"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="universal translation" id="llmuvtrl"/>potentially act as <em>universal translators</em> due to their proficiency in understanding and generating human-like text across multiple languages.</p>

<p>Let’s investigate this with a simple example:</p>
<ol>
<li>
<p>Generate some simple text.</p>
</li>
<li>
<p>Change the simple text and make it difficult to read.</p>
</li>
<li>
<p>Then transform the same text back into an easy to read format in Spanish (instead of English).</p>
</li>
<li>
<p>Revert the simplistic text back into English.</p>
</li>

</ol>

<p class="pagebreak-before">Input:</p>

<pre data-type="programlisting">Create some simple text.</pre>

<p>Output:</p>

<pre data-type="programlisting">The sun gently rises, painting the sky with hues of orange and pink. Birds
chirp merrily as they welcome the new day. The cool breeze whispers through
the trees, carrying the scent of fresh grass and blooming flowers. Life
awakens with a sense of tranquility and promise.

...(shortened for brevity)...</pre>

<p>Input:</p>

<pre data-type="programlisting">Now make the text incredibly complex and hard to read.</pre>

<p>Output:</p>

<pre data-type="programlisting">In the ethereal realm of celestial ascendancy, wherein the resplendent
orb of the sun commences its gradual ascent, an exquisite phenomenon
unfolds,engendering a symphony of chromatic brilliance upon the
expansive canopy of the firmament. Avian denizens, endowed with
mellifluous voices, bespeak their jubilant salutations, punctuating
the aural tapestry with notes of felicity. The zephyr, tenderly
wafting amidst arboreal sentinels, carries upon its gentle currents
the redolence of verdant blades and blossoming flora.
...(shortened for brevity)...</pre>

<p>Input:</p>

<pre data-type="programlisting">Now transform the text into a simple, easy to read format that's in Spanish
, make sure that it is very easy to understand and read. You must ensure
that it is as simple to comprehend as possible.

- Use simple language.
- Make it concise.</pre>

<p>Output:</p>

<pre data-type="programlisting">En un lugar mágico y celestial, cuando el sol empieza a salir, ocurre algo
hermoso en el cielo. Los pájaros cantan alegremente, llenando el aire con
sus dulces melodías. Una brisa suave sopla entre los árboles, llevando
consigo el olor fresco de la naturaleza. Todo cobra vida y promete momentos
felices. Nos invita a disfrutar de las cosas simples y a encontrar la
felicidad en los pequeños detalles. En este escenario encantador,
encontramos paz y aprendemos que hasta los momentos más simples pueden
ser mágicos.</pre>

<p>Input:</p>

<pre data-type="programlisting">Finally translate it back into English.</pre>

<p class="pagebreak-before">Output:</p>

<pre data-type="programlisting">In a magical and celestial place, when the sun begins to rise, something
beautiful happens in the sky. The birds sing cheerfully, filling the air
with their sweet melodies. A gentle breeze blows among the trees, carrying
the fresh scent of nature. Everything comes alive and promises happy
moments. It invites us to enjoy the simple things and find happiness in the
little details. In this enchanting setting, we find peace and learn that
even the simplest moments can be magical.</pre>

<p>As you can see, part of the meaning is lost while doing the translation,
and this is to be expected. However, large parts of the original text’s intent remains, demonstrating that LLMs can act as <em>highly effective translators</em> for languages that have adequate resources on the internet. For languages that do not have a good amount of resources, it will yield bad results.</p>

<p>The same logic applies to coding languages. LLMs are very good at generating code for established programming languages such as Python and JavaScript but perform worse for newer coding languages and packages.</p>

<p>The boundaries between different forms of <a data-type="indexterm" data-primary="fluid boundaries between information" id="id615"/>information are becoming <em>increasingly fluid</em>. The essence of information itself is evolving, allowing for effortless transformations of summaries into stories, poems, or other creative expressions, ultimately enriching our understanding and engagement with the content.</p>

<p><em>Diffusion models</em> are a unique class of <a data-type="indexterm" data-primary="diffusion models" id="id616"/>generative models utilized in machine learning, specifically designed to produce new images that mimic those found in the training set.</p>

<p>Moreover, when you combine language models with <a data-type="indexterm" data-primary="diffusion models" data-secondary="language models and" id="id617"/><a data-type="indexterm" data-primary="language models" data-secondary="diffusion models and" id="id618"/>diffusion models, it enables seamless transitions between text, video, and other modalities. This makes it even simpler for you to convey <a data-type="indexterm" data-primary="universal translators" data-startref="uvstrslt" id="id619"/><a data-type="indexterm" data-primary="translating languages" data-startref="trsltgg" id="id620"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="universal translation" data-startref="llmuvtrl" id="id621"/>complex ideas across various formats, facilitating a more accessible and comprehensive experience.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Ask for Context"><div class="sect1" id="id30">
<h1>Ask for Context</h1>

<p>LLMs are not only capable of generating text but can <a data-type="indexterm" data-primary="reasoning capabilities" id="rsgcpb"/><a data-type="indexterm" data-primary="context, asking for" id="cxttka"/><a data-type="indexterm" data-primary="asking for context" id="asktxct"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="context, asking for" id="lmlxkf"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="context" id="cppttx"/>also act as simple agents with a limited amount of <em>reasoning capability.</em> This allows you to write a prompt asking the language model to either:</p>

<ul>
<li>
<p>Return a valid result to a question or statement</p>
</li>
<li>
<p>Ask for more context to appropriately answer the question</p>
</li>
</ul>

<p>In this section, you’ll learn about the importance of <em>asking for context</em> when working with LLMs such as GPT-4. We will start with an example of a prompt that doesn’t provide enough context, resulting in a less useful response.</p>

<p>Then, we will provide a better prompt that encourages the model to ask for additional context if needed. Finally, we will use the additional context provided to generate a more informed response.</p>

<p>Input:</p>

<pre data-type="programlisting">Should I use MongoDB or PostgreSQL for my project?</pre>

<p>Output:</p>

<pre data-type="programlisting">It's difficult to make a recommendation without knowing more about your
specific software requirements. Both MongoDB and PostgreSQL have their
advantages, but the best choice depends on the specific needs of your
project.</pre>

<p>In the preceding prompt, the model does not have enough context to make a meaningful
recommendation. Instead, you can ask ChatGPT for a list of recommended points
that would help it to make an effective decision.</p>

<p>Input:</p>

<pre data-type="programlisting">Should I use MongoDB or PostgreSQL for my project?

Provide a list of requirements that would help you to make an informed
and high-quality decision about which database solution I should choose?</pre>

<p>Output:</p>

<pre data-type="programlisting">To make an informed decision between MongoDB and PostgreSQL for your
project, consider the following requirements:

1. Data Structure: Evaluate the structure of your data. If your data
is highly unstructured or schema-less, MongoDB's flexible document model
may be more suitable. On the other hand, if your data has a fixed schema
or requires complex relationships, PostgreSQL's relational model might
be a better fit.

...

10. Cost: Assess the cost implications of each database solution. MongoDB
offers both a community edition (free) and a commercial edition with
additional features and support.PostgreSQL is open-source and free to use,
with optional commercial support available from various vendors.</pre>

<p>After prompting ChatGPT about how to make an informed decision, now you’re aware of what to include within your prompt to help the language model with deciding.</p>

<p class="pagebreak-before">Input:</p>

<pre data-type="programlisting">Should I use MongoDB or PostgreSQL for my project?

Context:
- Data Structure: Highly unstructured data with varying document formats
and no fixed schema. Scalability: Project requires the ability to handle
massive amounts of data and distribute it across multiple servers.
- ACID Compliance: Strong data consistency and integrity are crucial for
the project's financial transactions.
- Querying Capabilities: Complex geospatial queries and aggregations on
location-based data.
Community and Ecosystem: Active community with numerous third-party
libraries and tools specifically tailored for the project's industry.
- Data Integrity and Constraints: Enforcing strict foreign key
relationships and complex data validation rules.
- Transactions and Concurrency: Requires handling high levels of
concurrent read and write operations while maintaining data consistency.
- Maturity and Stability: Prioritizing a well-established and extensively
tested database for long-term stability.
- Development and Operational Ease: Agile development approach with a need
for rapid prototyping and iterative schema design.
- Cost: Budget constraints necessitate using an open-source solution with
no additional licensing or support costs.

If you need more context, please specify what would help you to make
a better decision.</pre>

<p>Output:</p>

<pre data-type="programlisting">Based on these considerations, both MongoDB and PostgreSQL have their
strengths, but given the emphasis on strong data consistency, complex
querying, and data integrity constraints, PostgreSQL seems to be a more
suitable choice for your project.</pre>

<p>In this final example, the model uses the additional context provided to give a well-informed recommendation for using PostgreSQL. By asking for context when necessary, LLMs like ChatGPT and GPT-4 can deliver more valuable and accurate responses.</p>

<p><a data-type="xref" href="#figure-3-3">Figure 3-3</a> demonstrates how <em>asking for context</em> changes the decision-making process of LLMs. Upon receiving user input, the model first assesses whether the context given is sufficient. If not, it prompts the user to provide more detailed information, emphasizing the model’s reliance on  context-rich inputs. Once adequate context is acquired, the LLM then generates an informed and relevant response.</p>

<figure><div id="figure-3-3" class="figure">
<img src="assets/pega_0303.png" alt="The decision process of an LLM with asking for context." width="600" height="397"/>
<h6><span class="label">Figure 3-3. </span>The decision process of an LLM while asking for context</h6>
</div></figure>
<div data-type="note" epub:type="note"><h1>Allow the LLM to Ask for More Context by Default</h1>
<p>You can allow the LLM to ask for more context as a default by including this key phrase: <em>If you need more context, please specify what would help you to make a better decision.</em></p>
</div>

<p>In this section, you’ve seen how LLMs can act as agents that use environmental context to make decisions. By iteratively refining the prompt based on the model’s recommendations, we eventually reach a point where the model has <em>enough context to make a well-informed decision.</em></p>

<p>This process highlights the importance of providing sufficient context in your prompts and being prepared to ask for more information when necessary. By doing so, you can leverage the power of LLMs like GPT-4 to make more accurate and valuable recommendations.</p>

<p>In agent-based systems like GPT-4, the ability to ask for more context and provide a finalized answer is crucial for making well-informed decisions.  <a href="https://oreil.ly/l3Ihy">AutoGPT</a>, a multiagent system, has a self-evaluation step that automatically checks whether the task can be completed given the current context within the prompt. This technique uses an actor–critic relationship, where the existing prompt <a data-type="indexterm" data-primary="reasoning capabilities" data-startref="rsgcpb" id="id622"/><a data-type="indexterm" data-primary="context, asking for" data-startref="cxttka" id="id623"/><a data-type="indexterm" data-primary="asking for context" data-startref="asktxct" id="id624"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="context, asking for" data-startref="lmlxkf" id="id625"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="context" data-startref="cppttx" id="id626"/>context is being analyzed to see whether it could be further refined before being executed.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Text Style Unbundling"><div class="sect1" id="id31">
<h1>Text Style Unbundling</h1>

<p><em>Text style unbundling</em> is a powerful technique in <a data-type="indexterm" data-primary="text style unbundling" id="id627"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="text style unbundling" id="id628"/>prompt engineering that allows you to extract and isolate specific textual features from a given document, such as tone, length, vocabulary, and structure.</p>

<p>This allows you to create new content that shares similar characteristics with the original document, ensuring consistency in style and tone across various forms of communication.</p>

<p>This consistency can be crucial for businesses and organizations that need to communicate with a unified voice across different channels and platforms. The benefits of this technique include:</p>
<dl>
<dt>Improved brand consistency</dt>
<dd>
<p>By ensuring that all content follows a similar style, organizations can strengthen their brand identity and maintain a cohesive image.</p>
</dd>
<dt>Streamlined content creation</dt>
<dd>
<p>By providing a clear set of guidelines, writers and content creators can more easily produce materials that align with a desired style.</p>
</dd>
<dt>Adaptability</dt>
<dd>
<p>Text style unbundling allows for the easy adaptation of existing content to new formats or styles while preserving the core message and tone.</p>
</dd>
</dl>

<p>The process of text style unbundling involves <em>identifying the desired textual features</em> or creating a meta prompt (a prompt to create prompts) to extract these features and then using the extracted features to guide the generation of new content.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Identifying the Desired Textual Features"><div class="sect1" id="id144">
<h1>Identifying the Desired Textual Features</h1>

<p>To successfully unbundle a text style, you <a data-type="indexterm" data-primary="text style unbundling" data-secondary="textual features" data-tertiary="identifying" id="txybdxy"/><a data-type="indexterm" data-primary="textual features" data-secondary="identifying" id="txftfy"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="textual features" id="id629"/>must first identify the specific features you want to extract from the input document. Common textual features to consider include:</p>
<dl>
<dt>Tone of voice</dt>
<dd>
<p>The overall mood or attitude conveyed by the text, such as formal, casual, humorous, or authoritative</p>
</dd>
<dt>Length</dt>
<dd>
<p>The desired word count or general length of the content</p>
</dd>
<dt>Vocabulary and phrasing</dt>
<dd>
<p>The choice of words and expressions, including industry-specific jargon, colloquialisms, and complexity</p>
</dd>
</dl>

<p>Additionally, you can even create a meta prompt <a data-type="indexterm" data-primary="text style unbundling" data-secondary="textual features" data-tertiary="identifying" data-startref="txybdxy" id="id630"/><a data-type="indexterm" data-primary="textual features" data-secondary="identifying" data-startref="txftfy" id="id631"/>that will dynamically show you these.</p>

<p>Input:</p>

<pre data-type="programlisting">Analyze the following text and identify core features that will allow
further documents to imitate the same style, consistency, and formatting
as the document below.

Provide all of the features as a style writing guide that will be used
for future documents.

Analyze the following text:

Artificial intelligence is rapidly transforming the world around us,
opening up new possibilities and revolutionizing industries. From
health care to entertainment, AI is making our lives better and more
convenient in countless ways. However, with great power comes great
responsibility. We must ensure that AI technologies are developed and
deployed ethically and responsibly to protect our privacy and uphold our
values.</pre>

<p>Output:</p>

<pre data-type="programlisting">Style Writing Guide:

Tone of Voice
- Maintain an informative and optimistic tone throughout the document.
- Emphasize the positive impact of the subject matter while acknowledging
any potential concerns or responsibilities.

Length:
...

Vocabulary:
...

Structure:
...

Content:
- Focus on the transformative nature and positive impacts of the subject
matter.
- Highlight specific industries or areas where the subject matter has
made a difference.
- Acknowledge any associated responsibilities or potential risks,
emphasizing the need for ethical and responsible practices.
Stress the importance of privacy protection and upholding values in
relation to the subject matter.</pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Generating New Content with the Extracted Features"><div class="sect1" id="id191">
<h1>Generating New Content with the Extracted Features</h1>

<p>After you’ve extracted the desired <a data-type="indexterm" data-primary="text style unbundling" data-secondary="textual features" data-tertiary="generating new content" id="id632"/><a data-type="indexterm" data-primary="textual features" data-secondary="extracting" data-tertiary="generating new content" id="id633"/>features from the input document, you can use this information to guide future content generation. Remember to ensure that the AI model incorporates the extracted features into its output by specifying the desired style in your prompt. For example:</p>
<ul class="simplelist">
<li><code>Write a new blog post on [topic] using the same tone of
voice, length, vocabulary, and structure as the previously analyzed
text.</code></li>
</ul>

<p>By combining this technique with <em>reference text</em> (documents that act as grounding truth), you can produce credible, branded content that requires minimal revisions.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Extracting Specific Textual Features with LLMs"><div class="sect1" id="id145">
<h1>Extracting Specific Textual Features with LLMs</h1>

<p>You can easily tailor a prompt to guide an LLM in extracting particular textual features from a document. This can <a data-type="indexterm" data-primary="textual features" data-secondary="extracting" data-tertiary="specific" id="id634"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="textual features, extracting" id="id635"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="textual features" data-tertiary="extracting" id="id636"/>be applied beyond just analyzing text for copywriting purposes. For instance, recognizing entities or discerning sentiment from the text can be achieved by crafting a precise instruction for the LLM.</p>

<p>Input:</p>

<pre data-type="programlisting">Analyze the following text to identify and list the entities mentioned:

Artificial intelligence is rapidly transforming the world around us,
opening up new possibilities and revolutionizing industries. From
health care to entertainment, AI is making our lives better and more
convenient in countless ways. However, with great power comes great
responsibility. We must ensure that AI technologies are developed
and deployed ethically and responsibly to protect our privacy and
uphold our values.</pre>

<p>Output:</p>

<pre data-type="programlisting">Entities:
- Artificial Intelligence (AI)
- Health care
- Entertainment
- Privacy
- Values</pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summarization"><div class="sect1" id="id32">
<h1>Summarization</h1>

<p>In the era of information overload, the ability to condense large amounts of text into concise, digestible summaries <a data-type="indexterm" data-primary="summarization" id="smmzr"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="summarization" id="cgpmmz"/>has become an essential skill. As the volume of data we produce and consume continues to grow, the demand for effective summarization techniques has increased significantly.</p>

<p>AI has made significant advancements in this domain, providing powerful tools for generating summaries that retain the most critical information while discarding less relevant details.</p>

<p>Let’s explore how to summarize using a language model.</p>

<p>Input:</p>

<pre data-type="programlisting">Summarize the following text:

Whereas the party of the first part, hereinafter referred to as the
"Purchaser," and the party of the second part, hereinafter referred to
as the "Vendor," being fully cognizant of their respective legal
obligations and responsibilities pursuant to the applicable statutory
provisions and common law doctrines, have duly entered into a legally
binding agreement, hereinafter referred to as the "Agreement," for the
purpose of effectuating a transaction involving the transfer of goods,
hereinafter referred to as the "Goods," between the Purchaser and the
Vendor, subject to the terms and conditions set forth herein, the Parties
do hereby solemnly acknowledge and affirm the following:
...</pre>

<p>Output:</p>

<pre data-type="programlisting">This is a legally binding agreement between the Purchaser and the Vendor
for the transfer of goods. The agreement incorporates the recitals as an
integral part, but in case of any conflict, the operative provisions of the
agreement prevail. The interpretation clause states that singular words
include the plural and vice versa, and words referring to a gender include
all genders. References to statutory provisions include any amendments or
replacements.</pre>

<p>Summarization is an invaluable application of AI, enabling users to quickly extract key insights from lengthy articles, reports, or research papers. This process can help individuals make informed decisions, save time, and prioritize their reading. AI-generated summaries can also facilitate information sharing among teams, allowing <a data-type="indexterm" data-primary="summarization" data-startref="smmzr" id="id637"/>for more efficient collaboration and communication.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summarizing Given Context Window Limitations"><div class="sect1" id="id146">
<h1>Summarizing Given Context Window Limitations</h1>

<p>For documents larger than an LLM can handle <a data-type="indexterm" data-primary="summarization" data-secondary="context window limitations" id="mztxww"/><a data-type="indexterm" data-primary="context window" data-secondary="summarization and" id="cxwwmz"/>in a single API request, a common approach is to chunk the document, summarize each chunk, and then combine these summaries into a final summary, as shown in <a data-type="xref" href="#figure-3-4">Figure 3-4</a>.</p>

<figure><div id="figure-3-4" class="figure">
<img src="assets/pega_0304.png" alt=".A summarization pipeline that uses text splitting and multiple summarization steps." width="600" height="775"/>
<h6><span class="label">Figure 3-4. </span>A summarization pipeline that uses text splitting and multiple summarization steps</h6>
</div></figure>

<p>Additionally, people may require different types of summaries for various reasons, and this is where AI summarization comes in handy. As illustrated in the preceding diagram, a large PDF document could easily be processed using AI summarization to generate distinct summaries tailored to individual needs:</p>
<dl>
<dt>Summary A</dt>
<dd>
<p>Provides key insights, which is perfect for users seeking a quick understanding of the document’s content, enabling them to focus on the most crucial points</p>
</dd>
<dt>Summary B</dt>
<dd>
<p>On the other hand, offers decision-making information, allowing users to make informed decisions based on the content’s implications and recommendations</p>
</dd>
<dt>Summary C</dt>
<dd>
<p>Caters to collaboration and communication, ensuring that users can efficiently share the document’s information and work together seamlessly</p>
</dd>
</dl>

<p class="pagebreak-before">By customizing the summaries <a data-type="indexterm" data-primary="summarization" data-secondary="customizing" id="id638"/>for different users, AI summarization contributes to increased information retrieval for all users, making the entire process more efficient and targeted.</p>

<p>Let’s assume you’re only interested in finding and summarizing information about the advantages of digital marketing. Simply change your summarization prompt to <code>Provide a concise, abstractive summary of the above text. Only summarize the advantages: ...</code></p>

<p>AI-powered summarization has emerged as an essential tool for quickly distilling vast amounts of information into concise, digestible summaries that cater to various user needs. By leveraging advanced language models like GPT-4, AI summarization techniques can efficiently extract key insights and decision-making information, and also facilitate collaboration and communication.</p>

<p>As the volume of data continues to grow, the demand for effective and targeted summarization will only increase, making AI a crucial asset for individuals and organizations alike in <a data-type="indexterm" data-primary="summarization" data-secondary="context window limitations" data-startref="mztxww" id="id639"/><a data-type="indexterm" data-primary="context window" data-secondary="summarization and" data-startref="cxwwmz" id="id640"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="summarization" data-startref="cgpmmz" id="id641"/>navigating the Information Age.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Chunking Text"><div class="sect1" id="id33">
<h1>Chunking Text</h1>

<p>LLMs continue to develop and play an <a data-type="indexterm" data-primary="chunking text" id="id642"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="chunking text" id="mlmchk"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="chunking text" id="cgxnk"/>increasingly crucial role in various applications, as the ability to process and manage large volumes of text becomes ever more important. An essential technique for handling large-scale text is known as <em>chunking.</em></p>

<p><em>Chunking</em> refers to the process of breaking down large pieces of text into smaller, more manageable units or chunks. These chunks can be based on various criteria, such as sentence, paragraph, topic, complexity, or length. By dividing text into smaller segments, AI models can more efficiently process, analyze, and generate responses.</p>

<p><a data-type="xref" href="#figure-3-5">Figure 3-5</a> illustrates the process of chunking a large piece of text and subsequently extracting topics from the individual chunks.</p>

<figure><div id="figure-3-5" class="figure">
<img src="assets/pega_0305.png" alt="Topic extraction process after chunking text." width="600" height="484"/>
<h6><span class="label">Figure 3-5. </span>Topic extraction with an LLM after chunking text</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Benefits of Chunking Text"><div class="sect2" id="id147">
<h2>Benefits of Chunking Text</h2>

<p>There are several advantages to chunking text, which include:</p>
<dl>
<dt>Fitting within a given context length</dt>
<dd>
<p>LLMs only have a certain amount of input and output tokens, which is called a <em>context length</em>. By reducing the input tokens you can make sure <a data-type="indexterm" data-primary="chunking text" data-secondary="advantages" id="id643"/>the output won’t be cut off and the initial request won’t be rejected.</p>
</dd>
<dt>Reducing cost</dt>
<dd>
<p>Chunking helps you to only retrieve the most important points from documents, which reduces your token usage and API costs.</p>
</dd>
<dt>Improved performance</dt>
<dd>
<p>Chunking reduces the processing load on LLMs, allowing for faster response times and more efficient resource utilization.</p>
</dd>
<dt>Increased flexibility</dt>
<dd>
<p>Chunking allows developers to tailor AI responses based on the specific needs of a given task or application.</p>
</dd>
</dl>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Scenarios for Chunking Text"><div class="sect2" id="id349">
<h2>Scenarios for Chunking Text</h2>

<p>Chunking text can be particularly beneficial in certain scenarios, while in others it may not be required. Understanding when to apply this technique can help in optimizing the performance and cost efficiency of LLMs.</p>










<section data-type="sect3" data-pdf-bookmark="When to chunk"><div class="sect3" id="id148">
<h3>When to chunk</h3>
<dl>
<dt>Large documents</dt>
<dd>
<p>When dealing <a data-type="indexterm" data-primary="chunking text" data-secondary="when to chunk" id="id644"/>with extensive documents that exceed the maximum token limit of the LLM</p>
</dd>
<dt>Complex analysis</dt>
<dd>
<p>In scenarios where a detailed analysis is required and the document needs to be broken down for better comprehension and processing</p>
</dd>
<dt>Multitopic documents</dt>
<dd>
<p>When a document covers multiple topics and it’s beneficial to handle them individually</p>
</dd>
</dl>
</div></section>










<section data-type="sect3" data-pdf-bookmark="When not to chunk"><div class="sect3" id="id149">
<h3>When not to chunk</h3>
<dl>
<dt>Short documents</dt>
<dd>
<p>When the document is <a data-type="indexterm" data-primary="chunking text" data-secondary="when not to chunk" id="id645"/>short and well within the token limits of the LLM</p>
</dd>
<dt>Simple analysis</dt>
<dd>
<p>In cases where the analysis or processing required is straightforward and doesn’t benefit from chunking</p>
</dd>
<dt>Single-topic documents</dt>
<dd>
<p>When a document is focused on a single topic and chunking doesn’t add value to the processing</p>
</dd>
</dl>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Poor Chunking Example"><div class="sect2" id="id150">
<h2>Poor Chunking Example</h2>

<p>When text is not chunked correctly, it can lead to <a data-type="indexterm" data-primary="chunking text" data-secondary="example of incorrect" id="ckxxrr"/>reduced LLM performance. Consider the following paragraph from a news article:</p>

<pre data-type="programlisting">The local council has decided to increase the budget for education by 10%
this year, a move that has been welcomed by parents and teachers alike. The
additional funds will be used to improve school infrastructure, hire more
teachers, and provide better resources for students. However, some critics
argue that the increase is not enough to address the growing demands of the
education system.</pre>

<p>When the text is fragmented into isolated words, the resulting list lacks the original context:</p>

<pre data-type="programlisting">["The", "local", "council", "has", "decided", "to", "increase", "the",
"budget", ...]</pre>

<p class="pagebreak-before">The main issues with this poor chunking example include:</p>
<dl>
<dt>Loss of context</dt>
<dd>
<p>By splitting the text into individual words, the original meaning and relationships between the words are lost. This makes it difficult for AI models to understand and respond effectively.</p>
</dd>
<dt>Increased processing load</dt>
<dd>
<p>Processing individual words requires more computational resources, making it less efficient than processing larger chunks of text.</p>
</dd>
</dl>

<p>As a result of the poor chunking in this example, an LLM may face several challenges:</p>

<ul>
<li>
<p>Difficulty understanding the main ideas or themes of the text</p>
</li>
<li>
<p>Struggling to generate accurate summaries or translations</p>
</li>
<li>
<p>Inability to effectively perform tasks such as sentiment analysis or text <span class="keep-together"><code>classification</code></span></p>
</li>
</ul>

<p>By understanding the pitfalls of poor chunking, you can apply prompt engineering principles to improve the process and achieve better results with AI language models.</p>

<p>Let’s explore an improved chunking example using the same news article paragraph from the previous section; you’ll now chunk the text by sentence:</p>

<pre data-type="programlisting">["""The local council has decided to increase the budget for education
by 10% this year, a move that has been welcomed by parents and teachers alike.
""",

"""The additional funds will be used to improve school infrastructure,
hire more teachers, and provide better resources for students.""",

""""However, some critics argue that the increase is not enough to
address the growing demands of the education system."""]</pre>
<div data-type="tip"><h1>Divide Labor and Evaluate Quality</h1>
<p>Define the granularity at which the <a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="chunking text" id="id646"/><a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="chunking text" id="id647"/>text should be chunked, such as by sentence, paragraph, or topic. Adjust parameters like the number of tokens or model temperature to optimize the chunking process.</p>
</div>

<p>By chunking the text in this manner, you could <a data-type="indexterm" data-primary="chunking text" data-secondary="example of incorrect" data-startref="ckxxrr" id="id648"/>insert whole sentences into an LLM prompt with the most relevant sentences.</p>
</div></section>
</div></section>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Chunking Strategies"><div class="sect1" id="id151">
<h1>Chunking Strategies</h1>

<p>There are many different chunking strategies, including:</p>
<dl>
<dt>Splitting by sentence</dt>
<dd>
<p>Preserves the <a data-type="indexterm" data-primary="chunking text" data-secondary="strategies for" id="ckgxstg"/>context and structure of the original content, making it easier for LLMs to understand and process the information. Sentence-based chunking is particularly useful for tasks like summarization, translation, and sentiment analysis.</p>
</dd>
<dt>Splitting by paragraph</dt>
<dd>
<p>This approach is especially effective when dealing with longer content, as it allows the LLM to focus on one cohesive unit at a time. Paragraph-based chunking is ideal for applications like document analysis, topic modeling, and information extraction.</p>
</dd>
<dt>Splitting by topic or section</dt>
<dd>
<p>This method can help AI models better identify and understand the main themes and ideas within the content. Topic-based chunking is well suited for tasks like text classification, content recommendations, and clustering.</p>
</dd>
<dt>Splitting by complexity</dt>
<dd>
<p>For certain applications, it might be helpful to split text based on its complexity, such as the reading level or technicality of the content. By grouping similar complexity levels together, LLMs can more effectively process and analyze the text. This approach is useful for tasks like readability analysis, content adaptation, and personalized learning.</p>
</dd>
<dt>Splitting by length</dt>
<dd>
<p>This technique is particularly helpful when working with very long or complex documents, as it allows LLMs to process the content more efficiently. Length-based chunking is suitable for applications like large-scale text analysis, search engine indexing, and text preprocessing.</p>
</dd>
<dt>Splitting by tokens using a tokenizer</dt>
<dd>
<p>Utilizing a tokenizer is a crucial step in many natural language processing tasks, as it enables the process of splitting text into individual tokens. Tokenizers divide text into smaller units, such as words, phrases, or symbols, which can then be analyzed and processed by AI models more effectively. You’ll shortly be using a package called <code>tiktoken</code>, which is a bytes-pair encoding tokenizer (BPE) for chunking.</p>
</dd>
</dl>

<p><a data-type="xref" href="#table-3-1">Table 3-1</a> provides a high-level overview of the different chunking strategies; it’s worth considering what matters to you most when performing chunking.</p>

<p>Are you more interested in preserving semantic context, or would naively splitting by length suffice?</p>
<table id="table-3-1">
<caption><span class="label">Table 3-1. </span>Six chunking strategies highlighting their advantages and disadvantages</caption>
<thead>
<tr>
<th>Splitting strategy</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Splitting by sentence</p></td>
<td><p>Preserves context, suitable for various tasks</p></td>
<td><p>May not be efficient for very long content</p></td>
</tr>
<tr>
<td><p>Splitting by paragraph</p></td>
<td><p>Handles longer content, focuses on cohesive units</p></td>
<td><p>Less granularity, may miss subtle connections</p></td>
</tr>
<tr>
<td><p>Splitting by topic</p></td>
<td><p>Identifies main themes, better for classification</p></td>
<td><p>Requires topic identification, may miss fine details</p></td>
</tr>
<tr>
<td><p>Splitting by complexity</p></td>
<td><p>Groups similar complexity levels, adaptive</p></td>
<td><p>Requires complexity measurement, not suitable for all tasks</p></td>
</tr>
<tr>
<td><p>Splitting by length</p></td>
<td><p>Manages very long content, efficient processing</p></td>
<td><p>Loss of context, may require more preprocessing steps</p></td>
</tr>
<tr>
<td><p>Using a tokenizer: Splitting by tokens</p></td>
<td><p>Accurate token counts, which helps in avoiding LLM prompt token limits</p></td>
<td><p>Requires tokenization, may increase computational complexity</p></td>
</tr>
</tbody>
</table>

<p>By choosing the appropriate chunking <a data-type="indexterm" data-primary="chunking text" data-secondary="strategies for" data-startref="ckgxstg" id="id649"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="chunking text" data-startref="mlmchk" id="id650"/>strategy for your specific use case, you can optimize the performance and accuracy of AI language models.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Sentence Detection Using SpaCy"><div class="sect1" id="id34">
<h1>Sentence Detection Using SpaCy</h1>

<p><em>Sentence detection</em>, also known as sentence <a data-type="indexterm" data-primary="sentence detection, spaCy and" id="stdtcpy"/><a data-type="indexterm" data-primary="spaCy, sentence detection and" id="spcysttc"/><a data-type="indexterm" data-primary="chunking text" data-secondary="spaCy and" id="chkspy"/>boundary disambiguation, is the process used in NLP that involves identifying the start and end of sentences within a given text. It can be particularly useful for tasks that require preserving the context and structure of the original content. By splitting the text into sentences, LLMs can better understand and process the information for tasks such as summarization, translation, and sentiment analysis.</p>

<p>Splitting by sentence is possible using NLP libraries such as <a href="https://spacy.io">spaCy</a>. Ensure that you have spaCy installed in your Python environment. You can install it with <code>pip install spacy</code>. Download the <code>en_core_web_sm</code> model using the command <code>python -m spacy download en_core_web_sm</code>.</p>

<p>In <a data-type="xref" href="#ex-3-3">Example 3-3</a>, the code demonstrates sentence detection using the spaCy library in Python.</p>
<div id="ex-3-3" data-type="example">
<h5><span class="label">Example 3-3. </span><a href="https://oreil.ly/GKDnc">Sentence detection with spaCy</a></h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">spacy</code>

<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"en_core_web_sm"</code><code class="p">)</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"This is a sentence. This is another sentence."</code>

<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="k">for</code> <code class="n">sent</code> <code class="ow">in</code> <code class="n">doc</code><code class="o">.</code><code class="n">sents</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">sent</code><code class="o">.</code><code class="n">text</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">This is a sentence.
This is another sentence.</pre></div>

<p>First, you’ll import the spaCy library and load the English model <code>(en_core_web_sm)</code> to initialize an <code>nlp</code> object. Define an input text with two sentences; the text is then processed with <code>doc = nlp(text)</code>, creating a <code>doc</code> object as a result. Finally, the code iterates through the detected sentences using the <code>doc.sents</code> attribute <a data-type="indexterm" data-primary="sentence detection, spaCy and" data-startref="stdtcpy" id="id651"/><a data-type="indexterm" data-primary="spaCy, sentence detection and" data-startref="spcysttc" id="id652"/><a data-type="indexterm" data-primary="chunking text" data-secondary="spaCy and" data-startref="chkspy" id="id653"/>and prints each sentence.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Building a Simple Chunking Algorithm in Python"><div class="sect1" id="id152">
<h1>Building a Simple Chunking Algorithm in Python</h1>

<p>After exploring many chunking <a data-type="indexterm" data-primary="chunking text" data-secondary="Python and" id="chkpy"/><a data-type="indexterm" data-primary="Python" data-secondary="chunking text" id="phtcktc"/>strategies, it’s important to build your intuition by writing a simple chunking algorithm from scatch.</p>

<p><a data-type="xref" href="#ex-3-4">Example 3-4</a> shows how to chunk text based on the length of characters from the blog post “Hubspot - What Is Digital Marketing?” This file can be found in the Github repository at <em><a href="https://oreil.ly/30rlQ">content/chapter_3/hubspot_blog_post.txt</a></em>.</p>

<p>To correctly read the <em>hubspot_blog_post.txt</em> file, make sure your current working directory is set to the <a href="https://oreil.ly/OHurh"><em>content/chapter_3</em></a> GitHub directory. This applies for both running the Python code or launching the Jupyter Notebook server.</p>
<div id="ex-3-4" data-type="example">
<h5><span class="label">Example 3-4. </span><a href="https://oreil.ly/n3sNy">Character chunking</a></h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"hubspot_blog_post.txt"</code><code class="p">,</code> <code class="s2">"r"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>

<code class="n">chunks</code> <code class="o">=</code> <code class="p">[</code><code class="n">text</code><code class="p">[</code><code class="n">i</code> <code class="p">:</code> <code class="n">i</code> <code class="o">+</code> <code class="mi">200</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">text</code><code class="p">),</code> <code class="mi">200</code><code class="p">)]</code>

<code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">chunks</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"-"</code> <code class="o">*</code> <code class="mi">20</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">chunk</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">search engine optimization strategy for many local businesses is an optimized
Google My Business profile to appear in local search results when people look for
products or services related to what yo
--------------------
u offer.

For Keeps Bookstore, a local bookstore in Atlanta, GA, has optimized its
Google My Business profile for local SEO so it appears in queries for
“atlanta bookstore.”
--------------------
...(shortened for brevity)...</pre></div>

<p>First, you open the text file <em>hubspot_blog_post.txt</em> with the <code>open</code> function and read its contents into the variable text. Then using a list comprehension you create a list of chunks, where each <code>chunk</code> is a 200 character substring of text.</p>

<p>Then you use the <code>range</code> function to generate indices for each 200 character substring, and the <code>i:i+200</code> slice notation to extract the substring from text.</p>

<p>Finally, you loop through each chunk in the <code>chunks</code> list and <code>print</code> it to the console.</p>

<p>As you can see, because the chunking implementation is relatively simple and only based on length, there are gaps within the sentences and even words.</p>

<p>For these reasons we believe that good NLP chunking has the following properties:</p>

<ul>
<li>
<p>Preserves entire words, ideally sentences and contextual points made by speakers</p>
</li>
<li>
<p>Handles for when sentences span across several pages, for example, page 1 into page 2</p>
</li>
<li>
<p>Provides an adequate token count for each <code>chunk</code> so that the total number of input tokens will appropriately <a data-type="indexterm" data-primary="chunking text" data-secondary="Python and" data-startref="chkpy" id="id654"/><a data-type="indexterm" data-primary="Python" data-secondary="chunking text" data-startref="phtcktc" id="id655"/>fit into a given token context window for any LLM</p>
</li>
</ul>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Sliding Window Chunking"><div class="sect1" id="id35">
<h1>Sliding Window Chunking</h1>

<p><em>Sliding window chunking</em> is a technique used <a data-type="indexterm" data-primary="chunking text" data-secondary="sliding window chunking" id="ckxldww"/><a data-type="indexterm" data-primary="sliding window chunking" id="slwwck"/>for dividing text data into overlapping chunks, or <em>windows</em>, based on a specified number of characters, tokens, or words.</p>

<p>But what exactly is a sliding window?</p>

<p>Imagine viewing a long piece of text through a small window. This window is only capable of displaying a fixed number of items at a time. As you slide this window from the beginning to the end of the text, you see <em>overlapping chunks of text</em>. This mechanism forms the essence of the sliding window approach.</p>

<p>Each window size is defined by a <em>fixed number of characters, tokens, or words</em>, and the <em>step size</em> determines how far the window moves with each slide.</p>

<p>In <a data-type="xref" href="#figure-3-6">Figure 3-6</a>,  with a window size of 4 words and a step size of 1, the first chunk would contain the first 4 words of the text. The window then slides 1 word to the right to create the second chunk, which contains words 2 through 5.</p>

<p>This process repeats until the end of the text is reached, ensuring each chunk overlaps with the previous and next ones to retain some shared context.</p>

<figure><div id="figure-3-6" class="figure">
<img src="assets/pega_0306.png" alt="pega 0306" width="600" height="389"/>
<h6><span class="label">Figure 3-6. </span>A sliding window, with a window size of 4 and a step size of 1</h6>
</div></figure>

<p>Due to the step size being 1, there is a lot of duplicate information between chunks, and at the same time the risk of losing information between chunks is dramatically reduced.</p>

<p>This is in stark contrast to <a data-type="xref" href="#figure-3-7">Figure 3-7</a>, which has a window size of 4 words and a step size of 2. You’ll notice that because of the 100% increase in step size, the amount of information shared between the chunks is greatly reduced.</p>

<figure><div id="figure-3-7" class="figure">
<img src="assets/pega_0307.png" alt="pega 0307" width="600" height="322"/>
<h6><span class="label">Figure 3-7. </span>A sliding window, with a window size of 4 and a step size of 2</h6>
</div></figure>

<p>You will likely need a larger overlap if accuracy and preserving semanatic context are more important than minimizing token inputs or the number of requests made to an LLM.</p>

<p><a data-type="xref" href="#ex-3-5">Example 3-5</a> shows how you can implement a sliding window using Python’s <code>len()</code> function. The <code>len()</code> function provides us with the total number of characters rather than words in a given text string, which subsequently aids in defining the parameters of our sliding windows.</p>
<div id="ex-3-5" data-type="example">
<h5><span class="label">Example 3-5. </span><a href="https://oreil.ly/aCkDo">Sliding window</a></h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">sliding_window</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">window_size</code><code class="p">,</code> <code class="n">step_size</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">window_size</code> <code class="o">&gt;</code> <code class="nb">len</code><code class="p">(</code><code class="n">text</code><code class="p">)</code> <code class="ow">or</code> <code class="n">step_size</code> <code class="o">&lt;</code> <code class="mi">1</code><code class="p">:</code>
        <code class="k">return</code> <code class="p">[]</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">text</code><code class="p">[</code><code class="n">i</code><code class="p">:</code><code class="n">i</code><code class="o">+</code><code class="n">window_size</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code>
    <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">text</code><code class="p">)</code> <code class="o">-</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">step_size</code><code class="p">)]</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"This is an example of sliding window text chunking."</code>
<code class="n">window_size</code> <code class="o">=</code> <code class="mi">20</code>
<code class="n">step_size</code> <code class="o">=</code> <code class="mi">5</code>

<code class="n">chunks</code> <code class="o">=</code> <code class="n">sliding_window</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">window_size</code><code class="p">,</code> <code class="n">step_size</code><code class="p">)</code>

<code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">chunks</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Chunk </code><code class="si">{</code><code class="n">idx</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">: </code><code class="si">{</code><code class="n">chunk</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>

<p>This code outputs:</p>

<pre data-type="programlisting">Chunk 1: This is an example o
Chunk 2: is an example of sli
Chunk 3:  example of sliding
Chunk 4: ple of sliding windo
Chunk 5: f sliding window tex
Chunk 6: ding window text chu
Chunk 7: window text chunking</pre></div>

<p>In the context of prompt engineering, the sliding window approach offers several benefits over fixed chunking methods. It allows LLMs to retain a higher degree of context, as there is an overlap between the chunks and offers an alternative approach to preserving context <a data-type="indexterm" data-primary="chunking text" data-secondary="sliding window chunking" data-startref="ckxldww" id="id656"/><a data-type="indexterm" data-primary="sliding window chunking" data-startref="slwwck" id="id657"/>compared to sentence detection.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Text Chunking Packages"><div class="sect1" id="id36">
<h1>Text Chunking Packages</h1>

<p>When working with LLMs such as GPT-4, always <a data-type="indexterm" data-primary="chunking text" id="id658"/>remain wary of the maximum context length:</p>
<ul class="simplelist">
<li><code>maximum_context_length = input_tokens + output_tokens</code></li>
</ul>

<p>There are various tokenizers available to break your text down into manageable units, the most popular ones being NLTK, spaCy, and tiktoken.</p>

<p>Both <a href="https://oreil.ly/wTmI7">NLTK</a> and <a href="https://oreil.ly/c4MvQ">spaCy</a> provide comprehensive support for text processing, but you’ll be focusing on tiktoken.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Text Chunking with Tiktoken"><div class="sect1" id="id37">
<h1>Text Chunking with Tiktoken</h1>

<p><a href="https://oreil.ly/oSpVe">Tiktoken</a> is a fast <em>byte pair encoding (BPE)</em> tokenizer that breaks down text into <a data-type="indexterm" data-primary="chunking text" data-secondary="Tiktoken" id="id659"/><a data-type="indexterm" data-primary="Tiktoken" id="id660"/>subword units and is designed for use with OpenAI’s models. Tiktoken offers faster performance than comparable open source tokenizers.</p>

<p>As a developer working with GPT-4 applications, using tiktoken offers you several key advantages:</p>
<dl>
<dt>Accurate token breakdown</dt>
<dd>
<p>It’s crucial to divide text into tokens because GPT models interpret text as individual tokens. Identifying the number of tokens in your text helps you figure out whether the text is too lengthy for a model to process.</p>
</dd>
<dt>Effective resource utilization</dt>
<dd>
<p>Having the correct token count enables you to manage resources efficiently, particularly when using the OpenAI API. Being aware of the exact number of tokens helps you regulate and optimize API usage, maintaining a balance between costs and <a data-type="indexterm" data-primary="ChatGPT" data-secondary="chunking text" data-startref="cgxnk" id="id661"/>resource usage.</p>
</dd>
</dl>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Encodings"><div class="sect1" id="id38">
<h1>Encodings</h1>

<p>Encodings define the method of <a data-type="indexterm" data-primary="chunking text" data-secondary="Tiktoken" data-tertiary="encodings" id="ckxtkkc"/><a data-type="indexterm" data-primary="Tiktoken" data-secondary="encodings" id="tktkcdg"/><a data-type="indexterm" data-primary="encodings, Tiktoken" id="ecdtktk"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="encodings" id="gpcdgs"/>converting text into tokens, with different models utilizing different encodings. Tiktoken supports three encodings commonly used by OpenAI models:</p>
<table>

<thead>
<tr>
<th>Encoding name</th>
<th>OpenAI models</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>cl100k_base</p></td>
<td><p>GPT-4, GPT-3.5-turbo, text-embedding-ada-002</p></td>
</tr>
<tr>
<td><p>p50k_base</p></td>
<td><p>Codex models, text-davinci-002, text-davinci-003</p></td>
</tr>
<tr>
<td><p>r50k_base (or gpt2)</p></td>
<td><p>GPT-3 models like davinci</p></td>
</tr>
</tbody>
</table>








<section data-type="sect2" data-pdf-bookmark="Understanding the Tokenization of Strings"><div class="sect2" id="id39">
<h2>Understanding the Tokenization of Strings</h2>

<p>In English, tokens can vary in length, ranging from a <a data-type="indexterm" data-primary="tokens" id="id662"/><a data-type="indexterm" data-primary="tokenization" data-secondary="strings" id="id663"/>single character like <em>t</em>, to an entire word such as <em>great</em>. This is due to the adaptable nature of tokenization, which can accommodate even tokens shorter than a character in complex script languages or tokens longer than a word in languages without spaces or where phrases function as single units.</p>

<p>It is not uncommon for spaces to be included within tokens, such as <code>"is"</code> rather than <code>"is "</code> or <code>" "+"is"</code>. This practice helps maintain the original text formatting and can capture specific linguistic characteristics.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To easily examine the tokenization of a string, you can use <a href="https://oreil.ly/K6ZQK">OpenAI Tokenizer</a>.</p>
</div>

<p>You can install <a href="https://oreil.ly/HA2QD">tiktoken from PyPI</a> with <code>pip install</code> <span class="keep-together"><code>tiktoken</code></span>. In the following example, you’ll see how to easily encode text into tokens and decode tokens into text:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># 1. Import the package:</code>
<code class="kn">import</code> <code class="nn">tiktoken</code>

<code class="c1"># 2. Load an encoding with tiktoken.get_encoding()</code>
<code class="n">encoding</code> <code class="o">=</code> <code class="n">tiktoken</code><code class="o">.</code><code class="n">get_encoding</code><code class="p">(</code><code class="s2">"cl100k_base"</code><code class="p">)</code>

<code class="c1"># 3. Turn some text into tokens with encoding.encode()</code>
<code class="c1"># while turning tokens into text with encoding.decode()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">encoding</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"Learning how to use Tiktoken is fun!"</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">encoding</code><code class="o">.</code><code class="n">decode</code><code class="p">([</code><code class="mi">1061</code><code class="p">,</code> <code class="mi">15009</code><code class="p">,</code> <code class="mi">374</code><code class="p">,</code> <code class="mi">264</code><code class="p">,</code> <code class="mi">2294</code><code class="p">,</code> <code class="mi">1648</code><code class="p">,</code>
<code class="mi">311</code><code class="p">,</code> <code class="mi">4048</code><code class="p">,</code> <code class="mi">922</code><code class="p">,</code> <code class="mi">15592</code><code class="p">,</code> <code class="mi">0</code><code class="p">]))</code>

<code class="c1"># [48567, 1268, 311, 1005, 73842, 5963, 374, 2523, 0]</code>
<code class="c1"># "Data engineering is a great way to learn about AI!"</code></pre>

<p>Additionally let’s write a function that will tokenize <a data-type="indexterm" data-primary="chunking text" data-secondary="Tiktoken" data-tertiary="encodings" data-startref="ckxtkkc" id="id664"/><a data-type="indexterm" data-primary="Tiktoken" data-secondary="encodings" data-startref="tktkcdg" id="id665"/><a data-type="indexterm" data-primary="encodings, Tiktoken" data-startref="ecdtktk" id="id666"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="encodings" data-startref="gpcdgs" id="id667"/>the text and then count the number of tokens given a <code>text_string</code> and <code>encoding_name</code>.</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">count_tokens</code><code class="p">(</code><code class="n">text_string</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">encoding_name</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="sd">"""</code>
<code class="sd">    Returns the number of tokens in a text string using a given encoding.</code>

<code class="sd">    Args:</code>
<code class="sd">        text: The text string to be tokenized.</code>
<code class="sd">        encoding_name: The name of the encoding to be used for tokenization.</code>

<code class="sd">    Returns:</code>
<code class="sd">        The number of tokens in the text string.</code>

<code class="sd">    Raises:</code>
<code class="sd">        ValueError: If the encoding name is not recognized.</code>
<code class="sd">    """</code>
    <code class="n">encoding</code> <code class="o">=</code> <code class="n">tiktoken</code><code class="o">.</code><code class="n">get_encoding</code><code class="p">(</code><code class="n">encoding_name</code><code class="p">)</code>
    <code class="n">num_tokens</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">encoding</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">text_string</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">num_tokens</code>

<code class="c1"># 4. Use the function to count the number of tokens in a text string.</code>
<code class="n">text_string</code> <code class="o">=</code> <code class="s2">"Hello world! This is a test."</code>
<code class="nb">print</code><code class="p">(</code><code class="n">count_tokens</code><code class="p">(</code><code class="n">text_string</code><code class="p">,</code> <code class="s2">"cl100k_base"</code><code class="p">))</code></pre>

<p>This code outputs <code>8</code>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Estimating Token Usage for Chat API Calls"><div class="sect1" id="id40">
<h1>Estimating Token Usage for Chat API Calls</h1>

<p>ChatGPT models, such as GPT-3.5-turbo and GPT-4, utilize <a data-type="indexterm" data-primary="tokens" data-secondary="usage estimates" id="tkusst"/><a data-type="indexterm" data-primary="GPT-3.5-turbo" data-secondary="tokens" id="gpbok"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="tokens" id="tpgktn"/>tokens similarly to previous completion models. However, the message-based structure makes token counting for conversations more challenging:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">num_tokens_from_messages</code><code class="p">(</code><code class="n">messages</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-0613"</code><code class="p">):</code>
    <code class="sd">"""Return the number of tokens used by a list of messages."""</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="n">encoding</code> <code class="o">=</code> <code class="n">tiktoken</code><code class="o">.</code><code class="n">encoding_for_model</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
    <code class="k">except</code> <code class="ne">KeyError</code><code class="p">:</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Warning: model not found. Using cl100k_base encoding."</code><code class="p">)</code>
        <code class="n">encoding</code> <code class="o">=</code> <code class="n">tiktoken</code><code class="o">.</code><code class="n">get_encoding</code><code class="p">(</code><code class="s2">"cl100k_base"</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">model</code> <code class="ow">in</code> <code class="p">{</code>
        <code class="s2">"gpt-3.5-turbo-0613"</code><code class="p">,</code>
        <code class="s2">"gpt-3.5-turbo-16k-0613"</code><code class="p">,</code>
        <code class="s2">"gpt-4-0314"</code><code class="p">,</code>
        <code class="s2">"gpt-4-32k-0314"</code><code class="p">,</code>
        <code class="s2">"gpt-4-0613"</code><code class="p">,</code>
        <code class="s2">"gpt-4-32k-0613"</code><code class="p">,</code>
        <code class="p">}:</code>
        <code class="n">tokens_per_message</code> <code class="o">=</code> <code class="mi">3</code>
        <code class="n">tokens_per_name</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="k">elif</code> <code class="n">model</code> <code class="o">==</code> <code class="s2">"gpt-3.5-turbo-0301"</code><code class="p">:</code>
        <code class="n">tokens_per_message</code> <code class="o">=</code> <code class="mi">4</code>  <code class="c1"># every message follows</code>
        <code class="c1"># &lt;|start|&gt;{role/name}\n{content}&lt;|end|&gt;\n</code>
        <code class="n">tokens_per_name</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>  <code class="c1"># if there's a name, the role is omitted</code>
    <code class="k">elif</code> <code class="s2">"gpt-3.5-turbo"</code> <code class="ow">in</code> <code class="n">model</code><code class="p">:</code>
        <code class="nb">print</code><code class="p">(</code><code class="s1">'''Warning: gpt-3.5-turbo may update over time. Returning</code>
<code class="s1">        num tokens assuming gpt-3.5-turbo-0613.'''</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">num_tokens_from_messages</code><code class="p">(</code><code class="n">messages</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo-0613"</code><code class="p">)</code>
    <code class="k">elif</code> <code class="s2">"gpt-4"</code> <code class="ow">in</code> <code class="n">model</code><code class="p">:</code>
        <code class="nb">print</code><code class="p">(</code><code class="s1">'''Warning: gpt-4 may update over time.</code>
<code class="s1">        Returning num tokens assuming gpt-4-0613.'''</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">num_tokens_from_messages</code><code class="p">(</code><code class="n">messages</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4-0613"</code><code class="p">)</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">NotImplementedError</code><code class="p">(</code>
            <code class="sa">f</code><code class="s2">"""num_tokens_from_messages() is not implemented for model</code>
<code class="s2">            </code><code class="si">{</code><code class="n">model</code><code class="si">}</code><code class="s2">."""</code>
        <code class="p">)</code>
    <code class="n">num_tokens</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">for</code> <code class="n">message</code> <code class="ow">in</code> <code class="n">messages</code><code class="p">:</code>
        <code class="n">num_tokens</code> <code class="o">+=</code> <code class="n">tokens_per_message</code>
        <code class="k">for</code> <code class="n">key</code><code class="p">,</code> <code class="n">value</code> <code class="ow">in</code> <code class="n">message</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
            <code class="n">num_tokens</code> <code class="o">+=</code> <code class="nb">len</code><code class="p">(</code><code class="n">encoding</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">value</code><code class="p">))</code>
            <code class="k">if</code> <code class="n">key</code> <code class="o">==</code> <code class="s2">"name"</code><code class="p">:</code>
                <code class="n">num_tokens</code> <code class="o">+=</code> <code class="n">tokens_per_name</code>
    <code class="n">num_tokens</code> <code class="o">+=</code> <code class="mi">3</code>  <code class="c1"># every reply is primed with</code>
    <code class="c1"># &lt;|start|&gt;assistant&lt;|message|&gt;</code>
    <code class="k">return</code> <code class="n">num_tokens</code></pre>

<p><a data-type="xref" href="#ex-3-6">Example 3-6</a> highlights the specific structure required to make a request against any of the chat models, which are currently GPT-3x and GPT-4.</p>

<p>Normally, chat history is <a data-type="indexterm" data-primary="chat history, token usage and" id="id668"/>structured with a <code>system</code> message first, and then succeeded by alternating exchanges between the <code>user</code> and the <code>assistant</code>.</p>
<div id="ex-3-6" data-type="example">
<h5><span class="label">Example 3-6. </span>A payload for the Chat Completions API on OpenAI</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">example_messages</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s1">'''You are a helpful, pattern-following assistant that</code>
<code class="s1">        translates corporate jargon into plain English.'''</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
        <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"example_user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"New synergies will help drive top-line growth."</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
        <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"example_assistant"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"Things working well together will increase revenue."</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
        <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"example_user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s1">'''Let's circle back when we have more bandwidth to touch</code>
<code class="s1">        base on opportunities for increased leverage.'''</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
        <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"example_assistant"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s1">'''Let's talk later when we're less busy about how to</code>
<code class="s1">        do better.'''</code><code class="p">,</code>
    <code class="p">},</code>
    <code class="p">{</code>
        <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
        <code class="s2">"content"</code><code class="p">:</code> <code class="s1">'''This late pivot means we don't have</code>
<code class="s1">        time to boil the ocean for the client deliverable.'''</code><code class="p">,</code>
    <code class="p">},</code>
<code class="p">]</code>

<code class="k">for</code> <code class="n">model</code> <code class="ow">in</code> <code class="p">[</code><code class="s2">"gpt-3.5-turbo-0301"</code><code class="p">,</code> <code class="s2">"gpt-4-0314"</code><code class="p">]:</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
    <code class="c1"># example token count from the function defined above</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'''</code><code class="si">{</code><code class="n">num_tokens_from_messages</code><code class="p">(</code><code class="n">example_messages</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code><code class="si">}</code><code class="s1"/>
<code class="s1">    prompt tokens counted by num_tokens_from_messages().'''</code><code class="p">)</code></pre></div>

<p class="less_space pagebreak-before"><code>"role": "system"</code> describes a system message <a data-type="indexterm" data-primary="roles" id="id669"/>that’s useful for <em>providing prompt instructions</em>. It offers a means to tweak the assistant’s character or provide explicit directives regarding its interactive approach. It’s crucial to understand, though, that the system command isn’t a prerequisite, and the model’s default demeanor without a system command could closely resemble the behavior of “You are a helpful assistant.”</p>

<p>The roles that you can have are <code>["system", "user", "assistant"]</code>.</p>

<p><code>"content": "Some content"</code> is where you place the prompt or responses from a language model, depending upon the message’s role. It <a data-type="indexterm" data-primary="tokens" data-secondary="usage estimates" data-startref="tkusst" id="id670"/><a data-type="indexterm" data-primary="GPT-3.5-turbo" data-secondary="tokens" data-startref="gpbok" id="id671"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="tokens" data-startref="tpgktn" id="id672"/>can be either <span class="keep-together"><code>"assistant"</code></span>, <code>"system"</code>, or <code>"user"</code>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Sentiment Analysis"><div class="sect1" id="id41">
<h1>Sentiment Analysis</h1>

<p><em>Sentiment analysis</em> is a widely used NLP <a data-type="indexterm" data-primary="sentiment analysis" id="sntays"/><a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="sentiment analysis" id="nplsygss"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="sentiment analysis" id="lmmstays"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="sentiment analysis" id="ppsttys"/>technique that helps in identifying, extracting, and understanding the emotions, opinions, or sentiments expressed in a piece of text. By leveraging the power of LLMs like GPT-4, sentiment analysis has become an essential tool for businesses, researchers, and developers across various industries.</p>

<p>The primary goal of sentiment analysis is to determine the attitude or emotional tone conveyed in a text, whether it’s positive, negative, or neutral. This information can provide valuable insights into consumer opinions about products or services, help monitor brand reputation, and even assist in predicting market trends.</p>

<p>The following are several prompt engineering techniques for creating effective sentiment analysis prompts:</p>

<p>Input:</p>

<pre data-type="programlisting">Is this text positive or negative?

I absolutely love the design of this phone, but the battery life is quite
disappointing.</pre>

<p>Output:</p>

<pre data-type="programlisting">The text has a mixed tone, as it contains both positive and negative
aspects. The positive part is "I absolutely love the design of this phone,"
while the negative part is "the battery life is quite disappointing."</pre>

<p>Although GPT-4 identifies a “mixed tone,” the outcome is a result of several shortcomings in the prompt:</p>
<dl>
<dt>Lack of clarity</dt>
<dd>
<p>The prompt does not clearly define the desired output format.</p>
</dd>
<dt>Insufficient examples</dt>
<dd>
<p>The prompt does not include any examples of positive, negative, or neutral sentiments, which could help guide the LLM in understanding the distinctions between them.</p>
</dd>
<dt>No guidance on handling mixed sentiments</dt>
<dd>
<p>The prompt does not specify how to handle cases where the text contains a mix of positive and negative sentiments.</p>
</dd>
</dl>

<p>Input:</p>

<pre data-type="programlisting">Using the following examples as a guide:
positive: 'I absolutely love the design of this phone!'
negative: 'The battery life is quite disappointing.'
neutral: 'I liked the product, but it has short battery life.'

Only return either a single word of:
- positive
- negative
- neutral

Please classify the sentiment of the following text as positive, negative,
or neutral: I absolutely love the design of this phone, but the battery
life is quite disappointing.</pre>

<p>Output:</p>

<pre data-type="programlisting">neutral</pre>

<p>This prompt is much better because it:</p>
<dl>
<dt>Provides clear instructions</dt>
<dd>
<p>The prompt clearly states the task, which is to classify the sentiment of the given text into one of three categories: positive, negative, or neutral.</p>
</dd>
<dt>Offers examples</dt>
<dd>
<p>The prompt provides examples for each of the sentiment categories, which helps in understanding the context and desired output.</p>
</dd>
<dt>Defines the output format</dt>
<dd>
<p>The prompt specifies that the output should be a single word, ensuring that the response is <a data-type="indexterm" data-primary="sentiment analysis" data-startref="sntays" id="id673"/><a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="sentiment analysis" data-startref="nplsygss" id="id674"/>concise and easy to understand.</p>
</dd>
</dl>








<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Techniques for Improving Sentiment Analysis"><div class="sect2" id="id42">
<h2>Techniques for Improving Sentiment Analysis</h2>

<p>To enhance sentiment analysis accuracy, preprocessing <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="enhancing" id="id675"/><a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="sentiment analysis" data-tertiary="enhancing" id="id676"/>the input text is a vital step. This involves the following:</p>
<dl>
<dt>Special characters removal</dt>
<dd>
<p>Exceptional characters <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="enhancing" data-tertiary="special characters removal" id="id677"/><a data-type="indexterm" data-primary="special characters, sentiment analysis" id="id678"/>such as emojis, hashtags, and punctuation may skew the rule-based sentiment algorithm’s judgment. Besides, these characters might not be recognized by machine learning and deep learning models, resulting in misclassification.</p>
</dd>
<dt>Lowercase conversion</dt>
<dd>
<p>Converting all <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="enhancing" data-tertiary="lowercase conversion" id="id679"/><a data-type="indexterm" data-primary="lowercase text in sentiment analysis" id="id680"/>the characters to lowercase aids in creating uniformity. For instance, words like <em>Happy</em> and <em>happy</em> are treated as different words by models, which can cause duplication and inaccuracies.</p>
</dd>
<dt>Spelling correction</dt>
<dd>
<p>Spelling errors <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="enhancing" data-tertiary="spelling correction" id="id681"/><a data-type="indexterm" data-primary="spelling correction, sentiment analysis" id="id682"/>can cause misinterpretation and misclassification. Creating a spell-check pipeline can significantly reduce such errors and improve results.</p>
</dd>
</dl>

<p>For industry- or domain-specific text, embedding domain-specific content in the prompt helps in navigating the LLM’s sense of the text’s framework and sentiment. It enhances accuracy in the classification and provides a heightened understanding of particular jargon and expressions.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Limitations and Challenges in Sentiment Analysis"><div class="sect2" id="id43">
<h2>Limitations and Challenges in Sentiment Analysis</h2>

<p>Despite the advancements in LLMs and the application of prompt engineering techniques, sentiment analysis still faces some limitations and challenges:</p>
<dl>
<dt>Handling sarcasm and irony</dt>
<dd>
<p>Detecting <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="sarcasm" id="id683"/><a data-type="indexterm" data-primary="sentiment analysis" data-secondary="irony" id="id684"/><a data-type="indexterm" data-primary="sarcasm, sentiment analysis" id="id685"/><a data-type="indexterm" data-primary="irony, sentiment analysis" id="id686"/>sarcasm and irony in text can be difficult for LLMs, as it often requires understanding the context and subtle cues that humans can easily recognize. Misinterpreting sarcastic or ironic statements may lead to inaccurate sentiment classification.</p>
</dd>
<dt>Identifying context-specific sentiment</dt>
<dd>
<p>Sentiment <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="context-specific sentiment" id="id687"/><a data-type="indexterm" data-primary="context-specific sentiment" id="id688"/>analysis can be challenging when dealing with context-specific sentiments, such as those related to domain-specific jargon or cultural expressions. LLMs may struggle to accurately classify sentiments in <a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="sentiment analysis" data-tertiary="enhancing" data-startref="jplggys" id="id689"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="sentiment analysis" data-startref="lmmstays" id="id690"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="sentiment analysis" data-startref="ppsttys" id="id691"/>these cases without proper guidance or domain-specific examples.</p>
</dd>
</dl>
</div></section>
</div></section>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Least to Most"><div class="sect1" id="id44">
<h1>Least to Most</h1>

<p>The <em>least to most</em> technique in prompt engineering is a powerful method for sequentially generating or extracting <a data-type="indexterm" data-primary="least to most technique" id="id692"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="Flask Hello World" id="lomfkw"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="least to most technique" id="lllllmtchq"/>increasingly detailed knowledge on a given topic. This method is particularly effective when dealing with complex subjects or when a high level of detail is necessary.</p>

<p>Least to most uses a <em>chain</em> of prompts where each new prompt is based on the last answer. This step-by-step approach helps gather more detailed information each time, making it easier to dive deeper into any topic.</p>

<p>This technique can also be applied to code generation, as demonstrated in a Flask <code>Hello World</code> app example.</p>








<section data-type="sect2" data-pdf-bookmark="Planning the Architecture"><div class="sect2" id="id153">
<h2>Planning the Architecture</h2>

<p>Before diving into the architecture, let’s briefly <a data-type="indexterm" data-primary="least to most technique" data-secondary="architecture planning" id="id693"/>understand what Flask is. <a href="https://oreil.ly/7N-bs">Flask</a> is a lightweight web application framework in Python, widely used for creating web applications quickly and with minimal code. (Flask is only used for demonstration purposes here and isn’t included within the <a href="https://oreil.ly/TRK0i"><em>requirements.txt</em> file</a> for the book.</p>

<p>Now, let’s ask an LLM to outline the basic architecture for a simple Flask “Hello World” application.</p>

<p>Input:</p>

<pre data-type="programlisting">Outline the basic architecture for a simple Flask "Hello World" application.</pre>

<p>Output:</p>

<pre data-type="programlisting">Sure! Here's an outline for a basic Flask "Hello World" application:

Import Flask module: Import the Flask module
to use its ((("Flask module code generation")))functionality.

Create an instance of the Flask application: Create an instance of the
Flask class to represent your web application.
Define a route and view function: ...
Implement the view function: ...
Run the application: Start the Flask development server to run the application.</pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Coding Individual Functions"><div class="sect2" id="id154">
<h2>Coding Individual Functions</h2>

<p>Before coding, let’s clarify <a data-type="indexterm" data-primary="least to most technique" data-secondary="function coding" id="id694"/>what a Flask route is: it’s a function linked to a URL pattern that determines what users see <a data-type="indexterm" data-primary="Flask module code generation" data-secondary="Flask route" id="id695"/>on a specific web page. Next, we’ll provide the code snippet for a simple “Hello World” route in Flask.</p>

<p class="pagebreak-before">Input:</p>

<pre data-type="programlisting">Provide a code snippet for a simple "Hello World" route in Flask.</pre>

<p>Output:</p>

<pre data-type="programlisting">Certainly! Here's a simple code snippet for a "Hello World" route using Flask:

from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'

if __name__ == '__main__':
    app.run()</pre>

<p>In this code, we import the Flask module and create a Flask application instance named <code>app</code>. We then define a route using the <code>@app.route</code> decorator, specifying the root URL path <code>'/'</code>. The <code>hello_world</code> function is associated with this route, and it returns the string <code>'Hello, World!'</code> as the response.</p>

<p>The Flask application will start by calling <code>app.run()</code> when your Python script is executed directly.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Adding Tests"><div class="sect2" id="id155">
<h2>Adding Tests</h2>

<p>Finally, let’s ask it to generate a <a data-type="indexterm" data-primary="Flask module code generation" data-secondary="Hello World test" id="id696"/><a data-type="indexterm" data-primary="least to most technique" data-secondary="Flask Hello World" id="id697"/>test case for the “Hello World” route in the Flask application.</p>

<p>Input:</p>

<pre data-type="programlisting">Provide an example of a test case for the "Hello World" route in a Flask
application.</pre>

<p>Output:</p>

<pre data-type="programlisting">Sure! Here's an example of a test case for the "Hello World" route in a
Flask application:

import unittest
from flask import Flask
from flask.testing import FlaskClient

# Create the Flask application
app = Flask(__name__)

# Create the route
@app.route('/')
def hello_world():
    return 'Hello, World!'

class FlaskAppTestCase(unittest.TestCase):
    def setUp(self):
        # Create a test client
        self.app = app.test_client()

    def test_hello_world_route(self):
        # Send a GET request to the route
        response = self.app.get('/')

        # Check the response status code
        self.assertEqual(response.status_code, 200)

        # Check the response data
        self.assertEqual(response.data, b'Hello, World!')

if __name__ == '__main__':
    unittest.main()</pre>

<p>By using the least to most prompt technique, you can gradually build up the knowledge and understanding <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="Flask Hello World" data-startref="lomfkw" id="id698"/>required for the task at hand.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Benefits of the Least to Most Technique"><div class="sect2" id="id156">
<h2>Benefits of the Least to Most Technique</h2>

<p>This method is particularly useful for <a data-type="indexterm" data-primary="least to most technique" data-secondary="benefits" id="id699"/>complex tasks, as it allows an LLM to generate relevant knowledge that will subsequently be used as context for future tasks.</p>

<p>Let’s dive deeper into the benefits of using this approach in various other scenarios:</p>
<dl>
<dt>Progressive exploration</dt>
<dd>
<p>Breaking a complex problem into smaller tasks allows an LLM to provide more detailed and accurate information at each step. This approach is especially helpful when working with a new subject matter or a multifaceted problem.</p>
</dd>
<dt>Flexibility</dt>
<dd>
<p>The least to most technique offers flexibility in addressing different aspects of a problem. It enables you to pivot, explore alternative solutions, or dive deeper into specific areas as needed.</p>
</dd>
<dt>Improved comprehension</dt>
<dd>
<p>By breaking down a task into smaller steps, an LLM can deliver information in a more digestible format, making it easier for you to understand and follow.</p>
</dd>
<dt>Collaborative learning</dt>
<dd>
<p>This technique promotes collaboration between you and an LLM, as it encourages an iterative process of refining the output and adjusting your responses to achieve the desired outcome.</p>
</dd>
</dl>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Challenges with the Least to Most Technique"><div class="sect2" id="id157">
<h2>Challenges with the Least to Most Technique</h2>
<dl>
<dt>Overreliance on previously generated <a data-type="indexterm" data-primary="least to most technique" data-secondary="challenges" id="id700"/>knowledge</dt>
<dd>
<p>Using previous chat history to store the state may lead to larger tasks forgetting their initial tasks/outputs due to limitations in context length.</p>
</dd>
<dt>Dependence on prior prompts</dt>
<dd>
<p>Since each prompt is built upon preceding outputs, it is imperative to ensure that the quality and responses of previous prompts provide ample information for <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="least to most technique" data-startref="lllllmtchq" id="id701"/>the next step.</p>
</dd>
</dl>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>In the process of designing prompts, make <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="least to most technique" id="id702"/>sure to evaluate the quality of each prior LLM response. The performance of the next task depends upon the quality of information provided by the preceding one.</p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Role Prompting"><div class="sect1" id="id45">
<h1>Role Prompting</h1>

<p><em>Role prompting</em> is a technique in which <a data-type="indexterm" data-primary="role prompting" id="rlprmtp"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="role prompting" id="cppgrpr"/>the AI is given a specific role or character to assume while generating a response. This helps guide the AI’s response style and content to better align with the user’s needs or desired outcome.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a href="https://oreil.ly/8pf40">Awesome ChatGPT prompts</a> provides an extensive list of role prompts that you can use.</p>
</div>

<p>Let’s ask ChatGPT to take on the role of a tech reviewer for MongoDB.</p>

<p>Input:</p>

<pre data-type="programlisting">I want you to act as a tech reviewer. I will give you the name of a piece
of technology and you will provide me with an in-depth review - including
pros, cons, features, and comparisons to other technologies on the market.
My first suggestion request is: MongoDB.</pre>

<p>Output:</p>

<pre data-type="programlisting">MongoDB is a popular NoSQL, document-oriented database management system
designed to handle high-volume data storage and retrieval.

Features:

1. Document-Oriented Storage: MongoDB stores data in a flexible, JSON-like
format called BSON, making it easy to store complex data structures like
arrays and nested objects.
...

Pros:
1. High Performance: MongoDB's indexing and caching capabilities deliver
fast read and write operations.
...

Cons:
1. Limited Support for Transactions: While MongoDB supports multidocument
ACID transactions, it may not be the best choice for applications that
require complex transactional operations.
...


Comparisons to other technologies:
1. Couchbase, another popular NoSQL database, offers similar features to
MongoDB, including flexible schema and JSON-based storage. However,
Couchbase is more focused on distributed caching and may be more
suitable for applications requiring low-latency data access.

Conclusion:

MongoDB is a powerful and versatile NoSQL database solution suitable for
various applications, particularly those requiring high performance and
scalability. Its flexible data model, rich query language, and strong
consistency make it a popular choice among developers.</pre>

<p>The prompt is an excellent example of role prompting, as it clearly defines
the role the AI should assume (a tech reviewer) and sets expectations for the type of response <a data-type="indexterm" data-primary="role prompting" data-startref="rlprmtp" id="id703"/>desired (an in-depth review of MongoDB).</p>
<div data-type="tip"><h1>Give Direction</h1>
<p>When crafting prompts, consider <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="role prompting" id="id704"/>assigning a specific role to the AI. This sets the proper context for the response, creating a more focused and relevant output.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Benefits of Role Prompting"><div class="sect1" id="id158">
<h1>Benefits of Role Prompting</h1>

<p>Role prompting helps narrow down <a data-type="indexterm" data-primary="role prompting" data-secondary="benefits" id="id705"/>the AI’s responses, ensuring more focused, contextually appropriate, and tailored results. It can also enhance creativity by pushing the AI to think and respond from unique perspectives.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Challenges of Role Prompting"><div class="sect1" id="id159">
<h1>Challenges of Role Prompting</h1>

<p>Role prompting can pose certain <a data-type="indexterm" data-primary="role prompting" data-secondary="challenges" id="id706"/>challenges. There might be potential risks for bias or stereotyping based on the role assigned. Assigning stereotyped roles can lead to generating biased responses, which could harm usability or offend individuals. 
<span class="keep-together">Additionally,</span> maintaining consistency in the role throughout an extended interaction can be difficult. The model might drift off-topic or respond with information irrelevant to the assigned role.</p>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>Consistently check the quality <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="role prompting" id="id707"/>of the LLM’s responses, especially when role prompting is in play. Monitor if the AI is sticking to the role assigned or if it is veering off-topic.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="When to Use Role Prompting"><div class="sect1" id="id160">
<h1>When to Use Role Prompting</h1>

<p>Role prompting is particularly useful <a data-type="indexterm" data-primary="role prompting" data-secondary="uses" id="id708"/>when you want to:</p>
<dl>
<dt>Elicit specific expertise</dt>
<dd>
<p>If you need a response that requires domain knowledge or specialized expertise, role prompting can help guide the LLM to generate more informed and accurate responses.</p>
</dd>
<dt>Tailor response style</dt>
<dd>
<p>Assigning a role can help an LLM generate responses that match a specific tone, style, or perspective, such as a formal, casual, or humorous response.</p>
</dd>
<dt>Encourage creative responses</dt>
<dd>
<p>Role prompting can be used to create fictional scenarios or generate imaginative answers by assigning roles like a storyteller, a character from a novel, or a historical figure.</p>

<ul>
<li>
<p><em>Explore diverse perspectives</em>: If you want to explore different viewpoints on a topic, role prompting can help by asking the AI to assume various roles or personas, allowing for a more comprehensive understanding of the subject.</p>
</li>
<li>
<p><em>Enhance user engagement</em>: Role prompting can make interactions more engaging and entertaining by enabling an LLM to take on characters or personas that resonate with the user.</p>
</li>
</ul>
</dd>
</dl>

<p>If you’re using OpenAI, then the best place to <a data-type="indexterm" data-primary="role prompting" data-secondary="uses" data-startref="rlppuse" id="id709"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="role prompting" data-startref="cppgrpr" id="id710"/>add a role is within the <code>System Message</code> for chat models.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="GPT Prompting Tactics"><div class="sect1" id="id161">
<h1>GPT Prompting Tactics</h1>

<p>So far you’ve already covered several prompting tactics, including asking for context, text style bundling, least to most, and role prompting.</p>

<p>Let’s cover several more tactics, from <a data-type="indexterm" data-primary="ChatGPT" data-secondary="prompting tactics" id="pgpppt"/>managing potential hallucinations with appropriate reference text, to providing an LLM with critical <em>thinking time</em>, to understanding the concept of <em>task decomposition</em>—we have plenty for you to explore.</p>

<p>These methodologies have been designed to significantly boost the precision of your AI’s output and are recommended by <a href="https://oreil.ly/QZE8n">OpenAI</a>. Also, each tactic utilizes one or more of the prompt engineering principles discussed in <a data-type="xref" href="ch01.html#five_principles_01">Chapter 1</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Avoiding Hallucinations with Reference"><div class="sect2" id="id46">
<h2>Avoiding Hallucinations with Reference</h2>

<p>The first method for avoiding text-based <a data-type="indexterm" data-primary="hallucinations" id="hlctcn"/><a data-type="indexterm" data-primary="reference text, hallucinations and" id="rfrxhc"/>hallucinations is to instruct the model to <em>only answer using reference text.</em></p>

<p>By supplying an AI model with accurate and relevant information about a given query, the model can be directed to use this information to generate its response.</p>

<p>Input:</p>

<pre data-type="programlisting">Refer to the articles enclosed within triple quotes to respond to queries.

You must follow the following principles:
- In cases where the answer isn't found within these articles, simply
return "I could not find an answer".

"""
B2B clients tend to have longer decision-making processes, and thus longer
sales funnels. Relationship-building strategies work better for these
clients, whereas B2C customers tend to respond better to short-term offers
and messages.
"""

Example responses:
- I could not find an answer.
- Yes, B2B clients tend to have longer decision-making processes, and thus
longer sales funnels.</pre>

<p>Output:</p>

<pre data-type="programlisting">Yes, B2B clients tend to have longer decision-making processes, which leads
to longer sales cycles.</pre>

<p>If you were to ask the same reference text this question:</p>

<p>Input:</p>

<pre data-type="programlisting">...The rest of the prompt...

Question: Are B2C sales more cost-effective?</pre>

<p>Output:</p>

<pre data-type="programlisting">I could not find an answer.</pre>
<div data-type="tip"><h1>Give Direction and Specify Format</h1>
<p>The preceding prompt is excellent as it <a data-type="indexterm" data-primary="hallucinations" data-startref="hlctcn" id="id711"/><a data-type="indexterm" data-primary="reference text, hallucinations and" data-startref="rfrxhc" id="id712"/><a data-type="indexterm" data-primary="Give Direction principle" data-secondary="GPT prompting tactics" id="id713"/><a data-type="indexterm" data-primary="Specify Format principle" data-secondary="GPT prompting tactics" id="id714"/>both instructs the model on how to find answers and also sets a specific response format for any unanswerable questions.</p>
</div>

<p>Considering the constrained context windows of GPTs, a method for dynamically retrieving information relevant to the asked query might be necessary to utilize this strategy.</p>

<p>Another approach is to direct <a data-type="indexterm" data-primary="references, incorporating" id="id715"/><a data-type="indexterm" data-primary="incorporate references" id="id716"/><a data-type="indexterm" data-primary="authentication, string matching" id="id717"/><a data-type="indexterm" data-primary="strings" data-secondary="matching" id="id718"/>the model to <em>incorporate references</em> from a given text in its response. When the input is enhanced with relevant information, the model can be guided to include citations in its responses by referring to sections of the supplied documents. This approach has the added benefit that citations in the output can be <em>authenticated automatically by matching strings</em> within the given documents.</p>

<p>Input:</p>

<pre data-type="programlisting">You will be provided with a document delimited by triple quotes and a
question. Your task is to answer the question using only the provided
document and to cite the passage(s) of the document used to answer the
question. If the document does not contain the information needed to answer
this question then simply write: "Insufficient information." If an answer
to the question is provided, it must be annotated with a citation. Use the
following format to cite relevant passages ({"citation": …}).

"""
A nuclear reactor is a device that initiates, controls, and maintains a
nuclear chain reaction. The nuclear reactor uses the energy released by
nuclear fission to generate heat, which is then often used to produce steam
to drive a turbine and produce electricity.

-Excerpted from https://en.wikipedia.org/wiki/Nuclear_reactor
"""

Question: What is a nuclear reactor?</pre>

<p>Output:</p>

<pre data-type="programlisting">A nuclear reactor is a device that initiates, controls, and maintains a
nuclear chain reaction.It uses the energy released by nuclear fission to
generate heat, which is then often used to produce steam to drive a turbine
and produce electricity. {"citation": "A nuclear reactor is a device that
initiates, controls, and maintains a nuclear chain reaction. The nuclear
reactor uses the energy released by nuclear fission to generate heat, which
is then often used to produce steam to drive a turbine and produce
electricity."}</pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Give GPTs “Thinking Time”"><div class="sect2" id="id47">
<h2>Give GPTs “Thinking Time”</h2>

<p>Often, by explicitly guiding an LLM to <em>derive solutions from first principles</em> before reaching a verdict, you can <a data-type="indexterm" data-primary="thinking time" id="id719"/><a data-type="indexterm" data-primary="deriving solutions from first principles" id="id720"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="thinking time" id="id721"/>garner more accurate responses. Providing an LLM with <em>thinking time</em> can often lead to better results.</p>

<p>Input:</p>

<pre data-type="programlisting">First, try to solve the problem on your own. Then, see if your answer
matches the proposed solution. Don't judge the given answer until you've
tried solving it yourself.

Problem:
Jane has 5 apples, then John takes 2 from Mary. Mary originally had 15
apples. John then gives his apples to Jane.

How many apples does Jane have?

Proposed solution:
1. Janes originally had 5 apples.
2. John takes 2 apples from Mary, but this doesn't affect Jane's count.
3. John then gives his apples to Jane.

So, Jane's final count of apples is her original 5 apples plus the 2 apples
that John gave her, for a total of 7 apples.</pre>

<p>Output:</p>

<pre data-type="programlisting">Let's break down the problem:

1. Jane originally has 5 apples.
2. John takes 2 apples from Mary. However, this action doesn't directly
impact the number of apples Jane has at this point.
3. John gives his apples to Jane.

Following these steps, we need to add the 2 apples John gave to Jane to
Jane's original number of apples. Therefore:

5 apples (Jane's original number) + 2 apples (given by John) = 7 apples

So, Jane now has 7 apples. My solution matches the proposed solution.</pre>

<p>In some cases, when using GPT-4 or other generative AI models, you may not want the model’s reasoning process to be visible to the user.</p>

<p>You can achieve this by asking an LLM to <em>generate an inner monologue</em>. This is particularly useful in tutoring applications, where revealing the model’s reasoning might give away the <a data-type="indexterm" data-primary="thinking time" data-startref="thktme" id="id722"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="thinking time" data-startref="lglgmkg" id="id723"/>solution prematurely.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Inner Monologue Tactic"><div class="sect2" id="id48">
<h2>The Inner Monologue Tactic</h2>

<p>The <em>inner monologue tactic</em> instructs the model <a data-type="indexterm" data-primary="inner monologue tactic" id="id724"/>to structure parts of the output that should be hidden from the user in a specific format. This makes it easy to remove these parts before presenting the final output to the user.</p>

<p>Here’s how you can utilize this tactic to answer user queries effectively.</p>

<p>Input:</p>

<pre data-type="programlisting">Step 1: Solve the Problem

First, you need to work out your solution to the problem. Be sure not to
rely on the user's solution, as it might be incorrect. You can encapsulate
your workings within triple quotes (`"""`) to indicate that this section
should be hidden.

Step 2: Compare Solutions

Next, compare your solution with the user's solution. Evaluate whether the
user's solution is correct or not. Again, encapsulate this section within
triple quotes (`"""`).

Step 3: Prepare a Hint

If the user made a mistake, determine an appropriate hint that can help
guide them toward the correct answer, without giving it away outright.
This section should also be within triple quotes (`"""`).

Step 4: Deliver the Hint

If a mistake was made, present the hint from the previous step to the user.
Remember not to use triple quotes for this step. Instead of using a title
like "Step 4", use a heading such as `Hint:`



Problem Statement: A deck of cards has 52 cards: 13 hearts, 13 diamonds, 13
clubs, and 13 spades. If two cards are drawn from the deck without
replacement, what is the probability that one of the drawn cards is a heart
and the other one is a club?

User's Solution: (13/52)*(13/51) = 169/2652 = 13/204</pre>

<p>Output:</p>

<pre data-type="programlisting">"""
Step 1 - My Solution:

First, calculate the probability of drawing 1 heart and 1 club from a
deck of 52 cards without replacement.

In the first draw, the probability of drawing a heart is 13/52.

For the second draw, after removing 1 card (assuming it's a heart), the
total number of cards becomes 51. So, the probability of drawing a club now
is 13/51.

Multiplying these two probabilities gives the probability of both events
happening in sequence,which is P(Heart then Club) = (13/52)*(13/51).

Repeat the calculation assuming the first drawn card is a club and the
second drawn card is a heart. Remember, order matters in this case as
we're dealing with separate events.
"""
"""
Step 2 - Comparison with the User's Solution:

Compare your solution with the user's solution. The user's solution only
considers the sequence of drawing a heart first and then a club. They
have not accounted for the alternate sequence  (Club then Heart), and
thus, the solution is incomplete.
"""
"""
Step 3 - Preparation of Hint:

Given that the user hasn't considered both sequences (Heart then Club
and Club then Heart), a hint at the consideration of both possible
sequences will be helpful. This could nudge the user in the right
direction without revealing the solution outright.
"""
# Step 4 - Delivery of Hint:
# Hint: Remember that the two cards could be drawn in any order. Consider
both possible sequences: Heart then Club and Club then Heart.</pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Self-Eval LLM Responses"><div class="sect2" id="id49">
<h2>Self-Eval LLM Responses</h2>

<p>Another tactic you can use <a data-type="indexterm" data-primary="self-evaluation responses" id="slvrsp"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="self-evaluate responses" id="gglmvr"/>is to <em>critque a generated LLM output</em> and ask whether the LLM missed any information or important facts. You’re essentially asking an LLM to <em>evaluate itself</em> based on its previous output.</p>

<p>First, create a prompt to generate a <code>"Hello World"</code> function.</p>

<p>Input:</p>

<pre data-type="programlisting">Generate a "Hello World" Python function.</pre>

<p>Output:</p>

<pre data-type="programlisting">Sure, here's a very simple Python function that prints "Hello, World!" to
the console:

def print_hello_world():
    print("Hello, World!")

print_hello_world()

When you run this function, it will print the string "Hello, World!" to the
console.</pre>

<p>The LLM returns a simple Python function called <code>print_hello_world()</code> that prints the traditional “Hello, World!” greeting.</p>

<p>Then make a second LLM request with the previous chat history, asking for the initial output to be improved.</p>

<p>Input:</p>

<pre data-type="programlisting">...Previous Chat History...

Given the previous function, can you improve it again with anything else?
Perhaps defaults or type hints?</pre>

<p>Output:</p>

<pre data-type="programlisting">We can add type hints to make it clear that the function doesn't take any
arguments and doesn't return anything. We can also add a docstring to
describe what the function does.

Here's the improved code:

def print_hello_world() -&gt; None:
    """
    Prints "Hello, World!" to the standard output.
    """
    print("Hello, World!")

If you wanted to make the function more flexible, you could add a default
argument that allows you to print a different message:

def print_message(message: str = "Hello, World!") -&gt; None:
    """
    Prints the given message to the standard output. If no message is given
    , it defaults to "Hello, World!".

    Args:
        message (str, optional): The message to be printed.
        Defaults to "Hello, World!".
    """
    print(message)</pre>

<p>Consequently GPT-4 returns an upgraded version of the function, now furnished with Python type hints and a default argument. This enriches the function with greater flexibility, allowing it to print not just <code>"Hello, World!"</code> but any user-specified message.</p>

<p>These prompt-response exchanges <a data-type="indexterm" data-primary="self-evaluation responses" data-startref="slvrsp" id="id725"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="self-evaluate responses" data-startref="gglmvr" id="id726"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="prompting tactics" data-startref="pgpppt" id="id727"/>illustrate how you can easily refine generated LLM outputs until you’re satisfied with the final output.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It’s possible to critique an LLM’s response multiple times, until no further refinement is provided by the LLM.</p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Classification with LLMs"><div class="sect1" id="id50">
<h1>Classification with LLMs</h1>

<p>Classifying, in the context of AI, refers <a data-type="indexterm" data-primary="classification" id="id728"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification" id="id729"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="classifications" id="chppssf"/>to the process of predicting the class or category of a given data point or sample. It’s a common task in machine learning where models are trained to assign predefined labels to unlabeled data based on learned patterns.</p>

<p>LLMs are powerful assets when it comes to classification, even with zero or only a small number of examples provided within a prompt. Why? That’s because LLMs, like GPT-4, have been previously trained on an extensive dataset and now possess a degree of reasoning.</p>

<p>There are two overarching strategies in solving classification problems with LLMs: <em>zero-shot learning</em> and <em>few-shot learning</em>.</p>
<dl>
<dt>Zero-shot learning</dt>
<dd>
<p>In this process, the <a data-type="indexterm" data-primary="classification" data-secondary="zero-shot learning" id="id730"/><a data-type="indexterm" data-primary="zero-shot learning, classification and" id="id731"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification" data-tertiary="zero-shot learning" id="id732"/>LLM classifies data with exceptional accuracy, without the aid of any prior specific examples. It’s akin to acing a project without any preparation—impressive, right?</p>
</dd>
<dt>Few-shot learning</dt>
<dd>
<p>Here, you provide <a data-type="indexterm" data-primary="few-shot learning" data-secondary="classification and" id="id733"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification" data-tertiary="few-shot learning" id="id734"/>your LLM with a small number of examples. This strategy can significantly influence the structure of your output format and enhance the overall classification accuracy.</p>
</dd>
</dl>

<p>Why is this groundbreaking for you?</p>

<p>Leveraging LLMs lets you sidestep lengthy processes that traditional machine learning processes demand. Therefore, you can quickly prototype a classification model, determine a base level accuracy, and create immediate business value.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Although an LLM can perform classification, depending upon your problem and training data you might find that using a traditional machine learning process could yield better results.</p>
</div>
</div></section>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Building a Classification Model"><div class="sect1" id="id51">
<h1>Building a Classification Model</h1>

<p>Let’s explore a few-shot learning example to determine the <a data-type="indexterm" data-primary="classification" data-secondary="few-shot learning" id="id735"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification model building" id="id736"/>sentiment of text into either <code>'Compliment'</code>, <code>'Complaint'</code>, or <code>'Neutral'</code>.</p>

<pre data-type="programlisting">Given the statement, classify it as either "Compliment", "Complaint", or
"Neutral":
1. "The sun is shining." - Neutral
2. "Your support team is fantastic!" - Compliment
3. "I had a terrible experience with your software." - Complaint

You must follow the following principles:
- Only return the single classification word. The response should be either
"Compliment", "Complaint", or "Neutral".
- Perform the classification on the text enclosed within """ delimiters.

"""The user interface is intuitive."""

Classification:</pre>

<pre data-type="programlisting">Compliment</pre>

<p>Several good use cases for LLM classification include:</p>
<dl>
<dt>Customer reviews</dt>
<dd>
<p>Classify user reviews <a data-type="indexterm" data-primary="customer reviews, classification and" id="id737"/>into categories like “Positive,” “Negative,” or “Neutral.” Dive deeper by further identifying subthemes such as “Usability,” “Customer Support,” or “Price.”</p>
</dd>
<dt>Email filtering</dt>
<dd>
<p>Detect the intent or <a data-type="indexterm" data-primary="email filtering, classification and" id="id738"/>purpose of emails and classify them as “Inquiry,” “Complaint,” “Feedback,” or “Spam.” This can help businesses prioritize responses and manage communications efficiently.</p>
</dd>
<dt>Social media sentiment analysis</dt>
<dd>
<p>Monitor <a data-type="indexterm" data-primary="sentiment analysis" data-secondary="social media" id="id739"/><a data-type="indexterm" data-primary="social media, sentiment analysis" id="id740"/>brand mentions and sentiment across social media platforms. Classify posts or comments as “Praise,” “Critic,” “Query,” or “Neutral.” Gain insights into public perception and adapt marketing or PR strategies accordingly.</p>
</dd>
<dt>News article categorization</dt>
<dd>
<p>Given the <a data-type="indexterm" data-primary="news articles, classification and" id="id741"/>vast amount of news generated daily, LLMs can classify articles by themes or topics such as “Politics,” “Technology,” “Environment,” or “Entertainment.”</p>
</dd>
<dt>Résumé screening</dt>
<dd>
<p>For HR departments <a data-type="indexterm" data-primary="resume screening, classification and" id="id742"/>inundated with résumés, classify them based on predefined criteria like “Qualified,” “Overqualified,” “Underqualified,” or categorize by expertise areas <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification model building" data-startref="lmsgfbd" id="id743"/>such as “Software Development,” “Marketing,” or “Sales.”</p>
</dd>
</dl>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be aware that exposing emails, résumés, or sensitive data does run the risk of data being leaked into OpenAI’s future models as training data.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Majority Vote for Classification"><div class="sect1" id="id52">
<h1>Majority Vote for Classification</h1>

<p>Utilizing multiple LLM requests can <a data-type="indexterm" data-primary="classification" data-secondary="majority vote" id="clfsjyv"/><a data-type="indexterm" data-primary="majority vote, classification" id="mjvclss"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification" data-tertiary="majority vote" id="gggmsjy"/>help in reducing the variance of your classification labels. This process, known as <em>majority vote</em>, is somewhat like choosing the most common fruit out of a bunch. For instance, if you have 10 pieces of fruit and 6 out of them are apples, then apples are the majority. The same principle goes for choosing the majority vote in classification labels.</p>

<p>By soliciting several classifications and taking the <em>most frequent classification</em>, you’re able to reduce the impact of potential outliers or unusual interpretations from a single model inference. However, do bear in mind that there can be significant downsides to this approach, including the increased time required and cost for multiple API calls.</p>

<p>Let’s classify the same piece of text three times, and then take the majority vote:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="kn">import</code> <code class="nn">os</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"OPENAI_API_KEY"</code><code class="p">))</code>

<code class="n">base_template</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">Given the statement, classify it as either "Compliment", "Complaint", or</code>
<code class="s2">"Neutral":</code>
<code class="s2">1. "The sun is shining." - Neutral</code>
<code class="s2">2. "Your support team is fantastic!" - Compliment</code>
<code class="s2">3. "I had a terrible experience with your software." - Complaint</code>

<code class="s2">You must follow the following principles:</code>
<code class="s2">- Only return the single classification word. The response should be either</code>
<code class="s2">"Compliment", "Complaint", or "Neutral".</code>
<code class="s2">- Perform the classification on the text enclosed within ''' delimiters.</code>

<code class="s2">'''</code><code class="si">{content}</code><code class="s2">'''</code>

<code class="s2">Classification:</code>
<code class="s2">"""</code>

<code class="n">responses</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">3</code><code class="p">):</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4"</code><code class="p">,</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
            <code class="s2">"content"</code><code class="p">:</code> <code class="n">base_template</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="s1">'''Outside is rainy,</code>
<code class="s1">            but I am having a great day, I just don't understand how people</code>
<code class="s1">            live, I'm so sad!'''</code><code class="p">),}],)</code>
    <code class="n">responses</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">strip</code><code class="p">())</code>

<code class="k">def</code> <code class="nf">most_frequent_classification</code><code class="p">(</code><code class="n">responses</code><code class="p">):</code>
    <code class="c1"># Use a dictionary to count occurrences of each classification</code>
    <code class="n">count_dict</code> <code class="o">=</code> <code class="p">{}</code>
    <code class="k">for</code> <code class="n">classification</code> <code class="ow">in</code> <code class="n">responses</code><code class="p">:</code>
        <code class="n">count_dict</code><code class="p">[</code><code class="n">classification</code><code class="p">]</code> <code class="o">=</code> <code class="n">count_dict</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">classification</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code>

    <code class="c1"># Return the classification with the maximum count</code>
    <code class="k">return</code> <code class="nb">max</code><code class="p">(</code><code class="n">count_dict</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="n">count_dict</code><code class="o">.</code><code class="n">get</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">most_frequent_classification</code><code class="p">(</code><code class="n">responses</code><code class="p">))</code>  <code class="c1"># Expected Output: Neutral</code></pre>

<p>Calling the <code>most_frequent_classification(responses)</code> function should pinpoint <code>'Neutral'</code> as the dominant sentiment. You’ve now learned how to use the OpenAI package for majority vote <a data-type="indexterm" data-primary="classification" data-secondary="majority vote" data-startref="clfsjyv" id="id744"/><a data-type="indexterm" data-primary="majority vote, classification" data-startref="mjvclss" id="id745"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="classification" data-tertiary="majority vote" data-startref="gggmsjy" id="id746"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="classifications" data-startref="chppssf" id="id747"/>classification.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Criteria Evaluation"><div class="sect1" id="id162">
<h1>Criteria Evaluation</h1>

<p>In <a data-type="xref" href="ch01.html#five_principles_01">Chapter 1</a>, a human-based evaluation system was used with a simple thumbs-up/thumbs-down rating system <a data-type="indexterm" data-primary="evaluation" data-secondary="criteria" id="evlucrtr"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="criteria evaluation" id="cppgcvl"/>to identify how often a response met our expectations. Rating manually can be expensive and tedious, requiring a qualified human to judge quality or identify errors. While this work can be outsourced to low-cost raters on services such as <a href="https://www.mturk.com">Mechanical Turk</a>, designing such a task in a way that gets valid results can itself be time-consuming and error prone. One increasingly common approach is to use a more sophisticated LLM to evaluate the responses of a smaller model.</p>

<p>The evidence is mixed on whether LLMs can <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="as evaluators" id="id748"/>act as effective evaluators, with some studies <a href="https://oreil.ly/nfc3f">claiming LLMs are human-level evaluators</a> and others <a href="https://oreil.ly/ykkzY">identifying inconsistencies in how LLMs evaluate</a>. In our experience, GPT-4 is a useful evaluator with consistent results across a diverse set of tasks. In particular, GPT-4 is effective and reliable in evaluating the responses from smaller, less sophisticated models like GPT-3.5-turbo. In the example that follows, we generate concise and verbose examples of answers to a question using GPT-3.5-turbo, ready for rating with GPT-4.</p>

<p>Input:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code>
<code class="kn">import</code> <code class="nn">os</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"OPENAI_API_KEY"</code><code class="p">))</code>

<code class="n">responses</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
    <code class="c1"># concise if even, verbose if odd</code>
    <code class="n">style</code> <code class="o">=</code> <code class="s2">"concise"</code> <code class="k">if</code> <code class="n">i</code> <code class="o">%</code> <code class="mi">2</code> <code class="o">==</code> <code class="mi">0</code> <code class="k">else</code> <code class="s2">"verbose"</code>

    <code class="k">if</code> <code class="n">style</code> <code class="o">==</code> <code class="s2">"concise"</code><code class="p">:</code>
        <code class="n">prompt</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"""Return a </code><code class="si">{</code><code class="n">style</code><code class="si">}</code><code class="s2"> answer to the</code>
<code class="s2">        following question: What is the meaning of life?"""</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">prompt</code> <code class="o">=</code> <code class="sa">f</code><code class="s2">"""Return an answer to the following</code>
<code class="s2">        question: What is the meaning of life?"""</code>

    <code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="c1"># using GPT-3.5 Turbo for this example</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
            <code class="s2">"content"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}])</code>
    <code class="n">responses</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
        <code class="n">response</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">strip</code><code class="p">())</code>

<code class="n">system_prompt</code> <code class="o">=</code> <code class="s2">"""You are assessing the conciseness of a</code>
<code class="s2">response from a chatbot.</code>
<code class="s2">You only respond with a 1 if the response is concise,</code>
<code class="s2">and a 0 if it is not.</code>
<code class="s2">"""</code>

<code class="n">ratings</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="n">response</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">responses</code><code class="p">):</code>
    <code class="n">rating</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4"</code><code class="p">,</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
            <code class="s2">"content"</code><code class="p">:</code> <code class="n">system_prompt</code><code class="p">},</code>
            <code class="p">{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"system"</code><code class="p">,</code>
            <code class="s2">"content"</code><code class="p">:</code> <code class="n">response</code><code class="p">}])</code>
    <code class="n">ratings</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
        <code class="n">rating</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">strip</code><code class="p">())</code>

<code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="n">rating</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ratings</code><code class="p">):</code>
    <code class="n">style</code> <code class="o">=</code> <code class="s2">"concise"</code> <code class="k">if</code> <code class="n">idx</code> <code class="o">%</code> <code class="mi">2</code> <code class="o">==</code> <code class="mi">0</code> <code class="k">else</code> <code class="s2">"verbose"</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Style: </code><code class="si">{</code><code class="n">style</code><code class="si">}</code><code class="s2">, "</code><code class="p">,</code> <code class="sa">f</code><code class="s2">"Rating: </code><code class="si">{</code><code class="n">rating</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0</pre>

<p>This script is a Python program that <a data-type="indexterm" data-primary="OpenAI" data-secondary="response evaluation" id="id749"/>interacts with the OpenAI API to generate and evaluate responses based on their conciseness. Here’s a step-by-step explanation:</p>
<ol>
<li>
<p><code>responses = []</code> creates an empty list named <code>responses</code> to store the responses generated by the OpenAI API.</p>
</li>
<li>
<p>The <code>for</code> loop runs 10 times, generating a response for each iteration.</p>
</li>
<li>
<p>Inside the loop, <code>style</code> is determined based on the current iteration number (<code>i</code>). It alternates between “concise” and “verbose” for even and odd iterations, respectively.</p>
</li>
<li>
<p>Depending on the <code>style</code>, a <code>prompt</code> string is formatted to ask, “What is the meaning of life?” in either a concise or verbose manner.</p>
</li>
<li>
<p><code>response = client.chat.completions.create(...)</code> makes a request to the OpenAI API to generate a response based on the <code>prompt</code>. The model used here is specified as “gpt-3.5-turbo.”</p>
</li>
<li>
<p>The generated response is then stripped of any leading or trailing whitespace and added to the <code>responses</code> list.</p>
</li>
<li>
<p><code>system_prompt = """You are assessing..."""</code> sets up a prompt used for evaluating the conciseness of the generated responses.</p>
</li>
<li>
<p><code>ratings = []</code> initializes an empty list to store the conciseness ratings.</p>
</li>
<li>
<p>Another <code>for</code> loop iterates over each response in <code>responses</code>.</p>
</li>
<li>
<p>For each response, the script sends it along with the <code>system_prompt</code> to the OpenAI API, requesting a conciseness evaluation. This time, the model used is “gpt-4.”</p>
</li>
<li>
<p>The evaluation rating (either 1 for concise or 0 for not concise) is then stripped of whitespace and added to the <code>ratings</code> list.</p>
</li>
<li>
<p>The final <code>for</code> loop iterates over the <code>ratings</code> list. For each rating, it prints the <code>style</code> of the response (either “concise” or “verbose”) and its corresponding conciseness <code>rating</code>.</p>
</li>

</ol>

<p>For simple ratings like conciseness, GPT-4 performs with near 100% accuracy; however, for more complex ratings, it’s important to spend some time evaluating the evaluator. For example, by setting test cases that contain an issue, as well as test cases that do not contain an issue, you can identify the accuracy of your evaluation metric. An evaluator can itself be evaluated by counting the number of false positives (when the LLM hallucinates an issue in a test case that is known not to contain an issue), as well as the number of false negatives (when the LLM misses an issue in a test case that is known to contain an issue). In our example we generated the concise and verbose examples, so we can easily check the rating accuracy, but in more complex examples you may need human evaluators to validate the ratings.</p>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>Using GPT-4 to evaluate the <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="criteria evaluation" id="id750"/>responses of less sophisticated models is an emerging standard practice, but care must be taken that the results are reliable and consistent.</p>
</div>

<p>Compared to human-based evaluation, LLM-based or synthetic evaluation typically costs an order of magnitude less and completes in a few minutes rather than taking days or weeks. Even in important or sensitive cases where a final manual review by a human is necessary, rapid iteration and A/B testing of the prompt through synthetic reviews can save significant time and improve results considerably. However, the cost of running many tests at scale can add up, and the latency or rate limits of GPT-4 can be a blocker. If at all possible, a prompt engineer should first <a data-type="indexterm" data-primary="criteria" data-secondary="evaluating" data-startref="crtvltu" id="id751"/><a data-type="indexterm" data-primary="evaluation" data-secondary="criteria" data-startref="evlucrtr" id="id752"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="criteria evaluation" data-startref="cppgcvl" id="id753"/>test using programmatic techniques that don’t require a call to an LLM, such as simply measuring the length of the response, which runs near instantly for close to zero cost.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Meta Prompting"><div class="sect1" id="id53">
<h1>Meta Prompting</h1>

<p><em>Meta prompting</em> is a technique that <a data-type="indexterm" data-primary="meta prompting" id="mtprpt"/><a data-type="indexterm" data-primary="prompt engineering" data-secondary="meta prompting" id="ppemtpp"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="meta prompting" id="rgmmtpp"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="meta prompting" id="tgptmtpp"/>involves the creation of text prompts that, in turn, generate other text prompts. These text prompts are then used to generate new assets in many mediums such as images, videos, and more text.</p>

<p>To better understand meta prompting, let’s take the example of authoring a children’s book with the assistance of GPT-4. First, you direct the LLM to generate the text for your children’s book. Afterward, you invoke meta prompting by instructing GPT-4 to produce prompts that are suitable for image-generation models. This could mean creating situational descriptions or specific scenes based on the storyline of your book, which then can be given to AI models like Midjourney or Stable Diffusion. These image-generation models can, therefore, deliver images in harmony with your AI-crafted children’s story.</p>

<p><a data-type="xref" href="#figure-3-8">Figure 3-8</a> visually describes the process of meta prompting in the context of crafting a children’s book.</p>

<figure><div id="figure-3-8" class="figure">
<img src="assets/pega_0308.png" alt="Creating image prompts from an LLM that will later be used by MidJourney for image creation." width="600" height="429"/>
<h6><span class="label">Figure 3-8. </span>Utilizing an LLM to generate image prompts for MidJourney’s image creation in the process of crafting a children’s book</h6>
</div></figure>

<p>Meta prompts offer a multitude of benefits for a variety of applications:</p>
<dl>
<dt>Image generation from product descriptions</dt>
<dd>
<p>Meta prompts <a data-type="indexterm" data-primary="image generation" data-secondary="meta prompts and" id="id754"/>can be employed to derive an image generation prompt for image models like <a href="https://www.midjourney.com">Midjourney</a>, effectively creating a visual representation of product descriptions.</p>
</dd>
<dt>Generating style/feature prompts</dt>
<dd>
<p>Let’s consider you <a data-type="indexterm" data-primary="style guide, meta prompting" id="id755"/>are a copywriter needing to develop a unique style guide prompt from a couple of blog posts. Given each client has a distinctive tone and style, it’s beneficial to utilize a <em>meta prompt</em> that encapsulates all the varied features, rather than producing a single prompt output.</p>
</dd>
<dt>Optimizing prompts to achieve specific goals</dt>
<dd>
<p>A common approach is to ask ChatGPT or another language model to refine or improve <code>Prompt A</code> in order to attain <code>Goal 1</code>, given <code>Prompt A</code> and <code>Goal 1</code>. This method aids in identifying any missed input features, that by adding could assist the language model in generating a more detailed and desirable response.</p>
</dd>
</dl>

<p>Let’s experiment with creating a meta prompt for the copywriting example.</p>

<p>Input:</p>

<pre data-type="programlisting">Act as a GPT Prompt Engineer, you are solely responsible for producing
highly effective large language model text prompts given a task.

You must follow the following principles:
- Return only a single prompt, never return the output of a prompt.
- You must follow the task properly.
- You must either refine the prompt or create a new prompt depending upon
the Task.
- Bullet point out any important features so that another Prompt Engineer
can easily copy inputs into the prompt.

Task: You must create a copywriting guide ChatGPT several blog posts.
Include relevant features within the ChatGPT prompt that will allow ChatGPT
to imitate the writer. You must decide what the most important features are
to mimic the writer's style etc. You will only be applying this style to
new blog posts.

Blog post text: You need a variety of soft skills in addition to technical
skills to succeed in the technology sector. Soft skills are used by
software professionals to collaborate with their peers...</pre>

<p>Output:</p>

<pre data-type="programlisting">Prompt:

Based on the extensive text provided, your task is to create a series of
blog posts following the same style and structure of the writer. As a guide
you should follow these features:

1. Clear, direct language: Your writing should be in clear and direct
language, breaking down complex subjects into easily understandable
concepts.
2. Use of subheadings: To provide structure to your blog posts, break them
up with relevant subheadings. This helps to guide the reader and make your
text more readable.
...
9. Inclusion of technical terminology: When discussing technical topics,
use appropriate technical language to ensure accuracy and credibility.
However, be sure to provide explanations or definitions for more complex
terms.</pre>

<p>As previously mentioned, it can be beneficial to <em>critique the model’s output</em> and to ask for a refined prompt either via the interface or API.</p>

<p>ChatGPT can perform web searches and can make use of the content of web pages, along with the previously generated copywriting style guide. Let’s provide two URLs that ChatGPT will read to enhance the output.</p>

<p class="pagebreak-before">Input<a data-type="indexterm" data-primary="blog post" data-secondary="meta prompting" id="id756"/>:</p>

<pre data-type="programlisting">Given the following style guide and these two blog posts, I want you to
improve the copywriting style prompt so an LLM or copywriter can write
effectively for my client.

...Previously Generated Copywriting guide...

URL 1: https://understandingdata.com/posts/how-to-clean-and-process-data
URL 2: https://understandingdata.com/posts/how-to-scrape-twitter-data</pre>

<p><a data-type="xref" href="#figure-3-9">Figure 3-9</a> shows the output.</p>

<figure><div id="figure-3-9" class="figure">
<img src="assets/pega_0309.png" alt="pega 0309" width="579" height="645"/>
<h6><span class="label">Figure 3-9. </span>ChatGPT refining a meta prompt by two URL web pages</h6>
</div></figure>

<pre data-type="programlisting">Based on the style guide and the two blog posts, here's an improved
copywriting style prompt:

To create a blog post for the client, adhere to the following guidelines:

1. Clarity and Directness: Use clear, direct language to explain complex
topics. Present information in an easy-to-understand manner. Aim for your
audience to grasp concepts without needing prior knowledge.
...
8. Use of Technical Terminology: Use the appropriate technical language
when discussing complex topics. However, make sure to provide definitions
or explanations for these terms to keep the text accessible.

Model your writing after these blog post examples:

"What is an API? And How Do They Relate to Data Engineering?": This blog
post uses clear and direct language to explain the concept of an API. It
provides detailed explanations and uses practical examples of commonly used
APIs. It links the technical concept of APIs to business growth and
productivity. The information is well-categorized using subheadings like
'Types of API’s.'

"How To Easily Setup A Google Cloud Project With APIs": This blog post is a
practical guide on setting up a Google Cloud Project. It's broken down into
numbered steps, each with a clear subheading. The post uses detailed
explanations and includes practical examples such as screenshots. It also
uses clear, direct language to guide the reader through the process.</pre>

<p>Meta prompting offers a dynamic and innovative way to harness the power of generative AI models, fostering the creation of complex, multifaceted prompts and even prompts that generate other prompts. It broadens the application scope, from text and image generation to style and feature prompts, and optimization toward specific goals. As you continue to refine and explore the potential of meta prompting, it promises to be a game changer in how you utilize, interact with, and benefit <a data-type="indexterm" data-primary="meta prompting" data-startref="mtprpt" id="id757"/><a data-type="indexterm" data-primary="prompt engineering" data-secondary="meta prompting" data-startref="ppemtpp" id="id758"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="meta prompting" data-startref="rgmmtpp" id="id759"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="meta prompting" data-startref="tgptmtpp" id="id760"/>from using LLMs.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id350">
<h1>Summary</h1>

<p>After reading this chapter, you are now aware of how crucial it is to give clear directions and examples to generate desired outputs. Also, you have hands-on experience extracting structured data from a hierarchical list using regular expressions in Python, and you’ve learned to utilize nested data structures like JSON and YAML to produce robust, parsable outputs.</p>

<p>You’ve learned several best practices and effective prompt engineering techniques, including the famous “Explain it like I’m five”, role prompting, and meta prompting techniques. In the next chapter, you will learn how to use a popular LLM package called LangChain that’ll help you to create more advanced prompt engineering 
<span class="keep-together">workflows.</span></p>
</div></section>
</div></section></div></div></body></html>