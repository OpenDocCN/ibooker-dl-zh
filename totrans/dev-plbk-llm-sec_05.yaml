- en: Chapter 5\. Can Your LLM Know Too Much?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2023, a rash of companies began banning or heavily restricting the usage
    of LLM services, like ChatGPT, based on concerns about possible leaks of confidential
    data. A partial list of such companies includes Samsung, JPMorgan Chase, Amazon,
    Bank of America, Citigroup, Deutsche Bank, Wells Fargo, and Goldman Sachs. These
    actions by giant finance and tech corporations show substantial concern about
    LLMs disclosing confidential and sensitive information, but how critical is the
    risk? As the developer of an LLM application, do you need to care?
  prefs: []
  type: TYPE_NORMAL
- en: In the Tay story from [Chapter 1](ch01.html#chatbots_breaking_bad), Microsoft’s
    chatbot was attacked by hackers. As bad as the damage was, it was limited because
    Tay didn’t have access to much sensitive data she could have disclosed. However,
    the intersection of LLMs with real-world data can harbor the potential of unintended
    information disclosure, as seen in cases where employees have inadvertently fed
    sensitive business data to ChatGPT, which then became integrated into the system’s
    training base so that others could discover it.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will dig into the various ways that LLMs acquire access to data.
    We will examine the three predominant knowledge acquisition methods and the risks
    associated with your LLM having this access. Along the way, we’ll try to answer
    the question “Can your LLM know too much?” and discuss how to mitigate the risks
    associated with your application disclosing sensitive, private, or confidential
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Real-World Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s examine two examples of the impacts seen in the real world. We’ll start
    with a chatbot example, which was somewhat similar to Tay, except the damage was
    much more significant due to the data to which the chatbot had access and how
    it was disclosed. Then we’ll look at a copilot example that put its owner at elevated
    legal and reputational risk.
  prefs: []
  type: TYPE_NORMAL
- en: Lee Luda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Seoul-based start-up Scatter Lab, also briefly mentioned in [Chapter 1](ch01.html#chatbots_breaking_bad),
    faced severe legal and reputational repercussions due to its irresponsible handling
    of personal data. The company operated a popular app called Science of Love, which
    helped users analyze their compatibility with a romantic partner by analyzing
    their text messages. This service accumulated 9.4 billion conversations from 600,000
    users. The company later introduced Lee Luda, [“an A.I. chatbot that people prefer
    as a conversation partner over a person.”](https://oreil.ly/PDF3e) Lee Luda used
    Science of Love’s massive dataset as its training base—without applying any proper
    sanitization. Not only did Lee Luda exhibit some of the toxic behavior we saw
    from Tay, but, more concerning, she began to leak sensitive data such as users’
    names, private nicknames, and home addresses.
  prefs: []
  type: TYPE_NORMAL
- en: South Korea’s Personal Information Protection Commission imposed a fine of 103.3
    million won (around US$93k) on Scatter Lab for failing to obtain proper user permissions,
    marking a precedent in penalizing AI technology firms for data mismanagement in
    South Korea.
  prefs: []
  type: TYPE_NORMAL
- en: 'There was substantial impact from this incident. Let’s look at the various
    facets:'
  prefs: []
  type: TYPE_NORMAL
- en: Public exposure of sensitive data
  prefs: []
  type: TYPE_NORMAL
- en: The exposure of sensitive data jeopardized user privacy, revealing personal
    information like names, locations, relationship statuses, and medical information.
  prefs: []
  type: TYPE_NORMAL
- en: Financial penalty
  prefs: []
  type: TYPE_NORMAL
- en: Scatter Lab incurred a substantial fine for neglecting to manage user data responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: Reputational damage
  prefs: []
  type: TYPE_NORMAL
- en: The incident significantly tarnished Scatter Lab’s reputation, as evidenced
    by mainstream press coverage and a deluge of negative reviews on Google Play,
    especially targeting the Science of Love app.
  prefs: []
  type: TYPE_NORMAL
- en: Service discontinuation
  prefs: []
  type: TYPE_NORMAL
- en: The offending chatbot service, Lee Luda, was shut down following the incident,
    halting the company’s expansion plans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s examine the lessons you can learn and apply to your own projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Stringent data privacy protocols
  prefs: []
  type: TYPE_NORMAL
- en: This incident highlights the imperative for robust data privacy protocols to
    ensure user data is handled with the utmost care and within legal frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: User consent
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining explicit and informed consent before collecting and processing users’
    data is legally mandated and a cornerstone of ethical data practices.
  prefs: []
  type: TYPE_NORMAL
- en: Age verification mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the damage was more severe because some of the data gathered by
    Science of Love belonged to minors. Data mining from minors requires special care
    in many regulatory environments.
  prefs: []
  type: TYPE_NORMAL
- en: Public awareness
  prefs: []
  type: TYPE_NORMAL
- en: Companies must be transparent with users regarding how they will utilize data
    and effectively communicate the risks.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and auditing
  prefs: []
  type: TYPE_NORMAL
- en: Regular monitoring and auditing of data handling practices can help identify
    and rectify privacy issues promptly, mitigating the risk of sensitive data exposure.
  prefs: []
  type: TYPE_NORMAL
- en: This account emphasizes the delicate balance between leveraging user data to
    enhance LLM capabilities and ensuring the stringent safeguarding of user privacy
    and data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot and OpenAI’s Codex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A notable incident in 2023 highlighted the risks associated with sensitive data
    disclosure through LLMs involving GitHub Copilot, a tool powered by OpenAI’s Codex
    model. GitHub designed Copilot to assist developers by autocompleting code, a
    feat achieved by training on a vast corpus of code from GitHub’s public repositories.
    However, the tool soon found itself in a quagmire of legal and ethical challenges.
    Some developers discovered Copilot suggesting snippets of their copyrighted code—despite
    the original code being under a license that restricted such use. This possible
    copyright violation sparked a lawsuit against GitHub, Microsoft, and OpenAI, with
    the developers alleging copyright, contract, and privacy violations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The case unfolded in a US district court. The developers’ argument hinged on
    two primary claims: Codex’s ability to reproduce portions of their code breached
    software licensing terms and violated the Digital Millennium Copyright Act by
    reproducing copyrighted code without the necessary copyright management information.
    The judge denied a motion to dismiss these two claims, keeping the lawsuit alive.
    While the court rejected some allegations, the crux of the case revolved around
    the potential infringement of the developers’ intellectual property rights due
    to the reproduction of code by Codex and Copilot.'
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, the lawsuit is still being litigated, and we may not know
    the full impact for some time. The lawsuit underscores a critical concern in the
    field of LLMs—the potential for unintentional sensitive data disclosure. The repercussions
    extended beyond the parties involved, resonating across the tech industry and
    sparking discussions on the legal and ethical implications of LLMs accessing and
    learning from publicly available data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the full intellectual property issues raised by this case are not
    yet fully settled, there are several lessons you can take from this and apply
    to your own projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Data governance
  prefs: []
  type: TYPE_NORMAL
- en: This incident emphasized the importance of robust data governance frameworks,
    underscoring the need for clear guidelines on data usage, especially concerning
    publicly available or open source data.
  prefs: []
  type: TYPE_NORMAL
- en: Legal clarity
  prefs: []
  type: TYPE_NORMAL
- en: The case illuminated the legal gray areas surrounding the interaction of LLMs
    with real-world data, suggesting a need for more explicit laws and regulations
    defining the bounds of permissible data usage and copyright adherence.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical engagement
  prefs: []
  type: TYPE_NORMAL
- en: Beyond legal compliance, the ethical dimensions of data usage by LLMs call for
    a conscientious approach by developers and organizations, respecting both the
    letter and spirit of open source contributions and licensing agreements.
  prefs: []
  type: TYPE_NORMAL
- en: User awareness
  prefs: []
  type: TYPE_NORMAL
- en: The incident also highlighted the importance of user awareness regarding how
    corporations might utilize their data, suggesting a precedent for more transparent
    disclosures by organizations employing LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The unfolding of this lawsuit provides a real-world tableau illustrating the
    complex interplay of legal, ethical, and technical factors in the domain of LLM
    applications. It is a harbinger of the challenges (particularly concerning sensitive
    data disclosure risks) to come as LLMs evolve and interact with diverse data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Acquisition Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The power of your LLM application will grow with the amount of data it has access
    to. At the same time, risks associated with that data also increase. If your LLM
    has been exposed to data of a particular type, you’ll need to manage the risk
    of disclosure. Let’s look at three common ways that LLMs acquire knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Central to an LLM’s knowledge base is its *model training*. The process begins
    with *foundation model training*, where the LLM immerses itself in vast datasets,
    acquiring a broad grasp of language, context, and worldly insights. This foundational
    knowledge can then be refined through *model fine-tuning*, adapting the LLM to
    cater to more specialized tasks or niche domains using targeted datasets.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs learn in a distinct, infrequent training phase, which means their information
    is often out of date, and that limits their use in applications that require up-to-date
    knowledge. This is where *retrieval-augmented generation* (RAG) comes into play.
    LLMs can venture into the expansive realms of the public web, garner real-time
    updates, or dive deep into structured or unstructured databases. Further amplifying
    their knowledge spectrum, LLMs can connect with external systems, databases, or
    online platforms via APIs, enriching their responses with a wealth of external
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications can go even further. User interactions like queries, conversations,
    and feedback enable LLMs to acquire new knowledge continuously. Processing these
    inputs allows the LLM to expand its understanding, refining its capabilities with
    each interaction and delivering increasingly personalized and relevant responses.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these categories—training, retrieval-augmented generation, and user
    interaction—possesses nuances that can significantly influence the security landscape
    of your LLM application. While they serve as conduits for knowledge acquisition,
    they also introduce potential vulnerabilities and challenges that need careful
    consideration. As we progress through this chapter, we’ll probe each category
    to expose the crucial security implications inherent in each method. Through this
    exploration, we aim to equip you with a comprehensive understanding of the potential
    risks and the measures to mitigate them.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training is a crucial step in developing and refining LLMs. It encompasses
    two distinct phases: creating the foundation model and its subsequent fine-tuning.
    The *foundation model training* establishes broad linguistic and contextual understanding,
    while *fine-tuning* hones this generalized knowledge for specific tasks or domains.
    In this section, we’ll explore the intricacies of both these phases, emphasizing
    their respective methodologies. Following this, we’ll expand on the crucial security
    implications inherent in each step, equipping you with insights into potential
    vulnerabilities and best practices for safeguarding against them.'
  prefs: []
  type: TYPE_NORMAL
- en: Foundation Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Foundation model training is the initial step in building an LLM. In this phase,
    the model is trained on a vast and diverse dataset, often encompassing various
    topics, languages, and text formats. The objective is to equip the model with
    a broad understanding of language, contextual relationships, and general world
    knowledge. This foundational training forms the base upon which the LLM can generate
    coherent, contextually relevant, and informed responses, akin to a basic understanding
    of the world, much like a human before specializing in a particular field.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, the process of foundation model training an LLM is a sophisticated
    exercise in pattern recognition. Training involves using advanced algorithms to
    analyze vast datasets, identify relationships between words, understand context,
    and generate coherent responses based on this understanding. Let’s look at the
    steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Pattern recognition
  prefs: []
  type: TYPE_NORMAL
- en: The training foundation feeds the model vast text data—sometimes billions of
    tokens. As it processes this data, the model learns to recognize patterns. For
    instance, it starts to understand that the word “apple” can be associated with
    “fruit,” “tree,” “pie,” or “technology,” depending on the context.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Contextual understanding
  prefs: []
  type: TYPE_NORMAL
- en: Next, the model starts discerning the nuanced differences in word usage based
    on context. It learns, for example, that the phrase “Apple’s growth” can refer
    to the expansion of a tech company or the development of fruit on a tree, based
    on surrounding words and phrases. Training algorithms will adjust internal parameters,
    often numbering billions, to capture these intricate contextual relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Response generation
  prefs: []
  type: TYPE_NORMAL
- en: The model’s ability to generate responses is developed through repeated iterations
    of training, continuously refining its understanding of language and context.
    Unlike human memory recall, the model analyzes input, matches it with learned
    patterns, understands the context, and constructs a response based on training
    data. The diversity and breadth of the training data are critical, as they directly
    influence the model’s capability to produce accurate and contextually appropriate
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: Security Considerations for Foundation Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding steps show why training a custom foundation model can be complex
    and costly. That’s why most projects today start with an existing foundation model.
    The starting point might be a proprietary model accessed via a SaaS (software
    as a service) product, such as OpenAI’s GPT series, or a privately hosted open
    source model, such as Meta’s Llama. In either of those cases, the foundation model’s
    creator has hopefully done some level of work to ensure that things like personally
    identifiable information (PII) are kept out of the training base, although that
    might not always be the case. Choose your foundation model carefully! Even with
    the best intentions, there are numerous examples of these foundational models
    accumulating sensitive information that might be inappropriate in some contexts.
    A few examples of potentially problematic information types to look out for include:'
  prefs: []
  type: TYPE_NORMAL
- en: Someone else’s intellectual property, such as copyrighted text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dangerous or illegal information related to weapons, drugs, or other topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cultural or religious texts that may be inappropriate in specific contexts or
    discussions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you decide to train your own foundational model, you can achieve a higher
    level of control over many aspects of your system. This control may be highly
    advantageous. However, you’re now assuming some responsibility for every part
    of the training data you use in your model. Keeping it free of sensitive information
    may prove a significant challenge for you. We’ll discuss this more later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model fine-tuning is an optional step following foundation model training, aimed
    at specializing a general-purpose model for specific tasks or domains. You will
    use a smaller, domain-specific dataset to adjust the model’s weights during fine-tuning.
    This way, you can refine its responses to perform well in the targeted application.
    This process significantly enhances the model’s performance, making it more relevant
    and accurate for the intended use case. The specialized data used for fine-tuning
    allows the model to adapt its generalized understanding acquired during foundation
    training to the nuances and specifics of the task, providing a more tailored and
    effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, fine-tuning addresses a fundamental challenge in machine learning:
    while foundational models have broad knowledge, they often need more depth and
    specificity for particular tasks. For example, while a general model might have
    been trained using some medical information, it might generate responses at a
    different level of precision than those expected by medical professionals. Fine-tuning
    bridges this gap by adapting the general knowledge of the foundational model to
    a specific domain or task.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether training a foundation model from scratch or fine-tuning an existing
    model, you must carefully consider the risks of incorporating sensitive data into
    your training set. Any data used in training your model might become a long-term
    memory. And even with attempts to align your model and provide guardrails against
    inappropriate disclosure, your model might disclose this information to a third
    party.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some risks you’ll want to consider as you craft the dataset for training
    your model:'
  prefs: []
  type: TYPE_NORMAL
- en: Direct data leakage
  prefs: []
  type: TYPE_NORMAL
- en: If you expose a model to PII or confidential information during training, it
    might generate outputs that inadvertently disclose this data.
  prefs: []
  type: TYPE_NORMAL
- en: Inference attacks
  prefs: []
  type: TYPE_NORMAL
- en: An attacker might use prompt injection to extract sensitive data from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory and compliance violations
  prefs: []
  type: TYPE_NORMAL
- en: Training models with a dataset that includes PII, especially without user consent,
    can lead to breaches of data protection regulations like the Health Insurance
    Portability and Accountability Act (HIPAA), the General Data Protection Regulation
    (GDPR), or the California Consumer Privacy Act (CCPA). This can result in hefty
    fines and legal consequences, not to mention reputational damage.
  prefs: []
  type: TYPE_NORMAL
- en: Loss of public trust
  prefs: []
  type: TYPE_NORMAL
- en: If it becomes public knowledge that a corporation trained its model with PII
    or confidential data and can leak such data, the organization might face significant
    backlash and loss of trust.
  prefs: []
  type: TYPE_NORMAL
- en: Compromised data anonymization
  prefs: []
  type: TYPE_NORMAL
- en: Even if PII is “anonymized” before training, models might still discern patterns
    that allow data de-anonymization, particularly if they correlate inputs with other
    publicly available datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Increased attractiveness as a target
  prefs: []
  type: TYPE_NORMAL
- en: If malicious actors believe that a model contains confidential information or
    PII, they might be more motivated to launch sophisticated attacks against it,
    aiming to extract valuable data.
  prefs: []
  type: TYPE_NORMAL
- en: Model rollbacks and financial implications
  prefs: []
  type: TYPE_NORMAL
- en: If a team later discovers that a model was previously trained using PII, it
    might need to roll back to a previous version, leading to financial implications
    and project delays.
  prefs: []
  type: TYPE_NORMAL
- en: Given these significant risks, it’s crucial to ensure that data used in training
    is thoroughly sanitized. Furthermore, periodic audits, rigorous data vetting,
    and advanced differential privacy techniques can help mitigate potential risks.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a transformative approach in LLM data acquisition and response generation.
    Instead of solely relying on a vast internal knowledge base acquired from training,
    as traditional LLMs do, RAG first retrieves relevant document snippets or *passages*
    from an external dataset. Then, the LLM utilizes these passages to inform its
    generated responses. This two-step approach—retrieving relevant information and
    then developing an answer based on that retrieval—allows the model to pull in
    real-time or more updated information that wasn’t part of its original training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is a significant leap forward in the ability of language models to handle
    large amounts of real-time data. No matter how expansive their training data,
    traditional LLMs are inherently limited to their last training cutoff, making
    them potentially outdated for specific topics or real-time events. RAG solves
    this limitation by allowing LLMs to access and integrate external, up-to-date
    information seamlessly. This dynamic capability enhances the accuracy and relevance
    of the model’s outputs and positions LLMs to be more versatile and adaptive in
    rapidly evolving domains. The ability to fuse retrieval and generation processes
    promises a new frontier of more informed and context-aware conversational AI.
  prefs: []
  type: TYPE_NORMAL
- en: However, attaching your LLM to large, live data stores opens up a Pandora’s
    box of security considerations. One issue is indirect prompt injection, which
    we discussed in [Chapter 4](ch04.html#prompt_injection). Prompt injection attacks
    are possible when you feed your LLM untrusted data as part of a RAG prompt. But,
    for this chapter, we’ll focus on the risks associated with sensitive data disclosure
    to help answer the question “Can your LLM know too much?”
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review some common ways a RAG system gets access to larger data stores.
    By understanding how your LLM might access these knowledge bases, we can better
    plan for the security risks and considerations. Here, we’ll look at accessing
    data directly from the web and accessing databases.
  prefs: []
  type: TYPE_NORMAL
- en: Direct Web Access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Providing your LLM with a direct connection to the web can be a powerful mechanism
    to get real-time or updated information to augment its knowledge base. A web connection
    enables the model to fetch the latest data, stay current with evolving topics,
    and provide more accurate and up-to-date responses. By interacting with the web,
    the LLM can bridge the gap between its last training cutoff and the present, ensuring
    its information is relevant and timely. This feature significantly enhances the
    utility of LLMs in dynamic or rapidly changing domains.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a couple of patterns for accessing the web.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping a specific URL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Directly accessing a predetermined URL to extract its content is a particularly
    useful approach when you know the exact source of the information you want the
    LLM to access. This technique is appropriate for many cases, such as extracting
    daily stock prices from a financial news website, pulling regular updates from
    a specific news source or blog, or retrieving product details or reviews from
    an ecommerce site.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these types of use cases, there are several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs: []
  type: TYPE_NORMAL
- en: Targets the desired web page, eliminating potential noise from unrelated sources.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Since the URL is predetermined, you can optimize the scraping process for that
    page’s specific structure and content.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs: []
  type: TYPE_NORMAL
- en: Consistently accessing a single or a set of known URLs can provide more stable
    results over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are also some critical challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Page structure changes
  prefs: []
  type: TYPE_NORMAL
- en: Web pages often undergo redesigns or structural changes. If the specific URL’s
    content structure changes, the scraping mechanism might need adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: Access restrictions
  prefs: []
  type: TYPE_NORMAL
- en: Some websites use CAPTCHAs, rate limits, or *robots.txt* restrictions to prevent
    or limit automated access.
  prefs: []
  type: TYPE_NORMAL
- en: Legal or ethical challenges
  prefs: []
  type: TYPE_NORMAL
- en: If you do not own the content on the web page you’re scraping, you must consider
    whether the owner of that page could object to how you’re using that data within
    your system. Consider copyrights and other licensing terms as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Using a search engine followed by content scraping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this method, you issue a search query to a platform like Google or Bing to
    find relevant content based on specific keywords or topics and then scrape the
    content from one or more top search results. This approach is most appropriate
    for use cases such as researching current public sentiment on a specific topic
    or product by scraping top news articles or blogs, retrieving recent academic
    publications or articles on a particular subject, and understanding market trends
    by analyzing the top results for industry-specific keywords.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these types of use cases, there are several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Relevance
  prefs: []
  type: TYPE_NORMAL
- en: Search engines rank content based on relevance, ensuring the LLM accesses high-quality
    and pertinent information.
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  prefs: []
  type: TYPE_NORMAL
- en: Search engines constantly index new content, making them a valuable resource
    for obtaining recent information on a topic.
  prefs: []
  type: TYPE_NORMAL
- en: Diversity
  prefs: []
  type: TYPE_NORMAL
- en: By accessing multiple top results, LLMs can gain a more comprehensive understanding
    of a topic from various perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenges include:'
  prefs: []
  type: TYPE_NORMAL
- en: Indirect prompt injection
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 4](ch04.html#prompt_injection), malicious prompts may
    not come directly from users. They may be secretly embedded into data included
    in a prompt in a RAG system. In this case, an attacker may embed malicious data
    within a web page, leading to an indirect prompt injection attack when the page
    is parsed and data is included in a prompt passed to the LLM by the application.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic results
  prefs: []
  type: TYPE_NORMAL
- en: Search results for a particular query can change over time, introducing variability
    in the content the LLM accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Search limitations
  prefs: []
  type: TYPE_NORMAL
- en: Search engines may have request limits, especially for automated queries, which
    could restrict the number of searches.
  prefs: []
  type: TYPE_NORMAL
- en: Depth of scraping
  prefs: []
  type: TYPE_NORMAL
- en: Deciding how many top results to scrape can affect the quality and breadth of
    information. Scraping too many might dilute the relevance; scraping too few might
    miss out on valuable perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Legal and ethical concerns
  prefs: []
  type: TYPE_NORMAL
- en: When scraping content, it’s important to abide by search engines’ terms of service
    and consider copyright and licensing terms.
  prefs: []
  type: TYPE_NORMAL
- en: Example risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Direct web access or search engines carry various risks related to the unintentional
    acquisition or disclosure of PII and other sensitive information. Here are some
    examples of how this might happen:'
  prefs: []
  type: TYPE_NORMAL
- en: Comment sections and forums
  prefs: []
  type: TYPE_NORMAL
- en: A model might scrape a technical article or news piece from a reputable source,
    but in doing so, it could also unintentionally pull in comments or forum posts
    attached to the article. These sections often contain personal anecdotes, email
    addresses, or other identifiable details. For example, a user might ask the LLM
    for recent discussions on a particular health topic. The model could pull data
    from a health forum where users have shared personal stories, names, ages, or
    even specific medical details.
  prefs: []
  type: TYPE_NORMAL
- en: User profiles
  prefs: []
  type: TYPE_NORMAL
- en: Some websites include user profiles or author bios at the end of articles or
    posts. Scraping such sites might accidentally gather personal details or contacts
    in these profiles. For example, an LLM fetching entries from a blogging platform
    might also scrape the author’s bio, including their full name, location, workplace,
    and email address.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden data in web pages
  prefs: []
  type: TYPE_NORMAL
- en: Some web pages store metadata or secret information in the background. While
    this data may not be visible to human viewers, an LLM with web access might still
    access and process it. For example, an LLM scraping a corporate website might
    unintentionally access embedded metadata that contains internal document paths
    or even confidential revision comments.
  prefs: []
  type: TYPE_NORMAL
- en: Inaccurate or broad search queries
  prefs: []
  type: TYPE_NORMAL
- en: When using search engines, if the queries are too broad or not accurately defined,
    the model might pull in unrelated content that contains sensitive information.
    For example, a query like “John Doe’s presentation” intended to find a public
    lecture by a notable figure might also yield results from a different John Doe’s
    blog where he shared his phone number for contact.
  prefs: []
  type: TYPE_NORMAL
- en: Advertisements and sponsored content
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping might inadvertently gather data from ads or sponsored posts that
    can sometimes contain personalized content based on prior user behavior or other
    targeted criteria. For example, an LLM scraping news from a web page might also
    pull in an ad that says “Special deals for residents of [location],” revealing
    location data.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic content and pop-ups
  prefs: []
  type: TYPE_NORMAL
- en: Many websites have dynamic content that changes based on user interaction, location,
    or time. Pop-up surveys, chatbots, or feedback forms can contain prompts for personal
    information. For example, in scraping a service provider’s web page, the LLM might
    pull a pop-up content asking, “Are you from [city]? Answer this survey!” which
    can disclose geolocation details.
  prefs: []
  type: TYPE_NORMAL
- en: Document metadata and properties
  prefs: []
  type: TYPE_NORMAL
- en: When accessing online documents or files, their associated metadata can contain
    author names, editing histories, or internal comments. For example, the LLM might
    pull a company’s public financial report, but along with it, the properties might
    show “last edited by [employee name] from [department],” revealing internal company
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing a Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This pattern involves an LLM retrieving data stored in structured or unstructured
    databases. This approach can include querying traditional databases for specific
    data or accessing vector databases for embeddings. By leveraging databases, LLMs
    can provide precise and data-driven responses, making them significantly more
    valuable in scenarios requiring real-time or historical data retrieval. This method
    of knowledge acquisition allows LLMs to operate in data-rich environments and
    provide highly accurate, context-aware, and personalized responses based on the
    data available in the databases.
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relational databases have been the de facto standard since the late 1970s, underpinning
    the infrastructure of countless industries and applications. They revolutionized
    how developers organize and access data using tables and ensure data integrity
    through established relationships. Their structured approach to data management,
    paired with the power of SQL (Structured Query Language) for data manipulation,
    enabled organizations to handle complex datasets efficiently and precisely. While
    modern technological advancements have brought forth new types of databases, the
    robustness of relational databases continues to make them a trusted choice for
    many enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: 'Giving your LLM access to the vast data stores inside your enterprise is powerful
    and thus tempting. The advantages are clear: instant access to enormous amounts
    of historical and real-time data allows for richer, more informed responses tailored
    to specific organizational needs and contexts. The LLM can provide insights, answer
    intricate queries, or even automate tasks that would otherwise take hours for
    a human to compile. It can transform the user experience, offering a seamless
    interface between vast data repositories and end users, whether employees, stakeholders,
    or customers. However, with this immense power comes an equally tremendous responsibility
    to safeguard sensitive information and ensure data access remains securely and
    ethically managed. Let’s examine risk areas related to accessing databases as
    part of your LLM application:'
  prefs: []
  type: TYPE_NORMAL
- en: Complex relationships amplify exposure
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases link structured datasets through relationships. While one
    table might seem benign, its linkage to another could inadvertently reveal sensitive
    patterns. For instance, an innocent list of product IDs can become sensitive when
    linked to specific customer transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Unintended queries
  prefs: []
  type: TYPE_NORMAL
- en: A misinterpreted command or a poorly phrased question could lead the LLM to
    fetch data the developer didn’t intend the user to access. Imagine a scenario
    where a casual inquiry inadvertently brings up a detailed record, revealing more
    than was asked.
  prefs: []
  type: TYPE_NORMAL
- en: Permission oversights
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases have intricate permission systems. In the integration process,
    an LLM might be granted broader access than necessary due to oversight or misconfiguration,
    opening doors to data that should remain restricted.
  prefs: []
  type: TYPE_NORMAL
- en: Inadvertent data inferences
  prefs: []
  type: TYPE_NORMAL
- en: LLMs identify patterns. Over multiple interactions, they might collate seemingly
    nonsensitive data, leading to unintended sensitive insights. For example, while
    individual purchases might not disclose much, a pattern might hint at a company’s
    upcoming product launch or a shift in strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Auditability and accountability challenges
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases traditionally offer robust audit trails, tying actions
    to specific users. With LLMs as intermediaries, ensuring that every query and
    data fetch remains traceable is vital. Without clear audit trails, pinpointing
    the origin of a data breach or understanding unexpected behaviors becomes intricate.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, integrating LLMs with trusted relational databases can improve
    functionality and performance. Still, it is important to use these integrations
    with an awareness of the associated risks. Implementing stringent safeguards and
    oversight can harness the LLM’s capabilities while ensuring data integrity and
    security.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Vector databases* represent a significant evolution in the way we think about
    and handle data, particularly in the context of machine learning and AI operations.
    Unlike relational databases that organize data into rows and columns, vector databases
    store data as high-dimensional vectors. These vectors are arrays of numbers that
    effectively capture the essence of objects or data points in terms of their features
    or attributes. This structure is advantageous for performing similarity or proximity-based
    operations in a vector space.'
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional vectors are adept at handling complex operations like *nearest
    neighbor* searches, which are crucial for many AI applications. These searches
    allow the database to quickly find data points closest to a given query point
    in the vector space, facilitating operations that rely on finding the most similar
    items or patterns. By managing data as *vectors*—essentially mathematical representations
    that encode information about data items—vector databases excel in rapidly retrieving
    and comparing data, thereby enabling efficient and accurate similarity searches
    across vast datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating your LLM with vector databases via the RAG pattern can supercharge
    its capabilities. By linking the model to these databases, you can harness the
    power of similarity-based searches, allowing for contextually richer responses
    that are more attuned to nuanced user queries. The model can swiftly locate and
    leverage embeddings that resonate with the query’s intent, serving accurate and
    relevant results. For certain this is revolutionary. Let’s examine some examples
    where combining a vector database with the RAG pattern can produce excellent results:'
  prefs: []
  type: TYPE_NORMAL
- en: Question answering systems
  prefs: []
  type: TYPE_NORMAL
- en: Users expect precise and accurate responses when answering questions. RAG systems
    can retrieve relevant documents or data snippets from the vector DB to inform
    the LLM’s responses, making the answers more accurate and detailed than those
    generated from the model’s knowledge alone.
  prefs: []
  type: TYPE_NORMAL
- en: Content recommendation
  prefs: []
  type: TYPE_NORMAL
- en: For platforms requiring personalized content recommendations—such as news aggregators,
    streaming services, and ecommerce websites—RAG can enhance recommendation engines
    by retrieving content from the vector DB that closely matches user profiles or
    previous interactions, thus improving user engagement and satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Academic research and summarization
  prefs: []
  type: TYPE_NORMAL
- en: RAG systems can significantly speed up the research process by retrieving relevant
    documents from the vector DB and providing summaries or connections between them.
  prefs: []
  type: TYPE_NORMAL
- en: Customer support
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots can pull from FAQs, product manuals, and customer interaction logs
    to provide support agents or automated chatbots with the information needed to
    answer customer inquiries effectively and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Legal and compliance review
  prefs: []
  type: TYPE_NORMAL
- en: For applications requiring review of large volumes of legal or regulatory documents,
    RAG can quickly retrieve relevant documents based on queries, thereby aiding in
    compliance checks or legal research.
  prefs: []
  type: TYPE_NORMAL
- en: Medical information systems
  prefs: []
  type: TYPE_NORMAL
- en: In health care, RAG can support diagnostic processes, patient management, and
    medical research by retrieving patient records, scientific studies, and clinical
    trial results relevant to a doctor’s query or a specific medical condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'This architecture has great power. However, the dynamic nature of vector databases
    and their unique data-handling mechanisms present security challenges that development
    teams must address:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding reversibility
  prefs: []
  type: TYPE_NORMAL
- en: While embeddings in vector databases are abstract numerical representations,
    there is a risk that sophisticated techniques might reverse engineer these embeddings,
    revealing the sensitive information from which they were derived. For instance,
    embeddings created from confidential documents might have unique patterns that
    can hint at the document’s content.
  prefs: []
  type: TYPE_NORMAL
- en: Information leakage via similarity searches
  prefs: []
  type: TYPE_NORMAL
- en: Similarity searches, the core advantage of vector databases, can pose a risk
    in the context of sensitive data disclosure. An attacker might infer certain sensitive
    aspects about the dataset by analyzing the results of proximity-based queries.
    If, for instance, a user finds that specific queries yield close matches, they
    might deduce the nature or specifics of the data behind the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Data granularity and vector representations
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the granularity of the embeddings, specific patterns or clusters
    in the vector space might indirectly disclose information about the nature of
    the data. For instance, if particular data points are always clustered together,
    it might reveal relationships or characteristics about the original data.
  prefs: []
  type: TYPE_NORMAL
- en: Interactions with other systems
  prefs: []
  type: TYPE_NORMAL
- en: Often, vector databases aren’t standalone but interact with other systems. The
    flow of embeddings or derived vectors between systems can become a point of exposure,
    especially if data lineage and flow aren’t securely managed.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, while vector databases enhance the capabilities of LLMs by offering
    a nuanced, similarity-based approach to data, it’s paramount to be vigilant about
    potential avenues of sensitive data disclosure. These databases’ very strengths
    can be leveraged by malicious actors if not safeguarded adequately. Understanding
    these risks and taking proactive measures will be essential in maintaining the
    integrity and confidentiality of the data they manage.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing database risk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some ideas for best practices and mitigation strategies for reducing
    the risks of sensitive data exposure when connecting your LLM to a database:'
  prefs: []
  type: TYPE_NORMAL
- en: Role-based access control (RBAC)
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the LLM has restricted access to the database. Grant only the necessary
    permissions and avoid giving the LLM blanket access. Using roles, you can ensure
    the LLM can pull only the information that it absolutely needs.
  prefs: []
  type: TYPE_NORMAL
- en: Data classification
  prefs: []
  type: TYPE_NORMAL
- en: Categorize your data based on sensitivity (public, internal, confidential, restricted).
    Ensure that LLMs have no access or limited, sanitized access to high-sensitivity
    data categories.
  prefs: []
  type: TYPE_NORMAL
- en: Audit trails
  prefs: []
  type: TYPE_NORMAL
- en: Maintain logs of every database query made by the application. Review these
    logs regularly to identify patterns, anomalies, or unintended data access.
  prefs: []
  type: TYPE_NORMAL
- en: Data redaction and masking
  prefs: []
  type: TYPE_NORMAL
- en: For sensitive fields, consider using redaction (completely hiding the data)
    or masking (obfuscating part of the data) to limit the exposure of sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Input sanitization
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that any queries or inputs processed by the LLM to access the database
    are sanitized and checked to prevent SQL injection or other data manipulation
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Automated data scanners
  prefs: []
  type: TYPE_NORMAL
- en: Use automated tools to scan and flag sensitive information, ensuring such data
    is removed or adequately safeguarded before the LLM can access it.
  prefs: []
  type: TYPE_NORMAL
- en: Use views instead of direct table access
  prefs: []
  type: TYPE_NORMAL
- en: For relational databases, consider providing the LLM with access to views that
    are sanitized versions of tables, rather than giving access to the actual raw
    tables.
  prefs: []
  type: TYPE_NORMAL
- en: Data retention policies
  prefs: []
  type: TYPE_NORMAL
- en: Implement policies that dictate how long a database should retain certain data.
    Regularly purge data that is no longer needed to reduce the potential data exposure
    footprint.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from User Interaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While simple LLMs don’t modify their behaviors based on usage, we now see increasingly
    common scenarios where developers add this capability. By processing queries,
    feedback, or other forms of input from users, LLMs can refine their understanding,
    provide more accurate responses, and even learn new information over time. This
    dynamic interaction allows the LLM to stay updated, learn from user feedback,
    and tailor its responses to individual or collective user preferences, thus enhancing
    the user experience and the utility of the LLM in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#chatbots_breaking_bad), we saw one type of risk associated
    with directly incorporating untrusted user input into your LLM’s knowledge base.
    In that case, Microsoft’s Tay picked up toxic language and bias. However, there
    is another set of risks related to sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: When an LLM continually interacts with diverse users, there’s a potential influx
    of sensitive data, intentionally or inadvertently. While the learning capacity
    of an LLM ensures it evolves and becomes more efficient over time, this continuous
    learning can also be its Achilles’ heel when it comes to data protection. The
    very nature of user interaction, being diverse and unpredictable, means there’s
    a potential for users to input or reference personal, confidential, or proprietary
    information.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider a business executive using an LLM to draft a message.
    They might feed the system snippets of confidential business strategies, expecting
    a more polished output. We’ve seen real-world scenarios of this at Samsung and
    other major corporations. Or, a user might query the LLM with personal medical
    symptoms, hoping for insights into potential conditions. In both situations, the
    user shared sensitive data with your application. If you’re using any of this
    data in future training or storing it for real-time access, this information could
    become part of the LLM’s internal knowledge structure, or your application could
    store it for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the challenge with user interactions is that the LLM might only
    sometimes recognize sensitive data when it sees it. Whereas a human might realize
    the importance of a Social Security number, proprietary formula, or a unique business
    strategy, an LLM might treat it as just another piece of information. This lack
    of understanding could lead to scenarios where an LLM, when queried later by another
    user on a related topic, might inadvertently disclose fragments of the previously
    fed sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, with the rise of multimodal LLMs that can process not just text but
    also images, audio, and video, the potential for sensitive data disclosure multiplies.
    A user might input a photo for image recognition, not realizing that the background
    contains identifiable information or copyrighted material.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these issues, employ the following mitigation strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Clear communication
  prefs: []
  type: TYPE_NORMAL
- en: Users should be informed about the LLM’s learning capabilities and data retention
    policies. An initial disclaimer about not sharing personal or sensitive information
    can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Data sanitization
  prefs: []
  type: TYPE_NORMAL
- en: Implement algorithms that identify and remove potential PII or other sensitive
    data from user inputs before processing.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary memory
  prefs: []
  type: TYPE_NORMAL
- en: Consider giving the LLM a temporary memory for user-specific information that
    the system automatically erases after the session ends, ensuring no long-term
    retention of sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: No persistent learning
  prefs: []
  type: TYPE_NORMAL
- en: Design the LLM so it doesn’t persistently learn from user interactions, thus
    minimizing the risk of internalizing sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core question of this chapter was “Can your LLM know too much?” The answer
    is clearly yes. We need our LLMs to have access to information to be helpful.
    However, we must carefully evaluate what types of information we provide to these
    systems and view that information through a lens asking, “What happens if this
    information is disclosed?” If the penalty for unintentional disclosure is too
    high, then you must carefully weigh the risk of training or equipping your model
    with such data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We studied the three main avenues through which LLMs acquire their vast knowledge:
    training, retrieval-augmented generation, and user interaction. Each method came
    with its own advantages and unique challenges when guarding against unintentional
    data exposure. Key insights garnered include:'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of LLMs. While training equips LLMs with vast knowledge, it is
    imperative to vet training data meticulously, eliminating any traces of PII, proprietary
    insights, or controversial content. Periodic audits and employing data sanitization
    strategies are nonnegotiable.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-augmented generation
  prefs: []
  type: TYPE_NORMAL
- en: A bridge between the LLM and the vast sea of unstructured data on the web. The
    power of real-time data comes with the responsibility of filtering out sensitive
    or misleading information. When accessing APIs or databases, setting stringent
    access controls is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from user interaction
  prefs: []
  type: TYPE_NORMAL
- en: The most dynamic knowledge source. Every user query carries the potential of
    revealing personal or corporate secrets. Protecting against this necessitates
    clear user communication, data sanitization, and judicious use of persistent learning.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, your LLM’s ability to process vast knowledge stores can be of
    substantial value, but that’s also where the danger may lie. The key is to balance
    empowering LLMs with ensuring they don’t inadvertently “know too much.” This chapter
    was dedicated to understanding this delicate balance, hoping to guide readers
    in harnessing the power of LLMs responsibly, ensuring they are both potent tools
    and trustworthy guardians of sensitive information.
  prefs: []
  type: TYPE_NORMAL
