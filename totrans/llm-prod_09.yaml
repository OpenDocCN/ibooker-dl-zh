- en: '10 *Creating a coding copilot project: This would have helped you earlier*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploying a coding model to an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a VectorDB locally and using it for a retrieval-augmented generation
    system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a VS Code extension to use our LLM service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insights and lessons learned from the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Progress doesn’t come from early risers—progress is made by lazy men looking
    for easier ways to do things.—Robert Heinlein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you touch code for your day job, you’ve probably dreamed about having an
    AI assistant helping you out. In fact, maybe you already do. With tools like GitHub
    Copilot out on the market, we have seen LLMs take autocomplete to the next level.
    However, not every company is happy with the offerings on the market, and not
    every enthusiast can afford them. So let’s build our own!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build a Visual Studio Code (VS Code) extension that
    will allow us to use our LLM in the code editor. The editor of choice will be
    VS Code, as it is a popular open source code editor. Popular might be an understatement,
    as the Stack Overflow 2023 Developer Survey showed it’s the preferred editor for
    81% of developers.[¹](#footnote-129) It’s essentially a lightweight version of
    Visual Studio, which is a full IDE that’s been around since 1997\.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond just choosing a specific editor, we will also make some other judicious
    decisions to limit the scope of the project and make it more meaningful. For example,
    in the last project, we focused on building an awesome LLM model we could deploy.
    In this project, we will instead be starting with an open source model that has
    already been trained on coding problems. To customize it, instead of finetuning,
    we’ll build a RAG system around it, which will allow us to keep it up to date
    more easily. Also, since we aren’t training our own model, we’ll focus on building
    a copilot that is good at Python, the main language we’ve used throughout this
    book, and not worry about every language out there.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a clear idea of what we are building and a goal in mind, let’s
    get to it!
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Our model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we are only going to be focusing on Python, we decided to use DeciCoder
    as our model. DeciCoder is a commercial open source model that has only 1B parameters.[²](#footnote-130)
    Despite its tiny size, it’s really good at what it does. It has been trained on
    the Stack dataset but filtered to only include Python, Java, and JavaScript code.
    It’s only trained on three languages, which would typically be a limitation, but
    it is actually part of the secret sauce of why it’s so good despite its small
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Some other limitations to be aware of are that it only has a context window
    of 2,048 tokens, which isn’t bad for a model of this size, but it is relatively
    small when we consider that we plan to use a RAG system and will need to give
    it examples of code. Code samples tend to be quite large, which limits what we
    can do and how many examples we can give it.
  prefs: []
  type: TYPE_NORMAL
- en: A bigger problem using DeciCoder with RAG is that the model wasn’t instruction
    tuned. Instead, it was designed to beat the HumanEval dataset ([https://github.com/openai/human-eval](https://github.com/openai/human-eval)).
    In this evaluation dataset, a model is given only a function name and docstring
    describing what the function should do. From just this input, the model will generate
    functioning code to complete the function. As a result, it’s hard to know if giving
    the model more context from a RAG system will help it, but we’re going to go ahead
    and try to find out!
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, its tiny size actually makes it an interesting choice for another reason.
    Because it’s so small, we could potentially put the model right inside the VS
    Code extension we are building, using compiling methods we’ve discussed in other
    chapters. This would allow us to build a very compact application! We won’t be
    doing that in this book, mostly because it would require us to write a lot of
    JavaScript. That’s a problem because we only expect our readers to be familiar
    with Python, so it’s a tad too adventurous here to explain the details in-depth,
    but we leave it as an exercise for the readers who are JavaScript pros.
  prefs: []
  type: TYPE_NORMAL
- en: What we will do instead is serve our model as an API that you can run locally
    and will be able to call from the extension. In listing 10.1, we create a simple
    FastAPI service to serve our model. In fact, most of this code you’ve already
    seen back in chapter 6, and we have only made a few slight changes. The first
    is that we have changed the code to use the DeciCoder model and tokenizer. The
    second is a bit more involved, but we have added `stop` tokens. These are tokens
    that will inform the model to stop generating when it runs into them. This is
    done by creating a `StoppingCriteria` class. The tokens we have chosen will make
    a bit more sense once we’ve defined our prompt, but essentially, we are looking
    to have our model create one function at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 A simple FastAPI endpoint using DeciCoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Torch settings'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Δefines the stopping behavior'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loads tokenizer and models'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Runs FastAPI'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 RAG will go here.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Generates response'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Starts service; defaults to localhost on port 8000'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming this listing is in a Python script server.py, you can start up the
    server by running `$` `python` `server.py`. Once you have it up and running, let’s
    go ahead and make sure it’s working correctly by sending it a request. In a new
    terminal, we can send the API a `curl` request with a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The response should be a simple Python function to complete a “Hello World”
    function. The response we got back from the server was `return` `f"Hello` `{name}!"`.
    So far, so good! Next, we’ll customize the API to utilize a RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Data is king
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have decided on a model, let’s prepare a dataset for our RAG system.
    RAG is an effective way to introduce context to our model without having to finetune
    it; it also allows us to customize the results based on our data. Essentially,
    RAG is a good system to follow if you want your model to know the context of your
    organization’s ever-changing code base. It’s great to have a model that’s good
    at coding, but we want it to be good at *our* code. We want it to use the right
    variable names and import custom dependencies built in-house—that sort of thing.
    In this section, we’ll set up a VectorDB, upload a Python coding dataset, and
    then update the API we just built to utilize it all.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Our VectorDB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can really dive into our dataset, we need to first set up our infrastructure.
    Of course, if your dataset is small enough, it is possible to load it into memory
    and run similarity search with tools like Faiss or USearch directly in Python,
    but where’s the fun in that? Plus, we want to show you Milvus.
  prefs: []
  type: TYPE_NORMAL
- en: Milvus is an awesome open source VectorDB that competes with the big players
    in this space. You can run it locally or across a large cloud cluster, so it scales
    easily to your needs. If you’d rather not deal with the setup, there are managed
    Milvus clusters available. One of my favorite features is its GPU-enabled version,
    which makes vector search lightning fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, the community has also made Milvus extremely approachable and easy
    to set up. In fact, the standalone version only requires Docker to run and comes
    with a startup script to make it even easier. Since we are going to run everything
    locally for this project, we will use the standalone version (to learn more, see
    [https://mng.bz/aVE9](https://mng.bz/aVE9)). To do so, we need to run the following
    commands in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first command will download a shell script, and the second will run it.
    This script is really only out of convenience since the Docker `run` command gets
    rather long. It also includes two more commands you should know about. The `Stop`
    command, which will stop your Milvus docker container, is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: and the `delete` command, which will delete all the data from your computer
    when you no longer wish to keep it, is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You don’t need to run those yet, but remember them for when we are done. Now
    that we have our database set up, let’s make it useful and load some data into
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Our dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If this were a workshop, we’d show you how to write a script to pull your organization’s
    code from GitHub and use that to augment your prompts. We could even set up a
    GitHub Actions pipeline to update our VectorDB with your code whenever it merges
    into the main branch. But since we don’t have access to your code and this is
    only a book, we’ll do the reasonable thing and use an open source dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We will choose the Alpaca dataset for our project. The Alpaca dataset was compiled
    by Stanford when it trained the model of the same name using distillation and
    GPT-3 as the mentor model. Since it’s synthetic data, the dataset is extremely
    clean, making it easy to work with. In fact, it’s so easy that multiple copies
    online have already filtered out all the Python code examples. This subset comprises
    18.6K Python coding challenges, consisting of a task or instruction and generated
    code—perfect for what we are trying to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 10.2, we create our pipeline to load the dataset into Milvus. We
    create a `PythonCodeIngestion` class to handle the details of chunking our dataset
    and uploading it in batches. Note that we use the `krlvi/sentence-t5-base-nlpl-code_search_`
    `net` embedding model. This embedding model has been specifically trained on the
    CodeSearchNet dataset ([https://github.com/github/CodeSearchNet](https://github.com/github/CodeSearchNet))
    and is excellent for creating meaningful embeddings of code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 A data pipeline to ingest Alpaca
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Connects to Milvus'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our ingestion class created, we can move forward with the pipeline.
    First, we’ll need to create our collection if this is the first time we’ve run
    it. A collection is like a table in other databases or an index in Pinecone. We’ll
    define our schema, which is simply an ID field, our embeddings field, and a metadata
    field, which contains freeform JSON. Once that’s set, we’ll upload our data using
    our `PythonCodeIngestion` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create our search index. The index type we’ll use is `IVF_FLAT`,
    which is the most basic index in Milvus and splits the embedding space into `nlist`
    number of clusters. This accelerates the similarity search by first comparing
    our search embedding against the cluster centers and then against the embedding
    in the cluster it is closest to. We will also use `L2` for our metric type, which
    means we’ll be using Euclidean distance. These are common settings, but we don’t
    need anything special for our dataset. Milvus supports a larger selection of options
    when building an index, and we encourage you to check out their documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a collection if it doesn’t exist'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Connects to the collection and shows its size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Ingests data and shows the stats now that data is ingested'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Builds the search index'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The number of clusters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything is set up, we are good to move on to the next step. But
    first, let’s test it by running a query. We’ll want to make sure our data and
    index are giving us reasonable search results. With Milvus, we’ll first load the
    collection into memory and convert our query into an embedding with our embedder.
    Next, we’ll define some search parameters. Again, `L2` stands for Euclidean distance,
    and the `nprobe` parameter states how many clusters to search. In our case, of
    the 128 clusters we set up, we’ll search the 10 closest ones to our query embedding.
    Lastly, in the actual search, we’ll limit our results to the three best matches
    and return the metadata field along with our queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Before conducting a search, you need to load the data into memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Makes a query'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The number of clusters to search'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that for our query, the search results are returning strong candidates
    from our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our VectorDB set up with data loaded in, let’s update our API
    to retrieve results from our RAG system and inject the context into our prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Using RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will update listing 10.1 to include our retrieval code.
    In listing 10.3, we won’t be repeating everything we did before, in the interests
    of time and space, but will simply be showing the new parts to add. In the repo
    accompanying this book, you’ll be able to find the code that puts everything together
    if you are struggling to understand which piece goes where. First, near the top
    of the script, we’ll need to add our imports, connect to our Milvus service, and
    load our embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 Adding RAG to our API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Connects to Milvus'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Loads our embedding model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll add some convenience functions, including a token counter and a
    FastAPI lifecycle, to ensure we load and release our Milvus collection from memory.
    Since we are adding a lifecycle, be sure to update the FastAPI call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Load collection on startup'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Releases collection from memory on shutdown'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs FastAPI'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have all that set up, we can get to the good part—running the query
    and updating our prompt in our `generate` endpoint. The first part should look
    familiar since we just did it. We’ll encode the user’s prompt and search our collection
    for the nearest neighbors. We’re using all the same search parameters as before,
    except one. We increase our limit from `3` to `5` to potentially add more examples
    to our prompt. Next, we take those results and format them into a few-shot prompt
    example dataset. Then we create our instruction prompt and format the user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are almost at the point where we can combine our instruction, examples,
    and user prompt; however, we need to ensure our examples don’t take up too much
    space. Using a `for` loop utilizing our token counter, we’ll filter out any examples
    that don’t fit our context window. With that, we can now combine everything to
    create our final prompt for our DeciCoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Inside the generate function'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Makes a query'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright! Now that we’ve made our updates to our API, let’s start it up and
    test it again like we did before. We’ll send another request to the server to
    make sure everything is still working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This time we got a response of `print(“Hello,` `World!”)`, which is slightly
    worse than our previous response, but it’s still in the same vein, so there’s
    nothing to be worried about. You’ll likely get something similar. And that concludes
    setting up our LLM service with a RAG system for customization. All we need to
    do now is call it.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Build the VS Code extension
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alright, now all we need to do is build our VS Code extension. VS Code extensions
    are written primarily in TypeScript or JavaScript (JS). If you aren’t familiar
    with these languages, don’t worry; we’ll walk you through it. To get started,
    you’ll need Node and npm installed. Node is the JS interpreter, and npm is like
    pip for JS. You can add these tools in multiple ways, but we recommend first installing
    nvm or another node version manager. It’s also a good idea at this time to update
    your VS Code (or install it if you haven’t already). Updating your editor will
    help you avoid many problems, so be sure to do it. From here, we can install the
    VS Code extension template generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'NOTE  You can find the instructions to install nvm here: [https://mng.bz/gAv8](https://mng.bz/gAv8).
    Then simply run `nvm` `install` `node` to install the latest versions of Node
    and npm.'
  prefs: []
  type: TYPE_NORMAL
- en: The template generator will create a basic “Hello World” project repo for us
    that we can use as scaffolding to build off of. To run the generator, use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This command will start a walkthrough in your terminal, where you’ll be greeted
    by what appears to us to be an ASCII art representation of a Canadian Mountie
    who will ask you several questions to customize the scaffolding being generated.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 10.1, you can see an example with our selected answers to the walkthrough
    questions. Guiding you through the questions quickly, we’ll create a new JavaScript
    extension, which you can name whatever you like. We chose `llm_coding_ copilot`,
    if you’d like to follow along with us. For the identifier, press Enter, and it
    will hyphenate the name you chose. Give it a description; anything will do. No,
    we don’t want to enable type-checking. You can choose whether to initialize the
    project as a new Git repository. We chose No, since we are already working in
    one. Lastly, we’ll use npm.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 The VS Code extension generator with example inputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'When it’s done, it will generate a project repository with all the files we
    need. If you look at figure 10.2, you can see an example of a built project repository.
    It has several different configuration files, which you are welcome to familiarize
    yourself with, but we only care about two of these files: the package.json file
    where we define the extension manifest, which tells VS Code how to use the extension
    we will build to (well, actually extend VS Code), and the extension.js file, which
    holds the actual extension code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Example directory structure created with the VS Code extension generator
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the package.json file, the boilerplate gets us almost all the way there,
    but the `activationEvents` field is currently empty and needs to be set. This
    field tells VS Code when to start up our extension. Extensions typically aren’t
    loaded when you open VS Code, which helps keep it lightweight. If it’s not set,
    the extension will only be loaded when the user opens it, which can be a pain.
    A smart strategy typically is to load the extension only when the user opens a
    file of the type we care about—for example, if we were building a Python-specific
    extension, it would only load when a .py file is opened.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `"onCommand:editor.action.inlineSuggest.trigger"` event trigger.
    This trigger fires when a user manually asks for an inline suggestion. It typically
    fires whenever a user stops typing, but we want more control over the process
    to avoid sending unnecessary requests to our LLM service. There’s just one problem:
    VS Code doesn’t have a default shortcut key for users to manually do this! Thankfully,
    we can set this too by adding a `"keybindings"` field to the `"contributes"` section.
    We will set it to the keybindings of `Alt+S`. We are using `S` for “suggestion”
    to be memorable; this keybinding should be available unless another extension
    is using it. Users can always customize their keybindings regardless. You can
    see the finished package.json file in the following listing. It should look very
    similar to what we started with from the scaffolding.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Extension manifest for our coding copilot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have an extension manifest file, let’s go ahead and test it. From
    your project repo in VS Code, you can press F5 to compile your extension and launch
    a new VS Code Extension Development Host window with your extension installed.
    In the new window, you should be able to press Alt+S to trigger an inline suggestion.
    If everything is working, then you’ll see a console log in the original window
    that states, `Congratulations,` `your` `extension` `"llm-coding-copilot"` `is`
    `now` `active!`, as shown in figure 10.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Example console of successfully activating our VS Code extension
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Alright, not bad! We can now both run our extension and activate it, as well
    as capture the logs, which is helpful for debugging. Now all we need to do is
    build it, so let’s turn our attention to the extension.js file.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, things get a bit tricky to explain. Even for our readers who
    are familiar with JavaScript, it’s unlikely many are familiar with the VS Code
    API ([https://mng.bz/eVoG](https://mng.bz/eVoG)). Before we get into the weeds,
    let’s remind ourselves what we are building. This will be an extension in VS Code
    that will give us coding suggestions. We already have an LLM trained on code data
    behind an API that is ready for us. We have a dataset in a RAG system loaded to
    give context and improve results, and we have our prompt crafted. All we need
    to do is build the extension that will call our API service. But we also want
    something that allows users an easy way to interact with our model that gives
    us lots of control. We will do this by allowing a user to highlight portions of
    the code, and we’ll send that when our shortcut keybindings, Alt+S, are pressed.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the template extension.js file that the generator created
    for us. Listing 10.5 shows us the template with the comments changed for simplicity.
    It simply loads the vscode library and defines `activate` and `deactivate` functions
    that run when you start the extension. The `activate` function demonstrates how
    to create and register a new command, but we won’t be using it. Instead of a command,
    we will create an inline suggestion provider and register it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 Boilerplate extension.js from template
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Since we won’t be using commands, let’s take a look at what we will be using
    instead, an inline suggestion provider. This provider will add our suggestions
    as ghost text where the cursor is. This allows the user to preview what is generated
    and then accept the suggestion with a tab or reject it with another action. Essentially,
    it is doing all the heavy lifting for the user interface in the code completion
    extension we are building.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 10.6, we show you how to create and register a provider, which returns
    inline completion items. It will be an array of potential items the user may cycle
    through to select the best option, but for our extension, we’ll keep things simple
    by only returning one suggestion. The provider takes in several arguments that
    are automatically passed in, like the document the inline suggestion is requested
    for, the position of the user’s cursor, context on how the provider was called
    (manually or automatically), and a cancel token. Lastly, we’ll register the provider,
    telling VS Code which types of documents to call it for; here, we give examples
    of registering it to only Python files or adding it to everything.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Example inline suggestion provider
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a provider, we need a way to grab the user’s highlighted text
    to send it to the LLM service and ensure our provider only runs when manually
    triggered via the keybindings, not automatically, which happens every time the
    user stops typing. In listing 10.7, we add this piece to the equation inside our
    provider.
  prefs: []
  type: TYPE_NORMAL
- en: First, we grab the editor window and anything selected or highlighted. Then
    we determine whether the provider was called because it was automatically or manually
    triggered. Next, we do a little trick for a better user experience. If our users
    highlight their code backward to forward, the cursor will be at the front of their
    code, and our code suggestion won’t be displayed. So we’ll re-highlight the selection,
    which will put the cursor at the end, and retrigger the inline suggestion. Thankfully,
    this retriggering will also be counted as a manual trigger. Lastly, if everything
    is in order—the inline suggestion was called manually, we have highlighted text,
    and our cursor is in the right location—then we’ll go ahead and start the process
    of using our LLM code copilot by grabbing the highlighted text from the selection.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Working with the VS Code API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Alright! Now that we have all the VS Code–specific code out of the way, we just
    need to make a request to our LLM service. This action should feel like familiar
    territory at this point; in fact, we’ll use the code we’ve already discussed in
    chapter 7\. Nothing to fear here! In the next listing, we finish the provider
    by grabbing the highlighted text and using an async `fetch` request to send it
    to our API. Then we take the response and return it to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Sending a request to our coding copilot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that all the pieces are in place, let’s see it in action. Press F5 again
    to compile your extension anew, launching another VS Code Extension Development
    Host window with our updated extension installed. Create a new Python file with
    a .py extension, and start typing out some code. When you’re ready, highlight
    the portion you’d like to get your copilot’s help with, and press Alt+S to get
    a suggestion. After a little bit, you should see some ghost text pop up with the
    copilot’s suggestion. If you like it, press Tab to accept. Figure 10.4 shows an
    example of our VS Code extension in action.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Example console of successfully activating our VS Code extension
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Congratulations! You did it! You created your very own coding copilot! It runs
    on your own data and is completely local—a pretty big achievement if you started
    this book knowing nothing about LLMs. In the next section, we’ll talk about next
    steps and some lessons learned from this project.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Lessons learned and next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have working code, we could call it a day. However, our project
    is far from completed; there’s still so much we could do with it! To begin, the
    results don’t appear to be all that great. Looking back at figure 10.4, the generated
    code doesn’t reverse a linked list but reverses a regular ol’ list. That’s not
    what we wanted. What are some things we could do to improve it?
  prefs: []
  type: TYPE_NORMAL
- en: Well, for starters, remember our test “Hello World” functions we sent to the
    API to test it out? It seemed we got better results when using the model before
    we added RAG. For fun, let’s spin up our old API with RAG disabled and see what
    we get while using our VS Code extension. Figure 10.5 shows an example result
    of using this API.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Results of our extension using DeciCoder without RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Wow! That code looks way better! It actually reverses a linked list and is already
    formatted in such a way you wouldn’t even need to edit or format it. What’s going
    on here? Aren’t models supposed to generate better results when we give them a
    few examples of how we want them to behave? Maybe our RAG system isn’t finding
    very good examples. Let’s do some digging and take a look at the prompt generated
    from our RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: MA**Instruction:** What is the most efficient way to reverse a singly linked
    list in 7 lines of Python code?
  prefs: []
  type: TYPE_NORMAL
- en: '![chatGpt](../Images/chatGpt.png)**Output:** # Definition for singly-linked
    list.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: MA**Instruction:** What is the most efficient way to reverse a linked list in
    Python?
  prefs: []
  type: TYPE_NORMAL
- en: '**![chatGpt](../Images/chatGpt.png)**Output:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'MA**Instruction:** def reverse_linked_list(list):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Reverses a linked list"""'
  prefs: []
  type: TYPE_NORMAL
- en: '**![chatGpt](../Images/chatGpt.png)**Output:****'
  prefs: []
  type: TYPE_NORMAL
- en: Wow! Those examples seem to be spot on! What exactly could be going on then?
  prefs: []
  type: TYPE_NORMAL
- en: Well, first, take a look at the prompt again. The example instructions from
    our dataset are tasks in plain English, but the prompt our users will be sending
    is half-written code. We’d likely get better results if our users wrote in plain
    English. Of course, that’s likely a bit of an awkward experience when our users
    are coding in an editor. It’s more natural to write code and ask for help on the
    hard parts.
  prefs: []
  type: TYPE_NORMAL
- en: Second, remember our notes on how DeciCoder was trained? It was trained to beat
    the HumanEval dataset, so it’s really good at taking code as input and generating
    code as output. This makes it good at the task from the get-go without the need
    for prompt tuning. More importantly, it hasn’t been instruction tuned! It’s likely
    a bit confused when it sees our few-shot examples since it didn’t see input like
    that during its training. Being a much smaller model trained for a specific purpose,
    it’s just not as good at generalizing to new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few key takeaways to highlight from this. First and foremost, while
    prompt tuning is a powerful technique to customize an LLM for new tasks, it is
    still limited in what you can achieve with it alone, even when using a RAG system
    to give highly relevant examples. One has to consider how the model was trained
    or finetuned and what data it was exposed to. In addition, it’s important to consider
    how a user will interact with the model to make sure you are crafting your prompts
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: So what are some next steps you can try to improve the results? At this stage,
    things appear to be mostly working, so the first thing we might try is adjusting
    the prompt in our RAG system. It doesn’t appear that the instruction data written
    in plain English is very useful to our model, so we could simply try giving the
    model example code and see if that improves the results. Next, we could try to
    finetune the model to take instruction datasets or just look for another model
    entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond just making our app work better, there are likely many next steps to
    customize this project. For example, we could create a collection in Milvus with
    our own code dataset. This way, we could inject the context of relevant code in
    our code base into our prompt. Our model wouldn’t just be good at writing general
    Python code but also code specific to the organization we work for. If we go down
    that route, we might as well deploy our API and Milvus database to a production
    server where we could serve it for other engineers and data scientists in the
    company.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we could abandon the customization idea and use DeciCoder alone
    since it appears to already give great results. No customization needed. If we
    do that, it would be worth compiling the model to GGUF format and running it via
    the JavaScript SDK directly in the extension. Doing so would allow us to encapsulate
    all the code into a single place and make it easier to distribute and share.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you might consider publishing the extension and sharing it with the
    community. Currently, the project isn’t ready to be shared, since we are running
    our model and RAG system locally, but if you are interested, you can find the
    official instructions online at [https://mng.bz/GNZA](https://mng.bz/GNZA). It
    goes over everything from obtaining API keys, to packaging, publishing, and even
    becoming a verified publisher.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DeciCoder is a small but mighty model designed for coding tasks in Python, JavaScript,
    and Java.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Milvus is a powerful open source VectorDB that can scale to meet your needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your dataset is key to making your RAG system work, so spend the time cleaning
    and preparing it properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual Studio Code is a popular editor that makes it easy to build extensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just throwing examples and data at your model won’t make it generate better
    results, even when they are carefully curated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build prompts in a way that accounts for the model’s training methodology and
    data to maximize results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) D. Ramel, “Stack Overflow dev survey: VS Code, Visual
    Studio still top IDEs 5 years running,” Visual Studio Magazine, June 28, 2023,
    [https://mng.bz/zn86](https://mng.bz/zn86).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) Deci, “Introducing DeciCoder: The new gold standard
    in efficient and accurate code generation,” August 15, 2023, [https://mng.bz/yo8o](https://mng.bz/yo8o).'
  prefs: []
  type: TYPE_NORMAL
