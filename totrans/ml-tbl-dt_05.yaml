- en: 4 Classical algorithms for tabular data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 表格数据的经典算法
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to Scikit-learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn简介
- en: Exploring and processing features of the Airbnb NYC dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和处理Airbnb纽约市数据集的特征
- en: Some classic machine learning techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些经典的机器学习技术
- en: Depending on the problem, classic machine learning algorithms are often the
    most practical approach to working with tabular data. With decades of research
    and practice behind these tools and algorithms, there is a rich palette of solutions
    to choose from.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题不同，经典机器学习算法通常是处理表格数据的最实用方法。这些工具和算法背后有着数十年的研究和实践，提供了丰富的解决方案供选择。
- en: In this chapter, we’ll cover essential algorithms in classical machine learning
    for making predictions using tabular data. We have focused on the linear models
    because they are still the most common solutions for both a challenging baseline
    and a solid and robust model in production. In addition, discussing linear models
    helps us build concepts and ideas that we can find in deep learning architectures
    and in more advanced machine learning algorithms, such as gradient-boosting decision
    trees (which will be one of the topics of the next chapter).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍用于使用表格数据进行预测的经典机器学习中的基本算法。我们专注于线性模型，因为它们仍然是挑战性基线和生产中稳健模型的常见解决方案。此外，讨论线性模型有助于我们构建可以在深度学习架构和更高级的机器学习算法（如梯度提升决策树，这是下一章的主题之一）中找到的概念和思想。
- en: We’ll also give you a quick introduction to Scikit-learn, a powerful and versatile
    machine learning library that we’ll use to continue exploring the Airbnb NYC dataset.
    We’ll stay away from lengthy mathematical definitions and textbook details in
    favor of examples and practical recommendations for applying these models to tabular
    data problems.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为您快速介绍Scikit-learn，这是一个强大且多功能的机器学习库，我们将使用它继续探索Airbnb纽约市数据集。我们将避免冗长的数学定义和教科书细节，而是提供示例和实际建议，以将这些模型应用于表格数据问题。
- en: 4.1 Introducing Scikit-learn
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 Scikit-learn简介
- en: 'Scikit-learn is an open-source library for classic machine learning. It started
    in 2007 as a Google Summer of Code project by David Cournapeau, and it later became
    part of the SciKits (short for Scipy Toolkits: [https://projects.scipy.org/scikits.html](https://projects.scipy.org/scikits.html))
    until the INRIA (Institut National de Recherche en Informatique et en Automatique)
    and its foundation took the leadership of the project and of its development.
    We provide a short example of how Scikit-learn can quickly solve most machine
    learning problems. In our starting example,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn是一个用于经典机器学习的开源库。它始于2007年，是David Cournapeau的一个Google Summer of Code项目，后来成为SciKits（Scipy工具包的简称：[https://projects.scipy.org/scikits.html](https://projects.scipy.org/scikits.html)）的一部分，直到INRIA（法国国家信息与自动化研究所）及其基金会接管了项目及其开发。我们提供了一个简短的例子，说明Scikit-learn如何快速解决大多数机器学习问题。在我们的起始示例中，
- en: We create a synthetic dataset for a classification problem with a binary balanced
    target with half labels positive and half negative.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个用于分类问题的合成数据集，目标具有二进制平衡标签，一半为正标签，一半为负标签。
- en: We set up a pipeline standardizing the features and passing them to a logistic
    regression model, one of the simplest and most effective statistical-based machine
    learning algorithms for classification problems.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了一个管道，标准化特征并将它们传递给逻辑回归模型，这是分类问题中最简单和最有效的基于统计的机器学习算法之一。
- en: We evaluate its performance using cross-validation.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用交叉验证来评估其性能。
- en: Finally, assured by the cross-validation results that our work with the problem
    is fine, we train a model on all the available data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在交叉验证结果确保我们处理问题的方法正确后，我们在所有可用数据上训练了一个模型。
- en: Listing 4.1 shows the complete listing and most of the features offered by Scikit-learn
    applied in a simple classification problem based on synthetically generated data.
    After creating the data, we define a pipeline, putting together statistical standardization
    with a basic model, logistic regression, for classification. Everything is first
    sent into a function that automatically estimates its performance on an evaluation
    metric, the accuracy, and the time its predictions are correct. Finally, figuring
    that its evaluated performances are suitable, we refit the same machine learning
    algorithm with all the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1展示了基于合成数据的简单分类问题中Scikit-learn提供的完整列表和大多数功能。在创建数据后，我们定义了一个管道，将统计标准化与基本模型、逻辑回归相结合用于分类。所有内容首先被送入一个函数，该函数自动估计其在评估指标（准确率）上的性能以及预测正确的时间。最后，考虑到其评估的性能是合适的，我们使用所有数据重新拟合相同的机器学习算法。
- en: Listing 4.1 Example using Scikit-learn for a classification problem
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 使用Scikit-learn解决分类问题的示例
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Generates a synthetic dataset with specified characteristics
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成具有指定特性的合成数据集
- en: ② Creates an instance of the LogisticRegression model
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建LogisticRegression模型的实例
- en: ③ Creates a pipeline that sequentially applies standard scaling and the logistic
    regression model
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个管道，按顺序应用标准缩放和逻辑回归模型
- en: ④ Performs a five-fold cross-validation using the defined pipeline, calculating
    accuracy scores
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用定义的管道执行五折交叉验证，计算准确率得分
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印交叉验证测试准确率得分的平均值和标准差
- en: ⑥ Fits the logistic regression model to the entire dataset X with corresponding
    labels y
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将逻辑回归模型拟合到整个数据集X及其对应的标签y
- en: 'The resulting output reports the obtained cross-validation accuracy on the
    classification:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出报告了分类中获得的交叉验证准确率：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The key point here is not the model but the procedure of doing things, which
    is standard for all tabular problems, whether you work with classical machine
    learning models or cutting-edge deep learning algorithms. Scikit-learn perfectly
    embeds such a procedure in its API, thus demonstrating a versatile and indispensable
    tool for tabular data problems. In the following sections, we will explore its
    characteristics and workings since we will reuse its procedures multiple times
    in our examples in the book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里关键的不是模型，而是做事的程序，这对于所有表格问题都是标准的，无论你使用的是经典机器学习模型还是前沿的深度学习算法。Scikit-learn完美地将这样的程序嵌入到其API中，从而证明了它是表格数据问题的多才多艺且不可或缺的工具。在接下来的章节中，我们将探讨其特性和工作原理，因为我们将在本书的示例中多次重用其程序。
- en: 4.1.1 Common features of Scikit-learn packages
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 Scikit-learn包的常见特性
- en: The key characteristics of the Scikit-learn package are
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包的关键特性是
- en: It offers a wide range of models for classification and regression, as well
    as functions for clustering, dimensionality reduction, preprocessing, and model
    selection. Most models will work in-memory when data is processed in the computer
    memory and out-of-core when data cannot fit into memory and is accessed from disk,
    allowing learning from data that exceeds your available computer memory.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了广泛的分类和回归模型，以及聚类、降维、预处理和模型选择的功能。大多数模型在数据在计算机内存中处理时将在内存中工作，当数据无法适应内存并从磁盘访问时将在内存外工作，从而允许从超出可用计算机内存的数据中学习。
- en: Across its range of models, it presents a consistent API (class methods such
    as `fit`, `partial_fit`, `predict`, `predict_proba`, `transform`) that can be
    quickly learned and reused and that focuses exclusively on the transformations
    and processes necessary for a model to learn from data and predict from it. Scikit-learn’s
    API also offers automatic segregation of train and test data, the ability to chain
    and reuse its elements in a data pipeline, and accessibility of its parameters
    by simply inspecting the used class’s public attributes.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其模型范围内，它提供了一个一致的API（类方法如`fit`、`partial_fit`、`predict`、`predict_proba`、`transform`），这些API可以快速学习和重用，并且专注于模型从数据中学习和预测所必需的转换和处理过程。Scikit-learn的API还提供了自动分离训练数据和测试数据的功能，能够在数据管道中链式使用其元素，并通过检查使用的类的公共属性来访问其参数。
- en: Initially working on NumPy arrays and sparse matrices, Scikit-learn later extended
    to pandas DataFrames, enabling the practitioner to use them as inputs. In later
    versions (since version 1.1.3), you can retain key DataFrame characteristics,
    such as the name of columns and the transformations operated by Scikit-learn functions
    and classes. The support recently provided by Scikit-learn for pandas DataFrames
    has been long yearned for and is indeed essential for the topic of our book, tabular
    data.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初Scikit-learn专注于NumPy数组和无序矩阵，后来扩展到了pandas DataFrame，使得实践者可以将它们作为输入使用。在后续版本中（自1.1.3版本起），您可以保留关键DataFrame特征，例如列名以及Scikit-learn函数和类所进行的转换。Scikit-learn最近为pandas
    DataFrame提供的支持一直备受期待，确实对于本书的主题——表格数据，是至关重要的。
- en: To define the working parameters of each Scikit-learn class, you just use standard
    Python types and classes (strings, floats, lists). In addition, the default values
    of all such parameters are already set to a proper value for you to create a baseline
    to start with and improve.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要定义每个Scikit-learn类的工作参数，您只需使用标准的Python类型和类（字符串、浮点数、列表）。此外，所有此类参数的默认值已经设置为适当的值，以便您可以从一个基线开始创建，并在此基础上进行改进。
- en: Thanks to a core group of top contributors (such as Andreas Mueller, Oliver
    Grisel, Fabian Pedregosa, Gael Varoquaux, and Gilles Loupe), Scikit-learn is in
    continuous development. There is constant debugging, and new functionalities and
    new models are added every time or old ones are excluded based on their robustness
    and scalability.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感谢一群顶尖的贡献者（如Andreas Mueller、Oliver Grisel、Fabian Pedregosa、Gael Varoquaux和Gilles
    Loupe），Scikit-learn一直在持续发展中。团队不断进行调试，并会根据算法的稳健性和可扩展性，定期添加新功能和新模型，或淘汰旧模型。
- en: The package also presents extensive and easily accessible documentation with
    examples you can consult online ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html))
    or offline using the `help()` command.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该包还提供了广泛且易于访问的文档，其中包含您可以在线咨询的示例（[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)）或使用`help()`命令离线查看。
- en: Depending on your operating system and installation preferences, if you want
    to install Scikit-learn, you just need to follow the instructions at [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html).
    Together with pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)),
    Scikit-learn is the core library for tabular data analysis and modeling. It offers
    a vast range of machine learning and statistical algorithms exclusively for structured
    data; in fact, the input has to be a pandas Dataframe, a NumPy array, or a sparse
    matrix to choose from. These algorithms are all well-established because the Scikit-learn
    team decided to include any algorithm in the package based on “at least three
    years since publication, 200+ citations, and wide use and usefulness” criteria.
    For more details on the algorithm inclusion requirements in Scikit-learn, see
    [https://mng.bz/8OMw](https://mng.bz/8OMw).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的操作系统和安装偏好，如果您想安装Scikit-learn，只需遵循[https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html)上的说明。与pandas
    ([https://pandas.pydata.org/](https://pandas.pydata.org/))一起，Scikit-learn是表格数据分析与建模的核心库。它提供了一系列专门针对结构化数据的机器学习和统计算法；实际上，输入必须是一个pandas
    DataFrame、NumPy数组或无序矩阵。这些算法都得到了良好的建立，因为Scikit-learn团队决定根据“自发表以来至少三年、200+引用、广泛使用和实用性”的标准将任何算法包含在包中。有关Scikit-learn中算法包含要求的更多详细信息，请参阅[https://mng.bz/8OMw](https://mng.bz/8OMw)。
- en: 4.1.2 Common Scikit-learn interface
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 常见Scikit-learn接口
- en: The other key aspect of Scikit-learn that makes it so apt for tabular data problems
    is its current estimator API, the *fit, predict/transform* interface. Such an
    estimator API is not just limited to Scikit-learn, and it is widely recognized
    as the most effective approach to handling training and test data. Many other
    projects have adopted it (see [https://mng.bz/EaWO](https://mng.bz/EaWO)). In
    fact, following Scikit-learn API, you automatically incorporate all the best practices
    in your data science project. In particular, you strictly separate training from
    validation and test data, an indispensable step for the success of any tabular
    data modeling, as we will demonstrate in the next section by reprising the Airbnb
    NYC dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的另一个关键特性是它非常适合表格数据问题，那就是它当前的估计器API，即*fit, predict/transform*接口。这样的估计器API不仅限于Scikit-learn，而且被广泛认为是处理训练和测试数据最有效的方法。许多其他项目都采用了它（见[https://mng.bz/EaWO](https://mng.bz/EaWO)）。实际上，遵循Scikit-learn
    API，你自动将数据科学项目中所有最佳实践融入其中。特别是，你严格区分训练数据、验证数据和测试数据，这是任何表格数据建模成功不可或缺的一步，我们将在下一节通过重新介绍Airbnb
    NYC数据集来展示这一点。
- en: 'Before delving into more practical examples, we provide some basics about Scikit-learn
    estimators. First, we distinguish four kinds of objects in Scikit-learn, each
    with a different interface. One class can implement multiple objects at the same
    time. Estimators are just one of them, though they are the most important ones
    because most of the Scikit-learn classes are estimators. In the following example,
    we define a machine learning estimator, a logistic regression (to be later discussed
    in this same chapter) for classification using the LogisticRegression class offered
    by Scikit-learn:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨更多实际例子之前，我们提供一些关于Scikit-learn估计器的基础知识。首先，我们在Scikit-learn中区分四种类型的对象，每种对象都有不同的接口。一个类可以同时实现多个对象。估计器只是其中之一，尽管它们是最重要的，因为Scikit-learn的大多数类都是估计器。在下面的例子中，我们定义了一个机器学习估计器，即使用Scikit-learn提供的LogisticRegression类进行分类的逻辑回归（将在本章稍后讨论）：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An *estimator* is an object focused on learning from data using the .fit method.
    It can be applied to supervised learning, relating data to a target, or to unsupervised
    learning where only data is involved:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*估计器*是一个对象，它通过`.fit`方法从数据中学习。它可以应用于监督学习，将数据与目标相关联，或者应用于仅涉及数据的无监督学习：'
- en: 'For supervised learning: `estimator` = `estimator.fit(data, targets)`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于监督学习：`estimator` = `estimator.fit(data, targets)`
- en: 'For unsupervised learning: `estimator` = `estimator.fit(data)`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无监督学习：`estimator` = `estimator.fit(data)`
- en: Under the hood, an estimator uses data to estimate some parameters that serve
    for later mapping back data to predictions or transforming it. The parameters
    and other information collected in the process are made available as object attributes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，估计器使用数据来估计一些参数，这些参数用于后续将数据映射回预测或转换它。在过程中收集的参数和其他信息作为对象属性提供。
- en: 'Other typical Scikit-learn objects include the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的其他典型对象包括以下内容：
- en: '*Transformer* is an object focused on mapping a transformation on data:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换器*是一个对象，它专注于对数据进行转换：'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Predictor* is an object focussed on mapping a predicted response given some
    data by the methods `.predict` (predicting a general outcome) and `.predict_proba`
    (predicting a probability):'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测器*是一个对象，它通过`.predict`方法（预测一般结果）和`.predict_proba`方法（预测概率）来映射给定数据的一些预测响应：'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Model* is an object focused on providing the goodness of fit in respect of
    some data, typical of many statistical methods, by the method `.score`:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型*是一个对象，它通过`.score`方法提供一些数据的拟合优度，这在许多统计方法中很典型：'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Whether you need an estimator or a transformer, each class is always instantiated
    by assigning it to a variable and specifying its parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你需要估计器还是转换器，每个类都是通过将其分配给一个变量并指定其参数来实例化的。
- en: 'Under the hood, all these classes store parameters for their task. Some parameters
    are learned directly from the data and are commonly referred to as the weights
    or parameters of the models. You can think of these as the coefficients in a mathematical
    formulation: unknown values to be determined by data and computations. Others
    are given by the user at instantiation and can be configuration or initialization
    settings or parameters that influence how the algorithm learns from data. We usually
    refer to the latter ones as *hyperparameters*. They tend to differ depending on
    the machine learning model; hence, we will discuss the most important ones when
    explaining each algorithm.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，所有这些类都存储着它们任务的参数。一些参数直接从数据中学习，通常被称为模型的权重或参数。你可以将这些视为数学公式中的系数：由数据和计算确定的未知值。其他的是用户在实例化时提供的，可以是配置或初始化设置，或者影响算法如何从数据中学习的参数。我们通常将后者称为*超参数*。它们往往根据机器学习模型的不同而有所不同；因此，当解释每个算法时，我们将讨论最重要的那些。
- en: Configuration and setting parameters are similar for all the algorithms. For
    instance, the `random_state` setting helps to define a random seed for replicating
    the exact behavior of the model when using the same data. The results won’t change
    in different runs thanks to setting a random seed. The configuration parameter
    `n_jobs` will allow you to set how many CPU processors you want to be used in
    the computations, thus speeding up the time necessary for the model to complete
    its work but preventing you from doing other computer operations simultaneously.
    Depending on the algorithm, other available settings of the same kind may define
    the tolerance or the memory cache used by the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所有算法的配置和设置参数都是相似的。例如，`random_state`设置有助于定义一个随机种子，以便在用相同的数据使用模型时复制模型的确切行为。由于设置了随机种子，结果在不同的运行中不会改变。配置参数`n_jobs`将允许你设置在计算中想要使用的CPU处理器数量，从而加快模型完成工作所需的时间，但同时也防止你同时进行其他计算机操作。根据算法的不同，其他可用的类似设置可能定义模型使用的容限或内存缓存。
- en: 'As we mentioned, some of these hyperparameters affect how the model operates
    and others how it learns from data. Let’s reprise our previous example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，这些超参数中的一些影响模型的操作方式，而另一些则影响模型从数据中学习的方式。让我们回顾一下我们之前的例子：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Among the hyperparameters that affect how the model learns from data, in our
    example, we can quote the C parameter, which, by taking different values, instructs
    the machine learning algorithm to apply some constraints in elaborating patterns
    from the data. We will address all the parameters to be fixed for each machine
    learning algorithm as we present them. It is important to notice that you usually
    set the hyperparameters at the time when the class is instantiated.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在影响模型从数据中学习过程的超参数中，在我们的例子中，我们可以引用C参数，该参数通过取不同的值，指导机器学习算法在从数据中提炼模式时应用一些约束。当我们介绍每个机器学习算法时，我们将解决每个算法需要固定的所有参数。重要的是要注意，你通常在实例化类的时候设置超参数。
- en: 'After the class instantiation, you usually provide the data to learn from and
    some limited instruction on how to deal with it—for instance, by giving different
    weights to each data example. At this stage, we say you train or fit the class
    on data. This phase is commonly mentioned as “fitting an estimator,” and it is
    done by providing data as a NumPy array, a sparse matrix, or a pandas DataFrame
    to the `.fit` method:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在类实例化之后，你通常提供用于学习的数据以及一些关于如何处理它的有限指令——例如，通过给每个数据实例赋予不同的权重。在这个阶段，我们说你在数据上训练或调整类的参数。这个阶段通常被称为“调整估计器”，是通过将数据作为NumPy数组、稀疏矩阵或pandas
    DataFrame传递给`.fit`方法来完成的：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since training a model requires mapping an answer to some data, the `.fit`
    method inputs the data matrix and the answer vector. Such behavior is more than
    typical to models because some other Scikit-learn classes input data. The `.fit`
    method is also common to all transformative classes in Scikit-learn. For instance,
    fitting just data is typical of all the classes dealing with preprocessing, as
    you can check at [https://mng.bz/N161](https://mng.bz/N161), because transformations
    also require learning some information from features. For example, if you need
    to standardize data, you must first learn the standard deviation and the mean
    of each numeric feature in the data. The Scikit-learn’s StandardScaler ([https://mng.bz/DMgw](https://mng.bz/DMgw))
    does exactly this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练模型需要将答案映射到某些数据上，`.fit`方法需要输入数据矩阵和答案向量。这种行为对于模型来说是典型的，因为Scikit-learn中的某些其他类也会输入数据。`.fit`方法在Scikit-learn中的所有转换类中也很常见。例如，仅对数据进行拟合是所有处理预处理的类的典型做法，正如你可以在[https://mng.bz/N161](https://mng.bz/N161)中检查的那样，因为转换也需要从特征中学习一些信息。例如，如果你需要标准化数据，你必须首先学习数据中每个数值特征的均值和标准差。Scikit-learn的StandardScaler([https://mng.bz/DMgw](https://mng.bz/DMgw))正是这样做的：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In our example, we instantiate the class necessary for standardizing the data
    (StandardScaler), and we immediately afterward fit the data itself. Since the
    `.fit` method returns the instantiated class we used for the fitting procedure,
    you can safely get in return the class with all the learned parameters by combining
    these two steps. Such an approach will be helpful when building data pipelines
    and training models because it helps you separate the activities that learn something
    from data from the actions that apply what they learned to new data. This way,
    you won’t mistake mixing information from training and validation or test data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们实例化了用于标准化数据的类（StandardScaler），然后立即拟合数据本身。由于`.fit`方法返回用于拟合过程的实例化类，你可以通过结合这两个步骤安全地获取包含所有学习参数的类。这种做法在构建数据管道和训练模型时非常有用，因为它帮助你将学习数据中的某些活动与将所学内容应用于新数据的行为分开。这样，你就不会混淆训练、验证或测试数据中的信息。
- en: Depending on the complexity of the underlying operations and the quantity of
    provided data, fitting a model or a function processing data may take some time.
    After the fitting has been completed, many more attributes will become available
    for you to use afterward, depending on the algorithm you used.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据底层操作的复杂性和提供的数据量，拟合模型或处理数据的函数可能需要一些时间。拟合完成后，将会有更多属性可供你使用，具体取决于你使用的算法。
- en: 'For a trained model, you will obtain a vector of responses of predictions based
    on any new data by applying the `.predict` method. This will work both for a classification
    or a regression problem:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个训练好的模型，你可以通过应用`.predict`方法，基于任何新数据获得预测的响应向量。这既适用于分类问题，也适用于回归问题：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Suppose you are working on a classification; instead, you must get the probability
    that a certain class is a correct prediction for a new sample. In that case, you
    need to use the `.predict_proba` method, which is available only to certain models:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理一个分类任务；相反，你必须获取一个特定类别对新样本进行正确预测的概率。在这种情况下，你需要使用`.predict_proba`方法，这个方法仅适用于某些模型：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Classes that process data, instead, do not have a `.predict` method. Still,
    they use the `.transform` one, which returns transformed data if the class has
    been previously instantiated and fitted with some training data for learning the
    key parameters necessary for the transformation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据的类没有`.predict`方法。然而，它们使用`.transform`方法，如果该类已经通过一些训练数据实例化和拟合以学习转换所需的关键参数，它将返回转换后的数据：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since the transformation is often applied on the very same data that provided
    the key parameters, the `.fit_transform` method, which concatenates the two fit
    and transform phases, will result in a handy shortcut:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换通常应用于提供关键参数的相同数据，`.fit_transform`方法，它结合了拟合和转换两个阶段，将导致一个方便的快捷方式：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 4.1.3 Introduction to Scikit-learn pipelines
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 Scikit-learn管道简介
- en: You can also wrap a sequence of transformations and then predictions, selectively
    deciding what to transform and joining different sequences of transformations
    by using utility functions offered by Scikit-learn, such as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用Scikit-learn提供的实用函数，如将一系列转换和预测包装起来，选择性决定要转换的内容，并通过这些函数将不同的转换序列连接起来：
- en: '*Pipeline* ([https://mng.bz/lYx8](https://mng.bz/lYx8))'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管道* ([https://mng.bz/lYx8](https://mng.bz/lYx8))'
- en: '*ColumnTransformer* ([https://mng.bz/BXM8](https://mng.bz/BXM8))'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ColumnTransformer* ([https://mng.bz/BXM8](https://mng.bz/BXM8))'
- en: '*FeatureUnion* ([https://mng.bz/dX2O](https://mng.bz/dX2O))'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FeatureUnion* ([https://mng.bz/dX2O](https://mng.bz/dX2O))'
- en: The Pipeline command allows you to create a sequence of Scikit-learn classes
    that results in a series of transformations of the data, and it can end up with
    a model and its predictions. In this way, you can integrate any model with the
    transformations it requires for the data and deal with all the involved parameters
    at once—those of the transformations and those of the model itself. The Pipeline
    command is the core command to move tabular data from source to predictions in
    the Scikit-learn package. To set it, at instantiation time, you just need to provide
    a list of tuples, each containing the name of the step in the pipeline and the
    Scikit-learn class or model to be executed. Once instantiated, you can use it
    following the common API specifications of Scikit-learn (fit, transform/predict).
    The pipeline will execute all the predefined steps in sequence, returning the
    final result. Naturally, you can access, inspect, and tune the single steps of
    the pipeline sequence for better results and performance, but you can handle the
    pipeline as a single macro command.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline 命令允许你创建一系列 Scikit-learn 类，这些类将导致数据的一系列转换，并且最终可以结束于一个模型及其预测。通过这种方式，你可以将任何模型与其所需的数据转换集成在一起，并一次性处理所有相关参数——转换的参数和模型本身的参数。Pipeline
    命令是 Scikit-learn 包中将表格数据从源头移动到预测的核心命令。在实例化时设置它，你只需要提供一个包含步骤名称和要执行的 Scikit-learn
    类或模型的元组的列表。一旦实例化，你可以按照 Scikit-learn 的常见 API 规范（fit，transform/predict）使用它。管道将按顺序执行所有预定义的步骤，返回最终结果。当然，你可以访问、检查和调整管道序列的单个步骤以获得更好的结果和性能，但你也可以将管道作为一个单独的宏命令来处理。
- en: However, tabular columns may have different types and require quite different
    transformation sequences, or you may have devised two different ways to process
    your data that you would like to combine. ColumnTrasformer and FeatureUnion are
    Scikit-learn commands that can help you in such occurrences. ColumnTrasformer
    allows you to apply a certain transformation or pipeline of transformations only
    on certain columns (which you can define by their name or position index in the
    columns’ sequence). The command takes a list of tuples, as the Pipeline command,
    but it requires a name for the transformation, a Scikit-learn class for executing
    it, and a list of column names or indexes to which the transformation should be
    applied. Since it is just a transformative command, its ideal usage is inside
    a pipeline, where its transformations can be part of the data feeding of a model.
    FeatureUnion, instead, is just an easy way to concatenate the results of two distinct
    pipelines. You may achieve the same result with a simple NumPy command such as
    `np.hstack` ([https://mng.bz/rKJD](https://mng.bz/rKJD)). However, when using
    FeatureUnion you have the advantage that the command can fit into a Scikit-learn
    pipeline and hence automatically be used as part of the data feeding to the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，表格列可能具有不同的类型，需要相当不同的转换序列，或者你可能已经设计了两种不同的数据处理方式，你希望将它们结合起来。ColumnTransformer
    和 FeatureUnion 是 Scikit-learn 命令，可以帮助你在这种情况下。ColumnTransformer 允许你仅对某些列（你可以通过它们的名称或列序列中的位置索引来定义）应用特定的转换或转换序列。该命令接受一个元组的列表，就像
    Pipeline 命令一样，但它需要一个转换的名称，一个执行它的 Scikit-learn 类，以及一个要应用转换的列名称或索引列表。由于它只是一个转换命令，其理想的使用方式是在管道内部，其中其转换可以是模型数据输入的一部分。FeatureUnion，相反，只是将两个不同管道的结果连接起来的简单方法。你可以使用简单的
    NumPy 命令，如 `np.hstack` ([https://mng.bz/rKJD](https://mng.bz/rKJD)) 来实现相同的结果。然而，当使用
    FeatureUnion 时，你有优势，即该命令可以适应 Scikit-learn 管道，因此可以自动作为模型数据输入的一部分使用。
- en: The modularity of operations and API consistency offered by Scikit-learn and
    its Pipeline, ColumnTrasformer, and FeatureUnion will allow you to easily create
    complex data transformations to be handled as a single command, thus making your
    code highly readable, compact, and easily maintainable. In the next section, we
    will return to the Airbnb NYC dataset we used. We will create a series of transforming
    sequences in Scikit-learn that will allow us to demonstrate how Scikit-learn and
    its pipeline functions are the right choices for tackling your tabular data problems.
    We will also point out how easily you can switch between the different options
    for machine learning with tabular data thanks to a well-defined pipeline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn及其Pipeline、ColumnTransformer和FeatureUnion提供的操作模块化和API一致性将允许您轻松创建复杂的数据转换，将其作为一个单独的命令处理，从而使您的代码高度可读、紧凑且易于维护。在下一节中，我们将回到我们使用的Airbnb纽约市数据集。我们将创建一系列Scikit-learn中的转换序列，以展示Scikit-learn及其管道函数是如何正确处理您的表格数据问题的。我们还将指出，通过一个定义良好的管道，您如何轻松地在表格数据的机器学习不同选项之间切换。
- en: 4.2 Exploring and processing features of the Airbnb NYC dataset
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 探索和处理Airbnb纽约市数据集的特征
- en: The previously introduced Airbnb NYC dataset is a perfect example for demonstrative
    purposes because it is a dataset representative of a real-world problem and because
    of its various types of columns. We will have to create and combine different
    pipelines to handle the different features, and the following chapters will give
    us a chance to present even more advanced processing techniques than the ones
    you can find in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 之前介绍的Airbnb纽约市数据集是演示目的的一个完美例子，因为它是一个代表现实世界问题的数据集，并且由于其各种类型的列。我们将不得不创建和组合不同的管道来处理不同的特征，接下来的章节将给我们一个机会来展示比本章中可以找到的更高级的处理技术。
- en: 'For the moment, we will place the features we will deal with into a list named
    `excluding_list`. They are features, such as the latitude and longitude degrees
    or the data of the last review (`last_review`), which need special ad hoc processing.
    Also, the dataset presents a few possible columns that may act as targets: the
    price, the availability of the listed properties (`availability_365`), and the
    number of reviews (`number_of_reviews`). For our purposes, we prefer to use the
    price. Because it is a continuous set of values above zero, we can immediately
    use it as a regression target. In addition, by applying a split on the mean or
    the median, or binning the values into deciles, we can quickly turn the price
    variable into a binary or multiclass classification target. Apart from price,
    we use all the other features as predictive ones or for more advanced feature
    engineering.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将我们将要处理的特征放入一个名为`excluding_list`的列表中。它们是需要特殊处理的特征，例如纬度和经度度数或最后评论的日期（`last_review`）。此外，数据集还展示了一些可能作为目标的列：价格、列出物业的可用性（`availability_365`）和评论数量（`number_of_reviews`）。就我们的目的而言，我们更倾向于使用价格。因为它是一个大于零的连续值集合，我们可以立即将其用作回归目标。此外，通过在平均值或中位数上进行拆分，或将值分箱到十分位，我们可以快速将价格变量转换为二元或多类分类目标。除了价格之外，我们使用所有其他特征作为预测特征或进行更高级的特征工程。
- en: In the following subsection, we will demonstrate a step-by-step approach to
    exploring the dataset, filtering the dataset based on the useful columns, and
    setting up our target variables. In principle, we will follow the hints and examples
    provided in chapter 2 when discussing *exploratory data analysis* (EDA). In the
    next section, we will take advantage of our discoveries and prepare suitable data
    pipelines that will be reused in the following paragraphs when revising the different
    options for machine learning for tabular data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们将展示一种逐步探索数据集的方法，基于有用的列过滤数据集，并设置我们的目标变量。原则上，我们将遵循第2章中讨论*探索性数据分析*（EDA）时提供的提示和示例。在下一节中，我们将利用我们的发现，准备合适的数据管道，这些管道将在以下段落中修订表格数据的机器学习不同选项时被重用。
- en: 4.2.1 Dataset exploration
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 数据集探索
- en: As a first step in exploring the dataset, we import the relevant packages (NumPy
    and pandas), define the list of excluded features as well as separate lists for
    categorical and continuous features based on our prior knowledge built in the
    previous chapter, and load the data from our current working directory. The code
    to be executed is
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① List of column names to be excluded from the analysis
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: ② List of names of columns that likely represent categorical variables in the
    dataset
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: ③ List of names of columns that represent continuous numerical variables in
    the dataset
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the code snippet has completed the loading of the data, we first check
    how many rows and columns have been returned in the data frame:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will get 48,895 rows available—a fair number for a tabular problem, allowing
    us to use any available learning algorithm—and 16 columns. Since we are interested
    only in some of the columns—the ones we defined in the variables named categorical
    and continuous—we start by refining the classification of categorical features
    into low cardinality and high cardinality ones based on the number of unique values
    they have:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The command results in the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our standard approach when dealing with categorical features is to apply *one-hot
    encoding*, creating one binary variable for each unique value in the original
    feature. However, by using one-hot encoding, features presenting over 20 unique
    values will result in an excessive number of columns in the dataset and data sparsity.
    You have sparsity in your data when your data is predominantly of zero values,
    which is a problem, especially for neural networks and generally for online algorithms
    because learning becomes more difficult. In chapter 6, we will present techniques,
    such as target encoding, to deal with features with too many unique values, called
    *high cardinality categorical features*. For the examples in this chapter, we
    will separate the low from the high cardinality categorical features and process
    only the low cardinality ones:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, having defined that (for the moment, we will be working only with numeric
    and low cardinality categorical features), we need to figure out if there are
    any missing cases in our data. The following command asks to flag true missing
    values and then computes a count of them across features:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We obtain the following result that points out a problem only with the `reviews_per_month`
    feature:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we mentioned in chapter 2, dealing with missing values shouldn’t be an automated
    procedure; rather, it requires some reflection on the data scientist’s part to
    determine if there is some reason for them to be missing. In this case, it becomes
    evident that there is a processing problem with the data at the source because
    if you check the minimum value, this will result in a value above zero:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The minimum reported is 0.01\. Here we have a missing value when there are
    not enough reviews to make statistics. Hence, we could replace the missing value
    on this feature with a zero value. Having filtered our features to be used for
    predictions and having checked missing values because most machine learning algorithms
    won’t work in the presence of missing input data, apart from a few such as the
    gradient boosting implementations XGBoost or LightGBM (discussed in the next chapter),
    we can proceed to check about our target. This part of EDA, *target analysis*,
    is often overlooked, yet it is quite important because, in tabular problems, not
    all machine learning algorithms can handle the same kind of targets. For example,
    targets with many zeros, fat tails, and multiple mode values are difficult for
    certain models and result in your model underfitting. Let’s start by checking
    the distribution of the price feature. A histogram, plotting the frequency of
    values falling into ranges of values (called bins), is particularly helpful in
    figuring out how your data distributes. For instance, a histogram can tell you
    if your data resembles a known distribution, such as the normal distribution,
    or highlight at around what values there are peaks and where the data is denser
    (see figure 4.1). If you are working with a pandas DataFrame, the plot can be
    made just by calling the `hist` method that depicts data distribution by plotting
    the frequency of values falling into ranges of values (bins):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/CH04_F01_Ryan2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 A histogram describing how the Price feature is distributed
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution shown in figure 4.1 is extremely skewed to the right, with
    many outlying values because the plotted values range to 10,000\. However, just
    before 2,000, it is hard to distinguish any bar depicting frequencies. This becomes
    even more evident by plotting a boxplot, which is a very useful tool when one
    wants to visualize where the core part of the distribution of a variable lies.
    A *boxplot* for a variable is a plot where the key measurements of the distribution
    are depicted as a box with “whiskers”: two lines outside the box that stretch
    to the expected limits of the variables’ distribution. The box is delimited by
    the interquartile range (IQR), determined by the 25th and 75th percentiles, and
    split into two by the line of the median. The whiskers stretch up and down to
    values 1.5 times the IQR. Everything above or below the whiskers’ edges is considered
    an *outlier*: an unusual or unexpected value. Let’s plot a boxplot for the price
    variable, again using a built-in method in pandas DataFrame, the boxplot method
    (see figure 4.2):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/CH04_F02_Ryan2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 A boxplot highlighting the distribution of the Price feature and
    its right heavy tail on large price values
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Not surprisingly, the box and whiskers are squeezed in the lower part of the
    chart and are almost indistinguishable from each other. A long queue of outliers
    elongates from the upper limit of the upper extremity of the boxplot. This is
    an evident case of a right-skewed distribution. In such cases, a standard solution
    to remediate the variable is to transform the target using a logarithm transformation.
    It is common practice to add a constant to offset the values into the positive
    number field for handling values of zero and below. In our case, it is unnecessary,
    since all the values are positive and above zero. In the following code snippet,
    we represent the transformed price feature by application of a logarithmic transformation
    (see figures 4.3 and 4.4):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/CH04_F03_Ryan2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 A histogram of the Price feature being more symmetrical after log
    transformation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F04_Ryan2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 A boxplot of the Price feature after log transformation signaling
    the persistence of extreme values at both the tails of the distribution
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the distribution, represented both by the new histogram and the boxplot,
    is more symmetric, though it is evident that there are outlying observations on
    both sides of the distribution. Since our aim is illustrative, we can ignore the
    original distribution and focus on a meaningful target representation. For instance,
    we can keep only the price values below 1,000 (see figure 4.5). In the following
    code snippet, we produce a histogram focused only on price values below 1,000:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/CH04_F05_Ryan2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 A histogram of the Price feature for values under 1,000 still showing
    a right-skewed long tail
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the represented distribution is still right-skewed, but it resembles more
    common distributions found in e-commerce or other sales with long-tail products.
    In addition, if we focus on the range between 50 and 200, the distribution will
    appear more uniform (see figure 4.6). In the following code snippet, we restrict
    our focus further only to prices between 50 and 200 and plot the relative histogram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/CH04_F06_Ryan2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 A histogram of the Price feature for values between 50 and 200, showing
    distributed values across the range
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can create two masking variables, made of booleans, that can
    help us filter the target according to the type of algorithm we would like to
    test. The `price_capped` variable will be instrumental when demonstrating how
    certain machine learning algorithms can handle long tails easily:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Figure 4.7 shows the boxplot relative to the capped price, which presents right-sided
    outliers, but at least the boxplot is visible.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F07_Ryan2.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 A boxplot of the Price feature for values under 1,000 showing a long
    tail of extreme values in its right tail
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8 shows the boxplot relative to the windowed price, showing no sign
    of outliers:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/CH04_F08_Ryan2.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 A boxplot of the Price feature for values between 50 and 200 showing
    a slightly right-skewed distribution with no extreme values
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing our exploration of the predictors and the target, we are ready
    to prepare four different targets that will be used along with our examples:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We prepared two binary targets, `target_mean` and `target_median`, and a multiclass
    target with five distinct classes based on percentiles for classification purposes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, it is important to notice that our `target_median` is a binary
    balanced target. Hence, we can safely use accuracy as a good performance measurement.
    As a test, you get an almost equal number of cases for the positive and negative
    classes if you try to count the values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You get the result
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Instead, if you try doing the same on the `target_mean` target variable, you
    get
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You will obtain a distribution that is imbalanced toward the negative cases;
    that is, there are more cases below the mean because of the skewed distribution
    we previously observed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In such a case, when evaluating the results of a machine learning classifier,
    we prefer to use metrics such as the Receiver Operating Characteristic Area Under
    the Curve (ROC-AUC) or Average Precision—both quite sensible for ordering. Finally,
    as for the multiclass target, counting the cases for each one of the five classes
    reveals that they are also balanced in distribution:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This command returns the result of
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As for the regression target, `target_regression` is the original target without
    transformations. However, we will use subsets of it and accordingly transform
    them based on the machine learning algorithm we will demonstrate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Having completed our exploration of the data, the target, and some basic feature
    selection in the next paragraph, using a building blocks approach, we will prepare
    a few pipelines to accompany our discovery of different machine learning options
    for tabular data problems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Pipelines preparation
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use the previously seen Pipeline and ColumnTransformer classes from
    Scikit-learn to prepare the pipelines. In a building blocks approach, we first
    create the different operations to be applied to the other data types that characterize
    features in a tabular dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines three core procedures that will be reused multiple
    times in this chapter:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '*Categorical one-hot encoding*—Categorical features are transformed into binary
    ones. If a value has never been seen before, it will be ignored.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Numeric pass-through*—Numeric features are imputed using zero as a value.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Numeric standardization*—After imputing missing values, numeric features are
    rescaled by subtracting their mean and dividing them by their standard deviation'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code defining these procedures is shown in the following listing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Setting up building blocks for tabular learning pipelines
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① Converts categorical features into one-hot encoded format
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces missing numeric values with zero
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline replaces missing numeric values with zero and standardizes the features
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can compose specific transformation pipelines that handle
    the data according to our needs for each machine learning algorithm. For instance,
    in this example, we set a pipeline that will one-hot encode low categorical features
    and just impute missing values as zero for numeric ones. Such a pipeline is made
    by the ColumnTransformer function, a glue function that combines operations applied
    on different sets of features simultaneously. This is an excellent transformative
    strategy suitable for most machine learning models:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '① First step of the pipeline: one-hot encoding categorical features'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '② Second step of the pipeline: handling numeric features'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: ③ The features not processed by the pipeline are dropped from the result.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: ④ Names of the features are kept as they originally are.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The result is always a dense matrix (i.e., a NumPy array)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'We can immediately run this code snippet and check how this pipeline transforms
    our Airbnb NYC data:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The result is that the output is now a NumPy array made of floats and that
    the shape has increased to 13 columns. In fact, because of one-hot encoding, each
    value in the categorical features has turned into a separate feature:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The following section will explore the main machine learning techniques for
    tabular data. Each will be accompanied by its column transforming class, which
    will be integrated into the pipeline containing the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Classical machine learning
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To explain the different models from the classic machine learning techniques
    for tabular data, we will first introduce the core characteristics of the algorithm,
    and then demonstrate a code snippet, seeing it at work on our reference tabular
    problem, the Airbnb NYC dataset. The following are some best practices that we
    will use in our examples to allow reproducibility and comparability of the different
    approaches:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: We define a pipeline incorporating both data transformation and modeling.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set an error measure, such as root mean squared error (RMSE) for regression
    or accuracy for classification, and measure it using the same cross-validation
    strategy.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We report the average and standard deviation—crucial to figure out if the model
    has a constant performance across different data samples—of the cross-validated
    estimate of the error.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous section, we introduced the different tools Scikit-learn offers
    for building data pipelines integrating feature processing and machine learning
    models. In this section, we will introduce the recommended evaluation measures
    and how the cross-validation estimate by the Scikit-learn `cross_validate` command
    works.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review *evaluation metrics* first. We decided to use RMSE, a common measure
    for regression tasks, and accuracy, another standard measure for balanced binary
    and multiclass classification problems when the classes have approximately the
    same sample sizes. In subsequent chapters, we will also use metrics suitable for
    unbalanced classification problems, such as ROC-AUC and average precision.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-validation* is the de facto standard in data science when you intend
    to estimate the expected performance of a machine learning model on any data different
    from the training data but drawn from the same data distribution. It is important
    to note that cross-validation estimates the future performance of your model based
    on the idea that your data may change in the future but won’t be radically different.
    To work correctly, the model expects that you will use the same features in the
    future and that they will have the same unique values (if a categorical feature)
    with similar distributions (both for categorical and numeric features) and, most
    importantly, that features will be in the same relation with your target variable.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The assumption that data distributions will remain consistent in the future
    is frequently not true because economic dynamics, consumer markets, and social
    and political situations change rapidly in the real world. In the real world,
    your model may experience concept drifting, when the modeled relationships between
    features and targets no longer represent reality. Hence, your model will underperform
    when dealing with new data. Cross-validation is the best tool to evaluate your
    models at the time of their creation because it is based on your available information
    at that moment and because, if well designed, it is not influenced by the ability
    of your machine learning model to overfit the training data. Its usefulness stays
    true even after cross-validated results are disproved compared to future performances,
    usually because the underlying data distribution has changed. In addition, alternative
    methods, such as leave-one-out or bootstrapping, offer better estimates with increasing
    computational costs, whereas more straightforward methods, such as train/test
    split, are less reliable in their estimates.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'In its most uncomplicated flavor, the *k-fold cross-validation* (implemented
    in Scikit-learn with the KFold function: [https://mng.bz/VVM0](https://mng.bz/VVM0))
    is based on the splitting of your available training data into k partitions and
    the building of k versions of your model fed each time by different sets of k-1
    partitions and then tested on the remaining left out partition (the out-of-sample
    performance). The average and standard deviation of the resulting k scores will
    provide an estimate and a quantification of its uncertainty level to be used as
    a model estimate for expected performance on future unseen data. Figure 4.9 illustrates
    the k-fold validation when k is set to 5: each row represents the data partitioning
    at each fold. The validation part of a fold is always distinct from the others,
    and the training part is always differently composed.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F09_Ryan2.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 How data is distributed between train and validation across the folds
    of a five-fold cross-validation strategy
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Setting the correct value to k is a matter of how much training data you have
    available, how computationally costly it is to train your model on it, how the
    sample you received catches all the possible variations of the data distribution
    you want to model, and for what purpose you intend to get a performance estimate.
    As a general rule of thumb, values of k such as 5 or 10 are optimal choices, with
    k = 10 being more suitable for precise performance evaluation and k = 5 a good
    value compromising precision and computation costs for activities such as model,
    features, and hyperparameters evaluation (hence it will be used for our examples).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a general performance estimation for your model, you can build the necessary
    cross-validation iterations using a series of iterations on the KFold function
    (or its variations, offering sample stratification or control on the time dimension:
    [https://mng.bz/xKne](https://mng.bz/xKne)) or rely on the `cross_validate` procedure
    ([https://mng.bz/AQyK](https://mng.bz/AQyK)) that will handle everything for you
    and just return the results. For our purposes of testing different algorithms,
    `cross_validate` is quite handy because, given the proper parameters, it will
    produce a series of metrics:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation test scores (out-of-sample performance)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation train scores (in-sample performance)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit time and predict time (to evaluate the computational cost)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trained estimators on the different cross-validation folds
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All we have to do is provide an estimator, which can be any Scikit-learn object
    with a fit method, predictors and target, a cross-validation strategy, and a single
    or multiple scoring functions in a list. This estimator should be provided in
    the form of a callable to be created using the `make_scorer` command ([https://mng.bz/ZlAO](https://mng.bz/ZlAO)).
    In the next section, we will start seeing how we can get cross-validated performance
    estimates using such inputs, starting with classical machine learning algorithms
    such as linear regression and logistic regression.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Linear and logistic regression
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *linear regression*, a statistical method that models the relationship between
    a dependent variable and one or more independent variables by fitting a linear
    equation to observed data, you first have all your features converted to numeric
    ones and put them into a matrix, including one-hot encoded categorical features.
    The algorithm’s goal is to optimally find the weight values in a column vector
    (the coefficients) so that, when multiplied against the matrix of features, you
    get a vector of results best approximating your targets (the predictions). In
    other words, the algorithm strives to minimize the residual sum of squares between
    the targets and the predictions obtained by multiplying features with the weight
    vector. In the process, you can consider using a prediction baseline (the so-called
    intercept or bias) or placing constraints on the weight values for them to be
    only positive.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the linear regression algorithm is just a weighted summation, you have
    to take care of three key aspects:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Ensure there are no missing values since they cannot be used for multiplications
    or additions unless you have imputed them to some value.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure you have handled outliers because they can affect the algorithm’s work
    both in training and prediction.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validate that the features and the target are linearly related as much as possible
    (i.e., they have a good Pearson correlation): features weakly related to the target
    tend just to add noise to the model, and they tend to make it underfit or even,
    when in high numbers, overfit.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since a summation of your weighted features gives the prediction, it is easy
    to determine the most significant effect on the predicted output and how each
    feature contributes to it. Observing the coefficients relative to each feature
    gives you insight into how the algorithm behaves. Such understanding can prove
    valuable when you have to explain how the model works to regulatory authorities
    or stakeholders and when you want to check if the predictions are justifiable
    from the point of view of a hypothesis or expert knowledge of the domain.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: However, there are also hidden perils in the easy way that a regression model
    shows how it works under the hood. When two or more features in the data are highly
    correlated, a condition known as “multicollinearity” in statistics, the interpretation
    in a regression model can be much more complicated, even if both features effectively
    contribute to the prediction. Usually, only one of many takes a notable coefficient,
    whereas the others take small values as if they were unrelated to the target.
    In reality, the opposite is often true, and the relative ease in understanding
    the role of a feature in a regression prediction can lead to important conceptual
    misunderstanding.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Another great advantage of the linear regression algorithm is that, since it
    is just some multiplications and summations, it is a breeze to implement it on
    any software platform, even by hand-coding it in a script. Other machine learning
    algorithms are more complex to replicate, and hence, implementation from scratch
    of algorithms more complicated than a linear regression may be susceptible to
    errors and bugs. However, though unfeasible for delivering your projects, we have
    to note that hand-coding any machine learning model can be a valuable learning
    experience, allowing you to gain a deeper understanding of the inner workings
    of the algorithm and making yourself more equipped to troubleshoot and optimize
    the performance of the similar models in the future. We present some manageable
    from-scratch implementations of some algorithms for learning purposes in chapter
    5.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We will start with an example of a linear regression model applied end to end
    to our Airbnb NYC data. The example follows the schema proposed in figure 4.10,
    a schema that we will replicate for every classical machine learning algorithm
    we will present and that is based on Scikit-learn’s pipelines and cross-validation
    evaluation functions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F10_Ryan2.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 Schema of how we will organize the examples for classical machine
    learning algorithms
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The schema is quite linear. The input from a comma-separated values file first
    goes through a ColumnTransformer, which constitutes the data preparation part,
    which applies transformation on data, discards data, or lets it pass as it is,
    based on column names and then a machine learning model. Both are wrapped into
    a pipeline tested by a `cross_validate` function that executes cross-validation
    and records computation times, trained models, and performances on a certain number
    of folds. Finally, the results are selected to demonstrate how the model worked.
    In addition, we can access, passing by the pipeline, the model coefficients and
    weights to get more insights into the functionalities of the algorithm we tested.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Applying such a schema, we just use a vanilla linear regression model in listing
    4.3 since this algorithm usually does not need to specify any parameter. For special
    applications related to model interpretability, you could have specified the `fit_intercept`
    to be false to remove the intercept from the model and derive all the predictions
    from the features only or the positive parameter to be true to get only positive
    coefficients.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Linear regression
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① ColumnTransformer, transforming data into numeric features and imputing missing
    data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: ② Vanilla linear regression model
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline assembling ColumnTransformer and model
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: ④ Cross-validation strategy based on five folds and random sampling
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Function for evaluation metric derived from mean squared error
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Automated cross-validate procedure
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Reports the results in terms of evaluation metric, standard deviation, fitting,
    and prediction time
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the listed code will produce the following RMSE results:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: That’s a good result, obtained in a minimal time (using a standard Google Colab
    instance or a Kaggle notebook), and can act as a baseline for more sophisticated
    attempts. For example, if you try to run the code in listing 4.4, you will realize
    that you can get similar results with fewer but accurately prepared features.
    That’s called *feature engineering,* and the interesting point of doing it is
    that you can get better results or the same results but with fewer features meaningful
    for domain or business experts. For example, we create various new features in
    the code listing by generating binary features relative to specific values, combining
    features, and transforming them using a logarithmic function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Customized data preparation for linear regression
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ① Creates an empty DataFrame
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: ② A binary column indicating whether the 'neighbourhood_group' is 'Manhattan'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: ③ A binary column indicating whether the 'neighbourhood_group' is 'Queens'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: ④ A binary column indicating whether the 'room_type' is 'Entire home/apt'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ A column containing the natural logarithm of the values in the 'minimum_nights'
    column plus 1
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ A column containing the natural logarithm of the values in the 'number_of_reviews'
    column plus 1
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ A product of the binary 'neighbourhood_group_Manhattan' and 'room_type_Entire
    home/apt' columns
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ A product of 'availability_365' and the binary 'neighbourhood_group_Manhattan'
    column
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ A product of 'availability_365' and the binary 'room_type_Entire home/apt'
    column
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The resulting RMSE is
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Though the result is comparable to the previous experiment, this time you are
    using a dataset with fewer features that have been created by specific transformations,
    such as the one-hot encoding of categorical features, the transformations applied
    to numeric ones by specific functions (i.e., cubed, squared, logarithm, or square
    root transformation) and by multiplying features together. In our experience,
    a model presenting fewer, more meaningful features generated by reasoned feature
    engineering and domain expertise is usually more accepted by business users, even
    if it has comparable or even less predictive performance than a purely data-driven
    one.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying features together is an operation that you find only when working
    with linear regression models; the obtained result is called interactions between
    features. Interactions work by multiplying two or more features to get a new one.
    All such transformations on the features are intended to render the relationship
    between each feature and the target as linear as possible. Good results can be
    obtained automatically or based on your knowledge of the data and the problem.
    Applying such transformations to the features is typical of the family of linear
    regression models. They have little or no effect on the more complex algorithms
    we will explore later in this chapter and subsequent chapters. Investing time
    in defining how the features should be expressed is both an advantage and a disadvantage
    of linear regression models. However, there are ways to automatically perform
    it using regularization, as we will propose in the next section.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss regularization in linear models (linear regression
    and logistic regression). Regularization is the best solution to implement when
    you have many features and their reciprocal multicollinearity (you have multicollinearity
    when two predictors are highly correlated with one another) doesn’t allow the
    linear regression model to find the best coefficients for the prediction because
    they are unstable and unreliable—for instance, showing a coefficient you didn’t
    expect in terms of sign and size.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Regularized methods
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression models are usually simple enough for humans to understand
    directly as formulas of coefficients applied to features. This means that, when
    applied to a real-world problem, they can turn out to be a rough approximation
    of complex dynamics and thus systematically miss correct predictions. Technically,
    they are models with a high bias. A remedy for this is to make their formulations
    more complex by adding more and more features and their transformations (logarithmic,
    squared, root transformations, and so on) and by making features interact with
    many others (through multiplication). In this way, a linear regression model can
    diminish its bias and become a better predictor. At the same time, however, the
    variance of the model will also increase, and it can start overfitting.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Occam’s razor principle, which states that among competing hypotheses, the
    one with the fewest assumptions should be selected ([https://mng.bz/RV40](https://mng.bz/RV40)),
    works perfectly for linear models, whereas it doesn’t matter for neural networks
    applied to tabular data where the more complex, the better. Hence, linear models
    should be as simple as possible to meet the needs of the problem. Here is where
    regularization enters the scene, helping you reduce the complexity of a linear
    model until it fits the problem. Regularization is a technique used to reduce
    overfitting in machine learning by limiting the complexity of the model, thus
    effectively improving its generalization performance. Regularization works because
    the linear regression model is penalized as it looks for the best coefficients
    for its predictions. The used penalization is based on the summation of the coefficients.
    Therefore, the regression model is incentivized to keep them as small as possible,
    if not to set them to zero. Constraining regression coefficients to limit their
    magnitude has two significant effects:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: It avoids any form of data memorization and overfitting (i.e., certain specific
    coefficient values to be taken when there is a large number of features compared
    to the available examples).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As coefficient shrinking happens, estimates are stabilized because multicollinear
    features will have the values of their coefficients resized or concentrated on
    only one of the features.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the optimization process, coefficients are updated multiple times, and these
    steps are called iterations. At each step, each regression coefficient incorporates
    a correction toward its optimal value. The optimal value is determined by the
    gradient, which can be intended as a number representing the direction that greatly
    improves the coefficient at that step. A more detailed explanation closes this
    chapter. Penalization is a form of constraint that forces the weights deriving
    from the optimization of the model to have specific characteristics. We have two
    variants of regularization:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'The first variant is where the penalization is computed by summing the absolute
    values of the coefficients: this is called L1 regularization. It makes the coefficients
    sparse because it can push some coefficients to zero, making their related features
    irrelevant.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second option is where the penalization is computed by summing the squared
    coefficients: this is called L2 regularization, and its effect is generally to
    reduce the size of the coefficients (it is also relatively fast to compute).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization (or Lasso regression) pushes many coefficients to zero values,
    thus operating an implicit selection of the useful features (setting a coefficient
    to zero means that a feature doesn’t play any role in prediction). In addition,
    coefficients are always pushed toward zero with the same strength (technically,
    the gradients toward the solution are always +1 or –1). Hence, through the optimization
    steps, the features less associated with the target tend quickly to be assigned
    a zero coefficient and become totally irrelevant regarding the predictions. In
    short, if two or more features are multicollinear and all quite predictive, by
    applying L1 regularization, you will have only one of them with a coefficient
    different from zero.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Instead, in L2 regularization (or Ridge regression), the fact that coefficients
    are squared prevents negative and positive values from canceling each other in
    the penalization and puts more weight on larger coefficients. The result is a
    set of generally smaller coefficients, and multicollinear features tend to have
    similar coefficient values. All the features involved are included in the summation.
    You can notice better important features because, contrary to what happens with
    standard regression, the role of a feature in the prediction is not hidden by
    its correlation with other features. L2 regularization tends to attenuate the
    coefficients. It does so proportionally during the optimization steps; technically,
    the gradients toward the solution tend to be smaller and smaller. Hence, coefficients
    can reach the zero value or be near it. Still, even if the feature must be completely
    irrelevant to the prediction, it takes many optimization iterations and is quite
    time-consuming. Consequently, reprising the previous example of two or more multicollinear
    features in L2 regularization, instead of L1 regression that keeps only one non-zero
    coefficient, all the features would have a non-zero, similar size coefficient.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we first try to create new features through systematic interactions
    between our available features and then perform an L2 and L1 penalized regression
    to compare their results and resulting coefficients. PolynomialFeatures is a Scikit-learn
    function ([https://mng.bz/2ynd](https://mng.bz/2ynd)) that automatically creates
    multiplications between features by multiplying them many times with other features
    and by themselves. The process is reminiscent of the mathematical *polynomial
    expansion* where a power of sums is expressed into its single terms:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: (*a* + *b*)² = *a*² + 2*ab* + *b*²
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn makes it easier because when you state a degree, the function automatically
    creates the polynomial expansions up to that degree. You can decide whether to
    keep only the interactions. Such a process is interesting for a regression model
    because
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '*Interactions* help the regression model to better take into account the conjoint
    values of more features since features usually do not relate to the target in
    isolation but in synergy with others.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of *powers* of a feature helps to model it as a curve. For instance,
    a + a² is a curve in the shape of a parabola.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though using polynomial expansion can avoid the heavy task of creating specific
    features for your problem, it has a downside because it dramatically increases
    the number of features your model uses. More features usually provide more predictive
    power, but they also mean more noise, more multicollinearity, and more chances
    that the model has just to memorize examples and overfit the problem. Applying
    penalties can help us fix this problem with the L2 penalty and select only the
    features to be kept with the L1 penalty.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: In the code in listing 4.5, we test applying L2 and, successively in listing
    4.6, L1 regularization to the same polynomial expansion. It is important to note
    the effect of each kind of regularization. In this first example, we apply L2
    regularization (Ridge). Since regularization makes sense if you have plenty of
    features for your prediction, we create new features from the old ones using a
    polynomial expansion. Our ridge model is then set to a high alpha value to handle
    the increased number of collinear features.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 L2 regularized linear regression
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ① PolynomialFeatures instance performing second-degree polynomial expansion
    on the features
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ② A Ridge regression model instance with a regularization strength (alpha) of
    2,500
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline for column transformation, polynomial expansion, standardization,
    and Ridge regression modeling
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ④ Five-fold cross-validation using the defined pipeline and calculating RMSE
    scores
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test RMSE scores from cross-validation
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'The script results in the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If we count the number of non-zero coefficients (after rounding to five decimals
    to exclude extremely small values), we get
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Ninety-one coefficients out of 105 have non-zero values.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: In the next example, we apply an L1 regularization and compare the results with
    the previous example. The procedure is the same as the last code listing, though
    we resort to a lasso model this time.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 L1 regularized linear regression
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ① A Lasso regression model instance with a regularization strength (alpha) of
    0.1
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: ② Pipeline applying column transformation, polynomial expansion, standardization,
    and Lasso regression modeling
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: ③ Five-fold cross-validation using the defined pipeline and calculating RMSE
    scores
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the mean and standard deviation of the test RMSE scores from cross-validation
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output is
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If we check how many coefficients have non-zero-values by taking the first
    model built by the cross-validation cycle, this time we have fewer:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: With 53 non-zero coefficients, the number of working coefficients has been halved.
    By increasing the alpha parameter of the Lasso call, we can obtain an even sharper
    reduction of used coefficients, albeit at the price of a higher computation time.
    There’s a sweet spot after which applying a higher L1 penalty doesn’t improve
    the prediction results. For prediction purposes, you have to find the correct
    alpha by trial and error or using convenient automatic functions such as LassoCV
    ([https://mng.bz/1XoV](https://mng.bz/1XoV)) or RidgeCV ([https://mng.bz/Pdn9](https://mng.bz/Pdn9))
    that will do the experimentation for you.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, regularization is also used in neural networks. Neural networks
    use sequential matrix multiplications based on matrices of coefficients to transit
    from features to predictions, which is an extension of the working of linear regression.
    Neural networks have more complexities, though; yet in such an aspect of matrix
    multiplication, they resemble a regression model. Based on similar workings, you
    may find it beneficial for your tabular data problem to fit a deep learning architecture
    and, in doing so, to apply an L2 penalty, so the coefficients of the network are
    attenuated and distributed, and/or an L1 penalty, so coefficients are instead
    sparse with many of them set to zeros. In the next section, we will continue our
    discussion of linear models by discovering how to solve a classification problem.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Logistic regression
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear regression model can be effectively extended to classification. In
    a binary classification problem, where you have two classes (a positive one and
    a negative one), you use the same approach as in a regression (feature matrix,
    vector of coefficients, bias). Still, you transform the target using the logit
    function (for details about this statistical distribution, see [https://mng.bz/JY20](https://mng.bz/JY20)).
    The transformative function is called the *link function*. On the optimization
    side, the algorithm uses as a reference the Bernoulli conditional distribution
    (for revising this distribution, see [https://mng.bz/wJoq](https://mng.bz/wJoq))
    instead of the normal distribution. As a result, you get output values ranging
    from 0 to 1, representing the probability that the sample belongs to the positive
    class. This is called logistic regression. Logistic regression is quite an intuitive
    and practical approach to solving binary classification problems and multiclass
    and multilabel ones.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: In listing 4.7, we replicate the same approach as seen with linear regression—this
    time trying to build a model to guess if an example has a target value above the
    median. Please note that transformations are the same, though we use a logistic
    regression model this time. Our target is a class that tells if the target value
    is above the median. Such a target is a binary balanced outcome, where half of
    the labels are positive and half are negative.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Logistic regression
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ① A logistic regression model instance with the “saga” solver, no penalty, and
    a maximum of 1,000 iterations
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: ② A column transformer applying one-hot encoding to categorical features and
    standardization to numeric features
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: ③ A pipeline that sequentially applies column transformation and logistic regression
    modeling
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: ④ Five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'The script results in the following scores:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As the feature processing is the same, we just focus on noticing how the logistic
    regression has some specific parameters with respect to linear regression. In
    particular, you can set the penalty directly without changing the algorithm and
    decide what optimizer will be used (using the parameter solver). Each optimizer
    allows specific penalties, and it can be more or less efficient based on the characteristics
    of your data:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: lbfgs for L2 or no penalty.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: liblinear for L1 and L2 penalties—better for smaller datasets, limited to one-versus-rest
    schemes for multiclass problems.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: newton-cg for L2 or no penalty.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: newton-cholesky for L2 or no penalty.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sag for L2 or no penalty—ideal for large datasets. It requires standardized
    features (or features all with similar scale/standard deviation).
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saga for no penalty, L1, L2, elasticnet (a mix of L1 and L2) penalties—ideal
    for large datasets, it requires standardized features (or features all with similar
    scale/standard deviation).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In listing 4.8, we use an L2 penalty on the multiclass target to test how multiple
    targets are easily dealt with using the `multi_class` parameter set to “ovr” (one-versus-rest),
    a solution that takes a multiclass problem and builds a binary model for each
    of the classes to be predicted. At prediction time, prediction probabilities across
    all the classes are normalized to sum 1.0, and the class corresponding to the
    highest probability is taken and the predicted class. Such an approach is analogous
    to the softmax function approach used in neural networks where a vector of arbitrary
    real values is turned into a probability distribution, where the sum of all elements
    is 1 (for a more detailed explanation of softmax, see [https://mng.bz/qxYw](https://mng.bz/qxYw)).
    The alternative to the one-versus-rest approach is the multinomial option, where
    a single regression model directly models the probability distribution across
    all classes simultaneously.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The multinomial approach is preferred when inter-class relationships are important
    (e.g., for ranking or confidence-based decisions) or when a compact, single-model
    solution is desired.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 L2 regularized multiclass linear regression
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ① Logistic regression model instance with L2 penalty, regularization C=0.1,
    “sag” solver, “ovr” multiclass strategy
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: ② Column transformer that applies one-hot encoding to categorical features and
    standardization to numeric features
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline that sequentially applies column transformation and logistic regression
    modeling
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: ④ Cross-validation using the defined pipeline and calculating accuracy scores
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Predicting the target as a class is certainly more complicated than guessing
    if the target price is over a threshold value or not:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: From this output, it is important to notice how the training time for a cross-validation
    fold has skyrocketed 10 times more. The reason is that applying a penalty to the
    coefficients involves more iterations of the algorithm’s optimization processes
    before reaching a stable result and because now a model for each class is being
    built. As a general rule, consider that penalization requires longer computations
    for the L2 penalty and even longer for the L1 penalty. By setting the `max_iter`
    parameter, you can impose a limit to the algorithm’s iterations, but be aware
    that the result you obtain by cutting off the time required for the algorithm
    to converge won’t be assured to be the best.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Generalized linear methods
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea of extending linear regression to binary classification by logit transformation
    can be applied to distributions other than Bernoulli conditional distribution.
    This is dictated by the target that may represent categorical data, count data,
    or other data whose distribution is known not to be from a normal distribution.
    As we have seen in the previous paragraph, multiclass problems can be modeled
    using the Bernoulli distribution (the one-versus-rest strategy of fitting multiple
    logistic regressions) and the multinomial one. Other problems, more typical of
    domains such as finance or insurance, require different approaches. For instance,
    the Scikit-learn package mentions a few real-world applications and their best-fitting
    distributions (for reference, see [https://mng.bz/7py9](https://mng.bz/7py9)):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '*Climate modeling*—Number of rain events per year (Poisson distribution for
    count data and discrete events). The Poisson distribution is used for modeling
    events such as the number of calls to a call center or the number of customers
    visiting a restaurant), amount of rainfall per event (using Gamma distribution,
    a theoretical distribution useful for modeling because of its skewness and long
    tail), or total rain precipitation per year (Tweedie distribution, a distribution
    which is a compound of Poisson and Gamma distributions).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Risk modeling or insurance policy pricing*—Number of claim events or policyholder
    per year (Poisson), cost per event (Gamma), the total cost per policyholder per
    year (Tweedie).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Predictive maintenance*—Number of production interruption events per year
    (Poisson), the duration of interruption (Gamma), and the total interruption time
    per year (Tweedie).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 4.11 shows the three distributions—Poisson, Tweedie, and Gamma—for different
    averages. The Tweedie distribution is calculated for power equal to 1.5, a blend
    between the Poisson and Gamma distributions.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F11_Ryan2.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 Comparing Poisson, Tweedie, and Gamma distributions at different
    mean values (mu) of the distribution
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you may try any distribution you want—even a plain regression model—for
    any such situation. However, approaching each of them using the appropriate generalized
    linear model that optimizes that specific distribution assures the best result
    in most cases.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: We don’t enter into the specifics of each distribution; you just need to know
    that the Swiss Army Knife of general linear models is the `TweedieRegressor` ([https://mng.bz/mGOr](https://mng.bz/mGOr)).
    This Scikit-learn implementation, depending on the power parameter, can allow
    you to quickly test normal distribution (a regular regression), Poisson distribution
    ([https://mng.bz/4a4w](https://mng.bz/4a4w)), Gamma distribution ([https://mng.bz/QDvG](https://mng.bz/QDvG)),
    Inverse Gaussian distribution (for nonnegative positively skewed data), and a
    blend of Gamma and Poisson (the Tweedie distribution) (see table 4.1).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Power values and their corresponding statistical distributions
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '| Power | Distribution |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| 0 | Normal |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| 1 | Poisson |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| (1,2) | Compound Poisson Gamma |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| 2 | Gamma |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| 3 | Inverse Gaussian |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: 'In listing 4.9, we test the different distributions offered by the TweedieRegressor
    on the entire distribution of prices of the Airbnb NYC dataset, a model fitting
    that we previously avoided because of the heavy distribution tails revealed by
    the EDA. We do so by testing each of these distributions one by one on the full
    range of price values since we are confident that using such specialized distribution
    will solve our problem of a target with heavy tails. It is important to remember
    that such distributions have limitations due to their formulations:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '*Norma**l*—Any kind of value'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Poisso**n*—Zero or positive values'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tweedie, Gamma, Inverse Gaussia**n*—Only non-zero positive values'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This implies that you must adapt your data if you have negative or zero values
    by adding an offset value. Hence, depending on the modeled distribution, we clip
    the target values to a lower bound based on the aforementioned limitations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 Tweedie regression
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ① A list of experiments, made of a distribution name, power parameter, and minimum
    target value
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ② Loops through the experiments list with distribution names and power parameters
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ③ Instance of the TweedieRegressor model with the specified power parameter
    for the current experiment
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ④ Clips the target regression data to a minimum value, according to the used
    distribution
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the experiment name along with the results from cross-validation
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting best-fitting are the Poisson and Tweedie with power 1.5 distributions:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: It is crucial to remember that the secret of the performances of the generalized
    linear models lies in the specific distribution they strive to model during the
    optimization phase. When faced with similar problems, we could resort to similar
    distributions on some more advanced algorithms than generalized linear models,
    particularly on gradient boosting implementations such as XGBoost or LightGBM,
    which will be discussed in the next chapter. In the next section, we will deal
    with a different approach related to large datasets.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Handling large datasets with stochastic gradient descent
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When your tabular dataset cannot fit into your system’s memory, whether it
    is a cloud instance or your desktop computer, your options in modeling shrink.
    Apart from deep learning solutions, which will be discussed in the third part
    of this book, one other option, using classical machine learning, is to resort
    to out-of-core learning. In out-of-core learning, you keep your data in its storage
    (for instance, your data warehouse), and you have your model learn from it bit
    by bit, using small samples extracted from your data, called *batches*. This is
    practically feasible because modern data storage allows for the picking of specific
    data samples at a certain cost in terms of latency: the time interval between
    the initiation of a data-related operation and its completion or response. In
    addition, there are also tools for handling and processing data on the fly (for
    instance, Apache Kafka or Amazon Kinetics) that can redirect the data to out-of-core
    learning algorithms.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: It is also algorithmically feasible because of linear/logistic regression models.
    Both models are made up of additions of coefficients relative to the features
    you use for learning. Out-of-core learning involves first estimating these coefficients
    using some small samples from the data and then updating such coefficients using
    more and more batches extracted from your data. In the end, though the process
    is particularly long, your final estimated coefficient would not be much different
    from the ones you would have obtained if you could have fit all the data into
    the memory.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'How many such batches you have to use for your out-of-core modeling, and if
    you have to reuse them multiple times, is a matter of empirical experimentation:
    it depends on the problem and the data you are using. Though providing new batches
    of unseen data may simply prolong your training phase, having the algorithm see
    the same batches again may cause it to overfit. Unfortunately, in most situations,
    you need to reiterate the same batches multiple times because out-of-core learning
    is not as straightforward as when optimizing; it takes a long time, and you may
    need more passes on the same data, even if we are talking about massive amounts
    of it. Fortunately, you can rely on regularization techniques, such as L1 and
    L2 regularization, to avoid overfitting.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: In listing 4.10, we reprise our logistic regression example and make it out-of-core.
    First, we split our data into a training set and a test set since it is complicated
    to create a cross-validation procedure when using out-of-core learning strategies.
    In real out-of-core learning settings, cross-validation is not just complicated
    but often infeasible because, in such settings, you often handle examples a single
    time. After all, they are streamed from sources and often discarded. The usual
    validation strategy is to collect a list of examples for testing purposes or to
    use a batch of every n-one as an out-of-sample testing batch. In our example,
    we prefer reserving a test set.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.10 Out-of-core logistic regression with L2 regularization
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ① Defines a function to generate batches of data for training
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates batches of data indices for processing
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shuffles the sequence of examples if a random state is provided
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: ④ Yields batches of input features and corresponding labels
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates an instance of the SGDClassifier model with logistic loss, averaging,
    L2 penalty, and alpha regularization
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Splits the data and target into training and testing sets using an 80-20 ratio
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Iterates through training data batches, fitting the column transformer on
    the first batch
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Uses partial fitting to train the model on the first batch, specifying the
    classes
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Uses partial fitting to further train the model on subsequent batches
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Prints accuracy score of test data predictions
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: The train data is then split into multiple batches, and each batch is proposed
    for learning to the *stochastic gradient descent* (SGD) algorithm. SGD is not
    a stand-alone algorithm but an optimization procedure for linear models, optimizing
    the model weights by iteratively learning them from small batches of the data
    or even just single examples taken alone. It is based on the *gradient descent*
    optimization procedure and is also used in deep learning. Gradient descent starts
    with an initial guess for the model weights and computes the error. The next step
    involves computing the gradient of the error, which is obtained by taking the
    negative of the vector that contains the partial derivatives of the error with
    respect to the model weights. Since the gradient can be interpreted as taking
    the steepest descent on an error surface, a common example for gradient descent
    is always to figure it as descending from highs in the mountains to the lowest
    valley by taking the steepest path downward. The “mountains” in this analogy represent
    the error surface, and the “lowest valley” represents the minimum of the error
    function. Figure 4.12 visually represents this process by the progressive descent
    from a random high place to the lowest point in a bowl-shaped error curve.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Besides the analogy, it is important to remember that the gradient determines
    how the weights should be adjusted to reduce the error at that step. Through repeated
    iterations, the error can be minimized by adjusting the model’s weights. However,
    how the weights are updated can significantly affect the outcome. If the updates
    are too large and decisive, the algorithm may take overly wide steps, potentially
    causing the model to overshoot the target and climb the error curve. In the worst-case
    scenario, this can result in a continuous worsening of the error, with no possibility
    of recovery. Conversely, taking smaller steps is generally safer but may be computationally
    burdensome. The size of such steps is decided by the learning rate, a parameter
    that regulates how the updates are done.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F12_Ryan2.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 Gradient descent optimization in action in a simple optimization
    landscape
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Linear models can be optimized easily using gradient descent because their error
    surface is simple and bowl-shaped. However, more complex models like gradient
    boosting (which will be discussed in the next chapter) and deep learning architectures
    may encounter challenges in optimization due to their higher complexity, with
    interrelated parameters and a more complex error landscape. Depending on the starting
    point, as illustrated in figure 4.13, these models may become stuck in a local
    minimum or plateau during optimization, leading to suboptimal results.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F13_Ryan2.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 Gradient descent in a complex error landscape showing how local
    minima and plateaus can lead to less-than-optimal solutions
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Learning a linear model by SGD is made possible using Scikit-learn’s method
    `partial_fit`, which, after an informed start (the algorithm needs to know the
    target labels), can learn by partially fitting one batch after the other. The
    same procedure is repeated multiple times, called iterations or epochs, to consolidate
    and improve the learning, though repeating the same examples too often may also
    cause overfitting. The algorithm will see, though in a different order, the same
    examples multiple times and update its coefficients every time. To avoid abrupt
    changes in the coefficients, which frequently occur when an outlier is present
    in the batch, the updated coefficients are not substituted for the existing ones.
    Instead, they are averaged together, allowing a more gradual transition.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the learning process is completed, you will get the following result:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The result is quite comparable with in-core learning logistic regression. Out-of-core
    learning, though limited to only the simplest machine learning algorithms such
    as linear or logistic regression, is an effective way to train on your tabular
    data when too many samples cannot fit into memory. All deep learning solutions
    also use the idea of a stream of batches, and it will be discussed again in the
    chapters devoted to deep neural network methods for tabular data, together with
    strategies such as *early stopping*, a technique interrupting the iterations over
    data when necessary to avoid overfitting the data because of an excessive exposition
    of the algorithm to examples seen in previous iterations.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: We can now anticipate that a fundamental recipe of such learning strategies
    is the randomization of the order of the examples. Since the optimization is progressive,
    if your data is ordered in a specific way, it will cause a biased optimization,
    which may result in suboptimal learning. Repeating the same batches in the same
    order can negatively influence your results. Hence, randomizing the order is critical
    for a better-trained algorithm. Another important point with SGD, however, is
    the data preparation phase. In such a phase, you should include all feature rescaling
    operations, because the optimization process is sensible to the scale of features
    and all the feature engineering and feature interaction computations, and set
    it in a way as deterministic as possible since it could be difficult to use global
    parameters, such as the maximum/minimum or average and the standard deviation
    of a feature, when your data is split into multiple batches.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Choosing your algorithm
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a general rule of thumb, you should first consider that machine learning
    algorithms scale differently based on how many rows and columns you have. Starting
    from the number of available rows, you must strictly resort to simple rule-based
    or statistical-based algorithms when operating with about or fewer than 10² rows
    of data. For up to 10³ rows, models based on linear combinations, such as linear
    and logistic regression, are best suited because they tend not to overfit the
    little data available. You usually cannot tell what algorithm will work better
    from about 10³ to 10⁴ – 10⁵ rows. Hence, it is all a matter of testing and experimenting.
    Here, deep learning solutions may outrun other choices only if there is some structure
    to exploit, such as an ordered series of information or a hierarchical structure.
    Up to 10⁹ rows, solutions from the gradient boosting family are likely the most
    effective. Again, you may find that something like out-of-core learning is a much
    better solution for specific problems, such as in the advertisement industry,
    where you have many fixed interactions that you need to estimate—for instance,
    between display devices, websites, and advertisements.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Out-of-core learning refers to a learning strategy that certain machine learning
    algorithms can adopt when learning from data: instead of learning all at once
    from the data, they learn bit by bit from smaller samples of the data, the batches,
    or even from single examples, one by one, which is also mentioned as online learning.
    Finally, in our experience, in situations with datasets above 10⁹ rows, deep learning
    solutions, and some out-of-core learning algorithms tend to perform better because
    they can effectively deal with such an amount of data, whereas other machine learning
    algorithms may be forced to learn from subsamples from the data or find other
    suboptimal solutions.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Regarding columns, we find that some algorithms need to scale better with datasets
    characterized by multiple columns, especially if they present sparse information—that
    is, many binary features. The sparser the datasets, which can be measured by the
    percentage of zeros values in relation to the total numeric values in the dataset,
    the earlier you may need to apply online learning algorithms or deep learning.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: However, apart from scalability reasons, which relate to memory and computational
    complexity, each machine learning solution also suits different needs in terms
    of model control, openness, and understandability of the solution. In such a way,
    the variety of needs in tabular problems and of models in machine learning defies
    the notion of one best algorithm that is all you need for your work. In other
    words, it is not just that you need to try more machine learning models because
    “there’s no free lunch,” as stated in the well-known theorem by David Wolpert
    and William Macready (see [http://www.no-free-lunch.org](http://www.no-free-lunch.org)
    for more details). More often than expected, there are cases where the underdog
    algorithm surprisingly beats the best-in-class algorithm. The necessity of more
    algorithms is mostly dictated because, as you change perspective on your problem
    as an artisan/artist creating their work from different angles, you may need different
    tools for the task.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will present a more powerful class of machine learning
    algorithms, the ensembles, and finally, the gradient boosting family and its successful
    and popular implementations, such as XGBoost and LightGBM.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Determining what machine learning algorithm involves several factors: the number
    of examples and features, the expected performances, speed at prediction time,
    and interpretability. As a general rule of thumb,'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical machine learning is suitable for datasets with few cases.
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical machine learning is suitable for datasets with a moderate number of
    cases.
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting algorithms are particularly effective for datasets with a
    moderate to large number of cases.
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning solutions are the most feasible and effective for datasets with
    massive amounts of data.
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-learn is an open-source library for machine learning that offers a wide
    range of models for classification and regression, as well as functions for clustering,
    dimensionality reduction, preprocessing, and model selection. We can summarize
    its core advantages as follows:'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent API across models
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports in-memory and out-of-core learning
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports working with pandas DataFrames
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for tabular problems
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to install
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive documentation
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear regression is the summation of weighted features that have been converted
    to numeric values (one-hot encoding for categorical features):'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm finds optimal weight values (coefficients) to minimize the residual
    sum of squares between targets and predictions.
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression is easy to explain and understand how each feature contributes
    to the final result.
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A high correlation between features (multicollinearity) can cause conceptual
    misunderstanding.
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression is computationally simple and easy to implement.
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression is limited in its ability to fit complex problems with nonlinear
    data unless features are carefully prepared beforehand with feature engineering,
    such as creating polynomial features, which can help linear regression capture
    nonlinear relationships.
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularization is used to prevent overfitting by reducing the complexity of
    a regression model and improving its generalization performance. There are two
    types of regularization:'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization (or Lasso regression) pushes many coefficients to zero values,
    thus making some features irrelevant in the model.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 regularization generally reduces the size of coefficients.
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization can be helpful for feature selection, while L2 regularization
    reduces overfitting when using many features while being faster to compute.
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression can be extended to classification problems using the logit
    function to transform the target and the Bernoulli conditional distribution to
    optimize the algorithm. This results in a logistic regression model that can be
    used for binary classification, multiclass, and multilabel problems. Logistic
    regression is easy to implement and understand but has the same limitations as
    linear regression.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same approach of transforming the target can be applied to other distributions
    as well, such as Poisson and Gamma, depending on the nature of the data. The resulting
    generalized linear models can be used for various real-world applications, such
    as climate modeling, risk modeling, and predictive maintenance. However, it’s
    important to note that the results may not be optimal without a proper understanding
    of the specific distribution applied to each situation.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
