- en: 4 Classical algorithms for tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and processing features of the Airbnb NYC dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some classic machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the problem, classic machine learning algorithms are often the
    most practical approach to working with tabular data. With decades of research
    and practice behind these tools and algorithms, there is a rich palette of solutions
    to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover essential algorithms in classical machine learning
    for making predictions using tabular data. We have focused on the linear models
    because they are still the most common solutions for both a challenging baseline
    and a solid and robust model in production. In addition, discussing linear models
    helps us build concepts and ideas that we can find in deep learning architectures
    and in more advanced machine learning algorithms, such as gradient-boosting decision
    trees (which will be one of the topics of the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also give you a quick introduction to Scikit-learn, a powerful and versatile
    machine learning library that we’ll use to continue exploring the Airbnb NYC dataset.
    We’ll stay away from lengthy mathematical definitions and textbook details in
    favor of examples and practical recommendations for applying these models to tabular
    data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Introducing Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-learn is an open-source library for classic machine learning. It started
    in 2007 as a Google Summer of Code project by David Cournapeau, and it later became
    part of the SciKits (short for Scipy Toolkits: [https://projects.scipy.org/scikits.html](https://projects.scipy.org/scikits.html))
    until the INRIA (Institut National de Recherche en Informatique et en Automatique)
    and its foundation took the leadership of the project and of its development.
    We provide a short example of how Scikit-learn can quickly solve most machine
    learning problems. In our starting example,'
  prefs: []
  type: TYPE_NORMAL
- en: We create a synthetic dataset for a classification problem with a binary balanced
    target with half labels positive and half negative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set up a pipeline standardizing the features and passing them to a logistic
    regression model, one of the simplest and most effective statistical-based machine
    learning algorithms for classification problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We evaluate its performance using cross-validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, assured by the cross-validation results that our work with the problem
    is fine, we train a model on all the available data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listing 4.1 shows the complete listing and most of the features offered by Scikit-learn
    applied in a simple classification problem based on synthetically generated data.
    After creating the data, we define a pipeline, putting together statistical standardization
    with a basic model, logistic regression, for classification. Everything is first
    sent into a function that automatically estimates its performance on an evaluation
    metric, the accuracy, and the time its predictions are correct. Finally, figuring
    that its evaluated performances are suitable, we refit the same machine learning
    algorithm with all the data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Example using Scikit-learn for a classification problem
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates a synthetic dataset with specified characteristics
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates an instance of the LogisticRegression model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a pipeline that sequentially applies standard scaling and the logistic
    regression model
  prefs: []
  type: TYPE_NORMAL
- en: ④ Performs a five-fold cross-validation using the defined pipeline, calculating
    accuracy scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Fits the logistic regression model to the entire dataset X with corresponding
    labels y
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting output reports the obtained cross-validation accuracy on the
    classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The key point here is not the model but the procedure of doing things, which
    is standard for all tabular problems, whether you work with classical machine
    learning models or cutting-edge deep learning algorithms. Scikit-learn perfectly
    embeds such a procedure in its API, thus demonstrating a versatile and indispensable
    tool for tabular data problems. In the following sections, we will explore its
    characteristics and workings since we will reuse its procedures multiple times
    in our examples in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Common features of Scikit-learn packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key characteristics of the Scikit-learn package are
  prefs: []
  type: TYPE_NORMAL
- en: It offers a wide range of models for classification and regression, as well
    as functions for clustering, dimensionality reduction, preprocessing, and model
    selection. Most models will work in-memory when data is processed in the computer
    memory and out-of-core when data cannot fit into memory and is accessed from disk,
    allowing learning from data that exceeds your available computer memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Across its range of models, it presents a consistent API (class methods such
    as `fit`, `partial_fit`, `predict`, `predict_proba`, `transform`) that can be
    quickly learned and reused and that focuses exclusively on the transformations
    and processes necessary for a model to learn from data and predict from it. Scikit-learn’s
    API also offers automatic segregation of train and test data, the ability to chain
    and reuse its elements in a data pipeline, and accessibility of its parameters
    by simply inspecting the used class’s public attributes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initially working on NumPy arrays and sparse matrices, Scikit-learn later extended
    to pandas DataFrames, enabling the practitioner to use them as inputs. In later
    versions (since version 1.1.3), you can retain key DataFrame characteristics,
    such as the name of columns and the transformations operated by Scikit-learn functions
    and classes. The support recently provided by Scikit-learn for pandas DataFrames
    has been long yearned for and is indeed essential for the topic of our book, tabular
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To define the working parameters of each Scikit-learn class, you just use standard
    Python types and classes (strings, floats, lists). In addition, the default values
    of all such parameters are already set to a proper value for you to create a baseline
    to start with and improve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to a core group of top contributors (such as Andreas Mueller, Oliver
    Grisel, Fabian Pedregosa, Gael Varoquaux, and Gilles Loupe), Scikit-learn is in
    continuous development. There is constant debugging, and new functionalities and
    new models are added every time or old ones are excluded based on their robustness
    and scalability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The package also presents extensive and easily accessible documentation with
    examples you can consult online ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html))
    or offline using the `help()` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on your operating system and installation preferences, if you want
    to install Scikit-learn, you just need to follow the instructions at [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html).
    Together with pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)),
    Scikit-learn is the core library for tabular data analysis and modeling. It offers
    a vast range of machine learning and statistical algorithms exclusively for structured
    data; in fact, the input has to be a pandas Dataframe, a NumPy array, or a sparse
    matrix to choose from. These algorithms are all well-established because the Scikit-learn
    team decided to include any algorithm in the package based on “at least three
    years since publication, 200+ citations, and wide use and usefulness” criteria.
    For more details on the algorithm inclusion requirements in Scikit-learn, see
    [https://mng.bz/8OMw](https://mng.bz/8OMw).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Common Scikit-learn interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other key aspect of Scikit-learn that makes it so apt for tabular data problems
    is its current estimator API, the *fit, predict/transform* interface. Such an
    estimator API is not just limited to Scikit-learn, and it is widely recognized
    as the most effective approach to handling training and test data. Many other
    projects have adopted it (see [https://mng.bz/EaWO](https://mng.bz/EaWO)). In
    fact, following Scikit-learn API, you automatically incorporate all the best practices
    in your data science project. In particular, you strictly separate training from
    validation and test data, an indispensable step for the success of any tabular
    data modeling, as we will demonstrate in the next section by reprising the Airbnb
    NYC dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before delving into more practical examples, we provide some basics about Scikit-learn
    estimators. First, we distinguish four kinds of objects in Scikit-learn, each
    with a different interface. One class can implement multiple objects at the same
    time. Estimators are just one of them, though they are the most important ones
    because most of the Scikit-learn classes are estimators. In the following example,
    we define a machine learning estimator, a logistic regression (to be later discussed
    in this same chapter) for classification using the LogisticRegression class offered
    by Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'An *estimator* is an object focused on learning from data using the .fit method.
    It can be applied to supervised learning, relating data to a target, or to unsupervised
    learning where only data is involved:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For supervised learning: `estimator` = `estimator.fit(data, targets)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For unsupervised learning: `estimator` = `estimator.fit(data)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood, an estimator uses data to estimate some parameters that serve
    for later mapping back data to predictions or transforming it. The parameters
    and other information collected in the process are made available as object attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other typical Scikit-learn objects include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transformer* is an object focused on mapping a transformation on data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Predictor* is an object focussed on mapping a predicted response given some
    data by the methods `.predict` (predicting a general outcome) and `.predict_proba`
    (predicting a probability):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Model* is an object focused on providing the goodness of fit in respect of
    some data, typical of many statistical methods, by the method `.score`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Whether you need an estimator or a transformer, each class is always instantiated
    by assigning it to a variable and specifying its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, all these classes store parameters for their task. Some parameters
    are learned directly from the data and are commonly referred to as the weights
    or parameters of the models. You can think of these as the coefficients in a mathematical
    formulation: unknown values to be determined by data and computations. Others
    are given by the user at instantiation and can be configuration or initialization
    settings or parameters that influence how the algorithm learns from data. We usually
    refer to the latter ones as *hyperparameters*. They tend to differ depending on
    the machine learning model; hence, we will discuss the most important ones when
    explaining each algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration and setting parameters are similar for all the algorithms. For
    instance, the `random_state` setting helps to define a random seed for replicating
    the exact behavior of the model when using the same data. The results won’t change
    in different runs thanks to setting a random seed. The configuration parameter
    `n_jobs` will allow you to set how many CPU processors you want to be used in
    the computations, thus speeding up the time necessary for the model to complete
    its work but preventing you from doing other computer operations simultaneously.
    Depending on the algorithm, other available settings of the same kind may define
    the tolerance or the memory cache used by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned, some of these hyperparameters affect how the model operates
    and others how it learns from data. Let’s reprise our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Among the hyperparameters that affect how the model learns from data, in our
    example, we can quote the C parameter, which, by taking different values, instructs
    the machine learning algorithm to apply some constraints in elaborating patterns
    from the data. We will address all the parameters to be fixed for each machine
    learning algorithm as we present them. It is important to notice that you usually
    set the hyperparameters at the time when the class is instantiated.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the class instantiation, you usually provide the data to learn from and
    some limited instruction on how to deal with it—for instance, by giving different
    weights to each data example. At this stage, we say you train or fit the class
    on data. This phase is commonly mentioned as “fitting an estimator,” and it is
    done by providing data as a NumPy array, a sparse matrix, or a pandas DataFrame
    to the `.fit` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since training a model requires mapping an answer to some data, the `.fit`
    method inputs the data matrix and the answer vector. Such behavior is more than
    typical to models because some other Scikit-learn classes input data. The `.fit`
    method is also common to all transformative classes in Scikit-learn. For instance,
    fitting just data is typical of all the classes dealing with preprocessing, as
    you can check at [https://mng.bz/N161](https://mng.bz/N161), because transformations
    also require learning some information from features. For example, if you need
    to standardize data, you must first learn the standard deviation and the mean
    of each numeric feature in the data. The Scikit-learn’s StandardScaler ([https://mng.bz/DMgw](https://mng.bz/DMgw))
    does exactly this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we instantiate the class necessary for standardizing the data
    (StandardScaler), and we immediately afterward fit the data itself. Since the
    `.fit` method returns the instantiated class we used for the fitting procedure,
    you can safely get in return the class with all the learned parameters by combining
    these two steps. Such an approach will be helpful when building data pipelines
    and training models because it helps you separate the activities that learn something
    from data from the actions that apply what they learned to new data. This way,
    you won’t mistake mixing information from training and validation or test data.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the complexity of the underlying operations and the quantity of
    provided data, fitting a model or a function processing data may take some time.
    After the fitting has been completed, many more attributes will become available
    for you to use afterward, depending on the algorithm you used.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a trained model, you will obtain a vector of responses of predictions based
    on any new data by applying the `.predict` method. This will work both for a classification
    or a regression problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose you are working on a classification; instead, you must get the probability
    that a certain class is a correct prediction for a new sample. In that case, you
    need to use the `.predict_proba` method, which is available only to certain models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Classes that process data, instead, do not have a `.predict` method. Still,
    they use the `.transform` one, which returns transformed data if the class has
    been previously instantiated and fitted with some training data for learning the
    key parameters necessary for the transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the transformation is often applied on the very same data that provided
    the key parameters, the `.fit_transform` method, which concatenates the two fit
    and transform phases, will result in a handy shortcut:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 4.1.3 Introduction to Scikit-learn pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also wrap a sequence of transformations and then predictions, selectively
    deciding what to transform and joining different sequences of transformations
    by using utility functions offered by Scikit-learn, such as
  prefs: []
  type: TYPE_NORMAL
- en: '*Pipeline* ([https://mng.bz/lYx8](https://mng.bz/lYx8))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ColumnTransformer* ([https://mng.bz/BXM8](https://mng.bz/BXM8))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FeatureUnion* ([https://mng.bz/dX2O](https://mng.bz/dX2O))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Pipeline command allows you to create a sequence of Scikit-learn classes
    that results in a series of transformations of the data, and it can end up with
    a model and its predictions. In this way, you can integrate any model with the
    transformations it requires for the data and deal with all the involved parameters
    at once—those of the transformations and those of the model itself. The Pipeline
    command is the core command to move tabular data from source to predictions in
    the Scikit-learn package. To set it, at instantiation time, you just need to provide
    a list of tuples, each containing the name of the step in the pipeline and the
    Scikit-learn class or model to be executed. Once instantiated, you can use it
    following the common API specifications of Scikit-learn (fit, transform/predict).
    The pipeline will execute all the predefined steps in sequence, returning the
    final result. Naturally, you can access, inspect, and tune the single steps of
    the pipeline sequence for better results and performance, but you can handle the
    pipeline as a single macro command.
  prefs: []
  type: TYPE_NORMAL
- en: However, tabular columns may have different types and require quite different
    transformation sequences, or you may have devised two different ways to process
    your data that you would like to combine. ColumnTrasformer and FeatureUnion are
    Scikit-learn commands that can help you in such occurrences. ColumnTrasformer
    allows you to apply a certain transformation or pipeline of transformations only
    on certain columns (which you can define by their name or position index in the
    columns’ sequence). The command takes a list of tuples, as the Pipeline command,
    but it requires a name for the transformation, a Scikit-learn class for executing
    it, and a list of column names or indexes to which the transformation should be
    applied. Since it is just a transformative command, its ideal usage is inside
    a pipeline, where its transformations can be part of the data feeding of a model.
    FeatureUnion, instead, is just an easy way to concatenate the results of two distinct
    pipelines. You may achieve the same result with a simple NumPy command such as
    `np.hstack` ([https://mng.bz/rKJD](https://mng.bz/rKJD)). However, when using
    FeatureUnion you have the advantage that the command can fit into a Scikit-learn
    pipeline and hence automatically be used as part of the data feeding to the model.
  prefs: []
  type: TYPE_NORMAL
- en: The modularity of operations and API consistency offered by Scikit-learn and
    its Pipeline, ColumnTrasformer, and FeatureUnion will allow you to easily create
    complex data transformations to be handled as a single command, thus making your
    code highly readable, compact, and easily maintainable. In the next section, we
    will return to the Airbnb NYC dataset we used. We will create a series of transforming
    sequences in Scikit-learn that will allow us to demonstrate how Scikit-learn and
    its pipeline functions are the right choices for tackling your tabular data problems.
    We will also point out how easily you can switch between the different options
    for machine learning with tabular data thanks to a well-defined pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Exploring and processing features of the Airbnb NYC dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previously introduced Airbnb NYC dataset is a perfect example for demonstrative
    purposes because it is a dataset representative of a real-world problem and because
    of its various types of columns. We will have to create and combine different
    pipelines to handle the different features, and the following chapters will give
    us a chance to present even more advanced processing techniques than the ones
    you can find in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the moment, we will place the features we will deal with into a list named
    `excluding_list`. They are features, such as the latitude and longitude degrees
    or the data of the last review (`last_review`), which need special ad hoc processing.
    Also, the dataset presents a few possible columns that may act as targets: the
    price, the availability of the listed properties (`availability_365`), and the
    number of reviews (`number_of_reviews`). For our purposes, we prefer to use the
    price. Because it is a continuous set of values above zero, we can immediately
    use it as a regression target. In addition, by applying a split on the mean or
    the median, or binning the values into deciles, we can quickly turn the price
    variable into a binary or multiclass classification target. Apart from price,
    we use all the other features as predictive ones or for more advanced feature
    engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsection, we will demonstrate a step-by-step approach to
    exploring the dataset, filtering the dataset based on the useful columns, and
    setting up our target variables. In principle, we will follow the hints and examples
    provided in chapter 2 when discussing *exploratory data analysis* (EDA). In the
    next section, we will take advantage of our discoveries and prepare suitable data
    pipelines that will be reused in the following paragraphs when revising the different
    options for machine learning for tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Dataset exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a first step in exploring the dataset, we import the relevant packages (NumPy
    and pandas), define the list of excluded features as well as separate lists for
    categorical and continuous features based on our prior knowledge built in the
    previous chapter, and load the data from our current working directory. The code
    to be executed is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① List of column names to be excluded from the analysis
  prefs: []
  type: TYPE_NORMAL
- en: ② List of names of columns that likely represent categorical variables in the
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: ③ List of names of columns that represent continuous numerical variables in
    the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the code snippet has completed the loading of the data, we first check
    how many rows and columns have been returned in the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get 48,895 rows available—a fair number for a tabular problem, allowing
    us to use any available learning algorithm—and 16 columns. Since we are interested
    only in some of the columns—the ones we defined in the variables named categorical
    and continuous—we start by refining the classification of categorical features
    into low cardinality and high cardinality ones based on the number of unique values
    they have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The command results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Our standard approach when dealing with categorical features is to apply *one-hot
    encoding*, creating one binary variable for each unique value in the original
    feature. However, by using one-hot encoding, features presenting over 20 unique
    values will result in an excessive number of columns in the dataset and data sparsity.
    You have sparsity in your data when your data is predominantly of zero values,
    which is a problem, especially for neural networks and generally for online algorithms
    because learning becomes more difficult. In chapter 6, we will present techniques,
    such as target encoding, to deal with features with too many unique values, called
    *high cardinality categorical features*. For the examples in this chapter, we
    will separate the low from the high cardinality categorical features and process
    only the low cardinality ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, having defined that (for the moment, we will be working only with numeric
    and low cardinality categorical features), we need to figure out if there are
    any missing cases in our data. The following command asks to flag true missing
    values and then computes a count of them across features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following result that points out a problem only with the `reviews_per_month`
    feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned in chapter 2, dealing with missing values shouldn’t be an automated
    procedure; rather, it requires some reflection on the data scientist’s part to
    determine if there is some reason for them to be missing. In this case, it becomes
    evident that there is a processing problem with the data at the source because
    if you check the minimum value, this will result in a value above zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The minimum reported is 0.01\. Here we have a missing value when there are
    not enough reviews to make statistics. Hence, we could replace the missing value
    on this feature with a zero value. Having filtered our features to be used for
    predictions and having checked missing values because most machine learning algorithms
    won’t work in the presence of missing input data, apart from a few such as the
    gradient boosting implementations XGBoost or LightGBM (discussed in the next chapter),
    we can proceed to check about our target. This part of EDA, *target analysis*,
    is often overlooked, yet it is quite important because, in tabular problems, not
    all machine learning algorithms can handle the same kind of targets. For example,
    targets with many zeros, fat tails, and multiple mode values are difficult for
    certain models and result in your model underfitting. Let’s start by checking
    the distribution of the price feature. A histogram, plotting the frequency of
    values falling into ranges of values (called bins), is particularly helpful in
    figuring out how your data distributes. For instance, a histogram can tell you
    if your data resembles a known distribution, such as the normal distribution,
    or highlight at around what values there are peaks and where the data is denser
    (see figure 4.1). If you are working with a pandas DataFrame, the plot can be
    made just by calling the `hist` method that depicts data distribution by plotting
    the frequency of values falling into ranges of values (bins):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH04_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 A histogram describing how the Price feature is distributed
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution shown in figure 4.1 is extremely skewed to the right, with
    many outlying values because the plotted values range to 10,000\. However, just
    before 2,000, it is hard to distinguish any bar depicting frequencies. This becomes
    even more evident by plotting a boxplot, which is a very useful tool when one
    wants to visualize where the core part of the distribution of a variable lies.
    A *boxplot* for a variable is a plot where the key measurements of the distribution
    are depicted as a box with “whiskers”: two lines outside the box that stretch
    to the expected limits of the variables’ distribution. The box is delimited by
    the interquartile range (IQR), determined by the 25th and 75th percentiles, and
    split into two by the line of the median. The whiskers stretch up and down to
    values 1.5 times the IQR. Everything above or below the whiskers’ edges is considered
    an *outlier*: an unusual or unexpected value. Let’s plot a boxplot for the price
    variable, again using a built-in method in pandas DataFrame, the boxplot method
    (see figure 4.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH04_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 A boxplot highlighting the distribution of the Price feature and
    its right heavy tail on large price values
  prefs: []
  type: TYPE_NORMAL
- en: 'Not surprisingly, the box and whiskers are squeezed in the lower part of the
    chart and are almost indistinguishable from each other. A long queue of outliers
    elongates from the upper limit of the upper extremity of the boxplot. This is
    an evident case of a right-skewed distribution. In such cases, a standard solution
    to remediate the variable is to transform the target using a logarithm transformation.
    It is common practice to add a constant to offset the values into the positive
    number field for handling values of zero and below. In our case, it is unnecessary,
    since all the values are positive and above zero. In the following code snippet,
    we represent the transformed price feature by application of a logarithmic transformation
    (see figures 4.3 and 4.4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH04_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 A histogram of the Price feature being more symmetrical after log
    transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 A boxplot of the Price feature after log transformation signaling
    the persistence of extreme values at both the tails of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the distribution, represented both by the new histogram and the boxplot,
    is more symmetric, though it is evident that there are outlying observations on
    both sides of the distribution. Since our aim is illustrative, we can ignore the
    original distribution and focus on a meaningful target representation. For instance,
    we can keep only the price values below 1,000 (see figure 4.5). In the following
    code snippet, we produce a histogram focused only on price values below 1,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH04_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 A histogram of the Price feature for values under 1,000 still showing
    a right-skewed long tail
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the represented distribution is still right-skewed, but it resembles more
    common distributions found in e-commerce or other sales with long-tail products.
    In addition, if we focus on the range between 50 and 200, the distribution will
    appear more uniform (see figure 4.6). In the following code snippet, we restrict
    our focus further only to prices between 50 and 200 and plot the relative histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH04_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 A histogram of the Price feature for values between 50 and 200, showing
    distributed values across the range
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can create two masking variables, made of booleans, that can
    help us filter the target according to the type of algorithm we would like to
    test. The `price_capped` variable will be instrumental when demonstrating how
    certain machine learning algorithms can handle long tails easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Figure 4.7 shows the boxplot relative to the capped price, which presents right-sided
    outliers, but at least the boxplot is visible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 A boxplot of the Price feature for values under 1,000 showing a long
    tail of extreme values in its right tail
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8 shows the boxplot relative to the windowed price, showing no sign
    of outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH04_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 A boxplot of the Price feature for values between 50 and 200 showing
    a slightly right-skewed distribution with no extreme values
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing our exploration of the predictors and the target, we are ready
    to prepare four different targets that will be used along with our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We prepared two binary targets, `target_mean` and `target_median`, and a multiclass
    target with five distinct classes based on percentiles for classification purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, it is important to notice that our `target_median` is a binary
    balanced target. Hence, we can safely use accuracy as a good performance measurement.
    As a test, you get an almost equal number of cases for the positive and negative
    classes if you try to count the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You get the result
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Instead, if you try doing the same on the `target_mean` target variable, you
    get
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You will obtain a distribution that is imbalanced toward the negative cases;
    that is, there are more cases below the mean because of the skewed distribution
    we previously observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In such a case, when evaluating the results of a machine learning classifier,
    we prefer to use metrics such as the Receiver Operating Characteristic Area Under
    the Curve (ROC-AUC) or Average Precision—both quite sensible for ordering. Finally,
    as for the multiclass target, counting the cases for each one of the five classes
    reveals that they are also balanced in distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This command returns the result of
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As for the regression target, `target_regression` is the original target without
    transformations. However, we will use subsets of it and accordingly transform
    them based on the machine learning algorithm we will demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: Having completed our exploration of the data, the target, and some basic feature
    selection in the next paragraph, using a building blocks approach, we will prepare
    a few pipelines to accompany our discovery of different machine learning options
    for tabular data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Pipelines preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use the previously seen Pipeline and ColumnTransformer classes from
    Scikit-learn to prepare the pipelines. In a building blocks approach, we first
    create the different operations to be applied to the other data types that characterize
    features in a tabular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines three core procedures that will be reused multiple
    times in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Categorical one-hot encoding*—Categorical features are transformed into binary
    ones. If a value has never been seen before, it will be ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Numeric pass-through*—Numeric features are imputed using zero as a value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Numeric standardization*—After imputing missing values, numeric features are
    rescaled by subtracting their mean and dividing them by their standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code defining these procedures is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Setting up building blocks for tabular learning pipelines
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: ① Converts categorical features into one-hot encoded format
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces missing numeric values with zero
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline replaces missing numeric values with zero and standardizes the features
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can compose specific transformation pipelines that handle
    the data according to our needs for each machine learning algorithm. For instance,
    in this example, we set a pipeline that will one-hot encode low categorical features
    and just impute missing values as zero for numeric ones. Such a pipeline is made
    by the ColumnTransformer function, a glue function that combines operations applied
    on different sets of features simultaneously. This is an excellent transformative
    strategy suitable for most machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '① First step of the pipeline: one-hot encoding categorical features'
  prefs: []
  type: TYPE_NORMAL
- en: '② Second step of the pipeline: handling numeric features'
  prefs: []
  type: TYPE_NORMAL
- en: ③ The features not processed by the pipeline are dropped from the result.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Names of the features are kept as they originally are.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The result is always a dense matrix (i.e., a NumPy array)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can immediately run this code snippet and check how this pipeline transforms
    our Airbnb NYC data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is that the output is now a NumPy array made of floats and that
    the shape has increased to 13 columns. In fact, because of one-hot encoding, each
    value in the categorical features has turned into a separate feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The following section will explore the main machine learning techniques for
    tabular data. Each will be accompanied by its column transforming class, which
    will be integrated into the pipeline containing the model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Classical machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To explain the different models from the classic machine learning techniques
    for tabular data, we will first introduce the core characteristics of the algorithm,
    and then demonstrate a code snippet, seeing it at work on our reference tabular
    problem, the Airbnb NYC dataset. The following are some best practices that we
    will use in our examples to allow reproducibility and comparability of the different
    approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: We define a pipeline incorporating both data transformation and modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set an error measure, such as root mean squared error (RMSE) for regression
    or accuracy for classification, and measure it using the same cross-validation
    strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We report the average and standard deviation—crucial to figure out if the model
    has a constant performance across different data samples—of the cross-validated
    estimate of the error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous section, we introduced the different tools Scikit-learn offers
    for building data pipelines integrating feature processing and machine learning
    models. In this section, we will introduce the recommended evaluation measures
    and how the cross-validation estimate by the Scikit-learn `cross_validate` command
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review *evaluation metrics* first. We decided to use RMSE, a common measure
    for regression tasks, and accuracy, another standard measure for balanced binary
    and multiclass classification problems when the classes have approximately the
    same sample sizes. In subsequent chapters, we will also use metrics suitable for
    unbalanced classification problems, such as ROC-AUC and average precision.
  prefs: []
  type: TYPE_NORMAL
- en: '*Cross-validation* is the de facto standard in data science when you intend
    to estimate the expected performance of a machine learning model on any data different
    from the training data but drawn from the same data distribution. It is important
    to note that cross-validation estimates the future performance of your model based
    on the idea that your data may change in the future but won’t be radically different.
    To work correctly, the model expects that you will use the same features in the
    future and that they will have the same unique values (if a categorical feature)
    with similar distributions (both for categorical and numeric features) and, most
    importantly, that features will be in the same relation with your target variable.'
  prefs: []
  type: TYPE_NORMAL
- en: The assumption that data distributions will remain consistent in the future
    is frequently not true because economic dynamics, consumer markets, and social
    and political situations change rapidly in the real world. In the real world,
    your model may experience concept drifting, when the modeled relationships between
    features and targets no longer represent reality. Hence, your model will underperform
    when dealing with new data. Cross-validation is the best tool to evaluate your
    models at the time of their creation because it is based on your available information
    at that moment and because, if well designed, it is not influenced by the ability
    of your machine learning model to overfit the training data. Its usefulness stays
    true even after cross-validated results are disproved compared to future performances,
    usually because the underlying data distribution has changed. In addition, alternative
    methods, such as leave-one-out or bootstrapping, offer better estimates with increasing
    computational costs, whereas more straightforward methods, such as train/test
    split, are less reliable in their estimates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In its most uncomplicated flavor, the *k-fold cross-validation* (implemented
    in Scikit-learn with the KFold function: [https://mng.bz/VVM0](https://mng.bz/VVM0))
    is based on the splitting of your available training data into k partitions and
    the building of k versions of your model fed each time by different sets of k-1
    partitions and then tested on the remaining left out partition (the out-of-sample
    performance). The average and standard deviation of the resulting k scores will
    provide an estimate and a quantification of its uncertainty level to be used as
    a model estimate for expected performance on future unseen data. Figure 4.9 illustrates
    the k-fold validation when k is set to 5: each row represents the data partitioning
    at each fold. The validation part of a fold is always distinct from the others,
    and the training part is always differently composed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 How data is distributed between train and validation across the folds
    of a five-fold cross-validation strategy
  prefs: []
  type: TYPE_NORMAL
- en: Setting the correct value to k is a matter of how much training data you have
    available, how computationally costly it is to train your model on it, how the
    sample you received catches all the possible variations of the data distribution
    you want to model, and for what purpose you intend to get a performance estimate.
    As a general rule of thumb, values of k such as 5 or 10 are optimal choices, with
    k = 10 being more suitable for precise performance evaluation and k = 5 a good
    value compromising precision and computation costs for activities such as model,
    features, and hyperparameters evaluation (hence it will be used for our examples).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a general performance estimation for your model, you can build the necessary
    cross-validation iterations using a series of iterations on the KFold function
    (or its variations, offering sample stratification or control on the time dimension:
    [https://mng.bz/xKne](https://mng.bz/xKne)) or rely on the `cross_validate` procedure
    ([https://mng.bz/AQyK](https://mng.bz/AQyK)) that will handle everything for you
    and just return the results. For our purposes of testing different algorithms,
    `cross_validate` is quite handy because, given the proper parameters, it will
    produce a series of metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation test scores (out-of-sample performance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation train scores (in-sample performance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit time and predict time (to evaluate the computational cost)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trained estimators on the different cross-validation folds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All we have to do is provide an estimator, which can be any Scikit-learn object
    with a fit method, predictors and target, a cross-validation strategy, and a single
    or multiple scoring functions in a list. This estimator should be provided in
    the form of a callable to be created using the `make_scorer` command ([https://mng.bz/ZlAO](https://mng.bz/ZlAO)).
    In the next section, we will start seeing how we can get cross-validated performance
    estimates using such inputs, starting with classical machine learning algorithms
    such as linear regression and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Linear and logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *linear regression*, a statistical method that models the relationship between
    a dependent variable and one or more independent variables by fitting a linear
    equation to observed data, you first have all your features converted to numeric
    ones and put them into a matrix, including one-hot encoded categorical features.
    The algorithm’s goal is to optimally find the weight values in a column vector
    (the coefficients) so that, when multiplied against the matrix of features, you
    get a vector of results best approximating your targets (the predictions). In
    other words, the algorithm strives to minimize the residual sum of squares between
    the targets and the predictions obtained by multiplying features with the weight
    vector. In the process, you can consider using a prediction baseline (the so-called
    intercept or bias) or placing constraints on the weight values for them to be
    only positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the linear regression algorithm is just a weighted summation, you have
    to take care of three key aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure there are no missing values since they cannot be used for multiplications
    or additions unless you have imputed them to some value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure you have handled outliers because they can affect the algorithm’s work
    both in training and prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validate that the features and the target are linearly related as much as possible
    (i.e., they have a good Pearson correlation): features weakly related to the target
    tend just to add noise to the model, and they tend to make it underfit or even,
    when in high numbers, overfit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since a summation of your weighted features gives the prediction, it is easy
    to determine the most significant effect on the predicted output and how each
    feature contributes to it. Observing the coefficients relative to each feature
    gives you insight into how the algorithm behaves. Such understanding can prove
    valuable when you have to explain how the model works to regulatory authorities
    or stakeholders and when you want to check if the predictions are justifiable
    from the point of view of a hypothesis or expert knowledge of the domain.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are also hidden perils in the easy way that a regression model
    shows how it works under the hood. When two or more features in the data are highly
    correlated, a condition known as “multicollinearity” in statistics, the interpretation
    in a regression model can be much more complicated, even if both features effectively
    contribute to the prediction. Usually, only one of many takes a notable coefficient,
    whereas the others take small values as if they were unrelated to the target.
    In reality, the opposite is often true, and the relative ease in understanding
    the role of a feature in a regression prediction can lead to important conceptual
    misunderstanding.
  prefs: []
  type: TYPE_NORMAL
- en: Another great advantage of the linear regression algorithm is that, since it
    is just some multiplications and summations, it is a breeze to implement it on
    any software platform, even by hand-coding it in a script. Other machine learning
    algorithms are more complex to replicate, and hence, implementation from scratch
    of algorithms more complicated than a linear regression may be susceptible to
    errors and bugs. However, though unfeasible for delivering your projects, we have
    to note that hand-coding any machine learning model can be a valuable learning
    experience, allowing you to gain a deeper understanding of the inner workings
    of the algorithm and making yourself more equipped to troubleshoot and optimize
    the performance of the similar models in the future. We present some manageable
    from-scratch implementations of some algorithms for learning purposes in chapter
    5.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with an example of a linear regression model applied end to end
    to our Airbnb NYC data. The example follows the schema proposed in figure 4.10,
    a schema that we will replicate for every classical machine learning algorithm
    we will present and that is based on Scikit-learn’s pipelines and cross-validation
    evaluation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 Schema of how we will organize the examples for classical machine
    learning algorithms
  prefs: []
  type: TYPE_NORMAL
- en: The schema is quite linear. The input from a comma-separated values file first
    goes through a ColumnTransformer, which constitutes the data preparation part,
    which applies transformation on data, discards data, or lets it pass as it is,
    based on column names and then a machine learning model. Both are wrapped into
    a pipeline tested by a `cross_validate` function that executes cross-validation
    and records computation times, trained models, and performances on a certain number
    of folds. Finally, the results are selected to demonstrate how the model worked.
    In addition, we can access, passing by the pipeline, the model coefficients and
    weights to get more insights into the functionalities of the algorithm we tested.
  prefs: []
  type: TYPE_NORMAL
- en: Applying such a schema, we just use a vanilla linear regression model in listing
    4.3 since this algorithm usually does not need to specify any parameter. For special
    applications related to model interpretability, you could have specified the `fit_intercept`
    to be false to remove the intercept from the model and derive all the predictions
    from the features only or the positive parameter to be true to get only positive
    coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: ① ColumnTransformer, transforming data into numeric features and imputing missing
    data
  prefs: []
  type: TYPE_NORMAL
- en: ② Vanilla linear regression model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline assembling ColumnTransformer and model
  prefs: []
  type: TYPE_NORMAL
- en: ④ Cross-validation strategy based on five folds and random sampling
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Function for evaluation metric derived from mean squared error
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Automated cross-validate procedure
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Reports the results in terms of evaluation metric, standard deviation, fitting,
    and prediction time
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the listed code will produce the following RMSE results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: That’s a good result, obtained in a minimal time (using a standard Google Colab
    instance or a Kaggle notebook), and can act as a baseline for more sophisticated
    attempts. For example, if you try to run the code in listing 4.4, you will realize
    that you can get similar results with fewer but accurately prepared features.
    That’s called *feature engineering,* and the interesting point of doing it is
    that you can get better results or the same results but with fewer features meaningful
    for domain or business experts. For example, we create various new features in
    the code listing by generating binary features relative to specific values, combining
    features, and transforming them using a logarithmic function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Customized data preparation for linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates an empty DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: ② A binary column indicating whether the 'neighbourhood_group' is 'Manhattan'
  prefs: []
  type: TYPE_NORMAL
- en: ③ A binary column indicating whether the 'neighbourhood_group' is 'Queens'
  prefs: []
  type: TYPE_NORMAL
- en: ④ A binary column indicating whether the 'room_type' is 'Entire home/apt'
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ A column containing the natural logarithm of the values in the 'minimum_nights'
    column plus 1
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ A column containing the natural logarithm of the values in the 'number_of_reviews'
    column plus 1
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ A product of the binary 'neighbourhood_group_Manhattan' and 'room_type_Entire
    home/apt' columns
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ A product of 'availability_365' and the binary 'neighbourhood_group_Manhattan'
    column
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ A product of 'availability_365' and the binary 'room_type_Entire home/apt'
    column
  prefs: []
  type: TYPE_NORMAL
- en: The resulting RMSE is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Though the result is comparable to the previous experiment, this time you are
    using a dataset with fewer features that have been created by specific transformations,
    such as the one-hot encoding of categorical features, the transformations applied
    to numeric ones by specific functions (i.e., cubed, squared, logarithm, or square
    root transformation) and by multiplying features together. In our experience,
    a model presenting fewer, more meaningful features generated by reasoned feature
    engineering and domain expertise is usually more accepted by business users, even
    if it has comparable or even less predictive performance than a purely data-driven
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying features together is an operation that you find only when working
    with linear regression models; the obtained result is called interactions between
    features. Interactions work by multiplying two or more features to get a new one.
    All such transformations on the features are intended to render the relationship
    between each feature and the target as linear as possible. Good results can be
    obtained automatically or based on your knowledge of the data and the problem.
    Applying such transformations to the features is typical of the family of linear
    regression models. They have little or no effect on the more complex algorithms
    we will explore later in this chapter and subsequent chapters. Investing time
    in defining how the features should be expressed is both an advantage and a disadvantage
    of linear regression models. However, there are ways to automatically perform
    it using regularization, as we will propose in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss regularization in linear models (linear regression
    and logistic regression). Regularization is the best solution to implement when
    you have many features and their reciprocal multicollinearity (you have multicollinearity
    when two predictors are highly correlated with one another) doesn’t allow the
    linear regression model to find the best coefficients for the prediction because
    they are unstable and unreliable—for instance, showing a coefficient you didn’t
    expect in terms of sign and size.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Regularized methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression models are usually simple enough for humans to understand
    directly as formulas of coefficients applied to features. This means that, when
    applied to a real-world problem, they can turn out to be a rough approximation
    of complex dynamics and thus systematically miss correct predictions. Technically,
    they are models with a high bias. A remedy for this is to make their formulations
    more complex by adding more and more features and their transformations (logarithmic,
    squared, root transformations, and so on) and by making features interact with
    many others (through multiplication). In this way, a linear regression model can
    diminish its bias and become a better predictor. At the same time, however, the
    variance of the model will also increase, and it can start overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Occam’s razor principle, which states that among competing hypotheses, the
    one with the fewest assumptions should be selected ([https://mng.bz/RV40](https://mng.bz/RV40)),
    works perfectly for linear models, whereas it doesn’t matter for neural networks
    applied to tabular data where the more complex, the better. Hence, linear models
    should be as simple as possible to meet the needs of the problem. Here is where
    regularization enters the scene, helping you reduce the complexity of a linear
    model until it fits the problem. Regularization is a technique used to reduce
    overfitting in machine learning by limiting the complexity of the model, thus
    effectively improving its generalization performance. Regularization works because
    the linear regression model is penalized as it looks for the best coefficients
    for its predictions. The used penalization is based on the summation of the coefficients.
    Therefore, the regression model is incentivized to keep them as small as possible,
    if not to set them to zero. Constraining regression coefficients to limit their
    magnitude has two significant effects:'
  prefs: []
  type: TYPE_NORMAL
- en: It avoids any form of data memorization and overfitting (i.e., certain specific
    coefficient values to be taken when there is a large number of features compared
    to the available examples).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As coefficient shrinking happens, estimates are stabilized because multicollinear
    features will have the values of their coefficients resized or concentrated on
    only one of the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the optimization process, coefficients are updated multiple times, and these
    steps are called iterations. At each step, each regression coefficient incorporates
    a correction toward its optimal value. The optimal value is determined by the
    gradient, which can be intended as a number representing the direction that greatly
    improves the coefficient at that step. A more detailed explanation closes this
    chapter. Penalization is a form of constraint that forces the weights deriving
    from the optimization of the model to have specific characteristics. We have two
    variants of regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first variant is where the penalization is computed by summing the absolute
    values of the coefficients: this is called L1 regularization. It makes the coefficients
    sparse because it can push some coefficients to zero, making their related features
    irrelevant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second option is where the penalization is computed by summing the squared
    coefficients: this is called L2 regularization, and its effect is generally to
    reduce the size of the coefficients (it is also relatively fast to compute).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization (or Lasso regression) pushes many coefficients to zero values,
    thus operating an implicit selection of the useful features (setting a coefficient
    to zero means that a feature doesn’t play any role in prediction). In addition,
    coefficients are always pushed toward zero with the same strength (technically,
    the gradients toward the solution are always +1 or –1). Hence, through the optimization
    steps, the features less associated with the target tend quickly to be assigned
    a zero coefficient and become totally irrelevant regarding the predictions. In
    short, if two or more features are multicollinear and all quite predictive, by
    applying L1 regularization, you will have only one of them with a coefficient
    different from zero.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, in L2 regularization (or Ridge regression), the fact that coefficients
    are squared prevents negative and positive values from canceling each other in
    the penalization and puts more weight on larger coefficients. The result is a
    set of generally smaller coefficients, and multicollinear features tend to have
    similar coefficient values. All the features involved are included in the summation.
    You can notice better important features because, contrary to what happens with
    standard regression, the role of a feature in the prediction is not hidden by
    its correlation with other features. L2 regularization tends to attenuate the
    coefficients. It does so proportionally during the optimization steps; technically,
    the gradients toward the solution tend to be smaller and smaller. Hence, coefficients
    can reach the zero value or be near it. Still, even if the feature must be completely
    irrelevant to the prediction, it takes many optimization iterations and is quite
    time-consuming. Consequently, reprising the previous example of two or more multicollinear
    features in L2 regularization, instead of L1 regression that keeps only one non-zero
    coefficient, all the features would have a non-zero, similar size coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we first try to create new features through systematic interactions
    between our available features and then perform an L2 and L1 penalized regression
    to compare their results and resulting coefficients. PolynomialFeatures is a Scikit-learn
    function ([https://mng.bz/2ynd](https://mng.bz/2ynd)) that automatically creates
    multiplications between features by multiplying them many times with other features
    and by themselves. The process is reminiscent of the mathematical *polynomial
    expansion* where a power of sums is expressed into its single terms:'
  prefs: []
  type: TYPE_NORMAL
- en: (*a* + *b*)² = *a*² + 2*ab* + *b*²
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn makes it easier because when you state a degree, the function automatically
    creates the polynomial expansions up to that degree. You can decide whether to
    keep only the interactions. Such a process is interesting for a regression model
    because
  prefs: []
  type: TYPE_NORMAL
- en: '*Interactions* help the regression model to better take into account the conjoint
    values of more features since features usually do not relate to the target in
    isolation but in synergy with others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of *powers* of a feature helps to model it as a curve. For instance,
    a + a² is a curve in the shape of a parabola.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though using polynomial expansion can avoid the heavy task of creating specific
    features for your problem, it has a downside because it dramatically increases
    the number of features your model uses. More features usually provide more predictive
    power, but they also mean more noise, more multicollinearity, and more chances
    that the model has just to memorize examples and overfit the problem. Applying
    penalties can help us fix this problem with the L2 penalty and select only the
    features to be kept with the L1 penalty.
  prefs: []
  type: TYPE_NORMAL
- en: In the code in listing 4.5, we test applying L2 and, successively in listing
    4.6, L1 regularization to the same polynomial expansion. It is important to note
    the effect of each kind of regularization. In this first example, we apply L2
    regularization (Ridge). Since regularization makes sense if you have plenty of
    features for your prediction, we create new features from the old ones using a
    polynomial expansion. Our ridge model is then set to a high alpha value to handle
    the increased number of collinear features.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 L2 regularized linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: ① PolynomialFeatures instance performing second-degree polynomial expansion
    on the features
  prefs: []
  type: TYPE_NORMAL
- en: ② A Ridge regression model instance with a regularization strength (alpha) of
    2,500
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline for column transformation, polynomial expansion, standardization,
    and Ridge regression modeling
  prefs: []
  type: TYPE_NORMAL
- en: ④ Five-fold cross-validation using the defined pipeline and calculating RMSE
    scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test RMSE scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'The script results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: If we count the number of non-zero coefficients (after rounding to five decimals
    to exclude extremely small values), we get
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Ninety-one coefficients out of 105 have non-zero values.
  prefs: []
  type: TYPE_NORMAL
- en: In the next example, we apply an L1 regularization and compare the results with
    the previous example. The procedure is the same as the last code listing, though
    we resort to a lasso model this time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 L1 regularized linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: ① A Lasso regression model instance with a regularization strength (alpha) of
    0.1
  prefs: []
  type: TYPE_NORMAL
- en: ② Pipeline applying column transformation, polynomial expansion, standardization,
    and Lasso regression modeling
  prefs: []
  type: TYPE_NORMAL
- en: ③ Five-fold cross-validation using the defined pipeline and calculating RMSE
    scores
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints the mean and standard deviation of the test RMSE scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If we check how many coefficients have non-zero-values by taking the first
    model built by the cross-validation cycle, this time we have fewer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: With 53 non-zero coefficients, the number of working coefficients has been halved.
    By increasing the alpha parameter of the Lasso call, we can obtain an even sharper
    reduction of used coefficients, albeit at the price of a higher computation time.
    There’s a sweet spot after which applying a higher L1 penalty doesn’t improve
    the prediction results. For prediction purposes, you have to find the correct
    alpha by trial and error or using convenient automatic functions such as LassoCV
    ([https://mng.bz/1XoV](https://mng.bz/1XoV)) or RidgeCV ([https://mng.bz/Pdn9](https://mng.bz/Pdn9))
    that will do the experimentation for you.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, regularization is also used in neural networks. Neural networks
    use sequential matrix multiplications based on matrices of coefficients to transit
    from features to predictions, which is an extension of the working of linear regression.
    Neural networks have more complexities, though; yet in such an aspect of matrix
    multiplication, they resemble a regression model. Based on similar workings, you
    may find it beneficial for your tabular data problem to fit a deep learning architecture
    and, in doing so, to apply an L2 penalty, so the coefficients of the network are
    attenuated and distributed, and/or an L1 penalty, so coefficients are instead
    sparse with many of them set to zeros. In the next section, we will continue our
    discussion of linear models by discovering how to solve a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear regression model can be effectively extended to classification. In
    a binary classification problem, where you have two classes (a positive one and
    a negative one), you use the same approach as in a regression (feature matrix,
    vector of coefficients, bias). Still, you transform the target using the logit
    function (for details about this statistical distribution, see [https://mng.bz/JY20](https://mng.bz/JY20)).
    The transformative function is called the *link function*. On the optimization
    side, the algorithm uses as a reference the Bernoulli conditional distribution
    (for revising this distribution, see [https://mng.bz/wJoq](https://mng.bz/wJoq))
    instead of the normal distribution. As a result, you get output values ranging
    from 0 to 1, representing the probability that the sample belongs to the positive
    class. This is called logistic regression. Logistic regression is quite an intuitive
    and practical approach to solving binary classification problems and multiclass
    and multilabel ones.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 4.7, we replicate the same approach as seen with linear regression—this
    time trying to build a model to guess if an example has a target value above the
    median. Please note that transformations are the same, though we use a logistic
    regression model this time. Our target is a class that tells if the target value
    is above the median. Such a target is a binary balanced outcome, where half of
    the labels are positive and half are negative.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Logistic regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: ① A logistic regression model instance with the “saga” solver, no penalty, and
    a maximum of 1,000 iterations
  prefs: []
  type: TYPE_NORMAL
- en: ② A column transformer applying one-hot encoding to categorical features and
    standardization to numeric features
  prefs: []
  type: TYPE_NORMAL
- en: ③ A pipeline that sequentially applies column transformation and logistic regression
    modeling
  prefs: []
  type: TYPE_NORMAL
- en: ④ Five-fold cross-validation using the defined pipeline and calculating accuracy
    scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'The script results in the following scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'As the feature processing is the same, we just focus on noticing how the logistic
    regression has some specific parameters with respect to linear regression. In
    particular, you can set the penalty directly without changing the algorithm and
    decide what optimizer will be used (using the parameter solver). Each optimizer
    allows specific penalties, and it can be more or less efficient based on the characteristics
    of your data:'
  prefs: []
  type: TYPE_NORMAL
- en: lbfgs for L2 or no penalty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: liblinear for L1 and L2 penalties—better for smaller datasets, limited to one-versus-rest
    schemes for multiclass problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: newton-cg for L2 or no penalty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: newton-cholesky for L2 or no penalty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sag for L2 or no penalty—ideal for large datasets. It requires standardized
    features (or features all with similar scale/standard deviation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saga for no penalty, L1, L2, elasticnet (a mix of L1 and L2) penalties—ideal
    for large datasets, it requires standardized features (or features all with similar
    scale/standard deviation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In listing 4.8, we use an L2 penalty on the multiclass target to test how multiple
    targets are easily dealt with using the `multi_class` parameter set to “ovr” (one-versus-rest),
    a solution that takes a multiclass problem and builds a binary model for each
    of the classes to be predicted. At prediction time, prediction probabilities across
    all the classes are normalized to sum 1.0, and the class corresponding to the
    highest probability is taken and the predicted class. Such an approach is analogous
    to the softmax function approach used in neural networks where a vector of arbitrary
    real values is turned into a probability distribution, where the sum of all elements
    is 1 (for a more detailed explanation of softmax, see [https://mng.bz/qxYw](https://mng.bz/qxYw)).
    The alternative to the one-versus-rest approach is the multinomial option, where
    a single regression model directly models the probability distribution across
    all classes simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: The multinomial approach is preferred when inter-class relationships are important
    (e.g., for ranking or confidence-based decisions) or when a compact, single-model
    solution is desired.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 L2 regularized multiclass linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: ① Logistic regression model instance with L2 penalty, regularization C=0.1,
    “sag” solver, “ovr” multiclass strategy
  prefs: []
  type: TYPE_NORMAL
- en: ② Column transformer that applies one-hot encoding to categorical features and
    standardization to numeric features
  prefs: []
  type: TYPE_NORMAL
- en: ③ Pipeline that sequentially applies column transformation and logistic regression
    modeling
  prefs: []
  type: TYPE_NORMAL
- en: ④ Cross-validation using the defined pipeline and calculating accuracy scores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the mean and standard deviation of the test accuracy scores from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'Predicting the target as a class is certainly more complicated than guessing
    if the target price is over a threshold value or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: From this output, it is important to notice how the training time for a cross-validation
    fold has skyrocketed 10 times more. The reason is that applying a penalty to the
    coefficients involves more iterations of the algorithm’s optimization processes
    before reaching a stable result and because now a model for each class is being
    built. As a general rule, consider that penalization requires longer computations
    for the L2 penalty and even longer for the L1 penalty. By setting the `max_iter`
    parameter, you can impose a limit to the algorithm’s iterations, but be aware
    that the result you obtain by cutting off the time required for the algorithm
    to converge won’t be assured to be the best.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Generalized linear methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea of extending linear regression to binary classification by logit transformation
    can be applied to distributions other than Bernoulli conditional distribution.
    This is dictated by the target that may represent categorical data, count data,
    or other data whose distribution is known not to be from a normal distribution.
    As we have seen in the previous paragraph, multiclass problems can be modeled
    using the Bernoulli distribution (the one-versus-rest strategy of fitting multiple
    logistic regressions) and the multinomial one. Other problems, more typical of
    domains such as finance or insurance, require different approaches. For instance,
    the Scikit-learn package mentions a few real-world applications and their best-fitting
    distributions (for reference, see [https://mng.bz/7py9](https://mng.bz/7py9)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Climate modeling*—Number of rain events per year (Poisson distribution for
    count data and discrete events). The Poisson distribution is used for modeling
    events such as the number of calls to a call center or the number of customers
    visiting a restaurant), amount of rainfall per event (using Gamma distribution,
    a theoretical distribution useful for modeling because of its skewness and long
    tail), or total rain precipitation per year (Tweedie distribution, a distribution
    which is a compound of Poisson and Gamma distributions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Risk modeling or insurance policy pricing*—Number of claim events or policyholder
    per year (Poisson), cost per event (Gamma), the total cost per policyholder per
    year (Tweedie).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Predictive maintenance*—Number of production interruption events per year
    (Poisson), the duration of interruption (Gamma), and the total interruption time
    per year (Tweedie).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 4.11 shows the three distributions—Poisson, Tweedie, and Gamma—for different
    averages. The Tweedie distribution is calculated for power equal to 1.5, a blend
    between the Poisson and Gamma distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 Comparing Poisson, Tweedie, and Gamma distributions at different
    mean values (mu) of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you may try any distribution you want—even a plain regression model—for
    any such situation. However, approaching each of them using the appropriate generalized
    linear model that optimizes that specific distribution assures the best result
    in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t enter into the specifics of each distribution; you just need to know
    that the Swiss Army Knife of general linear models is the `TweedieRegressor` ([https://mng.bz/mGOr](https://mng.bz/mGOr)).
    This Scikit-learn implementation, depending on the power parameter, can allow
    you to quickly test normal distribution (a regular regression), Poisson distribution
    ([https://mng.bz/4a4w](https://mng.bz/4a4w)), Gamma distribution ([https://mng.bz/QDvG](https://mng.bz/QDvG)),
    Inverse Gaussian distribution (for nonnegative positively skewed data), and a
    blend of Gamma and Poisson (the Tweedie distribution) (see table 4.1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Power values and their corresponding statistical distributions
  prefs: []
  type: TYPE_NORMAL
- en: '| Power | Distribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Normal |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Poisson |'
  prefs: []
  type: TYPE_TB
- en: '| (1,2) | Compound Poisson Gamma |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Gamma |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Inverse Gaussian |'
  prefs: []
  type: TYPE_TB
- en: 'In listing 4.9, we test the different distributions offered by the TweedieRegressor
    on the entire distribution of prices of the Airbnb NYC dataset, a model fitting
    that we previously avoided because of the heavy distribution tails revealed by
    the EDA. We do so by testing each of these distributions one by one on the full
    range of price values since we are confident that using such specialized distribution
    will solve our problem of a target with heavy tails. It is important to remember
    that such distributions have limitations due to their formulations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Norma**l*—Any kind of value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Poisso**n*—Zero or positive values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tweedie, Gamma, Inverse Gaussia**n*—Only non-zero positive values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This implies that you must adapt your data if you have negative or zero values
    by adding an offset value. Hence, depending on the modeled distribution, we clip
    the target values to a lower bound based on the aforementioned limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 Tweedie regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: ① A list of experiments, made of a distribution name, power parameter, and minimum
    target value
  prefs: []
  type: TYPE_NORMAL
- en: ② Loops through the experiments list with distribution names and power parameters
  prefs: []
  type: TYPE_NORMAL
- en: ③ Instance of the TweedieRegressor model with the specified power parameter
    for the current experiment
  prefs: []
  type: TYPE_NORMAL
- en: ④ Clips the target regression data to a minimum value, according to the used
    distribution
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the experiment name along with the results from cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting best-fitting are the Poisson and Tweedie with power 1.5 distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: It is crucial to remember that the secret of the performances of the generalized
    linear models lies in the specific distribution they strive to model during the
    optimization phase. When faced with similar problems, we could resort to similar
    distributions on some more advanced algorithms than generalized linear models,
    particularly on gradient boosting implementations such as XGBoost or LightGBM,
    which will be discussed in the next chapter. In the next section, we will deal
    with a different approach related to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Handling large datasets with stochastic gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When your tabular dataset cannot fit into your system’s memory, whether it
    is a cloud instance or your desktop computer, your options in modeling shrink.
    Apart from deep learning solutions, which will be discussed in the third part
    of this book, one other option, using classical machine learning, is to resort
    to out-of-core learning. In out-of-core learning, you keep your data in its storage
    (for instance, your data warehouse), and you have your model learn from it bit
    by bit, using small samples extracted from your data, called *batches*. This is
    practically feasible because modern data storage allows for the picking of specific
    data samples at a certain cost in terms of latency: the time interval between
    the initiation of a data-related operation and its completion or response. In
    addition, there are also tools for handling and processing data on the fly (for
    instance, Apache Kafka or Amazon Kinetics) that can redirect the data to out-of-core
    learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also algorithmically feasible because of linear/logistic regression models.
    Both models are made up of additions of coefficients relative to the features
    you use for learning. Out-of-core learning involves first estimating these coefficients
    using some small samples from the data and then updating such coefficients using
    more and more batches extracted from your data. In the end, though the process
    is particularly long, your final estimated coefficient would not be much different
    from the ones you would have obtained if you could have fit all the data into
    the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'How many such batches you have to use for your out-of-core modeling, and if
    you have to reuse them multiple times, is a matter of empirical experimentation:
    it depends on the problem and the data you are using. Though providing new batches
    of unseen data may simply prolong your training phase, having the algorithm see
    the same batches again may cause it to overfit. Unfortunately, in most situations,
    you need to reiterate the same batches multiple times because out-of-core learning
    is not as straightforward as when optimizing; it takes a long time, and you may
    need more passes on the same data, even if we are talking about massive amounts
    of it. Fortunately, you can rely on regularization techniques, such as L1 and
    L2 regularization, to avoid overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: In listing 4.10, we reprise our logistic regression example and make it out-of-core.
    First, we split our data into a training set and a test set since it is complicated
    to create a cross-validation procedure when using out-of-core learning strategies.
    In real out-of-core learning settings, cross-validation is not just complicated
    but often infeasible because, in such settings, you often handle examples a single
    time. After all, they are streamed from sources and often discarded. The usual
    validation strategy is to collect a list of examples for testing purposes or to
    use a batch of every n-one as an out-of-sample testing batch. In our example,
    we prefer reserving a test set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.10 Out-of-core logistic regression with L2 regularization
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a function to generate batches of data for training
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates batches of data indices for processing
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shuffles the sequence of examples if a random state is provided
  prefs: []
  type: TYPE_NORMAL
- en: ④ Yields batches of input features and corresponding labels
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates an instance of the SGDClassifier model with logistic loss, averaging,
    L2 penalty, and alpha regularization
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Splits the data and target into training and testing sets using an 80-20 ratio
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Iterates through training data batches, fitting the column transformer on
    the first batch
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Uses partial fitting to train the model on the first batch, specifying the
    classes
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Uses partial fitting to further train the model on subsequent batches
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Prints accuracy score of test data predictions
  prefs: []
  type: TYPE_NORMAL
- en: The train data is then split into multiple batches, and each batch is proposed
    for learning to the *stochastic gradient descent* (SGD) algorithm. SGD is not
    a stand-alone algorithm but an optimization procedure for linear models, optimizing
    the model weights by iteratively learning them from small batches of the data
    or even just single examples taken alone. It is based on the *gradient descent*
    optimization procedure and is also used in deep learning. Gradient descent starts
    with an initial guess for the model weights and computes the error. The next step
    involves computing the gradient of the error, which is obtained by taking the
    negative of the vector that contains the partial derivatives of the error with
    respect to the model weights. Since the gradient can be interpreted as taking
    the steepest descent on an error surface, a common example for gradient descent
    is always to figure it as descending from highs in the mountains to the lowest
    valley by taking the steepest path downward. The “mountains” in this analogy represent
    the error surface, and the “lowest valley” represents the minimum of the error
    function. Figure 4.12 visually represents this process by the progressive descent
    from a random high place to the lowest point in a bowl-shaped error curve.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the analogy, it is important to remember that the gradient determines
    how the weights should be adjusted to reduce the error at that step. Through repeated
    iterations, the error can be minimized by adjusting the model’s weights. However,
    how the weights are updated can significantly affect the outcome. If the updates
    are too large and decisive, the algorithm may take overly wide steps, potentially
    causing the model to overshoot the target and climb the error curve. In the worst-case
    scenario, this can result in a continuous worsening of the error, with no possibility
    of recovery. Conversely, taking smaller steps is generally safer but may be computationally
    burdensome. The size of such steps is decided by the learning rate, a parameter
    that regulates how the updates are done.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 Gradient descent optimization in action in a simple optimization
    landscape
  prefs: []
  type: TYPE_NORMAL
- en: Linear models can be optimized easily using gradient descent because their error
    surface is simple and bowl-shaped. However, more complex models like gradient
    boosting (which will be discussed in the next chapter) and deep learning architectures
    may encounter challenges in optimization due to their higher complexity, with
    interrelated parameters and a more complex error landscape. Depending on the starting
    point, as illustrated in figure 4.13, these models may become stuck in a local
    minimum or plateau during optimization, leading to suboptimal results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH04_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 Gradient descent in a complex error landscape showing how local
    minima and plateaus can lead to less-than-optimal solutions
  prefs: []
  type: TYPE_NORMAL
- en: Learning a linear model by SGD is made possible using Scikit-learn’s method
    `partial_fit`, which, after an informed start (the algorithm needs to know the
    target labels), can learn by partially fitting one batch after the other. The
    same procedure is repeated multiple times, called iterations or epochs, to consolidate
    and improve the learning, though repeating the same examples too often may also
    cause overfitting. The algorithm will see, though in a different order, the same
    examples multiple times and update its coefficients every time. To avoid abrupt
    changes in the coefficients, which frequently occur when an outlier is present
    in the batch, the updated coefficients are not substituted for the existing ones.
    Instead, they are averaged together, allowing a more gradual transition.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the learning process is completed, you will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The result is quite comparable with in-core learning logistic regression. Out-of-core
    learning, though limited to only the simplest machine learning algorithms such
    as linear or logistic regression, is an effective way to train on your tabular
    data when too many samples cannot fit into memory. All deep learning solutions
    also use the idea of a stream of batches, and it will be discussed again in the
    chapters devoted to deep neural network methods for tabular data, together with
    strategies such as *early stopping*, a technique interrupting the iterations over
    data when necessary to avoid overfitting the data because of an excessive exposition
    of the algorithm to examples seen in previous iterations.
  prefs: []
  type: TYPE_NORMAL
- en: We can now anticipate that a fundamental recipe of such learning strategies
    is the randomization of the order of the examples. Since the optimization is progressive,
    if your data is ordered in a specific way, it will cause a biased optimization,
    which may result in suboptimal learning. Repeating the same batches in the same
    order can negatively influence your results. Hence, randomizing the order is critical
    for a better-trained algorithm. Another important point with SGD, however, is
    the data preparation phase. In such a phase, you should include all feature rescaling
    operations, because the optimization process is sensible to the scale of features
    and all the feature engineering and feature interaction computations, and set
    it in a way as deterministic as possible since it could be difficult to use global
    parameters, such as the maximum/minimum or average and the standard deviation
    of a feature, when your data is split into multiple batches.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6 Choosing your algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a general rule of thumb, you should first consider that machine learning
    algorithms scale differently based on how many rows and columns you have. Starting
    from the number of available rows, you must strictly resort to simple rule-based
    or statistical-based algorithms when operating with about or fewer than 10² rows
    of data. For up to 10³ rows, models based on linear combinations, such as linear
    and logistic regression, are best suited because they tend not to overfit the
    little data available. You usually cannot tell what algorithm will work better
    from about 10³ to 10⁴ – 10⁵ rows. Hence, it is all a matter of testing and experimenting.
    Here, deep learning solutions may outrun other choices only if there is some structure
    to exploit, such as an ordered series of information or a hierarchical structure.
    Up to 10⁹ rows, solutions from the gradient boosting family are likely the most
    effective. Again, you may find that something like out-of-core learning is a much
    better solution for specific problems, such as in the advertisement industry,
    where you have many fixed interactions that you need to estimate—for instance,
    between display devices, websites, and advertisements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out-of-core learning refers to a learning strategy that certain machine learning
    algorithms can adopt when learning from data: instead of learning all at once
    from the data, they learn bit by bit from smaller samples of the data, the batches,
    or even from single examples, one by one, which is also mentioned as online learning.
    Finally, in our experience, in situations with datasets above 10⁹ rows, deep learning
    solutions, and some out-of-core learning algorithms tend to perform better because
    they can effectively deal with such an amount of data, whereas other machine learning
    algorithms may be forced to learn from subsamples from the data or find other
    suboptimal solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding columns, we find that some algorithms need to scale better with datasets
    characterized by multiple columns, especially if they present sparse information—that
    is, many binary features. The sparser the datasets, which can be measured by the
    percentage of zeros values in relation to the total numeric values in the dataset,
    the earlier you may need to apply online learning algorithms or deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, apart from scalability reasons, which relate to memory and computational
    complexity, each machine learning solution also suits different needs in terms
    of model control, openness, and understandability of the solution. In such a way,
    the variety of needs in tabular problems and of models in machine learning defies
    the notion of one best algorithm that is all you need for your work. In other
    words, it is not just that you need to try more machine learning models because
    “there’s no free lunch,” as stated in the well-known theorem by David Wolpert
    and William Macready (see [http://www.no-free-lunch.org](http://www.no-free-lunch.org)
    for more details). More often than expected, there are cases where the underdog
    algorithm surprisingly beats the best-in-class algorithm. The necessity of more
    algorithms is mostly dictated because, as you change perspective on your problem
    as an artisan/artist creating their work from different angles, you may need different
    tools for the task.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will present a more powerful class of machine learning
    algorithms, the ensembles, and finally, the gradient boosting family and its successful
    and popular implementations, such as XGBoost and LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Determining what machine learning algorithm involves several factors: the number
    of examples and features, the expected performances, speed at prediction time,
    and interpretability. As a general rule of thumb,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical machine learning is suitable for datasets with few cases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical machine learning is suitable for datasets with a moderate number of
    cases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting algorithms are particularly effective for datasets with a
    moderate to large number of cases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning solutions are the most feasible and effective for datasets with
    massive amounts of data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-learn is an open-source library for machine learning that offers a wide
    range of models for classification and regression, as well as functions for clustering,
    dimensionality reduction, preprocessing, and model selection. We can summarize
    its core advantages as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent API across models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports in-memory and out-of-core learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports working with pandas DataFrames
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for tabular problems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to install
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive documentation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear regression is the summation of weighted features that have been converted
    to numeric values (one-hot encoding for categorical features):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm finds optimal weight values (coefficients) to minimize the residual
    sum of squares between targets and predictions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression is easy to explain and understand how each feature contributes
    to the final result.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A high correlation between features (multicollinearity) can cause conceptual
    misunderstanding.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression is computationally simple and easy to implement.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression is limited in its ability to fit complex problems with nonlinear
    data unless features are carefully prepared beforehand with feature engineering,
    such as creating polynomial features, which can help linear regression capture
    nonlinear relationships.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularization is used to prevent overfitting by reducing the complexity of
    a regression model and improving its generalization performance. There are two
    types of regularization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization (or Lasso regression) pushes many coefficients to zero values,
    thus making some features irrelevant in the model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 regularization generally reduces the size of coefficients.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization can be helpful for feature selection, while L2 regularization
    reduces overfitting when using many features while being faster to compute.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression can be extended to classification problems using the logit
    function to transform the target and the Bernoulli conditional distribution to
    optimize the algorithm. This results in a logistic regression model that can be
    used for binary classification, multiclass, and multilabel problems. Logistic
    regression is easy to implement and understand but has the same limitations as
    linear regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same approach of transforming the target can be applied to other distributions
    as well, such as Poisson and Gamma, depending on the nature of the data. The resulting
    generalized linear models can be used for various real-world applications, such
    as climate modeling, risk modeling, and predictive maintenance. However, it’s
    important to note that the results may not be optimal without a proper understanding
    of the specific distribution applied to each situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
