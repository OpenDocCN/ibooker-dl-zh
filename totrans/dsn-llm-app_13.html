<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Interfacing LLMs with External Tools"><div class="chapter" id="ch10">
<h1><span class="label">Chapter 10. </span>Interfacing LLMs with External Tools</h1>


<p>In the first two parts of the book<a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="interfacing with external tools" id="xi_LLMsLargeLanguageModelsinterfacingwithexternaltools10435"/>, we have seen how impactful standalone LLMs can be in solving a wide variety of tasks. To effectively harness their full range of capabilities in an organization, they have to be integrated into the existing data and software ecosystem. Unlike traditional software systems, LLMs can generate autonomous actions to interact with other ecosystem components, bringing a degree of flexibility never seen before in the software world. This flexibility unlocks a whole host of use cases that were previously considered impossible.</p>

<p>Another reason we need LLMs to interact with software and external data: as we know all too well, current LLMs have significant limitations, some of which we discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>. To recap some key points:</p>

<ul>
<li>
<p>Since it is expensive to retrain LLMs or keep them continuously updated, they have a knowledge cutoff date and thus possess no knowledge of more recent events.</p>
</li>
<li>
<p>Even though they are getting better over time, LLMs don’t always get math right.</p>
</li>
<li>
<p>They can’t provide factuality guarantees or accurately cite the sources of their outputs.</p>
</li>
<li>
<p>Feeding them your own data effectively is a challenge; fine-tuning is nontrivial, and in-context learning is limited by the length of the effective context window.</p>
</li>
</ul>

<p>As we have been noticing throughout the book, the consolidation effect is leading us to a future (unless we hit a technological wall) where many of the aforementioned limitations might be addressed within the model itself. But we don’t necessarily need to wait for that moment to arrive, as many of these limitations can be addressed today by offloading the tasks and subtasks to external tools.</p>

<p>In this chapter, we will define the three canonical LLM interaction paradigms and provide guidance on how to choose between them for your application. Broadly speaking, there are two types of external entities that LLMs need to interact with: data stores and software/models, collectively called tools.  We will demonstrate how to interface LLMs  with various tools like APIs and code interpreters.  We will show how to make the best use of libraries like LangChain and LlamaIndex, which have vastly simplified LLM integrations. We will explore the various scaffolding software that needs to be constructed to facilitate seamless interactions with the environment. We will also push the limits of what today’s LLMs are capable of, by demonstrating how they can be deployed as an agent that can make autonomous decisions.</p>






<section data-type="sect1" data-pdf-bookmark="LLM Interaction Paradigms"><div class="sect1" id="id161">
<h1>LLM Interaction Paradigms</h1>

<p>Suppose you have a task you want the LLM to solve<a data-type="indexterm" data-primary="interaction paradigms" id="xi_interactionparadigms102050"/>. There are several possible options:</p>

<ul>
<li>
<p>The LLM uses its own memory and capabilities encoded in its parameters to solve the task.</p>
</li>
<li>
<p>You feed the LLM all the context it needs to solve the task within the prompt, and the LLM uses the provided context and its capabilities to solve it.</p>
</li>
<li>
<p>The LLM doesn’t have the requisite information or skills to solve this task, so you update the model parameters (fine-tuning etc., as detailed in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#llm-fine-tuning">6</a>–<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#ch8">8</a>) so that it is able to activate the skills and knowledge needed to solve it.</p>
</li>
<li>
<p>You don’t know a priori what context is needed to solve the task, so you use mechanisms to automatically fetch the relevant context and insert it into the prompt (passive approach).</p>
</li>
<li>
<p>You provide explicit instructions to the LLM on how to interact with external tools and data stores to solve your task, which the LLM follows (explicit approach).</p>
</li>
<li>
<p>The LLM breaks the task into multiple subtasks if needed, interacts with its environment to gather the information/knowledge needed to solve the task, and delegates subtasks to external models and tools when it doesn’t have the requisite capabilities to solve that subtask (autonomous approach).</p>
</li>
</ul>

<p>As you can see, the last three involve the LLM interacting with its environment (passive, explicit, and autonomous). Let’s explore the three interaction paradigms in detail.</p>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Passive Approach"><div class="sect2" id="id162">
<h2>Passive Approach</h2>

<p><a data-type="xref" href="#passive-interaction">Figure 10-1</a> shows the typical workflow of an application<a data-type="indexterm" data-primary="passive approach, LLM interaction paradigm" id="xi_passiveapproachLLMinteractionparadigm103869"/><a data-type="indexterm" data-primary="data stores, LLM interaction with" id="xi_datastoresLLMinteractionwith103869"/> that involves an LLM passively interacting with a data store.</p>

<figure><div id="passive-interaction" class="figure">
<img src="assets/dllm_1001.png" alt="Passive Interaction" width="600" height="173"/>
<h6><span class="label">Figure 10-1. </span>An LLM passively interacting with a data store</h6>
</div></figure>

<p>A large number of use cases involve leveraging LLMs to use your own data. Examples include building a question-answering assistant over your company’s internal knowledge base that is spread over a bunch of Notion documents, or an airline chatbot that responds to customer queries about flight status or booking policies.</p>

<p>To allow the LLM to access external information, we need two types of components: “data stores”  that contain the required information and retrieval engines<a data-type="indexterm" data-primary="retrieval engines, to access external information" id="id1346"/> that can retrieve relevant data from data stores given a query. The retrieval engine can be powered by an LLM itself, or it can be as simple as a keyword-matching algorithm. The data store(s) can be a repository of data like a database, knowledge graph, vector database, or even just a collection of text files. Data in the data store is represented and indexed to make retrieval more efficient. Data representation, indexing, and retrieval are topics important enough to merit their own chapter: we will defer detailed discussions on them to <a data-type="xref" href="ch11.html#chapter_llm_interfaces">Chapter 11</a>.</p>

<p>When a user issues a query, the retrieval engine uses the query to find the documents or text segments that are most relevant to answering this query. After ensuring that these fit into the context window<a data-type="indexterm" data-primary="context window" data-secondary="passive interaction with data store" id="id1347"/> of the LLM, they are fed to the LLM along with the query. The LLM is expected to answer the query given the relevant context provided in the prompt. This approach is popularly known as RAG<a data-type="indexterm" data-primary="retrieval-augmented generation (RAG)" data-secondary="role in interaction paradigm" id="id1348"/>, although as we will see in <a data-type="xref" href="ch12.html#ch12">Chapter 12</a>, RAG refers to an even broader concept. RAG is an important paradigm that deserves its own chapter, so we will defer detailed coverage of the paradigm to <a data-type="xref" href="ch12.html#ch12">Chapter 12</a>.</p>

<p class="pagebreak-before">Note that the distinguishing feature of this paradigm is the passive nature of the LLM in the interaction. The LLM simply responds to the prompt and furnishes an answer. It does not know the source of the content inside the prompt. This paradigm is often used for building QA assistants or chatbots, where external information is required to understand the context of the conversation.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>From this point forward, we will refer to user requests<a data-type="indexterm" data-primary="queries" id="id1349"/><a data-type="indexterm" data-primary="chunks" id="id1350"/> to the LLM as <em>queries</em> and textual units that are retrieved from external data stores as <em>documents</em>. Documents can be full documents, passages, paragraphs, or sentences<a data-type="indexterm" data-startref="xi_passiveapproachLLMinteractionparadigm103869" id="id1351"/><a data-type="indexterm" data-startref="xi_datastoresLLMinteractionwith103869" id="id1352"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Explicit Approach"><div class="sect2" id="id163">
<h2>The Explicit Approach</h2>

<p><a data-type="xref" href="#explicit-approach">Figure 10-2</a> demonstrates the explicit approach<a data-type="indexterm" data-primary="explicit approach, LLM interaction paradigm" id="xi_explicitapproachLLMinteractionparadigm105957"/> to interface LLMs with external tools.</p>

<figure><div id="explicit-approach" class="figure">
<img src="assets/dllm_1002.png" alt="Explicit Approach" width="600" height="208"/>
<h6><span class="label">Figure 10-2. </span>The explicit interaction approach in action</h6>
</div></figure>

<p>Unlike in the passive approach, the LLM is no longer a passive participant.  We provide the LLM with explicit instructions on how and when to invoke external data stores and tools. The LLM interacts with its environment based on a pre-programmed set of conditions. This approach is recommended when the interaction sequence is fixed, limited in scope, and preferably involves a very small number of steps.</p>

<p>For an AI data analyst assistant, an example interaction sequence could be:</p>
<ol>
<li>
<p>User expresses query in natural language asking to visualize some data trends</p>
</li>
<li>
<p>The LLM generates SQL to retrieve the data needed to resolve the user query</p>
</li>
<li>
<p>After receiving the data, the LLM uses it to generate code that can be run by a code interpreter to generate statistics or visualizations</p>
</li>

</ol>

<p><a data-type="xref" href="#ai-data-analyst">Figure 10-3</a> shows a fixed interaction sequence implemented for an AI data analyst.</p>

<figure><div id="ai-data-analyst" class="figure">
<img src="assets/dllm_1003.png" alt="ai-data-analyst" width="600" height="377"/>
<h6><span class="label">Figure 10-3. </span>An example workflow for an AI data analyst</h6>
</div></figure>

<p>In this paradigm, the interaction sequence is predetermined and rule-based. The LLM exercises no agency in determining which step to take next. I recommend this approach for building robust applications that have stricter reliability requirements<a data-type="indexterm" data-startref="xi_explicitapproachLLMinteractionparadigm105957" id="id1353"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="The Autonomous Approach"><div class="sect2" id="id164">
<h2>The Autonomous Approach</h2>

<p><a data-type="xref" href="#agentic-approach">Figure 10-4</a> shows how we can turn an LLM into an autonomous<a data-type="indexterm" data-primary="autonomous approach, LLM interact paradigm" data-seealso="agentic systems" id="id1354"/><a data-type="indexterm" data-primary="agentic systems" id="xi_agenticsystems107969"/> agent that can solve complex tasks by itself.</p>

<figure><div id="agentic-approach" class="figure">
<img src="assets/dllm_1004.png" alt="Agentic Approach" width="600" height="208"/>
<h6><span class="label">Figure 10-4. </span>A typical autonomous LLM-driven agent workflow</h6>
</div></figure>

<p>The autonomous approach, or the Holy Grail approach as I like to call it, turns an LLM into an autonomous agent that can solve tasks on its own by interacting with its environment. Here is a typical workflow of an autonomous agent:</p>
<ol>
<li>
<p>The user formulates their requirements in natural language, optionally providing the format in which they want the LLM to provide the answer.</p>
</li>
<li>
<p>The LLM decomposes the user query into manageable subtasks.</p>
</li>
<li>
<p>The LLM synchronously or asynchronously solves each subtask of the problem. Where possible, the LLM uses its own memory and knowledge to solve a specific subtask. For subtasks where the LLM cannot answer on its own, it chooses a tool to invoke from a list of available tools. Where possible, the LLM uses the outputs from solutions of already executed subtasks as inputs to other subtasks.</p>
</li>
<li>
<p>The LLM synthesizes the final answer using the solutions of the subtasks, generating the output in the requested output format.</p>
</li>

</ol>

<p>This paradigm is general enough to capture just about any use case. It is also a risky paradigm, as we are assigning the LLM too much responsibility and agency. At this juncture, I would not recommend using this paradigm for any mission-critical 
<span class="keep-together">applications</span><a data-type="indexterm" data-startref="xi_interactionparadigms102050" id="id1355"/>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why am I calling for caution in deploying agents<a data-type="indexterm" data-primary="agentic systems" data-secondary="risks in using" id="id1356"/>? Humans often underestimate the accuracy requirements for applications. For a lot of use cases, getting it right 99% of the time is still not good enough, especially when the failures are unpredictable and the 1% of failures can be potentially catastrophic. The 99% problem is also the one that has long plagued self-driving cars and prevented their broader adoption. This doesn’t mean we can’t deploy autonomous LLM agents; we just need clever product design that can shield the user from their failures. We also need
robust human-in-the-loop paradigms.</p>
</div>

<p>We have used the word “agent” several times now without defining it. Let’s correct that and consider what agents mean and how we can build them.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Defining Agents"><div class="sect1" id="id371">
<h1>Defining Agents</h1>

<p>As the hype starts building over LLM-based agents, the colloquial definition of agents has already started to expand from its traditional definition. This is because truly agentic systems are hard to build, so there is a tendency to shift the goalposts and claim best-effort systems to be already agentic even though they technically may not fit the requirements. In this book, we will stick to a more conservative definition of agents, defining them as:</p>
<blockquote>
<p>LLM-driven software systems that are able to interact with their environment and take autonomous actions to complete a task.</p></blockquote>

<p>Key characteristics of agents are:</p>
<dl>
<dt>Their autonomous nature</dt>
<dd>
<p>The sequence of steps required to perform a task need not be specified to the agent. Agents can decide to perform any sequence of actions, unprompted by humans.</p>
</dd>
<dt>Their ability to interact with their environment</dt>
<dd>
<p>Agents can be connected to external data sources and software tools, which allows agents to retrieve data, invoke tools, execute code, and provide instructions when appropriate to solve a task.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Many definitions of “agent” do not require them to be autonomous. According to their definitions, applications following the explicit paradigm can also be called agents (albeit as non-autonomous or semi-autonomous agents).</p>
</div>

<p>The agentic paradigm as we defined it is extremely powerful and general. Let’s take a moment to appreciate it. If an agent receives a task that it doesn’t know how to solve (and it <em>knows</em> that it doesn’t know), then instead of just giving up, it can potentially learn to solve the task by itself by searching the web or knowledge bases for pointers, or even by collecting data and fine-tuning a model that can help solve the task.</p>

<p>Given these enviable abilities, are machines going to take over the world? In practice, current autonomous agents are limited in what they can actually achieve. They tend to get stuck in loops, they take incorrect actions, and they are unable to reliably self-correct. It is more practical to build partially autonomous agents, where the LLM is provided with guidance throughout its workflow, either through agent orchestration software or with a human in the loop. For the rest of this chapter, our focus will be on building practical agents that can reliably solve a narrower class of tasks.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Agentic Workflow"><div class="sect1" id="id165">
<h1>Agentic Workflow</h1>

<p>Using our definition of agents<a data-type="indexterm" data-primary="agentic systems" data-secondary="workflows" id="xi_agenticsystemsworkflow1013331"/><a data-type="indexterm" data-primary="workflows, agentic systems" id="xi_workflowagenticsystems1013331"/>, let’s explore how agents work in practice. As an example, let’s consider an agent that is asked to answer this question:</p>
<blockquote>
<p>Who was the CFO of Apple when its stock price was at its lowest point in the last 10 years?</p></blockquote>

<p>Let’s say the agent has all the information it needs to solve this task. It has access to the web, to SQL databases containing stock price information, and to knowledge bases containing CFO tenure information. It is connected to a code interpreter so that it can generate and run code, and it has access to financial APIs. The system prompt contains details about all the tools and data stores the LLM has access to.</p>

<p>To answer the given query, the LLM has to perform this sequence of steps:</p>
<ol>
<li>
<p>To calculate the date range, it needs the current date. If this is not included in the system prompt, it either searches the web to find the current date or generates code for returning the system time, which is then executed by a code interpreter.</p>
</li>
<li>
<p>Using the current date, it finds the other end of the date range by executing a simple arithmetic operation by itself, or by generating code for it. Steps 1 and 2 could be combined into a single program.</p>
</li>
<li>
<p>It finds a database table in the available datastore list that contains stock price information. It retrieves the schema of the table, inserts it into the prompt, and generates a SQL query for finding the date when the stock price was at its minimum in the last 10 years.</p>
</li>
<li>
<p>With the date in hand, it needs to find the CFO of Apple on that date. It can call a search engine API to check if there is an explicit mention of the CFO on that particular date.</p>
</li>
<li>
<p>If the search engine query fails to provide a result, it finds a financial API in its tools list and retrieves and inserts the API documentation into its context. It then generates and invokes code for an API call to retrieve the list of Apple CFOs and their tenures.</p>
</li>
<li>
<p>It uses its arithmetic reasoning skills to find the CFO tenure that matches the date of the lowest stock price.</p>
</li>
<li>
<p>It generates the final answer. If there is a requested output format, it tries to adhere to that.</p>
</li>

</ol>

<p>Depending on the implementation, the sequence of steps could vary slightly. For example, you can fine-tune a model so that it can generate code for API calls or SQL queries directly without having to retrieve the schema from a data store or API.</p>

<p>To perform the given sequence of tasks, the model should first understand that the given task needs to be decomposed into a series of subtasks. This is called task decomposition<a data-type="indexterm" data-primary="task decomposition, agentic systems" id="id1357"/>. Task decomposition and planning can be performed by the LLM or offloaded to an external tool<a data-type="indexterm" data-startref="xi_agenticsystemsworkflow1013331" id="id1358"/><a data-type="indexterm" data-startref="xi_workflowagenticsystems1013331" id="id1359"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1360">
<h1>Exercise</h1>
<p>Try out the Apple CFO query with consumer LLM tools that have access to the web, like ChatGPT, Perplexity, and Gemini. Are they able to solve the answer correctly? If not, where are they falling short? Note that the earlier in the task sequence the LLM fails, the harder for it to recover.</p>

<p>Try to solve this question using a web search and no LLMs. You can see that it takes several search engine queries even for a financial domain expert to find the answer to this question.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Components of an Agentic System"><div class="sect1" id="id372">
<h1>Components of an Agentic System</h1>

<p>While the specific architecture of any given agentic system depends heavily on the use cases it is intended to support, each of its components can be classified into one of the following types:</p>

<ul>
<li>
<p>Models</p>
</li>
<li>
<p>Tools</p>
</li>
<li>
<p>Data stores</p>
</li>
<li>
<p>Agent loop prompt</p>
</li>
<li>
<p>Guardrails and verifiers</p>
</li>
<li>
<p>Orchestration software</p>
</li>
</ul>

<p><a data-type="xref" href="#agentic-system">Figure 10-5</a> shows a canonical agentic system and how its components interact.</p>

<figure><div id="agentic-system" class="figure">
<img src="assets/dllm_1005.png" alt="agentic-system" width="600" height="446"/>
<h6><span class="label">Figure 10-5. </span>A production-grade agentic system</h6>
</div></figure>

<p>Let’s explore each of these types.</p>








<section data-type="sect2" data-pdf-bookmark="Models"><div class="sect2" id="id166">
<h2>Models</h2>

<p>Language models<a data-type="indexterm" data-primary="agentic systems" data-secondary="models" id="xi_agenticsystemsmodels1019616"/><a data-type="indexterm" data-primary="models" data-secondary="agentic systems" id="xi_modelsagenticsystems1019616"/> are the backbone of agentic systems, responsible for their autonomous nature and problem-solving capabilities. A single agentic system could be composed of multiple language models, with each model playing a distinct role.</p>

<p>For example, you can build an agent consisting of two models; one model solves user tasks and another model takes its output and converts it into a structured form according to user requirements.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Agentic workflows<a data-type="indexterm" data-primary="agentic systems" data-secondary="workflows" id="id1361"/><a data-type="indexterm" data-primary="workflows, agentic systems" id="id1362"/> can consume a lot of language model tokens, which can be cost prohibitive. To keep costs under control, consider using multiple language models of different sizes, with the smaller (and cheaper) models performing easier tasks. For more details on how to accomplish division of labor among these models, see <a data-type="xref" href="ch13.html#ch13">Chapter 13</a>.</p>
</div>

<p>More generally, you can build agents with specialized models catering to each part of the agentic workflow. For example, a code-LLM can be used to generate code, and task-specific fine-tuned models that specialize in individual workflow steps can be used. This setup can be interpreted<a data-type="indexterm" data-primary="multi-agent architecture" id="id1363"/><a data-type="indexterm" data-primary="architectures" data-secondary="multi-agent" id="id1364"/> as a <em>multi-agent architecture</em>.</p>

<p><a data-type="xref" href="#multi-agent-setup">Figure 10-6</a> shows an agentic system made up of multiple LLMs.</p>

<figure><div id="multi-agent-setup" class="figure">
<img src="assets/dllm_1006.png" alt="multi-agent-setup" width="600" height="388"/>
<h6><span class="label">Figure 10-6. </span>An agentic system with multiple LLMs</h6>
</div></figure>

<p>Finally, any kind of model, including non-LLMs, can be plugged into an agentic system to solve specific tasks<a data-type="indexterm" data-startref="xi_agenticsystemsmodels1019616" id="id1365"/><a data-type="indexterm" data-startref="xi_modelsagenticsystems1019616" id="id1366"/>. For example, the planning<a data-type="indexterm" data-primary="symbolic planners" id="id1367"/> stage can be performed using <a href="https://oreil.ly/sXPWG">symbolic planners</a>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Tools"><div class="sect2" id="id167">
<h2>Tools</h2>

<p>As described earlier, software or models that can be invoked by an LLM are called tools<a data-type="indexterm" data-primary="agentic systems" data-secondary="tools" id="xi_agenticsystemstools1021688"/>.
Libraries<a data-type="indexterm" data-primary="LangChain" id="xi_LangChain1021710"/><a data-type="indexterm" data-primary="LlamaIndex" id="id1368"/> like <a href="https://oreil.ly/35Lgu">LangChain</a> and <a href="https://oreil.ly/WF-d1">LlamaIndex</a> provide connectors to various software interfaces, including code interpreters, search engines, databases, ML models,
and a variety of APIs. Let’s explore how to work with some of these in practice.</p>










<section data-type="sect3" data-pdf-bookmark="Web search"><div class="sect3" id="id168">
<h3>Web search</h3>

<p>LangChain<a data-type="indexterm" data-primary="web search, agentic tool" id="id1369"/><a data-type="indexterm" data-primary="search systems" data-secondary="web search" id="id1370"/> provides connectors for major search engines like Google, Bing, and DuckDuckGo. Let’s try out DuckDuckGo:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_community.tools</code> <code class="kn">import</code> <code class="n">DuckDuckGoSearchRun</code>

<code class="n">query</code> <code class="o">=</code> <code class="s2">"What's the weather today in Toronto?"</code>

<code class="n">search_engine</code> <code class="o">=</code> <code class="n">DuckDuckGoSearchRun</code><code class="p">()</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">search_engine</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">query</code><code class="p">)</code></pre>

<p>The response can be fed back to the language model where it is further processed.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="API connectors"><div class="sect3" id="id169">
<h3>API connectors</h3>

<p>To illustrate calling APIs<a data-type="indexterm" data-primary="API connectors, agentic tool" id="id1371"/>, we will showcase LangChain’s Wikipedia API wrapper:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">wikipedia</code>

<code class="kn">from</code> <code class="nn">langchain.tools</code> <code class="kn">import</code> <code class="n">WikipediaQueryRun</code>
<code class="kn">from</code> <code class="nn">langchain_community.utilities</code> <code class="kn">import</code> <code class="n">WikipediaAPIWrapper</code>
<code class="n">wikipedia</code> <code class="o">=</code> <code class="n">WikipediaQueryRun</code><code class="p">(</code><code class="n">api_wrapper</code><code class="o">=</code><code class="n">WikipediaAPIWrapper</code><code class="p">())</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">wikipedia</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"Winter Olympics"</code><code class="p">)</code></pre>

<p>The <code>load()</code> function runs a search on Wikipedia and returns the page text and metadata information of the top-k results. (top-k = 3 by default). You can also use the <code>run()</code> function to return only page summaries of the top-k matches.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Code interpreter"><div class="sect3" id="id170">
<h3>Code interpreter</h3>

<p>Next, let’s explore how you can invoke a code interpreter<a data-type="indexterm" data-primary="code interpreter, agentic tool" id="id1372"/> and run arbitrary code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain_experimental.utilities</code> <code class="kn">import</code> <code class="n">PythonREPL</code>

<code class="n">python</code> <code class="o">=</code> <code class="n">PythonREPL</code><code class="p">()</code>
<code class="n">python</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="s2">"456 * 345"</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be wary of running code<a data-type="indexterm" data-primary="security" data-secondary="risks of running LLM code generation" id="id1373"/><a data-type="indexterm" data-primary="generative AI" data-secondary="code generation risks" id="id1374"/> generated by LLMs in response to user prompts. Users can induce the model to generate malicious code!</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Database connectors"><div class="sect3" id="id171">
<h3>Database connectors</h3>

<p>Finally, let’s check out how to connect to a database<a data-type="indexterm" data-primary="database connectors, agentic tool" id="id1375"/><a data-type="indexterm" data-primary="queries" data-secondary="database connectors" id="id1376"/> and run queries:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">sqlalchemy</code> <code class="k">as</code> <code class="nn">sa</code>
<code class="kn">from</code> <code class="nn">langchain_community.utilities</code> <code class="kn">import</code> <code class="n">SQLDatabase</code>

<code class="n">DATABASE_URI</code> <code class="o">=</code> <code class="o">&lt;</code><code class="n">database_uri</code><code class="o">&gt;</code>

<code class="n">db</code> <code class="o">=</code> <code class="n">SQLDatabase</code><code class="o">.</code><code class="n">from_uri</code><code class="p">(</code><code class="n">DATABASE_URI</code><code class="p">)</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">db</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="s2">"SELECT * FROM COMPANIES WHERE Name LIKE :comp;"</code><code class="p">,</code>
    <code class="n">parameters</code><code class="o">=</code><code class="p">{</code><code class="s2">"comp"</code><code class="p">:</code> <code class="s2">"Apple%"</code><code class="p">},</code>
    <code class="n">fetch</code><code class="o">=</code><code class="s2">"all"</code><code class="p">)</code></pre>

<p>The <code>run()</code> function executes the provided SQL query and returns the response as a string. Replace <code><em>DATABASE_URI</em></code> with your own database and queries, and verify the responses.</p>
<div data-type="tip"><h6>Tip</h6>
<p>For more customizability, you can fork the LangChain connectors and repurpose them for your own use.</p>
</div>

<p>Next, let’s see how we can interface LLMs with these tools in an agentic workflow<a data-type="indexterm" data-primary="agentic systems" data-secondary="workflows" id="xi_agenticsystemsworkflow1029782"/><a data-type="indexterm" data-primary="workflows, agentic systems" id="xi_workflowagenticsystems1029782"/>.</p>

<p>First, we need to make the LLM aware that it has access to these tools. One of the ways to achieve this is to provide the names and short descriptions of the tools<a data-type="indexterm" data-primary="tool list, agentic workflow" id="id1377"/>, called the <em>tool list</em>, to the LLM through the system prompt.</p>

<p>Next, the LLM needs to be able to select the right tool at the appropriate juncture in the workflow. For example, if the next step in solving a task is to find the weather in Chicago this evening, the web search tool has to be invoked rather than the Wikipedia one. Later in this chapter, we will discuss techniques to help the LLM select the right tool.</p>

<p>Under the hood, tool invocation<a data-type="indexterm" data-primary="tool invocation, agentic workflow" id="xi_toolinvocationagenticworkflow1030332"/> is typically achieved by the LLM generating special tokens indicating that it is entering tool invocation mode, along with tokens representing the tool functions and arguments to be invoked. The actual tool invocation is performed by an agent orchestration framework<a data-type="indexterm" data-primary="orchestration framework, agentic systems" id="id1378"/>.</p>

<p class="pagebreak-before">In LangChain, we can make a tool available to an LLM and have it invoked<a data-type="indexterm" data-startref="xi_LangChain1021710" id="id1379"/>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">langchain.agents</code> <code class="kn">import</code> <code class="n">initialize_agent</code><code class="p">,</code> <code class="n">Tool</code>
<code class="kn">from</code> <code class="nn">langchain.agents</code> <code class="kn">import</code> <code class="n">AgentType</code>
<code class="kn">from</code> <code class="nn">langchain_community.tools</code> <code class="kn">import</code> <code class="n">DuckDuckGoSearchRun</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langchain_core.messages</code> <code class="kn">import</code> <code class="n">HumanMessage</code>

<code class="n">search_engine</code> <code class="o">=</code> <code class="n">DuckDuckGoSearchRun</code><code class="p">()</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4o"</code><code class="p">)</code>

<code class="n">tools</code> <code class="o">=</code> <code class="p">[</code>
       <code class="n">Tool</code><code class="p">(</code>
           <code class="n">name</code><code class="o">=</code><code class="s2">"Search"</code><code class="p">,</code>
           <code class="n">func</code><code class="o">=</code><code class="n">search_engine</code><code class="o">.</code><code class="n">run</code><code class="p">,</code>
           <code class="n">description</code><code class="o">=</code><code class="s2">"search engine for answer factual queries"</code>
       <code class="p">)</code>
   <code class="p">]</code>
<code class="n">agent</code> <code class="o">=</code> <code class="n">initialize_agent</code><code class="p">(</code><code class="n">tools</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">agent</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="s2">"What are some tourist destinations in North Germany?"</code><code class="p">)</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1380">
<h1>Exercise</h1>
<p>Using LangChain, create an agent that is initialized with at least five tools from the list found on its <a href="https://oreil.ly/LumHc">website</a>. Tools that do not require an API key include Wikipedia, DuckDuckGo, and arXiv, the latter being a repository of scientific papers including ones on LLMs. For a given user query, do you notice the model selecting the right tool? Give the model clues on selecting the right tool in the system prompt. Do you notice any improvements?</p>
</div></aside>

<p>Some models come with native tool-calling abilities. For models that don’t, you can fine-tune the base model to impart them with tool-calling abilities. Among open models, Llama 3.1 Instruct<a data-type="indexterm" data-primary="Llama 3.1 Instruct" id="id1381"/> (8B/70B/405B) is an example of a model having native tool-calling support. Here’s how tool calling works with Llama 3.1.</p>

<p>Llama 3.1 comes with native support for three tools: Brave web search, Wolfram|Alpha mathematical engine, and a code interpreter. These can be <em>activated</em> by defining them in the system prompt:</p>

<pre data-type="programlisting" data-code-language="python"><code class="o">&lt;|</code><code class="n">begin_of_text</code><code class="o">|&gt;&lt;|</code><code class="n">start_header_id</code><code class="o">|&gt;</code><code class="n">system</code><code class="o">&lt;|</code><code class="n">end_header_id</code><code class="o">|&gt;</code>
<code class="n">Environment</code><code class="p">:</code> <code class="n">ipython</code>
<code class="n">Tools</code><code class="p">:</code> <code class="n">brave_search</code><code class="p">,</code> <code class="n">wolfram_alpha</code>

<code class="n">Give</code> <code class="n">responses</code> <code class="n">to</code> <code class="n">answers</code> <code class="ow">in</code> <code class="n">a</code> <code class="n">concise</code> <code class="n">fashion</code><code class="o">.</code> <code class="o">&lt;|</code><code class="n">eot_id</code><code class="o">|&gt;</code></pre>

<p class="pagebreak-before">Let’s ask the LLM a question by appending a user prompt to the system prompt:</p>

<pre data-type="programlisting" data-code-language="python"><code class="o">&lt;|</code><code class="n">start_header_id</code><code class="o">|&gt;</code><code class="n">user</code><code class="o">&lt;|</code><code class="n">end_header_id</code><code class="o">|&gt;</code>

<code class="n">How</code> <code class="n">many</code> <code class="n">medals</code> <code class="n">did</code> <code class="n">Azerbaijan</code> <code class="n">win</code> <code class="ow">in</code> <code class="n">the</code> <code class="mi">2024</code> <code class="n">Summer</code> <code class="n">Olympics</code><code class="err">?</code>
<code class="o">&lt;|</code><code class="n">eot_id</code><code class="o">|&gt;&lt;|</code><code class="n">start_header_id</code><code class="o">|&gt;</code><code class="n">assistant</code><code class="o">&lt;|</code><code class="n">end_header_id</code><code class="o">|&gt;</code></pre>

<p>Llama 3.1 responds with a tool invocation that looks like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="o">&lt;|</code><code class="n">python_tag</code><code class="o">|&gt;</code><code class="n">brave_search</code><code class="o">.</code><code class="n">call</code><code class="p">(</code><code class="n">query</code><code class="o">=</code><code class="s2">"How many medals did Azerbaijan win in</code><code class="w"/>

<code class="n">the</code> <code class="mi">2024</code> <code class="n">Summer</code> <code class="n">Olympics</code><code class="err">?</code><code class="s2">")&lt;|eom_id|&gt;</code><code class="w"/></pre>

<p>The <code>&lt;|python_tag|&gt;</code> token is a special token generated by Llama 3.1 to indicate that it is entering tool-calling mode. The <code>&lt;|eom_id|&gt;</code> special token indicates that the model has not ended its turn yet and will wait to be fed with the results of the tool invocation.</p>

<p>You can also provide your own tools in the prompt: using JSON is recommended.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you have a lot of tools, then the detailed descriptions of the tools can be represented in a data store and retrieved only if they are selected. The prompt then needs to contain only the name of the tool and a short description.</p>
</div>

<p>Here is an example of a tool definition in JSON describing a local function that can be called:</p>

<pre data-type="programlisting" data-code-language="python"><code class="o">&lt;|</code><code class="n">start_header_id</code><code class="o">|&gt;</code><code class="n">user</code><code class="o">&lt;|</code><code class="n">end_header_id</code><code class="o">|&gt;</code>

<code class="n">Here</code> <code class="ow">is</code> <code class="n">a</code> <code class="nb">list</code> <code class="n">of</code> <code class="n">tools</code> <code class="n">available</code><code class="o">.</code>
<code class="n">While</code> <code class="n">invoking</code> <code class="n">a</code> <code class="n">tool</code><code class="p">,</code> <code class="n">respond</code> <code class="ow">in</code> <code class="n">JSON</code><code class="o">.</code> <code class="n">The</code> <code class="nb">format</code> <code class="ow">is</code> <code class="k">as</code> <code class="n">follows</code><code class="p">:</code>

<code class="p">{</code><code class="s2">"tool_name"</code><code class="p">:</code> <code class="n">tool</code> <code class="n">name</code><code class="p">,</code> <code class="s2">"arguments"</code><code class="p">:</code> <code class="n">dictionary</code> <code class="k">with</code> <code class="n">keys</code> <code class="n">representing</code>

<code class="n">argument</code> <code class="n">names</code> <code class="ow">and</code> <code class="n">values</code> <code class="n">representing</code> <code class="n">argument</code> <code class="n">values</code><code class="p">}</code><code class="o">.</code>

<code class="p">{</code>
    <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"local_function"</code><code class="p">,</code>
    <code class="s2">"function"</code><code class="p">:</code> <code class="p">{</code>
    <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"find_citations"</code><code class="p">,</code>
    <code class="s2">"description"</code><code class="p">:</code> <code class="s2">"Find the citations for any claims made"</code><code class="p">,</code>
    <code class="s2">"parameters"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"object"</code><code class="p">,</code>
        <code class="s2">"properties"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"claim_sentence"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"string"</code><code class="p">,</code>
            <code class="s2">"description"</code><code class="p">:</code> <code class="s2">"A sentence in the input representing a claim"</code>
        <code class="p">},</code>
        <code class="s2">"model"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"string"</code><code class="p">,</code>
            <code class="s2">"enum"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"weak"</code><code class="p">,</code> <code class="s2">"strong"</code><code class="p">],</code>
            <code class="s2">"description"</code><code class="p">:</code> <code class="s2">"The type of citation model to use. A weak model is</code><code class="w"/>
            <code class="n">preferred</code> <code class="k">if</code> <code class="n">the</code> <code class="n">claim</code> <code class="n">sentence</code> <code class="n">contains</code> <code class="n">entities</code> <code class="ow">and</code> <code class="n">numbers</code><code class="o">.</code> <code class="s2">"</code><code class="w"/>
        <code class="p">}</code>
        <code class="p">},</code>
        <code class="s2">"required"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"claim_sentence"</code><code class="p">,</code> <code class="s2">"model"</code><code class="p">]</code>
    <code class="p">}</code>
    <code class="p">}</code>
<code class="p">}</code></pre>

<p>The tool call is generated by the model in JSON with the prescribed format.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The actual tool invocation is performed by an agent orchestration software. Llama 3.1 comes with <a href="https://oreil.ly/SSmkI">llama-stack-apps</a>, a library that facilitates agentic workflows.</p>
</div>

<p>Sometimes the tool call can be more complex than just returning the name of a function and its arguments. An example of this is querying a database. For the LLM to generate the right SQL query, you should provide the schema of the database tables in the system prompt. If the database has too many tables, then their schema can be retrieved on demand by the LLM<a data-type="indexterm" data-startref="xi_toolinvocationagenticworkflow1030332" id="id1382"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can use a separate specialized model for code and SQL query generation. A general-purpose model can generate a textual description of the desired outcome, and this can be used as input to a code LLM or an LLM fine-tuned on text-to-SQL.</p>
</div>

<p>For large-scale or high-stakes applications, you can fine-tune your models to make them better at tool use<a data-type="indexterm" data-startref="xi_agenticsystemsworkflow1029782" id="id1383"/><a data-type="indexterm" data-startref="xi_workflowagenticsystems1029782" id="id1384"/>. A good fine-tuning recipe<a data-type="indexterm" data-primary="ToolLlama" id="id1385"/> to follow is Qin et al.’s <a href="https://oreil.ly/Ewlxt">ToolLLaMA</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1386">
<h1>Exercise</h1>
<p>Try creating some custom tools and making them available for Llama 3.1. The tools are just functions that can represent a wrapper to some software, or they can be a function that performs a specific task like converting from Celsius to Fahrenheit. Can you create these five tools and provide their definitions in JSON in the system prompt:</p>

<ul>
<li>
<p>Tool to query the Wikipedia API</p>
</li>
<li>
<p>Tool to query the arXiv API</p>
</li>
<li>
<p>Tool that converts from Celsius to Fahrenheit</p>
</li>
<li>
<p>Tool that saves the input to a text file</p>
</li>
<li>
<p>Tool that makes a copy of a file</p>
</li>
</ul>

<p>Try asking queries that can lead to these tools being invoked. Are they being invoked as expected? Modify your tool descriptions if not, and see if that helps<a data-type="indexterm" data-startref="xi_agenticsystemstools1021688" id="id1387"/>.</p>
</div></aside>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Data Stores"><div class="sect2" id="id172">
<h2>Data Stores</h2>

<p>A typical agent may need to interact with several types of data<a data-type="indexterm" data-primary="agentic systems" data-secondary="data stores" id="xi_agenticsystemsdatastores1044864"/><a data-type="indexterm" data-primary="data stores, LLM interaction with" id="xi_datastoresLLMinteractionwith1044864"/> sources to accomplish its tasks. Commonly used data sources include prompt repositories, session memory, and tools data.</p>










<section data-type="sect3" data-pdf-bookmark="Prompt repository"><div class="sect3" id="id173">
<h3>Prompt repository</h3>

<p>A prompt repository<a data-type="indexterm" data-primary="prompt repository, data stores" id="id1388"/> is a collection of detailed prompts instructing the language model how to perform a specific task. If you can anticipate the types of tasks that an agent will be asked to perform while in production, you can construct prompts providing detailed instructions on how to solve them. The prompts can even include directions on how to advance a specific workflow. Let’s look at an example.</p>

<p>Many language models struggle with basic arithmetic operations<a data-type="indexterm" data-primary="arithmetic operations" id="id1389"/>, even simple questions like:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Is</code> <code class="mf">9.11</code> <code class="n">greater</code> <code class="n">than</code> <code class="mf">9.9</code><code class="err">?</code></pre>

<p>Until recently, even state-of-the-art language models claimed that 9.11 is greater than 9.9. (They were recently updated with a fix after this limitation went viral on <a href="https://oreil.ly/ztWGW">social media</a>.)</p>

<p>If you are aware of such limitations that are relevant to your use case, then you can mitigate a proportion of them using detailed prompts. For the number comparison issue, for example:</p>
<blockquote>
<p><em>Prompt:</em> If you are asked to compare two numbers using the greater than/lesser than operation, then perform the following:</p>

<p>Take the two numbers and ensure they have the same number of decimal places. After that, subtract one from the other.
If the result is a positive number, then the first number is greater.
If the result is a negative number, then the second number is greater.
If the result is zero, the two numbers are equal.</p></blockquote>

<p>Now, if the agent needs to perform a task that includes number comparison, it first retrieves this prompt from the prompt repository. This enables it to overcome its inherent limitation, as it will follow the detailed step-by-step instructions in the prompt.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why don’t we just add all these prompts to the context window, thus eschewing retrieval? For one, the prompts may be too numerous and may not fit within the context window. Secondly, tokens are expensive, and it is inefficient to include prompts that may not be relevant to the current task. Finally, language models can adhere to only a limited set of concurrent instructions, so it is more efficient to retrieve them on demand.</p>
</div>

<p>Prompts can also include input-output examples, known as few-shot learning<a data-type="indexterm" data-primary="few-shot learning" id="id1390"/><a data-type="indexterm" data-primary="prompting" data-secondary="few-shot" id="id1391"/>, as introduced in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>. Agents can retrieve them on demand to help accomplish their tasks. We will discuss effective ways of retrieving relevant examples in 
<span class="keep-together"><a data-type="xref" href="ch12.html#ch12">Chapter 12</a></span>.</p>

<p>Next, let’s explore how agents use session memory to advance their goals.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Session memory"><div class="sect3" id="id174">
<h3>Session memory</h3>

<p>We would like to store logs of the steps undertaken by the agent during current and past sessions<a data-type="indexterm" data-primary="session memory, data stores" id="id1392"/><a data-type="indexterm" data-primary="memory management" data-secondary="session memory in data stores" id="id1393"/>. These logs can also be augmented with feedback from verification processes, both automated and human. The agent can then retrieve session logs for better context understanding and as a guide to determine next actions in the current workflow. Let’s explore this with an example:</p>
<blockquote>
<p><em>Prompt:</em> Is it going to be warmer tomorrow than today in Paris? Give me details in 
<span class="keep-together">Celsius</span>.</p></blockquote>

<p>The agent goes through a sequence of steps to solve this query, which is all logged in the session history. The session history for this query might look like this:</p>
<blockquote>
<p><em>User prompt</em>: Is it going to be warmer tomorrow than today in Paris? Give me details in Celsius.</p>

<p><em>LLM observation</em>: The user has a preference for Celsius units for temperature data.</p>

<p><em>LLM Output</em>: Let’s perform this step-by-step.</p>
<ol>
<li>
<p>First, let’s retrieve today’s temperature.</p>
</li>
<li>
<p>Next, let’s retrieve tomorrow’s temperature.</p>
</li>
<li>
<p>We calculate the difference and generate the answer.</p>
</li>

</ol>

<p><em>External verifier</em>: Task decomposition is correct</p>

<p><em>Tool selection:</em> &lt;WeatherAPI&gt; selected</p>

<p><em>Tool info retrieval</em>: Retrieve information relevant to the query <em>Find temperature data from &lt;WeatherAPI&gt;</em></p>

<p><em>Tool invocation</em>: get_temperature_curweek(<em>Paris</em>)</p>

<p><em>Tool output</em> : Weather data - Paris…​</p>

<p><em>LLM output</em>: Today’s temperature is 27 degrees Celsius. Tomorrow’s forecast is
23 degrees Celsius. Therefore, tomorrow is not going to be warmer in Paris than today.</p>

<p><em>External Verifier</em>: Arithmetic operation is correct.</p>

<p><em>Agent</em>: LLM output is dispatched to the user</p>

<p><em>User feedback</em>: User marked this as correct</p></blockquote>

<p>As we can see, session history can contain very rich information that can provide valuable personalized context to the LLM about the current user as well as guide the model toward the correct agentic workflow.</p>

<p>In more advanced implementations, multiple levels of logging can be defined, so that during retrieval, one can retrieve all the logs of a session or only the important steps, based on the logging level specified.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Along with session history, the agent could also be provided with access to gold-truth<a data-type="indexterm" data-primary="gold truth" id="id1394"/> training examples representing correct workflows, which can be used by the agent to guide its trajectory during test time.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1395">
<h1>Exercise</h1>
<p>Tools for facilitating agent observability include <a href="https://oreil.ly/6fn7p">LangSmith</a>, <a href="https://oreil.ly/0FhZ9">Langtrace</a>, <a href="https://oreil.ly/1GRDY">OpenLLMetry</a>, etc. Many of these tools operate freemium models. Use LangSmith observability tools for the Llama 3.1 agent you built in the previous exercise and observe the agent traces.</p>
</div></aside>

<p>Session memory can also include records of interaction between the human and the agentic system. These can be used to personalize models. We will discuss this further in <a data-type="xref" href="ch12.html#ch12">Chapter 12</a>.</p>

<p>Next, let’s explore how the agent can interact with tools data.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Tools data"><div class="sect3" id="id175">
<h3>Tools data</h3>

<p>Tools data<a data-type="indexterm" data-primary="tools data, data stores" id="id1396"/> comprise detailed information necessary to invoke a tool, such as database schemas, API documentation, sample API calls, and more. When the agent decides to invoke a tool, the model retrieves the pertinent tool information from the tools data store.</p>

<p>For example, consider a SQL tool for retrieving data from a database. To generate the right SQL query<a data-type="indexterm" data-primary="queries" data-secondary="tools data" id="id1397"/>, the model could retrieve the database schema from the tools data store. The tools data contains information about the tables and columns, the descriptions of each column and their data types, and optionally information about indices and primary/secondary keys.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can also fine-tune the LLM on a dataset representing valid SQL queries to your database, which can potentially remove the need to consult the schema before generating a query.</p>
</div>

<p>To sum it up, agents can use data stores in several ways. They can access prompts and few-shot examples from a prompt repository, they can access agentic workflow history and intermediate outputs by models in previous sessions for better personalized context understanding and workflow guidance, and they can access tool documentation to invoke tools correctly.</p>

<p>Agents can also access external knowledge from the web, databases, knowledge graphs, etc. Retrieving the right information from these sources is an entire sub-system unto itself<a data-type="indexterm" data-startref="xi_agenticsystemsdatastores1044864" id="id1398"/><a data-type="indexterm" data-startref="xi_datastoresLLMinteractionwith1044864" id="id1399"/>. We will discuss the mechanics of retrieval in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch11.html#chapter_llm_interfaces">11</a> and
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch12.html#ch12">12</a>.</p>

<p>We will now discuss the agent loop prompt, which is responsible for driving the LLM’s behavior during an agentic session.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Agent Loop Prompt"><div class="sect2" id="id176">
<h2>Agent Loop Prompt</h2>

<p>Recall that LLMs do not have session memory<a data-type="indexterm" data-primary="agentic systems" data-secondary="agent loop system prompt" id="xi_agenticsystemsagentloopsystemprompt1056444"/><a data-type="indexterm" data-primary="agent loop system prompt" id="xi_agentloopsystemprompt1056444"/>. But a typical agentic workflow relies on several LLM calls! We need a mechanism to provide information about session state and the expected role of the LLM at any given time in the session. This agent loop is driven by a system prompt.</p>

<p>An example of a simple agent loop system prompt is:</p>
<blockquote>
<p><em>Prompt:</em> You are an AI model currently answering questions. You have access to the following tools: {tool_description}. For each question, you can invoke one or more tools where necessary to access information or execute actions. You can invoke a tool in this format:
&lt;TOOLNAME&gt; &lt;Tool Arguments&gt;.
The results of these tool calls are not provided to the user. When you are ready with the final answer, output the answer using the &lt;Answer&gt; tag.</p></blockquote>

<p>I find that a prompt like this is sufficient for most use cases. However, if you feel like the model is not reasoning correctly, you can try ReAct prompting.</p>










<section data-type="sect3" data-pdf-bookmark="ReAct"><div class="sect3" id="id177">
<h3>ReAct</h3>

<p>At the time of this writing, ReAct (Reasoning + Acting)<a data-type="indexterm" data-primary="ReAct (Reasoning + Acting)" id="id1400"/> prompting is the most popular prompt for the agent loop. A typical ReAct prompt looks like this:</p>
<blockquote>
<p><em>Prompt:</em> You are an AI assistant capable of reasoning and acting. For each question, follow this process:</p>
<ol>
<li>
<p>Thought: Reflect on the current state and plan your next steps.</p>
</li>
<li>
<p>Action: Execute the steps to gather information or call tools.</p>
</li>
<li>
<p>Observation: Record the results of your actions.</p>
</li>
<li>
<p>Final Answer: If you have an answer, provide a final response. Else continue the Thought → Action → Observation → loop until you have an answer.</p>
</li>

</ol></blockquote>

<p>Despite its popularity, ReAct prompting has been shown to be <a href="https://oreil.ly/RRZO9">brittle</a>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Reflection"><div class="sect3" id="id178">
<h3>Reflection</h3>

<p>The agent loop may include self-verification<a data-type="indexterm" data-primary="reflection, agentic systems" id="id1401"/> or correction steps. This was pioneered by <a href="https://oreil.ly/xFVt0">Shinn et al.</a> with the Reflexion paradigm.</p>

<p>Here is the system prompt for <a href="https://oreil.ly/foB-P">Reflection-Llama-3.1</a> that uses reflection techniques:</p>
<blockquote>
<p><em>Prompt:</em> You are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside &lt;thinking&gt; tags, and then provide your final response inside &lt;output&gt; tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside &lt;reflection&gt; tags.</p></blockquote>

<p>The &lt;reflection&gt; tags are meant for the model to self-introspect and self-correct. We can also specify conditions when &lt;reflection&gt; tags should be activated, for example, when the agent performs the same action consecutively more than three times (which might mean it is stuck in a loop).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The effectiveness of reflection-based methods are overstated. They might do more harm than good if they are invoked too often, causing the model to second-guess solutions.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1402">
<h1>Exercise</h1>
<p>Let’s test the reliability of ReAct to the test. Use the ReAct and Reflexion prompts provided in this chapter to drive the movie recommendation agent provided in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>. Can you use a simpler prompt instead and see how it compares to ReAct<a data-type="indexterm" data-startref="xi_agenticsystemsagentloopsystemprompt1056444" id="id1403"/><a data-type="indexterm" data-startref="xi_agentloopsystemprompt1056444" id="id1404"/>?</p>
</div></aside>

<p>Next, let’s discuss guardrails and verifiers, components that ensure that an agentic system can thrive in production.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Guardrails and Verifiers"><div class="sect2" id="id179">
<h2>Guardrails and Verifiers</h2>

<p>In production environments, mistakes can be catastrophic. Depending on the use case, the agent might need to adhere to strict standards in factuality, safety, accuracy, and many other criteria.</p>

<p>Safety is ensured by using guardrails<a data-type="indexterm" data-primary="safety guardrails" id="xi_safetyguardrails1062338"/>, components that ensure models do not overstep their bounds during the course of their workflows. Some examples of guardrails include toxic language detectors, personally identifiable information (PII) detectors, input filters that restrict the type of queries users are permitted to make, and more.</p>

<p>Verifiers ensure that quality standards of the agentic system are so that the agent is able to recover and self-correct from mistakes. As agentic systems are still in their infancy, the importance of good and well-placed verifiers is paramount. Verifiers can be as simple as token-matching tools but can also be fine-tuned models, symbolic verifiers, and so on.</p>

<p>Let’s learn more about guardrails and verifiers.</p>










<section data-type="sect3" data-pdf-bookmark="Safety Guardrails"><div class="sect3" id="id180">
<h3>Safety Guardrails</h3>

<p>Recall from <a data-type="xref" href="ch02.html#ch02">Chapter 2</a> that LLMs are trained largely on human-generated web text<a data-type="indexterm" data-primary="agentic systems" data-secondary="safety guardrails" id="xi_agenticsystemssafetyguardrails1063179"/>. Unfortunately a significant proportion of human-generated text contains toxic, abusive, violent, or pornographic content. We do not want our LLM applications to generate content that violates the safety of the user, nor do we want users to misuse the model to generate unsafe content. While we can certainly use techniques like alignment training to make the model less likely to emit harmful content, we cannot guarantee 100% success and therefore need to institute inference-time guardrails to ensure safe usage. Libraries like <a href="https://oreil.ly/F7yax">Guardrails</a> and NVIDIA’s <a href="https://oreil.ly/p7Dqz">NeMo-Guardrails</a>, and models like <a href="https://oreil.ly/8S08P">Llama Guard</a>  facilitate setting up these guardrails.</p>

<p>The Guardrails<a data-type="indexterm" data-primary="Guardrails library" id="id1405"/> library provides a large (and growing) number of data validators to ensure safety and validity of LLM inputs and outputs. Here are some important ones:</p>
<dl>
<dt>Detect PII</dt>
<dd>
<p>This validator can be used to detect personally identifiable information in both the input and output text. <a href="https://oreil.ly/eG8T1">Microsoft Presidio</a> is employed under the hood to perform the PII identification.</p>
</dd>
<dt>Prompt injection</dt>
<dd>
<p>This validator can detect certain types of adversarial prompting and thus can be used to prevent users from misusing the LLM. The <a href="https://oreil.ly/nIyE5">Rebuff</a> library is used under the hood to detect prompt injection.</p>
</dd>
<dt>Not safe for work (NSFW) text</dt>
<dd>
<p>This validator detects NSFW text in the LLM output. This includes text with profanity, violence, and sexual content. The <em>Profanity free</em> validator also exists for detecting only profanity in text.</p>
</dd>
<dt>Politeness check</dt>
<dd>
<p>This validator checks if the LLM output text is sufficiently polite. A related validator is <em>Toxic language</em>.</p>
</dd>
<dt>Web sanitization</dt>
<dd>
<p>This validator checks the LLM output for any security vulnerabilities, including if it contains code that can be executed in a browser. The <a href="https://oreil.ly/r3Xrl">Bleach</a> library is used under the hood to find potential vulnerabilities and sanitize the output.</p>
</dd>
</dl>

<p>What happens if the validation checks fail and there is indeed harmful content in the input or output? Guardrails provides a few options:</p>
<dl>
<dt>Re-ask</dt>
<dd>
<p>In this method, the LLM is asked to regenerate the output, with the prompt containing instructions to specifically abide by the criteria on which the output previously failed validation.</p>
</dd>
<dt>Fix</dt>
<dd>
<p>In this method, the library fixes the output by itself without asking the LLM for a regeneration. Fixes can involve deletion or replacement of certain parts of the input or output.</p>
</dd>
<dt>Filter</dt>
<dd>
<p>If structured data generation is used, this option enables filtering out only the attribute for which the validation failed. The rest of the output will be fed back to the user.</p>
</dd>
<dt>Refrain</dt>
<dd>
<p>In this setting, the output is simply not returned to the user, and the user receives a refusal.</p>
</dd>
<dt>Noop</dt>
<dd>
<p>No action is taken, but the validation failure is logged for further inspection.</p>
</dd>
<dt>Exception</dt>
<dd>
<p>This raises a software exception when the validation fails. Exception handlers can be written to activate custom behavior.</p>
</dd>
<dt>fix_reask</dt>
<dd>
<p>In this method, the library tries to fix the output by itself and then runs validation on the new output. If the validation still fails, then the LLM is asked to regenerate the output<a data-type="indexterm" data-startref="xi_agenticsystemssafetyguardrails1063179" id="id1406"/><a data-type="indexterm" data-startref="xi_safetyguardrails1062338" id="id1407"/>.</p>
</dd>
</dl>

<p>Let’s look at the PII guardrail as an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">guardrails</code> <code class="kn">import</code> <code class="n">Guard</code>
<code class="kn">from</code> <code class="nn">guardrails.hub</code> <code class="kn">import</code> <code class="n">DetectPII</code>

<code class="n">guard</code> <code class="o">=</code> <code class="n">Guard</code><code class="p">()</code><code class="o">.</code><code class="n">use</code><code class="p">(</code>
    <code class="n">DetectPII</code><code class="p">,</code> <code class="p">[</code><code class="s2">"EMAIL_ADDRESS"</code><code class="p">,</code> <code class="s2">"PHONE_NUMBER"</code><code class="p">],</code> <code class="s2">"reask"</code><code class="p">)</code>

<code class="n">guard</code><code class="o">.</code><code class="n">validate</code><code class="p">(</code><code class="s2">"The Nobel prize this year was won by Geoff Hinton,</code><code class="w"/>
<code class="n">who</code> <code class="n">can</code> <code class="n">be</code> <code class="n">reached</code> <code class="n">at</code> <code class="o">+</code><code class="mi">1</code> <code class="mi">234</code> <code class="mi">567</code> <code class="mi">8900</code><code class="s2">")</code><code class="w"/></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1408">
<h1>Exercise</h1>
<p>Extracting the system prompt is a popular form of jailbreak. Can you write a guardrail that prevents users from extracting the system prompt?</p>
</div></aside>

<p>Next, let’s look at how verification modules work.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Verification modules"><div class="sect3" id="id181">
<h3>Verification modules</h3>

<p>As we have seen throughout the book<a data-type="indexterm" data-primary="agentic systems" data-secondary="verification modules" id="xi_agenticsystemsverificationmodules1069936"/><a data-type="indexterm" data-primary="verification modules, agentic systems" id="xi_verificationmodulesagenticsystems1069936"/>, current LLMs suffer from problems like reasoning limitations and hallucinations that severely limit their robustness. However, production-ready applications need to demonstrate a certain level of reliability to be accepted by users. One way to extend the reliability of LLM-based systems is to use a human-in-the-loop who can manually verify the output and provide feedback. However, in the real world a human-in-the-loop is not always desired or feasible. The most popular alternative is to use external verification modules as part of the LLM system. These modules can range from rule-based programs to smaller fine-tuned LLMs to symbolic solvers. There are also efforts to use LLMs as verifiers, called “LLM-as-a-judge.”</p>

<p>Related components include fallback modules. These modules are activated when the verification process fails and retrying/fixing doesn’t work. Fallback modules can be as simple as messages like, “I am sorry I cannot entertain your request” to more complex workflows.</p>

<p>Let’s discuss an example. Consider an abstractive summarization application that operates on financial documents. To ensure quality and reliability of the generated summaries, we need to embed verification and self-fixing into the system 
<span class="keep-together">architecture</span>.</p>

<p>How do we verify the quality of an abstractive summary? While single-number metrics are available to automatically quantify the quality of a summary, a more holistic approach would be to define a list of criteria that a good summary should satisfy and verify whether each criterion is fulfilled.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Several single-number quantitative metrics exist for evaluating summaries. These include metrics like <a href="https://oreil.ly/LPlFJ">BLEU, ROUGE</a>, and <a href="https://oreil.ly/gsOGl">BERTScore</a>. BLEU and ROUGE rely on token overlap heuristics and have been shown to be <a href="https://oreil.ly/rSzbR">woefully inadequate</a>. Techniques like BERTScore that apply semantic similarity have been shown to be more promising, but in the end, the reality is that summaries have subjective notions of quality and need a more holistic approach for verification.</p>
</div>

<p>For the summarization of financial documents application, here is a list of important criteria:</p>
<dl>
<dt>Factuality</dt>
<dd>
<p>The summary is factually correct and does not make incorrect assumptions or conclusions from the source text.</p>
</dd>
<dt>Specificity</dt>
<dd>
<p>The summary doesn’t <em>oversummarize</em>; it avoids being generic and provides specific details, whether numbers or named entities.</p>
</dd>
<dt>Relevance</dt>
<dd>
<p>Also called precision, this is calculated as the percentage of sentences in the summary that are deemed relevant and thus merit inclusion in the summary.</p>
</dd>
<dt>Completeness</dt>
<dd>
<p>Also called recall, this is calculated as the percentage of relevant items in the source document that are included in the summary.</p>
</dd>
<dt>Repetitiveness</dt>
<dd>
<p>The summary should not be repetitive, even if there is repetition in the source document.</p>
</dd>
<dt>Coherence</dt>
<dd>
<p>When read in full, the summary should provide a clear picture of the content in the source document, while minimizing ambiguity. This is one of the list’s more subjective criteria.</p>
</dd>
<dt>Structure</dt>
<dd>
<p>While defining the summarization task, we might specify a structure for the summaries. For example, the summary could be expected to contain some predefined sections and subsections. The generated summary should follow the specified structure.</p>
</dd>
<dt>Formatting</dt>
<dd>
<p>The generated summary should follow proper formatting. For example, if the summary is to be generated as a bulleted list, then all the items in the summary should be represented by bullets.</p>
</dd>
<dt>Ordering</dt>
<dd>
<p>The ordering of the items in the summary should not impede the understanding of the summary content. We also might want to specify an order for the summaries, for example, chronological.</p>
</dd>
<dt>Error handling</dt>
<dd>
<p>In case of errors or omissions in the source document,
there should be appropriate error handling.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1409">
<h1>Exercise</h1>
<p>Define the verification criteria for the question-answering assistant for the Canadian parliamentary proceedings dataset provided in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>. In what ways do the criteria differ from the abstractive summarization task?</p>
</div></aside>

<p>How do we automatically verify whether a given summary meets all these criteria? We can use a combination of rule-based methods and fine-tuned models. Ultimately, the rigor of the methods used for verification depends on the degree of reliability needed for your application. However, we notice that once we reduce the scope of the verification process to verify fitness of individual criteria rather than the application as a whole, it becomes easier to verify accurately using inexpensive techniques. Let’s look at how we can build verifiers for each criteria of the abstractive summarization task:</p>
<dl>
<dt>Factuality</dt>
<dd>
<p>Verifying whether an LLM-generated statement is factual is extremely difficult if we do not have access to ground truth<a data-type="indexterm" data-primary="ground truth" data-secondary="summarization applications" id="id1410"/>. But for summarization applications, we do have access to the ground truth. Therefore, we can verify factuality by taking each sentence in the summary and checking whether, given the source text, one can logically conclude the statement in the summary. This can be framed as a natural language inference (NLI) problem, which is a standard NLP task.</p>

<p>In the NLI task, we have a hypothesis and a premise, and the goal is to check if the hypothesis is logically entailed by the premise. In our example, the hypothesis is a sentence in the summary and the premise is the source text.</p>

<p>Training an NLI model specific to your domain might be a cumbersome task. If you do not have access to an NLI model, you can use token overlap and similar statistics to approximate factuality verification.</p>

<p>For numbers and named entities, factuality verification can be performed by using string matches. You can verify if all the numbers and named entities in the summary are indeed present in the source text.</p>
</dd>
<dt>Specificity</dt>
<dd>
<p>One way for a summary to be specific is to include numbers and named entities where relevant. For each sentence in the summary, we can check whether the content in the source document related to the topic of the sentence contains any numbers and named entities, and if these are reflected in the summary. Numbers and named entities can be tagged and detected using regular expressions or libraries like <a href="https://oreil.ly/zatAW">spaCy</a>.</p>
</dd>
<dt>Relevance/precision</dt>
<dd>
<p>We can train a classification model that detects whether a sentence in the summary is relevant. Note that there are limits to this approach. If this classification model was good enough, we could have directly used it to select relevant sentences from the source text to build the summary! In practice, this classification model can be used to remove irrelevant content that is more obvious.</p>
</dd>
<dt>Recall/completeness</dt>
<dd>
<p>What content merits inclusion in the summary is a difficult question, especially if there is a hard limit on the summary length. You can train a ranking model that ranks sentences in the source document by importance, and then verify if the top-ranked sentences are represented in the summary. You can also specify beforehand the type of content that you need represented in the summary and build a classification model for determining which parts of the source document contain pertinent information. Using similarity metrics like embedding similarity, you can then find if the content has been adequately represented in the 
<span class="keep-together">summary</span>.</p>
</dd>
<dt>Repetitiveness</dt>
<dd>
<p>This can be discovered by using string difference algorithms like the <a href="https://oreil.ly/Ny_Ku">Jaccard distance</a> or by calculating the embedding similarity between pairs of summary sentences.</p>
</dd>
<dt>Coherence</dt>
<dd>
<p>This is perhaps one of the most difficult criteria to verify. One way to solve this, albeit a more expensive solution, is to build a prerequisite detection model. For each sentence in the summary, we detect if all the sentences that come before it are sufficient prerequisites for understanding the correct sentence. For more information on prerequisite detection techniques, see <a href="https://oreil.ly/6JnRs">Thareja et al.</a></p>
</dd>
<dt>Structure</dt>
<dd>
<p>If we specify a predetermined structure (sections and subsections) for the summary, we can easily identify if the structure is adhered to by checking if the desired section and subsection titles are present in the summary. We can also verify using embedding similarity techniques if the content within the sections and subsections is faithful to the title of the section/subsection.</p>
</dd>
<dt>Formatting</dt>
<dd>
<p>This involves checking whether the content is in the appropriate formatting, for example, whether it is a bulleted list or a valid JSON object.</p>
</dd>
<dt>Ordering</dt>
<dd>
<p>The desired order can be chronological, alphabetical, a domain, or task-specific ordering. If it is supposed to be chronological, you can verify by extracting dates in the summary and checking if the summary contains dates in a chronological order. If the ordering requirements are more complex, then verifying adherence to order may become an extremely difficult task.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>Do not expect your verification process to be strictly better than your summary model. If that was the case, you could have used the verification process to generate the summary!</p>
</div>

<p>We can also deploy symbolic verifiers like <a href="https://oreil.ly/lOsg_">SAT</a> (Boolean satisfiability) solvers and logic planners. This type of verification is beyond the scope of this book.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1411">
<h1>Exercise</h1>
<p>For the task presented in the previous exercise (question-answering assistant for Canadian parliamentary proceedings), how would you build verification modules for each of the criteria you have identified? Would you be able to perform robust verification based on heuristics-based techniques alone?</p>
</div></aside>

<p>Once verification modules are part of our system architecture, we will also need to decide what action to perform when the verification fails. One option is to just resample from the language model again. Regeneration can be performed for the full output or only for the output that failed verification<a data-type="indexterm" data-startref="xi_agenticsystemsverificationmodules1069936" id="id1412"/><a data-type="indexterm" data-startref="xi_verificationmodulesagenticsystems1069936" id="id1413"/>. We can also develop antifragile architectures that have fallbacks in case of failure, which we will discuss in 
<span class="keep-together"><a data-type="xref" href="ch13.html#ch13">Chapter 13</a></span>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Adding more verifiers can drastically increase system latency<a data-type="indexterm" data-primary="system latency, verifier impact on" id="id1414"/>. Thus, their inclusion has to be balanced with accuracy and system latency needs.</p>
</div>

<p>Finally,  let’s discuss agent orchestration software that connects all these components.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Agent Orchestration Software"><div class="sect2" id="id182">
<h2>Agent Orchestration Software</h2>

<p>For agentic workflows to proceed smoothly, we need software that connects all the components. Orchestration<a data-type="indexterm" data-primary="agentic systems" data-secondary="orchestration software" id="id1415"/><a data-type="indexterm" data-primary="orchestration framework, agentic systems" id="id1416"/> software manages state; invokes tools; initiates retrieval; pipes buffers; and logs intermediate and final outputs. Many agentic frameworks, both open source and proprietary, perform this function, including <a href="https://oreil.ly/7vmlY">LangChain</a>, <a href="https://oreil.ly/uxejK">LlamaIndex</a>, <a href="https://oreil.ly/Ntxii">CrewAI</a>, <a href="https://oreil.ly/tx3qy">AutoGen</a>, <a href="https://oreil.ly/HI-Jn">MetaGPT</a>, <a href="https://oreil.ly/sA_DR">XAgent</a>, <a href="https://oreil.ly/SBGC_">llama-stack-apps</a>, and so on.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Agents are a relatively new paradigm, so all these agentic frameworks are expected to change a lot in the coming months and years. These frameworks are implemented in an opinionated fashion and hence are less flexible. For prototyping, I suggest picking LangChain or LlamaIndex<a data-type="indexterm" data-primary="LlamaIndex" id="id1417"/><a data-type="indexterm" data-primary="LangChain" id="id1418"/> for ease of use. For production use, you might want to build a framework internally from scratch or by extending the open source ones. This book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> contains a rudimentary agentic framework as well.</p>
</div>

<p>Now that we have learned all the different agentic system components, it is time to get building! The book’s <a href="https://oreil.ly/llm-playbooks">GitHub repository</a> contains sample implementations of various types of agents. Try modifying them for your use case to understand the tradeoffs being made.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The keep it simple, stupid (KISS) principle applies to agents perhaps more than any other recent paradigm. Don’t complicate your agentic architecture unless there is a compelling reason to do so. We will discuss this more in <a data-type="xref" href="ch13.html#ch13">Chapter 13</a>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id1419">
<h1>Web Agents and Computer Use</h1>
<p>Throughout this chapter<a data-type="indexterm" data-primary="web agents" id="id1420"/>, we have seen examples of agents executing actions in the real world, primarily by invoking a software interface. However, in the quest for automating human tasks, we find there are many cases where an external software interface doesn’t exist, with only a GUI like a web page available. A lot of tedious work for humans involves actions on a computer like copying/pasting between systems, filling columns in an Excel sheet using data from another system, and so on. Can agents help us automate these kinds of tasks?</p>

<p>A new paradigm of agents called web agents promises to do so. Web agents use the Document Object Model (DOM)<a data-type="indexterm" data-primary="Document Object Model (DOM)" id="id1421"/><a data-type="indexterm" data-primary="DOM (Document Object Model)" id="id1422"/> or the screenshot of a web page to understand the page’s current state and perform actions like entering information into fields, clicking on elements, and navigating to links. A working web agent could help you automatically book a flight by navigating to a travel website, entering information, choosing between different options, and completing payment. As of today, this is still a fledgling technology, with poor results on <a href="https://oreil.ly/Dyp2L">benchmark tasks</a>.</p>

<p>Companies like Anthropic<a data-type="indexterm" data-primary="Anthropic" data-secondary="computer use features" id="id1423"/> have launched initial versions of <a href="https://oreil.ly/FT44u">computer use</a> features that enables agents to control a computer desktop 
<span class="keep-together">environment</span><a data-type="indexterm" data-startref="xi_agenticsystems107969" id="id1424"/>.</p>

<p>Run Anthropic’s <a href="https://oreil.ly/nUtar">Computer Use Demo</a>. Pay attention to the <a href="https://oreil.ly/AsFpP">system prompt</a> provided. What are the common failure modes you observe?</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id183">
<h1>Summary</h1>

<p>In this chapter, we discussed the different ways in which LLMs can interface with external tools<a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsinterfacingwithexternaltools10435" id="id1425"/>. We introduced the agentic paradigm and provided a formal definition of agents. We identified the components of an agentic system in detail, exploring models, tools, data stores, guardrails and verifiers, and agentic orchestration software. We learned how to define and implement our own tools.</p>

<p>In the next chapter, we will explore data representation and retrieval, crucial elements of interfacing LLMs with external data.</p>
</div></section>
</div></section></div>
</div>
</body></html>