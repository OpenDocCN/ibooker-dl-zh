- en: 3 Machine learning vs. deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: A comparison of machine learning and deep learning as methods to tackle tabular
    data problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of machine learning and deep learning in terms of simplicity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of machine learning and deep learning in terms of transparency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of machine learning and deep learning in terms of efficacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There’s an open debate in the data science community about the best machine
    learning approach for tabular data. Some assert that classic machine learning
    techniques, such as gradient boosting using tools like XGBoost or LightGBM, are
    superior for most tabular data problems. Others advocate for including deep learning
    in your analysis toolkit. In this chapter, we’ll examine these two approaches
    using two concrete examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the price of Airbnb New York City listings. In this example, we use
    a real-world dataset of Airbnb listings to train models that predict whether a
    new listing will have a price above or below the average listing price. We’ll
    use this example to examine simplicity, transparency, and efficacy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the length of time a property in a local real estate market is on the
    market before it is sold. In this example, we use a contrived real estate listing
    dataset to illustrate the explainability aspect of transparency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will focus on three criteria that are of particular value in interpreting
    scientific and business data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Simplicity*—The simpler the solution in terms of the code for the application
    and the core API of the framework, the better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transparency*—A solution that is interpretable and that can be explained easily
    to a business stakeholder is best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficacy*—The solution that provides the best results and takes less time
    to train and implement is preferable. Also, research interest can lead to more
    effective results as new approaches are discovered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.1 Predicting Airbnb prices in New York City
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compare the simplicity of machine learning and deep learning, we will contrast
    two solutions to a particular tabular data classification problem: predicting
    whether a New York City Airbnb listing will have a price that is greater than
    or less than the average price for Airbnb listings in that market. The two solutions
    we will compare are'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine learning*—Represented by a solution that uses XGBoost, a popular gradient-based
    approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep learning*—Represented by a solution that uses the Keras functional API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will compare the code complexity of these solutions and review what these
    two solutions tell us about the overall question of the relative simplicity of
    machine learning and deep learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 The Airbnb NYC dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To solve the problem of predicting whether a New York City Airbnb listing will
    have a price that is greater than or less than the average price, we use a tabular
    dataset with details about Airbnb listings in New York City. Figure 3.1 includes
    descriptions of the columns of the NYC Airbnb dataset along with the type of data
    in each column, and the dataset, which you can see a sample of in figure 3.2,
    has been shared in Kaggle: [https://mng.bz/avJ7](https://mng.bz/avJ7).'
  prefs: []
  type: TYPE_NORMAL
- en: Each row in this dataset has the information for a single listing, and each
    column in the dataset has the values for all listings for a given characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F01_Ryan2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Details about the columns in the Airbnb NYC dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F02_Ryan2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 Sample of rows from the Airbnb NYC dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The Airbnb NYC dataset has characteristics that make it a good choice for comparing
    approaches to solving tabular data problems:'
  prefs: []
  type: TYPE_NORMAL
- en: It has a convenient size. With around 49,000 records, it is big enough to be
    interesting but not so huge that it requires special “big data” tools such as
    Spark to handle it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a good number of columns for a comparison between machine learning and
    deep learning. As we will find out in subsequent chapters, a dataset with only
    three or four columns would immediately favor a classic machine learning approach.
    A dataset with hundreds of columns would be difficult to examine. The Airbnb NYC
    dataset has a “just right” number of columns—enough to give deep learning a chance
    to shine but not so many columns that the dataset is difficult for a human to
    comprehend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the dataset has a reasonable number of rows and columns, it is easy
    to take a quick look at it in a spreadsheet, which means we don’t have to write
    Python code every time we want to answer a question about the dataset. With a
    spreadsheet, you can sort, filter, and count aspects of the dataset quickly and
    take advantage of scripting for Excel or Google Sheets to do a more detailed investigation.
    The Airbnb NYC dataset is amenable to being examined in a spreadsheet, which means
    it can be examined with minimal effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset includes an interesting range of column types, including several
    kinds of continuous columns (`minimum_nights` is integer values, `price` and `reviews_per_month`
    are floating point values, and `latitude` and `longitude` are geospatial values),
    categorical columns (`neighbourhood_group`, `neighbourhood`, and `room_type`),
    and free-form text columns (`name` and `host_name`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset has some warts—for example, there are missing values in some of
    the columns—but it is not so messy that it requires massive cleanup before it
    can be used to train a model. This makes it convenient to build an application
    around this dataset without getting too distracted by cleanup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is both open sourced and based on real data from a real business. As we will
    see in subsequent chapters of this book, one of the challenges of exploring machine
    learning and deep learning with tabular data is the scarcity of substantial open
    source tabular datasets that represent real business problems. The Airbnb dataset
    is a rare example of a nontrivial tabular dataset that contains information from
    a working business.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this dataset, the target for the model is evident: `price`. In our case,
    we are deriving a target based on `price`—namely whether a given listing will
    have a price that is above or below the median price for the listings in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this subsection we have taken a first look at the Airbnb NYC dataset. In
    the next subsection we will look at the code that ingests this dataset to train
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Introduction to the code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have introduced the Airbnb NYC dataset, let’s take a look at the
    code for the solutions. We won’t go into all the details of the code in this section,
    but it’s important to have an overview of how the pieces fit together.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 summarizes the files that make up the two solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Files that make up the Airbnb NYC solution
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are more details on the files that make up the solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input CSV* ([https://mng.bz/avJ7](https://mng.bz/avJ7)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data cleanup notebook* ([https://mng.bz/gawV](https://mng.bz/gawV)). Note
    that while the XGBoost and Keras versions of the code use the same cleanup notebook,
    XGBoost has built-in capabilities, such as handling missing values, which means
    an XGBoost-only version of the cleanup notebook could be simpler than the common
    version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cleanup config file* ([https://mng.bz/ey7Q](https://mng.bz/ey7Q)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training config file* for XGBoost ([https://mng.bz/pKOz](https://mng.bz/pKOz)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training config file* for Keras ([https://mng.bz/vKpr](https://mng.bz/vKpr)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*XGBoost training notebook* ([https://mng.bz/YDGA](https://mng.bz/YDGA)). Among
    the gradient boosting solutions we chose XGBoost because it is very popular and
    there is plenty of guidance online about XGBoost if we happen to run into any
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Keras training notebook* ([https://mng.bz/JYwP](https://mng.bz/JYwP)). We
    chose Keras as a representative deep learning approach because, unlike alternatives
    like PyTorch or fastai on top of PyTorch, Keras is, along with raw TensorFlow,
    most commonly used in business applications. We chose Keras over one of the tabular
    deep learning libraries introduced in the section “Comparing the research success
    of gradient boosted approaches with deep learning” because Keras is more widely
    used than any of those libraries and its APIs can be compared to the APIs of XGBoost
    in a way that is more “apples to apples” than a comparison between a deep learning
    library specifically designed for tabular data and the general capabilities of
    XGBoost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For your convenience, the XGBoost and Keras solutions are shared in two separate
    folders, but most of the code is common between the two solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for the XGBoost solution at [https://mng.bz/GeEO](https://mng.bz/GeEO).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code for the Keras solution at [https://mng.bz/zZ4Q](https://mng.bz/zZ4Q).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between these two repos are limited to the training notebooks
    and the training config files.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 A deep learning solution using Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before digging deeper into the details of the Airbnb dataset and the code used
    in the solutions, let’s put the Keras solution into context of its software stack.
    Figure 3.4 shows the stack for the Keras solution to the Airbnb price prediction
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 The stack for the Airbnb NYC Keras solution
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 8 we will go into more detail about the stack layers shown in figure
    3.4\. For now, we can observe that Keras is the high-level deep learning API that
    we use to implement the deep learning solution to the Airbnb problem that we will
    examine in this chapter. There are two low-level deep learning frameworks, and
    the deep learning solution we will examine in this chapter depends on the TensorFlow
    low-level framework because that is the framework on which Keras is built.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Training features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of both solutions is to predict whether a given Airbnb listing will
    have a price that is above or below the average price in the input dataset. To
    achieve this goal, both models are trained on the same set of features. The subset
    of features that we will use to train the model is defined in the config file
    for model training: [https://mng.bz/OBoE](https://mng.bz/OBoE). The following
    is the portion of the config file that specifies the features used to train the
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The config file also includes a list of features that are explicitly excluded
    from the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The reasons why these columns are not used as features to train the model are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`price` is not included as a feature for training the model because it defines
    the target for the model, whether or not a listing has a price above or below
    the average price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two ID columns are not included as features because they don’t carry any
    signal about the price of a listing because they are just numeric IDs assigned
    to listings and hosts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We don’t use `latitude` or `longitude` as features because the geographic location
    of listings is already encoded in the `neighbourhood_group` and `neighbourhood`
    features that are used to train the model. If we didn’t have these features to
    use for the geographic location of the listings, we could use the `latitude` and
    `longitude` values (or polar coordinates derived from them: [https://mng.bz/0Q66](https://mng.bz/0Q66))
    to cluster listings according to their locations or convert them into polar coordinates.
    Using the raw latitude and longitude for each listing as features could lead to
    overfitting because each listing would have a unique value for the pair (`latitude`,
    `longitude`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `name` and `host_name` columns are not included as features because they
    are somewhat arbitrary sets of tokens that allow human readers to identify the
    listings. An interesting exercise would be to include `host_name` as a feature
    to see if it provides some kind of signal related to the price of listings that
    have the same host.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We decided to not include `availabiltiy_365` in the feature set because the
    column is challenging to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have examined the features that we will use to train the model to predict
    whether an Airbnb listing will have a price above or below the average price.
    In the next section, we will compare the simplicity of the code for the gradient
    boosting and deep learning models trained on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Comparing gradient boosting and deep learning solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned previously, the two solutions for the Airbnb NYC problem only
    differ in a few places. Figure 3.5 shows the file structure of the solution again
    with the files that contain differences between the two approaches highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 Airbnb solution files that differ between the XGBoost and Keras solutions
  prefs: []
  type: TYPE_NORMAL
- en: If the solutions only differ in these four files, how can we use this example
    to contrast the simplicity of XGBoost vs Keras deep learning? Table 3.1 compares
    the code complexity across several aspects of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 Comparing the code complexity of XGBoost and Keras in three areas
  prefs: []
  type: TYPE_NORMAL
- en: '| Aspect of code complexity | XGBoost | Keras deep learning model |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation | Code block required to transform the list of numpy arrays
    into a numpy array of lists | NA—the data preparation was designed with the Keras
    deep learning solution in mind |'
  prefs: []
  type: TYPE_TB
- en: '| Model definition | Single statement, consistent with the Scikit-learn pattern
    | Block of code required to define the layers of the model, with unique sets of
    layers for each column type: continuous, categorical, text |'
  prefs: []
  type: TYPE_TB
- en: '| Model training | Single statement, consistent with the Scikit-learn pattern
    | Block of code required to allow for callback control of the training process
    to avoid training iterations that provide no benefit and to ensure the training
    process outputs the trained model with the best performanceBlock of code to define
    the callback objects required to have an efficient Keras training cycle |'
  prefs: []
  type: TYPE_TB
- en: '| Model saving | Single statement, consistent with the Scikit-learn pattern
    | Included as part of model saving callback |'
  prefs: []
  type: TYPE_TB
- en: '| Model loading | Block of code—requires installing the latest version of XGBoost
    or loaded model will fail with error:`AttributeError: ''XGBClassifier'' object
    has no attribute ''_le``''`  | Single statement |'
  prefs: []
  type: TYPE_TB
- en: Let’s look at the code in the Airbnb NYC solution for each of these aspects
    of code complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The XGBoost solution has some additional data preparation code. The Airbnb NYC
    solution was originally coded using Keras deep learning and then the XGBoost solution
    was created using the Keras solution as a starting point. The original Keras model
    required training input in the form of a list of numpy arrays. XGBoost requires
    input in the form of a numpy array of lists. The following listing contains the
    code in the XGBoost training notebook that converts the original data format into
    the format required by XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Data preparation code for XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines lists of lists for the training and test datasets (one list for each
    feature)
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts the train list of lists into a numpy array of lists
  prefs: []
  type: TYPE_NORMAL
- en: ③ Converts the test list of lists into a numpy array of lists
  prefs: []
  type: TYPE_NORMAL
- en: Note that while this is a genuine example of the XGBoost code having some additional
    complexity compared to the Keras code, this additional code is not intrinsically
    required by XGBoost but rather is needed because of the way that the XGBoost solution
    was adapted from the Keras solution.
  prefs: []
  type: TYPE_NORMAL
- en: To get additional confirmation of what the XGBoost data preparation code does,
    go to Gemini ([https://gemini.google.com](https://gemini.google.com)) and paste
    the code into the entry field along with the prompt “what does this code do?”
    and click the Submit button, as shown in figure 3.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 Entering a request to interpret code in Gemini
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get a response that explains what the code does along with this summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have looked at the difference between XGBoost and Keras in terms
    of data preparation, let’s compare the model definition code for the two solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the XGBoost model definition for the Airbnb NYC problem—one
    line of code that follows the pattern of Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at what the model definition code looks like for the Keras solution.
    Figure 3.7 shows the initial part of the function that defines the deep learning
    model for the Airbnb NYC problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 Model definition for the Keras deep learning solution for the Airbnb
    NYC problem (part 1)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 shows the rest of the function that defines the deep learning model
    for the Airbnb NYC problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 Model definition for the Keras deep learning solution for the Airbnb
    NYC problem (part 2)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 shows the visualization for the model for the Airbnb problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 Visualization of the Keras model for the Airbnb problem
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a bit of an extreme contrast—the deep learning model could have been
    defined more simply. This model definition specifies a different set of Keras
    layers for each column type (continuous, categorical, and text). This is not the
    minimal layer definition possible for a tabular data model, but it is very flexible.
    It will be able to deal with tabular datasets with various combinations of continuous,
    categorical, and text columns. Also, this model definition includes code that
    specifies the layers for text columns, and we did not choose any text columns
    to train the Airbnb model, so this code could have been omitted without affecting
    the Keras version of the Airbnb NYC solution. Nevertheless, the difference between
    the simplicity of the XGBoost model definition and the complexity of the Keras
    model definition underlines an advantage of XGBoost: model definition code is
    simpler in XGBoost than it is in Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have compared the data preparation code and the model definition code. Next,
    let’s compare the model training code for the XGBoost and Keras solutions. In
    the XGBoost solution, training is accomplished with a single line of code with
    defaults accepted for all parameters that have default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The following listing shows that for the Keras solution there are two different
    versions of the fit statement and several additional parameters (including the
    batch size and the default number of epochs that will be run in the training process)
    need to be set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Fit statements for Keras
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Fit statement including a parameter that lists callbacks for early stopping
    and model saving
  prefs: []
  type: TYPE_NORMAL
- en: ② Fit statement without callbacks
  prefs: []
  type: TYPE_NORMAL
- en: There isn’t a huge difference in the complexity of the training code between
    XGBoost and Keras. However, to make the Keras training process efficient, we need
    to use callbacks to avoid wasted training cycles and ending up with a suboptimal
    trained model at the end of the training process. See chapter 6 of *Deep Learning
    with Structured Data* ([https://mng.bz/KGx0](https://mng.bz/KGx0)) for details
    on using Keras callbacks to optimize the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before taking a look at the callback statements for the Keras solution, let’s
    see what Gemini can tell us about the fit statements. Again, let’s submit this
    code to Gemini preceded with the prompt “what does this code do?” If you don’t
    get a satisfactory answer the first time, click Regenerate draft to get another
    answer. On the second try with Gemini, we got a detailed description of the code
    that included the following description of the parameters for the fit statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Gemini also provides the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note how in the last line Gemini qualifies the limitations of its analysis by
    correctly noting that given only the training snippet, it cannot infer all the
    details about the complete solution.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 shows the code that defines the callbacks that are used in the model
    training step for Keras. This code adds additional complexity in the Keras version
    of the Airbnb NYC solution.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Callback statements for Keras
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines an early stopping callback object to specify that the training process
    should stop once the performance stops improving
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds the first callback to the list of callbacks to be used in the training
    process
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a model-saving callback object to ensure that the trained model that
    has the optimal performance in the whole training run is the model that is saved
    at the end of the training run
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds the second callback to the list of callbacks to be used in the training
    process
  prefs: []
  type: TYPE_NORMAL
- en: Once we have trained the model, we want to save it to a file so that we can
    load it and exercise it in another session or as part of the model’s deployment.
    In our simple example, we save and reload the model in the same notebook as the
    one we use to train the model. The statement to save the model for XGBoost is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For Keras, we don’t need an explicit statement to save the model because the
    model is saved automatically with the model-saving callback.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to load the model in XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Model-loading statements for XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a new XGBoost classifier object
  prefs: []
  type: TYPE_NORMAL
- en: ② Loads the new XGBoost classifier object with the model you saved with the
    save_model statement
  prefs: []
  type: TYPE_NORMAL
- en: The statement to load a model for Keras is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'One additional difference between XGBoost and Keras is that if you try to load
    a saved XGBoost classifier model (like the one we trained for the Airbnb NYC problem)
    and run a prediction, you will get an error if you are not on a very current version
    of XGBoost. To get around this, the XGBoost model training notebook includes the
    following statement to ensure that the XGBoost is at the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have compared the simplicity of the XGBoost and Keras code
    for data preparation, model definition, model training, and model saving. Next,
    we will discuss the conclusions that we can draw from this comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the best pieces of advice for data science projects is to take the simplest
    approach possible. Apply Occam’s razor to a data science project to make the task
    easier. If there is more than one way to solve a problem, pick the simplest approach.
    If linear regression will solve the problem, then why use support vector machines?
    If a conventional coding approach will solve the problem, then why use machine
    learning at all? If you take the simplest approach, you will likely get initial
    results faster, complete development of the whole solution faster, and have an
    easier time maintaining the system once it has been deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer the question of how classic machine learning and deep learning compare
    in terms of simplicity, we compared solutions using each approach to a concrete
    problem: the Airbnb NYC price prediction problem. By answering this question,
    we can apply the dictum “keep it simple” to tabular data problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous subsection we reviewed the Airbnb NYC tabular dataset and compared
    the complexity of two solutions trained with this dataset—one that uses a gradient
    boosting approach (XGBoost) and one that uses deep learning (Keras). The XGBoost
    solution required some additional data preparation code, but this additional code
    is an artifact of the XGBoost solution being adapted from the deep learning solution,
    not a direct requirement of XGBoost. For the XGBoost solution, only one line of
    code was needed for model definition and one line of code for model training.
    For Keras, on the other hand, model definition required many lines of code, and
    model training required more lines of code than XGBoost, in particular to take
    advantage of Keras callbacks to ensure an efficient training process. The complexity
    of the code to save and load the model was about the same in the XGBoost solution
    and the Keras solution. While the Airbnb NYC problem is not representative of
    all tabular data problems, it does give us the opportunity to make an apples to
    apples comparison between a gradient boosting approach and a deep learning approach,
    and the conclusion of this comparison is that the XGBoost code is simpler. The
    XGBoost solution has fewer lines of code, and the statements for model definition
    and model training are simpler and require fewer nondefault parameter values in
    XGBoost than they do in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Before closing our conclusion on the comparative simplicity of gradient boosting
    and deep learning, we need to note that Keras isn’t the only deep learning approach
    available. There are other deep learning approaches that deal with tabular data,
    and some of these approaches have code for defining and training models that is
    simpler than the Keras code that we examined for the Airbnb NYC problem. For example,
    with the fastai framework ([https://docs.fast.ai/](https://docs.fast.ai/)), which
    we will review in more detail in chapter 9, you can define a model to work with
    tabular data, train it, and use it to get predictions in less than 10 lines of
    code. Tensorflow canned estimators ([https://mng.bz/9Y51](https://mng.bz/9Y51))
    are another simple approach to deep learning with tabular data. With these canned
    estimators, you can train a model on a tabular dataset and get predictions from
    the model with an API that is as simple as the XGBoost API. These are just two
    examples of deep learning approaches with code that is simpler than Keras. The
    benefit of Keras is that it is very flexible, and its flexibility is one of the
    reasons that businesses frequently use Keras for production systems while simpler
    approaches, such as fastai, are rarely seen in production deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have compared machine learning and deep learning in terms of simplicity,
    in the next section we will compare the two approaches in terms of transparency.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two aspects of transparency that are relevant when comparing gradient
    boosting techniques with deep learning: explainability (that is, how easy it is
    to explain how the model works) and feature importance (that is, how easy it is
    to determine which feature has the biggest effect). In this section, we’ll compare
    gradient boosting with deep learning according to these two aspects of transparency.'
  prefs: []
  type: TYPE_NORMAL
- en: To compare the explainability of gradient boosting and deep learning, we will
    consider a simple, contrived dataset and how a model trained on this dataset could
    be explained.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we will use contains information about houses in a particular
    real estate market, as shown table 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 House time on the market dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Time on market (weeks) | City | Asking price (thousand $) | Distance to transit
    station (km) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Kitchener | 600 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Waterloo | 700 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Kitchener | 900 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Waterloo | 700 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Waterloo | 500 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Waterloo | 600 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Waterloo | 750 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Kitchener | 500 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Kitchener | 1000 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Waterloo | 750 | 10 |'
  prefs: []
  type: TYPE_TB
- en: We will return to the Airbnb dataset in the next section. For now, this real
    estate dataset is simple enough to easily illustrate explainability.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset includes the city the house is located in, the asking price for
    the house, and the distance from the house to the closest transit station, along
    with the number of weeks the house was on the market before it was sold. We want
    to train a model using this dataset that will predict whether a house that is
    a fresh listing will be on the market for more or less than a month.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that we wanted to give a nonspecialist, business audience an idea of
    how a decision tree model could be used to solve the “time in the market” problem
    for this dataset. We could create an illustration like the one in figure 3.10
    to give a rough idea of how such a decision tree works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 Decision tree illustration
  prefs: []
  type: TYPE_NORMAL
- en: Note that the illustration doesn’t contain any jargon or assume that the reader
    has any background in machine learning. The point of the decision tree is evident.
    Note this decision tree is a gross simplification, and there are important technical
    differences between a simple decision tree like this and a model that uses gradient
    boosting. XGBoost, for example, uses multiple decision trees, so this illustration
    by itself would not be sufficient to explain an XGBoost model. Nevertheless, it
    shows that, for some classic machine learning algorithms, it’s possible to give
    nonspecialists an intuition about how the algorithms work without forcing the
    nonspecialists to learn technical details about them.
  prefs: []
  type: TYPE_NORMAL
- en: What if we wanted to give the same nonspecialist audience a general sense of
    how a deep learning model would be trained to solve the same “time on the market”
    problem? We could start with a generic neural network schematic like the one shown
    in figure 3.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 Neural network illustration
  prefs: []
  type: TYPE_NORMAL
- en: Such an illustration may help to explain the “deep” in “deep learning,” but
    it provides no insight about how the model is actually trained. Would it help
    if we zoomed in to show how an individual node in the network works, as shown
    in figure 3.12?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 Simple illustration of a node in a neural network
  prefs: []
  type: TYPE_NORMAL
- en: Most nonspecialists would find it hard to interpret figure 3.12\. If zooming
    into details of the neural network doesn’t yield better explainability, what if
    we take a different approach and lean on the analogy between neural networks and
    biological neurons, as shown in figure 3.13? This illustration attempts to relate
    the overall neural network to the working of an individual node in the network
    (a “neuron”) and then relate that node to a biological neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 Relating a neural network to a biological neuron
  prefs: []
  type: TYPE_NORMAL
- en: Appealing to the biological analogy raises two problems. First, the analogy
    itself is controversial. Some experts in the industry think that neural networks
    work nothing like biological neurons (e.g., [https://mng.bz/jp2P](https://mng.bz/jp2P)).
    Even if you accept that the analogy between neural networks and biological neurons
    is valid, using this analogy to explain a deep learning system can contribute
    to serious misunderstanding if nonspecialists infer from the analogy that simple
    deep learning systems have brain-like capabilities. Second, the analogy doesn’t
    really clarify how a deep learning model is trained. Most people know what a biological
    neuron is, but they don’t know how biological neurons actually work. An analogy
    is not helpful if the thing that is the source of your analogy is a bit of a mystery
    itself. In sum, the analogy between nodes of a neural network and biological neurons
    is, at best, an isolated curiosity that doesn’t help a nonspecialist understand
    how deep learning actually works.
  prefs: []
  type: TYPE_NORMAL
- en: What can we conclude from this example? While millions of people have learned
    enough about deep learning to appreciate what it can and can’t do, deep learning
    still presents some formidable obstacles when you try to explain it to a business
    audience. Unlike decision trees, the basics of which can be explained in an easily
    understood illustration, deep learning does not lend itself to being explained
    in one easy picture. Even today, with accessible deep learning frameworks like
    fastai and hundreds of free online resources about deep learning, people with
    a decent background in linear algebra, calculus, and programming still need several
    months of study to get a solid intuition of how deep learning works. We assert
    that it’s not possible to pass on this intuition to a nonspecialist audience in
    one simple illustration, let alone create an instantly accessible explanation
    of how deep learning will work with a specific dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Feature importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section we compared machine learning approaches and deep learning
    according to one aspect of transparency: how easy it is to explain the approach
    to a nonspecialist audience. In this section we’ll look at the other aspect of
    transparency: how easy it is to determine the importance of a given feature to
    the performance of the model as a whole.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the Airbnb NYC example, we can see that the XGBoost solution uses
    XGBoost’s built-in API for determining feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost offers several different options for calculating feature importance.
    The default is gain, which is the average gain across all splits the feature is
    used in, where gain means the degree to which a feature separates the input examples
    (in our case, Airbnb listings) according to the target (in our case, whether the
    price of the listing is above or below the median price).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Airbnb NYC example, the output for this API shows the gain value for
    each of the features. The third feature used to train the model (`room_type`)
    has the biggest effect, followed by the first feature (`neighbourhood_group`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can show the feature importance values in chart form using the following
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The output of this statement is a chart that shows the relative importance of
    each of the features, as shown in figure 3.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F14_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 Feature importance for the Airbnb NYC problem according to XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: This chart makes it clear that, according to XGBoost built-in feature importance,
    `room_type` is the most important feature, with `neighbourhood_group` a distant
    second and all the other features being relatively unimportant to the behavior
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The values for the `room_type` feature are
  prefs: []
  type: TYPE_NORMAL
- en: Entire home/apartment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Private room
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared room
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It intuitively makes sense that `room_type` would have a significant effect
    on the price of a listing. We would expect a large difference in price between
    a listing for an entire home and a listing for a shared room.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at how we can get feature importance for the XGBoost
    model, let’s consider how we can get feature importance for the Keras solution
    for the Airbnb NYC problem. Unlike XGBoost, Keras (and deep learning frameworks
    in general) does not have a built-in way to determine feature importance. However,
    you can apply external methods to get feature importance for Keras similar to
    the built-in feature importance in XGBoost. For example, you can use a utility
    like lime ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime))
    or shap ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    to get feature importance for a Keras deep learning model. Examining these approaches
    to feature importance analysis is beyond the scope of this chapter. For now, we’ll
    note that with XGBoost you can get a basic idea of feature importance with a couple
    of lines of code while such a simple approach is not available for deep learning
    frameworks like Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section we have compared machine learning and deep learning according
    to the following two aspects of transparency:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Explainability*—How easy is it to explain how the model works, particularly
    to business stakeholders or other people who are not data science specialists?
    Business stakeholders will have greater faith in a model that they can grasp in
    some intuitive way compared to a model that seems like a black box. More critically,
    for regulated industries like auto insurance, transparency isn’t just a question
    of reassuring business stakeholders with accessible abstractions of how the model
    works. Regulators in such industries expect to get detailed and comprehensible
    explanations of how models work and how the model’s behavior changes as new versions
    of the model are deployed externally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feature importance*—How easy is it to determine which features have the biggest
    effect on the behavior of the model?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen that machine learning is more explainable than deep learning and
    that XGBoost provides a built-in feature importance API while Keras does not have
    a built-in facility for determining feature importance. Now that we have compared
    machine learning and deep learning in terms of transparency, in the next section
    we will compare the two approaches in terms of their efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Efficacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have compared machine learning with deep learning in terms of simplicity
    and transparency. Now let’s look at how the two approaches compare to each other
    in terms of efficacy. We’ll look at two aspects of efficacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Performance*—We will return to the Airbnb NYC example to compare the relative
    performance of the XGBoost version of the application with the Keras version of
    the application. In the Airbnb NYC problem, we train a model to predict whether
    a new listing will have a price above or below the average price. We will compare
    the accuracy of the predictions produced by each approach and the time required
    to run the code for each approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Research*—We will compare the amount of research that argues for and against
    the idea of applying deep learning to tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3.1 Evaluating performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we’ll look at the performance of the XGBoost and Keras versions of the
    Airbnb NYC application. We’ll compare the results we get “out of the box” for
    each approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have trained the XGBoost model, we can get the accuracy of the trained
    model on the test dataset with the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For a given run of the training notebook, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Running the code repeatedly, we get accuracy between 79% and 81% with the original
    parameter settings in the training configuration file ([https://mng.bz/pKOz](https://mng.bz/pKOz)).
    Elapsed time to run the notebook is between 3 and 4 seconds in a vanilla Colab
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Keras model, with the original parameter settings from the training
    configuration file ([https://mng.bz/vKpr](https://mng.bz/vKpr)) running in a vanilla
    Colab environment, the model has the following key performance characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy*—Test accuracy between 80*%* and 81%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elapsed time to run the notebook*—Between 10 and 15 seconds. This is without
    using a GPU on Colab. As an exercise, you can try to run the Keras training notebook
    with and without a GPU on Colab and compare the time it takes to run the notebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we compare the performance of XGBoost and Keras for the Airbnb NYC problem,
    XGBoost comes out ahead in terms of speed of training. This one example doesn’t
    tell the whole story, and we shall see in later chapters that, with some patience
    and tuning, a deep learning solution can rival or, in some cases, exceed the performance
    of XGBoost. The point of this simple performance comparison is to demonstrate
    that XGBoost provides good performance “out of the box” without having to do a
    lot of tweaking and tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Digging Deeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s beyond the scope of this book to provide a detailed survey of all the recent
    research advocating for and against deep learning with tabular data, but in this
    section we will take a closer look at the research and try to get some idea of
    which side of the argument is “winning” the research game.
  prefs: []
  type: TYPE_NORMAL
- en: The article “A Short Chronology of Deep Learning for Tabular Data” ([https://mng.bz/W2x1](https://mng.bz/W2x1))
    is a great summary of recent academic work, and it’s a good place to start a deeper
    look into research about deep learning and tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Following is a list of some current research that supports the use of deep learning
    with tabular data. Compared to the thousands of research papers published on deep
    learning with nontabular data, including text and images, only a tiny number of
    papers have been published on deep learning with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to making an argument for deep learning with tabular data, the following
    papers introduce libraries for deep learning with tabular data. These libraries
    offer convenient ways to apply deep learning to tabular datasets. We will be using
    some of these libraries in chapter 8 when we go through additional examples of
    applying deep learning to tabular data and examine alternatives to the Keras-based
    approach that we used in this chapter for the Airbnb NYC problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive
    Pre-Training ([https://arxiv.org/abs/2106.01342](https://arxiv.org/abs/2106.01342)).
    We will examine this framework in more detail in a later chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TabNet: Attentive Interpretable Tabular Learning ([https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442)).
    This paper introduces another framework for deep learning with tabular data that
    we will explore in more detail in a later chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch Tabular: A Framework for Deep Learning with Tabular Data ([https://arxiv.org/abs/2104.13638](https://arxiv.org/abs/2104.13638)).
    This paper introduces a library based on PyTorch, and it is another library that
    we will revisit in a later chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fastai: A Layered API for Deep Learning ([https://arxiv.org/abs/2002.04688](https://arxiv.org/abs/2002.04688)).
    This paper introduces fastai, a high-level framework built on top of PyTorch.
    This framework includes explicit support for tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Tables ([https://deeptables.readthedocs.io/en/latest/](https://deeptables.readthedocs.io/en/latest/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DANets: Deep Abstract Networks for Tabular Data Classification and Regression
    ([https://arxiv.org/abs/2112.02962](https://arxiv.org/abs/2112.02962)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these papers features code that we can exercise to validate the research
    results and, more importantly, determine the robustness of the libraries. If we
    want to use any of these libraries to solve real-world tabular data problems with
    deep learning, we need to assess whether the libraries are easy to use and work
    with current deep learning frameworks. Figure 3.15 shows the relative popularity
    of some of these libraries based on the number of citations their papers received
    along with the number of stars received by their repos.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F15_Ryan2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 Popularity of libraries for deep learning with tabular data
  prefs: []
  type: TYPE_NORMAL
- en: The popularity of libraries matters. If a library is widely used, it is more
    likely to work in a variety of environments. We cannot take it for granted that
    a library will work in every environment. As we shall see in chapter 8, some libraries
    don’t work in Colab, for example, which means it is difficult to assess them.
    Also, if you use a library that hundreds or thousands of other machine learning
    practitioners are using, you are more likely to find answers to questions and
    resolutions to problems. If you are one of a handful of people using a library,
    you could end up being the first person who has encountered a given problem, and
    you will need to spend your time resolving it rather than simply finding an existing
    resolution on Stack Overflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The critics of deep learning with tabular data have contributed research to
    make their case, including the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data? ([https://arxiv.org/abs/2207.08815](https://arxiv.org/abs/2207.08815))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tabular Data: Deep Learning Is Not All You Need ([https://arxiv.org/abs/2106.03253](https://arxiv.org/abs/2106.03253))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Neural Networks and Tabular Data: A Survey ([https://arxiv.org/abs/2110.01889](https://arxiv.org/abs/2110.01889))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list, and the preceding list of pro deep learning papers, is by no means
    exhaustive. However, it is fair to say that more research is published that advocates
    for deep learning with tabular data than research published to argue against deep
    learning with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Of all the research papers published on deep learning, what proportion deals
    with tabular data? It’s hard to get an exact ratio, but consider figure 3.16,
    which shows the number of papers published on deep learning for the decade and
    a half preceding 2018.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F16_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 Number of published deep learning articles by year. The numbers
    of articles were obtained from the search results on Scopus and Google Scholar
    with the query “deep learning.”
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows that recently tens of thousands of papers are published every
    year on the subject of deep learning. In the last few years, less than 100 papers
    have been published each year on deep learning with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of getting an idea of what proportion of deep learning research deals
    with tabular data is to do some searches on Google Scholar ([https://scholar.google.com/](https://scholar.google.com/)).
    Consider the number of search hits on Google Scholar that match the following
    search criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '“deep learning”: ~1.6 million'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“deep learning” along with “tabular data” or “structured data” and excluding
    “graph-structured”: ~34,500'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“deep neural networks”: ~530,000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“deep neural networks” along with “tabular data” or “structured data” and excluding
    “graph-structured” and “deep learning”: ~1,500'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another way of determining what proportion of deep learning research deals
    with tabular data is to do some searches on arXiv ([https://arxiv.org/](https://arxiv.org/)).
    Consider the number of papers on arXiv that match the following search criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '“deep learning” in the title: ~32,000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“deep learning” along with “tabular data” or “structured data”: ~200'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“deep neural networks” in the title: ~17,500'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“deep neural networks” along with “tabular data” or “structured data”: ~11'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the searches on Google Scholar and arXiv, it’s clear that only a tiny proportion
    of the research published in deep learning deals with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we can conclude the following points about research on deep learning
    with tabular data:'
  prefs: []
  type: TYPE_NORMAL
- en: There are more publications that support deep learning with tabular data than
    there are publications that argue against deep learning with tabular data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several research papers on deep learning with tabular data include libraries
    that implement the approach described in the paper. So far, none of these libraries
    has emerged as a clear favorite for data scientists who are interested in deep
    learning with tabular data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the research done on deep learning, research dealing with deep learning
    and tabular data makes up only a tiny fraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these conclusions, it’s clear that neither machine learning nor deep learning
    is the unambiguous winner in the area of research. In the argument for and against
    deep learning with tabular data, when it comes to research, the jury is still
    out.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve compared machine learning with deep learning across three criteria: simplicity,
    transparency, and efficacy. Figure 3.17 figure summarizes how the two approaches
    compare with each other in each of these three criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH03_F17_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 Summary of the comparison of machine learning and deep learning
    with tabular data
  prefs: []
  type: TYPE_NORMAL
- en: That wraps up our comparison of machine learning and deep learning. In the next
    chapter we will go beyond the simple Airbnb NYC example using XGBoost and dig
    into the details of machine learning with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three characteristics that we can use to compare machine learning
    with deep learning: simplicity, transparency, and efficacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comparative simplicity of the code gives us an idea of which solution will
    be easier to build in the first place and easier to maintain in the long term.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transparency of the solution includes how easy it is to explain the model
    to a nonspecialist audience and how easy it is to assess the relative importance
    of the features used to train the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficacy includes the success of each approach in commercial applications and
    in research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing machine learning and deep learning in terms of code simplicity,
    machine learning comes out ahead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing machine learning and deep learning in terms of transparency,
    machine learning comes out ahead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing machine learning and deep learning in terms of efficacy, the
    two approaches are too close to call “out of the box,” though additional tuning
    could expose more differences between the results of the two approaches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The jury is still out for other measures of success, including success in Kaggle
    competitions, success in business, and research focus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
