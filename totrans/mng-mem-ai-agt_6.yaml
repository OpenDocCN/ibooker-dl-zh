- en: Conclusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As stated throughout this book, you cannot predict the future. We’re just at
    the starting line of the AI era. But if there were a prediction to make, the smart
    money isn’t on tooling integration—it’s on memory.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书中所述，你无法预测未来。我们正处于人工智能时代的起点。但如果必须做出预测，明智的资金不会押注于工具集成——而是押注于记忆。
- en: 'Tools remain relatively static: once you’ve integrated with them, you generally
    retain access. Memory, on the other hand, is dynamic. A project that was a high
    priority one week may become history the next. The best agents will integrate
    this changing context and proactively expand critical memory and expunge extraneous
    information.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 工具保持相对静态：一旦你与它们集成，你通常可以保留访问权限。另一方面，内存是动态的。一个上周还是高优先级的项目，下周可能就成为了历史。最好的代理将整合这种变化的环境，主动扩展关键内存并删除无关信息。
- en: This dynamic nature of memory is why the current moment presents such a unique
    opportunity. Unlike traditional software where expertise accumulates over years,
    there are no true experts in agents yet. The world of AI is simply too fast-paced
    and fluid for one person to be an expert in everything there is to know. This
    levels the playing field—the key to expertise for agents is exposure to these
    systems, to the use of agents, to what they can be used for successfully, and
    to how they can augment, not replace, your work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这种内存的动态特性，使得当前时刻呈现出如此独特的机遇。与传统的软件不同，其中专业知识是逐年积累的，在代理领域还没有真正的专家。人工智能的世界发展得太快，变化太多，一个人不可能成为所有知识的专家。这使竞技场变得公平——代理专业知识的关键是接触这些系统，使用代理，了解它们可以成功用于什么，以及它们如何增强而不是取代你的工作。
- en: For organizations looking to capitalize on this opportunity, experimentation
    is key. For some, that will look like giving users licenses and letting them experiment.
    For other, likely larger organizations, it may be wiser to slowly integrate agents
    together in seminars and hackathons. It’s an equal playing field, and the only
    way to truly learn is by doing with agents. Through this collective exploration,
    teams naturally develop their own TMSs, learning not just what agents can do but
    also who knows what about using them effectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望利用这一机会的组织来说，实验至关重要。对于一些人来说，这可能看起来像是向用户提供许可证并让他们进行实验。对于其他可能更大的组织，可能更明智的做法是在研讨会和黑客马拉松中缓慢地将代理集成在一起。这是一个公平的竞技场，唯一真正学习的方法是通过与代理一起实践。通过这种集体探索，团队自然会发展出自己的TMS（任务管理系统），不仅学习代理能做什么，还学习谁知道如何有效地使用它们。
- en: As these experiments unfold, organizations will quickly discover that memory
    management is the critical challenge. True, memory will always be finite—because
    memory is data, and data always needs to be stored somewhere. We also know that,
    unless the attention mechanism that underlies LLMs fundamentally changes, the
    search space for LLMs will remain quadratic. That means that retrieval will be
    limited into the known future as well.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些实验的展开，组织将很快发现内存管理是关键挑战。确实，内存总是有限的——因为内存是数据，而数据总是需要存储在某处。我们还知道，除非LLM（大型语言模型）背后的注意力机制发生根本性的变化，否则LLM的搜索空间将保持二次方。这意味着检索将在可预见的未来也受到限制。
- en: 'But it’s not about having infinite memory—it’s about *how* we retain and access
    memory. The algorithms we use, how we route between them based on context—these
    are the real differentiators. We can be confident that caching will play a growing
    role in agentic memory management. After all, think about search engines: when
    a question is asked—such as “What time are the playoffs today?”—it’s very likely
    not the only instance of that question being asked. Smart search engines cache
    these queries to make them more readily available—so too will agents of the future.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是关于拥有无限的内存——而是关于我们如何保留和访问内存。我们使用的算法，以及我们如何根据上下文在这些算法之间进行路由——这些才是真正的区别。我们可以有信心，缓存将在代理式内存管理中扮演越来越重要的角色。毕竟，想想搜索引擎：当一个问题时——比如“今天季后赛是什么时间？”——它很可能不是唯一一次被提出的问题。智能搜索引擎缓存这些查询以使它们更容易获取——未来的代理也将如此。
- en: The same principle applies to queries on knowledge bases within an organization.
    As we explored in [Chapter 2](ch02.html#ch02_long_term_memory_building_persistent_learning_age_1758256567757093),
    semantic caching retains the relative context of an information retrieval system
    over time by processing the semantics of the content being passed. Information
    that is retrieved frequently gets prioritized. For systems like internal LLMs
    or RAG where many users talk to the same corpus of information, it’s both more
    computationally effective and cost effective to semantically cache that information.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的原则适用于组织内部的知识库查询。正如我们在[第二章](ch02.html#ch02_long_term_memory_building_persistent_learning_age_1758256567757093)中探讨的那样，语义缓存通过处理传递内容的意义，在时间上保留信息检索系统的相对上下文。频繁检索的信息得到优先处理。对于像内部LLM或RAG这样的系统，其中许多用户与相同的信息库进行交流，在语义上缓存这些信息在计算上更有效，成本也更低。
- en: This is where the real innovation is happening. Researchers aren’t just fighting
    the quadratic-attention problem with bigger context windows; they’re getting smarter
    about importance scoring—calculating memory importance based on recency, frequency
    of reference, user-engagement metrics, and keyword relevance. They’re building
    cascading memory systems that allow the agent itself to choose what to promote
    to long-term storage. This adaptive approach—knowing what to keep, what to compress,
    and what to let go—is the key to scalable, dynamic memory systems moving forward.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是真正的创新正在发生的地方。研究人员不仅在与更大的上下文窗口的二次注意力问题作斗争，他们还在重要性评分上变得更聪明——根据近期性、引用频率、用户参与度指标和关键词相关性来计算记忆的重要性。他们正在构建级联记忆系统，使代理本身能够选择将什么内容提升到长期存储。这种自适应方法——知道要保留什么，要压缩什么，要放弃什么——是未来可扩展、动态记忆系统的关键。
- en: The organizations that get memory right, that prioritize shared memory systems,
    that understand these trade-offs of memory retention and retrieval will be the
    organizations that have an edge in productivity. After all, if memory equals data,
    then those with the best systems for managing that data will understand not just
    what is, but what can be.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在记忆管理上做得正确、优先考虑共享记忆系统、理解记忆保留和检索权衡的组织将在生产力上具有优势。毕竟，如果记忆等于数据，那么那些拥有最佳数据管理系统的人不仅会理解现状，还会理解可能的情况。
- en: 'Yet in all this discussion of systems and algorithms, we must remember a crucial
    truth: the most important agent will always be the human agent. We are the conductors
    guiding the machine. Like an orchestra playing Beethoven’s Fifth, the music may
    be known, the notes memorized, but it’s the conductor who knows exactly what they
    want from this rendition—the conductor who has done the research, who has an overarching
    vision. In the same way, even if agents begin to write code for us, do our research,
    communicate to others for us—it will be us, the humans, who will decide on what
    we want, who will guide the agent not only on what success is but also what *we*
    want success to be.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在所有关于系统和算法的讨论中，我们必须记住一个关键真理：最重要的代理始终是人类代理。我们是引导机器的指挥者。就像一个演奏贝多芬第五交响乐的乐团，音乐可能已知，音符可能已记牢，但知道从这次演绎中想要什么的是指挥——他进行了研究，有一个全面的愿景。同样，即使代理开始为我们编写代码，为我们进行研究，为我们与他人沟通——决定我们想要什么，指导代理不仅要知道什么是成功，还要知道我们想要的成功是什么的，将是我们人类。
- en: While we cannot know what the future will bring, we can anticipate that the
    users of agents will play a bigger role in personalizing how they “conduct”—instructing
    them in just the right ways to retain important information. The future isn’t
    about perfect recall or infinite memory. It’s about designing systems that can
    manage information intelligently, guided by human judgment and values. With guidance
    from classic software engineering principles and research into how to better leverage
    our new stochastic systems, we’re only just getting started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们无法预知未来会带来什么，但我们预期代理的使用者将在如何“执行”个性化方面扮演更重要的角色——指导他们以恰当的方式保留重要信息。未来不在于完美的回忆或无限的内存。它在于设计能够智能管理信息、受人类判断和价值观引导的系统。在经典软件工程原则的指导下，以及研究如何更好地利用我们新的随机系统，我们才刚刚开始。
- en: About the Authors
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '**Benjamin Labaschin** is currently a principal machine learning engineer and
    founding hire at Workhelix. Prior to this role, he worked as a senior data scientist
    at Hopper, XPO Logistics, and Blackstone. He holds a background in economics and
    environmental science. His areas of expertise include machine learning, MLOps,
    DevOps, and backend engineering. Ben has a particular passion for scaling engineering
    systems to support the evolving needs of hyper-growth startups and for advancing
    sustainable engineering practices.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**本杰明·拉巴申**目前是Workhelix的首席机器学习工程师和创始团队成员。在此职位之前，他在Hopper、XPO Logistics和Blackstone担任高级数据科学家。他的背景是经济学和环境科学。他的专业领域包括机器学习、MLOps、DevOps和后端工程。本杰明特别热衷于扩展工程系统以支持高速增长初创企业的不断变化的需求，并推动可持续的工程实践。'
- en: '**Jim Allen Wallace** is group manager of product marketing at Redis, where
    he bridges technical know-how and market insights to help developers apply Redis
    in the age of AI. He’s worn hats as an engineer, salesperson, and entrepreneur,
    giving him a front-row seat to how products get built, sold, and loved (or ignored).
    When he’s not shaping go-to-market strategy, Jim can be found hiking the mountains
    outside Denver, Colorado, or maybe pushing his cat Chutney off his keyboard.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**吉姆·艾伦·华莱士**是Redis的产品市场营销团队经理，他连接技术知识和市场洞察力，帮助开发者将Redis应用于人工智能时代。他曾担任工程师、销售人员和企业家，这使他有机会近距离观察产品的构建、销售和受欢迎程度（或被忽视）。当吉姆不在塑造市场推广策略时，他可以在科罗拉多州丹佛的山上徒步旅行，或者可能把他的猫Chutney从键盘上推开。'
- en: '**Andrew Brookins** is a principal applied AI engineer at Redis, where he builds
    open source infrastructure for memory, retrieval, and intelligent agents. He created
    the Agent Memory Server and Redis OM Python, and contributes to Redis libraries
    and applied AI tooling. With a background in backend systems and engineering leadership,
    he focuses on tools that help developers build smarter, more reliable systems.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**安德鲁·布鲁金斯**是Redis的首席应用人工智能工程师，在那里他构建了用于内存、检索和智能代理的开源基础设施。他创建了代理内存服务器和Redis
    OM Python，并为Redis库和应用人工智能工具做出贡献。他拥有后端系统和工程领导背景，专注于帮助开发者构建更智能、更可靠的系统的工具。'
- en: '**Manvinder Singh** is VP of AI products at Redis, where he is responsible
    for the portfolio of AI offerings, including vector search, semantic caching,
    and agent memory. Previously, he spent over 10 years in various AI and cloud roles
    at Google. As a director of product management for AI ecosystem at Google, he
    was responsible for orchestrating and driving product efforts to engage with and
    build with the ecosystem across their AI portfolio, including the Gemma family
    of models.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**曼文德·辛格**是Redis的人工智能产品副总裁，负责人工智能产品组合，包括向量搜索、语义缓存和代理内存。此前，他在谷歌担任了超过10年的各种人工智能和云角色。作为谷歌人工智能生态系统产品管理总监，他负责协调和推动产品努力，以参与和构建其人工智能产品组合中的生态系统，包括Gemma系列模型。'
