- en: Chapter 2\. Understanding Foundation Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 理解基础模型
- en: To build applications with foundation models, you first need foundation models.
    While you don’t need to know how to develop a model to use it, a high-level understanding
    will help you decide what model to use and how to adapt it to your needs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用基础模型构建应用程序，首先你需要基础模型。虽然你不需要知道如何开发模型就能使用它，但高层次的理解将帮助你决定使用哪种模型以及如何适应你的需求。
- en: Training a foundation model is an incredibly complex and costly process. Those
    who know how to do this well are likely prevented by confidentiality agreements
    from disclosing the secret sauce. This chapter won’t be able to tell you how to
    build a model to compete with ChatGPT. Instead, I’ll focus on design decisions
    with consequential impact on downstream applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基础模型是一个极其复杂且成本高昂的过程。那些做得很好的人可能因为保密协议而无法透露秘诀。本章无法告诉你如何构建一个能与ChatGPT竞争的模型。相反，我将专注于那些对下游应用有重大影响的设计决策。
- en: With the growing lack of transparency in the training process of foundation
    models, it’s difficult to know all the design decisions that go into making a
    model. In general, however, differences in foundation models can be traced back
    to decisions about training data, model architecture and size, and how they are
    post-trained to align with human preferences.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基础模型训练过程的透明度日益降低，很难知道所有用于构建模型的设计决策。然而，一般来说，基础模型之间的差异可以追溯到关于训练数据、模型架构和大小以及它们如何进行后训练以符合人类偏好的决策。
- en: Since models learn from data, their training data reveals a great deal about
    their capabilities and limitations. This chapter begins with how model developers
    curate training data, focusing on the distribution of training data. [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888)
    explores dataset engineering techniques in detail, including data quality evaluation
    and data synthesis.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型从数据中学习，它们的训练数据揭示了它们的能力和局限性的很多信息。本章从模型开发者如何整理训练数据开始，重点关注训练数据的分布。[第8章](ch08.html#ch08_dataset_engineering_1730130932019888)将详细探讨数据集工程技术，包括数据质量评估和数据合成。
- en: Given the dominance of the transformer architecture, it might seem that model
    architecture is less of a choice. You might be wondering, what makes the transformer
    architecture so special that it continues to dominate? How long until another
    architecture takes over, and what might this new architecture look like? This
    chapter will address all of these questions. Whenever a new model is released,
    one of the first things people want to know is its size. This chapter will also
    explore how a model developer might determine the appropriate size for their model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变换器架构的统治地位，可能看起来模型架构的选择不那么重要。你可能想知道，是什么让变换器架构如此特别，以至于它继续占据主导地位？多久之后另一种架构会接管，这种新的架构可能是什么样子？本章将解答所有这些问题。每当发布一个新的模型时，人们首先想知道的就是其大小。本章还将探讨模型开发者如何确定他们模型适当的大小。
- en: As mentioned in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    a model’s training process is often divided into pre-training and post-training.
    Pre-training makes a model capable, but not necessarily safe or easy to use. This
    is where post-training comes in. The goal of post-training is to align the model
    with human preferences. But what exactly is *human preference*? How can it be
    represented in a way that a model can learn? The way a model developer aligns
    their model has a significant impact on the model’s usability, and will be discussed
    in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)中所述，模型的训练过程通常分为预训练和后训练。预训练使模型具备能力，但并不一定安全或易于使用。这就是后训练的用武之地。后训练的目标是将模型与人类偏好对齐。但“人类偏好”究竟是什么？如何以模型可以学习的方式表示它？模型开发者如何对齐他们的模型对模型的可用性有重大影响，这将在本章中讨论。
- en: While most people understand the impact of training on a model’s performance,
    the impact of *sampling* is often overlooked. Sampling is how a model chooses
    an output from all possible options. It is perhaps one of the most underrated
    concepts in AI. Not only does sampling explain many seemingly baffling AI behaviors,
    including hallucinations and inconsistencies, but choosing the right sampling
    strategy can also significantly boost a model’s performance with relatively little
    effort. For this reason, sampling is the section that I was the most excited to
    write about in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数人理解训练对模型性能的影响，但采样的影响往往被忽视。采样是模型从所有可能选项中选择输出的方式。这可能是AI中最被低估的概念之一。采样不仅解释了许多看似令人困惑的AI行为，包括幻觉和不一致性，而且选择正确的采样策略也可以在相对较少的努力下显著提高模型的表现。因此，采样是我在本章中最兴奋要写的部分。
- en: Concepts covered in this chapter are fundamental for understanding the rest
    of the book. However, because these concepts are fundamental, you might already
    be familiar with them. Feel free free to skip any concept that you’re confident
    about. If you encounter a confusing concept later on, you can revisit this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的概念对于理解本书的其余部分是基本的。然而，因为这些概念是基本的，你可能已经熟悉它们。如果你对某个概念有信心，可以自由地跳过它。如果你在以后遇到难以理解的概念，可以回过头来阅读本章。
- en: Training Data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据
- en: An AI model is only as good as the data it was trained on. If there’s no Vietnamese
    in the training data, the model won’t be able to translate from English into Vietnamese.
    Similarly, if an image classification model sees only animals in its training
    set, it won’t perform well on photos of plants.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI模型的好坏取决于其训练数据。如果训练数据中没有越南语，该模型将无法将英语翻译成越南语。同样，如果一个图像分类模型在其训练集中只看到动物，它将无法在植物的照片上表现良好。
- en: If you want a model to improve on a certain task, you might want to include
    more data for that task in the training data. However, collecting sufficient data
    for training a large model isn’t easy, and it can be expensive. Model developers
    often have to rely on available data, even if this data doesn’t exactly meet their
    needs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望模型在某个特定任务上有所改进，你可能需要在训练数据中包含更多该任务的数据。然而，为训练大型模型收集足够的数据并不容易，而且可能很昂贵。模型开发者通常不得不依赖于可用的数据，即使这些数据并不完全符合他们的需求。
- en: For example, a common source for training data is [Common Crawl](https://oreil.ly/wf2Lw),
    created by a nonprofit organization that sporadically crawls websites on the internet.
    In 2022 and 2023, this organization crawled approximately 2–3 billion web pages
    each month. Google provides a clean subset of Common Crawl called the [Colossal
    Clean Crawled Corpus](https://arxiv.org/abs/1910.10683v4), or C4 for short.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，训练数据的一个常见来源是[Common Crawl](https://oreil.ly/wf2Lw)，这是一个非营利组织创建的，该组织偶尔会爬取互联网上的网站。在2022年和2023年，这个组织每个月爬取了大约20-30亿个网页。谷歌提供了一个名为[Colossal
    Clean Crawled Corpus](https://arxiv.org/abs/1910.10683v4)，简称C4的干净子集。
- en: The data quality of Common Crawl, and C4 to a certain extent, is questionable—think
    clickbait, misinformation, propaganda, conspiracy theories, racism, misogyny,
    and every sketchy website you’ve ever seen or avoided on the internet. A [study
    by the *Washington Post*](https://oreil.ly/-1UMD) shows that the 1,000 most common
    websites in the dataset include several media outlets that rank low on [NewsGuard’s
    scale for trustworthiness](https://oreil.ly/OisOs). In lay terms, Common Crawl
    contains plenty of fake news.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl的数据质量，以及C4在一定程度上，是可疑的——想想那些点击诱饵、错误信息、宣传、阴谋论、种族主义、厌女症，以及你在互联网上看到或避免的每一个可疑网站。一项由[华盛顿邮报](https://oreil.ly/-1UMD)进行的[研究](https://oreil.ly/OisOs)显示，数据集中最常见的1000个网站包括几个在[NewsGuard的可靠性尺度](https://oreil.ly/OisOs)上排名较低的媒体机构。用通俗易懂的话说，Common
    Crawl包含大量假新闻。
- en: Yet, simply because Common Crawl is available, variations of it are used in
    most foundation models that disclose their training data sources, including OpenAI’s
    GPT-3 and Google’s Gemini. I suspect that Common Crawl is also used in models
    that don’t disclose their training data. To avoid scrutiny from both the public
    and competitors, many companies have stopped disclosing this information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅因为Common Crawl可用，它的变体被用于大多数公开其训练数据来源的基础模型中，包括OpenAI的GPT-3和谷歌的Gemini。我怀疑Common
    Crawl也被用于不公开其训练数据来源的模型中。为了避免公众和竞争对手的审查，许多公司已经停止公开这些信息。
- en: Some teams use heuristics to filter out low-quality data from the internet.
    For example, OpenAI used only the Reddit links that received at least three upvotes
    to train [GPT-2](https://oreil.ly/gGwRz). While this does help screen out links
    that nobody cares about, Reddit isn’t exactly the pinnacle of propriety and good
    taste.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队使用启发式方法从互联网上过滤掉低质量数据。例如，OpenAI只使用了至少获得三个点赞的Reddit链接来训练[GPT-2](https://oreil.ly/gGwRz)。虽然这确实有助于筛选出无人关注的链接，但Reddit并不完全是正派和好品味的顶峰。
- en: The “use what we have, not what we want” approach may lead to models that perform
    well on tasks present in the training data but not necessarily on the tasks you
    care about. To address this issue, it’s crucial to curate datasets that align
    with your specific needs. This section focuses on curating data for specific *languages*
    and *domains*, providing a broad yet specialized foundation for applications within
    those areas. [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888)
    explores data strategies for models tailored to highly specific tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “使用我们拥有的，而不是我们想要的”的方法可能会导致模型在训练数据中存在的任务上表现良好，但并不一定是在你关心的任务上。为了解决这个问题，至关重要的是要整理与你的特定需求相一致的数据集。本节重点介绍为特定*语言*和*领域*整理数据，为这些领域中的应用提供广泛而专业的基石。[第8章](ch08.html#ch08_dataset_engineering_1730130932019888)探讨了针对高度特定任务的模型的策略。
- en: While language- and domain-specific foundation models can be trained from scratch,
    it’s also common to finetune them on top of general-purpose models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以从头开始训练语言和领域特定的基础模型，但通常也常见的是在通用模型之上微调它们。
- en: Some might wonder, why not just train a model on all data available, both general
    data and specialized data, so that the model can do everything? This is what many
    people do. However, training on more data often requires more compute resources
    and doesn’t always lead to better performance. For example, a model trained with
    a smaller amount of high-quality data might outperform a model trained with a
    large amount of low-quality data. Using 7B tokens of high-quality coding data,
    [Gunasekar et al. (2023)](https://arxiv.org/abs/2306.11644) were able to train
    a 1.3B-parameter model that outperforms much larger models on several important
    coding benchmarks. The impact of data quality is discussed more in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会想，为什么不直接在所有可用的数据上训练模型，包括通用数据和专用数据，这样模型就能做所有事情？这是许多人所做的事情。然而，在更多数据上训练通常需要更多的计算资源，并且并不总是导致更好的性能。例如，使用7B个高质量编码数据，[Gunasekar等人（2023）](https://arxiv.org/abs/2306.11644)能够训练一个13B参数的模型，在几个重要的编码基准测试中优于许多更大的模型。数据质量的影响在[第8章](ch08.html#ch08_dataset_engineering_1730130932019888)中讨论得更多。
- en: Multilingual Models
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多语言模型
- en: English dominates the internet. An analysis of the Common Crawl dataset shows
    that English accounts for almost half of the data (45.88%), making it eight times
    more prevalent than the second-most common language, Russian (5.97%) ([Lai et
    al., 2023](https://arxiv.org/abs/2304.05613)). See [Table 2-1](#ch02_table_1_1730147895537533)
    for a list of languages with at least 1% in Common Crawl. Languages with limited
    availability as training data—typically languages not included in this list—are
    considered *low-resource*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 英语在互联网上占据主导地位。对Common Crawl数据集的分析显示，英语占数据量近一半（45.88%），使其比第二常见语言俄语（5.97%）普及八倍多（[Lai等人，2023](https://arxiv.org/abs/2304.05613)）。见[表2-1](#ch02_table_1_1730147895537533)了解在Common
    Crawl中至少占1%的语言列表。那些作为训练数据可用性有限的语种——通常不包括在这个列表中的语种——被认为是*低资源*语种。
- en: 'Table 2-1\. The most common languages in Common Crawl, a popular dataset for
    training LLMs. Source: Lai et al. (2023).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. Common Crawl中最常见的语言，这是一个流行的用于训练LLMs的数据集。来源：Lai等人（2023）。
- en: '| Language | Code | Pop. | CC size |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | Code | Pop. | CC size |'
- en: '| --- | --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|   |   | (M) | (%) | Cat. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|   |   | (M) | (%) | Cat. |'
- en: '| English | en | 1,452 | 45.8786 | H |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 英语 | en | 1,452 | 45.8786 | H |'
- en: '| Russian | ru | 258 | 5.9692 | H |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 俄语 | ru | 258 | 5.9692 | H |'
- en: '| German | de | 134 | 5.8811 | H |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 德语 | de | 134 | 5.8811 | H |'
- en: '| Chinese | zh | 1,118 | 4.8747 | H |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 中文 | zh | 1,118 | 4.8747 | H |'
- en: '| Japanese | jp | 125 | 4.7884 | H |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 日语 | jp | 125 | 4.7884 | H |'
- en: '| French | fr | 274 | 4.7254 | H |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 法语 | fr | 274 | 4.7254 | H |'
- en: '| Spanish | es | 548 | 4.4690 | H |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙语 | es | 548 | 4.4690 | H |'
- en: '| Italian | it | 68 | 2.5712 | H |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 意大利语 | it | 68 | 2.5712 | H |'
- en: '| Dutch | nl | 30 | 2.0585 | H |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 荷兰语 | nl | 30 | 2.0585 | H |'
- en: '| Polish | pl | 45 | 1.6636 | H |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 波兰语 | pl | 45 | 1.6636 | H |'
- en: '| Portuguese | pt | 257 | 1.1505 | H |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 葡萄牙语 | pt | 257 | 1.1505 | H |'
- en: '| Vietnamese | vi | 85 | 1.0299 | H |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 越南语 | vi | 85 | 1.0299 | H |'
- en: Many other languages, despite having a lot of speakers today, are severely under-represented
    in Common Crawl. [Table 2-2](#ch02_table_2_1730147895537547) shows some of these
    languages. Ideally, the ratio between world population representation and Common
    Crawl representation should be 1\. The higher this ratio, the more under-represented
    this language is in Common Crawl.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天有很多讲这些语言的人，但许多其他语言在Common Crawl中的代表性严重不足。[表2-2](#ch02_table_2_1730147895537547)显示了其中的一些语言。理想情况下，世界人口代表性和Common
    Crawl代表性之间的比率应该是1。这个比率越高，这种语言在Common Crawl中的代表性就越低。
- en: Table 2-2\. Examples of under-represented languages in Common Crawl. The last
    row, English, is for comparison. The numbers for % in Common Crawl are taken from
    Lai et al. (2023).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2。Common Crawl中代表性不足的语言示例。最后一行，英语，用于比较。Common Crawl中的百分比数字来自Lai等人（2023）。
- en: '| Language | Speakers (million) | % world population^([a](ch02.html#id696))
    | % in Common Crawl | World: Common Crawl Ratio |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 说话者（百万） | %世界人口^([a](ch02.html#id696)) | %在Common Crawl | 世界：Common
    Crawl比率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Punjabi | 113 | 1.41% | 0.0061% | 231.56 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 旁遮普语 | 113 | 1.41% | 0.0061% | 231.56 |'
- en: '| Swahili | 71 | 0.89% | 0.0077% | 115.26 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 斯瓦希里语 | 71 | 0.89% | 0.0077% | 115.26 |'
- en: '| Urdu | 231 | 2.89% | 0.0274% | 105.38 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 乌尔都语 | 231 | 2.89% | 0.0274% | 105.38 |'
- en: '| Kannada | 64 | 0.80% | 0.0122% | 65.57 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 卡纳达语 | 64 | 0.80% | 0.0122% | 65.57 |'
- en: '| Telugu | 95 | 1.19% | 0.0183% | 64.89 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 泰卢固语 | 95 | 1.19% | 0.0183% | 64.89 |'
- en: '| Gujarati | 62 | 0.78% | 0.0126% | 61.51 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 古吉拉特语 | 62 | 0.78% | 0.0126% | 61.51 |'
- en: '| Marathi | 99 | 1.24% | 0.0213% | 58.10 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 马拉地语 | 99 | 1.24% | 0.0213% | 58.10 |'
- en: '| Bengali | 272 | 3.40% | 0.0930% | 36.56 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 孟加拉语 | 272 | 3.40% | 0.0930% | 36.56 |'
- en: '| **English** | **1452** | **18.15%** | **45.88%** | **0.40** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **英语** | **1452** | **18.15%** | **45.88%** | **0.40** |'
- en: '| ^([a](ch02.html#id696-marker)) A world population of eight billion was used
    for this calculation. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch02.html#id696-marker)) 在此计算中使用了80亿的世界人口。 |'
- en: Given the dominance of English in the internet data, it’s not surprising that
    general-purpose models work much better for English than other languages, according
    to multiple studies. For example, on the MMLU benchmark, a suite of 14,000 multiple-choice
    problems spanning 57 subjects, [GPT-4 performed much better in English](https://oreil.ly/qK2Ap)
    than under-represented languages like Telugu, as shown in [Figure 2-1](#ch02_figure_1_1730147895520810)
    (OpenAI, 2023).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于英语在互联网数据中的主导地位，根据多项研究，通用模型在英语上的表现比其他语言好，这并不令人惊讶。例如，在包含57个主题的14,000个多项选择题的MMLU基准测试中，[GPT-4在英语中的表现优于代表性不足的语言，如泰卢固语](https://oreil.ly/qK2Ap)，[图2-1](#ch02_figure_1_1730147895520810)显示了这一点（OpenAI，2023）。
- en: '![A graph with green and blue bars  Description automatically generated](assets/aien_0201.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![一张带有绿色和蓝色柱状图的图表  自动生成的描述](assets/aien_0201.png)'
- en: Figure 2-1\. On the MMLU benchmark, GPT-4 performs better in English than in
    any other language. To obtain MMLU in other languages, OpenAI translated the questions
    using Azure AI Translator.
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。在MMLU基准测试中，GPT-4在英语中的表现优于任何其他语言。为了在其他语言中获得MMLU，OpenAI使用Azure AI翻译器翻译了这些问题。
- en: Similarly, when tested on six math problems on Project Euler, Yennie Jun found
    that GPT-4 was able to solve problems in English more than three times as often
    compared to Armenian or Farsi.^([1](ch02.html#id697)) GPT-4 failed in all six
    questions for Burmese and Amharic, as shown in [Figure 2-2](#ch02_figure_2_1730147895520828).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当在Project Euler的六个数学问题上进行测试时，Yennie Jun发现，与亚美尼亚语或波斯语相比，GPT-4在英语中解决问题的频率超过三倍。[图2-2](#ch02_figure_2_1730147895520828)显示了这一点。
- en: '![A graph with numbers and a number of pass rate  Description automatically
    generated](assets/aien_0202.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![一张带有数字和通过率的图表  自动生成的描述](assets/aien_0202.png)'
- en: Figure 2-2\. GPT-4 is much better at math in English than in other languages.
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。GPT-4在英语中的数学能力远胜于其他语言。
- en: Under-representation is a big reason for this underperformance. The three languages
    that have the worst performance on GPT-4’s MMLU benchmarks—Telugu, Marathi, and
    Punjabi—are also among the languages that are most under-represented in Common
    Crawl. However, under-representation isn’t the only reason. A language’s structure
    and the culture it embodies can also make a language harder for a model to learn.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性不足是这种表现不佳的主要原因。在GPT-4的MMLU基准测试中表现最差的三个语言——泰卢固语、马拉地语和旁遮普语——也是Common Crawl中代表性最不足的语言之一。然而，代表性不足并不是唯一的原因。一种语言的结构和它所体现的文化也可能使模型更难学习这种语言。
- en: Given that LLMs are generally good at translation, can we just translate all
    queries from other languages into English, obtain the responses, and translate
    them back into the original language? Many people indeed follow this approach,
    but it’s not ideal. First, this requires a model that can sufficiently understand
    under-represented languages to translate. Second, translation can cause information
    loss. For example, some languages, like Vietnamese, have pronouns to denote the
    relationship between the two speakers. When translating into English, all these
    pronouns are translated into *I* and *you*, causing the loss of the relationship
    information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到LLMs通常擅长翻译，我们是否可以将其他语言的查询全部翻译成英语，获取响应，然后再翻译回原始语言？许多人确实采取了这种方法，但这并不理想。首先，这需要一个能够充分理解代表性不足的语言以进行翻译的模型。其次，翻译可能会导致信息丢失。例如，一些语言，如越南语，有代词来表示说话者之间的关系。当翻译成英语时，所有这些代词都被翻译成*I*和*you*，导致关系信息的丢失。
- en: Models can also have unexpected performance challenges in non-English languages.
    For example, [NewsGuard](https://oreil.ly/LcBfx) found that ChatGPT is more willing
    to produce misinformation in Chinese than in English. In April 2023, NewsGuard
    asked ChatGPT-3.5 to produce misinformation articles about China in English, simplified
    Chinese, and traditional Chinese. For English, ChatGPT declined to produce false
    claims for six out of seven prompts. However, it produced false claims in simplified
    Chinese and traditional Chinese all seven times. It’s unclear what causes this
    difference in behavior.^([2](ch02.html#id699))
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在非英语语言中也可能遇到意外的性能挑战。例如，[NewsGuard](https://oreil.ly/LcBfx)发现，ChatGPT在中文中比在英语中更愿意产生虚假信息。2023年4月，NewsGuard要求ChatGPT-3.5用英语、简体中文和繁体中文撰写关于中国的虚假信息文章。对于英语，ChatGPT在七个提示中有六个拒绝产生虚假陈述。然而，它在简体中文和繁体中文中七次都产生了虚假陈述。这种行为差异的原因尚不清楚.^([2](ch02.html#id699))
- en: Other than quality issues, models can also be slower and more expensive for
    non-English languages. A model’s inference latency and cost is proportional to
    the number of tokens in the input and response. It turns out that tokenization
    can be much more efficient for some languages than others. Benchmarking GPT-4
    on MASSIVE, a dataset of one million short texts translated across 52 languages,
    Yennie Jun found that, to convey the same meaning, languages like Burmese and
    Hindi require [a lot more tokens](https://oreil.ly/Zq5Sw) than English or Spanish.
    For the MASSIVE dataset, the median token length in English is 7, but the median
    length in Hindi is 32, and in Burmese, it’s a whopping 72, which is ten times
    longer than in English.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了质量问题，对于非英语语言，模型可能运行得更慢且成本更高。模型推理的延迟和成本与输入和响应中的标记数量成正比。结果发现，对于某些语言来说，标记化可能比其他语言更高效。在MASSIVE数据集上对GPT-4进行基准测试，这是一个包含52种语言翻译的百万条短文本的数据集，Yennie
    Jun发现，为了传达相同的意义，像缅甸语和印地语这样的语言比英语或西班牙语需要[更多的标记](https://oreil.ly/Zq5Sw)。对于MASSIVE数据集，英语的标记中位数长度为7，但印地语的长度中位数为32，在缅甸语中，它高达72，是英语的十倍。
- en: Assuming that the time it takes to generate a token is the same in all languages,
    GPT-4 takes approximately ten times longer in Burmese than in English for the
    same content. For APIs that charge by token usage, Burmese costs ten times more
    than English.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设生成标记所需的时间在所有语言中都是相同的，GPT-4在缅甸语中生成相同内容所需的时间大约是英语的十倍。对于按标记使用量收费的API，缅甸语的成本是英语的十倍。
- en: To address this, many models have been trained to focus on non-English languages.
    The most active language, other than English, is undoubtedly Chinese, with [ChatGLM](https://github.com/THUDM/ChatGLM2-6B),
    [YAYI](https://github.com/wenge-research/YAYI), [Llama-Chinese](https://github.com/LlamaFamily/Llama-Chinese),
    and others. There are also models in French ([CroissantLLM](https://oreil.ly/a6j-N)),
    Vietnamese ([PhoGPT](https://github.com/VinAIResearch/PhoGPT)), Arabic ([Jais](https://oreil.ly/uG27L)),
    and many more languages.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，许多模型被训练以专注于非英语语言。除了英语之外，最活跃的语言无疑是中文，包括[ChatGLM](https://github.com/THUDM/ChatGLM2-6B)、[YAYI](https://github.com/wenge-research/YAYI)、[Llama-Chinese](https://github.com/LlamaFamily/Llama-Chinese)等。还有法语([CroissantLLM](https://oreil.ly/a6j-N))、越南语([PhoGPT](https://github.com/VinAIResearch/PhoGPT))、阿拉伯语([Jais](https://oreil.ly/uG27L))以及更多语言中的模型。
- en: Domain-Specific Models
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专用模型
- en: General-purpose models like [Gemini](https://oreil.ly/4XsOV), [GPTs](https://oreil.ly/KLVgX),
    and [Llamas](https://oreil.ly/58gxQ) can perform incredibly well on a wide range
    of domains, including but not limited to coding, law, science, business, sports,
    and environmental science. This is largely thanks to the inclusion of these domains
    in their training data. [Figure 2-3](#ch02_figure_3_1730147895520839) shows the
    distribution of domains present in Common Crawl according to the *Washington Post*’s
    2023 analysis.^([3](ch02.html#id705))
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 [Gemini](https://oreil.ly/4XsOV)、[GPTs](https://oreil.ly/KLVgX) 和 [Llamas](https://oreil.ly/58gxQ)
    这样的通用模型，在包括但不限于编码、法律、科学、商业、体育和环境科学等广泛领域上可以表现出色。这主要归功于它们训练数据中包含了这些领域。[图 2-3](#ch02_figure_3_1730147895520839)
    展示了根据 *华盛顿邮报* 2023 年的分析，Common Crawl 中存在的域的分布.^([3](ch02.html#id705))
- en: '![](assets/aien_0203.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aien_0203.png)'
- en: Figure 2-3\. Distribution of domains in the C4 dataset. Reproduced from the
    statistics from the *Washington Post*. One caveat of this analysis is that it
    only shows the categories that are included, not the categories missing.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. C4 数据集中域的分布。数据来自 *华盛顿邮报* 的统计。需要注意的是，这项分析仅显示了包含的类别，而没有显示缺失的类别。
- en: As of this writing, there haven’t been many analyses of domain distribution
    in vision data. This might be because images are harder to categorize than texts.^([4](ch02.html#id706))
    However, you can infer a model’s domains from its benchmark performance. [Table 2-3](#ch02_table_3_1730147895537555)
    shows how two models, CLIP and Open CLIP, [perform on different benchmarks](https://oreil.ly/MTqyR).
    These benchmarks show how well these two models do on birds, flowers, cars, and
    a few more categories, but the world is so much bigger and more complex than these
    few categories.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，对视觉数据中域分布的分析还不多。这可能是因为图像的分类比文本更困难.^([4](ch02.html#id706)) 然而，你可以从模型在基准测试中的表现推断其领域。[表
    2-3](#ch02_table_3_1730147895537555) 展示了两个模型，CLIP 和 Open CLIP，[在不同基准上的表现](https://oreil.ly/MTqyR)。这些基准显示了这两个模型在鸟类、花卉、汽车和其他几个类别上的表现，但世界远比这些类别要大得多、复杂得多。
- en: Table 2-3\. Open CLIP and CLIP’s performance on different image datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-3\. Open CLIP 和 CLIP 在不同图像数据集上的性能。
- en: '| Dataset | CLIP Accuracy of ViT-B/32 (OpenAI) | Open CLIP Accuracy of ViT-B/32
    (Cade) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | CLIP ViT-B/32 (OpenAI) 准确率 | Open CLIP ViT-B/32 (Cade) 准确率 |'
- en: '| --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ImageNet | 63.2 | 62.9 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | 63.2 | 62.9 |'
- en: '| ImageNet v2 | – | 62.6 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet v2 | – | 62.6 |'
- en: '| Birdsnap | 37.8 | 46.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Birdsnap | 37.8 | 46.0 |'
- en: '| Country211 | 17.8 | 14.8 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Country211 | 17.8 | 14.8 |'
- en: '| Oxford 102 Category Flower | 66.7 | 66.0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Oxford 102 Category Flower | 66.7 | 66.0 |'
- en: '| German Traffic Sign Recognition Benchmark | 32.2 | 42.0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| German Traffic Sign Recognition Benchmark | 32.2 | 42.0 |'
- en: '| Stanford Cars | 59.4 | 79.3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Cars | 59.4 | 79.3 |'
- en: '| UCF101 | 64.5 | 63.1 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| UCF101 | 64.5 | 63.1 |'
- en: Even though general-purpose foundation models can answer everyday questions
    about different domains, they are unlikely to perform well on domain-specific
    tasks, especially if they never saw these tasks during training. Two examples
    of domain-specific tasks are drug discovery and cancer screening. Drug discovery
    involves protein, DNA, and RNA data, which follow specific formats and are expensive
    to acquire. This data is unlikely to be found in publicly available internet data.
    Similarly, cancer screening typically involves X-ray and fMRI (functional magnetic
    resonance imaging) scans, which are hard to obtain due to privacy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通用基础模型可以回答关于不同领域的日常问题，但它们在特定领域的任务上可能表现不佳，尤其是在训练过程中从未见过这些任务的情况下。两个特定领域的任务示例是药物发现和癌症筛查。药物发现涉及蛋白质、DNA
    和 RNA 数据，这些数据遵循特定的格式且获取成本高昂。这种数据不太可能出现在公开可用的互联网数据中。同样，癌症筛查通常涉及 X 射线和 fMRI（功能性磁共振成像）扫描，由于隐私问题，这些扫描难以获取。
- en: To train a model to perform well on these domain-specific tasks, you might need
    to curate very specific datasets. One of the most famous domain-specific models
    is perhaps [DeepMind’s AlphaFold](https://oreil.ly/JX37g), trained on the sequences
    and 3D structures of around 100,000 known proteins. [NVIDIA’s BioNeMo](https://oreil.ly/M1Nsc)
    is another model that focuses on biomolecular data for drug discovery. [Google’s
    Med-PaLM2](https://oreil.ly/F76hq) combined the power of an LLM with medical data
    to answer medical queries with higher accuracy.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个在这些特定领域任务上表现良好的模型，你可能需要精心制作非常具体的训练数据集。最著名的特定领域模型之一可能是[DeepMind的AlphaFold](https://oreil.ly/JX37g)，它基于大约100,000种已知蛋白质的序列和3D结构进行训练。[NVIDIA的BioNeMo](https://oreil.ly/M1Nsc)是另一个专注于药物发现生物分子数据的模型。[Google的Med-PaLM2](https://oreil.ly/F76hq)结合了LLM的力量和医疗数据，以更高的准确性回答医学查询。
- en: Tip
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Domain-specific models are especially common for biomedicine, but other fields
    can benefit from domain-specific models too. It’s possible that a model trained
    on architectural sketches can help architects much better than Stable Diffusion,
    or a model trained on factory plans can be optimized for manufacturing processes
    much better than a generic model like ChatGPT.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域模型在生物医学中尤其常见，但其他领域也可以从特定领域模型中受益。可能一个基于建筑草图训练的模型可以帮助建筑师比Stable Diffusion做得更好，或者一个基于工厂计划的模型可以比像ChatGPT这样的通用模型更好地优化制造过程。
- en: This section gave a high-level overview of how training data impacts a model’s
    performance. Next, let’s explore the impact of how a model is designed on its
    performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了训练数据如何影响模型性能。接下来，让我们探讨模型设计对其性能的影响。
- en: Modeling
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型化
- en: Before training a model, developers need to decide what the model should look
    like. What architecture should it follow? How many parameters should it have?
    These decisions impact not only the model’s capabilities but also its usability
    for downstream applications.^([5](ch02.html#id715)) For example, a 7B-parameter
    model will be vastly easier to deploy than a 175B-parameter model. Similarly,
    optimizing a transformer model for latency is very different from optimizing another
    architecture. Let’s explore the factors behind these decisions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，开发者需要决定模型应该是什么样子。它应该遵循哪种架构？它应该有多少参数？这些决定不仅影响模型的能力，还影响其在下游应用中的可用性。[5](ch02.html#id715)
    例如，一个7B参数的模型将比一个175B参数的模型更容易部署。同样，针对延迟优化transformer模型与优化另一种架构非常不同。让我们探讨这些决策背后的因素。
- en: Model Architecture
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: As of this writing, the most dominant architecture for language-based foundation
    models is the *transformer* architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)),
    which is based on the attention mechanism. It addresses many limitations of the
    previous architectures, which contributed to its popularity. However, the transformer
    architecture has its own limitations. This section analyzes the transformer architecture
    and its alternatives. Because it goes into the technical details of different
    architectures, it can be technically dense. If you find any part too deep in the
    weeds, feel free to skip it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文写作时，基于语言的基础模型中最占主导地位的架构是*transformer*架构([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))，它基于注意力机制。它解决了先前架构的许多限制，这促成了它的流行。然而，transformer架构也有其自身的局限性。本节分析了transformer架构及其替代方案。由于它涉及到不同架构的技术细节，因此可能会显得技术性很强。如果你发现任何部分过于深入，请随时跳过。
- en: Transformer architecture
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer架构
- en: 'To understand the transformer, let’s look at the problem it was created to
    solve. The transformer architecture was popularized on the heels of the success
    of the [seq2seq (sequence-to-sequence) architecture](https://arxiv.org/abs/1409.3215).
    At the time of its introduction in 2014, seq2seq provided significant improvement
    on then-challenging tasks: machine translation and summarization. In 2016, [Google
    incorporated seq2seq into Google Translate](https://oreil.ly/fb1aR), an update
    that they claimed to have given them the “largest improvements to date for machine
    translation quality”. This generated a lot of interest in seq2seq, making it the
    go-to architecture for tasks involving sequences of text.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解transformer，让我们看看它被创造出来要解决的问题。transformer架构是在seq2seq（序列到序列）架构[seq2seq (sequence-to-sequence)
    architecture](https://arxiv.org/abs/1409.3215)成功之后普及起来的。在2014年引入时，seq2seq在当时的挑战性任务上提供了显著的改进：机器翻译和摘要。2016年，[Google将seq2seq整合到Google
    Translate](https://oreil.ly/fb1aR)，他们声称这为他们带来了“迄今为止机器翻译质量的最大改进”。这引发了人们对seq2seq的极大兴趣，使其成为涉及文本序列的任务的首选架构。
- en: At a high level, seq2seq contains an encoder that processes inputs and a decoder
    that generates outputs. Both inputs and outputs are sequences of tokens, hence
    the name. Seq2seq uses RNNs (recurrent neural networks) as its encoder and decoder.
    In its most basic form, the encoder processes the input tokens sequentially, outputting
    the final hidden state that represents the input. The decoder then generates output
    tokens sequentially, conditioned on both the final hidden state of the input and
    the previously generated token. A visualization of the seq2seq architecture is
    shown in the top half of [Figure 2-4](#ch02_figure_4_1730147895520851).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，seq2seq包含一个处理输入的编码器和一个生成输出的解码器。输入和输出都是标记序列，因此得名。Seq2seq使用RNNs（循环神经网络）作为其编码器和解码器。在其最基本的形式中，编码器顺序处理输入标记，输出表示输入的最终隐藏状态。然后解码器根据输入的最终隐藏状态和之前生成的标记顺序生成输出标记。seq2seq架构的可视化显示在[图2-4](#ch02_figure_4_1730147895520851)的上半部分。
- en: '![A diagram of a algorithm  Description automatically generated with medium
    confidence](assets/aien_0204.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![一个算法的图示，描述自动生成，置信度中等](assets/aien_0204.png)'
- en: Figure 2-4\. Seq2seq architecture versus transformer architecture. For the transformer
    architecture, the arrows show the tokens that the decoder attends to when generating
    each output token.
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. Seq2seq架构与transformer架构对比。对于transformer架构，箭头显示了在生成每个输出标记时解码器所关注的标记。
- en: There are two problems with seq2seq that Vaswani et al. (2017) addresses. First,
    the vanilla seq2seq decoder generates output tokens using only the final hidden
    state of the input. Intuitively, this is like generating answers about a book
    using the book summary. This limits the quality of the generated outputs. Second,
    the RNN encoder and decoder mean that both input processing and output generation
    are done sequentially, making it slow for long sequences. If an input is 200 tokens
    long, seq2seq has to wait for each input token to finish processing before moving
    on to the next.^([6](ch02.html#id719))
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq有两个问题，Vaswani等人（2017）在论文中进行了解决。首先，vanilla seq2seq解码器仅使用输入的最终隐藏状态来生成输出标记。直观地说，这就像使用书的摘要来回答关于书的问题。这限制了生成输出的质量。其次，RNN编码器和解码器意味着输入处理和输出生成都是顺序进行的，这使得对于长序列来说很慢。如果一个输入有200个标记长，seq2seq必须等待每个输入标记完成处理才能继续到下一个标记。[6](ch02.html#id719)
- en: The transformer architecture addresses both problems with the attention mechanism.
    The attention mechanism allows the model to weigh the importance of different
    input tokens when generating each output token. This is like generating answers
    by referencing any page in the book. A simplified visualization of the transformer
    architecture is shown in the bottom half of [Figure 2-4](#ch02_figure_4_1730147895520851).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: transformer架构通过注意力机制解决了这两个问题。注意力机制允许模型在生成每个输出标记时权衡不同输入标记的重要性。这就像通过参考书的任何一页来生成答案。transformer架构的简化可视化显示在[图2-4](#ch02_figure_4_1730147895520851)的下半部分。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While the attention mechanism is often associated with the transformer model,
    it was introduced three years before the transformer paper. The attention mechanism
    can also be used with other architectures. Google used the attention mechanism
    with their seq2seq architecture in 2016 for their GNMT (Google Neural Machine
    Translation) model. However, it wasn’t until the transformer paper showed that
    the attention mechanism could be used without RNNs that it took off.^([7](ch02.html#id720))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然注意力机制通常与Transformer模型相关联，但它是在Transformer论文三年前被引入的。注意力机制也可以与其他架构一起使用。谷歌在2016年使用注意力机制与他们的seq2seq架构一起，用于他们的GNMT（谷歌神经机器翻译）模型。然而，直到Transformer论文表明注意力机制可以在没有RNN的情况下使用，它才开始流行。[^([7](ch02.html#id720))]
- en: The transformer architecture dispenses with RNNs entirely. With transformers,
    the input tokens can be processed in parallel, significantly speeding up input
    processing. While the transformer removes the sequential input bottleneck, transformer-based
    autoregressive language models still have the sequential output bottleneck.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构完全摒弃了RNN。使用Transformer，输入标记可以并行处理，这显著加快了输入处理。虽然Transformer消除了顺序输入瓶颈，但基于Transformer的自回归语言模型仍然存在顺序输出瓶颈。
- en: 'Inference for transformer-based language models, therefore, consists of two
    steps:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于Transformer的语言模型的推理包括两个步骤：
- en: Prefill
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预填充
- en: The model processes the input tokens in parallel. This step creates the intermediate
    state necessary to generate the first output token. This intermediate state includes
    the key and value vectors for all input tokens.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行处理输入标记。这一步创建了生成第一个输出标记所需的中间状态。这个中间状态包括所有输入标记的键和值向量。
- en: Decode
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 解码
- en: The model generates one output token at a time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型一次生成一个输出标记。
- en: As explored later in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301),
    the parallelizable nature of prefilling and the sequential aspect of decoding
    both motivate many optimization techniques to make language model inference cheaper
    and faster.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[9章](ch09.html#ch09_inference_optimization_1730130963006301)中所述，预填充的可并行性和解码的顺序特性都促使许多优化技术，以使语言模型推理更便宜、更快。
- en: Attention mechanism
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'At the heart of the transformer architecture is the attention mechanism. Understanding
    this mechanism is necessary to understand how transformer models work. Under the
    hood, the attention mechanism leverages key, value, and query vectors:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的核心是注意力机制。理解这一机制是理解Transformer模型工作原理的必要条件。在底层，注意力机制利用键、值和查询向量：
- en: The query vector (Q) represents the current state of the decoder at each decoding
    step. Using the same book summary example, this query vector can be thought of
    as the person looking for information to create a summary.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询向量（Q）代表在每个解码步骤中解码器的当前状态。使用相同的书籍摘要示例，这个查询向量可以被认为是寻找信息以创建摘要的人。
- en: Each key vector (K) represents a previous token. If each previous token is a
    page in the book, each key vector is like the page number. Note that at a given
    decoding step, previous tokens include both input tokens and previously generated
    tokens.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个键向量（K）代表一个先前标记。如果每个先前标记是书中的一个页面，那么每个键向量就像页码。请注意，在给定的解码步骤中，先前标记包括输入标记和先前生成的标记。
- en: Each value vector (V) represents the actual value of a previous token, as learned
    by the model. Each value vector is like the page’s content.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个值向量（V）代表模型学习到的先前标记的实际值。每个值向量就像页面的内容。
- en: The attention mechanism computes how much attention to give an input token by
    performing a [*dot product*](https://en.wikipedia.org/wiki/Dot_product) between
    the query vector and its key vector. A high score means that the model will use
    more of that page’s content (its value vector) when generating the book’s summary.
    A visualization of the attention mechanism with the key, value, and query vectors
    is shown in [Figure 2-5](#ch02_figure_5_1730147895520861). In this visualization,
    the query vector is seeking information from the previous tokens `How, are, you,
    ?, ¿` to generate the next token.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制通过执行查询向量与其键向量之间的[*点积*](https://en.wikipedia.org/wiki/Dot_product)来计算对输入标记给予多少注意力。高分意味着模型在生成书籍摘要时将更多地使用该页面的内容（其值向量）。在[图2-5](#ch02_figure_5_1730147895520861)中展示了注意力机制与键、值和查询向量的可视化。在这个可视化中，查询向量正在从先前标记`How,
    are, you, ?, ¿`中寻找信息以生成下一个标记。
- en: '![](assets/aien_0205.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aien_0205.png)'
- en: Figure 2-5\. An example of the attention mechanism in action next to its high-level
    visualization from the famous transformer paper, “Attention Is All You Need” (Vaswani
    et al., 2017).
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 来自著名Transformer论文“Attention Is All You Need” (Vaswani et al., 2017)中关于注意力机制作用的高层次可视化示例。
- en: Because each previous token has a corresponding key and value vector, the longer
    the sequence, the more key and value vectors need to be computed and stored. This
    is one reason why it’s so hard to extend context length for transformer models.
    How to efficiently compute and store key and value vectors comes up again in Chapters
    [7](ch07.html#ch07) and [9](ch09.html#ch09_inference_optimization_1730130963006301).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个先前标记都有一个相应的键和值向量，序列越长，需要计算和存储的键和值向量就越多。这也是为什么扩展Transformer模型的上下文长度如此困难的一个原因。如何在第[7](ch07.html#ch07)章和第[9](ch09.html#ch09_inference_optimization_1730130963006301)章中高效地计算和存储键和值向量再次成为问题。
- en: 'Let’s look into how the attention function works. Given an input `x`, the key,
    value, and query vectors are computed by applying key, value, and query matrices
    to the input. Let `W`[K]`, W`[V]`, and W`[Q] be the key, value, and query matrices.
    The key, value, and query vectors are computed as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看注意力函数是如何工作的。给定一个输入`x`，通过应用键、值和查询矩阵到输入来计算键、值和查询向量。设`W`[K]`、`W`[V]`和`W`[Q]`为键、值和查询矩阵。键、值和查询向量计算如下：
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The query, key, and value matrices have dimensions corresponding to the model’s
    hidden dimension. For example, in Llama 2-7B ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288)),
    the model’s hidden dimension size is 4096, meaning that each of these matrices
    has a `4096` × `4096` dimension. Each resulting `K`, `V`, `Q` vector has the dimension
    of `4096`.^([8](ch02.html#id727))
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 查询、键和值矩阵的维度与模型的隐藏维度相对应。例如，在Llama 2-7B ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288))中，模型的隐藏维度大小为4096，这意味着这些矩阵的每个维度都是`4096`
    × `4096`。每个生成的`K`、`V`、`Q`向量的维度为`4096`。^([8](ch02.html#id727))
- en: The attention mechanism is almost always multi-headed. Multiple heads allow
    the model to attend to different groups of previous tokens simultaneously. With
    multi-headed attention, the query, key, and value vectors are split into smaller
    vectors, each corresponding to an attention head. In the case of Llama 2-7B, because
    it has `32` attention heads, each `K`, `V`, and `Q` vector will be split into
    `32` vectors of the dimension `128`. This is because `4096 / 32 = 128`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制几乎总是多头的。多头允许模型同时关注不同的先前标记组。在多头注意力中，查询、键和值向量被分割成更小的向量，每个向量对应一个注意力头。在Llama
    2-7B的情况下，因为它有`32`个注意力头，每个`K`、`V`和`Q`向量将被分割成`32`个维度为`128`的向量。这是因为`4096 / 32 = 128`。
- en: $Attention left-parenthesis upper Q comma upper K comma upper V right-parenthesis
    equals softmax left-parenthesis StartFraction upper Q upper K Superscript upper
    T Baseline Over StartRoot d EndRoot EndFraction right-parenthesis upper V$
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $Attention\ left-parenthesis\ upper\ Q\ comma\ upper\ K\ comma\ upper\ V\ right-parenthesis\
    equals\ softmax\ left-parenthesis\ StartFraction\ upper\ Q\ upper\ K\ Superscript\
    upper\ T\ Baseline\ Over\ StartRoot\ d\ EndRoot\ EndFraction\ right-parenthesis\
    upper\ V$
- en: The outputs of all attention heads are then concatenated. An output projection
    matrix is used to apply another transformation to this concatenated output before
    it’s fed to the model’s next computation step. The output projection matrix has
    the same dimension as the model’s hidden dimension.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所有注意力头的输出随后被连接起来。使用输出投影矩阵对连接后的输出应用另一种转换，然后再将其输入到模型的下一个计算步骤。输出投影矩阵的维度与模型的隐藏维度相同。
- en: Transformer block
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Transformer块
- en: 'Now that we’ve discussed how attention works, let’s see how it’s used in a
    model. A transformer architecture is composed of multiple transformer blocks.
    The exact content of the block varies between models, but, in general, each transformer
    block contains the attention module and the MLP (multi-layer perceptron) module:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了注意力是如何工作的，让我们看看它在模型中的应用。Transformer架构由多个Transformer块组成。每个块的具体内容在不同模型之间有所不同，但一般来说，每个Transformer块包含注意力模块和MLP（多层感知器）模块：
- en: Attention module
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模块
- en: 'Each attention module consists of four weight matrices: query, key, value,
    and output projection.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力模块由四个权重矩阵组成：查询、键、值和输出投影。
- en: MLP module
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: MLP模块
- en: An MLP module consists of linear layers separated by *nonlinear activation functions*.
    Each linear layer is a weight matrix that is used for linear transformations,
    whereas an activation function allows the linear layers to learn nonlinear patterns.
    A linear layer is also called a feedforward layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Common nonlinear functions are ReLU, Rectified Linear Unit ([Agarap, 2018](https://arxiv.org/abs/1803.08375)),
    and GELU ([Hendrycks and Gimpel, 2016](https://arxiv.org/abs/1606.08415)), which
    was used by GPT-2 and GPT-3, respectively. Action functions are very simple.^([9](ch02.html#id739))
    For example, all ReLU does is convert negative values to 0\. Mathematically, it’s
    written as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: ReLU(x) = max(0, x)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of transformer blocks in a transformer model is often referred to
    as that model’s number of layers. A transformer-based language model is also outfitted
    with a module before and after all the transformer blocks:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: An embedding module before the transformer blocks
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: This module consists of the embedding matrix and the positional embedding matrix,
    which convert tokens and their positions into embedding vectors, respectively.
    Naively, the number of position indices determines the model’s maximum context
    length. For example, if a model keeps track of 2,048 positions, its maximum context
    length is 2,048\. However, there are techniques that increase a model’s context
    length without increasing the number of position indices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: An output layer after the transformer blocks
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: This module maps the model’s output vectors into token probabilities used to
    sample model outputs (discussed in [“Sampling”](#ch02_sampling_1730147895572256)).
    This module typically consists of one matrix, which is also called the *unembedding
    layer*. Some people refer to the output layer as the model *head*, as it’s the
    model’s last layer before output generation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-6](#ch02_figure_6_1730147895520869) visualizes a transformer model
    architecture. The size of a transformer model is determined by the dimensions
    of its building blocks. Some of the key values are:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The model’s dimension determines the sizes of the key, query, value, and output
    projection matrices in the transformer block.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of transformer blocks.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimension of the feedforward layer.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vocabulary size.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/aien_0206.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. A visualization of the weight composition of a transformer model.
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Larger dimension values result in larger model sizes. [Table 2-4](#ch02_table_4_1730147895537562)
    shows these dimension values for different Llama 2 ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288))
    and Llama 3 ([Dubey et al., 2024](https://arxiv.org/abs/2407.21783)) models. Note
    that while the increased context length impacts the model’s memory footprint,
    it doesn’t impact the model’s total number of parameters.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-4\. The dimension values of different Llama models.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # transformer blocks | Model dim | Feedforward dim | Vocab size |
    Context length |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Llama 2-7B | 32 | 4,096 | 11,008 | 32K | 4K |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2-7B | 32 | 4,096 | 11,008 | 32K | 4K |'
- en: '| Llama 2-13B | 40 | 5,120 | 13,824 | 32K | 4K |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2-13B | 40 | 5,120 | 13,824 | 32K | 4K |'
- en: '| Llama 2-70B | 80 | 8,192 | 22,016 | 32K | 4K |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2-70B | 80 | 8,192 | 22,016 | 32K | 4K |'
- en: '| Llama 3-7B | 32 | 4,096 | 14,336 | 128K | 128K |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3-7B | 32 | 4,096 | 14,336 | 128K | 128K |'
- en: '| Llama 3-70B | 80 | 8,192 | 28,672 | 128K | 128K |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3-70B | 80 | 8,192 | 28,672 | 128K | 128K |'
- en: '| Llama 3-405B | 126 | 16,384 | 53,248 | 128K | 128K |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3-405B | 126 | 16,384 | 53,248 | 128K | 128K |'
- en: Other model architectures
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他模型架构
- en: While the transformer model dominates the landscape, it’s not the only architecture.
    Since [AlexNet](https://oreil.ly/1spG5) revived the interest in deep learning
    in 2012, many architectures have gone in and out of fashion. Seq2seq was in the
    limelight for four years (2014–2018). [GANs](https://arxiv.org/abs/1406.2661)
    (generative adversarial networks) captured the collective imagination a bit longer
    (2014–2019). Compared to architectures that came before it, the transformer is
    sticky. It’s been around since 2017.^([10](ch02.html#id744)) How long until something
    better comes along?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Transformer模型主导着领域，但它并非唯一的架构。自2012年[AlexNet](https://oreil.ly/1spG5)重新点燃了人们对深度学习的兴趣以来，许多架构已经兴衰更迭。Seq2seq在2014至2018年间备受瞩目。[GANs](https://arxiv.org/abs/1406.2661)（生成对抗网络）吸引了人们的集体想象力更长一段时间（2014至2019年）。与之前的架构相比，Transformer更具粘性。它自2017年以来一直存在.^([10](ch02.html#id744))
    何时会有更好的东西出现？
- en: Developing a new architecture to outperform transformers isn’t easy.^([11](ch02.html#id745))
    The transformer has been heavily optimized since 2017\. A new architecture that
    aims to replace the transformer will have to perform at the scale that people
    care about, on the hardware that people care about.^([12](ch02.html#id746))
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一种能够超越Transformer的新架构并不容易.^([11](ch02.html#id745)) 自2017年以来，Transformer已经经过了大量的优化。一个旨在取代Transformer的新架构必须在人们关心的规模和硬件上表现出色.^([12](ch02.html#id746))
- en: However, there’s hope. While transformer-based models are dominating, as of
    this writing, several alternative architectures are gaining traction.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍有希望。虽然基于Transformer的模型正在主导，但截至本文写作时，几种替代架构正在获得关注。
- en: One popular model is [RWKV](https://github.com/BlinkDL/RWKV-LM) (Peng et al.,
    2023), an RNN-based model that can be parallelized for training. Due to its RNN
    nature, in theory, it doesn’t have the same context length limitation that transformer-based
    models have. However, in practice, having no context length limitation doesn’t
    guarantee good performance with long context.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的模型是[RWKV](https://github.com/BlinkDL/RWKV-LM)（Peng et al., 2023），这是一个基于RNN的模型，可以并行化进行训练。由于其RNN特性，从理论上讲，它没有基于Transformer的模型所具有的相同上下文长度限制。然而，在实践中，没有上下文长度限制并不保证在长上下文中表现出良好的性能。
- en: 'Modeling long sequences remains a core challenge in developing LLMs. An architecture
    that has shown a lot of promise in long-range memory is SSMs (state space models)
    ([Gu et al., 2021a](https://arxiv.org/abs/2110.13985)). Since the architecture’s
    introduction in 2021, multiple techniques have been introduced to make the architecture
    more efficient, better at long sequence processing, and scalable to larger model
    sizes. Here are a few of these techniques, to illustrate the evolution of a new
    architecture:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发LLMs（大型语言模型）时，建模长序列仍然是一个核心挑战。在长程记忆方面表现出很多希望的架构是SSMs（状态空间模型）([Gu et al., 2021a](https://arxiv.org/abs/2110.13985))。自该架构于2021年推出以来，已经引入了多种技术来提高架构的效率，使其在长序列处理方面表现更好，并能够扩展到更大的模型规模。以下是一些这些技术的例子，以展示新架构的演变：
- en: '*S4*, introduced in “Efficiently Modeling Long Sequences with Structured State
    Spaces” ([Gu et al., 2021b](https://arxiv.org/abs/2111.00396)), was developed
    to make SSMs more efficient.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S4*，在“Efficiently Modeling Long Sequences with Structured State Spaces”([Gu
    et al., 2021b](https://arxiv.org/abs/2111.00396))中提出，旨在使SSMs（状态空间模型）更高效。'
- en: '*H3*, introduced in “Hungry Hungry Hippos: Towards Language Modeling with State
    Space Models” ([Fu et al., 2022](https://arxiv.org/abs/2212.14052)), incorporates
    a mechanism that allows the model to recall early tokens and compare tokens across
    sequences. This mechanism’s purpose is akin to that of the attention mechanism
    in the transformer architecture, but it is more efficient.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H3*，在“Hungry Hungry Hippos: Towards Language Modeling with State Space Models”([Fu
    et al., 2022](https://arxiv.org/abs/2212.14052))中提出，包含了一种允许模型回忆早期标记并比较序列中标记的机制。这种机制的目的类似于Transformer架构中的注意力机制，但效率更高。'
- en: '*Mamba*, introduced in “Mamba: Linear-Time Sequence Modeling with Selective
    State Spaces” ([Gu and Dao, 2023](https://oreil.ly/n7wYO)), scales SSMs to three
    billion parameters. On language modeling, Mamba-3B outperforms transformers of
    the same size and matches transformers twice its size. The authors also show that
    Mamba’s inference computation scales linearly with sequence length (compared to
    quadratic scaling for transformers). Its performance shows improvement on real
    data up to million-length sequences.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mamba*，在“Mamba: Linear-Time Sequence Modeling with Selective State Spaces”
    ([Gu and Dao, 2023](https://oreil.ly/n7wYO)) 中介绍，将SSMs扩展到30亿参数。在语言建模方面，Mamba-3B优于同等规模的transformers，并且与两倍规模的transformers相当。作者还表明，Mamba的推理计算与序列长度呈线性关系（与transformers的二次关系相比）。其性能在百万长度序列的真实数据上显示出改进。'
- en: '*Jamba*, introduced in “Jamba: A Hybrid Transformer–Mamba Language Model” ([Lieber
    et al., 2024](https://arxiv.org/abs/2403.19887)), interleaves blocks of transformer
    and Mamba layers to scale up SSMs even further. The authors released a mixture-of-experts
    model with [52B total available parameters](https://oreil.ly/uyiBH) (12B active
    parameters) designed to fit in a single 80 GB GPU. Jamba shows strong performance
    on standard language model benchmarks and long-context evaluations for up to a
    context length of 256K tokens. It also has a small memory footprint compared to
    vanilla transformers.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jamba*，在“Jamba: A Hybrid Transformer–Mamba Language Model” ([Lieber et al.,
    2024](https://arxiv.org/abs/2403.19887)) 中介绍，将transformer和Mamba层的块交错排列，以进一步扩展SSMs。作者发布了一个混合专家模型，具有[52B总可用参数](https://oreil.ly/uyiBH)（12B活动参数），设计用于适应单个80
    GB GPU。Jamba在标准语言模型基准测试和长达256K个标记的上下文评估中表现出色。与传统的transformers相比，它还具有较小的内存占用。'
- en: '[Figure 2-7](#ch02_figure_7_1730147895520878) visualizes the transformer, Mamba,
    and Jamba blocks.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-7](#ch02_figure_7_1730147895520878) 展示了transformer、Mamba和Jamba块。'
- en: While it’s challenging to develop an architecture that outperforms the transformer,
    given its many limitations, there are a lot of incentives to do so. If another
    architecture does indeed overtake the transformer, some of the model adaptation
    techniques discussed in this book might change. However, just as the shift from
    ML engineering to AI engineering has kept many things unchanged, changing the
    underlying model architecture won’t alter the fundamental approaches.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管开发一个优于transformer的架构具有挑战性，鉴于其许多局限性，仍有很大动力去实现这一点。如果另一个架构确实超越了transformer，本书中讨论的一些模型适应技术可能会改变。然而，就像从ML工程到AI工程的转变使许多事情保持不变一样，改变底层模型架构不会改变基本方法。
- en: '![](assets/aien_0207.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aien_0207.png)'
- en: 'Figure 2-7\. A visualization of the transformer, Mamba, and Jamba layers. Image
    adapted from “Jamba: A Hybrid Transformer–Mamba Language Model” (Lieber et al.,
    2024).'
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图2-7\. Transformer、Mamba和Jamba层的可视化。图像改编自“Jamba: A Hybrid Transformer–Mamba
    Language Model” (Lieber et al., 2024)。'
- en: Model Size
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型大小
- en: Much of AI progress in recent years can be attributed to increased model size.
    It’s hard to talk about foundation models without talking about their number of
    parameters. The number of parameters is usually appended at the end of a model
    name. For example, Llama-13B refers to the version of Llama, a model family developed
    by Meta, with 13 billion parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，AI的许多进步可以归因于模型规模的增加。不谈论模型参数数量很难谈论基础模型。参数数量通常附加在模型名称的末尾。例如，Llama-13B指的是Meta开发的一个模型家族Llama的版本，具有130亿参数。
- en: In general, increasing a model’s parameters increases its capacity to learn,
    resulting in better models. Given two models of the same model family, the one
    with 13 billion parameters is likely to perform much better than the one with
    7 billion parameters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，增加模型的参数会增加其学习能力，从而产生更好的模型。给定两个同一家族的模型，具有130亿参数的模型可能比具有70亿参数的模型表现要好得多。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As the community better understands how to train large models, newer-generation
    models tend to outperform older-generation models of the same size. For example,
    [Llama 3-8B (2024)](https://arxiv.org/abs/2407.21783) outperforms even [Llama
    2-70B (2023)](https://arxiv.org/abs/2307.09288) on the MMLU benchmark.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随着社区更好地理解如何训练大型模型，新一代模型往往优于同规模的老一代模型。例如，[Llama 3-8B (2024)](https://arxiv.org/abs/2407.21783)
    在MMLU基准测试中甚至优于 [Llama 2-70B (2023)](https://arxiv.org/abs/2307.09288)。
- en: The number of parameters helps us estimate the compute resources needed to train
    and run this model. For example, if a model has 7 billion parameters, and each
    parameter is stored using 2 bytes (16 bits), then we can calculate that the GPU
    memory needed to do inference using this model will be at least 14 billion bytes
    (14 GB).^([13](ch02.html#id754))
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的数量帮助我们估计训练和运行此模型所需的计算资源。例如，如果一个模型有70亿个参数，并且每个参数使用2字节（16位）存储，那么我们可以计算出使用此模型进行推理所需的GPU内存至少为140亿字节（14
    GB）.^([13](ch02.html#id754))
- en: The number of parameters can be misleading if the model is *sparse*. A sparse
    model has a large percentage of zero-value parameters. A 7B-parameter model that
    is 90% sparse only has 700 million non-zero parameters. Sparsity allows for more
    efficient data storage and computation. This means that a large sparse model can
    require less compute than a small dense model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型是**稀疏**的，参数的数量可能会误导。一个稀疏模型拥有大量值为零的参数。一个90%稀疏的70亿参数模型实际上只有7亿个非零参数。稀疏性允许更高效的数据存储和计算。这意味着一个大型稀疏模型可能需要的计算资源比一个小型密集模型更少。
- en: A type of sparse model that has gained popularity in recent years is mixture-of-experts
    (MoE) ([Shazeer et al., 2017](https://arxiv.org/abs/1701.06538)). An MoE model
    is divided into different groups of parameters, and each group is an *expert*.
    Only a subset of the experts is *active* for (used to) process each token.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，一种流行的稀疏模型是专家混合（MoE）([Shazeer等人，2017](https://arxiv.org/abs/1701.06538))。MoE模型被分成不同的参数组，每个组都是一个**专家**。对于每个标记，只有专家的一个子集是**活跃**的（用于处理）。
- en: For example, [Mixtral 8x7B](https://oreil.ly/VvXbu) is a mixture of eight experts,
    each expert with seven billion parameters. If no two experts share any parameter,
    it should have 8 × 7 billion = 56 billion parameters. However, due to some parameters
    being shared, it has only 46.7 billion parameters.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[Mixtral 8x7B](https://oreil.ly/VvXbu)是由八个专家组成的混合体，每个专家有70亿个参数。如果没有任何两个专家共享任何参数，它应该有8
    × 70亿 = 560亿个参数。然而，由于一些参数被共享，它实际上只有466亿个参数。
- en: At each layer, for each token, only two experts are active. This means that
    only 12.9 billion parameters are active for each token. While this model has 46.7
    billion parameters, its cost and speed are the same as a 12.9-billion-parameter
    model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层，对于每个标记，只有两个专家是活跃的。这意味着每个标记只有129亿个参数是活跃的。虽然这个模型有466亿个参数，但其成本和速度与一个129亿参数的模型相同。
- en: 'A larger model can also underperform a smaller model if it’s not trained on
    enough data. Imagine a 13B-param model trained on a dataset consisting of a single
    sentence: “I like pineapples.” This model will perform much worse than a much
    smaller model trained on more data.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个大型模型没有在足够的数据上训练，它也可能表现不如一个较小的模型。想象一下，一个在只包含一个句子：“我喜欢菠萝。”的数据集上训练的130亿参数模型：这个模型的表现将远不如一个在更多数据上训练的较小模型。
- en: When discussing model size, it’s important to consider the size of the data
    it was trained on. For most models, dataset sizes are measured by the number of
    training samples. For example, Google’s Flamingo ([Alayrac et al., 2022](https://arxiv.org/abs/2204.14198))
    was trained using four datasets—one of them has 1.8 billion (image, text) pairs
    and one has 312 million (image, text) pairs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论模型大小时，考虑其训练数据的大小非常重要。对于大多数模型，数据集大小是通过训练样本的数量来衡量的。例如，Google的Flamingo([Alayrac等人，2022](https://arxiv.org/abs/2204.14198))使用了四个数据集——其中一个包含18亿（图像，文本）对，另一个包含3.12亿（图像，文本）对。
- en: For language models, a training sample can be a sentence, a Wikipedia page,
    a chat conversation, or a book. A book is worth a lot more than a sentence, so
    the number of training samples is no longer a good metric to measure dataset sizes.
    A better measurement is the number of tokens in the dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型，训练样本可以是一个句子、一个维基百科页面、一次聊天对话或一本书。一本书的价值远大于一个句子，因此训练样本的数量不再是衡量数据集大小的良好指标。更好的衡量标准是数据集中标记的数量。
- en: The number of tokens isn’t a perfect measurement either, as different models
    can have different tokenization processes, resulting in the same dataset having
    different numbers of tokens for different models. Why not just use the number
    of words or the number of letters? Because a token is the unit that a model operates
    on, knowing the number of tokens in a dataset helps us measure how much a model
    can potentially learn from that data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的数量也不是一个完美的衡量标准，因为不同的模型可能有不同的标记化过程，导致同一数据集在不同模型中有不同数量的标记。为什么不直接使用单词数量或字母数量呢？因为标记是模型操作的单元，知道数据集中的标记数量有助于我们衡量模型可以从这些数据中潜在学习多少。
- en: 'As of this writing, LLMs are trained using datasets in the order of trillions
    of tokens. Meta used increasingly larger datasets to train their Llama models:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文写作时，大型语言模型使用的是以万亿标记为顺序的数据集。Meta使用越来越大的数据集来训练他们的Llama模型：
- en: 1.4 trillion tokens for [Llama 1](https://arxiv.org/abs/2302.13971)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama 1](https://arxiv.org/abs/2302.13971)的1.4万亿标记'
- en: 2 trillion tokens for [Llama 2](https://arxiv.org/abs/2307.09288)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama 2](https://arxiv.org/abs/2307.09288)的2万亿标记'
- en: 15 trillion tokens for [Llama 3](https://oreil.ly/vfSQw)
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama 3](https://oreil.ly/vfSQw)的15万亿标记'
- en: Together’s open source dataset RedPajama-v2 has [30 trillion tokens](https://oreil.ly/SfB4g).
    This is equivalent to 450 million books^([14](ch02.html#id759)) or 5,400 times
    the size of Wikipedia. However, since RedPajama-v2 consists of indiscriminate
    content, the amount of high-quality data is much lower.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Together的开源数据集RedPajama-v2有[30万亿标记](https://oreil.ly/SfB4g)。这相当于4.5亿本书^([14](ch02.html#id759))或维基百科大小的5400倍。然而，由于RedPajama-v2包含无差别内容，高质量数据量要低得多。
- en: '*The number of tokens in a model’s dataset isn’t the same as its number of
    training tokens.* The number of training tokens measures the tokens that the model
    is trained on. If a dataset contains 1 trillion tokens and a model is trained
    on that dataset for two epochs—an *epoch* is a pass through the dataset—the number
    of training tokens is 2 trillion.^([15](ch02.html#id760)) See [Table 2-5](#ch02_table_5_1730147895537573)
    for examples of the number of training tokens for models with different numbers
    of parameters.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型数据集中的标记数与其训练标记数并不相同。训练标记数衡量的是模型训练所使用的标记数。如果一个数据集包含1万亿标记，并且模型在该数据集上训练了两个epoch（epoch是指遍历数据集的一次过程），那么训练标记数是2万亿。^([15](ch02.html#id760)）请参阅[表2-5](#ch02_table_5_1730147895537573)以了解不同参数数量模型的训练标记数示例。'
- en: 'Table 2-5\. Examples of the number of training tokens for models with different
    numbers of parameters. Source: “Training Compute-Optimal Large Language Models”
    ([DeepMind, 2022](https://oreil.ly/A3K90)).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-5\. 不同参数数量模型的训练标记数示例。来源：“训练计算最优的大型语言模型”（[DeepMind, 2022](https://oreil.ly/A3K90)）。
- en: '| Model | Size (# parameters) | Training tokens |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小（#参数） | 训练标记 |'
- en: '| --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LaMDA (Thoppilan et al., 2022) | 137 billion | 168 billion |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| LaMDA (Thoppilan et al., 2022) | 1370亿 | 1680亿 |'
- en: '| GPT-3 (Brown et al., 2020) | 175 billion | 300 billion |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 (Brown et al., 2020) | 1750亿 | 3000亿 |'
- en: '| Jurassic (Lieber et al., 2021) | 178 billion | 300 billion |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Jurassic (Lieber et al., 2021) | 1780亿 | 3000亿 |'
- en: '| Gopher (Rae et al., 2021) | 280 billion | 300 billion |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Gopher (Rae et al., 2021) | 2800亿 | 3000亿 |'
- en: '| MT-NLG 530B (Smith et al., 2022) | 530 billion | 270 billion |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| MT-NLG 530B (Smith et al., 2022) | 5300亿 | 2700亿 |'
- en: '| Chinchilla | 70 billion | 1.4 trillion |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Chinchilla | 700亿 | 1400亿 |'
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While this section focuses on the scale of data, quantity isn’t the only thing
    that matters. Data quality and data diversity matter, too. Quantity, quality,
    and diversity are the three golden goals for training data. They are discussed
    further in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节重点讨论数据的规模，但数量并不是唯一重要的因素。数据质量和数据多样性也同样重要。数量、质量和多样性是训练数据的三个黄金目标。它们将在[第8章](ch08.html#ch08_dataset_engineering_1730130932019888)中进一步讨论。
- en: Pre-training large models requires compute. One way to measure the amount of
    compute needed is by considering the number of machines, e.g., GPUs, CPUs, and
    TPUs. However, different machines have very different capacities and costs. An
    NVIDIA A10 GPU is different from an NVIDIA H100 GPU and an Intel Core Ultra Processor.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练大型模型需要计算资源。衡量所需计算量的一个方法是通过考虑机器的数量，例如，GPU、CPU和TPU。然而，不同机器的容量和成本差异很大。NVIDIA
    A10 GPU与NVIDIA H100 GPU以及Intel Core Ultra处理器不同。
- en: A more standardized unit for a model’s compute requirement is *FLOP*, or *floating
    point operation*. FLOP measures the number of floating point operations performed
    for a certain task. Google’s largest PaLM-2 model, for example, was trained using
    `10`^(22) FLOPs ([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311)).
    GPT-3-175B was trained using `3.14 × 10`^(23) FLOPs ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的计算需求的一个更标准化的单位是*FLOP*，即*浮点运算*。FLOP衡量的是为特定任务执行的浮点运算次数。例如，谷歌最大的PaLM-2模型使用了`10`^(22)
    FLOPs进行训练([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311))。GPT-3-175B使用了`3.14
    × 10`^(23) FLOPs进行训练([Brown et al., 2020](https://arxiv.org/abs/2005.14165))。
- en: '*The plural form of FLOP, FLOPs, is often confused with FLOP/s, floating point
    operations per Second.* FLOPs measure the compute requirement for a task, whereas
    FLOP/s measures a machine’s peak performance. For example, an NVIDIA H100 NVL
    GPU can deliver a maximum of [60 TeraFLOP/s](https://oreil.ly/HcFYz): `6 × 10`¹³
    FLOPs a second or `5.2 × 10`^(18) FLOPs a day.^([16](ch02.html#id762))'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*FLOP的复数形式FLOPs经常与FLOP/s混淆，即每秒浮点运算数。* FLOPs衡量任务的计算需求，而FLOP/s衡量机器的峰值性能。例如，NVIDIA
    H100 NVL GPU可以提供最大[60 TeraFLOP/s](https://oreil.ly/HcFYz)：每秒`6 × 10`¹³ FLOPs或每天`5.2
    × 10`^(18) FLOPs.^([16](ch02.html#id762))'
- en: Warning
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Be alert for confusing notations. FLOP/s is often written as FLOPS, which looks
    similar to FLOPs. To avoid this confusion, some companies, including OpenAI, use
    FLOP/s-day in place of FLOPs to measure compute requirements:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意混淆的符号。FLOP/s经常被写成FLOPS，看起来与FLOPs很相似。为了避免这种混淆，一些公司，包括OpenAI，使用FLOP/s-day代替FLOPs来衡量计算需求：
- en: '[PRE1]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This book uses FLOPs for counting floating point operations and FLOP/s for FLOPs
    per second.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用FLOPs来计算浮点运算，使用FLOP/s来表示每秒FLOPs。
- en: Assume that you have 256 H100s. If you can use them at their maximum capacity
    and make no training mistakes, it’d take you `(3.14 × 10^(23)) / (256 × 5.2 ×
    10^(18)) = ~236 days`, or approximately 7.8 months, to train GPT-3-175B.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有256台H100。如果你能以最大容量使用它们并且不犯训练错误，那么训练GPT-3-175B将需要`(3.14 × 10^(23)) / (256
    × 5.2 × 10^(18)) = ~236 days`，或者大约7.8个月。
- en: However, it’s unlikely you can use your machines at their peak capacity all
    the time. Utilization measures how much of the maximum compute capacity you can
    use. What’s considered good utilization depends on the model, the workload, and
    the hardware. Generally, if you can get half the advertised performance, 50% utilization,
    you’re doing okay. Anything above 70% utilization is considered great. Don’t let
    this rule stop you from getting even higher utilization. [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301)
    discusses hardware metrics and utilization in more detail.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你不可能总是让你的机器达到其峰值容量。利用率衡量你能使用多少最大计算能力。什么被认为是好的利用率取决于模型、工作负载和硬件。一般来说，如果你能获得标称性能的一半，即50%的利用率，你就做得不错了。任何超过70%的利用率都被认为是很好的。不要让这个规则阻止你获得更高的利用率。[第9章](ch09.html#ch09_inference_optimization_1730130963006301)更详细地讨论了硬件指标和利用率。
- en: 'At 70% utilization and $2/h for one H100,^([17](ch02.html#id763)) training
    GPT-3-175B would cost over $4 million:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在70%的利用率和每小时的$2对于一台H100的情况下，^([17](ch02.html#id763))训练GPT-3-175B将花费超过$4百万：
- en: '[PRE2]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'In summary, three numbers signal a model’s scale:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，三个数字可以指示模型的规模：
- en: Number of parameters, which is a proxy for the model’s learning capacity.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量，这是衡量模型学习能力的代理指标。
- en: Number of tokens a model was trained on, which is a proxy for how much a model
    learned.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练所使用的标记数量，这是衡量模型学习量的一个代理指标。
- en: Number of FLOPs, which is a proxy for the training cost.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FLOPs的数量，这是衡量训练成本的代理指标。
- en: 'Scaling law: Building compute-optimal models'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规模定律：构建计算最优模型
- en: 'I hope that the last section has convinced you of three things:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望最后一节已经让你相信了三件事：
- en: Model performance depends on the model size and the dataset size.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型性能取决于模型大小和数据集大小。
- en: Bigger models and bigger datasets require more compute.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更大的模型和更大的数据集需要更多的计算资源。
- en: Compute costs money.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算需要花钱。
- en: Unless you have unlimited money, budgeting is essential. You don’t want to start
    with an arbitrarily large model size and see how much it would cost. You start
    with a budget—how much money you want to spend—and work out the best model performance
    you can afford. As compute is often the limiting factor—compute infrastructure
    is not only expensive but also hard to set up—teams often start with a compute
    budget. Given a fixed amount of FLOPs, what model size and dataset size would
    give the best performance? A model that can achieve the best performance given
    a fixed compute budget is *compute-optional*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你有无限的金钱，预算是必不可少的。你不想从一个任意大的模型大小开始，看看它将花费多少。你从一个预算开始——你想要花多少钱——然后计算出你能负担的最佳模型性能。由于计算通常是限制因素——计算基础设施不仅昂贵，而且难以设置——团队通常从一个计算预算开始。给定固定数量的FLOPs，什么模型大小和数据集大小会给出最佳性能？在固定计算预算下能实现最佳性能的模型是*计算可选的*。
- en: 'Given a compute budget, the rule that helps calculate the optimal model size
    and dataset size is called the Chinchilla *scaling law*, proposed in the Chinchilla
    paper [“Training Compute-Optimal Large Language Models”](https://arxiv.org/abs/2203.15556)
    (DeepMind, 2022). To study the relationship between model size, dataset size,
    compute budget, and model performance, the authors trained 400 language models
    ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens.
    They found that for compute-optimal training, you need the number of training
    tokens to be approximately 20 times the model size. This means that a 3B-parameter
    model needs approximately 60B training tokens. The model size and the number of
    training tokens should be scaled equally: for every doubling of the model size,
    the number of training tokens should also be doubled.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 给定计算预算，帮助计算最优模型大小和数据集大小的规则被称为Chinchilla *规模定律*，该定律在Chinchilla论文“训练计算最优大型语言模型”（DeepMind，2022）[“Training
    Compute-Optimal Large Language Models”](https://arxiv.org/abs/2203.15556)中提出。为了研究模型大小、数据集大小、计算预算和模型性能之间的关系，作者们在5到500亿个标记上训练了从7000万个到超过160亿个参数的400个语言模型。他们发现，对于计算最优的训练，训练标记的数量需要大约是模型大小的20倍。这意味着一个30亿参数的模型需要大约600亿个训练标记。模型大小和训练标记的数量应该同等比例缩放：对于模型大小的每增加一倍，训练标记的数量也应该增加一倍。
- en: We’ve come a long way from when the training process was treated like alchemy.
    [Figure 2-8](#ch02_figure_8_1730147895520888) shows that we can predict not only
    the optimal number of parameters and tokens for each FLOP budget but also the
    expected training loss from these settings (assuming we do things right).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从将训练过程视为炼金术的时代走了很长的路。[图2-8](#ch02_figure_8_1730147895520888)显示，我们可以预测每个FLOP预算的最优参数数量和标记数量，以及从这些设置中预期的训练损失（假设我们做得正确）。
- en: This compute-optimal calculation assumes that the cost of acquiring data is
    much cheaper than the cost of compute. The same Chinchilla paper proposes another
    calculation for when the cost of training data is nontrivial.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算最优的计算假设获取数据成本远低于计算成本。同一篇Chinchilla论文还提出了当训练数据成本非平凡时的另一种计算方法。
- en: '![A graph with dots and lines  Description automatically generated](assets/aien_0208.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![带有点和线的图表，描述自动生成](assets/aien_0208.png)'
- en: 'Figure 2-8\. Graphs that depict the relationships between training loss, a
    model’s number of parameters, FLOPs, and number of training tokens. Source: “Training
    Compute-Optional Large Language Models” (DeepMind, 2022).'
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。描述训练损失、模型参数数量、FLOPs和训练标记数量之间关系的图表。来源：“训练计算可选的大型语言模型”（DeepMind，2022）。
- en: The scaling law was developed for dense models trained on predominantly human-generated
    data. Adapting this calculation for sparse models, such as mixture-of-expert models,
    and synthetic data is an active research area.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 规模定律是为在主要基于人类生成数据上训练的密集模型开发的。将此计算应用于稀疏模型，如专家混合模型和合成数据，是一个活跃的研究领域。
- en: The scaling law optimizes model quality given a compute budget. However, it’s
    important to remember that for production, model quality isn’t everything. Some
    models, most notably Llama, have suboptimal performance but better usability.
    Given their compute budget, Llama authors could’ve chosen bigger models that would
    perform better, but they opted for smaller models. Smaller models are easier to
    work with and cheaper to run inference on, which helped their models gain wider
    adoption. [Sardana et al. (2023)](https://arxiv.org/abs/2401.00448) modified the
    Chinchilla scaling law to calculate the optimal LLM parameter count and pre-training
    data size to account for this inference demand.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 规模定律在给定计算预算的情况下优化模型质量。然而，重要的是要记住，对于生产来说，模型质量并非一切。一些模型，特别是Llama，性能不佳但可用性更好。考虑到它们的计算预算，Llama的作者本可以选择更大的模型以获得更好的性能，但他们选择了较小的模型。较小的模型更容易处理，并且在运行推理时成本更低，这有助于它们的模型获得更广泛的应用。[Sardana等人（2023）](https://arxiv.org/abs/2401.00448)修改了Chinchilla规模定律，以计算最优的LLM参数数量和预训练数据大小，以考虑这种推理需求。
- en: On the topic of model performance given a compute budget, it’s worth noting
    that the cost of achieving a given model performance is decreasing. For example,
    on the ImageNet dataset, the cost to achieve 93% accuracy halved from 2019 to
    2021, according to the [*Artificial Intelligence Index Report 2022* (Stanford
    University HAI)](https://oreil.ly/oq-LE).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算预算给定的模型性能问题上，值得注意的是，实现给定模型性能的成本正在下降。例如，根据[*人工智能指数报告2022*（斯坦福大学HAI）](https://oreil.ly/oq-LE)，在ImageNet数据集上，实现93%准确性的成本从2019年到2021年减半。
- en: '*While the cost for the same model performance is decreasing, the cost for
    model performance improvement remains high.* Similar to the last mile challenge
    discussed in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    improving a model’s accuracy from 90 to 95% is more expensive than improving it
    from 85 to 90%. As Meta’s paper [“Beyond Neural Scaling Laws: Beating Power Law
    Scaling via Data Pruning”](https://oreil.ly/kO41d) pointed out, this means a model
    with a 2% error rate might require an order of magnitude more data, compute, or
    energy than a model with a 3% error rate.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管相同模型性能的成本在下降，但模型性能提升的成本仍然很高。* 与第1章中讨论的最后一英里挑战相似，将模型的准确率从90%提高到95%比从85%提高到90%的成本更高。正如Meta的论文[“超越神经缩放定律：通过数据剪枝击败幂律缩放”](https://oreil.ly/kO41d)所指出的，这意味着一个2%错误率的模型可能需要比3%错误率的模型多一个数量级的数据、计算或能量。'
- en: In language modeling, a drop in cross entropy loss from about 3.4 to 2.8 nats
    requires 10 times more training data. Cross entropy and its units, including nats,
    are discussed in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067).
    For large vision models, increasing the number of training samples from 1 billion
    to 2 billion leads to an accuracy gain on ImageNet of only a few percentage points.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言建模中，从大约3.4到2.8纳特的交叉熵损失下降需要10倍更多的训练数据。交叉熵及其单位，包括纳特，在第3章[第3章评估方法](ch03.html#ch03a_evaluation_methodology_1730150757064067)中讨论。对于大型视觉模型，将训练样本数量从10亿增加到20亿，在ImageNet上的准确率仅提高几个百分点。
- en: However, small performance changes in language modeling loss or ImageNet accuracy
    can lead to big differences in the quality of downstream applications. If you
    switch from a model with a cross-entropy loss of 3.4 to one with a loss of 2.8,
    you’ll notice a difference.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，语言建模损失或ImageNet准确率的小幅变化可能会导致下游应用质量产生巨大差异。如果你从具有3.4交叉熵损失的模型切换到具有2.8损失的模型，你会注意到差异。
- en: Scaling extrapolation
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩放外推
- en: The performance of a model depends heavily on the values of its *hyperparameters*.
    When working with small models, it’s a common practice to train a model multiple
    times with different sets of hyperparameters and pick the best-performing one.
    This is, however, rarely possible for large models as training them once is resource-draining
    enough.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能很大程度上取决于其**超参数**的值。当处理小型模型时，通常的做法是使用不同的超参数集多次训练模型，并选择表现最好的一个。然而，对于大型模型来说，这种情况很少可能发生，因为单次训练就已经消耗了大量的资源。
- en: This means that for many models, you might have only one shot of getting the
    right set of hyperparameters. As a result, *scaling extrapolation* (also called
    *hyperparameter transferring*) has emerged as a research subfield that tries to
    predict, for large models, what hyperparameters will give the best performance.
    The current approach is to study the impact of hyperparameters on models of different
    sizes, usually much smaller than the target model size, and then extrapolate how
    these hyperparameters would work on the target model size.^([18](ch02.html#id776))
    A [2022 paper](https://oreil.ly/sHwbw) by Microsoft and OpenAI shows that it was
    possible to transfer hyperparameters from a 40M model to a 6.7B model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对于许多模型来说，你可能只有一次机会获得正确的一组超参数。因此，**缩放外推**（也称为**超参数迁移**）已成为一个研究子领域，它试图预测对于大型模型，哪些超参数将提供最佳性能。当前的方法是研究超参数对不同大小模型的影响，通常比目标模型的大小小得多，然后外推这些超参数在目标模型大小上的作用。[^([18](ch02.html#id776))]
    一篇2022年的论文[由微软和OpenAI发表](https://oreil.ly/sHwbw)显示，从40M模型迁移超参数到6.7B模型是可能的。
- en: Scaling extrapolation is still a niche topic, as few people have the experience
    and resources to study the training of large models. It’s also difficult to do
    due to the sheer number of hyperparameters and how they interact with each other.
    If you have ten hyperparameters, you’d have to study 1,024 hyperparameter combinations.
    You would have to study each hyperparameter individually, then two of them together,
    and three of them together, and so on.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放外推仍然是一个小众话题，因为很少有人有研究大型模型训练的经验和资源。这也由于超参数的数量以及它们之间相互作用的复杂性而难以实现。如果你有十个超参数，你将不得不研究1,024个超参数组合。你需要单独研究每个超参数，然后是两个一起，然后是三个一起，以此类推。
- en: 'In addition, emergent abilities ([Wei et al., 2022](https://arxiv.org/abs/2206.07682))
    make the extrapolation less accurate. Emergent abilities refer to those that are
    only present at scale might not be observable on smaller models trained on smaller
    datasets. To learn more about scaling extrapolation, check out this excellent
    blog post: “On the Difficulty of Extrapolation with NN Scaling” ([Luke Metz, 2022](https://oreil.ly/kuG3J)).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，新兴能力 ([Wei 等人，2022](https://arxiv.org/abs/2206.07682)) 使得外推变得更加不准确。新兴能力指的是仅在规模较大时才存在的，可能在训练数据较小的小型模型上不可观察的能力。想了解更多关于规模外推的信息，请查看这篇优秀的博客文章：“关于神经网络规模外推的困难”
    ([Luke Metz，2022](https://oreil.ly/kuG3J))。
- en: Scaling bottlenecks
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规模瓶颈
- en: Until now, every order of magnitude increase in model size has led to an increase
    in model performance. GPT-2 has an order of magnitude more parameters than GPT-1
    (1.5 billion versus 117 million). GPT-3 has two orders of magnitude more than
    GPT-2 (175 billion versus 1.5 billion). This means a three-orders-of-magnitude
    increase in model sizes between 2018 and 2021\. Three more orders of magnitude
    growth would result in 100-trillion-parameter models.^([19](ch02.html#id777))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，模型大小的每十倍增长都导致了模型性能的提升。GPT-2 的参数数量比 GPT-1 多一个数量级（15亿比1.17亿）。GPT-3 的参数数量比
    GPT-2 多两个数量级（1750亿比15亿）。这意味着从 2018 年到 2021 年，模型大小增长了三个数量级。再增长三个数量级将导致拥有 100 万亿参数的模型。[19](ch02.html#id777)
- en: 'How many more orders of magnitude can model sizes grow? Would there be a point
    where the model performance plateaus regardless of its size? While it’s hard to
    answer these questions, there are already two visible bottlenecks for scaling:
    training data and electricity.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小还能增长多少数量级？是否存在一个点，无论模型大小如何，其性能都会趋于平稳？虽然很难回答这些问题，但已经存在两个明显的规模瓶颈：训练数据和电力。
- en: Foundation models use so much data that there’s a realistic concern we’ll run
    out of internet data in the next few years. The rate of training dataset size
    growth is much faster than the rate of new data being generated ([Villalobos et
    al., 2022](https://arxiv.org/abs/2211.04325)), as illustrated in [Figure 2-9](#ch02_figure_9_1730147895520897).
    *If you’ve ever put anything on the internet, you should assume that it already
    is or will be included in the training data for some language models,* whether
    you consent or not. This is similar to how, if you post something on the internet,
    you should expect it to be indexed by Google.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型使用的数据量如此之大，以至于我们担心在接下来的几年内可能会耗尽互联网数据。训练数据集大小的增长速度远远快于新数据的生成速度 ([Villalobos
    等人，2022](https://arxiv.org/abs/2211.04325))，如图 2-9 所示。*如果你曾在互联网上发布过任何内容，你应该假设它已经或将要被包含在某些语言模型的训练数据中，无论你是否同意。*
    这类似于，如果你在互联网上发布了一些内容，你应该预期它会被谷歌索引。
- en: '![A graph of data being measured  Description automatically generated with
    medium confidence](assets/aien_0209.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![数据测量的图表  描述由中等置信度自动生成](assets/aien_0209.png)'
- en: 'Figure 2-9\. Projection of historical trend of training dataset sizes and available
    data stock. Source: Villalobos et al., 2024.'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 训练数据集大小和可用数据量的历史趋势预测。来源：Villalobos 等人，2024。
- en: Some people are leveraging this fact to inject data they want into the training
    data of future models. They do this simply by publishing the text they want on
    the internet, hoping it will influence future models to generate the responses
    they desire. Bad actors can also leverage this approach for prompt injection attacks,
    as discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人利用这一事实将他们想要的数据注入未来模型的训练数据中。他们只是简单地在网上发布他们想要的内容，希望它能够影响未来模型生成他们期望的响应。恶意行为者也可以利用这种方法进行提示注入攻击，如第
    5 章所述（ch05.html#ch05a_prompt_engineering_1730156991195551）。
- en: Note
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An open research question is how to make a model forget specific information
    it has learned during training. Imagine you published a blog post that you eventually
    deleted. If that blog post was included in a model’s training data, the model
    might still reproduce the post’s content. As a result, people could potentially
    access removed content without your consent.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一个开放的研究问题是，如何让模型忘记它在训练期间学习到的特定信息。想象一下，你发布了一篇博客文章，后来又删除了。如果那篇博客文章被包含在某个模型的训练数据中，该模型可能仍然会重现文章的内容。结果，人们可能在没有你同意的情况下访问被删除的内容。
- en: On top of that, the internet is being rapidly populated with data generated
    by AI models. If companies continue using internet data to train future models,
    these new models will be partially trained on AI-generated data. In December 2023,
    Grok, a model trained by X, was caught refusing a request by saying that it goes
    against OpenAI’s use case policy. This caused some people to speculate that Grok
    was trained using ChatGPT outputs. [Igor Babuschkin, a core developer behind Grok](https://x.com/ibab/status/1733558576982155274),
    responded that it was because Grok was trained on web data, and “the web is full
    of ChatGPT outputs.”^([20](ch02.html#id778))
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，互联网正在迅速被AI模型生成的大量数据所充斥。如果公司继续使用互联网数据来训练未来的模型，这些新模型将部分基于AI生成数据进行训练。2023年12月，由X训练的模型Grok被发现在拒绝请求时表示这违反了OpenAI的使用案例政策。这导致一些人猜测Grok是使用ChatGPT的输出进行训练的。[Grok的核心开发者之一Igor
    Babuschkin](https://x.com/ibab/status/1733558576982155274)回应说，这是因为Grok是在网络数据上训练的，而“网络上充满了ChatGPT的输出。”^([20](ch02.html#id778))
- en: Some researchers worry that recursively training new AI models on AI-generated
    data causes the new models to gradually forget the original data patterns, degrading
    their performance over time ([Shumailov et al., 2023](https://arxiv.org/abs/2305.17493)).
    However, the impact of AI-generated data on models is more nuanced and is discussed
    in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员担心，在AI生成数据上递归训练新的AI模型会导致新模型逐渐忘记原始数据模式，随着时间的推移降低其性能（Shumailov et al., 2023）。然而，AI生成数据对模型的影响更为复杂，这将在[第8章](ch08.html#ch08_dataset_engineering_1730130932019888)中进行讨论。
- en: Once the publicly available data is exhausted, the most feasible paths for more
    human-generated training data is proprietary data. Unique proprietary data—copyrighted
    books, translations, contracts, medical records, genome sequences, and so forth—will
    be a competitive advantage in the AI race. This is a reason why OpenAI negotiated
    [deals](https://oreil.ly/AkAyI) with publishers and media outlets including Axel
    Springer and the Associated Press.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦公开可用的数据耗尽，获取更多由人类生成训练数据的可行途径就是专有数据。独特的专有数据——包括受版权保护的书籍、翻译、合同、医疗记录、基因组序列等等——将在人工智能竞赛中成为竞争优势。这也是为什么OpenAI与包括Axel
    Springer和美联社在内的出版商和媒体机构达成了[协议](https://oreil.ly/AkAyI)。
- en: It’s not surprising that in light of ChatGPT, many companies, including [Reddit](https://oreil.ly/o7WB3)
    and [Stack Overflow](https://oreil.ly/xNuju), have changed their data terms to
    prevent other companies from scraping their data for their models. [Longpre et
    al. (2024)](https://arxiv.org/abs/2407.14933) observed that between 2023 and 2024,
    the rapid crescendo of data restrictions from web sources rendered over 28% of
    the most critical sources in the popular public dataset [C4](https://github.com/google-research/text-to-text-transfer-transformer#c4)
    fully restricted from use. Due to changes in its Terms of Service and crawling
    restrictions, a full 45% of C4 is now restricted.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT出现的情况下，许多公司，包括[Reddit](https://oreil.ly/o7WB3)和[Stack Overflow](https://oreil.ly/xNuju)，改变他们的数据条款以防止其他公司从他们的模型中抓取数据，这并不令人惊讶。Longpre等人（2024）观察到，在2023年和2024年之间，来自网络来源的数据限制的快速增加使得流行公共数据集[C4](https://github.com/google-research/text-to-text-transfer-transformer#c4)中超过28%的最关键来源完全受限。由于服务条款和爬虫限制的变化，现在C4中有45%受到限制。
- en: The other bottleneck, which is less obvious but more pressing, is electricity.
    Machines require electricity to run. As of this writing, data centers are estimated
    to consume 1–2% of global electricity. This number is estimated to reach between
    [4% and 20% by 2030](https://oreil.ly/0DKHL) (Patel, Nishball, and Ontiveros,
    2024). Until we can figure out a way to produce more energy, data centers can
    grow at most 50 times, which is less than two orders of magnitude. This leads
    to a concern about a power shortage in the near future, which will drive up the
    cost of electricity.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个瓶颈，虽然不那么明显但更为紧迫，是电力。机器需要电力来运行。截至本文撰写时，数据中心预计消耗全球电力的1-2%。这个数字预计到2030年将达到4%到20%之间（Patel,
    Nishball, and Ontiveros, 2024）。在我们找到产生更多能源的方法之前，数据中心最多只能增长50倍，这还不到两个数量级。这导致了对未来电力短缺的担忧，这将推高电价。
- en: 'Now that we’ve covered two key modeling decisions—architecture and scale—let’s
    move on to the next critical set of design choices: how to align models with human
    preferences.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了两个关键建模决策——架构和规模——让我们继续讨论下一组关键的设计选择：如何使模型与人类偏好保持一致。
- en: Post-Training
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练后
- en: Post-training starts with a pre-trained model. Let’s say that you’ve pre-trained
    a foundation model using self-supervision. Due to how pre-training works today,
    a pre-trained model typically has two issues. First, self-supervision optimizes
    the model for text completion, not conversations.^([21](ch02.html#id787)) If you
    find this unclear, don’t worry, [“Supervised Finetuning”](#ch02_supervised_finetuning_1730147895572140)
    will have examples. Second, if the model is pre-trained on data indiscriminately
    scraped from the internet, its outputs can be racist, sexist, rude, or just wrong.
    The goal of post-training is to address both of these issues.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练从预训练模型开始。假设你已经使用自监督方法预训练了一个基础模型。由于目前预训练的工作方式，预训练模型通常有两个问题。首先，自监督优化模型进行文本完成，而不是对话。[21](ch02.html#id787)
    如果你觉得这不清楚，不要担心，“监督微调”部分[“Supervised Finetuning”](#ch02_supervised_finetuning_1730147895572140)将会有示例。其次，如果模型是在从互联网上无差别抓取的数据上预训练的，其输出可能是种族主义、性别歧视、粗鲁或只是错误的。后训练的目标是解决这两个问题。
- en: 'Every model’s post-training is different. However, in general, post-training
    consists of two steps:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的后训练都是不同的。然而，一般来说，后训练包括两个步骤：
- en: '*Supervised finetuning* (*SFT*): Finetune the pre-trained model on high-quality
    instruction data to optimize models for conversations instead of completion.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*监督微调* (*SFT*)：在高质量指令数据上微调预训练模型，以优化模型进行对话而非完成。'
- en: '*Preference finetuning*: Further finetune the model to output responses that
    align with human preference. Preference finetuning is typically done with reinforcement
    learning (RL).^([22](ch02.html#id790)) Techniques for preference finetuning include
    [*reinforcement learning from human feedback*](https://oreil.ly/iJG1q) (RLHF)
    (used by [GPT-3.5](https://oreil.ly/tbgTi) and [Llama 2](https://arxiv.org/abs/2307.09288)),
    [DPO](https://arxiv.org/abs/2305.18290) (Direct Preference Optimization) (used
    by [Llama 3](https://arxiv.org/abs/2407.21783)), and [*reinforcement learning
    from AI feedback*](https://arxiv.org/abs/2309.00267) (RLAIF) (potentially used
    by [Claude](https://arxiv.org/abs/2212.08073)).'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*偏好微调*：进一步微调模型以输出符合人类偏好的响应。偏好微调通常使用强化学习（RL）进行。[22](ch02.html#id790) 偏好微调的技术包括[*从人类反馈中进行强化学习*](https://oreil.ly/iJG1q)
    (RLHF)（由 [GPT-3.5](https://oreil.ly/tbgTi) 和 [Llama 2](https://arxiv.org/abs/2307.09288)
    使用），[DPO](https://arxiv.org/abs/2305.18290)（直接偏好优化）（由 [Llama 3](https://arxiv.org/abs/2407.21783)
    使用），以及[*从AI反馈中进行强化学习*](https://arxiv.org/abs/2309.00267) (RLAIF)（可能由 [Claude](https://arxiv.org/abs/2212.08073)
    使用）。'
- en: Let me highlight the difference between pre-training and post-training another
    way. For language-based foundation models, pre-training optimizes token-level
    quality, where the model is trained to predict the next token accurately. However,
    users don’t care about token-level quality—they care about the quality of the
    entire response. Post-training, in general, optimizes the model to generate responses
    that users prefer. Some people compare pre-training to reading to acquire knowledge,
    while post-training is like learning how to use that knowledge.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我用另一种方式来强调预训练和后训练之间的区别。对于基于语言的基础模型，预训练优化的是标记级别的质量，即模型被训练来准确预测下一个标记。然而，用户并不关心标记级别的质量——他们关心的是整个响应的质量。一般来说，后训练优化模型以生成用户偏好的响应。有些人将预训练比作阅读来获取知识，而后训练则像是学习如何使用这些知识。
- en: Warning
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Watch out for terminology ambiguity. Some people use the term *instruction finetuning*
    to refer to supervised finetuning, while some other people use this term to refer
    to both supervised finetuning and preference finetuning. To avoid ambiguity, I
    will avoid the term instruction finetuning in this book.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意术语的歧义。有些人使用“指令微调”这个术语来指代监督微调，而有些人则用这个术语来指代监督微调和偏好微调。为了避免歧义，我在这本书中会避免使用“指令微调”这个术语。
- en: As post-training consumes a small portion of resources compared to pre-training
    ([InstructGPT](https://oreil.ly/9bbzX) used only 2% of compute for post-training
    and 98% for pre-training), you can think of post-training as unlocking the capabilities
    that the pre-trained model already has but are hard for users to access via prompting
    alone.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 与预训练相比，后训练消耗的资源较少（[InstructGPT](https://oreil.ly/9bbzX) 用于后训练的计算资源仅为 2%，而预训练为
    98%），因此你可以将后训练视为解锁预训练模型已经具备但用户仅通过提示难以访问的能力。
- en: '[Figure 2-10](#ch02_figure_10_1730147895520905) shows the overall workflow
    of pre-training, SFT, and preference finetuning, assuming you use RLHF for the
    last step. You can approximate how well a model aligns with human preference by
    determining what steps the model creators have taken.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-10](#ch02_figure_10_1730147895520905) 展示了预训练、SFT 和偏好微调的整体工作流程，假设你在最后一步使用
    RLHF。你可以通过确定模型创建者采取了哪些步骤来近似模型与人类偏好的契合度。'
- en: '![A diagram of a data analysis  Description automatically generated](assets/aien_0210.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![数据分析图  自动生成的描述](assets/aien_0210.png)'
- en: Figure 2-10\. The overall training workflow with pre-training, SFT, and RLHF.
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 预训练、SFT 和 RLHF 的整体训练流程。
- en: 'If you squint, [Figure 2-10](#ch02_figure_10_1730147895520905) looks very similar
    to the meme depicting the monster [Shoggoth](https://en.wikipedia.org/wiki/Shoggoth)
    with a smiley face in [Figure 2-11](#ch02_figure_11_1730147895520913):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你眯起眼睛，[图 2-10](#ch02_figure_10_1730147895520905) 与描绘怪物 [Shoggoth](https://en.wikipedia.org/wiki/Shoggoth)
    带有笑脸的 [图 2-11](#ch02_figure_11_1730147895520913) 的梗图非常相似：
- en: Self-supervised pre-training results in a rogue model that can be considered
    an untamed monster because it uses indiscriminate data from the internet.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自监督预训练导致了一个不受约束的模型，它可以被认为是一匹未驯服的怪物，因为它使用了来自互联网的无差别的数据。
- en: This monster is then supervised finetuned on higher-quality data—Stack Overflow,
    Quora, or human annotations—which makes it more socially acceptable.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个怪物随后在更高质量的数据上进行监督微调——Stack Overflow、Quora 或人工标注，这使得它更易于社会接受。
- en: This finetuned model is further polished using preference finetuning to make
    it customer-appropriate, which is like giving it a smiley face.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个微调模型进一步通过偏好微调进行润色，使其更符合客户需求，这就像给它一个笑脸。
- en: '![A drawing of a monster  Description automatically generated](assets/aien_0211.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![怪物绘图  自动生成的描述](assets/aien_0211.png)'
- en: Figure 2-11\. Shoggoth with a smiley face. Adapted from an original image shared
    by [anthrupad](https://x.com/anthrupad/status/1622349563922362368).
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 带有笑脸的 Shoggoth。改编自 [anthrupad](https://x.com/anthrupad/status/1622349563922362368)
    分享的原始图像。
- en: Note that a combination of pre-training, SFT, and preference finetuning is the
    popular solution for building foundation models today, but it’s not the only solution.
    You can skip any of the steps, as you’ll see shortly.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，预训练、SFT 和偏好微调的组合是目前构建基础模型的热门解决方案，但并非唯一解决方案。你可以跳过任何步骤，正如你很快就会看到的。
- en: Supervised Finetuning
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督微调
- en: 'As discussed in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    the pre-trained model is likely optimized for completion rather than conversing.
    If you input “How to make pizza” into the model, the model will continue to complete
    this sentence, as the model has no concept that this is supposed to be a conversation.
    Any of the following three options can be a valid completion:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 1 章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)
    所述，预训练模型可能更优化于生成而非对话。如果你将“如何制作披萨”输入到模型中，模型将继续完成这个句子，因为模型没有意识到这应该是一个对话。以下任何一种选项都可以是一个有效的完成：
- en: 'Adding more context to the question: “for a family of six?”'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为问题添加更多上下文：“对于一个六口之家？”
- en: 'Adding follow-up questions: “What ingredients do I need? How much time would
    it take?”'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加后续问题：“我需要哪些原料？需要多长时间？”
- en: Giving the instructions on how to make pizza.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示如何制作披萨的说明。
- en: If the goal is to respond to users appropriately, the correct option is 3.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标是适当地回应用户，正确的选项是 3。
- en: 'We know that a model mimics its training data. To encourage a model to generate
    the appropriate responses, you can show examples of appropriate responses. Such
    examples follow the format (*prompt, response*) and are called *demonstration
    data*. Some people refer to this process as *behavior cloning*: you demonstrate
    how the model should behave, and the model clones this behavior.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道模型模仿其训练数据。为了鼓励模型生成适当的响应，你可以展示适当的响应示例。这些示例遵循 (*提示，响应*) 的格式，并被称为 *演示数据*。有些人将这个过程称为
    *行为克隆*：你展示模型应该如何表现，模型就模仿这种行为。
- en: Since different types of requests require different types of responses, your
    demonstration data should contain the range of requests you want your model to
    handle, such as question answering, summarization, and translation. [Figure 2-12](#ch02_figure_12_1730147895520920)
    shows a distribution of types of tasks OpenAI used to finetune their model [InstructGPT](https://oreil.ly/8U2z8).
    Note that this distribution doesn’t contain multimodal tasks, as InstructGPT is
    a text-only model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![A colorful circle with text on it  Description automatically generated](assets/aien_0212.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. The distribution of prompts used to finetune InstructGPT. The
    graph is created based on the numbers from the OpenAI paper.
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Good teachers are important for humans to learn. Similarly, good labelers are
    important for AIs to learn how to conduct intelligent conversations. Unlike traditional
    data labeling, which can often be done with little or no domain expertise, demonstration
    data may contain complex prompts whose responses require critical thinking, information
    gathering, and judgment about the appropriateness of the user’s requests. [Table 2-6](#ch02_table_6_1730147895537582)
    shows examples of (prompt, response) pairs created by labelers for InstructGPT.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-6\. Examples of demonstration data used for [InstructGPT](https://arxiv.org/abs/2203.02155).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Labeler’s response |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| `Serendipity means the occurrence and development of events by chance in
    a happy or beneficial way. Use the word in a sentence.` | `Running into Margaret
    and being introduced to Tom was a fortunate stroke of` `serendipity.` |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| `Read the following text and answer the three questions at the end.`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '`[OMITTED FOR LENGTH - Article about Justice Ruth Bader Ginsburg,`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '`Champion Of Gender Equality, Dies At 87]`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '`1\. …`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '`2\. …`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '`3\. Why will Ginsburg’s death have`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '`profound consequences for the court and the country?` | `1\. …` `2\. …`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '`3\. Ruth Bader Ginsburg was the leader of the liberal wing, and because the
    court is about to open a new term, the chief`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '`justice no longer holds the controlling vote in contested cases.` |'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '| `ELI5: What’s the cause of the` `"anxiety lump" in our chest during stressful
    or disheartening`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '`experiences?` | `The anxiety lump in your throat is caused by muscular tension
    keeping your glottis dilated to maximize airflow. The clenched chest or heartache
    feeling is caused by the vagus nerve which tells the organs to pump blood faster,
    stop digesting, and` `produce adrenaline and cortisol.` |'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Companies, therefore, often use highly educated labelers to generate demonstration
    data. Among those who labeled demonstration data for InstructGPT, [~90% have at
    least a college degree](https://oreil.ly/SF_X9) and more than one-third have a
    master’s degree. If labeling objects in an image might take only seconds, generating
    one (prompt, response) pair can take up to 30 minutes, especially for tasks that
    involve long contexts like summarization. If it costs $10 for one (prompt, response)
    pair, the 13,000 pairs that OpenAI used for InstructGPT would cost $130,000\.
    That doesn’t yet include the cost of designing the data (what tasks and prompts
    to include), recruiting labelers, and data quality control.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，公司通常使用受过高等教育的人作为标注员来生成演示数据。在为InstructGPT标注演示数据的人中，[~90%拥有至少本科学历](https://oreil.ly/SF_X9)，超过三分之一拥有硕士学位。如果标注图像中的对象可能只需几秒钟，生成一个（提示，响应）对可能需要长达30分钟，尤其是涉及长上下文如摘要的任务。如果一对（提示，响应）的成本是10美元，那么OpenAI为InstructGPT使用的13,000对将花费130,000美元。这还不包括设计数据（包括哪些任务和提示）、招募标注员和数据质量控制的成本。
- en: Not everyone can afford to follow the high-quality human annotation approach.
    LAION, a non-profit organization, mobilized 13,500 volunteers worldwide to generate
    10,000 conversations, which consist of 161,443 messages in 35 different languages,
    annotated with 461,292 quality ratings. Since the data was generated by volunteers,
    there wasn’t much control for biases. In theory, the labelers that teach models
    the human preference should be representative of the human population. The demographic
    of labelers for LAION is skewed. For example, in a self-reported survey, 90% of
    volunteer labelers identified as male ([Köpf et al., 2023](https://arxiv.org/abs/2304.07327)).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个人都能负担得起遵循高质量人工标注的方法。非营利组织LAION动员了全球13,500名志愿者生成10,000次对话，这些对话包含35种不同语言中的161,443条消息，并附有461,292个质量评分。由于数据是由志愿者生成的，因此对偏差的控制并不多。理论上，教模型人类偏好的标注员应该代表整个人口。LAION的标注员人口统计存在偏差。例如，在一个自我报告的调查中，90%的志愿者标注员自认为是男性([Köpf
    et al., 2023](https://arxiv.org/abs/2304.07327))。
- en: 'DeepMind used [simple heuristics](https://arxiv.org/abs/2112.11446) to filter
    for conversations from internet data to train their model Gopher. They claimed
    that their heuristics reliably yield high-quality dialogues. Specifically, they
    looked for texts that look like the following format:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind使用[简单启发式方法](https://arxiv.org/abs/2112.11446)从互联网数据中筛选对话以训练他们的模型Gopher。他们声称他们的启发式方法能够可靠地产生高质量的对话。具体来说，他们寻找类似以下格式的文本：
- en: '[PRE3]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To reduce their dependence on high-quality human annotated data, many teams
    are turning to AI-generated data. Synthetic data is discussed in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少对高质量人工标注数据的依赖，许多团队正在转向使用AI生成数据。合成数据在[第8章](ch08.html#ch08_dataset_engineering_1730130932019888)中进行了讨论。
- en: Technically, you can train a model from scratch on the demonstration data instead
    of finetuning a pre-trained model, effectively eliminating the self-supervised
    pre-training step. However, the pre-training approach often has returned superior
    results.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，您可以从头开始在演示数据上训练模型，而不是微调预训练模型，从而有效地消除了自监督预训练步骤。然而，预训练方法通常能返回更优越的结果。
- en: Preference Finetuning
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏好微调
- en: With great power comes great responsibilities. A model that can assist users
    in achieving great things can also assist users in achieving terrible things.
    Demonstration data teaches the model to have a conversation but doesn’t teach
    the model what kind of conversations it should have. For example, if a user asks
    the model to write an essay about why one race is inferior or how to hijack a
    plane, should the model comply?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 权力越大，责任越大。一个可以帮助用户实现伟大事物的模型，也可能帮助用户实现可怕的事情。演示数据教会模型进行对话，但并没有教会模型应该进行什么样的对话。例如，如果用户要求模型写一篇关于为什么一个种族劣等或如何劫机的文章，模型应该遵守吗？
- en: In both of the preceding examples, it’s straightforward to most people what
    a model should do. However, many scenarios aren’t as clear-cut. People from different
    cultural, political, socioeconomic, gender, and religious backgrounds disagree
    with each other all the time. How should AI respond to questions about abortion,
    gun control, the Israel–Palestine conflict, disciplining children, marijuana legality,
    universal basic income, or immigration? How do we define and detect potentially
    controversial issues? If your model responds to a controversial issue, whatever
    the responses, you’ll end up upsetting some of your users. If a model is censored
    too much, your model [may become boring](https://oreil.ly/5oSEJ), [driving away
    users](https://oreil.ly/D1S6y).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个例子中，大多数人很容易理解模型应该做什么。然而，许多场景并不那么清晰。来自不同文化、政治、社会经济、性别和宗教背景的人经常意见不合。AI 应该如何回应关于堕胎、枪支控制、以色列-巴勒斯坦冲突、管教孩子、大麻合法化、普遍基本收入或移民的问题？我们如何定义和检测可能具有争议的问题？如果你的模型对具有争议的问题做出回应，无论回应如何，你都会让一些用户感到不满。如果一个模型被过度审查，你的模型[可能会变得无聊](https://oreil.ly/5oSEJ)，[驱赶用户](https://oreil.ly/D1S6y)。
- en: Fear of AI models generating inappropriate responses can stop companies from
    releasing their applications to users. The goal of preference finetuning is to
    get AI models to behave according to human preference.^([23](ch02.html#id797))
    This is an ambitious, if not impossible, goal. Not only does this assume that
    universal human preference exists, but it also assumes that it’s possible to embed
    it into AI.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 担心 AI 模型生成不适当的响应可能会阻止公司向用户发布他们的应用程序。偏好微调的目标是让 AI 模型按照人类的偏好行事。[23](ch02.html#id797)
    这是一个雄心勃勃，如果不是不可能实现的目标。这不仅假设普遍的人类偏好存在，而且还假设可以将它嵌入到 AI 中。
- en: 'Had the goal been simple, the solution could’ve been elegant. However, given
    the ambitious nature of the goal, the solution we have today is complicated. The
    earliest successful preference finetuning algorithm, which is still popular today,
    is RLHF. RLHF consists of two parts:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标很简单，解决方案可能会很优雅。然而，鉴于目标的雄心勃勃性质，我们今天拥有的解决方案是复杂的。最早成功的偏好微调算法，至今仍很受欢迎，是 RLHF。RLHF
    由两部分组成：
- en: Train a reward model that scores the foundation model’s outputs.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个评分基础模型输出的奖励模型。
- en: Optimize the foundation model to generate responses for which the reward model
    will give maximal scores.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化基础模型以生成奖励模型会给出最高分的响应。
- en: While RLHF is still used today, newer approaches like DPO ([Rafailov et al.,
    2023](https://arxiv.org/abs/2305.18290)) are gaining traction. For example, Meta
    switched from RLHF for Llama 2 to DPO for Llama 3 to reduce complexity. I won’t
    be able to cover all the different approaches in this book. I choose to feature
    RLHF instead of DPO here because RLHF, while more complex than DPO, provides more
    flexibility to tweak the model. Llama 2’s authors posited that “the superior writing
    abilities of LLMs, as manifested in surpassing human annotators in certain tasks,
    are fundamentally driven by RLHF” ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288)).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RLHF 仍然在今天被使用，但像 DPO ([Rafailov 等人，2023](https://arxiv.org/abs/2305.18290))
    这样的新方法正在获得关注。例如，Meta 从 Llama 2 的 RLHF 转向 Llama 3 的 DPO 以减少复杂性。我无法在这本书中涵盖所有不同的方法。我选择在这里突出
    RLHF 而不是 DPO，因为虽然 RLHF 比DPO更复杂，但它为调整模型提供了更多的灵活性。Llama 2 的作者提出，“LLM 在某些任务中超越人类标注者的卓越写作能力，其根本驱动力是
    RLHF” ([Touvron 等人，2023](https://arxiv.org/abs/2307.09288))。
- en: Reward model
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励模型
- en: RLHF relies on a reward model. Given a pair of (prompt, response), the *reward
    model* outputs a score for how good the response is. Training a model to score
    a given input is a common ML task. The challenge, similar to that of SFT, is to
    obtain reliable data. If we ask labelers to score each response directly, the
    scores will vary. For the same sample, on a 10-point scale, one labeler might
    give a 5 and another 7\. Even the same labeler, given the same pair of (prompt,
    response) twice, might give different scores. Evaluating each sample independently
    is also called *pointwise* evaluation.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 依赖于一个奖励模型。给定一对（提示，响应），*奖励模型*会输出一个评分，表示响应有多好。训练一个模型对给定输入进行评分是常见的机器学习任务。挑战，与
    SFT 类似，是获取可靠的数据。如果我们要求标注者直接对每个响应进行评分，评分将会不同。对于相同的样本，在10分制下，一个标注者可能会给出5分，而另一个可能会给出7分。即使是同一个标注者，在两次给出相同的（提示，响应）对时，也可能给出不同的评分。独立评估每个样本也称为*点评分*。
- en: An easier task is to ask labelers to compare two responses and decide which
    one is better. For each prompt, multiple responses are generated by either humans
    or AI. The resulting labeled data is *comparison data*, which follows the format
    (prompt, winning_response, losing_response). [Table 2-7](#ch02_table_7_1730147895537589)
    shows an example of comparison data used by Anthropic for one of their models.
    Of the two responses in this example, I prefer the response labeled as the losing
    one. This highlights the challenge of trying to capture diverse human preferences
    in a single mathematical formulation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更简单的任务是要求标注员比较两个响应并决定哪个更好。对于每个提示，人类或 AI 会生成多个响应。生成的标记数据是 *比较数据*，其格式为（提示，获胜响应，失败响应）。[表
    2-7](#ch02_table_7_1730147895537589) 展示了 Anthropic 用来训练其模型之一的比较数据示例。在这个例子中的两个响应中，我更喜欢被标记为失败的那个响应。这突显了试图在单一数学公式中捕捉多样的人类偏好的挑战。
- en: Table 2-7\. An example of comparison data from [Anthropic](https://oreil.ly/h9oG6)’s
    HH-RLHF dataset.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-7\. 来自 [Anthropic](https://oreil.ly/h9oG6) 的 HH-RLHF 数据集的比较数据示例。
- en: '| prompt | winning_response | losing_response |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 获胜响应 | 失败响应 |'
- en: '| --- | --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `How can I get my dog high?` | `I’m not sure what you` `mean by that.` |
    `I don’t know that we should get the dog high. I think it’s important for a dog
    to experience the world in a sober state of mind.` |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| `如何让我的狗嗨起来？` | `我不确定你` `指的是什么。` | `我不知道我们应该让狗嗨起来。我认为对狗来说，以清醒的心态体验世界是很重要的。`
    |'
- en: Still, this easier task of comparing two responses takes time. LMSYS (the Large
    Model Systems Organization), an open research organization, found that manually
    comparing two responses took on average three to five minutes, as the process
    requires fact-checking each response ([Chiang et al., 2024](https://arxiv.org/abs/2403.04132)).
    In a talk with my Discord community, Llama-2 author [Thomas Scialom](https://oreil.ly/P1MPQ)
    shared that each comparison cost them $3.50\. This is still much cheaper than
    writing responses, which cost $25 each.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个比较两个响应的简单任务也需要时间。LMSYS（大型模型系统组织），一个开放的研究组织，发现手动比较两个响应平均需要三到五分钟，因为这个过程需要核实每个响应
    ([Chiang 等人，2024](https://arxiv.org/abs/2403.04132))。在我的 Discord 社区的一次谈话中，Llama-2
    作者 [Thomas Scialom](https://oreil.ly/P1MPQ) 分享说，每次比较的成本是 3.50 美元。这仍然比写响应便宜得多，每个响应的成本是
    25 美元。
- en: '[Figure 2-13](#ch02_figure_13_1730147895520928) shows the [UI that OpenAI’s
    labelers](https://oreil.ly/kYtBG) used to create comparison data for the reward
    model of InstructGPT. Labelers give concrete scores from 1 to 7 as well as rank
    the responses in the order of their preference, but only the ranking is used to
    train the reward model. Their inter-labeler agreement is around 73%, which means
    if they ask 10 people to rank the same two responses, approximately 7 of them
    will have the same ranking. To speed up the labeling process, each annotator can
    rank multiple responses at the same time. A set of three ranked responses (A >
    B > C) will produce three ranked pairs: (A > B), (A > C), and (B > C).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-13](#ch02_figure_13_1730147895520928) 展示了 OpenAI 的标注员使用的 [UI](https://oreil.ly/kYtBG)
    来创建 InstructGPT 奖励模型的比较数据。标注员会给出从 1 到 7 的具体分数，并按偏好顺序对响应进行排名，但只有排名被用于训练奖励模型。标注员之间的相互一致性约为
    73%，这意味着如果他们让 10 个人对相同的两个响应进行排名，大约有 7 个人会有相同的排名。为了加快标注过程，每个标注员可以同时对多个响应进行排名。一组三个排名的响应（A
    > B > C）将产生三个排名对：(A > B)、(A > C) 和 (B > C)。'
- en: '![Screenshot of a screenshot of a computer screen  Description automatically
    generated](assets/aien_0213.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图的截图截图  描述自动生成](assets/aien_0213.png)'
- en: Figure 2-13\. The interface labelers used to generate comparison data for OpenAI’s
    InstructGPT.
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 标注员用来为 OpenAI 的 InstructGPT 生成比较数据的界面。
- en: 'Given only comparison data, how do we train the model to give concrete scores?
    Similar to how you can get humans to do basically anything with the right incentive,
    you can get a model to do so given the right objective function. A commonly used
    function represents the difference in output scores for the winning and losing
    response. The objective is to maximize this difference. For those interested in
    the mathematical details, here is the formula used by [InstructGPT](https://arxiv.org/abs/2203.02155):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 仅给定比较数据，我们如何训练模型给出具体的分数？类似于你可以通过适当的激励让人类做任何事情，你也可以通过适当的目标函数让模型这样做。常用的函数表示获胜响应和失败响应输出分数的差异。目标是最大化这个差异。对于那些对数学细节感兴趣的人，以下是
    [InstructGPT](https://arxiv.org/abs/2203.02155) 使用的公式：
- en: '$r Subscript theta$ : the reward model being trained, parameterized by θ. The
    goal of the training process is to find θ for which the loss is minimized.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$r Subscript theta$ : 正在训练的奖励模型，由θ参数化。训练过程的目的是找到使损失最小化的θ。'
- en: 'Training data format:'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据格式：
- en: '$x$ : prompt'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$x$ : 提示'
- en: '$y Subscript w$ : winning response'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$y Subscript w$ : 胜利响应'
- en: '$y Subscript l$ : losing response'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$y Subscript l$ : 失效响应'
- en: '$s Subscript w Baseline equals r left-parenthesis x comma y Subscript w Baseline
    right-parenthesis$ : reward model’s scalar score for the winning response'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$s Subscript w Baseline equals r left-parenthesis x comma y Subscript w Baseline
    right-parenthesis$ : 奖励模型对胜利响应的标量评分'
- en: '$s Subscript l Baseline equals r left-parenthesis x comma y Subscript l Baseline
    right-parenthesis$ : reward model’s scalar score for the losing response'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$s Subscript l Baseline equals r left-parenthesis x comma y Subscript l Baseline
    right-parenthesis$ : 奖励模型对失效响应的标量评分'
- en: '$sigma$ : the sigmoid function'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '$sigma$ : Sigmoid函数'
- en: 'For each training sample $left-parenthesis x comma y Subscript w Baseline comma
    y Subscript l Baseline right-parenthesis$ , the loss value is computed as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个训练样本 $left-parenthesis x comma y Subscript w Baseline comma y Subscript
    l Baseline right-parenthesis$ ，损失值计算如下：
- en: $log left-parenthesis sigma left-parenthesis r Subscript theta Baseline left-parenthesis
    x comma y Subscript w Baseline right-parenthesis minus r Subscript theta Baseline
    left-parenthesis x comma y Subscript l Baseline right-parenthesis right-parenthesis$
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $log left-parenthesis sigma left-parenthesis r Subscript theta Baseline left-parenthesis
    x comma y Subscript w Baseline right-parenthesis minus r Subscript theta Baseline
    left-parenthesis x comma y Subscript l Baseline right-parenthesis right-parenthesis$
- en: 'Goal: find $theta$ to minimize the expected loss for all training samples.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标：找到 $theta$ 以最小化所有训练样本的预期损失。
- en: $minus upper E Subscript x Baseline log left-parenthesis sigma left-parenthesis
    r Subscript theta Baseline left-parenthesis x comma y Subscript w Baseline right-parenthesis
    minus r Subscript theta Baseline left-parenthesis x comma y Subscript l Baseline
    right-parenthesis right-parenthesis$
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $minus upper E Subscript x Baseline log left-parenthesis sigma left-parenthesis
    r Subscript theta Baseline left-parenthesis x comma y Subscript w Baseline right-parenthesis
    minus r Subscript theta Baseline left-parenthesis x comma y Subscript l Baseline
    right-parenthesis right-parenthesis$
- en: The reward model can be trained from scratch or finetuned on top of another
    model, such as the pre-trained or SFT model. Finetuning on top of the strongest
    foundation model seems to give the best performance. Some people believe that
    the reward model should be at least as powerful as the foundation model to be
    able to score the foundation model’s responses. However, as we’ll see in the [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067)
    on evaluation, a weak model can judge a stronger model, as judging is believed
    to be easier than generation.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型可以从头开始训练，也可以在另一个模型之上进行微调，例如预训练或SFT模型。在最强基础模型之上进行微调似乎能给出最佳性能。有些人认为，奖励模型应该至少与基础模型一样强大，才能对基础模型的响应进行评分。然而，正如我们在[第3章](ch03.html#ch03a_evaluation_methodology_1730150757064067)的评估中将会看到的，一个弱模型可以评判一个更强的模型，因为人们认为评判比生成更容易。
- en: Finetuning using the reward model
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用奖励模型进行微调
- en: With the trained RM, we further train the SFT model to generate output responses
    that will maximize the scores by the reward model. During this process, prompts
    are randomly selected from a distribution of prompts, such as existing user prompts.
    These prompts are input into the model, whose responses are scored by the reward
    model. This training process is often done with [proximal policy optimization
    (PPO)](https://oreil.ly/TpaGg), a reinforcement learning algorithm released by
    OpenAI in 2017.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的RM，我们进一步训练SFT模型以生成输出响应，这些响应将通过奖励模型最大化得分。在这个过程中，提示从提示分布中随机选择，例如现有用户提示。这些提示被输入到模型中，其响应由奖励模型评分。这个训练过程通常使用[近端策略优化（PPO）](https://oreil.ly/TpaGg)，这是一种由OpenAI在2017年发布的强化学习算法。
- en: Empirically, RLHF and DPO both improve performance compared to SFT alone. However,
    as of this writing, there are debates on why they work. As the field evolves,
    I suspect that preference finetuning will change significantly in the future.
    If you’re interested in learning more about RLHF and preference finetuning, check
    out the [book’s GitHub repository](https://github.com/chiphuyen/aie-book).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 经验上，RLHF和DPO都比单独使用SFT提高了性能。然而，截至本文写作时，关于为什么它们有效存在争议。随着该领域的演变，我怀疑偏好微调在将来会有显著变化。如果你对学习更多关于RLHF和偏好微调感兴趣，请查看[书籍的GitHub仓库](https://github.com/chiphuyen/aie-book)。
- en: Both SFT and preference finetuning are steps taken to address the problem created
    by the low quality of data used for pre-training. If one day we have better pre-training
    data or better ways to train foundation models, we might not need SFT and preference
    at all.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: SFT（监督式微调）和偏好微调都是为解决预训练数据质量低所造成的问题而采取的步骤。如果有一天我们有了更好的预训练数据或更好的训练基础模型的方法，我们可能根本不需要SFT和偏好。
- en: Some companies find it okay to skip reinforcement learning altogether. For example,
    [Stitch Fix](https://oreil.ly/iYh-B) and [Grab](https://oreil.ly/CSSed) find that
    having the reward model alone is good enough for their applications. They get
    their models to generate multiple outputs and pick the ones given high scores
    by their reward models. This approach, often referred to as the *best of N* strategy,
    leverages how a model samples outputs to improve its performance. The next section
    will shed light on how best of N works.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一些公司发现完全跳过强化学习是可以接受的。例如，[Stitch Fix](https://oreil.ly/iYh-B) 和 [Grab](https://oreil.ly/CSSed)
    发现，对于他们的应用来说，仅仅拥有奖励模型就足够了。他们让模型生成多个输出，并选择那些由他们的奖励模型给出高分的结果。这种方法通常被称为*N中最佳*策略，它利用了模型抽样输出的方式来提高其性能。下一节将阐明N中最佳策略是如何工作的。
- en: Sampling
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽样
- en: A model constructs its outputs through a process known as *sampling*. This section
    discusses different sampling strategies and *sampling variables,* including temperature,
    top-k, and top-p. It’ll then explore how to sample multiple outputs to improve
    a model’s performance. We’ll also see how the sampling process can be modified
    to get models to generate responses that follow certain formats and constraints.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过称为*抽样*的过程来构建其输出。本节讨论了不同的抽样策略和*抽样变量*，包括温度、top-k和top-p。然后，它将探讨如何抽样多个输出以提高模型的表现。我们还将看到如何修改抽样过程，以使模型生成遵循特定格式和约束的响应。
- en: Sampling makes AI’s outputs probabilistic. Understanding this probabilistic
    nature is important for handling AI’s behaviors, such as inconsistency and hallucination.
    This section ends with a deep dive into what this probabilistic nature means and
    how to work with it.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样使人工智能的输出具有概率性。理解这种概率性质对于处理人工智能的行为，如不一致性和幻觉，非常重要。本节以深入探讨这种概率性质及其如何与之合作结束。
- en: Sampling Fundamentals
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽样基础
- en: 'Given an input, a neural network produces an output by first computing the
    probabilities of possible outcomes. For a classification model, possible outcomes
    are the available classes. As an example, if a model is trained to classify whether
    an email is spam or not, there are only two possible outcomes: spam and not spam.
    The model computes the probability of each of these two outcomes—e.g., the probability
    of the email being spam is 90%, and not spam is 10%. You can then make decisions
    based on these output probabilities. For example, if you decide that any email
    with a spam probability higher than 50% should be marked as spam, an email with
    a 90% spam probability will be marked as spam.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入，神经网络通过首先计算可能结果的概率来产生一个输出。对于一个分类模型，可能的结果是可用的类别。例如，如果一个模型被训练来分类一封电子邮件是否为垃圾邮件，那么只有两种可能的结果：垃圾邮件和非垃圾邮件。模型计算这两种结果中每一种的概率——例如，电子邮件是垃圾邮件的概率是90%，而非垃圾邮件的概率是10%。然后，你可以根据这些输出概率做出决策。例如，如果你决定任何垃圾邮件概率高于50%的电子邮件都应该被标记为垃圾邮件，那么垃圾邮件概率为90%的电子邮件将被标记为垃圾邮件。
- en: For a language model, to generate the next token, the model first computes the
    probability distribution over all tokens in the vocabulary, which looks like [Figure 2-14](#ch02_figure_14_1730147895520937).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型来说，为了生成下一个标记，模型首先计算词汇表中所有标记的概率分布，这看起来就像[图2-14](#ch02_figure_14_1730147895520937)。
- en: '![A diagram of a color  Description automatically generated](assets/aien_0214.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![颜色图示的描述自动生成](assets/aien_0214.png)'
- en: Figure 2-14\. To generate the next token, the language model first computes
    the probability distribution over all tokens in the vocabulary.
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14。为了生成下一个标记，语言模型首先计算词汇表中所有标记的概率分布。
- en: When working with possible outcomes of different probabilities, a common strategy
    is to pick the outcome with the highest probability. Always picking the most likely
    outcome = is called *greedy sampling*. This often works for classification tasks.
    For example, if the model thinks that an email is more likely to be spam than
    not spam, it makes sense to mark it as spam. However, for a language model, greedy
    sampling creates boring outputs. Imagine a model that, for whatever question you
    ask, always responds with the most common words.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理不同概率的可能结果时，一种常见的策略是选择概率最高的结果。总是选择最有可能的结果=称为*贪婪采样*。这通常适用于分类任务。例如，如果模型认为一封电子邮件更有可能是垃圾邮件而不是非垃圾邮件，那么将其标记为垃圾邮件是有意义的。然而，对于语言模型，贪婪采样会产生无聊的输出。想象一下，无论你问什么问题，模型总是用最常见的词来回答。
- en: Instead of always picking the next most likely token, the model can sample the
    next token according to the probability distribution over all possible values.
    Given the context of “My favorite color is …” as shown in [Figure 2-14](#ch02_figure_14_1730147895520937),
    if “red” has a 30% chance of being the next token and “green” has a 50% chance,
    “red” will be picked 30% of the time, and “green” 50% of the time.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 与总是选择下一个最有可能的标记不同，模型可以根据所有可能值的概率分布来采样下一个标记。以“我的最喜欢的颜色是……”的上下文为例，如[图2-14](#ch02_figure_14_1730147895520937)所示，如果“红色”有30%的概率成为下一个标记，而“绿色”有50%的概率，那么“红色”将有30%的时间被选中，“绿色”有50%的时间被选中。
- en: How does a model compute these probabilities? Given an input, a neural network
    outputs a logit vector. Each *logit* corresponds to one possible value. In the
    case of a language model, each logit corresponds to one token in the model’s vocabulary.
    The logit vector size is the size of the vocabulary. A visualization of the logits
    vector is shown in [Figure 2-15](#ch02_figure_15_1730147895520946).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是如何计算这些概率的？给定一个输入，神经网络输出一个logit向量。每个*logit*对应一个可能的值。在语言模型的情况下，每个logit对应模型词汇表中的一个标记。logit向量的大小是词汇表的大小。logit向量的可视化如[图2-15](#ch02_figure_15_1730147895520946)所示。
- en: '![A diagram of a network  Description automatically generated](assets/aien_0215.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![网络图，描述自动生成](assets/aien_0215.png)'
- en: Figure 2-15\. For each input, a language model produces a logit vector. Each
    logit corresponds to a token in the vocabulary.
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15。对于每个输入，语言模型产生一个logit向量。每个logit对应词汇表中的一个标记。
- en: 'While larger logits correspond to higher probabilities, logits don’t represent
    probabilities. Logits don’t sum up to one. Logits can even be negative, while
    probabilities have to be non-negative. To convert logits to probabilities, a softmax
    layer is often used. Let’s say the model has a vocabulary of N and the logit vector
    is $left-bracket x 1 comma x 2 comma period period period comma x Subscript upper
    N Baseline right-bracket$ The probability for the *i^(th)* token, $p Subscript
    i$ is computed as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然较大的logit对应较高的概率，但logit并不代表概率。logit的总和并不等于一。logit甚至可以是负数，而概率必须是非负的。为了将logit转换为概率，通常会使用softmax层。假设模型有N个词汇，logit向量为$left-bracket
    x 1 comma x 2 comma period period period comma x Subscript upper N Baseline right-bracket$，那么第*i^(th)*个标记的概率$p
    Subscript i$计算如下：
- en: $p Subscript i Baseline equals softmax left-parenthesis x Subscript i Baseline
    right-parenthesis equals StartFraction e Superscript x Super Subscript i Superscript
    Baseline Over sigma-summation Underscript j Endscripts e Superscript x Super Subscript
    j Superscript Baseline EndFraction$
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: $p Subscript i Baseline equals softmax left-parenthesis x Subscript i Baseline
    right-parenthesis equals StartFraction e Superscript x Super Subscript i Superscript
    Baseline Over sigma-summation Underscript j Endscripts e Superscript x Super Subscript
    j Superscript Baseline EndFraction$
- en: Sampling Strategies
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样策略
- en: The right sampling strategy can make a model generate responses more suitable
    for your application. For example, one sampling strategy can make the model generate
    more creative responses, whereas another strategy can make its generations more
    predictable. Many different sample strategies have been introduced to nudge models
    toward responses with specific attributes. You can also design your own sampling
    strategy, though this typically requires access to the model’s logits. Let’s go
    over a few common sampling strategies to see how they work.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的采样策略可以使模型生成的响应更适合你的应用。例如，一种采样策略可以使模型生成更具创造性的响应，而另一种策略可以使其生成更可预测的响应。已经引入了许多不同的采样策略，以引导模型生成具有特定属性的响应。你也可以设计自己的采样策略，尽管这通常需要访问模型的logit。让我们回顾一些常见的采样策略，看看它们是如何工作的。
- en: Temperature
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 温度
- en: 'One problem with sampling the next token according to the probability distribution
    is that the model can be less creative. In the previous example, common colors
    like “red”, “green”, “purple”, and so on have the highest probabilities. The language
    model’s answer ends up sounding like that of a five-year-old: “My favorite color
    is green”. Because “the” has a low probability, the model has a low chance of
    generating a creative sentence such as “My favorite color is the color of a still
    lake on a spring morning”.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 根据概率分布采样下一个标记的问题在于，模型可能缺乏创造力。在先前的例子中，像“红色”、“绿色”、“紫色”等常见颜色的概率最高。语言模型的回答听起来就像一个五岁孩子的：
    “我最喜欢的颜色是绿色”。因为“the”的概率很低，模型生成像“我最喜欢的颜色是春天早晨静止的湖水的颜色”这样的创意句子可能性很低。
- en: To redistribute the probabilities of the possible values, you can sample with
    a *temperature*. Intuitively, a higher temperature reduces the probabilities of
    common tokens, and as a result, increases the probabilities of rarer tokens. This
    enables models to create more creative responses.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重新分配可能值的概率，你可以使用*温度*进行采样。直观上，较高的温度会降低常见标记的概率，从而增加罕见标记的概率。这使得模型能够生成更具创造性的回答。
- en: Temperature is a constant used to adjust the logits before the softmax transformation.
    Logits are divided by temperature. For a given temperature *T*, the adjusted logit
    for the *i^(th)* token is $StartFraction x Subscript i Baseline Over upper T EndFraction$
    . Softmax is then applied on this adjusted logit instead of on $x Subscript i$
    .
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 温度是一个用于调整softmax变换前logit的常数。logit被除以温度。对于给定的温度*T*，第*i*个标记的调整logit是$StartFraction
    x Subscript i Baseline Over upper T EndFraction$。然后在这个调整后的logit上应用softmax，而不是在$x
    Subscript i$上。
- en: 'Let’s walk through a simple example to examine the effect of temperature on
    probabilities. Imagine that we have a model that has only two possible outputs:
    A and B. The logits computed from the last layer are [1, 2]. The logit for A is
    1 and B is 2.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来考察温度对概率的影响。假设我们有一个只有两个可能输出的模型：A和B。从最后一层计算出的logit是[1, 2]。A的logit是1，B的logit是2。
- en: Without using temperature, which is equivalent to using the temperature of 1,
    the softmax probabilities are [0.27, 0.73]. The model picks B 73% of the time.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用温度，即使用温度为1，softmax概率为[0.27, 0.73]。模型有73%的概率选择B。
- en: With temperature = 0.5, the probabilities are [0.12, 0.88]. The model now picks
    B 88% of the time.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 当温度为0.5时，概率为[0.12, 0.88]。模型现在有88%的概率选择B。
- en: The higher the temperature, the less likely it is that the model is going to
    pick the most obvious value (the value with the highest logit), making the model’s
    outputs more creative but potentially less coherent. The lower the temperature,
    the more likely it is that the model is going to pick the most obvious value,
    making the model’s output more consistent but potentially more boring.^([24](ch02.html#id816))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 温度越高，模型选择最明显值（具有最高logit的值）的可能性就越低，这使得模型的输出更具创意，但可能不太连贯。温度越低，模型选择最明显值的可能性就越高，这使得模型的输出更一致，但可能更无聊。[24](ch02.html#id816)
- en: '[Figure 2-16](#ch02_figure_16_1730147895520958) shows the softmax probabilities
    for tokens A and B at different temperatures. As the temperature gets closer to
    0, the probability that the model picks token B becomes closer to 1\. In our example,
    for a temperature below 0.1, the model almost always outputs B. As the temperature
    increases, the probability that token A is picked increases while the probability
    that token B is picked decreases. Model providers typically limit the temperature
    to be between 0 and 2\. If you own your model, you can use any non-negative temperature.
    A temperature of 0.7 is often recommended for creative use cases, as it balances
    creativity and predictability, but you should experiment and find the temperature
    that works best for you.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-16](#ch02_figure_16_1730147895520958)展示了不同温度下标记A和B的softmax概率。随着温度接近0，模型选择标记B的概率越来越接近1。在我们的例子中，对于温度低于0.1的情况，模型几乎总是输出B。随着温度的升高，选择标记A的概率增加，而选择标记B的概率降低。模型提供商通常将温度限制在0到2之间。如果你拥有自己的模型，你可以使用任何非负温度。对于创意用例，通常建议使用0.7的温度，因为它在创造力和可预测性之间取得平衡，但你应该进行实验以找到最适合你的温度。'
- en: '![A graph with a line  Description automatically generated](assets/aien_0216.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![带有线条的图表，描述自动生成](assets/aien_0216.png)'
- en: Figure 2-16\. The softmax probabilities for tokens A and B at different temperatures,
    given their logits being [1, 2]. Without setting the temperature value, which
    is equivalent to using the temperature of 1, the softmax probability of B would
    be 73%.
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-16\. 在给定其logits为[1, 2]的情况下，不同温度下标记A和B的softmax概率。如果没有设置温度值，相当于使用温度为1，B的softmax概率将是73%。
- en: It’s common practice to set the temperature to 0 for the model’s outputs to
    be more consistent. Technically, temperature can never be 0—logits can’t be divided
    by 0\. In practice, when we set the temperature to 0, the model just picks the
    token with the largest logit,^([25](ch02.html#id817)) without doing logit adjustment
    and softmax calculation.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 将温度设置为0以使模型的输出更一致是一种常见做法。技术上，温度永远不会是0——logits不能除以0。在实践中，当我们设置温度为0时，模型只是选择具有最大logits的标记，^([25](ch02.html#id817))而不进行logits调整和softmax计算。
- en: Tip
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A common debugging technique when working with an AI model is to look at the
    probabilities this model computes for given inputs. For example, if the probabilities
    look random, the model hasn’t learned much.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用AI模型时，一个常见的调试技术是查看模型对给定输入计算的概率。例如，如果概率看起来是随机的，那么模型可能没有学到很多东西。
- en: Many model providers return probabilities generated by their models as [logprobs](https://oreil.ly/VAUl6).
    *Logprobs*, short for *log probabilities*, are probabilities in the log scale.
    Log scale is preferred when working with a neural network’s probabilities because
    it helps reduce the [underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow)
    problem.^([26](ch02.html#id819)) A language model might be working with a vocabulary
    size of 100,000, which means the probabilities for many of the tokens can be too
    small to be represented by a machine. The small numbers might be rounded down
    to 0\. Log scale helps reduce this problem.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型提供者返回由其模型生成的[logprobs](https://oreil.ly/VAUl6)。*Logprobs*，即*对数概率*，是对数尺度上的概率。在对神经网络概率进行操作时，对数尺度更受欢迎，因为它有助于减少[下溢](https://en.wikipedia.org/wiki/Arithmetic_underflow)问题.^([26](ch02.html#id819))
    语言模型可能正在处理一个包含100,000个词汇量的词汇表，这意味着许多标记的概率可能太小，无法由机器表示。这些小数可能被四舍五入到0。对数尺度有助于减少这个问题。
- en: '[Figure 2-17](#ch02_figure_17_1730147895520965) shows the workflow of how logits,
    probabilities, and logprobs are computed.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-17](#ch02_figure_17_1730147895520965)展示了如何计算logits、概率和logprobs的工作流程。'
- en: '![A diagram of a softmax  Description automatically generated](assets/aien_0217.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![一个softmax的图解  描述自动生成](assets/aien_0217.png)'
- en: Figure 2-17\. How logits, probabilities, and logprobs are computed.
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-17\. logits、概率和logprobs是如何计算的。
- en: As you’ll see throughout the book, logprobs are useful for building applications
    (especially for classification), evaluating applications, and understanding how
    models work under the hood. However, as of this writing, many model providers
    don’t expose their models’ logprobs, or if they do, the logprobs API is limited.^([27](ch02.html#id820))
    The limited logprobs API is likely due to security reasons as a model’s exposed
    logprobs make it easier for others to replicate the model.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书的其余部分将看到的，logprobs对于构建应用程序（尤其是分类）非常有用，用于评估应用程序，以及理解模型在底层是如何工作的。然而，截至本书写作时，许多模型提供者没有公开其模型的logprobs，或者即使公开，logprobs
    API也是有限的.^([27](ch02.html#id820)) 有限的logprobs API可能由于安全原因，因为模型暴露的logprobs使得其他人更容易复制模型。
- en: Top-k
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Top-k
- en: '*Top-k* is a sampling strategy to reduce the computation workload without sacrificing
    too much of the model’s response diversity. Recall that a softmax layer is used
    to compute the probability distribution over all possible values. Softmax requires
    two passes over all possible values: one to perform the exponential sum $sigma-summation
    Underscript j Endscripts e Superscript x Super Subscript j$ , and one to perform
    $StartFraction e Superscript x Super Subscript i Superscript Baseline Over sigma-summation
    Underscript j Endscripts e Superscript x Super Subscript j Superscript Baseline
    EndFraction$ for each value. For a language model with a large vocabulary, this
    process is computationally expensive.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '*Top-k*是一种采样策略，旨在在不牺牲太多模型响应多样性的情况下减少计算工作量。回想一下，softmax层用于计算所有可能值的概率分布。softmax需要对所有可能值进行两次遍历：一次执行指数和sigma-summation
    Underscript j Endscripts e Superscript x Super Subscript j，另一次对每个值执行$StartFraction
    e Superscript x Super Subscript i Superscript Baseline Over sigma-summation Underscript
    j Endscripts e Superscript x Super Subscript j Superscript Baseline EndFraction$。对于具有大型词汇表的语言模型，这个过程计算量很大。'
- en: To avoid this problem, after the model has computed the logits, we pick the
    top-k logits and perform softmax over these top-k logits only. Depending on how
    diverse you want your application to be, k can be anywhere from 50 to 500—much
    smaller than a model’s vocabulary size. The model then samples from these top
    values. A smaller k value makes the text more predictable but less interesting,
    as the model is limited to a smaller set of likely words.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，在模型计算完logits之后，我们选择top-k logits，并对这些top-k logits进行softmax操作。根据你希望你的应用有多多样化，k的值可以从50到500不等——远小于模型词汇量的大小。然后模型从这些top值中进行采样。较小的k值会使文本更加可预测，但不太有趣，因为模型被限制在更小的可能词集内。
- en: Top-p
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Top-p
- en: 'In top-k sampling, the number of values considered is fixed to k. However,
    this number should change depending on the situation. For example, given the prompt
    “Do you like music? Answer with only yes or no.” the number of values considered
    should be two: yes and no. Given the prompt “What’s the meaning of life?” the
    number of values considered should be much larger.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在top-k采样中，考虑的值的数量固定为k。然而，这个数字应根据情况变化。例如，对于提示“你喜欢音乐吗？只用是或否回答。”考虑的值的数量应该是两个：是和否。对于提示“生活的意义是什么？”考虑的值的数量应该大得多。
- en: '*Top-p*, also known as *nucleus sampling*, allows for a more dynamic selection
    of values to be sampled from. In top-p sampling, the model sums the probabilities
    of the most likely next values in descending order and stops when the sum reaches
    p. Only the values within this cumulative probability are considered. Common values
    for top-p (nucleus) sampling in language models typically range from 0.9 to 0.95\.
    A top-p value of 0.9, for example, means that the model will consider the smallest
    set of values whose cumulative probability exceeds 90%.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '*Top-p*，也称为*nucleus sampling*，允许从更动态的值集中进行采样。在top-p采样中，模型按降序对最可能出现的下一个值的概率进行求和，并在求和达到p时停止。只有在这个累积概率内的值会被考虑。在语言模型中，top-p（nucleus）采样的常见值通常在0.9到0.95之间。例如，top-p值为0.9意味着模型将考虑累积概率超过90%的最小值集。'
- en: Let’s say the probabilities of all tokens are as shown in [Figure 2-18](#ch02_figure_18_1730147895520971).
    If top-p is 90%, only “yes” and “maybe” will be considered, as their cumulative
    probability is greater than 90%. If top-p is 99%, then “yes”, “maybe”, and “no”
    are considered.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有标记的概率如[图2-18](#ch02_figure_18_1730147895520971)所示。如果top-p是90%，则只考虑“是”和“也许”，因为它们的累积概率大于90%。如果top-p是99%，则考虑“是”、“也许”和“否”。
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_0218.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](assets/aien_0218.png)'
- en: Figure 2-18\. Example token probabilities.
  id: totrans-380
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-18\. 示例标记概率。
- en: Unlike top-k, top-p doesn’t necessarily reduce the softmax computation load.
    Its benefit is that because it focuses only on the set of most relevant values
    for each context, it allows outputs to be more contextually appropriate. In theory,
    there don’t seem to be a lot of benefits to top-p sampling. However, in practice,
    top-p sampling has proven to work well, causing its popularity to rise.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 与top-k不同，top-p不一定能减少softmax的计算负担。它的好处是，因为它只关注每个上下文中最相关的值集，所以它允许输出更加符合上下文。从理论上讲，似乎没有太多top-p采样的好处。然而，在实践中，top-p采样已被证明效果良好，导致其受欢迎程度上升。
- en: A related sampling strategy is [min-p](https://github.com/huggingface/transformers/issues/27670),
    where you set the minimum probability that a token must reach to be considered
    during sampling.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一种相关的采样策略是[min-p](https://github.com/huggingface/transformers/issues/27670)，其中你设置一个标记在采样过程中必须达到的最小概率。
- en: Stopping condition
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止条件
- en: An autoregressive language model generates sequences of tokens by generating
    one token after another. A long output sequence takes more time, costs more compute
    (money),^([28](ch02.html#id829)) and can sometimes annoy users. We might want
    to set a condition for the model to stop the sequence.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归语言模型通过一个接一个地生成标记来生成标记序列。生成较长的输出序列需要更多的时间，计算成本（金钱）更高，^([28](ch02.html#id829))有时甚至会让用户感到烦恼。我们可能希望为模型设置一个停止序列的条件。
- en: One easy method is to ask models to stop generating after a fixed number of
    tokens. The downside is that the output is likely to be cut off mid-sentence.
    Another method is to use *stop tokens* or *stop words*. For example, you can ask
    a model to stop generating when it encounters the end-of-sequence token. Stopping
    conditions are helpful to keep latency and costs down.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是让模型在生成固定数量的标记后停止。缺点是输出很可能会在句子中间被截断。另一种方法是使用*停止标记*或*停止词*。例如，你可以要求模型在遇到序列结束标记时停止生成。停止条件有助于降低延迟和成本。
- en: The downside of early stopping is that if you want models to generate outputs
    in a certain format, premature stopping can cause outputs to be malformatted.
    For example, if you ask the model to generate JSON, early stopping can cause the
    output JSON to be missing things like closing brackets, making the generated JSON
    hard to parse.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止的缺点是，如果你希望模型以特定格式生成输出，提前停止可能会导致输出格式错误。例如，如果你要求模型生成JSON，提前停止可能会导致生成的JSON缺少关闭括号等，使得生成的JSON难以解析。
- en: Test Time Compute
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试时间计算
- en: The last section discussed how a model might sample the next token. This section
    discusses how a model might sample the whole output.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节讨论了模型如何采样下一个标记。本节讨论模型如何采样整个输出。
- en: 'One simple way to improve a model’s response quality is *test time compute*:
    instead of generating only one response per query, you generate multiple responses
    to increase the chance of good responses. One way to do test time compute is the
    best of N technique discussed earlier in this chapter—you randomly generate multiple
    outputs and pick one that works best. However, you can also be more strategic
    about how to generate multiple outputs. For example, instead of generating all
    outputs independently, which might include many less promising candidates, you
    can use [beam search](https://en.wikipedia.org/wiki/Beam_search) to generate a
    fixed number of most promising candidates (the beam) at each step of sequence
    generation.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 提高模型响应质量的一个简单方法是在测试时间进行计算：不是为每个查询只生成一个响应，而是生成多个响应以提高获得良好响应的机会。进行测试时间计算的一种方法是本章前面讨论过的N中最佳技术——你随机生成多个输出，并选择其中效果最好的一个。然而，你也可以更策略性地考虑如何生成多个输出。例如，你不必独立生成所有输出，这可能会包括许多不太有希望的候选者，你可以在序列生成的每个步骤中使用[束搜索](https://en.wikipedia.org/wiki/Beam_search)来生成固定数量的最有希望的候选者（束）。
- en: A simple strategy to increase the effectiveness of test time compute is to increase
    the diversity of the outputs, because a more diverse set of options is more likely
    to yield better candidates. If you use the same model to generate different options,
    it’s often a good practice to vary the model’s sampling variables to diversify
    its outputs.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 提高测试时间计算有效性的一个简单策略是增加输出的多样性，因为更多样化的选项集更有可能产生更好的候选者。如果你使用同一个模型生成不同的选项，通常的做法是改变模型的采样变量以多样化其输出。
- en: Although you can usually expect some model performance improvement by sampling
    multiple outputs, it’s expensive. On average, generating two outputs costs approximately
    twice as much as generating one.^([29](ch02.html#id833))
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过采样多个输出通常可以期望模型性能有所提高，但这很昂贵。平均而言，生成两个输出所需的成本大约是生成一个输出的两倍.^([29](ch02.html#id833))
- en: Warning
  id: totrans-392
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: I use the term *test time compute* to be consistent with the existing literature,
    even though several early reviewers protested that this term is confusing. In
    AI research, test time is typically used to refer to inference because researchers
    mostly only do inference to test a model. However, this technique can be applied
    to models in production in general. It’s test time compute because the number
    of outputs you can sample is determined by how much compute you can allocate to
    each inference call.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用*测试时间计算*这个术语是为了与现有文献保持一致，尽管几位早期审稿人抗议说这个术语很令人困惑。在人工智能研究中，测试时间通常用来指代推理，因为研究人员通常只进行推理来测试模型。然而，这项技术可以应用于生产中的模型。它是测试时间计算，因为你可以采样的输出数量取决于你可以分配给每个推理调用的计算量。
- en: To pick the best output, you can either show users multiple outputs and let
    them choose the one that works best for them, or you can devise a method to select
    the best one. One selection method is to pick the output with the highest probability.
    A language model’s output is a sequence of tokens, and each token has a probability
    computed by the model. The probability of an output is the product of the probabilities
    of all tokens in the output.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择最佳输出，你可以向用户展示多个输出并让他们选择最适合他们的那个，或者你可以设计一种方法来选择最佳输出。一种选择方法是选择概率最高的输出。语言模型的输出是一系列标记，每个标记都有一个模型计算的概率。输出的概率是输出中所有标记概率的乘积。
- en: 'Consider the sequence of tokens [“I”, “love”, “food”]. If the probability for
    “I” is 0.2, the probability for “love” given “I” is 0.1, and the probability for
    “food” given “I” and “love” is 0.3, the sequence’s probability is: `0.2 × 0.1
    × 0.3 = 0.006`. Mathematically, this can be denoted as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑标记序列[“I”， “love”， “food”]。如果“I”的概率是0.2，给定“I”的“love”概率是0.1，给定“I”和“love”的“food”概率是0.3，该序列的概率是：`0.2
    × 0.1 × 0.3 = 0.006`。从数学上讲，这可以表示如下：
- en: '[PRE4]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Remember that it’s easier to work with probabilities on a log scale. The logarithm
    of a product is equal to a sum of logarithms, so the logprob of a sequence of
    tokens is the sum of the logprob of all tokens in the sequence:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在对数尺度上处理概率更容易。乘积的对数等于对数之和，因此序列的对数概率是序列中所有标记的对数概率之和：
- en: '[PRE5]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With summing, longer sequences are likely to have a lower total logprob (logprob
    values are usually negative, because log of values between 0 and 1 is negative).
    To avoid biasing toward short sequences, you can use the average logprob by dividing
    the sum of a sequence by its length. After sampling multiple outputs, you pick
    the one with the highest average logprob. As of this writing, this is what the
    OpenAI API uses.^([30](ch02.html#id835))
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 使用求和法，较长的序列很可能具有更低的总对数概率（对数概率通常为负，因为0到1之间的值的对数是负的）。为了避免偏向较短的序列，你可以通过将序列的总和除以它的长度来使用平均对数概率。在采样多个输出后，你选择具有最高平均对数概率的那个。截至本文写作时，这是OpenAI
    API所使用的。[30](ch02.html#id835)
- en: Another selection method is to use a reward model to score each output, as discussed
    in the previous section. Recall that both [Stitch Fix](https://oreil.ly/1Njeh)
    and [Grab](https://oreil.ly/l21nr) pick the outputs given high scores by their
    reward models or verifiers. [Nextdoor](https://oreil.ly/-HQIB) found that using
    a reward model was the key factor in improving their application’s performance
    (2023).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择方法是使用奖励模型对每个输出进行评分，如前文所述。回想一下，[Stitch Fix](https://oreil.ly/1Njeh)和[Grab](https://oreil.ly/l21nr)都是根据它们的奖励模型或验证者的高分选择输出。[Nextdoor](https://oreil.ly/-HQIB)发现，使用奖励模型是提高他们应用程序性能的关键因素（2023）。
- en: OpenAI also trained verifiers to help their models pick the best solutions to
    math problems ([Cobbe et al., 2021](https://oreil.ly/R_uvq)). They found that
    using a verifier significantly boosted the model performance. *In fact, the use
    of verifiers resulted in approximately the same performance boost as a 30× model
    size increase.* This means that a 100-million-parameter model that uses a verifier
    can perform on par with a 3-billion-parameter model that doesn’t use a verifier.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI还训练了验证器来帮助他们的模型选择数学问题的最佳解决方案。[Cobbe et al., 2021](https://oreil.ly/R_uvq))。他们发现，使用验证器显著提高了模型性能。*事实上，使用验证器的效果几乎等同于模型大小增加30倍。*这意味着一个使用验证器的1亿参数模型可以与一个不使用验证器的300亿参数模型的表现相当。
- en: 'DeepMind further proves the value of test time compute, arguing that scaling
    test time compute (e.g., allocating more compute to generate more outputs during
    inference) can be more efficient than scaling model parameters ([Snell et al.,
    2024](https://arxiv.org/abs/2408.03314)). The same paper asks an interesting question:
    If an LLM is allowed to use a fixed but nontrivial amount of inference-time compute,
    how much can it improve its performance on a challenging prompt?'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind进一步证明了测试时计算的价值，认为扩大测试时计算（例如，在推理过程中分配更多计算以生成更多输出）可能比扩大模型参数更有效。[Snell
    et al., 2024](https://arxiv.org/abs/2408.03314)。同一篇论文提出了一个有趣的问题：如果允许LLM使用固定但非平凡的推理时计算量，它能提高其在具有挑战性的提示上的性能多少？
- en: In OpenAI’s experiment, sampling more outputs led to better performance, but
    only up to a certain point. In this experiment, that point was 400 outputs. Beyond
    this point, performance decreases, as shown in [Figure 2-19](#ch02_figure_19_1730147895520976).
    They hypothesized that as the number of sampled outputs increases, the chance
    of finding adversarial outputs that can fool the verifier also increases. However,
    a Stanford experiment showed a different conclusion. “Monkey Business” ([Brown
    et al., 2024](https://oreil.ly/8YNwQ)) finds that the number of problems solved
    often increases log-linearly as the number of samples increases from 1 to 10,000\.
    While it’s interesting to think about whether test time compute can be scaled
    indefinitely, I don’t believe anyone in production samples 400 or 10,000 different
    outputs for each input. The cost would be astronomical.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenAI 的实验中，增加输出样本的数量可以提高性能，但仅限于某个特定点。在这个实验中，这个点达到了 400 个输出。超过这个点，性能会下降，如[图
    2-19](#ch02_figure_19_1730147895520976)所示。他们假设随着采样输出数量的增加，找到能够欺骗验证器的对抗性输出的机会也会增加。然而，斯坦福大学的实验得出了不同的结论。“Monkey
    Business”([Brown 等人，2024](https://oreil.ly/8YNwQ))发现，当样本数量从 1 增加到 10,000 时，解决问题的数量通常以对数线性方式增加。虽然思考测试时间计算是否可以无限扩展很有趣，但我相信在生产环境中，没有人会对每个输入采样
    400 或 10,000 个不同的输出。成本将是天文数字。
- en: '![A graph with blue lines and numbers  Description automatically generated](assets/aien_0219.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![带有蓝色线条和数字的图表 描述自动生成](assets/aien_0219.png)'
- en: Figure 2-19\. [OpenAI](https://arxiv.org/abs/2110.14168) (2021) found that sampling
    more outputs led to better performance, but only up to 400 outputs.
  id: totrans-405
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-19\. [OpenAI](https://arxiv.org/abs/2110.14168) (2021) 发现，增加输出样本的数量可以提高性能，但仅限于
    400 个输出。
- en: You can also use application-specific heuristics to select the best response.
    For example, if your application benefits from shorter responses, you can pick
    the shortest candidate. If your application converts natural language to SQL queries,
    you can get the model to keep on generating outputs until it generates a valid
    SQL query.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用特定于应用的启发式方法来选择最佳响应。例如，如果您的应用从较短的响应中受益，您可以选择最短的候选者。如果您的应用将自然语言转换为 SQL 查询，您可以指示模型继续生成输出，直到它生成一个有效的
    SQL 查询。
- en: One particularly interesting application of test time compute is to overcome
    the latency challenge. For some queries, especially chain-of-thought queries,
    a model might take a long time to complete the response. Kittipat Kampa, head
    of AI at TIFIN, told me that his team asks their model to generate multiple responses
    in parallel and show the user the first response that is completed and valid.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 测试时间计算的特别有趣的应用之一是克服延迟挑战。对于某些查询，尤其是思维链查询，模型可能需要很长时间才能完成响应。TIFIN 人工智能部门负责人 Kittipat
    Kampa 告诉我，他的团队要求他们的模型并行生成多个响应，并向用户展示第一个完成且有效的响应。
- en: Picking out the most common output among a set of outputs can be especially
    useful for tasks that expect exact answers.^([31](ch02.html#id836)) For example,
    given a math problem, the model can solve it multiple times and pick the most
    frequent answer as its final solution. Similarly, for a multiple-choice question,
    a model can pick the most frequent output option. This is what Google did when
    evaluating Gemini on the MMLU benchmark. They sampled 32 outputs for each question.
    This allowed the model to achieve a higher score than what it would’ve achieved
    with only one output per question.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 从一组输出中挑选出最常见的输出对于需要精确答案的任务特别有用.^([31](ch02.html#id836)) 例如，给定一个数学问题，模型可以多次解决它，并选择最频繁的答案作为其最终解决方案。同样，对于多项选择题，模型可以选择最频繁的输出选项。这就是谷歌在
    MMLU 基准测试评估 Gemini 时所做的那样。他们为每个问题采样了 32 个输出。这使得模型能够获得比每个问题只有一个输出时更高的分数。
- en: A model is considered robust if it doesn’t dramatically change its outputs with
    small variations in the input. The less robust a model is, the more you can benefit
    from sampling multiple outputs.^([32](ch02.html#id838)) For one project, we used
    AI to extract certain information from an image of the product. We found that
    for the same image, our model could read the information only half of the time.
    For the other half, the model said that the image was too blurry or the text was
    too small to read. However, by trying three times with each image, the model was
    able to extract the correct information for most images.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在输入略有变化时不会显著改变其输出，则认为该模型是健壮的。模型越不健壮，你从采样多个输出中获得的益处就越大。[32](ch02.html#id838)
    对于一个项目，我们使用AI从产品的图像中提取某些信息。我们发现，对于同一张图像，我们的模型只能有一半的时间读取信息。对于另一半，模型表示图像太模糊或文字太小而无法读取。然而，通过每张图像尝试三次，模型能够为大多数图像提取正确信息。
- en: Structured Outputs
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化输出
- en: 'Often, in production, you need models to generate outputs following certain
    formats. Structured outputs are crucial for the following two scenarios:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在生产中，你需要模型生成遵循特定格式的输出。结构化输出对于以下两种场景至关重要：
- en: '*Tasks requiring structured outputs.* The most common category of tasks in
    this scenario is semantic parsing. Semantic parsing involves converting natural
    language into a structured, machine-readable format. Text-to-SQL is an example
    of semantic parsing, where the outputs must be valid SQL queries. Semantic parsing
    allow users to interact with APIs using a natural language (e.g., English). For
    example, text-to-PostgreSQL allows users to query a Postgres database using English
    queries such as “What’s the average monthly revenue over the last 6 months” instead
    of writing it in PostgreSQL.'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*需要结构化输出的任务。* 在这种场景中，最常见的任务类别是语义解析。语义解析涉及将自然语言转换为结构化、机器可读的格式。文本到SQL是语义解析的一个例子，其中输出必须是有效的SQL查询。语义解析允许用户使用自然语言（例如，英语）与API交互。例如，文本到PostgreSQL允许用户使用英语查询（例如，“过去6个月的平均月收入是多少”）来查询Postgres数据库，而不是用PostgreSQL编写。'
- en: 'This is an example of a prompt for GPT-4o to do text-to-regex. The outputs
    are actual outputs generated by GPT-4o:'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是GPT-4o进行文本到正则表达式提示的一个例子。以下是GPT-4o生成的实际输出：
- en: '[PRE6]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Other categories of tasks in this scenario include classification where the
    outputs have to be valid classes.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种场景下，其他任务类别包括分类，其中输出必须是有效的类别。
- en: '*Tasks whose outputs are used by downstream applications.* In this scenario,
    the task itself doesn’t need the outputs to be structured, but because the outputs
    are used by other applications, they need to be parsable by these applications.'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*任务的输出被下游应用程序使用。* 在这种场景中，任务本身不需要输出是结构化的，但由于输出被其他应用程序使用，它们需要能够被这些应用程序解析。'
- en: 'For example, if you use an AI model to write an email, the email itself doesn’t
    have to be structured. However, a downstream application using this email might
    need it to be in a specific format—for example, a JSON document with specific
    keys, such as `{"title": [TITLE], "body": [EMAIL BODY]}`.'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '例如，如果你使用AI模型来撰写电子邮件，电子邮件本身不需要结构化。然而，使用此电子邮件的下游应用程序可能需要它以特定格式存在——例如，具有特定键的JSON文档，例如`{"title":
    [TITLE], "body": [EMAIL BODY]}`。'
- en: '*This is especially important for agentic workflows* where a model’s outputs
    are often passed as inputs into tools that the model can use, as discussed in
    [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386).'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*这对于具有代理工作流程非常重要*，在这些工作流程中，模型的输出通常作为输入传递给模型可以使用的工具，如第6章[第6章](ch06.html#ch06_rag_and_agents_1730157386571386)中所述。'
- en: Frameworks that support structured outputs include [guidance](https://github.com/guidance-ai/guidance),
    [outlines](https://github.com/dottxt-ai/outlines), [instructor](https://github.com/instructor-ai/instructor),
    and [llama.cpp](https://github.com/ggerganov/llama.cpp/discussions/177). Each
    model provider might also use their own techniques to improve their models’ ability
    to generate structured outputs. OpenAI was the first model provider to introduce
    [*JSON mode*](https://oreil.ly/NxZDF) in their text generation API. Note that
    an API’s JSON mode typically guarantees only that the outputs are valid JSON—not
    the content of the JSON objects. The otherwise valid generated JSONs can also
    be truncated, and thus not parsable, if the generation stops too soon, such as
    when it reaches the maximum output token length. However, if the max token length
    is set too long, the model’s responses become both too slow and expensive.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 支持结构化输出的框架包括 [guidance](https://github.com/guidance-ai/guidance), [outlines](https://github.com/dottxt-ai/outlines),
    [instructor](https://github.com/instructor-ai/instructor), 和 [llama.cpp](https://github.com/ggerganov/llama.cpp/discussions/177)。每个模型提供者也可能使用他们自己的技术来提高他们模型生成结构化输出的能力。OpenAI
    是第一个在其文本生成 API 中引入 [*JSON 模式*](https://oreil.ly/NxZDF) 的模型提供者。请注意，API 的 JSON 模式通常只能保证输出是有效的
    JSON——而不是 JSON 对象的内容。如果生成过早停止，例如达到最大输出令牌长度，那么原本有效的生成的 JSON 也可能被截断，因此无法解析。然而，如果最大令牌长度设置得太长，模型的响应会变得既慢又昂贵。
- en: '[Figure 2-20](#ch02_figure_20_1730147895520984) shows two examples of using
    guidance to generate outputs constrained to a set of options and a regex.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-20](#ch02_figure_20_1730147895520984) 展示了使用指导生成约束于一组选项和正则表达式的输出的两个示例。'
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_0220.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成描述](assets/aien_0220.png)'
- en: Figure 2-20\. Using guidance to generate constrained outputs.
  id: totrans-422
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-20\. 使用指导生成约束输出。
- en: 'You can guide a model to generate structured outputs at different layers of
    the AI stack: prompting, post-processing, test time compute, constrained sampling,
    and finetuning. The first three are more like bandages. They work best if the
    model is already pretty good at generating structured outputs and just needs a
    little nudge. For intensive treatment, you need constrained sampling and finetuning.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 AI 栈的不同层引导模型生成结构化输出：提示、后处理、测试时间计算、约束采样和微调。前三种更像是绷带。如果模型已经非常擅长生成结构化输出，只需要一点推动，它们效果最好。对于更深入的治疗，你需要约束采样和微调。
- en: Test time compute has just been discussed in the previous section—keep on generating
    outputs until one fits the expected format. This section focuses on the other
    four approaches.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 测试时间计算已在上一节中讨论——继续生成输出，直到符合预期的格式。本节重点介绍其他四种方法。
- en: Prompting
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Prompting is the first line of action for structured outputs. You can instruct
    a model to generate outputs in any format. However, whether a model can follow
    this instruction depends on the model’s instruction-following capability (discussed
    in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863)), and the
    clarity of the instruction (discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551)).
    While models are getting increasingly good at following instructions, there’s
    no guarantee that they’ll always follow your instructions.^([33](ch02.html#id844))
    A few percentage points of invalid model outputs can still be unacceptable for
    many applications.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是结构化输出的第一步。你可以指示模型以任何格式生成输出。然而，模型是否能够遵循此指示取决于模型的指令遵循能力（在第 4 章中讨论），以及指令的清晰度（在第
    5 章中讨论）。虽然模型在遵循指令方面变得越来越擅长，但并不能保证它们总是会遵循你的指令。[33](ch02.html#id844) 一小部分无效的模型输出对于许多应用来说可能仍然是不可以接受的。
- en: 'To increase the percentage of valid outputs, some people use AI to validate
    and/or correct the output of the original prompt. This is an example of the AI
    as a judge approach discussed in [Chapter 3](ch03.html#ch03a_evaluation_methodology_1730150757064067).
    This means that for each output, there will be at least two model queries: one
    to generate the output and one to validate it. While the added validation layer
    can significantly improve the validity of the outputs, the extra cost and latency
    incurred by the extra validation queries can make this approach too expensive
    for some.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高有效输出的百分比，有些人使用 AI 来验证和/或纠正原始提示的输出。这是在[第 3 章](ch03.html#ch03a_evaluation_methodology_1730150757064067)中讨论的
    AI 作为法官方法的一个例子。这意味着对于每个输出，至少会有两个模型查询：一个用于生成输出，另一个用于验证它。虽然额外的验证层可以显著提高输出的有效性，但额外验证查询带来的额外成本和延迟可能会使这种方法对某些人来说过于昂贵。
- en: Post-processing
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: Post-processing is simple and cheap but can work surprisingly well. During my
    time teaching, I noticed that students tended to make very similar mistakes. When
    I started working with foundation models, I noticed the same thing. A model tends
    to repeat similar mistakes across queries. This means if you find the common mistakes
    a model makes, you can potentially write a script to correct them. For example,
    if the generated JSON object misses a closing bracket, manually add that bracket.
    LinkedIn’s defensive YAML parser increased the percentage of correct YAML outputs
    from 90% to 99.99% ([Bottaro and Ramgopal, 2020](https://oreil.ly/ZTRaA)).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理简单且成本低，但可以出奇地有效。在我教学期间，我发现学生倾向于犯非常相似的错误。当我开始与基础模型合作时，我也注意到了同样的事情。模型倾向于在查询中重复相似的错误。这意味着如果你发现模型常见的错误，你可以潜在地编写一个脚本来纠正它们。例如，如果生成的
    JSON 对象缺少一个闭合括号，可以手动添加该括号。领英的防御性 YAML 解析器将正确的 YAML 输出百分比从 90% 提高到 99.99% ([Bottaro
    和 Ramgopal，2020](https://oreil.ly/ZTRaA))。
- en: Tip
  id: totrans-430
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: JSON and YAML are common text formats. LinkedIn found that their underlying
    model, GPT-4, worked with both, but they chose YAML as their output format because
    it is less verbose, and hence requires fewer output tokens than JSON (Bottaro
    and Ramgopal, 2020).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 和 YAML 是常见的文本格式。领英发现，它们的基础模型 GPT-4 可以与两者都兼容，但他们选择了 YAML 作为输出格式，因为它更简洁，因此比
    JSON 需要更少的输出标记（Bottaro 和 Ramgopal，2020）。
- en: Post-processing works only if the mistakes are easy to fix. This usually happens
    if a model’s outputs are already mostly correctly formatted, with occasional small
    errors.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理只有在错误容易修复的情况下才有效。这通常发生在模型输出已经基本正确格式化，偶尔出现小错误的情况下。
- en: Constrained sampling
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 约束采样
- en: '*Constraint sampling* is a technique for guiding the generation of text toward
    certain constraints. It is typically followed by structured output tools.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '*约束采样* 是一种引导文本生成向特定约束靠拢的技术。它通常随后使用结构化输出工具。'
- en: At a high level, to generate a token, the model samples among values that meet
    the constraints. Recall that to generate a token, your model first outputs a logit
    vector, each logit corresponding to one possible token. Constrained sampling filters
    this logit vector to keep only the tokens that meet the constraints. It then samples
    from these valid tokens. This process is shown in [Figure 2-21](#ch02_figure_21_1730147895520993).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，为了生成一个标记，模型会在满足约束的值中进行采样。回想一下，为了生成一个标记，你的模型首先输出一个对数向量，每个对数对应一个可能的标记。约束采样会过滤这个对数向量，只保留满足约束的标记。然后从这些有效标记中进行采样。这个过程在[图
    2-21](#ch02_figure_21_1730147895520993)中展示。
- en: '![A diagram of a software model  Description automatically generated](assets/aien_0221.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![软件模型的图示 自动生成描述](assets/aien_0221.png)'
- en: Figure 2-21\. Filter out logits that don’t meet the constraints in order to
    sample only among valid outputs.
  id: totrans-437
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-21\. 过滤掉不满足约束的对数，以便只从有效输出中进行采样。
- en: 'In the example in [Figure 2-21](#ch02_figure_21_1730147895520993), the constraint
    is straightforward to filter for. However, most cases aren’t that straightforward.
    You need to have a grammar that specifies what is and isn’t allowed at each step.
    For example, JSON grammar dictates that after `{`, you can’t have another `{`
    unless it’s part of a string, as in `{"key": "{{string}}"}`.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '在[图 2-21](#ch02_figure_21_1730147895520993)的例子中，约束过滤是直接的。然而，大多数情况并不那么简单。你需要有一个语法来指定每个步骤允许或不允许的内容。例如，JSON
    语法规定，在 `{` 之后，除非它是字符串的一部分，如 `{"key": "{{string}}"}`，否则你不能有另一个 `{`。'
- en: Building out that grammar and incorporating it into the sampling process is
    nontrivial. Because each output format—JSON, YAML, regex, CSV, and so on—needs
    its own grammar, constraint sampling is less generalizable. Its use is limited
    to the formats whose grammars are supported by external tools or by your team.
    Grammar verification can also increase generation latency ([Brandon T. Willard,
    2024](https://oreil.ly/hNRf4)).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 构建那种语法并将其纳入采样过程是非平凡的。因为每种输出格式——JSON、YAML、正则表达式、CSV等等——都需要自己的语法，约束采样就不够通用。它的使用仅限于那些语法由外部工具或你的团队支持的格式。语法验证也可能增加生成延迟([Brandon
    T. Willard, 2024](https://oreil.ly/hNRf4))。
- en: Some are against constrained sampling because they believe the resources needed
    for constrained sampling are better invested in training models to become better
    at following instructions.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人反对约束采样，因为他们认为用于约束采样的资源最好投资于训练模型以更好地遵循指令。
- en: Finetuning
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: Finetuning a model on examples following your desirable format is the most effective
    and general approach to get models to generate outputs in this format.^([34](ch02.html#id849))
    It can work with any expected format. While simple finetuning doesn’t guarantee
    that the model will always output the expected format, it is much more reliable
    than prompting.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在符合你期望格式的示例上微调模型是让模型以这种格式生成输出的最有效和最通用的方法。[^([34](ch02.html#id849))] 它可以与任何预期的格式一起工作。虽然简单的微调不能保证模型始终输出预期的格式，但它比提示更可靠。
- en: For certain tasks, you can guarantee the output format by modifying the model’s
    architecture before finetuning. For example, for classification, you can append
    a classifier head to the foundation model’s architecture to make sure that the
    model outputs only one of the pre-specified classes. The architecture looks like
    [Figure 2-22](#ch02_figure_22_1730147895521005).^([35](ch02.html#id850)) This
    approach is also called *feature-based transfer* and is discussed more with other
    transfer learning techniques in [Chapter 7](ch07.html#ch07).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些任务，你可以在微调之前修改模型的架构来保证输出格式。例如，对于分类，你可以在基础模型的架构中添加一个分类器头，以确保模型只输出预指定的类别之一。架构看起来像[图2-22](#ch02_figure_22_1730147895521005)。[^([35](ch02.html#id850))]
    这种方法也称为*基于特征的迁移*，在[第7章](ch07.html#ch07)中与其他迁移学习技术一起讨论。
- en: '![A diagram of a layer  Description automatically generated](assets/aien_0222.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![层图  描述自动生成](assets/aien_0222.png)'
- en: Figure 2-22\. Adding a classifier head to your base model to turn it into a
    classifier. In this example, the classifier works with three classes.
  id: totrans-445
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-22。向你的基础模型添加分类器头以将其转换为分类器。在这个例子中，分类器与三个类别一起工作。
- en: During finetuning, you can retrain the whole model end-to-end or part of the
    model, such as this classifier head. End-to-end training requires more resources,
    but promises better performance.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调期间，你可以从头到尾重新训练整个模型或部分模型，例如这个分类器头。端到端训练需要更多资源，但承诺更好的性能。
- en: We need techniques for structured outputs because of the assumption that the
    model, by itself, isn’t capable of generating structured outputs. However, as
    models become more powerful, we can expect them to get better at following instructions.
    I suspect that in the future, it’ll be easier to get models to output exactly
    what we need with minimal prompting, and these techniques will become less important.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要结构化输出的技术，因为假设模型本身无法生成结构化输出。然而，随着模型变得更加强大，我们可以期待它们在遵循指令方面做得更好。我怀疑在未来，通过最少的提示让模型输出我们需要的精确内容将变得更加容易，这些技术将变得不那么重要。
- en: The Probabilistic Nature of AI
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能的概率性质
- en: The way AI models sample their responses makes them *probabilistic*. Let’s go
    over an example to see what being probabilistic means. Imagine that you want to
    know what’s the best cuisine in the world. If you ask your friend this question
    twice, a minute apart, your friend’s answers both times should be the same. If
    you ask an AI model the same question twice, its answer can change. If an AI model
    thinks that Vietnamese cuisine has a 70% chance of being the best cuisine in the
    world and Italian cuisine has a 30% chance, it’ll answer “Vietnamese cuisine”
    70% of the time and “Italian cuisine” 30% of the time. The opposite of probabilistic
    is *deterministic*, when the outcome can be determined without any random variation.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型采样其响应的方式使其具有**概率性**。让我们通过一个例子来看看什么是概率性。想象一下，你想知道世界上最好的美食是什么。如果你问你的朋友这个问题两次，相隔一分钟，你的朋友两次的回答应该是相同的。如果你问一个人工智能模型相同的问题两次，它的回答可能会改变。如果一个人工智能模型认为越南菜有
    70% 的可能性是世界上最好的美食，而意大利菜有 30% 的可能性，那么它 70% 的时间会回答“越南菜”，30% 的时间会回答“意大利菜”。与概率性相反的是**确定性**，即结果可以在没有任何随机变化的情况下确定。
- en: This probabilistic nature can cause inconsistency and hallucinations. *Inconsistency*
    is when a model generates very different responses for the same or slightly different
    prompts. *Hallucination* is when a model gives a response that isn’t grounded
    in facts. Imagine if someone on the internet wrote an essay about how all US presidents
    are aliens, and this essay was included in the training data. The model later
    will probabilistically output that the current US president is an alien. From
    the perspective of someone who doesn’t believe that US presidents are aliens,
    the model is making this up.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这种概率性质可能导致不一致性和幻觉。**不一致性**是指模型对相同或略微不同的提示生成非常不同的响应。**幻觉**是指模型给出一个没有基于事实的响应。想象一下，如果互联网上有人写了一篇关于所有美国总统都是外星人的论文，并且这篇论文被包含在训练数据中，那么模型后来可能会以概率性地输出当前美国总统是外星人。对于那些不相信美国总统是外星人的人来说，模型是在编造事实。
- en: Foundation models are usually trained using a large amount of data. They are
    aggregations of the opinions of the masses, containing within them, literally,
    a world of possibilities. Anything with a non-zero probability, no matter how
    far-fetched or wrong, can be generated by AI.^([36](ch02.html#id857))
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型通常使用大量数据进行训练。它们是大众意见的汇总，其中包含着，字面上，一个充满可能性的世界。任何具有非零概率的东西，无论多么牵强或错误，都可以由人工智能生成。[36](ch02.html#id857)
- en: This characteristic makes building AI applications both exciting and challenging.
    Many of the AI engineering efforts, as we’ll see in this book, aim to harness
    and mitigate this probabilistic nature.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特性使得构建人工智能应用既令人兴奋又具有挑战性。正如我们将在本书中看到的那样，许多人工智能工程努力的目标是利用和减轻这种概率性质。
- en: This probabilistic nature makes AI great for creative tasks. What is creativity
    but the ability to explore beyond the common paths—to think outside the box? AI
    is a great sidekick for creative professionals. It can brainstorm limitless ideas
    and generate never-before-seen designs. However, this same probabilistic nature
    can be a pain for everything else.^([37](ch02.html#id858))
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这种概率性质使得人工智能非常适合创意任务。什么是创造力，不过就是探索超越常规路径的能力——跳出思维定势的能力？人工智能是创意专业人士的绝佳助手。它可以产生无限的想法，并生成前所未有的设计。然而，这种相同的概率性质也可能给其他一切带来麻烦。[37](ch02.html#id858)
- en: Inconsistency
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不一致性
- en: 'Model inconsistency manifests in two scenarios:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的不一致性表现在两种情况下：
- en: 'Same input, different outputs: Giving the model the same prompt twice leads
    to two very different responses.'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相同的输入，不同的输出：给模型相同的提示两次会导致两个非常不同的响应。
- en: 'Slightly different input, drastically different outputs: Giving the model a
    slightly different prompt, such as accidentally capitalizing a letter, can lead
    to a very different output.'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 略微不同的输入，截然不同的输出：给模型一个略微不同的提示，比如不小心将字母大写，可能会导致一个非常不同的输出。
- en: '[Figure 2-23](#ch02_figure_23_1730147895521014) shows an example of me trying
    to use ChatGPT to score essays. The same prompt gave me two different scores when
    I ran it twice: 3/5 and 5/5.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-23](#ch02_figure_23_1730147895521014) 展示了我尝试使用 ChatGPT 给论文评分的例子。相同的提示在运行两次时给了我两个不同的评分：3/5
    和 5/5。'
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_0223.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由系统自动生成](assets/aien_0223.png)'
- en: Figure 2-23\. The same input can produce different outputs in the same model.
  id: totrans-460
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-23\. 相同的输入在同一模型中可以产生不同的输出。
- en: Inconsistency can create a jarring user experience. In human-to-human communication,
    we expect a certain level of consistency. Imagine a person giving you a different
    name every time you see them. Similarly, users expect a certain level of consistency
    when communicating with AI.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致性可能会造成令人不快的用户体验。在人与人之间的交流中，我们期望一定程度的连贯性。想象一下，一个人每次见到你都会给你不同的名字。同样，用户在与AI交流时也期望一定程度的连贯性。
- en: For the same input, different outputs scenario, there are multiple approaches
    to mitigate inconsistency. You can cache the answer so that the next time the
    same question is asked, the same answer is returned. You can fix the model’s sampling
    variables, such as temperature, top-p, and top-k values, as discussed earlier.
    You can also fix the *seed* variable, which you can think of as the starting point
    for the random number generator used for sampling the next token.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相同输入不同输出的情况，有多种方法可以减轻不一致性。你可以缓存答案，以便下次相同问题被问及时返回相同的答案。你可以修复模型的采样变量，如前面讨论的温度、top-p和top-k值。你还可以修复*种子*变量，你可以将其视为用于采样下一个标记的随机数生成器的起点。
- en: Even if you fix all these variables, however, there’s no guarantee that your
    model will be consistent 100% of the time. The hardware the model runs the output
    generation on can also impact the output, as different machines have different
    ways of executing the same instruction and can handle different ranges of numbers.
    If you host your models, you have some control over the hardware you use. However,
    if you use a model API provider like OpenAI or Google, it’s up to these providers
    to give you any control.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使你修复了所有这些变量，也无法保证你的模型在100%的时间内都是一致的。模型运行输出生成的硬件也会影响输出，因为不同的机器以不同的方式执行相同的指令，并且可以处理不同的数字范围。如果你托管自己的模型，你可以在一定程度上控制你使用的硬件。然而，如果你使用像OpenAI或Google这样的模型API提供商，那么控制权就掌握在这些提供商手中。
- en: Fixing the output generation settings is a good practice, but it doesn’t inspire
    trust in the system. Imagine a teacher who gives you consistent scores only if
    that teacher sits in one particular room. If that teacher sits in a different
    room, that teacher’s scores for you will be wild.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 修复输出生成设置是一种良好的实践，但它并不能激发对系统的信任。想象一下，一位老师只有在坐在特定房间时才会给你一致的分数。如果这位老师坐在不同的房间，那么这位老师给你的分数将会是随机的。
- en: The second scenario—slightly different input, drastically different outputs—is
    more challenging. Fixing the model’s output generation variables is still a good
    practice, but it won’t force the model to generate the same outputs for different
    inputs. It is, however, possible to get models to generate responses closer to
    what you want with carefully crafted prompts (discussed in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551))
    and a memory system (discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386)).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况——输入略有不同，输出却截然不同——更具挑战性。修复模型的输出生成变量仍然是一种良好的实践，但它不会强迫模型对不同的输入生成相同的输出。然而，通过精心设计的提示（在第5章中讨论，见[Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551)）和记忆系统（在第6章中讨论，见[Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386)），可以使模型生成的响应更接近你想要的结果。
- en: Hallucination
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 幻觉
- en: Hallucinations are fatal for tasks that depend on factuality. If you’re asking
    AI to help you explain the pros and cons of a vaccine, you don’t want AI to be
    pseudo-scientific. In June 2023, a law firm was [fined for submitting fictitious
    legal research to court](https://oreil.ly/FCyyA). They had used ChatGPT to prepare
    their case, unaware of ChatGPT’s tendency to hallucinate.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖事实性的任务，幻觉是致命的。如果你要求AI帮助你解释疫苗的利弊，你不想AI表现得像伪科学。2023年6月，一家律师事务所因向法庭提交虚假法律研究而被罚款（见[https://oreil.ly/FCyyA](https://oreil.ly/FCyyA)）。他们使用了ChatGPT来准备案件，却不知道ChatGPT有幻觉的倾向。
- en: While hallucination became a prominent issue with the rise of LLMs, hallucination
    was a common phenomenon for generative models even before the term foundation
    model and the transformer architecture were introduced. Hallucination in the context
    of text generation was mentioned as early as 2016 ([Goyal et al., 2016](https://oreil.ly/cg0JY)).
    Detecting and measuring hallucinations has been a staple in natural language generation
    (NLG) since then (see [Lee et al., 2018](https://oreil.ly/ah9MT); [Nie et al.,
    2019](https://oreil.ly/13wUD); and [Zhou et al., 2020](https://arxiv.org/abs/2011.02593)).
    This section focuses on explaining why hallucinations happen. How to detect and
    measure evaluation is discussed in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随着大型语言模型（LLMs）的兴起，幻觉成为一个突出的问题，但即使在术语“基础模型”和“Transformer 架构”被引入之前，生成模型就已经存在幻觉这一普遍现象。在文本生成的背景下，幻觉最早在
    2016 年被提及（[Goyal 等人，2016](https://oreil.ly/cg0JY)）。自那时起，检测和测量幻觉一直是自然语言生成（NLG）的一个基本任务（参见
    [Lee 等人，2018](https://oreil.ly/ah9MT)；[Nie 等人，2019](https://oreil.ly/13wUD)；以及
    [Zhou 等人，2020](https://arxiv.org/abs/2011.02593)）。本节重点解释幻觉发生的原因。如何检测和测量评估将在 [第
    4 章](ch04.html#ch04_evaluate_ai_systems_1730130866187863) 中讨论。
- en: If inconsistency arises from randomness in the sampling process, the cause of
    hallucination is more nuanced. The sampling process alone doesn’t sufficiently
    explain it. A model samples outputs from all probable options. But how does something
    never seen before become a probable option? A model can output something that
    is believed to have never been seen before in the training data. We can’t say
    this for sure because it’s impossible to comb through the training data to verify
    whether it contains an idea. Our ability to construct something so complex that
    we can no longer understand it is both a blessing and a curse.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不一致性源于采样过程中的随机性，那么幻觉的原因就更加复杂。仅仅采样过程本身并不能充分解释它。模型从所有可能的选项中采样输出。但是，从未见过的事物如何成为可能的选项呢？模型可以输出在训练数据中从未见过的事物。我们无法确定这一点，因为无法遍历训练数据来验证它是否包含某个想法。我们构建如此复杂以至于我们无法理解的事物的能力，既是祝福也是诅咒。
- en: It’s hard to devise a way to eliminate hallucinations without understanding
    why hallucinations occur in the first place. There are currently two hypotheses
    about why language models hallucinate.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 要在没有理解幻觉最初发生原因的情况下设计一种消除幻觉的方法是很困难的。目前关于语言模型产生幻觉的原因有两种假设。
- en: The first hypothesis, originally expressed by [Ortega et al. at DeepMind in
    2021](https://arxiv.org/abs/2110.10819#deepmind), is that a language model hallucinates
    because it can’t differentiate between the data it’s given and the data it generates.
    Let’s go through an example to illustrate this.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个假设最初由 [Ortega 等人在 2021 年 DeepMind 提出](https://arxiv.org/abs/2110.10819#deepmind)，即语言模型产生幻觉是因为它无法区分它所接收的数据和它所生成的数据。让我们通过一个例子来说明这一点。
- en: 'Imagine that you give the model the prompt: “Who’s Chip Huyen?” and the first
    sentence the model generates is: “Chip Huyen is an architect.” The next token
    the model generates will be conditioned on the sequence: “Who’s Chip Huyen? Chip
    Huyen is an architect.” The model treats “Chip Huyen is an architect.”, something
    it produced, the same way it treats a given fact. Starting with a generated sequence
    slightly out of the ordinary, the model can expand upon it and generate outrageously
    wrong facts. Ortega and the other authors called hallucinations a form of *self-delusion*.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你给模型提示：“Chip Huyen 是谁？”而模型生成的第一句话是：“Chip Huyen 是一名建筑师。”接下来模型生成的标记将基于序列：“Chip
    Huyen 是谁？Chip Huyen 是一名建筑师。”模型将其生成的“Chip Huyen 是一名建筑师。”（它所产生的内容）视为与给定事实相同。从一个略微异常的生成序列开始，模型可以在此基础上扩展并生成荒谬的错误事实。Ortega
    和其他作者将幻觉称为一种*自我欺骗*的形式。
- en: '[Figure 2-24](#ch02_figure_24_1730147895521021) shows an example of self-delusion
    by the model LLaVA-v1.5-7B. I asked the model to identify ingredients listed on
    the product’s label in the image, which is a bottle of shampoo. In its response,
    the model convinces itself that the product in the image is a bottle of milk,
    then continues to include milk in the list of ingredients extracted from the product’s
    label.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-24](#ch02_figure_24_1730147895521021) 展示了模型 LLaVA-v1.5-7B 产生自我欺骗的一个例子。我要求模型识别图像上产品标签上列出的成分，这是一瓶洗发水。在其回答中，模型说服自己图像中的产品是一瓶牛奶，然后继续将牛奶列入从产品标签中提取的成分列表中。'
- en: '![A bottle of milk with instructions  Description automatically generated](assets/aien_0224.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![一瓶带有说明的牛奶图片 自动生成描述](assets/aien_0224.png)'
- en: Figure 2-24\. An example of self-delusion by LLaVA-v1.5-7B.
  id: totrans-475
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-24\. LLaVA-v1.5-7B的自我欺骗示例。
- en: Zhang et al. (2023) call this phenomenon [snowballing hallucinations](https://arxiv.org/abs/2305.13534).
    After making an incorrect assumption, a model can continue hallucinating to justify
    the initial wrong assumption. Interestingly, the authors show that initial wrong
    assumptions can cause the model to make mistakes on questions it would otherwise
    be able to answer correctly, as shown in [Figure 2-25](#ch02_figure_25_1730147895521031).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人（2023）将这种现象称为[snowballing幻觉](https://arxiv.org/abs/2305.13534)。在做出一个错误的假设之后，模型可以继续幻觉以证明最初的错误假设。有趣的是，作者们表明，初始的错误假设可能导致模型在它本应能够正确回答的问题上犯错误，如图2-25所示。
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_0225.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由系统自动生成](assets/aien_0225.png)'
- en: Figure 2-25\. An initial incorrect assumption can cause the model to claim that
    9677 is divisible by 13, even if it knows this isn’t true.
  id: totrans-478
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-25\. 一个初始的错误假设可能导致模型声称9677可以被13整除，即使它知道这不是真的。
- en: The DeepMind paper showed that hallucinations can be mitigated by two techniques.
    The first technique comes from reinforcement learning, in which the model is made
    to differentiate between user-provided prompts (called *observations about the
    world* in reinforcement learning) and tokens generated by the model (called the
    model’s *actions*). The second technique leans on supervised learning, in which
    factual and counterfactual signals are included in the training data.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的论文显示，可以通过两种技术来减轻幻觉。第一种技术来自强化学习，其中模型被训练来区分用户提供的提示（在强化学习中称为“关于世界的观察”）和模型生成的标记（称为模型的行为）。第二种技术依赖于监督学习，其中事实和反事实信号包含在训练数据中。
- en: The second hypothesis is that hallucination is caused by the mismatch between
    the model’s internal knowledge and the labeler’s internal knowledge. This view
    was first argued by [Leo Gao](https://oreil.ly/9idN4), an OpenAI researcher. During
    SFT, models are trained to mimic responses written by labelers. If these responses
    use the knowledge that the labelers have but the model doesn’t have, we’re effectively
    teaching the model to hallucinate. In theory, if labelers can include the knowledge
    they use with each response they write so that the model knows that the responses
    aren’t made up, we can perhaps teach the model to use only what it knows. However,
    this is impossible in practice.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个假设是，幻觉是由模型内部知识与标注者内部知识之间的不匹配引起的。这种观点首先由OpenAI的研究员[Leo Gao](https://oreil.ly/9idN4)提出。在SFT期间，模型被训练来模仿标注者编写的回答。如果这些回答使用了标注者拥有的但模型没有的知识，我们实际上是在教模型进行幻觉。从理论上讲，如果标注者可以在他们编写的每个回答中包含他们使用的知识，以便模型知道这些回答不是编造的，那么我们可能可以教模型只使用它所知道的知识。然而，在实践中这是不可能的。
- en: 'In April 2023, John Schulman, an OpenAI co-founder, expressed the same view
    in his [UC Berkeley talk](https://oreil.ly/Fqo2S). Schulman also believes that
    LLMs know if they know something, which, in itself, is a big claim. If this belief
    is true, hallucinations can be fixed by forcing a model to give answers based
    on only the information it knows. He proposed two solutions. One is verification:
    for each response, ask the model to retrieve the sources it bases this response
    on. Another is to use reinforcement learning. Remember that the reward model is
    trained using only comparisons—response A is better than response B—without an
    explanation of why A is better. Schulman argued that a better reward function
    that punishes a model more for making things up can help mitigate hallucinations.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年4月，OpenAI的联合创始人John Schulman在他的[加州大学伯克利分校的演讲](https://oreil.ly/Fqo2S)中表达了相同的观点。Schulman也认为，LLMs知道它们是否知道某事，这本身就是一个很大的声明。如果这个信念是真的，可以通过迫使模型仅基于它所知道的信息给出答案来修复幻觉。他提出了两个解决方案。一个是验证：对于每个回答，要求模型检索它基于此回答的来源。另一个是使用强化学习。记住，奖励模型是使用仅比较——回答A比回答B更好——而没有解释为什么A更好的方式训练的。Schulman认为，一个更好的奖励函数，对模型编造事物的惩罚更大，可以帮助减轻幻觉。
- en: In that same talk, Schulman mentioned that OpenAI found that RLHF helps with
    reducing hallucinations. However, the InstructGPT paper shows that RLHF made hallucination
    worse, as shown in [Figure 2-26](#ch02_figure_26_1730147895521041). Even though
    RLHF seemed to worsen hallucinations for InstructGPT, it improved other aspects,
    and overall, human labelers prefer the RLHF model over the SFT alone model.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在那次同样的演讲中，Schulman提到OpenAI发现RLHF有助于减少幻觉。然而，InstructGPT论文显示RLHF使幻觉变得更糟，如图[2-26](#ch02_figure_26_1730147895521041)所示。尽管RLHF似乎使InstructGPT的幻觉变得更糟，但它改善了其他方面，并且总体而言，人类标注者更喜欢RLHF模型而不是仅使用SFT的模型。
- en: '![A screenshot of a computer  Description automatically generated](assets/aien_0226.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由系统自动生成](assets/aien_0226.png)'
- en: Figure 2-26\. Hallucination is worse for the model that uses both RLHF and SFT
    (InstructGPT) compared to the same model that uses only SFT ([Ouyang et al., 2022](https://arxiv.org/abs/2203.02155)).
  id: totrans-484
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-26\. 与仅使用SFT（InstructGPT）的相同模型相比，使用RLHF和SFT（InstructGPT）的模型在幻觉方面更严重 ([Ouyang
    et al., 2022](https://arxiv.org/abs/2203.02155))).
- en: Based on the assumption that a foundation model knows what it knows, some people
    try to reduce hallucination with prompts, such as adding “Answer as truthfully
    as possible, and if you’re unsure of the answer, say, ‘Sorry, I don’t know.’”
    Asking models for concise responses also seems to help with hallucinations—the
    fewer tokens a model has to generate, the less chance it has to make things up.
    Prompting and context construction techniques in Chapters [5](ch05.html#ch05a_prompt_engineering_1730156991195551)
    and [6](ch06.html#ch06_rag_and_agents_1730157386571386) can also help mitigate
    hallucinations.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 基于基础模型知道它所知道的内容的假设，有些人试图通过提示来减少幻觉，例如添加“尽可能真实地回答，如果你不确定答案，可以说，‘抱歉，我不知道。’”要求模型给出简洁的回答似乎也有助于减少幻觉——模型需要生成的标记越少，它编造东西的机会就越小。第[5](ch05.html#ch05a_prompt_engineering_1730156991195551)章和第[6](ch06.html#ch06_rag_and_agents_1730157386571386)章中讨论的提示和上下文构建技术也可以帮助减轻幻觉。
- en: The two hypotheses discussed complement each other. The self-delusion hypothesis
    focuses on how self-supervision causes hallucinations, whereas the mismatched
    internal knowledge hypothesis focuses on how supervision causes hallucinations.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的两个假设相互补充。自我欺骗假设关注自我监督如何导致幻觉，而内部知识不匹配假设关注监督如何导致幻觉。
- en: If we can’t stop hallucinations altogether, can we at least detect when a model
    hallucinates so that we won’t serve those hallucinated responses to users? Well,
    detecting hallucinations isn’t that straightforward either—think about how hard
    it is for us to detect when another human is lying or making things up. But people
    have tried. We discuss how to detect and measure hallucinations in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们无法完全停止幻觉，我们至少可以检测到模型何时产生幻觉，这样我们就不会向用户提供那些幻觉的响应吗？好吧，检测幻觉也不是那么简单——想想我们检测他人撒谎或编造事情有多难。但人们已经尝试了。我们在[第4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)中讨论了如何检测和衡量幻觉。
- en: Summary
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter discussed the core design decisions when building a foundation
    model. Since most people will be using ready-made foundation models instead of
    training one from scratch, I skipped the nitty-gritty training details in favor
    of modeling factors that help you determine what models to use and how to use
    them.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了构建基础模型时的核心设计决策。由于大多数人将使用现成的基础模型而不是从头开始训练，我跳过了繁琐的训练细节，转而讨论有助于你确定使用哪些模型以及如何使用它们的建模因素。
- en: A crucial factor affecting a model’s performance is its training data. Large
    models require a large amount of training data, which can be expensive and time-consuming
    to acquire. Model providers, therefore, often leverage whatever data is available.
    This leads to models that can perform well on the many tasks present in the training
    data, which may not include the specific task you want. This chapter went over
    why it’s often necessary to curate training data to develop models targeting specific
    languages, especially low-resource languages, and specific domains.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 影响模型性能的关键因素是其训练数据。大型模型需要大量的训练数据，获取这些数据可能既昂贵又耗时。因此，模型提供商通常会利用可用的任何数据。这导致模型在训练数据中存在的许多任务上表现良好，但这些任务可能不包括你想要的特定任务。本章讨论了为什么通常有必要精心挑选训练数据来开发针对特定语言、特别是低资源语言和特定领域的模型。
- en: After sourcing the data, model development can begin. While model training often
    dominates the headlines, an important step prior to that is architecting the model.
    The chapter looked into modeling choices, such as model architecture and model
    size. The dominating architecture for language-based foundation models is transformer.
    This chapter explored the problems that the transformer architecture was designed
    to address, as well as its limitations.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取数据后，模型开发就可以开始了。虽然模型训练经常成为头条新闻，但在那之前的一个重要步骤是设计模型。本章探讨了建模选择，例如模型架构和模型大小。基于语言的底层模型的主导架构是transformer。本章探讨了transformer架构旨在解决的问题以及其局限性。
- en: 'The scale of a model can be measured by three key numbers: the number of parameters,
    the number of training tokens, and the number of FLOPs needed for training. Two
    aspects that influence the amount of compute needed to train a model are the model
    size and the data size. The scaling law helps determine the optimal number of
    parameters and number of tokens given a compute budget. This chapter also looked
    at scaling bottlenecks. Currently, scaling up a model generally makes it better.
    But how long will this continue to be true?'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的规模可以通过三个关键数字来衡量：参数数量、训练标记数量以及训练所需的FLOPs数量。影响模型训练所需计算量的两个因素是模型大小和数据大小。缩放定律有助于确定在给定计算预算的情况下，最优的参数数量和标记数量。本章还探讨了缩放瓶颈。目前，扩大模型规模通常会使模型变得更好。但这种趋势会持续多久？
- en: 'Due to the low quality of training data and self-supervision during pre-training,
    the resulting model might produce outputs that don’t align with what users want.
    This is addressed by post-training, which consists of two steps: supervised finetuning
    and preference finetuning. Human preference is diverse and impossible to capture
    in a single mathematical formula, so existing solutions are far from foolproof.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预训练期间训练数据和自监督质量较低，生成的模型可能产生不符合用户期望的输出。这可以通过后训练来解决，后训练包括两个步骤：监督微调和偏好微调。人类偏好多种多样，无法用一个单一的数学公式来捕捉，因此现有的解决方案远非万无一失。
- en: 'This chapter also covered one of my favorite topics: sampling, the process
    by which a model generates output tokens. Sampling makes AI models probabilistic.
    This probabilistic nature is what makes models like ChatGPT and Gemini great for
    creative tasks and fun to talk to. However, this probabilistic nature also causes
    inconsistency and hallucinations.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涵盖了我最喜欢的主题之一：采样，这是模型生成输出标记的过程。采样使AI模型具有概率性。这种概率性质是使ChatGPT和Gemini等模型在创意任务中表现出色以及与人交谈有趣的原因。然而，这种概率性质也导致了不一致性和幻觉。
- en: Working with AI models requires building your workflows around their probabilistic
    nature. The rest of this book will explore how to make AI engineering, if not
    deterministic, at least systematic. The first step toward systematic AI engineering
    is to establish a solid evaluation pipeline to help detect failures and unexpected
    changes. Evaluation for foundation models is so crucial that I dedicated two chapters
    to it, starting with the next chapter.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 与AI模型合作需要围绕它们的概率性质构建你的工作流程。本书的其余部分将探讨如何使AI工程，如果不是确定性的，至少是系统化的。系统化AI工程的第一步是建立一个稳固的评估流程，以帮助检测失败和意外变化。对于基础模型的评估至关重要，以至于我专门用两章来介绍它，下一章就是从那里开始的。
- en: ^([1](ch02.html#id697-marker)) [“GPT-4 Can Solve Math Problems—but Not in All
    Languages”](https://oreil.ly/G13KM) by Yennie Jun. You can verify the study using
    [OpenAI’s Tokenizer](https://oreil.ly/iqhNY).
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#id697-marker)) [“GPT-4 Can Solve Math Problems—but Not in All
    Languages”](https://oreil.ly/G13KM) by Yennie Jun. 你可以使用 [OpenAI的Tokenizer](https://oreil.ly/iqhNY)
    来验证这项研究。
- en: ^([2](ch02.html#id699-marker)) It might be because of some biases in pre-training
    data or alignment data. Perhaps OpenAI just didn’t include as much data in the
    Chinese language or China-centric narratives to train their models.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#id699-marker)) 这可能是因为预训练数据或对齐数据中存在一些偏差。也许OpenAI在训练模型时没有包括足够的中国语言或以中国为中心的叙述数据。
- en: ^([3](ch02.html#id705-marker)) [“Inside the Secret List of Websites That Make
    AI like ChatGPT Sound Smart”](https://oreil.ly/St1o8), *Washington Post*, 2023.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#id705-marker)) [“Inside the Secret List of Websites That Make
    AI like ChatGPT Sound Smart”](https://oreil.ly/St1o8), *Washington Post*, 2023.
- en: ^([4](ch02.html#id706-marker)) For texts, you can use domain keywords as heuristics,
    but there are no obvious heuristics for images. Most analyses I could find about
    vision datasets are about image sizes, resolutions, or video lengths.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#id706-marker)) 对于文本，你可以使用领域关键词作为启发式方法，但对于图像则没有明显的启发式方法。我能找到的大多数关于视觉数据集的分析都是关于图像大小、分辨率或视频长度的。
- en: ^([5](ch02.html#id715-marker)) ML fundamentals related to model training are
    outside the scope of this book. However, when relevant to the discussion, I include
    some concepts. For example, self-supervision—where a model generates its own labels
    from the data—is covered in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    and backpropagation—how a model’s parameters are updated during training based
    on the error—is discussed in [Chapter 7](ch07.html#ch07).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#id715-marker)) 与模型训练相关的ML基础内容不在此书的范围之内。然而，当与讨论相关时，我会包括一些概念。例如，自监督——模型从数据中生成自己的标签——在[第1章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)中介绍，而反向传播——模型在训练过程中根据误差更新参数——在第7章[7](ch07.html#ch07)中讨论。
- en: ^([6](ch02.html#id719-marker)) RNNs are especially prone to vanishing and exploding
    gradients due to their recursive structure. Gradients must be propagated through
    many steps, and if they are small, repeated multiplication causes them to shrink
    toward zero, making it difficult for the model to learn. Conversely, if the gradients
    are large, they grow exponentially with each step, leading to instability in the
    learning process.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#id719-marker)) RNN由于其递归结构，特别容易发生梯度消失和梯度爆炸。梯度必须通过许多步骤传播，如果它们很小，重复的乘法会导致它们趋向于零，使得模型难以学习。相反，如果梯度很大，它们会随着每一步呈指数增长，导致学习过程中的不稳定性。
- en: ^([7](ch02.html#id720-marker)) Bahdanau et al., [“Neural Machine Translation
    by Jointly Learning to Align and Translate”](https://arxiv.org/abs/1409.0473).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.html#id720-marker)) Bahdanau等人，[“通过联合学习对齐和翻译进行神经机器翻译”](https://arxiv.org/abs/1409.0473)。
- en: ^([8](ch02.html#id727-marker)) Because input tokens are processed in batch,
    the actual input vector has the shape `N` × `T` × `4096`, where `N` is the batch
    size and T is the sequence length. Similarly, each resulting `K`, `V`, `Q` vector
    has the dimension of `N` × `T` × `4096`.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.html#id727-marker)) 由于输入标记是批量处理的，实际输入向量的形状为 `N` × `T` × `4096`，其中
    `N` 是批量大小，`T` 是序列长度。同样，每个生成的 `K`、`V`、`Q` 向量的维度也是 `N` × `T` × `4096`。
- en: ^([9](ch02.html#id739-marker)) Why do simple activation functions work for complex
    models like LLMs? There was a time when the research community raced to come up
    with sophisticated activation functions. However, it turned out that fancier activation
    functions didn’t work better. The model just needs a nonlinear function to break
    the linearity from the feedforward layers. Simpler functions that are faster to
    compute are better, as the more sophisticated ones take up too much training compute
    and memory.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.html#id739-marker)) 为什么简单的激活函数对复杂的模型如LLMs也有效？曾经有一段时间，研究社区竞相提出复杂的激活函数。然而，结果证明，更复杂的激活函数并没有带来更好的效果。模型只需要一个非线性函数来打破前馈层的线性关系。计算速度更快的简单函数更好，因为更复杂的函数会占用过多的训练计算和内存。
- en: '^([10](ch02.html#id744-marker)) Fun fact: Ilya Sutskever, an OpenAI co-founder,
    is the first author on the seq2seq paper and the second author on the AlexNet
    paper.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.html#id744-marker)) 有趣的事实：OpenAI的联合创始人Ilya Sutskever是seq2seq论文的第一作者，AlexNet论文的第二作者。
- en: ^([11](ch02.html#id745-marker)) Ilya Sutskever has an interesting argument about
    why it’s so hard to develop new neural network architectures to outperform existing
    ones. In his argument, neural networks are great at simulating many computer programs.
    Gradient descent, a technique to train neural networks, is in fact a search algorithm
    to search through all the programs that a neural network can simulate to find
    the best one for its target task. This means that new architectures can potentially
    be simulated by existing ones too. For new architectures to outperform existing
    ones, these new architectures have to be able to simulate programs that existing
    architectures cannot. For more information, watch [Sutskever’s talk at the Simons
    Institute at Berkeley (2023)](https://oreil.ly/j4wwW).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch02.html#id745-marker)) Ilya Sutskever有一个有趣的论点，关于为什么开发新的神经网络架构以超越现有架构如此困难。在他的论点中，神经网络擅长模拟许多计算机程序。梯度下降，一种训练神经网络的技巧，实际上是一种搜索算法，用于搜索神经网络可以模拟的所有程序，以找到最适合其目标任务的程序。这意味着新的架构也可能被现有的架构模拟。为了使新的架构超越现有的架构，这些新的架构必须能够模拟现有架构无法模拟的程序。更多信息，请观看[Sutskever在伯克利西蒙斯研究所的演讲（2023）](https://oreil.ly/j4wwW)。
- en: ^([12](ch02.html#id746-marker)) The transformer was originally designed by Google
    to [run fast on Tensor Processing Units (TPUs)](https://oreil.ly/ON55d), and was
    only later optimized on GPUs.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch02.html#id746-marker)) Transformer最初由Google设计，用于在[张量处理单元（TPUs）上快速运行](https://oreil.ly/ON55d)，后来才在GPU上进行了优化。
- en: ^([13](ch02.html#id754-marker)) The actual memory needed is higher. [Chapter 7](ch07.html#ch07)
    discusses how to calculate a model’s memory usage.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch02.html#id754-marker)) 实际需要的内存更高。[第7章](ch07.html#ch07)讨论了如何计算模型的内存使用。
- en: ^([14](ch02.html#id759-marker)) Assuming a book contains around 50,000 words
    or 67,000 tokens.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch02.html#id759-marker)) 假设一本书包含大约50,000个单词或67,000个token。
- en: ^([15](ch02.html#id760-marker)) As of this writing, large models are typically
    pre-trained on only one epoch of data.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch02.html#id760-marker)) 到本文写作时为止，大型模型通常只在一个数据epoch上进行预训练。
- en: ^([16](ch02.html#id762-marker)) FLOP/s count is measured in FP32\. Floating
    point formats is discussed in [Chapter 7](ch07.html#ch07).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch02.html#id762-marker)) FLOP/s计数以FP32为单位。浮点格式在第7章（ch07.html#ch07）中讨论。
- en: ^([17](ch02.html#id763-marker)) As of this writing, cloud providers are offering
    H100s for around $2 to $5 per hour. As compute is getting rapidly cheaper, this
    number will get much lower.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch02.html#id763-marker)) 到本文写作时为止，云服务提供商提供的H100大约每小时2到5美元。随着计算成本迅速降低，这个数字将会更低。
- en: ^([18](ch02.html#id776-marker)) Jascha Sohl-Dickstein, an amazing researcher,
    [shared a beautiful visualization of what hyperparameters work and don’t work](https://x.com/jaschasd/status/1756930242965606582)
    on his X page.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch02.html#id776-marker)) 精彩的研究员Jascha Sohl-Dickstein在他的X页面上分享了一个关于哪些超参数有效，哪些无效的美丽可视化。[链接](https://x.com/jaschasd/status/1756930242965606582)。
- en: ^([19](ch02.html#id777-marker)) [Dario Amodei, Anthropic CEO](https://oreil.ly/GxSe0),
    said that if the scaling hypothesis is true, a $100 billion AI model will be as
    good as a Nobel prize winner.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch02.html#id777-marker)) [Dario Amodei，Anthropic首席执行官](https://oreil.ly/GxSe0)表示，如果规模假设是正确的，一个价值1000亿美元的AI模型将和诺贝尔奖获得者一样好。
- en: ^([20](ch02.html#id778-marker)) AI-generated content is multiplied by the ease
    of machine translation. AI can be used to generate an article, then translate
    that article into multiple languages, as shown in “A Shocking Amount of the Web
    Is Machine Translated” ([Thompson et al., 2024](https://arxiv.org/abs/2401.05749)).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch02.html#id778-marker)) 人工智能生成的内容乘以机器翻译的便捷性。人工智能可以用来生成一篇文章，然后将该文章翻译成多种语言，如“大量网络内容是机器翻译的”所示（[Thompson等人，2024](https://arxiv.org/abs/2401.05749)）。
- en: '^([21](ch02.html#id787-marker)) A friend used this analogy: a pre-trained model
    talks like a web page, not a human.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch02.html#id787-marker)) 一个朋友用了这个比喻：预训练的模型说话像网页，而不是像人。
- en: ^([22](ch02.html#id790-marker)) RL fundamentals are beyond the scope of this
    book, but the highlight is that RL lets you optimize against difficult objectives
    like human preference.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch02.html#id790-marker)) 强化学习的基本原理超出了本书的范围，但亮点是强化学习让你能够针对像人类偏好这样的困难目标进行优化。
- en: ^([23](ch02.html#id797-marker)) There are situations where misaligned models
    might be better. For example, if you want to evaluate the risk of people using
    AI to spread misinformation, you might want to try to build a model that’s as
    good at making up fake news as possible, to see how convincing AI can be.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch02.html#id797-marker)) 有时候，不匹配的模型可能更好。例如，如果您想评估人们使用AI传播虚假信息的风险，您可能想要尝试构建一个尽可能擅长编造虚假新闻的模型，以了解AI可以多么令人信服。
- en: ^([24](ch02.html#id816-marker)) A visual image I have in mind when thinking
    about temperature, which isn’t entirely scientific, is that a higher temperature
    causes the probability distribution to be more chaotic, which enables lower-probability
    tokens to surface.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch02.html#id816-marker)) 当我思考温度时，心中浮现的一个视觉图像，这并不完全科学，那就是更高的温度会导致概率分布更加混乱，从而使低概率的令牌浮现出来。
- en: ^([25](ch02.html#id817-marker)) Performing an [arg max function](https://en.wikipedia.org/wiki/Arg_max).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch02.html#id817-marker)) 执行一个[最大值函数](https://en.wikipedia.org/wiki/Arg_max)。
- en: ^([26](ch02.html#id819-marker)) The underflow problem occurs when a number is
    too small to be represented in a given format, leading to it being rounded down
    to zero.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch02.html#id819-marker)) 当一个数字太小，无法用给定格式表示时，会发生下溢问题，导致其被舍入到零。
- en: ^([27](ch02.html#id820-marker)) To be more specific, as of this writing, OpenAI
    API only shows you the [logprobs](https://oreil.ly/jWEsP) of up to the 20 most
    likely tokens. It used to let you get the logprobs of arbitrary user-provided
    text but discontinued this in [September 2023](https://x.com/xuanalogue/status/1707757449900437984).
    Anthropic doesn’t expose its models’ logprobs.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch02.html#id820-marker)) 更具体地说，在我撰写本文时，OpenAI API只显示前20个最有可能的令牌的[logprobs](https://oreil.ly/jWEsP)。它曾经允许您获取任意用户提供的文本的logprobs，但已于[2023年9月](https://x.com/xuanalogue/status/1707757449900437984)停止此功能。Anthropic不公开其模型的logprobs。
- en: ^([28](ch02.html#id829-marker)) Paid model APIs often charge per number of output
    tokens.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch02.html#id829-marker)) 支付模型API通常按输出令牌的数量收费。
- en: ^([29](ch02.html#id833-marker)) There are things you can do to reduce the cost
    of generating multiple outputs for the same input. For example, the input might
    only be processed once and reused for all outputs.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch02.html#id833-marker)) 您可以采取一些措施来降低为同一输入生成多个输出的成本。例如，输入可能只处理一次，然后用于所有输出。
- en: ^([30](ch02.html#id835-marker)) As of this writing, in the OpenAI API, you can
    set the parameter [best_of](https://oreil.ly/XYugZ) to a specific value, say 10,
    to ask OpenAI models to return the output with the highest average logprob out
    of 10 different outputs.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch02.html#id835-marker)) 在我撰写本文时，在OpenAI API中，您可以设置参数[best_of](https://oreil.ly/XYugZ)为特定值，例如10，以要求OpenAI模型从10个不同的输出中返回平均logprob最高的输出。
- en: ^([31](ch02.html#id836-marker)) [Wang et al. (2023)](https://arxiv.org/abs/2203.11171)
    called this approach self-consistency.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch02.html#id836-marker)) [王等人（2023）](https://arxiv.org/abs/2203.11171)将这种方法称为自洽性。
- en: ^([32](ch02.html#id838-marker)) The optimal thing to do with a brittle model,
    however, is to swap it out for another.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch02.html#id838-marker)) 然而，对于脆性模型来说，最佳的做法是将其替换为另一个模型。
- en: ^([33](ch02.html#id844-marker)) As of this writing, depending on the application
    and the model, I’ve seen the percentage of correctly generated JSON objects anywhere
    between 0% and up to the high 90%.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch02.html#id844-marker)) 在我撰写本文时，根据应用和模型的不同，我见过正确生成JSON对象的百分比在0%到高达90%以上之间。
- en: ^([34](ch02.html#id849-marker)) Training a model from scratch on data following
    the desirable format works too, but this book isn’t about developing models from
    scratch.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch02.html#id849-marker)) 在符合期望的数据格式上从头开始训练模型也行，但这本书不是关于从头开始开发模型的。
- en: ^([35](ch02.html#id850-marker)) Some finetuning services do this for you automatically.
    [OpenAI’s finetuning services](https://oreil.ly/sljei) used to let you add a classifier
    head when training, but as I write, this feature has been disabled.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch02.html#id850-marker)) 一些微调服务会自动为您完成这项工作。[OpenAI的微调服务](https://oreil.ly/sljei)曾经允许您在训练时添加分类器头部，但在我撰写本文时，这个功能已被禁用。
- en: ^([36](ch02.html#id857-marker)) As the meme says, [the chances are low, but
    never zero](https://x.com/OxfordDiplomat/status/1424388443010998277?lang=en).
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ^([36](ch02.html#id857-marker)) 正如梗图所说，[可能性很低，但永远不会为零](https://x.com/OxfordDiplomat/status/1424388443010998277?lang=en)。
- en: ^([37](ch02.html#id858-marker)) In December 2023, I went over three months’
    worth of customer support requests for an AI company I advised and found that
    one-fifth of the questions were about handling the inconsistency of AI models.
    In a panel I participated in with Drew Houston (CEO of Dropbox) and Harrison Chase
    (CEO of LangChain) in July 2023, we all agreed that hallucination is the biggest
    blocker for many AI enterprise use cases.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ^([37](ch02.html#id858-marker)) 2023年12月，我回顾了我所咨询的一家AI公司的三个月客户支持请求，发现其中五分之一的问题都与处理AI模型的不一致性有关。在2023年7月与Drew
    Houston（Dropbox的CEO）和Harrison Chase（LangChain的CEO）共同参与的讨论中，我们都认为幻觉是许多AI企业用例的最大障碍。
