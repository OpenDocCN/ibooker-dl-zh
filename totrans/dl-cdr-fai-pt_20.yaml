- en: Chapter 16\. The Training Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章。训练过程
- en: You now know how to create state-of-the-art architectures for computer vision,
    natural image processing, tabular analysis, and collaborative filtering, and you
    know how to train them quickly. So we’re done, right? Not quite yet. We still
    have to explore a little bit more of the training process.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何为计算机视觉、自然图像处理、表格分析和协同过滤创建最先进的架构，也知道如何快速训练它们。所以我们完成了，对吧？还没有。我们仍然需要探索一下训练过程的更多内容。
- en: 'We explained in [Chapter 4](ch04.xhtml#chapter_mnist_basics) the basis of stochastic
    gradient descent: pass a mini-batch to the model, compare it to our target with
    the loss function, then compute the gradients of this loss function with regard
    to each weight before updating the weights with the formula:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](ch04.xhtml#chapter_mnist_basics)中解释了随机梯度下降的基础：将一个小批量数据传递给模型，用损失函数将其与目标进行比较，然后计算这个损失函数对每个权重的梯度，然后使用公式更新权重：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We implemented this from scratch in a training loop, and saw that PyTorch provides
    a simple `nn.SGD` class that does this calculation for each parameter for us.
    In this chapter, we will build some faster optimizers, using a flexible foundation.
    But that’s not all we might want to change in the training process. For any tweak
    of the training loop, we will need a way to add some code to the basis of SGD.
    The fastai library has a system of callbacks to do this, and we will teach you
    all about it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练循环中从头开始实现了这个，看到PyTorch提供了一个简单的`nn.SGD`类，可以为我们的每个参数进行这个计算。在本章中，我们将构建一些更快的优化器，使用一个灵活的基础。但在训练过程中，我们可能还想要改变一些东西。对于训练循环的任何调整，我们都需要一种方法来向SGD的基础添加一些代码。fastai库有一个回调系统来做到这一点，我们将教你所有相关知识。
- en: Let’s start with standard SGD to get a baseline; then we will introduce the
    most commonly used optimizers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从标准的SGD开始建立一个基线；然后我们将介绍最常用的优化器。
- en: Establishing a Baseline
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立基线
- en: 'First we’ll create a baseline using plain SGD and compare it to fastai’s default
    optimizer. We’ll start by grabbing Imagenette with the same `get_data` we used
    in [Chapter 14](ch14.xhtml#chapter_resnet):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用普通的SGD创建一个基线，并将其与fastai的默认优化器进行比较。我们将通过使用与[第14章](ch14.xhtml#chapter_resnet)中相同的`get_data`来获取Imagenette：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll create a ResNet-34 without pretraining and pass along any arguments received:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个没有预训练的ResNet-34，并传递任何接收到的参数：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s the default fastai optimizer, with the usual 3e-3 learning rate:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是默认的fastai优化器，具有通常的3e-3学习率：
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.571932 | 2.685040 | 0.322548 | 00:11 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.571932 | 2.685040 | 0.322548 | 00:11 |'
- en: '| 1 | 1.904674 | 1.852589 | 0.437452 | 00:11 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.904674 | 1.852589 | 0.437452 | 00:11 |'
- en: '| 2 | 1.586909 | 1.374908 | 0.594904 | 00:11 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.586909 | 1.374908 | 0.594904 | 00:11 |'
- en: 'Now let’s try plain SGD. We can pass `opt_func` (optimization function) to
    `cnn_learner` to get fastai to use any optimizer:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试普通的SGD。我们可以将`opt_func`（优化函数）传递给`cnn_learner`，以便让fastai使用任何优化器：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first thing to look at is `lr_find`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要看的是`lr_find`：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](Images/dlcf_16in01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_16in01.png)'
- en: 'It looks like we’ll need to use a higher learning rate than we normally use:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们需要使用比我们通常使用的更高的学习率：
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.969412 | 2.214596 | 0.242038 | 00:09 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.969412 | 2.214596 | 0.242038 | 00:09 |'
- en: '| 1 | 2.442730 | 1.845950 | 0.362548 | 00:09 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.442730 | 1.845950 | 0.362548 | 00:09 |'
- en: '| 2 | 2.157159 | 1.741143 | 0.408917 | 00:09 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2.157159 | 1.741143 | 0.408917 | 00:09 |'
- en: Because accelerating SGD with momentum is such a good idea, fastai does this
    by default in `fit_one_cycle`, so we turn it off with `moms=(0,0,0)`. We’ll be
    discussing momentum shortly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为用动量加速SGD是一个很好的主意，fastai在`fit_one_cycle`中默认执行这个操作，所以我们用`moms=(0,0,0)`关闭它。我们很快会讨论动量。
- en: Clearly, plain SGD isn’t training as fast as we’d like. So let’s learn some
    tricks to get accelerated training!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，普通的SGD训练速度不如我们所希望的快。所以让我们学习一些技巧来加速训练！
- en: A Generic Optimizer
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用优化器
- en: 'To build up our accelerated SGD tricks, we’ll need to start with a nice flexible
    optimizer foundation. No library prior to fastai provided such a foundation, but
    during fastai’s development, we realized that all the optimizer improvements we’d
    seen in the academic literature could be handled using *optimizer callbacks*.
    These are small pieces of code that we can compose, mix, and match in an optimizer
    to build the optimizer step. They are called by fastai’s lightweight `Optimizer`
    class. These are the definitions in `Optimizer` of the two key methods that we’ve
    been using in this book:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们加速的SGD技巧，我们需要从一个灵活的优化器基础开始。在fastai之前没有任何库提供这样的基础，但在fastai的开发过程中，我们意识到学术文献中看到的所有优化器改进都可以使用*优化器回调*来处理。这些是我们可以组合、混合和匹配在优化器中构建优化器步骤的小代码片段。它们由fastai的轻量级`Optimizer`类调用。这些是我们在本书中使用的两个关键方法在`Optimizer`中的定义：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we saw when training an MNIST model from scratch, `zero_grad` just loops
    through the parameters of the model and sets the gradients to zero. It also calls
    `detach_`, which removes any history of gradient computation, since it won’t be
    needed after `zero_grad`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在从头开始训练MNIST模型时看到的，`zero_grad`只是循环遍历模型的参数并将梯度设置为零。它还调用`detach_`，这会删除任何梯度计算的历史，因为在`zero_grad`之后不再需要它。
- en: The more interesting method is `step`, which loops through the callbacks (`cbs`)
    and calls them to update the parameters (the `_update` function just calls `state.update`
    if there’s anything returned by `cb`). As you can see, `Optimizer` doesn’t do
    any SGD steps itself. Let’s see how we can add SGD to `Optimizer`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的方法是`step`，它循环遍历回调（`cbs`）并调用它们来更新参数（如果`cb`返回任何内容，`_update`函数只是调用`state.update`）。正如你所看到的，`Optimizer`本身不执行任何SGD步骤。让我们看看如何将SGD添加到`Optimizer`中。
- en: 'Here’s an optimizer callback that does a single SGD step, by multiplying `-lr`
    by the gradients and adding that to the parameter (when `Tensor.add_` in PyTorch
    is passed two parameters, they are multiplied together before the addition):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个优化器回调，通过将`-lr`乘以梯度并将其添加到参数（当在PyTorch中传递`Tensor.add_`两个参数时，它们在相加之前相乘）来执行单个SGD步骤：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can pass this to `Optimizer` using the `cbs` parameter; we’ll need to use
    `partial` since `Learner` will call this function to create our optimizer later:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`cbs`参数将这个传递给`Optimizer`；我们需要使用`partial`，因为`Learner`将调用这个函数来创建我们的优化器：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s see if this trains:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否有效：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.730918 | 2.009971 | 0.332739 | 00:09 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.730918 | 2.009971 | 0.332739 | 00:09 |'
- en: '| 1 | 2.204893 | 1.747202 | 0.441529 | 00:09 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.204893 | 1.747202 | 0.441529 | 00:09 |'
- en: '| 2 | 1.875621 | 1.684515 | 0.445350 | 00:09 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.875621 | 1.684515 | 0.445350 | 00:09 |'
- en: It’s working! So that’s how we create SGD from scratch in fastai. Now let’s
    see what this “momentum” is.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它正在工作！这就是我们如何在fastai中从头开始创建SGD。现在让我们看看这个“动量”是什么。
- en: Momentum
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量
- en: As described in [Chapter 4](ch04.xhtml#chapter_mnist_basics), SGD can be thought
    of as standing at the top of a mountain and working your way down by taking a
    step in the direction of the steepest slope at each point in time. But what if
    we have a ball rolling down the mountain? It won’t, at each given point, exactly
    follow the direction of the gradient, as it will have *momentum*. A ball with
    more momentum (for instance, a heavier ball) will skip over little bumps and holes,
    and be more likely to get to the bottom of a bumpy mountain. A ping pong ball,
    on the other hand, will get stuck in every little crevice.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第4章](ch04.xhtml#chapter_mnist_basics)中所描述的，SGD可以被看作站在山顶上，通过在每个时间点沿着最陡峭的斜坡方向迈出一步来往下走。但如果我们有一个球在山上滚动呢？在每个给定点，它不会完全按照梯度的方向前进，因为它会有*动量*。具有更多动量的球（例如，一个更重的球）会跳过小凸起和洞，更有可能到达崎岖山脉的底部。另一方面，乒乓球会卡在每一个小缝隙中。
- en: 'So how can we bring this idea over to SGD? We can use a moving average, instead
    of only the current gradient, to make our step:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何将这个想法带到SGD中呢？我们可以使用移动平均值，而不仅仅是当前梯度，来进行我们的步骤：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here `beta` is some number we choose that defines how much momentum to use.
    If `beta` is 0, the first equation becomes `weight.avg = weight.grad`, so we end
    up with plain SGD. But if it’s a number close to 1, the main direction chosen
    is an average of the previous steps. (If you have done a bit of statistics, you
    may recognize in the first equation an *exponentially weighted moving average*,
    which is often used to denoise data and get the underlying tendency.)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`beta`是我们选择的一个数字，定义了要使用多少动量。如果`beta`为0，第一个方程变为`weight.avg = weight.grad`，因此我们最终得到普通的SGD。但如果它接近1，所选择的主要方向是以前步骤的平均值。（如果你对统计学有一点了解，你可能会在第一个方程中认出*指数加权移动平均*，它经常用于去噪数据并获得潜在趋势。）
- en: Note that we are writing `weight.avg` to highlight the fact that we need to
    store the moving averages for each parameter of the model (they all their own
    independent moving averages).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们写`weight.avg`以突出显示我们需要为模型的每个参数存储移动平均值（它们都有自己独立的移动平均值）。
- en: '[Figure 16-1](#img_momentum) shows an example of noisy data for a single parameter
    with the momentum curve plotted in red, and the gradients of the parameter plotted
    in blue. The gradients increase, then decrease, and the momentum does a good job
    of following the general trend without getting too influenced by noise.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-1](#img_momentum)显示了一个单参数的噪声数据示例，其中动量曲线以红色绘制，参数的梯度以蓝色绘制。梯度增加，然后减少，动量很好地跟随总体趋势，而不会受到噪声的太大影响。'
- en: '![Graph showing an example of momentum](Images/dlcf_1601.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![显示动量示例的图表](Images/dlcf_1601.png)'
- en: Figure 16-1\. An example of momentum
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-1。动量的一个例子
- en: 'It works particularly well if the loss function has narrow canyons we need
    to navigate: vanilla SGD would send us bouncing from one side to the other, while
    SGD with momentum will average those to roll smoothly down the side. The parameter
    `beta` determines the strength of the momentum we are using: with a small `beta`,
    we stay closer to the actual gradient values, whereas with a high `beta`, we will
    mostly go in the direction of the average of the gradients and it will take a
    while before any change in the gradients makes that trend move.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果损失函数有窄谷，我们需要导航：普通的SGD会使我们从一边反弹到另一边，而带有动量的SGD会将这些平均值平滑地滚动到一侧。参数`beta`确定我们使用的动量的强度：使用较小的`beta`，我们会保持接近实际梯度值，而使用较高的`beta`，我们将主要朝着梯度的平均值前进，直到梯度的任何变化使得该趋势移动。
- en: 'With a large `beta`, we might miss that the gradients have changed directions
    and roll over a small local minima. This is a desired side effect: intuitively,
    when we show a new input to our model, it will look like something in the training
    set but won’t be *exactly* like it. It will correspond to a point in the loss
    function that is close to the minimum we ended up with at the end of training,
    but not exactly *at* that minimum. So, we would rather end up training in a wide
    minimum, where nearby points have approximately the same loss (or if you prefer,
    a point where the loss is as flat as possible). [Figure 16-2](#img_betas) shows
    how the chart in [Figure 16-1](#img_momentum) varies as we change `beta`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较大的`beta`，我们可能会错过梯度改变方向并滚动到一个小的局部最小值。这是一个期望的副作用：直观地，当我们向模型展示一个新的输入时，它会看起来像训练集中的某个东西，但不会*完全*像它。它将对应于损失函数中接近我们在训练结束时得到的最小值的点，但不会*在*那个最小值。因此，我们宁愿在一个宽阔的最小值中进行训练，附近的点具有近似相同的损失（或者如果你喜欢的话，损失尽可能平坦的点）。[图16-2](#img_betas)显示了当我们改变`beta`时，[图16-1](#img_momentum)中的图表如何变化。
- en: '![Graph showing how the beta value imfluence momentum](Images/dlcf_1602.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![显示beta值如何影响动量的图表](Images/dlcf_1602.png)'
- en: Figure 16-2\. Momentum with different beta values
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-2。不同beta值的动量
- en: We can see in these examples that a `beta` that’s too high results in the overall
    changes in gradient getting ignored. In SGD with momentum, a value of `beta` that
    is often used is 0.9.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在这些示例中，`beta`太高会导致梯度的整体变化被忽略。在带动量的SGD中，通常使用的`beta`值为0.9。
- en: '`fit_one_cycle` by default starts with a `beta` of 0.95, gradually adjusts
    it to 0.85, and then gradually moves it back to 0.95 at the end of training. Let’s
    see how our training goes with momentum added to plain SGD.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit_one_cycle`默认从0.95开始，逐渐调整到0.85，然后在训练结束时逐渐移回到0.95。让我们看看在普通SGD中添加动量后我们的训练情况如何。'
- en: 'To add momentum to our optimizer, we’ll first need to keep track of the moving
    average gradient, which we can do with another callback. When an optimizer callback
    returns a `dict`, it is used to update the state of the optimizer and is passed
    back to the optimizer on the next step. So this callback will keep track of the
    gradient averages in a parameter called `grad_avg`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要向我们的优化器添加动量，我们首先需要跟踪移动平均梯度，我们可以使用另一个回调来实现。当优化器回调返回一个`dict`时，它用于更新优化器的状态，并在下一步传回优化器。因此，这个回调将跟踪梯度平均值，存储在名为`grad_avg`的参数中：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To use it, we just have to replace `p.grad.data` with `grad_avg` in our step
    function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，我们只需在我们的步骤函数中用`grad_avg`替换`p.grad.data`：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Learner` will automatically schedule `mom` and `lr`, so `fit_one_cycle` will
    even work with our custom `Optimizer`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`Learner`将自动调度`mom`和`lr`，因此`fit_one_cycle`甚至可以与我们自定义的`Optimizer`一起使用：'
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.856000 | 2.493429 | 0.246115 | 00:10 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.856000 | 2.493429 | 0.246115 | 00:10 |'
- en: '| 1 | 2.504205 | 2.463813 | 0.348280 | 00:10 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.504205 | 2.463813 | 0.348280 | 00:10 |'
- en: '| 2 | 2.187387 | 1.755670 | 0.418853 | 00:10 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2.187387 | 1.755670 | 0.418853 | 00:10 |'
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](Images/dlcf_16in02.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_16in02.png)'
- en: We’re still not getting great results, so let’s see what else we can do.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然没有得到很好的结果，所以让我们看看我们还能做什么。
- en: RMSProp
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RMSProp
- en: 'RMSProp is another variant of SGD introduced by Geoffrey Hinton in [Lecture
    6e of his Coursera class “Neural Networks for Machine Learning”](https://oreil.ly/FVcIE).
    The main difference from SGD is that it uses an adaptive learning rate: instead
    of using the same learning rate for every parameter, each parameter gets its own
    specific learning rate controlled by a global learning rate. That way, we can
    speed up training by giving a higher learning rate to the weights that need to
    change a lot, while the ones that are good enough get a lower learning rate.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp是由Geoffrey Hinton在[他的Coursera课程“神经网络机器学习”第6讲e](https://oreil.ly/FVcIE)中介绍的SGD的另一种变体。与SGD的主要区别在于它使用自适应学习率：每个参数都有自己特定的学习率，由全局学习率控制。这样，我们可以通过为需要大幅度改变的权重提供更高的学习率来加速训练，而对于已经足够好的权重，则提供较低的学习率。
- en: How do we decide which parameters should have a high learning rate and which
    should not? We can look at the gradients to get an idea. If a parameter’s gradients
    have been close to zero for a while, that parameter will need a higher learning
    rate because the loss is flat. On the other hand, if the gradients are all over
    the place, we should probably be careful and pick a low learning rate to avoid
    divergence. We can’t just average the gradients to see if they’re changing a lot,
    because the average of a large positive and a large negative number is close to
    zero. Instead, we can use the usual trick of either taking the absolute value
    or the squared values (and then taking the square root after the mean).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何决定哪些参数应该具有较高的学习率，哪些不应该？我们可以查看梯度来获取一个想法。如果一个参数的梯度一直接近于零，那么该参数将需要更高的学习率，因为损失是平的。另一方面，如果梯度到处都是，我们可能应该小心并选择一个较低的学习率以避免发散。我们不能简单地平均梯度来查看它们是否变化很多，因为大正数和大负数的平均值接近于零。相反，我们可以使用通常的技巧，即取绝对值或平方值（然后在平均后取平方根）。
- en: 'Once again, to determine the general tendency behind the noise, we will use
    a moving average—specifically, the moving average of the gradients squared. Then
    we will update the corresponding weight by using the current gradient (for the
    direction) divided by the square root of this moving average (that way, if it’s
    low, the effective learning rate will be higher, and if it’s high, the effective
    learning rate will be lower):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了确定噪声背后的一般趋势，我们将使用移动平均值，具体来说是梯度的平方的移动平均值。然后，我们将通过使用当前梯度（用于方向）除以这个移动平均值的平方根来更新相应的权重（这样，如果它很低，有效的学习率将更高，如果它很高，有效的学习率将更低）：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `eps` (*epsilon*) is added for numerical stability (usually set at 1e-8),
    and the default value for `alpha` is usually 0.99.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`eps`（*epsilon*）是为了数值稳定性而添加的（通常设置为1e-8），`alpha`的默认值通常为0.99。'
- en: 'We can add this to `Optimizer` by doing much the same thing we did for `avg_grad`,
    but with an extra `**2`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过做与`avg_grad`类似的事情将其添加到`Optimizer`中，但是多了一个`**2`：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And we can define our step function and optimizer as before:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前一样定义我们的步骤函数和优化器：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s try it out:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.766912 | 1.845900 | 0.402548 | 00:11 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.766912 | 1.845900 | 0.402548 | 00:11 |'
- en: '| 1 | 2.194586 | 1.510269 | 0.504459 | 00:11 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.194586 | 1.510269 | 0.504459 | 00:11 |'
- en: '| 2 | 1.869099 | 1.447939 | 0.544968 | 00:11 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.869099 | 1.447939 | 0.544968 | 00:11 |'
- en: Much better! Now we just have to bring these ideas together, and we have Adam,
    fastai’s default optimizer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！现在我们只需将这些想法结合起来，我们就有了Adam，fastai的默认优化器。
- en: Adam
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam
- en: 'Adam mixes the ideas of SGD with momentum and RMSProp together: it uses the
    moving average of the gradients as a direction and divides by the square root
    of the moving average of the gradients squared to give an adaptive learning rate
    to each parameter.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'Adam将SGD与动量和RMSProp的思想结合在一起：它使用梯度的移动平均作为方向，并除以梯度平方的移动平均的平方根，为每个参数提供自适应学习率。 '
- en: There is one other difference in how Adam calculates moving averages. It takes
    the *unbiased* moving average, which is
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: if we are the `i`-th iteration (starting at 0 as Python does). This divisor
    of `1 - (beta**(i+1))` makes sure the unbiased average looks more like the gradients
    at the beginning (since `beta < 1`, the denominator is very quickly close to 1).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting everything together, our update step looks like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As for RMSProp, `eps` is usually set to 1e-8, and the default for `(beta1,beta2)`
    suggested by the literature is `(0.9,0.999)`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: In fastai, Adam is the default optimizer we use since it allows faster training,
    but we’ve found that `beta2=0.99` is better suited to the type of schedule we
    are using. `beta1` is the momentum parameter, which we specify with the argument
    `moms` in our call to `fit_one_cycle`. As for `eps`, fastai uses a default of
    1e-5\. `eps` is not just useful for numerical stability. A higher `eps` limits
    the maximum value of the adjusted learning rate. To take an extreme example, if
    `eps` is 1, then the adjusted learning will never be higher than the base learning
    rate.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Rather than show all the code for this in the book, we’ll let you look at the
    optimizer notebook in fastai’s [*https://oreil.ly/*](https://oreil.ly/)*24_O[GitHub
    repository] (browse the _nbs* folder and search for the notebook called *optimizer*).
    You’ll see all the code we’ve shown so far, along with Adam and other optimizers,
    and lots of examples and tests.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: One thing that changes when we go from SGD to Adam is the way we apply weight
    decay, and it can have important consequences.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Decoupled Weight Decay
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Weight decay, which we’ve discussed in [Chapter 8](ch08.xhtml#chapter_collab),
    is equivalent to (in the case of vanilla SGD) updating the parameters with the
    following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The last part of that formula explains the name of this technique: each weight
    is decayed by a factor of `lr * wd`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The other name for weight decay is *L2 regularization*, which consists of adding
    the sum of all squared weights to the loss (multiplied by the weight decay). As
    we saw in [Chapter 8](ch08.xhtml#chapter_collab), this can be directly expressed
    on the gradients:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For SGD, those two formulas are equivalent. However, this equivalence holds
    only for standard SGD because, as we’ve seen with momentum, RMSProp, or in Adam,
    the update has some additional formulas around the gradient.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Most libraries use the second formulation, but it was pointed out in [“Decoupled
    Weight Decay Regularization”](https://oreil.ly/w37Ac) by Ilya Loshchilov and Frank
    Hutter that the first one is the only correct approach with the Adam optimizer
    or momentum, which is why fastai makes it its default.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Now you know everything that is hidden behind the line `learn.fit_one_cycle`!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers are only one part of the training process, however. When you need
    to change the training loop with fastai, you can’t directly change the code inside
    the library. Instead, we have designed a system of callbacks to let you write
    any tweaks you like in independent blocks that you can then mix and match.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes you need to change how things work a little bit. In fact, we have
    already seen examples of this: Mixup, fp16 training, resetting the model after
    each epoch for training RNNs, and so forth. How do we go about making these kinds
    of tweaks to the training process?'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen the basic training loop, which, with the help of the `Optimizer`
    class, looks like this for a single epoch:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Figure 16-3](#basic_loop) shows how to picture that.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic training loop](Images/dlcf_1603.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. Basic training loop
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The usual way for deep learning practitioners to customize the training loop
    is to make a copy of an existing training loop, and then insert the code necessary
    for their particular changes into it. This is how nearly all code that you find
    online will look. But it has serious problems.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: It’s not likely that some particular tweaked training loop is going to meet
    your particular needs. Hundreds of changes can be made to a training loop, which
    means there are billions and billions of possible permutations. You can’t just
    copy one tweak from a training loop here, another from a training loop there,
    and expect them all to work together. Each will be based on different assumptions
    about the environment that it’s working in, use different naming conventions,
    and expect the data to be in different formats.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 某个特定调整过的训练循环不太可能满足您的特定需求。可以对训练循环进行数百次更改，这意味着可能有数十亿种可能的排列组合。您不能只是从这里的一个训练循环中复制一个调整，从那里的另一个训练循环中复制另一个调整，然后期望它们都能一起工作。每个都将基于对其所在环境的不同假设，使用不同的命名约定，并期望数据以不同的格式存在。
- en: 'We need a way to allow users to insert their own code at any part of the training
    loop, but in a consistent and well-defined way. Computer scientists have already
    come up with an elegant solution: the callback. A *callback* is a piece of code
    that you write and inject into another piece of code at a predefined point. In
    fact, callbacks have been used with deep learning training loops for years. The
    problem is that in previous libraries, it was possible to inject code in only
    a small subset of places where this may have been required—and, more importantly,
    callbacks were not able to do all the things they needed to do.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方法，允许用户在训练循环的任何部分插入自己的代码，但以一种一致和明确定义的方式。计算机科学家已经提出了一个优雅的解决方案：回调。*回调*是您编写并注入到另一段代码中的代码片段，在预定义的点执行。事实上，回调已经多年用于深度学习训练循环。问题在于，在以前的库中，只能在可能需要的一小部分地方注入代码——更重要的是，回调无法执行它们需要执行的所有操作。
- en: In order to be just as flexible as manually copying and pasting a training loop
    and directly inserting code into it, a callback must be able to read every possible
    piece of information available in the training loop, modify all of it as needed,
    and fully control when a batch, epoch, or even the whole training loop should
    be terminated. fastai is the first library to provide all of this functionality.
    It modifies the training loop so it looks like [Figure 16-4](#cb_loop).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够像手动复制和粘贴训练循环并直接插入代码一样灵活，回调必须能够读取训练循环中的所有可能信息，根据需要修改所有信息，并完全控制批次、周期甚至整个训练循环何时应该终止。fastai是第一个提供所有这些功能的库。它修改了训练循环，使其看起来像[图16-4](#cb_loop)。
- en: '![Training loop with callbacks](Images/dlcf_1604.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![带有回调的训练循环](Images/dlcf_1604.png)'
- en: Figure 16-4\. Training loop with callbacks
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4. 带有回调的训练循环
- en: The effectiveness of this approach has been borne out over the last couple of
    years—by using the fastai callback system, we were able to implement every single
    new paper we tried and fulfill every user request for modifying the training loop.
    The training loop itself has not required modifications. [Figure 16-5](#some_cbs)
    shows just a few of the callbacks that have been added.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的有效性在过去几年中得到了验证——通过使用fastai回调系统，我们能够实现我们尝试的每一篇新论文，并满足每一个修改训练循环的用户请求。训练循环本身并不需要修改。[图16-5](#some_cbs)展示了添加的一些回调。
- en: '![Some fastai callbacks](Images/dlcf_1605.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![一些fastai回调](Images/dlcf_1605.png)'
- en: Figure 16-5\. Some fastai callbacks
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-5. 一些fastai回调
- en: This is important because it means that whatever ideas we have in our heads,
    we can implement them. We need never dig into the source code of PyTorch or fastai
    and hack together a one-off system to try out our ideas. And when we do implement
    our own callbacks to develop our own ideas, we know that they will work together
    with all of the other functionality provided by fastai—so we will get progress
    bars, mixed-precision training, hyperparameter annealing, and so forth.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为这意味着我们头脑中的任何想法，我们都可以实现。我们永远不需要深入PyTorch或fastai的源代码，并临时拼凑一个系统来尝试我们的想法。当我们实现自己的回调来开发自己的想法时，我们知道它们将与fastai提供的所有其他功能一起工作——因此我们将获得进度条、混合精度训练、超参数退火等等。
- en: Another advantage is that it makes it easy to gradually remove or add functionality
    and perform ablation studies. You just need to adjust the list of callbacks you
    pass along to your fit function.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优点是，它使逐渐删除或添加功能以及执行消融研究变得容易。您只需要调整传递给fit函数的回调列表。
- en: 'As an example, here is the fastai source code that is run for each batch of
    the training loop:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是每个训练循环批次运行的fastai源代码：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The calls of the form `self('...')` are where the callbacks are called. As you
    see, this happens after every step. The callback will receive the entire state
    of training and can also modify it. For instance, the input data and target labels
    are in `self.xb` and `self.yb`, respectively; a callback can modify these to modify
    the data the training loop sees. It can also modify `self.loss` or even the gradients.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 形式为`self('...')`的调用是回调被调用的地方。正如您所看到的，这发生在每一步之后。回调将接收整个训练状态，并且还可以修改它。例如，输入数据和目标标签分别在`self.xb`和`self.yb`中；回调可以修改这些以修改训练循环看到的数据。它还可以修改`self.loss`甚至梯度。
- en: Let’s see how this works in practice by writing a callback.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过编写一个回调来看看这在实践中是如何工作的。
- en: Creating a Callback
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建回调
- en: 'When you want to write your own callback, the full list of available events
    is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要编写自己的回调时，可用事件的完整列表如下：
- en: '`begin_fit`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`begin_fit`'
- en: Called before doing anything; ideal for initial setup.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在做任何事情之前调用；适用于初始设置。
- en: '`begin_epoch`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`begin_epoch`'
- en: Called at the beginning of each epoch; useful for any behavior you need to reset
    at each epoch.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期开始时调用；对于需要在每个周期重置的任何行为都很有用。
- en: '`begin_train`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`begin_train`'
- en: Called at the beginning of the training part of an epoch.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在周期的训练部分开始时调用。
- en: '`begin_batch`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`begin_batch`'
- en: Called at the beginning of each batch, just after drawing said batch. It can
    be used to do any setup necessary for the batch (like hyperparameter scheduling)
    or to change the input/target before it goes into the model (for instance, by
    applying Mixup).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批次开始时调用，就在绘制该批次之后。可以用于对批次进行任何必要的设置（如超参数调度）或在输入/目标进入模型之前对其进行更改（例如，通过应用Mixup）。
- en: '`after_pred`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_pred`'
- en: Called after computing the output of the model on the batch. It can be used
    to change that output before it’s fed to the loss function.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算模型对批次的输出后调用。可以用于在将其馈送到损失函数之前更改该输出。
- en: '`after_loss`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_loss`'
- en: Called after the loss has been computed, but before the backward pass. It can
    be used to add a penalty to the loss (AR or TAR in RNN training, for instance).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失之后但在反向传播之前调用。可以用于向损失添加惩罚（例如在RNN训练中的AR或TAR）。
- en: '`after_backward`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_backward`'
- en: Called after the backward pass, but before the update of the parameters. It
    can be used to make changes to the gradients before said update (via gradient
    clipping, for instance).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播之后调用，但在参数更新之前调用。可以在更新之前对梯度进行更改（例如通过梯度裁剪）。
- en: '`after_step`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_step`'
- en: Called after the step and before the gradients are zeroed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤之后和梯度归零之前调用。
- en: '`after_batch`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_batch`'
- en: Called at the end of a batch, to perform any required cleanup before the next
    one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在批次结束时调用，以在下一个批次之前执行任何必要的清理。
- en: '`after_train`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_train`'
- en: Called at the end of the training phase of an epoch.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在时代的训练阶段结束时调用。
- en: '`begin_validate`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`begin_validate`'
- en: Called at the beginning of the validation phase of an epoch; useful for any
    setup needed specifically for validation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在时代的验证阶段开始时调用；用于特定于验证所需的任何设置。
- en: '`after_validate`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_validate`'
- en: Called at the end of the validation part of an epoch.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在时代的验证部分结束时调用。
- en: '`after_epoch`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_epoch`'
- en: Called at the end of an epoch, for any cleanup before the next one.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在时代结束时调用，进行下一个时代之前的任何清理。
- en: '`after_fit`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`after_fit`'
- en: Called at the end of training, for final cleanup.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练结束时调用，进行最终清理。
- en: The elements of this list are available as attributes of the special variable
    `event`, so you can just type `event.` and hit Tab in your notebook to see a list
    of all the options
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表的元素可作为特殊变量`event`的属性使用，因此您只需在笔记本中键入`event.`并按Tab键即可查看所有选项的列表
- en: 'Let’s take a look at an example. Do you recall how in [Chapter 12](ch12.xhtml#chapter_nlp_dive)
    we needed to ensure that our special `reset` method was called at the start of
    training and validation for each epoch? We used the `ModelResetter` callback provided
    by fastai to do this for us. But how does it work exactly? Here’s the full source
    code for that class:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。您是否还记得在[第12章](ch12.xhtml#chapter_nlp_dive)中我们需要确保在每个时代的训练和验证开始时调用我们的特殊`reset`方法？我们使用fastai提供的`ModelResetter`回调来为我们执行此操作。但它究竟是如何工作的呢？这是该类的完整源代码：
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Yes, that’s actually it! It just does what we said in the preceding paragraph:
    after completing training or validation for an epoch, call a method named `reset`.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，实际上就是这样！它只是在完成时代的训练或验证后，调用一个名为`reset`的方法。
- en: 'Callbacks are often “short and sweet” like this one. In fact, let’s look at
    one more. Here’s the fastai source for the callback that adds RNN regularization
    (AR and TAR):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回调通常像这样“简短而甜美”。实际上，让我们再看一个。这是添加RNN正则化（AR和TAR）的fastai回调的源代码：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Code It Yourself
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自己编写代码。
- en: Go back and reread [“Activation Regularization and Temporal Activation Regularization”](ch12.xhtml#AR_and_TAR),
    and then take another look at the code here. Make sure you understand what it’s
    doing and why.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 回去重新阅读[“激活正则化和时间激活正则化”](ch12.xhtml#AR_and_TAR)，然后再看看这里的代码。确保您理解它在做什么以及为什么。
- en: In both of these examples, notice how we can access attributes of the training
    loop by directly checking `self.model` or `self.pred`. That’s because a `Callback`
    will always try to get an attribute it doesn’t have inside the `Learner` associated
    with it. These are shortcuts for `self.learn.model` or `self.learn.pred`. Note
    that they work for reading attributes, but not for writing them, which is why
    when `RNNRegularizer` changes the loss or the predictions, you see `self.learn.loss
    =` or `self.learn.pred =`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个示例中，请注意我们如何可以通过直接检查`self.model`或`self.pred`来访问训练循环的属性。这是因为`Callback`将始终尝试获取其内部`Learner`中没有的属性。这些是`self.learn.model`或`self.learn.pred`的快捷方式。请注意，它们适用于读取属性，但不适用于编写属性，这就是为什么当`RNNRegularizer`更改损失或预测时，您会看到`self.learn.loss
    =`或`self.learn.pred =`。
- en: 'When writing a callback, the following attributes of `Learner` are available:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写回调时，可以直接检查`Learner`的以下属性：
- en: '`model`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`'
- en: The model used for training/validation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练/验证的模型。
- en: '`data`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`data`'
- en: The underlying `DataLoaders`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 底层的`DataLoaders`。
- en: '`loss_func`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss_func`'
- en: The loss function used.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的损失函数。
- en: '`opt`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`opt`'
- en: The optimizer used to update the model parameters.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 用于更新模型参数的优化器。
- en: '`opt_func`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`opt_func`'
- en: The function used to create the optimizer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建优化器的函数。
- en: '`cbs`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`cbs`'
- en: The list containing all the `Callback`s.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 包含所有`Callback`的列表。
- en: '`dl`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`dl`'
- en: The current `DataLoader` used for iteration.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 用于迭代的当前`DataLoader`。
- en: '`x`/`xb`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`/`xb`'
- en: The last input drawn from `self.dl` (potentially modified by callbacks). `xb`
    is always a tuple (potentially with one element), and `x` is detuplified. You
    can assign only to `xb`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从`self.dl`中绘制的最后一个输入（可能由回调修改）。`xb`始终是一个元组（可能有一个元素），`x`是去元组化的。您只能分配给`xb`。
- en: '`y`/`yb`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`y`/`yb`'
- en: The last target drawn from `self.dl` (potentially modified by callbacks). `yb`
    is always a tuple (potentially with one element), and `y` is detuplified. You
    can assign only to `yb`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 从`self.dl`中绘制的最后一个目标（可能由回调修改）。`yb`始终是一个元组（可能有一个元素），`y`是去元组化的。您只能分配给`yb`。
- en: '`pred`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`pred`'
- en: The last predictions from `self.model` (potentially modified by callbacks).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从`self.model`中绘制的最后预测（可能由回调修改）。
- en: '`loss`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss`'
- en: The last computed loss (potentially modified by callbacks).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后计算的损失（可能由回调修改）。
- en: '`n_epoch`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_epoch`'
- en: The number of epochs in this training.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此次训练的时代数。
- en: '`n_iter`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_iter`'
- en: The number of iterations in the current `self.dl`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '`epoch`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The current epoch index (from 0 to `n_epoch-1`).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '`iter`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The current iteration index in `self.dl` (from 0 to `n_iter-1`).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The following attributes are added by `TrainEvalCallback` and should be available
    unless you went out of your way to remove that callback:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '`train_iter`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The number of training iterations done since the beginning of this training
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '`pct_train`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of training iterations completed (from 0 to 1)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '`training`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: A flag to indicate whether we’re in training mode
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The following attribute is added by `Recorder` and should be available unless
    you went out of your way to remove that callback:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '`smooth_loss`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: An exponentially averaged version of the training loss
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks can also interrupt any part of the training loop by using a system
    of exceptions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Callback Ordering and Exceptions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes callbacks need to be able to tell fastai to skip over a batch or
    an epoch, or stop training altogether. For instance, consider `TerminateOnNaNCallback`.
    This handy callback will automatically stop training anytime the loss becomes
    infinite or `NaN` (*not a number*). Here’s the fastai source for this callback:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The line `raise CancelFitException` tells the training loop to interrupt training
    at this point. The training loop catches this exception and does not run any further
    training or validation. The callback control flow exceptions available are as
    follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '`CancelFitException`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Skip the rest of this batch and go to `after_batch`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`CancelEpochException`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Skip the rest of the training part of the epoch and go to `after_train`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '`CancelTrainException`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Skip the rest of the validation part of the epoch and go to `after_validate`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`CancelValidException`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Skip the rest of this epoch and go to `after_epoch`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '`CancelBatchException`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Interrupt training and go to `after_fit`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'You can detect if one of those exceptions has occurred and add code that executes
    right after with the following events:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '`after_cancel_batch`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Reached immediately after a `CancelBatchException` before proceeding to `after_batch`
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '`after_cancel_train`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Reached immediately after a `CancelTrainException` before proceeding to `after_epoch`
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '`after_cancel_valid`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Reached immediately after a `CancelValidException` before proceeding to `after_epoch`
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '`after_cancel_epoch`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Reached immediately after a `CancelEpochException` before proceeding to `after_epoch`
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '`after_cancel_fit`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Reached immediately after a `CancelFitException` before proceeding to `after_fit`
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes callbacks need to be called in a particular order. For example, in
    the case of `TerminateOnNaNCallback`, it’s important that `Recorder` runs its
    `after_batch` after this callback, to avoid registering an `NaN` loss. You can
    specify `run_before` (this callback must run before…) or `run_after` (this callback
    must run after…) in your callback to ensure the ordering that you need.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a close look at the training loop, exploring variants
    of SGD and why they can be more powerful. At the time of writing, developing new
    optimizers is an active area of research, so by the time you read this chapter,
    there may be an addendum on the [book’s website](https://book.fast.ai) that presents
    new variants. Be sure to check out how our general optimizer framework can help
    you implement new optimizers quickly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We also examined the powerful callback system that allows you to customize every
    bit of the training loop by enabling you to inspect and modify any parameter you
    like between each step.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the equation for a step of SGD, in math or code (as you prefer)?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do we pass to `cnn_learner` to use a nondefault optimizer?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are optimizer callbacks?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `zero_grad` do in an optimizer?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `step` do in an optimizer? How is it implemented in the general optimizer?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rewrite `sgd_cb` to use the `+=` operator, instead of `add_`.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is momentum? Write out the equation.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s a physical analogy for momentum? How does it apply in our model training
    settings?
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动量的物理类比是什么？它如何应用在我们的模型训练设置中？
- en: What does a bigger value for momentum do to the gradients?
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动量值越大对梯度有什么影响？
- en: What are the default values of momentum for 1cycle training?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1cycle训练的动量的默认值是多少？
- en: What is RMSProp? Write out the equation.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RMSProp是什么？写出方程。
- en: What do the squared values of the gradients indicate?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度的平方值表示什么？
- en: How does Adam differ from momentum and RMSProp?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adam与动量和RMSProp有何不同？
- en: Write out the equation for Adam.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出Adam的方程。
- en: Calculate the values of `unbias_avg` and `w.avg` for a few batches of dummy
    values.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算几批虚拟值的`unbias_avg`和`w.avg`的值。
- en: What’s the impact of having a high `eps` in Adam?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Adam中，`eps`值较高会产生什么影响？
- en: Read through the optimizer notebook in fastai’s repo and execute it.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读fastai存储库中的优化器笔记本并执行它。
- en: In what situations do dynamic learning rate methods like Adam change the behavior
    of weight decay?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在哪些情况下，像Adam这样的动态学习率方法会改变权重衰减的行为？
- en: What are the four steps of a training loop?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练循环的四个步骤是什么？
- en: Why is using callbacks better than writing a new training loop for each tweak
    you want to add?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么使用回调比为每个想要添加的调整编写新的训练循环更好？
- en: What aspects of the design of fastai’s callback system make it as flexible as
    copying and pasting bits of code?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: fastai回调系统设计的哪些方面使其像复制和粘贴代码片段一样灵活？
- en: How can you get the list of events available to you when writing a callback?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编写回调时，如何获取可用事件的列表？
- en: Write the `ModelResetter` callback (without peeking).
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`ModelResetter`回调（请不要偷看）。
- en: How can you access the necessary attributes of the training loop inside a callback?
    When can you use or not use the shortcuts that go with them?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在回调内部访问训练循环的必要属性？何时可以使用或不使用与它们配套的快捷方式？
- en: How can a callback influence the control flow of the training loop?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回调如何影响训练循环的控制流？
- en: Write the `TerminateOnNaN` callback (without peeking, if possible).
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写`TerminateOnNaN`回调（如果可能的话，请不要偷看）。
- en: How do you make sure your callback runs after or before another callback?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何确保你的回调在另一个回调之后或之前运行？
- en: Further Research
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Look up the “Rectified Adam” paper, implement it using the general optimizer
    framework, and try it out. Search for other recent optimizers that work well in
    practice and pick one to implement.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查阅“修正的Adam”论文，使用通用优化器框架实现它，并尝试一下。搜索其他最近在实践中表现良好的优化器，并选择一个实现。
- en: Look at the mixed-precision callback inside the [documentation](https://docs.fast.ai).
    Try to understand what each event and line of code does.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看[文档](https://docs.fast.ai)中的混合精度回调。尝试理解每个事件和代码行的作用。
- en: Implement your own version of the learning rate finder from scratch. Compare
    it with fastai’s version.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始实现自己版本的学习率查找器。与fastai的版本进行比较。
- en: Look at the source code of the callbacks that ship with fastai. See if you can
    find one that’s similar to what you’re looking to do, to get some inspiration.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看fastai附带的回调的源代码。看看能否找到一个与你要做的类似的回调，以获得一些灵感。
- en: 'Foundations of Deep Learning: Wrap Up'
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础：总结
- en: Congratulations—you have made it to the end of the “foundations of deep learning”
    section of the book! You now understand how all of fastai’s applications and most
    important architectures are built, and the recommended ways to train them—and
    you have all the information you need to build these from scratch. While you probably
    won’t need to create your own training loop or batchnorm layer, for instance,
    knowing what is going on behind the scenes is very helpful for debugging, profiling,
    and deploying your solutions.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你已经完成了书中“深度学习基础”部分！现在你理解了fastai的所有应用程序和最重要的架构是如何构建的，以及训练它们的推荐方法——你拥有构建这些内容所需的所有信息。虽然你可能不需要创建自己的训练循环或批归一化层，但了解幕后发生的事情对于调试、性能分析和部署解决方案非常有帮助。
- en: Since you understand the foundations of fastai’s applications now, be sure to
    spend some time digging through the source notebooks and running and experimenting
    with parts of them. This will give you a better idea of exactly how everything
    in fastai is developed.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你现在理解了fastai应用的基础，一定要花时间深入研究源代码笔记本，并运行和实验它们的部分。这将让你更清楚地了解fastai中的所有内容是如何开发的。
- en: 'In the next section, we will be looking even further under the covers: we’ll
    explore how the actual forward and backward passes of a neural network are done,
    and we will see what tools are at our disposal to get better performance. We will
    then continue with a project that brings together all the material in the book,
    which we will use to build a tool for interpreting convolutional neural networks.
    Last but not least, we’ll finish by building fastai’s `Learner` class from scratch.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更深入地探讨：我们将探索神经网络的实际前向和后向传递是如何进行的，以及我们可以利用哪些工具来获得更好的性能。然后，我们将继续进行一个项目，将书中的所有材料汇集在一起，用它来构建一个用于解释卷积神经网络的工具。最后但并非最不重要的是，我们将从头开始构建fastai的`Learner`类。
