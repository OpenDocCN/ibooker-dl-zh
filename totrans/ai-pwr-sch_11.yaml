- en: 9 Personalized search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The personalization spectrum between search and recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing collaborative filtering and personalization using latent features
    from users’ signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using embeddings to create personalization profiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal personalization from content and behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying clustering-based personalization guardrails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding the pitfalls of personalized search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The better your search engine understands your users, the more likely it will
    be able to successfully interpret their queries. In chapter 1, we introduced the
    three key contexts needed to properly interpret query intent: content understanding,
    domain understanding, and user understanding. In this chapter, we’ll dive into
    the user understanding context.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already focused on learning domain-specific context from documents (chapter
    5) and on the most popular results according to many different users (chapter
    8), but it’s not always reasonable to assume that the “best” result is agreed
    upon across all users. Whereas signals-boosting models find the most *popular*
    answers across all users, personalized search instead attempts to learn about
    each *specific* user’s interests and to return search results catering to those
    interests.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when searching for restaurants, a user’s location clearly matters.
    When searching for a job, each user’s employment history (previous job titles,
    experience level, salary range) and location may matter. When searching for products,
    particular brand affinities, colors of appliances, complementary items purchased,
    and similar personal tastes may matter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll use user signals to learn latent features describing
    users’ interests. *Latent features* are features hidden within data, but which
    can be inferred about users or items by modeling the data. These latent features
    will be used to generate product recommendations and boosts to personalized search
    results. We’ll also use content-based embeddings to relate products and we’ll
    use embeddings of the products each user interacts with to generate vector-based
    personalization profiles to personalize search results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll cluster products by their embeddings to generate personalization
    guardrails to ensure that users don’t see personalized search results based on
    products from unrelated categories.
  prefs: []
  type: TYPE_NORMAL
- en: Personalization should be applied to search results very carefully. It’s easy
    to frustrate users by overriding their explicit intent (usually specified as search
    keywords) with assumptions based on their previous search activity. We’ll dive
    into the nuances of balancing the benefits of better-personalized search against
    potential user frustration caused by an engine trying too hard to read their minds.
    Not all searches should be personalized, but when it’s done well, you’ll see how
    it can greatly improve the search experience.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Personalized search vs. recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Search engines and recommendation engines represent two ends of a personalization
    spectrum, which we introduced in chapter 1 (see figure 1.5). We also discussed
    the dimensions of user intent in chapter 1 (see figure 1.7), noting that fully
    understanding user intent requires content understanding, user understanding,
    and domain understanding. Figure 9.1 resurfaces these two mental models.
  prefs: []
  type: TYPE_NORMAL
- en: While keyword search represents only content understanding, and collaborative
    recommendations represent only user understanding, they both can and should be
    combined when possible. *Personalized search* lies at the intersection between
    keyword search and collaborative recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F01_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 The personalization spectrum and the dimensions of user intent
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 9.2 superimposes the personalization spectrum on top of the diagram of
    the dimensions of user intent to paint a more nuanced picture of how personalized
    search fits along the personalization spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F02_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 Personalized search lies at the intersection between keyword search
    and collaborative recommendations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The key differentiating factor between search engines and recommendation engines
    is that search engines are typically guided by users and match their explicitly
    entered queries, whereas recommendation engines typically accept no direct user
    input and instead recommend content based on already known or inferred knowledge.
    The reality, however, is that these two kinds of systems form two sides of the
    same coin. The goal in both cases is to understand what a user is looking for
    and deliver relevant results to meet that user’s information need. In this section,
    we’ll discuss the broad spectrum of capabilities that lie within the personalization
    spectrum between search and recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 Personalized queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s imagine we’re running a restaurant search engine. Our user, Michelle,
    is on her phone in New York at lunchtime, and she types in a keyword search for
    `steamed bagels`. She sees top-rated steamed bagel shops in Greenville, South
    Carolina (USA); Columbus, Ohio (USA); and London (UK).
  prefs: []
  type: TYPE_NORMAL
- en: What’s wrong with these search results? Well, in this case, the answer is clear—Michelle
    is looking for lunch in New York, but the search engine is showing her results
    hundreds to thousands of kilometers away. But Michelle never *told* the search
    engine she only wanted to see results in New York, nor did she tell the search
    engine that she was looking for a lunch place close by because she wants to eat
    now. Nevertheless, the search engine should be able to infer this information
    and personalize the search results accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Consider another scenario—Michelle is at the airport after a long flight, and
    she searches on her phone for `driver`. The top results that come back are for
    a golf club for hitting the ball off a tee, followed by a link to printer drivers,
    followed by a screwdriver. If the search engine knows Michelle’s location, shouldn’t
    it be able to infer her intended meaning—that she is searching for a ride?
  prefs: []
  type: TYPE_NORMAL
- en: Using our job search example from earlier, let’s assume Michelle goes to her
    favorite job search engine and types in `nursing jobs`. Like our restaurant example
    earlier, wouldn’t it be ideal if nursing jobs in New York showed up at the top
    of the list? What if she later types `jobs in Seattle`? Wouldn’t it be ideal if—instead
    of seeing random jobs in Seattle (doctor, engineer, chef, etc.)—nursing jobs now
    showed up at the top of the list, since the engine previously learned that she
    is a nurse?
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these is an example of a personalized query: the combining of both
    an explicit user query *and* an implicit understanding of the user’s intent and
    preferences into a search that serves results specifically catering to that user.
    Doing this kind of personalized search well is tricky, as you must carefully balance
    your understanding of the user without overriding anything they explicitly want
    to query. When it’s done well, though, personalized queries can significantly
    improve search relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 User-guided recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as it’s possible to sprinkle an implicit understanding of user-specific
    attributes into an explicit keyword search to generate personalized search results,
    it’s also possible to enable user-guided recommendations by allowing user-overrides
    of the inputs into automatically generated recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is becoming increasingly common for recommendation engines to allow users
    to see and edit their recommendation preferences. These preferences usually include
    a list of items the user interacted with before by viewing, clicking, or purchasing
    them. Across a wide array of use cases, these preferences could include both specific
    item preferences, like favorite movies, restaurants, or places, as well as aggregated
    or inferred preferences, like clothing sizes, brand affinities, favorite colors,
    preferred local stores, desired job titles and skills, preferred salary ranges,
    and so on. These preferences make up a user profile: they define what is known
    about a customer, and the more control you can give a user to see, adjust, and
    improve this profile, the better you’ll be able to understand your users and the
    happier they’ll likely be with the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Recommendation algorithm approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll discuss the different types of recommendation algorithms.
    Recommendation engine implementations come in different flavors depending on what
    data is available to drive their recommendations. Some systems only have user
    behavioral signals and very little content or information about the items being
    recommended, whereas other systems have rich content about items, but very few
    user interactions with the items. We’ll cover content-based, behavior-based, and
    multimodal recommenders.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Content-based recommenders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Content-based recommendation algorithms recommend new content based on attributes
    shared between different entities (often between users and items, between items
    and items, or between users and users). For example, imagine a job search website.
    Jobs may have properties on them like “job title”, “industry”, “salary range”,
    “years of experience”, and “skills”. Users will have similar attributes on their
    profile or resume/CV. Based upon these properties, a content-based recommendation
    algorithm can figure out which of these features are most important and can then
    rank the best matching jobs for any given user based on the user’s desired attributes.
    This is what’s known as a user-item (or user-to-item) recommender.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if a user likes a particular job, it is possible to use this same
    process to recommend similar jobs based on how well those jobs match the attributes
    of the first job. This type of recommendation is popular on product details pages,
    where a user is already looking at an item and it may be desirable to help them
    explore related items. This kind of recommendation algorithm is known as an item-item
    (or item-to-item) recommender.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 demonstrates how a content-based recommender might use attributes
    about items with which a user has previously interacted to match similar items
    for that user. In this case, our user viewed the “detergent” product and was then
    recommended “fabric softener” and “dryer sheets” based upon these items matching
    within the same category field (the “laundry” category) and containing similar
    text to the “detergent” product within their product descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrated this kind of related attribute and category matching in chapter
    5 when covering knowledge graph learning and in chapter 6 when covering query
    expansion using knowledge graphs. In those cases, we were mostly expanding the
    keyword query to include additional related terms, but you could match items based
    on any other attributes, like brand, color, or size.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F03_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Content-based recommendations based upon matching attributes of an
    item of interest to a user, such as categories and text keywords
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s also possible to match users to other users, or any entity to any other
    entity. In the context of content-based recommenders, all recommendations can
    be seen as item-item recommendations, where each item is an arbitrary entity that
    shares attributes with the other entities being recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Behavior-based recommenders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Behavior-based recommenders use user interactions with items (documents) to
    discover similar patterns of interest among groups of items. This process is called
    *collaborative filtering*, referring to the use of a multiple-person (collaborative)
    voting process to filter matches to those demonstrating the highest similarity,
    as measured by how many overlapping users interacted with the same items. The
    idea here is that similar users (i.e., those with similar preferences) tend to
    interact with the same items, and when users interact with multiple items, they
    are more likely to be interacting with similar items as opposed to unrelated items.
  prefs: []
  type: TYPE_NORMAL
- en: One amazing characteristic of collaborative filtering algorithms is that they
    fully crowdsource the relevance scoring process from your end users. In fact,
    features of the items themselves (name, brand, color, text, and so on) are not
    needed—all that is required is a unique ID for each item and knowledge of which
    users interacted with which items. Further, the more user-interaction signals
    you have, the smarter these algorithms tend to get, because more people are continually
    voting and informing your ranking algorithm. This often leads to collaborative
    filtering algorithms significantly outperforming content-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 demonstrates how overlapping behavioral signals from multiple users
    can be used to drive collaborative recommendations. In this figure, a new user
    is expressing interest in fertilizer, and because other users who have previously
    expressed interest in fertilizer tended to also click on, add-to-cart, or purchase
    soil and mulch, then soil or mulch will be returned as recommendations. Another
    behavior-based cluster of items including a screwdriver, hammer, and nails is
    also depicted, but they don’t sufficiently overlap with the user’s current interest
    (fertilizer), so they are not returned as recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F04_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 Recommendations based on collaborative filtering, a technique using
    the overlap between behavioral signals across multiple users
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll implement an end-to-end collaborative filtering example in section 9.3,
    covering the process of discovering latent user and item features from user behavioral
    signals and using those to generate item recommendations for users. Because collaborative
    filtering is completely crowdsourced, it is immune to data quality problems with
    your documents or associated content attributes that may be missing or incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the same dependence upon user behavioral signals that makes collaborative
    filtering so powerful also turns out to be its weakness. What happens when there
    are only a few interactions with a particular item—or possibly none at all? The
    answer is that the item either never gets recommended (when there are no signals),
    or it will be likely to generate poor recommendations or show up as a bad match
    for other items (when there are few signals). This situation is known as the *cold-start
    problem*, and it’s a major challenge for behavior-based recommenders. To solve
    this problem, you typically need to combine behavior-based recommenders with content-based
    recommenders, as we’ll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Multimodal recommenders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Multimodal recommenders* (also sometimes called *hybrid recommenders*) combine
    both content-based and behavior-based recommender approaches. Since collaborative
    filtering tends to work best for items with many signals, but works poorly when
    few or no signals are present, it is often most effective to use content-based
    features as a baseline and then layer a collaborative filtering model on top.
    This way, if few signals are present, the content-based matcher will still return
    results, whereas if there are many signals, the collaborative filtering algorithm
    will take greater prominence when ranking results. Incorporating both approaches
    can give you the best of both worlds: high-quality crowdsourced matching, while
    avoiding the cold-start problem for newer and less-well-discovered content. Figure
    9.5 demonstrates how this can work in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F05_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 Multimodal recommendations combine both content-based matching and
    collaborative filtering into a hybrid matching algorithm.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can see in figure 9.5 that the user could interact with either the drill
    (which has no signals) or the screwdriver (which has previous signals from other
    users, as well as content), and the user would receive recommendations in both
    cases. This provides the benefit that signals-based collaborative filtering can
    be used, while also enabling content-based matching for items with insufficient
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll implement a collaborative filtering model in the next section, followed
    by a hybrid personalized search system in section 9.4\.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Implementing collaborative filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll implement a collaborative filtering algorithm. We’ll
    use user-item interaction signals and demonstrate how to learn latent (hidden)
    features from those signals that represent users’ preferences. We’ll then use
    those learned preferences to generate recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Pure collaborative filtering, as in figure 9.2, allows us to learn the similarity
    between items based entirely on user-interaction patterns with those items. This
    is a powerful concept, as it allows learning about items without any knowledge
    of the items themselves (such as titles, text, or other attributes).
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Learning latent user and item features through matrix factorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Collaborative filtering often uses a technique called *matrix factorization*
    to learn latent features about items based on user interactions. Latent features
    are features that are not directly observed but are inferred from other observed
    features. For example, assume you have four users with the following movie purchase
    history:'
  prefs: []
  type: TYPE_NORMAL
- en: 'User 1—*Avengers: Endgame*, *Black Panther*, and *Black Widow*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User 2—*Black Widow*, *Captain Marvel*, and *Black Panther*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User 3—*Black Widow*, *The Dark Knight*, and *The Batman*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User 4—*The Little Mermaid*, *The Lion King*, and *Toy Story*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User 5—*Frozen*, *Toy Story*, and *The Lion King*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Are there patterns to these purchases? If you know the titles or descriptions,
    you could infer the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Users 1–3:'
  prefs: []
  type: TYPE_NORMAL
- en: All of these are movies about superheroes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of them were made by Marvel Studios, though some were made by Warner Brothers
    (DC Comics).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are all action movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are not suitable for small children due to violence and/or language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Users 4–5:'
  prefs: []
  type: TYPE_NORMAL
- en: All of them are animated movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of them are suitable for small children.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of them are made by Disney/Pixar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine you don’t have access to anything other than the product IDs, though.
    By using matrix factorization, it is possible to observe how users interact with
    items and to infer latent features about those items. If the features listed in
    the previous bullet points are the most predictive of the purchasing behavior
    of similar users, they are likely to be represented in the latent features learned
    by matrix factorization. Matrix factorization is also likely to discover other
    features that are not as obvious.
  prefs: []
  type: TYPE_NORMAL
- en: As a different example, in the RetroTech dataset, user signals may show one
    group of users purchasing stainless-steel microwaves, stainless-steel refrigerators,
    and stainless-steel dishwashers. Another group of users may be purchasing black
    microwaves, black refrigerators, and black dishwashers. By clustering the user-item
    interactions together, it’s possible to statistically determine a latent feature
    that separates these items by color. Additionally, one group may be purchasing
    televisions, PlayStations, and DVD players, and another group may be purchasing
    iPhones, phone cases, and screen protectors. By clustering these behaviors together,
    we can differentiate these product categories (home theaters versus mobile phones)
    into one or more latent features.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 demonstrates an example user-item interaction matrix for a few products
    and users. The numbers are ratings representing how strongly a user (*y*-axis)
    is interested in an item (*x*-axis), with a purchase being weighted higher than
    an add-to-cart action, and an add-to-cart signal being weighted higher than a
    click. The empty cells represent no interaction between the user and the item.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F06_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 A user-item interaction matrix. Numbers represent a user’s preference
    for an item on a scale of 1 (very unfavorable) to 10 (very favorable). Empty cells
    represent no interaction between the user and the item.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Given the user-item interaction matrix, our goal is to figure out *why* particular
    items are preferred by each user. We assume that some combination of user interests
    and item similarities explain these preferences. Matrix factorization, therefore,
    takes the user-item ratings matrix and breaks it into two separate matrices—one
    mapping each user to a set of features, and one mapping each item to a set of
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 demonstrates the matrix factorization process, resulting in the conversion
    of the user-item rankings matrix `R` into a corresponding user feature matrix
    `U` and item feature matrix `I`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F07_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 Matrix factorization. The user-item matrix `R` is decomposed into
    two matrices, a user matrix `U` and an item matrix `I`. The product of these two
    matrices (`U` `.` `I`) should be as close as possible to the original user-item
    matrix `R`.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each row in the user matrix (`U`) is a vector representing one user, with each
    column representing one of three latent user features (labeled `Latent` `User`
    `Feature` `1`, `Latent` `User` `Feature` `2`, and `Latent` `User` `Feature` `3`).
    In the item matrix (`I`), each column is a vector representing one item, with
    each row representing one of three latent item features (labeled `Latent` `Item`
    `Feature` `1`, `Latent` `Item` `Feature` `2`, and `Latent` `Item` `Feature` `3`).
  prefs: []
  type: TYPE_NORMAL
- en: We don’t have names for these latent features or know exactly what they represent,
    but they are discovered mathematically and are predictive of actual user-item
    interests. The number of latent features is a hyperparameter that can be tuned,
    but it is set to `3` in this example. This means that each user is represented
    by a vector with three dimensions (latent features), and each item is also represented
    by a vector with three dimensions (latent features).
  prefs: []
  type: TYPE_NORMAL
- en: Once matrices `U` and `I` are learned, they can thereafter be used independently
    to predict the similarity between any user and item (by comparing users in `U`
    with items in `I`), between any two users (by comparing a user in `U` with another
    user in `U`), or between any two items (by comparing an item in `I` with another
    item in `I`). We will focus only on the user-item similarity as a means of personalizing
    recommendations for each user. Figure 9.8 demonstrates how to generate an item
    rating prediction for any user.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F08_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 Calculating a user-item preference from the factorized matrices.
    Multiply each latent user feature value (first, second, and third values in the
    row for the user) by the corresponding latent item feature value (first, second,
    and third values in the column for the item), and then sum the results. This is
    the predicted user-item preference for the chosen user and item.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the first user (first row in `U`), we can generate a predicted rating for
    the movie *Avengers: Endgame* (first column in `I`) by performing a dot product
    between the first row of the user matrix `U` (`0.67`, `-0.51`, `2.81`) and the
    first column of the item matrix `I` (`0.09`, `0.75`, `3.43`), which results in
    a predicted rating of `(0.67` `*` `0.09)` `+` `(-0.51` `*` `0.75)` `+` `(2.81`
    `*` `3.43)` `=` `9.32`. Likewise, for the second user (second row in `U`), we
    can generate a predicted rating for the movie *The Notebook* (fourth column in
    `I`) by performing a dot product between the second row of the user matrix `U`
    (`1.13`, `3.18`, `-0.13`) and the fourth column of the item matrix `I` (`1.74`,
    `2.54`, `0.46`), which results in a predicted rating of `9.98`.'
  prefs: []
  type: TYPE_NORMAL
- en: While performing individual predictions between a single user and item may be
    helpful in some cases, such as for generating real-time recommendations immediately
    after an incremental user interaction, it is often more useful to generate a full
    user-item matrix `R'` of predicted ratings for all users and items. Figure 9.9
    demonstrates a final user-item matrix `R'` generated (on the far-right) by performing
    a dot product of the user matrix `U` with the item matrix `I`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F09_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 Reconstituted user-item matrix `R'`, with previous calculations from
    figure 9.8 highlighted. Note that the empty values from the original user-item
    matrix `R` are now filled in with predicted values (highlighted in black).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When taking the dot product of the user matrix and the item matrix (`U` `.`
    `I`), the resulting user-item matrix `R'` should be as close as possible to the
    original user-item matrix `R`. Minimizing the difference between the original
    matrix `R` and predicted matrix `R'` is the training optimization goal of matrix
    factorization. The closer the two matrices are, the better the model’s ability
    to predict similar personalized recommendations in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the latent features don’t perfectly represent all the potentially
    relevant features. By training with a loss function that reduces the difference
    between the original `R` and predicted `R'`, however, the model will maximize
    the chances of representing `R` and thus be able to best predict future recommendations
    based upon past user-item interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Implementing collaborative filtering with Alternating Least Squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One popular algorithm for pure collaborative filtering (based only on user interaction
    with items) is *Alternating Least Squares* (ALS). ALS is an iterative algorithm
    that performs matrix factorization by alternating between learning the latent
    features of items and the latent features of users.
  prefs: []
  type: TYPE_NORMAL
- en: The logic behind ALS is that latent features in a user-item ratings matrix are
    a combination of the user’s latent features and the items’ latent features. While
    the relative weights between users and items for each latent feature are not known
    upfront, it is possible to begin learning user feature weights by initially using
    random item weights and freezing them (keeping them constant). As the user feature
    weights begin to coalesce, they can be frozen and used as inputs when learning
    the item feature weights. ALS then continues to alternate between further training
    the user features matrix (with the latest item feature weights frozen) and the
    item features matrix (with the latest user feature weights frozen). This process
    is repeated for a configurable number of iterations until the weights of both
    matrices are well-balanced and optimized. By alternating between learning the
    latent features of items and users, ALS can iteratively learn the best combined
    weights of both matrices to improve the predictive power of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The number of latent features learned using matrix factorization is a hyperparameter,
    called the *rank*. The higher the rank, the more granular the features you can
    learn, but you also tend to need more data points to reliably learn more granular
    features. While you won’t be able to apply a label to each latent feature (features
    are just represented as numbers), it’s still possible to discover meaningful categories
    in the data that best predict similar items. ALS is a popular algorithm for collaborative
    filtering because it is relatively easy to implement and can scale to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll discuss how to implement ALS using Spark to generate
    a recommendations model based on user-item interactions. We’ll use the RetroTech
    dataset, since it contains user-item interactions for a set of products. We’ll
    use user-item interactions to learn latent features about both users and items,
    and then we’ll use those latent features to generate future recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by generating a list of implicit preferences for each user-item
    pair using Spark’s built-in ALS implementation. Listing 9.1 generates a `user_product_
    implicit_preferences` collection, assigning a rating based on the strength of
    the user interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 Generating implicit user-item ratings from user signals
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Only click signals are currently weighted, but weights can be set per signal
    type.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Aggregates all signals to generate a single rating per user-item pair'
  prefs: []
  type: TYPE_NORMAL
- en: We modeled support for clicks, add-to-cart, and purchase signals, though we
    only assigned a weight of `1` to clicks and `0` to both add-to-cart and purchase
    signals. We did this to keep the math more straightforward for the ALS algorithm,
    but you can experiment with turning on add-to-cart or purchase signals by increasing
    their weights to a positive number. These weights are somewhat arbitrary, but
    the idea is to differentiate the strength of the user’s product interest based
    on their level of interaction. You could also simplify by just assigning a rating
    of `1` for each user-item pair if you don’t have confidence that more interactions
    by a user necessarily indicates a stronger rating or that the weights you’ve chosen
    are meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: With our user-item ratings prepared, we’ll generate a dataframe from the prepared
    collection to train and test the model. Our dataset contains less than 50,000
    products, and we’ll be using all of them in listing 9.2; however, you may want
    to modify the `top_product_count_for_recs` to a substantially lower number if
    you want to run through it quickly. Depending on your hardware and Docker resource
    configuration, it could take anywhere from several minutes to several days to
    run. For a quick (but low-quality) run, consider testing with 1,000 products initially
    (`top_product_count_ for_recs=1000`) and then scaling up as you feel comfortable.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 Preparing the user-product-ratings data for training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Decreasing the number of products can speed up training, but with reduced
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Returns the user, product, and rating'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Limits the number recommendations to the most popular products'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataframe contains three columns: `user`, `product`, and `rating`. For
    performance reasons, many machine learning algorithms (including Spark’s ALS implementation,
    which we will be using) prefer to deal with numeric IDs instead of strings. Spark
    contains a `StringIndexer` helper object that can be used to convert string IDs
    to numeric IDs, and a corresponding `IndexToString` object that can be used to
    convert the numeric IDs back to string IDs. Listing 9.3 integrates this ID conversion
    into our dataframe.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 Converting IDs to integers for Spark’s ALS algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Transforms user and product columns into index columns for the dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The numeric index-to-string mappings for product and user'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Performs the index-to-string transformation for the user identifier'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Maps the string user field to an integer index named userIndex'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Maps the string product field to an integer index named productIndex'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from listing 9.3, our dataframe now contains two additional
    columns: `userIndex` and `productIndex`. We’ll use these numeric IDs going forward
    in the ALS implementation code, before we call the `indexes_to_strings` function
    at the very end to convert back to our original string IDs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our user-item preferences dataframe is prepared, it’s time to invoke
    the ALS algorithm. ALS requires three parameters: `userCol`, `itemCol`, and `ratingCol`,
    which correspond to the `userIndex`, `productIndex`, and `rating` columns in our
    dataframe. We’ll also set a few other parameters, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxIter=3` (the maximum number of iterations to run)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank=10` (the number of latent features to learn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regParam=0.15` (the regularization parameter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`implicitPrefs=True` (whether to treat the ratings as implicit or explicit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coldStartStrategy=drop` (how to handle new users or items that were not present
    in the training data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 9.4 demonstrates how to invoke ALS with these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 Training an ALS model using Spark
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Splits the preferences, 95% as training data and 5% as test data'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Trains the ALS model with the user preferences in the training set'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Measures the trained model against the user preferences in the test set'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You have now trained a recommendation model! We split the data into a training
    set (95%) and a test set (5%), built the ALS model, and then ran an evaluator
    to calculate a root mean square error (RMSE) loss function to measure the quality
    of the model. The RMSE is a measure of how far off the predicted ratings are from
    the actual ratings, so the lower the RMSE, the better the model. The absolute
    value of the RMSE is less important than the relative value across different model
    training passes, as the calculation depends on the scale used in the underlying
    data. If you increase the `maxIter`, find an optimal `rank`, and increase the
    `top_product_count_for_recs` when preparing the user-product-ratings data, you’ll
    likely see the RMSE decrease a bit due to improvement in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model is trained, we can use it to generate recommendations. Listing
    9.5 demonstrates how to generate item recommendations for all users. We’ll generate
    10 recommendations for each user and display the top 5 users’ recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 Generating user-item recommendations from the ALS model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that the format of the recommendations is a bit awkward. We’re stuck with
    the `userIndex` instead of our original `user`, and the `recommendations` column
    is an array of structs, with each struct containing a `productIndex` and a `rating`.
    Let’s clean this up by converting each user-item recommendation into a row and
    replacing the `userIndex` and `productIndex` values with our original `user` and
    `product` IDs. Listing 9.6 demonstrates how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Converting recommendations into a final, cleaned-up format
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this listing, we first `explode` the recommendations into separate rows for
    each recommendation with `rec.productIndex` and `rec.rating` columns. After selecting
    `userIndex` onto each row, we select `rec.productIndex` as `productIndex` and
    `rec .rating` as `rating`. Finally, we convert back to `user` and `product` from
    `userIndex` and `productIndex`, and we return `user`, `product`, and `boost`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s save our recommendations to a collection for future use. This will enable
    us to serve the recommendations instantly from the search engine or to use them
    as boosts to personalize search results. Listing 9.7 writes our user-item recommendations
    dataframe to a `user_item_recommendations` collection in the search engine, following
    a data format similar to the one we used in chapter 8 to represent signal boosts.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.7 Indexing the recommendations into the search engine
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You have now generated item recommendations for users based on their interactions
    with items, and you’ve saved them for future use in the `user_item_recommendations`
    collection in the search engine. Next, we’ll demonstrate how we can serve these
    recommendations and use them to personalize search results.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Personalizing search results with recommendation boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With user-item recommendations generated, we can now personalize search results.
    The only difference between our collection schema for the `signals_boosts` collection
    in chapter 8 and the `user_item_recommendations` collection here is the replacement
    of the `query` column with a `user` column. In other words, whereas signals boosting
    is based on matching a particular keyword query and applying associated item relevance
    boosts, personalization is based on matching a particular user and applying associated
    item relevance boosts.
  prefs: []
  type: TYPE_NORMAL
- en: With our recommendations collection now populated from listing 9.7, we can either
    serve the recommendations directly (no keyword query) or use the recommendations
    to personalize search results by boosting them based on the user’s recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Pure collaborative recommendations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Serving recommendations directly is straightforward, so we’ll start there. Listing
    9.8 shows recent signals for one of our users, for whom we’ll demonstrate these
    personalization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 Interaction history for our target user
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 User for whom we’ll be personalizing results'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the user’s history, it’s clear that they are interested in Apple products,
    tablets, and computers. The following listing demonstrates how to serve up recommendations
    for this user from our `user_item_recommendations` collection.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.9 Serving recommendations using a signals boosting query
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Function omitted for brevity; it can be seen in listing 4.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Queries the recommendation collection for indexed product boosts'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 shows the output from listing 9.9\. At the top, you’ll notice a
    “Boost Query” listed, showing the top-recommended products for the user, along
    with their relative boost for the user (which was calculated as `rating * 100`).
    Under that boost query, you’ll see the boosted search results for this blank keyword
    search, which are the raw recommendations for the user.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F10_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 Recommendations for a user based only on collaborative filtering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The recommendations boost a 16 GB iPad to the top, which makes sense given that
    the user previously searched for and clicked on the 16 GB iPad, with another Apple
    iPad (a 32 GB model) ranked fourth. You also see other tablets made by competing
    manufacturers with similar configurations within the top recommendations. This
    is a good example of how collaborative filtering can help surface items that might
    not directly match the user’s previous interactions (with only an Apple laptop
    and iPads), but which may still be relevant to the user’s interests (similar tablets
    to iPads).
  prefs: []
  type: TYPE_NORMAL
- en: Recommendations like these can be useful to integrate alongside traditional
    search results, or possibly to even insert into a set of search results. But it’s
    also possible to use them as boosts to your keyword ranking algorithm to personalize
    search results, which we’ll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Pure keyword search vs. personalized search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of serving recommendations independently of keyword search, it can also
    be useful to blend them in as additional signals in your search ranking algorithm
    to personalize the results. Going back to our last example, imagine that our user
    who is interested in iPads and MacBooks from Apple performs a keyword search for
    `tablet`. How would this look different than if the tablet recommendations were
    used to personalize the search results? Listing 9.10 runs the query before and
    after applying signals boosts based on the user’s personalized recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 Non-personalized vs. personalized search results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Non-personalized search results (keyword search only)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Personalized search results (keyword + user-item recommendations boosting)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 shows the output of the non-personalized query for `tablet`, whereas
    figure 9.12 shows the output once the recommendation boosts have been applied
    to personalize the search results.
  prefs: []
  type: TYPE_NORMAL
- en: The personalized search results are likely much more relevant for this user
    than the non-personalized results. It’s worth noting that in our implementation,
    the personalization is *only* applied as a relevance boost. This means products
    that don’t match the user’s explicit query will not be returned, and all items
    that do match the query will still be returned; the only difference is the ordering
    of the products, as items personalized to the user should now show up on the first
    page.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F11_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 Traditional keyword search for `tablet` with no personalization
    applied
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F12_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 Personalized search for `tablet`, where the user has shown an interest
    in the Apple brand
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Also note that after the boosted recommendations (tablets from the recommendation
    example) the fifth search result is an item from the non-personalized search results,
    the “Memorial Tablet” titled CD.
  prefs: []
  type: TYPE_NORMAL
- en: 'This implies two things:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are personalizing search results and not just serving pure recommendations,
    you’ll likely want to generate more than 10 recommendations per user, particularly
    since recommendations only show up if they also match a user’s explicit query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The non-personalized relevance algorithm is still critical. If signals boosting
    based on the query (per chapter 8) was applied in addition to recommendations
    boosting (based on the user), you would see popular tablets at the top (and not
    the tablet sleeve and CD), with the personalized tablets then moving up higher
    among the popular results because of the personalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve now learned how collaborative filtering works through matrix factorization,
    we’ve implemented recommendations based on a collaborative filtering algorithm
    (ALS), and we’ve demonstrated how to use those recommendations to personalize
    search results. In the next section, we’ll explore another technique for personalization
    based on document embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Personalizing search using content-based embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we used user signals to learn personalized boosts for
    particular items. These boosts were generated by learning latent features about
    users and items using matrix factorization on user-item interaction patterns.
  prefs: []
  type: TYPE_NORMAL
- en: You could also use these latent factors directly to cluster users or items together.
    Unfortunately, there isn’t a great way to reliably map queries into particular
    clusters of items based only on user-interaction signals without having already
    seen the corresponding queries before (the cold-start problem, again). Thankfully,
    it is quite rare for a search engine to *not* have additional knowledge of items
    such as titles, descriptions, and other attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll look at a hybrid approach using both content-based understanding
    and user-interaction patterns to build an evolving user profile to personalize
    search results.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 Generating content-based latent features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve covered many techniques for utilizing fields to filter and boost on explicit
    attributes in documents. Chapters 5–7, in particular, focused on generating knowledge
    graphs and parsing domain-specific entities to help with context-dependent relevance.
  prefs: []
  type: TYPE_NORMAL
- en: While those techniques can certainly be useful for implementing personalized
    search (and we encourage you to experiment with them), we’re going to explore
    a different approach in this section. Instead of using explicit attributes, we’re
    going to use latent features learned from the content of documents to generate
    personalized search results. We’ll use a large language model (LLM) to generate
    embeddings for each document, and then we’ll use those embeddings along with user
    interactions with documents to build an evolving user profile. Finally, we’ll
    use that user profile to personalize search results.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 demonstrates conceptually how using an LLM to generate embeddings
    for documents works. Similar to how we used matrix factorization in section 9.3.1
    to create a matrix that mapped each item to its list of latent features, we’ll
    use an LLM to generate a vector of latent features for each document. We’ll extract
    these latent features based on the text of the document mapped into a vector space
    that has already been learned by the LLM. Don’t worry for now about the mechanics
    of *how* the LLM is trained—we will cover that in depth in chapters 14 and 15\.
    Just know that it is trained on a large corpus of text, and it learns how to map
    words and phrases into a vector space using some number of latent features that
    represent the meaning of the text. Each of the dimensions in the vector space
    represents a latent feature, and the value of each dimension represents how strongly
    that latent feature is represented in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F13_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 Item embeddings from an LLM. Each dimension in the vector space
    represents a latent feature, and the value of each dimension represents how strongly
    that latent feature is represented in the text for that item.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The values in figure 9.13 are for illustration purposes and are not the actual
    values that would be generated by our LLM. We have assigned simplified labels
    to the features to describe what they seem to represent (“size”, “color”, “computer-like”,
    and “cost”), but in a real-world scenario, these features would be unlabeled and
    would represent more complex latent features combining many different aspects
    learned by the LLM’s deep neural network during its training process.
  prefs: []
  type: TYPE_NORMAL
- en: For our examples, we’ll be using the `all-mpnet-base-v2` LLM, a publicly available
    model ([https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2))
    that serves as a good general-purpose LLM for semantic search and clustering over
    sentences and short paragraphs, like those in our RetroTech dataset. It is a lightweight
    model (only 768 dimensions) that was trained on over 1.17 billion sentence pairs
    from across the web, providing a good general-knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing retrieves the fields we need to pass to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.11 Retrieving product data to generate embeddings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To generate embeddings, we first use Spark to create a new `products_samples`
    table containing a subset of fields useful for generating embeddings and identifying
    the associated products. Listing 9.12 demonstrates how we can generate embeddings
    for each product using the `all-mpnet-base-v2` LLM and the `Sentence_Transformers`
    library. We’ll generate a `product_embeddings` object containing a 768-dimension
    vector for each product, along with a `product_names` object containing the name
    of each product and a `product_ids` object containing the ID of each product.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.12 Generating product embeddings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the all-mpnet-base-v2 LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Optimization code to cache generated embeddings is omitted for brevity.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generates the 768-dimension vector embedding for all products'
  prefs: []
  type: TYPE_NORMAL
- en: Because we’re using the out-of-the-box `all-mpnet-base-v2` model, loading and
    generating embeddings for all our products is as simple as the code in listing
    9.12\. Because the process of generating embeddings for all products can take
    a while, the notebooks additionally contain some omitted code optimizations to
    cache and reuse embeddings to save extra processing time.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to compare the similarity of two products, we can directly compare
    their vectors using dot product or cosine similarity calculations. The 768 features
    in the vector are pretrained latent features of each document, similar to the
    latent features represented in the item feature matrix in figure 9.7\. This means
    that we can now
  prefs: []
  type: TYPE_NORMAL
- en: Generate embeddings for any item or query to get a vector representation of
    that item or query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform semantic search starting with any query embedding and find the closest
    (cosine or dot product) other embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an item’s embedding to generate recommendations for other items by finding
    the ones with the most similar (cosine or dot product) embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But what about generating user-based recommendations or personalized search
    results? In figure 9.7, we not only factored out latent item features, but we
    also factored out latent user features. The whole idea behind collaborative filtering
    in section 9.2 is that similar users interact with similar items precisely *because*
    the items share features that overlap with those users’ interest. In other words,
    a vector representing a user’s interests should be similar to the vectors representing
    the items for which the user has expressed interest.
  prefs: []
  type: TYPE_NORMAL
- en: To personalize search results based on embedding vectors, we thus need to generate
    a vector representing the user’s interests. One way to do this is by taking the
    average of the vectors representing the items with which the user has interacted.
    This is a simple way to generate a vector representing *all* the user’s past interests,
    and it works surprisingly well in practice. Unfortunately, personalizing *every*
    future search based on every past search can be a bit too aggressive, as users
    often perform unrelated searches for different types of items at different times.
    To avoid unhelpful over-personalization in these cases, it can be useful to first
    apply some guardrails across different item categories, which we’ll cover next.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 Implementing categorical guardrails for personalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fact that someone searches for an item doesn’t always mean they want to
    see similar items. But if they do want personalization, it is usually a very bad
    idea to apply personalization across conceptual or categorical boundaries. For
    example, if someone watches the movie *The Terminator*, which contains violent
    time-traveling robots, it doesn’t mean they want to purchase a robot vacuum cleaner
    or a gun. As a concrete example from our dataset, imagine that someone previously
    expressed interest in a “Hello Kitty Water Bottle”, a “GE Electric Razor (Black)”,
    “GE Bright White Light Bulbs”, and a “Samsung Stainless-steel Refrigerator”. If
    they subsequently perform a search for `microwave`, which of the items from figure
    9.14 would be most appropriate to recommend?
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F14_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 Personalization guardrails can help prevent unrelated past interests
    from unexpectedly influencing future searches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the user previously looked at “white” lights and a “black” electric shaver,
    there is no good reason to apply those color preferences to the unrelated category
    of “kitchen appliances”. Additionally, it is questionable whether the interest
    in a “Hello Kitty Water Bottle” would transfer over to an interest in a “Hello
    Kitty microwave”, or whether looking at “light bulbs” and an “electric shaver”
    made by the company “GE” would in any way translate into a user having a brand
    affinity for “GE” when looking at “kitchen appliances”. Given that this particular
    user has already shown an interest in another appliance (a refrigerator that is
    “stainless-steel”) made by the company “Samsung”, however, it is very reasonable
    to assume they would be more interested in other “stainless-steel” appliances
    made by “Samsung” (or at least by other companies beyond “GE”), such as the microwave
    for which they are now searching.
  prefs: []
  type: TYPE_NORMAL
- en: Personalization should be applied with a light touch. It is easy to make mistakes
    and to apply personalization in ways that are not helpful to the user (or are
    even frustrating and counterproductive), so it’s usually better to err on the
    side of caution and ensure that personalization is only applied when it is likely
    to be helpful. One simple way to do this is to only apply personalization within
    similar categories as the query. This is one way of applying *guardrails* to personalization,
    and it is a very effective way to avoid applying personalization in ways that
    are likely to be unhelpful to the user.
  prefs: []
  type: TYPE_NORMAL
- en: While your data may or may not have an explicit category field to filter on,
    it’s also possible to dynamically generate categories by clustering items together
    based on their similarity. This can be done by taking the embeddings for all items
    and clustering them to dynamically create a data-driven set of categories. The
    following listing demonstrates a simple method for generating clusters of items
    from their embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.13 Generating dynamic categories from clustered products
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generates 100 clusters using a KMeans clustering algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Assigns each product name to its corresponding cluster label'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that our clustering worked well, we can inspect the top words in each
    cluster to ensure they are related and form a coherent category. Listing 9.14
    demonstrates code to identify the top words in each cluster and to generate a
    2D visualization of the clusters using principal component analysis (PCA) to map
    the 768-dimension embeddings down to two dimensions for visualization purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.14 Inspecting popular terms from each product cluster
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Performs PCA to reduce embeddings down to two dimensions for visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Loops through each cluster and plots it on the graph'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The top words function gets the most common words from a cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adds a text label for each cluster with the cluster ID and top-N words in
    each cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Display improvement: adjusts the text labels to minimize overlap'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.15 shows the output of listing 9.14\. Each dot represents a cluster,
    with the text label for each cluster including the cluster ID and the top words
    in that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: While figure 9.15 may appear chaotic, representing all 100 clusters into which
    our nearly 50,000 products are categorized, you can see clear patterns in the
    semantic space. The top left of the graph contains kitchen appliances, and music
    tends to be at the top right of the remaining populated area of the graph (CDs
    in the top right, musical instruments and speakers in the top middle), and items
    related to video and data storage tend to be at the bottom of the graph (DVDs
    and Blu-ray at the bottom right, home theaters and cameras in the bottom middle,
    computer memory cards and storage in the bottom left along with other computer
    peripherals). Feel free to inspect the various categories and relationships between
    the clusters, but realize that they have been mapped down from 768 dimensions
    into 2 dimensions, so much of the richness represented by the KMeans clustering
    algorithm will be lost in the visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F15_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 Clusters generated by KMeans clustering of all product embeddings,
    to be used for categorizing all queries and products
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now that we have clusters available to categorize products (and signals corresponding
    to interacted-with products), we need to ensure that we can map queries into the
    correct clusters. There are multiple ways to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model-driven*—Just pass the query through the LLM, and use the resulting embedding
    vector to find the closest categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Behavior-driven*—Use query signals and corresponding interaction signals (such
    as clicks) to determine the most likely categories for popular queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Content-driven*—Run a keyword or semantic search, and find the top categories
    in the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hybrid*—Use any combination of these approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The behavior-driven approach follows the signals-boosting methodology from chapter
    8, but it aggregates by the categories associated with the top boosted documents
    instead of by queries. The content-driven approach enables you to use the other
    semantic search techniques explored in chapters 5–7\. For simplicity, we’ll use
    the model-driven approach here and give deference to the LLM to determine the
    meaning of the query. The following listing demonstrates three different approaches
    for deriving the top categories for a query based on the embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.15 Comparing techniques for mapping queries to clusters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets the top N clusters based on cosine similarity with the cluster centroids'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets the cluster based on the KMeans model’s prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Option 1: Predicts the nearest cluster (KMeans)'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Option 2: Finds the most-similar cluster (cosine similarity)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Option 3 (recommended): Finds N-most-similar clusters (cosine similarity)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In listing 9.15, we see three predictions being calculated: nearest cluster
    (K-means), most-similar cluster (cosine similarity), and the *N*-most-similar
    clusters (cosine similarity). The `get_top_labels_centers` function calculates
    the top *N* clusters based on the cosine similarity with the cluster centroids.
    The clustering function `get_query_ cluster` calculates a cluster based on a K-means
    prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: The output from these three approaches demonstrates an important point. While
    the query is for `microwave`, we know that the categories were generated dynamically
    and may have overlap between products. The K-means model and cosine similarity
    approaches both choose category `44` `(Microwave_Cu._Ft._Stainless-Steel_Oven)`
    in this example. While you’re likely to find better results by relying on the
    cosine similarity to measure semantic similarity versus the K-means prediction,
    the categories returned from each are likely to be closely related. Thus, any
    personalization would benefit by being applied *across* each of the relevant categories
    instead of only one. Products can be split across multiple, related categories,
    and meaningful categories can be arbitrarily split based upon the number of items
    and nuances of the item descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the overlaps between similar categories, we recommend using the
    top-*N* cosine predicted clusters (Knn, option 3) instead of filtering to a single
    cluster. In the results from listing 9.15, this miscellaneous approach returns
    five related categories: `44` (“microwaves”), `52` (“stoves”), `5` (“miscellaneous
    appliances”), `83` (“counter appliances”), and `33` (“ovens”).'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll next use these predicted categories, along with the embeddings from a
    user’s previous interactions, to personalize search results.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.3 Integrating embedding-based personalization into search results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final step in our personalization journey is to execute the personalized
    search. We could accomplish this in many different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform a weighted average between the query vector (embedding for `microwave`)
    and the vectors for the user’s previous interactions within the predicted clusters.
    This would generate a single vector representing a personalized version of the
    user’s query, so all results would be personalized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform a standard search, but then boost the results based on the average of
    the embeddings from a user’s previous interactions within the predicted clusters.
    This would be a hybrid keyword- and vector-based ranking function, where the keyword
    search would be the primary driver of the results, but the user’s previous interactions
    would be used to boost related results higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do one of the above, but then only personalize a few items in the search results
    instead of all the results. This follows a light-touch mentality so as not to
    disturb all of the user’s search results, while still injecting novelty to enable
    the user to discover personalized items they may not have otherwise found.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform a standard search (keyword or vector), but then rerank the results based
    on the weighted average between the query vector and the vectors for the user’s
    previous interactions within the predicted clusters. This uses the original search
    to find the candidate results using the default relevance algorithm, but those
    results are reranked to boost personalized preferences higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll demonstrate the last technique, as it is easy to replicate across any
    search engine, since the personalization/reranking pass can be done as a final
    step after the original search. This technique will thus work well with both traditional
    search engines and vector databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.16 demonstrates the two key functions we’ll use to generate our personalization
    vector: a `get_user_embeddings` function that looks up the embeddings for a list
    of products and also returns the cluster associated with each product, and a `get_personalization_vector`
    function that can combine embeddings between a query and all relevant user-item
    interaction vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.16 Functions for generating personalization vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Returns a dataframe with the embedding and guardrail cluster for each product'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Returns a vector that combines (weighted average) an embedding for the query
    with the embeddings for the passed-in user_items'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 You can optionally specify a query_weight and user_item_ weights to influence
    how much each embedding influences the personalization vector.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 By default, the weight is split 1:1 (50% each) between the query embedding
    and the user_items_weight.'
  prefs: []
  type: TYPE_NORMAL
- en: With the ability to combine embeddings and to look up the guardrail cluster
    for any product, it’s time to generate a personalization vector for a user based
    on their incoming query and past product interactions. We’ll generate a personalization
    vector with guardrails as well as one without guardrails to compare the results
    side by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.17 demonstrates how to generate personalization vectors. In this
    case, the user has previously interacted with two products: a Hello Kitty water
    bottle and a stainless-steel electric range. They are now running a new query
    for the keyword `microwave`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.17 Generating personalization vectors from user queries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Personalization vector with no guardrails (uses query and all past item
    interactions)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets the top 5 clusters for the query to use as guardrails'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Filters down to only items in the guardrail query clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Generates a personalization vector with guardrails (uses query and only
    items related to the query)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9.17 performs a four-step process for generating a personalization
    vector for a user’s query:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the list of product interactions, along with the associated product embeddings
    and clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the *N* (5 in this case) most similar clusters for the query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter the list of user interactions down to only items in the query clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate the personalization vector (`filtered_personalization_vector`) by
    combining the query and filtered user-item interaction vectors. (Note: we also
    generated an `unfiltered_personalization_vector` that does not apply categorical
    guardrails, for later side-by-side comparison.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final `filtered_personalization_vector` could be used directly for a vector
    search across embeddings, as it represents an embedding for the query that has
    been pulled toward the user’s interests in the 768-dimension embedding vector
    space. In our case, we are going to run an independent search for the query instead,
    and then use the `filtered_personalization_vector` to rerank the top results.
    The following listing demonstrates this search and reranking process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.18 Using the personalization vector to rerank results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Reranks all search results based upon cosine similarity to the personalized
    query vector'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Displays the original search results (no personalization)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Personalized search with no guardrails (uses unfiltered_personalization_vector)'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Personalized search with guardrails (uses filtered_personalization_vector)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.18 walks through the entire process of applying personalization vectors
    to rerank the search results. The `rerank_with_personalization` function takes
    the original search results and a personalization vector and then reranks the
    search results based on the cosine similarity between the personalization vector
    and the embedding vectors for each search result. We invoke reranking twice for
    comparison purposes: once with and once without guardrails applied to the personalization
    vector. The final sets of ranked results are each passed to the `display_product_search`
    function to render the three result sets compared in figure 9.16: the non-personalized
    search results, personalized search results with no guardrails, and personalized
    search results with guardrails.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F16_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 Comparing non-personalized, always personalized (no guardrails),
    and contextually personalized (with guardrails) search results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: On the left, we see the original search results for `microwave`, including a
    microwave cover, some stainless-steel microwaves, and a basic microwave. In the
    middle, we see personalized search results with no categorical guardrails. The
    user’s personalization vector includes embeddings for a stainless-steel microwave,
    as well as a Hello Kitty water bottle. As you can see, the Hello Kitty microwave
    jumped straight to the top of the results, even though the user has previously
    looked at a stainless-steel refrigerator, and their interest in a water bottle
    is unlikely to translate into an interest in a Hello Kitty microwave. On the right,
    we see personalization with guardrails applied. We see that all of these results
    are now for stainless-steel microwaves, reflecting the user’s previous interest
    in a stainless-steel refrigerator, which was automatically identified as a similar
    category.
  prefs: []
  type: TYPE_NORMAL
- en: You have now implemented an end-to-end personalized search algorithm. Personalized
    search can significantly improve relevance when implemented carefully and with
    a light touch, but it is important to not frustrate your users by over-personalizing.
    In the next section, we’ll review some of the pitfalls and challenges with personalization
    that you’ll need to keep in mind to avoid potential user frustration.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Challenges with personalizing search results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we’ve highlighted many of the challenges with personalizing
    search results. While personalization can be a powerful tool for driving more
    relevant search results, it is important to be aware of the potential pitfalls
    and to ensure that personalization is only applied when it is likely to be helpful
    to the user. We touched on the following key challenges in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The cold-start problem*—When using collaborative filtering, users who have
    interacted with no items lack any information on which to base personalization.
    For such users, it’s important to fall back to non-personalized search results.
    Combining a content-based filtering approach (search or attribute-based matching)
    with collaborative filtering can help overcome the cold-start problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Guardrails are important*—Applying personalization across categorical boundaries
    is generally a bad idea. Otherwise, as a user switches context to look at unrelated
    items, search results are going to look strange and be counterproductive. Looking
    at “white paper” or “white light bulbs” doesn’t mean a user wants to later see
    “white” refrigerators when searching for appliances. Similarly, liking the movie
    *The Terminator* doesn’t mean someone wants to purchase a gun or a robot vacuum.
    When personalizing search results, it is important to understand the relevant
    scope in which learned user preferences should be applied. Modeling related categories
    for items and queries and restricting personalization to only using items related
    to the query is a good way to avoid these problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Over-personalization is frustrating*—When someone types in a search query,
    they expect the search engine to return the most relevant results for their specific
    query. While applying personalization can be very helpful in certain use cases
    (e.g., location personalization in a restaurant), it can also be very frustrating
    if the amount of personalization interferes with the user’s control over the search
    experience. As an extreme case, imagine if every query were boosted by features
    from every previous query or item interaction; the search experience would quickly
    degrade into an unusable mess that would prevent the user from finding what they’re
    looking for. Consider only personalizing a few of the top results instead of the
    entire set of search results so that if the personalization is ever wrong, the
    non-personalized results are still available. Also, consider providing a way for
    users to turn off personalization if they find it frustrating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feedback loops are critical*—User interests change over time. Whether you’re
    showing recommendations or building personalization profiles for search, users
    need to be able to provide feedback to the system to help it learn and adapt to
    their changing interests. This can be done by allowing users to provide explicit
    feedback (e.g., thumbs up or thumbs down) on recommendations, or by just continuing
    to collect implicit feedback from behavioral signals (clicks, purchases, and so
    on) and using newer interactions to update the personalization profile. In either
    case, it is important to provide a way for users to provide feedback to the system
    so that it can learn and adapt to their changing interests over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Privacy can be a concern*—Because personalization is based on previous user-interaction
    patterns, showing personalized recommendations and search results means collecting
    and exposing a user’s past behavior. Imagine a movie-streaming service suggesting
    violent or adult-themed movies, a bookstore suggesting romance novels or self-improvement
    titles, or a grocery store boosting junk food and alcohol. This could both be
    embarrassing and demoralizing for the user, eroding both trust and confidence
    in the service. It is important to be transparent about what signals are being
    collected and how they are being used. It is also important to provide a way for
    users to opt out of personalization if they are concerned about their privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Apply personalization with a light touch*—Most search engines do not personalize
    search results, leaving the user in full control of expressing their interests
    through their current query. Deviating from this paradigm can be beneficial in
    many cases, but it is important to ensure that personalization is only applied
    when it is likely to be helpful to the user. One strategy for ensuring a light
    touch is to apply personalization only to the top few results. It is usually better
    to err on the side of caution with personalization and apply it very conservatively.
    Most users will be less frustrated by a lack of personalization than by a search
    engine that tries too hard to read their minds and gets it wrong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of all the techniques in AI-powered search, personalization is both one
    of the most underutilized ways to better understand user intent and one of the
    most challenging. While recommendation engines are prevalent, the personalization
    spectrum between search and recommendations is more nuanced and less explored.
    So long as personalized search is implemented with care, it can be a powerful
    tool to drive more relevant search results and save the user time discovering
    the items that best meet their particular interests.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Personalized search sits in the middle of the personalization spectrum between
    keyword search (driven by explicit user input) and collaborative recommendations
    (driven by implicit input derived from user behavior).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborative recommendations can be learned entirely from user-interaction
    patterns across documents, but they suffer from the cold-start problem. Combining
    collaborative filtering with content-based attributes can overcome the cold-start
    problem and drive more flexible personalized search experiences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing documents and users as embedding vectors enables building dynamic
    personalization profiles that can be used to drive better-personalized search
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering products by their embedding vectors can be used to generate dynamic
    categories to serve as guardrails for personalized search, ensuring that users
    are not shown results that are personalized too far outside of their interests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating feedback loops to learn from user interactions is important, as
    long as user privacy is preserved and it is applied with a light touch to avoid
    over-personalizing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personalized search can drive more relevant search results, but it’s important
    to balance the benefits of personalization with the potential for user frustration
    if the personalization is too aggressive. Striking the right balance can drive
    significant improvements to your search engine’s understanding of user intent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
