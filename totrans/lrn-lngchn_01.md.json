["```py\n    pip install langchain langchain-openai langchain-community \n    pip install langchain-text-splitters langchain-postgres\n    ```", "```py\n    export OPENAI_API_KEY=your-key\n    ```", "```py\n    jupyter notebook\n    ```", "```py\n    export OPENAI_API_KEY=your-key\n    ```", "```py\n    npm install langchain @langchain/openai @langchain/community\n    npm install @langchain/core pg\n    ```", "```py\nfrom langchain_openai.llms import OpenAI\n\nmodel = OpenAI(model=\"gpt-3.5-turbo\")\n\nmodel.invoke(\"The sky is\")\n```", "```py\nimport { OpenAI } from \"@langchain/openai\";\n\nconst model = new OpenAI({ model: \"gpt-3.5-turbo\" });\n\nawait model.invoke(\"The sky is\");\n```", "```py\nBlue!\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nmodel = ChatOpenAI()\nprompt = [HumanMessage(\"What is the capital of France?\")]\n\nmodel.invoke(prompt)\n```", "```py\nimport { ChatOpenAI } from '@langchain/openai'\nimport { HumanMessage } from '@langchain/core/messages'\n\nconst model = new ChatOpenAI()\nconst prompt = [new HumanMessage('What is the capital of France?')]\n\nawait model.invoke(prompt)\n```", "```py\nAIMessage(content='The capital of France is Paris.')\n```", "```py\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI()\nsystem_msg = SystemMessage(\n    '''You are a helpful assistant that responds to questions with three \n exclamation marks.'''\n)\nhuman_msg = HumanMessage('What is the capital of France?')\n\nmodel.invoke([system_msg, human_msg])\n```", "```py\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI();\nconst prompt = [\n  new SystemMessage(\n    `You are a helpful assistant that responds to questions with three \n exclamation marks.`,\n  ),\n  new HumanMessage(\"What is the capital of France?\"),\n];\n\nawait model.invoke(prompt);\n```", "```py\nAIMessage('Paris!!!')\n```", "```py\nAnswer the question based on the context below. If the question cannot be\nanswered using the information provided, answer with \"I don't know\".\n\nContext: The most recent advancements in NLP are being driven by Large Language \nModels (LLMs). These models outperform their smaller counterparts and have\nbecome invaluable for developers who are creating applications with NLP \ncapabilities. Developers can tap into these models through Hugging Face's\n`transformers` library, or by utilizing OpenAI and Cohere's offerings through\nthe `openai` and `cohere` libraries, respectively.\n\nQuestion: Which model providers offer LLMs?\n\nAnswer:\n```", "```py\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = PromptTemplate.from_template(\"\"\"Answer the question based on the\n context below. If the question cannot be answered using the information \n provided, answer with \"I don't know\".\n\nContext: {context} `Question:` `{question}` ```", "```py\n```", "```py```", "```py```", "``` import { PromptTemplate } from '@langchain/core/prompts'  const template = PromptTemplate.fromTemplate(`Answer the question based on the   context below. If the question cannot be answered using the information   provided, answer with \"I don't know\".  Context: {context}  Question: {question}  Answer: `)  await template.invoke({   context: `The most recent advancements in NLP are being driven by Large   Language Models (LLMs). These models outperform their smaller   counterparts and have become invaluable for developers who are creating   applications with NLP capabilities. Developers can tap into these models   through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI   and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries,   respectively.`,   question: \"Which model providers offer LLMs?\" }) ```", "``` StringPromptValue(text='Answer the question based on the context below. If the      question cannot be answered using the information provided, answer with \"I     don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being      driven by Large Language Models (LLMs). These models outperform their      smaller counterparts and have become invaluable for developers who are      creating applications with NLP capabilities. Developers can tap into these      models through Hugging Face\\'s `transformers` library, or by utilizing      OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries,      respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: ') ```", "``` from langchain_openai.llms import OpenAI from langchain_core.prompts import PromptTemplate  # both `template` and `model` can be reused many times  template = PromptTemplate.from_template(\"\"\"Answer the question based on the   context below. If the question cannot be answered using the information   provided, answer with \"I don't know\".  Context: {context} `Question:` `{question}` ```", "``` ```", "````` ```py` *JavaScript*    ``` import { PromptTemplate } from '@langchain/core/prompts' import { OpenAI } from '@langchain/openai'  const model = new OpenAI() const template = PromptTemplate.fromTemplate(`Answer the question based on the   context below. If the question cannot be answered using the information   provided, answer with \"I don't know\".  Context: {context}  Question: {question}  Answer: `)  const prompt = await template.invoke({   context: `The most recent advancements in NLP are being driven by Large   Language Models (LLMs). These models outperform their smaller   counterparts and have become invaluable for developers who are creating   applications with NLP capabilities. Developers can tap into these models   through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI   and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries,   respectively.`,   question: \"Which model providers offer LLMs?\" })  await model.invoke(prompt) ```py    *The output:*    ``` Hugging Face's `transformers` library, OpenAI using the `openai` library, and  Cohere using the `cohere` library offer LLMs. ```py    If you’re looking to build an AI chat application, the `ChatPromptTemplate` can be used instead to provide dynamic inputs based on the role of the chat message:    *Python*    ``` from langchain_core.prompts import ChatPromptTemplate template = ChatPromptTemplate.from_messages([     ('system', '''Answer the question based on the context below. If the   question cannot be answered using the information provided, answer with   \"I don\\'t know\".'''),     ('human', 'Context: {context}'),     ('human', 'Question: {question}'), ])  template.invoke({     \"context\": \"\"\"The most recent advancements in NLP are being driven by Large   Language Models (LLMs). These models outperform their smaller   counterparts and have become invaluable for developers who are creating   applications with NLP capabilities. Developers can tap into these   models through Hugging Face's `transformers` library, or by utilizing   OpenAI and Cohere's offerings through the `openai` and `cohere`   libraries, respectively.\"\"\",     \"question\": \"Which model providers offer LLMs?\" }) ```py    *JavaScript*    ``` import { ChatPromptTemplate } from '@langchain/core/prompts'  const template = ChatPromptTemplate.fromMessages([   ['system', `Answer the question based on the context below. If the question   cannot be answered using the information provided, answer with \"I   don\\'t know\".`],   ['human', 'Context: {context}'],   ['human', 'Question: {question}'], ])  await template.invoke({   context: `The most recent advancements in NLP are being driven by Large   Language Models (LLMs). These models outperform their smaller   counterparts and have become invaluable for developers who are creating   applications with NLP capabilities. Developers can tap into these models   through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI   and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries,   respectively.`,   question: \"Which model providers offer LLMs?\" }) ```py    *The output:*    ``` ChatPromptValue(messages=[SystemMessage(content='Answer the question based on      the context below. If the question cannot be answered using the information      provided, answer with \"I don\\'t know\".'), HumanMessage(content=\"Context:      The most recent advancements in NLP are being driven by Large Language      Models (LLMs). These models outperform their smaller counterparts and have      become invaluable for developers who are creating applications with NLP      capabilities. Developers can tap into these models through Hugging Face\\'s      `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings      through the `openai` and `cohere` libraries, respectively.\"), HumanMessage     (content='Question: Which model providers offer LLMs?')]) ```py    Notice how the prompt contains instructions in a `SystemMessage` and two instances of `HumanMessage` that contain dynamic `context` and `question` variables. You can still format the template in the same way and get back a static prompt that you can pass to a large language model for a prediction output:    *Python*    ``` from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate  # both `template` and `model` can be reused many times  template = ChatPromptTemplate.from_messages([     ('system', '''Answer the question based on the context below. If the   question cannot be answered using the information provided, answer  with \"I don\\'t know\".'''),     ('human', 'Context: {context}'),     ('human', 'Question: {question}'), ])  model = ChatOpenAI()  # `prompt` and `completion` are the results of using template and model once  prompt = template.invoke({     \"context\": \"\"\"The most recent advancements in NLP are being driven by   Large Language Models (LLMs). These models outperform their smaller   counterparts and have become invaluable for developers who are creating   applications with NLP capabilities. Developers can tap into these   models through Hugging Face's `transformers` library, or by utilizing   OpenAI and Cohere's offerings through the `openai` and `cohere`   libraries, respectively.\"\"\",     \"question\": \"Which model providers offer LLMs?\" })  model.invoke(prompt) ```py    *JavaScript*    ``` import { ChatPromptTemplate } from '@langchain/core/prompts' import { ChatOpenAI } from '@langchain/openai'  const model = new ChatOpenAI() const template = ChatPromptTemplate.fromMessages([   ['system', `Answer the question based on the context below. If the question   cannot be answered using the information provided, answer with \"I   don\\'t know\".`],   ['human', 'Context: {context}'],   ['human', 'Question: {question}'], ])  const prompt = await template.invoke({   context: `The most recent advancements in NLP are being driven by Large   Language Models (LLMs). These models outperform their smaller   counterparts and have become invaluable for developers who are creating   applications with NLP capabilities. Developers can tap into these models   through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI   and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries,   respectively.`,   question: \"Which model providers offer LLMs?\" })  await model.invoke(prompt) ```py    *The output:*    ``` AIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the      `openai` library, and Cohere using the `cohere` library offer LLMs.\") ```py ```` ```py`` `````", "``````py`  `````", "```py`# Getting Specific Formats out of LLMs    Plain text outputs are useful, but there may be use cases where you need the LLM to generate a *structured* output—that is, output in a machine-readable format, such as JSON, XML, CSV, or even in a programming language such as Python or JavaScript. This is very useful when you intend to hand that output off to some other piece of code, making an LLM play a part in your larger application.    ## JSON Output    The most common format to generate with LLMs is JSON. JSON outputs can (for example) be sent over the wire to your frontend code or be saved to a database.    When generating JSON, the first task is to define the schema you want the LLM to respect when producing the output. Then, you should include that schema in the prompt, along with the text you want to use as the source. Let’s see an example:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output:*    ```", "```py    So, first define a schema. In Python, this is easiest to do with Pydantic (a library used for validating data against schemas). In JS, this is easiest to do with Zod (an equivalent library). The method `with_structured_output` will use that schema for two things:    *   The schema will be converted to a `JSONSchema` object (a JSON format used to describe the shape [types, names, descriptions] of JSON data), which will be sent to the LLM. For each LLM, LangChain picks the best method to do this, usually function calling or prompting.           *   The schema will also be used to validate the output returned by the LLM before returning it; this ensures the output produced respects the schema you passed in exactly.              ## Other Machine-Readable Formats with Output Parsers    You can also use an LLM or chat model to produce output in other formats, such as CSV or XML. This is where output parsers come in handy. *Output parsers* are classes that help you structure large language model responses. They serve two functions:    Providing format instructions      Output parsers can be used to inject some additional instructions in the prompt that will help guide the LLM to output text in the format it knows how to parse.      Validating and parsing output      The main function is to take the textual output of the LLM or chat model and render it to a more structured format, such as a list, XML, or other format. This can include removing extraneous information, correcting incomplete output, and validating the parsed values.      Here’s an example of how an output parser works:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output:*    ```", "```py    LangChain provides a variety of output parsers for various use cases, including CSV, XML, and more. We’ll see how to combine output parsers with models and prompts in the next section.    # Assembling the Many Pieces of an LLM Application    The key components you’ve learned about so far are essential building blocks of the LangChain framework. Which brings us to the critical question: How do you combine them effectively to build your LLM application?    ## Using the Runnable Interface    As you may have noticed, all the code examples used so far utilize a similar interface and the `invoke()` method to generate outputs from the model (or prompt template, or output parser). All components have the following:    *   There is a common interface with these methods:               *   `invoke`: transforms a single input into an output                       *   `batch`: efficiently transforms multiple inputs into multiple outputs                       *   `stream`: streams output from a single input as it’s produced                   *   There are built-in utilities for retries, fallbacks, schemas, and runtime configurability.           *   In Python, each of the three methods have `asyncio` equivalents.              As such, all components behave the same way, and the interface learned for one of them applies to all:    *Python*    ```", "```py    *JavaScript*    ```", "```py    In this example, you see how the three main methods work:    *   `invoke()` takes a single input and returns a single output.           *   `batch()` takes a list of outputs and returns a list of outputs.           *   `stream()` takes a single input and returns an iterator of parts of the output as they become available.              In some cases, where the underlying component doesn’t support iterative output, there will be a single part containing all output.    You can combine these components in two ways:    Imperative      Call your components directly, for example, with `model.invoke(...)`      Declarative      Use LangChain Expression Language (LCEL), as covered in an upcoming section      [Table 1-1](#ch01_table_1_1736545659767905) summarizes their differences, and we’ll see each in action next.      Table 1-1\\. The main differences between imperative and declarative composition.   |   | Imperative | Declarative | | --- | --- | --- | | Syntax | All of Python or JavaScript | LCEL | | Parallel execution | Python: with threads or coroutinesJavaScript: with `Promise.all` | Automatic | | Streaming | With yield keyword | Automatic | | Async execution | With async functions | Automatic |    ## Imperative Composition    *Imperative composition* is just a fancy name for writing the code you’re used to writing, composing these components into functions and classes. Here’s an example combining prompts, models, and output parsers:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output:*    ```", "```py    The preceding is a complete example of a chatbot, using a prompt and chat model. As you can see, it uses familiar Python syntax and supports any custom logic you might want to add in that function.    On the other hand, if you want to enable streaming or async support, you’d have to modify your function to support it. For example, streaming support can be added as follows:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output:*    ```", "```py    So, either in JS or Python, you can enable streaming for your custom function by yielding the values you want to stream and then calling it with `stream`.    For asynchronous execution, you’d rewrite your function like this:    *Python*    ```", "```py    This one applies to Python only, as asynchronous execution is the only option in JavaScript.    ## Declarative Composition    LCEL is a *declarative language* for composing LangChain components. LangChain compiles LCEL compositions to an *optimized execution plan*, with automatic parallelization, streaming, tracing, and async support.    Let’s see the same example using LCEL:    *Python*    ```", "```py    *JavaScript*    ```", "```py    *The output:*    ```", "```py    Crucially, the last line is the same between the two examples—that is, you use the function and the LCEL sequence in the same way, with `invoke/stream/batch`. And in this version, you don’t need to do anything else to use streaming:    *Python*    ```", "```py    *JavaScript*    ```", "```py    And, for Python only, it’s the same for using asynchronous methods:    *Python*    ```", "```py    # Summary    In this chapter, you’ve learned about the building blocks and key components necessary to build LLM applications using LangChain. LLM applications are essentially a chain consisting of the large language model to make predictions, the prompt instruction(s) to guide the model toward a desired output, and an optional output parser to transform the format of the model’s output.    All LangChain components share the same interface with `invoke`, `stream`, and `batch` methods to handle various inputs and outputs. They can either be combined and executed imperatively by calling them directly or declaratively using LCEL.    The imperative approach is useful if you intend to write a lot of custom logic, whereas the declarative approach is useful for simply assembling existing components with limited customization.    In [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927), you’ll learn how to provide external data to your AI chatbot as *context* so that you can build an LLM application that enables you to “chat” with your data.```"]