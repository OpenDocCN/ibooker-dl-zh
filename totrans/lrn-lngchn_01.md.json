["```py\n    pip install langchain langchain-openai langchain-community \n    pip install langchain-text-splitters langchain-postgres\n    ```", "```py\n    export OPENAI_API_KEY=your-key\n    ```", "```py\n    jupyter notebook\n    ```", "```py\n    export OPENAI_API_KEY=your-key\n    ```", "```py\n    npm install langchain @langchain/openai @langchain/community\n    npm install @langchain/core pg\n    ```", "```py\nfrom langchain_openai.llms import OpenAI\n\nmodel = OpenAI(model=\"gpt-3.5-turbo\")\n\nmodel.invoke(\"The sky is\")\n```", "```py\nimport { OpenAI } from \"@langchain/openai\";\n\nconst model = new OpenAI({ model: \"gpt-3.5-turbo\" });\n\nawait model.invoke(\"The sky is\");\n```", "```py\nBlue!\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nmodel = ChatOpenAI()\nprompt = [HumanMessage(\"What is the capital of France?\")]\n\nmodel.invoke(prompt)\n```", "```py\nimport { ChatOpenAI } from '@langchain/openai'\nimport { HumanMessage } from '@langchain/core/messages'\n\nconst model = new ChatOpenAI()\nconst prompt = [new HumanMessage('What is the capital of France?')]\n\nawait model.invoke(prompt)\n```", "```py\nAIMessage(content='The capital of France is Paris.')\n```", "```py\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI()\nsystem_msg = SystemMessage(\n    '''You are a helpful assistant that responds to questions with three \n exclamation marks.'''\n)\nhuman_msg = HumanMessage('What is the capital of France?')\n\nmodel.invoke([system_msg, human_msg])\n```", "```py\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI();\nconst prompt = [\n  new SystemMessage(\n    `You are a helpful assistant that responds to questions with three \n exclamation marks.`,\n  ),\n  new HumanMessage(\"What is the capital of France?\"),\n];\n\nawait model.invoke(prompt);\n```", "```py\nAIMessage('Paris!!!')\n```", "```py\nAnswer the question based on the context below. If the question cannot be\nanswered using the information provided, answer with \"I don't know\".\n\nContext: The most recent advancements in NLP are being driven by Large Language \nModels (LLMs). These models outperform their smaller counterparts and have\nbecome invaluable for developers who are creating applications with NLP \ncapabilities. Developers can tap into these models through Hugging Face's\n`transformers` library, or by utilizing OpenAI and Cohere's offerings through\nthe `openai` and `cohere` libraries, respectively.\n\nQuestion: Which model providers offer LLMs?\n\nAnswer:\n```", "```py\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = PromptTemplate.from_template(\"\"\"Answer the question based on the\n context below. If the question cannot be answered using the information \n provided, answer with \"I don't know\".\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer: \"\"\")\n\ntemplate.invoke({\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these \n models through Hugging Face's `transformers` library, or by utilizing \n OpenAI and Cohere's offerings through the `openai` and `cohere` \n libraries, respectively.\"\"\",\n    \"question\": \"Which model providers offer LLMs?\"\n})\n```", "```py\nimport { PromptTemplate } from '@langchain/core/prompts'\n\nconst template = PromptTemplate.fromTemplate(`Answer the question based on the \n context below. If the question cannot be answered using the information \n provided, answer with \"I don't know\".\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer: `)\n\nawait template.invoke({\n  context: `The most recent advancements in NLP are being driven by Large \n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these models \n through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI \n and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries, \n respectively.`,\n  question: \"Which model providers offer LLMs?\"\n})\n```", "```py\nStringPromptValue(text='Answer the question based on the context below. If the \n    question cannot be answered using the information provided, answer with \"I\n    don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being \n    driven by Large Language Models (LLMs). These models outperform their \n    smaller counterparts and have become invaluable for developers who are \n    creating applications with NLP capabilities. Developers can tap into these \n    models through Hugging Face\\'s `transformers` library, or by utilizing \n    OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries, \n    respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: ')\n```", "```py\nfrom langchain_openai.llms import OpenAI\nfrom langchain_core.prompts import PromptTemplate\n\n# both `template` and `model` can be reused many times\n\ntemplate = PromptTemplate.from_template(\"\"\"Answer the question based on the \n context below. If the question cannot be answered using the information \n provided, answer with \"I don't know\".\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer: \"\"\")\n\nmodel = OpenAI()\n\n# `prompt` and `completion` are the results of using template and model once\n\nprompt = template.invoke({\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these \n models through Hugging Face's `transformers` library, or by utilizing \n OpenAI and Cohere's offerings through the `openai` and `cohere` \n libraries, respectively.\"\"\",\n    \"question\": \"Which model providers offer LLMs?\"\n})\n\ncompletion = model.invoke(prompt)\n```", "```py\nimport { PromptTemplate } from '@langchain/core/prompts'\nimport { OpenAI } from '@langchain/openai'\n\nconst model = new OpenAI()\nconst template = PromptTemplate.fromTemplate(`Answer the question based on the \n context below. If the question cannot be answered using the information \n provided, answer with \"I don't know\".\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer: `)\n\nconst prompt = await template.invoke({\n  context: `The most recent advancements in NLP are being driven by Large \n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these models \n through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI \n and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries, \n respectively.`,\n  question: \"Which model providers offer LLMs?\"\n})\n\nawait model.invoke(prompt)\n```", "```py\nHugging Face's `transformers` library, OpenAI using the `openai` library, and \nCohere using the `cohere` library offer LLMs.\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate\ntemplate = ChatPromptTemplate.from_messages([\n    ('system', '''Answer the question based on the context below. If the \n question cannot be answered using the information provided, answer with \n \"I don\\'t know\".'''),\n    ('human', 'Context: {context}'),\n    ('human', 'Question: {question}'),\n])\n\ntemplate.invoke({\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these \n models through Hugging Face's `transformers` library, or by utilizing \n OpenAI and Cohere's offerings through the `openai` and `cohere` \n libraries, respectively.\"\"\",\n    \"question\": \"Which model providers offer LLMs?\"\n})\n```", "```py\nimport { ChatPromptTemplate } from '@langchain/core/prompts'\n\nconst template = ChatPromptTemplate.fromMessages([\n  ['system', `Answer the question based on the context below. If the question \n cannot be answered using the information provided, answer with \"I \n don\\'t know\".`],\n  ['human', 'Context: {context}'],\n  ['human', 'Question: {question}'],\n])\n\nawait template.invoke({\n  context: `The most recent advancements in NLP are being driven by Large \n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these models \n through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI \n and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries, \n respectively.`,\n  question: \"Which model providers offer LLMs?\"\n})\n```", "```py\nChatPromptValue(messages=[SystemMessage(content='Answer the question based on \n    the context below. If the question cannot be answered using the information \n    provided, answer with \"I don\\'t know\".'), HumanMessage(content=\"Context: \n    The most recent advancements in NLP are being driven by Large Language \n    Models (LLMs). These models outperform their smaller counterparts and have \n    become invaluable for developers who are creating applications with NLP \n    capabilities. Developers can tap into these models through Hugging Face\\'s \n    `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings \n    through the `openai` and `cohere` libraries, respectively.\"), HumanMessage\n    (content='Question: Which model providers offer LLMs?')])\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# both `template` and `model` can be reused many times\n\ntemplate = ChatPromptTemplate.from_messages([\n    ('system', '''Answer the question based on the context below. If the \n question cannot be answered using the information provided, answer\n with \"I don\\'t know\".'''),\n    ('human', 'Context: {context}'),\n    ('human', 'Question: {question}'),\n])\n\nmodel = ChatOpenAI()\n\n# `prompt` and `completion` are the results of using template and model once\n\nprompt = template.invoke({\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by \n Large Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these \n models through Hugging Face's `transformers` library, or by utilizing \n OpenAI and Cohere's offerings through the `openai` and `cohere` \n libraries, respectively.\"\"\",\n    \"question\": \"Which model providers offer LLMs?\"\n})\n\nmodel.invoke(prompt)\n```", "```py\nimport { ChatPromptTemplate } from '@langchain/core/prompts'\nimport { ChatOpenAI } from '@langchain/openai'\n\nconst model = new ChatOpenAI()\nconst template = ChatPromptTemplate.fromMessages([\n  ['system', `Answer the question based on the context below. If the question \n cannot be answered using the information provided, answer with \"I \n don\\'t know\".`],\n  ['human', 'Context: {context}'],\n  ['human', 'Question: {question}'],\n])\n\nconst prompt = await template.invoke({\n  context: `The most recent advancements in NLP are being driven by Large \n Language Models (LLMs). These models outperform their smaller \n counterparts and have become invaluable for developers who are creating \n applications with NLP capabilities. Developers can tap into these models \n through Hugging Face's \\`transformers\\` library, or by utilizing OpenAI \n and Cohere's offerings through the \\`openai\\` and \\`cohere\\` libraries, \n respectively.`,\n  question: \"Which model providers offer LLMs?\"\n})\n\nawait model.invoke(prompt)\n```", "```py\nAIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the \n    `openai` library, and Cohere using the `cohere` library offer LLMs.\")\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.pydantic_v1 import BaseModel\n\nclass AnswerWithJustification(BaseModel):\n    '''An answer to the user's question along with justification for the \n answer.'''\n    answer: str\n    '''The answer to the user's question'''\n    justification: str\n    '''Justification for the answer'''\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nstructured_llm = llm.with_structured_output(AnswerWithJustification)\n\nstructured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound \n of feathers\"\"\")\n```", "```py\nimport { ChatOpenAI } from '@langchain/openai'\nimport { z } from \"zod\";\n\nconst answerSchema = z\n  .object({\n    answer: z.string().describe(\"The answer to the user's question\"),\n    justification: z.string().describe(`Justification for the \n answer`),\n  })\n  .describe(`An answer to the user's question along with justification for \n the answer.`);\n\nconst model = new ChatOpenAI({\n  model: \"gpt-3.5-turbo\",\n  temperature: 0,\n}).withStructuredOutput(answerSchema)\nawait model.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")\n```", "```py\n{\n  answer: \"They weigh the same\",\n  justification: \"Both a pound of bricks and a pound of feathers weigh one pound. \n    The weight is the same, but the volu\"... 42 more characters\n}\n```", "```py\nfrom langchain_core.output_parsers import CommaSeparatedListOutputParser\nparser = CommaSeparatedListOutputParser()\nitems = parser.invoke(\"apple, banana, cherry\")\n```", "```py\nimport { CommaSeparatedListOutputParser } from '@langchain/core/output_parsers'\n\nconst parser = new CommaSeparatedListOutputParser()\n\nawait parser.invoke(\"apple, banana, cherry\")\n```", "```py\n['apple', 'banana', 'cherry']\n```", "```py\nfrom langchain_openai.llms import ChatOpenAI\n\nmodel = ChatOpenAI()\n\ncompletion = model.invoke('Hi there!') \n# Hi!\n\ncompletions = model.batch(['Hi there!', 'Bye!'])\n# ['Hi!', 'See you!']\n\nfor token in model.stream('Bye!'):\n    print(token)\n    # Good\n    # bye\n    # !\n```", "```py\nimport { ChatOpenAI } from '@langchain/openai'\n\nconst model = new ChatOpenAI()\n\nconst completion = await model.invoke('Hi there!') \n// Hi!\n\nconst completions = await model.batch(['Hi there!', 'Bye!'])\n// ['Hi!', 'See you!']\n\nfor await (const token of await model.stream('Bye!')) {\n  console.log(token)\n  // Good\n  // bye\n  // !\n}\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import chain\n\n# the building blocks\n\ntemplate = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant.'),\n    ('human', '{question}'),\n])\n\nmodel = ChatOpenAI()\n\n# combine them in a function\n# @chain decorator adds the same Runnable interface for any function you write\n\n@chain\ndef chatbot(values):\n    prompt = template.invoke(values)\n    return model.invoke(prompt)\n\n# use it\n\nchatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n```", "```py\nimport {ChatOpenAI} from '@langchain/openai'\nimport {ChatPromptTemplate} from '@langchain/core/prompts'\nimport {RunnableLambda} from '@langchain/core/runnables'\n\n// the building blocks\n\nconst template = ChatPromptTemplate.fromMessages([\n  ['system', 'You are a helpful assistant.'],\n  ['human', '{question}'],\n])\n\nconst model = new ChatOpenAI()\n\n// combine them in a function\n// RunnableLambda adds the same Runnable interface for any function you write\n\nconst chatbot = RunnableLambda.from(async values => {\n  const prompt = await template.invoke(values)\n  return await model.invoke(prompt)\n})\n\n// use it\n\nawait chatbot.invoke({\n  \"question\": \"Which model providers offer LLMs?\"\n})\n```", "```py\nAIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the \n    `openai` library, and Cohere using the `cohere` library offer LLMs.\")\n```", "```py\n@chain\ndef chatbot(values):\n    prompt = template.invoke(values)\n    for token in model.stream(prompt):\n        yield token\n\nfor part in chatbot.stream({\n    \"question\": \"Which model providers offer LLMs?\"\n}):\n    print(part)\n```", "```py\nconst chatbot = RunnableLambda.from(async function* (values) {\n  const prompt = await template.invoke(values)\n  for await (const token of await model.stream(prompt)) {\n    yield token\n  }\n})\n\nfor await (const token of await chatbot.stream({\n  \"question\": \"Which model providers offer LLMs?\"\n})) {\n  console.log(token)\n}\n```", "```py\nAIMessageChunk(content=\"Hugging\")\nAIMessageChunk(content=\" Face's\")\nAIMessageChunk(content=\" `transformers`\")\n...\n```", "```py\n@chain\nasync def chatbot(values):\n    prompt = await template.ainvoke(values)\n    return await model.ainvoke(prompt)\n\nawait chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})\n# > AIMessage(content=\"\"\"Hugging Face's `transformers` library, OpenAI using\n    the `openai` library, and Cohere using the `cohere` library offer LLMs.\"\"\")\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# the building blocks\n\ntemplate = ChatPromptTemplate.from_messages([\n    ('system', 'You are a helpful assistant.'),\n    ('human', '{question}'),\n])\n\nmodel = ChatOpenAI()\n\n# combine them with the | operator\n\nchatbot = template | model\n\n# use it\n\nchatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n```", "```py\nimport { ChatOpenAI } from '@langchain/openai'\nimport { ChatPromptTemplate } from '@langchain/core/prompts'\nimport { RunnableLambda } from '@langchain/core/runnables'\n\n// the building blocks\n\nconst template = ChatPromptTemplate.fromMessages([\n  ['system', 'You are a helpful assistant.'],\n  ['human', '{question}'],\n])\n\nconst model = new ChatOpenAI()\n\n// combine them in a function\n\nconst chatbot = template.pipe(model)\n\n// use it\n\nawait chatbot.invoke({\n  \"question\": \"Which model providers offer LLMs?\"\n})\n```", "```py\nAIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the \n    `openai` library, and Cohere using the `cohere` library offer LLMs.\")\n```", "```py\nchatbot = template | model\n\nfor part in chatbot.stream({\n    \"question\": \"Which model providers offer LLMs?\"\n}):\n    print(part)\n    # > AIMessageChunk(content=\"Hugging\")\n    # > AIMessageChunk(content=\" Face's\")\n    # > AIMessageChunk(content=\" `transformers`\")\n    # ...\n```", "```py\nconst chatbot = template.pipe(model)\n\nfor await (const token of await chatbot.stream({\n  \"question\": \"Which model providers offer LLMs?\"\n})) {\n  console.log(token)\n}\n```", "```py\nchatbot = template | model\n\nawait chatbot.ainvoke({\n    \"question\": \"Which model providers offer LLMs?\"\n})\n```"]