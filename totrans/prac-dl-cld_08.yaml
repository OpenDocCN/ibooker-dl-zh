- en: 'Chapter 8\. Cloud APIs for Computer Vision: Up and Running in 15 Minutes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。云计算机视觉API：15分钟内上手
- en: Due to repeated incidents of near meltdown at the nearby nuclear power plant,
    the library of the city of Springfield (we are not allowed to mention the state)
    decided that it was too risky to store all their valuable archives in physical
    form. After hearing that the library from their rival city of Shelbyville started
    digitizing their records, they wanted to get in on the game as well. After all,
    their collection of articles such as “Old man yells at cloud” and “Local man thinks
    wrestling is real” and the hundred-year-old iconic photographs of the Gorge and
    the statue of the city’s founder Jebediah Springfield are irreplaceable. In addition
    to making their archives resilient to catastrophes, they would make their archives
    easily searchable and retrievable. And, of course, the residents of Springfield
    now would be able to access all of this material from the comfort of their living
    room couches.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 由于附近核电站多次发生近乎核泄漏的事件，斯普林菲尔德市图书馆（我们不允许提及州名）决定将他们所有宝贵的档案以数字形式存储太过危险。在听说对手城市Shelbyville的图书馆开始数字化他们的记录后，他们也想加入这场游戏。毕竟，他们的文章收藏，如“老人对着云喊叫”和“本地人认为摔跤是真的”，以及百年历史的峡谷和城市创始人杰比达·斯普林菲尔德的标志性照片是无法替代的。除了使他们的档案对灾难具有弹性外，他们还将使他们的档案易于搜索和检索。当然，现在斯普林菲尔德的居民可以在他们的客厅沙发上轻松访问所有这些材料。
- en: The first step in digitizing documents is, of course, scanning. That’s the easy
    part. Then starts the real challenge—processing and understanding all of this
    visual imagery. The team in Springfield had a few different options in front of
    them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数字化文件的第一步当然是扫描。那是容易的部分。然后开始真正的挑战——处理和理解所有这些视觉图像。斯普林菲尔德的团队面前有几种不同的选择。
- en: Perform manual data entry for every single page and every single photograph.
    Given that the city has more than 200 years of rich history, it would take a really
    long time, and would be error prone and expensive. It would be quite an ordeal
    to transcribe all of that material.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每一页和每张照片执行手动数据输入。考虑到这座城市拥有超过200年的丰富历史，这将需要很长时间，而且容易出错且昂贵。转录所有这些材料将是一场痛苦的经历。
- en: Hire a team of data scientists to build an image understanding system. That
    would be a much better approach, but there’s just one tiny hitch in the plan.
    For a library that runs on charitable donations, hiring a team of data scientists
    would quickly exhaust its budget. A single data scientist might not only be the
    highest-paid employee at the library, they might also be the highest-earning worker
    in the entire city of Springfield (barring the wealthy industrialist Montgomery
    Burns).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雇佣一支数据科学家团队来构建图像理解系统。这将是一个更好的方法，但计划中有一个小小的问题。对于依靠慈善捐款运行的图书馆来说，雇佣一支数据科学家团队将很快耗尽其预算。一个数据科学家不仅可能是图书馆中薪水最高的员工，也可能是整个斯普林菲尔德市（除了富有的实业家蒙哥马利·伯恩斯）中收入最高的工人。
- en: Get someone who knows enough coding to use the intelligence of ready-to-use
    vision APIs.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找一个懂得足够编码的人来使用现成的视觉API的智能。
- en: Logically they went with the quick and inexpensive third option. They had a
    stroke of luck, too. Martin Prince, an industrious fourth grader from Springfield
    Elementary who happened to know some coding, volunteered to build out the system
    for them. Although Martin did not know much deep learning (he’s just 10 years
    old, after all), he did know how to do some general coding, including making REST
    API calls using Python. And that was all he really needed to know. In fact, it
    took him just under 15 minutes to figure out how to make his first API call.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑上，他们选择了快速且廉价的第三种选择。他们也有一点运气。斯普林菲尔德小学勤奋的四年级学生马丁·普林斯恰好懂一些编码，自愿为他们建立系统。尽管马丁并不懂太多深度学习（毕竟他只有10岁），但他知道如何进行一些一般编码，包括使用Python进行REST
    API调用。这就是他真正需要知道的。事实上，他只用了不到15分钟就弄清楚了如何进行第一次API调用。
- en: 'Martin’s *modus operandi* was simple: send a scanned image to the cloud API,
    get a prediction back, and store it in a database for future retrieval. And obviously,
    repeat this process for every single record the library owned. He just needed
    to select the correct tool for the job.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 马丁的*工作方式*很简单：将扫描的图像发送到云API，获得预测结果，并将其存储在数据库中以供将来检索。显然，对于图书馆拥有的每一条记录，都要重复这个过程。他只需要选择正确的工具来完成这项工作。
- en: All the big names—Amazon, Google, IBM, Microsoft—provide a similar set of computer-vision
    APIs that label images, detect and recognize faces and celebrities, identify similar
    images, read text, and sometimes even discern handwriting. Some of them even provide
    the ability to train our own classifier without having to write a single line
    of code. Sounds really convenient!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有大公司——亚马逊、谷歌、IBM、微软——都提供类似的计算机视觉API，可以标记图像、检测和识别人脸和名人、识别相似图像、读取文本，有时甚至可以识别手写。其中一些甚至提供了训练我们自己的分类器的能力，而无需编写一行代码。听起来真的很方便！
- en: In the background, these companies are constantly working to improve the state
    of the art in computer vision. They have spent millions in acquiring and labeling
    datasets with a granular taxonomy much beyond the ImageNet dataset. We might as
    well make good use of their researchers’ blood, sweat, and tears (and electricity
    bills).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在背景中，这些公司不断努力改进计算机视觉的最新技术。他们花费了数百万美元来获取和标记数据集，其分类法远远超出了ImageNet数据集。我们不妨充分利用他们研究人员的心血和汗水（以及电费）。
- en: The ease of use, speed of onboarding and development, the variety of functionality,
    richness of tags, and competitive pricing make cloud-based APIs difficult to ignore.
    And all of this without the need to hire an expensive data science team. Chapters
    [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b) and [Chapter 6](part0008.html#7K4G3-13fa565533764549a6f0ab7f11eed62b)
    optimized for accuracy and performance, respectively; this chapter essentially
    optimizes for human resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 易用性、入职和开发速度、功能的多样性、标签的丰富性以及竞争性定价使得基于云的API难以忽视。所有这些都不需要雇佣昂贵的数据科学团队。第5章和第6章分别针对准确性和性能进行了优化；而本章主要针对人力资源进行了优化。
- en: In this chapter, we explore several cloud-based visual recognition APIs. We
    compare them all both quantitatively as well as qualitatively. This should hopefully
    make it easier to choose the one that best suits your target application. And
    if they still don’t match your needs, we’ll investigate how to train a custom
    classifier with just a few clicks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了几种基于云的视觉识别API。我们在数量和质量上对它们进行了比较。这应该会让您更容易选择最适合您目标应用程序的API。如果它们仍然不符合您的需求，我们将探讨如何通过只需几次点击来训练自定义分类器。
- en: (In the interest of full disclosure, some of the authors of this book were previously
    employed at Microsoft, whose offerings are discussed here. We have attempted not
    to let that bias our results by building reproducible experiments and justifying
    our methodology.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （为了公开透明，本书的一些作者以前曾在微软工作，这里讨论的是微软的产品。我们尝试通过构建可重现的实验和证明我们的方法论来遏制这种偏见。）
- en: The Landscape of Visual Recognition APIs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉识别API的景观
- en: Let’s explore some of the different visual recognition APIs out there.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些不同的视觉识别API。
- en: Clarifai
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clarifai
- en: Clarifai ([Figure 8-1](part0010.html#sample_of_clarifaiapostrophes_results))
    was the winner of the 2013 ILSVRC classification task. Started by Matthew Zeiler,
    a graduate student from New York University, this was one of the first visual
    recognition API companies out there.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Clarifai（[图8-1](part0010.html#sample_of_clarifaiapostrophes_results)）是2013年ILSVRC分类任务的获胜者。由纽约大学的研究生Matthew
    Zeiler创立，这是最早的视觉识别API公司之一。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Fun fact: While investigating a classifier to detect NSFW (Not Safe For Work)
    images, it became important to understand and debug what was being learned by
    the CNN in order to reduce false positives. This led Clarifai to invent a visualization
    technique to expose which images stimulate feature maps at any layer in the CNN.
    As they say, necessity is the mother of invention.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实：在研究一个分类器来检测不安全的图像时，理解和调试CNN所学到的内容变得很重要，以减少误报。这导致Clarifai发明了一种可视化技术，以暴露哪些图像在CNN的任何层中刺激特征映射。正如他们所说，需求是发明之母。
- en: What’s unique about this API?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: It offers multilingual tagging in more than 23 languages, visual similarity
    search among previously uploaded photographs, face-based multicultural appearance
    classifier, photograph aesthetic scorer, focus scorer, and embedding vector generation
    to help us build our own reverse-image search. It also offers recognition in specialized
    domains including clothing and fashion, travel and hospitality, and weddings.
    Through its public API, the image tagger supports 11,000 concepts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供超过23种语言的多语言标记，以及在先前上传的照片中进行视觉相似性搜索，基于面部的多元文化外观分类器，照片美学评分器，焦点评分器，以及嵌入向量生成，以帮助我们构建自己的反向图像搜索。它还提供专业领域的识别，包括服装和时尚、旅行和款待以及婚礼。通过其公共API，图像标记器支持11,000个概念。
- en: '![Sample of Clarifai’s results](../images/00215.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Clarifai结果示例](../images/00215.jpeg)'
- en: Figure 8-1\. Sample of Clarifai’s results
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. Clarifai结果示例
- en: Microsoft Cognitive Services
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软认知服务
- en: With the creation of ResNet-152 in 2015, Microsoft was able to win seven tasks
    at the ILSVRC, the COCO Image Captioning Challenge as well as the Emotion Recognition
    in the Wild challenge, ranging from classification and detection (localization)
    to image descriptions. And most of this research was translated to cloud APIs.
    Originally starting out as Project Oxford from Microsoft Research in 2015, it
    was eventually renamed Cognitive Services in 2016\. It’s a comprehensive set of
    more than 50 APIs ranging from vision, natural language processing, speech, search,
    knowledge graph linkage, and more. Historically, many of the same libraries were
    being run at divisions at Xbox and Bing, but they are now being exposed to developers
    externally. Some viral applications showcasing creative ways developers use these
    APIs include *[how-old.net](http://how-old.net)* (How Old Do I Look?), Mimicker
    Alarm (which requires making a particular facial expression in order to defuse
    the morning alarm), and *CaptionBot.ai*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，随着ResNet-152的创建，微软能够在ILSVRC赢得七项任务，包括COCO图像字幕挑战以及野外情绪识别挑战，涵盖了从分类和检测（定位）到图像描述的各个方面。大部分这项研究都被转化为云API。最初是从2015年微软研究的Project
    Oxford开始，最终在2016年更名为认知服务。这是一个包含超过50个API的综合性集合，涵盖视觉、自然语言处理、语音、搜索、知识图链接等多个领域。历史上，许多相同的库在Xbox和必应的部门运行，但现在它们正在向外部开发人员开放。一些展示开发人员如何创造性地使用这些API的病毒应用程序包括*[how-old.net](http://how-old.net)*（我看起来多大？）、Mimicker
    Alarm（需要做出特定的面部表情才能关闭早晨的闹钟）和*CaptionBot.ai*。
- en: What’s unique about this API?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: As illustrated in [Figure 8-2](part0010.html#sample_of_microsoft_cognitive_services_r),
    the API offers image captioning, handwriting understanding, and headwear recognition.
    Due to many enterprise customers, Cognitive Services does not use customer image
    data for improving its services.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-2](part0010.html#sample_of_microsoft_cognitive_services_r)所示，该API提供图像字幕、手写理解和头饰识别。由于有许多企业客户，认知服务不使用客户图像数据来改进其服务。
- en: '![Sample of Microsoft Cognitive Services results](../images/00096.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![微软认知服务结果示例](../images/00096.jpeg)'
- en: Figure 8-2\. Sample of Microsoft Cognitive Services results
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 微软认知服务结果示例
- en: Google Cloud Vision
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌云视觉
- en: Google provided the winning entry at the 2014 ILSVRC with the help of the 22-layer
    GoogLeNet, which eventually paved the way for the now-staple Inception architectures.
    Supplementing the Inception models, in December 2015, Google released a suite
    of Vision APIs. In the world of deep learning, having large amounts of data is
    definitely an advantage to improve one’s classifier, and Google has a lot of consumer
    data. For example, with learnings from Google Street View, you should expect relatively
    good performance in real-world text extraction tasks, like on billboards.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在2014年ILSVRC比赛中凭借22层GoogLeNet获得了胜利，最终为现在常见的Inception架构铺平了道路。除了Inception模型，谷歌在2015年12月发布了一套视觉API。在深度学习领域，拥有大量数据肯定是提高分类器的优势，而谷歌拥有大量的消费者数据。例如，通过从Google街景中学到的知识，您应该期望在现实世界的文本提取任务中表现相对良好，比如在广告牌上。
- en: What’s unique about this API?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: For human faces, it provides the most detailed facial key points ([Figure 8-3](part0010.html#sample_of_google_cloud_visionapostrophes))
    including roll, tilt, and pan to accurately localize the facial features. The
    APIs also return similar images on the web to the given input. A simple way to
    try out the performance of Google’s system without writing code is by uploading
    photographs to Google Photos and searching through the tags.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人脸，它提供了最详细的面部关键点（[图8-3](part0010.html#sample_of_google_cloud_visionapostrophes)），包括滚动、倾斜和平移，以准确定位面部特征。API还会返回与给定输入相似的网络图片。尝试谷歌系统的性能的简单方法是将照片上传到Google照片并通过标签搜索。
- en: '![Sample of Google Cloud Vision’s results](../images/00093.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![谷歌云视觉结果示例](../images/00093.jpeg)'
- en: Figure 8-3\. Sample of Google Cloud Vision’s results
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3. 谷歌云视觉结果示例
- en: Amazon Rekognition
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊Rekognition
- en: No, that title is not a typo. Amazon Rekognition API ([Figure 8-4](part0010.html#sample_of_amazon_rekognitionapostrophes))
    is largely based on Orbeus, a Sunnyvale, California-based startup that was acquired
    by Amazon in late 2015\. Founded in 2012, its chief scientist also had winning
    entries in the ILSVRC 2014 detection challenge. The same APIs were used to power
    PhotoTime, a famous photo organization app. The API’s services are available as
    part of the AWS offerings. Considering most companies already offer photo analysis
    APIs, Amazon is doubling down on video recognition offerings to offer differentiation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不，这个标题并没有错别字。亚马逊Rekognition API（[图8-4](part0010.html#sample_of_amazon_rekognitionapostrophes)）主要基于Orbeus，这是一家位于加利福尼亚州圣尼维尔的初创公司，于2015年底被亚马逊收购。成立于2012年，其首席科学家还在2014年ILSVRC检测挑战中获奖。同样的API被用于推动著名的照片整理应用PhotoTime。该API的服务作为AWS产品的一部分提供。考虑到大多数公司已经提供了照片分析API，亚马逊正在加倍努力提供视频识别服务以实现差异化。
- en: What’s unique about this API?
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: License plate recognition, video recognition APIs, and better end-to-end integration
    examples of Rekognition APIs with AWS offerings like Kinesis Video Streams, Lambda,
    and others. Also, Amazon’s API is the only one that can determine whether the
    subject’s eyes are open or closed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 车牌识别，视频识别API以及与AWS产品（如Kinesis视频流、Lambda等）的更好端到端集成示例是Rekognition API的亮点。此外，亚马逊的API是唯一一个可以确定被拍摄对象的眼睛是睁着还是闭着的API。
- en: '![Sample of Amazon Rekognition’s results](../images/00051.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![亚马逊Rekognition结果示例](../images/00051.jpeg)'
- en: Figure 8-4\. Sample of Amazon Rekognition’s results
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4. 亚马逊Rekognition结果示例
- en: IBM Watson Visual Recognition
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM Watson视觉识别
- en: Under the Watson brand, IBM’s Visual Recognition offering started in early 2015\.
    After purchasing AlchemyAPI, a Denver-based startup, AlchemyVision has been used
    for powering the Visual Recognition APIs ([Figure 8-5](part0010.html#sample_of_ibm_watsonapostrophes_visual_r)).
    Like others, IBM also offers custom classifier training. Surprisingly, Watson
    does not offer optical character recognition yet.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在Watson品牌下，IBM的视觉识别服务于2015年初开始。在收购了位于丹佛的初创公司AlchemyAPI之后，AlchemyVision被用于推动视觉识别API（[图8-5](part0010.html#sample_of_ibm_watsonapostrophes_visual_r)）。与其他公司一样，IBM也提供自定义分类器训练。令人惊讶的是，Watson目前还没有提供光学字符识别功能。
- en: '![Sample of IBM Watson’s Visual Recognition results](../images/00283.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![IBM Watson视觉识别结果示例](../images/00283.jpeg)'
- en: Figure 8-5\. Sample of IBM Watson’s Visual Recognition results
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5. IBM Watson视觉识别结果示例
- en: Algorithmia
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Algorithmia
- en: Algorithmia is a marketplace for hosting algorithms as APIs on the cloud. Founded
    in 2013, this Seattle-based startup has both its own in-house algorithms as well
    as those created by others (in which case creators earn revenue based on the number
    of calls). In our experience, this API did tend to have the slowest response time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Algorithmia是一个在云上托管算法作为API的市场。成立于2013年，这家位于西雅图的初创公司既有自己的内部算法，也有其他人创建的算法（在这种情况下，创建者根据调用次数赚取收入）。根据我们的经验，这个API的响应时间似乎是最慢的。
- en: What’s unique about this API?
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个API有什么独特之处？
- en: Colorization service for black and white photos ([Figure 8-6](part0010.html#sample_of_algorithmiaapostrophes_style_t)),
    image stylization, image similarity, and the ability to run these services on-premises,
    or on any cloud provider.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 黑白照片的着色服务（[图8-6](part0010.html#sample_of_algorithmiaapostrophes_style_t)），图像风格化，图像相似度以及在本地或任何云提供商上运行这些服务的能力。
- en: '![Sample of Algorithmia’s style transfer results](../images/00301.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Algorithmia风格转移结果示例](../images/00301.jpeg)'
- en: Figure 8-6\. Sample of Algorithmia’s style transfer results
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6. Algorithmia风格转移结果示例
- en: With so many offerings, it can be overwhelming to choose a service. There are
    many reasons why we might choose one over another. Obviously, the biggest factors
    for most developers would be accuracy and price. Accuracy is the big promise that
    the deep learning revolution brings, and many applications require it on a consistent
    basis. Price of the service might be an additional factor to consider. We might
    also choose a service provider because our company already has a billing account
    with it, and it would take additional effort to integrate a different service
    provider. Speed of the API response might be another factor, especially if the
    user is waiting on the other end for a response. Because many of these API calls
    can be abstracted, it’s easy to switch between different providers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提供的服务太多，选择一个服务可能会让人感到不知所措。我们可能会因为各种原因选择一个服务而不是另一个。显然，对大多数开发人员来说，最重要的因素是准确性和价格。准确性是深度学习革命带来的重要承诺，许多应用程序需要它以保持一致。服务的价格可能是需要考虑的另一个因素。我们也可能选择一个服务提供商，因为我们的公司已经与其有结算账户，而与其他服务提供商集成将需要额外的努力。API响应速度可能是另一个因素，特别是如果用户正在等待响应。由于许多这些API调用可以被抽象化，因此很容易在不同的提供商之间切换。
- en: Comparing Visual Recognition APIs
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较视觉识别API
- en: To aid our decision making, let’s compare these APIs head to head. In this section,
    we examine service offerings, cost, and accuracy of each.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们做出决策，让我们逐个比较这些API。在本节中，我们将检查每个服务的提供、成本和准确性。
- en: Service Offerings
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务提供
- en: '[Table 8-1](part0010.html#comparison_shopping_of_vision_api_provid) lists what
    services are being offered by each cloud provider.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8-1](part0010.html#comparison_shopping_of_vision_api_provid)列出了每个云提供商提供的服务。'
- en: Table 8-1\. Comparison shopping of vision API providers (as of Aug. 2019)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-1\. 视觉API提供商的比较（截至2019年8月）
- en: '|  | **Algorithmia** | **Amazon Rekognition** | **Clarifai** | **Microsoft
    Cognitive Services** | **Google Cloud Vision** | **IBM Watson Visual Recognition**
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | **Algorithmia** | **Amazon Rekognition** | **Clarifai** | **Microsoft
    Cognitive Services** | **Google Cloud Vision** | **IBM Watson Visual Recognition**
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Image classification | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类 | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '| Image detection | ✔ | ✔ |  | ✔ | ✔ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 图像检测 | ✔ | ✔ |  | ✔ | ✔ |  |'
- en: '| OCR | ✔ | ✔ |  | ✔ | ✔ |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| OCR | ✔ | ✔ |  | ✔ | ✔ |  |'
- en: '| Face recognition | ✔ | ✔ |  | ✔ |  |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 人脸识别 | ✔ | ✔ |  | ✔ |  |  |'
- en: '| Emotionrecognition | ✔ |  | ✔ | ✔ | ✔ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 情绪识别 | ✔ |  | ✔ | ✔ | ✔ |  |'
- en: '| Logo recognition |  |  | ✔ | ✔ | ✔ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 标志识别 |  |  | ✔ | ✔ | ✔ |  |'
- en: '| Landmark recognition |  |  | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 地标识别 |  |  | ✔ | ✔ | ✔ | ✔ |'
- en: '| Celebrityrecognition | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 名人识别 | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '| Multilingual tagging |  |  | ✔ |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 多语言标记 |  |  | ✔ |  |  |  |'
- en: '| Image description |  |  |  | ✔ |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 图像描述 |  |  |  | ✔ |  |  |'
- en: '| Handwriting |  |  |  | ✔ | ✔ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 手写 |  |  |  | ✔ | ✔ |  |'
- en: '| Thumbnail generation | ✔ |  |  | ✔ | ✔ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 缩略图生成 | ✔ |  |  | ✔ | ✔ |  |'
- en: '| Content moderation | ✔ | ✔ | ✔ | ✔ | ✔ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 内容审核 | ✔ | ✔ | ✔ | ✔ | ✔ |  |'
- en: '| Custom classification training |  |  | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 自定义分类训练 |  |  | ✔ | ✔ | ✔ | ✔ |'
- en: '| Custom detector training |   |   |  | ✔ | ✔ |   |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 自定义检测器训练 |   |   |  | ✔ | ✔ |   |'
- en: '| Mobile custom models |   |   | ✔ | ✔ | ✔ |   |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 移动定制模型 |   |   | ✔ | ✔ | ✔ |   |'
- en: '| Free tier | 5,000 requests per month | 5,000 requests per month | 5,000 requests
    per month | 5,000 requests per month | 1,000 requests per month | 7,500 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 免费套餐 | 每月5000次请求 | 每月5000次请求 | 每月5000次请求 | 每月5000次请求 | 每月1000次请求 | 7500 |'
- en: 'That’s a mouthful of services already up and running, ready to be used in our
    application. Because numbers and hard data help make decisions easier, it’s time
    to analyze these services on two factors: cost and accuracy.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有很多服务在运行并准备在我们的应用程序中使用。因为数字和硬数据有助于做出决策，现在是时候在成本和准确性两个因素上分析这些服务了。
- en: Cost
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Money doesn’t grow on trees (yet), so it’s important to analyze the economics
    of using off-the-shelf APIs. Taking a heavy-duty example of querying these APIs
    at about 1 query per second (QPS) service for one full month (roughly 2.6 million
    requests per month), [Figure 8-7](part0010.html#a_cost_comparison_of_different_cloud-bas)
    presents a comparison of the different providers sorted by estimated costs (as
    of August 2019).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 金钱不是从树上长出来的（尚未），因此分析使用现成API的经济学是很重要的。以每秒约1次查询（QPS）的重型查询API为例，为一个完整的月份提供服务（大约每月260万次请求），[图8-7](part0010.html#a_cost_comparison_of_different_cloud-bas)按预估成本排序列出了不同提供商的比较（截至2019年8月）。
- en: '![A cost comparison of different cloud-based vision APIs](../images/00201.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![不同基于云的视觉API的成本比较](../images/00201.jpeg)'
- en: Figure 8-7\. A cost comparison of different cloud-based vision APIs
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-7\. 不同基于云的视觉API的成本比较
- en: Although for most developers, this is an extreme scenario, this would be a pretty
    realistic load for large corporations. We will eventually compare these prices
    against running our own service in the cloud to make sure we get the most bang
    for the buck fitting our scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于大多数开发人员来说，这是一个极端的情况，但对于大型公司来说，这将是一个相当现实的负载。我们最终将比较这些价格与在云中运行我们自己的服务，以确保我们在符合我们情景的情况下获得最大的性价比。
- en: That said, many developers might find negligible charges, considering that all
    of the cloud providers we look at here have a free tier of 5,000 calls per month
    (except Google Vision, which gives only 1,000 calls per month for free), and then
    roughly $1 per 1,000 calls.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，许多开发人员可能会发现费用微不足道，因为我们在这里看到的所有云提供商都有每月5000次调用的免费套餐（除了Google Vision，它每月只提供1000次免费调用），然后大约每1000次调用收取1美元。
- en: Accuracy
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确性
- en: In a world ruled by marketing departments who claim their organizations to be
    the market leaders, how do we judge who is actually the best? What we need are
    common metrics to compare these service providers on some external datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个由市场部门主导的世界中，他们声称自己是市场领导者，我们如何判断谁才是真正的最佳呢？我们需要一些共同的指标来比较这些服务提供商在一些外部数据集上的表现。
- en: To showcase building a reproducible benchmark, we assess the text extraction
    quality using the COCO-Text dataset, which is a subset of the MS COCO dataset.
    This 63,686-image set contains text in daily life settings, like on a banner,
    street sign, number on a bus, price tag in a grocery store, designer shirt, and
    more. This real-world imagery makes it a relatively tough set to test against.
    We use the Word Error Rate (WER) as our benchmarking metric. To keep things simple,
    we ignore the position of the word and focus only on whether a word is present
    (i.e., bag of words). To be a match, the entire word must be correct.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 展示构建可重复基准的过程中，我们使用COCO-Text数据集评估文本提取质量，该数据集是MS COCO数据集的子集。这个包含63,686张图像的数据集包含了日常生活场景中的文本，比如横幅上的文字、街道标志、公交车上的数字、杂货店的价格标签、设计师衬衫等等。这些真实世界的图像使得这个数据集相对较难测试。我们使用词错误率（WER）作为我们的基准度量。为了简化问题，我们忽略单词的位置，只关注单词是否存在（即词袋模型）。为了匹配，整个单词必须是正确的。
- en: In the COCO-Text validation dataset, we pick all images with one or more instances
    of legible text (full-text sequences without interruptions) and compare text instances
    of more than one-character length. We then send these images to various cloud
    vision APIs. [Figure 8-8](part0010.html#wer_for_different_text_extraction_apis_a)
    presents the results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在COCO-Text验证数据集中，我们挑选所有包含一个或多个可读文本实例（没有中断的完整文本序列）的图像，并比较长度超过一个字符的文本实例。然后我们将这些图像发送到各种云视觉API。[图8-8](part0010.html#wer_for_different_text_extraction_apis_a)呈现了结果。
- en: '![WER for different text extraction APIs as of August 2019](../images/00111.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![2019年8月不同文本提取API的WER](../images/00111.jpeg)'
- en: Figure 8-8\. WER for different text extraction APIs as of August 2019
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8. 2019年8月不同文本提取API的WER
- en: Considering how difficult the dataset is, these results are remarkable. Most
    state-of-the-art text extraction tools from earlier in the decade would not cross
    the 10% mark. This shows the power of deep learning. On a subset of manually tested
    images, we also noticed a year-on-year improvement in the performance of some
    of these APIs, which is another benefit enjoyed by cloud-based APIs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集的难度，这些结果是显著的。本十年早期的大多数最先进的文本提取工具都不会超过10%的标记。这展示了深度学习的力量。在一部分手动测试的图像中，我们还注意到这些API中一些的性能每年都有所提高，这是云端API所享有的另一个好处。
- en: As always, all of the code that we used for our experiment is hosted on GitHub
    (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们用于实验的所有代码都托管在GitHub上（请参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）。
- en: The results of our analysis depend significantly on the dataset we choose as
    well as our metrics. Depending on our dataset (which is in turn influenced by
    our use case) as well as our minimum quality metrics, our results can vary. Additionally,
    service providers are constantly improving their services in the background. As
    a consequence, these results are not set in stone and improve over time. These
    results can be replicated on any dataset with the scripts on GitHub.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析的结果在很大程度上取决于我们选择的数据集以及我们的度量标准。根据我们的数据集（又受我们的用例影响）以及我们的最低质量度量标准，我们的结果可能会有所不同。此外，服务提供商在背景中不断改进他们的服务。因此，这些结果并非一成不变，会随着时间的推移而改善。这些结果可以在GitHub上的脚本上复制到任何数据集上。
- en: Bias
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见
- en: In [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b), we explored
    how bias can creep into datasets and how it can have real-life consequences for
    people. The APIs we explore in this chapter are no exception. Joy Buolamwini,
    a researcher at the MIT Media Lab, discovered that among Microsoft, IBM, and Megvii
    (also known as Face++), none were able to detect her face and gender accurately.
    Wondering if she had unique facial features that made her undetectable to these
    APIs, she (working along with Timnit Gebru) compiled faces of members of legislative
    branches from six countries with a high representation of women, building the
    Pilot Parliaments Benchmark (PPB; see [Figure 8-9](part0010.html#averaged_faces_among_different_gender_an)).
    She chose members from three African countries and three European countries to
    test for how the APIs performed on different skin tones. If you haven’t been living
    under a rock, you can already see where this is going.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)中，我们探讨了偏见如何渗入数据集以及它对人们的现实生活造成的后果。本章中我们探索的API也不例外。麻省理工学院媒体实验室的研究员乔伊·布拉明尼发现，在微软、IBM和Megvii（也称为Face++）中，没有一个能够准确检测她的面部和性别。她怀疑自己是否有独特的面部特征，使得这些API无法检测到她（与Timnit
    Gebru合作），编制了六个国家立法机构成员的面部图像，这些国家女性代表比例较高，建立了Pilot Parliaments Benchmark（PPB；请参见[图8-9](part0010.html#averaged_faces_among_different_gender_an)）。她选择了来自三个非洲国家和三个欧洲国家的成员，以测试这些API在不同肤色上的表现。如果你没有生活在石头下，你已经可以看出这将会发展成什么样的情况。
- en: She observed that the APIs performed fairly well overall at accuracies between
    85% and 95%. It was only when she started slicing the data across the different
    categories that she observed there was a massive amount of difference in the accuracies
    for each. She first observed that there was a significant difference between detection
    accuracies of men and women. She also observed that breaking down by skin tone,
    the difference in the detection accuracy was even larger. Then, finally, taking
    both gender and skin tone into consideration, the differences grew painfully starker
    between the worse detected group (darker females) and the best detected group
    (lighter males). For example, in the case of IBM, the detection accuracy of African
    women was a mere 65.3%, whereas the same API gave a 99.7% accuracy for European
    men. A whopping 34.4% difference! Considering many of these APIs are used by law
    enforcement, the consequences of bias seeping in might have life or death consequences.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Averaged faces among different gender and skin tone, from Pilot Parliaments
    Benchmark (PPB)](../images/00239.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Averaged faces among different gender and skin tone, from Pilot
    Parliaments Benchmark (PPB)
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Following are a few insights we learned from this study:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is only as good as the data on which it’s trained. And this shows
    the need for diversity in the training dataset.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often the aggregate numbers don’t always reveal the true picture. The bias in
    the dataset is apparent only when slicing it across different subgroups.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bias does not belong to any specific company; rather, it’s an industry-wide
    phenomenon.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These numbers are not set in stone and reflect only the time at which the experiment
    was performed. As evident from the drastic change in numbers between 2017 ([Figure 8-10](part0010.html#face_detection_comparison_across_apiscom))
    and a subsequent study in 2018 ([Figure 8-11](part0010.html#face_detection_comparison_across_apis_in)),
    these companies are taking bias removal from their datasets quite seriously.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Researchers putting commercial companies to the test with public benchmarks
    results in industry-wide improvements (even if for the fear of bad PR, then so
    be it).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Face detection comparison across APIs, tested in April and May 2017 on the
    PPB](../images/00143.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Face detection comparison across APIs, tested in April and May
    2017 on the PPB
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Face detection comparison across APIs in August 2018 on the PPB, conducted
    by Inioluwa Deborah Raji et al.](../images/00099.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Face detection comparison across APIs in August 2018 on the PPB,
    conducted by Inioluwa Deborah Raji et al.
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How about bias in image-tagging APIs? Facebook AI Research pondered over the
    question “Does Object Recognition Work for Everyone?” in a paper by the same title
    (Terrance DeVries et al.). The group tested multiple cloud APIs in February 2019
    on Dollar Street, a diverse collection of images of household items from 264 different
    homes across 50 countries ([Figure 8-12](part0010.html#image-tagging_api_performance_on_geograp)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Image-tagging API performance on geographically diverse images from the Dollar
    Street dataset](../images/00157.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Image-tagging API performance on geographically diverse images
    from the Dollar Street dataset
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are some of the key learnings from this test:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy of object classification APIs was significantly lower in images from
    regions with lower income levels, as illustrated in [Figure 8-13](part0010.html#average_accuracy_left_parenthesisand_sta).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets such as ImageNet, COCO, and OpenImages severely undersample images
    from Africa, India, China, and Southeast Asia, hence leading to lower performance
    on images from the non-Western world.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the datasets were collected starting with keyword searches in English,
    omitting images that mentioned the same object with phrases in other languages.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Average accuracy (and standard deviation) of six cloud APIs versus income
    of the household where the images were collected](../images/00019.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![六个云API的平均准确率（和标准差）与收集图像的家庭收入之间的关系](../images/00019.jpeg)'
- en: Figure 8-13\. Average accuracy (and standard deviation) of six cloud APIs versus
    income of the household where the images were collected
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13\. 六个云API的平均准确率（和标准差）与收集图像的家庭收入之间的关系
- en: In summary, depending on the scenario for which we want to use these cloud APIs,
    we should build our own benchmarks and test them periodically to evaluate whether
    these APIs are appropriate for the use case.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，根据我们想要使用这些云API的场景，我们应该建立自己的基准，并定期测试它们，以评估这些API是否适合用例。
- en: Getting Up and Running with Cloud APIs
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用云API
- en: Calling these cloud services requires minimal code. At a high level, get an
    API key, load the image, specify the intent, make a POST request with the proper
    encoding (e.g., base64 for the image), and receive the results. Most of the cloud
    providers offer software development kits (SDKs) and sample code showcasing how
    to call their services. They additionally provide pip-installable Python packages
    to further simplify calling them. If you’re using Amazon Rekognition, we highly
    recommend using its `pip` package.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 调用这些云服务只需要很少的代码。在高层次上，获取一个API密钥，加载图像，指定意图，使用正确的编码（例如，对于图像使用base64），发送POST请求并接收结果。大多数云提供商提供软件开发工具包（SDK）和展示如何调用他们服务的示例代码。他们还提供可通过pip安装的Python包，以进一步简化调用过程。如果您正在使用Amazon
    Rekognition，我们强烈建议使用其`pip`包。
- en: Let’s reuse our thrilling image to test-run these services.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复使用我们激动人心的图像来测试这些服务。
- en: 'First, let’s try it on Microsoft Cognitive Services. Get an API key and replace
    it in the following code (the first 5,000 calls are free—more than enough for
    our experiments):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在Microsoft Cognitive Services上尝试一下。获取一个API密钥，并在以下代码中替换它（前5000次调用是免费的——对于我们的实验来说足够了）：
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: “A little girl sitting at a table with a dog”—pretty close! There are other
    options to generate more detailed results, including a probability along with
    each tag.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: “一个坐在桌子旁边的小女孩和一只狗”——非常接近！还有其他选项可以生成更详细的结果，包括每个标签的概率。
- en: Tip
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Although the ImageNet dataset is primarily tagged with nouns, many of these
    services go beyond and return verbs like “eating,” “sitting,” “jumping.” Additionally,
    they might contain adjectives like “red.” Chances are, these might not be appropriate
    for our application. We might want to filter out these adjectives and verbs. One
    option is to check their linguistic type against Princeton’s WordNet. This is
    available in Python with the Natural Language Processing Toolkit (NLTK). Additionally,
    we might want to filter out words like “indoor” and “outdoor” (often shown by
    Clarifai and Cognitive Services).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ImageNet数据集主要标记为名词，但许多这些服务超越了这一点，返回动词如“吃”、“坐”、“跳”。此外，它们可能包含形容词如“红色”。这些可能不适合我们的应用。我们可能希望过滤掉这些形容词和动词。一个选择是将它们的语言类型与普林斯顿的WordNet进行比较。这在Python中使用自然语言处理工具包（NLTK）可用。此外，我们可能希望过滤掉诸如“室内”和“室外”之类的词语（通常由Clarifai和Cognitive
    Services显示）。
- en: 'Now, let’s test the same image using Google Vision APIs. Get an API key from
    their website and use it in the following code (and rejoice, because the first
    1,000 calls are free):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Google Vision API测试相同的图像。从他们的网站获取一个API密钥，并在以下代码中使用它（并且高兴，因为前1000次调用是免费的）：
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Wasn’t that a little too easy? These APIs help us get to state-of-the-art results
    without needing a Ph.D.—in just 15 minutes!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是太容易了吗？这些API帮助我们在不需要博士学位的情况下获得最先进的结果，仅需15分钟！
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Even though these services return tags and image captions with probabilities,
    it’s up to the developer to determine a threshold. Usually, 60% and 40% are good
    thresholds for image tags and image captions, respectively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些服务返回带有概率的标签和图像标题，但开发人员需要确定一个阈值。通常，60%和40%分别是图像标签和图像标题的良好阈值。
- en: It’s also important to communicate the probability to the end-user from a UX
    standpoint. For example, if the result confidence is >80%, we might say prefix
    the tags with “This image *contains....*” For <80%, we might want to change that
    prefix to “This image *may contain…*” to reflect the lower confidence in the result.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户体验的角度，将概率传达给最终用户也很重要。例如，如果结果置信度>80%，我们可能会在标签前加上“这张图片*包含....*”。对于<80%，我们可能希望将该前缀更改为“这张图片*可能包含...*”以反映结果中较低的置信度。
- en: Training Our Own Custom Classifier
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们自己的自定义分类器
- en: Chances are these services were not quite sufficient to meet the requirements
    of our use case. Suppose that the photograph we sent to one of these services
    responded with the tag “dog.” We might be more interested in identifying the breed
    of the dog. Of course, we can follow [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)
    to train our own classifier in Keras. But wouldn’t it be more awesome if we didn’t
    need to write a single line of code? Help is on the way.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些服务可能并不完全满足我们用例的要求。假设我们发送给其中一个服务的照片返回标签“狗”，我们可能更感兴趣的是识别狗的品种。当然，我们可以按照[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)来在Keras中训练自己的分类器。但如果我们不需要编写一行代码，那不是更棒吗？帮助已经到来。
- en: 'A few of these cloud providers give us the ability to train our own custom
    classifier by merely using a drag-and-drop interface. The pretty user interfaces
    provide no indication that under the hood they are using transfer learning. As
    a result, Cognitive Services Custom Vision, Google AutoML, Clarifai, and IBM Watson
    all provide us the option for custom training. Additionally, some of them even
    allow building custom detectors, which can identify the location of objects with
    a bounding box. The key process in all of them being the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些云提供商通过简单的拖放界面让我们有能力训练自己的自定义分类器。漂亮的用户界面并没有表明它们在幕后使用迁移学习。因此，Cognitive Services
    Custom Vision、Google AutoML、Clarifai和IBM Watson都为我们提供了自定义训练的选项。此外，其中一些甚至允许构建自定义检测器，可以识别带有边界框的对象的位置。其中的关键过程如下：
- en: Upload images
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传图片
- en: Label them
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记它们
- en: Train a model
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Evaluate the model
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Publish the model as a REST API
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型发布为REST API
- en: 'Bonus: Download a mobile-friendly model for inference on smartphones and edge
    devices'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励：下载一个适用于智能手机和边缘设备推理的移动友好模型
- en: Let’s see a step-by-step example of Microsoft’s [Custom Vision](https://www.customvision.ai).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下微软的[Custom Vision](https://www.customvision.ai)的逐步示例。
- en: '*Create a project* ([Figure 8-14](part0010.html#creating_a_new_project_in_custom_vision)):
    Choose a domain that best describes our use case. For most purposes, “General”
    would be optimal. For more specialized scenarios, we might want to choose a relevant
    domain.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建一个项目* ([图8-14](part0010.html#creating_a_new_project_in_custom_vision)):
    选择最能描述我们用例的领域。对于大多数情况，“通用”可能是最佳选择。对于更专业的场景，我们可能想选择一个相关的领域。'
- en: '![Creating a new project in Custom Vision](../images/00115.jpeg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在Custom Vision中创建一个新项目](../images/00115.jpeg)'
- en: Figure 8-14\. Creating a new project in Custom Vision
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-14. 在Custom Vision中创建一个新项目
- en: As an example, if we have an ecommerce website with photos of products against
    a pure white background, we might want to select the “Retail” domain. If we intend
    to run this model on a mobile phone eventually, we should choose the “Compact”
    version of the model, instead; it is smaller in size with only a slight loss in
    accuracy.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个电子商务网站，网站上有产品照片背景是纯白色的，我们可能想选择“零售”领域。如果我们最终打算在手机上运行这个模型，我们应该选择模型的“紧凑”版本；它尺寸更小，只有轻微的准确度损失。
- en: '*Upload* ([Figure 8-15](part0010.html#uploading_images_on_customvisiondotai)):
    For each category, upload images and tag them. It’s important to upload at least
    30 photographs per category. For our test, we uploaded more than 30 images of
    Maltese dogs and tagged them appropriately.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*上传* ([图8-15](part0010.html#uploading_images_on_customvisiondotai)): 对于每个类别，上传图片并进行标记。每个类别至少上传30张照片是很重要的。在我们的测试中，我们上传了30多张马耳他犬的图片，并进行了适当的标记。'
- en: '![Uploading images on CustomVision.ai](../images/00326.jpeg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![在CustomVision.ai上上传图片](../images/00326.jpeg)'
- en: Figure 8-15\. Uploading images on CustomVision.ai
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-15. 在CustomVision.ai上上传图片
- en: '*Train* ([Figure 8-16](part0010.html#the_train_button_in_the_upper-right_corn)):
    Click the Train button, and then in about three minutes, we have a spanking new
    classifier ready.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练* ([图8-16](part0010.html#the_train_button_in_the_upper-right_corn)): 点击Train按钮，大约三分钟后，我们就有了一个全新的分类器。'
- en: '![The Train button in the upper-right corner of the CustomVision.ai page](../images/00039.jpeg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CustomVision.ai页面右上角的Train按钮](../images/00039.jpeg)'
- en: Figure 8-16\. The Train button in the upper-right corner of the CustomVision.ai
    page
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-16. CustomVision.ai页面右上角的Train按钮
- en: '*Analyze the model’s performance*: Check the precision and recall of the model.
    By default, the system sets the threshold at 90% confidence and gives the precision
    and recall metrics at that value. For higher precision, increase the confidence
    threshold. This would come at the expense of reduced recall. [Figure 8-17](part0010.html#relative_precision_and_recall_for_our_sa)
    shows example output.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分析模型的性能*：检查模型的精度和召回率。系统默认将阈值设置为90%置信度，并给出该值下的精度和召回率指标。要获得更高的精度，增加置信度阈值。这将以减少召回率为代价。[图8-17](part0010.html#relative_precision_and_recall_for_our_sa)展示了示例输出。'
- en: '*Ready to go*: We now have a production-ready API endpoint that we can call
    from any application.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*准备就绪*：我们现在有一个可以从任何应用程序调用的生产就绪的API端点。'
- en: To highlight the effect of the amount of data on model quality, let’s train
    a dog breed classifier. We can use the Stanford Dogs dataset, a collection of
    more than 100 dog categories. For simplicity, we randomly chose 10 breeds, which
    have more than 200 images available. With 10 classes, a random classifier would
    have one-tenth, or 10%, the chance of correctly identifying an image. We should
    easily be able to beat this number. [Table 8-2](part0010.html#effect_of_number_of_training_images_on_p)
    shows the effect of training on datasets with different volumes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出数据量对模型质量的影响，让我们训练一个狗品种分类器。我们可以使用斯坦福狗数据集，这是一个包含100多种狗品种的集合。为简单起见，我们随机选择了10种品种，这些品种有超过200张可用图片。有了10个类别，一个随机分类器正确识别图像的几率是十分之一，或者10%。我们应该很容易地超过这个数字。表8-2显示了在不同数据集上训练的效果。
- en: Table 8-2\. Effect of number of training images on precision and recall
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-2. 训练图片数量对精度和召回率的影响
- en: '|  | **30 training images/class** | **200 training images/class** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | **每类30张训练图片** | **每类200张训练图片** |'
- en: '| --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Precision | 91.2% | 93.5% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | 91.2% | 93.5% |'
- en: '| Recall | 85.3% | 89.6% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 85.3% | 89.6% |'
- en: '![Relative precision and recall for our sample training set with 200 images
    per class](../images/00087.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![相对精度和召回率，我们的样本训练集每类有200张图片](../images/00087.jpeg)'
- en: Figure 8-17\. Relative precision and recall for our sample training set with
    200 images per class
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-17. 相对精度和召回率，我们的样本训练集每类有200张图片
- en: Because we haven’t uploaded a test set, the performance figures reported here
    are on the full dataset using the common *k*-fold cross-validation technique.
    This means the data was randomly divided into *k* parts, then (*k –* 1) parts
    were used for training, and the remaining part was used for testing. This was
    performed a few times, each time with a randomized subset of images, and the averaged
    results are reported here.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们还没有上传测试集，所以这里报告的性能数据是在完整数据集上使用常见的*k*折交叉验证技术得出的。这意味着数据被随机分成*k*部分，然后(*k -
    1)部分用于训练，剩下的部分用于测试。这个过程进行了几次，每次使用随机子集的图像，并报告了平均结果。
- en: It is incredible that even with 30 images per class, the classifier’s precision
    is greater than 90%, as depicted in [Figure 8-18](part0010.html#some_of_the_possible_tags_returned_by_th).
    And, surprisingly, this took slightly less than 30 seconds to train.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 令人难以置信的是，即使每类只有30张图片，分类器的精度也超过90%，如[图8-18](part0010.html#some_of_the_possible_tags_returned_by_th)所示。令人惊讶的是，这个训练过程只花了不到30秒的时间。
- en: Not only this, we can dig down and investigate the performance on each class.
    Classes with high precision might visibly be more distinct, whereas those with
    low precision might look similar to another class.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Some of the possible tags returned by the API](../images/00205.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 8-18\. Some of the possible tags returned by the API
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This short and convenient approach is not without its downsides, as you will
    see in the following section. In that section, we also discuss mitigation strategies
    to help take advantage of this rather useful tool.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Top Reasons Why Our Classifier Does Not Work Satisfactorily
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of reasons why a classifier would not perform well. The
    following are some of them:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Not enough data
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: If we find that the accuracy is not quite sufficient for our needs, we might
    need to train the system with more data. Of course, 30 images per class just gets
    us started. But for a production-quality application, more images are better.
    200 images per class are usually recommended.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Nonrepresentative training data
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Often, the images on the internet are far too clean, set up in studio lighting
    with clean backgrounds, and close to the center of the frame. Images that our
    application might see on a daily basis might not be represented quite so well.
    It’s really important to train our classifier with real-world images for the best
    performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Unrelated domain
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, Custom Vision is running transfer learning. This makes it really
    important to choose the correct domain when creating the project. As an example,
    if we are trying to classify X-ray images, transfer learning from an ImageNet-based
    model might not yield as accurate a result. For cases like that, training our
    own classifier manually in Keras would work best, as demonstrated in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)
    (though this will probably take more than three minutes).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Using it for regression
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, there are two common categories of problems: classification
    and regression. Classification is predicting one or more classes for input. Regression,
    on the other hand, is predicting a numerical value given an input; for example,
    predicting house prices. Custom Vision is primarily a classification system. Using
    it to count objects by tagging the number of objects is the wrong approach, and
    will lead to unsatisfactory results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Counting objects is a type of regression problem. We can do it by localizing
    each instance of the object in an image (aka object detection) and counting their
    occurrences. Another example of a regression problem is predicting the age of
    a person based on their profile photo. We tackle both problems in later chapters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Classes are too similar
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: If our classes look too similar and rely heavily on smaller-level details for
    distinction, the model might not perform as well. For example, a five-dollar note
    and a 20-dollar note have very similar high-level features. It’s at the lower-level
    details that show they are really distinct. As another example, it might be easy
    to distinguish between a Chihuahua and a Siberian Husky, but it’s more difficult
    to distinguish between an Alaskan Malamute and a Siberian Husky. A fully retrained
    CNN, as demonstrated in [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b),
    should perform better than this Custom Vision-based system.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A great feature of Custom Vision is that if the model is unsure of any image
    that it encounters via its API endpoint, the web UI will show those images for
    a manual review. We can review and manually tag new images on a periodic basis
    and continuously improve the quality of the model. These images tend to improve
    the classifier the most for two reasons: first, they represent real-world usage.
    Second, and more importantly, they have more impact on the model in comparison
    to images that the model can already easily classify. This is known as semisupervised
    learning.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed a few different ways in which we can improve our
    model’s accuracy. In the real world, that is not the end-all-be-all of a user’s
    experience. How quickly we are able to respond to a request also matters a lot.
    In the following section, we cover a couple of different ways we can improve performance
    without sacrificing quality.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Custom Classification APIs
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you might have noticed throughout the book, we are pretty dogmatic about
    being data driven. If we are going to spend good money on a service, we better
    get the best bang for our buck. Time to put the hype to the test.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: For a good number of classification problems, these custom cloud-based classifiers
    perform pretty well. To truly test their limits, we need something more challenging.
    We need to unleash the toughest doggone dataset, train this animal, and fetch
    some insightful results—using the Stanford Dogs dataset.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Using the entire dataset might make it too easy for these classifiers (after
    all, ImageNet already has so many dog breeds), so we took it up a notch. Instead,
    we trained our own Keras classifier on the entire dataset and built a mini-dataset
    out of the top 34 worst-performing classes (each containing at least 140 images).
    The reason these classes performed poorly was because they often became confused
    with other similar-looking dog breeds. To perform better, they require a fine-grained
    understanding of the features. We divide the images into 100 randomly chosen images
    per class in the training dataset and 40 randomly chosen images per class in the
    test dataset. To avoid any class imbalances, which can have an impact on predictions,
    we chose the same number of train and test images for each class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we selected a minimum confidence threshold of 0.5 as it appeared to
    strike a good balance between precision and recall across all services. At a high
    confidence threshold such as 0.99, a classifier might be very accurate, but there
    might be only a handful of images with predictions; in other words, really low
    recall. On the other hand, a really low threshold of 0.01 would result in predictions
    for nearly all images. However, we should not rely on many of these results. After
    all, the classifier is not confident.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of reporting the precision and recall, we report the *F1 score* (also
    known as *F-measure*), which is a hybrid score that combines both of those values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>F</mi><mi>1</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo>
    <mfrac><mrow><mn>2</mn><mo>×</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>F</mi><mi>1</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo>
    <mfrac><mrow><mn>2</mn><mo>×</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></math>
- en: Additionally, we report the time it took to train, as shown in [Figure 8-19](part0010.html#a_chart_showing_the_f1_score_for_custom).
    Beyond just the cloud, we also trained using Apple’s Create ML tool on a MacBook
    Pro with and without data augmentations (rotate, crop, and flip).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Google and Microsoft provide the ability to customize the duration of training.
    Google Auto ML allows us to customize between 1 and 24 hours. Microsoft provides
    a free “Fast Training” option and a paid “Advanced Training” option (similar to
    Google’s offering) with which we can select the duration to be anywhere between
    1 and 24 hours.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![A chart showing the F1 score for custom classifier services, as of August
    2019 (higher is better)](../images/00173.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 8-19\. A chart showing the F1 score for custom classifier services, as
    of August 2019 (higher is better)
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Following are some interesting takeaways from this experiment:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Clarifai and Microsoft offered near-instant training for the 3,400 training
    images.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to “Fast Training,” Microsoft’s “Advanced Training” performed slightly
    better (roughly a 1-point increase) for the extra one hour of training. Because
    “Fast Training” took less than 15 seconds to train, we can infer that its base
    featurizer was already good at extracting fine-grained features.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surprisingly, Apple’s Create ML actually performed worse after adding in the
    augmentations, despite taking more than two extra hours to train, most of which
    was spent creating the augmentations. This was done on a top-of-the-line MacBook
    Pro and showed 100% GPU utilization in Activity Monitor.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, to test the featurizer’s strength, we varied the amount of training
    data supplied to the service ([Figure 8-20](part0010.html#effect_of_varying_size_of_training_data)).
    Because Microsoft took less than 15 seconds to train, it was easy (and cheap!)
    for us to perform the experiment there. We varied between 30 and 100 images per
    class for training while keeping the same 40 images per class for testing.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of varying size of training data per class on test F1 score (higher
    is better)](../images/00219.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 8-20\. Effect of varying size of training data per class on test F1 score
    (higher is better)
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even though Microsoft recommends using at least 50 images per class, going under
    that limit did not affect performance significantly. The fact that the F1 score
    did not vary as much as one would expect shows the value of transfer learning
    (enabling less data to build classifiers) and having a good featurizer capable
    of fine-grained classification.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: It bears repeating that this experiment was intentionally made difficult to
    stress-test these classifiers. On average, they would have performed much better
    on the entire Stanford Dogs dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Performance Tuning for Cloud APIs
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A photograph taken by a modern cell phone can have a resolution as high as 4000
    x 3000 pixels and be upward of 4 MB in size. Depending on the network quality,
    it can take a few seconds to upload such an image to the service. With each additional
    second, it can become more and more frustrating for our users. Could we make this
    faster?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to reduce the size of the image:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Resizing
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Most CNNs take an input image with a size of 224 x 224 or 448 x 448 pixels.
    Much of a cell phone photo’s resolution would be unnecessary for a CNN. It would
    make sense to downsize the image prior to sending it over the network, instead
    of sending a large image over the network and then downsizing it on the server.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Most image libraries perform *lossy* compression while saving a file. Even a
    little bit of compression can go a long way in reducing the size of the image
    while minimally affecting the quality of the image itself. Compression does introduce
    noise, but CNNs are usually robust enough to deal with some of it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Resizing on Image Labeling APIs
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We performed an experiment in which we took more than a hundred diverse unmodified
    images taken from an iPhone at the default resolution (4032 x 3024) and sent them
    to the Google Cloud Vision API to get labels for each of those images. We then
    downsized each of the original images in 5% increments (5%, 10%, 15%…95%) and
    collected the API results for those smaller images, too. We then calculated the
    agreement rate for each image using the following formula:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo>%</mo> <mi>a</mi> <mi>g</mi> <mi>r</mi> <mi>e</mi>
    <mi>e</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>a</mi> <mi>t</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>o</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow>
    <mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></mfrac>
    <mo>×</mo> <mn>100</mn></mrow></math>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo>%</mo> <mi>a</mi> <mi>g</mi> <mi>r</mi> <mi>e</mi>
    <mi>e</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>a</mi> <mi>t</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>o</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow>
    <mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></mfrac>
    <mo>×</mo> <mn>100</mn></mrow></math>
- en: '[Figure 8-21](part0010.html#effect_of_resizing_an_image_on_agreement) shows
    the results of this experiment. In the figure, the solid line shows the reduction
    in file size, and the dotted line represents the agreement rate. Our main conclusion
    from the experiment was that a 60% reduction in resolution led to a 95% reduction
    in file size, with little change in accuracy compared to the original images.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of resizing an image on agreement rate and file size reduction relative
    to the original image](../images/00174.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 8-21\. Effect of resizing an image on agreement rate and file size reduction
    relative to the original image
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Effect of Compression on Image Labeling APIs
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeated the same experiment, but instead of changing the resolution, we
    changed the compression factor for each image incrementally. In [Figure 8-22](part0010.html#effect_of_compressing_an_image_on_agreem),
    the solid line shows the reduction in file size and the dotted line represents
    the agreement rate. The main takeaway here is that a 60% compression score (or
    40% quality) leads to an 85% reduction in file size, with little change in accuracy
    compared to the original image.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of compressing an image on agreement rate and file size reduction
    relative to the original image](../images/00042.jpeg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Figure 8-22\. Effect of compressing an image on agreement rate and file size
    reduction relative to the original image
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Effect of Compression on OCR APIs
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We took a document containing 300-plus words at the default resolution of an
    iPhone (4032 x 3024), and sent it to the Microsoft Cognitive Services API to test
    text recognition. We then compressed it at 5% increments and then sent each image
    and compressed it. We sent these images to the same API and compared their results
    against the baseline to calculate the percentage WER. We observed that even setting
    the compression factor to 95% (i.e., 5% quality of the original image) had no
    effect on the quality of results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Resizing on OCR APIs
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeated the previous experiment, but this time by resizing each image instead
    of compressing. After a certain point, the WER jumped from none to almost 100%,
    with nearly all words being misclassified. Retesting this with another document
    having each word at a different font size showed that all words under a particular
    font size were getting misclassified. To effectively recognize text, OCR engines
    need the text to be bigger than a minimum height (a good rule of thumb is larger
    than 20 pixels). Hence the higher the resolution, the higher the accuracy.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: What have we learned?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: For text recognition, compress images heavily, but do not resize.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For image labeling, a combination of moderate resizing (say, 50%) and moderate
    compression (say, 30%) should lead to heavy file size reductions (and quicker
    API calls) without any difference in quality of API results.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on your application, you might be working with already resized and
    compressed images. Every processing step can introduce a slight difference in
    the results of these APIs, so aim to minimize them.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After receiving an image, cloud APIs internally resize it to fit their own
    implementation. For us, this means two levels of resizing: we first resize an
    image to reduce the size, then send it to the cloud API, which further resizes
    the image. Downsizing images introduces distortion, which is more evident at lower
    resolutions. We can minimize the effect of distortion by resizing from a higher
    resolution, which is bigger by a few multiples. For example, resizing 3024x3024
    (original) → 302x302 (being sent to cloud) → 224x224 (internally resized by APIs)
    would introduce much more distortion in the final image compared to 3024x3024
    → 896x896 → 224x224\. Hence, it’s best to find a happy intermediate size before
    sending the images. Additionally, specifying advanced interpolation options like
    `BICUBIC` and `LANCZOS` will lead to more accurate representation of the original
    image in the smaller version.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Case Studies
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some people say that the best things in life don’t come easy. We believe this
    chapter proves otherwise. In the following section, we take a look at how some
    tech industry titans use cloud APIs for AI to drive some very compelling scenarios.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The New York Times
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might seem like the scenario painted at the beginning of the chapter was
    taken out of a cartoon, but it was, in fact, pretty close to the case of the *New
    York Times* (NYT). With more than 160 years of illustrious history, NYT has a
    treasure trove of photographs in its archives. It stored many of these artifacts
    in the basement of its building three stories below the ground level, aptly called
    the “morgue.” The value of this collection is priceless. In 2015, due to a plumbing
    leak, parts of the basement were damaged including some of these archived records.
    Thankfully the damage was minimal. However, this prompted NYT to consider digitally
    archiving them to protect against another catastrophe.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The photographs were scanned and stored in high quality. However, the photographs
    themselves did not have any identifying information. What many of them did have
    were handwritten or printed notes on the backside giving context for the photographs.
    NYT used the Google Vision API to scan this text and tag the respective images
    with that information. Additionally, this pipeline provided opportunities to extract
    more metadata from the photographs, including landmark recognition, celebrity
    recognition, and so on. These newly added tags powered its search feature so that
    anyone within the company and outside could explore the gallery and search using
    keywords, dates, and so on without having to visit the morgue, three stories down.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Uber
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uber uses Microsoft Cognitive Services to identify each of its seven million-plus
    drivers in a couple of milliseconds. Imagine the sheer scale at which Uber must
    operate its new feature called “Real-Time ID Check.” This feature verifies that
    the current driver is indeed the registered driver by prompting them to take a
    selfie either randomly or every time they are assigned to a new rider. This selfie
    is compared to the driver’s photo on file, and only if the face models are a match
    is the driver allowed to continue. This security feature is helpful for building
    accountability by ensuring the security of the passengers and by ensuring that
    the driver’s account is not compromised. This safety feature is able to detect
    changes in the selfie, including a hat, beard, sunglasses, and more, and then
    prompts the driver to take a selfie without the hat or sunglasses.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![The Uber Drivers app prompts the driver to take a selfie to verify the identity
    of the driver](../images/00208.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: Figure 8-23\. The Uber Drivers app prompts the driver to take a selfie to verify
    the identity of the driver ([image source](https://oreil.ly/lw1Ho))
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Giphy
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in 1976, when Dr. Richard Dawkins coined the term “meme,” little did he
    know it would take on a life of its own four decades later. Instead of giving
    a simple textual reply, we live in a generation where most chat applications suggest
    an appropriate animated GIF matching the context. Several applications provide
    a search specific to memes and GIFs, such as Tenor, Facebook messenger, Swype,
    and Swiftkey. Most of them search through Giphy ([Figure 8-24](part0010.html#giphy_extracts_text_from_animations_as_m)),
    the world’s largest search engine for animated memes commonly in the GIF format.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Giphy extracts text from animations as metadata for searching](../images/00291.jpeg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 8-24\. Giphy extracts text from animations as metadata for searching
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GIFs often have text overlaid (like the dialogue being spoken) and sometimes
    we want to look for a GIF with a particular dialogue straight from a movie or
    TV show. For example, the image in [Figure 8-24](part0010.html#giphy_extracts_text_from_animations_as_m)
    from the 2010 *Futurama* episode in which the “eyePhone” (sic) was released is
    often used to express excitement toward a product or an idea. Having an understanding
    of the contents makes the GIFs more searchable. To make this happen, Giphy uses
    Google’s Vision API to extract the recognize text and objects—aiding the search
    for the perfect GIF.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: It’s obvious that tagging GIFs is a difficult task because a person must sift
    through millions of these animations and manually annotate them frame by frame.
    In 2017, Giphy figured out two solutions to automate this process. The first approach
    was to detect text from within the image. The second approach was to generate
    tags based on the objects in the image to supplement the metadata for their search
    engine. This metadata is stored and searched using ElasticSearch to make a scalable
    search engine.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'For text detection, the company used the OCR services from the Google Vision
    API on the first frame from the GIFs to confirm whether the GIF actually contained
    text. If the API replied in the affirmative, Giphy would send the next frames,
    receive their OCR-detected texts, and figure out the differences in the text;
    for instance, whether the text was static (remaining the same throughout the duration
    of the gif) or dynamic (different text in different frames). For generating the
    class labels corresponding to objects in the image, engineers had two options:
    label detection or web entities, both of which are available on Google Vision
    API. Label detection, as the name suggests, provides the actual class name of
    the object. Web entities provides an entity ID (which can be referenceable in
    the Google Knowledge Graph), which is the unique web URL for identical and similar
    images seen elsewhere on the net. Using these additional annotations gave the
    new system an increase in the click-through-rate (CTR) by 32%. Medium-to-long-tail
    searches (i.e., not-so-frequent searches) benefitted the most, becoming richer
    with relevant content as the extracted metadata surfaced previously unannotated
    GIFs that would have otherwise been hidden. Additionally, this metadata and click-through
    behavior of users provides data to make a similarity and deduplication feature.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: OmniEarth
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OmniEarth is a Virginia-based company that specializes in collecting, analyzing,
    and combining satellite and aerial imagery with other datasets to track water
    usage across the country, scalably, and at high speeds. The company is able to
    scan the entire United States at a total of 144 million parcels of land within
    hours. Internally, it uses the IBM Watson Visual Recognition API to classify images
    of land parcels for valuable information like how green it is. Combining this
    classification with other data points such as temperature and rainfall, OmniEarth
    can predict how much water was used to irrigate the field.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: For house properties, it infers data points from the image such as the presence
    of pools, trees, or irrigable landscaping to predict the amount of water usage.
    The company even predicted where water is being wasted due to malpractices like
    overwatering or leaks. OmniEarth helped the state of California understand water
    consumption by analyzing more than 150,000 parcels of land, and then devised an
    effective strategy to curb water waste.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Photobucket
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Photobucket is a popular online image- and video-hosting community where more
    than two million images are uploaded every day. Using Clarifai’s NSFW models,
    Photobucket automatically flags unwanted or offensive user-generated content and
    sends it for further review to its human moderation team. Previously, the company’s
    human moderation team was able to monitor only about 1% of the incoming content.
    About 70% of the flagged images turned out to be unacceptable content. Compared
    to previous manual efforts, Photobucket identified 700 times more unwanted content,
    thus cleaning the website and creating a better UX. This automation also helped
    discover two child pornography accounts, which were reported to the FBI.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Staples
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ecommerce stores like Staples often rely on organic search engine traffic to
    drive sales. One of the methods to appear high in search engine rankings is to
    put descriptive image tags in the ALT text field for the image. Staples Europe,
    which serves 12 different languages, found tagging product images and translating
    keywords to be an expensive proposition, which is traditionally outsourced to
    human agencies. Fortunately, Clarifai provides tags in 20 languages at a much
    cheaper rate, saving Staples costs into five figures. Using these relevant keywords
    led to an increase in traffic and eventually increased sales through its ecommerce
    store due to a surge of visitors to the product pages.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: InDro Robotics
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This Canadian drone company uses Microsoft Cognitive Services to power search
    and rescue operations, not only during natural disasters but also to proactively
    detect emergencies. The company utilizes Custom Vision to train models specifically
    for identifying objects such as boats and life vests in water ([Figure 8-25](part0010.html#detections_made_by_indro_robotics))
    and use this information to notify control stations. These drones are able to
    scan much larger ocean spans on their own, as compared to lifeguards. This automation
    alerts the lifeguard of emergencies, thus improving the speed of discovery and
    saving lives in the process.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Australia has begun using drones from other companies coupled with inflatable
    pods to be able to react until help reaches. Soon after deployment, these pods
    saved two teenagers stranded in the ocean, as demonstrated in [Figure 8-26](part0010.html#drone_identifies_two_stranded_swimmers_a).
    Australia is also utilizing drones to detect sharks so that beaches can be vacated.
    It’s easy to foresee the tremendous value these automated, custom training services
    can bring.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Detections made by InDro Robotics](../images/00250.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Figure 8-25\. Detections made by InDro Robotics
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Drone identifies two stranded swimmers and releases an inflatable pod that
    they cling onto (image source)](../images/00210.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: Figure 8-26\. Drone identifies two stranded swimmers and releases an inflatable
    pod that they cling onto ([image source](https://oreil.ly/dPxBv))
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various cloud APIs for computer vision, first qualitatively
    comparing the breadth of services offered and then quantitatively comparing their
    accuracy and price. We also looked at potential sources of bias that might appear
    in the results. We saw that with just a short code snippet, we can get started
    using these APIs in less than 15 minutes. Because one model doesn’t fit all, we
    trained a custom classifier using a drag-and-drop interface, and tested multiple
    companies against one another. Finally, we discussed compression and resizing
    recommendations to speed up image transmission and how they affect different tasks.
    To top it all off, we examined how companies across industries use these cloud
    APIs for building real-world applications. Congratulations on making it this far!
    In the next chapter, we will see how to deploy our own inference server for custom
    scenarios.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
