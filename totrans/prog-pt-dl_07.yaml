- en: Chapter 7\. Debugging PyTorch Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 调试PyTorch模型
- en: We’ve created a lot of models so far in this book, but in this chapter, we have
    a brief look at interpreting them and working out what’s going on underneath the
    covers. We take a look at using class activation mapping with PyTorch hooks to
    determine the focus of a model’s decision about how to connect PyTorch to Google’s
    TensorBoard for debugging purposes. I show how to use flame graphs to identify
    the bottlenecks in transforms and training pipelines, as well as provide a worked
    example of speeding up a slow transformation. Finally, we look at how to trade
    compute for memory when working with larger models using *checkpointing*. First,
    though, a brief word about your data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中创建了许多模型，但在本章中，我们简要地看一下如何解释它们并弄清楚底层发生了什么。我们看一下如何使用PyTorch钩子和类激活映射来确定模型决策的焦点，以及如何将PyTorch连接到Google的TensorBoard进行调试。我将展示如何使用火焰图来识别转换和训练管道中的瓶颈，并提供一个加速缓慢转换的示例。最后，我们将看看如何在处理更大的模型时通过*检查点*来交换计算和内存。不过，首先，简要谈谈您的数据。
- en: It’s 3 a.m. What Is Your Data Doing?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 凌晨3点。你的数据在做什么？
- en: 'Before we delve into all the shiny things like TensorBoard or gradient checkpointing
    to use massive models on a single GPU, ask yourself this: do you understand your
    data? If you’re classifying inputs, do you have a balanced sample across all the
    available labels? In the training, validation, and test sets?'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究像TensorBoard或梯度检查点这样的闪亮东西之前，问问自己：您了解您的数据吗？如果您正在对输入进行分类，您是否在所有可用标签上拥有平衡的样本？在训练、验证和测试集中？
- en: And furthermore, are you sure your labels are *right?* Important image-based
    datasets such as MNIST and CIFAR-10 (Canadian Institute for Advanced Research)
    are known to contain some incorrect labels. You should check yours, especially
    if categories are similar to one another, like dog breeds or plant varieties.
    Simply doing a sanity check of your data may end up saving a lot of time if you
    discover that, say, one category of labels has only tiny images, whereas all the
    others have large-resolution examples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，您确定您的标签是*正确的吗？*像MNIST和CIFAR-10（加拿大高级研究所）这样的重要基于图像的数据集已知包含一些不正确的标签。您应该检查您的数据，特别是如果类别彼此相似，比如狗品种或植物品种。简单地对数据进行合理性检查可能会节省大量时间，如果您发现，比如说，一个标签类别只有微小的图像，而其他所有类别都有大分辨率的示例。
- en: Once you’ve made sure your data is in good condition, then yes, let’s head over
    to TensorBoard to start checking out some possible issues in your model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您确保数据处于良好状态，那么是的，让我们转到TensorBoard开始检查模型中的一些可能问题。
- en: TensorBoard
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard
- en: '*TensorBoard* is a web application designed for visualizing various aspects
    of neural networks. It allows for easy, real-time viewing of statistics such as
    accuracy, losses activation values, and really anything you want to send across
    the wire. Although it was written with TensorFlow in mind, it has such an agnostic
    and fairly straightforward API that working with it in PyTorch is not that different
    from how you’d use it in TensorFlow. Let’s install it and see how we can use it
    to gain some insights about our models.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*TensorBoard*是一个用于可视化神经网络各个方面的Web应用程序。它允许轻松实时查看诸如准确性、损失激活值等统计数据，以及您想要发送的任何内容。尽管它是为TensorFlow编写的，但它具有如此通用和相当简单的API，以至于在PyTorch中使用它与在TensorFlow中使用它并没有太大不同。让我们安装它，看看我们如何使用它来获取有关我们模型的一些见解。'
- en: Note
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When reading up on PyTorch, you’ll likely come across references to an application
    called [Visdom](https://oreil.ly/rZqv2), which is Facebook’s alternative to TensorBoard.
    Before PyTorch v1.1, the way to support visualizations was to use Visdom with
    PyTorch while third-party libraries such as `tensorboardX` were available to integrate
    with TensorBoard. While Visdom continues be maintained, the inclusion of an official
    TensorBoard integration in v1.1 and above suggests that the developers of PyTorch
    have recognized that TensorBoard is the de facto neural net visualizer tool.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读PyTorch时，您可能会遇到一个名为[Visdom](https://oreil.ly/rZqv2)的应用程序，这是Facebook对TensorBoard的替代方案。在PyTorch
    v1.1之前，支持可视化的方式是使用Visdom与PyTorch，同时第三方库如`tensorboardX`可用于与TensorBoard集成。虽然Visdom仍在维护，但在v1.1及以上版本中包含了官方的TensorBoard集成，这表明PyTorch的开发人员已经认识到TensorBoard是事实上的神经网络可视化工具。
- en: Installing TensorBoard
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装TensorBoard
- en: 'Installing TensorBoard can be done with either `pip` or `conda`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 安装TensorBoard可以使用`pip`或`conda`：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: PyTorch requires v1.14 or above of TensorBoard.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch需要v1.14或更高版本的TensorBoard。
- en: 'TensorBoard can then be started on the command line:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以在命令行上启动TensorBoard：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can then go to *http://`[your-machine]`:6006*, where you’ll see the welcome
    screen shown in [Figure 7-1](#Tensorboard-with-no-data). We can now send data
    to the application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以转到*http://`[your-machine]`:6006*，您将看到[图7-1](#Tensorboard-with-no-data)中显示的欢迎屏幕。现在我们可以向应用程序发送数据。
- en: '![Tensorboard](assets/ppdl_0701.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Tensorboard](assets/ppdl_0701.png)'
- en: Figure 7-1\. TensorBoard
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1. TensorBoard
- en: Sending Data to TensorBoard
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据发送到TensorBoard
- en: 'The module for using TensorBoard with PyTorch is located in `torch.utils.tensorboard`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch的TensorBoard模块位于`torch.utils.tensorboard`中：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We use the `SummaryWriter` class to talk to TensorBoard using the standard location
    for logging output, *./runs*, and we can send a scalar by using `add_scalar` with
    a tag. Because `SummaryWriter` works asynchronously, it may take a moment, but
    you should see TensorBoard update as shown in [Figure 7-2](#example-data-point-in-tensorboard).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`SummaryWriter`类与TensorBoard通信，使用标准的日志输出位置*./runs*，可以通过使用带有标签的`add_scalar`发送标量。由于`SummaryWriter`是异步工作的，可能需要一会儿，但您应该看到TensorBoard更新，如[图7-2](#example-data-point-in-tensorboard)所示。
- en: '![Example data point in Tensorboard](assets/ppdl_0702.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Tensorboard中的示例数据点](assets/ppdl_0702.png)'
- en: Figure 7-2\. Example data point in TensorBoard
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2. TensorBoard中的示例数据点
- en: 'Not very exciting, is it? Let’s write a loop that sends updates from an initial
    starting point:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是很令人兴奋，对吧？让我们写一个循环，从初始起点发送更新：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By passing where we are in our loop, as shown in [Figure 7-3](#plotting-a-random-walk-in-tensorboard),
    TensorBoard gives us a plot of the random walk we’re doing from 10\. If we run
    the code again, we’ll see that it has generated a different *run* inside the display,
    and we can select on the left side of the web page whether we want to see all
    our runs or just some in particular.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Plotting a random walk in tensorboard](assets/ppdl_0703.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Plotting a random walk in TensorBoard
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can use this to replace our `print` statements in the training loop. We can
    also send the model itself to get a representation in TensorBoard!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When it comes to using `add_graph()`, we need to send in a tensor to trace through
    the model as well as the model itself. Once that happens, though, you should see
    `GRAPHS` appear in TensorBoard, and as shown in [Figure 7-4](#visualizing-resnet),
    clicking the large ResNet block reveals further detail of the model’s structure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing ResNet](assets/ppdl_0704.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Visualizing ResNet
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We now have the ability to send accuracy and loss information as well as model
    structure to TensorBoard. By aggregating multiple runs of accuracy and loss information,
    we can see whether anything is different in a particular run compared to others,
    which is a useful clue when trying to work out why a training run produced poor
    results. We return to TensorBoard shortly, but first let’s look at other features
    that PyTorch makes available for debugging.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Hooks
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch has *hooks*, which are functions that can be attached to either a tensor
    or a module on the forward or backward pass. When PyTorch encounters a module
    with a hook during a pass, it will call the registered hooks. A hook registered
    on a tensor will be called when its gradient is being calculated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Hooks are potentially powerful ways of manipulating modules and tensors because
    you can completely replace the output of what comes into the hook if you so desire.
    You could change the gradient, mask off activations, replace all the biases in
    the module, and so on. In this chapter, though, we’re just going to use them as
    a way of obtaining information about the network as data flows through.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a ResNet-18 model, we can attach a forward hook on a particular part
    of the model by using `register_forward_hook`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you run this code you should see text printed out showing the shape of the
    input to the linear classifier layer of the model. Note that the second time you
    pass a random tensor through the model, you shouldn’t see the `print` statement.
    When we add a hook to a module or tensor, PyTorch returns a reference to that
    hook. We should always save that reference (here we do it in `hook_ref`) and then
    call `remove()` when we’re finished. If you don’t store the reference, then it
    will just hang out and take up valuable memory (and potentially waste compute
    resources during a pass). Backward hooks work in the same way, except you call
    `register_backward_hook()` instead.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if we can `print()` something, we can certainly send it to TensorBoard!
    Let’s see how to use both hooks and TensorBoard to get important stats on our
    layers during training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Plotting Mean and Standard Deviation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start, we set up a function that will send the mean and standard deviation
    of an output layer to TensorBoard:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can’t use this by itself to set up a forward hook, but using the Python
    function `partial()`, we can create a series of forward hooks that will attach
    themselves to a layer with a set `i` value that will make sure that the correct
    values are routed to the right graphs in TensorBoard:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that we’re using `model.children()`, which will attach only to each top-level
    block of the model, so if we have an `nn.Sequential()` layer (which we will have
    in a ResNet-based model), we’ll attach a hook to only that block and not one for
    each individual module within the `nn.Sequential` list.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: If we train our model with our usual training function, we should see the activations
    start streaming into TensorBoard, as shown in [Figure 7-5](#mean-and-standard-deviation-of-modules-in-tensorboard).
    You’ll have to switch to wall-clock time within the UI as we’re no longer sending
    *step* information back to TensorBoard with the hook (as we’re getting the module
    information only when the PyTorch hook is called).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean and Standard Deviation of modules in Tensorboard](assets/ppdl_0705.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Mean and standard deviation of modules in TensorBoard
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, I mentioned in [Chapter 2](ch02.html#image-classification-with-pytorch)
    that, ideally, layers in a neural network should have a mean of 0 and a standard
    deviation of 1 to make sure that our calculations don’t run off to infinity or
    to zero. Have a look at the layers in TensorBoard. Do they look like they’re remaining
    in that value range? Does the plot sometimes spike and then collapse? If so, that
    could be a signal that the network is having difficulty training. In [Figure 7-5](#mean-and-standard-deviation-of-modules-in-tensorboard),
    our mean is close to zero, but our standard deviation is also pretty close to
    zero as well. If this is happening in many layers of your network, it may be a
    sign that your activation functions (e.g., `ReLU`) are not quite suited to your
    problem domain. It might be worth experimenting with other functions to see if
    they improve the model’s performance; PyTorch’s `LeakyReLU` is a good alternative
    offering similar activations to the standard `ReLU` but lets more information
    through, which might help in training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: That about wraps up our look at TensorBoard, but the [“Further Reading”](#chapter-7-further-reading)
    will point you to more resources. In the meantime, let’s see how we can get a
    model to explain how it came to a decision.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Class Activation Mapping
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Class activation mapping* (CAM) is a technique for visualizing the activations
    of a network after it has classified an incoming tensor. In image-based classifiers,
    it’s often shown as a heatmap on top of the original image, as shown in [Figure 7-6](#class-activation-mapping-with-caspert).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Class Activation Mapping with Casper](assets/ppdl_0706.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Class activation mapping with Casper
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the heatmap, we can get an intuitive idea of how the network reached the
    decision of *Persian Cat* from the available ImageNet classes. The activations
    of the network are at their highest around the face and body of the cat and low
    elsewhere in the image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the heatmap, we capture the activations of the final convolutional
    layer of a network, just before it goes into the `Linear` layer, as this allows
    us to see what the combined CNN layers thinks are important as we head into the
    final mapping from image to classes. Thankfully, with PyTorch’s hook feature,
    this is fairly straightforward. We wrap up the hook in a class, `SaveActivations`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then push our image of Casper through the network (normalizing for ImageNet),
    apply `softmax` to turn the output tensor into probabilities, and use `torch.topk()`
    as a way of pulling out both the max probability and its index:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I haven’t explained `torch.nn.functional` yet, but the best way to think about
    it is that it contains the implementation of the *functions* provided in `torch.nn`.
    For example, if you create an instance of `torch.nn.softmax()`, you get an object
    with a `forward()` method that performs `softmax`. If you look in the actual source
    for `torch.nn.softmax()`, you’ll see that all that method does is call `F.softmax()`.
    As we don’t need `softmax` here to be part of a network, we’re just calling the
    underlying function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now access `casper_activations.activations`, we’ll see that it has been
    populated by a tensor, which contains the activations of the final convolutional
    layer we need. We then do this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This calculates the dot product of the activations from Casper (we index into
    0 because of the batching in the first dimension of the input tensor, remember).
    As mentioned in [Chapter 1](ch01.html#getting-started-with-pytorch), PyTorch stores
    image data in C × H × W format, so we next need to rearrange the dimensions back
    to H × W × C for displaying the image. We then remove the minimums from the tensor
    and scale by the maximum to ensure that we’re focusing on only the highest activations
    in the resulting heatmap (i.e., what speaks to *Persian Cat*). Finally, we use
    some `matplot` magic to display Casper and then the tensor on top, resized and
    given a standard `jet` color map. Note that by replacing `idx` with a different
    class, you can see the heatmap indicating which activations (if any) are present
    in the image when classified. So if the model predicts *car*, you can see which
    parts of the image were used to make that decision. The second-highest probability
    for Casper is *Angora Rabbit*, and we can see from the CAM for that index that
    it focused on his very fluffy fur!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这计算了来自Casper的激活的点积（我们索引为0是因为输入张量的第一维中有批处理，记住）。如[第1章](ch01.html#getting-started-with-pytorch)中提到的，PyTorch以C
    × H × W格式存储图像数据，因此我们接下来需要将维度重新排列为H × W × C以显示图像。然后，我们从张量中去除最小值，并通过最大值进行缩放，以确保我们只关注结果热图中最高的激活（即，与*波斯猫*相关的内容）。最后，我们使用一些`matplot`魔法来显示Casper，然后在顶部显示张量，调整大小并给出标准的`jet`颜色映射。请注意，通过用不同的类替换`idx`，您可以看到热图指示图像中存在哪些激活（如果有的话）在分类时。因此，如果模型预测*汽车*，您可以看到图像的哪些部分被用来做出这个决定。Casper的第二高概率是*安哥拉兔*，我们可以从该索引的CAM中看到它专注于他非常蓬松的毛皮！
- en: That wraps up our look into what a model is doing when it makes a decision.
    Next, we’re going to investigate what a model spends most of its time doing while
    it’s in a training loop or during inference.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了模型在做出决策时的情况。接下来，我们将调查模型在训练循环或推断期间大部分时间都在做什么。
- en: Flame Graphs
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 火焰图
- en: In contrast to TensorBoard, *flame graphs* weren’t created specifically for
    neural networks. Nope, not even TensorFlow. In fact, flame graphs trace their
    origin back to 2011, when an engineer named Brendan Gregg, working at a company
    called Joyent, came up with the technique to help debug an issue he was having
    with MySQL. The idea was to take massive stacktraces and turn them into a single
    image, which by itself delivers a picture of what is happening on a CPU over a
    period of time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与TensorBoard相比，*火焰图*并不是专门为神经网络创建的。不，甚至不是为了TensorFlow。事实上，火焰图的起源可以追溯到2011年，当时一位名叫Brendan
    Gregg的工程师在一家名为Joyent的公司工作，他想出了这种技术来帮助调试他在MySQL中遇到的问题。这个想法是将大量的堆栈跟踪转换成单个图像，这本身就可以呈现出CPU在一段时间内的运行情况。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Brendan Gregg now works for Netflix and has a huge amount of [performance-related
    work available to read and digest](http://www.brendangregg.com).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Brendan Gregg现在在Netflix工作，并有大量与性能相关的工作可供阅读和消化。
- en: 'Using an example of MySQL inserting a row into a table, we sample the *stack*
    hundreds or thousand of times a second. Each time we sample, we get a *stacktrace*
    that shows us all the functions in the stack at that point in time. So if we are
    in a function that has been called by another function, we’ll get a trace that
    includes both the callee and caller functions. A sample trace looks like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以MySQL插入表中的一行为例，我们每秒对*堆栈*进行数百次或数千次的采样。每次采样时，我们会得到一个*堆栈跟踪*，显示出该时刻堆栈中的所有函数。因此，如果我们在一个被另一个函数调用的函数中，我们将得到一个包含调用者和被调用者函数的跟踪。一个采样跟踪看起来像这样：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There’s a *lot* of this information; that’s just a tiny sample of a 400KB set
    of stack traces. Even with this collation (which may not be present in all stacktraces),
    it’s difficult to see what’s going on here.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多信息；这只是一个400KB堆栈跟踪集的一个小样本。即使有这种整理（可能不是所有堆栈跟踪中都有），要看清楚这里发生了什么也是很困难的。
- en: The flame graph version, on the other hand, is simple and clear, as you can
    see in [Figure 7-7](#mysql-flame-graph). The y-axis is stack height, and the x-axis
    is, while *not time*, a representation of how often that function is on the stack
    when it has been sampled. So if we had a function at the top of the stack that
    was covering, say, 80% of the graph, we’d know that the program is spending an
    awful lot of running time in that function and that maybe we should look at the
    function to see just what is making it take so long.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，火焰图版本简单明了，如您在[图7-7](#mysql-flame-graph)中所见。y轴是堆栈高度，x轴是，虽然*不是时间*，但表示了在采样时该函数在堆栈中出现的频率。因此，如果我们在堆栈顶部有一个函数占据了80%的图形，我们就会知道程序在该函数中花费了大量的运行时间，也许我们应该查看该函数，看看是什么让它运行如此缓慢。
- en: '![MySQL flame graph](assets/ppdl_0707.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![MySQL火焰图](assets/ppdl_0707.png)'
- en: Figure 7-7\. MySQL flame graph
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7\. MySQL火焰图
- en: You might ask, “What does this have to do with deep learning?” Fair enough;
    it’s a common trope in deep learning research that when training slows down, you
    just buy another 10 GPUs or give Google a lot more money for TPU pods. But maybe
    your training pipeline isn’t GPU bound after all. Perhaps you have a really slow
    transformation, and when you get all those shiny new graphics cards, they don’t
    end up helping as much as you’d have thought. Flame graphs provide a simple, at-a-glance
    way of identifying CPU-bound bottlenecks, and these often occur in practical deep
    learning solutions. For example, remember all those image-based transforms we
    talked about in [Chapter 4](ch04.html#transfer-learning-and-other-tricks)? Most
    of them use the Python Imaging Library and are totally CPU bound. With large datasets,
    you’ll be doing those transforms over and over again within the training loop!
    So while they’re not often brought up in the context of deep learning, flame graphs
    are a great tool to have in your box. If nothing else, you can use them as evidence
    to your boss that you really are GPU bound and you need all those TPU credits
    by next Thursday! We’ll look at getting flame graphs from your training cycles
    and at fixing a slow transformation by moving it from the CPU to the GPU.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问，“这与深度学习有什么关系？”好吧，没错；在深度学习研究中，一个常见的说法是，当训练变慢时，您只需再购买10个GPU或向谷歌支付更多TPU Pod的费用。但也许您的训练流水线并不是完全受GPU限制。也许您有一个非常慢的转换，当您获得所有那些闪亮的新显卡时，它们并没有像您想象的那样有所帮助。火焰图提供了一种简单、一目了然的方法来识别CPU限制的瓶颈，这在实际的深度学习解决方案中经常发生。例如，还记得我们在[第4章](ch04.html#transfer-learning-and-other-tricks)中谈到的所有基于图像的转换吗？大多数都使用Python
    Imaging Library，并且完全受CPU限制。对于大型数据集，您将在训练循环中一遍又一遍地执行这些转换！因此，虽然它们在深度学习的背景下并不经常被提及，但火焰图是您工具箱中很好的工具。如果没有其他办法，您可以将它们用作向老板证明您确实受到GPU限制，并且您需要在下周四之前获得所有那些TPU积分！我们将看看如何从您的训练周期中获取火焰图，并通过将慢转换从CPU移动到GPU来修复它。
- en: Installing py-spy
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装py-spy
- en: 'There are many ways to generate the stacktraces that can be turned into flame
    graphs. The one in the previous section was generated using the Linux tool `perf`,
    which is a complex and powerful tool. We’ll take a somewhat easier option and
    use `py-spy`, a Rust-based stack profiler, to directly generate flame graphs.
    Install it via `pip`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以生成可以转换为火焰图的堆栈跟踪。前一节中生成的是使用Linux工具`perf`生成的，这是一个复杂而强大的工具。我们将采取一个相对简单的选项，并使用`py-spy`，一个基于Rust的堆栈分析器，直接生成火焰图。通过`pip`安装它：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can find the process identifier (PID) of a running process and attach `py-spy`
    by using a `--pid` argument:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用`--pid`参数找到正在运行进程的进程标识符（PID），并附加`py-spy`：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Or you can pass in a Python script, which is how we run it in this chapter.
    First, let’s run it on a simple Python script:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以传入一个Python脚本，这是我们在本章中运行它的方式。首先，让我们在一个简单的Python脚本上运行它：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Save this as *flametest.py* and let’s run `py-spy` on it, sampling 99 times
    a second and running for 30 seconds:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将此保存为*flametest.py*，然后让我们在其上运行`py-spy`，每秒采样99次，运行30秒：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Open the *profile.svg* file in your browser, and let’s take a look at the resulting
    graph.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中打开*profile.svg*文件，让我们看看生成的图形。
- en: Reading Flame Graphs
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读火焰图
- en: '[Figure 7-8](#resnet-flame-graph) shows what the graph should look like, roughly
    speaking (because of sampling, it may not look exactly like this on your machine).
    The first thing you’ll probably notice is that the graph is going down instead
    of up. `py-spy` writes out flame graphs in *icicle* format, so the stack looks
    like stalactites instead of the flames of the classic flame graph. I prefer the
    normal format, but `py-spy` doesn’t give us the option to change it, and it doesn’t
    make that much difference.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-8](#resnet-flame-graph)展示了图形大致应该是什么样子（由于采样的原因，它在您的机器上可能不会完全像这样）。您可能首先注意到的是图形是向下的，而不是向上的。`py-spy`以*icicle*格式编写火焰图，因此堆栈看起来像钟乳石，而不是经典火焰图的火焰。我更喜欢正常格式，但`py-spy`不提供更改选项，而且这并没有太大的区别。'
- en: '![Flame graph on ResNet loading and inference](assets/ppdl_0708.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ResNet加载和推理的火焰图](assets/ppdl_0708.png)'
- en: Figure 7-8\. Flame graph on ResNet loading and inference
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. ResNet加载和推理的火焰图
- en: At a glance, you should see that most of the execution time is spent in various
    `forward()` calls, which makes sense because we are making lots of predictions
    with the model. What about those tiny blocks on the left? If you click them, you
    should find that the SVG file zooms in as shown in [Figure 7-9](#zoomed-flame-graph).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一眼看去，您应该看到大部分执行时间都花在各种`forward()`调用中，这是有道理的，因为我们正在使用模型进行大量预测。左侧的那些小块呢？如果您单击它们，您会发现SVG文件会放大，如[图7-9](#zoomed-flame-graph)所示。
- en: '![Zoomed flame graph](assets/ppdl_0709.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![放大的火焰图](assets/ppdl_0709.png)'
- en: Figure 7-9\. Zoomed flame graph
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. 放大的火焰图
- en: Here, we can see the script setting up the ResNet-18 module and also calling
    `load_state_dict()` to load the saved weights from disk (because we called it
    with `pretrained=True`). You can click Reset Zoom to go back to the full flame
    graph. Also, a search bar on the right will highlight matching bars in purple,
    if you’re trying to hunt down a function. Try it with *resnet*, and it’ll show
    you every function call on the stack with *resnet* in its name. This can be useful
    for finding functions that aren’t on the stack much or seeing how much that pattern
    appears in the graph overall.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到脚本设置了ResNet-18模块，并调用`load_state_dict()`来从磁盘加载保存的权重（因为我们使用`pretrained=True`调用它）。您可以单击“重置缩放”以返回完整的火焰图。此外，右侧的搜索栏将用紫色突出显示匹配的条形，如果您试图查找一个函数。尝试使用*resnet*，它将显示堆栈中名称中带有*resnet*的每个函数调用。这对于查找不经常出现在堆栈中的函数或查看该模式在整个图中出现的频率很有用。
- en: Play around with the SVG for a bit and see how much CPU time things like BatchNorm
    and pooling are taking up in this toy example. Next, we’ll look at a way to use
    flame graphs to find an issue, fix it, and verify it with another flame graph.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 玩一下SVG，看看在这个示例中BatchNorm和池化等东西占用了多少CPU时间。接下来，我们将看一种使用火焰图来查找问题、修复问题并使用另一个火焰图验证的方法。
- en: Fixing a Slow Transformation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修复慢转换
- en: 'In real-world situations, part of your data pipeline may be causing a slowdown.
    This is a particular problem if you have a slow transformation, as it will be
    called many times during a training batch, causing a massive bottleneck in creating
    your model. Here’s an example transformation pipeline and a data loader:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实情况下，你的数据管道的一部分可能会导致减速。如果你有一个慢转换，这将是一个特别的问题，因为它将在训练批次期间被调用多次，导致在创建模型时出现巨大的瓶颈。这里是一个示例转换管道和一个数据加载器：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’re not going to run a full training loop; instead, we simulate 10 epochs
    of just pulling the images from the training data loader:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会运行完整的训练循环；相反，我们模拟了从训练数据加载器中提取图像的10个时期：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s run that code under `py-spy` as before:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像以前一样在`py-spy`下运行该代码：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you open the resulting *slowloader.svg*, you should hopefully see something
    like [Figure 7-10](#badrandom-flame-graph). Although the flame graph is mostly
    occupied with loading the images and converting them to tensors, we are spending
    16.87% of the sampled runtime in applying random noise. Looking at the code, our
    implementation of `BadRandom` is applying noise at the PIL stage rather than at
    the tensor stage, so we’re at the mercy of the imaging library and NumPy rather
    than PyTorch itself. So our first idea would likely be to rewrite the transform
    so that it operates on tensors instead of the PIL images. That’s likely to be
    faster, but not always—and the important thing when making performance changes
    is always to measure everything.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开生成的*slowloader.svg*，你应该会看到类似于[图7-10](#badrandom-flame-graph)的东西。尽管火焰图大部分时间都被用于加载图像并将其转换为张量，但我们在应用随机噪声上花费了采样运行时间的16.87%。看看代码，我们的`BadRandom`实现是在PIL阶段应用噪声，而不是在张量阶段，所以我们受制于图像处理库和NumPy，而不是PyTorch本身。因此，我们的第一个想法可能是重写转换，使其在张量而不是PIL图像上操作。这可能会更快，但并非总是如此——在进行性能更改时的重要事情始终是要测量一切。
- en: '![Flame graph with BadRandom](assets/ppdl_0710.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![带有BadRandom的火焰图](assets/ppdl_0710.png)'
- en: Figure 7-10\. Flame graph with BadRandom
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。带有BadRandom的火焰图
- en: 'But here’s a curious thing, which has been present all the way through the
    book, though I’ve not drawn attention to it until now: have you noticed that we
    pull batches from the data loader and then put those batches onto the GPU? Because
    the transforms occur as the loader gets batches from the dataset class, those
    transforms are always going to happen on the CPU. In some cases, that can lead
    to some crazy lateral thinking. We are applying random noise on every image. What
    if we could apply random noise on every image at once?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一件奇怪的事情，一直贯穿整本书，尽管我直到现在才注意到它：你是否注意到我们从数据加载器中提取批次，然后将这些批次放入GPU？因为转换发生在加载器从数据集类获取批次时，这些转换总是会在CPU上发生。在某些情况下，这可能会导致一些疯狂的横向思维。我们在每个图像上应用随机噪声。如果我们能一次在每个图像上应用随机噪声呢？
- en: 'Here’s the bit that might seem mind-bending at first: we’re adding random noise
    to an image. We can write that as *x + y*, with *x* being our image and *y* our
    noise. We know that both image and noise are 3D (width, height, channels), so
    all we’re doing here is matrix multiplication. And in a batch, we’ll be doing
    this *z* times. We’re just iterating over each image as we pull them out of the
    loader. But consider that at the end of the loading process, the images are transformed
    into tensors, a batch of *[z, c, h, w]*. Well, couldn’t you just add a random
    tensor of shape *[z, c, h, w]* and get the random noise applied that way? Instead
    of applying the noise in sequence, it happens all at once. We now have a matrix
    operation, and a very expensive GPU that just happens to be rather good at matrix
    operations. Try this in Jupyter Notebook to see the difference between CPU and
    GPU tensor matrix operations:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可能一开始看起来有点费解的部分是：我们向图像添加随机噪声。我们可以将其写为*x + y*，其中*x*是我们的图像，*y*是我们的噪声。我们知道图像和噪声都是3D的（宽度、高度、通道），所以这里我们所做的就是矩阵乘法。在一个批次中，我们将这样做*z*次。我们只是在从加载器中取出每个图像时对每个图像进行迭代。但请考虑，在加载过程结束时，图像被转换为张量，一个批次的*[z,
    c, h, w]*。那么，你难道不能只是添加一个形状为*[z, c, h, w]*的随机张量，以这种方式应用随机噪声吗？而不是按顺序应用噪声，它一次性完成。现在我们有了一个矩阵运算，以及一个非常昂贵的GPU，它碰巧非常擅长矩阵运算。在Jupyter
    Notebook中尝试这样做，看看CPU和GPU张量矩阵操作之间的差异：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'That’s just under 20 times faster. Instead of performing this transformation
    in our data loader, we can take it out and perform the matrix operations after
    we have the entire batch at our disposal:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的速度快了近20倍。我们可以将这个转换从我们的数据加载器中取出，等到整个批次都准备好后再执行矩阵运算：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In our training loop, add this line after `input.to(device)`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练循环中，在`input.to(device)`之后添加这行：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Then remove the `BadRandom` transform from the transform pipeline and test again
    with `py-spy`. The new flame graph is shown in [Figure 7-11](#goodrandom-flame-graph).
    It’s so fast that it no longer even shows up under our sampling frequency. We’ve
    just sped up the code by almost 17%! Now, not all standard transforms can be written
    in a GPU-friendly way, but if it’s possible and the transform is slowing you down,
    then it’s definitely an option worth considering.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从转换管道中移除`BadRandom`转换，并使用`py-spy`再次进行测试。新的火焰图显示在[图7-11](#goodrandom-flame-graph)中。它如此之快，以至于它甚至不再在我们的采样频率下显示。我们刚刚将代码加速了近17％！现在，并非所有标准转换都可以以GPU友好的方式编写，但如果可能的话，如果转换正在减慢你的速度，那么这绝对是一个值得考虑的选项。
- en: '![Flame graph with GPU-accelerated random noise](assets/ppdl_0711.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![带有GPU加速随机噪声的火焰图](assets/ppdl_0711.png)'
- en: Figure 7-11\. Flame graph with GPU-accelerated random noise
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11。带有GPU加速随机噪声的火焰图
- en: 'Now that we’ve considered compute, it’s time to look at the other elephant
    in the room: memory, especially memory on the GPU.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经考虑了计算，是时候看看房间里的另一个大象了：内存，特别是GPU上的内存。
- en: Debugging GPU Issues
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试GPU问题
- en: In this section, we drill down deeper into the GPU itself. One thing you’ll
    soon discover in training larger deep learning models is that the shiny GPU that
    you’ve spent so much money on (or, more wisely, attached to a cloud-based instance)
    is brought to its knees regularly, bitterly complaining about running out of memory.
    But that GPU has gigabytes and gigabytes of storage! How could you possibly run
    out?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更深入地研究GPU本身。在训练更大的深度学习模型时，您很快会发现，您花了很多钱购买的闪亮GPU（或者更明智地，连接到基于云的实例）经常陷入困境，痛苦地抱怨内存不足。但是那个GPU有几千兆字节的存储空间！您怎么可能用完？
- en: Models tend to soak up a lot of memory. ResNet-152, for example, has about 60
    million activations, all of which take up precious space on your GPU. Let’s see
    how to peer inside the GPU to determine what could be going on when you’re running
    low on memory.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型往往会占用大量内存。例如，ResNet-152大约有6000万个激活，所有这些都占据了GPU上宝贵的空间。让我们看看如何查看GPU内部，以确定在内存不足时可能发生了什么。
- en: Checking Your GPU
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查您的GPU
- en: Assuming you are using an NVIDIA GPU (check your alternate GPU supplier’s drivers
    website for their own utilities if you’re using something different), the CUDA
    installation includes a rather useful command-line tool called `nvidia-smi`. When
    run with no arguments, this tool can give you a snapshot of the memory being used
    on the GPU, and even better, what is using it! [Figure 7-12](#output-from-nvidia-smi)
    shows output from running `nvidia-smi` within the terminal. Within a notebook,
    you can call out to the utility by using `!nvidia-smi`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在使用NVIDIA GPU（如果使用其他设备，请查看备用GPU供应商的驱动程序网站以获取他们自己的实用程序），CUDA安装包括一个非常有用的命令行工具，称为`nvidia-smi`。当不带参数运行时，此工具可以为您提供有关GPU上使用的内存的快照，甚至更好的是，是谁在使用它！[图7-12](#output-from-nvidia-smi)显示了在终端中运行`nvidia-smi`的输出。在笔记本中，您可以通过使用`!nvidia-smi`调用该实用程序。
- en: '![Output from nvidia-smi](assets/ppdl_0712.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![从nvidia-smi输出](assets/ppdl_0712.png)'
- en: Figure 7-12\. Output from nvidia-smi
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12。从nvidia-smi输出
- en: This example is taken from my home machine running a 1080 Ti. I’m running a
    bunch of notebooks, each of which is taking up a chunk of memory, but one is using
    4GB! You can get the current PID of a notebook by using `os.getpid()`. It turns
    out that the process using the most memory was actually an experimental notebook
    I was using to test out the GPU transforms in the previous section! You can imagine
    that with the model, batch data, and data for the forward and backward passes,
    things get tight memory-wise rather quickly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例来自我家里运行的一台1080 Ti机器。我正在运行一堆笔记本，每个笔记本都占用了一部分内存，但有一个占用了4GB！您可以使用`os.getpid()`获取笔记本的当前PID。结果表明，占用最多内存的进程实际上是我用来测试上一节中GPU变换的实验性笔记本！您可以想象，随着模型、批数据以及前向和后向传递的数据，内存很快会变得紧张。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: I also have a couple of processes running that are, perhaps surprisingly, doing
    graphics—namely, the X server and GNOME. Unless you’ve built a local machine,
    you almost certainly won’t see these.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有一些进程在运行，也许令人惊讶的是，正在进行图形处理——即X服务器和GNOME。除非您构建了本地机器，否则几乎肯定看不到这些。
- en: In addition, PyTorch will dedicate a chunk of memory to itself and CUDA per
    process that is around 0.5GB of memory. This means that it’s a better idea to
    work on one project at a time and not leave Jupyter Notebook running all over
    the place as I have here (you can use the Kernel menu to shut down the Python
    process connected to a notebook).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PyTorch将为每个进程分配大约0.5GB的内存给自身和CUDA。这意味着最好一次只处理一个项目，而不要像我这样到处运行Jupyter Notebook（您可以使用内核菜单关闭与笔记本连接的Python进程）。
- en: 'Running `nvidia-smi` by itself will give you the current snapshot of the GPU’s
    usage, but you can get continual output by using the `-l` flag. Here’s an example
    command that will dump the timestamp, used memory, free memory, total memory,
    and GPU utilization every 5 seconds:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 仅运行`nvidia-smi`将为您提供GPU使用情况的当前快照，但您可以使用`-l`标志获得持续输出。以下是一个示例命令，每5秒将转储时间戳、已使用内存、空闲内存、总内存和GPU利用率：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If you really think that your GPU is using up more memory than it should be,
    you can try getting Python’s garbage collector involved. If you have a `tensor_to_be_deleted`
    that you no longer need and want it gone from the GPU, then a tip from the bowels
    of the fast.ai library is to give it a shove with `del`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您真的认为GPU使用的内存比应该使用的要多，可以尝试让Python的垃圾收集器参与其中。如果您有一个不再需要的`tensor_to_be_deleted`，并且希望它从GPU中消失，那么来自fast.ai库深处的一个提示是使用`del`将其推开：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you’re doing a lot of work inside Jupyter Notebook creating and re-creating
    models, you may find that deleting some references and invoking the garbage collector
    by using `gc.collect()` will claw back some memory. If you’re still having trouble
    with memory, read on, because there may be an answer to your woes!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Jupyter Notebook中进行大量工作，创建和重新创建模型，可能会发现删除一些引用并通过使用`gc.collect()`调用垃圾收集器将收回一些内存。如果您仍然遇到内存问题，请继续阅读，因为可能会有解决您困扰的答案！
- en: Gradient Checkpointing
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度检查点
- en: Despite all the deletion and garbage collection tricks presented in the previous
    section, you might still find yourself running out of memory. The next thing to
    do for most applications is to reduce the batch size of data going through a model
    during the training loop. This will work, but you’re going to increase training
    time for each epoch, and it’s likely that the model will not be as good as an
    equivalent one trained with enough memory to handle the larger batch sizes, because
    you’ll be seeing more of the dataset on every pass. However, we can trade compute
    against memory for large models in PyTorch by using *gradient checkpointing*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在上一节中介绍了所有删除和垃圾收集技巧，您可能仍然会发现自己内存不足。对于大多数应用程序来说，下一步要做的事情是减少在训练循环中通过模型的数据批量大小。这样做会起作用，但您将增加每个时代的训练时间，并且很可能模型不会像使用足够内存处理更大批量大小的等效模型那样好，因为您将在每次传递中看到更多数据集。但是，我们可以通过使用*梯度检查点*在PyTorch中为大型模型交换计算和内存。
- en: One of the problems when dealing with bigger models is that the forward and
    backward passes create lots of intermediate state, all of which occupy GPU memory.
    The goal of gradient checkpointing is to reduce the amount of state that may be
    on the GPU at any one time by *segmenting* the model. This approach means that
    you can have between four and ten times the batch size with a nonsegmented model,
    with that being offset by the training being more compute-intensive. During the
    forward pass, PyTorch saves the inputs and the parameters to a segment, but doesn’t
    actually do the forward pass itself. During the backward pass, these are retrieved
    by PyTorch, and the forward pass is computed for that segment. The intermediate
    values are passed onto the next segment, but those have to be performed on only
    a segment-by-segment basis.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 处理更大模型时的一个问题是，前向和后向传递会产生大量中间状态，所有这些状态都会占用GPU内存。梯度检查点的目标是通过*分段*模型来减少可能同时存在于GPU上的状态量。这种方法意味着您可以在非分段模型的情况下具有四到十倍的批量大小，但这会使训练更加计算密集。在前向传递期间，PyTorch会将输入和参数保存到一个段中，但实际上不执行前向传递。在后向传递期间，PyTorch会检索这些内容，并为该段计算前向传递。中间值会传递到下一个段，但这些值必须仅在段与段之间执行。
- en: 'Chopping up a model into these segments is handled by `torch.utils.checkpoint.checkpoint_sequential()`.
    It works on `nn.Sequential` layers or generated lists of layers, with the proviso
    that they need to be in sequence of how they occur in the model. Here’s how it
    would work on the `features` module in AlexNet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型分割成这些段的工作由`torch.utils.checkpoint.checkpoint_sequential()`处理。它适用于`nn.Sequential`层或生成的层列表，但需要注意它们需要按照模型中出现的顺序排列。以下是它在AlexNet的`features`模块上的工作方式：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, not much is different here, making checkpointing an easy addition
    to models when required. We’ve added a `chunks` parameter to the new version of
    the model, with the default being to split it into two segments. All we then need
    to do is make a call to `checkpoint_sequential` with the `features` module, the
    number of segments, and our inputs. And that’s it!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，当需要时，检查点是模型的一个简单补充。我们在新版本的模型中添加了一个`chunks`参数，默认情况下将其分成两个部分。然后，我们只需要调用`checkpoint_sequential`与`features`模块，段数和我们的输入。就是这样！
- en: 'One slight kink in checkpointing is that it doesn’t behave well with `BatchNorm`
    or `Dropout` layers because of how they interact with the forward pass. To work
    around that, you can just checkpoint parts of the model before and after those
    layers. In our `CheckpointedAlexNet`, we could perhaps break the `classifier`
    module into two parts: one containing the `Dropout` layers that are uncheckpointed,
    and a final `nn.Sequential` module containing our `Linear` layers that we could
    checkpoint in the same way we did with `features`.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查点中的一个小问题是，它与`BatchNorm`或`Dropout`层的交互方式会导致不良行为。为了解决这个问题，您可以在这些层之前和之后只检查点模型的部分。在我们的`CheckpointedAlexNet`中，我们可以将`classifier`模块分成两部分：一个包含未检查点的`Dropout`层，以及一个包含我们的`Linear`层的最终`nn.Sequential`模块，我们可以以与`features`相同的方式检查点。
- en: If you find yourself with diminishing batch sizes in order to get a model to
    run, consider checkpointing before you ask for a larger GPU!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现为了使模型运行而减少批量大小，请在要求更大的GPU之前考虑检查点！
- en: Conclusion
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Hopefully, you’re now equipped to go hunting in search of answers when training
    your model doesn’t go as planned. From sanitizing data to running flame graph
    or TensorBoard visualizations, you have a lot of tools at your disposal; you’ve
    also seen ways of trading memory for compute with GPU transforms, and vice versa
    using checkpointing.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在您已经具备了在训练模型不如预期时寻找答案的能力。从清理数据到运行火焰图或TensorBoard可视化，您有很多工具可供使用；您还看到了如何通过GPU转换以及使用检查点来交换内存和计算。
- en: 'Armed with a properly trained, debugged model, we’re on our way to that harshest
    of realms: *production*.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有经过适当训练和调试的模型，我们正走向最严酷的领域：*生产*。
- en: Further Reading
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[TensorBoard documentation](https://oreil.ly/MELKl)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorBoard文档](https://oreil.ly/MELKl)'
- en: '[TensorBoard GitHub](https://oreil.ly/21bIM)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorBoard GitHub](https://oreil.ly/21bIM)'
- en: 'Fast.ai Lesson 10: [Looking Inside The Model](https://oreil.ly/K4dz-)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fast.ai第10课：[深入了解模型](https://oreil.ly/K4dz-)
- en: '[Investigation into BatchNorm within a ResNet model](https://oreil.ly/EXdK3)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对ResNet模型中BatchNorm的调查](https://oreil.ly/EXdK3)'
- en: Deeper dive into generating [flame graphs](https://oreil.ly/4Ectg) with Brendan
    Gregg
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨如何使用Brendan Gregg生成[火焰图](https://oreil.ly/4Ectg)
- en: '[nvidia-smi documentation](https://oreil.ly/W1g0n)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[nvidia-smi文档](https://oreil.ly/W1g0n)'
- en: '[PyTorch gradient checkpointing documentation](https://oreil.ly/v0apy)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch梯度检查点文档](https://oreil.ly/v0apy)'
