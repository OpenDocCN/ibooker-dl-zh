["```py\nimport functools\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n```", "```py\nTRAIN_DATA_URL = \"https://storage.googleapis.com/\ntf-datasets/titanic/train.csv\"\nTEST_DATA_URL = \"https://storage.googleapis.com/\ntf-datasets/titanic/eval.csv\"\n\ntrain_file_path = tf.keras.utils.get_file(\"train.csv\", \nTRAIN_DATA_URL)\ntest_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n\n```", "```py\nprint(train_file_path)\n\n/root/.keras/datasets/train.csv\n```", "```py\ntitanic_df = pd.read_csv(train_file_path, header='infer')\n```", "```py\nLABEL_COLUMN = 'survived'\nLABELS = [0, 1]\n\ntrain_ds = tf.data.experimental.make_csv_dataset(\n      train_file_path,\n      batch_size=3,\n      label_name=LABEL_COLUMN,\n      na_value=\"?\",\n      num_epochs=1,\n      ignore_errors=True)\n\ntest_ds = tf.data.experimental.make_csv_dataset(\n      test_file_path,\n      batch_size=3,\n      label_name=LABEL_COLUMN,\n      na_value=\"?\",\n      num_epochs=1,\n      ignore_errors=True)\n```", "```py\nfor batch, label in train_ds.take(1):\n  print(label)\n  for key, value in batch.items():\n    print(\"{}: {}\".format(key,value.numpy()))\n```", "```py\nfeature_columns = []\n\n# numeric cols\nfor header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\nfeature_columns.append(feature_column.numeric_column(header))\n```", "```py\ntitanic_df.describe()\n```", "```py\nage = feature_column.numeric_column('age')\nage_buckets = feature_column.\nbucketized_column(age, boundaries=[23, 28, 35])\n```", "```py\nh = {}\nfor col in titanic_df:\n  if col in ['sex', 'class', 'deck', 'embark_town', 'alone']:\n    print(col, ':', titanic_df[col].unique())\n    h[col] = titanic_df[col].unique()\n```", "```py\nsex_type = feature_column.categorical_column_with_vocabulary_list(\n      'Type', ['male' 'female'])\nsex_type_one_hot = feature_column.indicator_column(sex_type)\n```", "```py\nsex_type = feature_column.\ncategorical_column_with_vocabulary_list(\n      'Type', h.get('sex').tolist())\nsex_type_one_hot = feature_column.\nindicator_column(sex_type)\n\nclass_type = feature_column.\ncategorical_column_with_vocabulary_list(\n      'Type', h.get('class').tolist())\nclass_type_one_hot = feature_column.\nindicator_column(class_type)\n\ndeck_type = feature_column.\ncategorical_column_with_vocabulary_list(\n      'Type', h.get('deck').tolist())\ndeck_type_one_hot = feature_column.\nindicator_column(deck_type)\n\nembark_town_type = feature_column.\ncategorical_column_with_vocabulary_list(\n      'Type', h.get('embark_town').tolist())\nembark_town_type_one_hot = feature_column.\nindicator_column(embark_town_type)\n\nalone_type = feature_column.\ncategorical_column_with_vocabulary_list(\n      'Type', h.get('alone').tolist())\nalone_one_hot = feature_column.\nindicator_column(alone_type)\n\n```", "```py\ndeck = feature_column.\ncategorical_column_with_vocabulary_list(\n      'deck', titanic_df.deck.unique())\ndeck_embedding = feature_column.\nembedding_column(deck, dimension=3)\n```", "```py\nclass_hashed = feature_column.categorical_column_with_hash_bucket(\n      'class', hash_bucket_size=4)\n```", "```py\n\ncross_type_feature = feature_column.\ncrossed_column(['sex', 'class'], hash_bucket_size=5)\n\n```", "```py\nfeature_columns = []\n\n```", "```py\n# append numeric columns\nfor header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n  feature_columns.append(feature_column.numeric_column(header))\n\n# append bucketized columns\nage = feature_column.numeric_column('age')\nage_buckets = feature_column.\nbucketized_column(age, boundaries=[23, 28, 35])\nfeature_columns.append(age_buckets)\n\n# append categorical columns\nindicator_column_names = \n['sex', 'class', 'deck', 'embark_town', 'alone']\nfor col_name in indicator_column_names:\n  categorical_column = feature_column.\n  categorical_column_with_vocabulary_list(\n      col_name, titanic_df[col_name].unique())\n  indicator_column = feature_column.\n\nindicator_column(categorical_column)\n  feature_columns.append(indicator_column)\n\n# append embedding columns\ndeck = feature_column.categorical_column_with_vocabulary_list(\n      'deck', titanic_df.deck.unique())\ndeck_embedding = feature_column.\nembedding_column(deck, dimension=3)\nfeature_columns.append(deck_embedding)\n\n# append crossed columns\nfeature_columns.\nappend(feature_column.indicator_column(cross_type_feature))\n\n```", "```py\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n```", "```py\nval_df, test_df = train_test_split(test_df, test_size=0.4)\n```", "```py\nbatch_size = 32\nlabels = train_df.pop('survived')\nworking_ds = tf.data.Dataset.\nfrom_tensor_slices((dict(train_df), labels))\nworking_ds = working_ds.shuffle(buffer_size=len(train_df))\ntrain_ds = working_ds.batch(batch_size)\n```", "```py\ndef pandas_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('survived')\n  ds = tf.data.Dataset.\nfrom_tensor_slices((dict(dataframe), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds\n```", "```py\nval_ds = pandas_to_dataset(val_df, shuffle=False, \nbatch_size=batch_size)\ntest_ds = pandas_to_dataset(test_df, shuffle=False, \nbatch_size=batch_size)\n```", "```py\nmodel = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(128, activation='relu'),\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(.1),\n  layers.Dense(1)\n])\n```", "```py\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(\nfrom_logits=True),\n              metrics=['accuracy'])\n```", "```py\nmodel.fit(train_ds,\n          validation_data=val_ds,\n          epochs=10)\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pylab as plt\nimport pathlib\n```", "```py\ndata_dir = tf.keras.utils.get_file(\n    'flower_photos',\n'https://storage.googleapis.com/download.tensorflow.org/\nexample_images/flower_photos.tgz',\n    untar=True)\n```", "```py\n!ls -lrt ~/.keras/datasets/flower_photos\n```", "```py\n!ls -lrt ~/.keras/datasets/flower_photos/roses | head -10\n```", "```py\ndef display_image_in_actual_size(im_path):\n\n    dpi = 100\n    im_data = plt.imread(im_path)\n    height, width, depth = im_data.shape\n    # What size does the figure need to be in inches to fit \n    # the image?\n    figsize = width / float(dpi), height / float(dpi)\n    # Create a figure of the right size with one axis that \n    # takes up the full figure\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_axes([0, 0, 1, 1])\n    # Hide spines, ticks, etc.\n    ax.axis('off')\n    # Display the image.\n    ax.imshow(im_data, cmap='gray')\n    plt.show()\n```", "```py\nIMAGE_PATH = \"/root/.keras/datasets/flower_photos/roses/\n7409458444_0bfc9a0682_n.jpg\"\ndisplay_image_in_actual_size(IMAGE_PATH)\n```", "```py\nIMAGE_PATH = \"/root/.keras/datasets/flower_photos/roses/\n5736328472_8f25e6f6e7.jpg\"\ndisplay_image_in_actual_size(IMAGE_PATH)\n```", "```py\nmy_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    **datagen_kwargs)\nmy_generator = my_datagen.flow_from_directory(\ndata_dir, **dataflow_kwargs)\n```", "```py\npixels =224\nBATCH_SIZE = 32\nIMAGE_SIZE = (pixels, pixels)\n\ndatagen_kwargs = dict(rescale=1./255, validation_split=.20)\ndataflow_kwargs = dict(target_size=IMAGE_SIZE, \nbatch_size=BATCH_SIZE,\ninterpolation=\"bilinear\")\n```", "```py\nvalid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n**datagen_kwargs)\n```", "```py\nvalid_generator = valid_datagen.flow_from_directory(\n    data_dir, subset=\"validation\", shuffle=False, \n    **dataflow_kwargs)\n```", "```py\ntrain_datagen = valid_datagen\ntrain_generator = train_datagen.flow_from_directory(\ndata_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)\n```", "```py\nfor image_batch, labels_batch in train_generator:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break\n\n(32, 224, 224, 3)\n(32, 5)\n```", "```py\nlabels_idx = (train_generator.class_indices)\nidx_labels = dict((v,k) for k,v in labels_idx.items())\nprint(idx_labels)\n\n{0: 'daisy', 1: 'dandelion', 2: 'roses', 3: 'sunflowers', \n4: 'tulips'}\n```", "```py\n(0.7, 0.1, 0.1, 0.05, 0.05)\n```", "```py\nimport pickle\nwith open('prediction_lookup.pickle', 'wb') as handle:\n    pickle.dump(idx_labels, handle, \n    protocol=pickle.HIGHEST_PROTOCOL)\n```", "```py\nwith open('prediction_lookup.pickle', 'rb') as handle:\n    lookup = pickle.load(handle)\n```", "```py\nimport tensorflow_hub as hub\nNUM_CLASSES = 5\nmdl = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n         hub.KerasLayer(\"https://tfhub.dev/google/imagenet/\nresnet_v1_101/feature_vector/4\", trainable=False),\ntf.keras.layers.Dense(NUM_CLASSES, activation='softmax', \n name = 'custom_class')\n])\nmdl.build([None, 224, 224, 3])\n```", "```py\nIMAGE_SIZE + (3,)\n```", "```py\nmdl.summary()\n```", "```py\nmdl.compile(\n  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9),\n  loss=tf.keras.losses.CategoricalCrossentropy(\nfrom_logits=True, \nlabel_smoothing=0.1),\n  metrics=['accuracy'])\n```", "```py\nsteps_per_epoch = train_generator.samples // \ntrain_generator.batch_size\nvalidation_steps = valid_generator.samples // \nvalid_generator.batch_size\nmdl.fit(\n    train_generator,\n    epochs=5, steps_per_epoch=steps_per_epoch,\n    validation_data=valid_generator,\n    validation_steps=validation_steps)\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\n\nFILE_URL = 'https://storage.googleapis.com/download.tensorflow.org/\ndata/shakespeare.txt'\nFILE_NAME = 'shakespeare.txt'\npath_to_file = tf.keras.utils.get_file('shakespeare.txt', FILE_URL)\n```", "```py\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\nprint ('Length of text: {} characters'.format(len(text)))\n```", "```py\nprint(text[:400])\n```", "```py\nvocabulary = sorted(set(text))\nprint ('There are {} unique characters'.format(len(vocabulary)))\n\nThere are 65 unique characters\n```", "```py\nvocabulary = sorted(set(text.lower()))\nprint ('There are {} unique characters'.format(len(vocabulary)))\n\nThere are 39 unique characters\n```", "```py\nvocabulary_word = sorted(set(text.lower().split(' ')))\nprint ('There are {} unique words'.format(len(vocabulary_word)))\n\nThere are 41623 unique words\n```", "```py\nfor i, u in enumerate(vocabulary):\n  print(i, u)\n```", "```py\nchar_to_index = {u:i for i, u in enumerate(vocabulary)}\n```", "```py\nindex_to_char = {i:u for i, u in enumerate(vocabulary)}\n```"]