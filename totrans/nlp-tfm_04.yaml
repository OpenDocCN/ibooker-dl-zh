- en: Chapter 3\. Transformer Anatomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml#chapter_classification), we saw what it takes to fine-tune
    and evaluate a transformer. Now let’s take a look at how they work under the hood.
    In this chapter we’ll explore the main building blocks of transformer models and
    how to implement them using PyTorch. We’ll also provide guidance on how to do
    the same in TensorFlow. We’ll first focus on building the attention mechanism,
    and then add the bits and pieces necessary to make a transformer encoder work.
    We’ll also have a brief look at the architectural differences between the encoder
    and decoder modules. By the end of this chapter you will be able to implement
    a simple transformer model yourself!
  prefs: []
  type: TYPE_NORMAL
- en: While a deep technical understanding of the Transformer architecture is generally
    not necessary to use ![nlpt_pin01](Images/nlpt_pin01.png) Transformers and fine-tune
    models for your use case, it can be helpful for comprehending and navigating the
    limitations of transformers and using them in new domains.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also introduces a taxonomy of transformers to help you understand
    the zoo of models that have emerged in recent years. Before diving into the code,
    let’s start with an overview of the original architecture that kick-started the
    transformer revolution.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 1](ch01.xhtml#chapter_introduction), the original Transformer
    is based on the *encoder-decoder* architecture that is widely used for tasks like
    machine translation, where a sequence of words is translated from one language
    to another. This architecture consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs: []
  type: TYPE_NORMAL
- en: Converts an input sequence of tokens into a sequence of embedding vectors, often
    called the *hidden state* or *context*
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs: []
  type: TYPE_NORMAL
- en: Uses the encoder’s hidden state to iteratively generate an output sequence of
    tokens, one token at a time
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 3-1](#transformer-encoder-decoder), the encoder and
    decoder are themselves composed of several building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![transformer-encoder-decoder](Images/nlpt_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Encoder-decoder architecture of the transformer, with the encoder
    shown in the upper half of the figure and the decoder in the lower half
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’ll look at each of the components in detail shortly, but we can already
    see a few things in [Figure 3-1](#transformer-encoder-decoder) that characterize
    the Transformer architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: The input text is tokenized and converted to *token embeddings* using the techniques
    we encountered in [Chapter 2](ch02.xhtml#chapter_classification). Since the attention
    mechanism is not aware of the relative positions of the tokens, we need a way
    to inject some information about token positions into the input to model the sequential
    nature of text. The token embeddings are thus combined with *positional embeddings*
    that contain positional information for each token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder is composed of a stack of *encoder layers* or “blocks,” which is
    analogous to stacking convolutional layers in computer vision. The same is true
    of the decoder, which has its own stack of *decoder layers*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder’s output is fed to each decoder layer, and the decoder then generates
    a prediction for the most probable next token in the sequence. The output of this
    step is then fed back into the decoder to generate the next token, and so on until
    a special end-of-sequence (EOS) token is reached. In the example from [Figure 3-1](#transformer-encoder-decoder),
    imagine the decoder has already predicted “Die” and “Zeit”. Now it gets these
    two as an input as well as all the encoder’s outputs to predict the next token,
    “fliegt”. In the next step the decoder gets “fliegt” as an additional input. We
    repeat the process until the decoder predicts the EOS token or we reached a maximum
    length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Transformer architecture was originally designed for sequence-to-sequence
    tasks like machine translation, but both the encoder and decoder blocks were soon
    adapted as standalone models. Although there are hundreds of different transformer
    models, most of them belong to one of three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-only
  prefs: []
  type: TYPE_NORMAL
- en: These models convert an input sequence of text into a rich numerical representation
    that is well suited for tasks like text classification or named entity recognition.
    BERT and its variants, like RoBERTa and DistilBERT, belong to this class of architectures.
    The representation computed for a given token in this architecture depends both
    on the left (before the token) and the right (after the token) contexts. This
    is often called *bidirectional attention*.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only
  prefs: []
  type: TYPE_NORMAL
- en: Given a prompt of text like “Thanks for lunch, I had a…” these models will auto-complete
    the sequence by iteratively predicting the most probable next word. The family
    of GPT models belong to this class. The representation computed for a given token
    in this architecture depends only on the left context. This is often called *causal*
    or *autoregressive attention*.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder
  prefs: []
  type: TYPE_NORMAL
- en: These are used for modeling complex mappings from one sequence of text to another;
    they’re suitable for machine translation and summarization tasks. In addition
    to the Transformer architecture, which as we’ve seen combines an encoder and a
    decoder, the BART and T5 models belong to this class.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In reality, the distinction between applications for decoder-only versus encoder-only
    architectures is a bit blurry. For example, decoder-only models like those in
    the GPT family can be primed for tasks like translation that are conventionally
    thought of as sequence-to-sequence tasks. Similarly, encoder-only models like
    BERT can be applied to summarization tasks that are usually associated with encoder-decoder
    or decoder-only models.^([1](ch03.xhtml#idm46238730148944))
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a high-level understanding of the Transformer architecture,
    let’s take a closer look at the inner workings of the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw earlier, the transformer’s encoder consists of many encoder layers
    stacked next to each other. As illustrated in [Figure 3-2](#encoder-zoom), each
    encoder layer receives a sequence of embeddings and feeds them through the following
    sublayers:'
  prefs: []
  type: TYPE_NORMAL
- en: A multi-head self-attention layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected feed-forward layer that is applied to each input embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output embeddings of each encoder layer have the same size as the inputs,
    and we’ll soon see that the main role of the encoder stack is to “update” the
    input embeddings to produce representations that encode some contextual information
    in the sequence. For example, the word “apple” will be updated to be more “company-like”
    and less “fruit-like” if the words “keynote” or “phone” are close to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![encoder-zoom](Images/nlpt_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Zooming into the encoder layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each of these sublayers also uses skip connections and layer normalization,
    which are standard tricks to train deep neural networks effectively. But to truly
    understand what makes a transformer work, we have to go deeper. Let’s start with
    the most important building block: the self-attention layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 1](ch01.xhtml#chapter_introduction), attention is
    a mechanism that allows neural networks to assign a different amount of weight
    or “attention” to each element in a sequence. For text sequences, the elements
    are *token embeddings* like the ones we encountered in [Chapter 2](ch02.xhtml#chapter_classification),
    where each token is mapped to a vector of some fixed dimension. For example, in
    BERT each token is represented as a 768-dimensional vector. The “self” part of
    self-attention refers to the fact that these weights are computed for all hidden
    states in the same set—for example, all the hidden states of the encoder. By contrast,
    the attention mechanism associated with recurrent models involves computing the
    relevance of each encoder hidden state to the decoder hidden state at a given
    decoding timestep.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea behind self-attention is that instead of using a fixed embedding
    for each token, we can use the whole sequence to compute a *weighted average*
    of each embedding. Another way to formulate this is to say that given a sequence
    of token embeddings <math alttext="x 1 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    , self-attention produces a sequence of new embeddings <math alttext="x prime
    1 comma ellipsis comma x prime Subscript n"><mrow><msubsup><mi>x</mi> <mn>1</mn>
    <mo>''</mo></msubsup> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msubsup><mi>x</mi> <mi>n</mi>
    <mo>''</mo></msubsup></mrow></math> where each <math alttext="x prime Subscript
    i"><msubsup><mi>x</mi> <mi>i</mi> <mo>''</mo></msubsup></math> is a linear combination
    of all the <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="x prime Subscript i Baseline equals sigma-summation Underscript
    j equals 1 Overscript n Endscripts w Subscript j i Baseline x Subscript j" display="block"><mrow><msubsup><mi>x</mi>
    <mi>i</mi> <mo>'</mo></msubsup> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub>
    <msub><mi>x</mi> <mi>j</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients <math alttext="w Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>
    are called *attention weights* and are normalized so that <math alttext="sigma-summation
    Underscript j Endscripts w Subscript j i Baseline equals 1"><mrow><msub><mo>∑</mo>
    <mi>j</mi></msub> <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub> <mo>=</mo>
    <mn>1</mn></mrow></math> . To see why averaging the token embeddings might be
    a good idea, consider what comes to mind when you see the word “flies”. You might
    think of annoying insects, but if you were given more context, like “time flies
    like an arrow”, then you would realize that “flies” refers to the verb instead.
    Similarly, we can create a representation for “flies” that incorporates this context
    by combining all the token embeddings in different proportions, perhaps by assigning
    a larger weight <math alttext="w Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>
    to the token embeddings for “time” and “arrow”. Embeddings that are generated
    in this way are called *contextualized embeddings* and predate the invention of
    transformers in language models like ELMo.^([2](ch03.xhtml#idm46238730081440))
    A diagram of the process is shown in [Figure 3-3](#contextualized-embeddings),
    where we illustrate how, depending on the context, two different representations
    for “flies” can be generated via self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![Contextualized embeddings](Images/nlpt_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Diagram showing how self-attention updates raw token embeddings
    (upper) into contextualized embeddings (lower) to create representations that
    incorporate information from the whole sequence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s now take a look at how we can calculate the attention weights.
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several ways to implement a self-attention layer, but the most common
    one is *scaled dot-product attention*, from the paper introducing the Transformer
    architecture.^([3](ch03.xhtml#idm46238730065680)) There are four main steps required
    to implement this mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: Project each token embedding into three vectors called *query*, *key*, and *value*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute attention scores. We determine how much the query and key vectors relate
    to each other using a *similarity function*. As the name suggests, the similarity
    function for scaled dot-product attention is the dot product, computed efficiently
    using matrix multiplication of the embeddings. Queries and keys that are similar
    will have a large dot product, while those that don’t share much in common will
    have little to no overlap. The outputs from this step are called the *attention
    scores*, and for a sequence with *n* input tokens there is a corresponding <math
    alttext="n times n"><mrow><mi>n</mi> <mo>×</mo> <mi>n</mi></mrow></math> matrix
    of attention scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute attention weights. Dot products can in general produce arbitrarily large
    numbers, which can destabilize the training process. To handle this, the attention
    scores are first multiplied by a scaling factor to normalize their variance and
    then normalized with a softmax to ensure all the column values sum to 1\. The
    resulting *n* × *n* matrix now contains all the attention weights, <math alttext="w
    Subscript j i"><msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the token embeddings. Once the attention weights are computed, we multiply
    them by the value vector <math alttext="v 1 comma ellipsis comma v Subscript n
    Baseline"><mrow><msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>v</mi> <mi>n</mi></msub></mrow></math> to obtain an updated representation
    for embedding <math alttext="x prime Subscript i Baseline equals sigma-summation
    Underscript j Endscripts w Subscript j i Baseline v Subscript j"><mrow><msubsup><mi>x</mi>
    <mi>i</mi> <mo>'</mo></msubsup> <mo>=</mo> <msub><mo>∑</mo> <mi>j</mi></msub>
    <msub><mi>w</mi> <mrow><mi>j</mi><mi>i</mi></mrow></msub> <msub><mi>v</mi> <mi>j</mi></msub></mrow></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can visualize how the attention weights are calculated with a nifty library
    called [*BertViz* for Jupyter](https://oreil.ly/eQK3I). This library provides
    several functions that can be used to visualize different aspects of attention
    in transformer models. To visualize the attention weights, we can use the `neuron_view`
    module, which traces the computation of the weights to show how the query and
    key vectors are combined to produce the final weight. Since BertViz needs to tap
    into the attention layers of the model, we’ll instantiate our BERT checkpoint
    with the model class from BertViz and then use the `show()` function to generate
    the interactive visualization for a specific encoder layer and attention head.
    Note that you need to click the “+” on the left to activate the attention visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_03in01.png)'
  prefs: []
  type: TYPE_IMG
- en: From the visualization, we can see the values of the query and key vectors are
    represented as vertical bands, where the intensity of each band corresponds to
    the magnitude. The connecting lines are weighted according to the attention between
    the tokens, and we can see that the query vector for “flies” has the strongest
    overlap with the key vector for “arrow”.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at this process in more detail by implementing the diagram
    of operations to compute scaled dot-product attention, as shown in [Figure 3-4](#attention-ops).
  prefs: []
  type: TYPE_NORMAL
- en: '![Operations in scaled dot-product attention](Images/nlpt_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Operations in scaled dot-product attention
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will use PyTorch to implement the Transformer architecture in this chapter,
    but the steps in TensorFlow are analogous. We provide a mapping between the most
    important functions in the two frameworks in [Table 3-1](#tf-pt-table).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. PyTorch and TensorFlow (Keras) classes and methods used in this
    chapter
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch | TensorFlow (Keras) | Creates/implements |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Linear` | `keras.layers.Dense` | A dense neural network layer |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Module` | `keras.layers.Layer` | The building blocks of models |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Dropout` | `keras.layers.Dropout` | A dropout layer |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.LayerNorm` | `keras.layers.LayerNormalization` | Layer normalization
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.Embedding` | `keras.layers.Embedding` | An embedding layer |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.GELU` | `keras.activations.gelu` | The Gaussian Error Linear Unit activation
    function |'
  prefs: []
  type: TYPE_TB
- en: '| `nn.bmm` | `tf.matmul` | Batched matrix multiplication |'
  prefs: []
  type: TYPE_TB
- en: '| `model.forward` | `model.call` | The model’s forward pass |'
  prefs: []
  type: TYPE_TB
- en: 'The first thing we need to do is tokenize the text, so let’s use our tokenizer
    to extract the input IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in [Chapter 2](ch02.xhtml#chapter_classification), each token in
    the sentence has been mapped to a unique ID in the tokenizer’s vocabulary. To
    keep things simple, we’ve also excluded the `[CLS]` and `[SEP]` tokens by setting
    `add_special_tokens=False`. Next, we need to create some dense embeddings. *Dense*
    in this context means that each entry in the embeddings contains a nonzero value.
    In contrast, the one-hot encodings we saw in [Chapter 2](ch02.xhtml#chapter_classification)
    are *sparse*, since all entries except one are zero. In PyTorch, we can do this
    by using a `torch.nn.Embedding` layer that acts as a lookup table for each input
    ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we’ve used the `AutoConfig` class to load the *config.json* file associated
    with the `bert-base-uncased` checkpoint. In ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, every checkpoint is assigned a configuration file that specifies
    various hyperparameters like `vocab_size` and `hidden_size`, which in our example
    shows us that each input ID will be mapped to one of the 30,522 embedding vectors
    stored in `nn.Embedding`, each with a size of 768\. The `AutoConfig` class also
    stores additional metadata, such as the label names, which are used to format
    the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the token embeddings at this point are independent of their context.
    This means that homonyms (words that have the same spelling but different meaning),
    like “flies” in the previous example, have the same representation. The role of
    the subsequent attention layers will be to mix these token embeddings to disambiguate
    and inform the representation of each token with the content of its context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our lookup table, we can generate the embeddings by feeding
    in the input IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This has given us a tensor of shape `[batch_size, seq_len, hidden_dim]`, just
    like we saw in [Chapter 2](ch02.xhtml#chapter_classification). We’ll postpone
    the positional encodings, so the next step is to create the query, key, and value
    vectors and calculate the attention scores using the dot product as the similarity
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This has created a <math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math>
    matrix of attention scores per sample in the batch. We’ll see later that the query,
    key, and value vectors are generated by applying independent weight matrices <math
    alttext="upper W Subscript upper Q comma upper K comma upper V"><msub><mi>W</mi>
    <mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></msub></math>
    to the embeddings, but for now we’ve kept them equal for simplicity. In scaled
    dot-product attention, the dot products are scaled by the size of the embedding
    vectors so that we don’t get too many large numbers during training that can cause
    the softmax we will apply next to saturate.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `torch.bmm()` function performs a *batch matrix-matrix product* that simplifies
    the computation of the attention scores where the query and key vectors have the
    shape `[batch_size, seq_len,` `hidden_dim``]`. If we ignored the batch dimension
    we could calculate the dot product between each query and key vector by simply
    transposing the key tensor to have the shape `[hidden_dim, seq_len]` and then
    using the matrix product to collect all the dot products in a `[seq_len, seq_len]`
    matrix. Since we want to do this for all sequences in the batch independently,
    we use `torch.bmm()`, which takes two batches of matrices and multiplies each
    matrix from the first batch with the corresponding matrix in the second batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply the softmax now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to multiply the attention weights by the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it—we’ve gone through all the steps to implement a simplified form
    of self-attention! Notice that the whole process is just two matrix multiplications
    and a softmax, so you can think of “self-attention” as just a fancy form of averaging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s wrap these steps into a function that we can use later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our attention mechanism with equal query and key vectors will assign a very
    large score to identical words in the context, and in particular to the current
    word itself: the dot product of a query with itself is always 1\. But in practice,
    the meaning of a word will be better informed by complementary words in the context
    than by identical words—for example, the meaning of “flies” is better defined
    by incorporating information from “time” and “arrow” than by another mention of
    “flies”. How can we promote this behavior?'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s allow the model to create a different set of vectors for the query, key,
    and value of a token by using three different linear projections to project our
    initial token vector into three different spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-headed attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our simple example, we only used the embeddings “as is” to compute the attention
    scores and weights, but that’s far from the whole story. In practice, the self-attention
    layer applies three independent linear transformations to each embedding to generate
    the query, key, and value vectors. These transformations project the embeddings
    and each projection carries its own set of learnable parameters, which allows
    the self-attention layer to focus on different semantic aspects of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: It also turns out to be beneficial to have *multiple* sets of linear projections,
    each one representing a so-called *attention head*. The resulting *multi-head
    attention layer* is illustrated in [Figure 3-5](#multihead-attention). But why
    do we need more than one attention head? The reason is that the softmax of one
    head tends to focus on mostly one aspect of similarity. Having several heads allows
    the model to focus on several aspects at once. For instance, one head can focus
    on subject-verb interaction, whereas another finds nearby adjectives. Obviously
    we don’t handcraft these relations into the model, and they are fully learned
    from the data. If you are familiar with computer vision models you might see the
    resemblance to filters in convolutional neural networks, where one filter can
    be responsible for detecting faces and another one finds wheels of cars in images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-head attention](Images/nlpt_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Multi-head attention
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s implement this layer by first coding up a single attention head:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here we’ve initialized three independent linear layers that apply matrix multiplication
    to the embedding vectors to produce tensors of shape `[batch_size, seq_len, head_dim]`,
    where `head_dim` is the number of dimensions we are projecting into. Although
    `head_dim` does not have to be smaller than the number of embedding dimensions
    of the tokens (`embed_dim`), in practice it is chosen to be a multiple of `embed_dim`
    so that the computation across each head is constant. For example, BERT has 12
    attention heads, so the dimension of each head is <math alttext="768 slash 12
    equals 64"><mrow><mn>768</mn> <mo>/</mo> <mn>12</mn> <mo>=</mo> <mn>64</mn></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a single attention head, we can concatenate the outputs of
    each one to implement the full multi-head attention layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the concatenated output from the attention heads is also fed through
    a final linear layer to produce an output tensor of shape `[batch_size, seq_len,`
    `hidden_dim``]` that is suitable for the feed-forward network downstream. To confirm,
    let’s see if the multi-head attention layer produces the expected shape of our
    inputs. We pass the configuration we loaded earlier from the pretrained BERT model
    when initializing the `MultiHeadAttention` module. This ensures that we use the
    same settings as BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It works! To wrap up this section on attention, let’s use BertViz again to
    visualize the attention for two different uses of the word “flies”. Here we can
    use the `head_view()` function from BertViz by computing the attentions of a pretrained
    checkpoint and indicating where the sentence boundary lies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_03in02.png)'
  prefs: []
  type: TYPE_IMG
- en: This visualization shows the attention weights as lines connecting the token
    whose embedding is getting updated (left) with every word that is being attended
    to (right). The intensity of the lines indicates the strength of the attention
    weights, with dark lines representing values close to 1, and faint lines representing
    values close to 0.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the input consists of two sentences and the `[CLS]` and `[SEP]`
    tokens are the special tokens in BERT’s tokenizer that we encountered in [Chapter 2](ch02.xhtml#chapter_classification).
    One thing we can see from the visualization is that the attention weights are
    strongest between words that belong to the same sentence, which suggests BERT
    can tell that it should attend to words in the same sentence. However, for the
    word “flies” we can see that BERT has identified “arrow” as important in the first
    sentence and “fruit” and “banana” in the second. These attention weights allow
    the model to distinguish the use of “flies” as a verb or noun, depending on the
    context in which it occurs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve covered attention, let’s take a look at implementing the missing
    piece of the encoder layer: position-wise feed-forward networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The Feed-Forward Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The feed-forward sublayer in the encoder and decoder is just a simple two-layer
    fully connected neural network, but with a twist: instead of processing the whole
    sequence of embeddings as a single vector, it processes each embedding *independently*.
    For this reason, this layer is often referred to as a *position-wise feed-forward
    layer*. You may also see it referred to as a one-dimensional convolution with
    a kernel size of one, typically by people with a computer vision background (e.g.,
    the OpenAI GPT codebase uses this nomenclature). A rule of thumb from the literature
    is for the hidden size of the first layer to be four times the size of the embeddings,
    and a GELU activation function is most commonly used. This is where most of the
    capacity and memorization is hypothesized to happen, and it’s the part that is
    most often scaled when scaling up the models. We can implement this as a simple
    `nn.Module` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that a feed-forward layer such as `nn.Linear` is usually applied to a
    tensor of shape `(batch_size, input_dim)`, where it acts on each element of the
    batch dimension independently. This is actually true for any dimension except
    the last one, so when we pass a tensor of shape `(batch_size, seq_len, hidden_dim)`
    the layer is applied to all token embeddings of the batch and sequence independently,
    which is exactly what we want. Let’s test this by passing the attention outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We now have all the ingredients to create a fully fledged transformer encoder
    layer! The only decision left to make is where to place the skip connections and
    layer normalization. Let’s take a look at how this affects the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Layer Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the Transformer architecture makes use of *layer normalization*
    and *skip connections*. The former normalizes each input in the batch to have
    zero mean and unity variance. Skip connections pass a tensor to the next layer
    of the model without processing and add it to the processed tensor. When it comes
    to placing the layer normalization in the encoder or decoder layers of a transformer,
    there are two main choices adopted in the literature:'
  prefs: []
  type: TYPE_NORMAL
- en: Post layer normalization
  prefs: []
  type: TYPE_NORMAL
- en: This is the arrangement used in the Transformer paper; it places layer normalization
    in between the skip connections. This arrangement is tricky to train from scratch
    as the gradients can diverge. For this reason, you will often see a concept known
    as *learning rate warm-up*, where the learning rate is gradually increased from
    a small value to some maximum value during training.
  prefs: []
  type: TYPE_NORMAL
- en: Pre layer normalization
  prefs: []
  type: TYPE_NORMAL
- en: This is the most common arrangement found in the literature; it places layer
    normalization within the span of the skip connections. This tends to be much more
    stable during training, and it does not usually require any learning rate warm-up.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the two arrangements is illustrated in [Figure 3-6](#layer-norm).
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformer layer normalization](Images/nlpt_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Different arrangements of layer normalization in a transformer
    encoder layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’ll use the second arrangement, so we can simply stick together our building
    blocks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now test this with our input embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve now implemented our very first transformer encoder layer from scratch!
    However, there is a caveat with the way we set up the encoder layers: they are
    totally invariant to the position of the tokens. Since the multi-head attention
    layer is effectively a fancy weighted sum, the information on token position is
    lost.^([4](ch03.xhtml#idm46238726765328))'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there is an easy trick to incorporate positional information using
    positional embeddings. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Positional Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Positional embeddings are based on a simple, yet very effective idea: augment
    the token embeddings with a position-dependent pattern of values arranged in a
    vector. If the pattern is characteristic for each position, the attention heads
    and feed-forward layers in each stack can learn to incorporate positional information
    into their transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to achieve this, and one of the most popular approaches
    is to use a learnable pattern, especially when the pretraining dataset is sufficiently
    large. This works exactly the same way as the token embeddings, but using the
    position index instead of the token ID as input. With that approach, an efficient
    way of encoding the positions of tokens is learned during pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a custom `Embeddings` module that combines a token embedding layer
    that projects the `input_ids` to a dense hidden state together with the positional
    embedding that does the same for `position_ids`. The resulting embedding is simply
    the sum of both embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We see that the embedding layer now creates a single, dense embedding for each
    token.
  prefs: []
  type: TYPE_NORMAL
- en: 'While learnable position embeddings are easy to implement and widely used,
    there are some alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: Absolute positional representations
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models can use static patterns consisting of modulated sine and
    cosine signals to encode the positions of the tokens. This works especially well
    when there are not large volumes of data available.
  prefs: []
  type: TYPE_NORMAL
- en: Relative positional representations
  prefs: []
  type: TYPE_NORMAL
- en: Although absolute positions are important, one can argue that when computing
    an embedding, the surrounding tokens are most important. Relative positional representations
    follow that intuition and encode the relative positions between tokens. This cannot
    be set up by just introducing a new relative embedding layer at the beginning,
    since the relative embedding changes for each token depending on where from the
    sequence we are attending to it. Instead, the attention mechanism itself is modified
    with additional terms that take the relative position between tokens into account.
    Models such as DeBERTa use such representations.^([5](ch03.xhtml#idm46238726501696))
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put all of this together now by building the full transformer encoder
    combining the embeddings with the encoder layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the output shapes of the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we get a hidden state for each token in the batch. This output
    format makes the architecture very flexible, and we can easily adapt it for various
    applications such as predicting missing tokens in masked language modeling or
    predicting the start and end position of an answer in question answering. In the
    following section we’ll see how we can build a classifier like the one we used
    in [Chapter 2](ch02.xhtml#chapter_classification).
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Classification Head
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformer models are usually divided into a task-independent body and a task-specific
    head. We’ll encounter this pattern again in [Chapter 4](ch04.xhtml#chapter_ner)
    when we look at the design pattern of ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
    What we have built so far is the body, so if we wish to build a text classifier,
    we will need to attach a classification head to that body. We have a hidden state
    for each token, but we only need to make one prediction. There are several options
    to approach this. Traditionally, the first token in such models is used for the
    prediction and we can attach a dropout and a linear layer to make a classification
    prediction. The following class extends the existing encoder for sequence classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Before initializing the model we need to define how many classes we would like
    to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: That is exactly what we have been looking for. For each example in the batch
    we get the unnormalized logits for each class in the output. This corresponds
    to the BERT model that we used in [Chapter 2](ch02.xhtml#chapter_classification)
    to detect emotions in tweets.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our analysis of the encoder and how we can combine it with a
    task-specific head. Let’s now cast our attention (pun intended!) to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As illustrated in [Figure 3-7](#decoder-zoom), the main difference between
    the decoder and encoder is that the decoder has *two* attention sublayers:'
  prefs: []
  type: TYPE_NORMAL
- en: Masked multi-head self-attention layer
  prefs: []
  type: TYPE_NORMAL
- en: Ensures that the tokens we generate at each timestep are only based on the past
    outputs and the current token being predicted. Without this, the decoder could
    cheat during training by simply copying the target translations; masking the inputs
    ensures the task is not trivial.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder attention layer
  prefs: []
  type: TYPE_NORMAL
- en: Performs multi-head attention over the output key and value vectors of the encoder
    stack, with the intermediate representations of the decoder acting as the queries.^([6](ch03.xhtml#idm46238726113792))
    This way the encoder-decoder attention layer learns how to relate tokens from
    two different sequences, such as two different languages. The decoder has access
    to the encoder keys and values in each block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the modifications we need to make to include masking in
    our self-attention layer, and leave the implementation of the encoder-decoder
    attention layer as a homework problem. The trick with masked self-attention is
    to introduce a *mask matrix* with ones on the lower diagonal and zeros above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ve used PyTorch’s `tril()` function to create the lower triangular
    matrix. Once we have this mask matrix, we can prevent each attention head from
    peeking at future tokens by using `Tensor.masked_fill()` to replace all the zeros
    with negative infinity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Transformer decoder zoom](Images/nlpt_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Zooming into the transformer decoder layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By setting the upper values to negative infinity, we guarantee that the attention
    weights are all zero once we take the softmax over the scores because <math alttext="e
    Superscript negative normal infinity Baseline equals 0"><mrow><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>∞</mi></mrow></msup> <mo>=</mo> <mn>0</mn></mrow></math> (recall
    that softmax calculates the normalized exponential). We can easily include this
    masking behavior with a small change to our scaled dot-product attention function
    that we implemented earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: From here it is a simple matter to build up the decoder layer; we point the
    reader to the excellent implementation of [minGPT](https://oreil.ly/kwsOP) by
    Andrej Karpathy for details.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve given you a lot of technical information here, but now you should have
    a good understanding of how every piece of the Transformer architecture works.
    Before we move on to building models for tasks more advanced than text classification,
    let’s round out the chapter by stepping back a bit and looking at the landscape
    of different transformer models and how they relate to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Meet the Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you’ve seen in this chapter, there are three main architectures for transformer
    models: encoders, decoders, and encoder-decoders. The initial success of the early
    transformer models triggered a Cambrian explosion in model development as researchers
    built models on various datasets of different size and nature, used new pretraining
    objectives, and tweaked the architecture to further improve performance. Although
    the zoo of models is still growing fast, they can still be divided into these
    three categories.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll provide a brief overview of the most important transformer
    models in each class. Let’s start by taking a look at the transformer family tree.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Tree of Life
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over time, each of the three main architectures has undergone an evolution of
    its own. This is illustrated in [Figure 3-8](#family-tree), which shows a few
    of the most prominent models and their descendants.
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformer family tree](Images/nlpt_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. An overview of some of the most prominent transformer architectures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With over 50 different architectures included in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, this family tree by no means provides a complete overview of all
    the ones that exist: it simply highlights a few of the architectural milestones.
    We’ve covered the original Transformer architecture in depth in this chapter,
    so let’s take a closer look at some of the key descendants, starting with the
    encoder branch.'
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder Branch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first encoder-only model based on the Transformer architecture was BERT.
    At the time it was published, it outperformed all the state-of-the-art models
    on the popular GLUE benchmark,^([7](ch03.xhtml#idm46238725854208)) which measures
    natural language understanding (NLU) across several tasks of varying difficulty.
    Subsequently, the pretraining objective and the architecture of BERT have been
    adapted to further improve performance. Encoder-only models still dominate research
    and industry on NLU tasks such as text classification, named entity recognition,
    and question answering. Let’s have a brief look at the BERT model and its variants:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs: []
  type: TYPE_NORMAL
- en: BERT is pretrained with the two objectives of predicting masked tokens in texts
    and determining if one text passage is likely to follow another.^([8](ch03.xhtml#idm46238725844992))
    The former task is called *masked language modeling* (MLM) and the latter *next
    sentence prediction* (NSP).
  prefs: []
  type: TYPE_NORMAL
- en: DistilBERT
  prefs: []
  type: TYPE_NORMAL
- en: Although BERT delivers great results, it’s size can make it tricky to deploy
    in environments where low latencies are required. By using a technique known as
    knowledge distillation during pretraining, DistilBERT achieves 97% of BERT’s performance
    while using 40% less memory and being 60% faster.^([9](ch03.xhtml#idm46238725839424))
    You can find more details on knowledge distillation in [Chapter 8](ch08.xhtml#chapter_compression).
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa
  prefs: []
  type: TYPE_NORMAL
- en: A study following the release of BERT revealed that its performance can be further
    improved by modifying the pretraining scheme. RoBERTa is trained longer, on larger
    batches with more training data, and it drops the NSP task.^([10](ch03.xhtml#idm46238725833888))
    Together, these changes significantly improve its performance compared to the
    original BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: XLM
  prefs: []
  type: TYPE_NORMAL
- en: Several pretraining objectives for building multilingual models were explored
    in the work on the cross-lingual language model (XLM),^([11](ch03.xhtml#idm46238725828528))
    including the autoregressive language modeling from GPT-like models and MLM from
    BERT. In addition, the authors of the paper on XLM pretraining introduced *translation
    language modeling* (TLM), which is an extension of MLM to multiple language inputs.
    Experimenting with these pretraining tasks, they achieved state-of-the-art results
    on several multilingual NLU benchmarks as well as on translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: XLM-RoBERTa
  prefs: []
  type: TYPE_NORMAL
- en: Following the work of XLM and RoBERTa, the XLM-RoBERTa or XLM-R model takes
    multilingual pretraining one step further by massively upscaling the training
    data.^([12](ch03.xhtml#idm46238725820864)) Using the [Common Crawl corpus](https://commoncrawl.org),
    its developers created a dataset with 2.5 terabytes of text; they then trained
    an encoder with MLM on this dataset. Since the dataset only contains data without
    parallel texts (i.e., translations), the TLM objective of XLM was dropped. This
    approach beats XLM and multilingual BERT variants by a large margin, especially
    on low-resource languages.
  prefs: []
  type: TYPE_NORMAL
- en: ALBERT
  prefs: []
  type: TYPE_NORMAL
- en: 'The ALBERT model introduced three changes to make the encoder architecture
    more efficient.^([13](ch03.xhtml#idm46238725814016)) First, it decouples the token
    embedding dimension from the hidden dimension, thus allowing the embedding dimension
    to be small and thereby saving parameters, especially when the vocabulary gets
    large. Second, all layers share the same parameters, which decreases the number
    of effective parameters even further. Finally, the NSP objective is replaced with
    a sentence-ordering prediction: the model needs to predict whether or not the
    order of two consecutive sentences was swapped rather than predicting if they
    belong together at all. These changes make it possible to train even larger models
    with fewer parameters and reach superior performance on NLU tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: ELECTRA
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of the standard MLM pretraining objective is that at each training
    step only the representations of the masked tokens are updated, while the other
    input tokens are not. To address this issue, ELECTRA uses a two-model approach:^([14](ch03.xhtml#idm46238725808096))
    the first model (which is typically small) works like a standard masked language
    model and predicts masked tokens. The second model, called the *discriminator*,
    is then tasked to predict which of the tokens in the first model’s output were
    originally masked. Therefore, the discriminator needs to make a binary classification
    for every token, which makes training 30 times more efficient. For downstream
    tasks the discriminator is fine-tuned like a standard BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: DeBERTa
  prefs: []
  type: TYPE_NORMAL
- en: 'The DeBERTa model introduces two architectural changes.^([15](ch03.xhtml#idm46238725800480))
    First, each token is represented as two vectors: one for the content, the other
    for relative position. By disentangling the tokens’ content from their relative
    positions, the self-attention layers can better model the dependency of nearby
    token pairs. On the other hand, the absolute position of a word is also important,
    especially for decoding. For this reason, an absolute position embedding is added
    just before the softmax layer of the token decoding head. DeBERTa is the first
    model (as an ensemble) to beat the human baseline on the SuperGLUE benchmark,^([16](ch03.xhtml#idm46238725798384))
    a more difficult version of GLUE consisting of several subtasks used to measure
    NLU performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve highlighted some of the major encoder-only architectures, let’s
    take a look at the decoder-only models.
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder Branch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The progress on transformer decoder models has been spearheaded to a large
    extent by OpenAI. These models are exceptionally good at predicting the next word
    in a sequence and are thus mostly used for text generation tasks (see [Chapter 5](ch05.xhtml#chapter_generation)
    for more details). Their progress has been fueled by using larger datasets and
    scaling the language models to larger and larger sizes. Let’s have a look at the
    evolution of these fascinating generation models:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of GPT combined two key ideas in NLP:^([17](ch03.xhtml#idm46238725786352))
    the novel and efficient transformer decoder architecture, and transfer learning.
    In that setup, the model was pretrained by predicting the next word based on the
    previous ones. The model was trained on the BookCorpus and achieved great results
    on downstream tasks such as classification.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the success of the simple and scalable pretraining approach, the
    original model and training set were upscaled to produce GPT-2.^([18](ch03.xhtml#idm46238725779984))
    This model is able to produce long sequences of coherent text. Due to concerns
    about possible misuse, the model was released in a staged fashion, with smaller
    models being published first and the full model later.
  prefs: []
  type: TYPE_NORMAL
- en: CTRL
  prefs: []
  type: TYPE_NORMAL
- en: Models like GPT-2 can continue an input sequence (also called a *prompt*). However,
    the user has little control over the style of the generated sequence. The Conditional
    Transformer Language (CTRL) model addresses this issue by adding “control tokens”
    at the beginning of the sequence.^([19](ch03.xhtml#idm46238725774016)) These allow
    the style of the generated text to be controlled, which allows for diverse generation.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the success of scaling GPT up to GPT-2, a thorough analysis on the
    behavior of language models at different scales revealed that there are simple
    power laws that govern the relation between compute, dataset size, model size,
    and the performance of a language model.^([20](ch03.xhtml#idm46238725769248))
    Inspired by these insights, GPT-2 was upscaled by a factor of 100 to yield GPT-3,^([21](ch03.xhtml#idm46238725767728))
    with 175 billion parameters. Besides being able to generate impressively realistic
    text passages, the model also exhibits few-shot learning capabilities: with a
    few examples of a novel task such as translating text to code, the model is able
    to accomplish the task on new examples. OpenAI has not open-sourced this model,
    but provides an interface through the [OpenAI API](https://oreil.ly/SEGRW).'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-Neo/GPT-J-6B
  prefs: []
  type: TYPE_NORMAL
- en: GPT-Neo and GPT-J-6B are GPT-like models that were trained by [EleutherAI](https://eleuther.ai),
    a collective of researchers who aim to re-create and release GPT-3 scale models.^([22](ch03.xhtml#idm46238725759200))
    The current models are smaller variants of the full 175-billion-parameter model,
    with 1.3, 2.7, and 6 billion parameters, and are competitive with the smaller
    GPT-3 models OpenAI offers.
  prefs: []
  type: TYPE_NORMAL
- en: The final branch in the transformers tree of life is the encoder-decoder models.
    Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder-Decoder Branch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although it has become common to build models using a single encoder or decoder
    stack, there are several encoder-decoder variants of the Transformer architecture
    that have novel applications across both NLU and NLG domains:'
  prefs: []
  type: TYPE_NORMAL
- en: T5
  prefs: []
  type: TYPE_NORMAL
- en: The T5 model unifies all NLU and NLG tasks by converting them into text-to-text
    tasks.^([23](ch03.xhtml#idm46238725745824)) All tasks are framed as sequence-to-sequence
    tasks, where adopting an encoder-decoder architecture is natural. For text classification
    problems, for example, this means that the text is used as the encoder input and
    the decoder has to generate the label as normal text instead of a class. We will
    look at this in more detail in [Chapter 6](ch06.xhtml#chapter_summarization).
    The T5 architecture uses the original Transformer architecture. Using the large
    crawled C4 dataset, the model is pretrained with masked language modeling as well
    as the SuperGLUE tasks by translating all of them to text-to-text tasks. The largest
    model with 11 billion parameters yielded state-of-the-art results on several benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: BART
  prefs: []
  type: TYPE_NORMAL
- en: BART combines the pretraining procedures of BERT and GPT within the encoder-decoder
    architecture.^([24](ch03.xhtml#idm46238725739936)) The input sequences undergo
    one of several possible transformations, from simple masking to sentence permutation,
    token deletion, and document rotation. These modified inputs are passed through
    the encoder, and the decoder has to reconstruct the original texts. This makes
    the model more flexible as it is possible to use it for NLU as well as NLG tasks,
    and it achieves state-of-the-art-performance on both.
  prefs: []
  type: TYPE_NORMAL
- en: M2M-100
  prefs: []
  type: TYPE_NORMAL
- en: Conventionally a translation model is built for one language pair and translation
    direction. Naturally, this does not scale to many languages, and in addition there
    might be shared knowledge between language pairs that could be leveraged for translation
    between rare languages. M2M-100 is the first translation model that can translate
    between any of 100 languages.^([25](ch03.xhtml#idm46238725734576)) This allows
    for high-quality translations between rare and underrepresented languages. The
    model uses prefix tokens (similar to the special `[CLS]` token) to indicate the
    source and target language.
  prefs: []
  type: TYPE_NORMAL
- en: BigBird
  prefs: []
  type: TYPE_NORMAL
- en: One main limitation of transformer models is the maximum context size, due to
    the quadratic memory requirements of the attention mechanism. BigBird addresses
    this issue by using a sparse form of attention that scales linearly.^([26](ch03.xhtml#idm46238725727968))
    This allows for the drastic scaling of contexts from 512 tokens in most BERT models
    to 4,096 in BigBird. This is especially useful in cases where long dependencies
    need to be conserved, such as in text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained checkpoints of all models that we have seen in this section are available
    on the [Hugging Face Hub](https://oreil.ly/EIOrN) and can be fine-tuned to your
    use case with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, as described
    in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we started at the heart of the Transformer architecture with
    a deep dive into self-attention, and we subsequently added all the necessary parts
    to build a transformer encoder model. We added embedding layers for tokens and
    positional information, we built in a feed-forward layer to complement the attention
    heads, and finally we added a classification head to the model body to make predictions.
    We also had a look at the decoder side of the Transformer architecture, and concluded
    the chapter with an overview of the most important model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a better understanding of the underlying principles, let’s
    go beyond simple classification and build a multilingual named entity recognition
    model.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#idm46238730148944-marker)) Y. Liu and M. Lapata, [“Text Summarization
    with Pretrained Encoder”](https://arxiv.org/abs/1908.08345), (2019).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.xhtml#idm46238730081440-marker)) M.E. Peters et al., [“Deep Contextualized
    Word Representations”](https://arxiv.org/abs/1802.05365), (2017).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.xhtml#idm46238730065680-marker)) A. Vaswani et al., [“Attention Is
    All You Need”](https://arxiv.org/abs/1706.03762), (2017).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.xhtml#idm46238726765328-marker)) In fancier terminology, the self-attention
    and feed-forward layers are said to be *permutation equivariant*—if the input
    is permuted then the corresponding output of the layer is permuted in exactly
    the same way.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.xhtml#idm46238726501696-marker)) By combining the idea of absolute
    and relative positional representations, rotary position embeddings achieve excellent
    results on many tasks. GPT-Neo is an example of a model with rotary position embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch03.xhtml#idm46238726113792-marker)) Note that unlike the self-attention
    layer, the key and query vectors in encoder-decoder attention can have different
    lengths. This is because the encoder and decoder inputs will generally involve
    sequences of differing length. As a result, the matrix of attention scores in
    this layer is rectangular, not square.
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch03.xhtml#idm46238725854208-marker)) A. Wang et al., [“GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding”](https://arxiv.org/abs/1804.07461),
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch03.xhtml#idm46238725844992-marker)) J. Devlin et al., [“BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding”](https://arxiv.org/abs/1810.04805),
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch03.xhtml#idm46238725839424-marker)) V. Sanh et al., [“DistilBERT, a
    Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”](https://arxiv.org/abs/1910.01108),
    (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch03.xhtml#idm46238725833888-marker)) Y. Liu et al., [“RoBERTa: A Robustly
    Optimized BERT Pretraining Approach”](https://arxiv.org/abs/1907.11692), (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch03.xhtml#idm46238725828528-marker)) G. Lample, and A. Conneau, [“Cross-Lingual
    Language Model Pretraining”](https://arxiv.org/abs/1901.07291), (2019).
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch03.xhtml#idm46238725820864-marker)) A. Conneau et al., [“Unsupervised
    Cross-Lingual Representation Learning at Scale”](https://arxiv.org/abs/1911.02116),
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch03.xhtml#idm46238725814016-marker)) Z. Lan et al., [“ALBERT: A Lite
    BERT for Self-Supervised Learning of Language Representations”](https://arxiv.org/abs/1909.11942),
    (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch03.xhtml#idm46238725808096-marker)) K. Clark et al., [“ELECTRA: Pre-Training
    Text Encoders as Discriminators Rather Than Generators”](https://arxiv.org/abs/2003.10555),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([15](ch03.xhtml#idm46238725800480-marker)) P. He et al., [“DeBERTa: Decoding-Enhanced
    BERT with Disentangled Attention”](https://arxiv.org/abs/2006.03654), (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch03.xhtml#idm46238725798384-marker)) A. Wang et al., [“SuperGLUE: A
    Stickier Benchmark for General-Purpose Language Understanding Systems”](https://arxiv.org/abs/1905.00537),
    (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch03.xhtml#idm46238725786352-marker)) A. Radford et al., [“Improving
    Language Understanding by Generative Pre-Training”](https://openai.com/blog/language-unsupervised),
    OpenAI (2018).
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch03.xhtml#idm46238725779984-marker)) A. Radford et al., [“Language Models
    Are Unsupervised Multitask Learners”](https://openai.com/blog/better-language-models),
    OpenAI (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch03.xhtml#idm46238725774016-marker)) N.S. Keskar et al., [“CTRL: A
    Conditional Transformer Language Model for Controllable Generation”](https://arxiv.org/abs/1909.05858),
    (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch03.xhtml#idm46238725769248-marker)) J. Kaplan et al., [“Scaling Laws
    for Neural Language Models”](https://arxiv.org/abs/2001.08361), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch03.xhtml#idm46238725767728-marker)) T. Brown et al., [“Language Models
    Are Few-Shot Learners”](https://arxiv.org/abs/2005.14165), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([22](ch03.xhtml#idm46238725759200-marker)) S. Black et al., [“GPT-Neo: Large
    Scale Autoregressive Language Modeling with Mesh-TensorFlow”](https://doi.org/10.5281/zenodo.5297715),
    (2021); B. Wang and A. Komatsuzaki, [“GPT-J-6B: A 6 Billion Parameter Autoregressive
    Language Model”](https://github.com/kingoflolz/mesh-transformer-jax), (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch03.xhtml#idm46238725745824-marker)) C. Raffel et al., [“Exploring the
    Limits of Transfer Learning with a Unified Text-to-Text Transformer”](https://arxiv.org/abs/1910.10683),
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '^([24](ch03.xhtml#idm46238725739936-marker)) M. Lewis et al., [“BART: Denoising
    Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation,
    and Comprehension”](https://arxiv.org/abs/1910.13461), (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([25](ch03.xhtml#idm46238725734576-marker)) A. Fan et al., [“Beyond English-Centric
    Multilingual Machine Translation”](https://arxiv.org/abs/2010.11125), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([26](ch03.xhtml#idm46238725727968-marker)) M. Zaheer et al., [“Big Bird:
    Transformers for Longer Sequences”](https://arxiv.org/abs/2007.14062), (2020).'
  prefs: []
  type: TYPE_NORMAL
