["```py\n$ ssh username@0.0.0.0\n```", "```py\n$ sudo apt update && sudo apt upgrade -y\n```", "```py\n$ sudo apt install git-all python3-pip\n```", "```py\n$ git clone https://github.com/ggerganov/llama.cpp.git  \n$ cd llama.cpp\n```", "```py\n$ git checkout 306d34be7ad19e768975409fc80791a274ea0230\n```", "```py\n$ python3 -m venv .venv\n$ source .venv/bin/activate\n$ pip install -r requirements.txt\n```", "```py\n$ make\n```", "```py\n$ cmake .. -DLLAMA_NATIVE=OFF -DLLAMA_BUILD_SERVER=ON -DLLAMA_CURL=ON \n↪ --DLLAMA_CUBLAS=ON -DCUDAToolkit_ROOT=/usr/local/cuda \n↪ --DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \n↪ --DCMAKE_CUDA_ARCHITECTURES=75 -DLLAMA_FATAL_WARNINGS=OFF \n↪ --DLLAMA_ALL_WARNINGS=OFF -DCMAKE_BUILD_TYPE=Release\n```", "```py\n$ pip install -U huggingface_hub\n$ huggingface-cli download liuhaotian/llava-v1.6-mistral-7b --local-dir \n↪ ./models/llava --local-dir-use-symlinks False\n```", "```py\n$ python3 convert.py ./models/llava/ --skip-unknown\n```", "```py\n$ find -name './models/llava/model-0000*-of-00004.safetensors' -exec  \nrm {} \\;\n```", "```py\n$ ./quantize ./models/llava/ggml-model-f16.gguf ./models/llava/llava-\n↪ v1.6-mistral-7b-q4_k_m.gguf Q4_K_M\n```", "```py\n$ huggingface-cli download cjpais/llava-1.6-mistral-7b-gguf --local-dir \n↪ ./models/llava --local-dir-use-symlinks False --include *Q4_K_M*\n```", "```py\n$./server -m ./models/llava/llava-v1.6-mistral-7b-q4_k_m.gguf --host\n↪ $PI_IP_ADDRESS --api-key $API_KEY\n```", "```py\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"http://0.0.0.0:8080/v1\",  # replace with your pi's ip address\n    api_key=\"1234\",  # replace with your server's api key\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are Capybara, an AI assistant. Your top \"\n            \"priority is achieving user fulfillment via helping them with \"\n            \"their requests.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Building a website can be done in 10 simple steps:\",\n        },\n    ],\n)\n\nprint(completion.choices[0].message)\n```", "```py\n$ wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-\n↪ gguf/resolve/main/mmproj-model-f16.gguf\n$ mv mmproj-model-f16.gguf ./models/llava/mmproj.gguf\n```", "```py\n$./server -m ./models/llava/llava-v1.6-mistral-7b-q4_k_m.gguf --host \n↪ $PI_IP_ADDRESS --api-key $API_KEY --MMPROJ ./models/llava/mmproj.gguf\n```", "```py\nimport openai\n\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\n\ndef encode_image(image_path, max_image=512):\n    with Image.open(image_path) as img:\n        width, height = img.size\n        max_dim = max(width, height)\n        if max_dim > max_image:\n            scale_factor = max_image / max_dim\n            new_width = int(width * scale_factor)\n            new_height = int(height * scale_factor)\n            img = img.resize((new_width, new_height))\n\n        buffered = BytesIO()\n        img.save(buffered, format=\"PNG\")\n        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n        return img_str\n\nclient = openai.OpenAI(\n    base_url=\"http://0.0.0.0:1234/v1\",  \n    api_key=\"1234\",     #1\n)\nimage_file = \"myImage.jpg\"\nmax_size = 512  \nencoded_string = encode_image(image_file, max_size)      #2\n\ncompletion = client.chat.completions.with_raw_response.create(\n    model=\"gpt-4-vision-preview\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert at analyzing images with computer vision. In case of error,\\nmake a full report of the cause of: any issues in receiving, understanding, or describing images\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Building a website can be done in 10 simple steps:\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{encoded_string}\"\n                    },\n                },\n            ],\n        },\n    ],\n    max_tokens=500,\n)\n\nchat = completion.parse()\nprint(chat.choices[0].message.content)\n```", "```py\n!git clone https://github.com/ggerganov/llama.cpp && cd \n↪ llama.cpp && make -j LLAMA_CUBLAS=1\n```", "```py\nimport os\nos.environ[“HF_HUB_ENABLE_HF_TRANSFER”] = “1”\n!huggingface-cli download repo/model_name name_of_downloaded_\n↪ model --local-dir . --local-dir-use-symlinks False\n```", "```py\n!./server -m content/model/path --log-disable --port 1337\n```", "```py\nfrom .googlecolab.output import eval_js\nprint(eval_js(“google.colab.kernel.proxyPort(1337)”))\n```", "```py\n$ CMAKE_ARGS=”-DLLAMA_CUBLAS=on” FORCE_CMAKE=1 pip install llama-cpp-python\n```", "```py\nimport os\nfrom langchain.chains import LLMChain\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom datasets import load_dataset\nimport tiktoken\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your API Key\"\nos.environ[\n    \"OPENAI_API_BASE\"\n] = \"http://0.0.0.0:1234/v1\"     #1\nos.environ[\n    \"OPENAI_API_HOST\"\n] = \"http://0.0.0.0:1234\"      #2\n\nllm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",      #3\n    temperature=0.25,\n    openai_api_base=os.environ[\"OPENAI_API_BASE\"],     #4\n    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n    max_tokens=500,\n    n=1,\n)\n\nembedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")   #5\ntiktoker = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")      #6\n\nprompt_template = \"\"\"Below is an instruction      #7\nthat describes a task, \npaired with an input that provides further context. \nWrite a response that appropriately completes the request.\n    ###Instruction:\n    You are an expert python developer.\n    Given a question, some conversation history, \nand the closest code snippet we could find for \nthe request, give your best suggestion for how \nto write the code needed to answer the User's question.\n\n    ###Input:\n\n    #Question: {question}\n\n    #Conversation History: {conversation_history}\n\n    Code Snippet:\n    {code_snippet}\n\n    ###Response:\n    \"\"\"\n\nvectorDB = load_dataset(     #8\n    \"csv\", data_files=\"your dataset with embeddings.csv\", split=\"train\"\n)\ntry:                                                           #9\n    vectorDB.load_faiss_index(\"embeddings\", \"my_index.faiss\")\nexcept:\n    print(\n    \"\"\"No faiss index, run vectorDB.add_faiss_index(column='embeddings')\n    and vectorDB.save_faiss_index('embeddings', 'my_index.faiss')\"\"\"\n    )\n\nmessage_history = []      #10\n\nquery = \"How can I train an LLM from scratch?\"      #11\nembedded = embedder.encode(query)\nq = np.array(embedded, dtype=np.float32)\n_, retrieved_example = vectorDB.get_nearest_examples(\"embeddings\", q, k=1)\n\nformatted_prompt = PromptTemplate(      #12\n    input_variables=[\"question\", \"conversation_history\", \"code_snippet\"],\n    template=prompt_template,\n)\nchain = LLMChain(llm=llm, prompt=formatted_prompt)    #13\n\nnum_tokens = len(           #14\n    tiktoker.encode(f\"{prompt_template},\\n\" + \"\\n\".join(message_history) +\n↪ query)\n    )\n)\nwhile num_tokens >= 4000:\n    message_history.pop(0)\n    num_tokens = len(\n    tiktoker.encode(f\"{prompt_template},\\n\" + \"\\n\".join(message_history) +\n↪ query)\n        )\n    )\nres = chain.run(       #15\n    {\n    \"question\": query,\n    \"conversation_history\": message_history,\n    \"code_snippet\": \"\",\n    }\n)\nmessage_history.append(f\"User: {query}\\nLlama: {res}\")\n\nprint(res)      #16\n```"]