<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">7 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/>Image generation with variational autoencoders</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-142"/><a id="idIndexMarker002"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Autoencoders vs. variational autoencoders<a class="calibre" id="idIndexMarker003"/><a class="calibre" id="idIndexMarker004"/></li>
<li class="co-summary-bullet">Building and training an Autoencoder toreconstruct handwritten digits</li>
<li class="co-summary-bullet">Building and training a variational autoencoder to generate human face images</li>
<li class="co-summary-bullet">Performing encoding arithmetic and interpolation with a trained variational autoencoder</li>
</ul>
<p class="body">So far, you have learned how to generate shapes, numbers, and images, all by using generative adversarial networks (GANs). In this chapter, you’ll learn to create images by using another generative model: variational autoencoders (VAEs). You’ll also learn the practical uses of VAEs by performing encoding arithmetic and encoding interpolation.</p>
<p class="body">To know how VAEs work, we first need to understand autoencoders (AEs). AEs have a dual-component structure: an encoder and a decoder. The encoder compresses the data into an abstract representation in a lower-dimensional space (the latent space), and the decoder decompresses the encoded information and reconstructs the data. The primary goal of an AE is to learn a compressed representation of the input data, focusing on minimizing the reconstruction <a id="idTextAnchor007"/>error—the difference between the original input and its reconstruction (at the pixel level, as we have seen in chapter 6 when calculating cycle consistency loss). The encoder-decoder architecture is a cornerstone in various generative models, including Transformers, which you’ll explore in detail in the latter half of this book. For example, in chapter 9, you’ll build a Transformer for machine language translation: the encoder converts an English phrase into an abstract representation while the decoder constructs the French translation based on the compressed representation generated by the encoder. Text-to-image Transformers like DALL-E 2 and Imagen also utilize an AE architecture in their design. This involves first encoding an image into a compact, low-dimensional probability distribution. Then, they decode from this distribution. Of course, what constitutes an encoder and a decoder is different in different models.<a id="marker-143"/><a id="idIndexMarker005"/></p>
<p class="body">Your first project in this chapter involves constructing and training an AE from scratch to generate handwritten digits. You’ll use 60,000 grayscale images of handwritten digits (0 to 9), each with a size of 28 <span class="cambria">×</span> 28 = 784 pixels, as the training data. The encoder in the AE compresses each image into a deterministic vector representation with only 20 values. The decoder in the AE reconstructs the image with the aim of minimizing the difference between the original image and the reconstructed image. This is achieved by minimizing the mean absolute error between the two images at the pixel level. The end result is an AE capable of generating handwritten digits almost identical to those in the training set.</p>
<p class="body">While AEs are good at replicating the input data, they often falter in generating new samples that are not present in the training set. More importantly, AEs are not good at input interpolation: they often fail to generate intermediate representations between two input data points. This leads us to VAEs. VAEs differ from AEs in two critical ways. First, while an AE encodes each input into a specific point in the latent space, a VAE encodes it into a probability distribution within this space. Second, an AE focuses solely on minimizing the reconstruction error, whereas a VAE learns the parameters of the probability distribution for latent variables, minimizing a loss function that includes both reconstruction loss and a regularization term, the Ku<a id="idTextAnchor008"/>llback–Liebler (KL) divergence. <a id="idIndexMarker006"/></p>
<p class="body">The KL-divergence encourages the latent space to approximate a certain distribution (a normal distribution in our example) and ensures that the latent variables don’t just memorize the training data but rather capture the underlying distribution. It helps in achieving a well-structured latent space where similar data points are mapped closely together, making the space continuous and interpretable. As a result, we can manipulate the encodings to achieve new outcomes, which makes encoding arithmetic and input interpolation possible in VAEs.<a id="idIndexMarker007"/></p>
<p class="body">In the second project in this chapter, you’ll build and train a VAE from the ground up to generate human face images. Here, your training set comprises eyeglasses images that you downloaded in chapter 5. The VAE’s encoder compresses an image of size <span class="times">3 <span class="cambria">×</span> 256 <span class="cambria">×</span> 256 = 196,608 pixels</span> into a 100-value probabilistic vector, each following a normal distribution. The decoder then reconstructs the image based on this probabilistic vector. The trained VAE can not only replicate human faces from the training set but also generate novel ones.</p>
<p class="body">You’ll learn how to conduct encoding arithmetic and input interpolation in VAEs. You’ll manipulate the encoded representations (latent vectors) of different inputs to achieve specific outcomes (i.e., with or without certain characteristics in images) when decoded. The latent vectors control different characteristics in the decoded images such as gender, whether there are eyeglasses in an image, and so on. For example, you can first obtain the latent vectors for men with glasses (<span class="times">z1</span>), women with glasses (<span class="times">z2</span>), and women without glasses (<span class="times">z3</span>). You then calculate a new latent vector, <span class="times">z4 = z1 – z2 + z3</span>. Since both <span class="times">z1</span> and <span class="times">z2</span> lead to eyeglasses in images when decoded, <span class="times">z1 – z2</span> cancels out the eyeglasses feature in the resulting image. Similarly, since both <span class="times">z2</span> and <span class="times">z3</span> lead to a female face, <span class="times">z3 – z2</span> cancels out the female feature in the resulting image. Therefore, if you decode <span class="times">z4 = z1 – z2 + z3</span> with the trained VAE, you’ll get an image of a man without glasses.</p>
<p class="body">You’ll also create a series of images transitioning from a woman with glasses to a woman without glasses by varying the weight assigned to the latent vectors <span class="times">z1</span> and <span class="times">z2</span>. These exercises exemplify the versatility and creative potential of VAEs in the field of generative models.</p>
<p class="body">Compared to GANs, which we studied in the last few chapters, AEs and VAEs have a simple architecture and are easy to construct. Further, AEs and VAEs are generally easier and more stable to train relative to GANs. However, images generated by AEs and VAEs tend to be blurrier compared to those generated by GANs. GANs excel in generating high-quality, realistic images but suffer from training difficulties and resource intensiveness. The choice between GANs and VAEs largely depends on the specific requirements of the task at hand, including the desired quality of the output, computational resources available, and the importance of having a stable training process.<a id="marker-144"/><a id="idIndexMarker008"/></p>
<p class="body">VAEs have a wide range of practical applications in the real world. Consider, for instance, that you run an eyewear store and have successfully marketed a new style of men’s glasses online. Now, you wish to target the female market with the same style but lack images of women wearing these glasses, and you face high costs for a professional photo shoot. Here’s where VAEs come into play: you can combine existing images of men wearing the glasses with pictures of both men and women without glasses. This way, you can create realistic images of women sporting the same eyewear style, as illustrated in figure 7.1, through encoding arithmetic, a technique you’ll learn in this chapter. <a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="199" src="../../OEBPS/Images/CH07_F01_Liu.png" width="675"/></p>
<p class="figurecaption">Figure 7.1 Generating images of women with glasses by performing encoding arithmetic</p>
</div>
<p class="body">In another scenario, suppose your store offers eyeglasses with dark and light frames, both of which are popular. You want to introduce a middle option with frames of an intermediate shade. With VAEs, through a method called encoding interpolation, you can effortlessly generate a smooth transition series of images, as shown in figure 7.2. These images would vary from dark to light-framed glasses, offering customers a visual spectrum of choices.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="146" src="../../OEBPS/Images/CH07_F02_Liu.png" width="728"/></p>
<p class="figurecaption">Figure 7.2 Generating a series of images that transition from glasses with dark frames to those with light frames</p>
</div>
<p class="body">The use of VAEs is not limited to eyeglasses; it extends to virtually any product category, be it clothing, furniture, or food. The technology provides a creative and cost-effective solution for visualizing and marketing a wide range of products. Furthermore, although image generation is a prominent example, VAEs can be applied to many other types of data, including music and text. Their versatility opens up endless possibilities in terms of practical use!</p>
<h2 class="fm-head" id="heading_id_3">7.1 An overview of AEs</h2>
<p class="body"><a id="marker-145"/>This section discusses what an AE is and its basic structure. For you to have a deep understanding of the inner workings of AEs, you’ll build and train an AE to generate handwritten digits as your first project in this chapter. This section provides an overview of an AE’s architecture and a blueprint for completing the first project. <a id="idIndexMarker012"/></p>
<h3 class="fm-head1" id="heading_id_4">7.1.1 What is an AE?</h3>
<p class="body">AEs are a type of neural network used in unsupervised learning that are particularly effective for tasks like image generation, compression, and denoising. An AE consists of two main parts: an encoder and a decoder. The encoder compresses the input into a lower-dimensional representation (latent space), and the decoder reconstructs the input from this representation.<a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>
<p class="body">The compressed representation, or latent space, captures the most important features of the input data. In image generation, this space encodes crucial aspects of the images that the network has been trained on. AEs are useful for their efficiency in learning data representations and their ability to work with unlabeled data, making them suitable for tasks like dimensionality reduction and feature learning. One challenge with AEs is the risk of losing information in the encoding process, which can lead to less accurate reconstructions. Using deeper architectures with multiple hidden layers can help in learning more complex and abstract representations, potentially mitigating information loss in AEs. Also, training AEs to generate high-quality images can be computationally intensive and requires large datasets.</p>
<p class="body">As we mentioned in chapter 1, the best way to learn something is to create it from scratch. To that end, you’ll learn to create an AE to generate handwritten digits in the first project in this chapter. The next subsection provides a blueprint for how to do that.</p>
<h3 class="fm-head1" id="heading_id_5">7.1.2 Steps in building and training an AE</h3>
<p class="body"><a id="marker-146"/>Imagine that you must build and train an AE from the ground up to generate grayscale images of handwritten digits so that you acquire the skills needed to use AEs for more complicated tasks such as color image generation or dimensionality reduction. How should you go about this task?<a id="idIndexMarker015"/><a id="idIndexMarker016"/></p>
<p class="body">Figure 7.3 provides a diagram of the architecture of an AE and the steps involved in training an AE to generate handwritten digits.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="360" src="../../OEBPS/Images/CH07_F03_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 7.3 The architecture of an AE and the steps to train one to generate handwritten digits. An AE consists of an encoder (middle left) and a decoder (middle right). In each iteration of training, images of handwritten digits are fed to the encoder (step 1). The encoder compresses the images to deterministic points in the latent space (step 2). The decoder takes the encoded vectors (step 3) from the latent space and reconstructs the images (step 4). The AE adjusts its parameters to minimize the reconstruction loss, the difference between the originals and the reconstructions (step 5).</p>
</div>
<p class="body">As you can see from the figure, the AE has two main parts: an encoder (middle left) that compresses images of handwritten digits into vectors in the latent space and a decoder (middle right) that reconstructs these images based on the encoded vectors. Both the encoder and decoder are deep neural networks that can potentially include different types of layers such as dense layers, convolutional layers, transposed convolutional layers, and so on. Since our example involves grayscale images of handwritten digits, we’ll use only dense layers. However, AEs can also be used to generate higher-resolution color images; for those tasks, convolutional neural networks (CNNs) are usually included in encoders and decoders. Whether to use CNNs in AEs depends on the resolution of the images you want to generate. <a id="idIndexMarker017"/></p>
<p class="body">When an AE is built, the parameters in it are randomly initialized. We need to obtain a training set to train the model: PyTorch provides 60,000 grayscale images of handwritten digits, evenly distributed among the 10 digits 0 to 9. The left side of figure 7.3 shows three examples, and they are images of digits 0, 1, and 9, respectively. In the first step in the training loop, we feed images in the training set to the encoder. The encoder compresses the images to 20-value vectors in the latent space (step 2). There is nothing magical about the number 20. If you use 25-value vectors in the latent space, you’ll get similar results. We then feed the vector representations to the decoder (step 3) and ask it to reconstruct the images (step 4). We calculate the reconstruct loss, which is the mean squared error, over all the pixels, between the original image and the reconstructed image. We then propagate this loss back through the network to update the parameters in the encoder and decoder to minimize the reconstruction loss (step 5) so that in the next iteration, the AE can reconstruct images closer to the original ones. This process is repeated for many epochs over the dataset.</p>
<p class="body">After the model is trained, you’ll feed unseen images of handwritten digits to the encoder and obtain encodings. You then feed the encodings to the decoder to obtain reconstructed images. You’ll notice that the reconstructed images look almost identical to the originals. The right side of figure 7.3 shows three examples of reconstructed images: they do look similar to the corresponding originals on the left side of the figure.<a id="marker-147"/><a id="idIndexMarker018"/></p>
<h2 class="fm-head" id="heading_id_6">7.2 Building and training an AE to generate digits</h2>
<p class="body">Now that you have a blueprint to build and train an AE to generate handwritten digits, let’s dive into the project and implement the steps outlined in the last section.</p>
<p class="body">Specifically, in this section, you’ll learn first how to obtain a training set and a test set of images of handwritten digits. You’ll then build an encoder and decoder with dense layers. You’ll train the AE with the training dataset and use the trained encoder to encode images in the test set. Finally, you’ll learn to use the trained decoder to reconstruct images and compare them to the originals.</p>
<h3 class="fm-head1" id="heading_id_7">7.2.1 Gathering handwritten digits</h3>
<p class="body">You can download grayscale images of handwritten images using the <i class="fm-italics">datasets</i> package in the Torchvision library, similar to how you downloaded images of clothing items in chapter 2. <a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>
<p class="body">First, let’s download a training set and a test set:</p>
<pre class="programlisting">import torchvision
import torchvision.transforms as T
  
transform=T.Compose([
    T.ToTensor()])
train_set=torchvision.datasets.MNIST(root=".",        <span class="fm-combinumeral">①</span>
    train=True,download=True,transform=transform)     <span class="fm-combinumeral">②</span>
test_set=torchvision.datasets.MNIST(root=".",
    train=False,download=True,transform=transform)    <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Downloads handwritten digits by using the MNIST() class in torchvision.datasets</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The train=True argument means you download the training set.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The train=False argument means you download the test set.</p>
<p class="body">Instead of using the <code class="fm-code-in-text">FashionMNIST()</code> class as we did in chapter 2, we use the <code class="fm-code-in-text">MNIST()</code> class here. The <code class="fm-code-in-text">train</code> argument in the class tells PyTorch whether to download the training set (when the argument is set to <code class="fm-code-in-text">True</code>) or the test set (when the argument is set to <code class="fm-code-in-text">False</code>). Before transformation, the image pixels are integers ranging from 0 to 255. The <code class="fm-code-in-text">ToTensor()</code> class in the preceding code block converts them to PyTorch float tensors with values between 0 to 1. There are 60,000 images in the training set and 10,000 in the test set, evenly distributed among 10 digits, 0 to 9, in each set. <a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>
<p class="body">We’ll create batches of data for training and testing, with 32 images in each batch:</p>
<pre class="programlisting">import torch
  
batch_size=32
train_loader=torch.utils.data.DataLoader(
    train_set,batch_size=batch_size,shuffle=True)
test_loader=torch.utils.data.DataLoader(
    test_set,batch_size=batch_size,shuffle=True)</pre>
<p class="body">Now that we have the data ready, we’ll build and train an AE next.</p>
<h3 class="fm-head1" id="heading_id_8">7.2.2 Building and training an AE</h3>
<p class="body"><a id="marker-148"/>An AE consists of two parts: the encoder and the decoder. We’ll define an <code class="fm-code-in-text">AE()</code> class, as shown in the following listing, to represent the AE.<a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<p class="fm-code-listing-caption">Listing 7.1 Creating an AE to generate handwritten digits</p>
<pre class="programlisting">import torch.nn.functional as F
from torch import nn
  
device="cuda" if torch.cuda.is_available() else "cpu"
input_dim = 784                                       <span class="fm-combinumeral">①</span>
z_dim = 20                                            <span class="fm-combinumeral">②</span>
h_dim = 200
class AE(nn.Module):
    def __init__(self,input_dim,z_dim,h_dim):
        super().__init__()
        self.common = nn.Linear(input_dim, h_dim)
        self.encoded = nn.Linear(h_dim, z_dim)
        self.l1 = nn.Linear(z_dim, h_dim)
        self.decode = nn.Linear(h_dim, input_dim)
    def encoder(self, x):                             <span class="fm-combinumeral">③</span>
        common = F.relu(self.common(x))
        mu = self.encoded(common)
        return mu
    def decoder(self, z):                             <span class="fm-combinumeral">④</span>
        out=F.relu(self.l1(z))
        out=torch.sigmoid(self.decode(out))
        return out
    def forward(self, x):                             <span class="fm-combinumeral">⑤</span>
        mu=self.encoder(x)
        out=self.decoder(mu)
        return out, mu</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The input to the AE has 28 <span class="cambria">×</span> 28 = 784 values in it.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The latent variable (encoding) has 20 values in it.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The encoder compresses images to latent variables.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The decoder reconstructs the images based on encodings.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The encoder and decoder form the AE.</p>
<p class="body">The input size is 784 because the grayscale images of handwritten digits have a size of 28 by 28 pixels. We flatten the images to 1D tensors and feed them to the AE. The images first go through the encoder: they are compressed into encodings in a lower dimensional space. Each image is now represented by a 20-value latent variable. The decoder reconstructs the images based on the latent variables. The output from the AE has two tensors: <code class="fm-code-in-text">out</code>, the reconstructed images, and <code class="fm-code-in-text">mu</code>, latent variables (i.e., encodings).</p>
<p class="body">Next, we instantiate the <code class="fm-code-in-text">AE()</code> class we defined earlier to create an AE. We also use the Adam optimizer during training, as we did in previous chapters: <a id="idIndexMarker031"/></p>
<pre class="programlisting">model = AE(input_dim,z_dim,h_dim).to(device)
lr=0.00025
optimizer = torch.optim.Adam(model.parameters(), lr=lr)</pre>
<p class="body">We define a function <code class="fm-code-in-text">plot_digits()</code> to visually inspect the reconstructed handwritten digits after each epoch of training, as shown in the following listing.<a id="idIndexMarker032"/><a id="marker-149"/></p>
<p class="fm-code-listing-caption">Listing 7.2 The <code class="fm-code-in-text">plot_digits</code>() function to inspect reconstructed images</p>
<pre class="programlisting">import matplotlib.pyplot as plt
  
originals = []                                           <span class="fm-combinumeral">①</span>
idx = 0
for img,label in test_set:
    if label == idx:
        originals.append(img)
        idx += 1
    if idx == 10:
        break
def plot_digits():
    reconstructed=[]
    for idx in range(10):
        with torch.no_grad():
            img = originals[idx].reshape((1,input_dim))
            out,mu = model(img.to(device))               <span class="fm-combinumeral">②</span>
        reconstructed.append(out)                        <span class="fm-combinumeral">③</span>
    imgs=originals+reconstructed
    plt.figure(figsize=(10,2),dpi=50)
    for i in range(20):
        ax = plt.subplot(2,10, i + 1)
        img=(imgs[i]).detach().cpu().numpy()
        plt.imshow(img.reshape(28,28),                   <span class="fm-combinumeral">④</span>
                   cmap="binary")
        plt.xticks([])
        plt.yticks([])
    plt.show()  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Collects a sample image of each digit in the test set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Feeds the image to the AE to obtain a reconstructed image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Collects the reconstructed image of each original image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Compares the originals to the reconstructed digits visually</p>
<p class="body">We first collect 10 sample images, one representing a different digit, and place them in a list, <code class="fm-code-in-text">originals</code>. We feed the images to the AE to obtain the reconstructed images. Finally, we plot both the originals and the reconstructed images so that we can compare them and assess the performance of the AE periodically.<a id="idIndexMarker033"/></p>
<p class="body">Before training starts, we call the function <code class="fm-code-in-text">plot_digits()</code> to visualize the output:</p>
<pre class="programlisting">plot_digits()</pre>
<p class="body">You’ll see the output as shown in figure 7.4.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="95" src="../../OEBPS/Images/CH07_F04_Liu.png" width="450"/></p>
<p class="figurecaption">Figure 7.4 Comparing reconstructed images by the AE with the originals before training starts. The top row shows 10 original images of handwritten digits in the test set. The bottom row shows the reconstructed images by the AE before training. The reconstructions are nothing more than pure noise.</p>
</div>
<p class="body"><a id="marker-150"/>Though we could divide our data into training and validation sets and train the model until no further improvements are seen on the validation set (as we have done in chapter 2), our primary aim here is to grasp how AEs work, not necessarily to achieve the best parameter tuning. Therefore, we’ll train the AE for 10 epochs.<a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/></p>
<p class="fm-code-listing-caption">Listing 7.3 Training the AE to generate handwritten digits</p>
<pre class="programlisting">for epoch in range(10):
    tloss=0
    for imgs, labels in train_loader:                   <span class="fm-combinumeral">①</span>
        imgs=imgs.to(device).view(-1, input_dim)
        out, mu=model(imgs)                             <span class="fm-combinumeral">②</span>
        loss=((out-imgs)**2).sum()                      <span class="fm-combinumeral">③</span>
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        tloss+=loss.item()
    print(f"at epoch {epoch} toal loss = {tloss/len(train_loader)}")
    plot_digits()                                       <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through batches in the training set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the AE to reconstruct images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates reconstruct loss as measured by mean squared error</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Visually inspects the performance of the AE</p>
<p class="body">In each epoch of training, we iterate through all batches of data in the training set. We feed the original images to the AE to obtain the reconstructed images. We then calculate the reconstruction loss, which is the mean squared error between the original images and the reconstructed images. Specifically, the reconstruction loss is obtained by first calculating the difference between the two images, pixel by pixel, squaring the values and averaging the squared difference. We adjust the model parameters to minimize the reconstruction loss, utilizing the Adam optimizer, which is a variation of the gradient descent method.</p>
<p class="body">The model takes about 2 minutes to train if you are using GPU training. Alternatively, you can download the trained model from my website: <a class="url" href="https://mng.bz/YV6K">https://mng.bz/YV6K</a>.</p>
<h3 class="fm-head1" id="heading_id_9">7.2.3 Saving and using the trained AE</h3>
<p class="body"><a id="marker-151"/>We’ll save the model in the local folder on your computer:<a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>
<pre class="programlisting">scripted = torch.jit.script(model) 
scripted.save('files/AEdigits.pt') </pre>
<p class="body">To use it to reconstruct an image of handwritten digits, we load up the model:</p>
<pre class="programlisting">model=torch.jit.load('files/AEdigits.pt',map_location=device)
model.eval()</pre>
<p class="body">We can use it to generate handwritten digits by calling the <code class="fm-code-in-text">plot_digits()</code> function we defined earlier:<a id="idIndexMarker040"/></p>
<pre class="programlisting"><code class="fm-code-in-text1">plot_digits</code>()</pre>
<p class="body">The output is shown in figure 7.5.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="95" src="../../OEBPS/Images/CH07_F05_Liu.png" width="450"/></p>
<p class="figurecaption">Figure 7.5 Comparing reconstructed images by the trained AE with the originals. The top row shows 10 original images of handwritten digits in the test set. The bottom row shows the reconstructed images by the trained AE. The reconstructed images look similar to the original ones.</p>
</div>
<p class="body">The reconstructed handwritten digits do resemble the original ones, although the reconstruction is not perfect. Some information gets lost during the encoding–decoding process. However, compared to GANs, AEs are easy to construct and take less time to train. Further, the encoder–decoder architecture is employed by many generative models. This project will help your understanding of later chapters, especially when we explore Transformers.</p>
<h2 class="fm-head" id="heading_id_10">7.3 What are VAEs?</h2>
<p class="body">While AEs are good at reconstructing original images, they fail at generating novel images that are unseen in the training set. Further, AEs tend not to map similar inputs to nearby points in the latent space. As a result, the latent space associated with an AE is neither continuous nor easily interpretable. For example, you cannot interpolate two input data points to generate meaningful intermediate representations. For these reasons, we’ll study an improvement in AEs: VAEs. <a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<p class="body">In this section, you’ll first learn the key differences between AEs and VAEs and why these differences lead to the ability of the latter to generate realistic images that are unseen in the training set. You’ll then learn the steps involved in training VAEs in general and training one to generate high-resolution human face images in particular.</p>
<h3 class="fm-head1" id="heading_id_11">7.3.1 Differences between AEs and VAEs</h3>
<p class="body">VAEs were first proposed by Diederik Kingma and Max Welling in 2013.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> They are a variant of AEs. Like an AE, a VAE also has two main parts: an encoder and a decoder.<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="marker-152"/></p>
<p class="body">However, there are two key differences between AEs and VAEs. First, the latent space in an AE is deterministic. Each input is mapped to a fixed point in the latent space. In contrast, the latent space in a VAE is probabilistic. Instead of encoding an input as a single vector in the latent space, a VAE encodes an input as a distribution over possible valu<a id="idTextAnchor009"/>es. In our second project, for example, we’ll encode a color image into a 100-value probabilistic vector. Additionally, we’ll assume that each element in this vector adheres to an independent normal distribution. Since defining a normal distribution requires just the mean <span class="times">(<i class="fm-italics">μ</i>)</span> and standard deviation <span class="times">(<i class="fm-italics">σ</i>)</span>, each element in our 100-element probabilistic vector will be characterized by these two parameters. To reconstruct the image, we sample a vector from this distribution and decode it. The uniqueness of VAEs is highlighted by the fact that each sampling from the distribution results in a slightly varied output.</p>
<p class="body">In statistical terms, the encoder in a VAE is trying to learn the true distribution of the training data <span class="times"><i class="timesitalic">x</i>, <i class="fm-italics">p</i>(<i class="timesitalic">x</i>|<i class="fm-italics">Θ</i>)</span>, where <i class="timesitalic">Θ</i> is the param<a id="idTextAnchor010"/>eters defining the distribution. For tractability, we usually assume that the distribution of the latent variable is normal. Because we only need the mean, <i class="timesitalic">μ</i>, and standard deviation, <i class="timesitalic">σ</i>, to define a normal distribution, we can rewrite the true distribution as <span class="times"><i class="fm-italics">p</i>(<i class="timesitalic">x</i>|<i class="fm-italics">Θ</i>) = <i class="fm-italics">p</i>(<i class="timesitalic">x</i>|<i class="fm-italics">μ</i>, <i class="fm-italics">σ</i>)</span>. The decoder in the VAE generates a sample based on the distribution learned by the encoder. That is, the decoder generates an instance probabilistically from the distribution <span class="times"><i class="fm-italics">p</i>(<i class="timesitalic">x</i>|<i class="fm-italics">μ</i>, <i class="fm-italics">σ</i>)</span>.</p>
<p class="body">The second key difference between AEs and VAEs lies in the loss function. When training an AE, we minimize the reconstruction loss so that the reconstructed images are as close to the originals as possible. In contrast, in VAEs, the loss function consists of two parts: the reconstruction loss and the KL divergence. KL divergence is a measure of how one probability distribution diverges from a second, expected probab<a id="idTextAnchor011"/>ility distribution. In VAEs, KL divergence is used to regularize the encoder by penalizing deviations of the learned distribution (the encoder’s output) from a prior distribution (a standard normal distribution). This encourages the encoder to learn meaningful and generalizable latent representations. By penalizing distributions that are too far from the prior, KL divergence helps to avoid overfitting.<a id="idIndexMarker045"/></p>
<p class="body">The KL divergence is calculated as follows in our setting since we assume a normal distribution (the formula is different if a nonnormal distribution is assumed):</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="90%"/>
<col class="contenttable-0-col" span="1" width="10%"/>
</colgroup>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre4" height="68" src="../../OEBPS/Images/CH07_F06_Liu_EQ01.png" width="471"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="fm-equation-caption">(7.1)</p>
</td>
</tr>
</tbody>
</table>
<p class="body">The summation is taken over all 100 dimensions of the latent space. When the encoder compresses the images into standard normal distributions in the latent space, such that <span class="times"><i class="timesitalic">μ</i>=0</span> and <span class="times"><i class="timesitalic">σ</i>=1</span>, the KL divergence becomes 0. In any other scenario, the value exceeds 0. Thus, the KL divergence is minimized when the encoder successfully compresses the images into standard normal distributions within the latent space.</p>
<h3 class="fm-head1" id="heading_id_12">7.3.2 The blueprint to train a VAE to generate human face images</h3>
<p class="body"><a id="marker-153"/>In the second project in this chapter, you’ll build and train a VAE from scratch to generate color images of human faces. The trained model can generate images that are unseen in the training set. Further, you can interpolate inputs to generate novel images that are intermediate representations between two input data points. The following is a blueprint for this second project. <a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>
<p class="body">Figure 7.6 provides a diagram of the architecture of a VAE and the steps in training a VAE to generate human face images.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="365" src="../../OEBPS/Images/CH07_F06_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 7.6 The architecture of a VAE and the steps to train one to generate human face images. A VAE consists of an encoder (middle upper left) and a decoder (middle bottom right). In each iteration of training, human face images are fed to the encoder (step 1). The encoder compresses the images to probabilistic points in the latent space (step 2; since we assume normal distributions, each probability point is characterized by a vector of means and a vector of standard deviations). We then sample encodings from the distribution and present them to the decoder. The decoder takes sampled encodings (step 3) and reconstructs images (step 4). The VAE adjusts its parameters to minimize the sum of reconstruction loss and the KL divergence. The KL divergence measures the difference between the encoder’s output and a standard normal distribution.</p>
</div>
<p class="body">Figure 7.6 shows that a VAE also has two parts: an encoder (middle top left) and a decoder (middle bottom right). Since the second project involves high-resolution color images, we’ll use CNNs to create the VAE. As we discussed in chapter 4, high-resolution color images contain many more pixels than low-resolution grayscale images. If we use fully connected (dense) layers only, the number of parameters in the model is too large, making learning slow and ineffective. CNNs require fewer parameters than fully connected networks of similar size, leading to faster and more effective learning.</p>
<p class="body">Once the VAE is created, you’ll use the eyeglasses dataset that you downloaded in chapter 5 to train the model. The left side of figure 7.6 shows three examples of the original human face images in the training set. In the first step in the training loop, we feed images in the training set, with a size of <span class="times">3 <span class="cambria">×</span> 256 <span class="cambria">×</span> 256 = 196,608 pixels</span>, to the encoder. The encoder compresses the images to 100-value probabilistic vectors in the latent space (step 2; vectors of means and standard deviations due to the assumption of normal distribution). We then sample from the distribution and feed the sampled vector representations to the decoder (step 3) and ask it to reconstruct the images (step 4). We calculate the total loss as the sum of the reconstruction loss at the pixel level and the KL divergence as specified in equation 7.1. We propagate this loss back through the network to update the parameters in the encoder and decoder to minimize the total loss (step 5). The total loss encourages the VAE to encode the inputs into more meaningful and generalizable latent representations and to reconstruct images closer to the originals.</p>
<p class="body">After the model is trained, you’ll feed human face images to the encoder and obtain encodings. You then feed the encodings to the decoder to obtain reconstructed images. You’ll notice that the reconstructed images look close to the originals. The right side of figure 7.6 shows three examples of reconstructed images: they look similar to the corresponding originals on the left side of the figure, though not perfectly.</p>
<p class="body">More importantly, you can discard the encoder and randomly draw encodings from the latent space and feed them to the trained decoder in VAE to generate novel human face images that are unseen in the training set. Further, you can manipulate the encoded representations of different inputs to achieve specific outcomes when decoded. You can also create a series of images transitioning from one instance to another by varying the weight assigned to any two encodings.<a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="marker-154"/><a id="idIndexMarker051"/></p>
<h2 class="fm-head" id="heading_id_13">7.4 A VAE to generate human face images</h2>
<p class="body">This section creates and trains a VAE from scratch to generate human face images by following the steps outlined in the last section.<a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/></p>
<p class="body">Compared to what we have done to build and train AEs, our approach for the second project incorporates several modifications. Firstly, we plan to use CNNs in both the encoders and decoders of VAEs, particularly because high-resolution color images possess a greater number of pixels. Relying solely on fully connected (dense) layers would result in an excessively large number of parameters, leading to slow and inefficient learning. Second, as part of our process to compress images into vectors that follow a normal distribution in the latent space, we will generate both a mean vector and a standard deviation vector during the encoding of each image. This differs from the fixed value vector used in AEs. From the encoded normal distribution, we’ll then sample to obtain encodings, which are subsequently decoded to produce images. Notably, each reconstructed image will vary slightly every time we sample from this distribution, which gives rise to VAEs’ ability to generate novel images. <a id="idIndexMarker055"/><a id="idIndexMarker056"/></p>
<h3 class="fm-head1" id="heading_id_14">7.4.1 Building a VAE</h3>
<p class="body">If you recall, the eyeglasses dataset that you downloaded in chapter 5 is saved in the folder /files/glasses/ on your computer after some labels are manually corrected. We’ll resize the images to 256 by 256 pixels with values between 0 and 1. We then create a batch iterator with 16 images in each batch:<a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="marker-155"/><a id="idIndexMarker060"/></p>
<pre class="programlisting">transform = T.Compose([
            T.Resize(256),                                 <span class="fm-combinumeral">①</span>
            T.ToTensor(),                                  <span class="fm-combinumeral">②</span>
            ])
data = torchvision.datasets.ImageFolder(
    root="files/glasses",    
    transform=transform)                                   <span class="fm-combinumeral">③</span>
batch_size=16
loader = torch.utils.data.DataLoader(data,                 <span class="fm-combinumeral">④</span>
     batch_size=batch_size,shuffle=True)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Resizes images to 256 by 256 pixels</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts images to tensors with values between 0 and 1</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Loads images from the folder and apply the transformations</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Places the data in a batch iterator</p>
<p class="body">Next, we’ll create a VAE that includes convolutional and transposed convolutional layers. We first define an <code class="fm-code-in-text">Encoder()</code> class as follows.<a id="idIndexMarker061"/></p>
<p class="fm-code-listing-caption">Listing 7.4 The encoder in the VAE</p>
<pre class="programlisting">latent_dims=100                                             <span class="fm-combinumeral">①</span>
class Encoder(nn.Module):
    def __init__(self, latent_dims=100):  
        super().__init__()
        self.conv1 = nn.Conv2d(3, 8, 3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)
        self.batch2 = nn.BatchNorm2d(16)
        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)
        self.linear1 = nn.Linear(31*31*32, 1024)
        self.linear2 = nn.Linear(1024, latent_dims)
        self.linear3 = nn.Linear(1024, latent_dims)
        self.N = torch.distributions.Normal(0, 1)
        self.N.loc = self.N.loc.cuda() 
        self.N.scale = self.N.scale.cuda()
    def forward(self, x):
        x = x.to(device)
        x = F.relu(self.conv1(x))
        x = F.relu(self.batch2(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = torch.flatten(x, start_dim=1)
        x = F.relu(self.linear1(x))
        mu =  self.linear2(x)                               <span class="fm-combinumeral">②</span>
        std = torch.exp(self.linear3(x))                    <span class="fm-combinumeral">③</span>
        z = mu + std*self.N.sample(mu.shape)                <span class="fm-combinumeral">④</span>
        return mu, std, z</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The dimension of the latent space is 100.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The mean of the distribution of the encodings</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The standard deviation of the encodings</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The encoded vector representation</p>
<p class="body">The encoder network consists of several convolutional layers, which extract the spatial features of the input images. The encoder compresses the inputs into vector representations, <code class="fm-code-in-text">z</code>, which are normally distributed with means, <code class="fm-code-in-text">mu</code>, and standard deviations, <code class="fm-code-in-text">std</code>. The output from the encoder consists of three tensors: <code class="fm-code-in-text">mu</code>, <code class="fm-code-in-text">std</code>, and <code class="fm-code-in-text">z</code>. While the <code class="fm-code-in-text">mu</code> and <code class="fm-code-in-text">std</code> are the mean and standard deviation of the probabilistic vector, respectively, <code class="fm-code-in-text">z</code> is an instance sampled from this distribution. <a id="idIndexMarker062"/><a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>
<p class="body">Specifically, the input image, with a size of (3, 256, 256), first goes through a Conv2d layer with a stride value of 2. As we explained in chapter 4, this means the filter skips two pixels each time it moves on the input image, which leads to downsampling of the image. The output has a size of (8, 128, 128). It then goes through two more Conv2d layers, and the size becomes (32, 31, 31). It is flattened and passed through linear layers to obtain values of <code class="fm-code-in-text">mu</code> and <code class="fm-code-in-text">std</code>.</p>
<p class="body">We define a <code class="fm-code-in-text">Decoder()</code> class to represent the decoder in the VAE.<a id="marker-156"/><a id="idIndexMarker065"/></p>
<p class="fm-code-listing-caption">Listing 7.5 The decoder in the VAE</p>
<pre class="programlisting">class Decoder(nn.Module):   
    def __init__(self, latent_dims=100):
        super().__init__()
        self.decoder_lin = nn.Sequential(                   <span class="fm-combinumeral">①</span>
            nn.Linear(latent_dims, 1024),
            nn.ReLU(True),
            nn.Linear(1024, 31*31*32),                      <span class="fm-combinumeral">②</span>
            nn.ReLU(True))
        self.unflatten = nn.Unflatten(dim=1, 
                  unflattened_size=(32,31,31))
        self.decoder_conv = nn.Sequential(                  <span class="fm-combinumeral">③</span>
            nn.ConvTranspose2d(32,16,3,stride=2,
                               output_padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, 3, stride=2, 
                               padding=1, output_padding=1),
            nn.BatchNorm2d(8),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 3, 3, stride=2,
                               padding=1, output_padding=1))
        
    def forward(self, x):
        x = self.decoder_lin(x)
        x = self.unflatten(x)
        x = self.decoder_conv(x)
        x = torch.sigmoid(x)                                <span class="fm-combinumeral">④</span>
        return x  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Encodings first go through two dense layers.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reshapes encodings into multidimensional objects so we can perform transposed convolutional operations on them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Passes the encodings through three transposed convolutional lay</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Squeezes the output to values between 0 and 1, the same as the values in the input images</p>
<p class="body">The decoder is a mirror image of the encoder: instead of performing convolutional operations, it performs transposed convolutional operations on the encodings to generate feature maps. It gradually converts encodings in the latent space back into high-resolution color images.</p>
<p class="body">Specifically, the encoding first goes through two linear layers. It’s then unflattened to a shape (32, 31, 31), mirroring the size of the image after the last Conv2d layer in the encoder. It then goes through three ConvTranspose2d layers, mirroring the Conv2d layers in the encoder. The output from the decoder has a shape of (3, 256, 256), the same as that of the training image.</p>
<p class="body">We’ll combine the encoder with the decoder to create a VAE:</p>
<pre class="programlisting">class VAE(nn.Module):
    def __init__(self, latent_dims=100):
        super().__init__()
        self.encoder = Encoder(latent_dims)                <span class="fm-combinumeral">①</span>
        self.decoder = Decoder(latent_dims)                <span class="fm-combinumeral">②</span>
    def forward(self, x):
        x = x.to(device)
        mu, std, z = self.encoder(x)                       <span class="fm-combinumeral">③</span>
        return mu, std, self.decoder(z)                    <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates an encoder by instantiating the Encoder() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a decoder by instantiating the Decoder() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Passes the input through the encoder to obtain the encoding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> The output of the VAE is the mean and standard deviation of the encodings, as well as the reconstructed images.</p>
<p class="body">The VAE consists of an encoder and a decoder, as defined by the <code class="fm-code-in-text">Encoder()</code> and <code class="fm-code-in-text">Decoder()</code> classes. When we pass images through the VAE, the output consists of three tensors: the mean and standard deviation of the encodings and the reconstructed images. <a id="idIndexMarker066"/><a id="idIndexMarker067"/></p>
<p class="body">Next, we create a VAE by instantiating the <code class="fm-code-in-text">VAE()</code> class and define the optimizer for the model:<a id="idIndexMarker068"/></p>
<pre class="programlisting">vae=VAE().to(device)
lr=1e-4 
optimizer=torch.optim.Adam(vae.parameters(),
                           lr=lr,weight_decay=1e-5)</pre>
<p class="body">We’ll manually calculate the reconstruction loss and the KL-divergence loss during training. Therefore, we don't define a loss function here.</p>
<h3 class="fm-head1" id="heading_id_15">7.4.2 Training the VAE</h3>
<p class="body"><a id="marker-157"/>To train the model, we first define a <code class="fm-code-in-text">train_epoch()</code> function to train the model for one epoch.<a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>
<p class="fm-code-listing-caption">Listing 7.6 Defining the <code class="fm-code-in-text">train_epoch()</code> function</p>
<pre class="programlisting">def train_epoch(epoch):
    vae.train()
    epoch_loss = 0.0
    for imgs, _ in loader: 
        imgs = imgs.to(device)
        mu, std, out = vae(imgs)                                   <span class="fm-combinumeral">①</span>
        reconstruction_loss = ((imgs-out)**2).sum()                <span class="fm-combinumeral">②</span>
        kl = ((std**2)/2 + (mu**2)/2 - torch.log(std) - 0.5).sum() <span class="fm-combinumeral">③</span>
        loss = reconstruction_loss + kl                            <span class="fm-combinumeral">④</span>
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss+=loss.item()
    print(f'at epoch {epoch}, loss is {epoch_loss}')  </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Obtains the reconstructed images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates the reconstruction loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates the KL divergence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sum of the reconstruction loss and the KL divergence.</p>
<p class="body">We iterate through all batches in the training set. We pass images through the VAE to obtain reconstructed images. The total loss is the sum of the reconstruction loss and the KL divergence. The model parameters are adjusted in each iteration to minimize the total loss.</p>
<p class="body"><a id="marker-158"/>We also define a <code class="fm-code-in-text">plot_epoch()</code> function to visually inspect the generated images by the VAE:<a id="idIndexMarker071"/></p>
<pre class="programlisting">import numpy as np
import matplotlib.pyplot as plt
  
def plot_epoch():
    with torch.no_grad():
        noise = torch.randn(18,latent_dims).to(device)
        imgs = vae.decoder(noise).cpu()
        imgs = torchvision.utils.make_grid(imgs,6,3).numpy()
        fig, ax = plt.subplots(figsize=(6,3),dpi=100)
        plt.imshow(np.transpose(imgs, (1, 2, 0)))
        plt.axis("off")
        plt.show()</pre>
<p class="body">A well-trained VAE can map similar inputs to nearby points in the latent space, leading to a more continuous and interpretable latent space. As a result, we can randomly draw vectors from the latent space, and the VAE can decode the vectors into meaningful outputs. Therefore, in the previous function <code class="fm-code-in-text">plot_epoch()</code>, we randomly draw 18 vectors from the latent space and use them to generate 18 images after each epoch of training. We plot them in a <span class="times">3 <span class="cambria">×</span> 6</span> grid and visually inspect them to see how the VAE is performing during the training process.<a id="idIndexMarker072"/></p>
<p class="body">Next, we train the VAE for 10 epochs:</p>
<pre class="programlisting">for epoch in range(1,11):
    train_epoch(epoch)
    plot_epoch()
torch.save(vae.state_dict(),"files/VAEglasses.pth")</pre>
<p class="body">This training takes about half an hour if you use GPU training or several hours otherwise. The trained model weights are saved on your computer. Alternatively, you can download the trained weights from my website: <a class="url" href="https://mng.bz/GNRR">https://mng.bz/GNRR</a>. Make sure you unzip the file after downloading.</p>
<h3 class="fm-head1" id="heading_id_16">7.4.3 Generating images with the trained VAE</h3>
<p class="body">Now that the VAE is trained, we can use it to generate images. We first load the weights of the trained model that we saved in the local folder:<a id="idIndexMarker073"/><a id="idIndexMarker074"/></p>
<pre class="programlisting">vae.eval()
vae.load_state_dict(torch.load('files/VAEglasses.pth',
    map_location=device))</pre>
<p class="body">We then check the VAE’s ability to reconstruct images and see how closely they resemble the originals:</p>
<pre class="programlisting">imgs,_=next(iter(loader))
imgs = imgs.to(device)
mu, std, out = vae(imgs)
images=torch.cat([imgs[:8],out[:8],imgs[8:16],out[8:16]],
                 dim=0).detach().cpu()
images = torchvision.utils.make_grid(images,8,4)
fig, ax = plt.subplots(figsize=(8,4),dpi=100)
plt.imshow(np.transpose(images, (1, 2, 0)))
plt.axis("off")
plt.show()</pre>
<p class="body">If you run the previous code block, you’ll see an output similar to figure 7.7.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="400" src="../../OEBPS/Images/CH07_F07_Liu.png" width="773"/></p>
<p class="figurecaption">Figure 7.7 Comparing the reconstructed images by a trained VAE with the originals. The first and the third rows are the original images. We feed them to the trained VAE to obtain the reconstructed images, which are shown below the original images.</p>
</div>
<p class="body"><a id="marker-159"/>The original images are shown in the first and third rows, while the reconstructed images are shown below the originals. The reconstructed images resemble the originals, as shown in figure 7.7. However, some information gets lost during the reconstruction process: they don’t look as realistic as the originals.</p>
<p class="body">Next, we test the VAE’s ability to generate novel images that are unseen in the training set, by calling the plot_epoch() function we defined before:</p>
<pre class="programlisting">plot_epoch()  </pre>
<p class="body">The function randomly draws 18 vectors from the latent space and passes them to the trained VAE to generate 18 images. The output is shown in figure 7.8.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="377" src="../../OEBPS/Images/CH07_F08_Liu.png" width="722"/></p>
<p class="figurecaption">Figure 7.8 Novel images generated by the trained VAE. We randomly draw vector representations in the latent space and feed them to the decoder in the trained VAE. The decoded images are shown in this figure. Since the vector representations are randomly drawn, the images don’t correspond to any originals in the training set.</p>
</div>
<p class="body">These images are not present in the training set: the encodings are randomly drawn from the latent space, not the encoded vectors after passing images in the training set through the encoder. This is because the latent space in VAEs is continuous and interpretable. New and unseen encodings in the latent space can be meaningfully decoded into images that resemble but differ from those in the training set.</p>
<h3 class="fm-head1" id="heading_id_17">7.4.4 Encoding arithmetic with the trained VAE</h3>
<p class="body"><a id="marker-160"/>VAEs include a regularization term (KL divergence) in their loss function, which encourages the latent space to approximate a normal distribution. This regularization ensures that the latent variables don’t just memorize the training data but rather capture the underlying distribution. It helps to achieve a well-structured latent space where similar data points are mapped closely together, making the space continuous and interpretable. As a result, we can manipulate the encodings to achieve new outcomes. <a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="idIndexMarker077"/></p>
<p class="body">To make results reproducible, I encourage you <a id="idTextAnchor012"/>to download the trained weights from my website (<a class="url" href="https://mng.bz/GNRR">https://mng.bz/GNRR</a>) and use the same code blocks for the rest of the chapter. As we explained in the introduction, encoding arithmetic allows us to generate images with certain features. To illustrate how encoding arithmetic works in VAEs, let’s first hand-collect three images in each of the following four groups: men with glasses, men without glasses, women with glasses, and women without glasses.</p>
<p class="fm-code-listing-caption">Listing 7.7 Collecting images with different characteristics</p>
<pre class="programlisting">torch.manual_seed(0)  
glasses=[]
for i in range(25):                                        <span class="fm-combinumeral">①</span>
    img,label=data[i]
    glasses.append(img)
    plt.subplot(5,5,i+1)
    plt.imshow(img.numpy().transpose((1,2,0)))
    plt.axis("off")
plt.show()
men_g=[glasses[0],glasses[3],glasses[14]]                  <span class="fm-combinumeral">②</span>
women_g=[glasses[9],glasses[15],glasses[21]]               <span class="fm-combinumeral">③</span>
  
noglasses=[]
for i in range(25):                                        <span class="fm-combinumeral">④</span>
    img,label=data[-i-1]
    noglasses.append(img)
    plt.subplot(5,5,i+1)
    plt.imshow(img.numpy().transpose((1,2,0)))
    plt.axis("off")
plt.show()
men_ng=[noglasses[1],noglasses[7],noglasses[22]]           <span class="fm-combinumeral">⑤</span>
women_ng=[noglasses[4],noglasses[9],noglasses[19]])        <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Displays 25 images with eyeglasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Selects three images of men with glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Selects three images of women with glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Displays 25 images without eyeglasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Selects three images of men without glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Selects three images of women without glasses</p>
<p class="body">We select three images in each group instead of just one so that we can calculate the average of multiple encodings in the same group when performing encoding arithmetic later. VAEs are designed to learn the distribution of the input data in the latent space. By averaging multiple encodings, we effectively smooth out the representation in this space. This helps us find an average representation that captures common features among different samples within a group.</p>
<p class="body"><a id="marker-161"/>Next we feed the three images of men with glasses to the trained VAE to obtain their encodings in the latent space. We then calculate the average encoding for the three images and use it to obtain a reconstructed image of a man with glasses. We then repeat this for the other three groups.</p>
<p class="fm-code-listing-caption">Listing 7.8 Encoding and decoding images in four different groups</p>
<pre class="programlisting"># create a batch of images of men with glasses
men_g_batch = torch.cat((men_g[0].unsqueeze(0),              <span class="fm-combinumeral">①</span>
             men_g[1].unsqueeze(0),
             men_g[2].unsqueeze(0)), dim=0).to(device)
# Obtain the three encodings
_,_,men_g_encodings=vae.encoder(men_g_batch)
# Average over the three images to obtain the encoding for the group
men_g_encoding=men_g_encodings.mean(dim=0)                   <span class="fm-combinumeral">②</span>
# Decode the average encoding to create an image of a man with glasses 
men_g_recon=vae.decoder(men_g_encoding.unsqueeze(0))         <span class="fm-combinumeral">③</span>
  
# Do the same for the other three groups
# group 2, women with glasses
women_g_batch = torch.cat((women_g[0].unsqueeze(0),
             women_g[1].unsqueeze(0),
             women_g[2].unsqueeze(0)), dim=0).to(device)
# group 3, men without glasses
men_ng_batch = torch.cat((men_ng[0].unsqueeze(0),
             men_ng[1].unsqueeze(0),
             men_ng[2].unsqueeze(0)), dim=0).to(device)
# group 4, women without glasses
women_ng_batch = torch.cat((women_ng[0].unsqueeze(0),
             women_ng[1].unsqueeze(0),
             women_ng[2].unsqueeze(0)), dim=0).to(device)
# obtain average encoding for each group
_,_,women_g_encodings=vae.encoder(women_g_batch)
women_g_encoding=women_g_encodings.mean(dim=0)
_,_,men_ng_encodings=vae.encoder(men_ng_batch)
men_ng_encoding=men_ng_encodings.mean(dim=0)
_,_,women_ng_encodings=vae.encoder(women_ng_batch)
women_ng_encoding=women_ng_encodings.mean(dim=0)              <span class="fm-combinumeral">④</span>
# decode for each group
women_g_recon=vae.decoder(women_g_encoding.unsqueeze(0))
men_ng_recon=vae.decoder(men_ng_encoding.unsqueeze(0))
women_ng_recon=vae.decoder(women_ng_encoding.unsqueeze(0))    <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a batch of images of men with glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Obtains the average encoding for men with glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Decodes the average encoding for men with glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Obtains the average encodings for the other three groups</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Decodes the average encodings for the other three groups</p>
<p class="body">The average encodings for the four groups are <code class="fm-code-in-text">men_g_encoding</code>, <code class="fm-code-in-text">women_g_encoding</code>, <code class="fm-code-in-text">men_ng_encoding</code>, and <code class="fm-code-in-text">women_ng_encoding</code>, respectively, where <code class="fm-code-in-text">g</code> stands for glasses and <code class="fm-code-in-text">ng</code> for no glasses. The decoded images for the four groups are <code class="fm-code-in-text">men_g_recon</code>, <code class="fm-code-in-text">women_g_recon</code>, <code class="fm-code-in-text">men_ng_recon</code>, and <code class="fm-code-in-text">women_ng_recon</code>, respectively. We plot the four images:<a id="marker-162"/></p>
<pre class="programlisting">imgs=torch.cat((men_g_recon,
                women_g_recon,
                men_ng_recon,
                women_ng_recon),dim=0)
imgs=torchvision.utils.make_grid(imgs,4,1).cpu().numpy()
imgs=np.transpose(imgs,(1,2,0))
fig, ax = plt.subplots(figsize=(8,2),dpi=100)
plt.imshow(imgs)
plt.axis("off")
plt.show()</pre>
<p class="body">You’ll see the output as shown in figure 7.9.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="185" src="../../OEBPS/Images/CH07_F09_Liu.png" width="675"/></p>
<p class="figurecaption">Figure 7.9 Decoded images based on average encodings. We first obtain three images in each of the following four groups: men with glasses, women with glasses, men without glasses, and women without glasses. We feed the 12 images to the encoder in the trained VAE to obtain their encodings in the latent space. We then calculate the average encoding of the three images in each group. The four average encodings are fed to the decoder in the trained VAE to obtain four images and they are shown in this figure.</p>
</div>
<p class="body">The four decoded images are shown in figure 7.9. They are the composite images representing the four groups. Notice that they are different from any of the original 12 images. At the same time, they preserve the defining characteristics of each group.</p>
<p class="body">Next, let’s manipulate the encodings to create a new encoding and then use the trained decoder in the VAE to decode the new encoding and see what happens. For example, we can subtract the average encoding of women with glasses from the average encoding of men with glasses and add the average encoding of women without glasses. We then feed the result to the decoder and see the output.</p>
<p class="fm-code-listing-caption">Listing 7.9 An example of encoding arithmetic</p>
<pre class="programlisting">z=men_g_encoding-women_g_encoding+women_ng_encoding         <span class="fm-combinumeral">①</span>
out=vae.decoder(z.unsqueeze(0))                             <span class="fm-combinumeral">②</span>
imgs=torch.cat((men_g_recon,
                women_g_recon,
                women_ng_recon,out),dim=0)
imgs=torchvision.utils.make_grid(imgs,4,1).cpu().numpy()
imgs=np.transpose(imgs,(1,2,0))
fig, ax = plt.subplots(figsize=(8,2),dpi=100)
plt.imshow(imgs)                                            <span class="fm-combinumeral">③</span>
plt.title("man with glasses - woman \
with glasses + woman without \
glasses = man without glasses ",fontsize=10,c="r")          <span class="fm-combinumeral">④</span>
plt.axis("off")
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines z as the encoding of men with glasses – women with glasses + women without glasses</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Decodes z to generate an image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Displays the four images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Displays a title on top of the images</p>
<p class="body"><a id="marker-163"/>If you run the code block in listing 7.9, you’ll see an output as shown in figure 7.10.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="199" src="../../OEBPS/Images/CH07_F10_Liu.png" width="675"/></p>
<p class="figurecaption">Figure 7.10 An example of encoding arithmetic with the trained VAE. We first obtain the average encodings for the following three groups: men with glasses (<span class="times">z1</span>), women with glasses (<span class="times">z2</span>), and women without glasses (<span class="times">z3</span>). We define a new encoding <span class="times">z = z1 – z2 + z3</span>. We then feed <span class="times">z</span> to the decoder in the trained VAE and obtain the decoded image, as shown at the far right of this figure.</p>
</div>
<p class="body">The first three images in figure 7.10 are the composite images representing the three input groups. The output image, at the far right, is an image of a man without glasses.</p>
<p class="body">Since both <code class="fm-code-in-text">men_g_encoding</code> and <code class="fm-code-in-text">women_g_encoding</code> lead to eyeglasses in images when decoded, <code class="fm-code-in-text">men_g_encoding</code> – <code class="fm-code-in-text">women_g_encoding</code> cancels out eyeglasses features in the resulting image. Similarly, since both <code class="fm-code-in-text">women_ng_encoding</code> and <code class="fm-code-in-text">women_g_encoding</code> lead to a female face, <code class="fm-code-in-text">women_ng_encoding</code> – <code class="fm-code-in-text">women_g_encoding</code> cancels out female features in the resulting image. Therefore, if you decode <code class="fm-code-in-text">men_g_encoding</code> + <code class="fm-code-in-text">women_g_encoding</code> –<code class="fm-code-in-text">women_ng_encoding</code> with the trained VAE, you’ll get an image of a man without glasses. The encoding arithmetic in this example shows that an encoding for men without glasses can be obtained by manipulating the average encodings in the other three groups.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 7.1</p>
<p class="fm-sidebar-text">Perform the following encoding arithmetics by modifying code listing 7.9:</p>
<ol class="calibre7">
<li class="fm-list-bullet1">
<p class="list-s">Subtract the average encoding of men without glasses from the average encoding of men with glasses and add the average encoding of women without glasses. Feed the result to the decoder and see what happens.</p>
</li>
<li class="fm-list-bullet1">
<p class="list-s">Subtract the average encoding of women without glasses from the average encoding of men without glasses and add the average encoding of women with glasses. Feed the result to the decoder and see what happens.</p>
</li>
<li class="fm-list-bullet1">
<p class="list-s">Subtract the average encoding of men without glasses from the average encoding of women without glasses and add the average encoding of men with glasses. Feed the result to the decoder and see what happens. Make sure you modify the image titles to reflect the changes. The solutions are provided in the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>.</p>
</li>
</ol>
</div>
<p class="body"><a id="marker-164"/>Further, we can interpolate any two encodings in the latent space by assigning different weights to them and creating a new encoding. We can then decode the new encoding and create a composite image as a result. By choosing different weights, we can create a series of intermediate images that transition from one image to another.</p>
<p class="body">Let’s use the encodings of women with and without glasses as an example. We’ll define a new encoding <code class="fm-code-in-text">z</code> as <code class="fm-code-in-text">w*women_ng_encoding+(1-w)*women_g_encoding</code>, where <code class="fm-code-in-text">w</code> is the weight we put on <code class="fm-code-in-text">women_ng_encoding</code>. We’ll change the value of <code class="fm-code-in-text">w</code> from 0 to 1 with an increment of 0.2 in each step. We then decode them and display the resulting six images.</p>
<p class="fm-code-listing-caption">Listing 7.10 Interpolating two encodings to create a series of images</p>
<pre class="programlisting">results=[]
for w in [0, 0.2, 0.4, 0.6, 0.8, 1.0]:           <span class="fm-combinumeral">①</span>
    z=w*women_ng_encoding+(1-w)*women_g_encoding <span class="fm-combinumeral">②</span>
    out=vae.decoder(z.unsqueeze(0))              <span class="fm-combinumeral">③</span>
    results.append(out)
imgs=torch.cat((results[0],results[1],results[2],
                results[3],results[4],results[5]),dim=0)
imgs=torchvision.utils.make_grid(imgs,6,1).cpu().numpy()
imgs=np.transpose(imgs,(1,2,0))
fig, ax = plt.subplots(dpi=100)
plt.imshow(imgs)                                 <span class="fm-combinumeral">④</span>
plt.axis("off")
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through six different values of w</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Interpolates between two encoding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Decodes the interpolated encoding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Displays the six resulting images</p>
<p class="body">After running the code in listing 7.10, you’ll see an output as shown in figure 7.11.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="146" src="../../OEBPS/Images/CH07_F11_Liu.png" width="728"/></p>
<p class="figurecaption">Figure 7.11 Interpolating encodings to create a series of intermediate images. We first obtain the average encodings for women with glasses (<code class="fm-code-in-text">women_g_encoding</code>) and women without glasses (<code class="fm-code-in-text">women_ng_encoding</code>). The interpolated encoding z is defined as <code class="fm-code-in-text">w*women_ng_encoding+(1-w)*women_g_encoding</code>, where w is the weight on <code class="fm-code-in-text">women_ng_encoding</code>. We change the value of w from 0 to 1 with an increment of 0.2 to create six interpolated encodings. We then decode them and display the resulting six images in the figure.</p>
</div>
<p class="body">As you can see in figure 7.11, as you move from left to right, the image gradually transitions from a woman with glasses to a woman without glasses. This shows that the encodings in the latent space are continuous, meaningful, and interpolatable.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 7.2</p>
<p class="fm-sidebar-text">Modify listing 7.10 to create a series of intermediate images by using the following pairs of encodings: (i) <code class="fm-code-in-text1">men_ng_encoding</code> and <code class="fm-code-in-text1">men_g_encoding</code>; (ii) <code class="fm-code-in-text1">men_ng_encoding</code> and <code class="fm-code-in-text1">women_ng_encoding</code>; (iii) <code class="fm-code-in-text1">men_g_encoding</code> and <code class="fm-code-in-text1">women_g_encoding</code>. The solutions are provided in the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>.</p>
</div>
<p class="body"><a id="marker-165"/>Starting in the next chapter, you’ll embark on a journey in natural language processing. This will enable you to generate another form of content: text. However, many tools you have used so far will be used again in later chapters, such as deep neural networks and the encoder-decoder architecture.</p>
<h2 class="fm-head" id="heading_id_18">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">AEs have a dual-component structure: an encoder and a decoder. The encoder compresses the data into an abstract representation in a lower-dimensional space (the latent space), and the decoder decompresses the encoded information and reconstructs the data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">VAEs also consist of an encoder and a decoder. They differ from AEs in two critical ways. First, while an AE encodes each input into a specific point in the latent space, a VAE encodes it into a probability distribution within this space. Second, an AE focuses solely on minimizing the reconstruction error, whereas a VAE learns the parameters of the probability distribution for latent variables, minimizing a loss function that includes both reconstruction loss and a regularization term, the KL divergence.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The KL divergence in the loss function when training VAEs ensures the distribution for latent variables resembles a normal distribution. This encourages the encoder to learn continuous, meaningful, and generalizable latent representations.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A well-trained VAE can map similar inputs to nearby points in the latent space, leading to a more continuous and interpretable latent space. As a result, VAEs can decode random vectors in the latent space into meaningful outputs, leading to images that are unseen in the training set.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The latent space in a VAE is continuous and interpretable, different from that in an AE. As a result, we can manipulate the encodings to achieve new outcomes. We can also create a series of intermediate images transitioning from one instance to another by varying weights on two encodings in the latent space.<a id="marker-166"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Diederik P Kingma and Max Welling, 2013, “Auto-Encoding Variational Bayes.” <a class="url" href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a>.</p>
</div></body></html>