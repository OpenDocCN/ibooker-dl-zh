- en: 4 Analyzing text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classifying text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text data is ubiquitous and contains valuable information. For instance, think
    of newspaper articles, emails, reviews, or perhaps this book you are reading!
    However, analyzing text via computational means was difficult until only a few
    years ago. After all, unlike formal languages such as Python, natural language
    was not designed to be easy for computers to parse. The latest generation of language
    models enables text analysis at almost human levels for many popular tasks. In
    some cases, the performance of language models for text analysis and generation
    has even been shown, on average, to surpass the capabilities of humans [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will see how to use large language models to analyze text.
    In certain ways, analyzing text data is a very “natural” application of language
    models. They have been trained on large amounts of text and can be applied directly
    for text analysis (i.e., without referring to external tools for the actual data
    analysis). This chapter covers several popular flavors of text analysis: classifying
    text documents, extracting tabular data from text, and clustering text documents
    into groups of semantically similar documents. For each of these use cases, we
    will see example code and discuss variants and extensions.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification, information extraction, and clustering are three important types
    of text analysis but by no means the only ones you may need in practice. However,
    working through the examples in this chapter will enable you to create custom
    data-processing pipelines for text data based on language models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s make sure your system is set up properly for the example projects. The
    following examples use OpenAI’s GPT model series, accessed via OpenAI’s Python
    library. This library was discussed in detail in chapter 3\. Make sure to follow
    the instructions in chapter 3 to be able to execute the example code.
  prefs: []
  type: TYPE_NORMAL
- en: Warning OpenAI’s Python library is changing quickly. The code in this chapter
    has been tested with version 1.29 of the OpenAI Python library but may not work
    with different versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the OpenAI library, we will use the popular `pandas` library. `pandas`
    is a popular library for handling tabular data (which we will use as input and
    output format). We will only use basic functionality from that library and explain
    the corresponding commands as they occur in the code. Make sure `pandas` is installed
    (e.g., try `import pandas` in the Python interpreter); if it isn’t, install it
    by entering the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, for the last section in this chapter, you will need the clustering
    algorithms from the `scikit-learn` library. Run the following command in the terminal
    to install the appropriate version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The following sections contain code for three mini-projects that use language
    models for text analysis. No need to type in the code—you can find all the code
    on the book’s companion website in the resource section for this chapter. Although
    you can execute the code on your own data, this book comes with a couple of sample
    data sets we use in the examples (also on the companion website). And now it’s
    time to use language models for text classification!
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So here you are, planning your Saturday evening and deliberating whether to
    go and see the newest installation of your favorite movie franchise. But is it
    worth it? Your social media feeds keep filling up with comments from your friends
    (and your friend’s friends), expanding on their movie experiences. You could browse
    through them manually, reading each one to get a better sense of whether the majority
    opinion about the movie is positive or negative. But who has time to do that?
    Can’t language models help us to automate this task?
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed they can. What we have here is an instance of one of the most classic
    text-processing problems: we have a text and want to classify it, mapping it to
    one of a fixed set of categories. In this case, the text to classify is a movie
    review. We want to classify it as positive (i.e., the writer thinks it was a great
    movie, and you should go see it!) or negative (save your money!). That means we
    have two categories. Table [4.1](#tab__moviereviews) shows extracts from a few
    example reviews with the associated class labels. A review praising a movie as
    “well realized” is clearly positive, whereas one describing the movie as “obviously
    weak, cheap” is negative. You can find these and a few other reviews in a corresponding
    file on the book’s companion website.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Extracts from movie reviews and associated class labels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Review** | **Class** |'
  prefs: []
  type: TYPE_TB
- en: '| First of all this movie is a piece of reality very well realized artistically.
    ... | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| Re-titled “Gangs, Inc.”, this is an obviously weak, cheap mobster melodrama.
    ... | Negative |'
  prefs: []
  type: TYPE_TB
- en: Classifying movie reviews is only one of many use cases for text classification.
    As another example, imagine trying to sort through your email inbox. Wouldn’t
    it be nice to automatically classify emails based on their content (e.g., using
    custom categories such as Work, Hobby, Childcare, etc.)? That’s yet another instance
    of text classification, this time with more than two categories. As a final example,
    imagine that you’re creating a website that enables users to leave free-text comments.
    Of course, you don’t want to show potentially offensive comments and would like
    to filter them out automatically. Again, that means you’re classifying text comments
    into one of two categories (Offensive and Inoffensive). We will now see how language
    models can easily be used for each scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll focus on classifying movie reviews (or, really, any type of review) into
    Positive (great movie!) and Negative (stay home!) reviews. For that, we’ll use
    OpenAI’s language models. We’ll assume that we have collected reviews to classify
    in a file on disk. The code we develop will iterate over all reviews, classify
    each using the language model, and return the classification result for each review.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how can we classify reviews? We will use OpenAI’s Python library, presented
    in chapter 3\. For each review to classify, we will first generate a prompt. The
    prompt describes a task to a language model. In our case, that task assigns a
    review to one of our two categories (Positive or Negative). For instance, consider
    the following prompt as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Review'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Question'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output format'
  prefs: []
  type: TYPE_NORMAL
- en: This prompt contains the review to classify (**1**), a question describing the
    classification task (**2**), and a final statement describing the desired output
    format (**3**). We will construct prompts of this type for each review, send the
    prompt to the language model, and (hopefully) get back one of the two possible
    answers (Positive or Negative). Figure [4.1](#fig__classificationOverview) illustrates
    the high-level classification process for each review.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F01_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 For each review, we generate a prompt that contains the review, together
    with instructions describing the classification task. Given the prompt as input,
    the language model outputs a class label for the review.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.2.2 Creating prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a review, we generate a prompt instructing the language model to classify
    it. All the prompts we generate for classification follow the same prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reminder: What is a prompt template?'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We briefly mentioned prompt templates in chapter 1\. A prompt template is a
    text that contains placeholders. By substituting actual text for these placeholders,
    we obtain a prompt that we can send to the language model. We also say that a
    prompt *instantiates* a prompt template if the prompt can be obtained by substituting
    the template’s placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example prompt from the previous section instantiates the following prompt
    template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Review (placeholder)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Question'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output format'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our template contains only a single placeholder: the text of the review to
    classify (**1**). For each review, we will replace this placeholder with the actual
    review text. We also instruct the language model on what to do with the review
    text (**2**) (check whether the underlying sentiment is positive or negative)
    and define the output format (**3**). The latter step is important because there
    may be many ways to express the underlying sentiment: for example, “P” for positive
    and “N” for negative, or a longer answer such as “The review is positive.” If
    we don’t explicitly tell the language model to use a specific output format, it
    may choose any of these possibilities! In our scenario, we ultimately want to
    aggregate the classification results to learn the majority opinion (do most people
    like the movie or not?), and aggregating the results from each review becomes
    much simpler if all the classifications follow the same output format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function follows the template to generate a prompt for a given
    review (specified as the input parameter `text`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The result of the function is the prompt, instantiating the template for the
    input review.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Calling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we send generated prompts to a language model to obtain a solution. More
    precisely, we are using OpenAI’s GPT-4o model, OpenAI’s latest model at the time
    of writing. As this is one of OpenAI’s chat models, optimized for multistep interactions
    with users, we use the chat completions endpoint to communicate with the model.
    As discussed in more detail in chapter 3, this endpoint expects as input a history
    of prior messages (in addition to the specific model name). Here, we have only
    one prior “message”: the prompt. We classify it as a `user` message, encouraging
    the model to solve whatever task is described in the message. For instance, we
    can send prompts to the language model and collect the answers using the following
    piece of code (assuming that `prompt` contains the previously generated prompt
    text):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, using this code directly is problematic. OpenAI’s GPT models are hosted
    online and accessed remotely. This creates opportunities for failed attempts to
    reach the corresponding endpoint: for example, due to a temporary connection loss.
    Because of that, it is good practice to allow for a couple of retries when calling
    the model. In particular, when processing large data sets requiring many consecutive
    calls to OpenAI’s models, the chances of at least one unsuccessful call increase.
    Instead of interrupting computation with an exception, it is better to wait a
    few seconds before starting another try. Here is a completed version of the previous
    code—a function that calls the language model with automated retries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `call_lm` function allows up to three retries with an increasing delay between
    them. This delay is realized by a call to the `time.sleep` function (using Python’s
    `time` library) whenever an exception (indicating, for instance, a temporary connection
    loss) is encountered. After three retries, the function fails with an exception
    (assuming, pessimistically, that whatever problem prevents us from contacting
    OpenAI will not be resolved any time soon). Whenever the call succeeds, the function
    returns the corresponding result.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 End-to-end classification code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s time to put it all together! The next listing shows the code that matches
    the classification process we’ve discussed. It also contains the function for
    generating prompts (**2**) and the one for calling the language model (**3**).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Classifying input text by sentiment (positive, negative)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generates classification prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calls the large language model'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Classifies one text document'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Reads text, classifies, and writes result'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Defines command-line arguments'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Reads input'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Classifies text'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Generates output'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s discuss the libraries used in listing [4.1](#code__classification)
    (**1**). We will reuse those libraries for the following projects, so it makes
    sense to have a closer look at them (and why we need them here). We want to start
    our code from the command line, specifying relevant parameters (e.g., the path
    of the input data) as arguments. The `argparse` library features useful functions
    to specify and read out such command-line arguments. Next, we need the `openai`
    library, discussed in chapter 3, to call OpenAI’s language model from Python.
    The `pandas` library supports standard operations on tabular data. Of course,
    tabular data is not our focus in this chapter. However, we will store text documents
    and related metadata as rows in tables, so the `pandas` library comes in handy.
    Finally, as discussed previously, we use the `time` library to implement delayed
    retries when calling the language model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 Classifying documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The classification of a single text document (**4**) combines the two functions
    discussed previously. Given an input text to classify, the code first creates
    a corresponding prompt (call to `create_prompt`) and then generates a suitable
    reply via a call to the language model (call to `call_llm`). The result is assumed
    to be the class label and is returned to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Now we put it together (**5**). This part of the code is executed when invoking
    the Python module from the command line and uses the functions we’ve introduced.
    The initial `if` condition (**5**) ensures that the following code is only executed
    when invoking the module directly (instead of importing it from a different module).
  prefs: []
  type: TYPE_NORMAL
- en: 'First (**6**), we define command-line arguments. We need only one argument
    here: a path to a .csv file containing the data to classify. We assume that each
    row contains one text document and that the text to classify is contained in the
    `text` column. We parse command-line arguments and make their values available
    in the `args` variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load our input data from disk (**7**). We assume that data is stored
    as a .csv file (comma-separated value): that is, a header line with column names,
    followed by lines containing data (fields are separated by commas, as the name
    suggests). Here, the `pandas` library comes in handy and enables us to load such
    data with a single command. The `df` variable then contains a `pandas` DataFrame
    containing data from the input file. We retrieve the DataFrame `text` column (**8**)
    and apply the previously defined `classify` function to each row (using `pandas`’
    `apply` method). Finally (**9**), we generate and print out aggregate statistics
    (the number of occurrences for each answer generated by the model) and write the
    resulting classifications into a file (result.csv).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6 Running the code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On the book’s companion website, download the file reviews.csv. This file contains
    a small number of movie reviews that we can use for classification. The file contains
    two columns: the review text and the associated sentiment (`neg` for negative
    sentiment and `pos` for positive sentiment). Of course, our goal is to detect
    such sentiments automatically. However, having the ground truth also enables us
    to assess the quality of the classifications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test the code for classification as described next (the following commands
    have been tested on a Linux operating system). Using a terminal, change to the
    directory containing a Python module (listing1.py) with the code in listing [4.1](#code__classification).
    Then, run the following command (replacing `python` with the name of your Python
    interpreter, such as `python3`, if needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we assume that the input file (reviews.csv) is stored in the same repository
    as the code (otherwise, you have to substitute the corresponding path for the
    filename). Typically, the code should not take more than a few seconds to execute
    (slightly more if your connection is unstable, requiring retries). If execution
    succeeds, the only output you will see summarizes the number of labels assigned
    for each of the two possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: After executing the code, you will find a result.csv file in the same repository.
    In addition to the columns of the input file, the result file contains a new `class`
    column. This column contains the classification results (positive and negative).
    Compare the label assigned by our classifier to the ground-truth sentiment. You
    will find that the classification is consistent in a majority of cases. Not bad
    for a few lines of Python code, right?
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.7 Trying out variants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, it is a good idea to play a bit more with the code and the data
    to get a better sense of how it works. For instance, try writing a few movie reviews
    yourself! For which reviews is the classification reliable, and where is it challenging?
    Also try a few variants of the prompt. Which instructions lead to better accuracy,
    and which degrade performance? To take just one example variation, try removing
    the part of the prompt that defines the output format precisely (the line `Answer
    ("Positive"/"Negative")`). Now try running the program with the changed prompt.
    What happens? In all likelihood, you will see more than two labels in your classification
    result (in the output of the program), including, for instance, abbreviations
    (e.g., “P” and “N”) as well as overly detailed answers (e.g., during testing,
    GPT-4o generated replies such as “The sentiment of this review is positive.”).
    In chapter 9, we evaluate the effect of different prompts on the model’s output
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to vary the model used for extraction. How about using one
    of the smaller model versions, such as GPT-3.5 (which is significantly cheaper
    per token processed)? And how about the model configuration? Listing [4.1](#code__classification)
    only uses two parameters (the model name and the message history), both of which
    are required. However, in chapter 3, we saw various configuration parameters that
    can be applied here. For instance, try changing the `temperature` parameter (e.g.,
    setting `temperature` to 0 will give you more deterministic results), or limit
    the length of the desired output! In rare cases, GPT models may generate output
    text that is longer than the desired classification result (which consists of
    a single token). You can avoid that by limiting the output length using the `max_tokens`
    parameter. At the same time, instead of restricting the output format only via
    instructions in the prompt, you may increase the likelihood of the two possible
    results (positive and negative) using the `logit_bias` parameter. We discuss model
    tuning further in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: As yet another variant, try changing the classification task! For instance,
    it is relatively easy to classify using a different set of categories. All it
    takes is changing the instructions in the prompt (outlining all answer options
    as before). By changing a few lines of code, you can even obtain a versatile classification
    tool that enables users to specify the classification task and corresponding classes
    as additional command-line arguments. For example, beyond movie reviews, you can
    use this tool to categorize newspaper articles into one of several topic categories
    or to classify emails as either Urgent or Nonurgent. By now, you are hopefully
    convinced that language models enable text classification with relatively high
    quality and moderate implementation overheads. Time to broaden our scope to different
    tasks!
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Text extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine that, given your expertise in data analysis with language models, you
    recently landed a highly sought-after job at Banana (a popular company producing
    various consumer electronics). The moment you sit down at the desk of your new
    office, emails from enthusiastic students inquiring about summer internships start
    rolling in. Having a summer intern would be nice, but how do you choose the best
    match? Ideally, you would like to compile a table comparing all applicants in
    terms of their GPA, their degree, the name of the company at which they did their
    most recent internship (if any), and so on. But combing through emails to compile
    that table manually seems tedious. Can’t you automate that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course you can. Let’s use language models to analyze emails to extract all
    the relevant factors to choose our lucky summer intern. What we have here is,
    again, a standard problem in text analysis: information extraction! In information
    extraction, we generally extract structured information (e.g., a data table) from
    text. Here, we consider emails (from applicants) as text documents. For each email,
    we want to extract a range of attributes: for example, name, GPA, and (current
    or most recent) degree. For instance, consider the following extract from an email
    from one of the hopeful applicants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Considering the three previously mentioned attributes, we can extract the name
    of the applicant (“Martin”), his GPA (“4.0”), and his degree (“Bachelor of Computer
    Science”). If analyzing emails from multiple applicants, we can represent the
    result as a data table, as shown in table [4.2](#tab__summerinterns). In the next
    section, we discuss how we can accomplish information extraction using language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2 Extracted information about applicants for summer internships
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Name** | **GPA** | **Degree** |'
  prefs: []
  type: TYPE_TB
- en: '| Martin | 4.0 | Bachelor of Computer Science |'
  prefs: []
  type: TYPE_TB
- en: '| Alice | 4.0 | Master of Software Engineering |'
  prefs: []
  type: TYPE_TB
- en: '| Bob | 3.7 | Bachelor of Design |'
  prefs: []
  type: TYPE_TB
- en: 4.3.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Again, we’ll assume that our emails are stored on disk (in a tabular data file
    where each row contains one email). We’ll iterate over emails and use the language
    model to extract all relevant attributes. Instead of hard-coding relevant attributes,
    we will allow users to specify those attributes on the command line (that way,
    you can easily reuse the code if your criteria for summer internships should change).
    As we use language models for text analysis (which are good at interpreting natural
    language), there is no need to specify attributes in any kind of formal language.
    Simply specify the attribute names (or, optionally, a short description in natural
    language), and the language model should be able to figure out what to extract.
    The output of our code will be a tabular data file (in .csv format) that contains
    content similar to table [4.2](#tab__summerinterns): the output table has one
    column for each extracted attribute and one row for each analyzed email.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So how can we extract attributes from a given email? Again, we want to generate
    a prompt that describes the extraction task to the language model. For instance,
    the following prompt should help us extract all relevant attributes from the previous
    email:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Text to analyze'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output format'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt consists of three parts: a task description, including a specification
    of the attributes to extract (**1**); the source text for extraction (**2**);
    and the desired output format, including values to use if the source text does
    not contain any information on specific attributes (**3**). Sending this prompt
    to the language model should yield text that contains the desired extraction results.'
  prefs: []
  type: TYPE_NORMAL
- en: The output from the language model is, first, a text string. Ultimately, we
    want to output a structured data table. That means we still need some postprocessing
    to extract values for all relevant attributes (name, GPA, and degree) from the
    output text. Figure [4.2](#fig__textExtractionOverview) illustrates the steps
    of the extraction process (for a single text document).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F02_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 For each email, we generate a prompt that contains the email and
    a description of the extraction task. This description references the attributes
    to extract specified by the user. Given the prompt as input, the language model
    generates an answer text containing extracted attribute values. Via postprocessing,
    we extract those values from the raw answer text.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.3.2 Generating prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We want to generate prompts that instantiate the following prompt template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Text to analyze'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output format'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt template contains a task description (**1**), the source text for
    extraction (**2**), and a specification of the output format (**3**). Note that
    this prompt now contains two placeholders (the template we used in the previous
    section contained only a single placeholder): the list of attributes to extract
    and the source text for extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will generate prompts using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generates a task description'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Adds source text'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds a description of the output format'
  prefs: []
  type: TYPE_NORMAL
- en: This function takes as input the text to analyze (which we certainly want to
    include in the prompt) along with a list of attributes we want to extract. After
    generating the task description (**1**), including the list of attributes to extract,
    the function adds the source text (**2**), as well as a specification of the desired
    output format (**3**). The prompt concatenates these parts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Postprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the previous project (text classification), our prompt has changed
    to adapt to the new task (text extraction). Even with a different prompt, we can
    still reuse the same function as in the last section to obtain an answer from
    the language model. On the other hand, we need to do a little more work than before
    to process the raw answer using the language model. For classification, we directly
    used the reply from the language model as the final result. In our current scenario
    (text extraction), we generally will want to extract values for multiple attributes
    for a single input text. As the output text from the language model contains values
    for all extracted attributes, we need to extract values for specific attributes
    from the raw answer text.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we might receive the following raw answer text from the language
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To extract values for each attribute, we can split the raw text using pipe symbols
    as field delimiters (while removing the first and last pipe symbols in the answer).
    Ideally, we want to expand our scope beyond the specific use case we are currently
    considering (extracting information on applicants from emails). In some scenarios,
    we may extract multiple rows from the same text (imagine a scenario where multiple
    applicants together submit a group email—but that’s admittedly a less likely case).
    To support such use cases, we may also have to split the raw answer into text
    associated with different rows. To do that, we can use the newline symbol as row
    delimiters (as rows are split by newline symbols).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do all these things with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Extracts table data'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Splits by row'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Splits by field'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input to this function is raw text produced by the language model for a
    single text document. The output is a list of rows (where each result row is,
    again, represented as a list). To get from input to output, we first need to extract
    the part of the raw answer that contains the actual table data (**1**). Answers
    generated by GPT-4o may contain a preamble or additional explanations beyond the
    extracted table (e.g., “Sure, here is the table you wanted: ...”). We need to
    separate the data we are interested in. Fortunately, that’s easy as long as GPT-4o
    is following our instructions (which, typically, it does): the data we’re interested
    in should be contained between two markers (`<BeginTable>` and `<EndTable>`).
    Hence, the regular expression `''<BeginTable>(.*)<EndTable>''` exactly matches
    the part of the output we’re interested in. We retrieve it using Python’s `re.findall`
    function, which, given a string and regular expression as input, returns a list
    of matching substrings. We use the `re.DOTALL` flag to ensure that the dot within
    the regular expression matches all characters and newlines (because the table
    may contain multiple lines). From the resulting matches, we take the first one.
    Note that this implicitly assumes at least one table in GPT’s output. Although
    that is typically the case, think about how to make the function more robust toward
    answers from the language model that do not comply with our instructions in the
    prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Having extracted the table data in a text representation, we first split it
    into data associated with specific rows (**2**) and data associated with specific
    cells (**3**). After some cleanup (the Python function `strip` removes whitespace),
    we add the resulting cell values to our result list. This list of rows (where
    each row is, again, represented as a list) is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 End-to-end extraction code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [4.2](#code__extraction) shows the completed Python code. The code structure
    is similar to listing [4.1](#code__classification), and some of the functions
    are shared among the two listings (rather than omitting repeated functions, this
    book aims to provide you with self-contained code so you don’t have to piece together
    code from multiple pages). In particular, the code uses the same libraries as
    before (**1**) and invokes the language model via the same function (**3**). You
    will recognize the function for creating prompts (**2**) and the one for postprocessing
    raw output from the language model (**4**), introduced earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Extracting user-defined attributes from text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports relevant libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generates prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Invokes the language model'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Postprocesses model output'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Extracts data tables from text'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Extracts information and writes the result'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Iterates over text'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main function (**6**) reads two input parameters from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: A path to a .csv file containing the text to analyze
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of attributes to extract, separated by pipe symbols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After opening the input file (using the `pandas` library), we iterate over all
    input text documents (**7**). Note that we expect the input text in the `text`
    column in the input file. To perform the actual extraction, we use the `extract_rows`
    function (**5**). Given input text and a list of attributes to extract, this function
    generates a suitable prompt, obtains a raw answer from the language model, and
    postprocesses the raw answer to get structured output (which it returns). After
    iterating over the input text (**7**), we store the final result in a file named
    result.csv (this file will be overwritten if it already exists).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Trying it out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find the code from listing [4.2](#code__extraction) as listing2.py on
    the companion website. You can also download the file biographies.csv there, giving
    you a small data set to experiment on with your extractor (this is a bit different
    from our motivating scenario, but publicly available data on email applications
    is sparse). This file contains biographies of five famous people, as well as the
    associated names, with one person per row. Change into the directory containing
    listing2.py (as well as the data), and run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter is the data set (if it is not in the same directory, adapt
    the path accordingly). The second parameter is the list of attributes to extract.
    We use the pipe symbol again to separate attributes. Note that we only identify
    attributes via their names; no need to refer to predefined categories. The language
    model can understand attribute semantics based on the name alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the code (which should not take more than a minute), you will
    find the results stored in a file named result.csv. For example, executing the
    code on the sample data could yield the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if you execute the same code on the same data, you may see slight variations
    (due to randomization when generating model output). Each row in that file (besides
    the header row) represents an extraction. We are extracting name, birth city,
    and birth date. Hence, we expect one extracted row per biography (and that is
    what happens in our sample run). Note that there are missing values: for Ann E.
    Wojcicki, the biography snippet does not contain the city of birth. The language
    model reacts appropriately and inserts a corresponding placeholder (“<N/A>”),
    instead of a concrete value.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You’re a few weeks in at your new job at Banana. The job is great, but there
    is one problem: your inbox keeps overflowing with emails! It’s not only applications
    from hopeful summer interns (we took care of that in the last section). Those
    emails cover a variety of different topics, and making sure to read all relevant
    emails takes a lot of your time. Looking closer, you notice that many of the emails
    are redundant. For example, you observe that many emails try to draw attention
    to the same company events. For a moment, you ponder using your code for text
    classification (discussed in section 4.2) to categorize emails into several categories
    (e.g., associated with specific company events). After that, you can read only
    a few emails from each category to have a full overview of what’s happening at
    Banana. Alas, there is one problem: it is hard to come up with and maintain an
    exhaustive list of topics because those topics will keep changing over the course
    of your employment. Instead, it would be nice to automatically group different
    emails that are somewhat similar because, for instance, they discuss the same
    event. That way, you wouldn’t have to come up with a list of topics in advance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we want is to group similar emails into clusters. That’s yet another classical
    text-processing problem: text clustering. If you want to bring related text documents
    together without knowing the set of categories beforehand, clustering methods
    are probably the way to go! In this section, we will see how to use language models
    for text clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Clustering is a classical approach in computer science. Clustering methods
    predate language models and advanced text analysis by quite a bit. However, traditionally,
    clustering focuses on elements that are expressed as vectors. We want to bring
    together (in the same cluster) vectors that have a small distance from each other
    (and, of course, there are various distance metrics that we can apply for vectors).
    However, that’s not really the case here: in our scenario, we want to assign similar
    emails (or, in general, similar text documents) to the same cluster. So how do
    we get from documents to vectors?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer is *embeddings*. An embedding represents a text document as a (typically
    high-dimensional) vector. That’s exactly what we’re looking for! Of course, this
    approach only makes sense if we map text documents to vectors that have something
    meaningful to say about the content of the documents. Ideally, we want documents
    with similar vectors (i.e., vectors with a small distance according to our preferred
    distance metric) to also have similar content. This means we cannot use naive
    methods to map text documents to vectors. Instead, we need an approach that considers
    the semantics of the text and takes them into account when generating a vector
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, language models can help! Providers like OpenAI offer language
    models that take text as input and produce embedding vectors as output. So, having
    a collection of text documents to cluster, we can calculate embedding vectors
    for all of them and apply any classical clustering algorithm to the resulting
    vectors. Figure [4.3](#fig__clusteringOverview) illustrates this process. Next,
    we discuss how to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F03_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Clustering emails. We first calculate embedding vectors for all emails.
    Then we cluster those vectors to assign emails with similar content to the same
    cluster.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4.2 Calculating embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the examples discussed so far, we have used OpenAI’s chat completions endpoint.
    For clustering, we will use OpenAI’s embedding endpoint instead. The goal of embedding
    is to create a vector that compresses the semantics of a text. Different models
    can be used to calculate embeddings. The dimension of the vector depends on the
    model used. For the following code, we will use the `text-embedding-ada-002` model.
    You can try substituting other models for this one (you can find a list of OpenAI
    models for calculating embeddings at [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings))
    to compare the output quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we can generate embeddings for text documents as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here you see an extract from the corresponding response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Embedding vector'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Model that generated embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Usage statistics'
  prefs: []
  type: TYPE_NORMAL
- en: The extract only shows values for the first few vector dimensions (**1**) (whereas
    the full vector has over 1,000 dimensions). Besides the embedding vector, the
    response contains the model name (**2**) and usage statistics (**3**). Unlike
    earlier, usage statistics only refer to the number of tokens in the prompt (which
    is also the total number of tokens processed). Unlike text completion, the language
    model only reads tokens but does not generate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most relevant part for us is, of course, the embedding vector itself. You
    can access that embedding vector via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the time, invoking the language model once should provide you with
    the embedding you are searching for. Of course, when calculating embedding vectors
    for a large number of emails, we may run into problems (i.e., failed connection
    attempts) every once in a while. This is why the final version of our embedding
    function again contains a retry mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Given a text as input, we try up to three times to get a corresponding embedding
    vector (increasing the delay between retries after each failed attempt). This
    is the function we will use.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Clustering vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To cluster vectors (representing documents), we will use the k-means clustering
    algorithm. K-means is a very popular clustering algorithm that works by iteratively
    refining the mapping from vectors to clusters. Unlike other clustering algorithms,
    the algorithm requires you to specify the number of clusters in advance. In our
    example scenario, that means choosing how fine-grained the partitioning of emails
    by their content should be.
  prefs: []
  type: TYPE_NORMAL
- en: How does the k-means algorithm work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The k-means algorithm takes as input a set of elements to cluster and a target
    number of clusters. It works by iteratively refining the mapping from elements
    to clusters until a termination criterion (e.g., a maximum number of iterations
    or minimal changes in cluster assignments between consecutive iterations) is met.
    The k-means algorithm associates each cluster with a vector (representing the
    center of that cluster). In each iteration, it assigns each vector to the cluster
    with the nearest center. Then, it recalculates the vectors associated with clusters
    (by averaging over the vectors of all elements currently assigned to the cluster).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the k-means implementation in the `scikit-learn` library.
    Follow the instructions in the first section of this chapter to ensure that this
    library is installed (import clustering methods via `from sklearn.cluster import
    KMeans`). After importing the library, we can invoke the k-means implementation
    with the following (concise) piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The function takes a list of embedding vectors and the number of target clusters
    as input and then clusters those vectors using the k-means implementation. The
    result of clustering is labels associated with each embedding vector. Those labels
    indicate the ID of the associated cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 End-to-end code for text clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the complete code for clustering text documents
    via embedding vectors. You will recognize the functions for calculating embedding
    vectors (**1**) and clustering them (**2**).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Clustering text documents using language models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calculates embedding vectors'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Clusters embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Reads text and writes out clusters'
  prefs: []
  type: TYPE_NORMAL
- en: The main function of listing [4.3](#code__clustering) (**3**) reads data from
    a file on disk. Again, we assume that data is contained in a .csv file and focus
    on the `text` column. First, we iterate over text documents and generate corresponding
    embeddings (by invoking the `get_embedding` function, discussed previously). Then,
    we cluster embedding vectors via the `get_kmeans` function. The cluster IDs become
    an additional column in the result table written to disk.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.5 Trying it out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Time to try clustering via embedding vectors! You can find the code from listing
    [4.3](#code__clustering) on the book’s companion website (listing3.py), as well
    as a suitable data set (textmix.csv). This data set contains a mix of text snippets
    from two sources: a collection of poems and a repository of emails. We’ll try
    to separate the two via clustering: we expect emails and poems to be assigned
    to different clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change into the directory containing the code and data, and run the following
    command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, textmix.csv is the name of the input file, and 2 is the number of target
    clusters (in this specific case, two seems like a reasonable choice, whereas determining
    the right number of clusters can be more difficult in other scenarios). The result
    will be stored in the file result.csv. It contains all the columns from the input
    file, as well as an additional column with the cluster ID (because we only use
    two clusters, this ID is either 0 or 1). Running the command, you will likely
    see a result that places emails in one cluster while putting poems in the other.
  prefs: []
  type: TYPE_NORMAL
- en: You may want to try different models to see differences in run time and result
    quality. You can also try different input text and vary the number of clusters.
    Besides that, you may want to implement some of the other use cases for embedding
    vectors, which are mentioned at the beginning of this section. For instance, how
    about implementing a retrieval interface that maps a natural language statement
    to the most closely related document (by comparing the embedding vectors of questions
    and documents)?
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.6 Other use cases for embedding vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have used vectors to identify similar documents via clustering. But
    this is not the only use case for embedding vectors! To name just a few examples,
    embedding vectors are often used to facilitate the retrieval of text documents
    related to a natural language question. Here, we compare an embedding vector associated
    with the question to embedding vectors associated with documents. Documents with
    similar vectors are more likely to be useful in answering the question.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we hope that the embedding vectors for the question “What is a
    Transformer model?” and the text “The Transformer is a neural network architecture,
    often used for language models” are similar due to related topics. If so, we can
    identify the document most relevant to the question by comparing embedding vectors.
    More precisely, we calculate embedding vectors once for each document that may
    be useful to answer questions. Then, whenever a new question is received, we calculate
    the associated embedding vector and retrieve documents with similar embedding
    vectors. We can then generate an answer based on those documents.
  prefs: []
  type: TYPE_NORMAL
- en: Another use case for embedding vectors is outlier detection. To identify text
    documents from a set that are strikingly different from other documents in the
    same set, we can compare their embedding vectors. Again, we only need to calculate
    embedding vectors once for each document. In doing so, we avoid having to use
    language models to compare documents. Instead, we simply compare embedding vectors
    (which is very fast).
  prefs: []
  type: TYPE_NORMAL
- en: In summary, although we have focused on clustering, there are many use cases
    for embedding vectors. This makes it worthwhile to learn how to generate and use
    them!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can apply language models directly to analyze text data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts typically contain text to analyze, along with instructions. Instructions
    describe the task to solve as well as the output format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use chat completion for classification, extraction, and question answering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw model output may need postprocessing to change the format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models can transform a text into embedding vectors. You can create
    embedding vectors via the embedding endpoint. Comparing embedding vectors is relatively
    efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use embeddings for clustering, retrieval, and outlier detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.6 References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Katz, D. M., Bommarito, M. J., Gao, S., et al. (2024). GPT-4 Passes the Bar
    Exam. *Philosophical Transactions of the Royal Society A: Mathematical, Physical
    and Engineering Sciences 382*(2270), 1–17.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
