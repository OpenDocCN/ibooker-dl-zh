<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-conv2d3dtranspose">10 Convolutions in neural networks</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">The graphical and algebraic view of neural networks</li>
<li class="co-summary-bullet">Two-dimensional and three-dimensional convolution with custom weights</li>
<li class="co-summary-bullet">Adding convolution layers to a neural network</li>
</ul>
<p class="body"><a id="marker-343"/>Image analysis typically involves identifying <i class="fm-italics">local</i> patterns. For instance, to do face recognition, we need to analyze local patterns of neighboring pixels corresponding to eyes, noses, and ears. The subject of the photograph may be standing on a beach in front of the ocean, but the big picture involving sand and water is irrelevant.</p>
<p class="body">Convolution is a specialized operation that examines local patterns in an input signal. These operators are typically used to analyze images: that is, the input is a <span class="math">2</span>D array of pixels. To illustrate this, we examine a few examples of special-purpose convolution operations that detect the edges, corners, and average illumination in a small neighborhood of pixels from an image. Once we have detected such local properties, we can combine them and recognize higher-level patterns like ears, noses, and eyes. We can combine those in turn to detect still higher-level structures like faces. The system naturally lends itself to multilayer convolutional neural</p>
<p class="body">networks—the lowest layers(closest to the input) detect edges and corners, and the next layers detect ears, eyes, noses, and so forth.</p>
<p class="body">In section <a class="url" href="../Text/08.xhtml#sec-linear-layer">8.3</a>, we discussed the <i class="fm-italics">linear</i> neural network layer (aka <i class="fm-italics">fully connected</i> layer). There, every output is connected to <i class="fm-italics">all</i> inputs. This means an output is derived by taking a weighted linear combination of <i class="fm-italics">all</i> input values. In other words, the output is derived from a <i class="fm-italics">global</i> view of the input. Convolution layers are different. These are characterized by:<a id="marker-344"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Local connections</i>—Only a small subset of neighboring input values are connected to one output value. Thus, each output is a weighted linear combination of only a small set of <i class="fm-italics">adjacent</i> input values. As a consequence, only local patterns in the input are captured.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Shared weights</i>—The same weights are slid over the entire input. Consequently,</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">The number of weights is drastically reduced. Since convolution is typically used on images where the input size is large number of pixels), fully connected layers are prohibitively expensive. Convolution repeats a (usually small) number of weights across the input, thereby keeping the number of weights manageable.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The nature of the local pattern extracted is fixed all over the input. If the convolution is an edge detector, it extracts edges all over the input. We cannot have an edge detector at one region of the input and a corner detector at another region, for instance. Of course, in a multilayered network, we can use different convolution layers to capture different local patterns. In particular, successive layers can capture local patterns in local patterns of the input, and so on, thereby capturing increasingly complex and increasingly global patterns at higher layers of the network.</p>
</li>
</ul>
</li>
</ul>
<p class="body">The exact local pattern captured depends on the weights of the convolution operator. We don’t know exactly what local patterns of the input to capture to recognize a specific higher-level structure of interest (such as a face). This means we do not want to <i class="fm-italics">specify</i> the weights of the convolutions. The whole point of neural networks is to avoid such tailored feature engineering. Rather, we want to <i class="fm-italics">learn</i>—through the process of <i class="fm-italics">training</i> described in chapter <a class="url" href="../Text/08.xhtml#ch-training-neural-networks">8</a>—the weights of the convolution layers. Losses can be backpropagated through convolution just as they are through fully connected (FC) layers.</p>
<p class="body">Just like FC layers, convolution layers can be expressed as matrix-vector multiplications. The structure of the weight matrix is a special case of equation <a class="url" href="../Text/08.xhtml#eq-MLP-weight-matrix">8.8</a>, but it is a matrix all the same. Consequently, the forward propagation equation <a class="url" href="../Text/08.xhtml#eq-linlayer-forwardprop">8.7</a> and backpropagation equations <a class="url" href="../Text/08.xhtml#eq-auxvar-vector">8.31</a> and <a class="url" href="../Text/08.xhtml#eq-partialderiv-loss-wt-vector">8.33</a> are still applicable. Forward propagation and backpropagation (training) through convolution proceed exactly as they do with FC layers.</p>
<p class="body">Since the convolution is learned—as opposed to specified—in a neural network, there is no telling what local patterns such layers will learn to extract (although, in practice, the initial layers often learn to recognize edges and corners). All we know is that each output in a given layer is derived from only a <i class="fm-italics">small subset of spatially adjoint</i> input values from previous layers. The final output is derived from a hierarchical local examination of the input.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for chapter 10, runnable via Jupyter Notebook, is available at our public GitHub repository at <a class="url" href="http://mng.bz/M2lW">http://mng.bz/M2lW</a>.</p>
<h2 class="fm-head" id="sec-conv1d">10.1 One-dimensional convolution: Graphical and algebraical view</h2>
<p class="body"><a id="marker-345"/>As always, we examine the process of convolution with a set of examples. We examine convolutions in one, two, and three dimensions, but we start with one dimension for ease of understanding.</p>
<p class="body">The best way to visualize <span class="math">1</span>D convolution is to imagine a stretched, straightened rope (the input array) over which a measuring ruler (the kernel) is sliding.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In figures <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <a class="url" href="#fig-conv1d-smoothing-stride2">10.2</a>, and <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a>, the ruler kernel) is shown as shaded boxes, while the rope (input array) is shown as a sequence of white boxes. Successive steps in the figure represent successive positions (aka slide stops) of the sliding ruler. Notice that the shaded portion occupies a different position in each step.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Rulers in successive positions during sliding can overlap. They overlap by varying amounts in figures <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <a class="url" href="#fig-conv1d-smoothing-stride2">10.2</a>, and <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The rope and the ruler are discrete <span class="math">1</span>D arrays in reality. At each slide stop, the ruler array elements rest on a subset of rope array elements.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We multiply each input array element by the kernel element resting on it and sum the products. This is equivalent to taking a weighted sum of the input (rope) elements that fall under the current position of the kernel (ruler), with the kernel elements serving as weights. This weighted sum is emitted as a single output element. One output element results from each slide stop of the tile. As the ruler slides over the entire rope, left to right, a <span class="math">1</span>D output array is generated.</p>
</li>
</ul>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="1223" id="fig-conv1d-smoothing-stride1" src="../../OEBPS/Images/CH10_F01_Chaudhury.png" width="1021"/></p>
<p class="figurecaption">Figure 10.1 <span class="math">1</span>D convolution with a <i class="fm-italics">local averaging kernel</i> of size <span class="math">3</span>, stride <span class="math">1</span>, and valid padding on the input array of size <span class="math">7</span><a id="marker-346"/></p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="816" id="fig-conv1d-smoothing-stride2" src="../../OEBPS/Images/CH10_F02_Chaudhury.png" width="885"/></p>
<p class="figurecaption">Figure 10.2 <span class="math">1</span>D convolution with a <i class="fm-italics">local averaging kernel</i> of size <span class="math">3</span>, stride <span class="math">2</span>, and valid padding<a id="marker-347"/></p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="891" id="fig-conv1d-edge-detection-stride1" src="../../OEBPS/Images/CH10_F03_Chaudhury.png" width="1086"/></p>
<p class="figurecaption">Figure 10.3 <span class="math">1</span>D convolution with an <i class="fm-italics">edge-detection kernel</i> of size <span class="math">2</span>, stride <span class="math">1</span>, valid padding. Not all slide stops (that is, steps) are shown.</p>
</div>
<p class="body">The following entities are defined for <span class="math">1</span>D convolution:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Input</i>—A one-dimensional array. We typically use the symbol <i class="timesitalic">n</i> to represent input array length in <span class="math">1</span>D convolution. In figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <span class="math"><i class="fm-italics">n</i> = 7</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Output</i>—A one-dimensional array. We typically use the symbol <i class="timesitalic">o</i> to represent the output array length in <span class="math">1</span>D convolution. In figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <span class="math"><i class="fm-italics">o</i> = 5</span>. Section <a class="url" href="#sec-conv1d-out-size">10.2</a> shows how to calculate the output size from the independent parameters.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Kernel</i>—A small array of weights whose size is a parameter of the convolution. We typically use the symbol <i class="timesitalic">k</i> to represent the kernel size in <span class="math">1</span>D convolution. In figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <span class="math"><i class="fm-italics">k</i> = 3</span>; in figure <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a>, <span class="math"><i class="fm-italics">k</i> = 2</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Stride</i>—The number of input elements over which the kernel slides after completing a single step. We typically use the symbol <i class="timesitalic">s</i> to represent the stride in <span class="math">1</span>D convolution. This is a parameter of the convolution. In figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <span class="math"><i class="fm-italics">s</i> = 1</span>; in figure <a class="url" href="#fig-conv1d-smoothing-stride2">10.2</a>, stride is <span class="math">2</span>. A stride of <span class="math">1</span> means there is a slide stop at each successive element of the input. So, the output has roughly the same number of elements as the input (they may not be exactly equal because of padding, explained next). A stride of <span class="math">2</span> means there is a slide stop at every <i class="fm-italics">other</i> input element. So, the output size is roughly half of the input size. A stride of <span class="math">3</span> means the output size is roughly one-third the input size.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Padding</i>—As the kernel slides toward the extremity of the input array, parts of it may fall outside the input array. In other words, part of the kernel falls over ghost input elements. Figure <a class="url" href="#fig-conv1d-smoothing-stride2-zeropad">10.4</a> shows such a situation: the ghost input array elements are shown with dashed boxes. There are multiple strategies to deal with this:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Valid padding</i>—We stop sliding whenever any element of the kernel falls outside the input array. No ghost input elements are involved; the <i class="fm-italics">entire</i> kernel always falls on valid input elements (hence the name <i class="fm-italics">valid</i>). Note that this implies we will have fewer outputs than inputs. If we try to generate an output corresponding to, say, the last input element, all but the first kernel element will fall outside the input on ghost elements. So, we have to stop when the right-most kernel element falls on the right-most input element (see figures <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <a class="url" href="#fig-conv1d-smoothing-stride2">10.2</a>, and <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a>). At this point, the left-most kernel element falls on the <span class="math">(<i class="fm-italics">N</i> − <i class="fm-italics">k</i>)</span>th input element. We do not generate output for the last <i class="timesitalic">k</i> inputs. Hence, even with a stride of <span class="math">1</span> for valid padding, the output is slightly smaller than the input.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Same (zero) padding</i>—Here, we do not want to stop early. If the stride is <span class="math">1</span>, the output size matches the input size (hence the name <i class="fm-italics">same</i>). We continue to slide the kernel until its left end falls on the right-most input. At this point, all but the left-most kernel element is falling on ghost input elements. We pretend these ghost input elements have a value of <span class="math">0</span> (<i class="fm-italics">zero padding</i>).</p>
</li>
</ul>
</li>
</ul>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="1081" id="fig-conv1d-smoothing-stride2-zeropad" src="../../OEBPS/Images/CH10_F04_Chaudhury.png" width="1020"/></p>
<p class="figurecaption">Figure 10.4 <span class="math">1</span>D convolution with a <i class="fm-italics">local averaging kernel</i> of size <span class="math">3</span>, stride <span class="math">2</span>, and zero padding</p>
</div>
<p class="body"><a id="marker-348"/>Let’s denote the input array’s domain by <i class="timesitalic">S</i>. It’s a <span class="math">1</span>D grid:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S</i> = [0, <i class="fm-italics">W</i> − 1]</span></p>
<p class="body">Every point in <i class="timesitalic">S</i> is associated with a value <i class="timesitalic">X<sub class="fm-subscript">x</sub></i>. Together, these values form the input <i class="timesitalic">X</i>. On this grid of input points, we define a subgrid <i class="timesitalic">S<sub class="fm-subscript">o</sub></i> of output points. <i class="timesitalic">S<sub class="fm-subscript">o</sub></i> is obtained from <i class="timesitalic">S</i> by applying stride-based stepping on the input. Assuming <span class="math"><i class="fm-italics">s</i> = [<i class="fm-italics">s<sub class="fm-subscript">W</sub></i>]</span> denotes the stride, the first slide stop has the top-left corner of the rope at <span class="math">(<i class="fm-italics">x</i> = 0)</span>. The next slide stop is at <span class="math">(<i class="fm-italics">x</i> = <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)</span>, and the next is <span class="math">(<i class="fm-italics">x</i> = 2<i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)</span>, and so on. When we reach the right end, we stop. Overall, the output grid consists of the slide-stop points at which the top-left corner of the kernel (ruler) rests as it sweeps over the input volume: that is, <span class="math"><i class="fm-italics">S<sub class="fm-subscript">o</sub></i> = {(<i class="fm-italics">x</i> = 0), (<i class="fm-italics">x</i> = <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)⋯,}</span>. There is an output for each point in <i class="timesitalic">S<sub class="fm-subscript">o</sub></i>.</p>
<p class="body">Equation <a class="url" href="#eq-1dconv-output">10.1</a> shows how a single output value is generated in <span class="math">1</span>D convolution. <i class="timesitalic">X</i> denotes input, <i class="timesitalic">Y</i> denotes output, and <i class="timesitalic">W</i> denote kernel weights:<a id="marker-349"/></p><!--<p class="Body"><span class="times">$$Y_{x} = \sum_{j=0}^{k_{W}} X_{x+j} W_{ j}  \;\;\;\;\;\; \forall \left(x\right) \in S_{o}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_10-01.png" width="213"/></p>
</div>
<p class="fm-equation-caption">Equation 10.1 <span class="calibre" id="eq-1dconv-output"/></p>
<p class="body">Note that when the kernel of dimension <i class="timesitalic">k<sub class="fm-subscript">W</sub></i> (ruler) has its origin on <i class="timesitalic">x</i>, it covers all input pixels in the domain <span class="math">[<i class="fm-italics">x</i>..(<i class="fm-italics">x</i> + <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>)]</span>. These are the pixels participating in equation <a class="url" href="#eq-1dconv-output">10.1</a>. Each of these input pixels is multiplied by the kernel element covering it. Match equation <a class="url" href="#eq-1dconv-output">10.1</a> with figures <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, <a class="url" href="#fig-conv1d-smoothing-stride2">10.2</a>, and <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a>.</p>
<h3 class="fm-head1" id="sec-1d_local_smoothing">10.1.1 Curve smoothing via 1D convolution</h3>
<p class="body">In this section, we look at how to perform local averaging via convolution, from a physical and algebraic viewpoint, to get a comprehensive understanding. The <span class="math">1</span>D kernel with weight vector <!--<span class="times">$\vec{w}=\begin{bmatrix}\frac{1}{3} &amp;\frac{1}{3} &amp;\frac{1}{3}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="28" src="../../OEBPS/Images/eq_10-01-a2.png" width="115"/></span> (shown in figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>) essentially takes the moving average of successive sets of three input values. As such, it is a local averaging (aka smoothing) operator. This becomes apparent if we examine the plots of the raw input vector with regard to the input vector convolved with the kernel (figure <a class="url" href="#fig-1d-smoothing-graph">10.5</a>). The input (solid line) weaves up and down, while the output is a smooth curve (dashed line) through the mean position of the input. In general, the output produced by convolving by a kernel with all equal weights (the weights must be normalized, meaning the sum of the weights is <span class="math">1</span>) is a smoothed (locally averaged) version of the input. Why do we want to smooth an input vector? Because it captures the broad trend in the input data while eliminating short-term fluctuations (often caused by noise). If you are familiar with Fourier transforms and frequency domains, you can see that this is essentially a low-pass filter, eliminating short-term, high-frequency noise and capturing the longer-term, low-frequency variation in the input data array.<a id="marker-350"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="579" id="fig-1d-smoothing-graph" src="../../OEBPS/Images/CH10_F05_Chaudhury.png" width="791"/></p>
<p class="figurecaption">Figure 10.5 Graph of the input array solid) and output array (dashed) from figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>. Note that the averaged) version of the input. Such local soothing captures the low-frequency (long-term) broad trend of the function by eliminating high-frequency (short-term) noise.</p>
</div>
<h3 class="fm-head1" id="curve-edge-detection-via-1d-convolution">10.1.2 Curve edge detection via 1D convolution</h3>
<p class="body"><a id="marker-363"/>As mentioned earlier, a convolution’s physical effect on an input array radically changes with the weights of the convolution kernel. Now let’s examine a very different kernel that detects edges in the input data.</p>
<p class="body">An <i class="fm-italics">edge</i> is defined as a sharp change in the values in an input array. For instance, if two successive elements in the input array have a large absolute difference in values, that is an edge. If we graph the input array (that is, plot the input array values in the <i class="timesitalic">y</i> axis against the array indices), an edge will appear in the graph. For instance, consider the input array in figure <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a> (graphed in figure 10.6). At indices <span class="math">0</span> to <span class="math">3</span>, we have values in the neighborhood of <span class="math">10</span>. At index <span class="math">4</span>, the value jumps to <span class="math">51</span>. We say there is an edge between indices <span class="math">3</span> and <span class="math">4</span>. The values then remain in the neighborhood of <span class="math">50</span> at indices <span class="math">4</span> to <span class="math">7</span>. Then they jump back to the neighborhood of <span class="math">10</span> in the remaining indices. We say there is another edge between indices <span class="math">7</span> and <span class="math">8</span>. The convolution we examine here will emit a high response (output value) exactly at the indices of the jump—<span class="math">3</span> and <span class="math">7</span>—while emitting a low response at other indices. This is an edge-detection convolution (filter). Why do we want to detect edges? Because edges are important for understanding images. Locations at which the signal changes rapidly provide more semantic clues than flat uniform regions. Experiments on the human visual cortex have established that humans pay more attention to locations where color or shade changes rapidly than to flat regions.<a id="marker-351"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="642" id="fig-1d-edge-detection-graph" src="../../OEBPS/Images/CH10_F06_Chaudhury.png" width="891"/></p>
<p class="figurecaption">Figure 10.6 Graph of the input array solid) and output array (dashed) from figure <a class="url" href="#fig-conv1d-edge-detection-stride1">10.3</a>. The output the input. Edges provide vital clues for understanding the signal.</p>
</div>
<h3 class="fm-head1" id="sec-conv1d-mat">10.1.3 One-dimensional convolution as matrix multiplication</h3>
<p class="body">Algebraically, the convolution with a kernel of size <span class="math">3</span>, stride <span class="math">1</span>, and valid padding can be shown as follows. Let the input array be</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = [<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">4</sub>  …  <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>–3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>–2</sub>   <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>-1</sub>]</span></p>
<p class="body">The convolving kernel is a matrix of weights of size <span class="math">3</span>; let’s denote it as</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> = [<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>   <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>   <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>]</span></p>
<p class="body"><a id="marker-352"/>As shown in figure <a class="url" href="#fig-conv1d-smoothing-stride1">10.1</a>, in step <span class="math">0</span> of the convolution, we place this kernel on the <span class="math">0</span>th element of the input <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub></span>. Thus, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span> falls on <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> falls on <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span>, and <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">2</sub></span> falls on <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span>:</p>
<p class="fm-equation"><span class="math">[<b class="fm-bold">x<sub class="fm-subscript">0</sub>   x<sub class="fm-subscript">1</sub>   x<sub class="fm-subscript">2</sub></b>   <i class="fm-italics">x</i><sub class="fm-subscript">3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">4</sub>  …  <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>–3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>–2</sub>   <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>-1</sub>]</span>,</p>
<p class="body">where the bold typeface identifies the input elements aligned with kernel weights. We multiply elements on corresponding positions and sum them, yielding the <span class="math">0</span>th element of the output <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript">0</sub> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><i class="fm-italics">x</i><sub class="fm-subscript">0</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span>. Then we shift the kernel by one position (assuming the stride is <span class="math">1</span>; if the stride were <span class="math">2</span>, we would move the kernel two positions, and so on). So <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub></span> falls on <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">1</sub></span>, <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> falls on <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span>, and <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">2</sub></span> falls on <span class="math"><i class="fm-italics">x</i><sub class="fm-subscript">3</sub></span>:</p>
<p class="fm-equation"><span class="math">[<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>   <b class="fm-bold">x<sub class="fm-subscript">1</sub>   x<sub class="fm-subscript">2</sub>   x<sub class="fm-subscript">3</sub>   x<sub class="fm-subscript">4</sub></b>  …  <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>–3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>–2</sub>   <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>-1</sub>]</span></p>
<p class="body">Again, we multiply elements at corresponding positions and sum them, yielding the first element of the output <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript">1</sub> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><i class="fm-italics">x</i><sub class="fm-subscript">1</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub></span>. Similarly, in the next step, we right-shift the kernel one more time:</p>
<p class="fm-equation"><span class="math">[<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>   <b class="fm-bold">x<sub class="fm-subscript">2</sub>   x<sub class="fm-subscript">3</sub>   x<sub class="fm-subscript">4</sub></b>  …  <i class="fm-italics">x<sub class="fm-subscript">n</sub></i>]</span></p>
<p class="body">The corresponding output is <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript">2</sub> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><i class="fm-italics">x</i><sub class="fm-subscript">2</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript">3</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript">4</sub></span>. Overall, a stride <span class="math">1</span>, valid padding convolution of a vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> with a weight kernel <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> yields the output</p><!--<p class="Body"><span class="times">$$\vec{y} = \vec{w} \circledast
\vec{x} =
\begin{bmatrix}
&amp;w_{0} x_{0} + w_{1} x_{1} + w_{2} x_{2}\\
&amp;w_{0} x_{1} + w_{1} x_{2} + w_{2} x_{2}\\
&amp;\vdots\\
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_10-01-b.png" width="266"/></p>
</div>
<p class="body">Can you see what is happening? We are effectively taking linear combinations (see section <a class="url" href="02.xhtml#sec-lin-dep">2.9</a>) of successive sets of <span class="math"><i class="fm-italics">kernel</i>_<i class="fm-italics">size</i></span> here, <span class="math">3</span>) input elements. In other words, the output is a <i class="fm-italics">moving weighted local sum</i> of the input array elements. Depending on the actual weights, we are extracting local properties of the input.</p>
<p class="body">For <i class="fm-italics">valid</i> padding, the last output is yielded by</p>
<p class="fm-equation"><span class="math">[<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">4</sub>  …  <b class="fm-bold">x<sub class="fm-subscript">n–3</sub>   x<sub class="fm-subscript">n–2</sub>   x<sub class="fm-subscript">n-1</sub></b>]</span></p>
<p class="body">which generates the output</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">y</i><sub class="fm-subscript"><i class="fm-italics1">n</i> − 3</sub> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i> − 3</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i> − 2</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub><i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i> − 1</sub></span></p>
<p class="body">For the <i class="fm-italics">same</i> zero padding, the last output is yielded by</p>
<p class="fm-equation"><span class="math">[<i class="fm-italics">x</i><sub class="fm-subscript">0</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">1</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">2</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">3</sub>   <i class="fm-italics">x</i><sub class="fm-subscript">4</sub>  …   <b class="fm-bold">x<sub class="fm-subscript">n-1</sub>   0</b>   <b class="fm-bold">0</b>]</span></p>
<p class="body">which generates the output</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">y</i><sub class="fm-subscript"><i class="fm-italics1">n</i>−1</sub> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub> ⋅ <i class="fm-italics">x</i><sub class="fm-subscript"><i class="fm-italics1">n</i>−1</sub> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub> ⋅ 0 + <i class="fm-italics">w</i><sub class="fm-subscript">2</sub> ⋅ 0</span></p>
<p class="body">In section <a class="url" href="../Text/08.xhtml#sec-linlayer-matmult">8.3.1</a>, we saw that the FC (aka linear) layer can be expressed as a multiplication of the input vector by a weight matrix. Now, we will express convolution as matrix-vector multiplication. The weight matrix has a block-diagonal structure, as shown in equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s1valid">10.2</a>. It is a special case of equation <a class="url" href="../Text/08.xhtml#eq-MLP-weight-matrix">8.8</a>. As such, the forward propagation equation <a class="url" href="../Text/08.xhtml#eq-linlayer-forwardprop">8.7</a> and backpropagation equations <a class="url" href="../Text/08.xhtml#eq-auxvar-vector">8.31</a> and <a class="url" href="../Text/08.xhtml#eq-partialderiv-loss-wt-vector">8.33</a> are still applicable. Thus, forward propagation and backpropagation training) through convolution proceeds exactly as with FC layers.<a id="marker-353"/></p>
<p class="body">Equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s1valid">10.2</a> expresses <span class="math"><i class="fm-italics">kernel</i>_<i class="fm-italics">size</i></span> <span class="math">3</span>, stride <span class="math">1</span>, valid padding convolution as a multiplication of a weight matrix <i class="timesitalic">W</i> with input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{w} \circledast \vec{x} = W \vec{x} =
\begin{bmatrix}
&amp;w_{0} x_{0} + w_{1} x_{1} + w_{2} x_{2}\\
&amp;w_{0} x_{1} + w_{1} x_{2} + w_{2} x_{2}\\
&amp;\vdots\\
\end{bmatrix} = \nonumber \\
&amp;\overbrace{
\begin{bmatrix} w_{0}     &amp;  w_{1}     &amp; w_{2}     &amp; 0           &amp; 0           &amp; 0            &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\ 0           &amp;  w_{0}     &amp; w_{1}     &amp; w_{2}     &amp; 0           &amp; 0            &amp; \cdots      &amp; 0           &amp; 0            &amp; 0 \\ 0           &amp;  0           &amp; w_{0}     &amp; w_{1}     &amp; w_{2}     &amp; 0            &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\ 0           &amp;  0           &amp; 0           &amp; w_{0}     &amp; w_{1}     &amp; w_{2}      &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\ 0           &amp;  0           &amp; 0           &amp; 0           &amp; w_{0}     &amp; w_{1}      &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\
\vdots\\ 0           &amp;  0           &amp; 0           &amp; 0           &amp; 0           &amp; 0           &amp; \cdots      &amp;  w_{0}    &amp; w_{1}     &amp; w_{2}
\end{bmatrix}
}^{ W \left( \text{ conv weight matrix: kernel size $2$, stride $1$, valid pad. dim: } \left( n-2 \right) \times n  \right) }
\overbrace{
\begin{bmatrix}
x_{0}\\
x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
\vdots\\
x_{n-1}
\end{bmatrix}
}^{
\vec{x}
\left(
\text{ input vector, size $n \times 1$
}
\right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="275" src="../../OEBPS/Images/eq_10-02.png" width="879"/></p>
</div>
<p class="fm-equation-caption">Equation 10.2 <span class="calibre" id="eq-conv1d-as-malt-mult-k3s1valid"/></p>
<p class="body">Notice the <i class="fm-italics">sparse</i>, <i class="fm-italics">block-diagonal</i> nature of the weight matrix in equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s1valid">10.2</a>. This is characteristic of convolution weight matrices. Each row contains all the kernel weights at contiguous positions. The size of the kernel is typically much less than the input vector size. Of course, for matrix multiplication to be possible, the number of columns in the weight matrix must match the size of the input vector. Thus, there are many vacant positions in the row besides those occupied by kernel weights. We fill these vacant elements with zeros. Each row of the weight matrix thus has all the kernel weights appearing somewhere contiguously, and the rest of the row is filled with zeros. <i class="fm-italics">The position of kernel weights shifts rightward with each successive row</i>. This is what gives the block-diagonal appearance to the weight matrix. It also simulates the sliding of the kernel required for convolution. Each row represents a specific slide stop and generates one element of the output vector. Since the kernel is at a fixed position of the row and all other row elements are zero, only the input elements corresponding to the kernel positions are picked up. Other input elements are multiplied by zero: that is, they are ignored.</p>
<p class="body">Equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s1valid">10.2</a> depicts a stride of <span class="math">1</span>. For instance, if the stride is <span class="math">2</span>, the kernel weights will shift by two positions in successive rows. This is shown in equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s2valid">10.3</a>:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{w} \circledast \vec{x} = W \vec{x} = \nonumber \\[-6pt]
&amp;\overbrace{
\begin{bmatrix} w_{0}     &amp;  w_{1}     &amp; w_{2}     &amp; 0           &amp; 0           &amp; 0            &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\ 0           &amp;  0           &amp; w_{0}     &amp; w_{1}     &amp; w_{2}     &amp; 0            &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\ 0           &amp;  0           &amp; 0           &amp; 0           &amp; w_{0}     &amp; w_{1}      &amp; \cdots      &amp; 0           &amp; 0            &amp; 0\\
\vdots\\ 0           &amp;  0           &amp; 0           &amp; 0           &amp; 0           &amp; 0           &amp; \cdots      &amp;  w_{0}    &amp; w_{1}     &amp; w_{2}
\end{bmatrix}}^{W\left(\textrm{conv weight matrix: kernel size $2$, stride $2$, valid pad. dim: } \lfloor\frac{\left(n-2\right)}{2} + 1
\rfloor \times n\right)}
\overbrace{
\begin{bmatrix}
x_{0}\\
x_{1}\\
x_{2}\\
x_{3}\\
x_{4}\\
\vdots\\
x_{n-1}
\end{bmatrix}
}^{
\vec{x}
\left(
\text{ input vector, size $n \times 1$
}
\right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="277" src="../../OEBPS/Images/eq_10-03.png" width="686"/></p>
</div>
<p class="fm-equation-caption">Equation 10.3 <span class="calibre" id="eq-conv1d-as-malt-mult-k3s2valid"/></p>
<p class="body">Note that while equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s2valid">10.3</a> provides a conceptual matrix-multiplication view of convolution, it is not the most efficient way of implementing convolution. PyTorch and other deep learning software have extremely efficient ways of implementing convolution.</p>
<h3 class="fm-head1" id="pytorch-one-dimensional-convolution-with-custom-weights">10.1.4 PyTorch- One-dimensional convolution with custom weights</h3>
<p class="body"><a id="marker-354"/>We have discussed the convolution of a <span class="math">1</span>D input vector with two specific <span class="math">1</span>D kernels. We have seen that a kernel with uniform weights, such as <!--<span class="times">$\begin{bmatrix}\frac{1}{3} &amp;\frac{1}{3}&amp;\frac{1}{3}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="28" src="../../OEBPS/Images/eq_10-03-a2.png" width="81"/></span>, results in local smoothing of the input vector, whereas a kernel with antisymmetric weights, such as <!--<span class="times">$\begin{bmatrix}\frac{1}{2}&amp;\frac{-1}{2}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="28" src="../../OEBPS/Images/eq_10-03-b2.png" width="60"/></span>, results in an output vector that spikes at the edge locations in the input vector. Now we will see how to set the weights of a <span class="math">1</span>D kernel and perform <span class="math">1</span>D convolution with that kernel in PyTorch.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> This is <i class="fm-italics">not</i> a typical PyTorch operation. The more typical operation is to create a neural network with a convolution layer (where we specify the size, stride, and padding but not the weights) and then train the network so that the weights are learned. We usually don’t care about the exact values of the learned weight. Then why are we discussing how to set the weights of a kernel in PyTorch? Mainly to show how convolution works in PyTorch, the various parameters of the convolution object, and so forth.</p>
<p class="fm-code-listing-caption" id="listing-10.1-pytorch-code-for-1d-local-averaging-convolution">Listing 10.1 PyTorch code for 1D local averaging convolution</p>
<pre class="programlisting">import torch

x = torch.tensor(                                          <span class="fm-combinumeral">①</span>
        [-1.,  4., 11., 14., 21., 25., 30.])

w = torch.tensor([0.33, 0.33, 0.33])                       <span class="fm-combinumeral">②</span>

x = x.unsqueeze(0).unsqueeze(0)
w = w.unsqueeze(0).unsqueeze(0)                            <span class="fm-combinumeral">③</span>

conv1d = torch.nn.Conv1d(1, 1, kernel_size=3,              <span class="fm-combinumeral">④</span>
            stride=1, padding=[0], bias=False)

conv1d.weight = torch.nn.Parameter(w, requires_grad=False) <span class="fm-combinumeral">⑤</span>

with torch.no_grad():                                      <span class="fm-combinumeral">⑥</span>

    y = conv1d(x)                                          <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates a noisy input vector. Follows equation <span class="math"><i class="fm-italics">y</i> = 5<i class="fm-italics">x</i></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the weights of the convolutional kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> PyTorch expects inputs and weights to be of the form <span class="math"><i class="fm-italics">N</i> × <i class="fm-italics">C</i> × <i class="fm-italics">L</i></span>, where <i class="timesitalic">N</i> is the batch size, <i class="timesitalic">C</i> is the number of channels, and <i class="timesitalic">L</i> is the sequence length. Here, <i class="timesitalic">N</i> and <i class="timesitalic">C</i> are 1. torch.unsqueeze converts our <i class="timesitalic">L</i>-length vector into a <span class="math">1 × 1 × <i class="fm-italics">L</i></span> tensor.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates the smoothing kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets the kernel weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Instructs PyTorch to not compute gradients since we currently don’t require them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Runs the convolution</p>
<p class="fm-code-listing-caption" id="listing-10.2-pytorch-code-for-1d-edge-detection">Listing 10.2 PyTorch code for 1D edge detection</p>
<pre class="programlisting">import torch

x = torch.tensor(                                           <span class="fm-combinumeral">①</span>
        [10.,  11., 9., 10., 101., 99.,
         100., 101., 9., 10., 11., 10.])

w = torch.tensor([0.5, -0.5])                               <span class="fm-combinumeral">②</span>

x = x.unsqueeze(0).unsqueeze(0)                             <span class="fm-combinumeral">③</span>
w = w.unsqueeze(0).unsqueeze(0) 

conv1d = torch.nn.Conv1d(1, 1, kernel_size=3,               <span class="fm-combinumeral">④</span>
            stride=1, padding=[0], bias=False)

conv1d.weight = torch.nn.Parameter(w, requires_grad=False)  <span class="fm-combinumeral">⑤</span>


with torch.no_grad():                                       <span class="fm-combinumeral">⑥</span>

    y = conv1d(x)                                           <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the input vector with edges</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the weights of the edge-detection kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts the inputs and weights to <span class="math">1 × 1× <i class="fm-italics">L</i></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates the edge-detection kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets the kernel weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Instructs PyTorch to not compute gradients since we currently don’t require them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Runs the convolution</p>
<p class="body"><a id="marker-355"/>These listings show how to perform <span class="math">1</span>D convolution in PyTorch using the <code class="fm-code-in-text">torch.nn.</code> <code class="fm-code-in-text">Conv1d</code> class. This is typically used in larger neural networks like those in subsequent chapters. We can alternatively use <code class="fm-code-in-text">torch.nn.functional.conv1d</code> to directly invoke the mathematical convolution operation. This takes input and weight tensors and returns the convolved output tensor, as shown in listing 10.3.</p>
<p class="fm-code-listing-caption" id="listing-10.3-pytorch-code-directly-invoking-the-convolution-function">Listing 10.3 PyTorch code directly invoking the convolution function</p>
<pre class="programlisting">import torch

x = torch.tensor(                              <span class="fm-combinumeral">①</span>
        [10.,  11., 9., 10., 101., 99.,
         100., 101., 9., 10., 11., 10.])

w = torch.tensor([0.5, -0.5])                  <span class="fm-combinumeral">②</span>

x = x.unsqueeze(0).unsqueeze(0)                <span class="fm-combinumeral">③</span>
w = w.unsqueeze(0).unsqueeze(0)

y = torch.nn.functional.conv1d(x, w, stride=1) <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the input tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the weight tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts the inputs and weights to <span class="math">1 × 1 × <i class="fm-italics">L</i></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Runs the convolution</p>
<h2 class="fm-head" id="sec-conv1d-out-size">10.2 Convolution output size</h2>
<p class="body"><a id="marker-356"/>Consider a kernel of size <i class="timesitalic">k</i> sliding over an input of size <i class="timesitalic">n</i> with stride <i class="timesitalic">s</i>. Given a kernel of size <i class="timesitalic">k</i>, if the left end is at index <i class="timesitalic">l</i>, the right end is at index <span class="math"><i class="fm-italics">l</i> + (<i class="fm-italics">k</i> − 1)</span>. Each shift advances the left (as well as the right) end of the kernel by <i class="timesitalic">s</i>. If the initial position of the kernel was at index <span class="math">0</span>, then after <i class="timesitalic">m</i> shifts, the left end is at <i class="timesitalic">ms</i>. Correspondingly, the right end is at <span class="math"><i class="fm-italics">ms</i> + (<i class="fm-italics">k</i> − 1)</span>. Assuming valid padding, this right-end position must be less than or equal to <span class="math">(<i class="fm-italics">n</i> − 1)</span> (the last valid position of the input array).</p>
<p class="body">How many times can we shift before the kernel spills out of the input? In other words, what is the maximum possible value of <i class="timesitalic">m</i>, such that</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ms</i> + (<i class="fm-italics">k</i> − 1) ≤ (<i class="fm-italics">n</i> − 1)</span></p>
<p class="body">The answer is</p><!--<p class="Body"><span class="times">$$m = \lfloor\frac{\left( n - 1
\right) -  \left( k - 1 \right) }{s}\rfloor = \lfloor\frac{\left( n - k
\right) }{ s }\rfloor$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_10-03-c.png" width="263"/></p>
</div>
<p class="body">But each shift produces one output value. The output size of the convolution, <i class="timesitalic">o</i>, with valid padding, is <span class="math"><i class="fm-italics">m</i> + 1</span> (the plus one is to account for the initial position). Hence,</p><!--<p class="Body"><span class="times">$$o =
\lfloor\frac{\left( n - k \right) }{ s }\rfloor + 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_10-03-d.png" width="127"/></p>
</div>
<p class="body">If we are zero-padding with <i class="timesitalic">p</i> zeroes on each side of the input, the input size becomes <span class="math"><i class="fm-italics">n</i> + 2<i class="fm-italics">p</i></span>. The corresponding output size is</p><!--<p class="Body"><span class="times">$$o = \lfloor\frac{\left( n + 2p - k \right) }{ s }\rfloor + 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="47" src="../../OEBPS/Images/eq_10-04.png" width="163"/></p>
</div>
<p class="fm-equation-caption">Equation 10.4 <span class="calibre" id="eq-conv-out-size"/></p>
<p class="body">This can be extended to an arbitrary number of dimensions by repeating it for each dimension.</p>
<h2 class="fm-head" id="sec-conv2d">10.3 Two-dimensional convolution: Graphical and algebraic view</h2>
<p class="body">It is often said that an image is worth a thousand words. What is an image? As far as deep learning is concerned, it is a discrete two-dimensional entity—a <span class="math">2</span>D array of pixel values describing a scene at a fixed time. Each pixel represents a color intensity value. The color value can be a single element representing a gray level, or it can be three-dimensional, corresponding to R(ed), G(reen), B(lue) intensity values. (You may want to revisit section <a class="url" href="02.xhtml#sec-matrices">2.3</a> before proceeding.)</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="989" id="fig-conv2d-smoothing-stride1" src="../../OEBPS/Images/CH10_F07_Chaudhury.png" width="1089"/></p>
<p class="figurecaption">Figure 10.7 <span class="math">2</span>D convolution with a <i class="fm-italics">local averaging kernel</i> of size <span class="math">[3,3]</span>, stride <span class="math">[1,1]</span>, and valid padding. Each pixel is shown as a small rectangle, with the pixel’s gray level written in the rectangle. The shaded order. Successive steps indicate slide stops. For each pixel that is overlapped by the kernel, the weight of the kernel element falling on it is written in small font.<a id="marker-357"/></p>
</div>
<p class="body">At the time of this writing, image analysis is the most popular application of convolution. These applications use convolution to extract local patterns. How do we do this? In particular, can we rasterize the image (thus converting it into a vector) and use one-dimensional convolution?</p>
<p class="body">The answer is <i class="fm-italics">no</i>. To see why, examine figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>. What is the spatial neighborhood of the pixel at location <span class="math">(<i class="fm-italics">x</i> = 0, <i class="fm-italics">y</i> = 0)</span>? If we define the <i class="fm-italics">neighborhood</i> of a pixel as the set of pixels within a Manhattan distance of <span class="math">[2,2]</span> with that pixel at the top-left corner, the neighborhood of <span class="math">(<i class="fm-italics">x</i> = 0, <i class="fm-italics">y</i> = 0)</span> consists of the set of pixels covered by the shaded rectangle in figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, step <span class="math">0</span>. But these pixels <i class="fm-italics">will not be neighboring elements in a rasterized array representation of the image</i>. For instance, the pixel <span class="math">(<i class="fm-italics">x</i> = 0, <i class="fm-italics">y</i> =1)</span>, with value <span class="math">6</span>, is the fifth element in the rasterized array and, as such, will <i class="fm-italics">not</i> be considered a neighbor of <span class="math">(<i class="fm-italics">x</i> = 0, <i class="fm-italics">y</i> = 0)</span>, which is the <span class="math">0</span>th element in the rasterized array. Two-dimensional neighborhoods are <i class="fm-italics">not</i> preserved by rasterization. So, two-dimensional convolution has to be a specialized operation beyond merely rasterizing <span class="math">2</span>D arrays into <span class="math">1</span>D and applying <span class="math">1</span>D convolution.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Euclidean distance and Manhattan distance</p>
<p class="fm-sidebar-text">Euclidean distance measures the straight line distance between two points, whereas Manhattan distance measures the distance between two points with a constraint that you can only walk parallel to the axes just like on the streets of Manhattan). Let’s look at an example.</p>
<p class="fm-sidebar-text">Consider two points A (3, 3) and B (6, 7). The Euclidean distance between A and B is the length of the line segment AB, which can be computed as <span class="math">√((6 - 3)<sup class="fm-superscript">2</sup> + (7 - 3)<sup class="fm-superscript">2</sup>) = 5</span>. The Manhattan distance between A and B is <span class="math">(6−3) + (7−3) = 3 + 4 = 7</span>. In this chapter, we represent the Manhattan distance as [3, 4] to capture the distance along each axis separately.</p>
</div>
<p class="body"><a id="marker-358"/>The best way to visualize <span class="math">2</span>D convolution is to imagine a wall (the input image) over which a tile the kernel) is sliding:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">In figures <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <a class="url" href="#fig-conv2d-smoothing-stride2">10.8</a> and <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>, the shaded rectangle depicts the tile (kernel), while the larger white rectangle containing it depicts the wall (input image). Successive steps in the figure represent successive positions (aka slide stops) of the sliding tile. Notice that the shaded rectangle occupies a different position in each step.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Tiles in successive positions during sliding can overlap. They overlap by varying amounts in figures <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <a class="url" href="#fig-conv2d-smoothing-stride2">10.8</a>, and <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The wall and the tile are discrete <span class="math">2</span>D arrays in reality. At each slide stop, the tile array elements rest on a subset of wall array elements.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We multiply each input array element by the kernel element resting on it and sum the products. This is equivalent to taking a weighted sum of the input (wall) elements that fall under the current position of the kernel (tile), with the kernel elements serving as weights. This weighted sum is emitted as a single output element. One output element results from each slide stop of the tile. As the tile slides over the entire wall, left to right and top to bottom, a <span class="math">2</span>D output array is generated.</p>
</li>
</ul>
<p class="body">In <span class="math">2</span>D convolution, the input array, kernel size, and stride are all <span class="math">2</span>D vectors. Just as in <span class="math">1</span>D convolution, the following entities are defined for <span class="math">2</span>D convolution:</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="965" id="fig-conv2d-smoothing-stride2" src="../../OEBPS/Images/CH10_F08_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 10.8 <span class="math">2</span>D convolution with a <i class="fm-italics">local averaging kernel</i> of size <span class="math">[3,3]</span>, stride <span class="math">[2,2]</span>, and valid padding</p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="812" id="fig-conv2d-edge-detection-stride1" src="../../OEBPS/Images/CH10_F09_Chaudhury.png" width="1084"/></p>
<p class="figurecaption">Figure 10.9 <span class="math">2</span>D convolution with an <i class="fm-italics">edge-detection kernel</i> of size <span class="math">2</span>, stride <span class="math">1</span>, and valid padding. Not all slide stops that is, steps) are shown. Notice how the output is zero at a uniform location but spikes when one-half of the kernel falls on low values while the other half falls on high values.</p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Input</i>—A two-dimensional array. We typically use the symbol <span class="math">[<i class="fm-italics">H</i>, <i class="fm-italics">W</i>]</span> indicating the height and width of the array, respectively) to represent the input array size in <span class="math">2</span>D convolution. In figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <span class="math"><i class="fm-italics">H</i> = 5, <i class="fm-italics">W</i> = 5</span>.<a id="marker-359"/></p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Output</i>—A two-dimensional array. We typically use the symbol <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_osm.png" width="14"/></span> = [<i class="fm-italics">o<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">o<sub class="fm-subscript">W</sub></i>]</span> to represent output array dimensions in <span class="math">2</span>D convolution. For instance, in figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_osm.png" width="14"/></span> = [3,3]</span>. In section <a class="url" href="#sec-conv1d-out-size">10.2</a>, we saw how to compute the output size for a single dimension. We have to repeat that computation once per dimension to obtain the output size in higher dimensions.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Kernel</i>—A small two-dimensional array of weights whose size is a parameter of the convolution. We typically use the symbol <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_k.png" width="15"/></span> = [<i class="fm-italics">k<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>]</span> to represent the kernel size (height, width) in <span class="math">2</span>D convolution. If <span class="math">(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>)</span> denotes the current position of the top-left corner of the <span class="math">2</span>D kernel, the bottom-right corner is at <span class="math">(<i class="fm-italics">x</i> + <i class="fm-italics">k<sub class="fm-subscript">W</sub></i> − 1, <i class="fm-italics">y</i> + <i class="fm-italics">k<sub class="fm-subscript">H</sub></i> − 1)</span>. In figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_k.png" width="15"/></span> = [3,3]</span>; in figure <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_k.png" width="15"/></span> = [2,2]</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Stride</i>—<a id="marker-360"/>The number of input elements over which the kernel slides on completing a single step. We typically use the symbol <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [<i class="fm-italics">s<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>]</span> to represent the stride size (height, width) in <span class="math">2</span>D convolution. If <span class="math">(<i class="fm-italics">x</i>, <i class="fm-italics">y</i>)</span> denotes the current position of the top-left corner of the <span class="math">2</span>D kernel, the next shift will put the top-left corner of the kernel at <span class="math">(<i class="fm-italics">x</i> + <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>, <i class="fm-italics">y</i>)</span> see, for instance, the transition from step 0 to step 1 or step 1 to step 2 in figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>). If this transition causes portions of the tile to fall outside the wall—that is, <span class="math"><i class="fm-italics">x</i> + <i class="fm-italics">s<sub class="fm-subscript">W</sub></i> ≥ <i class="fm-italics">W</i></span>—we set the next slide position such that the top-left corner of the kernel falls on <span class="math">(0, <i class="fm-italics">y</i> + 1)</span> (see, for instance, the transition from step 2 to step 3 or step 5 to step 6 in figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>). If <span class="math"><i class="fm-italics">y</i> + <i class="fm-italics">s<sub class="fm-subscript">H</sub></i> ≥ <i class="fm-italics">H</i></span>, we stop sliding. Stride size is a parameter of the convolution. In figure <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [1,1]</span>; in figure <a class="url" href="#fig-conv2d-smoothing-stride2">10.8</a>, stride is <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [2,2]</span>. As in the <span class="math">1</span>D case, a stride of <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [1,1]</span> means there is a slide stop at each successive element of the input. So, the output has roughly the same number of elements as the input (they may not be exactly equal because of padding). A stride of <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [2,2]</span> means each row of the input will yield half the row size worth of output elements, and each column will generate half the column size worth of output elements. Hence, the output size is roughly a quarter of the input size. Overall, the reduction factor of the input-to-output size roughly matches the product of the elements in the stride vector.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Padding</i>—As the kernel slides toward the extremity of the input array along the width and/or height, parts of it may fall outside the input array. In other words, part of the kernel falls over ghost input elements. As in the <span class="math">1</span>D case, we deal with this via padding. Padding strategies in <span class="math">2</span>D convolution are straightforward extensions from <span class="math">1</span>D:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Valid padding</i>—We stop sliding whenever any element of the kernel falls outside the input array, either in width and/or in height. No ghost input elements are involved; the <i class="fm-italics">entire</i> kernel always falls on valid input elements (hence the name <i class="fm-italics">valid</i>).</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Same (zero) padding</i>—Here, we do not want to stop early. We keep sliding as long as the top-left corner of the kernel falls on a valid input position. So, if the stride is <span class="math">1, 1</span>, the output size will match the input size (hence the name <i class="fm-italics">same</i>). When we slide near the end of an input row (right extremity of the input), the right-most columns of the kernel will fall outside the input. Similarly, when we slide toward the bottom of the input, the bottom rows of the kernel will fall outside. If we slide near the bottom-right corner of the input, both the right-most columns and bottommost rows will fall outside the input. The rule is that all ghost input values outside the true boundaries of the input array are replaced by zeros.</p>
</li>
</ul>
</li>
</ul>
<p class="body"><a id="marker-361"/>Let’s denote the input image domain by <i class="timesitalic">S</i>. It is a <span class="math">2</span>D grid whose domain is</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S</i> = [0, <i class="fm-italics">H</i> − 1] × [0, <i class="fm-italics">W</i> − 1]</span></p>
<p class="body">Every point in <i class="timesitalic">S</i> is a pixel with a color value (which can be a scalar—a gray-level value—or a vector of three values, R, G, B. On this grid of input points, we define a subgrid <i class="timesitalic">S<sub class="fm-subscript">o</sub></i> of output points. <i class="timesitalic">S<sub class="fm-subscript">o</sub></i> is obtained from <i class="timesitalic">S</i> by applying stride-based stepping on the input. Assuming <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [<i class="fm-italics">s<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>]</span> denotes the <span class="math">2</span>D stride vector, the first slide stop has the top-left corner of the brick at <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">0</sub> ≡ (<i class="fm-italics">y</i> = 0, <i class="fm-italics">x</i> = 0)</span>. The next slide stop is at <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">1</sub> ≡ (<i class="fm-italics">y</i> = 0, <i class="fm-italics">x</i> = <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)</span>, and the next is at <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">2</sub> ≡ (<i class="fm-italics">y</i> = 0, <i class="fm-italics">x</i> = 2<i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)</span>. When we reach the right end, we increment <i class="timesitalic">y</i>. Overall, the output grid consists of the slide-stop points where the top-left corner of the kernel (brick) rests as it sweeps over the input volume: <span class="math"><i class="fm-italics">S<sub class="fm-subscript">o</sub></i> = {<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">0</sub>, <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">1</sub>, … }</span>. There is an output for each point in <i class="timesitalic">S<sub class="fm-subscript">o</sub></i>.</p>
<p class="body">The kernel also has two dimensions (in practice, it has two more dimensions corresponding to the input channels and batch—we are ignoring them now for simplicity—as discussed in section <a class="url" href="#sec-pytorch-2d-conv">10.3.3</a>). Equation <a class="url" href="#eq-2dconv-output">10.5</a> shows how a single output value is generated in <span class="math">2</span>D convolution. <i class="timesitalic">X</i> denotes input, <i class="timesitalic">Y</i> denotes output, and <i class="timesitalic">W</i> denote kernel weights:</p><!--<p class="Body"><span class="times">$$Y_{y, x} = \sum_{i=0}^{k_{H}}  \sum_{j=0}^{k_{W}}
X_{y+i, x+j} W_{i, j}  \;\;\;\;\;\; \forall \left(y, x\right) \in S_{o}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="65" src="../../OEBPS/Images/eq_10-05.png" width="303"/></p>
</div>
<p class="fm-equation-caption">Equation 10.5 <span class="calibre" id="eq-2dconv-output"/></p>
<p class="body">Note that the kernel (tile) has its origin on <span class="math"><i class="fm-italics">X</i><sub class="fm-subscript"><i class="fm-italics1">y</i>, <i class="fm-italics1">x</i></sub></span>. Its dimensions are <span class="math">(<i class="fm-italics">k<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>)</span>. So, it covers all input pixels in the domain <span class="math">[<i class="fm-italics">y</i>..(<i class="fm-italics">y</i> + <i class="fm-italics">k<sub class="fm-subscript">H</sub></i>)] × [<i class="fm-italics">x</i>..(<i class="fm-italics">x</i> + <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>)]</span>. These are the pixels participating in equation <a class="url" href="#eq-2dconv-output">10.5</a>. Each of these input pixels is multiplied by the kernel element covering it. Match equation <a class="url" href="#eq-2dconv-output">10.5</a> with figures <a class="url" href="#fig-conv2d-smoothing-stride1">10.7</a>, <a class="url" href="#fig-conv2d-smoothing-stride2">10.8</a>, and <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>.</p>
<h3 class="fm-head1" id="sec-2d_denoising_local_smoothing">10.3.1 Image smoothing via 2D convolution</h3>
<p class="body"><a id="marker-362"/>In section <a class="url" href="#sec-1d_local_smoothing">10.1.1</a>, we discussed one-dimensional local smoothing. We observed how it gets rid of local fluctuations so that longer-term patterns are discernible more cleanly. The same thing happens in two dimensions. Figure <a class="url" href="#fig-conv2d-smoothing-example-input">10.10</a> shows an image with some text written on a background with salt-and-pepper noise. The noise has no semantic significance; it is the text that needs to be analyzed (perhaps via optical character recognition). We can eliminate the noise via <span class="math">2</span>D convolution using a kernel with uniform weights, such as</p><!--<p class="Body"><span class="times">$$W =
\begin{bmatrix} W_{0, 0} =  \frac{1}{9} &amp; W_{0, 1} = \frac{1}{9}  &amp;  W_{0, 2} =
\frac{1}{9} \\ W_{1, 0} =  \frac{1}{9} &amp; W_{1, 1} = \frac{1}{9}  &amp; W_{1, 2}
=  \frac{1}{9} \\ W_{2, 0} =  \frac{1}{9} &amp; W_{2, 1} = \frac{1}{9}  &amp; W_{2, 2}
=  \frac{1}{9}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="107" src="../../OEBPS/Images/eq_10-05-a.png" width="303"/></p>
</div>
<p class="body">The resulting denoised/smooth image is shown in figure <a class="url" href="#fig-conv2d-smoothing-example-vert">10.11</a>. What does the uniform kernel do? To see that, look at figure <a class="url" href="#fig-conv2d-smoothing-stride2">10.8</a>. It should be obvious that the kernel causes each output pixel to be a weighted local average of the neighboring <span class="math">3 × 3</span> input pixels.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="349" id="fig-conv2d-smoothing-example-input" src="../../OEBPS/Images/CH10_F10a_Chaudhury.jpg" width="697"/></p>
<p class="figurecaption">(a) Input image</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="356" id="fig-conv2d-smoothing-example-vert" src="../../OEBPS/Images/CH10_F10b_Chaudhury.jpg" width="710"/></p>
<p class="figurecaption">(b) Smoothed/denoised output image</p>
</div>
<p class="fm-table-caption" id="fig-conv2d-smoothing-example">Figure 10.10 Denoising/smoothing a noisy image by applying <span class="math">2</span>D convolution <!--<span class="times">$\protect \begin{bmatrix}\frac{1}{9} &amp;\frac{1}{9} &amp;\frac{1}{9}\frac{1}{9} &amp;\frac{1}{9} &amp;\frac{1}{9}\frac{1}{9} &amp;\frac{1}{9} &amp;\frac{1}{9}\protect \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="113" src="../../OEBPS/Images/eq_10-05-b2.png" width="128"/></span> to figure <a class="url" href="#fig-conv2d-edgedetect-example-input">10.11a</a></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for image smoothing, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/aDM7">http://mng.bz/aDM7</a>.</p>
<h3 class="fm-head1" id="image-edge-detection-via-2d-convolution">10.3.2 Image edge detection via 2D convolution</h3>
<p class="body">Not all pixels in an image have equal semantic importance. Imagine a photograph of a person standing in front of a white wall. The pixels belonging to the wall are uniform in color and uninteresting. The pixels that yield the most semantic clues are those belonging to the silhouette: the edge pixels. This agrees with the science of human vision, where, as we mentioned earlier, experiments indicate that the human brain pays more attention to regions with sharp changes in color. Humans treat sound in a very similar fashion, ignoring uniform buzz such sounds often induce sleep) but becoming alert when the volume or frequency of the sound changes. Thus, identifying edges in an image is vital for image understanding.</p>
<p class="body">Edges are local phenomena. As such, they can be identified by <span class="math">2</span>D convolution with specially chosen kernels. For instance, the vertical edges in figure <a class="url" href="#fig-conv2d-edgedetect-example-vert">10.11b</a> were produced by performing <span class="math">2</span>D convolution on the image in figure <a class="url" href="#fig-conv2d-edgedetect-example-input">10.11a</a> using the kernel</p><!--<p class="Body"><span class="times">$$W =
\underbrace{
\begin{bmatrix} W_{0,0} = -0.25 &amp; W_{0,1} = 0.25  \\  W_{1,0}
=  -0.25 &amp;  W_{1,1} =  0.25 \end{bmatrix}
}_{2\text{D kernel for vertical edge detection}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="104" src="../../OEBPS/Images/eq_10-05-c.png" width="260"/></p>
</div>
<p class="body">Likewise, the vertical edges in figure <a class="url" href="#fig-conv2d-edgedetect-example-horz">10.11c</a> were produced by performing <span class="math">2</span>D convolution on the image in figure <a class="url" href="#fig-conv2d-edgedetect-example-input">10.11a</a> using the kernel</p><!--<p class="Body"><span class="times">$$W =
\underbrace{
\begin{bmatrix} W_{0,0} = -0.25 &amp; W_{0,1} = -0.25  \\ W_{1,0}
=  0.25 &amp; W_{1,1} =  0.25 \end{bmatrix}
}_{2\text{D kernel for horizontal edge detection}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="104" src="../../OEBPS/Images/eq_10-05-d.png" width="272"/></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="506" id="fig-conv2d-edgedetect-example-input" src="../../OEBPS/Images/CH10_F11a_Chaudhury.png" width="449"/></p>
<p class="figurecaption">(a) Input image</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="457" id="fig-conv2d-edgedetect-example-vert" src="../../OEBPS/Images/CH10_F11b_Chaudhury.png" width="405"/></p>
<p class="figurecaption">(b) Vertical edges detected by applying <span class="math">2</span>D convolution <!--<span class="times">$\protect\begin{bmatrix}  -0.25 &amp; 0.25 \protect \\   -0.25 &amp; 0.25\protect \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_10-05-e2.png" width="116"/></span> to figure <a class="url" href="#fig-conv2d-edgedetect-example-input">10.11a</a></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="455" id="fig-conv2d-edgedetect-example-horz" src="../../OEBPS/Images/CH10_F11c_Chaudhury.png" width="404"/></p>
<p class="figurecaption">(c) Horizontal edges detected by applying <span class="math">2</span>D convolution <!--<span class="times">$\protect\begin{bmatrix}  -0.25 &amp; -0.25 \protect \\   0.25 &amp; 0.25\protect \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_10-05-f2.png" width="129"/></span> to figure <a class="url" href="#fig-conv2d-edgedetect-example-input">10.11a</a></p>
</div>
<p class="fm-table-caption" id="fig-conv2d-edgedetect-example">Figure 10.11 image often helps us analyze the image.</p>
<p class="body">How do these kernels identify edges? To see this, look at figure <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>. In a neighborhood with equal pixel values (for example, a flat wall), the kernel in figure <a class="url" href="#fig-conv2d-edgedetect-example-vert">10.11b</a> will yield zero (the positive and negative kernel elements fall on equal values, and their weighted sum is zero). Thus this kernel suppresses uniform regions. On the other hand, it has a high response if there is a sharp jump in color (the negative and positive halves of the kernel fall on very different values, and the weighted sum is a large negative or large positive).</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for edge detection, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/g4JV">http://mng.bz/g4JV</a>.</p>
<h3 class="fm-head1" id="sec-pytorch-2d-conv">10.3.3 PyTorch- 2D convolution with custom weights</h3>
<p class="body">We have discussed the convolution of <span class="math">2</span>D input arrays with two specific <span class="math">2</span>D kernels. We have seen that a kernel with uniform weights, such as <!--<span class="times">$\begin{bmatrix}\frac{1}{9} &amp;\frac{1}{9}&amp;\frac{1}{9} \\ \frac{1}{9} &amp;\frac{1}{9} &amp;\frac{1}{9}  \\\frac{1}{9} &amp;\frac{1}{9} &amp;\frac{1}{9}  \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="88" src="../../OEBPS/Images/eq_10-05-g2.png" width="81"/></span>, results in local smoothing of the input array, whereas a kernel with antisymmetric weights, such as <!--<span class="times">$\begin{bmatrix}\frac{1}{4} &amp;\frac{-1}{4} \\\frac{1}{4} &amp;\frac{-1}{4} \end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_10-05-h2.png" width="61"/></span>, results in an output array that spikes at the edge locations in the input array. Now we will see how to set the weights of a <span class="math">2</span>D kernel and perform <span class="math">2</span>D convolution with that kernel in PyTorch.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> This is <i class="fm-italics">not</i> a typical PyTorch operation. The more typical operation is to create a neural network with a convolution layer (where we specify the size, stride, and padding but not the weights) and then train the network so that the weights are learned. We usually don’t care about the exact values of the learned weight. A sample neural network with a 2D convolution layer can be seen in section <a class="url" href="#sec-conv2or3d-in-NN">10.6</a>.</p>
<p class="body"><a id="marker-365"/>Listing <a class="url" href="#id-2DlocalAveragingConvolution">10.4</a> shows local averaging convolution in two dimensions. While we saw in section that input arrays are <span class="math">2</span>D tensors of shape <i class="timesitalic">H</i> <span class="math">×</span> <i class="timesitalic">W</i>, the PyTorch interface to convolution expects <span class="math">4</span>D tensors of shape <i class="timesitalic">N</i> <span class="math">×</span> <i class="timesitalic">C</i> <span class="math">×</span> <i class="timesitalic">H</i> <span class="math">×</span> <i class="timesitalic">W</i> as input:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The first dimension, <i class="timesitalic">N</i>, stands for the batch size. In a real neural network, inputs are fed in minibatches instead of one input instance at a time (this is for efficiency reasons, as discussed in section <a class="url" href="../Text/09.xhtml#sec-SGD">9.2.2</a>). <i class="timesitalic">N</i> stands for the number of input images contained in the minibatch.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The second dimension, <i class="timesitalic">C</i>, stands for the number of channels. For the input to the entire neural network, in the case of RGB images, we have three channels R (red), G (green), and B (blue); in the case of grayscale images, we only have a single channel. For other layers, the number of channels can be anything, depending on the neural network’s architecture. Typically, layers further from the input and closer to the output have more channels. Only channels at the grand input have fixed, clearly discernible physical significance (like R, G, B). Channels at the input to successive layers do not.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The third dimension, <i class="timesitalic">H</i>, stands for the height.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The fourth dimension, <i class="timesitalic">W</i>, stands for the width.</p>
</li>
</ul>
<p class="body">The weight tensor of a PyTorch <code class="fm-code-in-text">Conv2D</code> object has to be a <span class="math">4</span>D tensor. The listing shows a single grayscale image of size <span class="math">5</span> <span class="math">×</span> <span class="math">5</span> as input. Hence <i class="timesitalic">N</i> = 1, <i class="timesitalic">C</i> = 1, <i class="timesitalic">H</i> = 5, and <i class="timesitalic">W</i> = 5. <i class="timesitalic">x</i> is instantiated as a <span class="math">2</span>D tensor of size <span class="math">5 × 5</span>. To convert it to a <span class="math">4</span>D tensor, we use the <code class="fm-code-in-text">torch.unsqueeze()</code> function, which adds an extra dimension to the input.</p>
<p class="fm-code-listing-caption" id="id-2DlocalAveragingConvolution">Listing 10.4 PyTorch code for 2D local averaging convolution</p>
<pre class="programlisting">import torch

x = load_img()                                             <span class="fm-combinumeral">①</span>

w = torch.tensor(                                          <span class="fm-combinumeral">②</span>
    [
        [0.11, 0.11, 0.11],
        [0.11, 0.11, 0.11],
        [0.11, 0.11, 0.11]
    ]
)

x = x.unsqueeze(0).unsqueeze(0)                            <span class="fm-combinumeral">③</span>
w = w.unsqueeze(0).unsqueeze(0)

conv2d = torch.nn.Conv2d(1, 1, kernel_size=2,
                    stride=1, bias=False)                  <span class="fm-combinumeral">④</span>

conv2d.weight = torch.nn.Parameter(w, requires_grad=False) <span class="fm-combinumeral">⑤</span>


with torch.no_grad():                                      <span class="fm-combinumeral">⑥</span>


    y = conv2d(x)                                          <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads a noisy grayscale input image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the weights of the convolutional kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> PyTorch expects inputs and weights to be of the form <i class="timesitalic">N</i> <span class="math">×</span> <i class="timesitalic">C</i> <span class="math">×</span> <i class="timesitalic">H</i> <span class="math">×</span> <i class="timesitalic">W</i>, where <i class="timesitalic">N</i> is the batch size, <i class="timesitalic">C</i> is the number of channels, <i class="timesitalic">H</i> is the height, and <i class="timesitalic">W</i> is the width. Here, <span class="math"><i class="fm-italics">N</i> = 1</span> because we have a single image. <span class="math"><i class="fm-italics">C</i> = 1</span> because we are considering a grayscale image. <i class="timesitalic">H</i> and <i class="timesitalic">W</i> are both 5 because the input is a <span class="math">5</span> <span class="math">×</span> <span class="math">5</span> array. unsqueeze converts our <span class="math">5 × 5</span> tensor into a <span class="math">1</span> <span class="math">×</span> <span class="math">1</span> <span class="math">×</span> <span class="math">5</span> <span class="math">×</span> <span class="math">5</span> tensor.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates the 2D smoothing kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets the kernel weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Instructs PyTorch to not compute gradients since we currently don’t require them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Runs the convolution</p>
<p class="fm-code-listing-caption" id="listing-10.5-pytorch-code-for-2d-edge-detection">Listing 10.5 PyTorch code for 2D edge detection</p>
<pre class="programlisting">import torch

x = load_img()                                             <span class="fm-combinumeral">①</span>

w = torch.tensor(                                          <span class="fm-combinumeral">②</span>
        [[-0.25, 0.25],
        [-0.25, 0.25]]
    )

x = x.unsqueeze(0).unsqueeze(0)                            <span class="fm-combinumeral">③</span>
w = w.unsqueeze(0).unsqueeze(0)

conv2d = torch.nn.Conv2d(1, 1, kernel_size=2,              <span class="fm-combinumeral">④</span>
                    stride=1, bias=False)

conv2d.weight = torch.nn.Parameter(w, requires_grad=False) <span class="fm-combinumeral">⑤</span>

with torch.no_grad():                                      <span class="fm-combinumeral">⑥</span>

    y = conv2d(x)                                          <span class="fm-combinumeral">⑦</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads a grayscale input image with edges</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the weights of the convolutional kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts the inputs to <span class="math">1</span> <span class="math">×</span> <span class="math">1</span> <span class="math">×</span> <span class="math">4</span> <span class="math">×</span> <span class="math">4</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a 2D edge-detection kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Sets the kernel weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Instructs PyTorch to not compute gradients since we currently don’t require them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Runs the convolution</p>
<h3 class="fm-head1" id="two-dimensional-convolution-as-matrix-multiplication">10.3.4 Two-dimensional convolution as matrix multiplication</h3>
<p class="body"><a id="marker-366"/>In section <a class="url" href="#sec-conv1d-mat">10.1.3</a>, we saw how <span class="math">1</span>D convolution can be viewed as multiplying the input vector by a block-diagonal matrix (shown in equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s2valid">10.3</a>). The idea can be extended to higher dimensions, although the matrix of weights becomes significantly more complex. Nonetheless, it is important to have a mental picture of this matrix. Among other things, it will help us better understand transposed convolution. In this matrix multiplication-oriented view of <span class="math">2</span>D convolution, the input image is represented as a rasterized <span class="math">1</span>D vector. Thus, an input matrix of size <span class="math"><i class="fm-italics">m</i> × <i class="fm-italics">n</i></span> becomes an <i class="timesitalic">mn</i>-sized vector. The corresponding weight matrix has rows of length <i class="timesitalic">mn</i>. Each row corresponds to a specific slide stop.</p>
<p class="body">For ease of understanding, let’s consider an input image with <span class="math">[<i class="fm-italics">H</i>, <i class="fm-italics">W</i>] = [4,4]</span> (never mind that this image is unrealistically small). On this image, we are performing <span class="math">2</span>D convolution with a <span class="math">[<i class="fm-italics">k<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>] = [2,2]</span> kernel with stride <span class="math">[<i class="fm-italics">s<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>] = [1,1]</span>. The situation is exactly as shown in figure <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>. The input image <i class="timesitalic">X</i> with size <span class="math"><i class="fm-italics">H</i> = 4, <i class="fm-italics">W</i> = 4</span></p><!--<p class="Body"><span class="times">$$X = \begin{bmatrix}
X_{0,0} &amp; X_{0, 1} &amp; X_{0, 2} &amp; X_{0,3} \\
X_{1,0} &amp; X_{1, 1} &amp; X_{1, 2} &amp; X_{1,3} \\
X_{2,0} &amp; X_{2, 1} &amp; X_{2, 2} &amp; X_{2,3} \\
X_{3,0} &amp; X_{3, 1} &amp; X_{3, 2} &amp; X_{3,3}
\end{bmatrix} \overset{\Delta}{=}
\vec{x} =
\begin{bmatrix}
X_{0,0} \\ X_{0, 1} \\ X_{0, 2} \\ X_{0,3} \\ X_{2,0} \\ X_{2, 1} \\
X_{2, 2} \\ X_{2,3} \\ X_{3,0} \\ X_{3, 1} \\ X_{3, 2} \\ X_{3,3}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="397" src="../../OEBPS/Images/eq_10-05-i.png" width="327"/></p>
</div>
<p class="body"><a id="marker-367"/>rasterizes to the input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> of length <span class="math">4 * 4 = 16</span>. Let the kernel weights be denoted as <!--<span class="times">$\begin{bmatrix} w_{0, 0} &amp; w_{0, 1}\\ w_{1, 0} &amp; w_{1, 1}\\\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_10-05-j2.png" width="92"/></span> Consider the successive slide stops (steps in figure <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>). The exact elements of the rasterized input vector that are multiplied by kernel weights for a specific step are shown below—these correspond to the shaded items for the same steps in figure <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>: <span class="math">2</span>D convolution between an image <i class="timesitalic">X</i> and a kernel <i class="timesitalic">W</i>, denoted <span class="math"><i class="fm-italics">Y</i> = <i class="fm-italics">W</i> ⊛ <i class="fm-italics">X</i></span>, in the special case of an input image with <span class="math">[<i class="fm-italics">H</i>, <i class="fm-italics">W</i>] = [4,4]</span>. For this image, <span class="math">2</span>D convolution with a <span class="math">[<i class="fm-italics">k<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>] = [2,2]</span> kernel with stride <span class="math">[<i class="fm-italics">s<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>] = [1,1]</span> and valid padding can be expressed as the following matrix multiplication:</p><!--<p class="Body"><span class="times">$$Y = W \circledast X =
\begin{bmatrix}
Y_{0, 0} \\ Y_{0, 1} \\ Y_{0, 2} \\
Y_{1, 0} \\ Y_{1, 1} \\ Y_{1, 2} \\ _{2, 0} \\ Y_{2, 1} \\ y_{2, 2}
\end{bmatrix}
=
\begin{bmatrix} w_{0, 0} X_{0, 0} + w_{0, 1} X_{0, 1} + w_{1, 0} X_{1, 0} + w_{1, 1}
X_{1, 1} \\ w_{0, 0} X_{0, 1} + w_{0, 1} X_{0, 2} + w_{1, 0} X_{1, 1} + w_{1, 1}
X_{1, 2} \\ w_{0, 0} X_{0, 2} + w_{0, 1} X_{0, 3} + w_{1, 0} X_{1, 2} + w_{1, 1}
X_{1, 3} \\ w_{0, 0} X_{1, 0} + w_{0, 1} X_{1, 1} + w_{1, 0} X_{2, 0} + w_{1, 1}
X_{2, 1} \\ w_{0, 0} X_{1, 1} + w_{0, 1} X_{1, 2} + w_{1, 0} X_{2, 1} + w_{1, 1}
X_{2, 2} \\ w_{0, 0} X_{1, 2} + w_{0, 1} X_{1, 3} + w_{1, 0} X_{2, 2} + w_{1, 1}
X_{2, 3} \\ w_{0, 0} X_{2, 0} + w_{0, 1} X_{2, 1} + w_{1, 0} X_{3, 0} + w_{1, 1}
X_{3, 1} \\ w_{0, 0} X_{2, 1} + w_{0, 1} X_{2, 2} + w_{1, 0} X_{3, 1} + w_{1, 1}
X_{3, 2} \\ w_{0, 0} X_{2, 2} + w_{0, 1} X_{2, 3} + w_{1, 0} X_{3, 2} + w_{1, 1}
X_{3, 3}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="298" src="../../OEBPS/Images/eq_10-05-k.png" width="484"/></p>
</div>
<p class="body">This can be expressed as</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;W \circledast X = \nonumber\\
&amp;\begin{bsmallmatrix} w_{0, 0}     &amp; w_{0, 1}      &amp; 0                &amp; 0                   &amp; w_{1, 0}      &amp; w_{1, 1}     &amp; 0               &amp; 0                        &amp; 0                 &amp; 0                   &amp; 0                 &amp; 0                 &amp;           0      &amp;           0      &amp; 0               &amp;           0\\[2pt]
          0     &amp; w_{0, 0}      &amp; w_{0, 1}      &amp; 0                   &amp;            0     &amp; w_{1, 0}     &amp; w_{1, 1}     &amp; 0                        &amp; 0                 &amp; 0                   &amp; 0                 &amp; 0                 &amp;           0      &amp;           0      &amp; 0               &amp;           0\\[2pt]
          0     &amp;            0     &amp; w_{0, 0}      &amp; w_{0, 1}         &amp; 0                &amp;           0     &amp; w_{1, 0}     &amp; w_{1, 1}              &amp; 0                 &amp; 0                   &amp; 0                 &amp; 0                 &amp;           0      &amp;           0      &amp; 0               &amp;           0\\[2pt]
          0     &amp; 0                &amp; 0                &amp; 0                   &amp; w_{0, 0}      &amp; w_{0, 1}      &amp;           0    &amp;            0             &amp; w_{1, 0}       &amp; w_{1, 1}         &amp;          0        &amp; 0                 &amp;           0       &amp;          0      &amp; 0               &amp;           0\\[2pt]
          0     &amp; 0                &amp; 0                &amp; 0                   &amp;            0     &amp; w_{0, 0}      &amp; w_{0, 1}    &amp;            0             &amp;           0       &amp; w_{1, 0}         &amp; w_{1, 1}       &amp; 0                 &amp;           0       &amp;          0      &amp; 0               &amp;           0\\[2pt]
          0     &amp; 0                &amp; 0                &amp; 0                   &amp;            0     &amp;          0       &amp; w_{0, 0}    &amp;  w_{0, 1}             &amp;           0       &amp;           0         &amp; w_{1, 0}       &amp; w_{1, 1}       &amp;           0       &amp;          0      &amp; 0               &amp;           0\\[2pt]
          0     &amp; 0                &amp; 0                &amp; 0                   &amp;            0      &amp;         0       &amp;           0    &amp;            0             &amp; w_{0, 0}       &amp; w_{0, 1}         &amp;           0       &amp;            0      &amp;  w_{1, 0}      &amp; w_{1, 1}     &amp;  0              &amp;           0\\[2pt]
          0     &amp; 0                &amp; 0                &amp; 0                   &amp;            0      &amp;         0       &amp;           0    &amp;            0             &amp;           0       &amp; w_{0, 0}         &amp; w_{0, 1}       &amp;            0      &amp;            0      &amp; w_{1, 0}     &amp; w_{1, 1}     &amp;           0\\[2pt]
          0     &amp; 0                &amp; 0                &amp; 0                   &amp;            0      &amp;         0       &amp;           0    &amp;            0             &amp;           0       &amp;           0         &amp; w_{0, 0}       &amp; w_{0, 1}       &amp;            0      &amp;            0    &amp; w_{1, 0}     &amp; w_{1, 1}\\
\end{bsmallmatrix}
\begin{bsmallmatrix}
X_{0,0} \\ X_{0, 1} \\ X_{0, 2} \\ X_{0,3} \\ X_{2,0} \\ X_{2, 1} \\
X_{2, 2} \\ X_{2,3} \\ X_{3,0} \\ X_{3, 1} \\ X_{3, 2} \\ X_{3,3}
\end{bsmallmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="189" src="../../OEBPS/Images/eq_10-06.png" width="595"/></p>
</div>
<p class="fm-equation-caption">Equation 10.6 <span class="calibre" id="eq-2dconvwt-mat"/></p>
<p class="body">Note the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The <span class="math">2</span>D convolution weight matrix shown in equation <a class="url" href="#eq-2dconvwt-mat">10.6</a> is for the special case, but it illustrates the general principle.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The <span class="math">2</span>D convolution weight matrix is block diagonal, just like the <span class="math">1</span>D version. The kernel weights are placed precisely to emulate figure <a class="url" href="#fig-conv2d-edge-detection-stride1">10.9</a>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The convolution weight matrix has <span class="math">9</span> rows and <span class="math">16</span> columns. Thus it takes a input vector rasterized from a <span class="math">4 × 4</span> input image) and generates a output matrix (which can be folded into a <span class="math">3 × 3</span> convolution output image.</p>
</li>
</ul>
<h2 class="fm-head" id="sec-conv-3d">10.4 Three-dimensional convolution</h2>
<p class="body"><a id="marker-368"/>If a picture is worth a thousand words, a video is worth 10,000 words. Videos are a rich source of information about dynamic real-life scenes. As deep learning-based image analysis (<span class="math">2</span>D convolution) is becoming more and more successful, at the time of this writing, video analysis is becoming the next research frontier to conquer.</p>
<p class="body">Videos are essentially three-dimensional entities. The representation is <i class="fm-italics">discrete</i> in all three dimensions. The three dimensions correspond to <i class="fm-italics">space</i>, which is two-dimensional, having <i class="fm-italics">height</i> and <i class="fm-italics">width</i>, and <i class="fm-italics">time</i>. A video consists of a <i class="fm-italics">sequence of frames</i>. Each frame is an image: a discrete <span class="math">2</span>D array of pixels. A frame represents the entire video scene at a specific (sampled) point. A pixel in a frame represents the color of a sampled location in space belonging to the scene at the time corresponding to the frame. Thus a video is a sequence of frames representing the dynamic scene at a sampled set of discrete points (pixels) in space and time. The video extends over a <i class="fm-italics">spatio-temporal volume</i> (aka <i class="fm-italics">ST volume</i>), which can be imagined as a cuboid. Each cross-section is a rectangle representing a frame. This is shown in figure <a class="url" href="#fig-st-vol-with-frame">10.12</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre17" height="531" id="fig-st-vol-with-frame" src="../../OEBPS/Images/CH10_F12_Chaudhury.jpg" width="600"/></p>
<p class="figurecaption">Figure 10.12 A spatio-temporal volume light-shaded cuboid) representing a video. Individual frames of the video are cross-sectional rectangles in this ST volume. A single frame is also shown in darker shading.</p>
</div>
<p class="body">To analyze the video, we need to extract local patterns from this <span class="math">3</span>D volume. Can we do it via repeated <span class="math">2</span>D convolutions?</p>
<p class="body">The answer is <i class="fm-italics">no</i>. There is extra information when we view the successive frames <i class="fm-italics">together</i>, which is absent when we view the frames one at a time. For instance, imagine you are presented with an image of a half-open door. From that single image, can you determine whether the door is <i class="fm-italics">opening</i> or <i class="fm-italics">closing</i>? No, you cannot. To make that determination, we need to see several successive frames. In other words, analyzing a video one frame at a time robs us of a vital modality of information: <i class="fm-italics">motion</i>, which can be understood only if we analyze multiple successive frames together. This is why we need <span class="math">3</span>D convolution.</p>
<p class="body">The best way to visualize a <span class="math">3</span>D convolution is to imagine a <i class="fm-italics">brick</i> sliding over the entire volume of a <i class="fm-italics">room</i>. The room corresponds to the ST volume of the video input to the convolution. The brick corresponds to the kernel. While sliding, the brick stops at successive positions; we call these slide stops. Figure <a class="url" href="#fig-3d-conv-with-kernel">10.13</a> shows four slide stops at different positions. Each slide stop emits one output point. As the brick sweeps over the entire input ST volume, an output ST volume is generated. At each slide stop, we multiply each input pixel value by the kernel element covering it and take a sum of the products. This is effectively a weighted sum of all the input (room) elements covered by the kernel (brick), with the covering kernel elements serving as the weights.<a id="marker-369"/></p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="312" id="fig-3d-conv-with-kernel-0" src="../../OEBPS/Images/CH10_F13a_Chaudhury.png" width="529"/></p>
<p class="figurecaption">(a) slide stop <span class="math"><i class="fm-italics">x</i> = 0, <i class="timesitalic">y</i> = 0, <i class="fm-italics">t</i> = 0</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="313" id="fig-3d-conv-with-kernel-1" src="../../OEBPS/Images/CH10_F13b_Chaudhury.png" width="529"/></p>
<p class="figurecaption">(b) slide stop <span class="math"><i class="fm-italics">x</i> = 0, <i class="timesitalic">y</i> = 0, <i class="fm-italics">t</i> = 0</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="308" id="fig-3d-conv-with-kernel-2" src="../../OEBPS/Images/CH10_F13c_Chaudhury.png" width="529"/></p>
<p class="figurecaption">(c) slide stop <span class="math"><i class="fm-italics">x</i> = 0, <i class="timesitalic">y</i> = 0, <i class="fm-italics">t</i> = 0</span>.</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre17" height="314" id="fig-3d-conv-with-kernel-3" src="../../OEBPS/Images/CH10_F13d_Chaudhury.png" width="530"/></p>
<p class="figurecaption">(d) slide stop <span class="math"><i class="fm-italics">x</i> = 0, <i class="timesitalic">y</i> = 0, <i class="fm-italics">t</i> = 0</span>.</p>
</div>
<p class="fm-table-caption" id="fig-3d-conv-with-kernel">Figure 10.13 Spatio-temporal view of <span class="math">3</span>D convolution. The larger, light-shaded cuboid on the left of each figure represents the input ST volume (room). The small, dark-shaded cuboid inside the room represents the kernel brick). The brick slides all over the room’s internal volume. Neighboring positions of the brick may overlap in volume. Each position of the brick represents a slide stop; a weighted sum is taken of all points in the room (input points) covered by the brick. The brick point kernel value) covering each input point serves as the weight. Four different slide stops are shown. Each slide stop generates a single output point. As the brick sweeps the input volume, an output ST volume the smaller light-shaded cuboid) is generated.</p>
<p class="body">Let’s denote the input ST volume by <i class="timesitalic">S</i>. It is a <span class="math">3</span>D grid whose domain is</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">S</i> = [0, <i class="fm-italics">T</i> − 1] × [0, <i class="fm-italics">H</i> − 1] × [0, <i class="fm-italics">W</i> − 1]</span></p>
<p class="body">Every point in <i class="timesitalic">S</i> is a pixel with a color value (which can be a scalar—a gray-level value—or a vector of three values, R, G, B. On this grid of input points, we define a subgrid <i class="timesitalic">S<sub class="fm-subscript">o</sub></i> of output points. <i class="timesitalic">S<sub class="fm-subscript">o</sub></i> is obtained from <i class="timesitalic">S</i> by applying stride-based stepping on the input. Assuming <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [<i class="fm-italics">s<sup class="fm-superscript">T</sup></i>, <i class="fm-italics">s<sub class="fm-subscript">K</sub></i>, <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>]</span> denotes the <span class="math">3</span>D stride vector, the first slide stop has the top-left corner of the brick at <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">0</sub> ≡ (<i class="fm-italics">t</i> = 0, <i class="fm-italics">y</i> = 0, <i class="fm-italics">x</i> = 0)</span>. The next slide stop is at <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">1</sub> ≡ (<i class="fm-italics">t</i> = 0, <i class="fm-italics">y</i> = 0, <i class="fm-italics">x</i> = <i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)</span>, and the next is at <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">2</sub> ≡ (<i class="fm-italics">t</i> = 0, <i class="fm-italics">y</i> = 0, <i class="fm-italics">x</i> = 2<i class="fm-italics">s<sub class="fm-subscript">W</sub></i>)</span>. When we reach the right end, we increment <i class="fm-italics">y</i>. When we reach the bottom, we increment <i class="timesitalic">t</i>. When we reach the end of the room, we stop. <span class="math"><i class="fm-italics">S<sub class="fm-subscript">o</sub></i> = {<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">0</sub>, <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span><sub class="fm-subscript">1</sub> … }</span> are the points at which the top-left corner of the kernel (brick) rests as it sweeps over the input volume. There is an output for each point in <i class="timesitalic">S<sub class="fm-subscript">o</sub></i>. The kernel also has three dimensions (in practice, it has two more dimensions corresponding to the input channels and batch—we are ignoring them now for simplicity—as discussed in section <a class="url" href="#sec-pytorch-3d-conv">10.4.2.1</a>).</p>
<p class="body"><a id="marker-370"/>Equation <a class="url" href="#eq-3dconv-output">10.7</a> shows how a single output value is generated in <span class="math">3</span>D convolution. <i class="timesitalic">X</i> denotes the input, <i class="timesitalic">Y</i> denotes the output, and <i class="timesitalic">W</i> denote the kernel weights:</p><!--<p class="Body"><span class="times">$$Y_{t, y, x} =
\sum_{k=0}^{k_{T}}  \sum_{i=0}^{k_{H}}  \sum_{j=0}^{k_{W}} X_{t+k, y+i,
x+j} W_{k, i, j}  \;\;\;\;\;\; \forall \left(t, y, x\right) \in S_{o}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_10-07.png" width="388"/></p>
</div>
<p class="fm-equation-caption">Equation 10.7 <span class="calibre" id="eq-3dconv-output"/></p>
<p class="body">Note that the kernel (brick) has its origin on <span class="math"><i class="fm-italics">X<sub class="fm-subscript">t</sub></i>, <i class="fm-italics">y</i>, <i class="fm-italics">x</i></span>. Its dimensions are <span class="math">(<i class="fm-italics">k<sup class="fm-superscript">T</sup></i>, <i class="fm-italics">k<sub class="fm-subscript">H</sub></i>, <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>)</span>. So, it covers all input pixels in the domain <span class="math">[<i class="fm-italics">t</i>..(<i class="fm-italics">t</i> + <i class="fm-italics">k<sup class="fm-superscript">T</sup></i>)] × [<i class="fm-italics">y</i>..(<i class="fm-italics">y</i> + <i class="fm-italics">k<sub class="fm-subscript">H</sub></i>)] × [<i class="fm-italics">x</i>..(<i class="fm-italics">x</i> + <i class="fm-italics">k<sub class="fm-subscript">W</sub></i>)]</span>. These are the pixels participating in equation <a class="url" href="#eq-3dconv-output">10.7</a>. Each of these input pixels is multiplied by the kernel element covering it. Match equation <a class="url" href="#eq-3dconv-output">10.7</a> with figure <a class="url" href="#fig-3d-conv-with-kernel">10.13</a>.</p>
<h3 class="fm-head1" id="sec-conv3d-motion-detect">10.4.1 Video motion detection via 3D convolution</h3>
<p class="body">A moving object in a dynamic scene changes position from one video frame to another. Consequently, pixels are covered or uncovered at the boundary of motion. Pixels belonging to the background in one frame may be covered by the object in a subsequent frame and vice versa. If the background is a different color than the object, this will cause a color difference between pixels at identical spatial locations at different times, as illustrated in figure <a class="url" href="#fig-3d-conv-motion-img">10.14</a>. The output of applying convolution to an ST volume is another ST volume. Figure <a class="url" href="#fig-3d-conv-output">10.15</a> shows a few frames from the output resulting from applying our video motion detector to the input shown in figure <a class="url" href="#fig-3d-conv-motion-img">10.14</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre22" height="394" id="fig-3d-conv-motion-img" src="../../OEBPS/Images/CH10_F14_Chaudhury.png" width="400"/></p>
<p class="figurecaption">Figure 10.14 Successive frames of a synthetic video of a moving ball, shown in a superimposed fashion with gradually increasing opacity for illustration purposes</p>
</div>
<p class="body"><a id="marker-371"/>How does a kernel extract motion information from a set of successive frames? As mentioned earlier, motion causes pixels at the same position in successive frames to have different colors. However, a single isolated pair of pixels may have different colors due to noise—we cannot draw any conclusions from that. If we average the pixel values in a small neighborhood in one frame and average the pixel values in the same neighborhood in the subsequent frames, and these two averages are different, that is a more reliable way to estimate motion. Following is a <span class="math">2 × 3 × 3</span> <span class="math">3</span>D kernel to do exactly that—average pixel values in a <span class="math">3 × 3</span> spatial neighborhood in two successive frames and subtract one from the other:</p><!--<p class="Body"><span class="times">$$\underbrace{
\underbrace{
\overbrace{
\begin{bmatrix} w_{0, 0, 0} = -1 &amp; w_{0, 0, 1} = -1 &amp; w_{0, 0, 2} = -1\\ w_{0, 1, 0} =-1 &amp; w_{0, 1, 1} = -1 &amp; w_{0, 1, 2} = -1\\ w_{0, 2, 0} =-1 &amp; w_{0, 2, 1} = -1 &amp; w_{0, 2, 2} = -1
\end{bmatrix}
}^{\text{kernel weights, } t = 0}
}_{\text{negative spatial average}}
\underbrace{
\overbrace{
\begin{bmatrix} w_{1, 0, 0} = 1 &amp; w_{1, 0, 1} = 1 &amp; w_{1, 0, 2} = 1\\ w_{1, 1, 0} =1 &amp; w_{1, 1, 1} = 1 &amp; w_{1, 1, 2} = 1\\ w_{1, 2, 0} =1 &amp; w_{1, 2, 1} = 1 &amp; w_{1, 2, 2} = 1
\end{bmatrix}
}^{\text{kernel weights, } t = 1}
}_{\text{positive spatial average}}
}_{\text{temporal difference of spatial averages; motion detector kernel}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="216" src="../../OEBPS/Images/eq_10-07-a.png" width="577"/></p>
</div>
<p class="body">The result of the subtraction is high in regions of motion and low in regions of no motion. In this context, it is worthwhile to note that since the object is of uniform color, pixels within the object are indistinguishable. Consequently, no motion is observed at the center of the object; motion is observed only at the boundary. A few individual frames of the result of this <span class="math">3</span>D convolution are shown in figure <a class="url" href="#fig-3d-conv-output">10.15</a>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for video motion detection, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/enJQ">http://mng.bz/enJQ</a>.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre28" height="375" src="../../OEBPS/Images/CH10_F15a_Chaudhury.png" width="375"/></p>
<p class="figurecaption">(a) Output frame <span class="math">0</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre28" height="375" src="../../OEBPS/Images/CH10_F15b_Chaudhury.png" width="375"/></p>
<p class="figurecaption">(b) Output frame <span class="math">1</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre28" height="375" src="../../OEBPS/Images/CH10_F15c_Chaudhury.png" width="375"/></p>
<p class="figurecaption">(c) Output frame <span class="math">2</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre28" height="373" src="../../OEBPS/Images/CH10_F15d_Chaudhury.png" width="374"/></p>
<p class="figurecaption">(d) Output frame <span class="math">3</span></p>
</div>
<p class="figurecaption" id="fig-3d-conv-output">Figure 10.15 Result of applying a <span class="math">3</span>D convolution motion detector to the synthetic video of a moving ball. Gray signifies “no motion"; most of the output frames are gray. White and black signify motion.</p>
<h3 class="fm-head1" id="pytorch-three-dimensional-convolution-with-custom-weights">10.4.2 PyTorch- Three-dimensional convolution with custom weights<a id="marker-372"/></h3>
<p class="body">In section <a class="url" href="#sec-conv3d-motion-detect">10.4.1</a>, we saw how to detect motion in a sequence of input images using <span class="math">3</span>D convolutions. In this section, we see how to implement this in PyTorch. The PyTorch interface to <span class="math">3</span>D convolutions expects <span class="math">5</span>-dimensional input tensors of the form <i class="timesitalic">N</i> <span class="math">×</span> <i class="timesitalic">C</i> <span class="math">×</span> <i class="timesitalic">D</i> <span class="math">×</span> <i class="timesitalic">H</i> <span class="math">×</span> <i class="timesitalic">W</i>. In addition to the dimensions discussed in section <a class="url" href="#sec-conv-3d">10.4</a>, there is an additional dimension for the input channels. Thus, there is a separate brick for each input channel. We are combining (taking the weighted sum of) them all:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">As discussed in the case of <span class="math">2</span>D convolutions (section <a class="url" href="#sec-pytorch-2d-conv">10.3.3</a>), the first dimension <i class="timesitalic">N</i> stands for the batch size minibatches are fed to a real neural network instead of individual input instances for efficiency reasons), and <i class="timesitalic">C</i> stands for the number of input channels.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">D</i> stands for the sequence length. In our motion detector example, <i class="timesitalic">D</i> represents the number of successive image frames fed to the <span class="math">3</span>D convolution layer.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The third dimension, <i class="timesitalic">H</i>, stands for height, and the fourth dimension, <i class="timesitalic">W</i>, stands for width.</p>
</li>
</ul>
<p class="body">In our motion detector example, we have a sequence of five grayscale images as input, each with height = 320 and width = 320. Since we are considering only a single image sequence, <i class="timesitalic">N</i> = 1. All images are grayscale, which implies that <i class="timesitalic">C</i> = 1. The sequence length, <i class="timesitalic">D</i>, is equal to 5. <i class="timesitalic">H</i> and <i class="timesitalic">W</i> are both 320.</p>
<p class="body">PyTorch expects the <span class="math">3</span>D kernels to be of the form <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> <span class="math">×</span> <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">T</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">H</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">W</sub></i>:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The first dimension, <i class="timesitalic">C<sub class="fm-subscript">out</sub></i>, represents the number of output channels. You can think of the convolutional kernel as a bank of <span class="math">3</span>D filters, where each filter produces one output channel. <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> is the number of <span class="math">3</span>D filters in the bank.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The second dimension, <i class="timesitalic">C<sub class="fm-subscript">in</sub></i>, represents the number of input channels. This depends on the number of channels in the input tensor. When we are dealing with grayscale images, <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> is 1 at the grand input. For RGB images, <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> is 3 at the grand input. For layers further from the input, <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> equals the number of channels in the tensor fed to that layer.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The third, fourth, and fifth dimensions, <i class="timesitalic">k<sub class="fm-subscript">T</sub></i>, <i class="timesitalic">k<sub class="fm-subscript">H</sub></i>, and <i class="timesitalic">k<sub class="fm-subscript">W</sub></i>, represent the kernel sizes along the <i class="timesitalic">T</i>, <i class="timesitalic">H</i>, and <i class="timesitalic">W</i> dimensions, respectively</p>
</li>
</ul>
<p class="body"><a id="marker-373"/>In our motion detector example, we have a single kernel with <i class="timesitalic">k<sub class="fm-subscript">T</sub></i>=<span class="math">2</span>, <i class="timesitalic">k<sub class="fm-subscript">H</sub></i>=<span class="math">3</span>, and <i class="timesitalic">k<sub class="fm-subscript">W</sub></i> = <span class="math">3</span>. Since we only have a single kernel, <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> = <span class="math">1</span>. And since we are dealing with grayscale images, <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> is also 1.</p>
<p class="fm-code-listing-caption" id="sec-pytorch-3d-conv">Listing 10.6 PyTorch code for 3D convolution</p>
<pre class="programlisting">import torch

images = load_images()                            <span class="fm-combinumeral">①</span>

x = torch.tensor(images)                          <span class="fm-combinumeral">②</span>

w_2d_smoothing = torch.tensor(                    <span class="fm-combinumeral">③</span>
        [[0.11, 0.11, 0.11],
         [0.11, 0.11, 0.11],
         [0.11, 0.11, 0.11]]).unsqueeze(0)

w = torch.cat(
        [-w_2d_smoothing, w_2d_smoothing])        <span class="fm-combinumeral">④</span>

x = x.unsqueeze(0).unsqueeze(0)                   <span class="fm-combinumeral">⑤</span>

w = w.unsqueeze(0).unsqueeze(0)                   <span class="fm-combinumeral">⑥</span>

conv3d = nn.Conv3d(1, 1, kernel_size=[2, 3, 3],   <span class="fm-combinumeral">⑦</span>
                stride=1, padding=0, bias=False)
conv3d.weight = torch.nn.Parameter(w, requires_grad=False)

with torch.no_grad():                             <span class="fm-combinumeral">⑧</span>

    y = conv3d(x)                                 <span class="fm-combinumeral">⑨</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads a sequence of five grayscale images with shape 320 <span class="math">×</span> 320</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts to a tensor of shape <span class="math"><i class="fm-italics">T</i> × <i class="fm-italics">H</i> × <i class="fm-italics">W</i> = 5 × 320 × 320</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates a <span class="math">2</span>D smoothing kernel of shape <span class="math">3 × 3</span>. Pads an extra dimension so that two <span class="math">2</span>D kernels can be stacked together to form a <span class="math">3</span>D kernel.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Concatenates the <span class="math">2</span>D smoothing kernel and its inverted version along the first dimension to form a <span class="math">3</span>D kernel of shape <span class="math">2 × 3 × 3</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Converts the input tensor to <span class="math"><i class="fm-italics">N</i> × <i class="fm-italics">C</i> × <i class="fm-italics">T</i> × <i class="fm-italics">H</i> × <i class="fm-italics">W</i> = 1 × 1 × 5 × 320 × 320</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Converts the <span class="math">3</span>D kernel to <span class="math"><i class="fm-italics">C<sub class="fm-subscript">out</sub></i> × <i class="fm-italics">C<sub class="fm-subscript">in</sub></i> × <i class="fm-italics">k<sub class="fm-subscript">T</sub></i> × <i class="fm-italics">k<sub class="fm-subscript">H</sub></i> × <i class="fm-italics">k<sub class="fm-subscript">W</sub></i> = 1 × 1 × 2 × 3 × 3</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Instantiates and sets the weights of the Conv3d layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Instructs PyTorch to not compute gradients since we currently don’t require them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Runs the convolution</p>
<h2 class="fm-head" id="sec-transposed-conv">10.5 Transposed convolution or fractionally strided convolution</h2>
<p class="body"><a id="marker-374"/>As usual, we examine this topic with an example. Consider a <span class="math">1</span>D convolution with kernel <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> = [<i class="fm-italics">w</i><sub class="fm-subscript">0</sub>   <i class="fm-italics">w</i><sub class="fm-subscript">1</sub>   <i class="fm-italics">w</i><sub class="fm-subscript">2</sub>]</span> of size <span class="math">3</span>, with valid padding. Let’s consider a special case where the input size <i class="timesitalic">n</i> is <span class="math">5</span>. Following equation <a class="url" href="#eq-conv1d-as-malt-mult-k3s1valid">10.2</a>, this convolution can be expressed as a multiplication of a block-diagonal matrix <i class="timesitalic">W</i> constructed from the weights vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, with input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> as follows:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{y} &amp;=\vec{w} \circledast \vec{x} = W \vec{x} \\
&amp;=
\begin{bmatrix} w_{0} x_{0} + w_{1} x_{1} + w_{2} x_{2}\\ w_{0} x_{1} + w_{1} x_{2} + w_{2} x_{2}\\ w_{0} x_{2} + w_{1} x_{32} + w_{2} x_{4}
\end{bmatrix} =
\begin{bmatrix} w_{0}     &amp;  w_{1}     &amp; w_{2}     &amp; 0           &amp; 0           \\ 0           &amp;  w_{0}     &amp; w_{1}     &amp; w_{2}     &amp; 0           \\ 0           &amp;  0           &amp; w_{0}     &amp; w_{1}     &amp; w_{2}
\end{bmatrix}
\begin{bmatrix}
x_{0}\\
x_{1}\\
x_{2}\\
x_{3}\\
x_{4}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="168" src="../../OEBPS/Images/eq_10-07-b.png" width="549"/></p>
</div>
<p class="body">What happens if we multiply the output vector <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> by the transposed matrix <i class="timesitalic">W<sup class="fm-superscript">T</sup></i>?</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\tilde{x} &amp;= W^{T} \vec{y} &amp; \\
&amp; =
\begin{bmatrix} w_{0}            &amp; 0                    &amp; 0           \\ w_{1}            &amp;  w_{0}             &amp; 0           \\ w_{2}            &amp; w_{1}             &amp; w_{0}      \\ 0                  &amp; w_{2}             &amp; w_{1}      \\ 0                  &amp;  0                  &amp; w_{2}
\end{bmatrix}
\begin{bmatrix} _{0}\\ _{1}\\ _{2}
\end{bmatrix}
=
\begin{bmatrix} w_{0} _{0}    &amp;                       &amp;                       \\ w_{1} y_{0} + &amp;w_{0} y_{1}      &amp;                       \\ w_{2} y_{0} + &amp;w_{1} y_{1}  + &amp;w_{0} y_{2}      \\
                     &amp;w_{2} y_{1}  + &amp;w_{1} y_{2}      \\
                     &amp;                      &amp;w_{2}  y_{2}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="171" src="../../OEBPS/Images/eq_10-07-c.png" width="438"/></p>
</div>
<p class="body">Following are some observations:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">We haven’t quite recovered <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> from <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>, but we have generated a vector, <i class="timesitalic">x̃</i>, the same size as <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>. Multiplying by the transpose of the weight matrix of the convolution performs a kind of upsampling, undoing the downsampling resulting from the forward convolution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">It is impossible to recover <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> from <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>. This is because when constructing <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, we multiplied by <i class="timesitalic">W</i> and converted a vector with five independent elements to a vector with three independent elements—some information was irretrievably lost. This intuition is consistent with the fact that a <span class="math">5 × 3</span> matrix <i class="timesitalic">W</i> is <i class="fm-italics">non-invertible</i>: there is no <span class="math"><i class="fm-italics">W</i><sup class="fm-superscript">−1</sup></span>, so there is no way to get <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> = <i class="fm-italics">W</i><sup class="fm-superscript">−1</sup><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">During transpose convolution, we are distributing elements of <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> back to the elements of <i class="timesitalic">x̃</i> in the same proportion as when we were doing the forward convolution (see figure <a class="url" href="#fig-transpose-conv1d">10.16</a>). This should remind you of backpropagation from chapter <a class="url" href="../Text/08.xhtml#ch-training-neural-networks">8</a>. There, in equation <a class="url" href="../Text/08.xhtml#eq-MLP-out-nosubscript-1">8.24</a> right-hand side), we saw that for linear layers, forward propagation amounts to multiplying by an arbitrary weight matrix <i class="timesitalic">W</i> (shown in equation <a class="url" href="../Text/08.xhtml#eq-MLP-weight-matrix">8.8</a>). Backpropagation involves multiplying by the transpose of the same weight matrix (equation <a class="url" href="../Text/08.xhtml#eq-auxvar-vector">8.31</a>). The backpropagation does a <i class="fm-italics">proportional blame distribution</i>—the loss is distributed back to the inputs in the same proportion as their contribution in creating the output. The same thing is happening here. Thus, multiplying by the transposed weight matrix in general distributes the output back in the same ratio in which it contributes to the output.</p>
</li>
</ul>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="1175" id="fig-transpose-conv1d" src="../../OEBPS/Images/CH10_F16_Chaudhury.png" width="978"/></p>
<p class="figurecaption">Figure 10.16 1D convolution and its transpose</p>
</div>
<p class="body">The idea extends to higher dimensions. Figure <a class="url" href="#fig-transpose-conv2d-stride-1-padding-0">10.17</a> illustrates a <span class="math">2</span>D transpose convolution operation.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="792" id="fig-transpose-conv2d-stride-1-padding-0" src="../../OEBPS/Images/CH10_F17_Chaudhury.png" width="1022"/></p>
<p class="figurecaption">Figure 10.17 2D convolution and its transpose</p>
</div>
<h3 class="fm-head1" id="subsec-auto-enc">10.5.1 Application of transposed convolution: Autoencoders and embeddings</h3>
<p class="body"><a id="marker-375"/>Transposed convolution is typically required in autoencoders. We provide a very brief outline of autoencoders at this point to explain why they need transposed convolution. Most of the neural networks we have looked at so far are examples of supervised classifiers in that they take an input and directly output the class to which the input belongs. This is not the only paradigm possible. As hinted in section <a class="url" href="../Text/06.xhtml#sec-GMM">6.9</a>, we can also map an input to a vector often called the <i class="fm-italics">embedding</i>, aka <i class="fm-italics">descriptor vector</i>) that captures the essential aspects of the class of interest and throws away the variable aspects. For instance, if the class of interest is a human, then given an image, the embedding will only capture the features that recognize the humans in the image and ignore the background (sky, sea, forest, building, and so on).</p>
<p class="body">The mapping from input to embedding is done by a neural network called an <i class="fm-italics">encoder</i>. If the input is an image, the encoder typically contains a sequence of convolution layers.</p>
<p class="body"><a id="marker-376"/>How do we train this neural network? How do we define its loss? Well, one possibility is that the embedding must maintain fidelity to the original input: that is, we should be able to reconstruct (at least approximately) the input from the embedding. Remember, the embedding is smaller in size (with fewer degrees of freedom) than the input, so perfect reconstruction is impossible. Still, we can define loss as the difference (for example, Euclidean distance or binarized cross-entropy loss) between the original input and the reconstructed input.</p>
<p class="body">How do we reconstruct the input from the embedding? This is where transposed convolution comes in. Remember, we did convolution (perhaps many times) in our encoder to generate the embedding. We can do a set of transposed convolutions on the embedding to generate a tensor of the same size as the input. The network to do this reconstruction is called the <i class="fm-italics">decoder</i>. The decoder generates our reconstructed input.</p>
<p class="body">We define a loss as the difference between the original and reconstructed input. We can train to minimize the loss and learn the weights of both the encoder and decoder. This is called <i class="fm-italics">end-to-end learning</i>, and the encoder-decoder pair is called an <i class="fm-italics">autoencoder</i>.</p>
<p class="body">We train the autoencoder with many data instances, all belonging to the class of interest. Since it does not have the luxury of remembering the entire image (the embedding being smaller in size than the input), it is forced to learn how to retain the features common to all the training images: that is, the features that describe the class of interest. In our example, the autoencoder will learn to retain features that identify a human and drop the background. Note that this could also lead to a very effective <i class="fm-italics">compression technique</i>—the embedding is a compact representation of the image in which only the objects of interest have been retained.<a id="marker-377"/></p>
<h3 class="fm-head1" id="sec-trans-conv1d-out-size">10.5.2 Transposed convolution output size</h3>
<p class="body">The output size of transposed convolution can be obtained by inverting equation <a class="url" href="#eq-conv-out-size">10.8</a>:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">o</i><sup class="fm-superscript">′</sup> = (<i class="fm-italics">n</i><sup class="fm-superscript">′</sup>−1)<i class="fm-italics">s</i> + <i class="fm-italics">k</i> − 2<i class="fm-italics">p</i></span></p>
<p class="fm-equation-caption">Equation 10.8 <span class="calibre" id="eq-trans-conv1d-out-size"/></p>
<p class="body">For instance, transposed convolution with stride <span class="math"><i class="fm-italics">s</i> = 1</span> on a <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> of size <span class="math"><i class="fm-italics">n</i><sup class="fm-superscript">′</sup> = 3</span> with valid padding <span class="math"><i class="fm-italics">p</i> = 0</span>) and a kernel of size <span class="math"><i class="fm-italics">k</i> = 3</span> creates an output <i class="timesitalic">x̃</i> of size <span class="math"><i class="fm-italics">o</i><sup class="fm-superscript">′</sup> = 5</span>.</p>
<h3 class="fm-head1" id="upsampling-via-transpose-convolution">10.5.3 Upsampling via transpose convolution</h3>
<p class="body">In the previous section, we briefly discussed autoencoders, where an encoder network maps an input image into an embedding and a decoder network tries to reconstruct the input image from the embedding. The encoder network converts a higher-resolution input into a lower-resolution embedding by passing the input through a series of convolution and pooling layers (we discuss pooling layers in detail in the next chapter). The decoder network, which tries to reconstruct the original image from the embedding, has to upscale/upsample a lower-resolution input into a higher-resolution output.</p>
<p class="body">Many interpolation techniques, such as nearest neighbor, bilinear, and bicubic interpolation, can be used to perform this upsampling operation. These techniques typically use predefined mathematical functions to map lower-resolution inputs to higher-resolution outputs. However, a more optimal way to perform upsampling is through transpose convolutions, where the mapping function is learned during the training process instead of being predefined. The neural network will learn the best way to distribute the input elements across a higher-resolution output map so that the final reconstruction error is minimized (that is, the final output is as close to the original input image as possible). We do not get into the details of training an autoencoder in this chapter; however, we show how input images can be upsampled using transpose convolutions:<a id="marker-378"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The input array is converted to a <span class="math">4</span>D tensor of shape <i class="timesitalic">N</i> <span class="math">×</span> <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> <span class="math">×</span> <i class="timesitalic">H</i> <span class="math">×</span> <i class="timesitalic">W</i>, where <i class="timesitalic">N</i> is the batch size, <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> is the number of input channels, <i class="timesitalic">H</i> is the height, and <i class="timesitalic">W</i> is the width.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The kernel is a <span class="math">4</span>D tensor of shape <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> <span class="math">×</span> <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">H</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">W</sub></i>, where <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> is the number of input channels, <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> is the number of output channels, <i class="timesitalic">k<sub class="fm-subscript">H</sub></i> is the kernel height, and <i class="timesitalic">k<sub class="fm-subscript">W</sub></i> is the kernel width. Note how this differs from the regular 2D convolutional kernel, which is expected to be of shape <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> <span class="math">×</span> <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">H</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">W</sub></i>. Essentially, the input and output channel dimensions are interchanged.</p>
</li>
</ul>
<p class="body">Figure <a class="url" href="#fig-transpose-conv2d-stride-2">10.18</a> shows an example with input of shape <span class="math">1 × 1 × 2 × 2</span>. The kernel is of shape <span class="math">1 × 1 × 2 × 2</span>. Transpose convolution with stride 2 results in an output of shape <span class="math">1 × 1 × 4 × 4</span>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="882" id="fig-transpose-conv2d-stride-2" src="../../OEBPS/Images/CH10_F18_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 10.18 Upscaling using 2D transpose convolution with stride 2<a id="marker-379"/></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for transpose convolution, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/radD">http://mng.bz/radD</a>.</p>
<p class="fm-code-listing-caption" id="listing-10.7-pytorch-code-for-upsampling-using-transpose-convolutions">Listing 10.7 PyTorch code for upsampling using transpose convolutions</p>
<pre class="programlisting">import torch

x = torch.tensor([                               <span class="fm-combinumeral">①</span>
        [5., 6.],
        [7., 8.]
    ])

w = torch.tensor([                               <span class="fm-combinumeral">②</span>
        [1., 2.],
        [3., 4.]
    ])

x = x.unsqueeze(0).unsqueeze(0)                  <span class="fm-combinumeral">③</span>

w = w.unsqueeze(0).unsqueeze(0)                  <span class="fm-combinumeral">④</span>

transpose_conv2d = torch.nn.ConvTranspose2d(     <span class="fm-combinumeral">⑤</span>
    1, 1, kernel_size=2, stride=2, bias=False)

transpose_conv2d.weight = torch.nn.Parameter(w,  <span class="fm-combinumeral">⑥</span>
                requires_grad=False)


with torch.no_grad():                            <span class="fm-combinumeral">⑦</span>

    y = transpose_conv2d(x)                      <span class="fm-combinumeral">⑧</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the input tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the weights of the kernel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts the input tensor to <i class="timesitalic">N</i> <span class="math">×</span> <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> <span class="math">×</span> <i class="timesitalic">H</i> <span class="math">×</span> <i class="timesitalic">W</i> = <span class="math">1</span> <span class="math">×</span> <span class="math">1</span> <span class="math">×</span> <span class="math">2</span> <span class="math">×</span> <span class="math">2</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Converts the kernel to <i class="timesitalic">C<sub class="fm-subscript">in</sub></i> <span class="math">×</span> <i class="timesitalic">C<sub class="fm-subscript">out</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">H</sub></i> <span class="math">×</span> <i class="timesitalic">k<sub class="fm-subscript">W</sub></i> = <span class="math">1</span> <span class="math">×</span> <span class="math">1</span> <span class="math">×</span> <span class="math">2</span> <span class="math">×</span> <span class="math">2</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Instantiates the transpose convolution layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Sets the kernel weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Instructs PyTorch to not compute gradients since we currently don’t require them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Runs the transpose convolution. <i class="timesitalic">y</i> is of shape <span class="math">4 × 4</span>.</p>
<h2 class="fm-head" id="sec-conv2or3d-in-NN">10.6 Adding convolution layers to a neural network</h2>
<p class="body">Until now, we have been discussing convolution layers with custom weights that we set. While this gives us a conceptual understanding of how convolution works, in real neural networks, we do <i class="fm-italics">not</i> set the convolution weights ourselves. Rather, we expect the weights to be learned from loss minimization via backpropagation, as described in chapters <a class="url" href="../Text/08.xhtml#ch-training-neural-networks">8</a> and <a class="url" href="../Text/09.xhtml#ch-loss-optim-reg">9</a>. We look at popular neural network architectures in the next chapter. But from a programming point of view, the most important thing to learn is how to add a convolution layer to a neural network. This is what we learn in the following section.<a id="marker-380"/></p>
<p class="body">As part of setting up the neural network, we specify its dimensions but not the weights. We also initialize the weight values. The weight values are updated during the backpropagation (the <code class="fm-code-in-text">loss.backward()</code> call) somewhat behind the scene (although PyTorch allows us to view their values if we choose to).</p>
<h3 class="fm-head1" id="sec-pytorch-conv2or3d-in-NN">10.6.1 PyTorch- Adding convolution layers to a neural network</h3>
<p class="body">Let’s see how a convolutional layer is implemented as part of a larger neural network in PyTorch (the full neural network architecture is discussed in detail in the next chapter):</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A neural network typically subclasses the <code class="fm-code-in-text">torch.nn.Module</code> base class and implements the <code class="fm-code-in-text">forward()</code> method. The layers of the neural network are instantiated in the <code class="fm-code-in-text">__init__()</code> function.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><code class="fm-code-in-text">torch.nn.Sequential</code> is used to chain multiple layers one after another. The output of the first layer is fed into the second layer, and so on.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Each <code class="fm-code-in-text">torch.nn.Conv2d()</code> represents a single convolutional layer. Our code snippet instantiates three such convolutional layers with other layers in between (details are covered in the next chapter).</p>
</li>
</ul>
<p class="fm-code-listing-caption" id="listing-10.8-pytorch-code-for-a-sample-convolutional-neural-network">Listing 10.8 PyTorch code for a sample convolutional neural network</p>
<pre class="programlisting">import torch

class SampleCNN(torch.nn.Module):
    def __init__(self, num_classes):
        super(LeNet, self).__init__()
        self.nn = torch.nn.Sequential(           <span class="fm-combinumeral">①</span>

            torch.nn.Conv2d(
                in_channels=1, out_channels=6,
                kernel_size=5, stride=1),        <span class="fm-combinumeral">②</span>
 
                         ... 
 
            torch.nn.Conv2d(
                in_channels=6, out_channels=16,
                kernel_size=5, stride=1),

                         ...

            torch.nn.Conv2d(
                in_channels=16, out_channels=120,
                kernel_size=5, stride=1),        <span class="fm-combinumeral">③</span>

                        ...
        )

    def forward(self, x):                        <span class="fm-combinumeral">④</span>
        out = self.nn(x)
        return out</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> torch.nn.Sequential is used to chain a sequence of layers together.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the convolutional layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Implements the forward pass</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Runs the convolution</p>
<h2 class="fm-head" id="sec-pooling">10.7 Pooling</h2>
<p class="body"><a id="marker-381"/>Until now, we have seen how a convolution layer slides over an input image and generates an output feature map that contains important features that describe the image. We looked at this in 1D, 2D, and 3D settings. In a typical deep neural network, multiple such convolution layers are stacked one after another to recognize more and more complex structures in the image. (We talk more about this in the next chapter.) A major drawback of the convolution layer is that it is very sensitive to the location of the features in the input. Minor variations in the position of input features can result in a different output feature map. Such variations can occur in the real world due to camera angle changes, rotations, crops, objects being present at varying distances from the camera, and so on. How do we handle such variations and make the neural network more robust?</p>
<p class="body">One way to do so is via downsampling. A lower-resolution version of the feature map still contains the important features but at a lower precision/granularity. So even if important features are present at slightly varying locations in higher-resolution feature maps, they will be more or less at the same location in the lower-resolution feature maps. This is also known as <i class="fm-italics">local translation invariance</i>.</p>
<p class="body">In convolution neural networks, the downsampling operation is performed by <i class="fm-italics">pooling</i> layers. Pooling layers essentially slide a small filter across the entire image. At each filter location, they capture a summary of the local patch using a pooling operation. The two most popular types of pooling operations are as follows:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Max pooling</i>—Calculates the maximum value for each patch</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Average pooling</i>—Calculates the average value for each patch</p>
</li>
</ul>
<p class="body">Figure <a class="url" href="#fig-2d-maxpool-stride-2">10.19</a> illustrates this in detail. The size of the output feature map depends on the kernel size and the stride of the pooling layer. For example, if we use a <span class="math">2 × 2</span> kernel with a stride of 2, as in figures <a class="url" href="#fig-2d-maxpool-stride-2">10.19</a> and <a class="url" href="#fig-2d-avgpool-stride-2">10.20</a>, the output feature map becomes half the size of the input feature map. Similarly, using a <span class="math">3 × 3</span> kernel with stride = 3 makes the output feature map one-third the size.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="825" id="fig-2d-maxpool-stride-2" src="../../OEBPS/Images/CH10_F19_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 10.19 Max pooling using a <span class="math">2 × 2</span> kernel with stride 2. The resulting output feature map is half the size of the input feature map. Each value of the output feature map is a max of the corresponding local patch in the input feature map.<a id="marker-382"/></p>
</div>
<p class="fm-code-listing-caption" id="listing-10.9-pytorch-code-for-max-and-average-pooling">Listing 10.9 PyTorch code for max and average pooling</p>
<pre class="programlisting">import torch

X = torch.tensor([                <span class="fm-combinumeral">①</span>
    [0, 12, 26, 39],
    [6, 19, 31, 44],
    [12, 25, 38, 50],
    [18, 31, 43, 57]
], dtype=torch.float32).unsqueeze(0).unsqueeze(0)

max_pool_2d = torch.nn.MaxPool2d( <span class="fm-combinumeral">②</span>
    kernel_size=2, stride=2)

out_max_pool = max_pool_2d(X)     <span class="fm-combinumeral">③</span>

avg_pool_2d = torch.nn.AvgPool2d( <span class="fm-combinumeral">④</span>
    kernel_size=2, stride=2)

out_avg_pool = avg_pool_2d(X)     <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates a <span class="math">4 × 4</span> input tensor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates a <span class="math">2 × 2</span> max pooling layer with stride 2</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Output feature map is of size <span class="math">2 × 2</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Instantiates a <span class="math">2 × 2</span> average pooling layer with stride 2</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Output feature map is of size <span class="math">2 × 2</span></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="825" id="fig-2d-avgpool-stride-2" src="../../OEBPS/Images/CH10_F20_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 10.20 Average pooling using a <span class="math">2 × 2</span> kernel with stride 2. The resulting output feature map is corresponding local patch in the input feature map.</p>
</div>
<h2 class="fm-head" id="summary-9">Summary</h2>
<p class="body"><a id="marker-383"/>In this chapter, we took an in-depth look at <span class="math">1</span>D, <span class="math">2</span>D, and <span class="math">3</span>D convolutions and their application to image and video analysis:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Convolutional layers help capture local patterns in input data because they connect only a small set of adjacent input values to an output value. This is different from the fully connected layers (aka linear layers) discussed in the previous chapters, where all inputs are connected to every output value.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A convolution operation involves sliding a kernel over an input array. It can conceptually be viewed as a matrix multiplication though it is not implemented this way for efficiency reasons). The kernel size, stride, and padding affect the size of the output.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The number of input elements over which the kernel slides upon completing a single step is known as <i class="fm-italics">stride</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">As the kernel reaches the extremities of the input array, parts of it may fall outside the array. To deal with such cases, multiple <i class="fm-italics">padding</i> strategies can be applied. In <i class="fm-italics">valid padding</i>, the convolution operation stops when even a single kernel element falls outside the input array. In <i class="fm-italics">same (zero) padding</i>, an input value of zero is assumed for all kernel elements that are outside the input array.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math">1</span>D convolutions can conceptually be viewed as sliding a measuring ruler (<span class="math">1</span>D kernel) across a stretched, straightened rope (<span class="math">1</span>D input array). Real-world applications of <span class="math">1</span>D convolutions include smoothing and edge detection in curves.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math">2</span>D convolutions can conceptually be viewed as sliding a tile (<span class="math">2</span>D kernel) over the entire surface area of a wall (<span class="math">2</span>D input array). Real-world applications of <span class="math">2</span>D convolutions include smoothing and edge detection in images.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math">3</span>D convolutions can conceptually be viewed as sliding a brick (<span class="math">3</span>D kernel) over the entire volume of a room <span class="math">3</span>D input array). Real-world applications of <span class="math">3</span>D convolutions include motion detection in an image sequence.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In transpose convolutions, the input array elements are multiplied by the kernel weights and then distributed across the output array. Real-world applications of transpose convolutions include upsampling, where lower-resolution inputs are converted into higher-resolution outputs. Autoencoders use transpose convolutions to reconstruct images from embeddings.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Pooling layers essentially slide a kernel across the input, capturing a summary of the local patch at each kernel location. They help improve the robustness of convolutional neural networks to minor variations in input features. The two most popular pooling operations are max pooling (calculates the maximum value of the local patch) and average pooling (calculates the average value of the local patch). Pooling layers result in downsampling of the input array. The output size depends on the size and stride of the pooling kernel.<a id="marker-384"/></p>
</li>
</ul>
</div></body></html>