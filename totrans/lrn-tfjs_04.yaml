- en: Chapter 3\. Introducing Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Whoa!”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Keanu Reeves *(Bill & Ted’s Excellent Adventure*)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ve mentioned the word *tensor* a few times, and it resides as the predominant
    word in TensorFlow.js, so it’s time we get to know what these structures are.
    This critical chapter will give you hands-on experience with the fundamental concept
    of managing and accelerating data, which is at the heart of teaching machines
    with data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain the concept and terminology of tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create, read, and destroy tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice concepts of structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the leap into utilizing tensors to build something useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take your time with this chapter if you’re new to tensors. Being comfortable
    with this aspect of data will help you be comfortable with machine learning altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Why Tensors?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We live in a world full of data, and deep down, we all know it ends in 1s and
    0s. To many of us, this happens quite magically. You take a photo with your phone,
    and some complex binary file gets created. Then, you swipe up and down, and our
    binary file changes from JPG to PNG within an instant. Thousands of unknown bytes
    are generated and destroyed in microseconds as files resize, reformat, and, for
    you hip kids, filter. You can’t be mollycoddled anymore. As you venture into actually
    touching, feeling, and feeding data, you have to wave goodbye to ignorant bliss.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quote the 1998 movie *Blade*:'
  prefs: []
  type: TYPE_NORMAL
- en: “You better wake up. The world you live in is nothing but a sugarcoated topping.
    There is another world beneath it.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OK, it’s like that but not as intense. To train an AI, you’ll need to make sure
    your data is uniform, and you’ll need to understand and see it. You’re not training
    your AI to do the task of decoding PNGs and JPGs uniformly; you’re training it
    on the decoded and imitated versions of what’s actually in a photo.
  prefs: []
  type: TYPE_NORMAL
- en: This means images, music, statistics, and whatever else you’re using in your
    TensorFlow.js models all need a uniform and optimized data format. Ideally, our
    data would be converted into numeric containers that quickly scale and work directly
    with calculation optimizations in the GPU or Web Assembly. You need something
    clean and straightforward for our informational data in and out. These containers
    should be unopinionated so they can hold anything. Welcome to tensors!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Understanding the use and properties of tensors is an ongoing exercise for even
    the most adept TensorFlow.js expert. While this chapter serves as an excellent
    introduction, you shouldn’t feel laggardly for having difficulty with wielding
    tensors. This chapter can serve as a reference as you progress.
  prefs: []
  type: TYPE_NORMAL
- en: Hello, Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensors are collections of data in a structured type. It’s nothing new for a
    framework to convert everything to numbers, but it might be a new concept to realize
    that it’s up to you to choose how the data is ultimately formed.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](ch01.html#the_chapter_1), all data needs to be distilled
    into numbers for the machines to understand it. Tensors are the preferred format
    of information, and they even have small abstractions for nonnumeric types. They
    are like electrical signals from the physical world to our AI’s brain. While there’s
    no specification of how your data should be structured, you do need to stay consistent
    to keep your signals organized so our brain can see the same pattern over and
    over. People generally organize their data in groups, like arrays and multidimensional
    arrays.
  prefs: []
  type: TYPE_NORMAL
- en: But what is a tensor? A *tensor*, as defined mathematically, is simply a structured
    set of values of any dimension. Ultimately, this resolves to an optimized grouping
    of data as numbers that are ready for calculation. That means, mathematically
    speaking, a traditional JavaScript array is a tensor, a 2D array is a tensor,
    and a 512D array is a tensor. TensorFlow.js tensors are the embodiment of these
    mathematical structures that hold the accelerated signals that feed data into
    and out of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re familiar with multidimensional arrays in JavaScript, you should feel
    right at home with the syntax for tensors. As you add a new dimension to each
    array, it’s often said you are increasing the *rank* of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of how you imported TensorFlow.js, the code in this book assumes
    you’ve consolidated the library to a variable named `tf`, which will be used to
    represent TensorFlow.js in all examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can read along or write the code from scratch, or even run these fundamental
    examples in the browser-based `/tfjs` solution available with the book source
    code. For simplicity, we’ll be avoiding the repetition of the `<script>` or `import`
    tags required to set up these examples and instead simply write the shared code.
  prefs: []
  type: TYPE_NORMAL
- en: To create your first tensor, we’ll keep things simple, and you’ll build it with
    a 1D JavaScript array ([Example 3-1](#create_tensor_example)). The array syntax
    and structure are carried over to tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. Creating your first tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.tensor` creates a 1D tensor if passed a 1D array. It would create a 2D
    tensor if passed a 2D array.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.tensor1d` creates a 1D tensor if passed a 1D array. It would error if passed
    a 2D array.'
  prefs: []
  type: TYPE_NORMAL
- en: This code creates a 1D tensor data structure of seven numbers in memory. Now
    those seven numbers are ready for manipulation, accelerated operations, or simply
    input. However, I’m sure you noticed we supplied two ways to perform the same
    action.
  prefs: []
  type: TYPE_NORMAL
- en: The second method provides an extra level of runtime checking since you’ve defined
    the expected dimensionality. Determining the desired dimensionality is useful
    when you’re looking to ensure the number of dimensions in the data you’re working
    with. Methods exist for verifying up to six dimensions with `tf.tensor6d`.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ll mostly be working with the generic `tf.tensor`, but if you
    find yourself deep into a complex project, don’t forget you can save yourself
    the headache of receiving unexpected dimensions by explicitly identifying your
    desired dimensionality of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: As an extra note, while the tensors in [Example 3-1](#create_tensor_example)
    were an array of natural numbers, the default data type to store numbers is Float32\.
    Floating-point numbers (that’s numbers with decimal places, e.g., 2.71828) are
    quite dynamic and impressive. They can usually handle most numbers you’ll need
    and be ready for values between. Unlike JavaScript arrays, a tensor’s data type
    must be homogeneous (all the same type). These types can be only `Float32`, `Int32`,
    bool, `complex64`, or string, with no mixing between.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to enforce that your tensor is created as a particular type, feel
    free to utilize the third parameter of the `tf.tensor` function, which explicitly
    defines the tensor’s type structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This tensor is created as a `Float32` tensor. The third parameter was redundant
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting tensor is Int32, and without the third parameter, it would have
    been a `Float32`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introducing_tensors_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting tensor is a Boolean tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introducing_tensors_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting tensor is an Int32 tensor, with the Boolean values cast to `0`
    for false, and `1` for true. So, the variable guess contains the data `[1, 0,
    0]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introducing_tensors_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: You might think this wild array will error, but each of the input values gets
    converted to its corresponding `Float32` with the resulting tensor data `[1, 3.1415927,
    0]`.
  prefs: []
  type: TYPE_NORMAL
- en: How can you identify the tensor type that was created? Just like any JavaScript
    array, tensors are equipped with methods to explain their properties. Useful properties
    include length (`size`), dimensionality (`rank`), and data type (`dtype`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply what we’ve learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This creates a successful tensor. You should know the data type, dimension,
    and size.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Since you’re using `tensor1d` to create a rank-two tensor, this will throw and
    cause the `catch` to run and log a message.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introducing_tensors_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The simple array is rank one, so it will print `1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introducing_tensors_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The size is the length of the array and will print `7`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introducing_tensors_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The tensor’s data type from an array of numbers will print `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on creating your first few tensors! It’s safe to say that being
    a master of tensors is at the core of taming data for TensorFlow.js. These structured
    buckets of values are the foundation for getting data into and back out of machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors for Data Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say you want to make an AI to play tic-tac-toe (noughts & crosses to my
    friends across the pond). As always with data, it’s time to get a coffee or tea
    and think of the right way to convert real-world data to tensor data.
  prefs: []
  type: TYPE_NORMAL
- en: You could store images of games, strings of tutorials, or simply the Xs and
    Os of the game. Images and tutorials would be pretty impressive, but for now,
    let’s just consider the idea of storing a game board’s state. There are only nine
    possible boxes to play in, so a simple array of nine values should represent any
    given state of the board.
  prefs: []
  type: TYPE_NORMAL
- en: Should the values read left to right and top to bottom? It rarely matters as
    long as you’re consistent. All encodings are made up. However, keep in mind a
    tensor resolves to numbers! This means that while you can store strings “X” and
    “O,” they would have to turn into numbers anyway. Let’s store our Xs and Os by
    mapping them to some kind of numeric value that makes sense. Does that mean you
    just assign one of them to 0 and the other to 42? I’m sure you can find a strategy
    that appropriately reflects the game state.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s evaluate the state of an active game for an exercise. Take a moment to
    review the grid of a match in progress, as shown in [Figure 3-1](#ttt_example).
    How could this be converted to tensors and numbers?
  prefs: []
  type: TYPE_NORMAL
- en: '![TicTacToe Game](assets/ltjs_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. A game with data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Perhaps the board displayed here could be read and represented as a one-dimensional
    tensor. You could read the values left to right, top to bottom. As for numbers,
    let’s choose -1, 0, and 1 to represent the three possible values for any square.
    [Table 3-1](#value_number_table) shows the lookup for each possible value.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Value-to-number table
  prefs: []
  type: TYPE_NORMAL
- en: '| Board value | Tensor value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| O | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| Empty | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'This would create a tensor like so: `[1, 0, 0, 0, -1, 0, 1, 0, 0]`. Or, it
    would create a 2D tensor, like so: `[[1, 0, 0],[0, -1, 0],[1, 0, 0]]`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a goal, let’s write some code to convert the board into a
    tensor. We’ll even explore the additional parameters of tensor creation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter of a tensor can identify the desired shape of the input
    data. Here, you convert the 1D array into a 2D tensor by specifying you would
    like the data to be rank-two structured as 3 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The third parameter of the tensor identifies the data type you would like to
    use over the inferred data type. Since you are storing round numbers, you can
    specify the type `int32`. However, the range of the default `float32` type is
    massive and can comfortably handle our numbers.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re creating tensors to represent data, it’s up to you to decide how
    you’re formatting the input data and what the resulting tensor structure should
    be. As you grasp the concepts of machine learning, you are always honing your
    intuition of what kind of data works best.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll come back to this tic-tac-toe problem later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors on Tour
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re going to get deeper into tensors as the book progresses, so it’s essential
    to take a moment and discuss why they’re so important. Without understanding the
    magnitude of the calculations we’re leveraging, it’s hard to understand the benefits
    of leaving the safety of the familiar JavaScript variables and engine for little
    old math.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors Provide Speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know you can make tensors and represent data as tensors, what’s
    the benefit of performing this conversion? We’ve mentioned that calculations with
    tensors are optimized by the TensorFlow.js framework. When you convert JavaScript
    arrays of numbers to tensors, you can perform matrix operations at breakneck speeds,
    but what does that really mean?
  prefs: []
  type: TYPE_NORMAL
- en: Computers are excellent at doing a single calculation, and there are benefits
    to performing mass groupings of calculations. Tensors are engineered for an immense
    number of side-by-side calculations. If you’ve ever performed matrix and vector
    calculations by hand, you can start to appreciate the benefits of accelerated
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors Provide Direct Access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without machine learning, you can still use tensors to make 3D graphics, content
    recommendation systems, and beautiful [iterated function systems (IFSs)](https://oreil.ly/jjnvk)
    like the Sierpiński triangle illustrated in [Figure 3-2](#sierpinski).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sierpiński triangle](assets/ltjs_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2\. IFS example: the Sierpiński triangle'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are plenty of libraries out there for images, sound, 3D models, video,
    and more. They all have one thing in common. Despite all the formats that exist,
    the libraries get you data in a universal format. Tensors are like that raw, unrolled
    data format, and with that access you can build, read, or predict anything you’d
    like.
  prefs: []
  type: TYPE_NORMAL
- en: You can even use these advanced structures to modify image data (you begin doing
    this in [Chapter 4](ch04.html#the_chapter_4)). You’ll start having more fun with
    tensor functions after you’ve graduated from the basics.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors Batch Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the data realm, you might find yourself looping through mountains of data
    and worrying about text editors crashing. Tensors are optimized for batch processing
    at high speeds. The small project at the end of this chapter has only four users
    to keep things simple, but any production environment needs to be ready to handle
    hundreds of thousands.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the benefits of tensors will be recognized when you ask trained models
    to perform the calculations to predict human-like operations in milliseconds.
    You’ll start to see examples of this as early as [Chapter 5](ch05.html#the_chapter_5).
    We’ve identified that tensors are impressive structures that bring a lot of acceleration
    and mathematical power to JavaScript, so it makes sense that you’ll commonly use
    this beneficial structure in batches.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors in Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensor speed comes with an overhead cost. Usually, when we’re done with a variable
    in JavaScript, the memory is cleanly removed when all references to that variable
    are completed. This is called *automatic garbage detection and collection* (AGDC),
    and it happens without most JavaScript developers understanding or caring how
    this works. However, your tensors don’t get that same kind of automatic care.
    They persist long after the variable that uses them has been collected.
  prefs: []
  type: TYPE_NORMAL
- en: Deallocating Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because tensors survive garbage collection, they behave differently from standard
    JavaScript and have to be accounted for and deallocated manually. Even if a variable
    is garbage-collected in JavaScript, the associated tensor is then orphaned in
    memory. You can access the current count and size using `tf.memory()`. This function
    returns an object with a report of active tensors.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Example 3-2](#source_memory_leak) illustrates noncollected tensor
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. Tensors left in memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code from [Example 3-2](#source_memory_leak) will result in printing the
    following in the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Since you already know tensors are for handling large accelerated data, the
    idea of leaving these sizable chunks in memory is a problem. With one small loop,
    you could leak an entire computer’s available RAM and GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, all tensors and models have a `.dispose()` method that purges a
    tensor from memory. When you call `.dispose()` on a tensor, the `numTensors` will
    go down by the number of tensors you just released.
  prefs: []
  type: TYPE_NORMAL
- en: This does mean you will have to think of tensors as managed in two ways, yielding
    four possible states. [Table 3-2](#tensor_var_table) shows all the combinations
    of what happens when JavaScript variables and TensorFlow.js tensors are created
    and destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. Tensor states
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Tensor live | Tensor disposed |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **JavaScript variable is live** | This variable is live; you can read the
    tensor. | An error will be raised if you attempt to use this tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| **JavaScript variable has no reference** | This is a memory leak. | This
    is a properly destroyed tensor. |'
  prefs: []
  type: TYPE_TB
- en: To put it succinctly, keep your variables and your tensors alive to access them,
    and when you’re done, dispose the tensor and do not attempt to access it.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Tensor Cleanup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, tensors do have an auto-clean option called `tidy()`. You can use
    `tidy` to create a functional encapsulation that will clean all tensors that aren’t
    returned or flagged for being kept with `keep()`. We’ll do a demo in a moment
    to help you grasp `tidy`, and we’ll be using it throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll get used to cleaning up tensors in no time. Make sure to study the following
    code, which will demonstrate `tidy()` and `keep()` in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `tidy` method takes a synchronous function and monitors the tensors created
    in this enclosure. You cannot use an async function or promise here. If you’re
    going to need anything async, you will have to call `.dispose` explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: All four tensors are effectively loaded into memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introducing_tensors_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Even though you haven’t called `dispose` explicitly, `tidy` has properly destroyed
    two of the created tensors (the two that weren’t kept or returned). If you try
    to access them now, you will get an error.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introducing_tensors_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly destroy the tensor that you saved with `tf.keep` from inside `tidy`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introducing_tensors_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly destroy the tensor that you returned from `tidy`.
  prefs: []
  type: TYPE_NORMAL
- en: If all of that makes sense, you’ve learned the practice of creating and removing
    tensors from their magical place in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors Come Home
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s worth noting that you can even mix tensors and JavaScript where applicable.
    The code in [Example 3-3](#mix_tensor_code) creates a normal JavaScript array
    of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. Mixing JS and tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The result of [Example 3-3](#mix_tensor_code) is an array of 10 tensors, with
    values `[0,0,0]` up to `[9,9,9]`. Unlike creating a 2D tensor to hold these values,
    you access a particular tensor with ease by retrieving a normal JavaScript index
    in the array. So if you want `[4,4,4]`, you can get it with `tensorArray[4]`.
    You can then destroy the whole collection from memory with a simple `tf.dispose(tensorArray)`.
  prefs: []
  type: TYPE_NORMAL
- en: After the dust settles, we’ve learned how to create and remove tensors, but
    we’re missing the critical part where tensors return their data to JavaScript.
    Tensors are great for large calculations and speed, but JavaScript has its benefits
    too. With JavaScript you can iterate, grab a specific index, or perform a world
    of NPM library calculations that are far more cumbersome in tensor form.
  prefs: []
  type: TYPE_NORMAL
- en: It’s safe to say that after you’ve reaped the benefits of calculating with a
    tensor, you’ll always need the results of that data to end up back in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving Tensor Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you try to print a tensor to the console, you can see the object, but not
    the underlying data values. To print a tensor’s data, you can call the tensor’s
    `.print()` method, but that will send the values directly to `console.log` and
    not a variable. While viewing the values of a tensor is helpful to the developer,
    we’ll need to ultimately get these values into JavaScript variables to use them.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways you retrieve tensors. Each of these methods has a synchronous
    method and an asynchronous method. First, if you’d like your data to be delivered
    in the same multidimensional array structure, you can use `.array()` for an asynchronous
    result or simply use `.arraySync()` for a sync value. Second, if you’d like to
    keep your values with extreme precision and flattened to a 1D typed array, you
    can use the synchronous `dataSync()` and an asynchronous method `data()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code explores converting, printing, and resolving tensors using
    the methods described earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This log shows the JavaScript structure that holds the tensor and its associated
    properties. You can see the shape, and `isDisposedInternal` is false because it
    hasn’t been disposed, but this serves as a pointer to the data rather than containing
    the data. This log prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling `.print` on the tensor gives us an actual printout of the internal
    value directly to the console. This prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![3](assets/3.png)](#co_introducing_tensors_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '`.arraySync` gives us the values of the 2D tensor back as a 2D JavaScript array.
    This log prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![4](assets/4.png)](#co_introducing_tensors_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: '`.dataSync` gives us the values of the 2D tensor as a 1D [Float32Array](https://oreil.ly/ozV2H)
    object, effectively flattening the data. Logging a typed array looks like an object
    with indices as properties. This log prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now you know how to manage tensors. You can take any JavaScript data and bring
    it into TensorFlow.js tensors for manipulation and then bring it back out cleanly
    when you’re done.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time to cash in on the value of moving all this data around. You now know
    how to move large amounts of data to and from tensors, but let’s get the perks
    of having done such a process. Machine learning models are driven by math. Any
    mathematical process that relies on linear algebra is going to benefit from tensors.
    You will also benefit because you don’t have to write any complex mathematical
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and Mathematics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say you had to multiply the contents of one array by another. In JavaScript,
    you’d have to write some iterative code. Additionally, if you’re familiar with
    matrix multiplication, you know that code isn’t as simple as you first thought.
    No developer at any level should resolve linear algebra for tensor manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: Remember how to multiply matrices correctly? I forgot, too.
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>91</mn></mtd>
    <mtd><mn>82</mn></mtd> <mtd><mn>13</mn></mtd></mtr> <mtr><mtd><mn>15</mn></mtd>
    <mtd><mn>23</mn></mtd> <mtd><mn>62</mn></mtd></mtr> <mtr><mtd><mn>25</mn></mtd>
    <mtd><mn>66</mn></mtd> <mtd><mn>63</mn></mtd></mtr></mtable></mfenced> <mi>X</mi>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>23</mn></mtd>
    <mtd><mn>83</mn></mtd></mtr> <mtr><mtd><mn>33</mn></mtd> <mtd><mn>12</mn></mtd>
    <mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>7</mn></mtd> <mtd><mn>23</mn></mtd>
    <mtd><mn>61</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>?</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: It’s not as simple as multiplying each number by the corresponding position;
    as some of you may recall, there’s multiplication and addition involved. Calculating
    the top-left value would be 91 x 1 + 82 x 33 + 13 x 7 = 2888\. Now do that eight
    more times for each index of the new matrix. The JavaScript to calculate that
    simple multiplication isn’t completely trivial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensors have mathematical benefits. I don’t have to write any code to perform
    the previous calculation. While writing custom code would not be complicated,
    it would be unoptimized and redundant. Useful, scalable mathematical operations
    are builtin. TensorFlow.js makes linear algebra accessible and optimized for structures
    like tensors. I can get a speedy answer for the previous matrix with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 2](ch02.html#the_chapter_2) the Toxicity detector downloaded megabytes
    and megabytes of numbers that are used in each classification calculation. The
    act of handling these large calculations in milliseconds is the power behind tensors.
    While we will continue to expand on the benefits of calculations in tensors, the
    whole reason for TensorFlow.js is that the complexity of such a large calculation
    is the domain of the framework and not the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: Recommending Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the skills you’ve learned so far, you can construct a simple example of
    how TensorFlow.js can handle calculations for a real-world scenario. The following
    example has been chosen as an illustration of the power of tensors that welcomes
    the elite as well as the mathematical avoiders.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section is probably the furthest you’ll get into mathematics. If you’d
    like to dig further into the linear algebra and calculus that fuels machine learning,
    there’s a fantastic free [online course, offered by Stanford and taught by Andrew
    Ng,](https://oreil.ly/OhvzW) that I recommend.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build something real with some tensor data. You’ll do a simple group of
    calculations to identify some user preferences. These systems are often called
    *recommendation engines*. You might be familiar with recommendation engines as
    they suggest everything from what you should buy to what movie you should watch
    next. These algorithms are at the heart of digital product giants like YouTube,
    Amazon, and Netflix. Recommendation engines are quite popular with any business
    that sells anything and could probably fill a book by themselves. We’ll be implementing
    a simple “content-based” recommendation system. Use your imagination because in
    a production system these tensors are significantly larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what you’ll do, at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: Ask users to rank bands from `1` to `10`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any unknown bands get a `0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bands and music styles will be our “features.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the matrix dot product to identify what styles each user likes!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s get started creating a recommender! This small dataset will serve as an
    example of what you need. As you’ll notice, you mix JavaScript arrays with tensors
    in the code. It’s quite common for labels to remain in JavaScript and calculations
    to be pushed into tensors. This not only keeps tensors focused on numbers; it
    also has the benefit of internationalizing the tensor results. The labels are
    the only language-dependent part of this operation. You’ll see this theme continue
    in several examples throughout the book and in the real world of practical machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The four name labels are simply stored in a normal JavaScript array.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: You’ve asked our users to rate seven bands.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introducing_tensors_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Some simple music genres can be used to describe our seven bands, again in a
    JavaScript array.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introducing_tensors_CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This is our first tensor, a rank-two description of each user’s vote from `1`
    to `10`, with “I don’t know this band” as `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introducing_tensors_CO8-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This tensor is also a 2D tensor that identifies the genres that match each given
    band. Each line index represents an encoding of true/false for the genres it can
    be classified as.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have all the data you need in tensors. As a quick review, you can see
    the way the information is organized. By reading the `user_votes` variable, you
    can see each user’s votes. For example, you can see user `0`, which maps to Gant,
    has rated Nirvana a `10` and Apashe `7`, while Jed has given Backstreet Boys a
    `10`.
  prefs: []
  type: TYPE_NORMAL
- en: The `band_feats` variable maps each band to the genres they fulfill. For example,
    the second band at index `1` is Nine Inch Nails and has a positive scoring for
    Grunge and Industrial styles of music. To keep this example simple, you’re using
    a binary `1` and `0` per genre, but a normalized scale of numbers would work here,
    too. In other words, `[1, 1, 0, 0, 0, 0]` would represent Grunge and Rock for
    the 0th band, which is Nirvana.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you’ll calculate each user’s favorite genres based on their votes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now `user_feats` contains a dot product of the user’s votes across the features
    of each band. The result from our print will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This tensor shows the value of the features (in this case, genres) of each user.
    User `0`, which aligns with Gant, has their highest value as `27` at index `0`,
    which means their top preferred genre from the surveyed data is Grunge. This data
    looks pretty good. Using this tensor, you can identify each user’s preferred tastes.
  prefs: []
  type: TYPE_NORMAL
- en: While the data is in tensor form, you can use a method called `topk` to help
    us identify the top values for each user with size *k*. To get the top *k* tensors
    or simply identify where the top values are via identifying their indices, you
    can call the function `topk` with the desired tensor and size. For this exercise,
    you’ll set *k* to be the full feature set size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s take this data home to JavaScript. The code to do this can be
    written like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introducing_tensors_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: You are returning the index tensor to a rank-two JavaScript array for the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introducing_tensors_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: You are mapping the indices back to musical genres.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting log looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the results, you can see Todd should check out more Industrial music, and
    Jed should brush up on his Boy Bands. Both will be happy with their recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: What did you just do?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You successfully loaded data into tensors in a way that makes sense, and then
    you applied a mathematical calculation to the entire set, rather than an iterative
    approach across each person. Once you got your answers, you sorted the entire
    set and brought the data back to JavaScript for recommendations!
  prefs: []
  type: TYPE_NORMAL
- en: Can you do more?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can do plenty more. From here, you can even use the 0s from each user’s
    votes to identify what bands the user has never listened to and recommend them
    in order of most-liked genre! There’s a really cool mathematical way to do this,
    but that’s a bit outside the scope of our first tensor exercise. Regardless, congratulations
    on implementing one of the most demanded and trending features of online sales!
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve done more than scratch the surface of tensors. You’ve
    dug your hands deep into the fundamental structure of TensorFlow.js and grasped
    the roots. You’re on your way to wielding machine learning in JavaScript. Tensors
    are a concept that permeates all machine learning frameworks and fundamentals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: What Makes You So Special?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you’re no longer a tensor-newb and you can manage tensors like a pro,
    let’s attempt a small exercise to solidify your skills. As of the time of this
    writing, JavaScript has no built-in method for clearing duplicates in an array.
    While other languages like Ruby have had the `uniq` method for more than a decade,
    JavaScript developers have either hand-rolled their solutions or imported popular
    libraries like Lodash. For fun, let’s use TensorFlow.js to solve the problem of
    unique values. As an exercise on lessons learned, muse over this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Given this array of US phone numbers, remove the duplicates.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Make sure your answer is a JavaScript array. If you get stuck with this exercise,
    you can review the [TensorFlow.js online documentation](https://oreil.ly/9thOd).
    Searching the documentation for key terms will point you in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  prefs: []
  type: TYPE_NORMAL
- en: Review Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we even use tensors?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which one of these is not a tensor data type?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Int32`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Float32`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Object
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Boolean
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the rank of a six-dimensional tensor?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the dimensionality of the return array for the method `dataSync`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens when you pass a 3D tensor to `tf.tensor1d`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `rank` and `size` in relation to a tensor’s shape?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the data type of the tensor `tf.tensor([1])`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the input array dimension for a tensor always the resulting tensor dimension?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you identify the number of tensors in memory?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can `tf.tidy` handle an async function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I keep a tensor created inside of `tf.tidy`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can I see the values of a tensor with `console.log`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the `tf.topk` method do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do tensors optimize for batch or iterative calculation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a recommendation engine?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  prefs: []
  type: TYPE_NORMAL
