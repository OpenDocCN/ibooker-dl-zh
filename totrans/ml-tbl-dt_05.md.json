["```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\n\nX, y = make_classification(n_features=32,\n                           n_redundant=0,\n                           n_informative=24,\n                           random_state=1,\n                           n_clusters_per_class=1\n                           )                            ①\n\nmodel = LogisticRegression()                            ②\n\npipeline = Pipeline(\n    [('processing', StandardScaler()),\n     ('modeling', model)])                              ③\n\ncv_scores = cross_validate(estimator=pipeline, \n                           X=X, \n                           y=y,\n                           scoring=\"accuracy\",\n                           cv=5)                        ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nprint(f\"accuracy: {mean_cv:0.3f} ({std_cv:0.3f})\")      ⑤\n\nmodel.fit(X, y) \n```", "```py\naccuracy 0.900 (0.032)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(C=1.0)\n```", "```py\ntransformed_data = transformer.transform(data)\n```", "```py\nprediction = predictor.predict(data) \nprobability = predictor.predict_proba(data)\n```", "```py\nscore = model.score(data)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(C=1.0)\n```", "```py\nX = [[-1, -1], [-2, -1], [1, 1], [2, 1]]\ny = [1, 1, 0, 0]\nmodel.fit(X, y)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nprocessing = StandardScaler().fit(X)\n```", "```py\nX_test = [[-1, 1], [2, -1]]\nmodel.predict(X_test)\n```", "```py\nmodel.predict_proba(X_test)\n```", "```py\nprocessing.transform(X)\n```", "```py\nprocessing.fit_transform(X)\n```", "```py\nimport numpy as np\nimport pandas as pd\nexcluding_list = ['price', 'id', 'latitude', \n                  'longitude', 'host_id', \n                  'last_review', 'name', \n                  'host_name']                       ①\ncategorical = ['neighbourhood_group',\n               'neighbourhood',\n               'room_type']                          ②\ncontinuous = ['minimum_nights',\n              'number_of_reviews',\n              'reviews_per_month',\n              'Calculated_host_listings_count']      ③\ndata = pd.read_csv(\"./AB_NYC_2019.csv\")\n```", "```py\ndata.shape\n```", "```py\ndata[categorical].nunique()\n```", "```py\nneighbourhood_group      5\nneighbourhood          221\nroom_type                3\n```", "```py\nlow_card_categorical = ['neighbourhood_group', 'room_type']\nhigh_card_categorical = ['neighbourhood']\n```", "```py\ndata[low_card_categorical + continuous].isna().sum()\n```", "```py\nneighbourhood_group                   0\nroom_type                             0\nminimum_nights                        0\nnumber_of_reviews                     0\nreviews_per_month                 10052\ncalculated_host_listings_count        0\navailability_365                      0\n```", "```py\ndata.reviews_per_month.min()\n```", "```py\ndata[[\"price\"]].hist(bins=10)\n```", "```py\ndata[[\"price\"]].boxplot()\n```", "```py\nnp.log1p(data[\"price\"]).hist(bins=20)\ndata[[\"price\"]].apply(lambda x: np.log1p(x)).boxplot()\n```", "```py\ndata[[\"price\"]][data.price <= 1000].hist(bins=20)\n```", "```py\ndata[[\"price\"]][(data.price >= 50) & (data.price <= 200)].hist(bins=20)\n```", "```py\nprice_capped = data.price <= 1000\nprice_window = (data.price >= 50) & (data.price <= 200)\n```", "```py\ndata[[\"price\"]][price_window].boxplot()\n```", "```py\ntarget_mean = (data[\"price\"] > data[\"price\"].mean()).astype(int)\ntarget_median = (data[\"price\"] > data[\"price\"].median()).astype(int)\ntarget_multiclass = pd.qcut(data[\"price\"], q=5, labels=False)\ntarget_regression = data[\"price\"]\n```", "```py\ntarget_median.value_counts()\n```", "```py\n0    24472\n1    24423\n```", "```py\ntarget_mean.value_counts()\n```", "```py\n0    34016\n1    14879\n```", "```py\ntarget_multiclass.value_counts()\n```", "```py\n0    10063\n1     9835\n2     9804\n3    10809\n4     8384\n```", "```py\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\ncategorical_onehot_encoding = OneHotEncoder(\n       handle_unknown='ignore')                       ①\nnumeric_passthrough = SimpleImputer(\n       strategy=\"constant\", fill_value=0)             ②\nnumeric_standardization = Pipeline([\n       (\"imputation\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n       (\"standardizing\", StandardScaler())\n       ])                                             ③\n```", "```py\ncolumn_transform = ColumnTransformer(\n                [('categories', \n                  categorical_onehot_encoding, \n                  low_card_categorical),              ①\n                 ('numeric', \n                  numeric_passthrough, \n                  continuous),                        ②\n                ],\n    remainder='drop',                                 ③\n    verbose_feature_names_out=False,                  ④\n    sparse_threshold=0.0                              ⑤\n)\n```", "```py\nX = column_transform.fit_transform(data)\nprint(type(X), X.dtype, X.shape)\n```", "```py\n<class 'numpy.ndarray'> float64 (48895, 13)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_passthrough, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                   ①\n\nmodel = LinearRegression()                                  ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)]                                   ③\n)\n\ncv = KFold(5, shuffle=True, random_state=0)                 ④\nrmse =  make_scorer(mean_squared_error, \n                    squared=False)                          ⑤\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data[price_window], \n                           y=target_regression[price_window],\n                           scoring=rmse,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)           ⑥\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\", \n      f\"secs pred: {score_time:0.2f} secs\")                 ⑦\n```", "```py\n33.949 (0.274) fit: 0.06 secs pred: 0.01 secs\n```", "```py\ndata_2 = data[[]].copy()                                            ①\ndata_2['neighbourhood_group_Manhattan'] = ( \n   (data['neighbourhood_group']=='Manhattan')\n   .astype(int))                                                    ②\ndata_2['neighbourhood_group_Queens'] = (\n\n(data['neighbourhood_group']=='Queens').astype(int))                ③\ndata_2['room_type_Entire home/apt'] = (\n                           (data['room_type']=='Entire \nhome/apt').astype(int))                                             ④\ndata_2['minimum_nights_log'] = np.log1p(\n                        data[\"minimum_nights\"])                     ⑤\ndata_2['number_of_reviews_log'] = np.log1p(\n                        data[\"number_of_reviews\"])                  ⑥\nlabel1 = 'neighbourhood_group_Manhattan*room_type_Entire home/apt'\ndata_2[label1] = (\n   data_2['neighbourhood_group_Manhattan'] *\n   data_2['room_type_Entire home/apt'])                             ⑦\nlabel2 = 'availability_365*neighbourhood_group_Manhattan'\ndata_2[label2] = (data['availability_365'] *\n   data_2['neighbourhood_group_Manhattan'])                         ⑧\nlabel3 = 'availability_365*room_type_Entire home/apt'\ndata_2[label3] = (data['availability_365'] *\n   data_2['room_type_Entire home/apt'])                             ⑨\n\nrmse = make_scorer(mean_squared_error, squared=False)\ncv = KFold(5, shuffle=True, random_state=0)\n\ncv_scores = cross_validate(estimator=LinearRegression(), \n                           X=data_2[price_window], \n                           y=target_regression[price_window],\n                           scoring=rmse,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nprint(f\"{mean_cv:0.5f}, {std_cv:0.5f}\")\n```", "```py\n33.937 (0.240)\n```", "```py\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge, Lasso\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_passthrough, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)\n\npolynomial_expansion = PolynomialFeatures(degree=2)              ①\n\nmodel = Ridge(alpha=2500.0)                                      ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('polynomial_expansion', polynomial_expansion),\n     ('standardizing', numeric_standardization),\n     ('modeling', model)]\n)                                                                ③\n\ncv = KFold(5, shuffle=True, random_state=0)\nrmse =  make_scorer(mean_squared_error, squared=False)\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data[price_window], \n                           y=target_regression[price_window],\n                           scoring=rmse,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)                ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs\") ⑤\n```", "```py\n33.738 (0.275) fit: 0.13 secs pred: 0.03 secs\n```", "```py\n(cv_scores['estimator'][0]['modeling'].coef_.round(5)!=0).sum()\n```", "```py\nmodel = Lasso(alpha=0.1)                                         ①\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('polynomial_expansion', polynomial_expansion),\n     ('standardizing', numeric_standardization),\n     ('modeling', model)]\n)                                                                ②\n\ncv = KFold(5, shuffle=True, random_state=0)\nrmse =  make_scorer(mean_squared_error, squared=False)\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data[price_window], \n                           y=target_regression[price_window],\n                           scoring=rmse,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)                ③\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs\") ④\n```", "```py\n33.718 (0.269) fit: 0.64 secs pred: 0.03 secs\n```", "```py\n(cv_scores['estimator'][0]['modeling'].coef_.round(5) !=0).sum()\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer, accuracy_scorehttps://mng.bz/JY20\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = LogisticRegression(solver=\"saga\",\n                           penalty=None,\n                           max_iter=1_000)                       ①\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_standardization, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                        ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                       ③\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_median,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)                ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f} secs pred: {score_time:0.2f} secs\") ⑤\n```", "```py\n0.821 (0.004) fit: 3.00 secs pred: 0.02 secs\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer, accuracy_score\n\naccuracy = make_scorer(accuracy_score)\ncv = KFold(5, shuffle=True, random_state=0)\nmodel = LogisticRegression(penalty=\"l2\", C=0.1, solver=\"sag\", \nmulti_class=\"ovr\", max_iter=1_000)                               ①\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_standardization, continuous)],\n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)                                        ②\n\nmodel_pipeline = Pipeline(\n    [('processing', column_transform),\n     ('modeling', model)])                                       ③\n\ncv_scores = cross_validate(estimator=model_pipeline, \n                           X=data, \n                           y=target_multiclass,\n                           scoring=accuracy,\n                           cv=cv, \n                           return_train_score=True,\n                           return_estimator=True)                ④\n\nmean_cv = np.mean(cv_scores['test_score'])\nstd_cv = np.std(cv_scores['test_score'])\nfit_time = np.mean(cv_scores['fit_time'])\nscore_time = np.mean(cv_scores['score_time'])\nprint(f\"{mean_cv:0.3f} ({std_cv:0.3f})\", \n      f\"fit: {fit_time:0.2f}\",\n      f\"secs pred: {score_time:0.2f} secs\")                      ⑤\n```", "```py\n0.435 (0.002) fit: 31.08 secs pred: 0.02 secs\n```", "```py\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\n\nexperiments = [\n     ['normal', 0, float('-inf')],\n     ['poisson', 1, 0.0],\n     ['tweedie', 1.5, 0.1],\n     ['gamma', 2, 0.1],\n     ['inverse gaussian', 3, 0.1]]                           ①\n\nfor experiment, power, min_val in experiments:               ②\n\n    column_transform = ColumnTransformer(\n        [('categories', categorical_onehot_encoding, low_card_categorical),\n         ('numeric', numeric_standardization, continuous)],\n        remainder='drop',\n        verbose_feature_names_out=False,\n        sparse_threshold=0.0)\n\n    model = TweedieRegressor(power=power, \n                             max_iter=1_000)                 ③\n\n    model_pipeline = Pipeline(\n        [('processing', column_transform),\n         ('modeling', model)])\n\n    cv = KFold(5, shuffle=True, random_state=0)\n    rmse =  make_scorer(mean_squared_error, squared=False)\n\n    cv_scores = cross_validate(estimator=model_pipeline, \n                   X=data, \n                            y=target_regression.clip(\n                                   lower=min_val),           ④\n                               scoring=rmse,\n                               cv=cv, \n                               return_train_score=True,\n                               return_estimator=True)\n\n    mean_cv = np.mean(cv_scores['test_score'])\n    std_cv = np.std(cv_scores['test_score'])\n    fit_time = np.mean(cv_scores['fit_time'])\n    score_time = np.mean(cv_scores['score_time'])\n    print(f\"{experiment:18}: {mean_cv:0.3f} ({std_cv:0.3f})\", \n          f\"fit: {fit_time:0.2f}\",\n          f\"secs pred: {score_time:0.2f} secs\")              ⑤\n```", "```py\nnormal            : 233.858 (15.826) fit: 0.13 secs pred: 0.03 secs\npoisson           : 229.189 (16.075) fit: 0.66 secs pred: 0.03 secs\ntweedie           : 229.607 (16.047) fit: 0.46 secs pred: 0.03 secs\ngamma             : 233.991 (15.828) fit: 0.22 secs pred: 0.03 secs\ninverse gaussian  : 239.577 (15.453) fit: 0.18 secs pred: 0.03 secs\n```", "```py\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import gen_batches\nfrom sklearn.metrics import accuracy_score\n\ndef generate_batches(X, \n                     y, \n                     batch_size, \n                     random_state):                                 ①\n    \"\"\"split dataset into batches \"\"\"\n    examples = len(X)\n    batches = gen_batches(n=examples, \n                          batch_size=batch_size)                    ②\n    sequence = np.arange(examples)\n    if random_state:\n        np.random.seed(random_state)                                ③\n        np.random.shuffle(sequence)\n\n    for batch in batches:\n        items = sequence[batch]\n        yield(X.iloc[items], y.iloc[items])                         ④\n\nmodel = SGDClassifier(loss=\"log_loss\", \n                      average=True,\n                      penalty='l2', \n                      alpha=0.001)                                  ⑤\n\ncolumn_transform = ColumnTransformer(\n    [('categories', categorical_onehot_encoding, low_card_categorical),\n     ('numeric', numeric_standardization, continuous)], \n    remainder='drop',\n    verbose_feature_names_out=False,\n    sparse_threshold=0.0)\n\nX_train, X_test, y_train, y_test = train_test_split(data, target_median, \n                                                    test_size=0.20, \n                                                    random_state=0) ⑥\niterations = 10\nfor j in range(iterations):                                         ⑦\n    generator = generate_batches(X_train, y_train, batch_size=256, \nrandom_state=j)\n    for k, (Xt, yt) in enumerate(generator):\n        if k == 0:\n            column_transform.fit(Xt)\n                    Xt = column_transform.transform(Xt)\n        if k == 0:\n            model.partial_fit(Xt, yt, classes=(0, 1))               ⑧\n        else:\n            model.partial_fit(Xt, yt)                               ⑨\n\npredictions = model.predict(column_transform.transform(X_test)) \nscore = accuracy_score(y_true=y_test, y_pred=predictions)\nprint(f\"Accuracy on test set: {score:0.3f}\")                        ⑩\n```", "```py\nAccuracy on test set: 0.818\n```"]