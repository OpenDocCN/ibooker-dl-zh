- en: Chapter 17\. Serving LLMs with Ollama
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve explored how to use transformers to download a model and put together
    an easy pipeline that lets you use it for inference or fine-tuning. However, I’d
    be remiss if I didn’t show you the open source Ollama project, which ties it all
    together by giving you an environment that gives you a full wrapper around an
    LLM that you can either chat with in your terminal or use as a server that you
    can HTTP POST to and read the output from.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Technologies like Ollama will be the vanguard of the next generation of LLMs,
    which will let you have dedicated servers inside your data center or dedicated
    processes on your computer. That will make them completely private to you.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: At its core, Ollama is an open source project that simplifies the process of
    downloading, running, and managing LLMs on your computer. It also handles nonfunctional
    difficult requirements, such as memory management and model optimization, and
    it provides standardized interfaces for interaction, such as the ability to HTTP
    POST to your models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Ollama is also a key strategic tool you should consider because it bridges the
    gap between cloud-based third-party services like GPT, Claude, and Gemini and
    locally deployed services. It goes beyond giving you a local development environment
    to giving you one that you could, for example, use within your own data center
    to serve multiple internal users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: By running models locally, you can ensure the complete privacy of your data,
    eliminate network latency, and work offline. This is especially crucial in scenarios
    involving sensitive data or applications that require consistent, low-latency
    responses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Ollama also supports a growing library of popular open source models, including
    Llama, Mistral, and Gemma, and it also supports various specialized models that
    are optimized for specific tasks. Each model can be pulled and run with simple
    commands, in a way that’s similar to how Docker containers work. The platform
    handles model quantization automatically, optimizing models to run efficiently
    on consumer hardware while maintaining good performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll explore Ollama in three ways: installing it and getting
    started, looking at how you can instantiate specific models and use them, and
    exploring the RESTful APIs that let you build LLM applications that preserve privacy.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Ollama
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Ollama project is hosted at [ollama.com](http://ollama.com). It’s pretty
    straightforward to get up and running, and the home screen gives download options
    for macOS, Linux, and Windows. Note also that the Windows version needs Windows
    Subsystem for Linux (WSL). For this chapter, I’m using the macOS version.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: When you navigate to the website, you’ll see a friendly welcome to download
    (see [Figure 17-1](#ch17_figure_1_1748550058907779)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1701.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. Getting started with Ollama
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve downloaded and installed Ollama, you can launch it, and you’ll see
    it in the system bar at the top of the screen. Your main interface with Ollama
    will be the command line.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你下载并安装了 Ollama，你就可以启动它，你会在屏幕顶部的系统栏中看到它。你与 Ollama 的主要界面将是命令行。
- en: 'Then, with the `ollama run` command, you can download and use models. So, for
    example, if you want to use Gemma, then from Google, you can do the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 `ollama run` 命令，你可以下载并使用模型。例如，如果你想使用 Gemma，那么从 Google，你可以执行以下操作：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll want to be sure to note the parameters used, which you can find in the
    [model’s documentation page on Ollama](https://oreil.ly/VMLKO). While Ollama can
    and will quantize models that are optimized to run locally, it can’t perform miracles,
    and only models that will fit in your system resources—most importantly, memory—will
    work. In this case, I ran the `gemma2:2b` (2-billion parameter) version, which
    requires about 8 GB of GPU RAM. On macOS, the shared RAM with the M-Series chips
    works well, while running on an M1 Mac with 16 Gb, the Gemma 2B is fast and smooth
    with Ollama.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要确保注意使用的参数，这些参数可以在 Ollama 的[模型文档页面](https://oreil.ly/VMLKO)上找到。虽然 Ollama 可以并且会量化那些优化以本地运行的模型，但它不能创造奇迹，只有适合你系统资源——最重要的是内存——的模型才能工作。在这种情况下，我运行了
    `gemma2:2b`（20 亿参数）版本，这需要大约 8 GB 的 GPU RAM。在 macOS 上，与 M-Series 芯片的共享 RAM 工作良好，而在
    16 Gb 的 M1 Mac 上运行时，Gemma 2B 与 Ollama 一起运行得既快又流畅。
- en: You can see me chatting with Gemma in [Figure 17-2](#ch17_figure_2_1748550058907817).
    These responses took less than one second to receive on my two-year-old laptop!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 17-2](#ch17_figure_2_1748550058907817)中看到我与 Gemma 的聊天。这些响应在我的两岁笔记本电脑上接收不到一秒！
- en: '![](assets/aiml_1702.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1702.png)'
- en: Figure 17-2\. Using Ollama in a terminal
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-2\. 在终端中使用 Ollama
- en: It’s great to have a localized chat like this, and you can experiment with different
    models, including multimodal ones like Llama 3.2\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有这样一个本地化的聊天真是太好了，你可以尝试不同的模型，包括像 Llama 3.2\ 这样的多模态模型。
- en: 'So, for example, you could issue the following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以执行以下命令：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, within the terminal, you could do multimodal processing. For example,
    if your terminal supported it, and you dragged and dropped an image into the terminal,
    you could ask the model what it could see in the image. The multimodal power of
    Llama would parse the image for you, and Ollama would handle all the technical
    difficulties.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在终端内，你可以进行多模态处理。例如，如果你的终端支持它，并且你将一个图像拖放到终端中，你可以询问模型它在图像中看到了什么。Llama 的多模态能力会为你解析图像，而
    Ollama 会处理所有的技术难题。
- en: 'In my case, all I had to do was give a prompt and then drag and drop the image
    onto it. So, I opened an Ollama chat window with the preceding command and then
    entered this prompt:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我只需要给出一个提示，然后将图像拖放到上面。因此，我使用前面的命令打开了一个 Ollama 聊天窗口，并输入了这个提示：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`And then I just dragged and dropped the image into the chat window, and Ollama
    did the rest.    You can see in [Figure 17-3](#ch17_figure_4_1748550058907880)
    how detailed the results were. This is a photo I took of Osaka castle one morning
    while on a run in 2018\. While Llama couldn’t guess the date, it was able to predict
    the season based on the foliage in the image. It got everything else correct and
    gave very detailed output!  ![](assets/aiml_1703.png)  ###### Figure 17-3\. Using
    Ollama for a multimodal model    While it’s really cool to have a local LLM that
    you can chat with in a privacy-preserving way, I think the real power in Ollama
    is in using it as a server that can then be the foundation of an application.
    We’ll explore that next.`  [PRE3][PRE4]py ollama serve [PRE5]py curl http://localhost:11434/api/generate
    -d ''{ `"model"``:` `"gemma2:2b"``,`   `"prompt"``:` `"Why is the sky blue?"``,`   `"stream"``:`
    `false` `}``''` [PRE6]py[PRE7] [PRE8] {     "model":"llama3.2-vision",     "created_at":"2024-12-10T17:15:06.264497Z",     "response":"The
    image depicts Osaka Castle...",     "done":true,     "done_reason":     "stop",     "context":[128006,882,128007,271,58,...],     "total_duration":88817301209,     "load_duration":21197292,     "prompt_eval_count":19,     "prompt_eval_duration":84560000000,     "eval_count":56,     "eval_duration":4050000000
    }% [PRE9]` [PRE10][PRE11][PRE12][PRE13][PRE14][PRE15][PRE16] You are an expert
    storyteller who understands story structure, nuance, and content. Attached is
    a novel, so please evaluate this novel for storylines and suggest improvements
    that could  be made in character development, plot, and emotional  content. Be
    as verbose as needed to provide an in-depth  analysis that would help the author
    understand how their work  would be accepted. [PRE17] # Read the file content
    with open(filepath, ''r'', encoding=''utf-8'') as file:     file_content = file.read()
    [PRE18] # Prepare the request url = "http://localhost:11434/api/generate" headers
    = {"Content-Type": "application/json"} [PRE19] payload = {     "model": model,     "prompt":
    f"You are an expert storyteller who understands                  `story` `structure``,`
    `nuance``,` `and` `content``.` `Attached` `is` `a` `novel``,`                  `please`
    `evaluate` `this` `novel` `for` `storylines``,` `and` `suggest`                  `improvements`
    `that` `could` `be` `made` `in` `character` `development``,`                  `plot``,`
    `and` `emotional` `content``.` `Be` `as` `verbose` `as` `needed`                  `to`
    `provide` `an` `in``-``depth` `analysis` `that` `would` `help` `the` `author`                  `understand`
    `how` `their` `work` `would` `be` `accepted``:` `{``file_content``}``",` [PRE20]
    [PRE21]`` [PRE22] try:     # Send the request     response = requests.post(url,
    headers=headers, json=payload)     response.raise_for_status()  # Raise an exception
    for bad status        # Parse the response     result = response.json()[''response'']       #
    Return the response text from Ollama     return result   except requests.exceptions.RequestException
    as e:     raise Exception(f"Error making request to Ollama: {str(e)}") except
    json.JSONDecodeError as e:     raise Exception(f"Error parsing Ollama response:
    {str(e)}") [PRE23] def analyze_file(filepath: str, model: str = "gemma2:2b") ->
    dict: [PRE24] result = analyze_file(str(input_path)) [PRE25] **Strengths:**   *
    **Intriguing Premise:** The concept of a deadly plague originating  from space
    with potentially disastrous consequences is both  suspenseful and timely, appealing
    to a wider audience. * **Realistic Characters:**  The characters feel grounded
    despite being  involved in extraordinary events, and their personalities shine  through
    (Aisha''s determination, Soo-Kyung''s wisdom). Their connection  adds emotional
    weight. * **Suspenseful Tone:** The story builds suspense gradually. You expertly  use
    cliffhangers like the three-year deadline for the plague  to leave readers wanting
    more. * **Worldbuilding Potential:**  The mention of the moon base and potential  interstellar
    jumps introduces a rich world with possibilities  for further exploration.   [PRE26]`
    [PRE27]`` [PRE28] const PORT = process.env.PORT || 3000; app.listen(PORT, () =>
    {   console.log(`Server running on port ${PORT}`); }); [PRE29] app.post(''/analyze'',
    upload.single(''novel''), async (req, res) => {   try {     if (!req.file) {       return
    res.status(400).send(''No file uploaded'');     } [PRE30] // Generate a unique
    job ID const jobId = Date.now().toString();   // Store job status analysisJobs.set(jobId,
    { status: ''processing'' });   // Start analysis in background const fileContent
    = await fs.readFile(req.file.path, ''utf8'');   // Clean up uploaded file await
    fs.unlink(req.file.path); [PRE31] // Process in background analyzeNovel(fileContent)   .then(result
    => {     analysisJobs.set(jobId, {       status: ''completed'',       result:
    result     });   })   .catch(error => {     analysisJobs.set(jobId, {       status:
    ''error'',       error: error.message     });   }); [PRE32] async function analyzeNovel(text)
    {   try {     console.log(''Sending request to Ollama...'');     const requestBody
    = {       model: ''gemma2:2b'',       prompt: `You are an expert storyteller who
    understands        story structure, nuance, and content. Attached is a novel.        Please
    evaluate this novel for storylines and suggest improvements        that could
    be made in character development, plot, and emotional content.        Be as verbose
    as needed to provide an in-depth analysis that would help        the author understand
    how their work would be accepted:\n\n${text}`,       stream: false     }; [PRE33]
    const response = await fetch(OLLAMA_URL, {   method: ''POST'',   headers: {     ''Content-Type'':
    ''application/json'',   },   body: JSON.stringify(requestBody) }); [PRE34] if
    (!response.ok) {   throw new Error(`HTTP error! status: ${response.status}`);
    }   const data = await response.json(); return data.response; [PRE35] <h1>Novel
    Analysis Tool</h1> <div class="upload-form">   <h2>Upload your novel</h2>   <form
    id="uploadForm">     <input type="file" name="novel" accept=".txt" required>     <br>     <button
    type="submit" class="submit-button">Analyze Novel</button>   </form> </div> [PRE36]
    document.getElementById(''uploadForm'')         .addEventListener(''submit'',
    async (e) => {e.preventDefault();   [PRE37] // Upload file const formData = new
    FormData(form); const response = await fetch(''/analyze'', {   method: ''POST'',   body:
    formData }); [PRE38] if (!response.ok) throw new Error(''Upload failed'');   const
    { jobId } = await response.json();   // Poll for results while (true) {   const
    statusResponse = await fetch(`/status/${jobId}`);   if (!statusResponse.ok) throw
    new Error(''Status check failed'');     const status = await statusResponse.json();     if
    (status.status === ''completed'') {     result.textContent = status.result;     result.style.display
    = ''block'';     loading.style.display = ''none'';     break;   } else if (status.status
    === ''error'') {     throw new Error(status.error);   }     // Wait before polling
    again   await new Promise(resolve => setTimeout(resolve, 1000)); } [PRE39] node
    app.js [PRE40]` [PRE41] ``# Summary    In this chapter, we looked at how the open
    source Ollama tool gives you the ability to wrap an LLM with an easy-to-use API
    that lets you build applications with it. You saw how to install Ollama and then
    explored some scenarios with it. You also downloaded and used models like the
    simple, lightweight Gemma from Google, as well as the powerful, multimodal Llama3.2-vision
    from Meta. You explored not just chatting with them but also attaching files to
    upload. Ollama gives you an HTTP endpoint that you saw how to experiment with
    by using a `curl` command to simulate HTTP traffic. Finally, you got into writing
    a prototype of a real-world LLM-based application that analyzed contents of books,
    first as a simple Python script that proved the concept and then as a more sophisticated
    web-based application in `node.js` that used an Ollama backend and a Gemma LLM
    to do the heavy lifting!    In the next chapter, we’ll build on this and explore
    the concepts of RAG, and you’ll build apps that use local vector databases to
    enhance the knowledge of LLMs.`` [PRE42][PRE43][PRE44][PRE45]``````'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
