- en: Chapter 17\. Serving LLMs with Ollama
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章\. 使用Ollama提供LLM服务
- en: We’ve explored how to use transformers to download a model and put together
    an easy pipeline that lets you use it for inference or fine-tuning. However, I’d
    be remiss if I didn’t show you the open source Ollama project, which ties it all
    together by giving you an environment that gives you a full wrapper around an
    LLM that you can either chat with in your terminal or use as a server that you
    can HTTP POST to and read the output from.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何使用transformers下载模型并构建一个简单的管道，让你可以使用它进行推理或微调。然而，如果我不向你展示开源的Ollama项目，那就太遗憾了，该项目通过为你提供一个环境，让你可以围绕LLM提供完整的包装，你可以在终端中与之聊天，或者将其用作服务器，你可以通过HTTP
    POST发送请求并读取输出。
- en: Technologies like Ollama will be the vanguard of the next generation of LLMs,
    which will let you have dedicated servers inside your data center or dedicated
    processes on your computer. That will make them completely private to you.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 像Ollama这样的技术将成为下一代LLM的先锋，这将让你在数据中心内部拥有专用服务器或在计算机上拥有专用进程。这将使它们对你来说完全私密。
- en: At its core, Ollama is an open source project that simplifies the process of
    downloading, running, and managing LLMs on your computer. It also handles nonfunctional
    difficult requirements, such as memory management and model optimization, and
    it provides standardized interfaces for interaction, such as the ability to HTTP
    POST to your models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Ollama是一个开源项目，简化了在计算机上下载、运行和管理LLM的过程。它还处理非功能性困难要求，例如内存管理和模型优化，并提供标准化的交互接口，例如能够向你的模型发送HTTP
    POST请求。
- en: Ollama is also a key strategic tool you should consider because it bridges the
    gap between cloud-based third-party services like GPT, Claude, and Gemini and
    locally deployed services. It goes beyond giving you a local development environment
    to giving you one that you could, for example, use within your own data center
    to serve multiple internal users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama也是你应该考虑的关键战略工具，因为它弥合了基于云的第三方服务（如GPT、Claude和Gemini）与本地部署服务之间的差距。它不仅提供了一个本地开发环境，还提供了一个你可以在自己的数据中心使用，例如，为多个内部用户提供服务的环境。
- en: By running models locally, you can ensure the complete privacy of your data,
    eliminate network latency, and work offline. This is especially crucial in scenarios
    involving sensitive data or applications that require consistent, low-latency
    responses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本地运行模型，你可以确保数据的完全隐私，消除网络延迟，并离线工作。这在涉及敏感数据或需要一致、低延迟响应的应用程序中尤为重要。
- en: Ollama also supports a growing library of popular open source models, including
    Llama, Mistral, and Gemma, and it also supports various specialized models that
    are optimized for specific tasks. Each model can be pulled and run with simple
    commands, in a way that’s similar to how Docker containers work. The platform
    handles model quantization automatically, optimizing models to run efficiently
    on consumer hardware while maintaining good performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama还支持越来越多的流行开源模型库，包括Llama、Mistral和Gemma，并且它还支持各种针对特定任务优化的专用模型。每个模型都可以通过简单的命令进行拉取和运行，其方式类似于Docker容器的工作方式。该平台自动处理模型量化，优化模型以便在消费级硬件上高效运行，同时保持良好的性能。
- en: 'In this chapter, we’ll explore Ollama in three ways: installing it and getting
    started, looking at how you can instantiate specific models and use them, and
    exploring the RESTful APIs that let you build LLM applications that preserve privacy.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将以三种方式探索Ollama：安装和入门，查看如何实例化特定模型并使用它们，以及探索RESTful API，这些API让你能够构建保留隐私的LLM应用程序。
- en: Getting Started with Ollama
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ollama入门
- en: The Ollama project is hosted at [ollama.com](http://ollama.com). It’s pretty
    straightforward to get up and running, and the home screen gives download options
    for macOS, Linux, and Windows. Note also that the Windows version needs Windows
    Subsystem for Linux (WSL). For this chapter, I’m using the macOS version.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama项目托管在[ollama.com](http://ollama.com)。启动起来非常简单，主屏幕提供了macOS、Linux和Windows的下载选项。注意，Windows版本需要Windows
    Subsystem for Linux (WSL)。对于本章，我使用的是macOS版本。
- en: When you navigate to the website, you’ll see a friendly welcome to download
    (see [Figure 17-1](#ch17_figure_1_1748550058907779)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当你导航到网站时，你会看到一个友好的下载欢迎界面（见图17-1）。
- en: '![](assets/aiml_1701.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1701.png)'
- en: Figure 17-1\. Getting started with Ollama
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-1\. 使用Ollama入门
- en: Once you’ve downloaded and installed Ollama, you can launch it, and you’ll see
    it in the system bar at the top of the screen. Your main interface with Ollama
    will be the command line.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并安装Ollama后，你可以启动它，并在屏幕顶部的系统栏中看到它。你与Ollama的主要界面将是命令行。
- en: 'Then, with the `ollama run` command, you can download and use models. So, for
    example, if you want to use Gemma, then from Google, you can do the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`ollama run`命令，你可以下载和使用模型。例如，如果你想使用Gemma，那么从Google，你可以做以下操作：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll want to be sure to note the parameters used, which you can find in the
    [model’s documentation page on Ollama](https://oreil.ly/VMLKO). While Ollama can
    and will quantize models that are optimized to run locally, it can’t perform miracles,
    and only models that will fit in your system resources—most importantly, memory—will
    work. In this case, I ran the `gemma2:2b` (2-billion parameter) version, which
    requires about 8 GB of GPU RAM. On macOS, the shared RAM with the M-Series chips
    works well, while running on an M1 Mac with 16 Gb, the Gemma 2B is fast and smooth
    with Ollama.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要确保注意使用的参数，这些参数可以在Ollama的[模型文档页面](https://oreil.ly/VMLKO)上找到。虽然Ollama可以并对本地运行优化的模型进行量化，但它不能创造奇迹，只有适合你系统资源——最重要的是内存——的模型才能工作。在这种情况下，我运行了`gemma2:2b`（20亿参数）版本，这需要大约8
    GB的GPU RAM。在macOS上，与M系列芯片共享的RAM表现良好，而在16 Gb的M1 Mac上运行时，Gemma 2B与Ollama配合快速且流畅。
- en: You can see me chatting with Gemma in [Figure 17-2](#ch17_figure_2_1748550058907817).
    These responses took less than one second to receive on my two-year-old laptop!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图17-2](#ch17_figure_2_1748550058907817)中看到我正在与Gemma聊天。这些响应在我的两年老笔记本电脑上接收不到一秒！
- en: '![](assets/aiml_1702.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1702.png)'
- en: Figure 17-2\. Using Ollama in a terminal
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-2\. 在终端中使用Ollama
- en: It’s great to have a localized chat like this, and you can experiment with different
    models, including multimodal ones like Llama 3.2\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有这样一个本地化的聊天真是太好了，你可以尝试不同的模型，包括像Llama 3.2这样的多模态模型。
- en: 'So, for example, you could issue the following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以发出以下命令：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, within the terminal, you could do multimodal processing. For example,
    if your terminal supported it, and you dragged and dropped an image into the terminal,
    you could ask the model what it could see in the image. The multimodal power of
    Llama would parse the image for you, and Ollama would handle all the technical
    difficulties.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在终端内部，你可以进行多模态处理。例如，如果你的终端支持，你可以将图像拖放到终端中，并询问模型它在图像中看到了什么。Llama的多模态能力会为你解析图像，而Ollama将处理所有的技术难题。
- en: 'In my case, all I had to do was give a prompt and then drag and drop the image
    onto it. So, I opened an Ollama chat window with the preceding command and then
    entered this prompt:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我只需给出提示，然后将图像拖放到它上面。因此，我使用前面的命令打开了一个Ollama聊天窗口，并输入了这个提示：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And then I just dragged and dropped the image into the chat window, and Ollama
    did the rest.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我只是将图像拖放到聊天窗口中，Ollama就完成了剩余的工作。
- en: You can see in [Figure 17-3](#ch17_figure_4_1748550058907880) how detailed the
    results were. This is a photo I took of Osaka castle one morning while on a run
    in 2018\. While Llama couldn’t guess the date, it was able to predict the season
    based on the foliage in the image. It got everything else correct and gave very
    detailed output!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图17-3](#ch17_figure_4_1748550058907880)中看到结果有多么详细。这是我在2018年一次晨跑时拍摄的大阪城照片。虽然Llama无法猜出日期，但它能根据图像中的叶状物预测季节。它对其他所有事情都判断正确，并给出了非常详细的结果！
- en: '![](assets/aiml_1703.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1703.png)'
- en: Figure 17-3\. Using Ollama for a multimodal model
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-3\. 使用Ollama进行多模态模型
- en: While it’s really cool to have a local LLM that you can chat with in a privacy-preserving
    way, I think the real power in Ollama is in using it as a server that can then
    be the foundation of an application. We’ll explore that next.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一个可以以隐私保护方式聊天的本地LLM确实很酷，但我认为Ollama真正的力量在于将其用作服务器，这可以成为应用程序的基础。我们将在下一部分探讨这一点。
- en: Running Ollama as a Server
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以服务器模式运行Ollama
- en: 'To put Ollama into server mode, you simply issue the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要将Ollama置于服务器模式，你只需发出以下命令：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will run the Ollama server on port 11434 by default, so you can hit it
    and ask it to do inference with a `curl` command to test it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将默认在端口11434上运行Ollama服务器，因此你可以点击它，并用`curl`命令进行推理测试。
- en: 'In a separate terminal window, you issue a `curl` command. Here’s an example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个独立的终端窗口中，你发出一个`curl`命令。以下是一个示例：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note the `stream` parameter. If you set it to true, you’ll get an active HTTP
    connection that will send the answer word by word. That will give you a faster
    time to the first word, which is very suitable for chat applications. And because
    the answer will appear little by little and usually faster than a person can read,
    it will make for a better user experience.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`stream`参数。如果你将其设置为true，你将获得一个活跃的HTTP连接，该连接将逐字发送答案。这将使你更快地得到第一个单词，这对于聊天应用来说非常合适。而且因为答案会逐渐出现，通常比人阅读的速度快，这将提供更好的用户体验。
- en: On the other hand, if you set the `stream` parameter to `false`, as I have done
    here, it will take longer to send something back, but when it does, you’ll get
    everything at once. The time to the last token will probably be about the same
    as in streaming, but given that there will be no output for a little while, it
    will feel slower.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你将`stream`参数设置为`false`，就像我在这里所做的那样，发送东西将需要更长的时间，但一旦发送，你将一次性得到所有内容。到达最后一个标记的时间可能和流式传输时差不多，但考虑到有一段时间没有输出，这会感觉更慢。
- en: 'The preceding `curl` to Gemma gave me this response:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 给Gemma的先前`curl`命令给了我以下响应：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I trimmed the response text and the context for brevity. Ultimately, you’d use
    the response text in an application, but I wanted to also show you how you can
    build more robust applications showing you everything else that the model provided.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我为了简洁起见，对响应文本和上下文进行了裁剪。最终，你会在应用中使用响应文本，但我还想向你展示如何构建更健壮的应用，展示模型提供的其他所有内容。
- en: The `done` parameter demonstrates that the prompt returned successfully. When
    the value is streaming, this parameter will be set to `false` until it has finished
    sending the text. That way, you can keep your UI updating the text word by word
    until the message is complete.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`done`参数表明提示成功返回。当值为流式传输时，此参数将设置为`false`，直到完成发送文本。这样，你可以在消息完成之前，逐字更新你的UI文本。'
- en: The `done_reason` parameter is useful in checking for errors, particularly when
    streaming. It will usually contain `stop` for normal completion, but in other
    circumstances, it might say `length`, which indicates that you’ve hit a token
    limit; `canceled` if the user cancels the request (by interrupting streaming,
    for example); or `error`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`done_reason`参数在检查错误时很有用，尤其是在流式传输时。它通常包含`stop`表示正常完成，但在其他情况下，它可能会说`length`，这表示你达到了标记限制；如果是用户取消请求（例如通过中断流式传输），则可能是`canceled`；或者`error`。'
- en: The count values are also useful if you want to manage or report on token usage.
    The `prompt_eval_count` parameter tells you how many tokens were used in your
    prompt, and in this case, it was 15\. Similarly, the `eval_count` parameter tells
    you how many tokens were used in the response, and in this case (of course), it
    was 282\. The various duration numbers are in nanoseconds, so in this case, we
    can see that the total was 0.8 seconds (or more accurately, 820325334 nanoseconds).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想管理或报告标记使用情况，计数值也很有用。`prompt_eval_count`参数告诉你你的提示中使用了多少标记，在这种情况下，是15。同样，`eval_count`参数告诉你响应中使用了多少标记，在这种情况下（当然），是282。各种持续时间数字以纳秒为单位，所以在这种情况下，我们可以看到总时间是0.8秒（或者更准确地说，820325334纳秒）。
- en: If you want to attach a file to your `curl` prompt (for example, to interpret
    the contents of an image), you can do so by encoding the image to `base64` and
    passing it in the images array.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在`curl`提示中附加文件（例如，解释图像内容），你可以通过将图像编码为`base64`并将其传递到图像数组中来实现。
- en: 'So, with the image of Osaka castle I used in [Figure 17-3](#ch17_figure_4_1748550058907880),
    I could do the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，使用我在[图17-3](#ch17_figure_4_1748550058907880)中使用的大阪城堡的图像，我可以做以下操作：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The key here is to note how the images are sent. They need to be `base64` encoded,
    where they are turned into a string-like blob that’s easy to put into a JSON payload,
    instead of uploading the binary image. But you should be careful with code here,
    because it depends on your system. The code I used—`$(cat ./osaka.jpg | base64
    | tr -d '\n')`—is based on how to do `base64` encoding on a Mac. Different systems
    may produce different `base64` encodings for images, and they can lead to errors
    on the backend.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要注意图像是如何发送的。它们需要被`base64`编码，这样它们就被转换成一个类似字符串的blob，便于放入JSON有效负载中，而不是上传二进制图像。但你应该小心这里的代码，因为它取决于你的系统。我使用的代码`$(cat
    ./osaka.jpg | base64 | tr -d '\n')`是基于如何在Mac上执行`base64`编码。不同的系统可能为图像产生不同的`base64`编码，这可能导致后端错误。
- en: 'The response, abbreviated for clarity, is this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，响应被简略如下：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you’re trying this on your development box, you may notice that it starts
    up slowly as it loads the model into memory. It can take one to two minutes, but
    once it’s loaded and warmed up, successive inferences will be quicker.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这台开发机上尝试这个操作，你可能会注意到它启动缓慢，因为它正在将模型加载到内存中。这可能需要一到两分钟，但一旦加载并预热，后续的推理将会更快。
- en: Building an App that Uses an Ollama LLM
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建使用Ollama LLM的应用程序
- en: It’s all very nice to be able to chat with a local model or curl to it like
    we just saw. But the next step is to consider building applications that use local
    LLMs, and in particular, building applications that work within your network to
    keep LLM inference local.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 能够与本地模型聊天或像我们刚才看到的那样curl它，这都很不错。但下一步是考虑构建使用本地LLM的应用程序，特别是构建在您的网络内工作以保持LLM推理本地化的应用程序。
- en: So, for example, consider [Figure 17-4](#ch17_figure_5_1748550058907900), which
    depicts the architecture of a typical application that uses the API for an LLM
    like Gemini, GPT, or Claude. In this case, the user has an application that invokes
    the LLM service via an API over the internet.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑[图17-4](#ch17_figure_5_1748550058907900)，它描述了一个典型应用架构，该应用使用Gemini、GPT或Claude等LLM的API。在这种情况下，用户有一个应用程序，通过互联网上的API调用LLM服务。
- en: '![](assets/aiml_1704.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1704.png)'
- en: Figure 17-4\. Accessing an LLM via API over the internet
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-4\. 通过互联网API访问LLM
- en: 'Another pattern is quite similar: a service provider provides a backend web
    server, and that backend uses LLM functionality on your behalf. Ultimately, it
    still “wraps” the LLM on your behalf (see [Figure 17-5](#ch17_figure_6_1748550058907917)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种模式相当相似：服务提供商提供后端Web服务器，该后端代表你使用LLM功能。最终，它仍然“封装”了代表你的LLM（参见[图17-5](#ch17_figure_6_1748550058907917)）。
- en: '![](assets/aiml_1705.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1705.png)'
- en: Figure 17-5\. Accessing an LLM via a backend web server
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-5\. 通过后端Web服务器访问LLM
- en: The issue here is that data that could be private to your users and that they
    share with you (in the blue boxes) gets passed to a third party across the internet
    (in the green boxes). This can lead to limitations in how your application might
    be useful to them. Consider scenarios where there’s information that should always
    be private or where there’s IP that you don’t want to share. In the early days
    of ChatGPT, a lot of companies banned its use for that reason. The classic case
    involves source code, where company IP might be sent to a competitor for analysis!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是，可能属于你的用户且他们与你共享的数据（在蓝色框中）通过互联网传递给第三方（在绿色框中）。这可能导致你的应用程序对他们有用的限制。考虑那些应该始终保密的信息或你不希望分享的IP的情况。在ChatGPT的早期，许多公司出于这个原因禁止了它的使用。一个典型的例子是源代码，公司IP可能被发送给竞争对手进行分析！
- en: You can mitigate this problem by using a technology like Ollama. In this case,
    the architecture would change so that instead of the data being passed across
    the internet, the API and the server for the LLM would both be on your home or
    company’s network, as in [Figure 17-6](#ch17_figure_7_1748550058907933).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用Ollama等技术来减轻这个问题。在这种情况下，架构将改变，使得数据不是通过互联网传递，而是LLM的API和服务器都位于你的家庭或公司的网络中，如[图17-6](#ch17_figure_7_1748550058907933)所示。
- en: '![](assets/aiml_1706.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1706.png)'
- en: Figure 17-6\. Using an LLM in your data center
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-6\. 在数据中心使用LLM
- en: Now, there are no privacy issues with sharing data with third parties. Additionally,
    you have control over the model version that is being used, so you don’t have
    to worry about the API causing regression issues. Look back to Figures [17-4](#ch17_figure_5_1748550058907900)
    and [17-5](#ch17_figure_6_1748550058907917), and you’ll see that by accessing
    the LLMs over the internet, you’re taking a strong dependency on a particular
    version of an LLM. Given that LLMs are not deterministic, this effectively means
    the prompts that work today may not work tomorrow!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与第三方共享数据没有隐私问题。此外，你可以控制正在使用的模型版本，因此你不必担心API导致回归问题。回顾[图17-4](#ch17_figure_5_1748550058907900)和[图17-5](#ch17_figure_6_1748550058907917)，你会发现通过互联网访问LLM，你实际上是在依赖LLM的特定版本。鉴于LLM不是确定性的，这意味着今天有效的提示可能在明天可能不再有效！
- en: So, with Ollama as a server to LLMs, you can build something like you saw in
    [Figure 17-6](#ch17_figure_7_1748550058907933). In the development environment
    and what we’ll be doing in the rest of this chapter, there’s no data center—the
    Ollama server will just run at the localhost, but changing your code to one that
    you have access to over the local network will just mean a change of server address.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以Ollama作为LLM的服务器，你可以构建类似于[图17-6](#ch17_figure_7_1748550058907933)中看到的东西。在开发环境和本章的其余部分中，没有数据中心——Ollama服务器将仅在本地主机上运行，但将代码更改为你可以通过本地网络访问的代码，只需更改服务器地址即可。
- en: The Scenario
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景
- en: As an example of a simple scenario, let’s build the basis of an app that uses
    a local llama to do analysis of books. It will allow the user to specify a book
    (as a text file, for simplicity’s sake), and it will bundle that to a call to
    an LLM to have it analyze the book. The app will also primarily be driven off
    a prompt to that backend. Perhaps this type of app could be used by a publishing
    house to determine how it should give feedback to an author to change a book,
    or by a book agent to work with the author to help them make the book more sellable.
    As you can imagine, the book content is valuable IP, and it should not be shared
    with a backend from a third party for analysis.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简单场景的一个例子，让我们构建一个使用本地llama分析书籍的应用程序的基础。它将允许用户指定一本书（作为一个文本文件，为了简单起见），并将它捆绑到对LLM的调用，以分析这本书。该应用程序也将主要由对后端的提示驱动。也许这种应用程序可以被出版社用来确定如何向作者提供反馈以更改书籍，或者由图书代理人用来与作者合作，帮助他们使书籍更具销售性。正如你可以想象的那样，书籍内容是宝贵的知识产权，不应与第三方后端共享以进行分析。
- en: 'So, for example, consider this prompt:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下提示：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Sending that to an LLM along with the text should have the desired effect:
    getting an analysis of the book based on the *artificial understanding* of the
    contents of the text and the generative abilities of a transformer-based model
    to create output guided by the prompt.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与文本一起发送给LLM应该会产生预期的效果：基于文本内容的*人工理解*和基于transformer模型的生成能力，根据提示生成输出。
- en: On this book’s GitHub page, I’ve provided [the full text of a novel](https://oreil.ly/pytorch_ch18)
    that I wrote several years ago and that I now have the full rights to, so you
    can try it with a real book like that one if you like. However, depending on your
    model, the context window size might not be big enough for a complete novel.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书的GitHub页面上，我提供了一部小说的[全文](https://oreil.ly/pytorch_ch18)，这是我几年前写的，现在我拥有完整的版权，所以如果你喜欢，可以用这样的真实书籍来尝试。然而，根据你的模型，上下文窗口大小可能不足以容纳整部小说。
- en: Generally, I like to build a simple proof-of-concept as a Python file to see
    how well it works and test it on my local machine. There are constraints in doing
    this, but it will at least let us see if the concept is feasible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我喜欢构建一个简单的原型作为Python文件，看看它的工作效果如何，并在我的本地机器上测试它。这样做有一些限制，但至少可以让我们看到这个概念是否可行。
- en: Building a Python Proof-of-Concept
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Python原型
- en: Now, let’s take a look at a Python script that can perform the analysis for
    us.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个可以为我们执行分析的Python脚本。
- en: 'Let’s start with reading the contents of the file:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从读取文件内容开始：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We’re going to use Ollama on the backend, so let’s set up details for the request
    by specifying the URL and the content headers we’ll call:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后端使用Ollama，所以让我们通过指定URL和我们将称之为内容头部的详细信息来设置请求的细节：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we’ll put together the payload that we’re going to post to the backend.
    This contains the model name (as a parameter called `model`), the prompt, and
    the stream flag. We don’t want to stream, so we’ll set the stream to `False`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将组合我们将要发送到后端的有效负载。这包含模型名称（作为一个名为`model`的参数），提示和流标志。我们不想流式传输，所以我们将流设置为`False`。
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we’re appending `file_content` to the prompt. At this point, from
    the preceding code, it’s just a text blob.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在将`file_content`附加到提示中。在这个阶段，根据前面的代码，它只是一个文本块。
- en: For the model parameter, you can use whatever you like. For this experiment,
    I tried using the very small `Gemma 2b` parameter model by specifying `gemma2:2b`
    as the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型参数，你可以使用任何你喜欢的。在这个实验中，我尝试使用非常小的`Gemma 2b`参数模型，通过指定`gemma2:2b`作为模型。
- en: 'Now, we can POST the request to Ollama like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以这样向Ollama发送请求：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is fully synchronous in that we post the data and block everything until
    we get the result. In a real application, you’d likely do that part asynchronously,
    but I’m keeping it simple for now.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全同步，因为我们提交数据并阻塞一切，直到我们得到结果。在实际应用中，你可能会异步地做这部分，但现在我保持简单。
- en: As we get the response back from the server, it will contain JSON fields, and
    the response field will contain the text, as we saw earlier in this chapter. We’ll
    return this as a `dict` data type.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从服务器获取响应时，它将包含 JSON 字段，响应字段将包含文本，正如我们在本章前面所见。我们将以 `dict` 数据类型返回这个响应。
- en: 'Next, we can wrap all this code in a function called `analyze`, with this signature:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将这些代码包裹在一个名为 `analyze` 的函数中，具有以下签名：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we can easily call it with this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以轻松地使用以下方式调用它：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Given that Gemma is such a small model, the output it gives is very impressive!
    My local instance, via Ollama, was able to digest the book and give back a detailed
    analysis in just a few seconds. Here’s an excerpt (with spoilers if you haven’t
    read the book yet):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Gemma 是一个如此小的模型，它给出的输出非常令人印象深刻！我的本地实例，通过 Ollama，能够在几秒钟内消化这本书并给出详细的分析。以下是一个摘录（如果你还没有读过这本书，可能会有剧透）：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’s pretty impressive work by Gemma to give me this analysis! Other than formatting
    issues (there are a lot of * characters, which Gemma may have added because it’s
    sci-fi), we have some great content here, and it’s worth looking further into
    building an app. So, let’s do that next and explore a web-based app that I can
    upload my files to so that I can get an analysis within a website.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Gemma 给我做了这项分析，真是令人印象深刻！除了格式问题（有很多 * 字符，Gemma 可能是因为它是科幻小说而添加的），这里还有一些非常好的内容，值得进一步研究以构建一个应用。所以，让我们继续这样做，探索一个基于网络的
    Web 应用，我可以上传我的文件，以便在网站上获得分析。
- en: Creating a Web App for Ollama
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Ollama 创建一个 Web 应用
- en: In this scenario, you’ll create a `node.js` app that provides a local website
    that the user can upload text to, and you’ll get an analysis back in the browser.
    The results look like those shown in [Figure 17-7](#ch17_figure_8_1748550058907947).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，你将创建一个 `node.js` 应用，它提供了一个本地网站，用户可以上传文本，你将在浏览器中获取分析结果。结果看起来就像图 17-7 所示。
- en: '![](assets/aiml_1707.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1707.png)'
- en: Figure 17-7\. The browser-based Gemma analysis tool
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-7\. 基于浏览器的 Gemma 分析工具
- en: If you’re not familiar with `node.js` and how to install it, full instructions
    are on [the Node website](http://nodejs.org). The architecture of a simple node
    app is shown in [Figure 17-8](#ch17_figure_9_1748550058907961).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉 `node.js` 以及如何安装它，完整的说明可以在 [Node 网站](http://nodejs.org) 上找到。简单 node
    应用的架构如图 17-8 所示。
- en: '![](assets/aiml_1708.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1708.png)'
- en: Figure 17-8\. The node app directory
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-8\. node 应用目录
- en: In its simplest form, a *node app* is a directory containing a JavaScript file
    called *app.js* that contains the core application logic, a *package.json* file
    that gives details of the dependencies, and an *index.html* file that has the
    template for the app output.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，一个 *node app* 是一个包含名为 *app.js* 的 JavaScript 文件、包含依赖项详细信息的 *package.json*
    文件以及具有应用输出模板的 *index.html* 文件的目录。
- en: The app.js File
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: app.js 文件
- en: 'This file contains the core logic for the service. A `node.js` app will run
    on a server by listening to a particular port, and it starts with this code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含服务的核心逻辑。一个 `node.js` 应用将通过监听特定端口在服务器上运行，它从以下代码开始：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To analyze a book, you can define an endpoint that the end user can post the
    book to with the `app.post` command in `node.js`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要分析一本书，你可以定义一个端点，最终用户可以使用 `node.js` 中的 `app.post` 命令将书发布到该端点：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This function is asynchronous, and it accepts a file. If the file isn’t present
    in the upload, an error will return.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是异步的，它接受一个文件。如果文件不在上传中，将返回错误。
- en: 'If the code continues, then there’s a file present. This code will start a
    new job (allowing the server to operate multiple processes in parallel) that uploads
    the file, reads its text into `fileContent`, and then cleans it up:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代码继续执行，则表示存在文件。此代码将启动一个新任务（允许服务器并行运行多个进程），上传文件，将其文本读入 `fileContent`，然后清理它：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The analysis of the novel is a long-running process. In it, the contents are
    appended to the prompt and uploaded to Ollama, which then passes it to Gemma—which,
    upon completion, sends us a result. We’ll look at the analysis code in a moment,
    but the wrapper for this that operates in the background is here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 小说的分析是一个长时间运行的过程。在这个过程中，内容会被附加到提示中，并上传到Ollama，然后它将传递给Gemma，Gemma在完成时发送给我们结果。我们稍后将查看分析代码，但这个在后台运行的包装器如下所示：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It simply calls the `analyzeNovel` method, sending the file content. If the
    process completes successfully, the `JobId` is updated with `completed`; otherwise,
    it’s updated with details of the failure. Note that upon a successful completion,
    the result is passed to `analysisJobs`. Later, when we look at the web client,
    we’ll see that after uploading the content, it will repeatedly poll the status
    of the job until it gets either a completion or an error. At that point, it can
    display the appropriate output.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是调用了`analyzeNovel`方法，并发送文件内容。如果过程成功完成，`JobId`会被更新为`completed`；否则，它会更新为失败的详细信息。请注意，在成功完成之后，结果会被传递到`analysisJobs`。稍后，当我们查看网络客户端时，我们会看到在上传内容后，它将反复轮询作业的状态，直到得到完成或错误。到那时，它可以显示适当的输出。
- en: 'The `analyzeNovel` function looks very similar to our Python code from earlier.
    First, we’ll create the request body with the prompt. It contains the `modelID`,
    the prompt text with the novel text appended, and the `stream` parameter set to
    false:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`analyzeNovel`函数看起来与我们之前看到的Python代码非常相似。首先，我们将创建带有提示的请求体。它包含`modelID`、附加了小说文本的提示文本，并将`stream`参数设置为false：'
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It will then post this to the Ollama backend and wait for the response. When
    it gets it, it will turn it into a string with `JSON.stringify`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它将把这个信息发送到Ollama后端，并等待响应。当它收到响应时，它将使用`JSON.stringify`将其转换为字符串：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If there’s an error, we’ll throw it; otherwise, we’ll read in the JSON payload
    from the HTTP response and filter out the response field, which contains the text
    response from the LLM. Note the two uses of the word *response* here. It can be
    confusing! The *response object* (`response.ok`, `response.status`, or `response.json`)
    is the HTTP response to the post that you made, while the *response property*
    (`data.response`) is the field within the `json` that contains the response from
    the LLM with the generated output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有错误，我们会抛出它；否则，我们会从HTTP响应中读取JSON有效负载，并过滤出包含LLM生成的文本响应的`response`字段。注意这里对“response”一词的两次使用。可能会令人困惑！这里的*response对象*（`response.ok`、`response.status`或`response.json`）是你所做的POST的HTTP响应，而*response属性*（`data.response`）是`json`中包含LLM生成的输出的字段：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The Index.html File
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Index.html 文件
- en: In the public folder, a file called *index.html* will render when you call the
    `node.js` server. By default, this is at `localhost:3000` if you’re running on
    your dev box. It will contain all the code to render the user interface for the
    app that we saw in [Figure 17-7](#ch17_figure_8_1748550058907947).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在公共文件夹中，当你调用`node.js`服务器时，会渲染一个名为*index.html*的文件。默认情况下，如果你在开发箱上运行，它位于`localhost:3000`。它将包含渲染我们之前在[图17-7](#ch17_figure_8_1748550058907947)中看到的app用户界面的所有代码。
- en: 'It interfaces with the backend through a form that is submitted via an HTTP-POST.
    The HTML code for the form looks like this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过一个通过HTTP-POST提交的表单与后端接口。表单的HTML代码看起来像这样：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The form is called `uploadForm`, so you can write code to execute when the
    user hits the submit button on this form like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该表单被称为`uploadForm`，因此你可以编写代码在用户点击此表单的提交按钮时执行，如下所示：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The magic happens within this code by taking the attached file (as `FormData`)
    and passing it to the `/analyze` endpoint of the backend, as we defined using
    `app.post` in the *app.js* file (seen previously):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 魔法就在这段代码中，通过将附加的文件（作为`FormData`）传递给后端的`/analyze`端点，正如我们在之前的`app.js`文件中定义的那样（见前文）：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once this is done, the browser will get the `jobID` back from the server and
    continually poll the server asking for the status of that `jobID`. Once the status
    of the job is `completed`, the server will output the results if the job succeeded
    or the error if it didn’t:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，浏览器将从服务器获取`jobID`，并不断轮询服务器以获取该`jobID`的状态。一旦作业状态为`completed`，服务器将输出结果，如果作业成功，或者错误，如果失败了：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `setTimeout` code at the bottom ensures that we poll every second, but you
    could change this to reduce load on your server.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的`setTimeout`代码确保我们每秒轮询一次，但你也可以将其更改为减少服务器的负载。
- en: 'To run this, simply navigate to the directory in your console and type this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此程序，只需在控制台中导航到目录，并输入以下内容：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: And that’s it! You can get the fully working code in the downloadable files
    for this book, and I’ve also included a copy of the novel as a text file so you
    can try it out for yourself.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！您可以在本书的可下载文件中找到完整的可工作代码，我还包括了一份小说的文本文件，以便您可以亲自尝试。
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how the open source Ollama tool gives you the
    ability to wrap an LLM with an easy-to-use API that lets you build applications
    with it. You saw how to install Ollama and then explored some scenarios with it.
    You also downloaded and used models like the simple, lightweight Gemma from Google,
    as well as the powerful, multimodal Llama3.2-vision from Meta. You explored not
    just chatting with them but also attaching files to upload. Ollama gives you an
    HTTP endpoint that you saw how to experiment with by using a `curl` command to
    simulate HTTP traffic. Finally, you got into writing a prototype of a real-world
    LLM-based application that analyzed contents of books, first as a simple Python
    script that proved the concept and then as a more sophisticated web-based application
    in `node.js` that used an Ollama backend and a Gemma LLM to do the heavy lifting!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了开源的Ollama工具如何让您能够通过一个易于使用的API将LLM包装起来，从而构建应用程序。您学习了如何安装Ollama，并使用它探索了一些场景。您还下载并使用了像Google的简单、轻量级的Gemma这样的模型，以及来自Meta的强大、多模态的Llama3.2-vision。您不仅探索了与他们聊天，还探索了附加文件上传的功能。Ollama为您提供了一个HTTP端点，您通过使用`curl`命令模拟HTTP流量来实验了如何使用它。最后，您开始编写一个基于真实世界LLM的应用程序的原型，该程序分析书籍内容，最初是一个证明概念的简单Python脚本，然后是一个更复杂的基于`node.js`的Web应用程序，该应用程序使用Ollama后端和Gemma
    LLM来完成繁重的工作！
- en: In the next chapter, we’ll build on this and explore the concepts of RAG, and
    you’ll build apps that use local vector databases to enhance the knowledge of
    LLMs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在此基础上构建，并探讨RAG的概念，您将构建使用本地向量数据库来增强LLM知识的应用程序。
