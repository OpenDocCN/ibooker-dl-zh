- en: Chapter 17\. Serving LLMs with Ollama
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve explored how to use transformers to download a model and put together
    an easy pipeline that lets you use it for inference or fine-tuning. However, I’d
    be remiss if I didn’t show you the open source Ollama project, which ties it all
    together by giving you an environment that gives you a full wrapper around an
    LLM that you can either chat with in your terminal or use as a server that you
    can HTTP POST to and read the output from.
  prefs: []
  type: TYPE_NORMAL
- en: Technologies like Ollama will be the vanguard of the next generation of LLMs,
    which will let you have dedicated servers inside your data center or dedicated
    processes on your computer. That will make them completely private to you.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, Ollama is an open source project that simplifies the process of
    downloading, running, and managing LLMs on your computer. It also handles nonfunctional
    difficult requirements, such as memory management and model optimization, and
    it provides standardized interfaces for interaction, such as the ability to HTTP
    POST to your models.
  prefs: []
  type: TYPE_NORMAL
- en: Ollama is also a key strategic tool you should consider because it bridges the
    gap between cloud-based third-party services like GPT, Claude, and Gemini and
    locally deployed services. It goes beyond giving you a local development environment
    to giving you one that you could, for example, use within your own data center
    to serve multiple internal users.
  prefs: []
  type: TYPE_NORMAL
- en: By running models locally, you can ensure the complete privacy of your data,
    eliminate network latency, and work offline. This is especially crucial in scenarios
    involving sensitive data or applications that require consistent, low-latency
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: Ollama also supports a growing library of popular open source models, including
    Llama, Mistral, and Gemma, and it also supports various specialized models that
    are optimized for specific tasks. Each model can be pulled and run with simple
    commands, in a way that’s similar to how Docker containers work. The platform
    handles model quantization automatically, optimizing models to run efficiently
    on consumer hardware while maintaining good performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll explore Ollama in three ways: installing it and getting
    started, looking at how you can instantiate specific models and use them, and
    exploring the RESTful APIs that let you build LLM applications that preserve privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Ollama
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Ollama project is hosted at [ollama.com](http://ollama.com). It’s pretty
    straightforward to get up and running, and the home screen gives download options
    for macOS, Linux, and Windows. Note also that the Windows version needs Windows
    Subsystem for Linux (WSL). For this chapter, I’m using the macOS version.
  prefs: []
  type: TYPE_NORMAL
- en: When you navigate to the website, you’ll see a friendly welcome to download
    (see [Figure 17-1](#ch17_figure_1_1748550058907779)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. Getting started with Ollama
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve downloaded and installed Ollama, you can launch it, and you’ll see
    it in the system bar at the top of the screen. Your main interface with Ollama
    will be the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, with the `ollama run` command, you can download and use models. So, for
    example, if you want to use Gemma, then from Google, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You’ll want to be sure to note the parameters used, which you can find in the
    [model’s documentation page on Ollama](https://oreil.ly/VMLKO). While Ollama can
    and will quantize models that are optimized to run locally, it can’t perform miracles,
    and only models that will fit in your system resources—most importantly, memory—will
    work. In this case, I ran the `gemma2:2b` (2-billion parameter) version, which
    requires about 8 GB of GPU RAM. On macOS, the shared RAM with the M-Series chips
    works well, while running on an M1 Mac with 16 Gb, the Gemma 2B is fast and smooth
    with Ollama.
  prefs: []
  type: TYPE_NORMAL
- en: You can see me chatting with Gemma in [Figure 17-2](#ch17_figure_2_1748550058907817).
    These responses took less than one second to receive on my two-year-old laptop!
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-2\. Using Ollama in a terminal
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s great to have a localized chat like this, and you can experiment with different
    models, including multimodal ones like Llama 3.2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, you could issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, within the terminal, you could do multimodal processing. For example,
    if your terminal supported it, and you dragged and dropped an image into the terminal,
    you could ask the model what it could see in the image. The multimodal power of
    Llama would parse the image for you, and Ollama would handle all the technical
    difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, all I had to do was give a prompt and then drag and drop the image
    onto it. So, I opened an Ollama chat window with the preceding command and then
    entered this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And then I just dragged and dropped the image into the chat window, and Ollama
    did the rest.
  prefs: []
  type: TYPE_NORMAL
- en: You can see in [Figure 17-3](#ch17_figure_4_1748550058907880) how detailed the
    results were. This is a photo I took of Osaka castle one morning while on a run
    in 2018\. While Llama couldn’t guess the date, it was able to predict the season
    based on the foliage in the image. It got everything else correct and gave very
    detailed output!
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-3\. Using Ollama for a multimodal model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While it’s really cool to have a local LLM that you can chat with in a privacy-preserving
    way, I think the real power in Ollama is in using it as a server that can then
    be the foundation of an application. We’ll explore that next.
  prefs: []
  type: TYPE_NORMAL
- en: Running Ollama as a Server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To put Ollama into server mode, you simply issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will run the Ollama server on port 11434 by default, so you can hit it
    and ask it to do inference with a `curl` command to test it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a separate terminal window, you issue a `curl` command. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note the `stream` parameter. If you set it to true, you’ll get an active HTTP
    connection that will send the answer word by word. That will give you a faster
    time to the first word, which is very suitable for chat applications. And because
    the answer will appear little by little and usually faster than a person can read,
    it will make for a better user experience.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you set the `stream` parameter to `false`, as I have done
    here, it will take longer to send something back, but when it does, you’ll get
    everything at once. The time to the last token will probably be about the same
    as in streaming, but given that there will be no output for a little while, it
    will feel slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding `curl` to Gemma gave me this response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I trimmed the response text and the context for brevity. Ultimately, you’d use
    the response text in an application, but I wanted to also show you how you can
    build more robust applications showing you everything else that the model provided.
  prefs: []
  type: TYPE_NORMAL
- en: The `done` parameter demonstrates that the prompt returned successfully. When
    the value is streaming, this parameter will be set to `false` until it has finished
    sending the text. That way, you can keep your UI updating the text word by word
    until the message is complete.
  prefs: []
  type: TYPE_NORMAL
- en: The `done_reason` parameter is useful in checking for errors, particularly when
    streaming. It will usually contain `stop` for normal completion, but in other
    circumstances, it might say `length`, which indicates that you’ve hit a token
    limit; `canceled` if the user cancels the request (by interrupting streaming,
    for example); or `error`.
  prefs: []
  type: TYPE_NORMAL
- en: The count values are also useful if you want to manage or report on token usage.
    The `prompt_eval_count` parameter tells you how many tokens were used in your
    prompt, and in this case, it was 15\. Similarly, the `eval_count` parameter tells
    you how many tokens were used in the response, and in this case (of course), it
    was 282\. The various duration numbers are in nanoseconds, so in this case, we
    can see that the total was 0.8 seconds (or more accurately, 820325334 nanoseconds).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to attach a file to your `curl` prompt (for example, to interpret
    the contents of an image), you can do so by encoding the image to `base64` and
    passing it in the images array.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, with the image of Osaka castle I used in [Figure 17-3](#ch17_figure_4_1748550058907880),
    I could do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The key here is to note how the images are sent. They need to be `base64` encoded,
    where they are turned into a string-like blob that’s easy to put into a JSON payload,
    instead of uploading the binary image. But you should be careful with code here,
    because it depends on your system. The code I used—`$(cat ./osaka.jpg | base64
    | tr -d '\n')`—is based on how to do `base64` encoding on a Mac. Different systems
    may produce different `base64` encodings for images, and they can lead to errors
    on the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response, abbreviated for clarity, is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you’re trying this on your development box, you may notice that it starts
    up slowly as it loads the model into memory. It can take one to two minutes, but
    once it’s loaded and warmed up, successive inferences will be quicker.
  prefs: []
  type: TYPE_NORMAL
- en: Building an App that Uses an Ollama LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s all very nice to be able to chat with a local model or curl to it like
    we just saw. But the next step is to consider building applications that use local
    LLMs, and in particular, building applications that work within your network to
    keep LLM inference local.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, consider [Figure 17-4](#ch17_figure_5_1748550058907900), which
    depicts the architecture of a typical application that uses the API for an LLM
    like Gemini, GPT, or Claude. In this case, the user has an application that invokes
    the LLM service via an API over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-4\. Accessing an LLM via API over the internet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Another pattern is quite similar: a service provider provides a backend web
    server, and that backend uses LLM functionality on your behalf. Ultimately, it
    still “wraps” the LLM on your behalf (see [Figure 17-5](#ch17_figure_6_1748550058907917)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-5\. Accessing an LLM via a backend web server
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The issue here is that data that could be private to your users and that they
    share with you (in the blue boxes) gets passed to a third party across the internet
    (in the green boxes). This can lead to limitations in how your application might
    be useful to them. Consider scenarios where there’s information that should always
    be private or where there’s IP that you don’t want to share. In the early days
    of ChatGPT, a lot of companies banned its use for that reason. The classic case
    involves source code, where company IP might be sent to a competitor for analysis!
  prefs: []
  type: TYPE_NORMAL
- en: You can mitigate this problem by using a technology like Ollama. In this case,
    the architecture would change so that instead of the data being passed across
    the internet, the API and the server for the LLM would both be on your home or
    company’s network, as in [Figure 17-6](#ch17_figure_7_1748550058907933).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-6\. Using an LLM in your data center
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, there are no privacy issues with sharing data with third parties. Additionally,
    you have control over the model version that is being used, so you don’t have
    to worry about the API causing regression issues. Look back to Figures [17-4](#ch17_figure_5_1748550058907900)
    and [17-5](#ch17_figure_6_1748550058907917), and you’ll see that by accessing
    the LLMs over the internet, you’re taking a strong dependency on a particular
    version of an LLM. Given that LLMs are not deterministic, this effectively means
    the prompts that work today may not work tomorrow!
  prefs: []
  type: TYPE_NORMAL
- en: So, with Ollama as a server to LLMs, you can build something like you saw in
    [Figure 17-6](#ch17_figure_7_1748550058907933). In the development environment
    and what we’ll be doing in the rest of this chapter, there’s no data center—the
    Ollama server will just run at the localhost, but changing your code to one that
    you have access to over the local network will just mean a change of server address.
  prefs: []
  type: TYPE_NORMAL
- en: The Scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example of a simple scenario, let’s build the basis of an app that uses
    a local llama to do analysis of books. It will allow the user to specify a book
    (as a text file, for simplicity’s sake), and it will bundle that to a call to
    an LLM to have it analyze the book. The app will also primarily be driven off
    a prompt to that backend. Perhaps this type of app could be used by a publishing
    house to determine how it should give feedback to an author to change a book,
    or by a book agent to work with the author to help them make the book more sellable.
    As you can imagine, the book content is valuable IP, and it should not be shared
    with a backend from a third party for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, consider this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending that to an LLM along with the text should have the desired effect:
    getting an analysis of the book based on the *artificial understanding* of the
    contents of the text and the generative abilities of a transformer-based model
    to create output guided by the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: On this book’s GitHub page, I’ve provided [the full text of a novel](https://oreil.ly/pytorch_ch18)
    that I wrote several years ago and that I now have the full rights to, so you
    can try it with a real book like that one if you like. However, depending on your
    model, the context window size might not be big enough for a complete novel.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, I like to build a simple proof-of-concept as a Python file to see
    how well it works and test it on my local machine. There are constraints in doing
    this, but it will at least let us see if the concept is feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Python Proof-of-Concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s take a look at a Python script that can perform the analysis for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with reading the contents of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to use Ollama on the backend, so let’s set up details for the request
    by specifying the URL and the content headers we’ll call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll put together the payload that we’re going to post to the backend.
    This contains the model name (as a parameter called `model`), the prompt, and
    the stream flag. We don’t want to stream, so we’ll set the stream to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’re appending `file_content` to the prompt. At this point, from
    the preceding code, it’s just a text blob.
  prefs: []
  type: TYPE_NORMAL
- en: For the model parameter, you can use whatever you like. For this experiment,
    I tried using the very small `Gemma 2b` parameter model by specifying `gemma2:2b`
    as the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can POST the request to Ollama like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is fully synchronous in that we post the data and block everything until
    we get the result. In a real application, you’d likely do that part asynchronously,
    but I’m keeping it simple for now.
  prefs: []
  type: TYPE_NORMAL
- en: As we get the response back from the server, it will contain JSON fields, and
    the response field will contain the text, as we saw earlier in this chapter. We’ll
    return this as a `dict` data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can wrap all this code in a function called `analyze`, with this signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can easily call it with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that Gemma is such a small model, the output it gives is very impressive!
    My local instance, via Ollama, was able to digest the book and give back a detailed
    analysis in just a few seconds. Here’s an excerpt (with spoilers if you haven’t
    read the book yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It’s pretty impressive work by Gemma to give me this analysis! Other than formatting
    issues (there are a lot of * characters, which Gemma may have added because it’s
    sci-fi), we have some great content here, and it’s worth looking further into
    building an app. So, let’s do that next and explore a web-based app that I can
    upload my files to so that I can get an analysis within a website.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Web App for Ollama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this scenario, you’ll create a `node.js` app that provides a local website
    that the user can upload text to, and you’ll get an analysis back in the browser.
    The results look like those shown in [Figure 17-7](#ch17_figure_8_1748550058907947).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-7\. The browser-based Gemma analysis tool
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re not familiar with `node.js` and how to install it, full instructions
    are on [the Node website](http://nodejs.org). The architecture of a simple node
    app is shown in [Figure 17-8](#ch17_figure_9_1748550058907961).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-8\. The node app directory
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In its simplest form, a *node app* is a directory containing a JavaScript file
    called *app.js* that contains the core application logic, a *package.json* file
    that gives details of the dependencies, and an *index.html* file that has the
    template for the app output.
  prefs: []
  type: TYPE_NORMAL
- en: The app.js File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This file contains the core logic for the service. A `node.js` app will run
    on a server by listening to a particular port, and it starts with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To analyze a book, you can define an endpoint that the end user can post the
    book to with the `app.post` command in `node.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This function is asynchronous, and it accepts a file. If the file isn’t present
    in the upload, an error will return.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the code continues, then there’s a file present. This code will start a
    new job (allowing the server to operate multiple processes in parallel) that uploads
    the file, reads its text into `fileContent`, and then cleans it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The analysis of the novel is a long-running process. In it, the contents are
    appended to the prompt and uploaded to Ollama, which then passes it to Gemma—which,
    upon completion, sends us a result. We’ll look at the analysis code in a moment,
    but the wrapper for this that operates in the background is here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It simply calls the `analyzeNovel` method, sending the file content. If the
    process completes successfully, the `JobId` is updated with `completed`; otherwise,
    it’s updated with details of the failure. Note that upon a successful completion,
    the result is passed to `analysisJobs`. Later, when we look at the web client,
    we’ll see that after uploading the content, it will repeatedly poll the status
    of the job until it gets either a completion or an error. At that point, it can
    display the appropriate output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `analyzeNovel` function looks very similar to our Python code from earlier.
    First, we’ll create the request body with the prompt. It contains the `modelID`,
    the prompt text with the novel text appended, and the `stream` parameter set to
    false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It will then post this to the Ollama backend and wait for the response. When
    it gets it, it will turn it into a string with `JSON.stringify`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If there’s an error, we’ll throw it; otherwise, we’ll read in the JSON payload
    from the HTTP response and filter out the response field, which contains the text
    response from the LLM. Note the two uses of the word *response* here. It can be
    confusing! The *response object* (`response.ok`, `response.status`, or `response.json`)
    is the HTTP response to the post that you made, while the *response property*
    (`data.response`) is the field within the `json` that contains the response from
    the LLM with the generated output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The Index.html File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the public folder, a file called *index.html* will render when you call the
    `node.js` server. By default, this is at `localhost:3000` if you’re running on
    your dev box. It will contain all the code to render the user interface for the
    app that we saw in [Figure 17-7](#ch17_figure_8_1748550058907947).
  prefs: []
  type: TYPE_NORMAL
- en: 'It interfaces with the backend through a form that is submitted via an HTTP-POST.
    The HTML code for the form looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The form is called `uploadForm`, so you can write code to execute when the
    user hits the submit button on this form like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The magic happens within this code by taking the attached file (as `FormData`)
    and passing it to the `/analyze` endpoint of the backend, as we defined using
    `app.post` in the *app.js* file (seen previously):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, the browser will get the `jobID` back from the server and
    continually poll the server asking for the status of that `jobID`. Once the status
    of the job is `completed`, the server will output the results if the job succeeded
    or the error if it didn’t:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `setTimeout` code at the bottom ensures that we poll every second, but you
    could change this to reduce load on your server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this, simply navigate to the directory in your console and type this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! You can get the fully working code in the downloadable files
    for this book, and I’ve also included a copy of the novel as a text file so you
    can try it out for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how the open source Ollama tool gives you the
    ability to wrap an LLM with an easy-to-use API that lets you build applications
    with it. You saw how to install Ollama and then explored some scenarios with it.
    You also downloaded and used models like the simple, lightweight Gemma from Google,
    as well as the powerful, multimodal Llama3.2-vision from Meta. You explored not
    just chatting with them but also attaching files to upload. Ollama gives you an
    HTTP endpoint that you saw how to experiment with by using a `curl` command to
    simulate HTTP traffic. Finally, you got into writing a prototype of a real-world
    LLM-based application that analyzed contents of books, first as a simple Python
    script that proved the concept and then as a more sophisticated web-based application
    in `node.js` that used an Ollama backend and a Gemma LLM to do the heavy lifting!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll build on this and explore the concepts of RAG, and
    you’ll build apps that use local vector databases to enhance the knowledge of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
