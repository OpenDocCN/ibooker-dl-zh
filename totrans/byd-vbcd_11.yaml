- en: Chapter 8\. Security, Maintainability, and Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter confronts a critical aspect of vibe coding and AI-assisted engineering—ensuring
    that the code you produce with AI assistance is secure, reliable, and maintainable.
    Speed and productivity mean little if the resulting software is riddled with vulnerabilities
    or prone to crashing.
  prefs: []
  type: TYPE_NORMAL
- en: First, I’ll examine common security pitfalls that arise in AI-generated code,
    from injection vulnerabilities to secrets leakage. You’ll learn techniques for
    auditing and reviewing AI-written code for such issues, effectively acting as
    the security safety net for your AI pair programmer.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I’ll discuss building effective testing and QA frameworks around AI-generated
    code to catch bugs and reliability issues early. Performance considerations will
    also be covered. AI might write correct code, but it’s not always the most efficient
    code, so I’ll outline how to identify and optimize performance bottlenecks. I’ll
    also explore strategies to ensure maintainability, such as enforcing consistent
    styles or refactoring AI code, since AI suggestions can sometimes be inconsistent
    or overly verbose.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll show you how to adapt your code-review practices to an AI-assisted workflow,
    highlighting what human reviewers should focus on when reviewing code that was
    partially or wholly machine-generated. Finally, I’ll round up best practices for
    deploying AI-assisted projects with confidence, from continuous integration pipelines
    to monitoring in production. By the end of this chapter, you’ll have a toolkit
    of approaches to keep your AI-accelerated development safe and robust.
  prefs: []
  type: TYPE_NORMAL
- en: Common Security Vulnerabilities in AI-Generated Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI coding assistants, while powerful, can inadvertently introduce security issues
    if not guided properly. They learn from lots of public code—which includes both
    good and bad practices—and may regurgitate insecure patterns if the prompt or
    context doesn’t steer them away. It’s vital for you to know these common pitfalls
    so you can spot and fix them. This can include using both manual and automated
    means to detect potential security issues (see [Figure 8-1](#ch08_figure_1_1752630044605613)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bevc_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-1\. AI-introduced security vulnerabilities: AI-generated code may
    contain subtle security flaws that require careful review and automated security
    scanning to identify and remediate.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Some typical security issues observed in AI-generated code include:'
  prefs: []
  type: TYPE_NORMAL
- en: Hard-coded secrets or credentials
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes AI outputs API keys, passwords, or tokens in code, especially if similar
    examples were in its training data. For instance, if you ask it to integrate with
    AWS, it might put a dummy AWS secret key directly in the code. This is dangerous
    if left in—it could leak sensitive info if the code is shared. Always ensure that
    secrets are properly managed via environment variables or config files. If an
    AI suggests something like `api_key = "ABC123SECRET"`, treat it as a flag—real
    keys should not be in source code.
  prefs: []
  type: TYPE_NORMAL
- en: SQL injection vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have your AI model generate SQL queries or ORM usage, check that it’s
    not constructing queries by concatenating user input directly. For example, an
    insecure pattern would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is susceptible to injection attacks. An AI might produce this if you don’t
    specifically tell it to parameterize queries. Always use prepared statements or
    parameter binding. Many AI assistants will do so if they recall best practices
    (like using `?` or placeholders for user inputs in SQL), but it’s not guaranteed.
    It’s on you to verify and ask the AI to fix it if needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Cross-site scripting (XSS) in web apps
  prefs: []
  type: TYPE_NORMAL
- en: 'When generating web code, AI tools don’t always automatically escape user input
    in outputs. For example, your AI might produce a templating snippet that directly
    inserts `{{comment.text}}` into HTML without escaping, which could allow a malicious
    script placed in a comment to run. If using frameworks, AIs often escape by default,
    but if they’re handling raw HTML construction, be careful. Implement output encoding
    or sanitization routines. You can prompt the AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Many modern frameworks have built-in mechanisms, so ensure that the AI uses
    them, like `innerText` versus `innerHTML` in [Document Object Model (DOM) manipulation](https://oreil.ly/5o_2x).
  prefs: []
  type: TYPE_NORMAL
- en: Improper authentication and authorization
  prefs: []
  type: TYPE_NORMAL
- en: 'AIs can write authentication flows, but subtle mistakes might creep in: for
    instance, generating a [JSON Web Token (JWT)](https://oreil.ly/rf7JL) without
    a sufficiently strong secret or not checking a password hash correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same is true for authorization: an AI might not automatically enforce that
    an action (like deleting a resource) is limited to the user who owns that resource.
    These logic issues are hard to catch automatically—they require thinking through
    the security model. When writing such code, specify clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then test those conditions. It’s easy for an AI to omit a check because it doesn’t
    truly “understand” the context unless told.
  prefs: []
  type: TYPE_NORMAL
- en: Insecure defaults or configurations
  prefs: []
  type: TYPE_NORMAL
- en: 'AI might choose convenience over security unless prompted to do otherwise.
    Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using HTTP instead of HTTPS for API calls (if TLS is not specified)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not validating SSL certificates (some code examples on the internet use `verify=false`
    in requests, which AI might copy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Widely enabling CORS for all origins and methods without restriction (potentially
    opening the app to any cross-origin requests)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing outdated cryptography (like MD5 or SHA1 for hashes, which are weak,
    instead of SHA-256/Bcrypt/Argon2 for passwords)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These issues are often subtle, which is one reason it’s good to audit your configuration
    files and initialization code. If the AI sets up something like `app.UseCors(allowAll)`
    or chooses an old cipher, you should spot that and correct it.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling revealing sensitive info
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated error handling might print or return stack traces. For example,
    a Node.js API might catch an error and do `res.send(err.toString())`, which could
    leak internal details. Ensure that error messages to users are sanitized and logs
    are properly handled. Adjust as needed to avoid giving attackers clues like full
    error messages or file paths.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency management and updates
  prefs: []
  type: TYPE_NORMAL
- en: 'If the AI adds dependencies (such as libraries) to your project, ensure that
    they’re up to date and from reputable sources. An AI might pick a library that
    was popular in its training data, but that is no longer maintained or has known
    vulnerabilities. For instance, if it suggests using an older version of a package,
    you should bump it to the latest stable. Running `npm audit` or equivalent after
    generation is wise too. Or ask the AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Is this library still maintained and secure?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It might not fully know, but it could tell you if there’s a known deprecation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A 2023 large-scale analysis of GitHub Copilot in real-world projects revealed
    that as much as 25%–33% of generated code—depending on language—contained potential
    security weaknesses, including high-severity CWEs such as command injection, code
    injection, and [cross-site scripting](https://arxiv.org/abs/2310.02059). These
    findings underscore that Copilot reflects insecure patterns present in its training
    data, as opposed to intentionally producing flawed code. The consistent recommendation?
    Developers must stay alert: manually review AI-generated code, use security-aware
    tooling, and maintain strict code hygiene. Especially during “vibe coding,” the
    speed and scope of AI-generated content demand even more vigilance. More code
    in less time means more surface area to audit.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a short example.
  prefs: []
  type: TYPE_NORMAL
- en: Improper Authentication and Authorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine you ask an AI to create a login route in an Express app. It might produce
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: What are the issues here?
  prefs: []
  type: TYPE_NORMAL
- en: It compares passwords directly, implying that the password is stored in plain
    text in the database—a big no-no.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It sends very generic responses, which may be appropriate for security but could
    also inadvertently expose sensitive information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider authentication  error messages as a critical example. A properly secure
    system should return a generic message like “Invalid credentials” when login fails,
    regardless of whether the username or password was incorrect. However, AI-generated
    code might produce more specific errors such as “Username not found” or “Incorrect
    password for this user.”
  prefs: []
  type: TYPE_NORMAL
- en: These specific messages create a security vulnerability by confirming to potential
    attackers which piece of information they have correct. If an attacker receives
    “Incorrect password” as an error, they now know they have discovered a valid username
    in your system. This enables them to build a list of legitimate usernames through
    repeated attempts, then focus their efforts on cracking passwords for those confirmed
    accounts. This technique, known as *user enumeration*, transforms a guessing game
    into a more targeted attack. The AI’s tendency toward helpful, specific error
    messages inadvertently aids malicious actors unless you explicitly instruct it
    to maintain appropriately vague responses for security-sensitive operations.
  prefs: []
  type: TYPE_NORMAL
- en: While it isn’t shown in the code, no rate limiting or other protections are
    included here.
  prefs: []
  type: TYPE_NORMAL
- en: 'A secure implementation would incorporate several critical safeguards:'
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, passwords should never be stored in plain text. Instead,
    the system should store cryptographically hashed passwords and use a secure comparison
    function to verify login attempts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This comparison process must employ constant-time algorithms to prevent timing
    attacks, where attackers analyze response times to deduce information about password
    correctness. Many security-focused libraries provide these constant-time comparison
    functions specifically to address this vulnerability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, the authentication system should implement rate limiting or throttling
    mechanisms to prevent brute-force attacks. Failed login attempts should be logged
    for security monitoring, allowing administrators to detect and respond to suspicious
    patterns. These measures work together to create a defense-in-depth approach that
    protects user credentials even if one security layer is compromised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can ask AI to help fix this:'
  prefs: []
  type: TYPE_NORMAL
- en: Improve the login route to use bcrypt to hash and compare passwords, and ensure
    the password in the database is hashed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It might then output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is better: it uses `bcrypt` to compare against a hashed password (assuming
    the variable `user.passwordHash` stores that). When creating users, you’d also
    want to make sure to use `bcrypt.hash` to hash their passwords.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a bit of guidance, the AI can do the right thing, but its initial naive
    output might well be insecure. This underscores the pattern: *review and refine*.'
  prefs: []
  type: TYPE_NORMAL
- en: Package Management Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common vulnerability category is package management. AI sometimes invents
    a library or misremembers a name, a problem known as *package hallucination*.
    Such a package might not exist, but an attacker could, theoretically, publish
    packages under commonly hallucinated names that contain malicious code. If you
    install such a package without confirming that it both exists and is the correct
    package, you could be introducing serious risk. If you’re not sure about a particular
    package, try a quick web search or check npm/PyPI directly.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the AI might inadvertently produce code that is identical to a
    licensed snippet from training data. This is more an intellectual property concern
    than a security issue, but it warrants careful attention. GitHub Copilot, for
    instance, includes a duplicate detection feature that can flag when generated
    code closely matches public repositories, helping developers avoid potential licensing
    conflicts. Similar tools are emerging to address this specific challenge of AI-generated
    code provenance. [Chapter 9](ch09.html#ch09_the_ethical_implications_of_vibe_coding_1752630044848930)
    will delve into licensing and intellectual property considerations in more detail,
    providing comprehensive guidance on navigating these complex issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the main message remains—and yes, I realize I’ve emphasized this
    point throughout the book to the point where you could probably recite it in your
    sleep—that *AI output requires the same careful review you would apply to a junior
    developer’s code*. The repetition is intentional, because this principle underpins
    virtually every aspect of safe and effective AI-assisted development. Whether
    you’re prototyping, building backends, or implementing security features, this
    mental model provides the right balance of trust and verification to make AI a
    powerful ally rather than a risky shortcut. It can write a lot of code fast, but
    you need to instill security best practices into it and double-check for vulnerabilities.
    Novelist Frank Herbert put it this way in an [often-quoted](https://oreil.ly/yr2B_)
    line from *God Emperor of Dune* (Putnam, 1981): “They increase the number of things
    we can do without thinking. Things we do without thinking—there’s the real danger.”'
  prefs: []
  type: TYPE_NORMAL
- en: Using AI can lull you into doing less thinking about routine code, and you should
    be consciously thinking about how to apply a security-review mindset. It’s crucial
    for catching those “things we can do without thinking.”
  prefs: []
  type: TYPE_NORMAL
- en: Security Audits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the types of vulnerabilities outlined, how can you effectively audit and
    secure our AI-generated code? This section looks at several techniques and tools
    you can employ.
  prefs: []
  type: TYPE_NORMAL
- en: Leverage Automated Security Scanners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Static analysis tools (SASTs) can scan your code for known vulnerability patterns;
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ESLint + security plug-ins](https://oreil.ly/55ppH) can detect insecure functions
    or unsanitized input in JavaScript and Node code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bandit](https://bandit.readthedocs.io) for Python can flag uses of assert
    in production, weak cryptography, hard-coded secrets, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GitHub CodeQL](https://github.com/github/codeql) lets you run queries across
    your codebase to find SQL injection, XSS, and other common patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semgrep](https://semgrep.dev) has rules for many languages, including community-maintained
    ones for JavaScript, Python, Java, Go, and more, and can spot top issues out of
    the box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can integrate these tools into your CI/CD or dev pipelines. Run them on
    your AI-generated code—it won’t catch everything, but it will probably flag the
    obvious mistakes (e.g., plain-text password checks, unsanitized SQL, insecure
    crypto). It’s a solid safety net.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Separate AI as a Reviewer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two distinct approaches can leverage AI for security review of generated code,
    each with unique advantages. The first involves using the same AI model that generated
    the code, asking it to switch perspectives and audit its own output. After generating
    code, you can prompt the model with something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Review this code for security vulnerabilities and explain any issues you find.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This approach often yields surprisingly effective results, as the model can
    identify common security problems such as plain-text password storage, missing
    input validation, or potential SQL injection vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach employs a different AI model as an independent reviewer.
    For instance, if you generated code using ChatGPT, you might paste that code into
    Claude or Gemini for security analysis. This cross-model review can surface different
    perspectives and catch issues the original model might have overlooked, much like
    how different security tools or human reviewers bring varying expertise and focus
    areas. Different models may have been trained with different emphases or datasets,
    potentially catching distinct categories of vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Both techniques serve as valuable additional layers of security review, complementing
    but never replacing proper security testing and human expertise. While AI reviewers
    may occasionally flag false positives or miss subtle vulnerabilities, they excel
    at catching common security antipatterns quickly. Think of this process as automated
    pair programming focused specifically on security considerations. The key lies
    in treating these AI-generated security reviews as another input to your security
    assessment process rather than as definitive security clearance.
  prefs: []
  type: TYPE_NORMAL
- en: Perform a Human Code Review with a Security Checklist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you’re in a team, have a checklist for reviewing code with an eye to security.
    AI often produces code that “works” for the expected case but isn’t hardened to
    deal with malicious cases. For AI-generated code, be sure to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Authentication flows: Are they solid?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any place data enters the system: Are we validating inputs?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any place data leaves the system: Are we sanitizing outputs? Are we protecting
    sensitive data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use of external APIs: Are we handling failures? Are we exposing keys?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Database access: Are we using ORMs safely? Are we using parameterized queries?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory management in low-level code: If AI is writing C/C++ or Rust, are there
    overflows? Is there any misuse?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Penetration Testing and Fuzzing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use dynamic approaches. For fuzz testing, feed random or specially crafted inputs
    into your functions or endpoints to see if they break or do weird things. AI can
    help generate fuzz cases, or you can use [existing fuzz tools](https://oreil.ly/OoFzT),
    such as [OSS Fuzz by Google](https://oreil.ly/FvKSU).
  prefs: []
  type: TYPE_NORMAL
- en: Running penetration-testing tools like OWASP’s ZAP against your AI-made web
    app can automate scanning for things like XSS and SQL injection vulnerabilities.
    For example, ZAP might attempt to inject a script and get it reflected, and detect
    that a certain input isn’t sanitized.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re building an API, tools like Postman or custom scripts can try sending
    ill-formed data to see how the system behaves: does it throw a 500 error or handle
    errors gracefully?'
  prefs: []
  type: TYPE_NORMAL
- en: Add Security-Focused Unit Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For critical pieces of code, write tests that assert security properties. For
    instance, you might test that your login rate limiter triggers after X bad attempts,
    or that certain inputs (like `"<script>alert(1)</script>"`) come out escaped in
    the response. To test that unauthorized users cannot access a protected resource,
    simulate both authorized and unauthorized calls and ensure the app behaves correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can ask the AI to help generate these tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Write tests to ensure an unauthorized user gets 403 on the /deleteUser endpoint.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And then run the tests.
  prefs: []
  type: TYPE_NORMAL
- en: Provide Updates to Compensate for Training Cutoffs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI models possess a fundamental limitation that directly impacts security:
    their knowledge freezes at a specific point in time. When a model completes training,
    it cannot learn about vulnerabilities discovered afterward, security patches released
    subsequently, or new best practices that emerge. This knowledge cutoff creates
    a critical gap between what the AI knows and current security standards.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a model trained in 2023 generating code in 2025\. During those intervening
    years, numerous security vulnerabilities have been discovered, patched, and documented.
    New attack vectors have emerged, frameworks have added security features, and
    best practices have evolved. The AI, however, remains unaware of these developments
    unless you explicitly provide updated information within your prompts.
  prefs: []
  type: TYPE_NORMAL
- en: This limitation becomes particularly acute with rapidly evolving security standards
    and vulnerability databases. The [OWASP Top 10](https://oreil.ly/US-uh), for instance,
    undergoes periodic updates to reflect the changing threat landscape. If you prompt
    an AI to “write a secure file upload function,” it might implement reasonable
    protections based on its training data—perhaps including file type validation,
    size limits, and storage outside the web root. However, it could miss recently
    discovered attack vectors or fail to implement newly recommended mitigations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution involves actively supplementing the AI’s knowledge with current
    security information. When requesting security-sensitive code, include references
    to current best practices in your prompts. For example, rather than simply asking
    for secure code, you might prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a file upload function that addresses the security concerns in the 2025
    OWASP Top 10, particularly focusing on injection attacks and server-side request
    forgery.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This approach grounds the AI’s response in current security standards rather
    than potentially outdated training data.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, framework-specific security features often emerge after an AI’s training
    cutoff. Express.js applications, for instance, benefit significantly from the
    [Helmet middleware](https://oreil.ly/WSPar) for setting security headers. An AI
    trained before Helmet became standard practice might generate Express applications
    without this crucial security layer. By explicitly mentioning current security
    tools and practices in your prompts, you help the AI generate code that aligns
    with contemporary security standards rather than historical ones.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize Your Logging Practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that the code (AI and human) has good logging, especially around critical
    operations or potential failure points. This helps in debugging issues in production.
    If an AI wrote a section with minimal logs, consider adding more. For example,
    if there’s an AI-generated catch block that just swallows an error, change it
    to log the error (and maybe some context) for visibility. Also, sanitize the logs
    so they contain no sensitive info.
  prefs: []
  type: TYPE_NORMAL
- en: Use Updated Models or Tools with a Security Focus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some AI coding tools aim to blend code generation with built-in security scanning.
    Snyk is a prime example: it uses a [hybrid approach](https://oreil.ly/0ZGFv) combining
    LLM-generated suggestions with rule-based taint analysis. According to Snyk, when
    you request code (even from LLM libraries like OpenAI, Anthropic, or Hugging Face),
    Snyk Code tracks potentially unsafe data flows and flags untrusted inputs before
    they reach sensitive sinks. In practice, that means if an AI suggests a database
    query, Snyk ensures it’s parameterized, preventing SQL injection—even if you forget
    to do so yourself. This kind of tool is particularly useful because it works to
    avoid introducing insecure code through AI-generated suggestions.'
  prefs: []
  type: TYPE_NORMAL
- en: Pay Attention to Warnings in Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re using an IDE, often you’ll see warnings or squiggly lines to highlight
    suspicious code. Modern IDEs with IntelliSense can sometimes catch, for instance,
    a string concatenation of SQL that looks suspicious. Don’t ignore those warnings
    and flags just because the AI writes them—address the issue. The AI doesn’t have
    the benefit of those real-time warnings when generating the code.
  prefs: []
  type: TYPE_NORMAL
- en: Slow Down
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After using AI to generate a lot of code quickly, shift gears and *slow down*
    when it’s time for auditing. When you can produce features fast, it’s tempting
    to chase the next one, but schedule time for a thorough review. Think of it as
    “AI-accelerated development, human-accelerated security.” Snyk’s [best practices](https://oreil.ly/uUExW)
    recommend scanning AI code right in the IDE, and caution against letting AI’s
    speed outpace your security checks. In other words, integrate security scanning
    into your dev loop, so you can catch vulnerabilities as soon as the code is written.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, when you audit AI-generated code, you’ll use many of the same tools
    you use in traditional development—static analysis, dynamic testing, code review—but
    you might apply them more frequently, because code is produced more quickly. *Treat
    every AI output as needing inspection*.
  prefs: []
  type: TYPE_NORMAL
- en: Building Effective Testing Frameworks for AI-Generated Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While security forms one pillar of reliability, the broader concept encompasses
    the fundamental dependability of your software system. *Reliability*, in software
    architecture terms, addresses critical questions about system failure and its
    consequences. Does your system need to be fail-safe? Is it mission critical in
    ways that could affect human lives or safety? If the system fails, will it result
    in significant financial losses for your organization? These considerations determine
    the rigor required in your development and testing practices.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re building with AI assistance, these reliability stakes remain unchanged.
    A banking application generated with AI assistance carries the same requirements
    for transaction accuracy and data integrity as one written entirely by humans.
    A healthcare system must meet identical standards for patient safety regardless
    of how its code originated. The AI’s involvement in code generation does not diminish
    these fundamental reliability requirements.
  prefs: []
  type: TYPE_NORMAL
- en: This reality underscores why comprehensive testing becomes even more critical
    in AI-assisted development. A strong testing framework ensures that your code
    performs its intended functions correctly and maintains that correctness as the
    project evolves. While testing AI-generated code follows the same fundamental
    principles as testing human-written code, certain nuances and opportunities emerge
    from the AI development process that warrant specific attention.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections explore how to leverage AI not just in generating code
    but in creating robust test suites that validate reliability, maintain system
    stability, and provide confidence that your software will perform correctly when
    the stakes are highest.
  prefs: []
  type: TYPE_NORMAL
- en: First, embrace automated testing early and often. It’s easy to skip writing
    tests when development is slow because you want to push features. Ironically,
    when development is *fast* (with AI), it’s *also* easy to skip tests, because
    new features keep coming at you. But when code is churned out rapidly, that’s
    precisely when you most need tests to catch regression or integration issues.
    So after implementing a feature with AI help, get into the habit of immediately
    writing tests for it (or even using AI to write those tests). This verifies the
    feature and also guards it as you change things later.
  prefs: []
  type: TYPE_NORMAL
- en: A [2022 study](https://oreil.ly/Vc8Gd) found that developers who were using
    an AI assistant were *more confident* in the security of the code they wrote even
    when it was objectively less secure than code written by those without AI assistance.
    You need to counteract that overconfidence with actual tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'As I noted in [Chapter 4](ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362),
    you can use the AI not just to generate the code but also to produce a suite of
    tests. This way, AI helps double-check itself. It’s like having it do both the
    implementation and an initial pass at validation. For example, after writing a
    new module, you could ask:'
  prefs: []
  type: TYPE_NORMAL
- en: Write unit tests for this module, covering edge cases.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If they pass, great. If they fail, either there’s a bug or the tests expected
    something else. Investigate and fix either code or test as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Be cautious that the AI may assume some output or behavior incorrectly; treat
    its tests, like its code, as suggestions, not the ground truth. You might need
    to adjust the test’s expectations to match the intended behavior—but even that
    process is valuable, because it forces you to define the intended behavior clearly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incorporate your test suite into a CI pipeline that runs on every commit. This
    way, whenever AI-generated code is added or changed, all tests run automatically.
    If something breaks, you’ll catch it early. Sometimes AI might introduce subtle
    breaking changes (like changing a function signature or output format slightly),
    and a robust test suite will detect that. Include security scans in the CI too
    (like `npm audit` or static analysis) so that any new introduction of a risky
    pattern is flagged. Types of tests to try include:'
  prefs: []
  type: TYPE_NORMAL
- en: Property-based testing and fuzzing
  prefs: []
  type: TYPE_NORMAL
- en: '*Property-based testing* (with tools like [Hypothesis for Python](https://oreil.ly/JcYBf)
    or [fast-check for JavaScript](https://fast-check.dev)) is another valuable technique.
    Instead of writing individual test cases with specific inputs and expected outputs,
    you define high-level properties that your code should always satisfy. The framework
    then generates a wide range of inputs to check whether those properties hold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take sorting as an example. Rather than asserting that `sort([3, 1, 2]) ===
    [1, 2, 3]`, you can define properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The output should be in order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should contain the same elements as the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tool then generates dozens or hundreds of input arrays to test those conditions—and
    finds edge cases you might not think of manually.
  prefs: []
  type: TYPE_NORMAL
- en: This can be especially useful for AI-generated code. If your AI writes a function
    to normalize email addresses (such as by lowercasing the domain), a property test
    might check that the output is *idempotent*—meaning running the function twice
    gives the same result as running it once. If an edge case violates that invariant,
    the test framework will generate a counterexample to help you diagnose the bug.
  prefs: []
  type: TYPE_NORMAL
- en: Load and performance testing
  prefs: []
  type: TYPE_NORMAL
- en: AI might write code that’s not optimized. It’s a good idea to test your system
    under load. This is reliability in terms of performance. Use tools like JMeter,
    Locust, or k6 to simulate many requests or heavy data and see if the system holds
    up. If not, identify the bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, maybe the AI writes a naive `O(n^2)` algorithm that works fine
    on 100 items but will tank at 10,000\. Without performance tests, you might not
    notice that until it’s in production. So incorporate some performance scenarios,
    if applicable. Time some critical operations with increasing input sizes, or use
    profiling tools to see where CPU time or memory goes for heavy tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs: []
  type: TYPE_NORMAL
- en: 'Intentionally cause errors to ensure the system responds gracefully, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: For an API, shut down the database and see if the API returns a friendly error
    or crashes. If it crashes, add code (or ask AI to add code) to handle DB connection
    errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the frontend, simulate the backend returning 500 errors and ensure the UI
    shows an error message, not a blank page or infinite spinner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI might not think of these failure modes on its own when writing code, so you
    have to test them and then refine. Testing these scenarios will improve reliability
    by prompting you to add proper fallback logic, retries, or user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate logging and perhaps use the logs in tests for verification. For
    instance, if a certain action should trigger an audit log entry, test for that.
    AI can generate log lines; verify they print out as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Also, think about setting up monitoring (like an in-memory simulation of how
    your service will be monitored in production). For example, you might track if
    any uncaught exceptions are logged during test runs. If yes, treat it as a test
    failure; that means there’s some case not properly handled.
  prefs: []
  type: TYPE_NORMAL
- en: Maintainability
  prefs: []
  type: TYPE_NORMAL
- en: Maintainability testing, like ensuring code style and standards, is important.
    Use linters and formatters to keep code consistent, since AI can produce slightly
    different styles from different prompts. A formatting tool like [Prettier](https://prettier.io)
    or [Black (for Python)](https://pypi.org/project/black) can unify style. For more
    logical consistency and to catch overly complex AI-generated code that might need
    refactoring, consider adding linting rules that enforce things like function complexity
    limits. (See [“Ensuring Maintainability in AI-Accelerated Codebases”](#ch08_ensuring_maintainability_in_ai_accelerated_codebas_1752630044622595)
    for more.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your tests are in place, you can refactor AI code more confidently. Perhaps
    the AI produces a working but clunky solution; you can improve it and rely on
    tests to ensure you haven’t broken its behavior. You might even ask AI to refactor
    its own code:'
  prefs: []
  type: TYPE_NORMAL
- en: Refactor this function for clarity while keeping it passing the current tests.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If your tests are good, you can check that the refactoring didn’t break anything.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding nondeterminism in AI systems requires distinguishing between two
    fundamentally different scenarios. When AI operates at runtime in production systems,
    such as a chatbot responding to customer queries or a recommendation engine personalizing
    content, the outputs can vary even with identical inputs. This variability stems
    from factors like model temperature settings, random seeds, or evolving model
    states. Testing such systems requires specialized approaches that account for
    acceptable variation ranges rather than expecting exact matches.
  prefs: []
  type: TYPE_NORMAL
- en: However, AI-assisted code generation presents a different paradigm entirely.
    Once an AI generates code and that code is committed to your repository, it becomes
    as deterministic as any human-written code. The function that calculates tax rates
    will produce the same output for the same input every time, regardless of whether
    a human or AI originally wrote it. This determinism is crucial for system reliability
    and makes traditional testing approaches entirely applicable to AI-generated code.
  prefs: []
  type: TYPE_NORMAL
- en: The more subtle challenge emerges when integrating multiple AI-generated components,
    each potentially created in isolation with different implicit assumptions. Consider
    a concrete example from an ecommerce system. You might prompt an AI to generate
    an order processing module, instructing it to handle international orders. Separately,
    you ask the AI to create a shipping calculation service for the same system. The
    order processing module, following American conventions, formats dates as “12/25/2024”
    for December 25\. Meanwhile, the shipping service, perhaps influenced by European
    examples in its generation, expects dates formatted as “25/12/2024.” Both components
    function perfectly in isolation, passing their individual unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: The mismatch only surfaces during integration testing when the order processor
    passes a date to the shipping calculator. The shipping service interprets “12/01/2024”
    as January 12 rather than December 1, potentially calculating shipping times based
    on the wrong month entirely. This type of assumption mismatch is particularly
    common with AI-generated components because the AI might draw from different examples
    or conventions when generating each piece independently. Comprehensive integration
    testing that exercises the actual data flow between components becomes essential
    for catching these subtle incompatibilities before they cause production failures.
  prefs: []
  type: TYPE_NORMAL
- en: The QA process for AI-assisted projects might require a bit more creativity,
    since AI can introduce unusual edge cases. For instance, an AI might output a
    feature you didn’t explicitly consider—if so, test that as well. If it added a
    hidden behavior, either remove it or properly test it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if possible, test your application in an environment similar to production,
    with a realistic data load. Sometimes performance issues only appear with larger
    data volumes or higher concurrency. Use those test results to pinpoint inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the AI often writes correct code, it may not always write *optimal* code.
    LLMs don’t inherently do performance analysis; they typically reproduce what is
    common in their training data. Therefore, be vigilant about potential performance
    issues, especially in critical paths or for large-scale use.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even chat with the AI for hints about performance optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the complexity of this code? Can it be improved?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function is slow—any ideas on how to make it faster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might not always be right, but it can sometimes give useful suggestions or
    at least confirm your thinking.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, don’t overoptimize, and don’t optimize prematurely or where it’s
    not needed. Sometimes the AI solution is perfectly fine, if the data sizes are
    small or the operation infrequent. Use your profiling data to focus on real bottlenecks
    and optimize the parts that really need it. The advantage of vibe coding is that
    you haven’t spent a ton of time handcrafting code from scratch, so you can afford
    to let some noncritical parts be simple and not superoptimized, as long as they
    don’t impact user experience or cost. This approach aligns with agile practices:
    make it work, then make it fast (if needed).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some areas to cover as you ensure your AI-augmented project runs efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: Complexity analysis
  prefs: []
  type: TYPE_NORMAL
- en: 'When the AI generates an algorithm, take a moment to consider its complexity.
    Sometimes it will use a brute-force solution where a more efficient algorithm
    exists. For example, it might double-sort a list because it didn’t recall a single-step
    method, resulting in O(n log n × 2) where O(n log n) could do (the capital *O*
    stands for memory usage). Or it might use nested loops that make an operation
    O(n²) when there’s a known O(n) approach. If you spot something like that, ask
    for improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: Can we optimize this to avoid nested loops? Perhaps use a set for lookups.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The AI often will oblige and give a better solution if you hint at the approach.
    If not, you might have to implement that part manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'To identify slow functions, run a profiler or measure execution time of key
    code paths with representative or worst-case data. If something is too slow, you
    can attempt to optimize manually or with AI assistance:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimize this function, which is currently a bottleneck; try to reduce its complexity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The AI might restructure the code for performance. Use tests to make sure it
    still works.
  prefs: []
  type: TYPE_NORMAL
- en: For critical algorithms, write a small benchmark harness. If AI gives you a
    piece of code to, say, compute something, test it against another approach, or
    at least measure how it scales with input size. You might decide to rewrite in
    a more efficient way if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage, leaks, and retention
  prefs: []
  type: TYPE_NORMAL
- en: 'AI-generated solutions might use more memory than necessary: reading entire
    files into memory instead of streaming, for example, and thus holding large data
    structures. If your use case involves big data, check your system’s memory usage
    and optimize by streaming or chunking if needed. For instance, if you need to
    process millions of records, you’d want to refactor your AI-generated function
    `loadAllRecords()` to process them in batches or stream from the database.'
  prefs: []
  type: TYPE_NORMAL
- en: Also check that the AI-generated code is releasing resources. In languages like
    Java or C#, maybe it opens a file or DB connection and doesn’t close it. In a
    frontend single-page app, maybe event listeners aren’t removed, leading to leaks.
    Tools can help (like Chrome dev tools’ Memory Inspector for frontends or Valgrind
    for C++ leaks), but often just reading the code helps. Identify these and fix
    them. If you see an open file handle not closed, add a close in a `finally` block.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and parallelism
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using languages that support threads or async, look for places where
    the AI code might be single-threaded when it could be parallel. AI might not automatically
    use async/await where appropriate, and may not know to offload a heavy CPU task
    to a worker thread. Identify such opportunities. For example, for I/O-bound tasks
    in Node or Python, ensure asynchronous usage so that the system doesn’t block.
    For CPU-bound tasks, maybe the AI can’t help much in code, but you might decide
    to implement in a more performant language or offload to a background job.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs: []
  type: TYPE_NORMAL
- en: 'A common performance optimization that AI doesn’t always automatically add
    is to cache results of expensive operations. Look at your code: is it recalculating
    something repeatedly? If so, implement caching (either in-memory or using an external
    cache like Redis). You can prompt AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Add caching to this function to avoid redundant calculations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It may implement a simple memorization or suggest using a caching library.
  prefs: []
  type: TYPE_NORMAL
- en: Database query optimization
  prefs: []
  type: TYPE_NORMAL
- en: If your application uses a database, examine the queries the AI creates. Are
    they using indexes properly? Perhaps the AI wrote `SELECT *` where only a few
    columns are needed. Or it’s fetching extensive data to filter in code, creating
    performance bottlenecks like the N + 1 query problem. These inefficiencies require
    optimization by pushing more work to the database or leveraging proper indexing.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if the generated code calls `findOne` repeatedly within a loop,
    resulting in multiple database round trips, you can refactor this into a single
    batch query using `WHERE id IN (...)`. Similarly, if the AI omitted index creation
    in a migration for frequently queried fields, adding those indexes becomes essential
    for maintaining acceptable performance. The AI often generates functionally correct
    but suboptimal database interactions that require human expertise to identify
    and resolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let’s take an example. Suppose AI writes you a function that
    merges two sorted arrays by simply concatenating and sorting the result: (O(n
    log n))—even though there’s a known linear algorithm it could be using to merge
    two sorted lists (like merge step or merge sort, O(n)). In code review, you realize
    this could be a bottleneck for large arrays, so you prompt AI to implement the
    linear merge:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimize the mergeSortedArrays function to perform the merge in linear time
    without using built-in sort.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The AI recognizes this as the classic merge algorithm and writes it. The solution
    passes your tests, so congratulations: you gained performance without sacrificing
    correctness.'
  prefs: []
  type: TYPE_NORMAL
- en: AI-assisted development doesn’t remove the need for performance tuning; it just
    shifts *when* you do that tuning. You’ll often get a correct solution first (which
    is extremely valuable), then turn your attention to measuring and optimizing targeted
    parts. When you do need to optimize something, the AI can help, as long as you
    guide it on what you need.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring Maintainability in AI-Accelerated Codebases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A codebase’s *maintainability* describes how easy it is to modify, extend, and
    comprehend over time. Some worry that AI-generated code could be messy or inconsistent,
    especially if multiple suggestions have varying styles or patterns. This section
    covers several practices you can use to address these concerns and keep your vibe-coded
    project clean and maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: While Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you prepare your prompts, a few things to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Use consistent coding standards
  prefs: []
  type: TYPE_NORMAL
- en: Use linters and formatters to enforce a consistent style. As mentioned, AI might
    sometimes use different naming conventions or formatting in different outputs.
    Running a formatter (like Prettier for JS, Black for Python, gofmt for Go, etc.)
    on all code after generation ensures it conforms to a unified style. This makes
    reading code much easier (no cognitive load switching styles). Additionally, define
    naming conventions for your project and stick to them. If the AI outputs `get_user_data`
    in one place and `fetchUserData` in another, decide which convention you prefer
    (`snake_case` versus `camelCase`, etc.) and refactor to one style.
  prefs: []
  type: TYPE_NORMAL
- en: Use architectural patterns to encourage modularity and avoid sprawl
  prefs: []
  type: TYPE_NORMAL
- en: 'Encourage the AI to write modular code by prompting it to separate concerns.
    For example, instead of asking it to write one huge file implementing everything,
    break the work into tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a UserService class for user logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a separate module for sending emails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This leads to a codebase that’s logically divided. It’s easier to maintain
    when each module has a clear responsibility. You can guide the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Put database access code in a separate file or class from the API routing code.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Because it’s so very easy to add features when using AI, it’s crucial to guard
    against feature creep and code sprawl. Without disciplined architectural thinking,
    you risk your codebase devolving into what software architects call a *big ball
    of mud*: an antipattern where code lacks clear structure or boundaries. This risk
    intensifies with AI assistance, as the friction traditionally associated with
    adding features disappears, potentially accelerating architectural decay.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To combat this, ground your AI-assisted development in proven architectural
    patterns and principles. When instructing AI, explicitly reference the patterns
    your project follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add this new feature following the repository/service pattern used in the project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement this using the hexagonal architecture established in our domain layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This specificity helps maintain consistency even as features accumulate rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For developers seeking deeper architectural grounding, several foundational
    texts provide essential guidance:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Design Patterns: Elements of Reusable Object-Oriented Software* (Addison-Wesley,
    1994) by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (the “Gang
    of Four”) remains the definitive catalog of reusable design solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Fundamentals of Software Architecture: An Engineering Approach*](https://learning.oreilly.com/library/view/fundamentals-of-software/9781098175504/)
    by Mark Richards and Neal Ford offers comprehensive coverage of architectural
    patterns and principles across technology stacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Domain-Driven Design: Tackling Complexity in the Heart of Software* by Eric
    Evans (Addison-Wesley, 2003) provides crucial techniques for aligning software
    design with business domains—particularly valuable when AI generates code that
    must reflect complex business logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These resources equip you to guide AI tools effectively, ensuring generated
    code adheres to sound architectural principles rather than contributing to technical
    debt. Remember: AI excels at implementing patterns but cannot determine which
    patterns are appropriate for your specific context. That architectural judgment
    remains fundamentally human.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with Code Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the AI responds with generated code, maintainability techniques to use
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Refactor continuously
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t hesitate to refactor AI-generated code when needed. Sometimes the first
    pass is correct but not ideally structured: for example, the AI might write a
    very long function or duplicate its logic in two places. A common challenge is
    unintentionally duplicated code: the AI might not realize two functions do similar
    things and create both. If you notice similar blocks, refactor to one. Tools like
    code linters can detect duplicates (there are linters for too-similar code). Running
    those could highlight places to “DRY out” (don’t repeat yourself).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ask the AI to help refactor, you could prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Refactor this code to remove duplication and improve clarity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It might create helper functions or simplify some logic. Always test after refactoring.
  prefs: []
  type: TYPE_NORMAL
- en: Test
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has already covered testing, so I’ll just note that a good test
    suite makes maintenance easier. When you or others modify code in the future (possibly
    with AI again), your tests will catch if the changes break anything, so you can
    refactor or change implementations with peace of mind. Testing decouples “what
    it does” from “how it does it,” giving you flexibility to maintain or improve
    “how” without altering “what.”
  prefs: []
  type: TYPE_NORMAL
- en: Avoid excessive complexity or overrelying on AI-specific constructs
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the AI might use a clever trick or less common function that other
    developers might not know. While that’s not inherently bad, consider maintainability:
    if an average developer would scratch their head at the code, maybe simplify it.
    For instance, if AI uses a bit of regex magic or list comprehension that’s too
    terse, rewrite it in a more explicit loop for clarity (or at least comment it).'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, an AI trying to be helpful might overengineer a solution, like adding
    layers that aren’t needed. For instance, maybe a direct approach was fine, but
    the AI introduced an abstraction that isn’t pulling its weight. Remove it to keep
    things straightforward. Simpler code is usually easier to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Build in resilience and fallbacks
  prefs: []
  type: TYPE_NORMAL
- en: Think about fallback strategies in case of failures. For example, if an AI-coded
    component calls an external API and that API is down or returns unexpected data,
    do we have a fallback (like using cached data or a default response)? Implementing
    such resilience patterns (circuit breakers, retries with backoff, etc.) can make
    the system more robust. The AI likely won’t do this on its own unless asked. Ensure
    the system can handle partial failures gracefully. One microservice going down
    shouldn’t take the whole app down, if possible. Use timeouts and fallback logic.
  prefs: []
  type: TYPE_NORMAL
- en: Follow-Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you’re satisfied with the code, a few more practices help to keep it maintainable:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide thorough documentation and comments
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure the code is properly documented. AI often writes minimal comments
    unless prompted. You can request docstrings or comments with prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: Add comments to explain the purpose of each section in this code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a docstring for this function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These can save future readers time. The AI can usually generate fairly good
    explanations but sometimes misexplains subtle points, so review for accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Also consider maintaining a high-level documentation (like a README or design
    doc) for the project, describing its architecture, main components, and so on.
    You can largely write this yourself, but AI can help by summarizing the codebase
    if needed.
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter some quirk like “The AI always names this parameter weirdly,”
    mention it in your dev notes for others. It’s part of the new collaborative environment.
    If it’s just you using the AI-generated code, a few quirks are fine—but if others
    join the project, they might wonder, “Why is this thing named like that?” Perhaps
    just standardize those names.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s also an aspect of maintainability in terms of knowing which pieces
    of code were AI-generated and which were human-written. It’s not strictly necessary
    to label, but some teams might comment, “Generated with the help of GPT-4 on 2025-05-01”
    for traceability. Ideally, flag anything you’re unsure about in your PR description:
    “Used ChatGPT to help with this function; it seems to work, but please check the
    error-handling logic carefully.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'This isn’t a widespread practice. It can be helpful during code review, but
    you might not need it if a human has already reviewed the code and it’s now just
    code. If you do keep any transcripts or prompts, you could link them in comments
    for complicated code: “This algorithm derived via GPT-4, based on prompt X; see
    docs for derivation.” A reviewer doesn’t need to treat it differently in terms
    of scrutiny (you should scrutinize all code), but it can help to understand the
    context. For example, if code has a certain style mismatch or an odd idiom, knowing
    it came from AI might clue the reviewer in that this isn’t a deliberate authorial
    choice but an AI artifact.'
  prefs: []
  type: TYPE_NORMAL
- en: Code reviews and team norms
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working in a team, have all team members review code—even if one person
    and AI cowrote it. They might spot awkward patterns or things that break team
    norms. Over time, you’ll develop a sense of how to prompt the AI to match your
    team’s style (maybe including specifics in system prompts or initial guidelines).
    If multiple developers use AI, make sure everyone knows the desired style patterns
    so they can prompt accordingly (like “Write this in functional style” or “Use
    async/await, not callbacks”). See the next section for some tips on code review
    with AI code.
  prefs: []
  type: TYPE_NORMAL
- en: Track technical debt
  prefs: []
  type: TYPE_NORMAL
- en: 'If, during development, you accept an AI solution that you know isn’t ideal,
    track it as technical debt in your comments or the project to-dos: “TODO: The
    solution works but is O(n²); if data grows, optimize this,” or “TODO: This uses
    a global variable for simplicity; refine this later.” The AI can even insert TODO
    comments itself if you ask:'
  prefs: []
  type: TYPE_NORMAL
- en: If there are any areas that need future improvement, add to-do comments.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Just address those to-dos eventually.
  prefs: []
  type: TYPE_NORMAL
- en: Learn from AI patterns
  prefs: []
  type: TYPE_NORMAL
- en: If AI introduces a design pattern or library you’re not familiar with, take
    time to learn more about it rather than ignoring it. Understanding a particular
    caching approach or a library it uses will help you maintain or modify that part
    confidently in the future. If it’s too arcane, you might decide to remove it in
    favor of something you know—but sometimes AI can pleasantly surprise you with
    a useful library or pattern you didn’t know. If it’s a well-known solution that
    you and the team can learn, this can even improve maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, maintainability comes down to applying the same good software-engineering
    principles as always—just applying them to code that was partially written by
    AI. Fortunately, because AI reduces the grunt work, you may have more time to
    focus on cleaning up the code and writing docs, which *improves* maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: Some companies [report](https://oreil.ly/2lrTW) that after an initial burst
    of generating code with AI, they invest time in a “hardening sprint” to refactor
    and document it all. Consider alternating between generation-heavy sprints and
    cleanup sprints as a potential strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Code Review Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 4](ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362),
    code review is a critical process in traditional development and remains so in
    AI-assisted development. This section discusses some nuances to consider when
    a chunk of the code under review is machine-suggested. Because AI can produce
    code so quickly, it’s reasonable to worry that code review will become a bottleneck—but
    don’t let that worry hamper the review process. It’s crucial to allocate proper
    time for reviews. Don’t skimp on the assumption that “we wrote it fast, let’s
    merge fast.” If anything, commit smaller changes more frequently to make reviews
    easier (generally a good practice anyway). Frequent, smaller pull requests (PRs)
    are easier to review thoroughly than one giant PR. The AI can help break tasks
    into smaller PRs as well, if you plan accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t assume code is correct just because “the AI wrote it and the tests pass.”
    Think critically and try to reason through the logic. If possible, test it mentally
    or with additional cases outside the provided tests, because tests might not cover
    everything. You can also run the code and even experiment by running a snippet
    with a tricky input to see if it behaves.
  prefs: []
  type: TYPE_NORMAL
- en: Code reviews can also be important learning moments. If the AI introduces a
    novel solution that is actually good, the reviewer might learn something new while
    verifying its correctness. Similarly, if the AI/human combination does something
    suboptimal, the reviewer can explain a better approach. Over time, this feedback
    loop can improve how the team uses AI (like helping everyone understand which
    things to avoid or ask differently). In a sense, code review helps to close the
    human learning loop, since the human author should learn and understand anything
    the AI wrote that is new to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you review code, your first priority should be making sure it meets the
    requirements and intended design. Does this code do what the feature/bugfix is
    supposed to? Does it cover any edge cases mentioned in the specifications? If
    the prompt is off, AI might solve a slightly different problem: maybe it handles
    a case that wasn’t needed or misses a case. This is normal, but watch that the
    developer didn’t just accept AI output that only partially addresses the issue.
    For example, an AI might produce code to format a date but assume a certain time
    zone, which might or might not align with requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: If something in the code isn’t obvious, ask the author to explain how it works
    or why it’s done that way. If they struggle to explain or reach for “the AI did
    it and I assume it’s right,” that’s a red flag. The team should understand everything
    in the codebase. Encourage the author to double-check with the AI or documentation
    and provide a proper explanation, possibly as a comment in code.
  prefs: []
  type: TYPE_NORMAL
- en: Pay attention to the security and performance vulnerabilities discussed earlier
    in this chapter, too, and if any known best practice is violated, call it out—like
    if output isn’t escaped (in web dev) or if you find credentials in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Request changes or refactoring if you see code that works but could be simpler
    or more in line with team style:'
  prefs: []
  type: TYPE_NORMAL
- en: The AI created 3 separate functions for different user roles that mostly duplicate
    each other. Can we merge these into one function with a parameter for role?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The code’s author can then do so (maybe with AI’s help). If the AI suggestion
    didn’t use the team’s consistent style or standard libraries, mention that too:'
  prefs: []
  type: TYPE_NORMAL
- en: We usually use the requests library for HTTP calls, but this code is using http.client.
    Let’s stick to requests for consistency.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The author can then prompt the AI to rewrite using the preferred library.
  prefs: []
  type: TYPE_NORMAL
- en: If the AI has written something really complex, like a tricky algorithm, consider
    discussing it with another reviewer or the team for a deeper review.
  prefs: []
  type: TYPE_NORMAL
- en: You may want to try some of the emerging tools that use AI to assist in code
    review—like GitHub’s Copilot for Pull Requests, which can generate summaries and
    flag potential bugs and other issues. Such a tool might highlight something like
    “This code snippet is similar to one in module X with slight differences” (pointing
    out possible duplication). These hints can complement the human review but should
    not replace it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, be respectful and constructive in your reviewing, even when the code
    has flaws due to AI. Avoid blaming the developer for what could be an AI artifact:
    while they are still responsible for their code, recognize the context. AI is
    a tool, and both author and reviewer are working with it. The goal is to improve
    the code and share knowledge, not point fingers. For example: “This part seems
    to have a security issue⁠—likely an oversight from the AI suggestion; let’s fix
    it.”'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, code review in vibe coding is how we fully exercise the *human intelligence*
    side of the human/AI partnership. It’s where oversight and expertise come in to
    catch what the AI might miss and to keep the quality bar high. It’s also a knowledge-sharing
    moment for the team, since discussing code in reviews spreads understanding of
    both the domain and how to best use AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code review also formalizes the concept of “developers as editors” [introduced
    by Grant Gross in *CIO*](https://oreil.ly/INPFV): the reviewer is an editor, making
    sure the code is polished and fit for production. This aligns perfectly with vibe
    coding as a concept, where the vibes (AI suggestions) are there but human judgment
    refines them.'
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Reliable Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you know your code is secure, tested, and maintainable, you need to deploy
    it and keep it running reliably in production.
  prefs: []
  type: TYPE_NORMAL
- en: While AI-assisted development doesn’t alter the core principles of software
    deployment, it does introduce considerations around deployment velocity and operational
    complexity. For those seeking comprehensive coverage of deployment fundamentals,
    *The DevOps Handbook* (IT Revolution Press, 2016), by Gene Kim, Jez Humble, Patrick
    Debois, John Willis, and Nicole Forsgren, provides the definitive guide, covering
    everything from continuous integration and deployment pipelines to monitoring,
    security, and organizational transformation. This foundational knowledge becomes
    even more critical when AI accelerates your ability to generate deployable code,
    as the principles ensure your deployment practices can scale with your increased
    development velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Before and During Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you ramp up to deployment, consider the following best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Automate your CI/CD pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Given the fast pace of AI development, a robust continuous integration/continuous
    deployment (CI/CD) pipeline is valuable. Every commit (with or without AI-generated
    code) should be built, tested, and potentially deployed through an automated pipeline.
    This reduces human error and confirms that all deployment steps (tests, lint,
    security scans) are consistently run. If AI code introduces something that breaks
    the build or fails the tests, the CI will catch it immediately. Also, an automated
    CI/CD pipeline allows for quick iteration, so you can patch any AI-introduced
    issues and deploy fixes rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code
  prefs: []
  type: TYPE_NORMAL
- en: 'Use infrastructure as code (Terraform, CloudFormation, etc.) to define your
    deployment environment. While not directly related to AI coding, it’s part of
    reliable deployments. You could even use AI to help write Terraform scripts, but
    treat those with the same caution and testing as other AI code, including perhaps
    testing them in a sandbox before applying them to production. A valuable starting
    point is the book [*Terraform: Up & Running*](https://learning.oreilly.com/library/view/terraform-up-and/9781098116736/)
    (O’Reilly, 2022), by Yevgeniy Brikman, which provides a comprehensive introduction
    to the principles and practices of IaC with Terraform.'
  prefs: []
  type: TYPE_NORMAL
- en: Use staged rollouts—and have a rollback plan
  prefs: []
  type: TYPE_NORMAL
- en: Use staged rollout strategies like deploying to a staging environment or a canary
    release before full production rollout. This way, you can catch anything you’ve
    overlooked before it affects all users. For example, you might deploy a new AI-coded
    feature to 5% of users and monitor (with metrics and logs) for any errors or performance
    issues. If all is good, roll it out to 100% of users.
  prefs: []
  type: TYPE_NORMAL
- en: Always have a rollback plan. Despite all tests and reviews, sometimes things
    slip through. If a new release goes wrong, be ready to revert to the last stable
    version. If you’re using a containerization strategy like Kubernetes, maintain
    previous deployments for quick switchback. If it’s a serverless function, keep
    the previous version alive until you’re confident in the new one.
  prefs: []
  type: TYPE_NORMAL
- en: Set up observability
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up comprehensive monitoring in production, of both system metrics and application
    logs:'
  prefs: []
  type: TYPE_NORMAL
- en: Use tools like Sentry to track errors and capture exceptions. If the AI code
    throws an unexpected error in production (perhaps an edge case wasn’t covered),
    you’ll get an alert so you can fix it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use performance-monitoring tools like application performance monitoring (APM)
    to track response times, throughput, and memory usage. This will show you if any
    code in the new deployment has introduced a slowdown or memory leak.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitor availability: for instance, ping the service endpoints to confirm they’re
    up. If something crashes (maybe due to some untested scenario), an alert should
    fire, so you can react quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay vigilant about security
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that secrets like API keys are handled properly in deployment. For
    example, if your AI wrote code that expects a secret in an environment variable,
    set up that secret in the CI/CD or cloud config, so it’s not accidentally logged
    or exposed. Use secret management tools like [HashiCorp Vault](https://oreil.ly/NqQ-T)
    (HashiCorp Vault offers secrets management, key management, and more with many
    integrations) or [AWS Secrets Manager](https://oreil.ly/LlYX-) (AWS Secrets Manager
    allows you to securely store and rotate secrets like database credentials, API
    keys, and tokens, and can integrate with CI/CD tools like GitHub). Also, if you’re
    using container images, scan them for vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Test using techniques like blue-green deployments or shadow testing
  prefs: []
  type: TYPE_NORMAL
- en: 'For major changes, consider a blue-green deploy. This involves setting up two
    identical production environments: “blue” (the current live version) and “green”
    (the new version). Traffic is initially directed to the blue environment. Once
    the green environment is ready and tested, traffic is switched over to it. If
    any issues arise with the green environment, traffic can be quickly rerouted back
    to the blue environment, minimizing downtime and risk. This method tests the new
    version in a full production setting before making it the sole live version.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if a specific AI-coded algorithm change is risky or you want
    to validate its behavior with real-world data without impacting users, you could
    shadow test it. This involves deploying the new version alongside the current
    live version. Real production inputs are fed to both versions in parallel. However,
    only the current version’s outputs are shown to users. The outputs from the new
    (shadow) version are collected and compared against the current version’s results
    to evaluate its performance, accuracy, and stability. If the shadow version’s
    results are satisfactory and performance is good, you can then confidently switch
    it to be the active version.
  prefs: []
  type: TYPE_NORMAL
- en: Ongoing Best Practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After deployment, these strategies can help keep everything running reliably:'
  prefs: []
  type: TYPE_NORMAL
- en: Create operational runbooks
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide runbooks for the ops team that describe any special aspects of the
    AI-generated parts of the code: “This service uses an AI model for X; if the model
    output seems erroneous, try restarting service or check the model’s version.”
    Or “Feature Y heavily uses caching to perform well; if performance issues arise,
    check the cache hit rate.” Essentially, document any operational considerations
    that might not be obvious. If AI has introduced a dependency (like using a temp
    file), note that, so ops will know to monitor disk space and the like.'
  prefs: []
  type: TYPE_NORMAL
- en: Test in production
  prefs: []
  type: TYPE_NORMAL
- en: In addition to testing during development and as part of the rollout, some companies
    do testing in production (TiP) in safe ways, like running continuous small experiments.
    For instance, you might use feature flags to turn on an AI-generated feature for
    a small subset of users and see if any error rates change. This overlaps with
    canary releases, but you can make it more granular using feature toggles.
  prefs: []
  type: TYPE_NORMAL
- en: Audit regularly
  prefs: []
  type: TYPE_NORMAL
- en: 'Schedule periodic security and performance audits of the codebase, especially
    as more AI contributions accumulate. This is similar to managing tech debt: it
    helps you catch things that were fine at first but that could turn problematic
    as the scale or context changes. Watch for “drift,” too—if AI code is generating
    SQL queries, make sure that your migrations and code stay in sync and that the
    deployment runs migrations properly before new code takes traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep humans in the loop
  prefs: []
  type: TYPE_NORMAL
- en: The theme continues—humans should monitor the automations. AI might help you
    write code, but it won’t fix a production incident at 2 a.m. Have someone on call
    who understands the system. Over time, you might enlist AI for troubleshooting
    help like analyzing logs (a feature of some emerging tools), but at the end of
    the day, a human should make decisions about fixes.
  prefs: []
  type: TYPE_NORMAL
- en: Learn from failures
  prefs: []
  type: TYPE_NORMAL
- en: No process is 100% perfect. If an error gets through your defenses and causes
    an incident, do a postmortem. Identify if the problem was related to AI usage
    (like “We trusted the AI code here and it failed under scenario X”), and update
    your processes and tests to prevent that class of issue. Doing this kind of analysis
    every time continuously improves reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability isn’t just about code, of course; it also involves the infrastructure
    and operations *around* the code. AI helps mostly on the code side. Robust operational
    practices (which can be partially assisted by AI) keep the overall system reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, treat an AI-heavy project the same as any high-quality software
    project when it comes to deployment: employ thorough testing, roll out gradually,
    monitor heavily, and make sure you can roll back quickly. Because AI can create
    changes faster, you may end up deploying more frequently (which is fine, if your
    CI/CD pipeline is good). [Frequent small deployments](https://oreil.ly/ATjYo)
    are actually [known to reduce risk](https://oreil.ly/Y5uDn) compared to infrequent
    big ones. The reason is that each individual change is smaller, making it easier
    to identify and fix any issues that arise. If a problem occurs, rolling back a
    small change is also simpler and faster. This approach contrasts with large, infrequent
    releases where numerous changes are bundled together, making it difficult to pinpoint
    the cause of any problems and increasing the potential impact of a failed deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: By following these best practices, you can be confident that even though a lot
    of its code was machine-generated, your system as a whole will behave reliably
    for users. The combination of automated testing, careful deployment, and monitoring
    closes the loop to catch anything that slipped through earlier stages. As a result,
    you can reap the speed and productivity benefits of AI development without sacrificing
    your ability to trust your software in production.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In summary, vibe coding does not remove the need for engineering rigor—it amplifies
    the productivity of the engineers who apply that rigor. Your mantra should be
    the old Russian proverb: Trust but verify. Trust the AI to handle the grunt work,
    but verify everything with your tools and expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: Security and reliability are one dimension of responsible development; ethics
    is another. AI-assisted coding raises important questions about intellectual property,
    bias, the impact on developer jobs, and more. [Chapter 9](ch09.html#ch09_the_ethical_implications_of_vibe_coding_1752630044848930)
    will delve into those broader implications. How can you use AI coding tools responsibly
    and fairly? How do you deal with licensing of AI-generated code and ensure your
    models and prompts are used ethically?
  prefs: []
  type: TYPE_NORMAL
