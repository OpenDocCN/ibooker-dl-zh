- en: 4 Crowdsourced relevance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Harnessing your users’ collective insights to improve the relevance of your
    search platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting and working with user behavioral signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using reflected intelligence to create self-tuning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an end-to-end signals boosting model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 1, we introduced the dimensions of user intent as content understanding,
    user understanding, and domain understanding. To create an optimal AI-powered
    search platform, we need to be able to combine each of these contexts to understand
    our users’ query intent. The question, though, is how do we derive these understandings?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can learn from many sources of information: documents, databases, internal
    knowledge graphs, user behavior, domain experts, and so on. Some organizations
    have teams that manually tag documents with topics or categories, and some even
    outsource these tasks using tools like Amazon Mechanical Turk, which allows them
    to crowdsource answers from people all around the world. For identifying malicious
    behavior or errors on websites, companies often allow their users to report problems
    and even suggest corrections. All of these are examples of crowdsourcing—relying
    upon input from many people to learn new information.'
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to search relevance, crowdsourcing can play a vital role, though
    it is usually important not to annoy your valued customers by constantly asking
    them for help. Fortunately, it is often possible to learn implicitly from your
    users, based on their behaviors. For example, to discover the most relevant documents
    for a query, we can examine logs to determine the documents most clicked on by
    other users when running that same search. Those clicks provide signals of which
    results are the most relevant for the query.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore how we can collect, analyze, and generate insights
    from these signals to crowdsource relevance. We’ll also cover the reflected intelligence
    process, introducing three key types of models for popularized relevance (signals
    boosting), for personalized relevance (collaborative filtering), and for generalized
    relevance (learning to rank). You’ll also index an e-commerce dataset and build
    your very first reflected intelligence model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Working with user signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every time a customer takes an action—such as issuing a query or purchasing
    a product—this provides a signal of that user’s intent. We can log and process
    these signals to learn insights about each user, different groups of users, or
    our entire user base.
  prefs: []
  type: TYPE_NORMAL
- en: This section introduces the power of using user signals with a sample e-commerce
    dataset we’ll use throughout the book, and it walks you through the mechanics
    of collecting, storing, and processing these signals.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Content vs. signals vs. models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When building search engines, two high-level sources of data affect search
    relevance: content and signals. Most content is in the form of documents, which
    can represent web pages, product listings, computer files, images, videos, facts,
    or any other type of searchable information. Content documents usually contain
    text or embedding fields that are used to search, along with other fields representing
    attributes related to the content (author, size, color, dates, and so on). The
    defining characteristic of the content documents is that they contain information
    users search on and ideally the answers to their queries.'
  prefs: []
  type: TYPE_NORMAL
- en: When a user sees content in response to a query, they may click on a result,
    add it to their shopping cart, or take some other actions. These actions are signals,
    and they’re key to providing insights into how users engage with the content.
    These signals can later be aggregated and used to build models to improve the
    relevance of your matching and ranking algorithms. The defining characteristic
    of signals is that they are user-supplied insights for demonstrating how users
    want to interact with content.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it can also be useful to rely on external data sources—or *models*—as
    part of the search experience. This can include querying a knowledge graph, referencing
    a list of entities, or invoking a large language model (LLM) or other foundation
    model that has been trained on external data sources. These external models can
    be used to better interpret user queries, reason about and understand the content,
    or even summarize or generate new content to return. While we can consider models
    as a third source of data for our search engine, they are trained on content and/or
    signals and thus serve as a derivative and refined representation of those two
    original data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we use three main sources of information to improve search: the
    attributes of the items (content), the observed user interactions with content
    (signals), and external models (which are derived from content and/or signals).'
  prefs: []
  type: TYPE_NORMAL
- en: For many tasks we undertake when building AI-powered search, we can derive similar
    outcomes using either content or signals, but they give us two different perspectives
    of relevance. In ideal cases, we can apply both perspectives to build an even
    smarter system, but it is useful to understand their strengths and weaknesses
    to best employ them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when trying to find a synonym for the word “driver”, we can look
    through the text content for words that commonly appear in the same documents.
    We may, in this case, find words (in priority order by the percentage of documents
    they appear within) like “taxi” (40%), “car” (35%), “golf” (15%), “club” (12%),
    “printer” (3%), “linux” (3%), and “windows” (1%). Similarly, we can look at the
    signals from users who searched for “driver” and aggregate common keywords from
    their other searches in priority order like “screwdriver” (50%), “printer” (30%),
    “windows” (25%), “mac” (15%), “golf” (2%), and “club” (2%). The lists derived
    from signals versus content might be similar, or they could look very different.
    The content-based approach tells us the most-represented meanings within our documents,
    whereas the signals-based approach tells us the most-represented meanings being
    looked for by our users.
  prefs: []
  type: TYPE_NORMAL
- en: Since our end goal is to present users with what they are looking for, it’s
    often more effective to rely on the signals-derived meanings than the content-derived
    meanings. But what if we don’t have good content that maps to the signals-derived
    meaning? Do we use the content-derived meaning, or do we try to suggest other
    related searches based on the signals data? What if we don’t have enough signals
    or if the signals data is not very clean? Can we somehow clean up the signals-derived
    data using the content-derived data?
  prefs: []
  type: TYPE_NORMAL
- en: We run into similar issues with recommendations. Content-based recommendations
    use attributes in documents but don’t understand users, whereas signals-based
    recommendations don’t understand content attributes and won’t work without sufficient
    interactions. Content-based recommendations may be based on features that are
    unimportant to users, whereas signals-based recommendations can create self-reinforcing
    loops where users only interact with items they are recommended, but those items
    are only recommended because users are interacting with them.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we want to create a balanced system that can use the best of both content-derived
    and signals-derived intelligence. While this chapter focuses primarily on signals-derived,
    crowdsourced intelligence, a major goal of this book is to show you how to balance
    and combine both approaches to yield a more optimal AI-powered search experience.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Setting up our product and signals datasets (RetroTech)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll use various datasets throughout this book as we explore different use
    cases, but it is also valuable to have a consistent example that we can build
    on as we progress. We’ll benefit by having a robust search use case with lots
    of data and user interactions, and we’ll set that up in the section.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that most techniques in this book apply across almost all
    search cases. The deciding factor for when to use a particular technique typically
    depends more on the volume and variety of content and signals than on the specific
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-commerce search provides one of the most concrete use cases for the value
    of AI-powered search techniques, and it is also one of the most well-understood
    problems among likely readers, so we’ve created an e-commerce dataset to help
    us explore this domain: the RetroTech dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The RetroTech use case
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With aggressive competition among retailers selling cutting-edge electronics,
    multimedia, and tech products, it is hard for a small online business to compete.
    However, a niche but emerging segment of the population chooses to avoid the latest
    and greatest products and instead falls back to the familiar technology of decades
    past. The RetroTech company was launched to meet the needs of this unique group
    of consumers, offering vintage hardware, software, and multimedia products that
    may be hard to find on today’s shelves.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s load the dataset for the RetroTech company so we can get started learning
    about the relationships between documents and user signals, and about how crowdsourced
    intelligence can improve our search relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the product catalog
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The RetroTech website has around 50,000 products available for sale, which we
    need to load into our search engine. If you built the AI-Powered Search codebase
    to run the chapter 3 examples, then your search engine is already up and running.
    Otherwise, the instructions for building and running all the book’s examples can
    be found in appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: With your search engine up, the next thing you need to do is download the RetroTech
    dataset that accompanies this book. The dataset includes two CSV files, one containing
    all of RetroTech’s products, and another containing one year of signals data from
    RetroTech’s users. The following listing shows a few rows of the product catalog
    dataset to get you familiar with the format.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Exploring the RetroTech product catalog
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the products are identified by a UPC (Universal Product Code)
    and also have a name, a manufacturer, and both a short description (used as a
    teaser in search results) and a long description (the full description used on
    the product details pages).
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re trying to search for products, our next step is to send them to
    the search engine to be indexed. To enable search on our RetroTech product catalog,
    let’s run the document indexing code in the following listing to send the product
    documents to the search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Send product documents to the search engine
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to verify that the documents are now indexed and searchable, let’s
    run an example keyword search. The following listing shows an example search for
    `ipod`, a true classic device!
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Running a search on the product catalog
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The results of the preceding `ipod` search are shown in figure 4.1, demonstrating
    that our products are now indexed and searchable. Unfortunately, the relevance
    of the results is quite poor.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F01_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 Product search results. We can see that the product catalog has been
    indexed and a query for `ipod` now returns search results.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the quality of the search results ranking is not yet very good, we have
    an out-of-the-box “keyword matching” search engine that we can begin improving.
    We’ll use this as our base and start introducing more intelligent AI-powered search
    features throughout the rest of the book. Our next step is to introduce our signals
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the signals data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because RetroTech is running on your computer, no live users are searching,
    clicking, or otherwise generating signals. Instead, we’ve generated a dataset
    that approximates the kind of signal activity you’d expect in similar real-world
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we’ll store our signals in the search engine so they can be
    accessed both in real-time search scenarios and for external processing. Running
    the following listing will simulate and index some sample signals that we can
    use throughout the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Indexing the user signals dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With our RetroTech product and signals data all loaded, we’ll soon begin exploring
    ways we can use the signals data to enhance search relevance. Let’s first familiarize
    ourselves with the signals data so we can understand how signals are structured,
    used, and collected in real-world systems.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Exploring the signals data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different types of signals have different attributes that need to be recorded.
    For a “query” signal, we want to record the user’s keywords. For a “click” signal,
    we want to record which document was clicked upon, as well as which query resulted
    in the click. For later analysis, we’d also want to record which documents were
    returned to and possibly viewed by a user after a query.
  prefs: []
  type: TYPE_NORMAL
- en: To make the examples more extensible and avoid custom code for every new signal
    type, we’ve adopted a generic format for representing signals in this book. This
    format may differ from how you currently log signals, but as long as you can ultimately
    map your signals into this format, all the code in this book should work without
    requiring use-case-specific modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The signals format we use in this book is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query_id`—A unique ID for the query signal that originated this signal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user`—An identifier representing a specific user of the search engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type`—What kind of signal (“query”, “click”, “purchase”, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target`—The content to which the signal at this `signal_time` applies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`signal_time`—The date and time the signal occurred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, assume a user performed the following sequence of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Issued a query for `ipad` and had three documents (doc1, doc2, and doc3) returned.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicked on doc1\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Went back and clicked on doc3\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Added doc3 to the shopping cart.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Went back and searched for `ipad` `cover` and had two documents returned (doc4
    and doc5).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicked on doc4\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Added doc4 to the shopping cart.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Purchased the items in the shopping cart (doc3 and doc4).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These interactions would result in the signals shown in Table 4.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Example signals format
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| query_id | user | type | target | signal_time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | u123  | query  | ipad  | 2024-05-01-09:00:00  |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | u123  | results  | doc1,doc2,doc3  | 2024-05-01-09:00:00  |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | u123  | click  | doc1  | 2024-05-01-09:00:10  |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | u123  | click  | doc3  | 2024-05-01-09:00:29  |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | u123  | add-to-cart  | doc3  | 2024-05-01-09:03:40  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | u123  | query  | ipad cover  | 2024-05-01-09:04:00  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | u123  | results  | doc4,doc5  | 2024-05-01-09:04:00  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | u123  | click  | doc4  | 2024-05-01-09:04:40  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | u123  | add-to-cart  | doc4  | 2024-05-01-09:05:50  |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | u123  | purchase  | doc3  | 2024-05-01-09:07:15  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | u123  | purchase  | doc4  | 2024-05-01-09:07:15  |'
  prefs: []
  type: TYPE_TB
- en: 'There are a few things to note about the format of the signals:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The “query” type and the “results” type are broken into separate signals.*
    This isn’t necessary, as they occur at the same time, but this allows us to keep
    the table structure consistent and not have to add an extra results column that
    would only apply to the query signal. Also, if the user clicks the Next Page link
    or scrolls down the page and sees additional results, this structure allows us
    to create a new signal without having to go back and modify the original signal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Every signal ties back to the* `query_id` *of the original “query” signal
    that started the series of content interactions.* The `query_id` is not merely
    a reference to the keywords entered by the user but is instead a reference to
    the specific “query” signal identifying a time-stamped instance of the user’s
    query keywords. Because results for the same query keywords can change over time,
    this enables us to do more sophisticated processing of how users reacted to the
    specific set of results they were shown for a query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Most signal types contain one item in the* `target`*, but the “results” signal
    type contains an ordered list of documents.* The order of results will matter
    for some algorithms we introduce later in the book, to measure relevance. It’s
    thus important to preserve the exact ordering of the search results. The `target`
    is an ordered list of documents in this case, instead of a single document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The checkout resulted in a separate “purchase” signal for each item instead
    of just one “checkout” signal.* This is done so we can track whether each purchase
    originated from separate queries. A “checkout” signal type could additionally
    be added to track the transaction, and possibly could list the two purchases as
    the `target`, but this is superfluous for our needs in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these raw signals available as our building blocks, we can now start thinking
    about how we could link the signals together to begin learning about our users
    and their interests. In the next section, we’ll discuss ways to model users, sessions,
    and requests within our search platform.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Modeling users, sessions, and requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last section, we looked at the structure of user signals as a list of
    independent interactions tied back to an original query. We assumed that a “user”
    was present with a unique ID, but how does one identify and track a unique user?
    Furthermore, once you have identified how to track unique users, what is the best
    way to break their interactions up into sessions to understand when their context
    may have changed?
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a user in web search can be quite fluid. If your search engine
    has authenticated (logged-in) users, then you already have an internal user ID
    to track them. If your search engine supports unauthenticated access or is publicly
    available, however, you’ll have many users running searches with no formal user
    ID. That doesn’t mean you can’t track them, however; it just requires a more fluid
    interpretation of what a “user” means. A unified tracking identifier allows us
    to relate multiple signals from the same user to learn their interaction patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we think of the trackable information as a hierarchy from the most durable
    representation of a user to the least, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*User ID*—A unique user ID that persists across devices (authenticated)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Device ID*—A unique ID that persists across sessions on the same device (such
    as a device ID or an IP address plus a device fingerprint)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Browser ID*—A unique ID that persists across sessions in the same application
    or browser only (a persistent cookie ID)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Session ID*—A unique ID that persists across a single session (such as a cookie
    in a browser’s incognito mode)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Request ID*—A unique ID that only persists for a single request (a browser
    with cookies turned off)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most modern search applications, and certainly in most e-commerce applications,
    we typically need to deal with all of these. As a rule of thumb, you want to tie
    a user to the most durable identifier—the one as high up the list as possible.
    Both the links between request IDs and session IDs, as well as the links between
    session IDs and browser IDs, are through the user’s cookie, so ultimately the
    browser ID (persistent unique ID stored in the cookie) is the common denominator
    for each of these.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically,
  prefs: []
  type: TYPE_NORMAL
- en: If a user has persistent cookies enabled, one browser ID can have many session
    IDs, which can have many request IDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a user clears cookies after each session (such as by using incognito mode),
    each browser ID has only one session ID, which can have many request IDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a user turns off cookies, then each request ID has a new session ID and a
    new browser ID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building search platforms, most organizations do not properly plan for
    and design their signals-tracking mechanisms. If they’re not able to correlate
    visitors’ queries with subsequent actions, it becomes difficult to maximize the
    capabilities of their AI-powered search platform. In some cases, it’s possible
    to derive missing signals-tracking information after the fact (such as by modeling
    signals into likely sessions using timestamps), but it’s usually best to design
    the system upfront to better handle user tracking to prevent potential information
    loss. In the next section, we’ll discuss how these rich signals can be used to
    improve relevance through a process known as “reflected intelligence”.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Introducing reflected intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section, we covered how to capture signals from users as they interact
    with our search engine. While the signals themselves are useful to help us understand
    how our search engine is being used, they also serve as the primary inputs for
    building models that can continually learn from user interactions and enable our
    search engine to self-tune its relevance model. In this section, we’ll introduce
    how these self-tuning models work through the concept of reflected intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 What is reflected intelligence?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you are an employee at a hardware store. Someone asks you where they
    can find a hammer, and you tell them “aisle two”. A few minutes later you see
    the same person walk from aisle two to aisle five without a hammer, and then they
    walk out of aisle five holding a hammer. The next day someone else asks for a
    hammer, you again tell them “aisle two”, and you observe a nearly identical pattern
    of behavior. You would be a lousy employee if you didn’t spot this pattern and
    adjust your advice going forward to provide a better experience for your customers.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, most search engines function in this manner by default—they have
    a largely static set of documents that are returned for each query, regardless
    of who each user is or how prior users have reacted to the list of documents shown.
    By applying machine learning to the collected signals, however, we can learn about
    users’ intent and reflect that knowledge to improve future search results. This
    process is called *reflected intelligence*.
  prefs: []
  type: TYPE_NORMAL
- en: Reflected intelligence is all about creating feedback loops that constantly
    learn and improve based on evolving user interactions. Figure 4.2 demonstrates
    a high-level overview of a reflected intelligence process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F02_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 The reflected intelligence process. A user issues a query, sees results,
    and takes a set of actions. Those actions (signals) are then processed to create
    learned relevance models that improve future searches.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In figure 4.2., a user (Alonzo) runs a search, entering the query `ipad` in
    the search box. A query signal is logged, containing the list of all search results
    displayed to Alonzo. Alonzo then sees the list of search results and takes two
    actions: clicking on a document (doc22) and then purchasing the product that document
    represents. Those two additional actions are logged as additional signals. All
    Alonzo’s signals, along with the signals from every other user, can then be aggregated
    and processed by various machine learning algorithms to create learned relevance
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: These learned relevance models may boost the most popular results for specific
    queries, personalize results for each user, or even learn which document attributes
    are generally most important across all users. The models could also learn how
    to better interpret user queries, such as identifying common misspellings, phrases,
    synonyms, or other linguistic patterns and domain-specific terminology.
  prefs: []
  type: TYPE_NORMAL
- en: Once these learned relevance models are generated, they can then be deployed
    back into the production search engine and immediately applied to enhance the
    outcomes of future queries. The process then begins again, with the next user
    running a search, seeing (now hopefully improved) search results, and interacting
    with those results. This process creates a self-learning system that improves
    with every additional user interaction, getting continually smarter and more relevant
    over time and automatically adjusting as user interests and content evolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we’ll explore a few categories of reflected intelligence
    models, including signals boosting (popularized relevance), collaborative filtering
    (personalized relevance), and learning to rank (generalized relevance). We’ll
    start with one of the simplest and most effective: signals-boosting models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Popularized relevance through signals boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most popular queries sent to your search engine tend to also be the most
    important ones to optimize from a relevance standpoint. Thankfully, since more
    popular queries generate more signals, we can often aggregate and boost the relevance
    of documents with the highest number of signals for each query. This kind of *popularized
    relevance* is known as *signals boosting*. It is one of the simplest forms of
    reflected intelligence and also one of the most effective for improving the relevance
    of your most popular, highest-volume queries. The following listing demonstrates
    an out-of-the-box search with the query `ipad` in our RetroTech search engine,
    prior to any signals boosting being applied.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Executing a keyword search for products matching `ipad`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected, this query returns many documents containing the keyword `ipad`,
    with the documents containing `ipad` typically ranking highest. Figure 4.3 shows
    the results for this query.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F03_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Results of a keyword search for the query `ipad`. Results are returned
    primarily based on the number of occurrences of the keyword, so accessories mentioning
    the keyword multiple times rank higher than the actual product the user intended
    to see.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While these results all contain the word “ipad” in their content multiple times,
    most users would be disappointed with these results, since they are secondary
    accessories as opposed to the main product type that was the focus of the search.
    This is a significant limitation when ranking just by document text. For very
    popular queries, however, it’s likely that many customers will run the same queries
    repeatedly and fight through the frustrating search results to find the actual
    products they are seeking. We can tie these repeated searches into a feedback
    loop that continuously updates a signals-boosting model based on new signals,
    as shown in figure 4.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F04_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 Signals-boosting feedback loop. A user’s search is logged, and the
    current signals-boosting model is applied to return the boosted results. After
    users take action on those results, all user interaction signals with documents
    are aggregated by the originating query to generate an updated model to further
    improve future searches.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once your products are indexed and you’ve started collecting signals for your
    users’ queries and document interactions, the only additional steps necessary
    for implementing signals boosting are to aggregate your signals and then add your
    aggregated signals as boosts to either your queries or your documents. Listing
    4.6 demonstrates a simple model for aggregating signals into a sidecar collection.
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar collections
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sidecar collections are additional collections that sit in your search engine
    alongside a primary collection and that contain other useful data to improve your
    search application. In our e-commerce example, our primary collection is `products`.
    Our `signals` collection, which has already been added, can be considered a sidecar
    collection. We’ll add another sidecar collection, `signals_boosting`, which we’ll
    use at query time to enhance our queries. Throughout the book, we’ll introduce
    many other sidecar collections to store the inputs for and outputs of our generated
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Creating a signals-boosting model by aggregating signals
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a view to make the signals collection queryable with SQL'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Counts total clicks for each document for each keyword'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Executes the signals aggregation SQL query'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Writes the results to a new signals_boosting collection'
  prefs: []
  type: TYPE_NORMAL
- en: The most important part of listing 4.6 is the `signals_aggregation_query`, defined
    as a SQL query for readability. For every query, we will get the list of documents
    that users have clicked on in the search results for that query, along with a
    count of how many times the document has been clicked on. By ordering the documents
    by the click count for each query, we get an ordered list of the documents ranked
    by popularity.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is that users tend to choose the products they believe are the
    most relevant, so if we were to boost these documents, we would expect our top
    search results to become more relevant. We’ll test this theory in the next listing
    by using these aggregated counts as signals boosts. Let’s revisit our previous
    query for `ipad`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Searching with signals boosting to improve relevance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Boost documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Boost query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The query in listing 4.7 does two noteworthy things:'
  prefs: []
  type: TYPE_NORMAL
- en: It queries the `signals_boosting` sidecar collection for ranked documents by
    boost and transforms those signals boosts into another query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then passes that boosting query to the search engine as a query-time boost
    in the `query_boosts` parameter. In the case of Solr (our default search engine),
    this translates internally to a `boost` parameter of `sum(1,query($boost_query))`
    being added to the search request to multiply the relevance score by `1` (so it
    always increases) plus the calculated relevance score of the `boost_query`. (See
    section 3.2 if you want a refresher on influencing ranking like this through functions
    and multiplicative boosts.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you remember from figure 4.3, our original keyword search for `ipad` returned
    mostly iPad accessories, as opposed to actual iPad devices. Figure 4.5 demonstrates
    the improved results after signals boosting is applied to that original query.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F05_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 Search results with signals boosting enabled. Instead of iPad accessories
    showing up as before, we now see actual iPads, because we have crowdsourced the
    answers based on the documents that users choose to interact with.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The new results are significantly better than the keyword-only results. We now
    see products that the user was more likely looking for—iPads! You can expect to
    see similar improvements for most other popular queries in your search engine.
    Of course, as we move further down the list of popular products, the relevance
    improvements from signals boosting will start to decline, and with insufficient
    signals we may even reduce relevance. Thankfully, we’ll introduce many other techniques
    to improve relevance for queries with inadequate signals volume.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section was to walk you through an initial, concrete example
    of implementing an end-to-end reflected intelligence model. The signals aggregation
    used in this implementation was very simple, though the results speak for themselves.
    There are many considerations and nuances to consider when implementing a signals-boosting
    model—whether to boost at query time or index time, how to increase the weight
    of newer signals versus older signals, how to avoid malicious users trying to
    boost particular products in the search results by generating bogus signals, how
    to introduce and blend signals from different sources, and so on. We’ll cover
    each of these topics in detail in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on from popularized relevance and signals boosting for now and discuss
    a few other types of reflected intelligence models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Personalized relevance through collaborative filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now look at a reflected intelligence approach called collaborative filtering,
    which we’ll categorize as *personalized relevance*. Whereas popularized relevance
    determines which results are usually the most popular across many users, personalized
    relevance focuses on determining which items are most likely to be relevant for
    a specific user.
  prefs: []
  type: TYPE_NORMAL
- en: '*Collaborative filtering* is the process of using observations about the preferences
    of some users to predict the preferences of other users. It is the most popular
    type of algorithm used by recommendation engines, and it is the source of the
    common “users who liked this item also liked these items” recommendation lists
    that appear on many websites. Figure 4.6 demonstrates how collaborative filtering
    follows this same reflected intelligence feedback loop that we saw for signals-boosting
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F06_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 Collaborative filtering for user-to-item recommendations. Based upon
    his past behavior, our user (Alonzo) receives recommendations based on items other
    users liked, where those users also interacted with some of the same items as
    Alonzo.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Like signals boosting, collaborative filtering involves a continuous feedback
    loop. Signals are collected, models are built by those signals, recommendations
    are generated by those models, and interactions against those recommendations
    are then logged again as additional signals. Collaborative filtering approaches
    typically generate a user-item interaction matrix, mapping each user to each item
    (document), with the relationship strength between each user and item being based
    on the strength of the positive interactions (clicks, purchases, ratings, and
    so on).
  prefs: []
  type: TYPE_NORMAL
- en: If the interaction matrix is sufficiently populated, it’s possible to infer
    recommendations from it for any user or item with interaction data. This is done
    by directly looking up the other users who interacted with the same item and then
    boosting other items (similar to signals boosting) that those users also interacted
    with. If the user-item interaction matrix is too sparse, however, a matrix factorization
    approach will typically need to be applied.
  prefs: []
  type: TYPE_NORMAL
- en: '*Matrix factorization* is the process of breaking the user-item interaction
    matrix into two matrices: one mapping users to latent features (or *factors*),
    and another mapping those latent factors to items. This is similar to the dimensionality
    reduction approach we described in chapter 3, where we switched from representing
    food items with many exact keywords (a vector including a feature for every word
    in the inverted index) to using a much smaller number of meaningful dimensions
    (a vector with eight features describing the food items) that we compressed the
    data into. This matrix factorization makes it possible to derive user preferences
    for items, as well as the similarity between items, by distilling limited signals
    data into a smaller number of more meaningful dimensions that better generalize
    the similarity between items.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of matrix factorization for collaborative filtering, the latent
    factors represent attributes of our documents that are learned to be important
    indicators of shared interests across users. By matching other documents based
    on these factors, we are using crowdsourcing to find other similar documents matching
    the same shared interests.
  prefs: []
  type: TYPE_NORMAL
- en: As powerful as collaborative filtering can be for learning user interests and
    tastes based entirely on crowdsourced relevance, it suffers from a major flaw
    known as the *cold start problem*. This is a scenario where the returning of results
    is dependent upon the existence of signals, but where new documents that have
    never generated signals will not be returned. This creates a catch-22 situation
    where new content is unlikely to be shown to users (a prerequisite for generating
    signals) because it has not yet generated any signals (which are required for
    the content to be shown). To some degree, signals-boosting models demonstrate
    a similar problem, where documents that are already popular tend to receive a
    higher boost, resulting in them getting even more signals, while unseen documents
    continue to not receive signals boosting. This process creates a self-reinforcing
    cycle that can lead to a lack of diversity in search results. This problem is
    called *presentation bias*, and we’ll show how to overcome it in chapter 12\.
  prefs: []
  type: TYPE_NORMAL
- en: You can also generate recommendations in other ways, such as through content-based
    recommendations, which we’ll explore in the next chapter (section 5.4.6). Collaborative
    filtering is unique, however, in that it can learn the preferences and tastes
    of users for other documents without having to know anything about the content
    of the documents. This is because all decisions are made entirely by observing
    the interactions of users with content and determining the strength of the similarity
    based on those observations. We’ll take a deeper dive into collaborative filtering
    when implementing personalized search in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of only utilizing popularized and personalized relevance models (which
    function best when documents already have signals), a search engine can also benefit
    from a more generalized relevance model that can apply to all searches and documents.
    This goes a long way toward solving the cold-start problem. We’ll next explore
    how crowdsourced relevance can be generalized through a technique called “learning
    to rank”.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Generalized relevance through learning to rank
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because signals boosting (popularized relevance, section 4.2.2) and collaborative
    filtering (personalized relevance, section 4.2.3) only work on documents that
    already have signals, a substantial proportion of queries won’t benefit until
    documents receive traffic. This is where learning to rank proves valuable as a
    form of generalized relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning to rank* (LTR), also known as *machine-learned ranking*, is the process
    of building and using a ranking classifier that can score how well any document
    matches any arbitrary query. You can think of the ranking classifier as a trained
    relevance model. Instead of manually tuning search boosts and other parameters,
    the LTR process trains a machine learning model that can understand the important
    features of your documents and then score search results appropriately. Figure
    4.7 shows the general flow for rolling out LTR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F07_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Learning to rank (generalized relevance). A ranking classifier is
    built from user judgments about the known relevance of documents for each query
    (training set). That ranking classifier model is then used to rerank search results
    so that the top-ranked documents are more relevant.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In an LTR system, the same high-level reflected intelligence process applies
    as in signals boosting and collaborative filtering (refer to figure 4.2). The
    difference is that LTR can use relevance judgment lists (maps of queries to their
    ideal ranked set of documents) to automatically train a relevance model that can
    then be applied generally to all queries. You’ll see that the output of the “Build
    ranking classifier” step in figure 4.7 is a model of relevance features (`title_match_any_terms`,
    `is_known_category`, `popularity`, and `content_age`), and that model is deployed
    into the production search engine periodically to enhance search results ranking.
    The features in a very simple machine-learned ranking model might be readable
    like this, but there is no requirement that a ranking classifier be interpretable
    or explainable, and many advanced, deep-learning-based ranking classifiers are
    not.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 4.7, notice that the live user flow starts with a query for `ipad`.
    The initial search results are then run through the deployed learning-to-rank
    classifier, which returns the final reranked set of search results. Since the
    ranking classifier is typically much more intelligent and uses more complicated
    ranking parameters than a traditional keyword-ranking relevance model, it is usually
    way too slow to use the ranking classifier to score all the matching documents
    in the search engine. Instead, LTR will often use an initial, faster ranking function
    (such as BM25) to find the top-N documents (usually hundreds or thousands of documents)
    and then only run that subset of documents through the ranking classifier. It
    is possible to use the ranking classifier as the main relevance function instead
    of applying this reranking technique, but it’s more common to see a reranking
    approach, as it’s typically much faster while still yielding approximately the
    same results.
  prefs: []
  type: TYPE_NORMAL
- en: LTR can use either explicit relevance judgments (created manually by experts)
    or implicit judgments (derived from user signals), or some combination of the
    two. We’ll cover examples of implementing LTR from both explicit and implicit
    judgment lists in chapters 10–12\.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 Other reflected intelligence models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to diving deeper into signals boosting (chapter 8), collaborative
    filtering (chapter 9), and learning to rank (chapter 10), we’ll explore many other
    kinds of reflected intelligence throughout this book. In chapter 6, we’ll explore
    mining user queries to automatically learn domain-specific phrases, common misspellings,
    synonyms, and related terms, and in chapters 11–12 we’ll explore automated ways
    of learning relevance judgments from users’ interactions so we can automatically
    generate training data for interesting machine learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In general, every interaction between a user and content creates a connection—an
    edge in a graph—that we can use to understand emerging relationships and derive
    deeper insights. Figure 4.8 demonstrates some of the various relationships we
    can learn by exploring this interaction graph. The same incoming signals data
    can be processed differently, through various signals aggregation and machine
    learning approaches, to learn
  prefs: []
  type: TYPE_NORMAL
- en: The similarity between users and items (user-item recommendations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The similarity between items and other items (item-item recommendations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific attribute-based preferences that can generate a profile of a user’s
    interests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The similarity between queries and items
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F08_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 Many reflected intelligence models. The leftmost box represents user-to-item
    similarity for recommendations, the next shows learning specific attribute-based
    preferences for a user’s profile, the third shows learning item-to-item–based
    similarity for recommendations, and the rightmost shows learning query-to-item
    recommendations.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll continue to explore these techniques in the chapters to come, but it’s
    good to keep in mind that signals data contains a treasure trove of potential
    insights and often provides just as much benefit as the content of the documents
    receiving the user interactions. Reflected intelligence and crowdsourcing are
    not limited to only the signals boosting, collaborative filtering, and learning-to-rank
    techniques we’ve described. They can also be derived from content instead of signals,
    as we’ll discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6 Crowdsourcing from content
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we typically think of crowdsourcing as asking users to provide input,
    we’ve seen in this chapter that implicit feedback can often provide as much or
    even more value in aggregate across many user signals. While this chapter has
    been entirely focused on using user signals to do this crowdsourcing, it’s also
    important to point out that content itself can be used as crowdsourced intelligence
    for your AI-powered search platform.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you’re trying to figure out the general quality of your documents,
    you may be able to look at customer reviews to generate a product rating or to
    see if the product has been reported as abusive or spam. If the customer has left
    comments, you may be able to run a *sentiment analysis* algorithm on the text,
    denoting if the comments are positive, neutral, or negative. Based on the detected
    sentiment, you can boost or penalize the source documents accordingly. This process
    is essentially deriving signals from user-submitted content, so it’s still a form
    of crowdsourcing, albeit from other content provided by the users.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned that in chapter 6 we’ll walk through how we can mine user signals
    to automatically learn domain-specific terminology (phrases, misspellings, synonyms,
    and so on). Just as you can take user queries and interactions to learn this terminology,
    you should also realize that documents are typically written by people and that
    very similar kinds of relationships between terminology are therefore reflected
    in the written content. We’ll explore these content-based relationships further
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best-known search algorithms in existence is the *Page Rank* algorithm—the
    breakthrough algorithm that initially enabled Google to rise to prominence as
    the most relevant web search engine. Page Rank goes beyond the text of any given
    web page and looks at the implied behavior of all other web page creators to see
    how they have linked to other web pages. By measuring the incoming and outgoing
    links, it’s possible to measure a web page’s “quality” on the assumption that
    websites are more likely to link to higher-quality, more authoritative sources
    and that those higher-quality sources are less likely to link to lower-quality
    sources. This idea of going beyond the content within a single document and relating
    it to other documents—whether by direct links between them, user comments or feedback,
    any other user interactions, or even just the use of terminology in different,
    nuanced ways across documents—is incredibly powerful. The art and science of using
    all the available information about your content and from your users is key to
    building a highly relevant AI-powered search engine. In chapter 5, we’ll look
    into the concept of knowledge graphs and how we can use some of the relationships
    embedded within the implied links between documents to automatically further domain
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Content, signals, and models (which are derived from content and signals) are
    the three main sources of “fuel” to power an AI-powered search engine, with signals
    being the primary source for crowdsourced relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflected intelligence is the process of creating learning feedback loops that
    improve from each user interaction and reflect that learned intelligence back,
    to continuously increase the relevance of future results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signals boosting is a form of “popularized relevance”, which usually has the
    biggest effect on your highest-volume, most popular queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborative filtering is a form of “personalized relevance”, which can use
    patterns of user interaction with items to learn user preferences or the strength
    of relationships between items and to then recommend similar items based upon
    those learned relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to rank (LTR) is a form of “generalized relevance” and is the process
    of training a ranking classifier based on relevance judgment lists (queries mapping
    to correctly ranked documents). LTR can be applied to rank all documents and avoid
    the cold-start problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other kinds of reflected intelligence exist, including techniques for using
    content (instead of just signals) for crowdsourced relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
