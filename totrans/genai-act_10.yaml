- en: 9 Tailoring models with model adaptation and fine-tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型适应和微调定制模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Basics of model adaptation and its advantages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型适应的基础及其优势
- en: How to train an LLM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练LLM
- en: How to fine-tune an LLM using both SDK and GUI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用SDK和GUI微调LLM
- en: Best practices for evaluation criteria and metrics for fine-tuned LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调LLMs的评估标准和指标的最佳实践
- en: How to deploy a fine-tuned model for inference
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何部署微调模型进行推理
- en: Gaining insight into key model adaptation techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得关键模型适应技术的见解
- en: As we explore the intricate world of large language models (LLMs), a key aspect
    that stands at the forefront of practical artificial intelligence (AI) deployment
    is the concept of model adaptation. In the context of LLMs, model adaptation involves
    modifying a pretrained model such as GPT-3.5 Turbo to enhance its performance
    on specific tasks or datasets. This process is important because while pretrained
    models offer a broad understanding of language and context, they may only excel
    in specialized tasks with adaptation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索大型语言模型（LLMs）错综复杂的世界的进程中，一个在实用人工智能（AI）部署的前沿关键概念是模型适应的概念。在LLMs的背景下，模型适应涉及修改预训练模型，如GPT-3.5
    Turbo，以增强其在特定任务或数据集上的性能。这个过程很重要，因为虽然预训练模型提供了对语言和上下文的广泛理解，但它们可能只有通过适应才能在特定任务上表现出色。
- en: Model adaptation encompasses a range of techniques, each designed to tailor
    a model’s vast general knowledge to particular applications. The path of model
    adaptation is not just about enhancing performance but about transforming a generalist
    AI model into a specialized tool adept at handling the nuanced demands of enterprise
    solutions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应包括一系列技术，每种技术都旨在将模型广泛的一般知识定制到特定应用中。模型适应的道路不仅关乎性能的提升，而且关乎将通用人工智能模型转变为擅长处理企业解决方案复杂需求的专用工具。
- en: For enterprises, adaptation enables LLMs to handle industry-specific jargon,
    comply with regulatory standards in some cases, and align with businesses’ unique
    operational contexts. This relevance is key to deploying AI solutions that add
    value to enterprise environments. It is important to note that most organizations
    should refrain from jumping directly to fine-tuning. We need to consider this
    as a continuum of various techniques, stacked on and complementing one another;
    in addition, they are not mutually exclusive. We have already seen many such techniques
    in the book. For most organizations, if there is a SaaS offering such as a copilot
    in the application they are already using, that is the best place to start. This
    application uses the SaaS out-of-the-box offerings of GenAI implementation and
    has the maximum ROI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业来说，适应能力使得大型语言模型（LLMs）能够处理行业特定的术语，在某些情况下遵守监管标准，并与企业独特的运营环境保持一致。这种相关性对于部署能够为企业环境增加价值的AI解决方案至关重要。值得注意的是，大多数组织应避免直接跳到微调。我们需要将其视为一系列技术的连续体，相互叠加和补充；此外，它们并非相互排斥。我们已经在书中看到了许多这样的技术。对于大多数组织来说，如果他们在使用的应用程序中有一个如协同驾驶员之类的SaaS产品，那么这就是最好的起点。该应用程序使用GenAI实现的SaaS即用产品，并具有最大的投资回报率。
- en: In scenarios where a SaaS solution is neither available nor suitable, and a
    PaaS approach is preferred, it is advisable to begin with prompt engineering as
    the foundational step and expand on it. When we need to ground the model generations
    using our data, we will use retrieval-augmented generation (RAG) combined with
    prompt engineering, as shown in figure 9.1\. When using advanced frontier models
    such as GPT-4, this combination solves 95% of enterprise business cases. At some
    point on this continuum, enterprises will reach a point where there is a need
    to fine-tune a model for specific requirements. Even if we fine-tune, this doesn’t
    eliminate the need to use prompt engineering and RAG. We will see this case in
    the chapter as we fine-tune and use a model that still needs prompt engineering
    to obtain desired results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在SaaS解决方案既不可用也不合适，且更倾向于PaaS方法的情况下，建议从提示工程作为基础步骤开始，并在此基础上扩展。当我们需要使用我们的数据来定位模型生成时，我们将使用检索增强生成（RAG）与提示工程相结合，如图9.1所示。当使用GPT-4等高级前沿模型时，这种组合解决了95%的企业业务案例。在这个连续体的某个点上，企业将达到需要针对特定要求微调模型的需求。即使我们进行了微调，这也不消除使用提示工程和RAG的需求。我们将在章节中看到这种情况，当我们微调和使用一个仍然需要提示工程来获得预期结果时。
- en: This chapter outlines various model adaptation techniques, helping us to understand
    their challenges, see how enterprises can adopt applications, and finally fine-tune
    and deploy a model in production. Let’s start by understanding what model adaptation
    is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了各种模型适应技术，帮助我们了解它们的挑战，了解企业如何采用应用，并最终在生产中微调和部署模型。让我们先了解什么是模型适应。
- en: '![figure](../Images/CH09_F01_Bahree.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F01_Bahree.png)'
- en: Figure 9.1 Model adaptation technique progression
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1 模型适应技术进展
- en: 9.1 What is model adaptation?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 什么是模型适应？
- en: Model adaptation is adjusting an LLM to perform better on a specific task in
    a specific domain, and it is quite similar to transfer learning. Both approaches
    involve using a pretrained model as a starting point. These models have typically
    been trained on large datasets and have developed a robust understanding of various
    features and patterns. The key idea in model adaptation and transfer learning
    is to take a model trained on one task and apply it to a different but related
    task. This saves time and resources that would otherwise be required to train
    a model from scratch.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应是调整LLM以在特定领域中的特定任务上表现更好，这与迁移学习非常相似。这两种方法都涉及使用预训练模型作为起点。这些模型通常在大型数据集上训练，并发展了对各种特征和模式的稳健理解。模型适应和迁移学习的关键思想是，从一个任务上训练的模型中提取，并将其应用于不同但相关的任务。这节省了时间和资源，否则这些资源将用于从头开始训练模型。
- en: As we know, LLMs are trained on a large amount of general text data, which gives
    them a broad understanding of language. Still, they may not be suitable for certain
    tasks or domains requiring specialized knowledge or vocabulary.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，LLM是在大量通用文本数据上训练的，这使它们对语言有广泛的理解。然而，它们可能不适合需要专业知识或词汇的某些任务或领域。
- en: 'The main idea behind model adaptation is that the knowledge learned from the
    original task can aid performance on the new task. At a high level, there are
    two broad categories of model adaptation—domain and task:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应背后的主要思想是，从原始任务中学到的知识可以帮助在新任务上的表现。在较高层次上，模型适应主要有两大类—领域和任务：
- en: '*Domain adaptation*—If you have a model trained in one domain (e.g., general
    news articles) and want it to perform well in a different but related domain (e.g.,
    medical news articles), you will use domain adaptation techniques.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域适应*—如果你有一个在一个领域（例如，一般新闻文章）中训练的模型，并希望它在不同但相关的领域（例如，医学新闻文章）中表现良好，你将使用领域适应技术。'
- en: '*Task adaptation*—If you have a model trained for one task (e.g., sentiment
    analysis) and you want it to perform a new but related task (e.g., emotion detection),
    task adaptation techniques can be utilized.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务适应*—如果你有一个针对一个任务（例如，情感分析）训练的模型，并且希望它执行一个新但相关的任务（例如，情绪检测），可以使用任务适应技术。'
- en: For example, an LLM trained on Wikipedia articles might perform poorly on medical
    questions or legal documents. Therefore, model adaptation is needed to fine-tune
    the LLM on a smaller, task-specific or domain-specific dataset, which helps the
    model learn the relevant patterns and features for the target task or domain.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在维基百科文章上训练的LLM可能在医学问题或法律文件上表现不佳。因此，需要模型适应来对LLM进行微调，使其在更小、任务特定或领域特定的数据集上学习，这有助于模型学习针对目标任务或领域的相关模式和特征。
- en: 9.1.1 Basics of model adaptation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 模型适应基础
- en: 'Model adaptation in LLMs involves refining a pretrained model to better fit
    specific tasks or data. This concept can be broadly divided into two main categories:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM中的模型适应涉及对预训练模型进行微调，以更好地适应特定任务或数据。这个概念可以大致分为两大类：
- en: '*Full fine-tuning*—This approach updates all LLM parameters. It involves comprehensive
    retraining of the model on new data, making substantial changes to its learned
    patterns.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全微调*—这种方法更新所有LLM参数。它涉及在新的数据上对模型进行全面的重新训练，对其学习到的模式进行重大改变。'
- en: '*Low-rank adaptation*—Unlike full fine-tuning, low-rank adaptation focuses
    on modifying a smaller set of the model’s parameters. This method introduces trainable
    matrixes into each LLM layer, effectively reducing the number of parameters that
    need adjustment. This section will primarily focus on this category of model adaptation.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*低秩适应*—与全微调不同，低秩适应专注于修改模型参数的较小集合。这种方法在每个LLM层中引入可训练的矩阵，有效地减少了需要调整的参数数量。本节将主要关注这类模型适应。'
- en: 'Let’s delve into key techniques underpinning model adaptation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解支撑模型适应的关键技术：
- en: '*Transfer learning*—This machine learning (ML) strategy involves applying a
    model trained for one task to a different but related task. For instance, a model
    trained on English text might be adapted to work with French text. Transfer learning
    is about using knowledge from one domain to improve performance in another.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迁移学习*——这种机器学习（ML）策略涉及将一个任务训练的模型应用于不同但相关的任务。例如，在英语文本上训练的模型可能被调整为处理法语文本。迁移学习是关于使用一个领域的知识来提高另一个领域的性能。'
- en: '*Fine-tuning*—Fine-tuning continues training a pretrained model on a new, usually
    smaller, and more specialized dataset. It subtly adjusts the model’s parameters
    to align its knowledge with the new task or data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*微调*——微调继续在新的、通常较小且更专业的数据集上训练预训练模型。它微妙地调整模型的参数，以使模型的知识与新的任务或数据保持一致。'
- en: 'Depending on the task, data, and the specific LLM, different model adaptation
    techniques can be applied:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务、数据和特定LLM的不同，可以应用不同的模型适应技术：
- en: '*Task-specific modules*—This technique adds a module (such as a classifier
    or decoder) to the LLM, tailored to a particular task. Both the module and the
    LLM are then fine-tuned on task-specific data. This allows the LLM to learn the
    intricacies of the specific task, while maintaining its broad linguistic knowledge.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特定任务的模块*——这种技术向LLM（大型语言模型）添加一个模块（例如分类器或解码器），针对特定任务进行定制。然后，模块和LLM都将在特定任务数据上进行微调。这允许LLM学习特定任务的复杂性，同时保持其广泛的语用知识。'
- en: '*Low-rank adaptation (LoRA)*—LoRA applies a low-rank approximation to the LLM
    and fine-tunes only these components. This method reduces the number of parameters
    needing adjustment, while maintaining the model’s performance and flexibility.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*低秩适应（LoRA）*——LoRA对LLM应用低秩近似，并仅对这些组件进行微调。这种方法减少了需要调整的参数数量，同时保持模型的表现力和灵活性。'
- en: '*Federated learning*—This approach fine-tunes the LLM across multiple distributed
    datasets, allowing the model to learn from diverse data, while upholding privacy.
    For example, federated learning could adapt BERT for medical text analysis using
    data from various hospitals, resulting in a specialized version such as Med-BERT.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联邦学习*——这种方法在多个分布式数据集上微调LLM，允许模型从多样化的数据中学习，同时维护隐私。例如，联邦学习可以利用来自不同医院的医疗数据来适应BERT进行医学文本分析，从而产生一个专门的版本，如Med-BERT。'
- en: No single technique is universally applicable—experimentation is key. Understanding
    these nuances is crucial for effectively using model adaptation and fine-tuning.
    These methods embody transfer learning principles and provide practical ways to
    enhance AI models’ performance and applicability in different scenarios.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何单一的技术是普遍适用的——实验是关键。理解这些细微差别对于有效地使用模型适应和微调至关重要。这些方法体现了迁移学习原则，并提供了增强AI模型在不同场景下的性能和适用性的实用方法。
- en: 9.1.2 Advantages and challenges for enterprises
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 企业优势和挑战
- en: Model adaptation is increasingly crucial for enterprises in some specific industries
    and scenarios. It offers substantial efficiency, competitiveness, and innovation
    benefits. By employing adapted AI models, businesses can achieve more accurate
    results in less time and with fewer resources than by developing models from scratch.
    For example, in highly specialized domains (e.g., medical and pharmaceutical),
    where the margin of error needs to be closer to zero, fine-tuning a model for
    the specific tasks is one of the few ways to achieve the desired outcome. Other
    specialized areas, such as complex finance details (e.g., fraud detection) and
    legacy code migration (e.g., Cobol, etc.), are high-value examples where enterprises
    would want to consider fine-tuning a model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应在某些特定行业和场景中对企业越来越重要。它提供了显著的效率、竞争力和创新优势。通过使用适应的AI模型，企业可以在更短的时间内、使用更少的资源实现比从头开始开发模型更准确的结果。例如，在高度专业化的领域（例如医疗和制药），其中误差范围需要接近零，为特定任务微调模型是达到预期结果的一小部分方法。其他专业领域，如复杂的金融细节（例如欺诈检测）和遗留代码迁移（例如Cobol等），是企业希望考虑微调模型的高价值示例。
- en: Furthermore, enterprises can also perform better on specialized tasks and gain
    a competitive advantage, depending on the use case. This is especially true in
    cases where enterprises deal with unique datasets and require models to understand
    their specific business context. Model adaptation enables customization, improving
    accuracy and relevance in sentiment analysis, market trend prediction, or personalized
    customer interactions. By using models adapted to their specific needs, businesses
    can gain insights and increase efficiency, which will provide them with a competitive
    advantage in their market.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，企业还可以在特定任务上表现更好，并凭借用例获得竞争优势。这在企业处理独特数据集并需要模型理解其特定业务背景的情况下尤其如此。模型适应性允许定制，提高情感分析、市场趋势预测或个性化客户互动的准确性和相关性。通过使用针对其特定需求定制的模型，企业可以获得见解并提高效率，这将在其市场上提供竞争优势。
- en: Enterprises can enhance efficiency and cost savings by reducing resource requirements
    and resource needs. Fine-tuning existing models requires significantly less computational
    power and data compared to training models from the ground up, which results in
    lower costs and quicker deployment times. For example, training Llama 2’s 70B
    parameter model took many months and 1,720,320 GPU hours, compared to fine-tuning
    a GPT-3.5 Turbo model, which takes only a few hours.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 企业可以通过减少资源需求和需求来提高效率和节省成本。与从头开始训练模型相比，微调现有模型需要显著更少的计算能力和数据，这导致成本更低和部署时间更快。例如，训练Llama
    2的70B参数模型花费了数月时间和1,720,320个GPU小时，而微调GPT-3.5 Turbo模型只需几个小时。
- en: Model adaptation comes with challenges, and several key areas must be considered.
    First, task-specific data is crucial. It is essential to have sufficient data
    to fine-tune an LLM, ensuring that this data is clean, consistent, and representative
    of the specific task. Depending on the task and LLM characteristics, this data
    may require preprocessing, augmentation, or labeling. Determining how much data
    for fine-tuning is enough can be a nuanced process, as it varies based on several
    factors; at a minimum, it is a few hundred to thousand examples, depending on
    the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应性伴随着挑战，必须考虑几个关键领域。首先，特定任务的数据至关重要。拥有足够的数据来微调一个大型语言模型（LLM）是必要的，确保这些数据是干净、一致且能代表特定任务的。根据任务和LLM的特性，这些数据可能需要预处理、增强或标注。确定用于微调的数据量是否足够可能是一个复杂的过程，因为它取决于多个因素；至少，它需要几百到几千个示例，具体取决于模型。
- en: Determining adequate data for fine-tuning models such as OpenAI’s GPT-3.5 depends
    on various factors. The complexity and specificity of the task heavily influence
    data requirements, with more complex tasks requiring more data. However, the quality
    of data is crucial and often outweighs the quantity. Larger models such as GPT-3.5
    can benefit from more data due to their extensive capacity, but they also can
    learn effectively from smaller, high-quality datasets. Organizations typically
    start with a baseline dataset and adjust it based on the model’s performance,
    which is continuously monitored for signs of overfitting or underfitting. Practical
    constraints such as computational resources and time also play a role in determining
    the dataset size. The experience and expertise of data scientists often guide
    the decision. Comparative analysis and continual evaluation are involved in finding
    the optimal balance of data quantity and quality for the specific task requirements.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 确定如OpenAI的GPT-3.5等模型的适当数据量取决于各种因素。任务的复杂性和特定性极大地影响了数据需求，更复杂的任务需要更多的数据。然而，数据质量至关重要，通常比数量更重要。更大的模型如GPT-3.5由于其广泛的能力，可以从更多的数据中受益，但它们也可以从较小的高质量数据集中有效地学习。组织通常从一个基线数据集开始，并根据模型的表现进行调整，模型的表现会持续监控以寻找过度拟合或欠拟合的迹象。实际约束，如计算资源和时间，也在确定数据集大小方面发挥作用。数据科学家的经验和专业知识通常指导决策。比较分析和持续评估涉及找到特定任务需求的数据数量和质量的最佳平衡。
- en: Another significant challenge is related to computational resources and costs.
    Fine-tuning LLMs can be resource intensive and costly, often requiring substantial
    processing power (specifically GPUs) connected with high-speed memory. To manage
    this, it might be necessary to utilize cloud services, invest in specialized hardware,
    or employ distributed systems. Additionally, the cost of accessing pretrained
    LLMs can vary, depending on the provider and licensing agreements, which can add
    to the overall expense.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重大挑战与计算资源和成本相关。微调LLM可能需要大量资源且成本高昂，通常需要连接高速内存的大量处理能力（特别是GPU）。为了管理这一点，可能需要利用云服务、投资专用硬件或采用分布式系统。此外，访问预训练LLM的成本可能因提供商和许可协议而异，这可能会增加总体费用。
- en: Performance and generalization are also critical considerations. Evaluating
    the performance of a fine-tuned LLM is imperative; it involves comparing it to
    other models or established baselines, which ensures that the fine-tuned LLM does
    not overfit the training data and can generalize well to new or unseen inputs.
    We cover evaluations later in this chapter, and more details on benchmarks and
    associated tools are covered in chapter 12.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 性能和泛化也是关键考虑因素。评估微调LLM的性能是必不可少的；这涉及到将其与其他模型或已建立的基线进行比较，确保微调的LLM不会过度拟合训练数据，并且能够很好地泛化到新的或未见过的输入。我们将在本章后面介绍评估方法，有关基准和关联工具的更多细节将在第12章介绍。
- en: The ethical and social implications of using fine-tuned LLMs must be addressed
    as well. This includes understanding potential risks and biases, such as concerns
    related to data privacy, model fairness, and social effects. Adhering to appropriate
    guidelines, standards, or regulations is necessary to ensure the ethical and responsible
    use of fine-tuned LLMs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微调LLM的伦理和社会影响也必须得到解决。这包括理解潜在的风险和偏差，例如与数据隐私、模型公平性和社会影响相关的担忧。遵守适当的指南、标准或法规是确保微调LLM的道德和负责任使用所必需的。
- en: Finally, finding the right talent is critical. The need for specialized talent
    and expertise is a significant factor in successfully fine-tuning LLMs, which
    includes individuals who deeply understand ML, natural language processing (NLP),
    and the specific architecture of LLMs. These experts must be skilled in various
    areas, such as data preparation, model architecture design, training strategies,
    and performance evaluation. The need for skilled personnel adds another layer
    of challenges to the already complex process of LLM fine-tuning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，找到合适的人才至关重要。对专业人才和专业知识的需求是成功微调LLM的重要因素，这包括对机器学习（ML）、自然语言处理（NLP）和LLM的具体架构有深刻理解的个人。这些专家必须擅长多个领域，如数据准备、模型架构设计、训练策略和性能评估。对熟练人员的需求给本已复杂的LLM微调过程增加了另一层挑战。
- en: 9.2 When to fine-tune an LLM
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 何时微调一个LLM
- en: Fine-tuning is a technique to improve a model’s performance on a specific task.
    However, it should be the last option and used only after applying other techniques,
    such as prompt engineering and RAG. These techniques complement each other and
    should be stacked for the best output, even when using fine-tuned models. As we
    saw in earlier chapters, prompt engineering and RAG are not mutually exclusive
    but are complementary and should be stacked, even when fine-tuning. This stacked
    approach gives the best outputs, even when using fine-tuned models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种提高模型在特定任务上性能的技术。然而，它应该是最后的选项，并且仅在应用其他技术（如提示工程和RAG）之后使用。这些技术相互补充，应该堆叠以获得最佳输出，即使在使用微调模型时也是如此。正如我们在前面的章节中看到的，提示工程和RAG不是相互排斥的，而是互补的，即使在微调时也应该堆叠。这种堆叠方法即使在使用微调模型时也能提供最佳输出。
- en: Once we decide to fine-tune a model, we prepare the dataset needed for training
    and start the fine-tuning process, which can take from a few hours to a few days.
    After training, we evaluate the fine-tuned model against the base model and the
    specific task’s baseline.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定微调模型，我们就准备用于训练的数据集，并开始微调过程，这个过程可能需要几个小时到几天。训练完成后，我们将评估微调模型与基模型和特定任务的基线。
- en: Let’s use an example to help us fine-tune and understand various aspects. Say
    we want to adapt a model to respond with emojis—a bot that can understand what
    we are asking but respond only using emojis. We will call this EmojiBot. We want
    to fine-tune GPT-3.5 Turbo and make it an EmojiBot. But to show that these emojis
    are different and specialized for a task, we don’t want the emojis that we would
    expect to see, say, in a chat application, on social media, or in our texts. Rather,
    we want the ones that follow the format used by Microsoft Teams.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来帮助我们微调和理解各个方面。假设我们想要调整一个模型以使用表情符号进行响应——一个能够理解我们提出的问题但只使用表情符号进行响应的机器人。我们将称之为EmojiBot。我们想要微调GPT-3.5
    Turbo并使其成为EmojiBot。但为了表明这些表情符号是不同的，并且针对特定任务而专门设计的，我们不希望看到我们在聊天应用、社交媒体或我们的文本中期望看到的那种表情符号。相反，我们希望看到那些遵循Microsoft
    Teams使用的格式的表情符号。
- en: Figure 9.2 shows the high-level flow for fine-tuning. First, we identify a task
    that would benefit from fine-tuning (such as EmojiBot). We identify which characteristics
    fall short of the task and create evaluation criteria. We then compare the default
    models’ performance against our needs. If they perform well, we establish a baseline
    and curate the dataset required for fine-tuning. The amount and format of data
    depend on the model; we’ll cover the details later. We obtain a fine-tuned model
    after training, which can take hours or days, depending on the task. Next, we
    must evaluate it against the base model and the baseline for the specific task
    using qualitative and quantitative measures.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2展示了微调的高级流程。首先，我们确定一个可以从微调中受益的任务（例如EmojiBot）。我们识别哪些特征不符合任务要求，并创建评估标准。然后，我们将默认模型的性能与我们的需求进行比较。如果表现良好，我们建立基线并整理微调所需的dataset。数据的数量和格式取决于模型；我们将在后面详细说明。经过训练后，我们获得一个微调模型，这可能需要数小时或数天，具体取决于任务。接下来，我们必须使用定性和定量指标，将其与基模型和特定任务的基线进行比较。
- en: '![figure](../Images/CH09_F02_Bahree.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F02_Bahree.png)'
- en: Figure 9.2 Fine-tuning end-to-end flow
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 微调端到端流程
- en: It is quite common and almost expected that the first fine-tuned model will
    be worse than the default model. Usually, finding a suitable deployment model
    takes 10–12 training iterations. Each iteration requires tweaking the training
    data to address weak areas, which can take hours to days. It’s a time- and effort-consuming
    process that should be one of the last steps.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个微调模型通常会比默认模型差，这是相当常见且几乎可以预见的。通常，找到一个合适的部署模型需要10-12次训练迭代。每次迭代都需要调整训练数据以解决薄弱环节，这可能需要数小时到数天。这是一个耗时且费力的过程，应该是最后一步之一。
- en: NOTE  Fine-tuning enhances the model’s performance on tasks similar to those
    outlined in the fine-tuning dataset. This process might manifest as improved accuracy,
    more relevant responses, or a better understanding of domain-specific language.
    Improved performance in terms of cheaper or faster models is a side advantage
    and not something guaranteed. One way to achieve this is to fine-tune a smaller
    model, such as GPT-3.5 Turbo, on a specific task to improve it instead of using
    a more expensive and powerful model, such as GPT-4.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：微调可以增强模型在类似微调dataset中概述的任务上的性能。这个过程可能表现为提高准确性、更相关的响应或对特定领域语言的更好理解。在更便宜或更快的模型方面提高性能是一个附带优势，并不保证。一种实现方式是在特定任务上微调一个较小的模型，如GPT-3.5
    Turbo，而不是使用更昂贵、更强大的模型，如GPT-4。
- en: Now that we have identified a task that makes sense to fine-tune—that is, an
    EmojiBot where we want to respond in emojis but in a certain pattern—let’s examine
    the steps needed to fine-tune an LLM such as GPT-3.5 Turbo.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了一个有意义的微调任务——即一个EmojiBot，我们想要以某种模式使用表情符号进行响应——让我们检查微调LLM如GPT-3.5 Turbo所需的步骤。
- en: 9.2.1 Key stages of fine-tuning an LLM
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 微调LLM的关键阶段
- en: 'When we want to fine-tune a model for an identified task, as outlined later
    in figure 9.6, section 9.3.5, there are five key stages:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要针对一个已识别的任务微调模型时，如图9.6和9.3.5节所述，有五个关键阶段：
- en: '*Choosing a model and fine-tuning method*—To fine-tune a language model, it
    is necessary to choose a foundation model that suits the task and data. Various
    models are available, such as GPT, BERT, and RoBERTa. Consider factors such as
    the model’s suitability for the task, input/output size, dataset size, and technical
    infrastructure. Fine-tuning methods can vary based on the task and data, such
    as transfer learning, sequential fine-tuning, or task-specific fine-tuning.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择模型和微调方法*——为了微调语言模型，有必要选择适合任务和数据的基座模型。各种模型可供选择，例如GPT、BERT和RoBERTa。考虑因素包括模型对任务的适用性、输入/输出大小、数据集大小和技术基础设施。微调方法可以根据任务和数据而变化，例如迁移学习、顺序微调或特定任务微调。'
- en: '*Data curation*—This stage involves preparing a task-specific dataset for fine-tuning
    and largely involves preparing and preprocessing the dataset. This process often
    includes data cleaning, text normalization (e.g., tokenization), and converting
    the data into a format compatible with the LLM’s input requirements (e.g., data
    labeling). It is essential to ensure that the data represents the task and domain
    and covers a range of scenarios the model is expected to encounter in production.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据整理*——这一阶段涉及为微调准备特定任务的训练数据集，主要涉及准备和预处理数据集。这个过程通常包括数据清洗、文本归一化（例如，分词）以及将数据转换为与LLM输入要求兼容的格式（例如，数据标注）。确保数据代表任务和领域，并涵盖模型在生产中预期遇到的多种场景至关重要。'
- en: '*Fine-tuning*—This stage is the actual process of fine-tuning and involves
    training the pretrained LLM on the task-specific dataset. The training process
    involves optimizing the model’s weights and parameters to minimize the loss function
    and improve its performance on the task. The fine-tuning process may involve several
    rounds of training on the training set, validation of the validation set, and
    hyperparameter tuning to optimize the model’s performance.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*微调*——这一阶段是实际的微调过程，涉及在特定任务数据集上训练预训练的LLM。训练过程包括优化模型的权重和参数，以最小化损失函数并提高其在任务上的性能。微调过程可能涉及在训练集上进行多轮训练，验证验证集，以及超参数调整以优化模型性能。'
- en: '*Evaluating*—Once the fine-tuning process is complete, we must evaluate the
    model’s performance on a test dataset. This helps to ensure that the model is
    generalizing well to new data and performing well on the specific task. Common
    metrics used for evaluation include accuracy, precision, recall, F1 score, Bilingual
    Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation
    (ROUGE), and so forth. This topic is covered later in detail in section 9.3.2\.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*评估*——一旦微调过程完成，我们必须在测试数据集上评估模型的表现。这有助于确保模型在新数据上具有良好的泛化能力，并在特定任务上表现良好。常用的评估指标包括准确率、精确率、召回率、F1分数、双语评估助手（BLEU）、基于召回的摘要评估助手（ROUGE）等等。这个主题将在9.3.2节中详细讨论。'
- en: '*Deployment (inference)*—Once the fine-tuned model is evaluated and we are
    happy with its performance, it can be deployed to production. The deployment process
    may involve integrating the model into a larger system, setting up the necessary
    infrastructure, and monitoring the model’s performance in real-world scenarios.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*部署（推理）*——一旦微调模型经过评估并且我们对它的性能感到满意，它就可以部署到生产环境中。部署过程可能涉及将模型集成到更大的系统中，设置必要的基础设施，并监控模型在实际场景中的性能。'
- en: Now that we have a basic concept of model adaptation and when to fine-tune,
    let’s see how to fine-tune.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了模型适应的基本概念以及何时进行微调，让我们看看如何进行微调。
- en: 9.3 Fine-tuning OpenAI models
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 微调OpenAI模型
- en: Here, we’ll use an example to fine-tune OpenAI’s GPT-3.5 Turbo model. Currently,
    for OpenAI, only GPT-4, GPT-3.5 Turbo, GPT-3 Babbage (Babbage-002), and GPT-3
    (Davinci-002) are available for fine-tuning. Several OSS LLMs, such as Meta’s
    Llama 2 and G42’s Falcon, can be fine-tuned. In our case, the book’s GitHub repository
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)) contains complete code
    samples and screenshots that we use and show how to fine-tune OpenAI GPT-3.5 Turbo.
    To make this as real for organizations as possible, we will show the process by
    using both Azure OpenAI and OpenAI.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将通过一个示例来微调OpenAI的GPT-3.5 Turbo模型。目前，对于OpenAI，只有GPT-4、GPT-3.5 Turbo、GPT-3
    Babbage（Babbage-002）和GPT-3（Davinci-002）可用于微调。一些开源LLM，如Meta的Llama 2和G42的Falcon，也可以进行微调。在我们的案例中，书籍的GitHub仓库（[https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)）包含了我们使用和展示如何微调OpenAI
    GPT-3.5 Turbo的完整代码示例和截图。为了尽可能使组织感到真实，我们将通过使用Azure OpenAI和OpenAI来展示这个过程。
- en: We want to fine-tune GPT-3.5 Turbo and make it an EmojiBot, where the model
    responds in emojis only. However, as we outlined earlier, we want emojis to follow
    the format used by Microsoft Teams.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望微调GPT-3.5 Turbo并使其成为EmojiBot，其中模型只以表情符号回应。然而，正如我们之前概述的，我们希望表情符号遵循Microsoft
    Teams使用的格式。
- en: In Microsoft Teams, the text in parentheses, such as `(dog)`, renders the relevant
    emojis. We will fine-tune the model to respond to this text, which represents
    the specific task we want the model to improve. To understand all the different
    options and the corresponding text in Teams, see [https://bit.ly/TeamEmojis](https://bit.ly/TeamEmojis).
    Given that we have a task, let’s start preparing the dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在Microsoft Teams中，括号中的文本，如`(dog)`，会渲染相关的表情符号。我们将微调模型以响应此类文本，这代表我们希望模型改进的特定任务。要了解所有不同的选项和在Teams中的相应文本，请参阅[https://bit.ly/TeamEmojis](https://bit.ly/TeamEmojis)。鉴于我们有一个任务，让我们开始准备数据集。
- en: 9.3.1 Preparing a dataset for fine-tuning
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 准备微调数据集
- en: 'Now that we have reached a point where we have identified a task for which
    fine-tuning would make sense, we need to create a dataset of examples required
    to fine-tune. We need to create two sets of datasets: one for training and another
    for validation. A validation dataset is a subset of data used to evaluate the
    performance of a fine-tuned model on the target task. It is different from the
    training dataset, which is used to update the model’s parameters, and the test
    dataset, which is used to measure the final accuracy of the model.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经到达了一个点，我们确定了一个适合微调的任务，我们需要创建一个用于微调的示例数据集。我们需要创建两组数据集：一组用于训练，另一组用于验证。验证数据集是用于评估微调模型在目标任务上性能的数据子集。它与训练数据集不同，训练数据集用于更新模型的参数，而测试数据集用于衡量模型的最终准确率。
- en: A validation dataset is important for fine-tuning LLMs because it helps us to
    avoid overfitting, which is when the model learns the specific patterns of the
    training data and fails to generalize to new data. Using a validation dataset,
    you can monitor the model’s progress and adjust the learning rate, the number
    of epochs, or other hyperparameters to optimize the model’s performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据集对于微调LLMs非常重要，因为它帮助我们避免过拟合，即模型学习训练数据的特定模式，但无法推广到新数据。使用验证数据集，您可以监控模型的进度并调整学习率、epoch数量或其他超参数以优化模型性能。
- en: These examples should show different ways to solve the problem and the results
    of each method. We also need to identify shortcomings using a base model, such
    as inconsistent performance on edge cases, inability to fit enough shot prompts
    in the context window to steer the model, high latency, and so forth.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例应展示解决问题的不同方法以及每种方法的成果。我们还需要使用基础模型（如边缘案例上的不一致性能、无法在上下文窗口中足够地适应提示以引导模型、高延迟等）来识别不足之处。
- en: It is highly recommended that a validation dataset be used to measure the effectiveness
    of fine-tuning. The training and validation datasets are in the JSONL format,
    with each line containing a JSON object with a text key for input text and a target
    key for desired output text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议使用验证数据集来衡量微调的有效性。训练和验证数据集都是JSONL格式，每行包含一个具有文本键的JSON对象，用于输入文本，以及一个目标键，用于期望的输出文本。
- en: Fine-tuned models are directly correlated with high-quality training data. Different
    models require varying amounts of training data. For effective training, we need
    hundreds to thousands of curated data examples. Although the API requires a minimum
    of 10 examples, having more is generally better. Ten examples aren’t enough to
    influence LLMs such as GPT-3.5 Turbo in any significant way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型与高质量的训练数据直接相关。不同的模型需要不同数量的训练数据。为了有效训练，我们需要数百到数千个精心挑选的数据示例。尽管API要求至少10个示例，但通常拥有更多示例更好。10个示例不足以对LLMs如GPT-3.5
    Turbo产生任何显著影响。
- en: OpenAI recommends having at least 50 good examples to train our model. They
    also recommend more good examples for better-fine-tuned models than bad ones,
    as those examples can negatively affect the model. Consequently, it is advisable
    only to use the best ones from your internal data. The following listing shows
    an example JSONL file for chat data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI建议至少有50个好的示例来训练我们的模型。他们还建议比坏示例更多的好示例，因为那些示例可能会对模型产生负面影响。因此，建议只使用您内部数据中的最佳示例。以下列表显示了一个用于聊天数据的示例JSONL文件。
- en: Listing 9.1 JSONL example
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.1 JSONL示例
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we can see, the model is being shown how to respond using emojis formatted
    in a certain pattern, such as `(sadkoala)`, `(tired)`, and `(like)`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型正在被展示如何使用特定格式的表情符号来响应，例如 `(sadkoala)`、`(tired)` 和 `(like)`。
- en: Basic checks
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本检查
- en: Before fine-tuning, it’s important to perform basic checks on the training data
    to avoid wasting time and resources. These checks can include data readability,
    formatting validation, lightweight analysis for missing pairs, and token length.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调之前，对训练数据进行基本检查非常重要，以避免浪费时间和资源。这些检查可以包括数据可读性、格式验证、缺失对的分析以及令牌长度。
- en: We validate the data file by loading and reading it using the `basic_checks()`
    function. It takes a filename as input and returns the number of messages found.
    The messages must be in the chat completion format for fine-tuning GPT-3.5 Turbo.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用`basic_checks()`函数加载和读取数据文件来验证数据文件。该函数接受一个文件名作为输入，并返回找到的消息数量。消息必须采用聊天完成格式，以便微调
    GPT-3.5 Turbo。
- en: 'Listing 9.2 Dataset validation: Basic checks'
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2 数据集验证：基本检查
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Opens the file in read-mode'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 以读取模式打开文件'
- en: '#2 Loads each line of the file as a JSON object and stores it in a list'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将文件的每一行加载为 JSON 对象，并将其存储在列表中'
- en: '#3 Prints the first example from the dataset and helps visually check whether
    things intuitively look OK'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打印数据集中的第一个示例，并帮助视觉检查事物是否直观上看起来正常'
- en: '#4 Loops through the messages in the first example and prints each one'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 遍历第一个示例中的消息并打印每个消息'
- en: Format checks
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 格式检查
- en: Once we have done the basic checks, the next step is to check the file for the
    format and ensure it is structured properly before processing it further. This
    is an important step, mainly because even if the format is incorrect, we won’t
    get an error when we start the training job, but the resulting model will be very
    poor, and we will only realize this posttraining when we deploy. To avoid much
    of this trouble, it is highly recommended that we check for formats.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成基本检查后，下一步是检查文件格式，并确保在进一步处理之前它被正确地结构化。这是一个重要的步骤，主要是因为即使格式不正确，我们在开始训练作业时也不会得到错误，但生成的模型将非常糟糕，我们只有在部署后才会意识到这一点。为了避免许多这样的麻烦，强烈建议我们检查格式。
- en: Listing 9.3 shows `format_checks()`, which checks for chat completion format
    and pairing, with dataset and filename as its two arguments. It catches most errors
    but not all. The function iterates over each example in the dataset and checks
    for data type checks, the presence of message lists, and message keys. It validates
    that it has the relevant roles and content validation. This function also helps
    debug data-related problems.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3 展示了 `format_checks()`，它检查聊天完成格式和配对，其两个参数是数据集和文件名。它捕获了大多数错误，但不是所有错误。该函数遍历数据集中的每个示例，并检查数据类型检查、消息列表的存在和消息键。它验证是否有相关的角色和内容验证。此函数还有助于调试数据相关的问题。
- en: 'Listing 9.3 Dataset validation: Checking for format'
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3 数据集验证：检查格式
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we should also understand how the dataset performs when it comes to
    simple data distributions, token counts, and costs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还应该了解数据集在简单数据分布、令牌计数和成本方面的表现。
- en: Note  The token count is important, not just for cost. If it is larger than
    the maximum number of tokens the model can handle, it will be truncated without
    warning. Knowing this up front is very helpful.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：令牌计数很重要，不仅仅是因为成本。如果它超过了模型可以处理的令牌最大数量，它将被截断而不会发出警告。提前了解这一点非常有帮助。
- en: The following listing shows how we can finish doing the checks on the dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了我们可以如何完成对数据集的检查。
- en: 'Listing 9.4 Dataset validation: Cost estimation and basic analysis'
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.4 数据集验证：成本估算和基本分析
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 9.3.2 LLM evaluation
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 LLM 评估
- en: Evaluating LLMs is important for ensuring their quality, reliability, and fairness.
    However, evaluating LLMs is complex, as it involves multiple dimensions and challenges.
    Maintaining diverse automatic metrics can help efficiently track model improvements
    during adaptation cycles, while reducing costly manual reviews. Metrics should
    be customized to each adapted model’s use cases and business needs. Continuous
    logging from production systems enables the evaluation of real-world performance
    over time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 评估 LLM 对于确保其质量、可靠性和公平性非常重要。然而，评估 LLM 是复杂的，因为它涉及多个维度和挑战。维护多样化的自动指标可以帮助在适应周期中有效地跟踪模型改进，同时减少昂贵的手动审查。指标应根据每个适应模型的使用案例和业务需求进行定制。从生产系统持续记录可以启用对随时间推移的实时性能的评估。
- en: Benchmarking against baselines is an essential step in evaluating fine-tuned
    GPT models. It involves comparing the performance of the fine-tuned model with
    a preestablished standard or baseline model. This baseline could be the model’s
    performance before fine-tuning or a different model known for its proficiency
    in a similar task. The purpose of this comparison is to quantify the improvements
    brought by fine-tuning. For instance, a fine-tuned model might be benchmarked
    against a standard translation model in a language translation task to assess
    translation accuracy or fluency improvements. This process helps in understanding
    the efficacy of fine-tuning and identifying areas where the model has improved
    or still needs enhancement.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将微调后的GPT模型与基线进行基准测试是评估微调模型的一个重要步骤。这涉及到将微调模型的性能与预先建立的标准或基线模型进行比较。这个基线可能是微调前的模型性能或在不同任务中因其熟练度而知名的不同模型。这种比较的目的是量化微调带来的改进。例如，在语言翻译任务中，微调后的模型可能被与标准翻译模型进行基准测试，以评估翻译准确度或流畅性的改进。这个过程有助于了解微调的有效性，并确定模型改进或仍需增强的领域。
- en: Evaluation criteria
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估标准
- en: When preparing the fine-tuning dataset, we should also define the evaluation
    criteria. When fine-tuning, the evaluation process begins by establishing clear
    criteria critical for assessing the performance and efficacy of the model in its
    intended application. These criteria often include relevance, coherence, accuracy,
    and language fluency (table 9.1).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备微调数据集时，我们还应该定义评估标准。在微调过程中，评估过程首先通过建立明确的评估标准来开始，这些标准对于评估模型在其预期应用中的性能和有效性至关重要。这些标准通常包括相关性、连贯性、准确性和语言流畅性（表9.1）。
- en: Table 9.1 Fine-tuning evaluation criteria
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表9.1 微调评估标准
- en: '| Evaluation criteria | Description |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 评估标准 | 描述 |'
- en: '| --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Relevance  | Gauges how well the model’s responses or outputs align with
    the context and intent of the input. This is especially crucial in applications
    such as chatbots, where providing contextually appropriate responses is key to
    user satisfaction. Relevance is often assessed by examining whether the model
    can stay on topic and provide information or responses directly applicable to
    the queries or tasks.  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 相关性 | 衡量模型响应或输出与输入上下文和意图的匹配程度。这在聊天机器人等应用中尤为重要，在这些应用中，提供上下文适当的响应对于用户满意度至关重要。相关性通常通过检查模型是否能够保持主题并提供与查询或任务直接相关的信息或响应来评估。
    |'
- en: '| Coherence  | Refers to the logical consistency of the model’s outputs. A
    fine-tuned model should generate contextually relevant, logically sound, and coherent
    text. This means the responses should follow a logical structure and narrative
    flow, making sense in the conversation or text context. Coherence is vital for
    maintaining user engagement and ensuring the model’s outputs are understandable
    and meaningful.  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 连贯性 | 指的是模型输出的逻辑一致性。一个微调后的模型应该生成上下文相关、逻辑合理且连贯的文本。这意味着响应应该遵循逻辑结构和叙事流程，在对话或文本上下文中有意义。连贯性对于保持用户参与度和确保模型输出可理解且有意义至关重要。
    |'
- en: '| Accuracy  | This particularly comes into play when the model is used for
    tasks involving factual information, such as educational tools, informational
    bots, or any application where providing correct information is critical. Accuracy
    is measured by how well the model’s responses align with factual correctness and
    objective truth.  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 准确性 | 这在模型用于涉及事实信息任务时尤其重要，例如教育工具、信息机器人或任何提供正确信息至关重要的应用。准确性是通过模型响应与事实正确性和客观真理的一致性来衡量的。
    |'
- en: '| Language fluency  | Pertains to the grammatical and syntactical correctness
    of the model’s outputs. Even if a model is highly relevant, coherent, and accurate,
    poor language fluency can significantly detract from the user’s experience. This
    includes proper grammar, punctuation, and style, ensuring the text generated is
    correct and reads naturally to the end user.  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 语言流畅性 | 涉及模型输出的语法和句法正确性。即使模型高度相关、连贯和准确，较差的语言流畅性也会显著影响用户体验。这包括正确的语法、标点符号和风格，确保生成的文本正确且对最终用户来说读起来自然。
    |'
- en: Evaluating a fine-tuned GPT model using these criteria involves a combination
    of automated metrics, manual review, and user feedback, ensuring that the model
    meets the high standards required for its specific application.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些标准评估微调后的GPT模型涉及自动指标、人工审查和用户反馈的结合，确保模型满足其特定应用所需的高标准。
- en: Choosing appropriate metrics
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择适当的指标
- en: When fine-tuning models, selecting the right metrics for evaluation is crucial
    to accurately assessing the model’s performance and improvements [1]. After fine-tuning,
    these metrics indicate how well the model adapts to specific tasks or domains.
    They provide insights into various aspects of model performance, such as prediction
    accuracy, language quality, and task-specific effectiveness. Enterprises should
    look for automated metrics evaluation where possible and have a set of quantitative
    and qualitative metrics.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调模型时，选择合适的评估指标对于准确评估模型性能和改进至关重要 [1]。微调后，这些指标表明模型如何适应特定任务或领域。它们提供了对模型性能各个方面的见解，例如预测准确性、语言质量和特定任务的有效性。企业应尽可能寻找自动化的指标评估，并拥有一套定量和定性指标。
- en: 'Quantitative metrics:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定量指标：
- en: Several metrics help measure the overlap between model outputs and human reference
    texts. The next section will outline some of them (BLEU, ROUGE, METEOR, etc.).
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几个指标有助于衡量模型输出与人类参考文本之间的重叠。下一节将概述其中的一些（BLEU、ROUGE、METEOR 等）。
- en: F1 score evaluates the accuracy tradeoff between precision and recall.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1 分数评估了精确度和召回率之间的准确度权衡。
- en: Perplexity assesses model uncertainty/confidence for generated text.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 困惑度评估生成文本的模型不确定性/信心。
- en: Task completion is used for goal-oriented dialog systems and the percentage
    of successful task resolution.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务完成度用于目标导向的对话系统和成功任务解决的百分比。
- en: 'Qualitative metrics:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定性指标：
- en: '*Fluency*—Rating grammaticality and readability of outputs'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流畅性*—评估输出的语法和可读性'
- en: '*Coherence*—Logical consistency and narrative flow'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连贯性*—逻辑一致性和叙事流畅性'
- en: '*Conciseness*—Avoiding repetitive and excessive text'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简洁性*—避免重复和过多的文字'
- en: '*Factual accuracy*—Avoiding objective falsehoods'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*事实准确性*—避免客观错误'
- en: 'The choice of metrics should align with the model’s intended application, whether
    translation, summarization, classification, or creative content generation. Metrics
    such as perplexity, BLEU score, ROUGE, F1 score, and human evaluation each offer
    a unique perspective on the model’s capabilities, helping to ensure a comprehensive
    and balanced evaluation of the fine-tuned model’s performance. Let’s look at each
    of these in more detail:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 指标的选择应与模型的预期应用相一致，无论是翻译、摘要、分类还是创意内容生成。如困惑度、BLEU 分数、ROUGE、F1 分数和人工评估等指标各自提供了对模型能力的独特视角，有助于确保对微调模型性能的全面和平衡评估。让我们更详细地看看每个指标：
- en: '*Perplexity*—This metric is a standard in language modeling, used to quantify
    how well a model predicts a sample. It measures the uncertainty of the language
    model in predicting the next token in a sequence [2]. A lower perplexity score
    indicates that the model is more confident and accurate in its predictions. This
    is particularly important in fine-tuning, as it can reflect how well the model
    has adapted to new styles or domains of text. It’s a crucial metric for assessing
    improvements in language generation tasks.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*困惑度*—这是语言模型中的标准指标，用于量化模型预测样本的能力。它衡量语言模型在预测序列中的下一个标记时的不确定性 [2]。较低的困惑度分数表明模型在预测方面更加自信和准确。这在微调中尤为重要，因为它可以反映模型对新风格或文本领域的适应程度。它是评估语言生成任务改进的关键指标。'
- en: '*BLEU score (Bilingual Evaluation Understudy)*—The BLEU score evaluates machine
    translation quality by comparing it to reference translations. It counts matching
    word groupings and computes a score based on these matches. A higher BLEU score
    indicates better translation quality, but it has limitations and may not capture
    semantic accuracy or fluency [3].'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BLEU 分数（双语评估辅助研究）*—BLEU 分数通过将其与参考翻译进行比较来评估机器翻译质量。它计算匹配的词组并基于这些匹配计算分数。较高的 BLEU
    分数表明翻译质量更好，但它有局限性，可能无法捕捉语义准确性或流畅性 [3]。'
- en: '*ROUGE (Recall-Oriented Understudy for Gisting Evaluation)*—ROUGE is a metric
    for automatic summarization evaluation. It measures the overlap between computer-generated
    output and reference summaries to assess the summary’s quality. Different variations
    of ROUGE provide insights into aspects of the summary’s quality [4].'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ROUGE（基于回忆的摘要评估的辅助研究）*—ROUGE 是用于自动摘要评估的指标。它通过衡量计算机生成的输出与参考摘要之间的重叠来评估摘要的质量。ROUGE
    的不同变体提供了对摘要质量方面的见解 [4]。'
- en: '*F1 score*—The F1 score is useful in classification tasks such as sentiment
    analysis and topic categorization. It balances the tradeoff between precision
    and recall, providing a single measure of a model’s accuracy in categorizing or
    classifying text.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F1分数*——F1分数在分类任务（如情感分析和主题分类）中很有用。它平衡了精确度和召回率之间的权衡，提供了一个衡量模型在分类或分类文本中的准确性的单一指标。'
- en: '*Human evaluation*—Despite the utility of automated metrics, human judgment
    remains crucial, especially for tasks that require subjective assessment, such
    as story generation, creative writing, and conversational agents. Human evaluators
    can provide insights into aspects such as the naturalness of the text, its appropriateness,
    creativity, and even the subtleties of humor or sarcasm. This qualitative evaluation
    complements quantitative metrics, offering a more holistic view of the model’s
    performance.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工评估*——尽管自动化指标很有用，但人类判断仍然至关重要，特别是对于需要主观评估的任务，如故事生成、创意写作和对话代理。人工评估者可以提供关于文本的自然性、适宜性、创造力和甚至幽默或讽刺的细微差别等方面的见解。这种定性评估补充了定量指标，为模型性能提供了一个更全面的视角。'
- en: Task-specific evaluation is essential to measuring a model’s performance in
    its intended application. It involves using different metrics and considerations
    based on the task. For instance, summarization models are evaluated using ROUGE
    scores and human summary coherence and informativeness assessments. Similarly,
    question-answering models are evaluated for accuracy and relevance to the given
    questions. This evaluation ensures the model performs well in general metrics
    and is effective and reliable for its specific use case.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 任务特定评估对于衡量模型在预期应用中的性能至关重要。它涉及根据任务使用不同的指标和考虑因素。例如，摘要模型使用ROUGE分数和人工摘要的连贯性和信息性评估进行评估。同样，问答模型根据给定问题的准确性和相关性进行评估。这种评估确保模型在一般指标上表现良好，并且对于其特定用例来说既有效又可靠。
- en: Error analysis
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误分析
- en: Error analysis is a critical component of the evaluation process, involving
    a detailed examination of where and why the fine-tuned model is underperforming.
    This analysis helps identify patterns in the model’s mistakes, which can be categorically
    broken down into semantic errors, factual inaccuracies, or language inconsistencies.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分析是评估过程中的一个关键组成部分，涉及对微调模型性能不佳的原因和地点的详细检查。这种分析有助于识别模型错误中的模式，这些模式可以按类别分解为语义错误、事实不准确或语言不一致。
- en: For example, if a model consistently makes errors in understanding certain types
    of queries or generates responses with factual errors, this would be highlighted
    in error analysis. Understanding these error patterns is crucial for further refining
    the model and making targeted improvements. It also aids in understanding the
    model’s limitations and areas where it might require additional data or more sophisticated
    fine-tuning approaches. Now let’s get to fine-tuning.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个模型在理解某些类型的查询或生成包含事实错误的响应时持续出错，这将在错误分析中突出显示。理解这些错误模式对于进一步改进模型和进行有针对性的改进至关重要。它还有助于理解模型的局限性以及可能需要更多数据或更复杂的微调方法的领域。现在让我们进入微调阶段。
- en: 9.3.3 Fine-tuning
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 微调
- en: Now that our dataset is ready and validated, we can kick off the fine-tuning
    process. There are two steps to perform when we need to fine-tune. First, we upload
    the dataset we worked on in the previous sections. When uploaded, each file gets
    a unique file ID that we need to save. This file ID is what we pass as one of
    the parameters to the fine-tuning job so it knows which file to use for which
    fine-tuning job.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了并验证了数据集，我们可以启动微调过程。当我们需要微调时，有两个步骤要执行。首先，我们上传在前几节中工作的数据集。上传后，每个文件都会获得一个唯一的文件ID，我们需要保存这个ID。这个文件ID是我们传递给微调作业的一个参数之一，以便它知道使用哪个文件进行哪个微调作业。
- en: We can use the API or GUI to do this. We will see how to achieve this using
    Python SDK and Azure AI Studio. I won’t show all the steps in the GUI book, but
    those details are available in the accompanying GitHub repo at [https://bit.ly/GenAIBook](https://bit.ly/GenAIBook).
    Let’s start by using the SDK.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用API或GUI来完成这个任务。我们将通过Python SDK和Azure AI Studio展示如何实现这一点。我不会在GUI手册中展示所有步骤，但这些详细信息可以在配套的GitHub仓库[https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)中找到。让我们先使用SDK。
- en: Fine-tuning using the SDK
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用SDK进行微调
- en: The following listing shows how to use the SDK and `files.create()` method,
    pass in the file name, and specify the purpose of the file (`fine-tune`).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了如何使用 SDK 和 `files.create()` 方法，传入文件名，并指定文件的目的（`fine-tune`）。
- en: Listing 9.5 Uploading dataset for fine-tune
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5 上传数据集进行微调
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 This version (or later) is required for fine-tuning.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 此版本（或更高版本）是进行微调所必需的。'
- en: '#2 Environment variables with the connection details'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 包含连接详情的环境变量'
- en: '#3 Dataset that we need to use for training'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们需要用于训练的数据集'
- en: 'When we run this snippet, we obtain the following output, with the file ID
    we need to be aware of when we run the second step:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此代码片段时，我们获得以下输出，其中包含我们在运行第二步时需要了解的文件 ID：
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After uploading our file, we must start the fine-tuning job. When using the
    SDK, this is done using the `fine_tunings.jobs.create()` method. This function
    needs the ID of the training dataset file from the previous steps and the model
    to use. In our case, we want to fine-tune GPT-3.5 Turbo, specifically the 0613
    model. We also specify how many epochs we need for fine-tuning. Finally, the `suffix`
    parameter is something we can use to help track and manage the fine-tuned model
    later.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上传我们的文件后，我们必须开始微调作业。当使用 SDK 时，这是通过 `fine_tunings.jobs.create()` 方法完成的。此函数需要来自之前步骤的训练数据集文件
    ID 和要使用的模型。在我们的情况下，我们想要微调 GPT-3.5 Turbo，特别是 0613 模型。我们还指定了微调所需的 epoch 数。最后，`suffix`
    参数是我们可以用来自助跟踪和管理微调模型的一个参数。
- en: Listing 9.6 Starting the fine-tuning job
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6 开始微调作业
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 This version (or later) is required for fine-tuning.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 此版本（或更高版本）是进行微调所必需的。'
- en: '#2 Environment variables with connection details'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 包含连接详情的环境变量'
- en: '#3 The file ID you see will differ from this one.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 您看到的文件 ID 将与此不同。'
- en: 'This snippet submits a finetuning job that gets queued up; depending on available
    capacity at the specific region and data center, the job will get executed. As
    a reminder, with Azure OpenAI, you can have multiple regions where fine-tuning
    is available. Our example shows the job ID from our API call:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段提交了一个微调作业，该作业将被排队；根据特定区域和数据中心可用的容量，作业将被执行。提醒一下，使用 Azure OpenAI，您可以在多个区域进行微调。我们的示例显示了我们的
    API 调用中的作业 ID：
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Depending on the dataset size, the model we want to fine-tune, and the fine-tuning
    hyperparameters, the fine-tuning job can take a few hours. The fine-tuning jobs
    API has a function call `list()` that we can use to see all the fine-tuning jobs
    we have submitted.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集大小、我们想要微调的模型以及微调超参数，微调作业可能需要几个小时。微调作业 API 有一个名为 `list()` 的函数调用，我们可以使用它来查看我们已提交的所有微调作业。
- en: Listing 9.7 Listing all the fine-tuning jobs
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.7 列出所有微调作业
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: One example of this output is presented in listing 9.8\. We see that we have
    completed two fine-tuning jobs, as shown by the `succeeded` status; one job is
    currently in the status `running`, which means that it has one active fine-tuning
    job ongoing. The last fine-tuning job (`ftjob-367ee1995...`) we have just submitted
    as `pending` means that the job is queued up to run at some point in the future.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.8 中的输出是一个示例，我们看到我们已经完成了两个微调作业，如 `succeeded` 状态所示；一个作业目前处于 `running` 状态，这意味着有一个正在进行的活跃微调作业。我们刚刚提交的最后微调作业（`ftjob-367ee1995...`）处于
    `pending` 状态，这意味着作业将在未来的某个时间排队运行。
- en: Listing 9.8 Output of fine-tuning jobs listing
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.8 微调作业列表输出
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For a specific fine-tuning job, we can also see the various events related to
    that job. The following listing shows an example of this, again using the ID of
    our newly submitted job (`ftjob-367ee1995...)`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个特定的微调作业，我们还可以看到与该作业相关的各种事件。以下列表显示了此示例，再次使用我们新提交作业的 ID (`ftjob-367ee1995...`)。
- en: Listing 9.9 Listing events from a fine-tuning job
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.9 从微调作业中列出事件
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output in this case is
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出如下
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can also poll to check the status of a job every few seconds and, using this,
    kick off another workflow. In this instance, this job ran for approximately two
    hours before finishing. For this, we need the `IPython` package, which can be
    installed in conda using `conda` `install` `ipython`, or if one is using pip,
    then via – `pip` `install` `ipython.`
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以每隔几秒轮询检查作业状态，并使用此信息启动另一个工作流程。在这种情况下，此作业运行了大约两小时后完成。为此，我们需要 `IPython` 包，该包可以通过
    conda 使用 `conda install ipython` 安装，或者如果使用 pip，则通过 `- pip install ipython` 安装。
- en: Listing 9.10 Polling to check fine-tuning job status
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.10 检查微调作业状态
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Depending on the model and the length of the queue to schedule the fine-tuning
    task, one fine-tuning job can take a few hours to finish. During this time, we
    get training metrics that help us understand how the training goes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型和排程微调任务的队列长度，一个微调作业可能需要几个小时才能完成。在这段时间内，我们获得训练指标，帮助我们了解训练过程。
- en: 9.3.4 Fine-tuning training metrics
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 微调训练指标
- en: As outlined earlier, the training can take a few hours; for more complex and
    bigger models, it can take a few days. The training during this time is not a
    black box; we can get details on key metrics during the process to get a high-level
    idea of what is happening. We have three key metrics that can be tracked—the training
    loss, mean token accuracy, and token accuracy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练可能需要几个小时；对于更复杂和更大的模型，可能需要几天。这段时间内的训练不是一个黑盒；我们可以在过程中获取关键指标的具体细节，以获得对正在发生的事情的高级了解。我们有三个关键指标可以跟踪——训练损失、平均标记准确率和标记准确率。
- en: Loss
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 损失
- en: 'There are two aspects of loss: training and validation loss. Training loss
    measures the difference between the model’s predictions and the actual outcomes.
    A lower loss means the model is more accurate and has less error. Lower loss values
    indicate better model performance, suggesting the model’s predictions are closer
    to the actual data.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 损失有两个方面：训练损失和验证损失。训练损失衡量模型预测与实际结果之间的差异。损失值越低，意味着模型越准确，错误越少。较低的损失值表示模型性能更好，表明模型的预测更接近实际数据。
- en: If we have a validation dataset (which is highly recommended), then we also
    have additional metrics that allow us to measure how the model is doing. The validation
    loss is a metric that measures the model’s error on the validation set, a portion
    of the dataset set aside to evaluate the model’s performance on new or unseen
    data. The validation loss is calculated by summing up the errors for each example
    in the validation set, using the same cost function as the training loss. The
    validation loss is usually measured after each epoch, a complete pass through
    the training set.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有验证数据集（强烈推荐），那么我们还有额外的指标，可以让我们衡量模型的表现。验证损失是一个衡量模型在验证集上误差的指标，验证集是从数据集中预留出来以评估模型在新或未见数据上的性能的部分。验证损失是通过使用与训练损失相同的成本函数，对验证集中的每个示例的错误进行求和来计算的。验证损失通常在每个epoch后测量，即对训练集的完整遍历。
- en: Figure 9.3 shows an example of the loss when we fine-tune using Azure OpenAI
    and the model performance during training. The graph in figure 9.3 showing the
    training loss for fine-tuning training results illustrates how well the model
    learns from the training data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3展示了使用Azure OpenAI微调时的损失示例以及训练过程中的模型性能。图9.3中显示的微调训练结果的训练损失图说明了模型从训练数据中学习的效果。
- en: We see the loss value for each training step, a batch of training examples.
    The *x*-axis is the step number, and the *y*-axis is the loss value. The graph
    shows that the loss decreases as the model trains on more data, indicating that
    it is improving its performance. However, the loss does not reach zero, which
    means the model still has some errors and cannot perfectly fit the data. This
    is normal, as overfitting the data can lead to poor generalization of new data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个训练步骤的损失值，一个训练示例的批次。*x*轴是步骤号，*y*轴是损失值。图表显示，随着模型在更多数据上训练，损失值下降，表明它在提高性能。然而，损失值不会达到零，这意味着模型仍然存在一些错误，无法完美地拟合数据。这是正常的，因为过度拟合数据可能导致对新数据的泛化能力差。
- en: '![figure](../Images/CH09_F03_Bahree.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F03_Bahree.png)'
- en: Figure 9.3 Training loss when fine-tuning GPT-3.5 Turbo
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3微调GPT-3.5 Turbo时的训练损失
- en: To interpret the graph and determine whether the model is performing well, ideally
    for a good fit, we want both training and validation loss to decrease to stability
    with a minimal gap between the two, which indicates that the model is learning
    and generalizing well. If the training loss decreases while the validation loss
    increases, the model may be overfitting the training data and not generalizing
    well to new data. Finally, if both training and validation loss remain high, the
    model may be underfitting, which means it’s not learning the underlying patterns
    in the data well enough. The scale of the loss and the number of training steps
    must be considered. The model might need more training if the loss is still high
    or the validation loss has yet to stabilize. For those with an ML model experience
    or background, the overall approach for splitting between training and validating
    datasets and interpreting these metrics is very similar.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释图表并确定模型是否表现良好，理想情况下，为了获得良好的拟合度，我们希望训练和验证损失都降低到稳定状态，两者之间的差距最小，这表明模型正在良好地学习和泛化。如果训练损失下降而验证损失上升，则模型可能过度拟合了训练数据，并且对新数据泛化不佳。最后，如果训练和验证损失都保持较高水平，则模型可能欠拟合，这意味着它没有足够好地学习数据中的潜在模式。必须考虑损失的规模和训练步骤的数量。如果损失仍然很高或验证损失尚未稳定，则模型可能需要更多的训练。对于那些有机器学习模型经验或背景的人来说，在训练和验证数据集之间划分的整体方法和解释这些指标是非常相似的。
- en: An interesting behavior is that the data in the loss graph fluctuates, indicating
    that the loss value can vary depending on the samples in each batch. It is normal
    for the model to be noisy; however, in fine-tuning, the model learns and improves
    its performance as long as the loss decreases over time.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的行为是损失图中的数据波动，表明损失值可能因每个批次的样本而异。模型存在噪声是正常的；然而，在微调过程中，只要损失随时间下降，模型就会学习和提高其性能。
- en: To find whether the fine-tuning is good, we would typically look for a low and
    stable validation loss close to the training loss. The thresholds for what would
    be considered good loss values are subjective and will vary depending on the task’s
    complexity and the nature of the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到微调是否良好，我们通常会寻找接近训练损失的较低且稳定的验证损失。被认为是良好损失值的阈值是主观的，并且将根据任务的复杂性和数据的性质而变化。
- en: Mean token accuracy
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均值标记准确度
- en: Mean token accuracy measures how well a fine-tuned model correctly predicts
    each token in the output sequence that the model generates or predicts during
    training. It is reflected as a percentage, that is, the percentage of tokens the
    model predicts correctly in a dataset. For example, if the mean token accuracy
    is 90%, it means that on average, the model correctly predicts 90% of the tokens.
    This is an average calculated by dividing the number of correctly predicted tokens
    by the total number of tokens in the output.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 均值标记准确度衡量了一个微调模型正确预测模型在训练过程中生成的或预测的输出序列中的每个标记的程度。它以百分比的形式反映出来，即模型在数据集中正确预测的标记百分比。例如，如果均值标记准确度为90%，则表示平均而言，模型正确预测了90%的标记。这是通过将正确预测的标记数量除以输出中的总标记数量来计算的。
- en: 'Similar to the loss for mean token accuracy, we have two metrics: one for the
    training and the other for validation (assuming one has provided a validation
    dataset). Figure 9.4 shows the mean token accuracy of a fine-tuning job for training
    and validation. The training mean token accuracy is the average accuracy of the
    model’s predictions on the training data. It measures how well the model learns
    from the training data and adapts to it. A high training mean token accuracy suggests
    that the model learns effectively from the training data. In contrast, the validation
    mean token accuracy is the average accuracy of the model’s predictions on the
    validation data. It measures how well the model generalizes to new data it has
    not seen before. A high validation mean token accuracy suggests that the model
    does not overfit the training data and can generalize well to new data.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与均值标记准确度的损失类似，我们有两个指标：一个用于训练，另一个用于验证（假设已经提供了一个验证数据集）。图9.4显示了训练和验证的微调作业的均值标记准确度。训练均值标记准确度是模型在训练数据上的预测的平均准确度。它衡量模型从训练数据中学习并适应的程度。高训练均值标记准确度表明模型有效地从训练数据中学习。相比之下，验证均值标记准确度是模型在验证数据上的预测的平均准确度。它衡量模型对新数据的泛化能力。高验证均值标记准确度表明模型没有过度拟合训练数据，并且可以很好地泛化到新数据。
- en: '![figure](../Images/CH09_F04_Bahree.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F04_Bahree.png)'
- en: Figure 9.4 Training mean token accuracy
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4 训练平均标记准确率
- en: The difference between the two metrics can help identify whether the model is
    overfitting to the training data. Suppose the training mean token accuracy is
    much higher than the validation mean token accuracy. In that case, it suggests
    that the model is overfitting to the training data and not generalizing well to
    new data. In contrast, if the validation mean token accuracy is much lower than
    the training mean token accuracy, it suggests that the model is underfitting the
    training data and not learning effectively.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 两个指标之间的差异可以帮助识别模型是否过度拟合训练数据。如果训练平均标记准确率远高于验证平均标记准确率，那么这表明模型过度拟合了训练数据，并且对新数据泛化不佳。相反，如果验证平均标记准确率远低于训练平均标记准确率，那么这表明模型欠拟合训练数据，并且没有有效学习。
- en: This metric is useful for evaluating the performance of a fine-tuned model on
    the training data. A good mean token accuracy can be relative and depends on the
    specific task or application. Generally, a higher value (closer to 1.0) indicates
    better performance. However, it does not reflect how well the model generalizes
    to new or unseen data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此指标用于评估微调模型在训练数据上的性能。良好的平均标记准确率可能是相对的，并取决于具体任务或应用。通常，较高的值（接近1.0）表示更好的性能。然而，它并不反映模型对新或未见数据的泛化能力。
- en: Note that the interpretation of these metrics can depend on the specific task
    or application. Therefore, it’s essential to consider other metrics and qualitative
    evaluations to get a comprehensive view of the model’s performance. The quality
    of mean token accuracy depends on the task’s complexity and the nature of text.
    Higher accuracy (closer to 100%) is expected for simpler tasks or texts with predictable
    patterns. A lower accuracy might still be good for more complex tasks or diverse
    texts.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些指标的解读可能取决于具体任务或应用。因此，考虑其他指标和定性评估以全面了解模型性能至关重要。平均标记准确率的质量取决于任务的复杂性和文本的性质。对于简单任务或具有可预测模式的文本，预期准确率（接近100%）会更高。对于更复杂任务或多样化的文本，较低的准确率可能仍然很好。
- en: One way to assess whether the mean token accuracy is good is to compare it with
    a baseline or with the performance of other models on the same task. If your model’s
    accuracy is higher than the baseline or similar models, it’s a positive sign.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 评估平均标记准确率是否良好的一种方法是将它与基线或其他模型在同一任务上的性能进行比较。如果你的模型准确率高于基线或类似模型，这是一个积极的信号。
- en: Now that we understand the basic constructs of fine-tuning and using a CLI or
    code, let’s take a look at how we can achieve this using Azure OpenAI and a GUI.
    As stated earlier, we will use Azure OpenAI as an example, but the same process
    applies to OpenAI.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了微调的基本结构和使用CLI或代码的方法，让我们看看如何使用Azure OpenAI和GUI来实现这一点。如前所述，我们将使用Azure
    OpenAI作为示例，但同样的过程也适用于OpenAI。
- en: 9.3.5 Fine-tuning using Azure OpenAI
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.5 使用Azure OpenAI进行微调
- en: Instead of using the SDK and the CLI, we also have a visual interface that we
    can employ to achieve the same outcome. Often, doing this manually would be a
    better approach than using code. To kick off a fine-tuning job in Azure OpenAI,
    when logged into the Azure Portal and in the AI Studio, under models, we choose
    the option to create a custom model (figure 9.5).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用SDK和CLI，我们还有一个可视化的界面可以用来达到相同的效果。通常，手动操作会比使用代码更有效。要在Azure OpenAI中启动微调作业，当登录到Azure门户并在AI
    Studio中时，在模型下选择创建自定义模型选项（图9.5）。
- en: 'We go through the wizard and choose to upload the training and validation datasets,
    as shown in figure 9.6\. Note: If these have been uploaded using the SDK, we will
    find them here, as long as they are in the same tenants and have the same end-point
    deployment.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向导选择上传训练和验证数据集，如图9.6所示。注意：如果这些数据集已经使用SDK上传，只要它们位于同一租户并且具有相同的端点部署，我们在这里就能找到它们。
- en: '![figure](../Images/CH09_F05_Bahree.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F05_Bahree.png)'
- en: 'Figure 9.5 Azure AI Studio: Creating a custom model'
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5 Azure AI Studio：创建自定义模型
- en: '![figure](../Images/CH09_F06_Bahree.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F06_Bahree.png)'
- en: Figure 9.6 Choosing a training and validation dataset
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6 选择训练和验证数据集
- en: Figure 9.7 shows the status and details of each of our training jobs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7显示了每个训练作业的状态和详情。
- en: '![figure](../Images/CH09_F07_Bahree.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F07_Bahree.png)'
- en: Figure 9.7 Training job details
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7 训练作业详情
- en: Now that we have a fine-tuned model, we need to deploy it to a test environment
    to run an evaluation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个微调好的模型，我们需要将其部署到测试环境中进行评估。
- en: 9.4 Deployment of a fine-tuned model
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deployment of a fine-tuned model is quite straightforward. The new fine-tuned
    model shows up as another model available for use in our Azure tenant or OpenAI
    subscription, as shown in figures 9.8 and 9.9, respectively.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F08_Bahree.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 Deploying fine-tuned model for inference
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: OpenAI has launched a feature in the playground that lets users see how a fine-tuned
    model differs from the base model side by side, which can be useful visually but
    not efficiently.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F09_Bahree.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 OpenAI fine-tuned model deployment
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '9.4.1 Inference: Fine-tuned model'
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Returning to our task, we now have a fine-tuned model for EmojiBot, where the
    bot responds in emojis using the format that Microsoft Teams uses. Figure 9.10
    shows how the out-of-the-box GPT-3.5 Turbo model behaves when asked to respond
    with emojis; this is expected but will not work with Teams.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F10_Bahree.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 Response with emojis using GPT-3.5 Turbo
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, the experience for the same questions using our fine-tuned powered
    EmojiBot is quite different, as shown in figure 9.11\. Here, for the same questions
    as before, we get the response in the format we’ll be able to use in Teams.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F11_Bahree.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 Fine-tuned EmojiBot inference
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, it is easy to get completely incorrect results on the same questions
    from earlier and with the same parameter settings (figure 9.12). We can see the
    fine-tuned model answer in emojis—`(Pizza)` and `(Feeling tired)—`but the result
    is not what we expected.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F12_Bahree.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 Fine-tuned EmojiBot with incorrect results
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To resolve this, we need to tweak the system prompt to steer the model to respond
    using emojis where possible, which is a great way to close out by reminding that
    a stacked approach of prompt engineering, RAG, and fine-tuning (where the task
    at hand warrants) is the right approach in almost all cases.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to fine-tune a model and the steps one needs to undertake,
    let us switch and look at some of the underpinnings of the technology that will
    make this work. Strictly speaking, we do not require this to do a fine-tuning,
    but it will help us to understand some of the nuances to achieve better outcomes
    for fine-tuning. We will start by understanding how we train an LLM and, at a
    high level, what the steps entail.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Training an LLM
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is helpful to our understanding of model adaptation and the techniques and
    their associated limitations to examine what it means and what it takes to do
    full training for an LLM. At a high level, if we were to do full training and
    build an LLM from scratch, that training would involve four major stages, as shown
    in figure 9.13.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F13_Bahree.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 Full end-to-end training of an LLM [5]
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s go through each stage in more detail.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.1 Pretraining
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Base LLMs are built during this initial stage. We touched on base LLMs in chapter
    2\. These are the original, pretrained models trained on a massive corpus of text
    data. They can generate text based on the patterns they learned during training.
    Some also call them raw language models.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Note  While powerful, these base models are less suitable for general-purpose
    applications because they may need to align their responses with the specific
    intentions or instructions of the user. They are more like raw engines for text
    generation, lacking the refined capability to understand and adhere to the nuances
    of user prompts. Base models do not answer questions and often respond with more
    questions. In contrast, instructors are tailored to be more interactive and user-friendly,
    which makes them more suitable for a wide range of applications, from customer
    service chatbots to educational tools, where understanding and following instructions
    accurately is crucial.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.2 Supervised fine-tuning
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised fine tuning (SFT) is the next stage. In this stage, the base model
    undergoes refining of the base model with high-quality, domain-specific data.
    These datasets consist of prompt–response pairs, manually created (often by human
    contractors), which are fewer in number than in the previous stage but of much
    higher quality. The contractors follow detailed documentation to create these
    prompt–response pairs, ensuring relevance and quality. Similar to the last pretraining
    stage, the SFT model is trained to predict the next token in these pairs, but
    these are less accurate and contextually aware when generating the response.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: SFT is a technique for optimizing LLMs on labeled data for a specific downstream
    task, such as sentiment analysis, text summarization, or machine translation.
    Later in the chapter, we will cover additional details of SFT methods and approaches.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.3 Reward modeling
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third phase is reward modeling, the first part of the Reinforcement Learning
    from Human Feedback (RLHF) process. The main goal at this stage is to develop
    a model that can evaluate and rank responses based on their quality and relevance.
    To do this, the SFT model (from the previous stage) generates multiple responses
    to a prompt, which human contractors then rank based on various criteria such
    as domain expertise, fact-checking, and code execution. These rankings train a
    reward model, which learns to score responses like human contractors.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.4 Reinforcement learning
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the second part of the RLHF process, and it aims to enhance the language
    model’s ability to generate high-quality responses through iterative feedback.
    In this final stage, the reward model scores responses generated by the SFT model
    for many prompts. These scores are used to further train the SFT model, ultimately
    leading to the creation of the RLHF model. The RLHF aligns the LLMs with human
    preferences or expectations for a given task or domain, such as chat, code, or
    creative writing. More details on RLHF methods will be covered later in this chapter.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.5 Direct policy optimization
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Direct policy optimization (DPO) [6] is another technique, which is a new type
    of reward model parameterization in RLHF that used for fine-tuning LLMs to align
    with our preferences. It exploits a relationship between reward functions and
    optimal policies. It allows us to skip the reward modeling step outlined earlier,
    as long as the human feedback can be expressed in binary terms—that is, a choice
    between two options. DPO can solve the reward maximization problem with constraints
    in a single policy training phase, essentially treating it as a classification
    problem. PPO (see section 9.7) requires a reward model and a complex RL-based
    optimization process; DPO, however, bypasses the reward modeling step and directly
    optimizes the language model on preference data, which can be simpler and more
    efficient. As DPO eliminates the need to train a reward model instead of training
    a reward model and optimizing a policy based on that model, we can directly optimize
    the policy. This characteristic makes this approach quicker, and fewer resources
    are used than in RLHF with PPO.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 9.6 Model adaptation techniques
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several techniques available for model adaptation, with each technique
    providing its unique approach and being suitable for different scenarios depending
    on the specific requirements (i.e., the model size, available computational resources,
    and the desired level of adaptation). One of the main techniques widely used for
    adapting LLMs is low adaptation ranking (LoRA), which will be covered in more
    detail in the next section. In LoRA, instead of updating all the weights in the
    model, only a small subset of parameters, introduced as low-rank matrixes, are
    modified. This approach allows efficient training and adaptation, while preserving
    most of the pretrained model’s structure and knowledge.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameter efficient fine-tuning (PEFT) is a concept in ML that refers to methods
    of adapting and fine-tuning large pretrained models, such as GPT-3.5, to minimize
    the number of parameters that need to be updated. This approach is particularly
    valuable when dealing with large models, as it reduces computational requirements
    and can mitigate problems such as overfitting. PEFT techniques are designed to
    make fine-tuning more accessible and efficient, especially for users with limited
    computational resources—LoRA is an example of the PEFT method. For more details
    on different types of PEFT techniques and details, see the paper “Scaling Down
    to Scale Up: A Guide to Parameter-Efficient Fine-Tuning” by Vladislav Lialin [7].'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic forgetting is a phenomenon where a model loses its ability to perform
    well on previous tasks after being fine-tuned on new tasks [8]. This can happen
    when the model overwrites its original parameters with task-specific ones, thus
    forgetting the general knowledge it learned from pretraining. When implementing
    PEFT to prevent catastrophic forgetting, we fine-tune only a small subset of parameters,
    while keeping most pretrained parameters fixed. This way, the model can retain
    its generalization ability and adapt to new tasks without losing its previous
    performance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning (SFT) is another type of adaptation technique; it is
    a specific type of fine-tuning where the model is further trained on a labeled
    dataset. It’s supervised because the training process uses a dataset that pairs
    the input data with the correct output (labels). SFT is particularly common in
    tasks such as classification, where the model must learn to associate specific
    inputs with labeled outputs. SFT is a subset of the broader fine-tuning process,
    specifically tailored to situations where labeled data is available.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Of course, each technique has its unique approach and is suitable for different
    scenarios depending on the specific requirements, such as the model size, available
    computational resources, and the desired level of adaptation. Table 9.2 outlines
    some notable ones in addition to LoRA.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.2 Model adaptation techniques
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Technique | Description |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| Prompt tuning  | Prompt tuning is a technique for adapting LLMS to different
    tasks by providing specific cues or prompts that guide their generation or prediction.
    It does not require retraining the model or updating its weights, which makes
    it faster and cheaper than fine-tuning. It is particularly useful for tasks where
    only a small amount of adaptation is required.  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| Adapter modules  | Adapter models are used in LLM fine-tuning to add small
    and task-specific modules (small neural networks) to the pretrained model and
    train only these modules on the task-specific data. They are also flexible and
    modular, as they can be easily added or removed for different tasks without affecting
    the pretrained model.  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| Bias-only (BitFit)  | Bias-only (BitFit) is a technique for fine-tuning LLMs
    by modifying only the bias terms of the model or a subset of them. It offers a
    minimalistic approach to adaptation, requiring even fewer trainable parameters
    than LoRA. BitFit is based on fine-tuning, mainly exposing the knowledge learned
    by pretraining rather than acquiring new task-specific knowledge.  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| Layer freezing  | Layer freezing is a fine-tuning technique that keeps some
    of the model layers fixed and only updates the rest. This method allows for more
    control over which aspects of the model are adapted and can reduce training time.  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| Knowledge distillation  | Involves training a smaller, more efficient model
    (student) to mimic the behavior of a larger pretrained model (teacher). This method
    is useful for deploying LLMs in resource-constrained environments.  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| Meta-learning  | Focuses on training the model to learn new tasks quickly
    with minimal data; often involves training on various tasks so the model can more
    efficiently adapt to new, unseen tasks  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Differential privacy fine-tuning  | Incorporates privacy-preserving techniques
    during fine-tuning to protect sensitive data. This is essential for applications
    where data privacy and security are paramount.  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| Reinforcement learning from human feedback (RLHF)  | Involves fine-tuning
    models based on feedback or rewards derived from human interactions or evaluations.
    It is useful for tasks where human judgment is crucial, such as content moderation.  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: Each technique involves tradeoffs between the computational resources required,
    the level of specialization achieved, and the retention of the model’s original
    capabilities. The choice of technique depends on the specific application, the
    constraints of the deployment environment, and the goals of the model adaptation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are more familiar with various techniques, let’s explore LoRA,
    the main technique for fine-tuning large models such as GPT.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 9.6.1 Low-rank adaptation
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LoRA, which stands for low-rank adaptation [9], is a method specifically designed
    for adapting LLMs. It presents an efficient alternative to traditional fine-tuning
    methods, which is particularly useful in scenarios where fine-tuning large models
    can be resource-intensive and challenging.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: LoRA is based on making minimal but strategic modifications to a pretrained
    model without altering its entire architecture. It achieves this by introducing
    the notion of low-rank matrices. Instead of modifying the entire weight matrices
    of a neural network, LoRA inserts small, low-rank matrices into the model. These
    matrices are applied to the model’s layers (attention and feed-forward) during
    forward and backward passes, as shown in figure 9.14\. As we have seen, LLMs are
    built on deep learning architectures consisting of multiple layers designed to
    process and understand human language. In addition, LoRA also retrains selectively
    only these low-rank matrices, while the original pretrained weights remain frozen.
    This selective retraining significantly reduces the computational resources needed.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F14_Bahree.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 LoRA reparameterization—only A and B are trained [9]
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 9.14, input (**X**) at the bottom of the diagram represents the input
    data fed to the layer with pretrained weights. *A* and *B* are the adaptation
    parameters that will be updated during fine-tuning, and *W* is the original pretrained
    weight that remains frozen.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: When we want to fine-tune a task, we can store and load only a few task-specific
    parameters along with the pretrained model. This approach helps improve the efficiency
    during runtime for various downstream adaptations. It gives LoRA several advantages,
    including
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '*Resource efficiency*—LoRA requires far less computational power than traditional
    full-model fine-tuning, making it more accessible for adapting large models.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Preservation of generalization*—LoRA maintains the base LLM’s generalization
    abilities by not altering the original pretrained weights, while allowing specialization.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Faster adaptation*—The process is quicker due to fewer updated parameters,
    enabling rapid deployment of adapted models.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability*—LoRA is particularly effective for large models, where full-model
    fine-tuning may be impractical due to resource constraints.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoRA is a cost-effective and efficient method for language models that allows
    fast switching between tasks. QLoRA is a variant of LoRA that further reduces
    the number of parameters by quantizing the low-rank matrices and can achieve up
    to 99% parameter reduction (via implementing an 8-bit optimizer for quantization),
    while maintaining or improving the model’s performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quantization is another technique that reduces the memory and computation requirements
    of the model by representing the parameters with fewer bits. Quantization of a
    model means reducing the precision of the model’s parameters, such as weights
    and biases, from high-precision floating-point numbers (32 bit or 16 bit) to low-precision
    numbers (8 bit or 4 bit). This can reduce the model size and speed up the inference
    but may also affect the model accuracy.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is especially useful for LLMs, which can have billions of parameters
    and require a lot of memory and computation. By quantizing the model, the deployment
    and inference of the model can be more efficient and scalable. For example, DistilBERT
    is a quantized version of BERT, an LLM for NLP. It has 40% fewer parameters than
    BERT but retains 97% of BERT’s performance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'At face value, quantization is similar to LoRA, as both aim to improve the
    efficiency and scalability of LLMs. Still, they are very different in their approaches
    and tradeoffs:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: LoRA reduces the number of trainable parameters by freezing the pretrained model
    weights and injecting low-rank matrices into each layer. This allows for faster
    fine-tuning and adaptation to new tasks. LoRA also preserves the full precision
    of the model weights, which means it does not reduce the model’s memory footprint
    or inference latency.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization reduces the model’s memory and computation requirements by representing
    the parameters with fewer bits, such as INT4\. This characteristic allows for
    smaller model sizes and faster inference, but it also introduces quantization
    errors and noise, which can degrade the model performance. Quantization also requires
    careful calibration and optimization to minimize the loss of accuracy and robustness.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantized LoRA (QLoRA) is another technique that aims to improve the parameter
    efficiency of fine-tuned LLMs. It extends LoRA by adding quantization to the process.
    This means that the LoRA adapters’ weights are quantized to a lower precision,
    such as 4 bit, which greatly shrinks the memory size of the model. The main benefit
    of QLoRA is its ability to balance performance and memory efficiency, making it
    a suitable option for scarce resources. Despite the decreased precision, QLoRA
    has been proven to keep a similar level of effectiveness to its nonquantized version,
    LoRA, in different tasks. This makes QLoRA an attractive choice for those who
    want to use powerful language models without the high computational costs.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main tradeoffs between LoRA and QLoRA are related to the balance of performance,
    memory efficiency, and computational resources. LoRA achieves a good balance between
    performance and efficiency, while QLoRA maximizes memory savings, which can be
    crucial for some use cases. The choice between the two would depend on the task’s
    specific needs and the deployment environment’s limitations. Experimentation is
    important to determine which method best fits your needs. If high accuracy is
    very important and computational resources are sufficient, LoRA might be the best
    choice. If memory efficiency is more important, then QLoRA would be better, especially
    if a small decrease in performance is acceptable for the application.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Teaching new knowledge using fine-tuning
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Often, there is a misconception that fine-tuning teaches the model new knowledge
    (or information). This is not correct. SFT does not exactly teach new knowledge
    to a model in the traditional sense. A fine-tuned model’s knowledge is limited
    to what was present in its pretraining data until its last update. It does not
    acquire new external knowledge during the fine-tuning process. Instead, it refines
    and adapts the model’s existing knowledge and capabilities to perform better on
    specific tasks or in certain domains. All SFT is doing is refining existing knowledge
    as outlined here:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '*Using pretrained knowledge*—In SFT, the model has already been trained on
    a large, diverse dataset. This initial training provides a broad base of general
    knowledge and language understanding.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Focusing on specific tasks*—During SFT, the model’s preexisting knowledge
    is honed to be more effective for specific tasks. For instance, if you fine-tune
    a language model on medical texts, the model becomes more adept at understanding
    and generating language related to medicine. Still, it does not necessarily teach
    new facts about medicine.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adjusting weights for relevance*—Fine-tuning effectively adjusts the model’s
    internal parameters (or weights) to make certain features or patterns more prominent
    when making predictions or generating text. This process makes the model more
    sensitive to the nuances of the specific data it was fine-tuned on.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SFT tailors a model’s existing knowledge and capabilities to be more effective
    for specific applications rather than teaching it new, external knowledge. The
    process involves adjusting the model’s internal understanding and response generation
    mechanisms to better align with the characteristics of the fine-tuning data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 9.7 RLHF overview
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) is a sophisticated ML approach
    that combines reinforcement learning (RL) and supervised learning. Unlike traditional
    RL, which relies on predefined reward functions, RLHF integrates human judgment
    into learning by asking humans to evaluate the agent’s behavior and provide feedback,
    such as ratings, preferences, or suggestions. This feedback helps the agent to
    improve its performance and align with human values or preferences. Specifically,
    it increases helpfulness and truthfulness in the generation, while mitigating
    harm and bias. OpenAI’s Instruct models, now the default models, are examples
    of models powered by RLHF and deployed at scale [10]. Anthropic, another AI startup
    founded by former OpenAI employees, aims to build LLMS such as Claude that are
    reliable, interpretable, and steerable. They have published their approach to
    RLHF [11], including associated human preference data [12].
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: RLHF is particularly valuable when defining an explicit reward function is challenging
    or where human preferences, subjective judgments, and nuances are crucial. It’s
    used in NLP tasks such as conversation and content generation, where subjective
    quality matters. RLHF aids in content moderation on social media by understanding
    context-specific nuances. Personalized recommendation systems help tailor suggestions
    to individual tastes. It’s valuable for socially acceptable and comfortable robotics
    and human–computer interaction behaviors. Ethical decision-making guides AI in
    aligning with human values. RLHF enhances AI’s performance in complex games, creative
    endeavors such as art and music, and healthcare for personalized medical decisions.
    These applications highlight RLHF’s role in aligning AI with the complex and subjective
    nature of human preferences and judgments.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: RLHF can improve LLMs’ performance, alignment, and diversity on various tasks,
    such as text generation, summarization, or dialogue. Instruct models are base
    LLMs that have been fine-tuned using the RLHF approach, and as shown in figure
    9.15, they significantly outperform the base LLMs [13].
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F15_Bahree.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 RLHF-trained models (PPO and PPO-ptx) significantly outperform base
    LLM models.
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  Base models are original pretrained models that have not been aligned
    with specific values and are not generally suitable for production use. For a
    reminder on the categories of LLM, see section 2.4 in chapter 2.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The RLHF framework teaches a model to perform tasks as humans would want, using
    human feedback as a guide. This method is especially relevant in fields where
    the desired output is subjective or highly context dependent, such as NLP, content
    generation, and decision-making systems. The key phases that comprise RLHF are
    outlined in figure 9.16.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F16_Bahree.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 RLHF fine-tuning approaches to aligning language models [13]
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These components work in tandem to create a robust learning system where a
    model can learn complex, human-centric tasks beyond the capabilities of traditional
    ML approaches. Integrating human feedback is key to bridging the gap between algorithmic
    decision-making and human judgment. Let’s look at the RLHF phases in more detail:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '*Supervised fine-tuning*—This phase involves training the model on a dataset
    of human-generated examples. These examples demonstrate the desired outcomes or
    behaviors, providing a baseline for the model to learn from. It helps the model
    understand the context and nuances of tasks as humans interpret.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reward modeling*—In this step, a separate model, known as the reward model,
    is trained to predict the quality of outputs generated by the primary model. The
    reward model is trained using human judgments, often involving ratings or preferences
    between different outputs. This model effectively translates subjective human
    evaluations into quantitative feedback that the primary model can use for learning.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Proximal policy optimization (PPO)*—PPO is an RL algorithm that iteratively
    improves the primary model’s policy (decision-making process). The algorithm updates
    the model’s policy to maximize the rewards predicted by the reward model. PPO
    is chosen for its stability and efficiency in handling large and complex models.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human feedback loop*—This loop involves continuous input from human evaluators
    who assess the quality of the model’s outputs. The feedback is used to train the
    reward model further, creating a dynamic learning environment where the model
    adapts to evolving human preferences and standards. The loop ensures that the
    model remains aligned with human expectations and can adapt to changes.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  PPO-ptx [13] is an adaptation of the PPO algorithm tailored for fine-tuning
    RLHF. It integrates a reference to the original LLM to maintain performance, while
    aligning the model’s outputs with human preferences. This approach helps mitigate
    the alignment tax, ensuring the LLM remains effective and diverse in its outputs
    after training. Essentially, PPO-ptx balances the model’s pretraining knowledge
    with the new feedback to create a high-performing LLM aligned with human values.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: RLHF might seem like the silver bullet in many ways, but enterprises must be
    aware of some challenges. Let’s explore these.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.1 Challenges with RLHF
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RLHF is a powerful technique for teaching models complex tasks, but it has many
    practical challenges and limitations for enterprises. An RLHF system needs a lot
    of human preference data, which is hard to get because it involves other people
    who are not part of the training process. How well RLHF works depends on how good
    the human annotations are, which humans can write, such as when they adjust the
    initial LLM in InstructGPT or provide ratings of how much they like different
    outputs from the model. Some of these challenges are
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '*Technical complexity*—Implementing RLHF requires advanced skills and knowledge
    in ML, RL, and NLP. It also involves complex setup and maintenance processes,
    such as configuring the model architecture, reward systems, and feedback mechanisms.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computationally intensive*—RLHF models need a lot of computational resources,
    such as GPUs and servers, which can be expensive. They also depend on the quality
    and quantity of human feedback, which can be hard to obtain and process. From
    a practical viewpoint, a lot of the human feedback is from contract workers (or
    gig workers) on crowdsourcing platforms where getting the right qualified people
    in certain domains might be challenging. Moreover, ensuring a diverse and unbiased
    dataset for training can be challenging and computationally heavy.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not scalable*—RLHF models are difficult to scale for large-scale applications,
    requiring continuous human feedback and increasing computational resources. They
    are also hard to adapt to different domains or changing data environments, resulting
    in limited adaptability and customization.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quality*—RLHF models are prone to bias, as they reflect human feedback providers’
    subjective opinions and potential prejudices. Ensuring ethical use and unbiased
    outputs is a major concern. Maintaining a consistent quality of human feedback
    can be difficult, as human judgment can vary and affect the model’s reliability
    and performance. When trying to build a helpful model that avoids harm, there
    is an inherent tension between those two dimensions. Providing too many polite
    responses, such as “Sorry, I am an AI model, and I cannot help you with that,”
    or something similar, limits the model’s usefulness. Organizations must balance
    and mitigate this using additional guidance, training, and other ML techniques
    to create synthetic data where possible.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost*—RLHF models are costly to implement and operate. Costs include infrastructure,
    computational resources, data acquisition, and hiring skilled professionals. There
    are also ongoing operational costs related to data management, model updates,
    and continuous feedback integration. These costs can be substantial, especially
    for large-scale implementations.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data*—It is hard to produce good human text that answers specific prompts
    because it usually means paying part-time workers (instead of product users or
    crowdsourcing). Luckily, the amount of data needed to train the reward model for
    most uses of RLHF (~50k preference labels) is not that costly. However, it is
    still more than what academic labs can usually afford. There is only one big dataset
    for RLHF on a general language model (from Anthropic) and a few smaller datasets
    for specific tasks (such as summarization data from OpenAI). Another problem with
    data for RLHF is that human annotators can disagree a lot, which makes the training
    data very noisy without a true answer.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RLHF offers advanced capabilities in teaching models to perform complex tasks;
    however, its adoption in enterprise settings is hindered by technical complexity,
    resource demands, scalability challenges, ethical considerations, and high costs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, these barriers make it difficult for many organizations to
    implement and sustain RLHF systems in their operations practically. On the other
    hand, those who can implement this, especially some of the technical companies
    such as OpenAI and Anthropic, can benefit from it. Let’s see how we can scale
    an RLHF implementation.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.2 Scaling an RLHF implementation
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scaling an RLHF implementation for LLMs involves a multifaceted approach that
    balances efficiency, diversity, and quality control. First, automating data collection
    and implementing efficient feedback mechanisms are crucial for handling large
    volumes of data and feedback. Automated systems can gather data from various sources
    or through interfaces designed for efficient human interaction.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Using a large, diverse pool of human evaluators is essential for capturing a
    wide range of perspectives, helping the model to be more robust and less biased.
    To ensure the feedback is informative, intelligent sampling strategies, such as
    active learning, can be used to identify and prioritize the most valuable instances
    for evaluation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization and distribution of tasks among multiple evaluators can significantly
    speed up the feedback process. The system can handle large-scale data processing
    and model training with scalable infrastructure.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Implement quality control measures, such as cross-validation among evaluators
    and algorithms, to detect biases and maintain the quality and consistency of feedback.
    Regular monitoring and evaluation of the model’s performance can help you understand
    the effects of RLHF and guide continuous improvement.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ethical considerations and bias mitigation are crucial. Ensuring that
    feedback does not reinforce harmful stereotypes and actively addressing potential
    biases is vital for developing fair and responsible models. Overall, scaling RLHF
    for LLMs requires a comprehensive approach that integrates technical, logistical,
    and ethical strategies, aiming for a system that effectively incorporates human
    feedback into the model’s learning process.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model adaptation should be anchored in a set of use cases, and it should be
    the last resort for enterprises trying to improve the model on those tasks.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering and RAG must work in conjunction with fine-tuning in a stacked
    manner.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When done correctly, fine-tuning has a high upside from enhanced efficiency
    and possible cost savings.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning has a high cost, and you should be aware of challenges such as the
    need for task-specific data, computational resources, performance evaluation,
    and ethical considerations.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning should be done in conjunction with evaluations and will often require
    multiple iterations to obtain a model ready for production deployment.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of metrics for evaluating fine-tuned models largely depends on the
    model’s specific application and objectives.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main model adaptation techniques that are more cost-efficient are supervised
    fine-tuning (SFT), parameter efficient fine-tuning (PEFT), and low-rank adaptation
    (LoRA).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
