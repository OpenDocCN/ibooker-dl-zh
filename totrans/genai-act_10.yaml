- en: 9 Tailoring models with model adaptation and fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Basics of model adaptation and its advantages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to fine-tune an LLM using both SDK and GUI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for evaluation criteria and metrics for fine-tuned LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy a fine-tuned model for inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaining insight into key model adaptation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we explore the intricate world of large language models (LLMs), a key aspect
    that stands at the forefront of practical artificial intelligence (AI) deployment
    is the concept of model adaptation. In the context of LLMs, model adaptation involves
    modifying a pretrained model such as GPT-3.5 Turbo to enhance its performance
    on specific tasks or datasets. This process is important because while pretrained
    models offer a broad understanding of language and context, they may only excel
    in specialized tasks with adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Model adaptation encompasses a range of techniques, each designed to tailor
    a model’s vast general knowledge to particular applications. The path of model
    adaptation is not just about enhancing performance but about transforming a generalist
    AI model into a specialized tool adept at handling the nuanced demands of enterprise
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises, adaptation enables LLMs to handle industry-specific jargon,
    comply with regulatory standards in some cases, and align with businesses’ unique
    operational contexts. This relevance is key to deploying AI solutions that add
    value to enterprise environments. It is important to note that most organizations
    should refrain from jumping directly to fine-tuning. We need to consider this
    as a continuum of various techniques, stacked on and complementing one another;
    in addition, they are not mutually exclusive. We have already seen many such techniques
    in the book. For most organizations, if there is a SaaS offering such as a copilot
    in the application they are already using, that is the best place to start. This
    application uses the SaaS out-of-the-box offerings of GenAI implementation and
    has the maximum ROI.
  prefs: []
  type: TYPE_NORMAL
- en: In scenarios where a SaaS solution is neither available nor suitable, and a
    PaaS approach is preferred, it is advisable to begin with prompt engineering as
    the foundational step and expand on it. When we need to ground the model generations
    using our data, we will use retrieval-augmented generation (RAG) combined with
    prompt engineering, as shown in figure 9.1\. When using advanced frontier models
    such as GPT-4, this combination solves 95% of enterprise business cases. At some
    point on this continuum, enterprises will reach a point where there is a need
    to fine-tune a model for specific requirements. Even if we fine-tune, this doesn’t
    eliminate the need to use prompt engineering and RAG. We will see this case in
    the chapter as we fine-tune and use a model that still needs prompt engineering
    to obtain desired results.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter outlines various model adaptation techniques, helping us to understand
    their challenges, see how enterprises can adopt applications, and finally fine-tune
    and deploy a model in production. Let’s start by understanding what model adaptation
    is.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 Model adaptation technique progression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 9.1 What is model adaptation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model adaptation is adjusting an LLM to perform better on a specific task in
    a specific domain, and it is quite similar to transfer learning. Both approaches
    involve using a pretrained model as a starting point. These models have typically
    been trained on large datasets and have developed a robust understanding of various
    features and patterns. The key idea in model adaptation and transfer learning
    is to take a model trained on one task and apply it to a different but related
    task. This saves time and resources that would otherwise be required to train
    a model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, LLMs are trained on a large amount of general text data, which gives
    them a broad understanding of language. Still, they may not be suitable for certain
    tasks or domains requiring specialized knowledge or vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea behind model adaptation is that the knowledge learned from the
    original task can aid performance on the new task. At a high level, there are
    two broad categories of model adaptation—domain and task:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Domain adaptation*—If you have a model trained in one domain (e.g., general
    news articles) and want it to perform well in a different but related domain (e.g.,
    medical news articles), you will use domain adaptation techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Task adaptation*—If you have a model trained for one task (e.g., sentiment
    analysis) and you want it to perform a new but related task (e.g., emotion detection),
    task adaptation techniques can be utilized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, an LLM trained on Wikipedia articles might perform poorly on medical
    questions or legal documents. Therefore, model adaptation is needed to fine-tune
    the LLM on a smaller, task-specific or domain-specific dataset, which helps the
    model learn the relevant patterns and features for the target task or domain.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 Basics of model adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model adaptation in LLMs involves refining a pretrained model to better fit
    specific tasks or data. This concept can be broadly divided into two main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Full fine-tuning*—This approach updates all LLM parameters. It involves comprehensive
    retraining of the model on new data, making substantial changes to its learned
    patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Low-rank adaptation*—Unlike full fine-tuning, low-rank adaptation focuses
    on modifying a smaller set of the model’s parameters. This method introduces trainable
    matrixes into each LLM layer, effectively reducing the number of parameters that
    need adjustment. This section will primarily focus on this category of model adaptation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s delve into key techniques underpinning model adaptation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transfer learning*—This machine learning (ML) strategy involves applying a
    model trained for one task to a different but related task. For instance, a model
    trained on English text might be adapted to work with French text. Transfer learning
    is about using knowledge from one domain to improve performance in another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fine-tuning*—Fine-tuning continues training a pretrained model on a new, usually
    smaller, and more specialized dataset. It subtly adjusts the model’s parameters
    to align its knowledge with the new task or data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the task, data, and the specific LLM, different model adaptation
    techniques can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Task-specific modules*—This technique adds a module (such as a classifier
    or decoder) to the LLM, tailored to a particular task. Both the module and the
    LLM are then fine-tuned on task-specific data. This allows the LLM to learn the
    intricacies of the specific task, while maintaining its broad linguistic knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Low-rank adaptation (LoRA)*—LoRA applies a low-rank approximation to the LLM
    and fine-tunes only these components. This method reduces the number of parameters
    needing adjustment, while maintaining the model’s performance and flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Federated learning*—This approach fine-tunes the LLM across multiple distributed
    datasets, allowing the model to learn from diverse data, while upholding privacy.
    For example, federated learning could adapt BERT for medical text analysis using
    data from various hospitals, resulting in a specialized version such as Med-BERT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No single technique is universally applicable—experimentation is key. Understanding
    these nuances is crucial for effectively using model adaptation and fine-tuning.
    These methods embody transfer learning principles and provide practical ways to
    enhance AI models’ performance and applicability in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 Advantages and challenges for enterprises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model adaptation is increasingly crucial for enterprises in some specific industries
    and scenarios. It offers substantial efficiency, competitiveness, and innovation
    benefits. By employing adapted AI models, businesses can achieve more accurate
    results in less time and with fewer resources than by developing models from scratch.
    For example, in highly specialized domains (e.g., medical and pharmaceutical),
    where the margin of error needs to be closer to zero, fine-tuning a model for
    the specific tasks is one of the few ways to achieve the desired outcome. Other
    specialized areas, such as complex finance details (e.g., fraud detection) and
    legacy code migration (e.g., Cobol, etc.), are high-value examples where enterprises
    would want to consider fine-tuning a model.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, enterprises can also perform better on specialized tasks and gain
    a competitive advantage, depending on the use case. This is especially true in
    cases where enterprises deal with unique datasets and require models to understand
    their specific business context. Model adaptation enables customization, improving
    accuracy and relevance in sentiment analysis, market trend prediction, or personalized
    customer interactions. By using models adapted to their specific needs, businesses
    can gain insights and increase efficiency, which will provide them with a competitive
    advantage in their market.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises can enhance efficiency and cost savings by reducing resource requirements
    and resource needs. Fine-tuning existing models requires significantly less computational
    power and data compared to training models from the ground up, which results in
    lower costs and quicker deployment times. For example, training Llama 2’s 70B
    parameter model took many months and 1,720,320 GPU hours, compared to fine-tuning
    a GPT-3.5 Turbo model, which takes only a few hours.
  prefs: []
  type: TYPE_NORMAL
- en: Model adaptation comes with challenges, and several key areas must be considered.
    First, task-specific data is crucial. It is essential to have sufficient data
    to fine-tune an LLM, ensuring that this data is clean, consistent, and representative
    of the specific task. Depending on the task and LLM characteristics, this data
    may require preprocessing, augmentation, or labeling. Determining how much data
    for fine-tuning is enough can be a nuanced process, as it varies based on several
    factors; at a minimum, it is a few hundred to thousand examples, depending on
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Determining adequate data for fine-tuning models such as OpenAI’s GPT-3.5 depends
    on various factors. The complexity and specificity of the task heavily influence
    data requirements, with more complex tasks requiring more data. However, the quality
    of data is crucial and often outweighs the quantity. Larger models such as GPT-3.5
    can benefit from more data due to their extensive capacity, but they also can
    learn effectively from smaller, high-quality datasets. Organizations typically
    start with a baseline dataset and adjust it based on the model’s performance,
    which is continuously monitored for signs of overfitting or underfitting. Practical
    constraints such as computational resources and time also play a role in determining
    the dataset size. The experience and expertise of data scientists often guide
    the decision. Comparative analysis and continual evaluation are involved in finding
    the optimal balance of data quantity and quality for the specific task requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant challenge is related to computational resources and costs.
    Fine-tuning LLMs can be resource intensive and costly, often requiring substantial
    processing power (specifically GPUs) connected with high-speed memory. To manage
    this, it might be necessary to utilize cloud services, invest in specialized hardware,
    or employ distributed systems. Additionally, the cost of accessing pretrained
    LLMs can vary, depending on the provider and licensing agreements, which can add
    to the overall expense.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and generalization are also critical considerations. Evaluating
    the performance of a fine-tuned LLM is imperative; it involves comparing it to
    other models or established baselines, which ensures that the fine-tuned LLM does
    not overfit the training data and can generalize well to new or unseen inputs.
    We cover evaluations later in this chapter, and more details on benchmarks and
    associated tools are covered in chapter 12.
  prefs: []
  type: TYPE_NORMAL
- en: The ethical and social implications of using fine-tuned LLMs must be addressed
    as well. This includes understanding potential risks and biases, such as concerns
    related to data privacy, model fairness, and social effects. Adhering to appropriate
    guidelines, standards, or regulations is necessary to ensure the ethical and responsible
    use of fine-tuned LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, finding the right talent is critical. The need for specialized talent
    and expertise is a significant factor in successfully fine-tuning LLMs, which
    includes individuals who deeply understand ML, natural language processing (NLP),
    and the specific architecture of LLMs. These experts must be skilled in various
    areas, such as data preparation, model architecture design, training strategies,
    and performance evaluation. The need for skilled personnel adds another layer
    of challenges to the already complex process of LLM fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 When to fine-tune an LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is a technique to improve a model’s performance on a specific task.
    However, it should be the last option and used only after applying other techniques,
    such as prompt engineering and RAG. These techniques complement each other and
    should be stacked for the best output, even when using fine-tuned models. As we
    saw in earlier chapters, prompt engineering and RAG are not mutually exclusive
    but are complementary and should be stacked, even when fine-tuning. This stacked
    approach gives the best outputs, even when using fine-tuned models.
  prefs: []
  type: TYPE_NORMAL
- en: Once we decide to fine-tune a model, we prepare the dataset needed for training
    and start the fine-tuning process, which can take from a few hours to a few days.
    After training, we evaluate the fine-tuned model against the base model and the
    specific task’s baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use an example to help us fine-tune and understand various aspects. Say
    we want to adapt a model to respond with emojis—a bot that can understand what
    we are asking but respond only using emojis. We will call this EmojiBot. We want
    to fine-tune GPT-3.5 Turbo and make it an EmojiBot. But to show that these emojis
    are different and specialized for a task, we don’t want the emojis that we would
    expect to see, say, in a chat application, on social media, or in our texts. Rather,
    we want the ones that follow the format used by Microsoft Teams.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 shows the high-level flow for fine-tuning. First, we identify a task
    that would benefit from fine-tuning (such as EmojiBot). We identify which characteristics
    fall short of the task and create evaluation criteria. We then compare the default
    models’ performance against our needs. If they perform well, we establish a baseline
    and curate the dataset required for fine-tuning. The amount and format of data
    depend on the model; we’ll cover the details later. We obtain a fine-tuned model
    after training, which can take hours or days, depending on the task. Next, we
    must evaluate it against the base model and the baseline for the specific task
    using qualitative and quantitative measures.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 Fine-tuning end-to-end flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is quite common and almost expected that the first fine-tuned model will
    be worse than the default model. Usually, finding a suitable deployment model
    takes 10–12 training iterations. Each iteration requires tweaking the training
    data to address weak areas, which can take hours to days. It’s a time- and effort-consuming
    process that should be one of the last steps.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Fine-tuning enhances the model’s performance on tasks similar to those
    outlined in the fine-tuning dataset. This process might manifest as improved accuracy,
    more relevant responses, or a better understanding of domain-specific language.
    Improved performance in terms of cheaper or faster models is a side advantage
    and not something guaranteed. One way to achieve this is to fine-tune a smaller
    model, such as GPT-3.5 Turbo, on a specific task to improve it instead of using
    a more expensive and powerful model, such as GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have identified a task that makes sense to fine-tune—that is, an
    EmojiBot where we want to respond in emojis but in a certain pattern—let’s examine
    the steps needed to fine-tune an LLM such as GPT-3.5 Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Key stages of fine-tuning an LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we want to fine-tune a model for an identified task, as outlined later
    in figure 9.6, section 9.3.5, there are five key stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Choosing a model and fine-tuning method*—To fine-tune a language model, it
    is necessary to choose a foundation model that suits the task and data. Various
    models are available, such as GPT, BERT, and RoBERTa. Consider factors such as
    the model’s suitability for the task, input/output size, dataset size, and technical
    infrastructure. Fine-tuning methods can vary based on the task and data, such
    as transfer learning, sequential fine-tuning, or task-specific fine-tuning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data curation*—This stage involves preparing a task-specific dataset for fine-tuning
    and largely involves preparing and preprocessing the dataset. This process often
    includes data cleaning, text normalization (e.g., tokenization), and converting
    the data into a format compatible with the LLM’s input requirements (e.g., data
    labeling). It is essential to ensure that the data represents the task and domain
    and covers a range of scenarios the model is expected to encounter in production.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fine-tuning*—This stage is the actual process of fine-tuning and involves
    training the pretrained LLM on the task-specific dataset. The training process
    involves optimizing the model’s weights and parameters to minimize the loss function
    and improve its performance on the task. The fine-tuning process may involve several
    rounds of training on the training set, validation of the validation set, and
    hyperparameter tuning to optimize the model’s performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Evaluating*—Once the fine-tuning process is complete, we must evaluate the
    model’s performance on a test dataset. This helps to ensure that the model is
    generalizing well to new data and performing well on the specific task. Common
    metrics used for evaluation include accuracy, precision, recall, F1 score, Bilingual
    Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation
    (ROUGE), and so forth. This topic is covered later in detail in section 9.3.2\.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Deployment (inference)*—Once the fine-tuned model is evaluated and we are
    happy with its performance, it can be deployed to production. The deployment process
    may involve integrating the model into a larger system, setting up the necessary
    infrastructure, and monitoring the model’s performance in real-world scenarios.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a basic concept of model adaptation and when to fine-tune,
    let’s see how to fine-tune.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Fine-tuning OpenAI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we’ll use an example to fine-tune OpenAI’s GPT-3.5 Turbo model. Currently,
    for OpenAI, only GPT-4, GPT-3.5 Turbo, GPT-3 Babbage (Babbage-002), and GPT-3
    (Davinci-002) are available for fine-tuning. Several OSS LLMs, such as Meta’s
    Llama 2 and G42’s Falcon, can be fine-tuned. In our case, the book’s GitHub repository
    ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)) contains complete code
    samples and screenshots that we use and show how to fine-tune OpenAI GPT-3.5 Turbo.
    To make this as real for organizations as possible, we will show the process by
    using both Azure OpenAI and OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: We want to fine-tune GPT-3.5 Turbo and make it an EmojiBot, where the model
    responds in emojis only. However, as we outlined earlier, we want emojis to follow
    the format used by Microsoft Teams.
  prefs: []
  type: TYPE_NORMAL
- en: In Microsoft Teams, the text in parentheses, such as `(dog)`, renders the relevant
    emojis. We will fine-tune the model to respond to this text, which represents
    the specific task we want the model to improve. To understand all the different
    options and the corresponding text in Teams, see [https://bit.ly/TeamEmojis](https://bit.ly/TeamEmojis).
    Given that we have a task, let’s start preparing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Preparing a dataset for fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have reached a point where we have identified a task for which
    fine-tuning would make sense, we need to create a dataset of examples required
    to fine-tune. We need to create two sets of datasets: one for training and another
    for validation. A validation dataset is a subset of data used to evaluate the
    performance of a fine-tuned model on the target task. It is different from the
    training dataset, which is used to update the model’s parameters, and the test
    dataset, which is used to measure the final accuracy of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: A validation dataset is important for fine-tuning LLMs because it helps us to
    avoid overfitting, which is when the model learns the specific patterns of the
    training data and fails to generalize to new data. Using a validation dataset,
    you can monitor the model’s progress and adjust the learning rate, the number
    of epochs, or other hyperparameters to optimize the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: These examples should show different ways to solve the problem and the results
    of each method. We also need to identify shortcomings using a base model, such
    as inconsistent performance on edge cases, inability to fit enough shot prompts
    in the context window to steer the model, high latency, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: It is highly recommended that a validation dataset be used to measure the effectiveness
    of fine-tuning. The training and validation datasets are in the JSONL format,
    with each line containing a JSON object with a text key for input text and a target
    key for desired output text.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuned models are directly correlated with high-quality training data. Different
    models require varying amounts of training data. For effective training, we need
    hundreds to thousands of curated data examples. Although the API requires a minimum
    of 10 examples, having more is generally better. Ten examples aren’t enough to
    influence LLMs such as GPT-3.5 Turbo in any significant way.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI recommends having at least 50 good examples to train our model. They
    also recommend more good examples for better-fine-tuned models than bad ones,
    as those examples can negatively affect the model. Consequently, it is advisable
    only to use the best ones from your internal data. The following listing shows
    an example JSONL file for chat data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 JSONL example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the model is being shown how to respond using emojis formatted
    in a certain pattern, such as `(sadkoala)`, `(tired)`, and `(like)`.
  prefs: []
  type: TYPE_NORMAL
- en: Basic checks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before fine-tuning, it’s important to perform basic checks on the training data
    to avoid wasting time and resources. These checks can include data readability,
    formatting validation, lightweight analysis for missing pairs, and token length.
  prefs: []
  type: TYPE_NORMAL
- en: We validate the data file by loading and reading it using the `basic_checks()`
    function. It takes a filename as input and returns the number of messages found.
    The messages must be in the chat completion format for fine-tuning GPT-3.5 Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.2 Dataset validation: Basic checks'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Opens the file in read-mode'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Loads each line of the file as a JSON object and stores it in a list'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prints the first example from the dataset and helps visually check whether
    things intuitively look OK'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loops through the messages in the first example and prints each one'
  prefs: []
  type: TYPE_NORMAL
- en: Format checks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once we have done the basic checks, the next step is to check the file for the
    format and ensure it is structured properly before processing it further. This
    is an important step, mainly because even if the format is incorrect, we won’t
    get an error when we start the training job, but the resulting model will be very
    poor, and we will only realize this posttraining when we deploy. To avoid much
    of this trouble, it is highly recommended that we check for formats.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 shows `format_checks()`, which checks for chat completion format
    and pairing, with dataset and filename as its two arguments. It catches most errors
    but not all. The function iterates over each example in the dataset and checks
    for data type checks, the presence of message lists, and message keys. It validates
    that it has the relevant roles and content validation. This function also helps
    debug data-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.3 Dataset validation: Checking for format'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we should also understand how the dataset performs when it comes to
    simple data distributions, token counts, and costs.
  prefs: []
  type: TYPE_NORMAL
- en: Note  The token count is important, not just for cost. If it is larger than
    the maximum number of tokens the model can handle, it will be truncated without
    warning. Knowing this up front is very helpful.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows how we can finish doing the checks on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.4 Dataset validation: Cost estimation and basic analysis'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 9.3.2 LLM evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluating LLMs is important for ensuring their quality, reliability, and fairness.
    However, evaluating LLMs is complex, as it involves multiple dimensions and challenges.
    Maintaining diverse automatic metrics can help efficiently track model improvements
    during adaptation cycles, while reducing costly manual reviews. Metrics should
    be customized to each adapted model’s use cases and business needs. Continuous
    logging from production systems enables the evaluation of real-world performance
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking against baselines is an essential step in evaluating fine-tuned
    GPT models. It involves comparing the performance of the fine-tuned model with
    a preestablished standard or baseline model. This baseline could be the model’s
    performance before fine-tuning or a different model known for its proficiency
    in a similar task. The purpose of this comparison is to quantify the improvements
    brought by fine-tuning. For instance, a fine-tuned model might be benchmarked
    against a standard translation model in a language translation task to assess
    translation accuracy or fluency improvements. This process helps in understanding
    the efficacy of fine-tuning and identifying areas where the model has improved
    or still needs enhancement.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation criteria
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When preparing the fine-tuning dataset, we should also define the evaluation
    criteria. When fine-tuning, the evaluation process begins by establishing clear
    criteria critical for assessing the performance and efficacy of the model in its
    intended application. These criteria often include relevance, coherence, accuracy,
    and language fluency (table 9.1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 Fine-tuning evaluation criteria
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Evaluation criteria | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Relevance  | Gauges how well the model’s responses or outputs align with
    the context and intent of the input. This is especially crucial in applications
    such as chatbots, where providing contextually appropriate responses is key to
    user satisfaction. Relevance is often assessed by examining whether the model
    can stay on topic and provide information or responses directly applicable to
    the queries or tasks.  |'
  prefs: []
  type: TYPE_TB
- en: '| Coherence  | Refers to the logical consistency of the model’s outputs. A
    fine-tuned model should generate contextually relevant, logically sound, and coherent
    text. This means the responses should follow a logical structure and narrative
    flow, making sense in the conversation or text context. Coherence is vital for
    maintaining user engagement and ensuring the model’s outputs are understandable
    and meaningful.  |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy  | This particularly comes into play when the model is used for
    tasks involving factual information, such as educational tools, informational
    bots, or any application where providing correct information is critical. Accuracy
    is measured by how well the model’s responses align with factual correctness and
    objective truth.  |'
  prefs: []
  type: TYPE_TB
- en: '| Language fluency  | Pertains to the grammatical and syntactical correctness
    of the model’s outputs. Even if a model is highly relevant, coherent, and accurate,
    poor language fluency can significantly detract from the user’s experience. This
    includes proper grammar, punctuation, and style, ensuring the text generated is
    correct and reads naturally to the end user.  |'
  prefs: []
  type: TYPE_TB
- en: Evaluating a fine-tuned GPT model using these criteria involves a combination
    of automated metrics, manual review, and user feedback, ensuring that the model
    meets the high standards required for its specific application.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing appropriate metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When fine-tuning models, selecting the right metrics for evaluation is crucial
    to accurately assessing the model’s performance and improvements [1]. After fine-tuning,
    these metrics indicate how well the model adapts to specific tasks or domains.
    They provide insights into various aspects of model performance, such as prediction
    accuracy, language quality, and task-specific effectiveness. Enterprises should
    look for automated metrics evaluation where possible and have a set of quantitative
    and qualitative metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitative metrics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several metrics help measure the overlap between model outputs and human reference
    texts. The next section will outline some of them (BLEU, ROUGE, METEOR, etc.).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: F1 score evaluates the accuracy tradeoff between precision and recall.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perplexity assesses model uncertainty/confidence for generated text.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Task completion is used for goal-oriented dialog systems and the percentage
    of successful task resolution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qualitative metrics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fluency*—Rating grammaticality and readability of outputs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coherence*—Logical consistency and narrative flow'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conciseness*—Avoiding repetitive and excessive text'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Factual accuracy*—Avoiding objective falsehoods'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The choice of metrics should align with the model’s intended application, whether
    translation, summarization, classification, or creative content generation. Metrics
    such as perplexity, BLEU score, ROUGE, F1 score, and human evaluation each offer
    a unique perspective on the model’s capabilities, helping to ensure a comprehensive
    and balanced evaluation of the fine-tuned model’s performance. Let’s look at each
    of these in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Perplexity*—This metric is a standard in language modeling, used to quantify
    how well a model predicts a sample. It measures the uncertainty of the language
    model in predicting the next token in a sequence [2]. A lower perplexity score
    indicates that the model is more confident and accurate in its predictions. This
    is particularly important in fine-tuning, as it can reflect how well the model
    has adapted to new styles or domains of text. It’s a crucial metric for assessing
    improvements in language generation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BLEU score (Bilingual Evaluation Understudy)*—The BLEU score evaluates machine
    translation quality by comparing it to reference translations. It counts matching
    word groupings and computes a score based on these matches. A higher BLEU score
    indicates better translation quality, but it has limitations and may not capture
    semantic accuracy or fluency [3].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ROUGE (Recall-Oriented Understudy for Gisting Evaluation)*—ROUGE is a metric
    for automatic summarization evaluation. It measures the overlap between computer-generated
    output and reference summaries to assess the summary’s quality. Different variations
    of ROUGE provide insights into aspects of the summary’s quality [4].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1 score*—The F1 score is useful in classification tasks such as sentiment
    analysis and topic categorization. It balances the tradeoff between precision
    and recall, providing a single measure of a model’s accuracy in categorizing or
    classifying text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human evaluation*—Despite the utility of automated metrics, human judgment
    remains crucial, especially for tasks that require subjective assessment, such
    as story generation, creative writing, and conversational agents. Human evaluators
    can provide insights into aspects such as the naturalness of the text, its appropriateness,
    creativity, and even the subtleties of humor or sarcasm. This qualitative evaluation
    complements quantitative metrics, offering a more holistic view of the model’s
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task-specific evaluation is essential to measuring a model’s performance in
    its intended application. It involves using different metrics and considerations
    based on the task. For instance, summarization models are evaluated using ROUGE
    scores and human summary coherence and informativeness assessments. Similarly,
    question-answering models are evaluated for accuracy and relevance to the given
    questions. This evaluation ensures the model performs well in general metrics
    and is effective and reliable for its specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Error analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Error analysis is a critical component of the evaluation process, involving
    a detailed examination of where and why the fine-tuned model is underperforming.
    This analysis helps identify patterns in the model’s mistakes, which can be categorically
    broken down into semantic errors, factual inaccuracies, or language inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a model consistently makes errors in understanding certain types
    of queries or generates responses with factual errors, this would be highlighted
    in error analysis. Understanding these error patterns is crucial for further refining
    the model and making targeted improvements. It also aids in understanding the
    model’s limitations and areas where it might require additional data or more sophisticated
    fine-tuning approaches. Now let’s get to fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that our dataset is ready and validated, we can kick off the fine-tuning
    process. There are two steps to perform when we need to fine-tune. First, we upload
    the dataset we worked on in the previous sections. When uploaded, each file gets
    a unique file ID that we need to save. This file ID is what we pass as one of
    the parameters to the fine-tuning job so it knows which file to use for which
    fine-tuning job.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the API or GUI to do this. We will see how to achieve this using
    Python SDK and Azure AI Studio. I won’t show all the steps in the GUI book, but
    those details are available in the accompanying GitHub repo at [https://bit.ly/GenAIBook](https://bit.ly/GenAIBook).
    Let’s start by using the SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning using the SDK
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following listing shows how to use the SDK and `files.create()` method,
    pass in the file name, and specify the purpose of the file (`fine-tune`).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 Uploading dataset for fine-tune
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This version (or later) is required for fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Environment variables with the connection details'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Dataset that we need to use for training'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this snippet, we obtain the following output, with the file ID
    we need to be aware of when we run the second step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After uploading our file, we must start the fine-tuning job. When using the
    SDK, this is done using the `fine_tunings.jobs.create()` method. This function
    needs the ID of the training dataset file from the previous steps and the model
    to use. In our case, we want to fine-tune GPT-3.5 Turbo, specifically the 0613
    model. We also specify how many epochs we need for fine-tuning. Finally, the `suffix`
    parameter is something we can use to help track and manage the fine-tuned model
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Starting the fine-tuning job
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This version (or later) is required for fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Environment variables with connection details'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The file ID you see will differ from this one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This snippet submits a finetuning job that gets queued up; depending on available
    capacity at the specific region and data center, the job will get executed. As
    a reminder, with Azure OpenAI, you can have multiple regions where fine-tuning
    is available. Our example shows the job ID from our API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the dataset size, the model we want to fine-tune, and the fine-tuning
    hyperparameters, the fine-tuning job can take a few hours. The fine-tuning jobs
    API has a function call `list()` that we can use to see all the fine-tuning jobs
    we have submitted.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.7 Listing all the fine-tuning jobs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: One example of this output is presented in listing 9.8\. We see that we have
    completed two fine-tuning jobs, as shown by the `succeeded` status; one job is
    currently in the status `running`, which means that it has one active fine-tuning
    job ongoing. The last fine-tuning job (`ftjob-367ee1995...`) we have just submitted
    as `pending` means that the job is queued up to run at some point in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 Output of fine-tuning jobs listing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For a specific fine-tuning job, we can also see the various events related to
    that job. The following listing shows an example of this, again using the ID of
    our newly submitted job (`ftjob-367ee1995...)`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.9 Listing events from a fine-tuning job
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output in this case is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can also poll to check the status of a job every few seconds and, using this,
    kick off another workflow. In this instance, this job ran for approximately two
    hours before finishing. For this, we need the `IPython` package, which can be
    installed in conda using `conda` `install` `ipython`, or if one is using pip,
    then via – `pip` `install` `ipython.`
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 Polling to check fine-tuning job status
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the model and the length of the queue to schedule the fine-tuning
    task, one fine-tuning job can take a few hours to finish. During this time, we
    get training metrics that help us understand how the training goes.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.4 Fine-tuning training metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As outlined earlier, the training can take a few hours; for more complex and
    bigger models, it can take a few days. The training during this time is not a
    black box; we can get details on key metrics during the process to get a high-level
    idea of what is happening. We have three key metrics that can be tracked—the training
    loss, mean token accuracy, and token accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two aspects of loss: training and validation loss. Training loss
    measures the difference between the model’s predictions and the actual outcomes.
    A lower loss means the model is more accurate and has less error. Lower loss values
    indicate better model performance, suggesting the model’s predictions are closer
    to the actual data.'
  prefs: []
  type: TYPE_NORMAL
- en: If we have a validation dataset (which is highly recommended), then we also
    have additional metrics that allow us to measure how the model is doing. The validation
    loss is a metric that measures the model’s error on the validation set, a portion
    of the dataset set aside to evaluate the model’s performance on new or unseen
    data. The validation loss is calculated by summing up the errors for each example
    in the validation set, using the same cost function as the training loss. The
    validation loss is usually measured after each epoch, a complete pass through
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 shows an example of the loss when we fine-tune using Azure OpenAI
    and the model performance during training. The graph in figure 9.3 showing the
    training loss for fine-tuning training results illustrates how well the model
    learns from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: We see the loss value for each training step, a batch of training examples.
    The *x*-axis is the step number, and the *y*-axis is the loss value. The graph
    shows that the loss decreases as the model trains on more data, indicating that
    it is improving its performance. However, the loss does not reach zero, which
    means the model still has some errors and cannot perfectly fit the data. This
    is normal, as overfitting the data can lead to poor generalization of new data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Training loss when fine-tuning GPT-3.5 Turbo
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To interpret the graph and determine whether the model is performing well, ideally
    for a good fit, we want both training and validation loss to decrease to stability
    with a minimal gap between the two, which indicates that the model is learning
    and generalizing well. If the training loss decreases while the validation loss
    increases, the model may be overfitting the training data and not generalizing
    well to new data. Finally, if both training and validation loss remain high, the
    model may be underfitting, which means it’s not learning the underlying patterns
    in the data well enough. The scale of the loss and the number of training steps
    must be considered. The model might need more training if the loss is still high
    or the validation loss has yet to stabilize. For those with an ML model experience
    or background, the overall approach for splitting between training and validating
    datasets and interpreting these metrics is very similar.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting behavior is that the data in the loss graph fluctuates, indicating
    that the loss value can vary depending on the samples in each batch. It is normal
    for the model to be noisy; however, in fine-tuning, the model learns and improves
    its performance as long as the loss decreases over time.
  prefs: []
  type: TYPE_NORMAL
- en: To find whether the fine-tuning is good, we would typically look for a low and
    stable validation loss close to the training loss. The thresholds for what would
    be considered good loss values are subjective and will vary depending on the task’s
    complexity and the nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Mean token accuracy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mean token accuracy measures how well a fine-tuned model correctly predicts
    each token in the output sequence that the model generates or predicts during
    training. It is reflected as a percentage, that is, the percentage of tokens the
    model predicts correctly in a dataset. For example, if the mean token accuracy
    is 90%, it means that on average, the model correctly predicts 90% of the tokens.
    This is an average calculated by dividing the number of correctly predicted tokens
    by the total number of tokens in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the loss for mean token accuracy, we have two metrics: one for the
    training and the other for validation (assuming one has provided a validation
    dataset). Figure 9.4 shows the mean token accuracy of a fine-tuning job for training
    and validation. The training mean token accuracy is the average accuracy of the
    model’s predictions on the training data. It measures how well the model learns
    from the training data and adapts to it. A high training mean token accuracy suggests
    that the model learns effectively from the training data. In contrast, the validation
    mean token accuracy is the average accuracy of the model’s predictions on the
    validation data. It measures how well the model generalizes to new data it has
    not seen before. A high validation mean token accuracy suggests that the model
    does not overfit the training data and can generalize well to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 Training mean token accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The difference between the two metrics can help identify whether the model is
    overfitting to the training data. Suppose the training mean token accuracy is
    much higher than the validation mean token accuracy. In that case, it suggests
    that the model is overfitting to the training data and not generalizing well to
    new data. In contrast, if the validation mean token accuracy is much lower than
    the training mean token accuracy, it suggests that the model is underfitting the
    training data and not learning effectively.
  prefs: []
  type: TYPE_NORMAL
- en: This metric is useful for evaluating the performance of a fine-tuned model on
    the training data. A good mean token accuracy can be relative and depends on the
    specific task or application. Generally, a higher value (closer to 1.0) indicates
    better performance. However, it does not reflect how well the model generalizes
    to new or unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the interpretation of these metrics can depend on the specific task
    or application. Therefore, it’s essential to consider other metrics and qualitative
    evaluations to get a comprehensive view of the model’s performance. The quality
    of mean token accuracy depends on the task’s complexity and the nature of text.
    Higher accuracy (closer to 100%) is expected for simpler tasks or texts with predictable
    patterns. A lower accuracy might still be good for more complex tasks or diverse
    texts.
  prefs: []
  type: TYPE_NORMAL
- en: One way to assess whether the mean token accuracy is good is to compare it with
    a baseline or with the performance of other models on the same task. If your model’s
    accuracy is higher than the baseline or similar models, it’s a positive sign.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basic constructs of fine-tuning and using a CLI or
    code, let’s take a look at how we can achieve this using Azure OpenAI and a GUI.
    As stated earlier, we will use Azure OpenAI as an example, but the same process
    applies to OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.5 Fine-tuning using Azure OpenAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using the SDK and the CLI, we also have a visual interface that we
    can employ to achieve the same outcome. Often, doing this manually would be a
    better approach than using code. To kick off a fine-tuning job in Azure OpenAI,
    when logged into the Azure Portal and in the AI Studio, under models, we choose
    the option to create a custom model (figure 9.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'We go through the wizard and choose to upload the training and validation datasets,
    as shown in figure 9.6\. Note: If these have been uploaded using the SDK, we will
    find them here, as long as they are in the same tenants and have the same end-point
    deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5 Azure AI Studio: Creating a custom model'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 Choosing a training and validation dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 9.7 shows the status and details of each of our training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 Training job details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we have a fine-tuned model, we need to deploy it to a test environment
    to run an evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Deployment of a fine-tuned model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deployment of a fine-tuned model is quite straightforward. The new fine-tuned
    model shows up as another model available for use in our Azure tenant or OpenAI
    subscription, as shown in figures 9.8 and 9.9, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 Deploying fine-tuned model for inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: OpenAI has launched a feature in the playground that lets users see how a fine-tuned
    model differs from the base model side by side, which can be useful visually but
    not efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F09_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 OpenAI fine-tuned model deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '9.4.1 Inference: Fine-tuned model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Returning to our task, we now have a fine-tuned model for EmojiBot, where the
    bot responds in emojis using the format that Microsoft Teams uses. Figure 9.10
    shows how the out-of-the-box GPT-3.5 Turbo model behaves when asked to respond
    with emojis; this is expected but will not work with Teams.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F10_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 Response with emojis using GPT-3.5 Turbo
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, the experience for the same questions using our fine-tuned powered
    EmojiBot is quite different, as shown in figure 9.11\. Here, for the same questions
    as before, we get the response in the format we’ll be able to use in Teams.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F11_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 Fine-tuned EmojiBot inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, it is easy to get completely incorrect results on the same questions
    from earlier and with the same parameter settings (figure 9.12). We can see the
    fine-tuned model answer in emojis—`(Pizza)` and `(Feeling tired)—`but the result
    is not what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F12_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 Fine-tuned EmojiBot with incorrect results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To resolve this, we need to tweak the system prompt to steer the model to respond
    using emojis where possible, which is a great way to close out by reminding that
    a stacked approach of prompt engineering, RAG, and fine-tuning (where the task
    at hand warrants) is the right approach in almost all cases.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to fine-tune a model and the steps one needs to undertake,
    let us switch and look at some of the underpinnings of the technology that will
    make this work. Strictly speaking, we do not require this to do a fine-tuning,
    but it will help us to understand some of the nuances to achieve better outcomes
    for fine-tuning. We will start by understanding how we train an LLM and, at a
    high level, what the steps entail.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Training an LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is helpful to our understanding of model adaptation and the techniques and
    their associated limitations to examine what it means and what it takes to do
    full training for an LLM. At a high level, if we were to do full training and
    build an LLM from scratch, that training would involve four major stages, as shown
    in figure 9.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F13_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 Full end-to-end training of an LLM [5]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s go through each stage in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.1 Pretraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Base LLMs are built during this initial stage. We touched on base LLMs in chapter
    2\. These are the original, pretrained models trained on a massive corpus of text
    data. They can generate text based on the patterns they learned during training.
    Some also call them raw language models.
  prefs: []
  type: TYPE_NORMAL
- en: Note  While powerful, these base models are less suitable for general-purpose
    applications because they may need to align their responses with the specific
    intentions or instructions of the user. They are more like raw engines for text
    generation, lacking the refined capability to understand and adhere to the nuances
    of user prompts. Base models do not answer questions and often respond with more
    questions. In contrast, instructors are tailored to be more interactive and user-friendly,
    which makes them more suitable for a wide range of applications, from customer
    service chatbots to educational tools, where understanding and following instructions
    accurately is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.2 Supervised fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised fine tuning (SFT) is the next stage. In this stage, the base model
    undergoes refining of the base model with high-quality, domain-specific data.
    These datasets consist of prompt–response pairs, manually created (often by human
    contractors), which are fewer in number than in the previous stage but of much
    higher quality. The contractors follow detailed documentation to create these
    prompt–response pairs, ensuring relevance and quality. Similar to the last pretraining
    stage, the SFT model is trained to predict the next token in these pairs, but
    these are less accurate and contextually aware when generating the response.
  prefs: []
  type: TYPE_NORMAL
- en: SFT is a technique for optimizing LLMs on labeled data for a specific downstream
    task, such as sentiment analysis, text summarization, or machine translation.
    Later in the chapter, we will cover additional details of SFT methods and approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.3 Reward modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third phase is reward modeling, the first part of the Reinforcement Learning
    from Human Feedback (RLHF) process. The main goal at this stage is to develop
    a model that can evaluate and rank responses based on their quality and relevance.
    To do this, the SFT model (from the previous stage) generates multiple responses
    to a prompt, which human contractors then rank based on various criteria such
    as domain expertise, fact-checking, and code execution. These rankings train a
    reward model, which learns to score responses like human contractors.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.4 Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the second part of the RLHF process, and it aims to enhance the language
    model’s ability to generate high-quality responses through iterative feedback.
    In this final stage, the reward model scores responses generated by the SFT model
    for many prompts. These scores are used to further train the SFT model, ultimately
    leading to the creation of the RLHF model. The RLHF aligns the LLMs with human
    preferences or expectations for a given task or domain, such as chat, code, or
    creative writing. More details on RLHF methods will be covered later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.5 Direct policy optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Direct policy optimization (DPO) [6] is another technique, which is a new type
    of reward model parameterization in RLHF that used for fine-tuning LLMs to align
    with our preferences. It exploits a relationship between reward functions and
    optimal policies. It allows us to skip the reward modeling step outlined earlier,
    as long as the human feedback can be expressed in binary terms—that is, a choice
    between two options. DPO can solve the reward maximization problem with constraints
    in a single policy training phase, essentially treating it as a classification
    problem. PPO (see section 9.7) requires a reward model and a complex RL-based
    optimization process; DPO, however, bypasses the reward modeling step and directly
    optimizes the language model on preference data, which can be simpler and more
    efficient. As DPO eliminates the need to train a reward model instead of training
    a reward model and optimizing a policy based on that model, we can directly optimize
    the policy. This characteristic makes this approach quicker, and fewer resources
    are used than in RLHF with PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6 Model adaptation techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several techniques available for model adaptation, with each technique
    providing its unique approach and being suitable for different scenarios depending
    on the specific requirements (i.e., the model size, available computational resources,
    and the desired level of adaptation). One of the main techniques widely used for
    adapting LLMs is low adaptation ranking (LoRA), which will be covered in more
    detail in the next section. In LoRA, instead of updating all the weights in the
    model, only a small subset of parameters, introduced as low-rank matrixes, are
    modified. This approach allows efficient training and adaptation, while preserving
    most of the pretrained model’s structure and knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameter efficient fine-tuning (PEFT) is a concept in ML that refers to methods
    of adapting and fine-tuning large pretrained models, such as GPT-3.5, to minimize
    the number of parameters that need to be updated. This approach is particularly
    valuable when dealing with large models, as it reduces computational requirements
    and can mitigate problems such as overfitting. PEFT techniques are designed to
    make fine-tuning more accessible and efficient, especially for users with limited
    computational resources—LoRA is an example of the PEFT method. For more details
    on different types of PEFT techniques and details, see the paper “Scaling Down
    to Scale Up: A Guide to Parameter-Efficient Fine-Tuning” by Vladislav Lialin [7].'
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic forgetting is a phenomenon where a model loses its ability to perform
    well on previous tasks after being fine-tuned on new tasks [8]. This can happen
    when the model overwrites its original parameters with task-specific ones, thus
    forgetting the general knowledge it learned from pretraining. When implementing
    PEFT to prevent catastrophic forgetting, we fine-tune only a small subset of parameters,
    while keeping most pretrained parameters fixed. This way, the model can retain
    its generalization ability and adapt to new tasks without losing its previous
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning (SFT) is another type of adaptation technique; it is
    a specific type of fine-tuning where the model is further trained on a labeled
    dataset. It’s supervised because the training process uses a dataset that pairs
    the input data with the correct output (labels). SFT is particularly common in
    tasks such as classification, where the model must learn to associate specific
    inputs with labeled outputs. SFT is a subset of the broader fine-tuning process,
    specifically tailored to situations where labeled data is available.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, each technique has its unique approach and is suitable for different
    scenarios depending on the specific requirements, such as the model size, available
    computational resources, and the desired level of adaptation. Table 9.2 outlines
    some notable ones in addition to LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.2 Model adaptation techniques
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Technique | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt tuning  | Prompt tuning is a technique for adapting LLMS to different
    tasks by providing specific cues or prompts that guide their generation or prediction.
    It does not require retraining the model or updating its weights, which makes
    it faster and cheaper than fine-tuning. It is particularly useful for tasks where
    only a small amount of adaptation is required.  |'
  prefs: []
  type: TYPE_TB
- en: '| Adapter modules  | Adapter models are used in LLM fine-tuning to add small
    and task-specific modules (small neural networks) to the pretrained model and
    train only these modules on the task-specific data. They are also flexible and
    modular, as they can be easily added or removed for different tasks without affecting
    the pretrained model.  |'
  prefs: []
  type: TYPE_TB
- en: '| Bias-only (BitFit)  | Bias-only (BitFit) is a technique for fine-tuning LLMs
    by modifying only the bias terms of the model or a subset of them. It offers a
    minimalistic approach to adaptation, requiring even fewer trainable parameters
    than LoRA. BitFit is based on fine-tuning, mainly exposing the knowledge learned
    by pretraining rather than acquiring new task-specific knowledge.  |'
  prefs: []
  type: TYPE_TB
- en: '| Layer freezing  | Layer freezing is a fine-tuning technique that keeps some
    of the model layers fixed and only updates the rest. This method allows for more
    control over which aspects of the model are adapted and can reduce training time.  |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge distillation  | Involves training a smaller, more efficient model
    (student) to mimic the behavior of a larger pretrained model (teacher). This method
    is useful for deploying LLMs in resource-constrained environments.  |'
  prefs: []
  type: TYPE_TB
- en: '| Meta-learning  | Focuses on training the model to learn new tasks quickly
    with minimal data; often involves training on various tasks so the model can more
    efficiently adapt to new, unseen tasks  |'
  prefs: []
  type: TYPE_TB
- en: '| Differential privacy fine-tuning  | Incorporates privacy-preserving techniques
    during fine-tuning to protect sensitive data. This is essential for applications
    where data privacy and security are paramount.  |'
  prefs: []
  type: TYPE_TB
- en: '| Reinforcement learning from human feedback (RLHF)  | Involves fine-tuning
    models based on feedback or rewards derived from human interactions or evaluations.
    It is useful for tasks where human judgment is crucial, such as content moderation.  |'
  prefs: []
  type: TYPE_TB
- en: Each technique involves tradeoffs between the computational resources required,
    the level of specialization achieved, and the retention of the model’s original
    capabilities. The choice of technique depends on the specific application, the
    constraints of the deployment environment, and the goals of the model adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are more familiar with various techniques, let’s explore LoRA,
    the main technique for fine-tuning large models such as GPT.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6.1 Low-rank adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LoRA, which stands for low-rank adaptation [9], is a method specifically designed
    for adapting LLMs. It presents an efficient alternative to traditional fine-tuning
    methods, which is particularly useful in scenarios where fine-tuning large models
    can be resource-intensive and challenging.
  prefs: []
  type: TYPE_NORMAL
- en: LoRA is based on making minimal but strategic modifications to a pretrained
    model without altering its entire architecture. It achieves this by introducing
    the notion of low-rank matrices. Instead of modifying the entire weight matrices
    of a neural network, LoRA inserts small, low-rank matrices into the model. These
    matrices are applied to the model’s layers (attention and feed-forward) during
    forward and backward passes, as shown in figure 9.14\. As we have seen, LLMs are
    built on deep learning architectures consisting of multiple layers designed to
    process and understand human language. In addition, LoRA also retrains selectively
    only these low-rank matrices, while the original pretrained weights remain frozen.
    This selective retraining significantly reduces the computational resources needed.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F14_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 LoRA reparameterization—only A and B are trained [9]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 9.14, input (**X**) at the bottom of the diagram represents the input
    data fed to the layer with pretrained weights. *A* and *B* are the adaptation
    parameters that will be updated during fine-tuning, and *W* is the original pretrained
    weight that remains frozen.
  prefs: []
  type: TYPE_NORMAL
- en: When we want to fine-tune a task, we can store and load only a few task-specific
    parameters along with the pretrained model. This approach helps improve the efficiency
    during runtime for various downstream adaptations. It gives LoRA several advantages,
    including
  prefs: []
  type: TYPE_NORMAL
- en: '*Resource efficiency*—LoRA requires far less computational power than traditional
    full-model fine-tuning, making it more accessible for adapting large models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Preservation of generalization*—LoRA maintains the base LLM’s generalization
    abilities by not altering the original pretrained weights, while allowing specialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Faster adaptation*—The process is quicker due to fewer updated parameters,
    enabling rapid deployment of adapted models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability*—LoRA is particularly effective for large models, where full-model
    fine-tuning may be impractical due to resource constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoRA is a cost-effective and efficient method for language models that allows
    fast switching between tasks. QLoRA is a variant of LoRA that further reduces
    the number of parameters by quantizing the low-rank matrices and can achieve up
    to 99% parameter reduction (via implementing an 8-bit optimizer for quantization),
    while maintaining or improving the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quantization is another technique that reduces the memory and computation requirements
    of the model by representing the parameters with fewer bits. Quantization of a
    model means reducing the precision of the model’s parameters, such as weights
    and biases, from high-precision floating-point numbers (32 bit or 16 bit) to low-precision
    numbers (8 bit or 4 bit). This can reduce the model size and speed up the inference
    but may also affect the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is especially useful for LLMs, which can have billions of parameters
    and require a lot of memory and computation. By quantizing the model, the deployment
    and inference of the model can be more efficient and scalable. For example, DistilBERT
    is a quantized version of BERT, an LLM for NLP. It has 40% fewer parameters than
    BERT but retains 97% of BERT’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'At face value, quantization is similar to LoRA, as both aim to improve the
    efficiency and scalability of LLMs. Still, they are very different in their approaches
    and tradeoffs:'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA reduces the number of trainable parameters by freezing the pretrained model
    weights and injecting low-rank matrices into each layer. This allows for faster
    fine-tuning and adaptation to new tasks. LoRA also preserves the full precision
    of the model weights, which means it does not reduce the model’s memory footprint
    or inference latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization reduces the model’s memory and computation requirements by representing
    the parameters with fewer bits, such as INT4\. This characteristic allows for
    smaller model sizes and faster inference, but it also introduces quantization
    errors and noise, which can degrade the model performance. Quantization also requires
    careful calibration and optimization to minimize the loss of accuracy and robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantized LoRA (QLoRA) is another technique that aims to improve the parameter
    efficiency of fine-tuned LLMs. It extends LoRA by adding quantization to the process.
    This means that the LoRA adapters’ weights are quantized to a lower precision,
    such as 4 bit, which greatly shrinks the memory size of the model. The main benefit
    of QLoRA is its ability to balance performance and memory efficiency, making it
    a suitable option for scarce resources. Despite the decreased precision, QLoRA
    has been proven to keep a similar level of effectiveness to its nonquantized version,
    LoRA, in different tasks. This makes QLoRA an attractive choice for those who
    want to use powerful language models without the high computational costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main tradeoffs between LoRA and QLoRA are related to the balance of performance,
    memory efficiency, and computational resources. LoRA achieves a good balance between
    performance and efficiency, while QLoRA maximizes memory savings, which can be
    crucial for some use cases. The choice between the two would depend on the task’s
    specific needs and the deployment environment’s limitations. Experimentation is
    important to determine which method best fits your needs. If high accuracy is
    very important and computational resources are sufficient, LoRA might be the best
    choice. If memory efficiency is more important, then QLoRA would be better, especially
    if a small decrease in performance is acceptable for the application.
  prefs: []
  type: TYPE_NORMAL
- en: Teaching new knowledge using fine-tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Often, there is a misconception that fine-tuning teaches the model new knowledge
    (or information). This is not correct. SFT does not exactly teach new knowledge
    to a model in the traditional sense. A fine-tuned model’s knowledge is limited
    to what was present in its pretraining data until its last update. It does not
    acquire new external knowledge during the fine-tuning process. Instead, it refines
    and adapts the model’s existing knowledge and capabilities to perform better on
    specific tasks or in certain domains. All SFT is doing is refining existing knowledge
    as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Using pretrained knowledge*—In SFT, the model has already been trained on
    a large, diverse dataset. This initial training provides a broad base of general
    knowledge and language understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Focusing on specific tasks*—During SFT, the model’s preexisting knowledge
    is honed to be more effective for specific tasks. For instance, if you fine-tune
    a language model on medical texts, the model becomes more adept at understanding
    and generating language related to medicine. Still, it does not necessarily teach
    new facts about medicine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adjusting weights for relevance*—Fine-tuning effectively adjusts the model’s
    internal parameters (or weights) to make certain features or patterns more prominent
    when making predictions or generating text. This process makes the model more
    sensitive to the nuances of the specific data it was fine-tuned on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SFT tailors a model’s existing knowledge and capabilities to be more effective
    for specific applications rather than teaching it new, external knowledge. The
    process involves adjusting the model’s internal understanding and response generation
    mechanisms to better align with the characteristics of the fine-tuning data.
  prefs: []
  type: TYPE_NORMAL
- en: 9.7 RLHF overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) is a sophisticated ML approach
    that combines reinforcement learning (RL) and supervised learning. Unlike traditional
    RL, which relies on predefined reward functions, RLHF integrates human judgment
    into learning by asking humans to evaluate the agent’s behavior and provide feedback,
    such as ratings, preferences, or suggestions. This feedback helps the agent to
    improve its performance and align with human values or preferences. Specifically,
    it increases helpfulness and truthfulness in the generation, while mitigating
    harm and bias. OpenAI’s Instruct models, now the default models, are examples
    of models powered by RLHF and deployed at scale [10]. Anthropic, another AI startup
    founded by former OpenAI employees, aims to build LLMS such as Claude that are
    reliable, interpretable, and steerable. They have published their approach to
    RLHF [11], including associated human preference data [12].
  prefs: []
  type: TYPE_NORMAL
- en: RLHF is particularly valuable when defining an explicit reward function is challenging
    or where human preferences, subjective judgments, and nuances are crucial. It’s
    used in NLP tasks such as conversation and content generation, where subjective
    quality matters. RLHF aids in content moderation on social media by understanding
    context-specific nuances. Personalized recommendation systems help tailor suggestions
    to individual tastes. It’s valuable for socially acceptable and comfortable robotics
    and human–computer interaction behaviors. Ethical decision-making guides AI in
    aligning with human values. RLHF enhances AI’s performance in complex games, creative
    endeavors such as art and music, and healthcare for personalized medical decisions.
    These applications highlight RLHF’s role in aligning AI with the complex and subjective
    nature of human preferences and judgments.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF can improve LLMs’ performance, alignment, and diversity on various tasks,
    such as text generation, summarization, or dialogue. Instruct models are base
    LLMs that have been fine-tuned using the RLHF approach, and as shown in figure
    9.15, they significantly outperform the base LLMs [13].
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F15_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 RLHF-trained models (PPO and PPO-ptx) significantly outperform base
    LLM models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  Base models are original pretrained models that have not been aligned
    with specific values and are not generally suitable for production use. For a
    reminder on the categories of LLM, see section 2.4 in chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: The RLHF framework teaches a model to perform tasks as humans would want, using
    human feedback as a guide. This method is especially relevant in fields where
    the desired output is subjective or highly context dependent, such as NLP, content
    generation, and decision-making systems. The key phases that comprise RLHF are
    outlined in figure 9.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F16_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 RLHF fine-tuning approaches to aligning language models [13]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These components work in tandem to create a robust learning system where a
    model can learn complex, human-centric tasks beyond the capabilities of traditional
    ML approaches. Integrating human feedback is key to bridging the gap between algorithmic
    decision-making and human judgment. Let’s look at the RLHF phases in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Supervised fine-tuning*—This phase involves training the model on a dataset
    of human-generated examples. These examples demonstrate the desired outcomes or
    behaviors, providing a baseline for the model to learn from. It helps the model
    understand the context and nuances of tasks as humans interpret.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reward modeling*—In this step, a separate model, known as the reward model,
    is trained to predict the quality of outputs generated by the primary model. The
    reward model is trained using human judgments, often involving ratings or preferences
    between different outputs. This model effectively translates subjective human
    evaluations into quantitative feedback that the primary model can use for learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Proximal policy optimization (PPO)*—PPO is an RL algorithm that iteratively
    improves the primary model’s policy (decision-making process). The algorithm updates
    the model’s policy to maximize the rewards predicted by the reward model. PPO
    is chosen for its stability and efficiency in handling large and complex models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human feedback loop*—This loop involves continuous input from human evaluators
    who assess the quality of the model’s outputs. The feedback is used to train the
    reward model further, creating a dynamic learning environment where the model
    adapts to evolving human preferences and standards. The loop ensures that the
    model remains aligned with human expectations and can adapt to changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  PPO-ptx [13] is an adaptation of the PPO algorithm tailored for fine-tuning
    RLHF. It integrates a reference to the original LLM to maintain performance, while
    aligning the model’s outputs with human preferences. This approach helps mitigate
    the alignment tax, ensuring the LLM remains effective and diverse in its outputs
    after training. Essentially, PPO-ptx balances the model’s pretraining knowledge
    with the new feedback to create a high-performing LLM aligned with human values.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF might seem like the silver bullet in many ways, but enterprises must be
    aware of some challenges. Let’s explore these.
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.1 Challenges with RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RLHF is a powerful technique for teaching models complex tasks, but it has many
    practical challenges and limitations for enterprises. An RLHF system needs a lot
    of human preference data, which is hard to get because it involves other people
    who are not part of the training process. How well RLHF works depends on how good
    the human annotations are, which humans can write, such as when they adjust the
    initial LLM in InstructGPT or provide ratings of how much they like different
    outputs from the model. Some of these challenges are
  prefs: []
  type: TYPE_NORMAL
- en: '*Technical complexity*—Implementing RLHF requires advanced skills and knowledge
    in ML, RL, and NLP. It also involves complex setup and maintenance processes,
    such as configuring the model architecture, reward systems, and feedback mechanisms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computationally intensive*—RLHF models need a lot of computational resources,
    such as GPUs and servers, which can be expensive. They also depend on the quality
    and quantity of human feedback, which can be hard to obtain and process. From
    a practical viewpoint, a lot of the human feedback is from contract workers (or
    gig workers) on crowdsourcing platforms where getting the right qualified people
    in certain domains might be challenging. Moreover, ensuring a diverse and unbiased
    dataset for training can be challenging and computationally heavy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not scalable*—RLHF models are difficult to scale for large-scale applications,
    requiring continuous human feedback and increasing computational resources. They
    are also hard to adapt to different domains or changing data environments, resulting
    in limited adaptability and customization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quality*—RLHF models are prone to bias, as they reflect human feedback providers’
    subjective opinions and potential prejudices. Ensuring ethical use and unbiased
    outputs is a major concern. Maintaining a consistent quality of human feedback
    can be difficult, as human judgment can vary and affect the model’s reliability
    and performance. When trying to build a helpful model that avoids harm, there
    is an inherent tension between those two dimensions. Providing too many polite
    responses, such as “Sorry, I am an AI model, and I cannot help you with that,”
    or something similar, limits the model’s usefulness. Organizations must balance
    and mitigate this using additional guidance, training, and other ML techniques
    to create synthetic data where possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost*—RLHF models are costly to implement and operate. Costs include infrastructure,
    computational resources, data acquisition, and hiring skilled professionals. There
    are also ongoing operational costs related to data management, model updates,
    and continuous feedback integration. These costs can be substantial, especially
    for large-scale implementations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data*—It is hard to produce good human text that answers specific prompts
    because it usually means paying part-time workers (instead of product users or
    crowdsourcing). Luckily, the amount of data needed to train the reward model for
    most uses of RLHF (~50k preference labels) is not that costly. However, it is
    still more than what academic labs can usually afford. There is only one big dataset
    for RLHF on a general language model (from Anthropic) and a few smaller datasets
    for specific tasks (such as summarization data from OpenAI). Another problem with
    data for RLHF is that human annotators can disagree a lot, which makes the training
    data very noisy without a true answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RLHF offers advanced capabilities in teaching models to perform complex tasks;
    however, its adoption in enterprise settings is hindered by technical complexity,
    resource demands, scalability challenges, ethical considerations, and high costs.
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, these barriers make it difficult for many organizations to
    implement and sustain RLHF systems in their operations practically. On the other
    hand, those who can implement this, especially some of the technical companies
    such as OpenAI and Anthropic, can benefit from it. Let’s see how we can scale
    an RLHF implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.2 Scaling an RLHF implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scaling an RLHF implementation for LLMs involves a multifaceted approach that
    balances efficiency, diversity, and quality control. First, automating data collection
    and implementing efficient feedback mechanisms are crucial for handling large
    volumes of data and feedback. Automated systems can gather data from various sources
    or through interfaces designed for efficient human interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Using a large, diverse pool of human evaluators is essential for capturing a
    wide range of perspectives, helping the model to be more robust and less biased.
    To ensure the feedback is informative, intelligent sampling strategies, such as
    active learning, can be used to identify and prioritize the most valuable instances
    for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization and distribution of tasks among multiple evaluators can significantly
    speed up the feedback process. The system can handle large-scale data processing
    and model training with scalable infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Implement quality control measures, such as cross-validation among evaluators
    and algorithms, to detect biases and maintain the quality and consistency of feedback.
    Regular monitoring and evaluation of the model’s performance can help you understand
    the effects of RLHF and guide continuous improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ethical considerations and bias mitigation are crucial. Ensuring that
    feedback does not reinforce harmful stereotypes and actively addressing potential
    biases is vital for developing fair and responsible models. Overall, scaling RLHF
    for LLMs requires a comprehensive approach that integrates technical, logistical,
    and ethical strategies, aiming for a system that effectively incorporates human
    feedback into the model’s learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model adaptation should be anchored in a set of use cases, and it should be
    the last resort for enterprises trying to improve the model on those tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering and RAG must work in conjunction with fine-tuning in a stacked
    manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When done correctly, fine-tuning has a high upside from enhanced efficiency
    and possible cost savings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning has a high cost, and you should be aware of challenges such as the
    need for task-specific data, computational resources, performance evaluation,
    and ethical considerations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning should be done in conjunction with evaluations and will often require
    multiple iterations to obtain a model ready for production deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of metrics for evaluating fine-tuned models largely depends on the
    model’s specific application and objectives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main model adaptation techniques that are more cost-efficient are supervised
    fine-tuning (SFT), parameter efficient fine-tuning (PEFT), and low-rank adaptation
    (LoRA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
