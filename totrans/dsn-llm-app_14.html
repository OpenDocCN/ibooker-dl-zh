<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 11. Representation Learning and Embeddings"><div class="chapter" id="chapter_llm_interfaces">
<h1><span class="label">Chapter 11. </span>Representation Learning and Embeddings</h1>


<p>In the previous chapter, we learned how we can interface language models<a data-type="indexterm" data-primary="embeddings" id="xi_embeddings11473"/> with external tools, including data stores. External data can be present in the form of text files, database tables, and knowledge graphs. Data can span a wide variety of content types, from proprietary domain-specific knowledge bases to intermediate results and outputs generated by LLMs.</p>

<p>If the data are structured, for example residing in a relational database, the language model can issue a SQL query to retrieve the data it needs. But what if the data are present in unstructured form?</p>

<p>One way to retrieve data from unstructured text datasets is to search by keywords or use regular expressions<a data-type="indexterm" data-primary="regular expressions" id="id1426"/>. For the Apple CFO example in the previous chapter, we can retrieve text containing CFO mentions from a corpus containing financial disclosures, hoping that it will contain the join date or tenure information. For instance, you can use the regex:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s2">"(?i)\b(?:C\.?F\.?O|Chief\s+Financial\s+Officer)\b"</code></pre>

<p>Keyword search<a data-type="indexterm" data-primary="keyword search, limitations of" id="id1427"/><a data-type="indexterm" data-primary="search systems" data-secondary="keyword search limitations" id="id1428"/> is limited in its effectiveness. There are a very large number of ways to express CFO join date or tenure in a corpus, if it is present at all. Trying to use a catch-all regex like the above could result in a large proportion of false positives.</p>

<p>Therefore, we need to move beyond keyword search. Over the last few decades, the field of information retrieval has developed several methods like BM25 that have shaped search systems<a data-type="indexterm" data-primary="search systems" id="id1429"/>. We will learn more about these methods in <a data-type="xref" href="ch12.html#ch12">Chapter 12</a>. In the LLM era, embedding-based search systems are fast becoming the standard way of implementing search.</p>

<p>In this chapter, we will learn how embeddings work. We will explore the concept of semantic similarity and examine various similarity measures. We will learn how to use popular embedding models and evaluate their performance. We will also show how to fine-tune embedding models to suit specific use cases and domains. We will show how to interpret these embeddings using sparse autoencoders (SAEs)<a data-type="indexterm" data-primary="sparse autoencoders (SAEs)" id="id1430"/><a data-type="indexterm" data-primary="SAEs (sparse autoencoders)" id="id1431"/>. Finally, we will discuss techniques for optimizing embeddings to reduce storage requirements and computational overhead.</p>






<section data-type="sect1" data-pdf-bookmark="Introduction to Embeddings"><div class="sect1" id="introduction-to-embeddings">
<h1>Introduction to Embeddings</h1>

<p>Representation learning<a data-type="indexterm" data-primary="representation learning" data-seealso="embeddings" id="id1432"/> is a subfield of machine learning that deals with learning to represent data in a way that captures its meaningful features, often in a low dimensional space. In the context of NLP, this involves transforming textual units like words, sentences, or paragraphs into vector form, called embeddings. Embeddings capture semantic (meaning-related) and pragmatic (social context-related) features of the input.</p>

<p>Embeddings can be generated using both open source libraries and paywalled APIs<a data-type="indexterm" data-primary="Sentence Transformers library" id="id1433"/>. <a href="https://oreil.ly/4OSVd">Sentence Transformers</a> is a very well-known open source library for generating embeddings, and it provides access to embedding models that performs competitively with respect to proprietary ones.</p>

<p>Let’s generate embeddings using the <code>Sentence Transformers</code> library:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformer</code><code class="p">,</code> <code class="n">util</code>
<code class="n">sbert_model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code><code class="s1">'msmarco-distilbert-base-tas-b'</code><code class="p">)</code>
<code class="n">embedding</code> <code class="o">=</code> <code class="n">sbert_model</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"American pizza is one of the nation's greatest</code><code class="w"/>
<code class="n">cultural</code> <code class="n">exports</code><code class="s2">", show_progress_bar=True, device='cuda',</code><code class="w"/>

<code class="n">convert_to_tensor</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Embedding size:"</code><code class="p">,</code> <code class="n">embedding</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">embedding</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Embedding size: 768

tensor([-3.9256e-01,  1.0734e-01,  1.3579e-01,  7.6147e-02,  5.2521e-02,
-6.5887e-03,  1.9225e-01,  3.5374e-01,  2.5725e-01,  5.6408e-02,...])</pre>

<p>For this model, the embedding size is 768, which means each vector has 768 dimensions. The sequence length of this particular model is 512, which means the input text is restricted to 512 tokens, beyond which it will be truncated. The embedding vector is made up of floating-point numbers, which by themselves are not interpretable. We will discuss techniques for interpreting embeddings later in this chapter.</p>

<p>Most embedding models used today are based on encoder-only language models<a data-type="indexterm" data-primary="encoder-only models" id="id1434"/><a data-type="indexterm" data-primary="models" data-secondary="encoder-only" id="id1435"/>, which we introduced in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>. The underlying models are BERT, RoBERTa, MPNet, etc., and are typically fine-tuned on paraphrasing/question-answering/natural language inference datasets. Let’s see how to derive embeddings from these types of models (which is what the <code>sentence_transformers.encode()</code> function does under the hood):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">AutoModel</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="n">tokenizer</code><code class="o">=</code>
<code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
  <code class="s2">"sentence-transformers/msmarco-distilbert-base-tas-b"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code>
<code class="n">AutoModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"sentence-transformers/msmarco-distilbert-base-tas-b"</code><code class="p">)</code>

<code class="nb">input</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="p">(</code>
  <code class="s1">'American pizza is one of the nation'</code><code class="n">s</code> <code class="n">greatest</code> <code class="n">cultural</code> <code class="n">exports</code><code class="s1">',</code><code class="w"/>
<code class="n">padding</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s1">'pt'</code><code class="p">)</code>

<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="o">**</code><code class="nb">input</code><code class="p">,</code> <code class="n">return_dict</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
       <code class="n">embedding</code> <code class="o">=</code> <code class="n">output</code><code class="o">.</code><code class="n">last_hidden_state</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code>
<code class="nb">print</code><code class="p">(</code><code class="n">embedding</code><code class="p">)</code></pre>

<p>In this example, the embedding is drawn from the [CLS] token of the last layer of the DistilBERT model. Other ways of extracting embeddings from models include:</p>

<ul>
<li>
<p>Mean pooling, where the average is taken across all token outputs in the sequence</p>
</li>
<li>
<p>Max pooling, where the maximum value in each dimension across all tokens is taken</p>
</li>
<li>
<p>Weighted mean, where more weight is given to the last few tokens</p>
</li>
<li>
<p>Last token, where the embedding is just the encoder output of the last token</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>Whether the last token (or the first token) contains good representations of the entire sequence depends a lot on the pre-training and the fine-tuning objective. BERT’s pre-training objective (next-sentence prediction) ensures that the [CLS] token is much richer in representation than, say, RoBERTa, which doesn’t use the next-sentence prediction objective and thus its &lt;s&gt; start sequence token isn’t as informative.</p>
</div>

<p>Recently, decoder-based embedding models<a data-type="indexterm" data-primary="decoder models" data-secondary="embeddings" id="id1436"/><a data-type="indexterm" data-primary="models" data-secondary="decoder" id="id1437"/> have started gaining prominence, like the<a data-type="indexterm" data-primary="SGPT" id="id1438"/> <a href="https://oreil.ly/AztT9">SGPT family of models</a>. OpenAI<a data-type="indexterm" data-primary="OpenAI" data-secondary="SGPT embeddings" id="id1439"/> exposes a single embedding endpoint for both search and similarity. OpenAI embeddings have a much larger maximum sequence length (8,192 tokens), and a much larger dimension size (1,536–3,072). Cohere and Jina are examples of other embedding providers.</p>

<p>Choosing the right model for your task depends on cost, latency, storage limitations, performance, and the data domain of your use case. I suggest starting off with the small but effective all-mpnet-base-v2 model available through the Sentence Transformers library<a data-type="indexterm" data-primary="Sentence Transformers library" id="id1440"/>, which I consider the workhorse of the field of NLP. As always, experimenting with different models never hurts. More tips on selecting the right models will be provided throughout the rest of the chapter. Later in the chapter, we will also show how to evaluate embedding models and introduce popular benchmarks.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>There is no such thing as infinite compression! Embedding sizes are fixed, so the longer your input, the less information can be encoded in its embedding. Managing this tradeoff differs by use case.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Semantic Search"><div class="sect1" id="id186">
<h1>Semantic Search</h1>

<p>The true value of embeddings<a data-type="indexterm" data-primary="embeddings" data-secondary="semantic search" id="xi_embeddingssemanticsearch119929"/><a data-type="indexterm" data-primary="semantic search" id="xi_semanticsearch119929"/><a data-type="indexterm" data-primary="search systems" data-secondary="semantic search" id="xi_searchsystemssemanticsearch119929"/> can be appreciated when we use them for representing a large text corpus. The vectors representing the data occupy what we call an embedding space<a data-type="indexterm" data-primary="embedding space" id="id1441"/>. Similar texts are located closer to each other in the embedding space. This property allows us to use similarity measures to accomplish meaningful tasks like clustering or semantic search. Semantic search refers to techniques that take into account the meaning and context of queries and documents to identify documents that are most relevant to a given query.</p>

<p>We can visualize the embedding space by using dimensionality reduction techniques<a data-type="indexterm" data-primary="dimensionality reduction, embedding" id="id1442"/> like <a href="https://oreil.ly/Rk1M9">PCA</a> or <a href="https://oreil.ly/0xNrB">t-SNE</a>.</p>

<p><a data-type="xref" href="#embedding-visualization">Figure 11-1</a> depicts the visualization of embeddings of posts on X (formerly Twitter) by members of the US Congress created by <a href="https://oreil.ly/XsXls">Nomic AI</a> using its Atlas tool. You can view a detailed version of the visualization at <a href="https://oreil.ly/AORpk">Nomic’s blog</a>.</p>

<p>Let’s explore how we can use embeddings for semantic search. For a given user query, we can generate an embedding of the query and then identify document embeddings closest to it in the vector space. The texts corresponding to the top-k<a data-type="indexterm" data-primary="top-k sampling" id="id1443"/> (k can be as small as 1 but can vary according to application needs) closest vectors are provided as a response to the search query. This process is called <em>retrieval</em>.  The texts are then fed into the LLM prompt along with the user query, and the LLM uses the information provided in the context to answer the user query. This two-step process has traditionally been called<a data-type="indexterm" data-primary="retrieval-reader framework" id="id1444"/> the <em>retriever-reader</em> framework, with the LLM playing the role of the reader in this example.</p>

<figure><div id="embedding-visualization" class="figure">
<img src="assets/dllm_1101.png" alt="embedding-visualization" width="600" height="535"/>
<h6><span class="label">Figure 11-1. </span>Embedding space visualization</h6>
</div></figure>

<p>As a simple illustrative example, consider two sentences that make up our corpus:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chunks</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'The President of the U.S is Joe Biden'</code><code class="p">,</code>
<code class="s1">'Ramen consumption has increased in the last 5 months'</code><code class="p">]</code></pre>

<p>Given the query “president of usa,” we can encode the query and the chunks using Sentence Transformers:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformer</code><code class="p">,</code> <code class="n">util</code>
<code class="n">sbert_model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code><code class="s1">'msmarco-distilbert-base-tas-b'</code><code class="p">)</code>
<code class="n">chunk_embeddings</code> <code class="o">=</code> <code class="n">sbert_model</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">chunks</code><code class="p">,</code> <code class="n">show_progress_bar</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="n">device</code><code class="o">=</code><code class="s1">'cuda'</code><code class="p">,</code> <code class="n">normalize_embeddings</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">convert_to_tensor</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">query_embedding</code> <code class="o">=</code> <code class="n">sbert_model</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">query</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="s1">'cuda'</code><code class="p">,</code>
<code class="n">normalize_embeddings</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">convert_to_tensor</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">matches</code> <code class="o">=</code> <code class="n">util</code><code class="o">.</code><code class="n">semantic_search</code><code class="p">(</code><code class="n">query_embedding</code><code class="p">,</code> <code class="n">chunk_embeddings</code><code class="p">,</code>
<code class="n">score_function</code><code class="o">=</code><code class="n">util</code><code class="o">.</code><code class="n">dot_score</code><code class="p">)</code></pre>

<p>The output is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[[{</code><code class="s1">'corpus_id'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code> <code class="s1">'score'</code><code class="p">:</code> <code class="mf">0.8643729090690613</code><code class="p">},</code>
  <code class="p">{</code><code class="s1">'corpus_id'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'score'</code><code class="p">:</code> <code class="mf">0.6223753690719604</code><code class="p">}]]</code></pre>

<p>As you can see, the similarity score is much higher for the first sentence, and thus we return the first sentence as the query response.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There is a distinction between symmetric semantic search and asymmetric semantic search<a data-type="indexterm" data-primary="symmetric versus asymmetric semantic search" id="id1445"/><a data-type="indexterm" data-primary="asymmetric versus symmetric semantic search" id="id1446"/>. In symmetric search, the query text is of similar size as the document text. In asymmetric search, the query text is much shorter than the document text, as with search engine and question-answering assistant queries. There are models available that are specialized for only symmetric or asymmetric search. In some models, the query and chunk texts are encoded using separate models<a data-type="indexterm" data-startref="xi_embeddingssemanticsearch119929" id="id1447"/><a data-type="indexterm" data-startref="xi_semanticsearch119929" id="id1448"/><a data-type="indexterm" data-startref="xi_searchsystemssemanticsearch119929" id="id1449"/>.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Similarity Measures"><div class="sect1" id="id187">
<h1>Similarity Measures</h1>

<p>Commonly used similarity measures<a data-type="indexterm" data-primary="embeddings" data-secondary="similarity measures" id="xi_embeddingssimilaritymeasures1115534"/><a data-type="indexterm" data-primary="similarity measures, embeddings" id="xi_similaritymeasuresembeddings1115534"/><a data-type="indexterm" data-primary="semantic similarity" id="xi_semanticsimilarity1115534"/> include dot product, cosine similarity, and Euclidean distance. Refer to the <a href="https://oreil.ly/X_qcD">Pinecone</a> tutorial on similarity measures if you need a backgrounder. While using embedding models, use the similarity measure that was used to train the model. You will find this information in the model card or Hugging Face model hub page.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you set <code>normalize_embeddings</code> to <code>True</code> as an argument in the <code>encode()</code> function, it will normalize the embeddings<a data-type="indexterm" data-primary="normalizing of embeddings" id="id1450"/> to unit length. This will ensure that both dot product and cosine similarity<a data-type="indexterm" data-primary="cosine similarity" id="id1451"/> will have the same values. Note that dot product is a faster operation than cosine similarity. Sentence Transformers provides <a href="https://oreil.ly/LOu75">separate models</a> trained on dot product and cosine similarity and mentions that models trained on dot product tend to prefer longer chunks during retrieval.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1452">
<h1>Exercise</h1>
<p>Experiment with different pooling methods to extract embeddings from models. For reference, you can use the code provided by <a href="https://oreil.ly/8QpBj">Sentence Transformers</a>. For the same sentences provided in the aforementioned example, how do you notice the similarity scores changing? Repeat the same by trying different similarity measures.</p>
</div></aside>

<p>While the notion of semantic similarity is powerful, it is not a panacea for all applications. The semantic similarity task is underspecified. To start with, there are several notions of similarity.
Similarity refers to the sameness or alikeness of the entities being compared. But for the same two entities, some dimensions are similar and some are different.</p>

<p>For example, consider the three sentences:</p>
<blockquote>
<p>After his 25th anniversary at the company, Mr. Pomorenko confirmed that he is not retiring.</p>

<p>Mr. Pomorenko announced his retirement yesterday.</p>

<p>Mr. Pomorenko did not announce his retirement yesterday.</p></blockquote>

<p>Now let’s use the Sentence Transformers all-mpnet-base-v2 embedding model to encode these sentences and calculate their similarity:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">sentence</code><code class="o">-</code><code class="n">transformers</code>

<code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformer</code><code class="p">,</code> <code class="n">util</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code><code class="s1">'all-mpnet-base-v2'</code><code class="p">)</code>

<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'After his 25th anniversary at the company, Mr. Pomorenko</code><code class="w"/>
<code class="n">confirmed</code> <code class="n">that</code> <code class="n">he</code> <code class="ow">is</code> <code class="ow">not</code> <code class="n">retiring</code><code class="s1">',  '</code><code class="n">Mr</code><code class="o">.</code> <code class="n">Pomorenko</code> <code class="n">announced</code> <code class="n">his</code> <code class="n">retirement</code>
<code class="n">yesterday</code><code class="s1">']</code><code class="w"/>
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code>
<code class="n">cosine_scores</code> <code class="o">=</code> <code class="n">util</code><code class="o">.</code><code class="n">cos_sim</code><code class="p">(</code><code class="n">embeddings</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">embeddings</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Cosine Similarity:"</code><code class="p">,</code> <code class="n">cosine_scores</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>

<p>Output:</p>

<pre data-type="programlisting">Cosine Similarity: 0.7870</pre>

<p>If you replace the second sentence with “Mr. Pomorenko did not announce his retirement yesterday,” the output is:</p>

<pre data-type="programlisting">Cosine Similarity: 0.7677!</pre>

<p>As you can see, both these sentences are perceived as equally similar to the first sentence. In some aspects, this is true. They are similar because they both talk about Mr. Pomorenko. They are also similar because both deal with the subject of retirement. On the other hand, one sentence conveys the opposite meaning to the other, by suggesting a retirement is happening versus not happening.</p>
<div data-type="tip"><h6>Tip</h6>
<p>One way to handle the false positives arising due to the model using undesirable similarity dimensions (like negation) is to just increase the k value in the top-k<a data-type="indexterm" data-primary="top-k sampling" id="id1453"/> results that are returned as a response to the query. Then, the LLM can distinguish between false positives and use the correct information for answering the query. However, increasing the top-k also increases the context length of the prompt, increasing latency and cost<a data-type="indexterm" data-startref="xi_embeddingssimilaritymeasures1115534" id="id1454"/><a data-type="indexterm" data-startref="xi_similaritymeasuresembeddings1115534" id="id1455"/><a data-type="indexterm" data-startref="xi_semanticsimilarity1115534" id="id1456"/>.</p>
</div>

<p>Our application requirements determine which similarity dimensions are important to us. If negation is an important relation for our application to distinguish, it might be a good idea to reflect that in our embedding space. This is where fine-tuning<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="embeddings" id="xi_finetuningmodelsembeddings11218251"/> embedding models can come in handy. Fine-tuning embedding models allows you to “edit” your embedding space to your own liking. The process is relatively simple and can be potentially quite beneficial.</p>

<p>Fine-tuning embeddings can also be very useful when you are working with specialized data domains whose token distribution deviates from general-purpose data. Let’s now discuss how to fine-tune embedding models.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1457">
<h1>Exercise</h1>
<p>In the example about Mr. Pomorenko’s retirement, check how the similarities for these sentences fare when using embeddings from Jina, Nomic, and OpenAI embeddings. What do their similarity scores look like? Is it better or worse than what we see with the all-mpnet-base-v2 model?</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Fine-Tuning Embedding Models"><div class="sect1" id="id188">
<h1>Fine-Tuning Embedding Models</h1>

<p>The Sentence Transformers library<a data-type="indexterm" data-primary="SentenceTransformerTrainer" id="id1458"/> facilitates fine-tuning<a data-type="indexterm" data-primary="embeddings" data-secondary="fine-tuning" id="xi_embeddingsfinetuning1122957"/> embedding models using the <a href="https://oreil.ly/Jahep"><code>SentenceTransformerTrainer</code> class</a>. To fine-tune an embedding model, we need a base model to fine-tune on, a training dataset, and a learning objective.</p>








<section data-type="sect2" data-pdf-bookmark="Base Models"><div class="sect2" id="id189">
<h2>Base Models</h2>

<p>You can fine-tune a fine-tuned model like all-mpnet-base-v2, or you can fine-tune a base model<a data-type="indexterm" data-primary="base models" data-secondary="fine-tuning" id="id1459"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="base models" id="id1460"/><a data-type="indexterm" data-primary="models" data-secondary="base" id="id1461"/> like MPNet<a data-type="indexterm" data-primary="MPNet" id="id1462"/>, from which all-mpnet-base-v2 is defined. You will need more training data to fine-tune a base model than to further fine-tune an already fine-tuned model. Other candidates’ models for fine-tuning include <a href="https://oreil.ly/Sh8pZ">BGE-M3</a> and <a href="https://oreil.ly/lFiWX">jina-embeddings-v3</a>. A full list of models available through Sentence Transformers can be accessed <a href="https://oreil.ly/Onyuv">online</a>. Remember to check the licenses for a given model before using it for commercial purposes.</p>

<p>Some of the factors to keep in mind while choosing a base model include the performance of the base model, the size of the embedding models (which determines how fast the model can encode text), the number of dimensions of the model (which determines the amount of storage taken up by the embeddings), and the licensing implications. The MPNet or all-mpnet-base-v2 is a solid first choice that has served me well on many projects.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If a model has been fine-tuned for a particular task like semantic search, it is not optimal to further fine-tune it on a different task.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Training Dataset"><div class="sect2" id="id190">
<h2>Training Dataset</h2>

<p>There are many different ways to structure your dataset<a data-type="indexterm" data-primary="datasets" data-secondary="fine-tuning" id="xi_datasetsfinetuning1124456"/>. The most common way is in the form of triplets consisting of (anchor, positive, negative) examples. For a given anchor sentence, the positive sentence is a sentence we would like to be closer to the anchor sentence in embedding space, and the negative sentence is a sentence we would like to be farther apart from the anchor in embedding space. For example, to fine-tune the model to help it distinguish negation<a data-type="indexterm" data-primary="negation, embedding dataset to distinguish" id="xi_negationembeddingdatasettodistinguish11244470"/> sentences, our training set can be composed of triplets where the negative sentence contradicts the anchor and the positive sentences.</p>

<p><a data-type="xref" href="#embed-dataset">Figure 11-2</a> shows an embedding dataset composed of triplets for helping the model distinguish negation.</p>

<figure><div id="embed-dataset" class="figure">
<img src="assets/dllm_1102.png" alt="embed-dataset" width="326" height="800"/>
<h6><span class="label">Figure 11-2. </span>Fine-tuning dataset for negation</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1463">
<h1>Hard Negatives</h1>
<p>For the negation dataset, it is trivial to fill in the negative examples. But in all other cases it is not exactly obvious what comprises a negative example.</p>

<p>One way is to use random sentences from the corpus as negative examples. But for more effective fine-tuning, it is customary to use hard negatives<a data-type="indexterm" data-primary="hard negatives" id="id1464"/>. Hard negatives are examples that are somewhat relevant to the anchor but just not as relevant as the positive example.</p>

<p>A simple method for selecting false negatives is to use the anchor as a query and find the top-k matches in a document corpus using an embedding model that are not already determined as positive examples. To ensure that the extracted examples are not false negatives, i.e., they are more relevant or just as relevant as the positive example, we can use a relevance score threshold (retrieve only examples with cosine similarity below 0.7) or a top-k range (only retrieve examples between top-30 and top-50).</p>

<p><a href="https://oreil.ly/tIs81">Moreira et al.</a> show that false negatives can further be alleviated by leveraging the relevance score of the positive example. The relevance score threshold for a negative example can be set as the relevance score of the positive example plus a fixed margin. The threshold can also be a percentage of the relevance score of the positive example<a data-type="indexterm" data-startref="xi_negationembeddingdatasettodistinguish11244470" id="id1465"/>.</p>
</div></aside>

<p>Datasets can also be composed of sentence pairs, where the sentences could represent a (query, response) pair, or a (passage, summary) pair, or a pair of paraphrases. The downstream use cases determine the type of dataset needed. The <a href="https://oreil.ly/geI1M">Sentence Transformers website</a> shows all the different ways a dataset can be formatted.</p>

<p>Training datasets can be as small as a few thousand examples, to <a href="https://oreil.ly/oNI4n">billions of tokens</a> when used for domain adaptation<a data-type="indexterm" data-startref="xi_datasetsfinetuning1124456" id="id1466"/>.</p>

<p>Note that certain loss functions require your dataset to be in a specific format. We will discuss loss functions in detail next.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Loss Functions"><div class="sect2" id="id191">
<h2>Loss Functions</h2>

<p>Recall our discussion on loss functions<a data-type="indexterm" data-primary="loss functions" id="xi_lossfunctions1127140"/> for training LLMs in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>. The <a href="https://oreil.ly/9Qaop">Sentence Transformers library</a> supports a wide range of loss functions for training embedding models. Let’s explore a few commonly used ones.</p>

<p>For a triplet dataset, you can compute a <a href="https://oreil.ly/yXHNU">triplet loss</a>. For a training dataset consisting of an (anchor, positive, negative) triplet, the triplet loss minimizes the distance between the anchor sentence and the positive sentence, and maximizes the distance between the anchor sentence and the negative sentence.</p>

<p>Mathematically, the loss is calculated as:</p>
<div data-type="equation">
Loss = max(d(a, p) – d(a, n) + margin, 0)
</div>

<p>where d is a distance measure, typically Euclidean distance.
The margin is a hyperparameter that represents the distance by which the negative example should be farther away from the anchor than the positive example. When using Euclidean distance as the distance measure, I suggest a margin of 5, but make sure to tune it if you are not getting sufficient results.</p>

<p>If you are using a dataset composed of pairs like (query, response), (passage, summary), etc., you can use<a data-type="indexterm" data-primary="Multiple Negatives Ranking Loss" id="id1467"/> the <a href="https://oreil.ly/oNcsQ">Multiple Negatives Ranking Loss</a>.</p>

<p>In a batch containing (query, response) pairs (q1, r1), (q2, r2)…​(qn, rn), for each query, there will be a positive pair, e.g., (q1, r1) and n – 1 negative pairs, e.g., (q1, r2), (q1, r3)…​etc. The loss function minimizes the negative log likelihood.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Use <a href="https://oreil.ly/QwBlI"><code>CachedMultipleNegativesRankingLoss</code></a>, available in Sentence Transformers<a data-type="indexterm" data-primary="CachedMultipleNegativesRankingLoss" id="id1468"/>, which allows you to use larger batch sizes, leading to better performance.</p>
</div>

<p>Now that we have discussed all the ingredients needed for fine-tuning, let’s put it all together with the <code>SentenceTransformerTrainer</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>
<code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformer</code><code class="p">,</code> <code class="n">SentenceTransformerTrainer</code>
<code class="kn">from</code> <code class="nn">sentence_transformers.losses</code> <code class="kn">import</code> <code class="n">TripletLoss</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code> <code class="s2">"'all-mpnet-base-v2'"</code><code class="p">)</code>

<code class="n">dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"csv"</code><code class="p">,</code> <code class="n">data_files</code><code class="o">=</code><code class="s2">"negatives_dataset.csv"</code><code class="p">)</code>

<code class="n">loss</code> <code class="o">=</code> <code class="n">TripletLoss</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>

<code class="n">trainer</code> <code class="o">=</code> <code class="n">SentenceTransformerTrainer</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">train_dataset</code><code class="o">=</code><code class="n">dataset</code>
    <code class="n">loss</code><code class="o">=</code><code class="n">loss</code>
   <code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">save_pretrained</code><code class="p">(</code><code class="s2">"mpnet_finetuned_negatives"</code><code class="p">)</code></pre>

<p>The full code is available in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Watch out for overfitting! You can reduce your learning rate if you notice the model overfitting<a data-type="indexterm" data-startref="xi_embeddingsfinetuning1122957" id="id1469"/><a data-type="indexterm" data-startref="xi_lossfunctions1127140" id="id1470"/><a data-type="indexterm" data-startref="xi_finetuningmodelsembeddings11218251" id="id1471"/>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1472">
<h1>Exercise</h1>
<p>Using an LLM of your choice, create a synthetic triplet dataset with around 8,000 examples where the negative example is the negation of the positive example, and fine-tune the all-mpnet-base-v2 model. After fine-tuning, test the cosine similarity of the sentences in the negation example provided earlier in the chapter. Do you see any improvements?</p>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a href="https://oreil.ly/BPdRD">Zhou et al.</a> show that in the context of embeddings, cosine similarity<a data-type="indexterm" data-primary="cosine similarity" id="id1473"/> tends to underestimate the similarity between high-frequency words. This is because high-frequency words occupy distinct regions in the embedding space, leading to larger distances from other words. On the other hand, low-frequency words tend to be more concentrated geometrically.</p>
</div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Instruction Embeddings"><div class="sect1" id="id192">
<h1>Instruction Embeddings</h1>

<p>So far we have seen that embedding models<a data-type="indexterm" data-primary="embeddings" data-secondary="instruction" id="id1474"/><a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="embeddings" id="id1475"/> are specialized for solving a specific task,  like semantic search or paraphrasing. A recent development ties together embedding models and the concept of instruction-tuning, which we discussed in <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>. Imagine if you could use the same embedding model to generate different embeddings for the same document, based on the task it is going to be used for. One such model is called Instructor<a data-type="indexterm" data-primary="Instructor embeddings" id="id1476"/>. <a href="https://oreil.ly/mSIhG">Instructor embeddings</a> allow you to optionally specify the domain, text type (whether it is a sentence, paragraph, etc.), and task, along with the text during encoding.</p>

<p>Here is an example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">InstructorEmbedding</code>

<code class="kn">from</code> <code class="nn">InstructorEmbedding</code> <code class="kn">import</code> <code class="n">INSTRUCTOR</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">INSTRUCTOR</code><code class="p">(</code><code class="s1">'hkunlp/instructor-large'</code><code class="p">)</code>

<code class="n">customized_embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code>
<code class="p">[[</code><code class="s1">'Represent the question for retrieving supporting documents:'</code><code class="p">,</code>
  <code class="s1">'Who is the CEO of Apple'</code><code class="p">],</code>
 <code class="p">[</code><code class="s1">'Represent the sentence for retrieval:'</code><code class="p">,</code>
  <code class="s1">'Tim Cook is the CEO of Apple'</code><code class="p">],</code>
 <code class="p">[</code><code class="s1">'Represent the sentence for retrieval:'</code><code class="p">,</code>
  <code class="s1">'He is a musically gifted CEO'</code><code class="p">],</code>
<code class="p">)</code></pre>

<p>The creators of Instructor recommend using this instruction template:</p>

<pre data-type="programlisting">‘Represent the {domain} {text_type} for {task_objective}:’</pre>

<p>where <code>{domain}</code> represents the domain of the text like law, finance, etc. The optional
<code>{text_type}</code> represents the unit of text being encoded, like a question, sentence, paragraph, etc.
<code>{task_objective}</code> represents the task for which we are using the embeddings, like semantic search, paraphrase detection, etc.</p>

<p>In the context of semantic search, they recommend the instruction “Represent the question for retrieving supporting documents” for queries, and “Represent the sentence for retrieval” for documents.</p>

<p>Another way the principle of instruction-tuning can be applied to retrieval<a data-type="indexterm" data-primary="description-based retrieval" id="id1477"/> is with <em>description-based retrieval</em>, where the query can be the description of the text that needs to be retrieved, rather than an instantiation (example) of the text that needs to be retrieved. <a href="https://oreil.ly/rp8Q-">Ravfogel et al.</a> have published description-based retrieval models that in my experience are very effective. Note that these models have a dual-encoder setup: separate models are used to encode the query and documents.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1478">
<h1>Exercise</h1>
<p>Encode the Wikipedia dataset found in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a> using the <code>INSTRUCTOR</code> and the description-based retrieval models <a href="https://oreil.ly/zYgAW"> abstract-sim-query</a> and <a href="https://oreil.ly/S7iat">abstract-sim-sentence</a>. For the question-answering task, how do they perform compared with the embedding models we have used so far?</p>
</div></aside>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1479">
<h1>Evaluating Embedding Models</h1>
<p>A dizzying number of embedding models are available these days. Which one should you use? <a href="https://oreil.ly/MJ-Di">Massive Text Embedding Benchmark (MTEB)</a> is a benchmark<a data-type="indexterm" data-primary="benchmarking" data-secondary="MTEB for embeddings" id="id1480"/><a data-type="indexterm" data-primary="Massive Text Embedding Benchmark (MTEB)" id="id1481"/><a data-type="indexterm" data-primary="MTEB (Massive Text Embedding Benchmark)" id="id1482"/> that can help you make the decision. MTEB covers a diverse set of tasks and benchmarks both latency and task performance, enabling you to reason about the tradeoff.</p>

<p>Check out the current <a href="https://oreil.ly/aLEPi">leaderboard</a>, which is updated regularly. While there is no clear winner across all tasks, you can see that larger models generally perform better, and not much separates the first 50 or even 100 models. Recall our discussion in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a> on the limitations of public benchmarks, so do not rely too much on MTEB rankings. Your final decision on embedding model choice should balance your application-specific needs, pricing, latency, and performance tradeoffs.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Optimizing Embedding Size"><div class="sect1" id="id253">
<h1>Optimizing Embedding Size</h1>

<p>Many applications involve generating billions of embeddings<a data-type="indexterm" data-primary="embeddings" data-secondary="optimizing size" id="xi_embeddingsoptimizingsize1138660"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="embedding sizes" id="xi_optimizationandoptimizersembeddingsizes1138660"/>. As we have seen, modern embeddings sometimes have as many as thousands of dimensions.
If each dimension is represented in float32, then it needs four bytes of memory per 
<span class="keep-together">dimension</span>. Therefore, storing 100 million vectors generated from the all-mpnet-base-v2 model, which has 768 dimensions, needs close to 300 GB of memory!</p>

<p>It is not uncommon to represent a single sentence, almost always no longer than 40 tokens, with a 768-dimension vector. Do we really need 768 dimensions to represent 40 tokens? The reality is that embedding training is very inefficient, and a large number of dimensions are not really useful.</p>

<p>Therefore, several embedding truncation and quantization approaches have been developed to optimize embedding size and reduce storage and compute requirements. If you are operating in an environment with more than a few million vectors, these techniques are likely to be useful to you. Let’s look at some of these approaches.</p>








<section data-type="sect2" data-pdf-bookmark="Matryoshka Embeddings"><div class="sect2" id="id193">
<h2>Matryoshka Embeddings</h2>

<p>Matryoshka embeddings<a data-type="indexterm" data-primary="Matryoshka embeddings" id="xi_Matryoshkaembeddings1139522"/><a data-type="indexterm" data-primary="truncation, embedding" id="xi_truncationembedding1139522"/> are named after <a href="https://oreil.ly/OC6Yj">Matryoshka dolls</a>, which refer to a set of wooden dolls that are placed inside each other in decreasing order of size, originating from Russia. Matryoshka embeddings are trained such that the earlier dimensions of the vector contain more important information than the later dimensions. This allows us to truncate vectors depending on the requirements of the application with respect to cost, latency, and performance.</p>

<p>The technique used to train these embeddings is called Matryoshka Representation Learning (MRL)<a data-type="indexterm" data-primary="Matryoshka Representation Learning (MRL)" id="id1483"/><a data-type="indexterm" data-primary="MRL (Matryoshka Representation Learning)" id="id1484"/>. In MRL, we first choose a set of truncation dimensions. For example a 1,024-dimension vector can have truncation dimensions 128, 256, 512, and 768. During the training process, we calculate the loss over each of the truncation dimensions as well as the full dimension. The losses are then added and weighted. In our example, the first 128 dimensions learn from the loss calculated over the first 128, 256, 512, 768, and 1,024 dimensions of the vector. The end result is that the initial dimensions of the vector will encode more important information because they learn from richer losses.</p>

<p>Training using MRL is supported by the Sentence Transformers library. Let’s see how it works in practice:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformer</code>
<code class="kn">from</code> <code class="nn">sentence_transformers</code> <code class="kn">import</code> <code class="n">SentenceTransformerTrainer</code><code class="p">,</code> <code class="n">losses</code>
<code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code><code class="s2">"all-mpnet-base-v2"</code><code class="p">)</code>
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"csv"</code><code class="p">,</code> <code class="n">data_files</code><code class="o">=</code><code class="s2">"finetune_dataset.csv"</code><code class="p">)</code>
<code class="n">loss</code> <code class="o">=</code> <code class="n">losses</code><code class="o">.</code><code class="n">MultipleNegativesRankingLoss</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
<code class="n">loss</code> <code class="o">=</code> <code class="n">losses</code><code class="o">.</code><code class="n">MatryoshkaLoss</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">loss</code><code class="p">,</code> <code class="p">[</code><code class="mi">768</code><code class="p">,</code> <code class="mi">512</code><code class="p">,</code> <code class="mi">256</code><code class="p">,</code> <code class="mi">128</code><code class="p">]])</code>

<code class="n">trainer</code> <code class="o">=</code> <code class="n">SentenceTransformerTrainer</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">train_dataset</code><code class="o">=</code><code class="n">train_dataset</code><code class="p">,</code>
    <code class="n">loss</code><code class="o">=</code><code class="n">loss</code><code class="p">,</code>
<code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code></pre>

<p><a href="https://oreil.ly/sA5fo">Tom Aarsen</a> observed in his experiments that even at 8.3% of the original embedding size, the Matryoshka model preserves 98.37% of the original performance. This makes it a very effective technique that will come in handy when you are working with large datasets.</p>

<p>Similar to how we can reduce the effective dimension of our embeddings using MRL, we can also reduce the effective number of layers<a data-type="indexterm" data-primary="layers" data-secondary="reduction in embeddings" id="id1485"/> of the embedding model, leading to faster inference. This is done by extracting embeddings from the lower layers of the model. To facilitate the lower layers of the model aligning high-quality embeddings with the embeddings of the last layer of the model, a K-L divergence loss<a data-type="indexterm" data-primary="Kullback-Liebler (K-L) divergence" id="id1486"/> is employed between the final layer and each of the lower layers. This technique was first introduced by <a href="https://oreil.ly/fzIPD">Li et al.’s</a> Espresso Sentence Embeddings<a data-type="indexterm" data-primary="Espresso Sentence Embeddings" id="id1487"/>.</p>

<p><a href="https://oreil.ly/DIoTe">Tom Aarsen</a> observed in his experiments<a data-type="indexterm" data-primary="Aarsen, Tom" id="id1488"/> that removing half the layers leads to a 2x improvement in speed with 85% of the original performance preserved.</p>

<p>The Sentence Transformers library allows you to combine Matryoshka representations with layer reduction<a data-type="indexterm" data-primary="Matryoshka2dLoss" id="id1489"/> using the <a href="https://oreil.ly/xzG-a">Matryoshka2dLoss</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1490">
<h1>Exercise</h1>
<p>Download this dataset by Rishabh Misra containing <a href="https://oreil.ly/Tu4XA">news headlines</a>. Use the <a href="https://oreil.ly/jALJE">nomic-embed-text-v1.5 model</a> from Nomic AI, which has been trained using MRL. Pick one of the headlines as the query and generate its query embedding. Generate document embeddings for all other headlines, and calculate similarity scores between query and document embeddings at truncation checkpoints 1,024, 768, 512, 256, and 128.</p>

<p>Perform error analysis on the top 25 results. At what dimension do you start seeing a noticeable performance drop?</p>

<p>Additionally, run the example training script for Matryoshka2dLoss provided by <a href="https://oreil.ly/Bxe5z">Sentence Transformers</a>, and test the embeddings at various layer and dimension cutoffs<a data-type="indexterm" data-startref="xi_Matryoshkaembeddings1139522" id="id1491"/><a data-type="indexterm" data-startref="xi_truncationembedding1139522" id="id1492"/>.</p>
</div></aside>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Binary and Integer Embeddings"><div class="sect2" id="id194">
<h2>Binary and Integer Embeddings</h2>

<p>An alternative to truncation is quantization<a data-type="indexterm" data-primary="quantization" data-secondary="binary and integer embeddings" id="id1493"/><a data-type="indexterm" data-primary="binary and integer embeddings" id="id1494"/><a data-type="indexterm" data-primary="integer and binary embeddings" id="id1495"/>. With binary and integer quantization, the number of vector dimensions remains the same, but each dimension is represented by fewer bits. Recall that typically embedding vectors are represented in float32, thus taking four bytes of memory per dimension.</p>

<p>At the extreme level, the four bytes can be represented with just one bit, resulting in a 32x reduction in storage requirements. This type of compression is generally done by sacrificing the precision of the vector values.</p>

<p>A simple way to convert a four-byte vector to a one-bit vector is to assign a value of 
<span class="keep-together">1 if</span> the original value is positive, and 0 if it is negative. Note that you might need 
<span class="keep-together">to perform</span> some scaling to achieve best results. After packing these bits into bytes, 
<span class="keep-together">a 512-dimension</span> vector can be represented in just 512 / 8 = 64 bytes, instead of 512 × 4 = 2,048 bytes.</p>

<p>Another advantage with using binary embeddings is that computing similarity only needs simple bitwise operations, thus vastly speeding up retrieval. However, quantization negatively affects performance.</p>

<p>You can use the <code>Sentence Transformers</code> library to quantize embeddings:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sentence_transformers.quantization</code> <code class="kn">import</code> <code class="n">quantize_embeddings</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">SentenceTransformer</code><code class="p">(</code><code class="s2">"all-mpnet-base-v2"</code><code class="p">)</code>
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">encode</code><code class="p">([</code><code class="s2">"I heard the horses are excited for Halloween."</code><code class="p">,</code>
<code class="s2">"Dalmatians are the most patriotic of dogs."</code><code class="p">,</code> <code class="s2">"This restaurant is making me</code><code class="w"/>
<code class="n">nostalgic</code><code class="o">.</code><code class="s2">"])</code><code class="w"/>
<code class="n">binary_embeddings</code> <code class="o">=</code> <code class="n">quantize_embeddings</code><code class="p">(</code><code class="n">embeddings</code><code class="p">,</code> <code class="n">precision</code><code class="o">=</code><code class="s2">"binary"</code><code class="p">)</code></pre>

<p><code>quantize_embeddings</code> also supports int8 quantization. In this scheme, the four bytes representing each dimension are converted into an integer value, represented in one byte. The integer can be either signed or unsigned, thus representing values between –127 and 127 or between 0 and 255, respectively. The conversion process is guided using a calibration dataset of embeddings, from which we calculate the minimum and maximum value of each dimension. These values are then used in the normalization formula to convert the numbers from one range to another.</p>
<div data-type="tip"><h6>Tip</h6>
<p>It has been shown that for some <a href="https://oreil.ly/Mp3pu">embedding models</a>, binary embeddings perform better than int8 embeddings despite the reduced precision! This is largely because of the calibration dataset used and the challenge involved in mapping float values to buckets of int8 values.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Product Quantization"><div class="sect2" id="id195">
<h2>Product Quantization</h2>

<p>Another promising quantization method<a data-type="indexterm" data-primary="quantization" data-secondary="product quantization" id="id1496"/><a data-type="indexterm" data-primary="product quantization" id="id1497"/> is called <a href="https://oreil.ly/aJq2C"><em>product quantization</em></a>. In this technique, a vector is divided into chunks of equal size. The chunks are then clustered. The number of clusters is set to the number of values that can be represented by the quantized embedding. For example, if we aim to quantize to int8, then the number of values that can be represented is 256, and thus the number of clusters is 256. Each cluster is associated with an identifier, which is a unique value between 0 and 255. Each chunk belongs to the cluster whose centroid the chunk is closest to.</p>

<p>Thus, the original float32 vector can now be represented by a list of cluster identifiers corresponding to the clusters the chunks belong to. The larger the chunk size, the more the compression. Thus if the vector is divided into five chunks, the resulting embedding will have only five dimensions. Unlike int8 and binary quantization, product quantization also reduces the number of dimensions needed to represent a vector. However, the performance drop is higher.</p>

<p>Choose your quantization technique by determining your relative product priorities for criteria like cost, performance, and speed.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Optimizing embeddings for storage come with a performance hit. However, if there is plenty of redundancy in the document corpus, answers to typical user queries might be found in several documents, and hence the user may not feel this performance drop<a data-type="indexterm" data-startref="xi_embeddingsoptimizingsize1138660" id="id1498"/><a data-type="indexterm" data-startref="xi_optimizationandoptimizersembeddingsizes1138660" id="id1499"/>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1500">
<h1>Exercise</h1>
<p>Download the <a href="https://oreil.ly/OUq5M">Wikipedia embeddings</a> encoded with Cohere’s embedding model and implement product quantization by setting the number of clusters to 256. You can also use a vector database, like Qdrant, that supports product quantization.
Experiment with different chunk sizes. Where do you see the highest performance drop-off?</p>

<p>Additionally, implement the similarity scoring function for product quantization.</p>
</div></aside>

<p>Now that we have seen various techniques to practically implement embedding-based retrieval, let’s next figure out the textual units we need to embed into distinct vectors.</p>
</div></section>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Chunking"><div class="sect1" id="id196">
<h1>Chunking</h1>

<p>As noted in <a data-type="xref" href="#introduction-to-embeddings">“Introduction to Embeddings”</a>, embedding models<a data-type="indexterm" data-primary="embeddings" data-secondary="chunking" id="xi_embeddingschunking1149550"/><a data-type="indexterm" data-primary="chunking, embeddings" id="xi_chunkingembeddings1149550"/> support very limited context lengths, and the effectiveness of embedding similarity matching decreases as the text length increases. Therefore, it is natural to split documents into manageable units called chunks and embed each chunk into one or more vectors.</p>

<p>A chunk can be defined as a semantically coherent and not necessarily contiguous part of a document. The average chunk length depends on the context length supported by the language model, and the number of chunks returned to the model (the top-k) in response to a user query. As models become increasingly affordable to operate and support ever-larger context lengths, the permissible chunk size grows.</p>

<p>Each chunk can either be represented by a single vector or can be further broken down into units, with each unit being represented by a separate vector. A unit could be a sentence, a paragraph, or even a section. Typically, the smaller the unit, the better. For your application, test your expected user queries against different granularities and see what works best.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1501">
<h1>Document Parsing</h1>
<p>Unstructured data<a data-type="indexterm" data-primary="document parsing" id="id1502"/><a data-type="indexterm" data-primary="sentence tokenization" id="id1503"/> first needs to be processed to make it amenable to retrieval. This usually involves parsing text from the document, splitting it into manageable units, associating metadata with these units, generating embeddings, storing, and indexing them for easy access.</p>

<p>If it makes sense for your use case to have sentences as the basic unit of text<a data-type="indexterm" data-primary="Punkt tokenizer" id="id1504"/>, <a href="https://oreil.ly/bgxrp">NLTK’s Punkt tokenizer</a> is a tried and tested tool for tokenizing text into sentences. Note that sentence tokenization is not a trivial task, especially if you have domain-specific text. Naive splitting on end marks (periods, question marks, and abbreviations) can only get you so far; abbreviations play spoilsport. You can train the Punkt tokenizer unsupervised over a large body of your target text to ensure it learns your domain-specific rules, as well as provide explicit rules and exceptions yourself. Other tools for sentence tokenization include <a href="https://oreil.ly/R7mFQ">spaCy</a>, <a href="https://oreil.ly/xKo43">Stanza</a>, and <a href="https://oreil.ly/fxvBi">ClarityNLP</a>.</p>

<p>Overall, effective document parsing (extracting section and subsection boundaries; detecting and extracting tables, images, etc.; dealing with heterogeneous document formats) is the bane of NLP projects. A large proportion of failure modes in RAG can be attributed to poor document parsing. Of all the steps in a typical NLP application pipeline, I have spent the most effort on document parsing. Yes, it might not be the most glamorous task in the world, but it is the foundation on which high-quality products are built. Ignore this at your own peril!</p>
</div></aside>

<p>Consider a scenario where a document corpus has been broken down into units represented by embeddings. For a given user query, we can calculate the cosine similarity<a data-type="indexterm" data-primary="cosine similarity" id="id1505"/> between the user query vector and each of the document vectors. The chunks 
<span class="keep-together">corresponding</span> to the most similar vectors are then retrieved. This ensures that the embedding matching happens at a lower granularity, like a sentence, but the model receives the entirety of the chunk the sentence belongs to, thus providing sufficient background context to the model.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1506">
<h1>Exercise</h1>
<p>Construct a sentence tokenizer for the Canadian parliamentary proceedings dataset provided in the book’s <a href="https://oreil.ly/llm-playbooks">GitHub repository</a>. What are the failure modes? Can you use rules to resolve these issues? Try unsupervised training of the Punkt tokenizer using this data. Is it effective in resolving the issues found?</p>
</div></aside>

<p>A question I am frequently asked by ML practitioners is, “What is the ideal chunk size and what are some effective chunking strategies?” Determining the right chunk size and boundaries are key challenges practitioners face when using embedding-based retrieval. In this section, we will discuss a few chunking strategies, introduced in order of increasing complexity.</p>

<p>In the basic implementation of embedding-based retrieval, each vector is a distinct island, disconnected from all other islands. The text represented by Vector A is not able to influence text represented by Vector B in any way. Therefore, we need to connect these islands in some way or make these islands as self-contained as possible. With these objectives in mind, let’s look at some chunking strategies that go beyond naive paragraph or section splitting.</p>








<section data-type="sect2" data-pdf-bookmark="Sliding Window Chunking"><div class="sect2" id="id197">
<h2>Sliding Window Chunking</h2>

<p>Consider a situation where the embedding<a data-type="indexterm" data-primary="sliding window chunking" id="id1507"/> similarity function returns a unit in Chunk 45 as the most similar vector to your query vector. However, text in Chunk 44, which immediately precedes Chunk 45 in the document, contains relevant information contextualizing Chunk 45. The vectors in Chunk 44 have a very low similarity score with the query, and as a result, Chunk 44 is not selected for retrieval. One way to fix this is by using sliding window chunking, where each text can be present in multiple chunks, thus allowing neighboring context to be effectively represented in a coherent block.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Metadata-Aware Chunking"><div class="sect2" id="id198">
<h2>Metadata-Aware Chunking</h2>

<p>Any metadata<a data-type="indexterm" data-primary="metadata-aware chunking" id="id1508"/> that you have about the document can be leveraged to determine chunking boundaries. Useful metadata information includes paragraph boundaries, section and subsection boundaries, etc. If the metadata isn’t already available, you might need to use document parsing techniques to extract this information. Several libraries can facilitate this, including <a href="https://oreil.ly/CoX46">Unstructured</a>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Layout-Aware Chunking"><div class="sect2" id="id199">
<h2>Layout-Aware Chunking</h2>

<p>A more involved form of metadata-aware chunking is layout-aware chunking<a data-type="indexterm" data-primary="layout-aware chunking" id="id1509"/>. In this approach we use computer vision techniques to extract layout information about the document, including the placement and scope of textual elements, the titles, subtitles, font size of text, etc.; use this metadata to inform the chunking process. Both open source and proprietary tools can facilitate layout extraction. They include tools like  <a href="https://oreil.ly/fvkiT">Amazon Textractor</a>, <a href="https://oreil.ly/CoX46">Unstructured</a>, and layout-aware language models like <a href="https://oreil.ly/Od5fA">LayoutLMv3</a>.</p>

<p>For example, using this approach we can know the scope of a subsection, and thus insert the subsection title at the beginning of each chunk comprising text from that subsection.</p>

<p>You can also use techniques like ColPali<a data-type="indexterm" data-primary="ColPali" id="id1510"/> that employ vision models to directly embed a page or section of the document and perform retrieval over it. This may remove the need for chunking entirely but might be more expensive overall.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Semantic Chunking"><div class="sect2" id="id200">
<h2>Semantic Chunking</h2>

<p>The principle behind semantic chunking<a data-type="indexterm" data-primary="semantic chunking" id="id1511"/> is that similar information should be grouped into coherent chunks. Paragraph boundaries provide a weak signal for semantic chunking, but more advanced methods can be employed. One approach is to cluster the document based on topics, with each chunk containing information pertaining to the same topic. The chunks need not necessarily be built from contiguous text if it makes sense for the application. A more advanced approach<a data-type="indexterm" data-primary="Bollinger bands based chunking" id="id1512"/> is to use <a href="https://oreil.ly/1MwK1">Bollinger bands-based chunking</a>. The book’s <a href="https://oreil.ly/llm-playbooks">GitHub repository</a> contains an experimental implementation of this form of chunking.</p>

<p>Semantic chunking can also be employed to connect different chunks with each other. Once the chunks have been assigned, similar chunks can be grouped based on embedding similarity, allowing them to be retrieved along with the chunk having the highest similarity score. Each chunk does not necessarily need to consist of content from the same document, as long as the metadata associated with each sub-chunk is retained.</p>

<p>A basic implementation of semantic chunking is available<a data-type="indexterm" data-primary="LangChain" id="id1513"/> in <a href="https://oreil.ly/tm8tk">LangChain</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Highly performant semantic chunking can be performed through LLMs. But it will be a huge cost overhead if the size of your data corpus is very large. Sometimes good old regex can be enough. Jina AI created a complex 50-line <a href="https://oreil.ly/x5UO8">regex-based chunker</a> that you can try as an initial option.</p>
</div>

<p>Despite using all these techniques, effective chunking still remains a problem.
Consider the following real-world example from a financial document:</p>
<blockquote>
<p>Page 5:
<em>All numbers in the document are in millions</em></p>

<p>Page 84:
<em>The related party transaction amounts to $213.45</em></p></blockquote>

<p>In this case the related party transaction actually amounts to $213M dollars but the LLM would never know this because the text from page 5 is not likely to be part of the same chunk.</p>

<p>A related problem is the difficulty in understanding scope boundaries. When does a subsection end and a new subsection begins? What is the scope of the rule in page 5 in the given example? What if it is overridden in the middle of a document? Not all documents have perfect visual cues or structure. Not all documents are well structured into sections, subsections, and paragraphs.
These are unsolved problems and are the cause of a sizable proportion of RAG failure modes.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Late Chunking"><div class="sect2" id="id201">
<h2>Late Chunking</h2>

<p>One way of supporting long-range dependencies<a data-type="indexterm" data-primary="late chunking" id="id1514"/> in text is to use <a href="https://oreil.ly/IxTQx">late chunking</a>, a method introduced by Jina AI<a data-type="indexterm" data-primary="Jina AI" id="id1515"/>. Recall from earlier in the chapter that embeddings are generated by typically pooling the vectors from the last layer of the underlying language model.</p>

<p>Given that we have access to long-context language models that can accept an entire long document in a single input, we can use such a long-context model as our underlying model for generating embeddings. We feed an entire document (or as large a part as the model can handle)  to the long-context model, so that vectors are generated for each of the input tokens. As explained in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, each token vector encapsulates its meaning based on its relationship with all other tokens in the sequence. This enables long-context dependencies to be captured.</p>

<p>The pooling operation to extract the embeddings is performed on smaller segments of the input, where the segment boundaries can be determined by any of the chunking algorithms. Thus, we can have several embeddings representing the same document but each of them representing distinct parts of the input.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1516">
<h1>Exercise</h1>
<p>Take Apple’s <a href="https://oreil.ly/Vfizz">annual report</a>, called a 10-K. This is a perfect document for experimenting with chunking strategies, as it is long, contains a large number of sections, sub-sections, and sub-sub-sections; contains tables; and its content has long-range dependencies. Split this document into chunks using all the different strategies presented in this chapter and generate vectors for them. Try asking questions about the annual report and evaluate the results. What chunking strategies are the most effective? What is the ideal granularity for generating the vectors?</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Vector Databases"><div class="sect1" id="id202">
<h1>Vector Databases</h1>

<p>Depending on your application<a data-type="indexterm" data-primary="vector databases" id="xi_vectordatabases1158430"/>, you may have to deal with millions or billions of vectors, with the need to generate and store new vectors and their associated metadata tags every day. Vector databases facilitate this. Both self-hosted and cloud-based, open source, and proprietary options are available. Weviate, Milvus, Pinecone, Chroma, Qdrant, and LanceDB are some of the popular vector databases. More established players like ElasticSearch, Redis, and Postgres also provide vector database support.</p>

<p>These days, the features provided by vector databases are converging, given the prevalence of a small set of very popular retrieval use cases.</p>

<p>Let’s now look at how vector databases work. Probably the simplest one to get started with is Chroma<a data-type="indexterm" data-primary="Chroma" id="id1517"/>, which is open source and can run locally on your machine or can be deployed on AWS:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">chromadb</code>

<code class="kn">import</code> <code class="nn">chromadb</code>
<code class="n">chroma_client</code> <code class="o">=</code> <code class="n">chromadb</code><code class="o">.</code><code class="n">Client</code><code class="p">()</code>

<code class="n">collection</code> <code class="o">=</code> <code class="n">chroma_client</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"mango_science"</code><code class="p">)</code>
<code class="n">chunks</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'353 varieties of mangoes are now extinct'</code><code class="p">,</code>
<code class="s1">'Mangoes are grown in the tropics'</code><code class="p">]</code>
<code class="n">metadata</code> <code class="o">=</code> <code class="p">[{</code><code class="s2">"topic"</code><code class="p">:</code> <code class="s2">"extinction"</code><code class="p">,</code> <code class="s2">"chapter"</code><code class="p">:</code> <code class="s2">"2"</code><code class="p">},</code> <code class="p">{</code><code class="s2">"topic"</code><code class="p">:</code> <code class="s2">"regions"</code><code class="p">,</code>
  <code class="s2">"chapter"</code><code class="p">:</code> <code class="s2">"5"</code><code class="p">}]</code>
<code class="n">unique_ids</code> <code class="o">=</code> <code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">chunks</code><code class="p">))]</code>

<code class="n">collection</code><code class="o">.</code><code class="n">add</code><code class="p">(</code>
   <code class="n">documents</code><code class="o">=</code><code class="n">chunks</code><code class="p">,</code>
   <code class="n">metadatas</code><code class="o">=</code><code class="n">metadata</code><code class="p">,</code>
   <code class="n">ids</code><code class="o">=</code><code class="n">unique_ids</code>
  <code class="p">)</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">collection</code><code class="o">.</code><code class="n">query</code><code class="p">(</code>
   <code class="n">query_texts</code><code class="o">=</code><code class="p">[</code><code class="s2">"Where are mangoes grown?"</code><code class="p">],</code>
   <code class="n">n_results</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
   <code class="n">where</code><code class="o">=</code><code class="p">{</code><code class="s2">"chapter"</code><code class="p">:</code> <code class="p">{</code> <code class="s2">"$ne"</code><code class="p">:</code> <code class="s2">"2"</code><code class="p">}},</code>
   <code class="n">where_document</code><code class="o">=</code><code class="p">{</code><code class="s2">"$contains"</code><code class="p">:</code><code class="s2">"grown"</code><code class="p">}</code>
<code class="p">)</code></pre>

<p>Most vector databases offer<a data-type="indexterm" data-startref="xi_embeddingschunking1149550" id="id1518"/><a data-type="indexterm" data-startref="xi_vectordatabases1158430" id="id1519"/><a data-type="indexterm" data-startref="xi_chunkingembeddings1149550" id="id1520"/>:</p>

<ul>
<li>
<p>Approximate nearest neighbor search in addition to exact search, to reduce latency</p>
</li>
<li>
<p>Ability to filter using metadata, like the <em>where</em>  clause in SQL</p>
</li>
<li>
<p>Ability to integrate keyword search or algorithms like BM25</p>
</li>
<li>
<p>Support Boolean search operations, so that multiple search clauses can be combined with AND or OR operations</p>
</li>
<li>
<p>Ability to update or delete entries in the database in real time</p>
</li>
</ul>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1521">
<h1>Multi-Level Embeddings</h1>
<p>If your retrieval performance requirements are stringent, a good strategy is to use multiple levels of embeddings<a data-type="indexterm" data-primary="multilevel embeddings" id="id1522"/> if the cost justifies it. As an example, you can have sentence embeddings, paragraph or dialog-turn embeddings, section/subsection embeddings, or even document embeddings. The higher-level embeddings can represent the summary of the text and not necessarily the verbatim text itself.</p>

<p>You can use different embedding models at each level. As you go up in granularity, you can use more expensive and high-quality embedding models.</p>

<p>Depending on your specific use case, you can start from the top level and then propagate to the bottom like a tree or directly target a particular level.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Interpreting Embeddings"><div class="sect1" id="id203">
<h1>Interpreting Embeddings</h1>

<p>What features of text do embeddings<a data-type="indexterm" data-primary="embeddings" data-secondary="interpreting" id="id1523"/><a data-type="indexterm" data-primary="interpretability, model" id="id1524"/> learn? Why are two sentences sometimes closer to/farther from each other in the embedding space than we expect? Can we know what each dimension of an embedding vector represents?</p>

<p>A key limitation in embedding-based retrieval compared to traditional techniques is the lack of interpretability in ranking decisions. There is a whole body of research dedicated to improving interpretability of neural networks, LLMs, and embeddings. In <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, we introduced some interpretability techniques for understanding LLMs. In this section, we will focus on embedding interpretability in particular. One benefit of understanding the features represented in embedding space is that we could leverage that knowledge to steer embeddings for our own purposes.</p>

<p>One promising technique for imparting interpretability is to use SAEs<a data-type="indexterm" data-primary="sparse autoencoders (SAEs)" id="id1525"/><a data-type="indexterm" data-primary="SAEs (sparse autoencoders)" id="id1526"/>. Let’s understand what they mean and how they are trained and used to enhance interpretability.</p>

<p>A language model may learn millions of features, but for any given input, only a few of those features are relevant or activated. This is what we mean by sparsity. Even as they learn lots of features, there are only a limited number of dimensions in an embedding vector. Therefore, each dimension contributes to many features that can interfere with each other. If you train a <a href="https://oreil.ly/oiXb7">sparse autoencoder</a> over these embeddings, you can derive independent interpretable features.</p>

<p>In his <a href="https://oreil.ly/efzz1">Prism project</a>,  Linus Lee uses SAEs to explore the features of a T5-based embedding model.</p>

<p>Some of the identified features include:</p>

<ul>
<li>
<p>Presence of negation</p>
</li>
<li>
<p>Expression of possibility or speculation</p>
</li>
<li>
<p>Employment and labor concepts</p>
</li>
<li>
<p>Possessive syntax at sentence start</p>
</li>
</ul>

<p>For a longer list of identified features, refer to <a href="https://oreil.ly/efzz1">Linus Lee’s blog post</a>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id204">
<h1>Summary</h1>

<p>In this chapter, we introduced the concept of embeddings, examined their internals, and showed various techniques for generating them<a data-type="indexterm" data-startref="xi_embeddings11473" id="id1527"/>. We also discussed techniques for fine-tuning embeddings on our own data. We learned how to determine the data granularities at which we construct embeddings, discussing several chunking techniques in the process. Finally, we explored techniques to visualize and interpret embeddings.</p>

<p>In the next chapter, we will explore RAG, an application paradigm that is by far the most popular use case for embeddings today. We will present the steps involved in a typical RAG workflow and review each of these steps in detail. We will also discuss the technical decisions involved in building a RAG application and provide pointers on how to think through various tradeoffs.</p>
</div></section>
</div></section></div>
</div>
</body></html>