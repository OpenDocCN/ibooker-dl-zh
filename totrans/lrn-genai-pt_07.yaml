- en: '6 CycleGAN: Converting blond hair to black hair'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind CycleGAN and cycle consistency loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CycleGAN model to translate images from one domain to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a CycleGAN by using any dataset with two domains of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting black hair to blond hair and vice versa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generative adversarial networks (GAN) models we have discussed in the last
    three chapters are all trying to produce images that are indistinguishable from
    those in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering: Can we translate images from one domain to another, such
    as transforming horses into zebras, converting black hair to blond hair or blond
    hair to black, adding or removing eyeglasses in images, turning photographs into
    paintings, or converting winter scenes to summer scenes? It turns out you can,
    and you’ll acquire such skills in this chapter through CycleGAN!'
  prefs: []
  type: TYPE_NORMAL
- en: CycleGAN was introduced in 2017.^([1](#footnote-000)) The key innovation of
    CycleGAN is its ability to learn to translate between domains without paired examples.
    CycleGAN has a variety of interesting and useful applications, such as simulating
    the aging or rejuvenation process on faces to assist digital identity verification
    or visualizing clothing in different colors or patterns without physically creating
    each variant to streamline the design process.
  prefs: []
  type: TYPE_NORMAL
- en: 'CycleGAN uses a cycle consistency loss function to ensure the original image
    can be reconstructed from the transformed image, encouraging the preservation
    of key features. The idea behind cycle consistency loss is truly ingenious and
    deserves to be highlighted here. The CycleGAN in this chapter has two generators:
    let’s call them the black hair generator and the blond hair generator, respectively.
    The black hair generator takes in an image with blond hair (instead of a random
    noise vector as you have seen before) and converts it to one with black hair,
    while the blond hair generator takes in an image with black hair and converts
    it to one with blond hair.'
  prefs: []
  type: TYPE_NORMAL
- en: To train the model, we’ll give a real image with black hair to the blond hair
    generator to produce a fake image with blond hair. We’ll then give the fake blond
    hair image to the black hair generator to convert it back to an image with black
    hair. If both generators work well, there is little difference between the original
    image with black hair and the fake one after a round-trip conversion. To train
    the CycleGAN, we adjust the model parameters to minimize the sum of adversarial
    losses and cycle consistency losses. As in chapters 3 and 4, adversarial losses
    are used to quantify how well the generator can fool the discriminator and how
    well the discriminator can differentiate between real and fake samples. Cycle
    consistency loss, a unique concept in CycleGANs, measures the difference between
    the original image and the fake image after a round-trip conversion. The inclusion
    of the cycle consistency loss in the total loss function is the key innovation
    in CycleGANs.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use black and blond hair images as examples of two domains when training
    CycleGAN. However, the model can be applied to any two domains of images. To drive
    home the message, I’ll ask you to train the same CycleGAN model by using images
    with and without eyeglasses that you used in chapter 5\. The solution is provided
    in the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)),
    and you’ll see that the trained model can indeed add or remove eyeglasses from
    human face images.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 CycleGAN and cycle consistency loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CycleGAN extends the basic GAN architecture to include two generators and two
    discriminators. Each generator-discriminator pair is responsible for learning
    the mapping between two distinct domains. It aims to translate images from one
    domain to another (e.g., horses to zebras, summer to winter scenes, and so on)
    while retaining the key characteristics of the original images. It uses a cycle
    consistency loss that ensures the original image can be reconstructed from the
    transformed image, encouraging the preservation of key features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll first discuss the architecture of CycleGAN. We’ll emphasize
    the key innovation of CycleGANs: cycle consistency loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 What is CycleGAN?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CycleGAN consists of two generators and two discriminators. The generators translate
    images from one domain to another, while the discriminators determine the authenticity
    of the images in their respective domains. These networks are capable of transforming
    photographs into artworks mimicking the style of famous painters or specific art
    movements, thereby bridging the gap between art and technology. They can also
    be used in healthcare for tasks like converting MRI images to CT scans or vice
    versa, which can be helpful in situations where one type of imaging is unavailable
    or too costly.
  prefs: []
  type: TYPE_NORMAL
- en: For our project in this chapter, we’ll convert between images with black hair
    and blond hair. We therefore use them as an example when explaining how CycleGAN
    works. Figure 6.1 is a diagram of the CycleGAN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 The architecture of a CycleGAN to convert images with black hair
    to ones with blond hair and to convert images with blond hair to ones with black
    hair. The diagram also outlines the training steps to minimize adversarial losses.
    How the model minimizes cycle consistency losses is explained in figure 6.2.
  prefs: []
  type: TYPE_NORMAL
- en: To train CycleGAN, we use unpaired datasets from the two domains we wish to
    translate between. We’ll use 48,472 celebrity face images with black hair and
    29,980 images with blond hair. We adjust the model parameters to minimize the
    sum of adversarial losses and cycle consistency losses. For ease of explanation,
    we’ll explain only adversarial losses in figure 6.1\. I’ll explain how the model
    minimizes cycle consistency losses in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: In each iteration of training, we feed real black hair images (top left in figure
    6.1) to the blond hair generator to obtain fake blond hair images. We then feed
    the fake blond hair images, along with real blond hair images, to the blond hair
    discriminator (top middle). The blond hair discriminator produces a probability
    that each one is a real blond hair image. We then compare the predictions with
    the ground truth (whether an image is a true image with blond hair) and calculate
    the loss to the discriminator (`Loss_D_Blond`) as well as the loss to the generator
    (`Loss_G_Blond`).
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, in each iteration of training, we feed real blond hair images
    (middle left) to the black hair generator (bottom left) to create fake black hair
    images. We present the fake black hair images, along with real ones, to the black
    hair discriminator (middle bottom) to obtain predictions that they are real. We
    compare the predictions from the black hair discriminator with the ground truth
    and calculate the loss to the discriminator (`Loss_D_Black`) and the loss to the
    generator (`Loss_G_Black`). We train the generators and discriminators simultaneously.
    To train the two discriminators, we adjust the model parameters to minimize the
    discriminator loss, which is the sum of `Loss_D_Black` and `Loss_D_Blond`.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Cycle consistency loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train the two generators, we adjust the model parameters to minimize the
    sum of the adversarial loss and cycle consistency loss. The adversarial loss is
    the sum of `Loss_G_Black` and `Loss_G_Blond` that we discussed in the previous
    subsection. To explain cycle consistency loss, let’s look at figure 6.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 How CycleGAN minimizes cycle consistency losses between original
    black hair images and fake ones after round trips and cycle consistency losses
    between original blond hair images and fake ones after round trips
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for the generators in CycleGAN consists of two parts. The
    first part, the adversarial loss, ensures that generated images are indistinguishable
    from real images in the target domain. For example, `Loss_G_Blond` (defined in
    the previous subsection) ensures that fake blond images produced by the blond
    hair generator resemble real images with blond hair in the training set. The second
    part, the cycle consistency loss, ensures that an image translated from one domain
    to another can be translated back to the original domain.
  prefs: []
  type: TYPE_NORMAL
- en: The cycle consistency loss is a crucial component of CycleGANs, ensuring that
    the original input image can be recovered after a round-trip translation. The
    idea is that if you translate a real black hair image (top left in figure 6.2)
    to a fake blond hair image and convert it back to a fake black hair image (top
    right), you should end up with an image close to the original black hair image.
    The cycle consistency loss for black hair images is the mean absolute error, at
    the pixel level, between the fake image and the original real one. Let’s call
    this loss `Loss_Cycle_Black`. The same applies to translating blond hair to black
    hair and then back to blond hair, and we call this loss `Loss_Cycle_Blond`. The
    total cycle consistency loss is the sum of `Loss_Cycle_Black` and `Loss_Cycle_Blond`.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 The celebrity faces dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use celebrity face images with black hair and blond hair as the two domains.
    You’ll first download the data in this section. You’ll then process the images
    to get them ready for training later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use two new Python libraries in this chapter: `pandas` and `albumentations`.
    To install these libraries, execute the following line of code in a new cell in
    your Jupyter Notebook application on your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Follow the on-screen instructions to finish the installation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Downloading the celebrity faces dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To download the celebrity faces dataset, log into Kaggle and go to the link
    [https://mng.bz/Ompo](https://mng.bz/Ompo). Unzip the dataset after downloading
    and place all image files inside the folder /files/img_align_celeba/img_align_celeba/
    on your computer (note there is a subfolder with the same name in the folder itself).
    There are about 200,000 images in the folder. Also download the file `list_attr_celeba.csv`
    from Kaggle and place it in the /files/ folder on your computer. The CSV file
    specifies various attributes of each image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The celebrity faces dataset contains images with many different hair colors:
    brown, gray, black, blond, and so on. We’ll select images with black or blond
    hair as our training set because these two types are the most abundant in the
    celebrity faces dataset. Run the code in the following listing to select all images
    with black or blond hair.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Selecting images with black or blond hair
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Loads the CSV file that contains image attributes
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates two folders to store images with black and blond hair
  prefs: []
  type: TYPE_NORMAL
- en: ③ If the attribute Black_Hair is 1, moves the image to the black folder.
  prefs: []
  type: TYPE_NORMAL
- en: ④ If the attribute Blond_Hair is 1, moves the image to the blond folder.
  prefs: []
  type: TYPE_NORMAL
- en: We first use the `pandas` library to load the file `list_attr_celeba.csv` so
    that we know whether each image has black or blond hair in it. We then create
    two folders locally, /files/black/ and /files/blond/, to store images with black
    and blond hair, respectively. Listing 6.1 then iterates through all images in
    the dataset. If an image’s attribute `Black_Hair` is 1, we move it to the folder
    /files/black/; if an image’s attribute `Blond_Hair` is 1, we move it to the folder
    /files/blond/. You’ll see 48,472 images with black hair and 29,980 images with
    blond hair. Figure 6.3 shows some examples of the images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Sample images of celebrity faces with black or blond hair
  prefs: []
  type: TYPE_NORMAL
- en: 'Images in the top row of figure 6.3 have black hair while images in the bottom
    row have blond hair. Further, the image quality is high: all faces are front and
    center, and hair colors are easy to identify. The quantity and quality of the
    training data will help the training of the CycleGAN model.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Process the black and blond hair image data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll generalize the CycleGAN model so that it can be trained on any dataset
    with two domains of images. We’ll also define a `LoadData()` class to process
    the training dataset for the CycleGAN model. The function can be applied to any
    dataset with two domains, whether human face images with different hair colors,
    images with or without eyeglasses, or images with summer and winter scenes.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, we have created a local module `ch06util`. Download the files `ch06util.py`
    and `__init__.py` from the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and place them in the folder /utils/ on your computer. In the local module, we
    have defined the following `LoadData()` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 The `LoadData()` class to process the training data in CycleGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① The two folders root_A and root_B are where the images in the two domains
    are stored
  prefs: []
  type: TYPE_NORMAL
- en: ② Loads all images in each domain
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a method to count the length of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a method to access individual elements in each domain
  prefs: []
  type: TYPE_NORMAL
- en: The `LoadData()` class is inherited from the `Dataset` class in PyTorch. The
    two lists `root_A` and `root_B` contain folders of images in domains A and B,
    respectively. The class loads up images in the two domains and produces a pair
    of images, one from domain A and one from domain B so that we can use the pair
    to train the CycleGAN model later.
  prefs: []
  type: TYPE_NORMAL
- en: As we did in previous chapters, we create a data iterator with batches to improve
    computational efficiency, memory usage, and optimization dynamics in the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Processing the black and blond hair images for training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Resizes the images to 256 by 256 pixels
  prefs: []
  type: TYPE_NORMAL
- en: ② Normalizes the images to the range of -1 to 1
  prefs: []
  type: TYPE_NORMAL
- en: ③ Applies the LoadData() class on the images
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates a data iterator for training
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define an instance of the `Compose()` class in the `albumentations`
    library (which is famous for fast and flexible image augmentations) and call it
    `transforms`. The class transforms the images in several ways: it resizes images
    to 256 by 256 pixels and normalizes the values to the range –1 to 1\. The `HorizontalFlip()`
    argument in listing 6.3 creates a mirror image of the original image in the training
    set. Horizontal flipping is a simple yet powerful augmentation technique that
    enhances the diversity of training data, helping models generalize better and
    become more robust. The augmentations and increase in size boost the performance
    of the CycleGAN model and make the generated images realistic.'
  prefs: []
  type: TYPE_NORMAL
- en: We then apply the `LoadData()` class to the black and blond hair images. We
    set the batch size to 1 since the images have a large file size, and we use a
    pair of images to train the model in each iteration. Setting the batch size to
    more than 1 may result in your machine running out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Building a CycleGAN model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll build a CycleGAN model from scratch in this section. We’ll take great
    care to make our CycleGAN model general so that it can be trained using any dataset
    with two domains of images. As a result, we’ll use A and B to denote the two domains
    instead of, for example, black and blond hair images. As an exercise, you’ll train
    the same CycleGAN model by using the eyeglasses dataset that you used in chapter
    5\. This helps you apply the skills you learned in this chapter to other real-world
    applications by using a different dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Creating two discriminators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Even though CycleGAN has two discriminators, they are identical ex ante. Therefore,
    we’ll create one single `Discriminator()` class and then instantiate the class
    twice: one instance is discriminator A and the other discriminator B. The two
    domains in CycleGAN are symmetric, and it doesn’t matter which domain we call
    domain A: images with black hair or images with blond hair.'
  prefs: []
  type: TYPE_NORMAL
- en: Open the file `ch06util.py` you just downloaded. In it, I have defined the `Discriminator()`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Defining the `Discriminator()` class in CycleGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① The first Conv2d layer has 3 input channels and 64 output channels.
  prefs: []
  type: TYPE_NORMAL
- en: ② Three more Conv2d layers with 126, 256, and 512 output channels, respectively
  prefs: []
  type: TYPE_NORMAL
- en: ③ The last Conv2d layer has 512 input channels and 1 output channel.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Applies the sigmoid activation function on the output so it can be interpreted
    as a probability
  prefs: []
  type: TYPE_NORMAL
- en: The previous code listing defines the discriminator network. The architecture
    is similar to the discriminator network in chapter 4 and the critic network in
    chapter 5\. The main components are five `Conv2d` layers. We apply the sigmoid
    activation function on the last layer because the discriminator performs a binary
    classification problem. The discriminator takes a three-channel color image as
    input and produces a single number between 0 and 1, which can be interpreted as
    the probability that the input image is a real image in the domain.
  prefs: []
  type: TYPE_NORMAL
- en: The `padding_mode="reflect"` argument we used in listing 6.4 means the padding
    added to the input tensor is a reflection of the input tensor itself. Reflect
    padding helps in preserving the edge information by not introducing artificial
    zero values at the borders. It creates smoother transitions at the boundaries
    of the input tensor, which is beneficial for differentiating images in different
    domains in our setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create two instances of the class and call them `disc_A` and `disc_B`,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Imports the Discriminator class from the local module
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates two instances of the Discriminator class
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initializes weights
  prefs: []
  type: TYPE_NORMAL
- en: In the local module `ch06util`, we also defined a `weights_init()` function
    to initialize model weights. The function is defined similarly to the one in chapter
    5\. We then initialize weights in the two newly created discriminators, `disc_A`
    and `disc_B`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have two discriminators, we’ll create two generators next.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Creating two generators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, we define a single `Generator()` class in the local module and instantiate
    the class twice: one instance is generator A, and the other is generator B. In
    the file `ch06util.py` you just downloaded, we have defined the `Generator()`
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 The `Generator()` class in CycleGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Three Conv2d layers
  prefs: []
  type: TYPE_NORMAL
- en: ② Nine residual blocks
  prefs: []
  type: TYPE_NORMAL
- en: ③ Two upsampling blocks
  prefs: []
  type: TYPE_NORMAL
- en: ④ Applies tanh activation on the output
  prefs: []
  type: TYPE_NORMAL
- en: The generator network consists of several `Conv2d` layers, followed by nine
    residual blocks (which I’ll explain in detail later). After that, the network
    has two upsampling blocks that consist of a `ConvTranspose2d` layer, an `InstanceNorm2d`
    layer, and a `ReLU` activation. As we have done in previous chapters, we use the
    tanh activation function at the output layer, so the output pixels are all in
    the range of –1 to 1, the same as the images in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The residual block in the generator is defined in the local module as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A residual connection is a concept in deep learning, particularly in the design
    of deep neural networks. You’ll see it quite often later in this book. It’s a
    technique used to address the problem of vanishing gradients, which often occurs
    in very deep networks. In a residual block, which is the basic unit of a network
    with residual connections, the input is passed through a series of transformations
    (like convolution, activation, and batch or instance normalization) and then added
    back to the output of these transformations. Figure 6.4 provides a diagram of
    the architecture of the residual block defined previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 The architecture of a residual block. The input x is passed through
    a series of transformations (two sets of Conv2d layer and InstanceNorm2d layer
    and a ReLU activation). The input x is then added back to the output of these
    transformations, f(x). The output of the residual block is therefore x + f(x).
  prefs: []
  type: TYPE_NORMAL
- en: The transformations in each residual block are different. In this example, the
    input x is passed through two sets of `Conv2d` layer and `InstanceNorm2d` layer
    and a ReLU activation in between. The input x is then added back to the output
    of these transformations, f(x), to form the final output, x+f(x)—hence the name
    residual connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create two instances of the `Generator()` class and call one of them
    `gen_A` and the other `gen_B`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When training the model, we’ll use the mean absolute error (i.e., L1 loss)
    to measure the cycle consistency loss. We’ll use the mean squared error (i.e.,
    L2 loss) to gauge the adversarial loss. L1 loss is often used if the data are
    noisy and have many outliers since it punishes extreme values less than the L2
    loss. Therefore, we import the following loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Both L1 and L2 losses are calculated at the pixel level. The original image
    has a shape of (3, 256, 256) and so is the fake image. To calculate the losses,
    we first calculate the difference (absolute value of this difference for L1 loss
    and the squared value of this difference for L2 loss) between the corresponding
    pixel values between two images at each of the 3 × 256 × 256 = 196608 positions
    and average them over the positions.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use PyTorch’s automatic mixed precision package `torch.cuda.amp` to speed
    up training. The default data type in PyTorch tensors is `float32`, a 32-bit floating-point
    number, which takes up twice as much memory as a 16-bit floating number, `float16`.
    Operations on the former are slower than those on the latter. There is a trade-off
    between precision and computational costs. Which data type to use depends on the
    task at hand. `torch.cuda.amp` provides an automatic mixed precision, where some
    operations use `float32` and others `float16`. Mixed precision tries to match
    each operation to its appropriate data type to speed up training.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have done in chapter 4, we’ll use the Adam optimizer for both the discriminators
    and the generators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll train the CycleGAN model by using images with black or blond hair.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Using CycleGAN to translate between black and blond hair
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the training data and the CycleGAN model, we’ll train the model
    by using images with black or blond hair. As with all GAN models, we’ll discard
    the discriminators after training. We’ll use the two trained generators to convert
    black hair images to blond hair ones and convert blond hair images to black hair
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Training a CycleGAN to translate between black and blond hair
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we explained in chapter 4, we’ll use visual inspections to determine when
    to stop training. To that end, we create a function to test what the real images
    look like and what the corresponding generated images look like so that we can
    compare the two to visually inspect the effectiveness of the model. In the local
    module `ch06util`, we define a `test()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Real images in domains A and B, saved in a local folder
  prefs: []
  type: TYPE_NORMAL
- en: ② The corresponding fake images in domains A and B, created by the generators
    in batch i
  prefs: []
  type: TYPE_NORMAL
- en: We save four images after every 100 batches of training. We save real images
    and the corresponding fake images in the two domains in the local folder so we
    can periodically check the generated images and compare them with the real ones
    to assess the progress of training. We made the function general so that it can
    be applied to images from any two domains.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we define a `train_epoch()` function in the local module `ch06util`
    to train the discriminators and the generators for an epoch. The following listing
    highlights the code we use to train the two discriminators.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Training the two discriminators in CycleGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all pairs of images in the two domains
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses PyTorch automatic mixed precision package to speed up training
  prefs: []
  type: TYPE_NORMAL
- en: ③ The total loss for the two discriminators is the simple average of the adversarial
    losses to the two discriminators.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `detach()` method here to remove gradients in tensors `fake_A` and
    `fake_B` to reduce memory and speed up computations. The training for the two
    discriminators is similar to what we have done in chapter 4, with a couple of
    differences. First, instead of having just one discriminator, we have two discriminators
    here: one for images in domain A and one for images in domain B. The total loss
    for the two discriminators is the simple average of the adversarial losses of
    the two discriminators. Second, we use the PyTorch automatic mixed precision package
    to speed up training, reducing the training time by more than 50%.'
  prefs: []
  type: TYPE_NORMAL
- en: We simultaneously train the two generators in the same iteration. The following
    listing highlights the code we use to train the two generators.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Training the two generators in CycleGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Adversarial losses to the two generators
  prefs: []
  type: TYPE_NORMAL
- en: ② Cycle consistency losses for the two generators
  prefs: []
  type: TYPE_NORMAL
- en: ③ The total loss for the two generators is the weighted sum of adversarial losses
    and cycle consistency losses.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Generates images for visual inspection after every 100 batches of training
  prefs: []
  type: TYPE_NORMAL
- en: The training for the two generators is different from what we have done in chapter
    4 in two important ways. First, instead of having just one generator, we train
    two generators simultaneously here. Second, the total loss for the two generators
    is the weighted sum of adversarial losses and cycle consistency losses, and we
    weigh the latter 10 times more than the former loss. However, if you change the
    value 10 to other numbers such as 9 or 12, you’ll get similar results.
  prefs: []
  type: TYPE_NORMAL
- en: The cycle consistency loss is the mean absolute error between the original image
    and the fake image that’s translated back to the original domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have everything ready, we’ll start the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Trains the CycleGAN for one epoch using the black and blond hair images
  prefs: []
  type: TYPE_NORMAL
- en: ② Saves the trained model weights
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding training takes a couple of hours if you use GPU training. It
    may take a whole day otherwise. If you don’t have the computing resources to train
    the model, download the pretrained generators from my website: [https://gattonweb.uky.edu/faculty/lium/ml/hair.zip](https://gattonweb.uky.edu/faculty/lium/ml/hair.zip).
    Unzip the file and place the files `gen_black.pth` and `gen_blond.pth` in the
    folder /files/ on your computer. You’ll be able to convert between black hair
    images and blond hair ones in the next subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.1
  prefs: []
  type: TYPE_NORMAL
- en: When training the CycleGAN model, we assume that domain A contains images with
    black hair and domain B contains images with blond hair. Modify the code in listing
    6.2 so that domain A contains images with blond hair and domain B contains images
    with black hair.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Round-trip conversions of black hair images and blond hair images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the high quality and the abundant quantity of the training dataset, we
    have trained the CycleGAN with great success. We’ll not only convert between images
    with black hair and images with blond hair, but we’ll also conduct round-trip
    conversions. For example, we’ll convert images with black hair to images with
    blond hair and then convert them back to images with black hair. That way, we
    can compare the original images with the generated images in the same domain after
    a round trip and see the difference.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing performs conversions of images between the two domains
    as well as round-trip conversions of images in each domain.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 Round-trip conversions of images with black or blond hair
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Original image with black hair
  prefs: []
  type: TYPE_NORMAL
- en: ② A fake image with black hair after a round trip
  prefs: []
  type: TYPE_NORMAL
- en: ③ Original image with blond hair
  prefs: []
  type: TYPE_NORMAL
- en: ④ A fake image with blond hair after a round trip
  prefs: []
  type: TYPE_NORMAL
- en: 'We have saved six sets of images in your local folder /files/. The first set
    is the original images with black hair. The second set is the fake blond images
    produced by the trained blond hair generator: the images are saved as `fakeblond0.png`,
    `fakeblond1.png`, and so on. The third set is the fake images with black hair
    after a round trip: we feed the fake images we just created to the trained black
    hair generator to obtain fake images with black hair. They are saved as `fake2black0.png`,
    `fake2black1.png`, and so on. Figure 6.5 shows the three sets of images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5 A round-trip conversion of images with black hair. Images in the
    top row are the original images with black hair from the training set. Images
    in the middle row are the corresponding fake images with blond hair, produced
    by the trained blond hair generator. Images in the bottom row are fake images
    with black hair after a round trip: we feed the images in the middle row to the
    trained black hair generator to create fake images with black hair.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three rows of images in figure 6.5\. The top row displays original
    images with black hair from the training set. The middle row displays fake blond
    hair images produced by the trained blond hair. The bottom row contains fake black
    hair images after a round-trip conversion: the images look almost identical to
    the ones in the top row! Our trained CycleGAN model works extremely well.'
  prefs: []
  type: TYPE_NORMAL
- en: The fourth set of images in the local folder /files/ are the original images
    with blond hair. The fifth set is the fake image produced by the trained black
    hair generator. Finally, the sixth set contains fake images with blond hair after
    a round trip. Figure 6.6 compares these three sets of images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6 A round-trip conversion of images with blond hair. Images in the
    top row are the original images with blond hair from the training set. Images
    in the middle row are the corresponding fake images with black hair, produced
    by the trained black hair generator. Images in the bottom row are fake images
    with blond hair after a round-trip conversion: we feed the images in the middle
    row to the trained blond hair generator to create fake images with blond hair.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 6.6, fake black hair images produced by the trained black hair generator
    are shown in the middle row: they have black hair on the same human faces as the
    top row. Fake blond hair images after a round trip are shown in the bottom row:
    they look almost identical to the original blond hair images in the top row.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.2
  prefs: []
  type: TYPE_NORMAL
- en: The CycleGAN model is general and can be applied to any training dataset with
    two domains of images. Train the CycleGAN model using the eyeglasses images that
    you downloaded in chapter 5\. Use images with glasses as domain A and images without
    glasses as domain B. Then use the trained CycleGAN to add and remove eyeglasses
    from images (i.e., translating images between the two domains). An example implementation
    and results are in the book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on one type of generative model, GANs. In the next chapter,
    you’ll learn to use another type of generative model, variational autoencoders
    (VAEs), to generate high-resolution images. You’ll learn the advantages and disadvantages
    of VAEs compared to GANs. More importantly, you’ll learn the encoder-decoder architecture
    in VAEs. The architecture is widely used in generative models, including Transformers,
    which we’ll study later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CycleGAN can translate images between two domains without paired examples. It
    consists of two discriminators and two generators. One generator converts images
    in domain A to domain B while the other generator converts images in domain B
    to domain A. The two discriminators classify if a given image is from a specific
    domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CycleGAN uses a cycle consistency loss function to ensure the original image
    can be reconstructed from the transformed image, encouraging the preservation
    of key features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A properly constructed CycleGAN model can be applied to any dataset with images
    from two domains. The same model can be trained with different datasets and be
    used to translate images in different domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we have abundant high-quality training data, the trained CycleGAN can convert
    images in one domain to another and convert them back to the original domain.
    The images after a round-trip conversion can potentially look almost identical
    to the original images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](#footnote-000-backlink))  Jun-Yan Zhu, Taesung Park, Phillip Isola, and
    Alexie Efros, 2017, “Unpaired Image-to-Image Translation Using Cycle Consistent
    Adversarial Networks.” [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593).
  prefs: []
  type: TYPE_NORMAL
