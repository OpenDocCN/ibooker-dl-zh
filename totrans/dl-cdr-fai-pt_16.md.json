["```py\nfrom fastai.text.all import *\npath = untar_data(URLs.HUMAN_NUMBERS)\n```", "```py\npath.ls()\n```", "```py\n(#2) [Path('train.txt'),Path('valid.txt')]\n```", "```py\nlines = L()\nwith open(path/'train.txt') as f: lines += L(*f.readlines())\nwith open(path/'valid.txt') as f: lines += L(*f.readlines())\nlines\n```", "```py\n(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven\n > \\n','eight \\n','nine \\n','ten \\n'...]\n```", "```py\ntext = ' . '.join([l.strip() for l in lines])\ntext[:100]\n```", "```py\n'one . two . three . four . five . six . seven . eight . nine . ten . eleven .\n > twelve . thirteen . fo'\n```", "```py\ntokens = text.split(' ')\ntokens[:10]\n```", "```py\n['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']\n```", "```py\nvocab = L(*tokens).unique()\nvocab\n```", "```py\n(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]\n```", "```py\nword2idx = {w:i for i,w in enumerate(vocab)}\nnums = L(word2idx[i] for i in tokens)\nnums\n```", "```py\n(#63095) [0,1,2,1,3,1,4,1,5,1...]\n```", "```py\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n```", "```py\n(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four',\n > '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'],\n > '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.',\n > 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.',\n > 'fifteen', '.'], 'sixteen')...]\n```", "```py\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\n```", "```py\n(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]),\n > 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]),\n > 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1,\n > 14]), 1),(tensor([ 1, 15,  1]), 16)...]\n```", "```py\nbs = 64\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n```", "```py\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n\n    def forward(self, x):\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        h = h + self.i_h(x[:,1])\n        h = F.relu(self.h_h(h))\n        h = h + self.i_h(x[:,2])\n        h = F.relu(self.h_h(h))\n        return self.h_o(h)\n```", "```py\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n```", "```py\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n```", "```py\n(tensor(29), 'thousand', 0.15165200855716662)\n```", "```py\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n\n    def forward(self, x):\n        h = 0\n        for i in range(3):\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)\n```", "```py\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n```", "```py\nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n\n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        self.h = self.h.detach()\n        return out\n\n    def reset(self): self.h = 0\n```", "```py\nm = len(seqs)//bs\nm,bs,len(seqs)\n```", "```py\n(328, 64, 21031)\n```", "```py\n(0, m, 2*m, ..., (bs-1)*m)\n```", "```py\n(1, m+1, 2*m+1, ..., (bs-1)*m+1)\n```", "```py\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n```", "```py\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs),\n    group_chunks(seqs[cut:], bs),\n    bs=bs, drop_last=True, shuffle=False)\n```", "```py\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(10, 3e-3)\n```", "```py\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n```", "```py\n[L(vocab[o] for o in s) for s in seqs[0]]\n```", "```py\n[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n (#16) ['.','two','.','three','.','four','.','five','.','six'...]]\n```", "```py\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n\n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n\n    def reset(self): self.h = 0\n```", "```py\ndef loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n```", "```py\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n```", "```py\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n\n    def reset(self): self.h.zero_()\n```", "```py\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2),\n                loss_func=CrossEntropyLossFlat(),\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n```", "```py\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)\n```", "```py\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n```", "```py\nt = torch.arange(0,10); t\n```", "```py\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n```", "```py\nt.chunk(2)\n```", "```py\n(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))\n```", "```py\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n\n    def reset(self):\n        for h in self.h: h.zero_()\n```", "```py\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2),\n                loss_func=CrossEntropyLossFlat(),\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n```", "```py\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)\n```", "```py\nloss += alpha * activations.pow(2).mean()\n```", "```py\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n```", "```py\nself.h_o.weight = self.i_h.weight\n```", "```py\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n\n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n\n    def reset(self):\n        for h in self.h: h.zero_()\n```", "```py\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])\n```", "```py\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n```", "```py\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n```", "```py\n    h = torch.stack([h, input], dim=1)\n    ```", "```py\n    if not self.training: return x\n    ```"]