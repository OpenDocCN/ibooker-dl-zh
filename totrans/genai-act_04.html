<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">4</span> </span> <span class="chapter-title-text">From pixels to pictures: Generating images</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Generative AI vision models, their model architecture, and key use cases for enterprises</li> 
    <li class="readable-text" id="p3">Using Stable Diffusion’s GUIs and APIs for image generation and editing</li> 
    <li class="readable-text" id="p4">Using advanced editing techniques, such as inpainting, outpainting, and image variations</li> 
    <li class="readable-text" id="p5">Practical image generation tips for enterprises to consider</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>Generating images represents one of the many uses of generative AI, resulting in unique and realistic content from a mere prompt. Enterprises have been increasingly adopting generative AI to develop innovative image generation and editing solutions, which has led to many innovative use cases—from AI-powered architecture for innovative designs of buildings to fashion design, avatar generation, virtual clothes try-on, and virtual patients for medical training, to name a few. They are accompanied by exciting products such as Microsoft Designer and Adobe Firefly, and they will be covered in this chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>In the previous chapters, we talked about the fundamentals of generative AI and the technology that enables us to generate text, including completions and chats. However, in this chapter, we shift gears and explore how generative AI can be utilized to produce and adjust images. We will see how creating images is a simple process and highlight some of the complexities of getting them right.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Initially, this chapter focuses on comprehending the generative AI methods that facilitate the generation of new images and the overall workflow an enterprise must consider. The applications of these techniques are immense and can be particularly useful in the e-commerce, entertainment, and healthcare sectors. In addition, we will examine various generative AI products and services for image manipulation. Let’s dive in!</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_64"><span class="num-string">4.1</span> Vision models</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Generative AI vision models can generate realistic new images and novel concepts from a prompt. Let’s start by looking at some enterprise use cases and examples of how these generative AI vision models can help:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p11"> <em>Content creation and editing</em><em> </em>—There are multiple use cases in different industries where generative AI vision models can help media and marketing professionals generate new themes and scenarios, remove unnecessary or unwanted things from images, or apply style transfer. The specific use cases vary by industry. </li> 
   <li class="readable-text" id="p12"> <em>Healthcare</em><em> </em>—There are multiple use cases of image-generative AI in the health domain, from educating and training medical students or using new techniques (see the next item) to improving a patient’s diagnosis and prognosis by helping enhance and clear medical images. It also accelerates drug discovery and development by analyzing new novel molecules, complex molecular interactions, and their predictions, and optimizing formulation and synthesis. </li> 
   <li class="readable-text" id="p13"> <em>Education</em><em> </em>—We can create interactive visuals on the fly based on a student’s progress and current learning. This includes realistic and diverse scenarios, training simulations using data augmentation, and helping improve the teaching quality of both the educator and the student. </li> 
   <li class="readable-text" id="p14"> <em>R&amp;D</em><em> </em>—We can create a more interpretable visual representation of complex data structures and relationships that might not be obvious otherwise. These core elements help create new product designs based on trends, unique visual elements, branding, and layouts, and can discover subtle patterns in the data. </li> 
   <li class="readable-text" id="p15"> <em>Marketing</em><em> </em>—Generative AI vision models generate specific visuals tailored to the specific individual or demographic, which can also include different sets of visuals for A/B testing for understanding successful marketing campaigns. </li> 
   <li class="readable-text" id="p16"> <em>Manufacturing</em><em> </em>—Generative AI vision models have the ability to rapidly iterate and visualize new materials and components, including the assembly process. </li> 
   <li class="readable-text" id="p17"> <em>Personalization</em><em> </em>—This horizontal use case can span different dimensions by allowing us to generate personalized visuals, for example, in e-commerce settings where a shopper can visualize objects, content, clothing, and so on to create highly customized and personalized avatars for gaming and social platforms. Finally, fashion and creative fields create new patterns, layouts, clothing, and furniture designs. </li> 
  </ul> 
  <div class="readable-text" id="p18"> 
   <p>Here are some real examples of how to generate and bring some of this content to life:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p19"> <em>Creative content</em><em> </em>—Generative AI vision models can produce novel and diverse images or videos for artistic, entertainment, or marketing purposes. Some of them create realistic faces of people who seem real but do not exist, or they modify existing faces to factor in different features such as age, gender, hairstyle, and so forth. Figure 4.1 shows a panda bear generated using strawberries.<span class="aframe-location"/> </li> 
  </ul> 
  <div class="browsable-container figure-container" id="p20">  
   <img alt="figure" src="../Images/CH04_F01_Bahree.png" width="692" height="702"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.1</span> A strawberry panda</h5>
  </div> 
  <ul> 
   <li class="readable-text" id="p21"> <em>Image editing, content improvement, and style transfer</em><em> </em>—We can use generative AI vision models to enhance existing images. These can address various artifacts, such as enhancing the resolution and quality and removing unwanted elements. We can also use the style and technique of one image and transpose it onto another. For example, figure 4.2 shows us an oil painting of Seattle’s Space Needle in the style of Vincent Van Gogh.<span class="aframe-location"/> </li> 
  </ul> 
  <div class="browsable-container figure-container" id="p22">  
   <img alt="figure" src="../Images/CH04_F02_Bahree.png" width="750" height="750"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.2</span> An oil painting of the Seattle Space Needle in the style of Vincent Van Gogh</h5>
  </div> 
  <ul> 
   <li class="readable-text" id="p23"> <em>Synthetic data</em><em> </em>—We can create realistic but synthetic images using generative AI vision models. These synthetic images can be used as training and validation data for other AI models. For example, the site <a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/</a> generates the faces of people who do not exist in real life. Synthetic data come with challenges; we will discuss them later in the book when we cover generative AI challenges. </li> 
   <li class="readable-text" id="p24"> <em>Generative engineering and design</em><em> </em>—We can generate new design options that include new objects and structures that can help us optimize certain criteria or constraints, such as functionality, performance, or aesthetics. These models can generate unique, novel designs for products or digital assets, reducing the time and resources spent on manual design. Figure 4.3 shows a chair optimized for various design characteristics such as material and aesthetics. These chairs have unique and futuristic shapes different from those in conventional chairs.<span class="aframe-location"/> </li> 
  </ul> 
  <div class="browsable-container figure-container" id="p25">  
   <img alt="figure" src="../Images/CH04_F03_Bahree.png" width="684" height="692"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.3</span> A chair designed for strength, aesthetics, material, and weight</h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <p>Four main generative AI model architecture types make these use cases and examples possible: variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, and vision transformers. Each technique has its strengths and weaknesses, and we outline the right approach for the scenario it can use:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p27"> <em>Variational autoencoders</em><em> </em>—VAEs generate realistic but simple images of animals, faces, and other objects. They are good for scenarios requiring data generation, that is, new data points similar to the original but with variations. This property also allows VAEs to be used for anomaly detection and recommendation systems. </li> 
   <li class="readable-text" id="p28"> <em>Generative adversarial networks</em><em> </em>—GANs are used for scenarios where data is complex and diverse and requires a high level of realism. This makes them suitable for high-quality images, data augmentation, and style transfers. </li> 
   <li class="readable-text" id="p29"> <em>Diffusion</em><em> </em>—Diffusion-based models are used for scenarios where the data is high-dimensional and continuous, and we need to model complex data distribution with quality, with the speed of generation being unimportant. These models are good for generating speech and video, some of which we will touch on in the next chapter. </li> 
   <li class="readable-text" id="p30"> <em>Vision transformers</em><em> </em>—These are great when we want to generate images that are sequenced-based tasks, highly flexible, and adaptable to many tasks; they need significant computational resources. </li> 
  </ul> 
  <div class="readable-text" id="p31"> 
   <p>Let’s explore each of these architectures in more detail.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_65"><span class="num-string">4.1.1</span> Variational autoencoders</h3> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>VAEs are a specific generative model that has a vital role. They represent complex data distributions by combining aspects of deep learning, probability theory, and statistical mechanics. </p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>VAEs include two neural networks: an encoder and a decoder (figure 4.4). The encoder maps an input image into a low-dimensional latent vector (a latent space) that captures its essential features. Not only does it find a single point in the latent space, but it can find a distribution. In contrast, the decoder takes samples from the latency space and reconstructs the original input image, while adding some randomness to make it more diverse. This randomness allows us to add new data points, like the input data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>The following two parameters define the latent state: mean and variance. As the name suggests, the mean is the average value of the latent state, and the variance is the measure difference of the latent state from the mean. The VAE uses these parameters to sample different latent states from a normal distribution, a mathematical function that describes how likely different values are to occur. By sampling different latent states, the VAE can generate different output data that is similar to the input data. Statistical mechanics allow us a framework to infer the probability distribution of the variables in the latent variables given the observer data.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p36">  
   <img alt="figure" src="../Images/CH04_F04_Bahree.png" width="837" height="188"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.4</span> Variational autoencoder architecture</h5>
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Some of the key uses that VAEs allow are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p38"> <em>Image generation</em><em> </em>—VAEs have been used extensively for image generation to create unique images that share similarities with their training data, be it human-like faces, fashion designs, or art. </li> 
   <li class="readable-text" id="p39"> <em>Image reconstruction and inpainting</em><em> </em>—By learning the underlying structure of the image data, VAEs can reconstruct either missing or corrupted parts of images. These properties of reconstructing or filling in missing aspects are tremendously useful in some domains, such as medical imaging, restoring old and archaeologically significant photographs, and so on. </li> 
   <li class="readable-text" id="p40"> <em>Style transfer</em><em> </em>—VAEs allow us to separate the image content from the style and transfer the stylistic elements from one image to another, as shown in figure 4.2. </li> 
   <li class="readable-text" id="p41"> <em>Semantic image manipulation</em><em> </em>—This is similar to image reconstruction. Because of the learned latent space, VAEs can provide us with much more fine-grained control of the features in the generated images by tweaking specific aspects of the generated images, such as facial expressions, without affecting other unrelated features. </li> 
  </ul> 
  <div class="readable-text" id="p42"> 
   <p>Although powerful, VAEs do have drawbacks, such as blurriness, lack of diversity, and difficulty modeling complex distributions. Training them can be demanding and unstable, leading to mode collapse. Irrespective of these challenges, the achievements and potential of VAEs remain at the forefront of vision AI research, building on the complex relationships between data, mathematics, and creativity.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p43"> 
   <p><span class="print-book-callout-head">Note </span> A latent space represents complex data in a simpler and more meaningful way. Think of it as a map where similar items are close to each other, and different items are far apart. This helps us find similarities, generate new data, and understand data better.</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_66"><span class="num-string">4.1.2</span> Generative adversarial networks</h3> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>GANs [1] are among the most popular techniques for creating images with generative AI. They consist of two neural networks: a generator that creates new examples and a discriminator that tries to differentiate between real and generated examples. </p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p>The generator tries to create fake images that look like real ones from random noise or input data, such as text or sketches. The discriminator takes real and fake images and tries to distinguish between the two.</p> 
  </div> 
  <div class="readable-text intended-text" id="p47"> 
   <p>The two networks are trained by simultaneously competing in a game-theoretic manner to improve their performance over time. GANs work through a min–max game where the generator tries to maximize the discriminator’s mistakes, while the discriminator tries to minimize them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>GANs use the prompt as an input to the generator, along with some random noise. The generator then produces an image that tries to match the prompt and sends it to the discriminator. The discriminator compares the generated image with a real image from the same prompt, giving a score that indicates how realistic it thinks the image is. The score is then used to update the weights of both networks using backpropagation and gradient descent. This process is repeated until the generator can create images that satisfy the prompt and fool the discriminator.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>The goal of the GAN is to make the generator produce realistic images that can fool the discriminator. Figure 4.5 shows what the GAN model architecture looks like at a high level. The latent space represents possible input for the generator, and the fine-tuning allows the parameters for the discriminator and the generator to be adjusted.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p50">  
   <img alt="figure" src="../Images/CH04_F05_Bahree.png" width="1011" height="438"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.5</span> GAN model architecture<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p51"> 
   <p>GANs offer many similar use cases, such as VAEs, but they are specifically good for</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p52"> <em>Image generation</em><em> </em>—Creating realistic images from noise, with specific applications in entertaining, design, and art, allows generating high-quality images. </li> 
   <li class="readable-text" id="p53"> <em>Style transfer</em><em> </em>—Enabling artistic styles to transpose from one image to another; this is the same as in VAEs. </li> 
   <li class="readable-text" id="p54"> <em>Super resolutions</em><em> </em>—GANs can help enhance resolution, making images more detailed and clearer. This is very helpful in some industries, such as medical and space imaging. </li> 
   <li class="readable-text" id="p55"> <em>Data augmentation</em><em> </em>—Similar to VAEs for creating synthetic data, GANs help create training data either for edge cases or where there is not enough data or data diversity. </li> 
  </ul> 
  <div class="readable-text" id="p56"> 
   <p>GANs can produce high-quality images that are indistinguishable from real ones. Still, they have drawbacks, such as mode collapse (i.e., the model repeatedly produces the same output), instability, and difficulty controlling the output. They also raise ethical concerns, as they can be quite easily used to create deepfakes that could lead to privacy invasion, potential misinformation, and misrepresentation. Finally, as with many other AI models, GANs can inadvertently perpetuate biases present in the training data in the generated output.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_67"><span class="num-string">4.1.3</span> Vision transformer models</h3> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>Transformers are another model architecture that can create images. We saw the same architecture earlier in the context of natural language processing (NLP) tasks. Transformers can also operate on vision-related tasks and are called vision transformers (ViT) [2].</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>Transformers are neural networks that use attention mechanisms to process sequential data, such as text or speech, and they can be used to generate image prompts. They are also very effective for specific tasks such as image recognition and have outperformed previous leading model architectures.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>A ViT model’s architecture is similar to that of NLP, albeit with some differences—it has a larger number of self-attention layers and a global attention mechanism allowing the model to attend to all parts of the image simultaneously. Transformers calculate how much each input token is related to every other input token. This is called attention. The more tokens there are, the more attention calculations are needed. The number of attention calculations grows as the square of the number of tokens, that is, is quadratically.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>For images, however, the basic unit of analysis is a pixel and not a token. The relationships for every pixel pair in a typical image are computationally prohibitive. Instead, ViT computes relationships among pixels in various small sections of the image (typically in 16 × 16-sized pixels), which helps reduce the computational cost. These 16 × 16-sized sections, along with their positional embeddings, are placed in a linear sequence and are the input to the transformer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>As shown in figure 4.6, a ViT model consists of three main sections: the left, middle, and right. The left section shows the input classes, such as <code>Class</code>, <code>Bird</code>, <code>Ball</code>, <code>Car</code>, and so forth. These are the possible labels that the model can assign to an image. The middle section shows the linear projection of flattened patches, which transform the input image into a sequence of vectors that can be fed to the transformer encoder. The final section is the transformer encoder. This comprises several multi-head attention and normalization layers and is used to learn the relationships between different image parts.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p63">  
   <img alt="figure" src="../Images/CH04_F06_Bahree.png" width="1014" height="540"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.6</span> Vision transformer (ViT) architecture [2]</h5>
  </div> 
  <div class="readable-text" id="p64"> 
   <p>ViTs are used for various image use cases, such as segmentation, classification, and detection, and they are often more accurate than previous techniques. They also support fine-tuning, which can be used in a few-shot manner with smaller datasets, making them quite useful for enterprise use cases where we might not have much data. The ViT model aims to produce a final vector representation for the class token, which contains information about the whole image.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>ViTs also have challenges such as high computational costs, data scarcity, and ethical issues. They are computationally complex both from a training and inference perspective and have low interpretability—both active research areas. Multimodal models, with ViTs such as GPT-4, hold much promise and unlock new enterprise possibilities.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_68"><span class="num-string">4.1.4</span> Diffusion models</h3> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Diffusion models are generative machine learning models that can create realistic data from random noise, such as images or audio. Their goal is to learn the latent structure of a dataset by modeling how data points diffuse through that latent space. The model is trained by slowly adding noise to an image and learning to reverse this by removing noise from the input until it resembles the desired output. For example, a diffusion model can generate an image of a panda by starting with a random image and then slowly removing noise until it looks like a panda.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>Vision diffusion models typically consist of two parts: a forward and a reverse diffusion process. The forward diffusion process is responsible for gradually adding noise to the latent representation of an image, which corrupts that latent space. The reverse diffusion process is just the opposite—it is responsible for reconstructing the original image from the corrupted latent representation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>The forward diffusion process is typically implemented as a Markov chain (i.e., a system with no memory of its past, and the probability of the next step depends on the current state). This means the corrupted latent representation at each step depends only on the previous step’s latent representation, which makes the forward diffusion process efficient and easy to train.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>The reverse diffusion process is typically implemented as a neural network, meaning the neural network learns to reverse the forward diffusion process by predicting the original latent representation from the corrupted one. This reverse diffusion process is slow, as it is a step-by-step repetition. </p> 
  </div> 
  <div class="readable-text intended-text" id="p71"> 
   <p>Some of the advantages that diffusion models have are the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p72"> They can produce high-quality images that match or beat GAN-generated images, especially for complex scenes, but they take much longer to generate. </li> 
   <li class="readable-text" id="p73"> They do not suffer from mode collapse, a common problem for GANs. Mode collapse occurs when the generator produces only a limited variety of outputs, ignoring some modes of data distribution. </li> 
   <li class="readable-text" id="p74"> Diffusion models can capture the full diversity of the data distribution by using a Markov chain process that adds noise to the input data. </li> 
   <li class="readable-text" id="p75"> Diffusion models can be combined with others, such as natural language models, to create text-guided generation systems. </li> 
  </ul> 
  <div class="readable-text" id="p76"> 
   <p>Stable Diffusion is one of the most popular diffusion-based models for image generation. Its architecture consists of three main parts (see figure 4.7):</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p77"> The text encoder, which converts the user’s prompt into a vector representation. </li> 
   <li class="readable-text" id="p78"> A denoising autoencoder (called UNet), which is used to reconstruct an image from the latency space, and a scheduler algorithm, which helps reconstruct the original image. We call it the <em>image information creator</em>. The UNet is a denoising autoencoder because it learns to remove noise from the input image and produce a clean output image. It is a neural network that has an encoder–decoder structure. The encoder part reduces the resolution of an input image and extracts its features. On the other hand, the decoder part increases the resolution and reconstructs the output image. </li> 
   <li class="readable-text" id="p79"> A variational autoencoder (VAE), which creates an image as close as possible to a normal distribution.<span class="aframe-location"/> </li> 
  </ul> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH04_F07_Bahree.png" width="732" height="803"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.7</span> Stable Diffusion logical architecture</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <p>The choice between these models depends on the specific application, the availability of computing resources, training data, and nonfunctional requirements such as image quality, speed, and so forth. Table 4.1 lists some of the more common generative AI vision systems that can create images from text.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p82"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.1</span> Most common AI vision tools</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         AI vision tool 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Imagen <br/></td> 
      <td>  Imagen is Google’s text-to-image diffusion model, which can generate realistic images from text descriptions. It is available in limited preview and has been shown to generate images indistinguishable from real photographs. <br/></td> 
     </tr> 
     <tr> 
      <td>  DALL-E <br/></td> 
      <td>  OpenAI developed a transformer language model to create diverse, original, realistic, and creative images and art from a prompt. It can edit images based on the context, such as adding, deleting, or changing specific parts. It has generated various images, from everyday objects to surrealistic art, from simple text prompts. DALL-E 3 is an improved version that can generate more realistic and accurate images with 4x greater resolution. <br/></td> 
     </tr> 
     <tr> 
      <td>  Midjourney <br/></td> 
      <td>  AI-based art generator that uses deep learning and neural networks to create artwork based on prompts and other images and videos. This is accessible only via a Discord server, and the results can be tailored to any aesthetics, from abstract to realistic, thus offering endless possibilities for creative expression. <br/></td> 
     </tr> 
     <tr> 
      <td>  Adobe Firefly <br/></td> 
      <td>  Adobe Firefly is a family of creative, generative AI diffusion models designed to help designers and creative professionals create images and text effects and edit and recolor. It is easy to use with Adobe’s other tools, such as Photoshop and Illustrator. Adobe has both text-to-image models and generative fill models. <br/></td> 
     </tr> 
     <tr> 
      <td>  Stable Diffusion <br/></td> 
      <td>  Popular models include versions of Stable Diffusion XL and v1.6, an image- generating model that uses diffusion models to create high-quality images using prompts with next-level photorealism capabilities. It can also generate novel images from text descriptions. The more recent v3 family of models comes in large and medium with 8B and 2B parameters, respectively. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Many of the AI vision models listed in table 4.1 are available only to those who were invited to test them. This is still a new space, and most providers are going slowly, learning with a handful of customers before rolling these out.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>Creating and manipulating images with generative AI is an exciting and challenging research area with many potential applications and implications. However, it raises ethical and social questions about the generated content’s ownership, authenticity, and effects. Therefore, it is important to use generative AI responsibly and ethically and to consider its benefits and risks to society.</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_69"><span class="num-string">4.1.5</span> Multimodal models</h3> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>A multimodal model can handle different types of input data. “Modal” refers to the mode or type of data, and “multimodal” refers to multiple data types. These types include text, images, audio, video, and more. For example, GPT-4 has a multimodal model variant that takes both an image and an associated prompt to make predictions or inferences.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>Bing Chat recently enabled this multimodal feature, allowing us to use images and text in the prompt. For example, as shown in figure 4.8, we give the model two things: an image and a prompt related to the image. In this case, we show some produce and ask the model what we can cook with it.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p88">  
   <img alt="figure" src="../Images/CH04_F08_Bahree.png" width="1100" height="1272"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.8</span> Multimodal example using both an image and a prompt</h5>
  </div> 
  <div class="readable-text" id="p89"> 
   <p>In this case, the model must understand the image and the different parts (i.e., ingredients in our example) and correlate to the prompt to generate an answer. We see the response in the shaded text, showing we can make guacamole, salsa, avocado toast, and so forth.</p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>Multimodal models often use different AI techniques. While they can use different combinations of model architecture, in our example, GPT-4 combines different transformer blocks (figure 4.9).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p91">  
   <img alt="figure" src="../Images/CH04_F09_Bahree.png" width="1009" height="981"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.9</span> Multimodal model design</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p92"> 
   <p><span class="print-book-callout-head">Note</span>  When showing transformer blocks, as in figure 4.9, the convention is to use Nx, referring to the transformer block repeating multiple times; in other words, it is stacked x number of times. In our multimodal example, this is the case for all three transformer blocks: the image on the left (Lx), the text on the right (Rx), and the combining layer (Nx).</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Multimodal models are particularly useful in complex real-world applications where data comes in various forms. For example:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p94"> <em>Web</em><em> </em>—Analyzes text and images for content moderation and sentiment analysis </li> 
   <li class="readable-text" id="p95"> <em>eCommerce</em><em> </em>—Recommends products using both photos and text descriptions </li> 
   <li class="readable-text" id="p96"> <em>Healthcare</em><em> </em>—Uses text data (patient medical history) and medical imaging (image data) for diagnosis </li> 
   <li class="readable-text" id="p97"> <em>Self-driving</em><em> </em>—Integrates sensor data (radar and lidar) with visual data (cameras) for situational awareness and decision-making </li> 
  </ul> 
  <div class="readable-text" id="p98"> 
   <p>Now that we have seen some models, their output, and a general sense of how vision AI models work, let us generate images with Stable Diffusion.</p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_70"><span class="num-string">4.2</span> Image generation with Stable Diffusion</h2> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Stability AI, the company behind the Stable Diffusion, has advanced diffusion-based models with SDXL as their latest and most powerful model thus far. They offer multiple options for us to use:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p101"> <em>Self-host</em><em> </em>—The model and associated weights have been published and are available via Hugging Face (<a href="https://huggingface.co/stabilityai">https://huggingface.co/stabilityai</a>). They can be self-hosted, requiring the appropriate computing hardware, including GPUs. </li> 
   <li class="readable-text" id="p102"> <em>DreamStudio</em><em> </em>—This StabilityAI’s consumer application targets consumers. It is a simple web interface that generates images. The company also has an open source version called StableStudio, driven by the community. More details on DreamStudio can be found at <a href="https://dreamstudio.ai">https://dreamstudio.ai</a>. </li> 
   <li class="readable-text" id="p103"> <em>Platform APIs</em><em> </em>—Stability AI has a platform API (<a href="https://platform.stability.ai">https://platform.stability.ai</a>) that we will use in this book, given that most enterprises would prefer an API that can be managed better at scale. REST API will be used for our example here, as it shows the most flexibility across all platforms. Stable Diffusion also has a gRPC API, which is quite similar. </li> 
  </ul> 
  <div class="readable-text" id="p104"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_71"><span class="num-string">4.2.1</span> Dependencies</h3> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>We will build on the packages required earlier in chapter 3 and assume that the following are installed: Python, development IDE, and a virtual environment (such as <code>conda</code>). For Stable Diffusion, we need the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p106"> A Stability AI account and associated API key; this can be acquired via the account page at <a href="https://platform.stability.ai/account/keys">https://platform.stability.ai/account/keys</a>. Billing details also need to be set up at the same place. We pip install the <code>stability-sdk</code> Python package: <code>pip</code> <code>install</code> <code>stability-sdk</code>. </li> 
   <li class="readable-text buletless-item" id="p107"> Keep the API key confidential, and follow best practices for managing secrets. We will use environmental variables to store the key securely, which can be configured as follows: 
    <ul> 
     <li> <em>Windows</em><em> </em>—<code>setx</code> <code>STABILITY</code> <code>API</code> <code>KEY</code> <code>“your-openai-key”</code> </li> 
     <li> <em>Linux/Mac</em><em> </em>—<code>export</code> <code>STABILITY</code> <code>API</code> <code>KEY=your-openai-endpoint</code> </li> 
     <li> <em>Bash</em><em> </em>—<code>echo</code> <code>export</code> <code>STABILITY_API_KEY="YOUR_KEY"</code> <code>&gt;&gt;</code> <code>/etc/environment &amp;&amp;</code> <code>source</code> <code>/etc/environment</code> </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p108"> 
   <p>We start by getting a list of all the models available using the engines API, including all the available engines (i.e., models).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.1</span> Stable Diffusion: Listing the models</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
import requests
import json

api_host = "https://api.stability.ai"
url = f"{api_host}/v1/engines/list"          <span class="aframe-location"/> #1

response = requests.get(url, headers={
    "Authorization": f"Bearer {api_key}"     <span class="aframe-location"/> #2
})

payload = response.json()                    <span class="aframe-location"/> #3

# format the payload for printing
payload = json.dumps(payload, indent=2)      <span class="aframe-location"/> #4
print(payload)</pre> 
    <div class="code-annotations-overlay-container">
     #1 REST API call for getting the models
     <br/>#2 HTTP header for authorization
     <br/>#3 Response back from the API
     <br/>#4 Making the JSON more human-readable
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>The output of this code is presented in the next listing. This shows us the engines we must use and helps in testing end to end to confirm that the API call works and that we can authenticate and get a response.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.2</span> Output: Stable Diffusion model lists</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
  {
    "description": "Real-ESRGAN_x2plus upscaler model",
    "id": "esrgan-v1-x2plus",
    "name": "Real-ESRGAN x2",
    "type": "PICTURE"
  },
  {
    "description": "Stability-AI Stable Diffusion XL v1.0",
    "id": "stable-diffusion-xl-1024-v1-0",
    "name": "Stable Diffusion XL v1.0",
    "type": "PICTURE"
  },
  {
    "description": "Stability-AI Stable Diffusion v1.5",
    "id": "stable-diffusion-v1-5",
    "name": "Stable Diffusion v1.5",
    "type": "PICTURE"
  },
  …
]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_72"><span class="num-string">4.2.2</span> Generating an image</h3> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>We use the Stable Diffusion image generation endpoint (REST API) for our image generation. We will use the latest model, the SDXL model, at the time of this publication. The corresponding engine ID for this model is <code>stable-diffusion-xl-1024-v1-0</code>, as shown in the previous example listing of models. This engine ID is required as part of the REST API path parameter and is available at <span class=" link-like">https://api.stability.ai/v1/generation/{engine_id}/text-to-image</span>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>Listing 4.3 shows an example of using this API to generate an image. Note that we use v1.0 of the API for the examples in this chapter. To use the newer models, we only need to change the REST API path in most cases. For example, to use the newer models that have just been announced, Stable Diffusion 3 and currently in Beta, switch to the following engine ID: <code>https://api.stability.ai/v2beta/stable-image/generate/sd3</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p115"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.3</span> Stable Diffusion: Image generation</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import base64
import os
import requests
import datetime
import re

engine_id = "stable-diffusion-xl-1024-v1-0"                  <span class="aframe-location"/> #1
api_host = "https://api.stability.ai"
api_key = os.getenv("STABILITY_API_KEY")

prompt = "Laughing panda in the clouds eating bamboo"          <span class="aframe-location"/> #2

# Set the folder to save the image; make sure it exists
image_dir = os.path.join(os.curdir, 'images')
if not os.path.isdir(image_dir):
    os.mkdir(image_dir)

# Function to clean up filenames
def valid_filename(s):                                        <span class="aframe-location"/> #3
    s = re.sub(r'[^\w_.)( -]', '', s).strip()
    return re.sub(r'[\s]+', '_', s)

response = requests.post(                                     <span class="aframe-location"/> #4
    f"{api_host}/v1/generation/{engine_id}/text-to-image",     <span class="aframe-location"/> #5
    headers={
        "Content-Type": "application/json",
        "Accept": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "text_prompts": [
            {
                "text": f"{prompt}",
            }
        ],
        "cfg_scale": 7,                                        <span class="aframe-location"/> #6
        "height": 1024,                                        #6
        "width": 1024,                                         #6
        "samples": 1,                                          #6
        "steps": 50,                                           #6
    },
)

data = response.json()                                 <span class="aframe-location"/> #7

for i, image in enumerate(data["artifacts"]):
    filename = f"sd_{valid_filename(prompt)}_{i}_{ [CR]
               datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
    image_path = os.path.join(image_dir, filename)
    with open(image_path, "wb") as f:
        f.write(base64.b64decode(image["base64"]))           <span class="aframe-location"/> #8</pre> 
    <div class="code-annotations-overlay-container">
     #1 Choose the model we want to use.
     <br/>#2 Prompts used to generate the image
     <br/>#3 Helper functions to create filenames
     <br/>#4 API call for generating the image
     <br/>#5 The REST API Endpoint includes the engine ID.
     <br/>#6 Parameters controlling the model generation
     <br/>#7 Response from the API once the generation finishes
     <br/>#8 Saves the image locally
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>The image of a “laughing panda in the clouds eating bamboo” was generated, as shown in figure 4.10. It is quite a happy and lifelike panda.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p117">  
   <img alt="figure" src="../Images/CH04_F10_Bahree.png" width="750" height="750"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.10</span> An image generated by Stability Diffusion</h5>
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Some of the Stability Diffusion API’s parameters are similar to those we have already seen. Still, some are different, given that the underlying model architecture differs from what is presented in table 4.2. Because we are using the REST API, there are also two sets of parameters—one set is the header parameters, and the other is for the body.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p119"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.2</span> Stable Diffusion header parameters: Image Create API</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Parameter 
       </div></th> 
      <th> 
       <div>
         Type 
       </div></th> 
      <th> 
       <div>
         Default value 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>Accept</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Blank <code>(application/json)</code> <br/></td> 
      <td>  The response format can be default (blank) JSON or set to <code>image/png</code> for PNG image. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>Organization</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  A tag that allows requests to be scoped to an organization other than the user’s default. This parameter can help debug, monitor, or detect abuse. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>Stability-Client-ID</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This parameter is used to identify the source of requests, such as the client application or suborganization. It can help debug, monitor, and detect abuse. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>Stability-Client-Version</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This parameter identifies the version of the application or service making the requests. It can help debug, monitor, and detect abuse. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>Authorization</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Bearer <code>API_KEY</code> <br/></td> 
      <td>  Key required to authenticate the API call <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Table 4.3 outlines the parameters that constitute the body of the API call. These parameters can fine-tune the model and steer it closer to what we want to generate.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p121"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.3</span> Stable Diffusion body parameters: Image Create API</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Parameter 
       </div></th> 
      <th> 
       <div>
         Type 
       </div></th> 
      <th> 
       <div>
         Default value 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>height</code>and <code>width</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  512 (optional) <br/></td> 
      <td>  The height and width of the image in pixels must be in increments of 64 and must be one of the following combinations: 1024 x 1024, 1152 x 896, 1216 x 832, 1344 x 768, 1536 x 640, 640 x 1536, 768 x 1344, 832 x 1216, and 896 x 1152. Note that some of these vary based on the engine used. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>text_prompts</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (required) <br/></td> 
      <td>  An array of text prompts is used to generate the image. Two properties make up each element in this array—one of the prompts itself and the other the associated weight of that prompt. The weights should be negative for negative prompts. For example: <br/>  <code>"text_prompts": [{</code> <br/>  <code>    "text": "A dog on a mat",</code> <br/>  <code>    "weight": 0.7</code> <br/>  <code>  }]</code> <br/>  The text property can be up to 2,000 characters. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>cfg_scale</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  7 (optional) <br/></td> 
      <td>  This can range between 0 and 35; it defines how strictly the diffusion process follows the prompt. Higher values keep the image closer to the prompt. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>clip_guidance_preset</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  <code>None</code> (optional) <br/></td> 
      <td>  Different values control how much CLIP guidance is used, and it controls the quality and relevance of the image being generated. Values are <code>NONE</code>, <code>FAST_BLUE</code>, <code>FAST_GREEN</code>, <code>SIMPLE</code>, <code>SLOW</code>, <code>SLOWER</code>, and <code>SLOWEST</code>. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>sampler</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  This defines the sampler to use for the diffusion process. If this value is omitted, the API automatically selects an appropriate sampler for you. Values are <code>DDIM</code>, <code>DDPM</code>, <code>K_DPMPP_2M</code>, <code>K_DPM_2</code>, <code>K_EULER K_DPMPP_2S_ANCESTRAL</code>, <code>K_HEUN</code>, <code>K_DPM_2_ANCESTRAL</code>, <code>K_LMS</code>, <code>K_EULER_ANCESTRAL</code>. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>samples</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  1 (optional) <br/></td> 
      <td>  Specifies the number of images to generate. Values need to range between 1 and 10. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>seed</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  0 (optional) <br/></td> 
      <td>  A random seed is a number that determines how the noise looks. Leave 0 for a random seed value. The possible value ranges between 0 and 4294967295. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>steps</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  50 (optional) <br/></td> 
      <td>  Defines the number of diffusion steps to run. Values range between 10 and 150. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>style_preset</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  Used to guide the image model toward a particular preset style. Values are <code>3d-model</code>, <code>analog-film</code>, <code>anime</code>, <code>cinematic</code>, <code>comic-book</code>, <code>digital-art</code>, <code>enhance</code>, <code>fantasy-art</code>, <code>isometric</code>, <code>line-art</code>, <code>low-poly</code>, <code>modeling-compound</code>, <code>neon-punk</code>, <code>origami</code>, <code>photographic</code>, <code>pixel-art</code>, and <code>tile-texture</code>. <br/>  Note: This list of style presets is subject to change over time. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Now let’s look at some other ways we can create images.</p> 
  </div> 
  <div class="readable-text" id="p123"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_73"><span class="num-string">4.3</span> Image generation with other providers</h2> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>When we want to generate images, a few other vendors also have generative AI vision models; however, they don’t have a platform or API. In this section, we will show other platforms that allow one to create images but don’t have APIs, and in most cases, they need to be accessed via their GUI.</p> 
  </div> 
  <div class="readable-text" id="p125"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_74"><span class="num-string">4.3.1</span> OpenAI DALLE 3</h3> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>DALLE 3 is the newer image generation model from OpenAI that can create images from a prompt. It was one of the first image generation models with which most people could interact. DALLE stands for Discrete Autoencoder Language Latent Encoder, which means it employs a special type of neural network to encode images and text to tokens and then uses those tokens to create images. DALLE can be used both via an API and a GUI. </p> 
  </div> 
  <div class="readable-text intended-text" id="p127"> 
   <p>Given that the images generated with DALLE are similar to Stable Diffusion, we don’t get into the API details here. The GitHub code repository accompanying the book (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>) has DALLE’s API and code samples.</p> 
  </div> 
  <div class="readable-text" id="p128"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_75"><span class="num-string">4.3.2</span> Bing image creator</h3> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>Bing has an image creator application that uses DALLE internally, but the images it creates are enhanced and a bit different. We only need a web browser to use it; an API isn’t exposed. We can generate images by going to <a href="https://www.bing.com/create">https://www.bing.com/create</a> and entering the prompt. There aren’t many tweaks one can make other than those specified in the prompt itself. Figure 4.11 shows the generation of a “serene vacation lake house, watercolor painting with a dog.” We will use one of these images later to see how to edit an image.<strong><span class="aframe-location"/></strong></p> 
  </div> 
  <div class="browsable-container figure-container" id="p130">  
   <img alt="figure" src="../Images/CH04_F11_Bahree.png" width="1050" height="1043"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.11</span> Bing Create: Creating an image depicted as a watercolor painting</h5>
  </div> 
  <div class="readable-text" id="p131"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_76"><span class="num-string">4.3.3</span> Adobe Firefly</h3> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>Adobe has a set of Generative AI tools, with Firefly being their family of Generative AI models. It is being integrated into various Adobe products, such as Photoshop, and is accessible via <a href="https://firefly.adobe.com/">https://firefly.adobe.com/</a>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>Although there isn’t an API, the overall process and modality are the same as we saw earlier with OpenAI. Once we log in, we are presented with a UI where we enter the prompt and generate the images. Let us use one of the previous examples: “laughing panda in the clouds eating bamboo.” Four images are created by default (figure 4.12). <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p134">  
   <img alt="figure" src="../Images/CH04_F12_Bahree.png" width="1023" height="503"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.12</span> Adobe Firefly generative vision</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p135"> 
   <p><span class="print-book-callout-head">Note </span> Google recently announced its generative AI suite of APIs called Vertex AI; at the time of publication, the Vision APIs, which are also built on diffusion models, weren’t available for use. </p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>Now that we have created an image, let’s see how to edit and enhance it.</p> 
  </div> 
  <div class="readable-text" id="p137"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_77"><span class="num-string">4.4</span> Editing and enhancing images using Stable Diffusion</h2> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>In addition to generating images, Stable Diffusion allows us to edit and enhance images. We use Stable Diffusion web UI, one of the open source web interfaces for Stable Diffusion, to show how to use inpainting and enhance the images. The web interface is a wrapper around the model, and while it doesn’t call the API, it has the same properties. </p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>We start by using one of the images of a watercolor painting we generated earlier. In this example, we mask two areas: the dog and the different colors on the bottom left of the image (figure 4.13).</p> 
  </div> 
  <div class="browsable-container figure-container" id="p140">  
   <img alt="figure" src="../Images/CH04_F13_Bahree.png" width="1100" height="1255"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.13</span> Inpainting sketch</h5>
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>When we upload the image to Inpaint, one of the web application’s features is to use a CLIP model to interrogate the image and guess the prompt. Even though we know the prompt from the original generation, this is a different model, and it would be advisable to let Stable Diffusion figure out the prompt. The results are shown in figure 4.14.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p142">  
   <img alt="figure" src="../Images/CH04_F14_Bahree.png" width="928" height="289"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.14</span> Guess the image prompt using a CLIP model</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p143"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">CLIP model</h5> 
   </div> 
   <div class="readable-text" id="p144"> 
    <p>CLIP (Contrastive Language–Image Pre-training) is a neural network created by OpenAI that links text and images. It can comprehend and classify images to match natural language descriptions. This is done through a technique called <em>contrastive learning</em>, where the model learns from a large number of images and related text pairs sourced from the internet.</p> 
   </div> 
   <div class="readable-text" id="p145"> 
    <p>CLIP’s unique ability to do “zero-shot” learning means it can accurately label images it has never encountered before based on text alone without requiring direct fine-tuning for that particular task. For example, CLIP can be given the names of visual classes and identify them in images, even if it wasn’t specifically trained on them.</p> 
   </div> 
   <div class="readable-text" id="p147"> 
    <p>CLIP encodes both text and images into a common representation space. It can estimate the most suitable text snippet for an image or vice versa. This gives it much flexibility and the ability to handle different kinds of visual tasks without requiring training data specific to each task.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p>As shown in figure 4.15, additional settings for inpainting allow for finer control. Some of these are the same as image generation and are equally important, such as the number of sampling steps and methods. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p149">  
   <img alt="figure" src="../Images/CH04_F15_Bahree.png" width="922" height="929"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.15</span> Stable Diffusion inpainting options</h5>
  </div> 
  <div class="readable-text" id="p150"> 
   <p>Outpainting is an additional setting that generates and expands the image in our chosen direction. This option is selected via the Script dropdown on the same settings tab (figure 4.16).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p151">  
   <img alt="figure" src="../Images/CH04_F16_Bahree.png" width="1013" height="440"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.16</span> Outpainting settings in Stable Diffusion</h5>
  </div> 
  <div class="readable-text" id="p152"> 
   <p>We go through the iteration of inpainting by removing the areas we want using the mask, regenerating, and then adding the new elements. The final result of these iterations is shown in figure 4.17.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p153">  
   <img alt="figure" src="../Images/CH04_F17_Bahree.png" width="750" height="750"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.17</span> Final edits of inpainting using Stable Diffusion</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p154"> 
   <p><span class="print-book-callout-head">Note </span> The details on Stable Diffusion web UI, including setup, configuration, and deployments, are outside the scope of this book; however, it is one of the very popular applications that allow one to self-host across Windows, Linux, and MacOS. You can find more details at their GitHub repository (<a href="https://mng.bz/znx1">https://mng.bz/znx1</a>).</p> 
  </div> 
  <div class="readable-text" id="p155"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_78"><span class="num-string">4.4.1</span> Generating using image-to-image API</h3> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>Image-to-image is a powerful tool for generating or modifying new images that use existing images as a starting point and a text prompt. We can use this API to generate a new image but change the style and mood and add or remove aspects.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>Let’s use our serene lake example from earlier and then use the image-to-image API to generate a new image. We build on both examples we have seen earlier—we use the serene lake as our input and ask the model to generate “a happy panda eating bamboo in the sky.” </p> 
  </div> 
  <div class="browsable-container listing-container" id="p158"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.4</span> Image-to-image generation</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import base64
import os
import requests
import datetime
import re

engine_id = "stable-diffusion-xl-1024-v1-0"
api_host = "https://api.stability.ai"
api_key = os.getenv("STABILITY_API_KEY")

orginal_image = "images/serene_vacation_lake_house.jpg"

#helper functions 
...

response = requests.post(
    f"{api_host}/v1/generation/{engine_id}/image-to-image",
    headers={
        "Accept": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    files={
        "init_image": open(orginal_image, "rb")
    },
    data={
        "image_strength": 0.35,
        "init_image_mode": "IMAGE_STRENGTH",
        "text_prompts[0][text]": "A happy panda eating bamboo in the sky",
        "cfg_scale": 7,
        "samples": 1,
        "steps": 50,
        "sampler": "K_DPMPP_2M"
    }
)

data = response.json()

for i, image in enumerate(data["artifacts"]):
    filename = f"{valid_filename(os.path.basename(orginal_image))}_
        <span class="">↪</span>img2img_{i}_{datetime.datetime.now(). 
        <span class="">↪</span>strftime('%Y%m%d_%H%M%S')}.png"
    image_path = os.path.join(image_dir, filename)

    with open(image_path, "wb") as f:
        f.write(base64.b64decode(image["base64"]))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>We see the generated image as shown on the left in figure 4.18 of the image-to-image API call; we see the panda and the bamboo and how the input image to set the scene and the type and aesthetic of the generated image are used. However, it doesn’t adhere to the cloud aspect of the prompt.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>We can tweak the parameters to make it adhere more to the prompt and less to the input image, as shown on the right side of figure 4.18. An example is when we see a panda in the sky, eating bamboo; overall, the image aesthetics follows the input image.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p161">  
   <img alt="figure" src="../Images/CH04_F18_Bahree.png" width="1100" height="521"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 4.18</span> Stable Diffusion image-to-image generation</h5>
  </div> 
  <div class="readable-text" id="p162"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_79"><span class="num-string">4.4.2</span> Using the masking API</h3> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>Stable Diffusion also has a masking API that allows us to edit portions of an image programmatically. The API is very similar to the creation API, as shown in the example in listing 4.5. It does have a few constraints: the mask image needs to be the same dimension as the original image, and a PNG, less than 4MB in size. The API has the same header parameters outlined earlier in the chapter when we discussed image generation; we will avoid duplicating that.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p164"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.5</span> Stable Diffusion masking API example</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import base64
import os
import requests
import datetime
import re

engine_id = "stable-inpainting-512-v2-0"    <span class="aframe-location"/>                      #1
api_host = "https://api.stability.ai"
api_key = os.getenv("STABILITY_API_KEY")

orginal_image = "images/serene_vacation_lake_house.jpg"           <span class="aframe-location"/> #2
mask_image = "images/mask_serene_vacation_lake_house.jpg"          <span class="aframe-location"/> #3
prompt = " boat with a person fishing and a dog in the boat"

# helper functions
...

response = requests.post(
    f"{api_host}/v1/generation/{engine_id}/image-to-image/masking",  <span class="aframe-location"/> #4
    headers={
        "Accept": 'application/json',
        "Authorization": f"Bearer {api_key}"
    },
    files={
        'init_image': open(orginal_image, 'rb'),
        'mask_image': open(mask_image, 'rb'),
    },
    data={
        "mask_source": "MASK_IMAGE_BLACK",                          <span class="aframe-location"/> #5
        "text_prompts[0][text]": prompt,                            <span class="aframe-location"/> #6
        "cfg_scale": 7,
        "clip_guidance_preset": "FAST_BLUE",
        "samples": 4,                                       <span class="aframe-location"/> #7
        "steps": 50, <span class="aframe-location"/> #8
    }
)

data = response.json()                                           <span class="aframe-location"/> #9

for i, image in enumerate(data["artifacts"]):
    filename = f"{valid_filename(os.path.basename(orginal_image))}_ 
                  <span class="">↪</span>masking_{i}_{datetime.datetime.now(). 
                  <span class="">↪</span>strftime('%Y%m%d_%H%M%S')}.png"
    image_path = os.path.join(image_dir, filename)
    with open(image_path, "wb") as f:
        f.write(base64.b64decode(image["base64"]))               <span class="aframe-location"/> #10</pre> 
    <div class="code-annotations-overlay-container">
     #1 Selects the inpainting model we want to use
     <br/>#2 Image we want to edit
     <br/>#3 Masks that we want to apply
     <br/>#4 Masks API call
     <br/>#5 Selects the black pixels of the image to be replaced
     <br/>#6 Prompts for the generation
     <br/>#7 Specifies the number of images to generate
     <br/>#8 Determines the number of steps for each of the images
     <br/>#9 Gets the response from the API
     <br/>#10 Saves the edited image to disk
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>Table 4.4 outlines all the API parameters. In terms of options to steer the model, much of it is similar to the previous image creation.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p166"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 4.4</span> Stable Diffusion masking API parameters</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Parameter 
       </div></th> 
      <th> 
       <div>
         Type 
       </div></th> 
      <th> 
       <div>
         Default value 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>init_image</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Binary (required) <br/></td> 
      <td>  The initial image that we want to edit <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>mask_source</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (required) <br/></td> 
      <td>  Mask details that determine the generation areas and associated strengths. It can be one of the following: <br/>  <code>MASK_IMAGE_WHITE</code>—Use white pixels as the mask; white pixels are modified; black pixels are unchanged. <br/>  <code>MASK_IMAGE_BLACK</code>—Use black pixels as the mask; black pixels are modified; white pixels are unchanged <br/>  <code>INIT_IMAGE_ALPHA</code>—Use the alpha channel as the mask. Edit fully transparent pixels, and leave fully opaque pixels unchanged. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>mask_image</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Binary (required) <br/></td> 
      <td>  Mask image that guides the model on which pixels need to be modified. This parameter is used only if the <code>mask_source</code> is either <code>MASK_IMAGE_BLACK</code> or <code>MASK_IMAGE_WHITE</code> <code>.</code> <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>text_prompts</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (required) <br/></td> 
      <td>  An array of text prompts is used to generate the image. Each element in this array comprises two properties—one of the prompt itself and the other of the associated weight. The weights should be negative for negative prompts. The prompts need to adhere to the following format: <code>text_prompts[index][text|weight]</code>, with the index being unique and not having to be sequential. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>cfg_scale</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  7 (optional) <br/></td> 
      <td>  Can range between 0 and 35; it defines how strictly the diffusion process follows the prompt. Higher values keep the image closer to the prompt. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>clip_guidance_preset</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  None (optional) <br/></td> 
      <td>  Different values control how much CLIP guidance is used and influence the quality and relevance of the image being generated. Possible values are <code>NONE</code>, <code>FAST_BLUE</code>, <code>FAST_GREEN</code>, <code>SIMPLE</code>, <code>SLOW</code>, <code>SLOWER</code> <code>,</code> and <code>SLOWEST</code>. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>sampler</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  Defines the sampler to use for the diffusion process. If this value is omitted, the API automatically selects an appropriate sampler for you. <br/>  Possible values are <code>DDIM</code>, <code>DDPM</code>, <code>K_DPMPP_2M</code>, <code>K_DPM_2</code>, <code>K_EULER K_DPMPP_2S_ANCESTRAL</code>, <code>K_HEUN</code>, <code>K_DPM_2_ANCESTRAL</code>, <code>K_LMS</code>, and <code>K_EULER_ANCESTRAL</code>. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>samples</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  1 (optional) <br/></td> 
      <td>  Defines the number of images to generate. Values need to range between 1 and 10. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>seed</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  0 (optional) <br/></td> 
      <td>  A random seed is a number that determines how the noise looks. Leave 0 for a random seed value. The possible value ranges between 0 and 4294967295. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>steps</code> <br/></td> 
      <td>  Integer <br/></td> 
      <td>  50 (optional) <br/></td> 
      <td>  Defines the number of diffusion steps to run. Possible values range between 10 and 150. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>style_preset</code> <br/></td> 
      <td>  String <br/></td> 
      <td>  Null (optional) <br/></td> 
      <td>  Used to guide the image model towards a particular preset style. Possible values are <code>3d-model</code>, <code>analog-film</code>, <code>anime</code>, <code>cinematic</code>, <code>comic-book</code>, <code>digital-art</code>, <code>enhance</code>, <code>fantasy-art</code>, <code>isometric</code>, <code>line-art</code>, <code>low-poly</code>, <code>modeling-compound</code>, <code>neon-punk</code>, <code>origami</code>, <code>photographic</code>, <code>pixel-art</code>, and <code>tile-texture</code>. <br/>  Note: This list of style presets is subject to change over time. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p167"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_80"><span class="num-string">4.4.3</span> Resize using the upscale API</h3> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>The final Stable Diffusion API we want to cover is used to upscale an image, that is, generate a higher-resolution image of a given image. The default is to upscale the input image by a factor of two, with a maximum pixel count of 4,194,304, equivalent to a maximum dimension of 2,04…,048 and 4,09…,024.</p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>The API is straightforward, as shown in the next listing. The main thing to be aware of is using the right model via the <code>engine_id</code> parameter.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p170"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 4.6</span> Stable Diffusion resizing API</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import base64
import os
import requests
import datetime
import re

engine_id = "esrgan-v1-x2plus"
api_host = "https://api.stability.ai"
api_key = os.getenv("STABILITY_API_KEY")

orginal_image = "images/serene_vacation_lake_house.jpg"

# helper functions
...

response = requests.post(
    f"{api_host}/v1/generation/{engine_id}/image-to-image/upscale",
    headers={
        "Accept": "image/png",
        "Authorization": f"Bearer {api_key}"
    },
    files={
        "image": open(orginal_image, "rb")
    },
    data={
        "width": 2048,
    }
)

filename = f"{valid_filename(os.path.basename(orginal_image))}_
                             <span class="">↪</span>upscale_{datetime.datetime.now().
                             <span class="">↪</span>strftime('%Y%m%d_%H%M%S')}.png"
image_path = os.path.join(image_dir, filename)

with open(image_path, "wb") as f:
    f.write(response.content)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>Now that we have examined numerous image-generation options using both GUIs and APIs, let’s examine some of the best practices for enterprises.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p172"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Watermark for AI-generated images</h5> 
   </div> 
   <div class="readable-text" id="p173"> 
    <p>Since AI-generated images are getting increasingly better, and we often cannot distinguish between real and AI-generated images, there is a push to watermark AI-generated images. There are two main ways to do this today: visible watermarks, like what Bing and DALLE do, and invisible watermarks, which are not visible to us but are embedded in the image and can be detected using special tools. </p> 
   </div> 
   <div class="readable-text" id="p174"> 
    <p>Google has gone a step further and developed a new type of watermark called SynthID. An invisible watermark is embedded in each image pixel, making it more resistant to image manipulation, such as filters, resizing, and cropping. It does so without degrading the image in any noticeable way and without changing the image size significantly.</p> 
   </div> 
   <div class="readable-text" id="p175"> 
    <p>There are multiple benefits of watermarking AI-generated images. In addition to indicating the origin and possibly ownership of the images, they help discourage unauthorized use and distribution and help prevent the spread of misinformation. Chapter 13 covers GenAI-related risks in more detail, including mitigation strategies and associated tooling.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p176"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_81"><span class="num-string">4.4.4</span> Image generation tips</h3> 
  </div> 
  <div class="readable-text" id="p177"> 
   <p>This section outlines some best practices for image generation. In the context of enterprises, outside of some functions, such as graphic designers and artists, many people with different skills need help. These suggestions will help them get started. We will cover more details later in the book when discussing prompt engineering:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p178"> <em>Describe in detail</em><em> </em>—Describe the main subject you want to generate in detail. The visual elements we imagine or want might not match how the model interprets them, so adding details and hints can steer the model more toward what you want. Many also forget to describe the background; it is also important to add those details. </li> 
   <li class="readable-text" id="p179"> <em>Vibes and art style</em><em> </em>—Specify the style of the vibe or the art that is your intent; for example, we outlined a painting in the prompts earlier. The list is endless and, in some ways, up to your imagination, going from oil painting to steampunk to action photography. </li> 
   <li class="readable-text" id="p180"> <em>Set the emotion, energy, and mood</em><em> </em>—Add adjectives and verbs that convey the mood, energy, and overall emotion—for example, the generated image aims to be positive and high energy, or positive but low energy, and so forth. </li> 
   <li class="readable-text" id="p181"> <em>Hands and face generations</em><em> </em>—These are problematic for many models, and while they are getting better, sometimes it is better to add stock or other images to generated images. </li> 
   <li class="readable-text" id="p182"> <em>Structure, size, light, and viewing perspectives</em><em> </em>—When thinking of the vibe and style of the target image, one also has to think of the size and structure of the artifacts. For example, do we expect something small and intricate or big and free-standing? And from what perspective are the artifacts being looked at—is it a closeup, a long shot, wide angle, outdoor, or in natural light? Of course, given that we are talking about a prompt, it can combine many of these things. </li> 
   <li class="readable-text" id="p183"> <em>Words, logos, and characters</em><em> </em>—The image models aren’t large language models and generally struggle with images wherein we expect words to be generated (e.g., a pet salon with its name on the outside). It is best to add these manually when editing the images. Once added, we can use inpainting. </li> 
   <li class="readable-text" id="p184"> <em>Avoid multiple characters together</em><em> </em>—If you add many characters in the same prompt and generation task, it is common for the model to get confused. It might be better to start with smaller tasks and then use inpainting or manually edit these elements. </li> 
  </ul> 
  <div class="readable-text" id="p185"> 
   <p>The next chapter will show other things that can be generated in addition to text and images. We will cover audio, video, and code generators.</p> 
  </div> 
  <div class="readable-text" id="p186"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_82">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p187"> Vision-based generative AI models allow us to create unique and realistic content, all from a simple prompt. These models can generate new content, edit and enhance existing images, and use simple prompts. </li> 
   <li class="readable-text" id="p188"> Generative AI vision models have multiple use cases in which they can be used for creative content, image editing, synthetic data creation, and generative design. </li> 
   <li class="readable-text" id="p189"> There are four primary generative AI model architectures, each with strengths and challenges. We explained variational autoencoders (VAEs), generative adversarial networks (GANs), vision transformer models (ViT), and diffusion models. </li> 
   <li class="readable-text" id="p190"> Multimodal models are different generative AI models that allow us to handle different types of input data, including text, images, audio, and video, simultaneously. </li> 
   <li class="readable-text" id="p191"> OpenAI’s DALLE, Bing, Adobe, and Stability AI’s Stable Diffusion are some of the more famous and common generative AI image models used by enterprises for image generation and editing. Most things exposed via an API have relevant GUI interfaces too. </li> 
   <li class="readable-text" id="p192"> Many generative AI vision models support inpainting (modifying parts within an image), outpainting (expanding an image beyond its original boundaries), and creating image variations. </li> 
   <li class="readable-text" id="p193"> Diffusion models are more robust in modeling collapse and supporting various outputs. </li> 
   <li class="readable-text" id="p194"> Finally, when it comes to images, we need to think about the scene, main character, structure, and elements such as text and faces, which are better done manually and edited into the image. These aspects have to be added to the prompt for the generation. Later in the book, we will discuss this topic as part of prompt engineering. </li> 
  </ul>
 </div></div></body></html>