- en: 8 Considerations for GNN projects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 GNN项目的考虑因素
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating a graph data model from nongraph data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从非图数据创建图数据模型
- en: Extract, transform, load and preprocessing from raw data sources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原始数据源提取、转换、加载和预处理
- en: Creating datasets and data loaders with PyTorch Geometric
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch Geometric创建数据集和数据加载器
- en: In this chapter, we describe the practical aspects of working with graph data,
    as well as how to convert nongraph data into a graph format. We’ll explain some
    of the considerations involved in taking data from a raw state to a preprocessed
    format. This includes turning tabular or other nongraph data into graphs and preprocessing
    them for a graph-based machine learning package. In our mental model, shown in
    figure 8.1, we are in the left half of the figure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述与图数据一起工作的实际方面，以及如何将非图数据转换为图格式。我们将解释将数据从原始状态转换为预处理格式所涉及的一些考虑因素。这包括将表格或其他非图数据转换为图，并对其进行预处理，以便用于基于图的机器学习包。在我们的思维模型中，如图8.1所示，我们处于图的一半左侧。
- en: '![figure](../Images/8-1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-1.png)'
- en: Figure 8.1 Mental model for graph training process. We’re at the start of the
    process, where we prepare our data for training.
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 图训练过程的心理模型。我们处于过程的开始阶段，其中我们为训练准备数据。
- en: We’ll proceed as follows. In section 8.1, we introduce an example problem that
    might require a graph neural network (GNN) and how to proceed with tackling this
    project. Section 8.2 goes into more detail on how to use nongraph data in graph
    models. We then put these ideas into action in section 8.3 by taking a dataset
    from a raw file to preprocessed data, ready for training. Finally, ideas for finding
    more graph datasets are given in section 8.4.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下步骤进行。在第8.1节中，我们将介绍一个可能需要图神经网络（GNN）的示例问题以及如何处理这个项目。第8.2节将更详细地介绍如何在图模型中使用非图数据。然后，在第8.3节中，我们将通过将数据集从原始文件转换为预处理数据，为训练做好准备，将这些想法付诸实践。最后，在第8.4节中，我们将给出寻找更多图数据集的建议。
- en: In this chapter, we’ll consider how to apply GNNs to a social graph created
    by a recruiting firm. In our example, nodes are job candidates, and edges represent
    relationships between job candidates. We generate graph data from raw data, in
    the form of edge lists and adjacency lists. We then use that data in a graph processing
    framework (`NetworkX`) and a GNN library (PyTorch Geometric [PyG]). The nodes
    in this data include the candidate’s *ID*, *job type* (accountant, engineer, etc.),
    and *industry* (banking, retail, tech, etc.).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将考虑如何将GNN应用于由招聘公司创建的社会图。在我们的例子中，节点是求职候选人，边代表求职候选人之间的关系。我们生成图数据，以边列表和邻接列表的形式，然后使用这些数据在图处理框架（`NetworkX`）和GNN库（PyTorch
    Geometric [PyG]）中。这些数据中的节点包括候选人的*ID*、*工作类型*（会计、工程师等）和*行业*（银行、零售、科技等）。
- en: We frame the goals of candidates as a graph-based challenge, detailing the steps
    to transform their data for graph learning. Our aim here is to map out the data
    workflow, starting with raw data, converting it into a graph format, and then
    preparing it for the GNN training we use in the rest of the book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将候选人的目标视为一个基于图的挑战，详细说明将他们的数据转换为图学习步骤。我们的目标是绘制数据工作流程图，从原始数据开始，将其转换为图格式，然后为本书中使用的GNN训练做准备。
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/Xxn1](https://mng.bz/Xxn1)). Colab links and data from this chapter
    can be accessed in the same locations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：本章的代码以笔记本形式可在GitHub仓库中找到（[https://mng.bz/Xxn1](https://mng.bz/Xxn1)）。本章的Colab链接和数据可以在相同的位置访问。
- en: 8.1 Data preparation and project planning
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 数据准备和项目规划
- en: Consider the case of a hypothetical recruiting firm called Whole Staffing. Whole
    Staffing headhunts employees for a variety of industries and maintains a database
    of their candidate profiles, including their history of engagement with the firm
    and other candidates. Some candidates get introduced to the firm via referrals
    from other candidates.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个假设的招聘公司Whole Staffing的案例。Whole Staffing为多个行业搜寻员工，并维护一个包含候选人档案的数据库，其中包括他们与公司的互动历史和其他候选人的信息。一些候选人通过其他候选人的推荐被介绍到公司。
- en: 8.1.1 Project definition
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 项目定义
- en: 'Whole Staffing wants to get the most value from its database. They have a few
    initial questions about their collection of job candidates:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Whole Staffing希望从其数据库中获得最大价值。他们对收集到的求职候选人有以下一些初步问题：
- en: Some profiles have missing data. Is it possible to fill in missing data without
    bothering the candidate?
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些档案缺少数据。在不打扰候选人的情况下，是否有可能填补这些缺失的数据？
- en: History has shown that candidates who have worked on similar projects in the
    past can work well together in future work. Is it possible to figure out which
    candidates could work well together?
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 历史表明，过去在类似项目上工作过的候选人可以在未来的工作中很好地合作。是否有可能找出哪些候选人可以很好地合作？
- en: Whole Staffing has tasked you with exploring the data to answer these questions.
    Among other analytical and machine learning methods, you think there may be an
    opportunity to represent the data as a graph and use a GNN to answer the client’s
    questions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Whole Staffing 委派你探索数据以回答这些问题。在众多分析和机器学习方法中，你认为有可能将数据表示为图，并使用 GNN 来回答客户的问题。
- en: Your idea is to take the collection of referrals and convert it into a social
    network where the job candidates are nodes and the referrals between candidates
    are edges. To simplify things, you can ignore the direction of the referrals so
    that the graph can be undirected. You also ignore repeat referrals, so that relationships
    between candidates remain unweighted.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你的想法是将推荐集合转化为一个社交网络，其中求职者是节点，候选人之间的推荐是边。为了简化问题，你可以忽略推荐的方向，以便图可以是无向的。你还可以忽略重复的推荐，以便候选人之间的关系保持无权重。
- en: We’ll walk through the steps needed to prepare the data and establish a pipeline
    to pass the data to a GNN model. First, let’s consider the project planning stage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步介绍准备数据和建立数据管道以将数据传递到 GNN 模型的步骤。首先，让我们考虑项目规划阶段。
- en: 8.1.2 Project objectives and scope
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 项目目标和范围
- en: Given any problem, having clear objectives, requirements, and scope will serve
    as a compass that steers all subsequent actions and decisions. Every facet, from
    planning and schema creation to tool selection should follow the core objectives
    and scope. Let’s consider each of these for our problem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何问题，拥有明确的目标、需求和范围将作为指南，引导所有后续行动和决策。从规划和模式创建到工具选择，每个方面都应遵循核心目标和范围。让我们考虑我们问题的每个方面。
- en: Project objectives
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 项目目标
- en: Whole Staffing wants to optimize the use of its candidate database. First, the
    project should enhance data quality by filling in missing information in candidate
    profiles, reducing the need for direct candidate engagement. Second, the work
    ahead should facilitate informed candidate suggestions, predicting which teams
    will work well using the historical success of candidates.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Whole Staffing 希望优化其候选人数据库的使用。首先，项目应通过填补候选人档案中的缺失信息来提高数据质量，减少对直接候选人参与的需求。其次，接下来的工作应便于提出有根据的候选人建议，预测哪些团队将利用候选人的历史成功表现而合作得很好。
- en: Project requirements and scope
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 项目需求和范围
- en: 'Several key requirements will directly affect your project. Let’s run through
    a few and point out their importance to our client’s industry. Then, we’ll draw
    some conclusions about the project at hand. Requirements include the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 几个关键需求将直接影响你的项目。让我们快速浏览几个，并指出它们对我们客户行业的意义。然后，我们将对当前项目得出一些结论。需求包括以下内容：
- en: '*Data size and velocity* —What is the size of the data, in terms of item counts,
    size in bytes, or number of nodes? How fast is new information added to the data,
    if at all? Is data expected to be uploaded from a real-time stream, or from a
    data lake that is updated daily?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据大小和速度* — 数据的大小从项目数量、字节大小还是节点数量来衡量是多少？如果有的话，新信息添加到数据中的速度有多快？数据是否预期从实时流或每日更新的数据湖上传？'
- en: The planned graph might grow with the increase in data, affecting the computational
    resources needed and the efficiency of algorithms. Accurately assessing data size
    and velocity ensures that the system can handle the expected load, can offer real-time
    insights, and is scalable for future growth.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 计划的图可能会随着数据的增加而增长，影响所需的计算资源和算法的效率。准确评估数据大小和速度可以确保系统可以处理预期的负载，可以提供实时洞察，并且可以扩展以适应未来的增长。
- en: '*Inference speed* —How fast are the application and the underlying machine
    learning models required to be? Some applications may require sub-second responses,
    while for others, there is no constraint on time.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*推理速度* — 应用程序和底层机器学习模型需要有多快？有些应用可能需要亚秒级响应，而其他应用则没有时间限制。'
- en: Response time is particularly vital in providing timely recommendations and
    insights. For a recruitment firm, matching candidates with suitable job openings
    is time-sensitive, with opportunities quickly becoming unavailable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 响应时间在提供及时的建议和洞察方面尤为重要。对于招聘公司来说，将候选人与合适的职位空缺匹配是时间敏感的，机会很快就会消失。
- en: '*Data privacy* —What are the policies and regulations regarding personally
    identifiable information (PII), and how would this involve data transformation
    and preprocessing?'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私* — 关于个人身份信息（PII）的政策和法规是什么，这会如何涉及数据转换和预处理？'
- en: Data privacy becomes a huge concern when dealing with sensitive information
    such as candidate profiles, contact details, and employment histories. In a graph
    and GNN setting, ensuring that nodes and edges don’t reveal PII is essential.
    Compliance with regulations such as General Data Protection Regulation (GDPR)
    or the California Consumer Privacy Act (CCPA) is mandatory to avoid legal complications.
    The graph data should be handled, stored, and processed in a way that respects
    privacy norms. Anonymization and encryption techniques may be needed to protect
    individuals’ privacy while still allowing for effective data analysis. Understanding
    these requirements early in the project planning ensures that the system architecture
    and data processing pipelines are designed with privacy preservation in mind.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理敏感信息，如候选人档案、联系详情和就业历史时，数据隐私成为一个巨大的关注点。在图和GNN设置中，确保节点和边不泄露PII是至关重要的。遵守如通用数据保护条例（GDPR）或加利福尼亚消费者隐私法案（CCPA）等法规是强制性的，以避免法律纠纷。图数据应以尊重隐私规范的方式处理、存储和处理。可能需要匿名化和加密技术来保护个人隐私，同时仍然允许有效的数据分析。在项目规划早期理解这些要求，确保系统架构和数据处理管道的设计考虑到隐私保护。
- en: '*Explainability* —How explainable should the responses be? Will direct answers
    be enough, or should there be additional data that sheds light on why a recommendation
    or prediction was made?'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性* — 响应应该有多高的可解释性？直接答案是否足够，或者是否应该有额外的数据来阐明为什么做出了推荐或预测？'
- en: In the recruitment sector, explainability and transparency are pivotal. They
    instill trust among candidates and employers by ensuring fairness and clarity
    in the talent-selection process. Ethical standards are upheld, and unintended
    biases should be mitigated. These elements aren’t just ethical imperatives but
    often legally binding.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在招聘领域，可解释性和透明度至关重要。它们通过确保人才选拔过程中的公平性和清晰性，在候选人和雇主之间建立信任。维护道德标准，并应减轻无意中的偏见。这些要素不仅是道德上的要求，而且往往是法律上的约束。
- en: 'Given the objectives and scope, for Whole Staffing, the deliverables might
    be a system that does the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据目标和范围，对于Whole Staffing，可交付成果可能是一个执行以下操作的系统：
- en: Fortnightly scan the candidate data for missing items. Missing items can be
    inferred and suggested or filled in.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每两周扫描一次候选人数据，查找缺失项。缺失项可以被推断、建议或填充。
- en: Predict candidates that will work well together by using link prediction and/or
    node classification. Unlike the first deliverable, the response time here should
    be fast.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用链接预测和/或节点分类来预测将很好地一起工作的候选人。与第一个可交付成果不同，这里的响应时间应该很快。
- en: 'The following lists some of the specifications for the preceding requirements:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了一些先前要求的具体规格：
- en: '*Data size* —This is conservatively set at enough capacity for 100,000 candidates
    and their properties, which is estimated to be 1 GB of data.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据大小* — 这被保守地设定为足够容纳10万个候选人和他们的属性，估计为1 GB的数据。'
- en: '*Inference speed* —Application will run biweekly and can be completed overnight
    so we don’t have a considerable speed constraint.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*推理速度* — 应用程序将每两周运行一次，可以在一夜间完成，因此我们没有明显的速度限制。'
- en: '*Data privacy* —No personal data that directly identifies a candidate can be
    used. However, data known to the recruitment company, such as whether employees
    have been successfully placed at the same employer, can be used to improve operations
    of the company, provided this data isn’t shared.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私* — 不能使用直接识别候选人的个人信息。然而，招聘公司已知的数据，例如员工是否在相同的雇主处成功安置，可以用来改善公司的运营，前提是这些数据不共享。'
- en: '*Explainability* —There must be some level of explainability for the results.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性* — 结果必须有一定的可解释性。'
- en: The objectives and requirements will guide the decisions regarding system design,
    data models, and, often, GNN architecture. The preceding gives an example for
    the type of considerations needed when beginning or scoping a graph-based project.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 目标和要求将指导系统设计、数据模型以及通常GNN架构的决策。上述内容给出了在开始或界定基于图的项目时所需考虑的类型示例。
- en: 8.2 Designing graph models
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 设计图模型
- en: Given an appropriate scope of work, the next step is in building the graph models.
    For most machine learning problems, data will be organized in a standard way.
    For example, when dealing with tabular data, rows are treated as observations,
    and columns are treated as features. We can join tables of such data by using
    indexes and keys. This framework is flexible and relatively unambiguous. We may
    quibble about which observations and features to include, but we know where to
    place them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定适当的工作范围后，下一步是构建图模型。对于大多数机器学习问题，数据将以标准方式组织。例如，当处理表格数据时，行被视为观察值，列被视为特征。我们可以通过使用索引和键来连接此类数据表。此框架灵活且相对明确。我们可能会对包含哪些观察值和特征有所争议，但我们知道它们的位置在哪里。
- en: When we want to express our data with graphs, in all but the simplest scenarios,
    we’ll have several options for what structure to use. With graphs, it’s not always
    intuitive where to place the entities of interest. It’s this ambiguity that drives
    the need for systemic methods in using graph data, but getting it right early
    on can serve as a foundation for downstream machine learning tasks [1].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要用图来表示我们的数据时，在除最简单场景之外的所有情况下，我们都会有几个选择来决定使用哪种结构。使用图时，并不总是直观地知道将感兴趣的实体放在哪里。正是这种不确定性推动了在图数据中使用系统方法的必要性，但尽早做对可以成为下游机器学习任务的基础[1]。
- en: 'In this section, we embark on a journey of transforming Whole Staffing’s recruitment
    data into graph-based data to support our downstream pipeline. We start by considering
    the domain and use case, a critical step to understanding the data. Next, we create
    and refine a schema, pivotal for organizing and interpreting complex datasets.
    Through rigorous testing of the schema, we could then ensure its robustness and
    reliability. Any necessary refinements should be made to optimize performance
    and accuracy. This approach ensures that our future analytic systems, which ingest
    graph-based data, can answer complex queries about job candidates with precision
    and reliability. Here’s the process to follow, and figure 8.2 provides a visual:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们开始将Whole Staffing的招聘数据转换为基于图的数据，以支持我们的下游流程。我们首先考虑领域和用例，这是理解数据的关键步骤。接下来，我们创建和细化一个模式，这对于组织和解释复杂数据集至关重要。通过严格测试模式，我们可以确保其健壮性和可靠性。任何必要的改进都应进行以优化性能和准确性。这种方法确保我们的未来分析系统，即摄入基于图的数据，可以精确可靠地回答关于求职者的复杂查询。以下是遵循的过程，图8.2提供了视觉说明：
- en: Understand the data and the use case.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解数据和用例。
- en: Create a data model, schema, and instance model.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据模型、模式和实例模型。
- en: Test your model using the schema and instance model.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模式和实例模型测试您的模型。
- en: Refactor if necessary.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有必要，进行重构。
- en: '![figure](../Images/8-2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-2.png)'
- en: Figure 8.2 Process of creating a robust graph data model from nongraph data
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 从非图数据创建健壮的图数据模型的过程
- en: 8.2.1 Get familiar with the domain and use case
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 熟悉领域和用例
- en: As with most data projects, to be effective, we have to come to grips with the
    dataset and the context. For our immediate goal of creating a model, understanding
    our referral data in its raw format and digging into the intricacies of the recruiting
    industry can provide critical insights. This knowledge also gives us a basis to
    design tests for the model during deployment. For example, preliminary analysis
    on the raw data gives us the information in table 8.1\.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数数据项目一样，为了有效，我们必须掌握数据集和上下文。对于我们创建模型的直接目标，了解原始格式的推荐数据并深入了解招聘行业的复杂性可以提供关键见解。这些知识也为我们提供了在部署期间为模型设计测试的基础。例如，对原始数据的初步分析为我们提供了表8.1中的信息。
- en: Table 8.1 Features of the dataset
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.1 数据集特征
- en: '| Number of candidates  | 1,933  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 候选人数 | 1,933 |'
- en: '| Number of referrals  | 12,239  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 推荐人数 | 12,239 |'
- en: From the raw data, it’s apparent that there are many relationships, offering
    potential insights into candidate referrals. The large number of referrals in
    comparison to the number of candidates suggests an interconnected network. Our
    models need to be sufficiently large to translate this structure into results
    within the recruitment problem-space.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据中可以看出，存在许多关系，这为候选人的推荐提供了潜在的见解。与候选人数量相比，大量的推荐表明了一个相互关联的网络。我们的模型需要足够大，才能将这种结构转化为招聘问题空间内的结果。
- en: Turning to domain knowledge, beyond the immediate asks of the client, we should
    be asking questions that solidify our understanding of the industry. In setting
    the requirements for our data model, we should consider the key questions and
    challenges to the industry. For the recruitment problem, we might ask how we can
    optimize the referral process or what underlying structures and patterns govern
    candidate referrals. By addressing these types of questions, we can align our
    model with domain expertise, with a likely boost in both its relevance and validity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 转向领域知识，除了客户的直接需求之外，我们还应该提出巩固我们对行业理解的问题。在设定我们的数据模型的要求时，我们应该考虑行业的关键问题和挑战。对于招聘问题，我们可能会问如何优化推荐流程或是什么底层结构和模式支配着候选人的推荐。通过解决这类问题，我们可以使我们的模型与领域专业知识相一致，这可能会提高其相关性和有效性。
- en: 8.2.2 Constructing the graph dataset and schemas
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 构建图数据集和模式
- en: 'Next, we’ll discuss how to design our database. The term *graph dataset* denotes
    a general effort to describe data using the elements and structure of a graph:
    nodes, edges, and node features and edge features. To achieve this, we need a
    *schema* and an *instance*. These specify the structure and rules of our graph
    explicitly and allow our graph dataset to be tested and refined. This section
    is drawn from several references, listed at the end of the book for further reading.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何设计我们的数据库。术语*图数据集*表示使用图的元素和结构（节点、边、节点特征和边特征）描述数据的一般努力。为了实现这一点，我们需要一个*模式*和一个*实例*。这些明确指定了我们的图的结构和规则，并允许我们的图数据集被测试和改进。本节内容来源于几篇参考文献，列在书末供进一步阅读。
- en: By addressing the details of our graph dataset up front, we can avoid technical
    debt and more easily test the integrity of our data. We can also experiment more
    systematically with different data structures. In addition, when the structure
    and rules of our graphs are designed explicitly, it increases the ease with which
    we can parameterize these rules and experiment with them in our GNN pipeline.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提前处理我们的图数据集的细节，我们可以避免技术债务，并更容易地测试我们数据的一致性。我们还可以更系统地实验不同的数据结构。此外，当我们明确设计我们的图的结构和规则时，它增加了我们参数化这些规则并在我们的GNN管道中实验它们的便利性。
- en: Graph datasets can be simple, consisting of one type of node and one type of
    edge. Or they can be complex, involving many types of nodes and edges, metadata,
    and, in the case of knowledge graphs, ontologies.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据集可以是简单的，只包含一种类型的节点和一种类型的边。或者它们可以是复杂的，涉及许多类型的节点和边、元数据，在知识图中还包括本体论。
- en: Key terms
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键术语
- en: 'The following are key terms used in this section (for more details on graph
    data models and types of graphs, see appendix A):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在本节中使用的术语（有关图数据模型和图类型的更多详细信息，请参阅附录A）：
- en: '*Bi-graph (or bipartite graph)*—A graph with two sets of nodes. There are no
    edges between nodes of the same set.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二部图（或二分图）*—具有两组节点的图。同一组内的节点之间没有边。'
- en: '*Entity-relationship diagram (ER diagram)*—A figure that shows the entities,
    relationships, and constraints of a graph.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实体-关系图（ER图）*—显示图中的实体、关系和约束的图形。'
- en: '*Graph dataset—*A representation of nodes, edges, and their relationships.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图数据集*—节点、边及其关系的表示。'
- en: '*Heterogeneous/homogeneous graphs—*A homogeneous graph has only one type of
    node or edge. A heterogeneous graph can have several different types of nodes
    or edges.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异构/同构图*—同构图只有一种类型的节点或边。异构图可以有多种不同类型的节点或边。'
- en: '*Instance model*—A model based on a schema that holds a subset of the actual
    data.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实例模型*—基于模式且包含实际数据子集的模型。'
- en: '*Ontology—*A way of describing the concepts and relationships in a specific
    domain of knowledge, for example, connections between different entities (writers)
    in a semantic web (of works of literature). The ontology is the structured framework
    that defines the roles, attributes, and interrelations of these writers and their
    literary works.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本体论*—描述特定知识领域中的概念和关系的一种方式，例如，在语义网（文学作品网）中不同实体（作家）之间的联系。本体论是定义这些作家及其文学作品的角色、属性和相互关系的结构化框架。'
- en: '*Property graph*—A model that uses metadata (labels, identifiers, attributes/
    properties) to define the graph’s elements.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*属性图*—使用元数据（标签、标识符、属性/属性）来定义图元素的模型。'
- en: '*Resource Description Framework graph (RDF graph, aka Triple Stores)*—Model
    that follows a subject-predicate-object pattern, where nodes are subjects and
    objects, and edges are predicates.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*资源描述框架图（RDF图，又称三元组存储）*—遵循主语-谓语-宾语模式的模型，其中节点是主语和宾语，边是谓语。'
- en: '*Schema*—A blueprint that defines how the elements of the graph will be organized
    as well as which specific rules and constraints will be used for these elements.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模式*—一个蓝图，定义了图元素的组织方式以及将用于这些元素的具体规则和约束。'
- en: '*Conceptual schema*—A schema not tied to any particular database or processing
    system.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概念模式*—一种与任何特定数据库或处理系统无关的模式。'
- en: '*System schema*—A schema designed with a specific graph database or processing
    system in mind.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统模式*—针对特定图数据库或处理系统设计的模式。'
- en: '*Technical debt*—The consequences of prioritizing speedy delivery over quality
    code, which later has to be refactored.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*技术债务*—优先考虑快速交付而非高质量代码的后果，这后来需要重构。'
- en: Graph datasets are good at providing conceptual descriptions of graphs that
    are quick and easy to grasp by others. For example, for people who understand
    what a property graph or an RDF graph is, telling them that a graph is a bi-graph
    implemented on a property graph can reveal much about the design of your data
    (property graphs and RDF graphs are explained in appendix A).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据集擅长提供快速且易于他人理解的图的概念描述。例如，对于理解属性图或RDF图的人来说，告诉他们一个图是在属性图上实现的二分图，可以揭示你数据设计（属性图和RDF图在附录A中解释）的很多信息。
- en: 'A *schema* is a blueprint that defines how data is organized in a data storage
    system, such as a database. A graph schema is a concrete implementation of a graph
    dataset, explaining in detail how the data in a specific use case is to be represented
    in a real system. Schemas can consist of diagrams and written documentation. Schemas
    can be implemented in a graph database using a query language or in a processing
    system using a programming language. A schema should answer the following questions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*模式*是一个蓝图，定义了数据在数据存储系统（如数据库）中的组织方式。图模式是图数据集的具体实现，详细说明了在特定用例中数据如何在真实系统中表示。模式可以由图表和书面文档组成。模式可以使用查询语言在图数据库中实现，也可以使用编程语言在处理系统中实现。模式应该回答以下问题：'
- en: What are the elements (nodes, edges, properties), and what real-world entities
    and relationships do they represent?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元素（节点、边、属性）是什么？它们代表了哪些现实世界的实体和关系？
- en: Does the graph include multiple types of nodes and edges?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图是否包含多种类型的节点和边？
- en: What are the constraints regarding what can be represented as a node?
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于节点表示的约束是什么？
- en: What are the constraints for relationships? Do certain nodes have restrictions
    regarding adjacency and incidence? Are there count restrictions for certain relationships?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系的约束是什么？某些节点是否有关于相邻和发生的限制？某些关系有计数限制吗？
- en: How are descriptors and metadata handled? What are the constraints on this data?
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何处理描述符和元数据？这些数据有哪些约束？
- en: 'Depending on the complexity of your data and the systems in use, you may use
    multiple but consistent schemas. A *conceptual schema* lays out the elements,
    rules, and constraints of the graph but isn’t tied to any system. A *system schema*
    reflects the conceptual schema’s rules but just for a specific system, such as
    a database of choice. A system schema could also omit unneeded elements from the
    conceptual schema. Here are the steps to create a schema:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您数据和使用系统的复杂性，您可能需要使用多个但一致的方案。一个*概念模式*概述了图元素、规则和约束，但并不绑定到任何系统。一个*系统模式*反映了概念模式的规则，但仅针对特定系统，例如选择的数据库。系统模式还可以从概念模式中省略不必要的元素。以下是创建模式的方法：
- en: '*Identify main entities and relationships*. For instance, in our social network
    example, entities can be candidates, recruiters, referrals, hiring events, and
    relationships.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*识别主要实体和关系*。例如，在我们的社交网络示例中，实体可以是候选人、招聘人员、推荐人、招聘活动，以及关系。'
- en: '*Define node and edge labels.* These labels serve as identifiers for the types
    of entities and their interrelationships in the graph.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*定义节点和边标签*。这些标签作为图中实体类型及其相互关系的标识符。'
- en: '*Specify properties and constraints.* Each vertex and edge label is associated
    with specific properties and constraints that store and restrict information,
    respectively.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*指定属性和约束*。每个顶点和边标签都与特定的属性和约束相关联，分别用于存储和限制信息。'
- en: '*Define indices (optional, for database-oriented schemas)*. Indexes, based
    on properties or combinations thereof, enhance query speeds on graph data.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*定义索引（可选，针对面向数据库的模式）*。基于属性或其组合的索引可以增强图数据的查询速度。'
- en: '*Apply the graph schema to a database (optional, for database-oriented schemas)*.
    Commands or codes, contingent on the specific graph database, are employed to
    create the graph schema, with specifications on its static or dynamic nature.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*将图模式应用于数据库（可选，针对面向数据库的模式）*。根据特定的图数据库，使用命令或代码来创建图模式，并指定其静态或动态性质。'
- en: Depending on the complexity of the graph dataset and the use cases, one or several
    schemas could be called for. In the case of more than one schema, compatibility
    between the schemas via a mapping must also be included.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图数据集的复杂性和用例，可能需要调用一个或多个模式。在存在多个模式的情况下，通过映射实现模式之间的兼容性也是必需的。
- en: For a dataset with few elements, a simple diagram with notes in prose can be
    sufficient to convey enough information to fellow developers to be able to implement
    in query language or code. For more complex network designs, ER diagrams and associated
    grammar are useful in illustrating network schemas in a visual and human readable
    way.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有少量元素的数据库，一个简单的带有注释的图表就足以传达足够的信息，以便其他开发者能够用查询语言或代码实现。对于更复杂的网络设计，ER 图和相关语法有助于以视觉和可读的方式说明网络模式。
- en: Entity-relationship diagrams (ER diagrams)
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实体-关系图（ER 图）
- en: ER diagrams have the elements to illustrate a graph’s nodes, edges, and attributes
    and the rules and constraints governing a graph [2, 3]. The following figure (left)
    shows some connectors notation that can be used to illustrate edges and relationship
    constraints. The figure (right) shows an example of a schema diagram conveying
    two node types that might be represented in our recruitment example (Recruiter
    and Candidate), and two edge types (Knows, and Recruits/Recruited By). The diagram
    conveys implicit and explicit constraints.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ER 图具有说明图节点、边和属性以及控制图的规则和约束的元素 [2, 3]。以下图（左）显示了可以用来说明边和关系约束的一些连接器符号。图（右）显示了一个模式图示例，传达了在我们的招聘示例（招聘人员和候选人）中可能表示的两个节点类型，以及两种边类型（了解和招聘/被招聘）。该图传达了隐含和显式约束。
- en: '![sidebar figure](../Images/8-unnumb.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![sidebar figure](../Images/8-unnumb.png)'
- en: At left is the relationship nomenclature for ER diagrams. At right is an example
    of a conceptual schema using an ER diagram.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在左侧是 ER 图的关系命名法。在右侧是一个使用 ER 图的概念模式示例。
- en: Some explicit constraints are that one employee can refer many other employees
    and that one referee can be referred by many employees. Another explicit constraint
    is that a person can only be employed full-time by one business, but one business
    might have many employees. An implicit constraint is that, for this graph model,
    there can be no relationship between a business and a referral.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一些显式约束包括一个员工可以推荐许多其他员工，以及一个推荐人可以被许多员工推荐。另一个显式约束是，一个人只能被一家企业全职雇佣，但一家企业可能有多个员工。一个隐式约束是，对于这个图模型，企业和推荐之间不能存在关系。
- en: 'Turning to our example, to design conceptual and system schemas for our example
    dataset, we should think about the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 转到我们的示例，为了为我们的示例数据集设计概念和系统模式，我们应该考虑以下因素：
- en: The entities and relationships in our data
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们数据中的实体和关系
- en: Possible rules and constraints
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的规则和约束
- en: Operational constraints, such as the databases and libraries at our disposal
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作约束，例如我们可用的数据库和库
- en: The output we want from our application
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望从应用程序中获得的结果
- en: Our data will consist of candidates and their profile data (e.g., industry,
    job type, company, etc.), as well as recruiters. Properties can also be treated
    as entities; for instance, Medical Industry could be treated as a node. Relations
    could be Candidate Knows Candidate, Candidate Recommended Candidate, or Recruiter
    Recruited Candidate. As stated previously, graph data can be extremely flexible
    in how entities can be represented.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据将包括候选人及其个人资料数据（例如，行业、工作类型、公司等），以及招聘人员。属性也可以被视为实体；例如，医疗行业可以被视为一个节点。关系可以是候选人了解候选人、候选人推荐候选人，或者招聘人员招聘候选人。如前所述，图数据在实体表示方面可以非常灵活。
- en: Given these choices, we show a few options for the conceptual schema. Option
    A is shown in figure 8.3.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些选择的基础上，我们展示了几个概念模式选项。选项 A 在图 8.3 中展示。
- en: '![figure](../Images/8-3.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-3.png)'
- en: Figure 8.3 Schema with one node type and one edge type
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3 一个节点类型和一个边类型的模式
- en: As you can see, example A consists of one node type (Candidate) connected by
    one undirected edge type (Knows). Node attributes are the candidate’s Industry
    and their Job Type. There are no restrictions on the relationships, as any candidate
    can know 0 to *n*-1 other candidates, where *n* is the number of candidates. The
    second conceptual schema is shown in figure 8.4.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，示例A由一个节点类型（候选人）通过一个无向边类型（了解）连接。节点属性是候选人的行业和他们的工作类型。关系没有限制，因为任何候选人都可以了解0到*n*-1个其他候选人，其中*n*是候选人的数量。第二个概念模式如图8.4所示。
- en: '![figure](../Images/8-4.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-4.png)'
- en: Figure 8.4 Schema with two node types and one edge type
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4 两个节点类型和一个边类型的模式
- en: 'Example B consists of two node types (Candidate and Recruiter), linked by one
    undirected edge type (Knows). Edges between candidates have no restrictions. Edges
    between candidates and recruiters have a constraint: a candidate can only link
    to one recruiter, while a recruiter can link to many candidates.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 示例B由两个节点类型（候选人、招聘人员）通过一个无向边类型（了解）连接。候选人之间的关系没有限制。候选人与招聘人员之间的关系有一个约束：候选人只能链接到一个招聘人员，而招聘人员可以链接到多个候选人。
- en: 'The third schema is shown in figure 8.5\. It has multiple node and relationship
    types. In example C, the types are Candidate, Recruiter, and Industry. Relation
    types include Candidate Knows Candidate, Recruiter Recruits Candidate, Candidate
    Is a Member of Industry. Note, we’ve made Industry a separate entity, rather than
    an attribute of a candidate. These types of graphs are known as *heterogeneous,*
    as they contain many different types of nodes and edges. In a way, we can imagine
    these as multiple graphs that are layered on top of each other. When we have only
    one type of nodes and edges, then graphs are known as *homogenous.* Some of the
    constraints for example C include the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个模式如图8.5所示。它有多个节点和关系类型。在示例C中，类型包括候选人、招聘人员和行业。关系类型包括候选人了解候选人、招聘人员招聘候选人、候选人属于行业。请注意，我们将行业作为一个单独的实体，而不是候选人的属性。这类图被称为*异构图*，因为它们包含许多不同类型的节点和边。从某种意义上说，我们可以想象这些图是叠加在一起的多个图。当我们只有一种类型的节点和边时，这些图被称为*同构图*。示例C的一些约束包括以下内容：
- en: Candidates can only have one Recruiter and one Industry.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选人只能有一个招聘人员和一个行业。
- en: Recruiters don’t link to Industries.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 招聘人员不链接到行业。
- en: '![figure](../Images/8-5.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-5.png)'
- en: Figure 8.5 Schema with three node types and three edge types
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5 三个节点类型和三个边类型的模式
- en: Depending on the queries and the objectives of the machine learning model, we
    could pick one schema or experiment with all three in the course of developing
    our application. Let’s stick with the first schema, which can serve as a simple
    structure for our exploration and experimentation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据查询和机器学习模型的目标，我们可以在开发应用程序的过程中选择一个模式或尝试所有三个模式。让我们坚持第一个模式，它可以作为我们探索和实验的简单结构。
- en: 8.2.3 Creating instance models
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 创建实例模型
- en: 'An *instance model* contrasts the abstract nature of the graph dataset by providing
    a tangible, specific example of the data, according to the schema. Such an example
    serves to validate and test the schema. Following are the steps to create an instance
    model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*实例模型*通过根据模式提供可触摸的、具体的示例数据，与图数据集的抽象性质形成对比。这样的示例用于验证和测试模式。以下是为创建实例模型所遵循的步骤：'
- en: '*Identify the schema*. Begin by identifying the general model or schema that
    your instance will be based upon. Ensure that the class definition, attributes,
    and methods are well established.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*识别模式*。首先，确定你的实例将基于的一般模型或模式。确保类定义、属性和方法都已确立。'
- en: '*Select a subset of the data*. Choose a specific subset of data to represent,
    adhering to the established graph schema.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择数据子集*。选择一个特定的数据子集来表示，并遵循已建立的图形模式。'
- en: '*Create nodes*. Develop nodes for each entity within your data subset, ensuring
    each has a label, unique identifier, and associated properties.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建节点*。为数据子集中的每个实体开发节点，确保每个节点都有一个标签、唯一的标识符和相关的属性。'
- en: '*Create edges*. Develop links for each relationship, assigning labels and properties
    and specifying edge directions and multiplicities.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建边*。为每个关系开发链接，分配标签和属性，并指定边方向和多重性。'
- en: '*Adhere to the rules and constraints of your schema*. In constructing the instance
    model, make sure to follow the rules and constraints of the schema.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*遵守您模式中的规则和约束*。在构建实例模型时，请确保遵循模式的规则和约束。'
- en: '*Visualization*. Use visualization tools to represent the instance model graphically.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可视化*。使用可视化工具以图形方式表示实例模型。'
- en: '*Instantiation*. Realize the instance model using a graph database or graph
    processing system. This will allow for queries that can test and validate it.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*实例化*。使用图数据库或图处理系统实现实例模型。这将允许进行查询以测试和验证它。'
- en: Figure 8.6 shows an example of an instance model derived from the schema discussed
    formerly. The nodes and edges have features filled with the real data of candidates
    instead of placeholders.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6显示了从前讨论的方案中派生出的实例模型的示例。节点和边填充了候选人的实际数据，而不是占位符。
- en: '![figure](../Images/8-6.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-6.png)'
- en: Figure 8.6 Example of an instance model with nodes filled with actual data from
    the recruiter example. Real instance models may have much more data.
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6示例：一个节点填充了招聘示例中实际数据的实例模型。实际的实例模型可能包含更多的数据。
- en: 8.2.4 Testing and refactoring
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 测试和重构
- en: '*Technical debt* can occur when we have to change and evolve our data or code,
    but we haven’t yet planned for backward- or forward-compatibility in our models.
    It can also happen when our modeling choices aren’t a good fit for our database
    and software choices, which may call for expensive (in time or money) workarounds
    or replacements.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*技术债务*可能发生在我们必须更改和演进我们的数据或代码，但我们尚未在我们的模型中计划向后或向前兼容性时。它也可能发生在我们的建模选择不适合我们的数据库和软件选择时，这可能导致昂贵的（在时间或金钱上）的解决方案或替代方案。'
- en: Having well-defined rules and constraints on our data and models gives us explicit
    ways to test our pipeline. For example, if we know that our nodes can at most
    have two degrees, we can design simple functions or queries to process and test
    every node against this criterion.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据和模型上定义明确的规则和约束为我们提供了测试管道的明确方法。例如，如果我们知道我们的节点最多只能有两个度数，我们可以设计简单的函数或查询来处理和测试每个节点是否符合这一标准。
- en: 'Testing and refactoring are iterative processes and crucial in scaling an optimized
    graph schema and instance model [4, 5]. It will involve executing queries, analyzing
    results, making necessary adjustments, and validating against metrics. In the
    context of Whole Staffing’s recruitment data, this practice would ensure the model
    is tailored to capture real-world relationships and robust new data streams. Following
    are some examples for tests and refactoring:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 测试和重构是迭代过程，对于扩展优化的图模式和实例模型至关重要[4, 5]。它将涉及执行查询、分析结果、进行必要的调整，并对照指标进行验证。在Whole
    Staffing的招聘数据背景下，这项实践将确保模型能够捕捉现实世界的关系和稳健的新数据流。以下是测试和重构的一些示例：
- en: '*Cast your instance model in a system.* Store the model in your graph database
    or processing system of choice.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*将实例模型映射到系统中*。将模型存储在您选择的图数据库或处理系统中。'
- en: '*Create tests and run queries*. Based on the specific requirements, draft queries
    to test the integrity of your model. Use query languages such as Cypher or SPARQL
    to execute queries on a graph database. Programming languages, for example, Python,
    can also be used to query graphs within graph processing systems such as NetworkX.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建测试和运行查询*。根据具体要求，起草查询以测试您模型的完整性。使用Cypher或SPARQL等查询语言在图数据库上执行查询。例如，Python等编程语言也可以用于查询图处理系统（如NetworkX）内的图。'
- en: 'For our example’s simple schema, here are some possible tests:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们示例的简单模式，以下是一些可能的测试：
- en: '*Node attributes verification* —Each node should be checked to confirm that
    it possesses the required attributes, specifically the candidate’s industry and
    job type, and that these attributes have non-null values.'
  id: totrans-137
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点属性验证* —应检查每个节点，以确认它具有所需的属性，特别是候选人的行业和职位类型，以及这些属性具有非空值。'
- en: '*Edge type verification* —All connections between candidates should be validated
    to confirm that they are of the Knows type, ensuring consistency in relationship
    labeling.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边类型验证* —应验证所有候选人间的关系，以确认它们是Knows类型，确保关系标签的一致性。'
- en: '*Relationship verification* —Check the average number of relationships that
    exist to ensure it’s consistent with the average number of referrals.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关系验证* —检查关系的平均数量，以确保它与平均推荐数量的平均数量一致。'
- en: '*Unique IDs* —Every candidate node should be checked for unique identifiers
    to prevent data duplication and ensure data integrity.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*唯一标识符* — 应检查每个候选节点是否有唯一标识符，以防止数据重复并确保数据完整性。'
- en: '*Attribute data type* —The data types of `industry` and `jobType` attributes
    should be validated to ensure consistency across all candidate nodes.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*属性数据类型* — 应验证`industry`和`jobType`属性的数据类型，以确保所有候选节点的一致性。'
- en: '*Network structure* —The structure of the network should be validated to ensure
    it’s undirected, confirming the bidirectional nature of the Knows relationships
    between candidate nodes.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络结构* — 应验证网络结构以确保其为无向的，确认候选节点之间Knows关系的双向性质。'
- en: '*Edge cases* —Determine edge cases and query for those. In our case, the nodes
    that are unconnected may present a problem. Using queries to understand the extent
    of unconnected nodes and their effect on the analytics will drive decisions to
    refactor. Another edge case could be an isolated group of candidates whose relationships
    form a cycle. It would be important to ensure the data model and the analytical
    tools could handle such complex or unusual data patterns and still produce valid
    answers.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边缘情况* — 确定边缘情况并查询这些情况。在我们的案例中，未连接的节点可能存在问题。使用查询来了解未连接节点的范围及其对分析的影响，将推动重构决策。另一个边缘情况可能是一组孤立候选人，他们的关系形成一个循环。确保数据模型和分析工具能够处理这种复杂或异常的数据模式，并仍然产生有效答案，这将是重要的。'
- en: 3\. *Validate and evaluate performance* —Based on the results of the tests,
    determine if there are logical problems with your model and your use case, or
    problems with the data and attributes.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. *验证和评估性能* — 根据测试结果，确定您的模型和用例是否存在逻辑问题，或者数据属性存在问题。
- en: 4\. *Refactor* —Make adjustments to labels, properties, relationships, or constraints
    as needed to minimize errors.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. *重构* — 根据需要调整标签、属性、关系或约束，以最大限度地减少错误。
- en: 5\. *Repeat* —Iterate the preceding steps, refining the model based on evaluations
    and ensuring alignment with the project needs and constraints.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. *重复* — 重复前面的步骤，根据评估结果精炼模型，并确保与项目需求和约束一致。
- en: 6\. *Final assessment* —Evaluate the final model against criteria and best practices
    to ensure its readiness for complex queries and machine learning applications.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. *最终评估* — 评估最终模型是否符合标准和最佳实践，以确保其适用于复杂查询和机器学习应用。
- en: With this iterative process of testing and refactoring, we refine the dataset
    for Whole Staffing’s recruitment data and use case. Attention to detail guarantees
    the model is ready to support evaluation of the complex, nuanced relationships
    hidden within the recruitment data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种测试和重构的迭代过程，我们精炼Whole Staffing招聘数据和使用案例的数据集。对细节的关注确保模型准备好支持对招聘数据中隐藏的复杂、细微关系的评估。
- en: As we transition into the next section, our focus shifts to the practical implementation
    of some of these concepts. We’ll look at creating data pipelines in PyG, showing
    how to convert data from its initial raw form to a preprocessed state, ready for
    input into other downstream model training and testing routines.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们过渡到下一节时，我们的重点转向这些概念的实际实施。我们将探讨在PyG中创建数据管道，展示如何将数据从其初始原始形式转换为预处理状态，以便输入到其他下游模型训练和测试程序。
- en: 8.3 Data pipeline example
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 数据管道示例
- en: With the schema decided, let’s walk through an example of a data pipeline. In
    this section, we assume our objective is to create a simple data workflow that
    takes data from a raw state and ends with a preprocessed dataset that can be passed
    to a GNN. These steps are summarized in figure 8.7.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定模式后，让我们通过一个数据管道的示例。在本节中，我们假设我们的目标是创建一个简单的数据工作流程，它从原始状态的数据开始，并以可以传递给GNN的预处理数据集结束。这些步骤在图8.7中进行了总结。
- en: Note that while the overall steps shown can be consistent from one problem to
    another, the details of implementation for each step can be unique to the problem,
    its data, and the chosen data storage, processing, and model training options.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然显示的总体步骤可以从一个问题到另一个问题保持一致，但每个步骤的实现细节可能因问题、其数据以及选择的数据存储、处理和模型训练选项而独特。
- en: '![figure](../Images/8-7.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-7.png)'
- en: Figure 8.7 Summary of steps in the data pipeline process in this section
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7 本节中数据管道处理步骤的总结
- en: Key terms
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键术语
- en: 'The following are key terms used in this section (for more details on graph
    data models and types of graphs, see appendix A):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在本节中使用的关键术语（有关图数据模型和图类型的更多详细信息，请参阅附录A）：
- en: '*Adjacency list—*A basic representation of graph data. In this format, each
    entry contains a node with a list of its adjacent nodes.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*邻接表*—图数据的基本表示。在这种格式中，每个条目包含一个节点及其相邻节点的列表。'
- en: '*Adjacency matrix*—A basic representation of graph data. In a matrix, each
    row and column correspond to a node. The cells, where these rows and columns intersect,
    signify the presence of edges between the nodes. A cell with a nonzero value indicates
    an edge between the nodes, while a zero value signifies no connection.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*邻接矩阵*—图数据的基本表示。在一个矩阵中，每一行和每一列对应一个节点。这些行和列交叉的单元格表示节点之间存在边。非零值的单元格表示节点之间存在边，而零值表示没有连接。'
- en: '*Degree*—The degree of a node is the count of its adjacent nodes.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度*—节点的度是其相邻节点的数量。'
- en: '*Edge list—*A basic representation of a graph. It’s an array of all the edges
    in a graph; each entry in the array contains a unique pair of connected nodes.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边列表*—图的基本表示。它是一个包含图中所有边的数组；数组中的每个条目包含一对连接的节点。'
- en: '*Mask—*A Boolean array (or tensor in the case of PyTorch) that is used to select
    specific subsets of data. Masks are commonly used for splitting a dataset into
    different parts, such as training, validation, and testing sets.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*掩码*—一个布尔数组（或PyTorch中的张量），用于选择特定的数据子集。掩码通常用于将数据集分割成不同的部分，如训练集、验证集和测试集。'
- en: '*Rank*—In our context, rank refers to the position of each node’s degree in
    a sorted list. So, the node with the highest degree has rank 1, the next highest
    has rank 2, and so on.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*排名*—在我们的上下文中，排名指的是每个节点度在排序列表中的位置。因此，度最高的节点排名为1，下一个最高的排名为2，依此类推。'
- en: '*Raw data*—Data in its most unprocessed form.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原始数据*—最未加工的数据形式。'
- en: '*Serialization*—Putting data into a format that is easily stored or exported.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列化*—将数据放入易于存储或导出的格式。'
- en: '*Subgraph*—A subgraph is a subset of a larger graph’s nodes and edges.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*子图*—子图是较大图节点和边的子集。'
- en: 8.3.1 Raw data
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 原始数据
- en: '*Raw data* refers to data in its most unprocessed state; such data is the starting
    point for our pipeline. This data can be in various databases, serialized in some
    way, or generated.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*原始数据*指的是最未加工状态的数据；此类数据是管道的起点。这些数据可以存储在各种数据库中，以某种方式序列化，或生成。'
- en: In the development stage of an application, it’s important to know how closely
    the raw data used will match the live data used in production. One way to do this
    is by sampling from data archives.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序的开发阶段，了解所使用的原始数据与生产中使用的实时数据之间的匹配程度非常重要。一种方法是通过对数据存档进行抽样。
- en: 'As mentioned in section 8.1, there are at least two sources for our example
    problem: relational database tables that contain recommendation logs and candidate
    profiles. To keep our example contained, we assume a helpful engineer has already
    queried the log data and transformed it into a JSON format, where keys are a recommending
    candidate, and the values are the recommended candidates. From our profile data,
    we have two other fields: *industry* and *job type*. For both data sources, our
    engineer has used a hash to protect PII, which we can consider a unique identifier
    for the candidate. In this section, we’ll use the JSON data, where an example
    snippet is shown in figure 8.8\. The data is displayed in two ways: with a hash
    and without a hash.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如第8.1节所述，我们的示例问题至少有两个来源：包含推荐日志和候选者资料的关联数据库表。为了使我们的示例保持简洁，我们假设一位有助的工程师已经查询了日志数据并将其转换为JSON格式，其中键是推荐候选者，值是推荐的候选者。从我们的资料数据中，我们还有两个其他字段：*行业*和*工作类型*。对于这两个数据源，我们的工程师已经使用散列来保护PII，我们可以将其视为候选者的唯一标识符。在本节中，我们将使用JSON数据，其中示例片段如图8.8所示。数据以两种方式显示：带有散列和不带散列。
- en: Data encoding and serialization
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据编码和序列化
- en: 'One key consideration when constructing the pipeline is the choice of what
    data format to use when importing and exporting data from one system to another.
    For transferring graph data into another system or sending it over the internet,
    *encoding* or *serialization* is typically used. These terms refer to the process
    of putting data in a form that is easily transferable [6, 7]. Before choosing
    an encoding format, you must have decided upon the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建管道时，一个关键的考虑因素是选择在从一个系统导入和导出数据到另一个系统时使用的数据格式。为了将图数据传输到另一个系统或通过互联网发送，通常使用*编码*或*序列化*。这些术语指的是将数据放入易于传输的形式[6,
    7]。在选择编码格式之前，你必须决定以下内容：
- en: '*Data model* —Simple model, property graph, or other?'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据模型*—简单模型、属性图或其他？'
- en: '*Schema* —Which entities in your data are nodes, edges, and properties?'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模式* — 你的数据中的哪些实体是节点、边和属性？'
- en: '*Data structure* —How is the data stored: in adjacency matrices, adjacency
    lists, or edge lists?'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据结构* — 数据是如何存储的：在邻接矩阵、邻接列表或边列表中？'
- en: '*Receiving systems* —How does the receiving system (in our case, GNN libraries
    and graph-processing systems) accept data? What encodings and data structures
    are preferred? Is imported data automatically recognized, or is custom programming
    required to read in data?'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*接收系统* — 接收系统（在我们的案例中，是 GNN 库和图处理系统）如何接受数据？首选的编码和数据结构是什么？导入的数据是否自动识别，或者是否需要自定义编程来读取数据？'
- en: '![figure](../Images/8-8.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/8-8.png)'
- en: 'Figure 8.8 View of raw data: JSON file. The figure on the left is in key/value
    format. The keys are the members, and the values are their known relationships.
    The figure on the right shows unhashed values, demonstrating example names for
    these individuals.'
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.8 原始数据视图：JSON 文件。左边的图是键/值格式。键是成员，值是它们已知的关联。右边的图显示了未散列的值，展示了这些个人的示例名称。
- en: 'Here are a few encoding choices you’re likely to encounter:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可能会遇到的编码选择：
- en: '*Language and system-agnostic encodings formats* —These are most popular as
    they are extremely flexible and work across many systems and languages. However,
    data arrangement can still differ from system to system. Therefore, an edge list
    in a CSV file, with a specific set of headers, may not be accepted or interpreted
    in the same way between two different systems. Following are some examples for
    this format:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言和系统无关的编码格式* — 这些格式因其极端灵活性和跨多个系统和语言的适用性而最受欢迎。然而，数据排列可能仍然因系统而异。因此，CSV 文件中的边列表，带有特定的标题集，可能在不同的系统之间不被接受或以相同的方式解释。以下是一些该格式的示例：'
- en: '*JSON* —Has advantages when reading from APIs or feeding into JavaScript applications.
    `Cytoscape.js`, a graph visualization library, accepts data in JSON format.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*JSON* — 在从 API 读取或输入到 JavaScript 应用程序时具有优势。图可视化库 `Cytoscape.js` 接受 JSON 格式的数据。'
- en: '*CSV* —Accepted by many processing systems and databases. However, the required
    arrangement and labeling of the data differs from system to system.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CSV* — 被许多处理系统和数据库接受。然而，数据排列和标签的要求因系统而异。'
- en: '*XML* —Graph Exchange XML (GEXF) format is of course an XML format.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*XML* — 图交换 XML (GEXF) 格式当然是一种 XML 格式。'
- en: '*Language specific* *—*Python, Java, and other languages have built-in encoding
    formats.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言特定* — Python、Java 和其他语言有内置的编码格式。'
- en: '*Pickle* —Python’s format. Some systems accept Pickle encoded files. Despite
    this, unless your data pipeline or workflow is governed extensively by Python,
    pickles should be used lightly. The same applies for other language-specific encodings.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pickle* — Python 的格式。一些系统接受 Pickle 编码的文件。尽管如此，除非你的数据管道或工作流程广泛受 Python 管理，否则应谨慎使用
    Pickle。其他语言特定的编码也适用同样的原则。'
- en: '*System driven* —Specific software, systems, and libraries have their own encoding
    formats. Though these may be limited in usability between systems, an advantage
    is that the schema in such formats is consistent. Software and systems that have
    their own encoding format include Stanford Network Analysis Platform (SNAP), NetworkX,
    and Gephi.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统驱动* — 特定软件、系统和库有自己的编码格式。尽管这些格式在系统间的可用性可能有限，但一个优点是这些格式的模式是一致的。拥有自己编码格式的软件和系统包括斯坦福网络分析平台
    (SNAP)、NetworkX 和 Gephi。'
- en: '*Big data* —Aside from the language-agnostic formats listed previously, there
    are other encoding formats used for larger sizes of data.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大数据* — 除了之前列出的语言无关格式外，还有其他用于更大数据量的编码格式。'
- en: '*Avro* —This encoding is used extensively in Hadoop workflows'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Avro* — 这种编码在 Hadoop 工作流程中被广泛使用'
- en: '*Matrix based* —Because graphs can be expressed as matrices, there are a few
    formats that are based on this data structure. For sparse graphs, the following
    formats provide substantial memory savings and computational advantages (for lookups
    and matrix/vector multiplication):'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于矩阵* — 因为图可以用矩阵表示，所以有一些基于这种数据结构的格式。对于稀疏图，以下格式提供了大量的内存节省和计算优势（对于查找和矩阵/向量乘法）：'
- en: Sparse column matrix (.csc filetype)
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏列矩阵 (.csc 文件类型)
- en: Sparse row matrix (.csr filetype)
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏行矩阵 (.csr 文件类型)
- en: Matrix market format (.mtx filetype)
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵市场格式 (.mtx 文件类型)
- en: 8.3.2 The ETL step
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 ETL 步骤
- en: With the schema chosen and data sources established, the *ETL* (*extract, transform,
    load*) step consists of taking raw data from its sources and then producing data
    that fits the schema and is ready for preprocessing or training. For our data,
    this consists of programming a set of actions that begin with pulling the data
    from the various databases and then joining them as needed.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模式和建立数据源后，*ETL*（*提取、转换、加载*）步骤包括从其来源提取原始数据，然后生成符合模式且准备进行预处理或训练的数据。对于我们的数据，这包括编写一系列操作，从各种数据库中提取数据，然后根据需要将它们连接起来。
- en: We need data that ends up in a specific format that we can input into a preprocessing
    step. This could be a JSON format or an edge list. For either the JSON example
    or edge list example, our schema is fulfilled; we’ll have nodes (the individual
    persons) and edges (the relationships between these people).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要最终以特定格式结束的数据，我们可以将其输入到预处理步骤中。这可以是JSON格式或边列表。对于JSON示例或边列表示例，我们的模式得到满足；我们将有节点（个人）和边（这些人之间的关系）。
- en: 'For our recruitment example, we want to transform our raw data into a graph
    data structure, encoded in CSV. This was chosen for ease of manipulation with
    Python. This file can then be loaded into our graph-processing system, NetworkX,
    or a GNN package such as PyG. To summarize the next steps, we’ll do the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的招聘示例，我们希望将我们的原始数据转换为编码在CSV中的图数据结构。这是为了方便用Python进行操作。然后，我们可以将此文件加载到我们的图处理系统NetworkX或PyG等GNN包中。为了总结下一步，我们将执行以下操作：
- en: Convert the raw data file to a graph format, following your chosen graph data
    model. In our case, we convert the raw data into an edge list and an adjacency
    list. We then saved it as a CSV file.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照您选择的图数据模型将原始数据文件转换为图格式。在我们的情况下，我们将原始数据转换为边列表和邻接表。然后我们将其保存为CSV文件。
- en: Load the CSV file into NetworkX for exploratory data analysis (EDA) and visualization.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV文件加载到NetworkX中进行探索性数据分析（EDA）和可视化。
- en: Load into PyG and preprocess.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载到PyG并进行预处理。
- en: Raw data to adjacency List and edge List
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将原始数据转换为邻接表和边列表
- en: 'Starting with our CSV and JSON files, we next convert the data into two key
    data models: an edge list and an adjacency list, which we define in appendix A.
    Both adjacency and edge lists are two basic data representations used with graphs.
    An edge list is a list where every item in this structure contains a node with
    a list of its adjacent nodes. These representations are illustrated in figure
    8.9.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的CSV和JSON文件开始，我们接下来将数据转换为两个关键数据模型：边列表和邻接列表，我们在附录A中定义了它们。邻接表和边列表都是与图一起使用的两种基本数据表示。边列表是一个列表，其中该结构中的每个条目都包含一个节点及其相邻节点的列表。这些表示在图8.9中说明。
- en: '![figure](../Images/8-9.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-9.png)'
- en: Figure 8.9 A graph with nodes and edges marked (top). An edge list representation
    (middle); each entry contains the edge number and the pair of nodes connected.
    An adjacency list representation in a dictionary (bottom); each key is a node,
    and the values are its adjacent nodes.
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9 一个带有节点和边标记的图（顶部）。边列表表示（中间）；每个条目包含边号和连接的节点对。字典中的邻接列表表示（底部）；每个键是一个节点，其值是该节点的相邻节点。
- en: First using the `json` module, we load the data from a JSON file into a Python
    dictionary. The Python dictionary has the same structure as the JSON, with member
    hashes as keys and their relationships as values.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`json`模块，我们将数据从JSON文件加载到Python字典中。Python字典的结构与JSON相同，成员哈希作为键，其关系作为值。
- en: Creating an adjacency list
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建邻接表
- en: Next, we create an adjacency list from this dictionary. This list will be stored
    as a text file. Each line of the file will contain the member hash, followed by
    hashes of that member’s relationships. The process for creating an adjacency list
    is illustrated in figure 8.10.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从该字典创建一个邻接表。此列表将存储为文本文件。文件的每一行将包含成员哈希，后跟该成员关系的哈希。创建邻接表的过程在图8.10中说明。
- en: 'This function transforms our raw data into an adjacency list, which we’ll apply
    to our recruitment example. We’ll have *inputs* that consist of the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将我们的原始数据转换为邻接表，我们将将其应用于我们的招聘示例。我们将有以下*输入*：
- en: A dictionary of candidate referrals where the keys are members who have referred
    other candidates, and the values are lists of the people who were referred
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字典，其中包含候选人的推荐，键是推荐其他候选人的成员，值是被推荐的人的列表
- en: A suffix to append to the filename
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要附加到文件名上的后缀
- en: '![figure](../Images/8-10.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-10.png)'
- en: Figure 8.10 Flow diagram illustrating the process of transforming a relationship
    dictionary into a well-structured adjacency list, stored in a text file, while
    ensuring the symmetry of connections in the undirected graph.
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.10 流程图说明了将关系字典转换为结构良好的邻接表的过程，该邻接表存储在文本文件中，同时确保无向图中连接的对称性。
- en: 'We’ll have *outputs* that consist of the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下内容的 *输出*：
- en: An encoded adjacency list in a txt file
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: txt 文件中的编码邻接表
- en: A list of the node IDs found
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到的节点 ID 列表
- en: This is shown in the following listing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下面的列表中显示。
- en: Listing 8.1 Create an adjacency list from relationship dictionary
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1 从关系字典创建邻接表
- en: '[PRE0]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Runs through every node in the input data dictionary'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 遍历输入数据字典中的每个节点'
- en: '#2 Because this is an undirected graph, there must be a symmetry in the values;
    that is, every value in a key must contain that key in its own entry. As an example,
    for entry F, if G is a value, then for entry G, F must be a value. These lines
    check for that and fix the dictionary if these conditions don’t exist.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 因为这是一个无向图，所以值之间必须存在对称性；也就是说，一个键中的每个值都必须包含该键在其自己的条目中。例如，对于条目 F，如果 G 是一个值，那么对于条目
    G，F 必须也是一个值。这些行检查这些条件是否存在，并在这些条件不存在时修复字典。'
- en: '#3 Creates a text file that will store the adjacency list'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建一个将存储邻接表的文本文件'
- en: '#4 For every key in the dictionary'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对于字典中的每个键'
- en: '#5 Creates a string from the list of dictionary values. This value is a string
    of member IDs separated by empty spaces.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从字典值的列表中创建一个字符串。这个值是成员 ID 的字符串，由空格分隔。'
- en: '#6 Optional print'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 可选打印'
- en: '#7 Writes a line to the text file. This line will contain the member hash,
    and then a string of relationship hashes.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 将一行写入文本文件。这一行将包含成员哈希，然后是一个关系哈希的字符串。'
- en: Creating an edge list
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建边列表
- en: Next, we show the process to create an edge list. As with the adjacency list,
    we transform the data to account for node pair symmetry. Note that either format
    could work for this project. For your own project, another format could also be
    warranted. Figure 8.11 illustrates the process.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示创建边列表的过程。与邻接表一样，我们转换数据以考虑节点对对称性。请注意，这两种格式都可以用于此项目。对于您自己的项目，另一种格式也可能是必要的。图
    8.11 说明了这个过程。
- en: '![figure](../Images/8-11.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-11.png)'
- en: Figure 8.11 Process of creating an edge list file programmed into listing 8.2
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.11 列表 8.2 中编程创建边列表文件的过程
- en: 'As with the adjacency list function, the edge list function illustrates the
    transformation of raw data into an edge list and has the same inputs as the previous
    function. The outputs consist of the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与邻接表函数一样，边列表函数说明了原始数据到边列表的转换，并且具有与上一个函数相同的输入。输出包括以下内容：
- en: An edge list in a .txt file
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .txt 文件中的边列表
- en: Lists of the node IDs found and the edges generated
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到的节点 ID 列表和生成的边列表
- en: By definition, every entry of an edge list must be unique, so we must ensure
    that our produced edge list is the same. Here’s the code to create an edge list
    from a relationship dictionary.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，边列表中的每个条目都必须是唯一的，因此我们必须确保我们生成的边列表是相同的。以下是创建从关系字典到边列表的代码。
- en: Listing 8.2 Create an edge list from relationship dictionary
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2 从关系字典创建边列表
- en: '[PRE1]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Each member dictionary value is a list of relationships. For every key,
    we iterate through every value.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 每个成员字典的值是一个关系列表。对于每个键，我们遍历每个值。'
- en: '#2 Because this graph is undirected, we don’t want to create duplicate edges.
    For example, because {F,G} is the same as {G,F}, we only need one of these. This
    line checks if a node pair exists already. We use a set object because the node
    order doesn’t matter.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 因为这个图是无向的，我们不希望创建重复的边。例如，因为 {F,G} 与 {G,F} 相同，我们只需要其中一个。这一行检查节点对是否已经存在。我们使用集合对象，因为节点顺序不重要。'
- en: '#3 Writes the line to the text file. This line will consist of the node pair.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将一行写入文本文件。这一行将包含节点对。'
- en: In the next sections, we’ll use the adjacency list to load our graph into NetworkX.
    One thing to note about the differences between loading a graph using the adjacency
    list versus the edge list is that edge lists can’t account for single, unlinked
    nodes. It turns out that quite a few of the candidates at Whole Staffing haven’t
    recommended anyone, and don’t have edges associated with them. These nodes would
    be invisible to an edge list representation of the data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用邻接表将我们的图加载到NetworkX中。关于使用邻接表与边列表加载图之间的区别，有一点需要注意，那就是边列表无法考虑单个未连接的节点。结果发现，Whole
    Staffing的许多候选人都没有推荐任何人，也没有与他们相关的边。这些节点在数据边列表表示中将是不可见的。
- en: 8.3.3 Data exploration and visualization
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 数据探索和可视化
- en: Next, we want to load our network data into a graph processing framework. We
    chose NetworkX, but there are many other choices available, depending on your
    task and language preferences. We chose NetworkX because we have a small graph,
    and we also want to do some light EDA and visualization.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望将我们的网络数据加载到图处理框架中。我们选择了NetworkX，但根据您的任务和语言偏好，还有许多其他选择。我们选择NetworkX是因为我们的图很小，我们还想做一些轻量级的EDA和可视化。
- en: With our newly created adjacency list, we can create a NetworkX graph object
    by calling the `read_edgelist` or `read_adjlist` methods. Next, we can load in
    the attributes `industry` and `job type`. In this example, these attributes are
    loaded in as a dictionary, where the node IDs serve as keys.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们新创建的邻接表，我们可以通过调用`read_edgelist`或`read_adjlist`方法来创建一个NetworkX图对象。接下来，我们可以加载属性`industry`和`job
    type`。在这个例子中，这些属性作为字典加载，其中节点ID作为键。
- en: With our graph loaded, we can explore and inspect our data to ensure that it
    aligns with our assumptions. First, the count of nodes and edges should match
    our member count, and the number of edges created in our edge list, respectively,
    as shown in the following listing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的图加载后，我们可以探索和检查我们的数据，以确保它与我们的假设一致。首先，节点和边的数量应该与我们的成员数量相匹配，以及我们在边列表中创建的边的数量，如下所示。
- en: Listing 8.3 Create an edge list from the relationship dictionary
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3 从关系字典创建边列表
- en: '[PRE2]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We want to check how many connected components our graph has:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想检查我们的图有多少个连通分量：
- en: '[PRE3]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `connected_components` method generates the connected components of a graph;
    a visualization is shown in figure 8.12 and generated using NetworkX. There are
    hundreds of components, but when we inspect this data, we find that there is one
    large component of 1,698 nodes, and the rest are composed of less than 4 nodes.
    Most of the disconnected components are singleton nodes (the candidates that never
    refer anyone). For more information about components of a graph, we give definitions
    and details in appendix A.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`connected_components`方法生成图的连通分量；可视化如图8.12所示，并使用NetworkX生成。有数百个分量，但当我们检查这些数据时，我们发现有一个由1,698个节点组成的大分量，其余的由不到4个节点组成。大多数断开连接的分量是单节点（从未推荐任何人的候选人）。有关图分量的更多信息，我们在附录A中给出了定义和细节。'
- en: '![figure](../Images/8-12.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-12.png)'
- en: Figure 8.12 The full graph, with its large connected component in the middle,
    surrounded by many smaller components. For our example, we’ll use only the nodes
    in the large connected component.
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.12 完整的图，其中包含中间的大连通分量，周围环绕着许多较小的分量。在我们的例子中，我们将只使用大连通分量中的节点。
- en: We’re interested in this large connected component and will work with that going
    forward. The `subgraph` method can help us to isolate this large component.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个大连通分量感兴趣，并将继续使用它。`subgraph`方法可以帮助我们隔离这个大分量。
- en: Finally, we use NetworkX to visualize our graph. For this, we’ll use a standard
    recipe for analyzing graphs which can also be found in the NetworkX documentation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用NetworkX来可视化我们的图。为此，我们将使用分析图的标准配方，该配方也可以在NetworkX文档中找到。
- en: 'Let’s go through the different steps (the full code sample for each step is
    also in the repository, labeled “Function that visualizes the social graph and
    shows degree statistics”):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一介绍不同的步骤（每个步骤的完整代码示例也存放在仓库中，标记为“可视化社交图并显示度统计的函数”）：
- en: '*Create the graph object*. Generate a distinct graph object, selecting the
    largest connected component from the given graph. In cases where there’s only
    one connected component, this step might be unnecessary but ensures the selection
    of the major component.'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建图对象*。从给定的图中选择最大的连通分量，生成一个独特的图对象。在只有一个连通分量的情况下，这一步可能是不必要的，但确保选择了主要分量。'
- en: '[PRE4]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '2\. *Determine the layout*. Decide the positioning of nodes and edges for visualization.
    Choose an appropriate layout algorithm; for example, the Spring Layout models
    the edges as springs and nodes as repelling masses:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. *确定布局*。决定节点和边的可视化位置。选择合适的布局算法；例如，Spring Layout将边建模为弹簧，节点为排斥质量：
- en: '[PRE5]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. *Draw nodes and edges*. Use the chosen layout to draw nodes on the visualization.
    Adjust visual parameters such as node size to enhance the clarity of the figure.
    Based on the selected layout, draw the edges. Modify appearance settings such
    as transparency to achieve the desired visual effect.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. *绘制节点和边*。使用所选布局在可视化中绘制节点。调整节点大小等视觉参数以增强图形的清晰度。根据所选布局绘制边。通过调整透明度等外观设置以实现所需的视觉效果。
- en: '[PRE6]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '4\. *Generate and plot node degrees*. Employ the degree method on the graph
    object to create an iterable of nodes with their respective degrees, and sort
    them from highest to lowest. Visualize the sorted list of node degrees on a plot
    to analyze the distribution and prominence of various nodes. Use NumPy’s `unique`
    method with the `return_counts` parameter to plot a histogram showing the degrees
    of nodes and their counts, providing insights into the graph’s structure and complexity:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. *生成和绘制节点度数*。在图对象上使用度数方法创建一个包含各自度数的节点可迭代对象，并按从高到低的顺序排序。在图上可视化排序后的节点度数列表，以分析各种节点的分布和突出度。使用NumPy的`unique`方法与`return_counts`参数来绘制一个直方图，显示节点的度数及其计数，从而深入了解图的结构和复杂性：
- en: '[PRE7]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These plots are shown in figure 8.13.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图示显示在图8.13中。
- en: '![figure](../Images/8-13.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-13.png)'
- en: Figure 8.13 Visualization and statistics of the social graph and its large connected
    component. Network visualization using NetworkX default settings (top). A rank
    plot of node degree of the entire graph (bottom left). We see that about three-fourths
    of nodes have less than 20 adjacent nodes. A histogram of degree (bottom right).
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.13展示了社交图及其大型连通组件的视觉化和统计数据。使用NetworkX默认设置进行网络可视化（顶部）。整个图节点度的排名图（左下角）。我们看到大约四分之三的节点相邻节点少于20个。度数的直方图（右下角）。
- en: 'Lastly, we can visualize an adjacency matrix of our graph, shown in figure
    8.14, using the following command:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下命令可视化我们图的邻接矩阵，如图8.14所示：
- en: '[PRE8]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![figure](../Images/8-14.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/8-14.png)'
- en: Figure 8.14 A visualized adjacency matrix of our social graph. Vertical and
    horizontal values refer to respective nodes.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.14展示了我们的社交图的视觉邻接矩阵。垂直和水平值分别指代相应的节点。
- en: As with the numerical adjacency matrix, for our undirected graph, this visual
    adjacency matrix has symmetry down the diagonal. All undirected graphs will have
    symmetric adjacency matrices. For directed graphs, this can happen but isn’t guaranteed.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 与数值邻接矩阵一样，对于我们的无向图，这个视觉邻接矩阵在对角线处具有对称性。所有无向图都将具有对称的邻接矩阵。对于有向图，这可能发生，但并不保证。
- en: 8.3.4 Preprocessing and loading data into PyG
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 预处理和将数据加载到PyG中
- en: For this book, *preprocessing* consists of putting our data, including its properties,
    labels, or other metadata, in a format suitable for downstream machine learning
    models. Feature engineering can also be a step in this process. For feature engineering,
    we’ll often use graph algorithms to calculate the properties of nodes, edges,
    or subgraphs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，*预处理*包括将我们的数据及其属性、标签或其他元数据放入适合下游机器学习模型的格式。特征工程也可以是这个过程的一个步骤。对于特征工程，我们通常会使用图算法来计算节点、边或子图的性质。
- en: An example for node features is betweenness centrality. If our schema allows,
    we can calculate and attach such properties to the node entities of our data.
    To perform this, we take the output of the ETL step, say an edge list, and import
    this into a graph processing framework to calculate betweenness centrality for
    each node. Once this quantity is obtained, we can store it using a dictionary
    with the node ID as keys, then use this as a node feature later on.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 节点特征的例子是介数中心性。如果我们的模式允许，我们可以计算并将此类属性附加到数据中节点实体。为此，我们取ETL步骤的输出，比如说一个边列表，并将其导入到图处理框架中，以计算每个节点的介数中心性。一旦获得这个量，我们可以使用以节点ID为键的字典来存储它，然后稍后将其用作节点特征。
- en: Betweenness centrality
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 介数中心性
- en: '*Betweenness centrality* is a critical measure of node importance that quantifies
    the tendency of a node to lie in the shortest paths from source to destination
    nodes. Given a graph with *n* nodes, you could determine the shortest path between
    every unique pair of nodes in this graph. We could take this set of shortest paths
    and look for the presence of a particular node. If the node appears in all or
    most of these paths, it has a high betweenness centrality and would be considered
    to be highly influential. Conversely, if the node appears a few times (or only
    once) in the set of shortest paths, it will have a low betweenness centrality,
    and a low influence.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '*中介中心性* 是衡量节点重要性的关键指标，它量化了节点位于源节点到目标节点最短路径中的趋势。给定一个包含 *n* 个节点的图，你可以确定图中每对唯一节点之间的最短路径。我们可以取这组最短路径，并查找特定节点的存在。如果一个节点出现在所有或大多数这些路径中，它就具有高中介中心性，并且被认为是高度有影响力的。相反，如果一个节点在
    shortest paths 集合中只出现几次（或只出现一次），它将具有低中介中心性，并且影响力低。'
- en: 'Now that we have our data, we want to make it ready for use in our selected
    GNN framework. In this book, we use PyG, due to its robust suite of tools and
    flexibility in handling complex graph data. However, most standard GNN packages
    have mechanisms to import custom data into their frameworks. For this section,
    we’ll focus on three modules within PyG:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，我们希望使其准备好在我们的所选 GNN 框架中使用。在这本书中，我们使用 PyG，因为它拥有强大的工具套件和灵活处理复杂图数据的能力。然而，大多数标准
    GNN 软件包都有导入自定义数据到其框架的机制。对于本节，我们将关注 PyG 中的三个模块：
- en: '`Data` *module* (`torch_geometric.data`)—Allows inspection, manipulation, and
    creation of data objects that are used by the PyG environment.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`数据` *模块* (`torch_geometric.data`) — 允许检查、操作和创建 PyG 环境中使用的数据对象。'
- en: '`Utils` *module* (`torch_geometric.utils`)—Many useful methods. Helpful in
    this section are methods that allow the quick import and export of graph data.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Utils` *模块* (`torch_geometric.utils`) — 许多有用的方法。在本节中，允许快速导入和导出图数据的方法很有帮助。'
- en: '`Datasets` *module* (`torch_geometric.datasets`)—Preloaded datasets, including
    benchmark datasets, and datasets from influential papers in the field.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`数据集` *模块* (`torch_geometric.datasets`) — 预加载的数据集，包括基准数据集和该领域有影响力的论文中的数据集。'
- en: Let’s begin with the `Datasets` module. This module contains datasets that have
    already been preprocessed and can readily be used by PyG’s methods. When starting
    with PyG, having these datasets allows for easy experimentation without worrying
    about creating a data pipeline. Similarly, by studying the codebase underlying
    these datasets, we can also learn how to create our own custom datasets.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `Datasets` 模块开始。此模块包含已经预处理的并可以由 PyG 的方法直接使用的数据集。当你开始使用 PyG 时，拥有这些数据集可以让你轻松地进行实验，无需担心创建数据管道。同样，通过研究这些数据集背后的代码库，我们也可以学习如何创建我们自己的自定义数据集。
- en: 'At the end of the previous section, we converted our raw data into a standard
    format and loaded our new graphs into a graph-processing framework. Now, we want
    to load our data into the PyG environment. Preprocessing in PyG has a few objectives:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的结尾，我们将原始数据转换为标准格式，并将我们的新图加载到图处理框架中。现在，我们希望将数据加载到 PyG 环境中。PyG 中的预处理有几个目标：
- en: Creating data objects with multiple attributes from the level of nodes and edges
    to the subgraph and graph level
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从节点和边级别到子图和图级别的多个属性的数据对象创建
- en: Combining different data sources into one object or set of related objects
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不同的数据源组合成一个对象或一组相关对象
- en: Converting data into objects that can be processed using GPUs
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据转换为可以使用 GPU 处理的对象
- en: Allowing splitting of training/testing/validation data
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许分割训练/测试/验证数据
- en: Enabling batching of data for training
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许数据批处理以进行训练
- en: 'These objectives are fulfilled by a hierarchy of classes within the `Data`
    module:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这些目标通过 `Data` 模块中的类层次结构来实现：
- en: '`Data` *class*—Creates graph objects. These objects can have optional built-in
    and custom-made attributes.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`数据` *类* — 创建图对象。这些对象可以具有可选的内置和自定义属性。'
- en: '`Dataset` *and* `InMemoryDataset` *classes* —Creates a repeatable data preprocessing
    pipeline. You can start from raw data files and add custom filters and transformations
    to achieve your preprocessed *data* objects. `Dataset` objects are larger than
    memory, while `InMemoryDataset` objects fit in memory.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset` *和* `InMemoryDataset` *类* — 创建可重复的数据预处理管道。你可以从原始数据文件开始，并添加自定义过滤器转换以实现你的预处理
    *数据* 对象。`Dataset` 对象比内存大，而 `InMemoryDataset` 对象适合内存。'
- en: '`Dataloader` *class* —Batches data objects for model training.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataloader` *类* —为模型训练批量数据对象。'
- en: This is shown in figure 8.15, including how different data and dataset classes
    connect to the dataloader.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图 8.15 中显示，包括不同的数据和数据集类如何连接到 dataloader。
- en: '![figure](../Images/8-15.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8-15.png)'
- en: Figure 8.15 Steps to preprocess data in PyG. From raw files, there are essentially
    two paths to prep data for ingestion by a PyG algorithm. The first path, shown
    here, directly creates an iterator of data instances, which is used by the dataloader.
    The second path mimics the first but performs this process within the dataloader
    class.
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.15 PyG 中预处理数据的步骤。从原始文件中，有两条基本路径用于为 PyG 算法准备数据。第一条路径，如这里所示，直接创建数据实例的迭代器，该迭代器由
    dataloader 使用。第二条路径模仿第一条路径，但在 dataloader 类内部执行此过程。
- en: 'There are two paths to preprocess data, one uses a `dataset` class and the
    other goes without it. The advantage of using the `dataset` class is that it allows
    us to save the generated datasets and also preserve filtering and transformation
    details. Dataset objects are flexible and can be modified to output variations
    of a dataset. On the other hand, if your custom dataset is simple or generated
    on the fly, and you have no use for saving the data or process long term, bypassing
    dataset objects may serve you well. So, in summary, we have the following different
    data-related classes:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理数据有两种途径，一种使用 `dataset` 类，另一种则不使用。使用 `dataset` 类的优势在于它允许我们保存生成的数据集，并保留过滤和转换的细节。数据集对象是灵活的，可以修改以输出数据集的不同变体。另一方面，如果你的自定义数据集很简单或即时生成，并且你不需要保存数据或长期处理，绕过数据集对象可能对你很有帮助。因此，总的来说，我们有以下不同的数据相关类：
- en: '`Datasets` *objects* —Preprocessed datasets for benchmarking or testing an
    algorithm or architecture (not to be confused with `Dataset`—no “s” at the end—objects).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`数据集` *对象* —用于基准测试或测试算法或架构的预处理数据集（不要与 `Dataset`—结尾没有“s”的对象混淆）。'
- en: '`Data` *objects into iterator* —Graph objects that are generated on the fly
    or for whom there is no need to save.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`数据` *对象转换为迭代器* —即时生成或无需保存的图对象。'
- en: '`Dataset` *object* —For graph objects that should be preserved, including the
    data pipeline, filtering and transformations, input raw data files, and output
    processed data files. Not to be confused with `Datasets` (with “s” at the end)
    objects.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset` *对象* —对于应该保留的图对象，包括数据管道、过滤和转换、输入原始数据文件和输出处理后的数据文件。不要与 `Datasets`（结尾有“s”）对象混淆。'
- en: 'With those basics, let’s preprocess our social graph data. We’ll cover the
    following cases:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握这些基础知识后，让我们预处理我们的社交图数据。我们将涵盖以下情况：
- en: '*Convert into a* `data` *instance using NetworkX.* For quick conversion from
    NetworkX to PyG, ideal for ad hoc processing or when using NetworkX’s functionalities.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将数据转换为* `数据实例` *使用 NetworkX.* 快速将 NetworkX 转换为 PyG，非常适合临时处理或使用 NetworkX 的功能。'
- en: '*Convert into a* `data` *instance using input files.* Offers control over the
    data import process, which is ideal for raw data and custom preprocessing requirements.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用输入文件* 将数据转换为 `数据实例`。这提供了对数据导入过程的控制，非常适合原始数据和定制预处理需求。'
- en: '*Convert to* `dataset` *instance.* For systematic, scalable, and reproducible
    data preprocessing and management, especially for complex or reusable datasets.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换为* `数据集` *实例*。对于系统化、可扩展和可重复的数据预处理和管理，特别是对于复杂或可重复使用的数据集。'
- en: '*Convert* `data` *objects for use in* `dataloader` *without the* `dataset`
    *class.* For scenarios where simplicity and speed are prioritized over systematic
    data management and preprocessing, or for on-the-fly and synthetic data.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将数据对象转换为* `dataloader` *使用，不使用* `dataset` *类*。对于优先考虑简单性和速度，而不是系统化数据管理和预处理的场景，或者对于即时和合成数据。'
- en: First, we’ll import the needed modules from PyG in the following listing.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入 PyG 中所需的模块，如下所示。
- en: Listing 8.4 Required imports, covering data object creation
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.4 必要的导入，包括数据对象创建
- en: '[PRE9]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Case A: Create PyG data object using the NetworkX object'
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 情况 A：使用 NetworkX 对象创建 PyG 数据对象
- en: 'In the previous sections, we’ve explored a graph expressed as a NetworkX `graph`
    object. PyG’s `util` module has a method that can directly create a PyG `data`
    object from a NetworkX `graph` object:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了以 NetworkX `graph` 对象表示的图。PyG 的 `util` 模块有一个方法可以直接从 NetworkX `graph`
    对象创建 PyG `data` 对象：
- en: '[PRE10]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `from_networkx` method preserves nodes, edges, and their attributes, but
    it should be checked to ensure the translation from one module to another went
    smoothly.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`from_networkx` 方法保留了节点、边及其属性，但应检查以确保从一个模块到另一个模块的转换顺利进行。'
- en: 'Case B: Create PyG data object using raw files'
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 情况 B：使用原始文件创建 PyG 数据对象
- en: For greater control over data import into PyG, we can start with raw files or
    files from any stage of the ETL process. In our social graph case, we can begin
    with the edge list file created earlier.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地控制数据导入 PyG，我们可以从原始文件或 ETL 流程的任何阶段的文件开始。在我们的社交图案例中，我们可以从之前创建的边列表文件开始。
- en: Now, let’s review an example where we use code to process and convert our social
    graph from an edge list text file into a format suitable for training a GNN model
    in PyG. We prepare node features, labels, edges, and training/testing sets for
    use in the PyG environment.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一个示例，其中我们使用代码将我们的社交图从边列表文本文件处理并转换为适合在 PyG 中训练 GNN 模型的格式。我们为 PyG 环境准备节点特征、标签、边和训练/测试集。
- en: 'Part 1: Import and prepare graph data'
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一部分：导入和准备图数据
- en: 'This part includes reading an edge list from a file to create a NetworkX graph,
    extracting the list of nodes, creating mappings from node names to indices, and
    vice versa:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分包括从文件中读取边列表以创建一个 NetworkX 图，提取节点列表，创建从节点名称到索引的映射，反之亦然：
- en: '[PRE11]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 An edge list is read from a text file and used to create a NetworkX graph.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从文本文件中读取边列表并用于创建一个 NetworkX 图。'
- en: '#2 All unique nodes in the graph are then extracted and listed.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 然后从图中提取并列出所有唯一的节点。'
- en: '#3 Indices for each node are also generated.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为每个节点生成索引。'
- en: '#4 Two dictionaries are created to allow easy conversion between node names
    and their respective indices, facilitating the handling and manipulation of graph
    data.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 创建了两个字典，以便于节点名称与其相应索引之间的转换，从而便于处理和操作图数据。'
- en: 'Part 2: Process edges and node features'
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二部分：处理边和节点特征
- en: 'This part focuses on converting the edges and node attributes into a format
    that can be easily used with PyTorch for machine learning tasks:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分专注于将边和节点属性转换为可以轻松用于 PyTorch 机器学习任务的格式：
- en: '[PRE12]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 A NetworkX edge list object is created.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建了一个 NetworkX 边列表对象。'
- en: '#2 It’s then transformed into two separate lists representing the source and
    destination nodes of each edge.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 然后将其转换为两个单独的列表，分别表示每条边的源节点和目标节点。'
- en: '#3 These lists are then indexed using the previously created node-to-index
    mapping.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这些列表随后使用先前创建的节点到索引映射进行索引。'
- en: '#4 The node features and labels are prepared using PyTorch tensor objects,
    assuming a simple scenario where all nodes have the same single feature.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用 PyTorch 张量对象准备节点特征和标签，假设所有节点具有相同的单个特征。'
- en: '#5 The node features and labels are prepared using PyTorch tensor objects,
    assuming a simple scenario where all nodes have the same single feature.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用 PyTorch 张量对象准备节点特征和标签，假设所有节点具有相同的单个特征。'
- en: 'Part 3: Prepare data for training and testing'
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三部分：准备训练和测试数据
- en: 'In this part, the dataset is prepared for training and testing by creating
    masks for data splitting and combining all the processed data into a single PyTorch
    data object:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，通过创建数据拆分的掩码并将所有处理过的数据组合成一个单一的 PyTorch 数据对象，为训练和测试准备数据集：
- en: '[PRE13]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 The edge indices created in part 2 are converted into a PyTorch tensor.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在第二部分中创建的边索引被转换为 PyTorch 张量。'
- en: '#2 Masks for training and testing datasets are created by splitting the nodes
    into two separate groups, ensuring that specific portions of the data are used
    for training and testing.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 通过将节点分成两个独立的组来创建训练和测试数据集的掩码，确保数据的具体部分用于训练和测试。'
- en: '#3 All the processed components, including node features, labels, edge indices,
    and data masks, are then combined into a single PyTorch Data object, preparing
    the data for subsequent machine learning tasks.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将所有处理过的组件，包括节点特征、标签、边索引和数据掩码，然后组合成一个单一的 PyTorch 数据对象，为后续的机器学习任务准备数据。'
- en: We’ve created a `data` object from an `edgelist` file. Such an object can be
    inspected with PyG commands, though the set of commands is limited compared to
    a graph processing library. Such a `data` object can also be further prepared
    so that it can be accessed by a `dataloader`, which we’ll cover next.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已从 `edgelist` 文件创建了一个 `data` 对象。这样的对象可以使用 PyG 命令进行检查，尽管与图处理库相比，命令集有限。这样的 `data`
    对象还可以进一步准备，以便可以通过 `dataloader` 访问，我们将在下一部分介绍。
- en: 'Case C: Create PyG dataset object using custom class and input files'
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 情况C：使用自定义类和输入文件创建PyG数据集对象
- en: If the previous listing is suitable for our purposes, and we want to use it
    repeatedly, a preferable option is to create a permanent class that we can include
    for our pipeline. This is what the `dataset` class does.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的列表适合我们的用途，并且我们希望重复使用它，一个更好的选择是创建一个永久的类，我们可以将其包含在我们的管道中。这就是`dataset`类的作用。
- en: Let’s next create a `dataset` object, shown in listing 8.5\. In this example,
    we name our `dataset` `MyOwnDataset` and have it inherit from `InMemoryDataset`
    because our social graph is small enough to sit in memory. As discussed earlier,
    for larger graphs, data can be accessed from disk by having the `dataset` object
    inherit from `Dataset` instead of `InMemoryDataset`.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个`dataset`对象，如列表8.5所示。在这个例子中，我们命名我们的`dataset`为`MyOwnDataset`，并让它继承自`InMemoryDataset`，因为我们的社交图足够小，可以放在内存中。如前所述，对于更大的图，可以通过让`dataset`对象继承自`Dataset`而不是`InMemoryDataset`从磁盘访问数据。
- en: This first part of the code initiates the custom `dataset` class, inheriting
    properties from the `InMemoryDataset` class. The constructor initializes the dataset,
    loads processed data, and defines the properties for raw and processed filenames.
    The raw files are kept empty as this example doesn’t require them, and the processed
    data is fetched from a specified path.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的这一部分初始化自定义的`dataset`类，继承自`InMemoryDataset`类。构造函数初始化数据集，加载处理后的数据，并定义原始和处理的文件名属性。由于这个例子不需要它们，所以原始文件被保留为空，处理后的数据从指定的路径获取。
- en: Listing 8.5 Class to create a dataset object (part 1)
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5 创建数据集对象类（第一部分）
- en: '[PRE14]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Initializes the dataset class. This class inherits from the InMemoryDataset
    class. This init method creates data and slices objects to be updated in the process
    method.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化数据集类。这个类继承自InMemoryDataset类。这个init方法创建数据和切片对象，在处理方法中更新。'
- en: '#2 An optional method that specifies the location of the raw files required
    for processing. For our more rudimentary example, we don’t make use of this but
    have included it for completeness. In later chapters, we’ll make use of this as
    our dataset becomes a bit more complex.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 一个可选的方法，用于指定处理所需的原始文件的位置。在我们的更基础的例子中，我们没有使用这个方法，但为了完整性，我们包括了它。在后面的章节中，我们将使用这个方法，因为我们的数据集变得稍微复杂一些。'
- en: '#3 This method saves our generated dataset to disk.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这个方法将我们生成的数据集保存到磁盘。'
- en: This segment of the code is for data downloading and processing. It reads an
    edge list from a text file and converts it into a NetworkX graph. The nodes and
    edges of the graph are then indexed and converted into tensors suitable for machine
    learning tasks. The method downloaded is kept as a placeholder in case there’s
    a need to download raw data in the future.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码用于数据下载和处理。它从一个文本文件中读取边列表并将其转换为NetworkX图。然后，图的节点和边被索引并转换为适合机器学习任务的张量。下载的方法被保留作为占位符，以防将来需要下载原始数据。
- en: Listing 8.6 Class to create a dataset object (part 2)
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6 创建数据集对象类（第二部分）
- en: '[PRE15]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Allows raw data to be downloaded to a local disk.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 允许将原始数据下载到本地磁盘。'
- en: '#2 The process method contains the preprocessing steps to create our data object,
    and then makes additional steps to partition our data for loading.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 处理方法包含创建我们的数据对象的预处理步骤，然后进行额外的步骤来分区我们的数据以便加载。'
- en: This final part of the code is focused on preparing and saving the data for
    machine learning models. It creates feature and label tensors, prepares the edge
    index, and generates training and testing masks to split the dataset. The data
    is then collated and saved in the processed path for easy retrieval during model
    training.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的这一部分专注于准备和保存数据以供机器学习模型使用。它创建了特征和标签张量，准备了边索引，并生成训练和测试掩码以分割数据集。然后，数据被整理并保存在处理路径中，以便在模型训练期间方便检索。
- en: Listing 8.7 Class to create a dataset object (part 3)
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7 创建数据集对象类（第三部分）
- en: '[PRE16]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 In this first simple use of a dataset class, we use a small dataset. In
    practice, we’ll process much larger datasets and wouldn’t do this all at once.
    We’d create examples of our data, then append them to a list. For our purposes
    (training on this data), pulling from a list object would be slow, so we take
    this iterable, and use collate to combine the data examples into one data object.
    The collate method also creates a dictionary named slices that is used to pull
    single samples from this data object.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在这个使用数据集类的第一个简单例子中，我们使用了一个小数据集。在实践中，我们会处理更大的数据集，并且不会一次性完成。我们会创建数据示例，然后将它们追加到一个列表中。对于我们的目的（在这个数据集上进行训练），从列表对象中提取数据会很慢，所以我们取这个可迭代对象，并使用collate将数据示例组合成一个数据对象。collate方法还会创建一个名为slices的字典，用于从这个数据对象中提取单个样本。'
- en: '#2 Saves our preprocessed data to disk'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将我们的预处理数据保存到磁盘'
- en: 'Case D: Create PyG data objects for use in dataloader without use of a dataset
    object'
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 情况D：创建用于dataloader的PyG数据对象，而不使用dataset对象
- en: Lastly, we explain how to bypass `dataset` object creation and have the `dataloader`
    work directly with your `data` object, as illustrated in figure 8.15\. In the
    PyG documentation, there is a section that outlines how to do this.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们解释了如何绕过`dataset`对象创建，并让`dataloader`直接与你的`data`对象一起工作，如图8.15所示。在PyG文档中，有一个部分概述了如何做到这一点。
- en: 'Just as in regular PyTorch, you don’t have to use datasets, for example, when
    you want to create synthetic data on the fly without saving them explicitly to
    disk. In this case, simply pass a regular Python list holding `torch_geometric.data.Data`
    objects and pass them to `torch_geometric.data.DataLoader`:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在常规PyTorch中一样，当你想要即时创建合成数据而不将其明确保存到磁盘时，你不必使用数据集。在这种情况下，只需传递一个包含`torch_geometric.data.Data`对象的常规Python列表，并将它们传递给`torch_geometric.data.DataLoader`：
- en: '[PRE17]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this chapter, we’ve covered the steps that go from project outline, through
    to converting raw data into a format ready for GNNs. As we conclude this section,
    it’s worth noting that every dataset is different. The procedures outlined in
    this discussion provide a structural framework that serves as a starting point,
    not a one-size-fits-all solution. In the final section, we turn to the subject
    of sourcing data to support data projects.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了从项目概述到将原始数据转换为GNN准备格式的步骤。在结束本节时，值得注意的是，每个数据集都是不同的。本讨论中概述的程序提供了一个结构框架，作为起点，而不是一个通用的解决方案。在最后一节中，我们将转向数据来源的主题，以支持数据项目。
- en: 8.4 Where to find graph data
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 哪里可以找到图数据
- en: To not start from scratch in developing a graph data model and schema for your
    problem, there are several sources of published models and schemas. They include
    industry standard data models, published datasets, published semantic models (including
    knowledge graphs), and academic papers. A set of example sources is provided in
    table 8.2\.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在开发针对你的问题的图数据模型和模式时不必从头开始，有几种已发布的模型和模式来源。它们包括行业标准数据模型、已发布的数据集、已发布的语义模型（包括知识图谱）和学术论文。表8.2提供了一个示例来源集。
- en: Sourcing graph data
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 获取图数据
- en: Details of different sources for graph-based data that can be used for GNN projects.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 详细介绍了可用于GNN项目的基于图的数据的不同来源。
- en: '*From nongraph data*—In this chapter, we assumed that the data lies in nongraph
    sources and must be transformed into a graph format using ETL and preprocessing.
    Having a schema can help guide such a transformation and keep it ready for further
    analysis.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从非图数据*—在本章中，我们假设数据位于非图源，并且必须使用ETL和预处理将其转换为图格式。拥有模式可以帮助指导这种转换，并使其为后续分析做好准备。'
- en: '*Existing graph datasets—*The number of freely available graph datasets is
    growing. Two GNN libraries we use in this book, Deep Graph Library (DGL) and PyG,
    come with a number of benchmark datasets installed. Many such datasets are from
    influential academic papers. However, such datasets are small scale, which limits
    reproducibility of results, and whose performance don’t necessarily scale for
    large datasets.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*现有的图数据集—*可自由获取的图数据集的数量正在增长。本书中我们使用的两个GNN库，Deep Graph Library (DGL) 和 PyG，都附带了一些预装的基准数据集。许多这样的数据集来自有影响力的学术论文。然而，这些数据集规模较小，这限制了结果的复现性，并且其性能不一定适用于大型数据集。'
- en: A source of data that seeks to mitigate the problems of earlier benchmark datasets
    in this space is Open Graph Benchmark (OGB). This initiative provides access to
    a variety of real-world datasets, of varying scales. OGB also publishes performance
    benchmarks by learning task. Table 8.2 lists a few repositories of graph datasets.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open Graph Benchmark（OGB）是寻求减轻该领域早期基准数据集问题的数据来源。这一倡议提供了各种不同规模的真实世界数据集。OGB还通过学习任务发布性能基准。表8.2列出了几个图数据集存储库。
- en: '*From generation*—Many graph processing frameworks and graph databases allow
    the generation of random graphs using a number of algorithms. Though random, depending
    on the generating algorithm, the resulting graph will have characteristics that
    are predictable.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从生成*——许多图处理框架和图数据库允许使用多种算法生成随机图。虽然随机，但根据生成算法的不同，生成的图将具有可预测的特征。'
- en: Table 8.2 Graph datasets and semantic models
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.2 图数据集和语义模型
- en: '| Source | Type | Problem Domains | URL |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 来源 | 类型 | 问题领域 | URL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Open Graph Benchmark (OGB)  | Graph datasets and benchmarks  | Social networks,
    drug discovery  | [https://ogb.stanford.edu/](https://ogb.stanford.edu/)  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 开放图基准（OGB） | 图数据集和基准 | 社交网络、药物发现 | [https://ogb.stanford.edu/](https://ogb.stanford.edu/)
    |'
- en: '| GraphChallenge Datasets  | Graph datasets  | Network science, biology  |
    [https://graphchallenge.mit.edu/data-sets](https://graphchallenge.mit.edu/data-sets)  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| GraphChallenge数据集 | 图数据集 | 网络科学、生物学 | [https://graphchallenge.mit.edu/data-sets](https://graphchallenge.mit.edu/data-sets)
    |'
- en: '| Network Repository  | Graph datasets  | Network science, bioinformatics,
    machine learning, data mining, physics, and social science  | [http://networkrepository.com/](http://networkrepository.com/)  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 网络存储库 | 图数据集 | 网络科学、生物信息学、机器学习、数据挖掘、物理学和社会科学 | [http://networkrepository.com/](http://networkrepository.com/)
    |'
- en: '| SNAP Datasets  | Graph datasets  | Social networks, network science, road
    networks, commercial networks, finance  | [http://snap.stanford.edu/data/](http://snap.stanford.edu/data/)  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| SNAP数据集 | 图数据集 | 社交网络、网络科学、道路网络、商业网络、金融 | [http://snap.stanford.edu/data/](http://snap.stanford.edu/data/)
    |'
- en: '| Schema.org  | Semantic data model  | Internet web pages  | [https://schema.org/](https://schema.org/)  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Schema.org | 语义数据模型 | 互联网网页 | [https://schema.org/](https://schema.org/)
    |'
- en: '| Wikidata  | Semantic data model  | Wikipedia pages  | [www.wikidata.org/](http://www.wikidata.org/)  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 维基数据 | 语义数据模型 | 维基百科页面 | [www.wikidata.org/](http://www.wikidata.org/) |'
- en: '| Financial Industry Business Ontology  | Semantic data model  | Finance  |
    [https://github.com/edmcouncil/fibo](https://github.com/edmcouncil/fibo)  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 金融行业业务本体 | 语义数据模型 | 金融 | [https://github.com/edmcouncil/fibo](https://github.com/edmcouncil/fibo)
    |'
- en: '| Bioportal  | List of medical semantic models  | Medical  | [https://bioportal.bioontology.org/ontologies/](https://bioportal.bioontology.org/ontologies/)  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 生物门户 | 医学语义模型列表 | 医学 | [https://bioportal.bioontology.org/ontologies/](https://bioportal.bioontology.org/ontologies/)
    |'
- en: Public graph datasets also exist in several places. Published datasets have
    accessible data, with summary statistics. Often, however, they lack explicit schemas,
    conceptual or otherwise. To derive the dataset’s entities, relations, rules, and
    constraints, querying the data becomes necessary.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 公共图数据集也存在于几个地方。已发布的数据集具有可访问的数据和总结统计信息。然而，通常它们缺乏明确的模式，无论是概念上的还是其他方面的。为了推导数据集的实体、关系、规则和约束，查询数据变得必要。
- en: For semantic models based on property, RDF, and other data models, there are
    some general datasets, and others are targeted to particular industries and verticals.
    Such references seldom use graph-centric terms (e.g., *node*, *vertex*, and *edge*)
    but will use terms related to semantics and ontologies (e.g., *entity*, *relationship*,
    *links*). Unlike the graph datasets, the semantic models offer data frameworks,
    not the data itself.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于属性、RDF和其他数据模型的语义模型，有一些通用数据集，还有一些针对特定行业和垂直领域。这样的参考很少使用以图为中心的术语（例如，*节点*、*顶点*和*边*），但会使用与语义和本体相关的术语（例如，*实体*、*关系*、*链接*）。与图数据集不同，语义模型提供数据框架，而不是数据本身。
- en: Reference papers and published schemas can provide ideas and templates that
    can help in developing your schema. There are a few use cases targeted toward
    industry verticals that both represent a situation using graphs and use graph
    algorithms, including GNNs, to solve a relevant problem. Transaction fraud in
    financial institutions, molecular fingerprinting in chemical engineering, and
    page rank in social networks are a few examples. Perusing such existing work can
    provide a boost to development efforts. On the other hand, often such published
    work is done for academic, not industry goals. A network that is developed to
    prove an academic point or make empirical observations may not have qualities
    amenable to an enterprise system that must be maintained and be used on dirty
    and dynamic data.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 参考论文和已发布的模式可以提供想法和模板，有助于开发您的模式。有几个针对行业垂直领域的用例，这些用例既使用图来表示情况，又使用图算法（包括GNN）来解决相关问题。金融机构的交易欺诈、化学工程中的分子指纹识别和社交网络中的页面排名是一些例子。查阅这些现有工作可以推动开发工作。另一方面，这种已发布的工作往往是出于学术目的，而不是行业目标。为了证明学术观点或进行经验观察而开发的网络可能不具备适合维护和使用在脏数据和动态数据上的企业系统所需的特性。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Planning for a graph learning project involves more steps than in traditional
    machine learning projects. The objectives and requirements will influence the
    design of the system, data models, and GNN architecture. The project includes
    creating robust graph data models, understanding and transforming raw data, and
    ensuring that the models effectively represent the complex relationships within
    the recruitment landscape.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划一个图学习项目比传统机器学习项目涉及更多步骤。目标和需求将影响系统的设计、数据模型和GNN架构。项目包括创建健壮的图数据模型、理解和转换原始数据，并确保模型能够有效地表示招聘领域的复杂关系。
- en: One important step is creating the data model and schema for your data. These
    processes are essential to avoid technical debt. This involves designing the elements,
    relationships, and constraints; running queries; analyzing results; making adjustments;
    and validating against criteria to ensure the model’s readiness for complex queries
    and machine learning applications. A graph data model will be refined through
    iterative testing and refactoring to ensure it effectively supports the analysis
    of complex relationships within the recruitment data.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个重要的步骤是为您的数据创建数据模型和模式。这些过程对于避免技术债务至关重要。这包括设计元素、关系和约束；运行查询；分析结果；进行调整；并验证是否符合标准，以确保模型能够为复杂查询和机器学习应用做好准备。图数据模型将通过迭代测试和重构来细化，以确保它能够有效地支持招聘数据中复杂关系的分析。
- en: There are many encoding and serialization options for keeping data in memory
    or in raw files, including language and system-agnostic formats such as JSON,
    CSV, and XML. Language-specific formats, such as Python’s Pickle, and system-driven
    formats from specific software and libraries such as SNAP, NetworkX, and Gephi,
    are also mentioned. For big data, Avro and matrix-based formats (sparse column
    matrix, sparse row matrix, and matrix market format) are highlighted as efficient
    options for handling large datasets.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储数据在内存或原始文件中有很多编码和序列化选项，包括JSON、CSV和XML等语言和系统无关的格式。还提到了特定语言的格式，如Python的Pickle，以及来自特定软件和库的系统驱动格式，例如SNAP、NetworkX和Gephi。对于大数据，Avro和基于矩阵的格式（稀疏列矩阵、稀疏行矩阵和矩阵市场格式）被突出显示为处理大型数据集的高效选项。
- en: A data pipeline can start with raw data that undergoes exploratory analysis
    and preprocessing to be usable by GNN libraries such as PyG. The raw data is transformed
    into standard formats such as edge lists or adjacency matrices, ensuring consistency
    and usability for different problems.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道可以从经过探索性分析和预处理以供GNN库如PyG使用的原始数据开始。原始数据被转换为标准格式，如边列表或邻接矩阵，确保了不同问题的一致性和可用性。
- en: Graph processing frameworks such as NetworkX are used for light exploratory
    data analysis (EDA) and visualization. Graph objects, such as adjacency and edge
    lists, are loaded into NetworkX. The visual representation and statistical analysis,
    such as the number of nodes, edges, and connected components, are derived to understand
    the graph’s structure and complexity.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络X等图处理框架用于轻量级的探索性数据分析（EDA）和可视化。将邻接和边列表等图对象加载到NetworkX中。通过统计分析，如节点数、边数和连通分量，得出视觉表示和结构复杂性，以理解图的结构和复杂性。
- en: The PyG library is used for preprocessing, involving the conversion of data
    into formats that can be easily manipulated and trained with. Data objects are
    created with multiple attributes at various levels, enabling GPU processing and
    facilitating the splitting of training, testing, and validation data. The choice
    between using dataset objects or bypassing them depends on the need for saving
    data and the complexity of the dataset.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyG 库用于预处理，涉及将数据转换为易于操作和训练的格式。数据对象以多个属性在各个级别上创建，使 GPU 处理成为可能，并便于将训练、测试和验证数据分开。是否使用数据集对象或绕过它们取决于保存数据的需求和数据集的复杂性。
- en: There are numerous repositories of ready-to-use graph datasets and semantic
    models covering various domains, such as social networks and drug discovery. However,
    while these datasets are useful for learning and benchmarking, they are often
    small-scale and may not be directly applicable for large, real-world problems.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在着许多包含各种领域（如社交网络和药物发现）的现成图数据集和语义模型的存储库。然而，尽管这些数据集对于学习和基准测试很有用，但它们通常规模较小，可能不适用于大型、现实世界的问题。
- en: While public graph datasets and semantic models provide a starting point, they
    often lack explicit schemas requiring additional work to derive entities, relations,
    and constraints. Additionally, while academic papers offer templates for developing
    schemas, they are typically designed for academic purposes and may not be directly
    transferable to real-world, industry-specific applications with dynamic and dirty
    data.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然公共图数据集和语义模型提供了一个起点，但它们通常缺乏明确的模式，需要额外的工作来推导实体、关系和约束。此外，虽然学术论文提供了开发模式的模板，但它们通常是为学术目的设计的，可能无法直接应用于具有动态和脏数据的现实世界、行业特定应用。
