- en: 8 Considerations for GNN projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating a graph data model from nongraph data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract, transform, load and preprocessing from raw data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating datasets and data loaders with PyTorch Geometric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we describe the practical aspects of working with graph data,
    as well as how to convert nongraph data into a graph format. We’ll explain some
    of the considerations involved in taking data from a raw state to a preprocessed
    format. This includes turning tabular or other nongraph data into graphs and preprocessing
    them for a graph-based machine learning package. In our mental model, shown in
    figure 8.1, we are in the left half of the figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 Mental model for graph training process. We’re at the start of the
    process, where we prepare our data for training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll proceed as follows. In section 8.1, we introduce an example problem that
    might require a graph neural network (GNN) and how to proceed with tackling this
    project. Section 8.2 goes into more detail on how to use nongraph data in graph
    models. We then put these ideas into action in section 8.3 by taking a dataset
    from a raw file to preprocessed data, ready for training. Finally, ideas for finding
    more graph datasets are given in section 8.4.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll consider how to apply GNNs to a social graph created
    by a recruiting firm. In our example, nodes are job candidates, and edges represent
    relationships between job candidates. We generate graph data from raw data, in
    the form of edge lists and adjacency lists. We then use that data in a graph processing
    framework (`NetworkX`) and a GNN library (PyTorch Geometric [PyG]). The nodes
    in this data include the candidate’s *ID*, *job type* (accountant, engineer, etc.),
    and *industry* (banking, retail, tech, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: We frame the goals of candidates as a graph-based challenge, detailing the steps
    to transform their data for graph learning. Our aim here is to map out the data
    workflow, starting with raw data, converting it into a graph format, and then
    preparing it for the GNN training we use in the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/Xxn1](https://mng.bz/Xxn1)). Colab links and data from this chapter
    can be accessed in the same locations.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Data preparation and project planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the case of a hypothetical recruiting firm called Whole Staffing. Whole
    Staffing headhunts employees for a variety of industries and maintains a database
    of their candidate profiles, including their history of engagement with the firm
    and other candidates. Some candidates get introduced to the firm via referrals
    from other candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Project definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Whole Staffing wants to get the most value from its database. They have a few
    initial questions about their collection of job candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: Some profiles have missing data. Is it possible to fill in missing data without
    bothering the candidate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: History has shown that candidates who have worked on similar projects in the
    past can work well together in future work. Is it possible to figure out which
    candidates could work well together?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whole Staffing has tasked you with exploring the data to answer these questions.
    Among other analytical and machine learning methods, you think there may be an
    opportunity to represent the data as a graph and use a GNN to answer the client’s
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: Your idea is to take the collection of referrals and convert it into a social
    network where the job candidates are nodes and the referrals between candidates
    are edges. To simplify things, you can ignore the direction of the referrals so
    that the graph can be undirected. You also ignore repeat referrals, so that relationships
    between candidates remain unweighted.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll walk through the steps needed to prepare the data and establish a pipeline
    to pass the data to a GNN model. First, let’s consider the project planning stage.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Project objectives and scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given any problem, having clear objectives, requirements, and scope will serve
    as a compass that steers all subsequent actions and decisions. Every facet, from
    planning and schema creation to tool selection should follow the core objectives
    and scope. Let’s consider each of these for our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Project objectives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whole Staffing wants to optimize the use of its candidate database. First, the
    project should enhance data quality by filling in missing information in candidate
    profiles, reducing the need for direct candidate engagement. Second, the work
    ahead should facilitate informed candidate suggestions, predicting which teams
    will work well using the historical success of candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Project requirements and scope
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several key requirements will directly affect your project. Let’s run through
    a few and point out their importance to our client’s industry. Then, we’ll draw
    some conclusions about the project at hand. Requirements include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data size and velocity* —What is the size of the data, in terms of item counts,
    size in bytes, or number of nodes? How fast is new information added to the data,
    if at all? Is data expected to be uploaded from a real-time stream, or from a
    data lake that is updated daily?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The planned graph might grow with the increase in data, affecting the computational
    resources needed and the efficiency of algorithms. Accurately assessing data size
    and velocity ensures that the system can handle the expected load, can offer real-time
    insights, and is scalable for future growth.
  prefs: []
  type: TYPE_NORMAL
- en: '*Inference speed* —How fast are the application and the underlying machine
    learning models required to be? Some applications may require sub-second responses,
    while for others, there is no constraint on time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response time is particularly vital in providing timely recommendations and
    insights. For a recruitment firm, matching candidates with suitable job openings
    is time-sensitive, with opportunities quickly becoming unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data privacy* —What are the policies and regulations regarding personally
    identifiable information (PII), and how would this involve data transformation
    and preprocessing?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data privacy becomes a huge concern when dealing with sensitive information
    such as candidate profiles, contact details, and employment histories. In a graph
    and GNN setting, ensuring that nodes and edges don’t reveal PII is essential.
    Compliance with regulations such as General Data Protection Regulation (GDPR)
    or the California Consumer Privacy Act (CCPA) is mandatory to avoid legal complications.
    The graph data should be handled, stored, and processed in a way that respects
    privacy norms. Anonymization and encryption techniques may be needed to protect
    individuals’ privacy while still allowing for effective data analysis. Understanding
    these requirements early in the project planning ensures that the system architecture
    and data processing pipelines are designed with privacy preservation in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '*Explainability* —How explainable should the responses be? Will direct answers
    be enough, or should there be additional data that sheds light on why a recommendation
    or prediction was made?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the recruitment sector, explainability and transparency are pivotal. They
    instill trust among candidates and employers by ensuring fairness and clarity
    in the talent-selection process. Ethical standards are upheld, and unintended
    biases should be mitigated. These elements aren’t just ethical imperatives but
    often legally binding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the objectives and scope, for Whole Staffing, the deliverables might
    be a system that does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fortnightly scan the candidate data for missing items. Missing items can be
    inferred and suggested or filled in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict candidates that will work well together by using link prediction and/or
    node classification. Unlike the first deliverable, the response time here should
    be fast.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following lists some of the specifications for the preceding requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data size* —This is conservatively set at enough capacity for 100,000 candidates
    and their properties, which is estimated to be 1 GB of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inference speed* —Application will run biweekly and can be completed overnight
    so we don’t have a considerable speed constraint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data privacy* —No personal data that directly identifies a candidate can be
    used. However, data known to the recruitment company, such as whether employees
    have been successfully placed at the same employer, can be used to improve operations
    of the company, provided this data isn’t shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Explainability* —There must be some level of explainability for the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objectives and requirements will guide the decisions regarding system design,
    data models, and, often, GNN architecture. The preceding gives an example for
    the type of considerations needed when beginning or scoping a graph-based project.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Designing graph models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given an appropriate scope of work, the next step is in building the graph models.
    For most machine learning problems, data will be organized in a standard way.
    For example, when dealing with tabular data, rows are treated as observations,
    and columns are treated as features. We can join tables of such data by using
    indexes and keys. This framework is flexible and relatively unambiguous. We may
    quibble about which observations and features to include, but we know where to
    place them.
  prefs: []
  type: TYPE_NORMAL
- en: When we want to express our data with graphs, in all but the simplest scenarios,
    we’ll have several options for what structure to use. With graphs, it’s not always
    intuitive where to place the entities of interest. It’s this ambiguity that drives
    the need for systemic methods in using graph data, but getting it right early
    on can serve as a foundation for downstream machine learning tasks [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we embark on a journey of transforming Whole Staffing’s recruitment
    data into graph-based data to support our downstream pipeline. We start by considering
    the domain and use case, a critical step to understanding the data. Next, we create
    and refine a schema, pivotal for organizing and interpreting complex datasets.
    Through rigorous testing of the schema, we could then ensure its robustness and
    reliability. Any necessary refinements should be made to optimize performance
    and accuracy. This approach ensures that our future analytic systems, which ingest
    graph-based data, can answer complex queries about job candidates with precision
    and reliability. Here’s the process to follow, and figure 8.2 provides a visual:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the data and the use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a data model, schema, and instance model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test your model using the schema and instance model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refactor if necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/8-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Process of creating a robust graph data model from nongraph data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.2.1 Get familiar with the domain and use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with most data projects, to be effective, we have to come to grips with the
    dataset and the context. For our immediate goal of creating a model, understanding
    our referral data in its raw format and digging into the intricacies of the recruiting
    industry can provide critical insights. This knowledge also gives us a basis to
    design tests for the model during deployment. For example, preliminary analysis
    on the raw data gives us the information in table 8.1\.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 Features of the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Number of candidates  | 1,933  |'
  prefs: []
  type: TYPE_TB
- en: '| Number of referrals  | 12,239  |'
  prefs: []
  type: TYPE_TB
- en: From the raw data, it’s apparent that there are many relationships, offering
    potential insights into candidate referrals. The large number of referrals in
    comparison to the number of candidates suggests an interconnected network. Our
    models need to be sufficiently large to translate this structure into results
    within the recruitment problem-space.
  prefs: []
  type: TYPE_NORMAL
- en: Turning to domain knowledge, beyond the immediate asks of the client, we should
    be asking questions that solidify our understanding of the industry. In setting
    the requirements for our data model, we should consider the key questions and
    challenges to the industry. For the recruitment problem, we might ask how we can
    optimize the referral process or what underlying structures and patterns govern
    candidate referrals. By addressing these types of questions, we can align our
    model with domain expertise, with a likely boost in both its relevance and validity.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Constructing the graph dataset and schemas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we’ll discuss how to design our database. The term *graph dataset* denotes
    a general effort to describe data using the elements and structure of a graph:
    nodes, edges, and node features and edge features. To achieve this, we need a
    *schema* and an *instance*. These specify the structure and rules of our graph
    explicitly and allow our graph dataset to be tested and refined. This section
    is drawn from several references, listed at the end of the book for further reading.'
  prefs: []
  type: TYPE_NORMAL
- en: By addressing the details of our graph dataset up front, we can avoid technical
    debt and more easily test the integrity of our data. We can also experiment more
    systematically with different data structures. In addition, when the structure
    and rules of our graphs are designed explicitly, it increases the ease with which
    we can parameterize these rules and experiment with them in our GNN pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Graph datasets can be simple, consisting of one type of node and one type of
    edge. Or they can be complex, involving many types of nodes and edges, metadata,
    and, in the case of knowledge graphs, ontologies.
  prefs: []
  type: TYPE_NORMAL
- en: Key terms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following are key terms used in this section (for more details on graph
    data models and types of graphs, see appendix A):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bi-graph (or bipartite graph)*—A graph with two sets of nodes. There are no
    edges between nodes of the same set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Entity-relationship diagram (ER diagram)*—A figure that shows the entities,
    relationships, and constraints of a graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph dataset—*A representation of nodes, edges, and their relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Heterogeneous/homogeneous graphs—*A homogeneous graph has only one type of
    node or edge. A heterogeneous graph can have several different types of nodes
    or edges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instance model*—A model based on a schema that holds a subset of the actual
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ontology—*A way of describing the concepts and relationships in a specific
    domain of knowledge, for example, connections between different entities (writers)
    in a semantic web (of works of literature). The ontology is the structured framework
    that defines the roles, attributes, and interrelations of these writers and their
    literary works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Property graph*—A model that uses metadata (labels, identifiers, attributes/
    properties) to define the graph’s elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource Description Framework graph (RDF graph, aka Triple Stores)*—Model
    that follows a subject-predicate-object pattern, where nodes are subjects and
    objects, and edges are predicates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Schema*—A blueprint that defines how the elements of the graph will be organized
    as well as which specific rules and constraints will be used for these elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conceptual schema*—A schema not tied to any particular database or processing
    system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System schema*—A schema designed with a specific graph database or processing
    system in mind.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Technical debt*—The consequences of prioritizing speedy delivery over quality
    code, which later has to be refactored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph datasets are good at providing conceptual descriptions of graphs that
    are quick and easy to grasp by others. For example, for people who understand
    what a property graph or an RDF graph is, telling them that a graph is a bi-graph
    implemented on a property graph can reveal much about the design of your data
    (property graphs and RDF graphs are explained in appendix A).
  prefs: []
  type: TYPE_NORMAL
- en: 'A *schema* is a blueprint that defines how data is organized in a data storage
    system, such as a database. A graph schema is a concrete implementation of a graph
    dataset, explaining in detail how the data in a specific use case is to be represented
    in a real system. Schemas can consist of diagrams and written documentation. Schemas
    can be implemented in a graph database using a query language or in a processing
    system using a programming language. A schema should answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the elements (nodes, edges, properties), and what real-world entities
    and relationships do they represent?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the graph include multiple types of nodes and edges?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the constraints regarding what can be represented as a node?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the constraints for relationships? Do certain nodes have restrictions
    regarding adjacency and incidence? Are there count restrictions for certain relationships?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are descriptors and metadata handled? What are the constraints on this data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the complexity of your data and the systems in use, you may use
    multiple but consistent schemas. A *conceptual schema* lays out the elements,
    rules, and constraints of the graph but isn’t tied to any system. A *system schema*
    reflects the conceptual schema’s rules but just for a specific system, such as
    a database of choice. A system schema could also omit unneeded elements from the
    conceptual schema. Here are the steps to create a schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Identify main entities and relationships*. For instance, in our social network
    example, entities can be candidates, recruiters, referrals, hiring events, and
    relationships.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Define node and edge labels.* These labels serve as identifiers for the types
    of entities and their interrelationships in the graph.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Specify properties and constraints.* Each vertex and edge label is associated
    with specific properties and constraints that store and restrict information,
    respectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Define indices (optional, for database-oriented schemas)*. Indexes, based
    on properties or combinations thereof, enhance query speeds on graph data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Apply the graph schema to a database (optional, for database-oriented schemas)*.
    Commands or codes, contingent on the specific graph database, are employed to
    create the graph schema, with specifications on its static or dynamic nature.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on the complexity of the graph dataset and the use cases, one or several
    schemas could be called for. In the case of more than one schema, compatibility
    between the schemas via a mapping must also be included.
  prefs: []
  type: TYPE_NORMAL
- en: For a dataset with few elements, a simple diagram with notes in prose can be
    sufficient to convey enough information to fellow developers to be able to implement
    in query language or code. For more complex network designs, ER diagrams and associated
    grammar are useful in illustrating network schemas in a visual and human readable
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Entity-relationship diagrams (ER diagrams)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ER diagrams have the elements to illustrate a graph’s nodes, edges, and attributes
    and the rules and constraints governing a graph [2, 3]. The following figure (left)
    shows some connectors notation that can be used to illustrate edges and relationship
    constraints. The figure (right) shows an example of a schema diagram conveying
    two node types that might be represented in our recruitment example (Recruiter
    and Candidate), and two edge types (Knows, and Recruits/Recruited By). The diagram
    conveys implicit and explicit constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '![sidebar figure](../Images/8-unnumb.png)'
  prefs: []
  type: TYPE_IMG
- en: At left is the relationship nomenclature for ER diagrams. At right is an example
    of a conceptual schema using an ER diagram.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some explicit constraints are that one employee can refer many other employees
    and that one referee can be referred by many employees. Another explicit constraint
    is that a person can only be employed full-time by one business, but one business
    might have many employees. An implicit constraint is that, for this graph model,
    there can be no relationship between a business and a referral.
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning to our example, to design conceptual and system schemas for our example
    dataset, we should think about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The entities and relationships in our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible rules and constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational constraints, such as the databases and libraries at our disposal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output we want from our application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our data will consist of candidates and their profile data (e.g., industry,
    job type, company, etc.), as well as recruiters. Properties can also be treated
    as entities; for instance, Medical Industry could be treated as a node. Relations
    could be Candidate Knows Candidate, Candidate Recommended Candidate, or Recruiter
    Recruited Candidate. As stated previously, graph data can be extremely flexible
    in how entities can be represented.
  prefs: []
  type: TYPE_NORMAL
- en: Given these choices, we show a few options for the conceptual schema. Option
    A is shown in figure 8.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Schema with one node type and one edge type
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you can see, example A consists of one node type (Candidate) connected by
    one undirected edge type (Knows). Node attributes are the candidate’s Industry
    and their Job Type. There are no restrictions on the relationships, as any candidate
    can know 0 to *n*-1 other candidates, where *n* is the number of candidates. The
    second conceptual schema is shown in figure 8.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 Schema with two node types and one edge type
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Example B consists of two node types (Candidate and Recruiter), linked by one
    undirected edge type (Knows). Edges between candidates have no restrictions. Edges
    between candidates and recruiters have a constraint: a candidate can only link
    to one recruiter, while a recruiter can link to many candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third schema is shown in figure 8.5\. It has multiple node and relationship
    types. In example C, the types are Candidate, Recruiter, and Industry. Relation
    types include Candidate Knows Candidate, Recruiter Recruits Candidate, Candidate
    Is a Member of Industry. Note, we’ve made Industry a separate entity, rather than
    an attribute of a candidate. These types of graphs are known as *heterogeneous,*
    as they contain many different types of nodes and edges. In a way, we can imagine
    these as multiple graphs that are layered on top of each other. When we have only
    one type of nodes and edges, then graphs are known as *homogenous.* Some of the
    constraints for example C include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Candidates can only have one Recruiter and one Industry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recruiters don’t link to Industries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/8-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 Schema with three node types and three edge types
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Depending on the queries and the objectives of the machine learning model, we
    could pick one schema or experiment with all three in the course of developing
    our application. Let’s stick with the first schema, which can serve as a simple
    structure for our exploration and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Creating instance models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An *instance model* contrasts the abstract nature of the graph dataset by providing
    a tangible, specific example of the data, according to the schema. Such an example
    serves to validate and test the schema. Following are the steps to create an instance
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Identify the schema*. Begin by identifying the general model or schema that
    your instance will be based upon. Ensure that the class definition, attributes,
    and methods are well established.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Select a subset of the data*. Choose a specific subset of data to represent,
    adhering to the established graph schema.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Create nodes*. Develop nodes for each entity within your data subset, ensuring
    each has a label, unique identifier, and associated properties.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Create edges*. Develop links for each relationship, assigning labels and properties
    and specifying edge directions and multiplicities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Adhere to the rules and constraints of your schema*. In constructing the instance
    model, make sure to follow the rules and constraints of the schema.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Visualization*. Use visualization tools to represent the instance model graphically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Instantiation*. Realize the instance model using a graph database or graph
    processing system. This will allow for queries that can test and validate it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8.6 shows an example of an instance model derived from the schema discussed
    formerly. The nodes and edges have features filled with the real data of candidates
    instead of placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 Example of an instance model with nodes filled with actual data from
    the recruiter example. Real instance models may have much more data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 8.2.4 Testing and refactoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Technical debt* can occur when we have to change and evolve our data or code,
    but we haven’t yet planned for backward- or forward-compatibility in our models.
    It can also happen when our modeling choices aren’t a good fit for our database
    and software choices, which may call for expensive (in time or money) workarounds
    or replacements.'
  prefs: []
  type: TYPE_NORMAL
- en: Having well-defined rules and constraints on our data and models gives us explicit
    ways to test our pipeline. For example, if we know that our nodes can at most
    have two degrees, we can design simple functions or queries to process and test
    every node against this criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing and refactoring are iterative processes and crucial in scaling an optimized
    graph schema and instance model [4, 5]. It will involve executing queries, analyzing
    results, making necessary adjustments, and validating against metrics. In the
    context of Whole Staffing’s recruitment data, this practice would ensure the model
    is tailored to capture real-world relationships and robust new data streams. Following
    are some examples for tests and refactoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cast your instance model in a system.* Store the model in your graph database
    or processing system of choice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Create tests and run queries*. Based on the specific requirements, draft queries
    to test the integrity of your model. Use query languages such as Cypher or SPARQL
    to execute queries on a graph database. Programming languages, for example, Python,
    can also be used to query graphs within graph processing systems such as NetworkX.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For our example’s simple schema, here are some possible tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Node attributes verification* —Each node should be checked to confirm that
    it possesses the required attributes, specifically the candidate’s industry and
    job type, and that these attributes have non-null values.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edge type verification* —All connections between candidates should be validated
    to confirm that they are of the Knows type, ensuring consistency in relationship
    labeling.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Relationship verification* —Check the average number of relationships that
    exist to ensure it’s consistent with the average number of referrals.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unique IDs* —Every candidate node should be checked for unique identifiers
    to prevent data duplication and ensure data integrity.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attribute data type* —The data types of `industry` and `jobType` attributes
    should be validated to ensure consistency across all candidate nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Network structure* —The structure of the network should be validated to ensure
    it’s undirected, confirming the bidirectional nature of the Knows relationships
    between candidate nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edge cases* —Determine edge cases and query for those. In our case, the nodes
    that are unconnected may present a problem. Using queries to understand the extent
    of unconnected nodes and their effect on the analytics will drive decisions to
    refactor. Another edge case could be an isolated group of candidates whose relationships
    form a cycle. It would be important to ensure the data model and the analytical
    tools could handle such complex or unusual data patterns and still produce valid
    answers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. *Validate and evaluate performance* —Based on the results of the tests,
    determine if there are logical problems with your model and your use case, or
    problems with the data and attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. *Refactor* —Make adjustments to labels, properties, relationships, or constraints
    as needed to minimize errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5\. *Repeat* —Iterate the preceding steps, refining the model based on evaluations
    and ensuring alignment with the project needs and constraints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 6\. *Final assessment* —Evaluate the final model against criteria and best practices
    to ensure its readiness for complex queries and machine learning applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this iterative process of testing and refactoring, we refine the dataset
    for Whole Staffing’s recruitment data and use case. Attention to detail guarantees
    the model is ready to support evaluation of the complex, nuanced relationships
    hidden within the recruitment data.
  prefs: []
  type: TYPE_NORMAL
- en: As we transition into the next section, our focus shifts to the practical implementation
    of some of these concepts. We’ll look at creating data pipelines in PyG, showing
    how to convert data from its initial raw form to a preprocessed state, ready for
    input into other downstream model training and testing routines.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Data pipeline example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the schema decided, let’s walk through an example of a data pipeline. In
    this section, we assume our objective is to create a simple data workflow that
    takes data from a raw state and ends with a preprocessed dataset that can be passed
    to a GNN. These steps are summarized in figure 8.7.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while the overall steps shown can be consistent from one problem to
    another, the details of implementation for each step can be unique to the problem,
    its data, and the chosen data storage, processing, and model training options.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Summary of steps in the data pipeline process in this section
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Key terms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following are key terms used in this section (for more details on graph
    data models and types of graphs, see appendix A):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adjacency list—*A basic representation of graph data. In this format, each
    entry contains a node with a list of its adjacent nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adjacency matrix*—A basic representation of graph data. In a matrix, each
    row and column correspond to a node. The cells, where these rows and columns intersect,
    signify the presence of edges between the nodes. A cell with a nonzero value indicates
    an edge between the nodes, while a zero value signifies no connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Degree*—The degree of a node is the count of its adjacent nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Edge list—*A basic representation of a graph. It’s an array of all the edges
    in a graph; each entry in the array contains a unique pair of connected nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mask—*A Boolean array (or tensor in the case of PyTorch) that is used to select
    specific subsets of data. Masks are commonly used for splitting a dataset into
    different parts, such as training, validation, and testing sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rank*—In our context, rank refers to the position of each node’s degree in
    a sorted list. So, the node with the highest degree has rank 1, the next highest
    has rank 2, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Raw data*—Data in its most unprocessed form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Serialization*—Putting data into a format that is easily stored or exported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Subgraph*—A subgraph is a subset of a larger graph’s nodes and edges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.3.1 Raw data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Raw data* refers to data in its most unprocessed state; such data is the starting
    point for our pipeline. This data can be in various databases, serialized in some
    way, or generated.'
  prefs: []
  type: TYPE_NORMAL
- en: In the development stage of an application, it’s important to know how closely
    the raw data used will match the live data used in production. One way to do this
    is by sampling from data archives.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in section 8.1, there are at least two sources for our example
    problem: relational database tables that contain recommendation logs and candidate
    profiles. To keep our example contained, we assume a helpful engineer has already
    queried the log data and transformed it into a JSON format, where keys are a recommending
    candidate, and the values are the recommended candidates. From our profile data,
    we have two other fields: *industry* and *job type*. For both data sources, our
    engineer has used a hash to protect PII, which we can consider a unique identifier
    for the candidate. In this section, we’ll use the JSON data, where an example
    snippet is shown in figure 8.8\. The data is displayed in two ways: with a hash
    and without a hash.'
  prefs: []
  type: TYPE_NORMAL
- en: Data encoding and serialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One key consideration when constructing the pipeline is the choice of what
    data format to use when importing and exporting data from one system to another.
    For transferring graph data into another system or sending it over the internet,
    *encoding* or *serialization* is typically used. These terms refer to the process
    of putting data in a form that is easily transferable [6, 7]. Before choosing
    an encoding format, you must have decided upon the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data model* —Simple model, property graph, or other?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Schema* —Which entities in your data are nodes, edges, and properties?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data structure* —How is the data stored: in adjacency matrices, adjacency
    lists, or edge lists?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Receiving systems* —How does the receiving system (in our case, GNN libraries
    and graph-processing systems) accept data? What encodings and data structures
    are preferred? Is imported data automatically recognized, or is custom programming
    required to read in data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/8-8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8 View of raw data: JSON file. The figure on the left is in key/value
    format. The keys are the members, and the values are their known relationships.
    The figure on the right shows unhashed values, demonstrating example names for
    these individuals.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here are a few encoding choices you’re likely to encounter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Language and system-agnostic encodings formats* —These are most popular as
    they are extremely flexible and work across many systems and languages. However,
    data arrangement can still differ from system to system. Therefore, an edge list
    in a CSV file, with a specific set of headers, may not be accepted or interpreted
    in the same way between two different systems. Following are some examples for
    this format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*JSON* —Has advantages when reading from APIs or feeding into JavaScript applications.
    `Cytoscape.js`, a graph visualization library, accepts data in JSON format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CSV* —Accepted by many processing systems and databases. However, the required
    arrangement and labeling of the data differs from system to system.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*XML* —Graph Exchange XML (GEXF) format is of course an XML format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Language specific* *—*Python, Java, and other languages have built-in encoding
    formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pickle* —Python’s format. Some systems accept Pickle encoded files. Despite
    this, unless your data pipeline or workflow is governed extensively by Python,
    pickles should be used lightly. The same applies for other language-specific encodings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System driven* —Specific software, systems, and libraries have their own encoding
    formats. Though these may be limited in usability between systems, an advantage
    is that the schema in such formats is consistent. Software and systems that have
    their own encoding format include Stanford Network Analysis Platform (SNAP), NetworkX,
    and Gephi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Big data* —Aside from the language-agnostic formats listed previously, there
    are other encoding formats used for larger sizes of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Avro* —This encoding is used extensively in Hadoop workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Matrix based* —Because graphs can be expressed as matrices, there are a few
    formats that are based on this data structure. For sparse graphs, the following
    formats provide substantial memory savings and computational advantages (for lookups
    and matrix/vector multiplication):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse column matrix (.csc filetype)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse row matrix (.csr filetype)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix market format (.mtx filetype)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.3.2 The ETL step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the schema chosen and data sources established, the *ETL* (*extract, transform,
    load*) step consists of taking raw data from its sources and then producing data
    that fits the schema and is ready for preprocessing or training. For our data,
    this consists of programming a set of actions that begin with pulling the data
    from the various databases and then joining them as needed.
  prefs: []
  type: TYPE_NORMAL
- en: We need data that ends up in a specific format that we can input into a preprocessing
    step. This could be a JSON format or an edge list. For either the JSON example
    or edge list example, our schema is fulfilled; we’ll have nodes (the individual
    persons) and edges (the relationships between these people).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our recruitment example, we want to transform our raw data into a graph
    data structure, encoded in CSV. This was chosen for ease of manipulation with
    Python. This file can then be loaded into our graph-processing system, NetworkX,
    or a GNN package such as PyG. To summarize the next steps, we’ll do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the raw data file to a graph format, following your chosen graph data
    model. In our case, we convert the raw data into an edge list and an adjacency
    list. We then saved it as a CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the CSV file into NetworkX for exploratory data analysis (EDA) and visualization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load into PyG and preprocess.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raw data to adjacency List and edge List
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Starting with our CSV and JSON files, we next convert the data into two key
    data models: an edge list and an adjacency list, which we define in appendix A.
    Both adjacency and edge lists are two basic data representations used with graphs.
    An edge list is a list where every item in this structure contains a node with
    a list of its adjacent nodes. These representations are illustrated in figure
    8.9.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 A graph with nodes and edges marked (top). An edge list representation
    (middle); each entry contains the edge number and the pair of nodes connected.
    An adjacency list representation in a dictionary (bottom); each key is a node,
    and the values are its adjacent nodes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First using the `json` module, we load the data from a JSON file into a Python
    dictionary. The Python dictionary has the same structure as the JSON, with member
    hashes as keys and their relationships as values.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an adjacency list
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we create an adjacency list from this dictionary. This list will be stored
    as a text file. Each line of the file will contain the member hash, followed by
    hashes of that member’s relationships. The process for creating an adjacency list
    is illustrated in figure 8.10.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function transforms our raw data into an adjacency list, which we’ll apply
    to our recruitment example. We’ll have *inputs* that consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary of candidate referrals where the keys are members who have referred
    other candidates, and the values are lists of the people who were referred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A suffix to append to the filename
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/8-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 Flow diagram illustrating the process of transforming a relationship
    dictionary into a well-structured adjacency list, stored in a text file, while
    ensuring the symmetry of connections in the undirected graph.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We’ll have *outputs* that consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An encoded adjacency list in a txt file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of the node IDs found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.1 Create an adjacency list from relationship dictionary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Runs through every node in the input data dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Because this is an undirected graph, there must be a symmetry in the values;
    that is, every value in a key must contain that key in its own entry. As an example,
    for entry F, if G is a value, then for entry G, F must be a value. These lines
    check for that and fix the dictionary if these conditions don’t exist.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates a text file that will store the adjacency list'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 For every key in the dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Creates a string from the list of dictionary values. This value is a string
    of member IDs separated by empty spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Optional print'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Writes a line to the text file. This line will contain the member hash,
    and then a string of relationship hashes.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an edge list
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we show the process to create an edge list. As with the adjacency list,
    we transform the data to account for node pair symmetry. Note that either format
    could work for this project. For your own project, another format could also be
    warranted. Figure 8.11 illustrates the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 Process of creating an edge list file programmed into listing 8.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As with the adjacency list function, the edge list function illustrates the
    transformation of raw data into an edge list and has the same inputs as the previous
    function. The outputs consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An edge list in a .txt file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lists of the node IDs found and the edges generated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By definition, every entry of an edge list must be unique, so we must ensure
    that our produced edge list is the same. Here’s the code to create an edge list
    from a relationship dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.2 Create an edge list from relationship dictionary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Each member dictionary value is a list of relationships. For every key,
    we iterate through every value.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Because this graph is undirected, we don’t want to create duplicate edges.
    For example, because {F,G} is the same as {G,F}, we only need one of these. This
    line checks if a node pair exists already. We use a set object because the node
    order doesn’t matter.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Writes the line to the text file. This line will consist of the node pair.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we’ll use the adjacency list to load our graph into NetworkX.
    One thing to note about the differences between loading a graph using the adjacency
    list versus the edge list is that edge lists can’t account for single, unlinked
    nodes. It turns out that quite a few of the candidates at Whole Staffing haven’t
    recommended anyone, and don’t have edges associated with them. These nodes would
    be invisible to an edge list representation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3 Data exploration and visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we want to load our network data into a graph processing framework. We
    chose NetworkX, but there are many other choices available, depending on your
    task and language preferences. We chose NetworkX because we have a small graph,
    and we also want to do some light EDA and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: With our newly created adjacency list, we can create a NetworkX graph object
    by calling the `read_edgelist` or `read_adjlist` methods. Next, we can load in
    the attributes `industry` and `job type`. In this example, these attributes are
    loaded in as a dictionary, where the node IDs serve as keys.
  prefs: []
  type: TYPE_NORMAL
- en: With our graph loaded, we can explore and inspect our data to ensure that it
    aligns with our assumptions. First, the count of nodes and edges should match
    our member count, and the number of edges created in our edge list, respectively,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.3 Create an edge list from the relationship dictionary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to check how many connected components our graph has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `connected_components` method generates the connected components of a graph;
    a visualization is shown in figure 8.12 and generated using NetworkX. There are
    hundreds of components, but when we inspect this data, we find that there is one
    large component of 1,698 nodes, and the rest are composed of less than 4 nodes.
    Most of the disconnected components are singleton nodes (the candidates that never
    refer anyone). For more information about components of a graph, we give definitions
    and details in appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 The full graph, with its large connected component in the middle,
    surrounded by many smaller components. For our example, we’ll use only the nodes
    in the large connected component.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’re interested in this large connected component and will work with that going
    forward. The `subgraph` method can help us to isolate this large component.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use NetworkX to visualize our graph. For this, we’ll use a standard
    recipe for analyzing graphs which can also be found in the NetworkX documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the different steps (the full code sample for each step is
    also in the repository, labeled “Function that visualizes the social graph and
    shows degree statistics”):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Create the graph object*. Generate a distinct graph object, selecting the
    largest connected component from the given graph. In cases where there’s only
    one connected component, this step might be unnecessary but ensures the selection
    of the major component.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '2\. *Determine the layout*. Decide the positioning of nodes and edges for visualization.
    Choose an appropriate layout algorithm; for example, the Spring Layout models
    the edges as springs and nodes as repelling masses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 3\. *Draw nodes and edges*. Use the chosen layout to draw nodes on the visualization.
    Adjust visual parameters such as node size to enhance the clarity of the figure.
    Based on the selected layout, draw the edges. Modify appearance settings such
    as transparency to achieve the desired visual effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '4\. *Generate and plot node degrees*. Employ the degree method on the graph
    object to create an iterable of nodes with their respective degrees, and sort
    them from highest to lowest. Visualize the sorted list of node degrees on a plot
    to analyze the distribution and prominence of various nodes. Use NumPy’s `unique`
    method with the `return_counts` parameter to plot a histogram showing the degrees
    of nodes and their counts, providing insights into the graph’s structure and complexity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These plots are shown in figure 8.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 Visualization and statistics of the social graph and its large connected
    component. Network visualization using NetworkX default settings (top). A rank
    plot of node degree of the entire graph (bottom left). We see that about three-fourths
    of nodes have less than 20 adjacent nodes. A histogram of degree (bottom right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Lastly, we can visualize an adjacency matrix of our graph, shown in figure
    8.14, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 A visualized adjacency matrix of our social graph. Vertical and
    horizontal values refer to respective nodes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As with the numerical adjacency matrix, for our undirected graph, this visual
    adjacency matrix has symmetry down the diagonal. All undirected graphs will have
    symmetric adjacency matrices. For directed graphs, this can happen but isn’t guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.4 Preprocessing and loading data into PyG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this book, *preprocessing* consists of putting our data, including its properties,
    labels, or other metadata, in a format suitable for downstream machine learning
    models. Feature engineering can also be a step in this process. For feature engineering,
    we’ll often use graph algorithms to calculate the properties of nodes, edges,
    or subgraphs.
  prefs: []
  type: TYPE_NORMAL
- en: An example for node features is betweenness centrality. If our schema allows,
    we can calculate and attach such properties to the node entities of our data.
    To perform this, we take the output of the ETL step, say an edge list, and import
    this into a graph processing framework to calculate betweenness centrality for
    each node. Once this quantity is obtained, we can store it using a dictionary
    with the node ID as keys, then use this as a node feature later on.
  prefs: []
  type: TYPE_NORMAL
- en: Betweenness centrality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Betweenness centrality* is a critical measure of node importance that quantifies
    the tendency of a node to lie in the shortest paths from source to destination
    nodes. Given a graph with *n* nodes, you could determine the shortest path between
    every unique pair of nodes in this graph. We could take this set of shortest paths
    and look for the presence of a particular node. If the node appears in all or
    most of these paths, it has a high betweenness centrality and would be considered
    to be highly influential. Conversely, if the node appears a few times (or only
    once) in the set of shortest paths, it will have a low betweenness centrality,
    and a low influence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our data, we want to make it ready for use in our selected
    GNN framework. In this book, we use PyG, due to its robust suite of tools and
    flexibility in handling complex graph data. However, most standard GNN packages
    have mechanisms to import custom data into their frameworks. For this section,
    we’ll focus on three modules within PyG:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Data` *module* (`torch_geometric.data`)—Allows inspection, manipulation, and
    creation of data objects that are used by the PyG environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Utils` *module* (`torch_geometric.utils`)—Many useful methods. Helpful in
    this section are methods that allow the quick import and export of graph data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Datasets` *module* (`torch_geometric.datasets`)—Preloaded datasets, including
    benchmark datasets, and datasets from influential papers in the field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with the `Datasets` module. This module contains datasets that have
    already been preprocessed and can readily be used by PyG’s methods. When starting
    with PyG, having these datasets allows for easy experimentation without worrying
    about creating a data pipeline. Similarly, by studying the codebase underlying
    these datasets, we can also learn how to create our own custom datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the previous section, we converted our raw data into a standard
    format and loaded our new graphs into a graph-processing framework. Now, we want
    to load our data into the PyG environment. Preprocessing in PyG has a few objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating data objects with multiple attributes from the level of nodes and edges
    to the subgraph and graph level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining different data sources into one object or set of related objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting data into objects that can be processed using GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing splitting of training/testing/validation data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling batching of data for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These objectives are fulfilled by a hierarchy of classes within the `Data`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Data` *class*—Creates graph objects. These objects can have optional built-in
    and custom-made attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` *and* `InMemoryDataset` *classes* —Creates a repeatable data preprocessing
    pipeline. You can start from raw data files and add custom filters and transformations
    to achieve your preprocessed *data* objects. `Dataset` objects are larger than
    memory, while `InMemoryDataset` objects fit in memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataloader` *class* —Batches data objects for model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is shown in figure 8.15, including how different data and dataset classes
    connect to the dataloader.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 Steps to preprocess data in PyG. From raw files, there are essentially
    two paths to prep data for ingestion by a PyG algorithm. The first path, shown
    here, directly creates an iterator of data instances, which is used by the dataloader.
    The second path mimics the first but performs this process within the dataloader
    class.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are two paths to preprocess data, one uses a `dataset` class and the
    other goes without it. The advantage of using the `dataset` class is that it allows
    us to save the generated datasets and also preserve filtering and transformation
    details. Dataset objects are flexible and can be modified to output variations
    of a dataset. On the other hand, if your custom dataset is simple or generated
    on the fly, and you have no use for saving the data or process long term, bypassing
    dataset objects may serve you well. So, in summary, we have the following different
    data-related classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Datasets` *objects* —Preprocessed datasets for benchmarking or testing an
    algorithm or architecture (not to be confused with `Dataset`—no “s” at the end—objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Data` *objects into iterator* —Graph objects that are generated on the fly
    or for whom there is no need to save.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` *object* —For graph objects that should be preserved, including the
    data pipeline, filtering and transformations, input raw data files, and output
    processed data files. Not to be confused with `Datasets` (with “s” at the end)
    objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With those basics, let’s preprocess our social graph data. We’ll cover the
    following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Convert into a* `data` *instance using NetworkX.* For quick conversion from
    NetworkX to PyG, ideal for ad hoc processing or when using NetworkX’s functionalities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Convert into a* `data` *instance using input files.* Offers control over the
    data import process, which is ideal for raw data and custom preprocessing requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Convert to* `dataset` *instance.* For systematic, scalable, and reproducible
    data preprocessing and management, especially for complex or reusable datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Convert* `data` *objects for use in* `dataloader` *without the* `dataset`
    *class.* For scenarios where simplicity and speed are prioritized over systematic
    data management and preprocessing, or for on-the-fly and synthetic data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we’ll import the needed modules from PyG in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.4 Required imports, covering data object creation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Case A: Create PyG data object using the NetworkX object'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous sections, we’ve explored a graph expressed as a NetworkX `graph`
    object. PyG’s `util` module has a method that can directly create a PyG `data`
    object from a NetworkX `graph` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `from_networkx` method preserves nodes, edges, and their attributes, but
    it should be checked to ensure the translation from one module to another went
    smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case B: Create PyG data object using raw files'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For greater control over data import into PyG, we can start with raw files or
    files from any stage of the ETL process. In our social graph case, we can begin
    with the edge list file created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s review an example where we use code to process and convert our social
    graph from an edge list text file into a format suitable for training a GNN model
    in PyG. We prepare node features, labels, edges, and training/testing sets for
    use in the PyG environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: Import and prepare graph data'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This part includes reading an edge list from a file to create a NetworkX graph,
    extracting the list of nodes, creating mappings from node names to indices, and
    vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 An edge list is read from a text file and used to create a NetworkX graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 All unique nodes in the graph are then extracted and listed.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Indices for each node are also generated.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Two dictionaries are created to allow easy conversion between node names
    and their respective indices, facilitating the handling and manipulation of graph
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Process edges and node features'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This part focuses on converting the edges and node attributes into a format
    that can be easily used with PyTorch for machine learning tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A NetworkX edge list object is created.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 It’s then transformed into two separate lists representing the source and
    destination nodes of each edge.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 These lists are then indexed using the previously created node-to-index
    mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The node features and labels are prepared using PyTorch tensor objects,
    assuming a simple scenario where all nodes have the same single feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The node features and labels are prepared using PyTorch tensor objects,
    assuming a simple scenario where all nodes have the same single feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Prepare data for training and testing'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this part, the dataset is prepared for training and testing by creating
    masks for data splitting and combining all the processed data into a single PyTorch
    data object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The edge indices created in part 2 are converted into a PyTorch tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Masks for training and testing datasets are created by splitting the nodes
    into two separate groups, ensuring that specific portions of the data are used
    for training and testing.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 All the processed components, including node features, labels, edge indices,
    and data masks, are then combined into a single PyTorch Data object, preparing
    the data for subsequent machine learning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve created a `data` object from an `edgelist` file. Such an object can be
    inspected with PyG commands, though the set of commands is limited compared to
    a graph processing library. Such a `data` object can also be further prepared
    so that it can be accessed by a `dataloader`, which we’ll cover next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case C: Create PyG dataset object using custom class and input files'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the previous listing is suitable for our purposes, and we want to use it
    repeatedly, a preferable option is to create a permanent class that we can include
    for our pipeline. This is what the `dataset` class does.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s next create a `dataset` object, shown in listing 8.5\. In this example,
    we name our `dataset` `MyOwnDataset` and have it inherit from `InMemoryDataset`
    because our social graph is small enough to sit in memory. As discussed earlier,
    for larger graphs, data can be accessed from disk by having the `dataset` object
    inherit from `Dataset` instead of `InMemoryDataset`.
  prefs: []
  type: TYPE_NORMAL
- en: This first part of the code initiates the custom `dataset` class, inheriting
    properties from the `InMemoryDataset` class. The constructor initializes the dataset,
    loads processed data, and defines the properties for raw and processed filenames.
    The raw files are kept empty as this example doesn’t require them, and the processed
    data is fetched from a specified path.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.5 Class to create a dataset object (part 1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initializes the dataset class. This class inherits from the InMemoryDataset
    class. This init method creates data and slices objects to be updated in the process
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 An optional method that specifies the location of the raw files required
    for processing. For our more rudimentary example, we don’t make use of this but
    have included it for completeness. In later chapters, we’ll make use of this as
    our dataset becomes a bit more complex.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This method saves our generated dataset to disk.'
  prefs: []
  type: TYPE_NORMAL
- en: This segment of the code is for data downloading and processing. It reads an
    edge list from a text file and converts it into a NetworkX graph. The nodes and
    edges of the graph are then indexed and converted into tensors suitable for machine
    learning tasks. The method downloaded is kept as a placeholder in case there’s
    a need to download raw data in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.6 Class to create a dataset object (part 2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Allows raw data to be downloaded to a local disk.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The process method contains the preprocessing steps to create our data object,
    and then makes additional steps to partition our data for loading.'
  prefs: []
  type: TYPE_NORMAL
- en: This final part of the code is focused on preparing and saving the data for
    machine learning models. It creates feature and label tensors, prepares the edge
    index, and generates training and testing masks to split the dataset. The data
    is then collated and saved in the processed path for easy retrieval during model
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 8.7 Class to create a dataset object (part 3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 In this first simple use of a dataset class, we use a small dataset. In
    practice, we’ll process much larger datasets and wouldn’t do this all at once.
    We’d create examples of our data, then append them to a list. For our purposes
    (training on this data), pulling from a list object would be slow, so we take
    this iterable, and use collate to combine the data examples into one data object.
    The collate method also creates a dictionary named slices that is used to pull
    single samples from this data object.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Saves our preprocessed data to disk'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case D: Create PyG data objects for use in dataloader without use of a dataset
    object'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lastly, we explain how to bypass `dataset` object creation and have the `dataloader`
    work directly with your `data` object, as illustrated in figure 8.15\. In the
    PyG documentation, there is a section that outlines how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as in regular PyTorch, you don’t have to use datasets, for example, when
    you want to create synthetic data on the fly without saving them explicitly to
    disk. In this case, simply pass a regular Python list holding `torch_geometric.data.Data`
    objects and pass them to `torch_geometric.data.DataLoader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, we’ve covered the steps that go from project outline, through
    to converting raw data into a format ready for GNNs. As we conclude this section,
    it’s worth noting that every dataset is different. The procedures outlined in
    this discussion provide a structural framework that serves as a starting point,
    not a one-size-fits-all solution. In the final section, we turn to the subject
    of sourcing data to support data projects.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Where to find graph data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To not start from scratch in developing a graph data model and schema for your
    problem, there are several sources of published models and schemas. They include
    industry standard data models, published datasets, published semantic models (including
    knowledge graphs), and academic papers. A set of example sources is provided in
    table 8.2\.
  prefs: []
  type: TYPE_NORMAL
- en: Sourcing graph data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Details of different sources for graph-based data that can be used for GNN projects.
  prefs: []
  type: TYPE_NORMAL
- en: '*From nongraph data*—In this chapter, we assumed that the data lies in nongraph
    sources and must be transformed into a graph format using ETL and preprocessing.
    Having a schema can help guide such a transformation and keep it ready for further
    analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Existing graph datasets—*The number of freely available graph datasets is
    growing. Two GNN libraries we use in this book, Deep Graph Library (DGL) and PyG,
    come with a number of benchmark datasets installed. Many such datasets are from
    influential academic papers. However, such datasets are small scale, which limits
    reproducibility of results, and whose performance don’t necessarily scale for
    large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A source of data that seeks to mitigate the problems of earlier benchmark datasets
    in this space is Open Graph Benchmark (OGB). This initiative provides access to
    a variety of real-world datasets, of varying scales. OGB also publishes performance
    benchmarks by learning task. Table 8.2 lists a few repositories of graph datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*From generation*—Many graph processing frameworks and graph databases allow
    the generation of random graphs using a number of algorithms. Though random, depending
    on the generating algorithm, the resulting graph will have characteristics that
    are predictable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 8.2 Graph datasets and semantic models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Source | Type | Problem Domains | URL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Open Graph Benchmark (OGB)  | Graph datasets and benchmarks  | Social networks,
    drug discovery  | [https://ogb.stanford.edu/](https://ogb.stanford.edu/)  |'
  prefs: []
  type: TYPE_TB
- en: '| GraphChallenge Datasets  | Graph datasets  | Network science, biology  |
    [https://graphchallenge.mit.edu/data-sets](https://graphchallenge.mit.edu/data-sets)  |'
  prefs: []
  type: TYPE_TB
- en: '| Network Repository  | Graph datasets  | Network science, bioinformatics,
    machine learning, data mining, physics, and social science  | [http://networkrepository.com/](http://networkrepository.com/)  |'
  prefs: []
  type: TYPE_TB
- en: '| SNAP Datasets  | Graph datasets  | Social networks, network science, road
    networks, commercial networks, finance  | [http://snap.stanford.edu/data/](http://snap.stanford.edu/data/)  |'
  prefs: []
  type: TYPE_TB
- en: '| Schema.org  | Semantic data model  | Internet web pages  | [https://schema.org/](https://schema.org/)  |'
  prefs: []
  type: TYPE_TB
- en: '| Wikidata  | Semantic data model  | Wikipedia pages  | [www.wikidata.org/](http://www.wikidata.org/)  |'
  prefs: []
  type: TYPE_TB
- en: '| Financial Industry Business Ontology  | Semantic data model  | Finance  |
    [https://github.com/edmcouncil/fibo](https://github.com/edmcouncil/fibo)  |'
  prefs: []
  type: TYPE_TB
- en: '| Bioportal  | List of medical semantic models  | Medical  | [https://bioportal.bioontology.org/ontologies/](https://bioportal.bioontology.org/ontologies/)  |'
  prefs: []
  type: TYPE_TB
- en: Public graph datasets also exist in several places. Published datasets have
    accessible data, with summary statistics. Often, however, they lack explicit schemas,
    conceptual or otherwise. To derive the dataset’s entities, relations, rules, and
    constraints, querying the data becomes necessary.
  prefs: []
  type: TYPE_NORMAL
- en: For semantic models based on property, RDF, and other data models, there are
    some general datasets, and others are targeted to particular industries and verticals.
    Such references seldom use graph-centric terms (e.g., *node*, *vertex*, and *edge*)
    but will use terms related to semantics and ontologies (e.g., *entity*, *relationship*,
    *links*). Unlike the graph datasets, the semantic models offer data frameworks,
    not the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: Reference papers and published schemas can provide ideas and templates that
    can help in developing your schema. There are a few use cases targeted toward
    industry verticals that both represent a situation using graphs and use graph
    algorithms, including GNNs, to solve a relevant problem. Transaction fraud in
    financial institutions, molecular fingerprinting in chemical engineering, and
    page rank in social networks are a few examples. Perusing such existing work can
    provide a boost to development efforts. On the other hand, often such published
    work is done for academic, not industry goals. A network that is developed to
    prove an academic point or make empirical observations may not have qualities
    amenable to an enterprise system that must be maintained and be used on dirty
    and dynamic data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Planning for a graph learning project involves more steps than in traditional
    machine learning projects. The objectives and requirements will influence the
    design of the system, data models, and GNN architecture. The project includes
    creating robust graph data models, understanding and transforming raw data, and
    ensuring that the models effectively represent the complex relationships within
    the recruitment landscape.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important step is creating the data model and schema for your data. These
    processes are essential to avoid technical debt. This involves designing the elements,
    relationships, and constraints; running queries; analyzing results; making adjustments;
    and validating against criteria to ensure the model’s readiness for complex queries
    and machine learning applications. A graph data model will be refined through
    iterative testing and refactoring to ensure it effectively supports the analysis
    of complex relationships within the recruitment data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many encoding and serialization options for keeping data in memory
    or in raw files, including language and system-agnostic formats such as JSON,
    CSV, and XML. Language-specific formats, such as Python’s Pickle, and system-driven
    formats from specific software and libraries such as SNAP, NetworkX, and Gephi,
    are also mentioned. For big data, Avro and matrix-based formats (sparse column
    matrix, sparse row matrix, and matrix market format) are highlighted as efficient
    options for handling large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data pipeline can start with raw data that undergoes exploratory analysis
    and preprocessing to be usable by GNN libraries such as PyG. The raw data is transformed
    into standard formats such as edge lists or adjacency matrices, ensuring consistency
    and usability for different problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph processing frameworks such as NetworkX are used for light exploratory
    data analysis (EDA) and visualization. Graph objects, such as adjacency and edge
    lists, are loaded into NetworkX. The visual representation and statistical analysis,
    such as the number of nodes, edges, and connected components, are derived to understand
    the graph’s structure and complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyG library is used for preprocessing, involving the conversion of data
    into formats that can be easily manipulated and trained with. Data objects are
    created with multiple attributes at various levels, enabling GPU processing and
    facilitating the splitting of training, testing, and validation data. The choice
    between using dataset objects or bypassing them depends on the need for saving
    data and the complexity of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are numerous repositories of ready-to-use graph datasets and semantic
    models covering various domains, such as social networks and drug discovery. However,
    while these datasets are useful for learning and benchmarking, they are often
    small-scale and may not be directly applicable for large, real-world problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While public graph datasets and semantic models provide a starting point, they
    often lack explicit schemas requiring additional work to derive entities, relations,
    and constraints. Additionally, while academic papers offer templates for developing
    schemas, they are typically designed for academic purposes and may not be directly
    transferable to real-world, industry-specific applications with dynamic and dirty
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
