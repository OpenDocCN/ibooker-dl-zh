["```py\ntf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n```", "```py\n[None, 28, 28, 1]\n\n```", "```py\n[5, 5, 1, 32]\n\n```", "```py\ntf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n```", "```py\ntf.nn.dropout(layer, keep_prob=keep_prob)\n\n```", "```py\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\ndef conv_layer(input, shape):\n    W = weight_variable(shape)\n    b = bias_variable([shape[3]])\n    return tf.nn.relu(conv2d(input, W) + b)\n\ndef full_layer(input, size):\n    in_size = int(input.get_shape()[1])\n    W = weight_variable([in_size, size])\n    b = bias_variable([size])\n    return tf.matmul(input, W) + b\n\n```", "```py\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\nconv1 = conv_layer(x_image, shape=[5, 5, 1, 32])\nconv1_pool = max_pool_2x2(conv1)\n\nconv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\nconv2_pool = max_pool_2x2(conv2)\n\nconv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\nfull_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n\nkeep_prob = tf.placeholder(tf.float32)\nfull1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n\ny_conv = full_layer(full1_drop, 10)\n\n```", "```py\ninitial = tf.truncated_normal(shape, stddev=0.1)\n```", "```py\nmnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=\n                                                           y_conv, labels=y_))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(STEPS):\n        batch = mnist.train.next_batch(50)\n\n        if i % 100 == 0:\n            train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], \n                                                           y_: batch[1],\n                                                           keep_prob: 1.0})\n            print \"step {}, training accuracy {}\".format(i, train_accuracy)\n\n        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1],\n                                        keep_prob: 0.5})\n\n    X = mnist.test.images.reshape(10, 1000, 784)\n    Y = mnist.test.labels.reshape(10, 1000, 10)\n    test_accuracy = np.mean([sess.run(accuracy, \n                            feed_dict={x:X[i], y_:Y[i],keep_prob:1.0}) \n                            for i in range(10)])\n\nprint \"test accuracy: {}\".format(test_accuracy)\n\n```", "```py\nclass CifarLoader(object):\ndef __init__(self, source_files):\nself._source = source_files\nself._i = 0\nself.images = None\nself.labels = None\n\ndef load(self):\ndata = [unpickle(f) for f in self._source]\nimages = np.vstack([d[\"data\"] for d in data])\nn = len(images)\nself.images = images.reshape(n, 3, 32, 32).transpose(0, 2, 3, 1)\\\n.astype(float) / 255\nself.labels = one_hot(np.hstack([d[\"labels\"] for d in data]), 10)\nreturn self\n\ndef next_batch(self, batch_size):\nx, y = self.images[self._i:self._i+batch_size], \nself.labels[self._i:self._i+batch_size]\nself._i = (self._i + batch_size) % len(self.images)\nreturn x, y\n\n```", "```py\nDATA_PATH=\"*`/path/to/CIFAR10`*\"defunpickle(file):withopen(os.path.join(DATA_PATH,file),'rb')asfo:dict=cPickle.load(fo)returndictdefone_hot(vec,vals=10):n=len(vec)out=np.zeros((n,vals))out[range(n),vec]=1returnout\n```", "```py\nclass CifarDataManager(object):\ndef __init__(self):\nself.train = CifarLoader([\"data_batch_{}\".format(i) \nfor i in range(1, 6)])\n.load()\nself.test = CifarLoader([\"test_batch\"]).load()\n\n```", "```py\ndef display_cifar(images, size):\n    n = len(images)\n    plt.figure()\n    plt.gca().set_axis_off()\n    im = np.vstack([np.hstack([images[np.random.choice(n)] for i in range(size)])\n                    for i in range(size)])\n    plt.imshow(im)\n    plt.show()\n\nd = CifarDataManager()\nprint \"Number of train images: {}\".format(len(d.train.images))\nprint \"Number of train labels: {}\".format(len(d.train.labels))\nprint \"Number of test images: {}\".format(len(d.test.images))\nprint \"Number of test images: {}\".format(len(d.test.labels))\nimages = d.train.images\ndisplay_cifar(images, 10)\n\n```", "```py\nNumber of train images: 50000\nNumber of train labels: 50000\nNumber of test images: 10000\nNumber of test images: 10000\n\n```", "```py\ncifar = CifarDataManager()\n\nx = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nkeep_prob = tf.placeholder(tf.float32)\n\nconv1 = conv_layer(x, shape=[5, 5, 3, 32])\nconv1_pool = max_pool_2x2(conv1)\n\nconv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\nconv2_pool = max_pool_2x2(conv2)\nconv2_flat = tf.reshape(conv2_pool, [-1, 8 * 8 * 64])\n\nfull_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\nfull1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n\ny_conv = full_layer(full1_drop, 10)\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, \n                                                               y_))\ntrain_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ndef test(sess):\nX = cifar.test.images.reshape(10, 1000, 32, 32, 3)\nY = cifar.test.labels.reshape(10, 1000, 10)\nacc = np.mean([sess.run(accuracy, feed_dict={x: X[i], y_: Y[i], \n                                             keep_prob: 1.0})\nfor i in range(10)])\nprint \"Accuracy: {:.4}%\".format(acc * 100)\n\nwith tf.Session() as sess:\nsess.run(tf.global_variables_initializer())\n\nfor i in range(STEPS):\nbatch = cifar.train.next_batch(BATCH_SIZE)\nsess.run(train_step, feed_dict={x: batch[0], y_: batch[1], \n                                keep_prob: 0.5})\n\ntest(sess)\n\n```", "```py\nx = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n\n```", "```py\nconv2_flat = tf.reshape(conv2_pool, [-1, 8 * 8 * 64])\n\n```", "```py\nx = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nkeep_prob = tf.placeholder(tf.float32)\n\nconv1 = conv_layer(x, shape=[5, 5, 3, 32])\nconv1_pool = max_pool_2x2(conv1)\n\nconv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\nconv2_pool = max_pool_2x2(conv2)\n\nconv3 = conv_layer(conv2_pool, shape=[5, 5, 64, 128])\nconv3_pool = max_pool_2x2(conv3)\nconv3_flat = tf.reshape(conv3_pool, [-1, 4 * 4 * 128])\nconv3_drop = tf.nn.dropout(conv3_flat, keep_prob=keep_prob)\n\nfull_1 = tf.nn.relu(full_layer(conv3_drop, 512))\nfull1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n\ny_conv = full_layer(full1_drop, 10)\n```", "```py\nC1, C2, C3 = 30, 50, 80\nF1 = 500\n\nconv1_1 = conv_layer(x, shape=[3, 3, 3, C1])\nconv1_2 = conv_layer(conv1_1, shape=[3, 3, C1, C1])\nconv1_3 = conv_layer(conv1_2, shape=[3, 3, C1, C1])\nconv1_pool = max_pool_2x2(conv1_3)\nconv1_drop = tf.nn.dropout(conv1_pool, keep_prob=keep_prob)\n\nconv2_1 = conv_layer(conv1_drop, shape=[3, 3, C1, C2])\nconv2_2 = conv_layer(conv2_1, shape=[3, 3, C2, C2])\nconv2_3 = conv_layer(conv2_2, shape=[3, 3, C2, C2])\nconv2_pool = max_pool_2x2(conv2_3)\nconv2_drop = tf.nn.dropout(conv2_pool, keep_prob=keep_prob)\n\nconv3_1 = conv_layer(conv2_drop, shape=[3, 3, C2, C3])\nconv3_2 = conv_layer(conv3_1, shape=[3, 3, C3, C3])\nconv3_3 = conv_layer(conv3_2, shape=[3, 3, C3, C3])\nconv3_pool = tf.nn.max_pool(conv3_3, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], \n                           padding='SAME')\nconv3_flat = tf.reshape(conv3_pool, [-1, C3])\nconv3_drop = tf.nn.dropout(conv3_flat, keep_prob=keep_prob)\n\nfull1 = tf.nn.relu(full_layer(conv3_drop, F1))\nfull1_drop = tf.nn.dropout(full1, keep_prob=keep_prob)\n\ny_conv = full_layer(full1_drop, 10)\n\n```", "```py\nconv3_pool = tf.nn.max_pool(conv3_3, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], \n                           padding='SAME')\n\n```"]