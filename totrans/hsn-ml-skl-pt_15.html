<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 13. Processing Sequences Using RNNs and CNNs"><div class="chapter" id="rnn_chapter">
<h1><span class="label">Chapter 13. </span>Processing Sequences Using <span class="keep-together">RNNs and CNNs</span></h1>


<p>Predicting the future<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" id="xi_recurrentneuralnetworksRNNs13322_1"/> is something you do all the time, whether you are finishing a friend‚Äôs sentence or anticipating the smell of coffee at breakfast. In this chapter we will discuss recurrent neural networks (RNNs)‚Äîa class of nets that can predict the future (well, up to a point). RNNs can analyze time series data, such as the number of daily active users on your website, the hourly temperature in your city, your home‚Äôs daily power consumption, the trajectories of nearby cars, and more. Once an RNN learns past patterns in the data, it is able to use its knowledge to forecast the future, assuming, of course, that past patterns still hold in the future.</p>

<p>More generally, RNNs can work on sequences of arbitrary lengths, rather than on fixed-sized inputs. For example, they can take sentences, documents, or audio samples as input, which makes them extremely useful for natural language processing applications such as automatic translation or speech-to-text.</p>

<p>In this chapter, we will first go through the fundamental concepts underlying RNNs and how to train them using backpropagation through time. Then, we will use them to forecast a time series<a data-type="indexterm" data-primary="time series data, forecasting" id="id3049"/>. Along the way, we will look at the popular autoregressive moving average (ARMA)<a data-type="indexterm" data-primary="autoregressive integrated moving average (ARIMA) model" id="id3050"/> family of models, often used to forecast time series, and use them as baselines to compare with our RNNs. After that, we‚Äôll explore the two main difficulties that RNNs face:</p>

<ul>
<li>
<p>Unstable gradients (discussed in <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>), which can be alleviated using various techniques<a data-type="indexterm" data-primary="recurrent dropout" id="id3051"/><a data-type="indexterm" data-primary="recurrent layer normalization" id="id3052"/><a data-type="indexterm" data-primary="layer normalization (LN)" id="id3053"/><a data-type="indexterm" data-primary="LN (layer normalization)" id="id3054"/>, including <em>recurrent dropout</em> and <em>recurrent layer normalization</em>.</p>
</li>
<li>
<p>A (very) limited short-term memory, which can be extended using long short-term memory (LSTM) and gated recurrent unit (GRU) cells.</p>
</li>
</ul>

<p>RNNs are not the only types of neural networks capable of handling sequential data. For small sequences, a regular dense network can do the trick, and for very long sequences, such as audio samples or text, convolutional neural networks can actually work quite well too. We will discuss both of these possibilities, and we will finish this chapter by implementing a WaveNet‚Äîa CNN architecture<a data-type="indexterm" data-primary="WaveNet" id="id3055"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="WaveNet" id="id3056"/> capable of handling sequences of tens of thousands of time steps. But we can do even better! In <a data-type="xref" href="ch14.html#nlp_chapter">Chapter¬†14</a>, we will apply RNNs to natural language processing (NLP), and we will see how to boost them using attention mechanisms. Attention is at the core of transformers, which we will discover in <a data-type="xref" href="ch15.html#transformer_chapter">Chapter¬†15</a>: they are now the state of the art for sequence processing, NLP, and even computer vision. But before we get there, let‚Äôs start with the simplest RNNs!</p>






<section data-type="sect1" data-pdf-bookmark="Recurrent Neurons and Layers"><div class="sect1" id="id241">
<h1>Recurrent Neurons and Layers</h1>

<p>Up to now we have focused on feedforward neural networks, where the activations flow only in one direction, from the input layer to the output layer. A recurrent neural network looks very much like a feedforward neural network<a data-type="indexterm" data-primary="feedforward neural network (FNN)" id="id3057"/><a data-type="indexterm" data-primary="FNN (feedforward neural network)" id="id3058"/>, except it also has connections pointing backward.</p>

<p>Let‚Äôs look at the simplest possible RNN, composed of one neuron<a data-type="indexterm" data-primary="recurrent neuron" id="id3059"/> receiving inputs, producing an output, and sending that output back to itself (see <a data-type="xref" href="#simple_rnn_diagram">Figure¬†13-1</a>, left). At each <em>time step</em> <em>t</em> (also called a <em>frame</em>), this <em>recurrent neuron</em> receives the inputs <strong>x</strong><sub>(<em>t</em>)</sub> as well as its own output from the previous time step, <em>≈∑</em><sub>(<em>t</em>‚Äì1)</sub>. Since there is no previous output at the first time step, it is generally set to zero. We can represent this tiny network against the time axis (see <a data-type="xref" href="#simple_rnn_diagram">Figure¬†13-1</a>, right). This<a data-type="indexterm" data-primary="unrolling the network through time" id="id3060"/> is called <em>unrolling the network through time</em> (it‚Äôs the same recurrent neuron represented once per time step).</p>

<figure class="smallerseventy"><div id="simple_rnn_diagram" class="figure">
<img src="assets/hmls_1301.png" alt="Diagram of a recurrent neuron loop (left) and its unrolled representation over time (right), illustrating input and output flow across time steps." width="1093" height="476"/>
<h6><span class="label">Figure 13-1. </span>A recurrent neuron (left) unrolled through time (right)</h6>
</div></figure>

<p>You can easily create a layer of recurrent neurons. At each time step <em>t</em>, every neuron receives both the input vector <strong>x</strong><sub>(<em>t</em>)</sub> and the output vector from the previous time step <strong>≈∑</strong><sub>(<em>t</em>‚Äì1)</sub>, as shown in <a data-type="xref" href="#rnn_layer_diagram">Figure¬†13-2</a>. Note that both the inputs and outputs are now vectors (when there was just a single neuron, the output was a scalar).</p>

<figure class="width-85"><div id="rnn_layer_diagram" class="figure">
<img src="assets/hmls_1302.png" alt="Diagram illustrating a layer of recurrent neurons on the left and their unrolled representation through time on the right, showing input and output vectors at each time step." width="1407" height="543"/>
<h6><span class="label">Figure 13-2. </span>A layer of recurrent neurons (left) unrolled through time (right)</h6>
</div></figure>

<p>Each recurrent neuron has two sets of weights: one for the inputs <strong>x</strong><sub>(<em>t</em>)</sub> and the other for the outputs of the previous time step, <strong>≈∑</strong><sub>(<em>t</em>‚Äì1)</sub>. Let‚Äôs call these weight vectors <strong>w</strong><sub><em>x</em></sub> and <strong>w</strong><sub><em>≈∑</em></sub>. If we consider the whole recurrent layer instead of just one recurrent neuron, we can place all the weight vectors in two weight matrices: <strong>W</strong><sub><em>x</em></sub> and <strong>W</strong><sub><em>≈∑</em></sub>.</p>

<p>The output vector of the whole recurrent layer can then be computed pretty much as you might expect, as shown in <a data-type="xref" href="#rnn_output_equation">Equation 13-1</a>, where <strong>b</strong> is the bias vector and <em>œï</em>(¬∑) is the activation function<a data-type="indexterm" data-primary="hyperbolic tangent (htan)" id="id3061"/> (e.g., ReLU‚Å†<sup><a data-type="noteref" id="id3062-marker" href="ch13.html#id3062">1</a></sup>).</p>
<div id="rnn_output_equation" data-type="equation">
<h5><span class="label">Equation 13-1. </span>Output of a recurrent layer for a single instance</h5>
<math alttext="dollar-sign ModifyingAbove bold y With caret Subscript left-parenthesis t right-parenthesis Baseline equals phi left-parenthesis bold upper W Subscript x Baseline Superscript upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline plus bold upper W Subscript ModifyingAbove y With caret Baseline Superscript upper T Baseline ModifyingAbove bold y With caret Subscript left-parenthesis t minus 1 right-parenthesis Baseline plus bold b right-parenthesis dollar-sign">
  <mrow>
    <msub><mover accent="true"><mi>ùê≤</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
    <mo>=</mo>
    <mi>œï</mi>
    <mrow>
      <mo>(</mo>
      <msup><mrow><msub><mi>ùêñ</mi> <mi>x</mi> </msub></mrow> <mtext>T</mtext> </msup>
      <msub><mi>ùê±</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      <mo>+</mo>
      <msup><mrow><msub><mi>ùêñ</mi> <mover accent="true"><mi>y</mi> <mo>^</mo></mover> </msub></mrow> <mtext>T</mtext> </msup>
      <msub><mover accent="true"><mi>ùê≤</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
      <mo>+</mo>
      <mi>ùêõ</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Just as with feedforward neural networks, we can compute a recurrent layer‚Äôs output in one shot for an entire mini-batch by placing all the inputs at time step <em>t</em> into an input matrix <strong>X</strong><sub>(<em>t</em>)</sub> (see <a data-type="xref" href="#rnn_output_vectorized_equation">Equation 13-2</a>).</p>
<div id="rnn_output_vectorized_equation" data-type="equation"><h5><span class="label">Equation 13-2. </span>Outputs of a layer of recurrent neurons for all instances in a <span class="keep-together">mini-batch</span></h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">≈∂</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>œï</mi>
         <mn>(</mn>
            <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <msub><mi mathvariant="bold">W</mi> <mi>x</mi> </msub>
            <mo>+</mo>
            <msub><mi mathvariant="bold">≈∂</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
            <msub><mi mathvariant="bold">W</mi> <mi>≈∑</mi> </msub>
            <mo>+</mo>
            <mi mathvariant="bold">b</mi>
          <mn>)</mn>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>œï</mi>
          <mn>(</mn>
            <mn>[</mn>
              <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
              <mspace width="1.em"/>
              <msub><mi mathvariant="bold">≈∂</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
            <mn>]</mn>
            <mi mathvariant="bold">W</mi>
            <mo>+</mo>
            <mi mathvariant="bold">b</mi>
          <mn>)</mn>
          <mspace width="4.pt"/>
          <mtext>with</mtext>
          <mspace width="4.pt"/>
          <mi mathvariant="bold">W</mi>
          <mo>=</mo>
          <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msub><mi mathvariant="bold">W</mi> <mi>x</mi> </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub><mi mathvariant="bold">W</mi> <mi>≈∑</mi> </msub>
                </mtd>
              </mtr>
            </mtable>
        <mo>]</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p class="pagebreak-before">In this equation:</p>

<ul>
<li>
<p><strong>≈∂</strong><sub>(<em>t</em>)</sub> is an <em>m</em> √ó <em>n</em><sub>neurons</sub> matrix containing the layer‚Äôs outputs at time step <em>t</em> for each instance in the mini-batch (<em>m</em> is the number of instances in the mini-batch, and <em>n</em><sub>neurons</sub> is the number of neurons).</p>
</li>
<li>
<p><strong>X</strong><sub>(<em>t</em>)</sub> is an <em>m</em> √ó <em>n</em><sub>inputs</sub> matrix containing the inputs for all instances (<em>n</em><sub>inputs</sub> is the number of input features).</p>
</li>
<li>
<p><strong>W</strong><sub><em>x</em></sub> is an <em>n</em><sub>inputs</sub> √ó <em>n</em><sub>neurons</sub> matrix containing the connection weights for the inputs of the current time step.</p>
</li>
<li>
<p><strong>W</strong><sub><em>≈∑</em></sub> is an <em>n</em><sub>neurons</sub> √ó <em>n</em><sub>neurons</sub> matrix containing the connection weights for the outputs of the previous time step.</p>
</li>
<li>
<p><strong>b</strong> is a vector of size <em>n</em><sub>neurons</sub> containing each neuron‚Äôs bias term.</p>
</li>
<li>
<p>The weight matrices <strong>W</strong><sub><em>x</em></sub> and <strong>W</strong><sub><em>≈∑</em></sub> are often concatenated vertically into a single weight matrix <strong>W</strong> of shape (<em>n</em><sub>inputs</sub> + <em>n</em><sub>neurons</sub>) √ó <em>n</em><sub>neurons</sub> (see the second line of <a data-type="xref" href="#rnn_output_vectorized_equation">Equation 13-2</a>).</p>
</li>
<li>
<p>The notation [<strong>X</strong><sub>(<em>t</em>)</sub> <strong>≈∂</strong><sub>(<em>t</em>‚Äì1)</sub>] represents the horizontal concatenation of the matrices <strong>X</strong><sub>(<em>t</em>)</sub> and <strong>≈∂</strong><sub>(<em>t</em>‚Äì1)</sub>.</p>
</li>
</ul>

<p>Notice that <strong>≈∂</strong><sub>(<em>t</em>)</sub> is a function of <strong>X</strong><sub>(<em>t</em>)</sub> and <strong>≈∂</strong><sub>(<em>t</em>‚Äì1)</sub>, which is a function of <strong>X</strong><sub>(<em>t</em>‚Äì1)</sub> and <strong>≈∂</strong><sub>(<em>t</em>‚Äì2)</sub>, which is a function of <strong>X</strong><sub>(<em>t</em>‚Äì2)</sub> and <strong>≈∂</strong><sub>(<em>t</em>‚Äì3)</sub>, and so on. This makes <strong>≈∂</strong><sub>(<em>t</em>)</sub> a function of all the inputs since time <em>t</em> = 0 (that is, <strong>X</strong><sub>(0)</sub>, <strong>X</strong><sub>(1)</sub>, ‚Ä¶‚Äã, <strong>X</strong><sub>(<em>t</em>)</sub>). At the first time step, <em>t</em> = 0, there are no previous outputs, so they are typically assumed to be all zeros.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The idea of introducing backward connections (i.e., loops) in artificial neural networks<a data-type="indexterm" data-primary="loops in RNNs" id="id3063"/><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="origins of" id="id3064"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="origins of" id="id3065"/> dates back to the very origins of ANNs, but the first modern RNN architecture was <a href="https://homl.info/jordanrnn">proposed by Michael I. Jordan in 1986</a>.‚Å†<sup><a data-type="noteref" id="id3066-marker" href="ch13.html#id3066">2</a></sup> At each time step, his RNN would look at the inputs for that time step, plus its own outputs from the previous time step. This is called <em>output feedback</em>.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Memory Cells"><div class="sect2" id="id242">
<h2>Memory Cells</h2>

<p>Since the output of a recurrent neuron at time step <em>t</em> is a function of all the inputs from previous time steps, you could say it has a form of <em>memory</em>. A part of a neural network that preserves some state across time steps is called<a data-type="indexterm" data-primary="memory cells, RNNs" id="id3067"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="memory cells" id="id3068"/> a <em>memory cell</em> (or simply a <em>cell</em>). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell, capable of learning only short patterns (typically about 10 steps long, but this varies depending on the task). Later in this chapter, we will look at some more complex and powerful types of cells capable of learning longer patterns (roughly 10 times longer, but again, this depends on the task).</p>

<p>A cell‚Äôs state at time step <em>t</em>, denoted <strong>h</strong><sub>(<em>t</em>)</sub> (the ‚Äúh‚Äù stands for ‚Äúhidden‚Äù), is a function of some inputs at that time step and its state at the previous time step: <strong>h</strong><sub>(<em>t</em>)</sub> = <em>f</em>(<strong>x</strong><sub>(<em>t</em>)</sub>, <strong>h</strong><sub>(<em>t</em>‚Äì1)</sub>). Its output at time step <em>t</em>, denoted <strong>≈∑</strong><sub>(<em>t</em>)</sub>, is also a function of the previous state and the current inputs, and typically it‚Äôs just a function of the current state. In the case of the basic cells we have discussed so far, the output is just equal to the state, but in more complex cells this is not always the case, as shown in <a data-type="xref" href="#hidden_state_diagram">Figure¬†13-3</a>.</p>

<figure class="width-65"><div id="hidden_state_diagram" class="figure">
<img src="assets/hmls_1303.png" alt="Diagram illustrating how a cell's hidden state and output evolve over time, with feedback loops showing the interaction between inputs, hidden states, and outputs at different time steps." width="923" height="407"/>
<h6><span class="label">Figure 13-3. </span>A cell‚Äôs hidden state and its output may be different</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The first modern RNN that fed back the hidden state rather than the outputs was <a href="https://homl.info/elmanrnn">proposed by Jeffrey Elman in 1990</a>.‚Å†<sup><a data-type="noteref" id="id3069-marker" href="ch13.html#id3069">3</a></sup> This<a data-type="indexterm" data-primary="state feedback" id="id3070"/> is called <em>state feedback</em>, and it‚Äôs the most common approach today.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Input and Output Sequences"><div class="sect2" id="id243">
<h2>Input and Output Sequences</h2>

<p>An RNN can simultaneously take a sequence of inputs<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="input and output sequences" id="xi_recurrentneuralnetworksRNNsinputandoutputsequences1314152_1"/><a data-type="indexterm" data-primary="input and output sequences, RNNs" id="xi_inputandoutputsequencesRNNs1314152_1"/> and produce a sequence of outputs (see the top-left network in <a data-type="xref" href="#seq_to_seq_diagram">Figure¬†13-4</a>). This type of <em>sequence-to-sequence network</em> is useful<a data-type="indexterm" data-primary="sequence-to-sequence (seq2seq) network" id="id3071"/> to forecast time series<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="with sequence-to-sequence model" data-secondary-sortas="sequence-to-sequence model" id="id3072"/>, such as your home‚Äôs daily power consumption: you feed it the data over the last <em>N</em> days, and you train it to output the power consumption shifted by one day into the future (i.e., from <em>N</em>¬†‚Äì¬†1 days ago to tomorrow).</p>

<p>Alternatively, you could feed the network a sequence of inputs and ignore all outputs except for the last one (see the top-right network in <a data-type="xref" href="#seq_to_seq_diagram">Figure¬†13-4</a>). This<a data-type="indexterm" data-primary="sequence-to-vector network" id="id3073"/> is a <em>sequence-to-vector network</em>. For example, you could feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score (e.g., from 0 [hate] to 1 [love]).</p>

<p>Conversely, you could feed the network the same input vector over and over again at each time step and let it output a sequence (see the bottom-left network of <a data-type="xref" href="#seq_to_seq_diagram">Figure¬†13-4</a>). This<a data-type="indexterm" data-primary="vector-to-sequence network" id="id3074"/> is a <em>vector-to-sequence network</em>. For example, the input could be an image (or the output of a CNN), and the output could be a caption for that image.</p>

<figure class="width-90"><div id="seq_to_seq_diagram" class="figure">
<img src="assets/hmls_1304.png" alt="Diagram illustrating different neural network architectures: sequence-to-sequence, sequence-to-vector, vector-to-sequence, and encoder-decoder models." width="1425" height="925"/>
<h6><span class="label">Figure 13-4. </span>Sequence-to-sequence (top left), sequence-to-vector (top right), vector-to-sequence (bottom left), and encoder-decoder (bottom right) networks</h6>
</div></figure>

<p>Lastly, you could have a sequence-to-vector network, called an <em>encoder</em>, followed by a vector-to-sequence network<a data-type="indexterm" data-primary="encoder-decoder models" id="id3075"/>, called a <em>decoder</em> (see the bottom-right network of <a data-type="xref" href="#seq_to_seq_diagram">Figure¬†13-4</a>). For example, this could be used for translating a sentence from one language to another. You would feed the network a sentence in one language, the encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language. This two-step model, called an <a href="https://homl.info/seq2seq"><em>encoder-decoder</em></a>,‚Å†<sup><a data-type="noteref" id="id3076-marker" href="ch13.html#id3076">4</a></sup> works much better than trying to translate on the fly with a single sequence-to-sequence RNN (like the one represented at the top left): the last words of a sentence can affect the first words of the translation, so you need to wait until you have seen the whole sentence before translating it. We will go through the implementation of an encoder-decoder in <a data-type="xref" href="ch14.html#nlp_chapter">Chapter¬†14</a> (as you will see, it is a bit more complex than what <a data-type="xref" href="#seq_to_seq_diagram">Figure¬†13-4</a> suggests).</p>

<p>This versatility sounds promising, but how do you train a recurrent neural network<a data-type="indexterm" data-startref="xi_recurrentneuralnetworksRNNsinputandoutputsequences1314152_1" id="id3077"/><a data-type="indexterm" data-startref="xi_inputandoutputsequencesRNNs1314152_1" id="id3078"/>?</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Training RNNs"><div class="sect1" id="id244">
<h1>Training RNNs</h1>

<p>To train an RNN, the trick is to unroll it through time (like we just did) and then use regular backpropagation<a data-type="indexterm" data-primary="backpropagation through time (BPTT)" id="id3079"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="training" id="id3080"/><a data-type="indexterm" data-primary="BPTT (backpropagation through time)" id="id3081"/> (see <a data-type="xref" href="#bptt_diagram">Figure¬†13-5</a>). This strategy is called <em>backpropagation through time</em> (BPTT).</p>

<p>Just like in regular backpropagation, there is a first forward pass through the unrolled network (represented by the dashed arrows). Then the output sequence is evaluated<a data-type="indexterm" data-primary="loss functions" data-secondary="in RNN training" data-secondary-sortas="RNN training" id="id3082"/> using a loss function ‚Ñí(<strong>Y</strong><sub>(0)</sub>, <strong>Y</strong><sub>(1)</sub>, ‚Ä¶‚Äã, <strong>Y</strong><sub>(<em>T</em>)</sub>; <strong>≈∂</strong><sub>(0)</sub>, <strong>≈∂</strong><sub>(1)</sub>, ‚Ä¶‚Äã, <strong>≈∂</strong><sub>(<em>T</em>)</sub>) (where <strong>Y</strong><sub>(<em>i</em>)</sub> is the <em>i</em><sup>th</sup> target, <strong>≈∂</strong><sub>(<em>i</em>)</sub> is the <em>i</em><sup>th</sup> prediction, and <em>T</em> is the max time step). Note that this loss function may ignore some outputs. For example, in a sequence-to-vector RNN, all outputs are ignored except for the very last one. In <a data-type="xref" href="#bptt_diagram">Figure¬†13-5</a>, the loss function is computed based on the last three outputs only. The gradients of that loss function are then propagated backward through the unrolled network (represented by the solid arrows). In this example, since the outputs <strong>≈∂</strong><sub>(0)</sub> and <strong>≈∂</strong><sub>(1)</sub> are not used to compute the loss, the gradients do not flow backward through them; they only flow through <strong>≈∂</strong><sub>(2)</sub>, <strong>≈∂</strong><sub>(3)</sub>, and <strong>≈∂</strong><sub>(4)</sub>. Moreover, since the same parameters <strong>W</strong> and <strong>b</strong> are used at each time step, their gradients will be tweaked multiple times during backprop. Once the backward phase is complete and all the gradients have been computed, BPTT can perform a gradient descent step to update the parameters (this is no different from regular backprop).</p>

<figure class="width-60"><div id="bptt_diagram" class="figure">
<img src="assets/hmls_1305.png" alt="Diagram illustrating backpropagation through time in an unrolled RNN, showing forward and backward passes with gradients flowing only through certain outputs for loss computation." width="708" height="567"/>
<h6><span class="label">Figure 13-5. </span>Backpropagation through time</h6>
</div></figure>

<p>Fortunately, PyTorch takes care of all of this complexity for you, as you will see. But before we get there, let‚Äôs load a time series and start analyzing it using classical tools to better understand what we‚Äôre dealing with, and to get some baseline metrics.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Forecasting a Time Series"><div class="sect1" id="id245">
<h1>Forecasting a Time Series</h1>

<p>All right! Let‚Äôs pretend you‚Äôve just been hired as a data scientist by Chicago‚Äôs Transit Authority<a data-type="indexterm" data-primary="time series data, forecasting" id="id3083"/>. Your first task is to build a model capable of forecasting the number of passengers that will ride on bus and rail the next day. You have access to daily ridership data since 2001. Let‚Äôs walk through how you would handle this. We‚Äôll start by loading and cleaning up the data:<sup><a data-type="noteref" id="id3084-marker" href="ch13.html#id3084">5</a></sup></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">from</code> <code class="nn">pathlib</code> <code class="kn">import</code> <code class="n">Path</code>

<code class="n">path</code> <code class="o">=</code> <code class="n">Path</code><code class="p">(</code><code class="s2">"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv"</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">path</code><code class="p">,</code> <code class="n">parse_dates</code><code class="o">=</code><code class="p">[</code><code class="s2">"service_date"</code><code class="p">])</code>
<code class="n">df</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"date"</code><code class="p">,</code> <code class="s2">"day_type"</code><code class="p">,</code> <code class="s2">"bus"</code><code class="p">,</code> <code class="s2">"rail"</code><code class="p">,</code> <code class="s2">"total"</code><code class="p">]</code>  <code class="c1"># shorter names</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s2">"date"</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"date"</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"total"</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># no need for total, it's just bus + rail</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop_duplicates</code><code class="p">()</code>  <code class="c1"># remove duplicated months (2011-10 and 2014-07)</code></pre>

<p>We load the CSV file, set short column names, sort the rows by date, remove the redundant <code translate="no">total</code> column, and drop duplicate rows. Now let‚Äôs check what the first few rows look like:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>
<code class="go">           day_type     bus    rail</code>
<code class="go">date</code>
<code class="go">2001-01-01        U  297192  126455</code>
<code class="go">2001-01-02        W  780827  501952</code>
<code class="go">2001-01-03        W  824923  536432</code>
<code class="go">2001-01-04        W  870021  550011</code>
<code class="go">2001-01-05        W  890426  557917</code></pre>

<p>On January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded a train. The <code translate="no">day_type</code> column contains <code translate="no">W</code> for <strong>W</strong>eekdays, <code translate="no">A</code> for S<strong>a</strong>turdays, and <code translate="no">U</code> for S<strong>u</strong>ndays or holidays.</p>

<p>Now let‚Äôs plot the bus and rail ridership figures over a few months in 2019, to see what it looks like (see <a data-type="xref" href="#daily_ridership_plot">Figure¬†13-6</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="n">df</code><code class="p">[</code><code class="s2">"2019-03"</code><code class="p">:</code><code class="s2">"2019-05"</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s2">"."</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="daily_ridership_plot" class="figure">
<img src="assets/hmls_1306.png" alt="Line graph showing daily bus and rail ridership in Chicago from March to May 2019, illustrating regular peaks and troughs over time." width="2274" height="911"/>
<h6><span class="label">Figure 13-6. </span>Daily ridership in Chicago</h6>
</div></figure>

<p>Note that Pandas includes both the start and end month in the range, so this plots the data from the 1st of March all the way up to the 31st of May. This is a <em>time series</em>: data with values at different time steps, usually at regular intervals. More specifically, since there are multiple values per time step, this is called<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="multivariate time series" id="id3085"/><a data-type="indexterm" data-primary="multivariate time series" id="id3086"/> a <em>multivariate time series</em>. If we only looked at the <code translate="no">bus</code> column, it would<a data-type="indexterm" data-primary="univariate time series" id="id3087"/> be a <em>univariate time series</em>, with a single value per time step. Predicting future values (i.e., forecasting) is the most typical task when dealing with time series, and this is what we will focus on in this chapter. Other tasks include imputation (filling in missing past values), classification, anomaly detection, and more.</p>

<p>Looking at <a data-type="xref" href="#daily_ridership_plot">Figure¬†13-6</a>, we can see that a similar pattern is clearly repeated every week. This is called<a data-type="indexterm" data-primary="seasonality, time series modeling" id="id3088"/> a weekly <em>seasonality</em>. In fact, it‚Äôs so strong in this case that forecasting tomorrow‚Äôs ridership by just copying the values from a week earlier will yield reasonably good results. This is called <em>naive forecasting</em>: simply copying a past value to make our forecast<a data-type="indexterm" data-primary="naive forecasting" id="id3089"/>. Naive forecasting is often a great baseline, and it can even be tricky to beat in some cases.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In general, naive forecasting means copying the latest known value (e.g., forecasting that tomorrow will be the same as today). However, in our case, copying the value from the previous week works better, due to the strong weekly seasonality.</p>
</div>

<p>To visualize these naive forecasts, let‚Äôs overlay the two time series (for bus and rail) as well as the same time series lagged by one week (i.e., shifted toward the right) using dotted lines. We‚Äôll also plot the difference between the two (i.e., the value at time <em>t</em> minus the value at time <em>t</em>¬†‚Äì¬†7); this<a data-type="indexterm" data-primary="differencing, time series forecasting" id="id3090"/> is called <em>differencing</em> (see <a data-type="xref" href="#differencing_plot">Figure¬†13-7</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="n">diff_7</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s2">"bus"</code><code class="p">,</code> <code class="s2">"rail"</code><code class="p">]]</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="mi">7</code><code class="p">)[</code><code class="s2">"2019-03"</code><code class="p">:</code><code class="s2">"2019-05"</code><code class="p">]</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">axs</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">sharex</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">df</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">axs</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">legend</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s2">"."</code><code class="p">)</code>  <code class="c1"># original time series</code>
<code class="n">df</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="mi">7</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">axs</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">legend</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s2">":"</code><code class="p">)</code>  <code class="c1"># lagged</code>
<code class="n">diff_7</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">axs</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s2">"."</code><code class="p">)</code>  <code class="c1"># 7-day difference time series</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="differencing_plot" class="figure">
<img src="assets/hmls_1307.png" alt="Two-panel plot showing the original and 7-day lagged time series for bus and rail (top), and the differences between them (bottom), demonstrating autocorrelation patterns." width="2272" height="1361"/>
<h6><span class="label">Figure 13-7. </span>Time series overlaid with 7-day lagged time series (top), and difference between <em>t</em> and <em>t</em>¬†‚Äì¬†7 (bottom)</h6>
</div></figure>

<p>Not too bad! Notice how closely the lagged time series track the actual time series. When a time series is correlated with a lagged version of itself, we say that the time series<a data-type="indexterm" data-primary="autocorrelated time series" id="id3091"/> is <em>autocorrelated</em>. As you can see, most of the differences are fairly small, except at the end of May. Maybe there was a holiday at that time? Let‚Äôs check the <code translate="no">day_type</code> column:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">list</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="s2">"2019-05-25"</code><code class="p">:</code><code class="s2">"2019-05-27"</code><code class="p">][</code><code class="s2">"day_type"</code><code class="p">])</code>
<code class="go">['A', 'U', 'U']</code></pre>

<p>Indeed, there was a long weekend back then: the Monday was the Memorial Day holiday. We could use this column to improve our forecasts, but for now let‚Äôs just measure the mean absolute error over the three-month period we‚Äôre arbitrarily focusing on‚ÄîMarch, April, and May 2019‚Äîto get a rough idea:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">diff_7</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="go">bus     43915.608696</code>
<code class="go">rail    42143.271739</code>
<code class="go">dtype: float64</code></pre>

<p>Our naive forecasts<a data-type="indexterm" data-primary="naive forecasting" id="id3092"/> get a mean absolute error (MAE) of about 43,916 bus riders, and about 42,143 rail riders. It‚Äôs hard to tell at a glance how good or bad this is, so let‚Äôs put the forecast errors into perspective by dividing them by the target values:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">targets</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s2">"bus"</code><code class="p">,</code> <code class="s2">"rail"</code><code class="p">]][</code><code class="s2">"2019-03"</code><code class="p">:</code><code class="s2">"2019-05"</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="p">(</code><code class="n">diff_7</code> <code class="o">/</code> <code class="n">targets</code><code class="p">)</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="go">bus     0.082938</code>
<code class="go">rail    0.089948</code>
<code class="go">dtype: float64</code></pre>

<p>What we just computed is called the <em>mean absolute percentage error</em> (MAPE)<a data-type="indexterm" data-primary="mean absolute percentage error (MAPE)" id="id3093"/><a data-type="indexterm" data-primary="MAPE (mean absolute percentage error)" id="id3094"/>. It looks like our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0% for rail. It‚Äôs interesting to note that the MAE for the rail forecasts looks slightly better than the MAE for the bus forecasts, while the opposite is true for the MAPE. That‚Äôs because the bus ridership is larger than the rail ridership, so naturally the forecast errors are also larger, but when we put the errors into perspective, it turns out that the bus forecasts are actually slightly better than the rail forecasts.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The MAE, MAPE, and mean squared error (MSE) are among the most common metrics you can use to evaluate your forecasts. As always, choosing the right metric depends on the task. For example, if your project suffers quadratically more from large errors than from small ones, then the MSE may be preferable, as it strongly penalizes large errors.</p>
</div>

<p>Looking at the time series, there doesn‚Äôt appear to be any significant monthly seasonality, but let‚Äôs check whether there‚Äôs any yearly seasonality. We‚Äôll look at the data from 2001 to 2019. To reduce the risk of data snooping, we‚Äôll ignore more recent data for now. Let‚Äôs also plot a 12-month rolling average for each series to visualize long-term trends (see <a data-type="xref" href="#long_term_ridership_plot">Figure¬†13-8</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">period</code> <code class="o">=</code> <code class="nb">slice</code><code class="p">(</code><code class="s2">"2001"</code><code class="p">,</code> <code class="s2">"2019"</code><code class="p">)</code>
<code class="n">df_monthly</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">select_dtypes</code><code class="p">(</code><code class="n">include</code><code class="o">=</code><code class="s2">"number"</code><code class="p">)</code><code class="o">.</code><code class="n">resample</code><code class="p">(</code><code class="s1">'ME'</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">rolling_average_12_months</code> <code class="o">=</code> <code class="n">df_monthly</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">period</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">window</code><code class="o">=</code><code class="mi">12</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">df_monthly</code><code class="p">[</code><code class="n">period</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s2">"."</code><code class="p">)</code>
<code class="n">rolling_average_12_months</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">legend</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="long_term_ridership_plot" class="figure">
<img src="assets/hmls_1308.png" alt="Line graph showing monthly data from 2001 to 2019 of bus (blue) and rail (orange) services with marked yearly seasonality and long-term trends, including a 12-month rolling average." width="2274" height="1060"/>
<h6><span class="label">Figure 13-8. </span>Yearly seasonality and long-term trends</h6>
</div></figure>

<p>Yep! There‚Äôs definitely some yearly seasonality as well, although it is noisier than the weekly seasonality, and more visible for the rail series than the bus series: we see peaks and troughs at roughly the same dates each year. Let‚Äôs check what we get if we plot the 12-month difference (see <a data-type="xref" href="#yearly_diff_plot">Figure¬†13-9</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">df_monthly</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="mi">12</code><code class="p">)[</code><code class="n">period</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">grid</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s2">"."</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="yearly_diff_plot" class="figure">
<img src="assets/hmls_1309.png" alt="Line graph showing the 12-month difference in data trends for bus and rail from 2001 to 2019, highlighting fluctuations and removal of seasonality and trends." width="2272" height="763"/>
<h6><span class="label">Figure 13-9. </span>The 12-month difference</h6>
</div></figure>

<p>Notice how differencing not only removed the yearly seasonality, but it also removed the long-term trends. For example, the linear downward trend present in the time series from 2016 to 2019 became a roughly constant negative value in the differenced time series. In fact, differencing<a data-type="indexterm" data-primary="differencing, time series forecasting" id="id3095"/> is a common technique used to remove trend and seasonality from a time series: it‚Äôs easier to study a <em>stationary</em> time series<a data-type="indexterm" data-primary="stationary time series" id="id3096"/>, meaning one whose statistical properties remain the same over time, without any seasonality or trends. Once you‚Äôre able to make accurate forecasts on the differenced time series, it‚Äôs easy to turn them into forecasts for the actual time series by just adding back the past values that were previously subtracted.</p>

<p>You may be thinking that we‚Äôre only trying to predict tomorrow‚Äôs ridership, so the long-term patterns matter much less than the short-term ones. You‚Äôre right, but still, we may be able to improve performance slightly by taking long-term patterns into account. For example, daily bus ridership dropped by about 2,500 in October 2017, which represents about 570 fewer passengers each week, so if we were at the end of October 2017, it would make sense to forecast tomorrow‚Äôs ridership by copying the value from last week, minus 570. Accounting for the trend will make your forecasts a bit more accurate on average.</p>

<p>Now that you‚Äôre familiar with the ridership time series, as well as some of the most important concepts in time series analysis, including seasonality, trend, differencing, and moving averages, let‚Äôs take a quick look at a very popular family of statistical models that are commonly used to analyze time series.</p>








<section data-type="sect2" data-pdf-bookmark="The ARMA Model Family"><div class="sect2" id="id246">
<h2>The ARMA Model Family</h2>

<p>We‚Äôll start with the <em>autoregressive moving average</em> (ARMA) model<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="ARMA model family" id="xi_timeseriesdataforecastingARMAmodelfamily1331066_1"/><a data-type="indexterm" data-primary="ARMA model family" id="xi_ARMAmodelfamily1331066_1"/><a data-type="indexterm" data-primary="autoregressive moving average (ARMA) model family" id="xi_autoregressivemovingaverageARMAmodelfamily1331066_1"/>, developed by Herman Wold in the 1930s: it computes its forecasts using a simple weighted sum of lagged values and corrects these forecasts by adding a moving average, very much like we just discussed. Specifically, the moving average component is computed using a weighted sum of the last few forecast errors. <a data-type="xref" href="#arma_equation">Equation 13-3</a> shows how the model makes its forecasts.</p>
<div data-type="equation" id="arma_equation">
<h5><span class="label">Equation 13-3. </span>Forecasting using an ARMA model</h5>
<math display="block"><mtable columnalign="left"><mtr><mtd><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>Œ±</mi><mi>i</mi></msub><mo>‚Ää</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>+</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><msub><mi>Œ∏</mi><mi>i</mi></msub><mo>‚Ää</mo><msub><mi>Œµ</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd></mtr><mtr><mtd><mtext>with¬†</mtext><msub><mi>Œµ</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>-</mo><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd></mtr></mtable></math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>≈∑</em><sub>(<em>t</em>)</sub> is the model‚Äôs forecast for time step <em>t</em>.</p>
</li>
<li>
<p><em>y</em><sub>(<em>t</em>)</sub> is the time series‚Äô value at time step <em>t</em>.</p>
</li>
<li>
<p>The first sum is the weighted sum of the past <em>p</em> values of the time series, using the learned weights <em>Œ±</em><sub><em>i</em></sub>. The number <em>p</em> is a hyperparameter, and it determines how far back into the past the model should look. This sum is the <em>autoregressive</em> component of the model<a data-type="indexterm" data-primary="autoregressive model" id="id3097"/>: it performs regression based on past values.</p>
</li>
<li>
<p>The second sum is the weighted sum over the past <em>q</em> forecast errors <em>Œµ</em><sub>(<em>t</em>)</sub>, using the learned weights <em>Œ∏</em><sub><em>i</em></sub>. The number <em>q</em> is a hyperparameter. This sum is the moving average component<a data-type="indexterm" data-primary="moving average component of model" id="id3098"/> of the model.</p>
</li>
</ul>

<p>Importantly, this model assumes that the time series is stationary. If it is not, then differencing may help. Using differencing over a single time step will produce an approximation of the derivative of the time series: indeed, it will give the slope of the series at each time step. This means that it will eliminate any linear trend, transforming it into a constant value. For example, if you apply one-step differencing to the series [3, 5, 7, 9, 11], you get the differenced series [2, 2, 2, 2].</p>

<p>If the original time series has a quadratic trend instead of a linear trend, then a single round of differencing will not be enough. For example, the series [1, 4, 9, 16, 25, 36] becomes [3, 5, 7, 9, 11] after one round of differencing, but if you run differencing for a second round, then you get [2, 2, 2, 2]. So, running two rounds of differencing will eliminate quadratic trends. More generally, running <em>d</em> consecutive rounds of differencing computes an approximation of the <em>d</em><sup>th</sup> order derivative of the time series, so it will eliminate polynomial trends up to degree <em>d</em>. This hyperparameter <em>d</em> is called<a data-type="indexterm" data-primary="order of integration (d) hyperparameter" id="id3099"/> the <em>order of integration</em>.</p>

<p>Differencing<a data-type="indexterm" data-primary="differencing, time series forecasting" id="id3100"/> is the central contribution of the <em>autoregressive integrated moving average</em> (ARIMA) model<a data-type="indexterm" data-primary="autoregressive integrated moving average (ARIMA) model" id="xi_autoregressiveintegratedmovingaverageARIMAmodel13330105_1"/><a data-type="indexterm" data-primary="ARIMA model" id="xi_ARIMAmodel13330105_1"/>, introduced in 1970 by George Box and Gwilym Jenkins in their book <em>Time Series Analysis</em> (Wiley). This model runs <em>d</em> rounds of differencing to make the time series more stationary, then it applies a regular ARMA model. When making forecasts, it uses this ARMA model, then it adds back the terms that were subtracted by differencing.</p>

<p>One last member of the ARMA family is the <em>seasonal ARIMA</em> (SARIMA) model<a data-type="indexterm" data-primary="SARIMA (seasonal ARIMA) model" id="xi_SARIMAseasonalARIMAmodel1333274_1"/><a data-type="indexterm" data-primary="seasonal ARIMA model (SARIMA)" id="xi_seasonalARIMAmodelSARIMA1333274_1"/>: it models the time series in the same way as ARIMA, but it additionally models a seasonal component for a given frequency (e.g., weekly), using the exact same ARIMA approach. It has a total of seven hyperparameters: the same <em>p</em>, <em>d</em>, and <em>q</em> hyperparameters as ARIMA; plus additional <em>P</em>, <em>D</em>, and <em>Q</em> hyperparameters to model the seasonal pattern; and lastly the period of the seasonal pattern, denoted <em>s</em>. The hyperparameters <em>P</em>, <em>D</em>, and <em>Q</em> are just like <em>p</em>, <em>d</em>, and <em>q</em>, but they are used to model the time series at <em>t</em>¬†‚Äì¬†<em>s</em>, <em>t</em>¬†‚Äì¬†2<em>s</em>, <em>t</em>¬†‚Äì¬†3<em>s</em>, etc.</p>

<p>Let‚Äôs see how to fit a SARIMA model to the rail time series, and use it to make a forecast for tomorrow‚Äôs ridership. We‚Äôll pretend today is the last day of May 2019, and we want to forecast the rail ridership for ‚Äútomorrow‚Äù, the 1st of June, 2019. For this, we can use the <code translate="no">statsmodels</code> library<a data-type="indexterm" data-primary="statsmodels library" id="id3101"/>, which contains many different statistical models, including the ARMA model and its variants, implemented by the <code translate="no">ARIMA</code> class:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">statsmodels.tsa.arima.model</code> <code class="kn">import</code> <code class="n">ARIMA</code>

<code class="n">origin</code><code class="p">,</code> <code class="n">today</code> <code class="o">=</code> <code class="s2">"2019-01-01"</code><code class="p">,</code> <code class="s2">"2019-05-31"</code>
<code class="n">rail_series</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">origin</code><code class="p">:</code><code class="n">today</code><code class="p">][</code><code class="s2">"rail"</code><code class="p">]</code><code class="o">.</code><code class="n">asfreq</code><code class="p">(</code><code class="s2">"D"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ARIMA</code><code class="p">(</code><code class="n">rail_series</code><code class="p">,</code>
              <code class="n">order</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code>
              <code class="n">seasonal_order</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">7</code><code class="p">))</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">forecast</code><code class="p">()</code>  <code class="c1"># returns 427,758.6</code></pre>

<p>In this code example:</p>

<ul>
<li>
<p>We start by importing the <code translate="no">ARIMA</code> class, then we take the rail ridership data from the start of 2019 up to ‚Äútoday‚Äù, and we use <code translate="no">asfreq("D")</code> to set the time series‚Äô frequency to daily. This doesn‚Äôt change the data at all in this case, since it‚Äôs already daily, but without this the <code translate="no">ARIMA</code> class would have to guess the frequency, and it would display a warning.</p>
</li>
<li>
<p>Next, we create an <code translate="no">ARIMA</code> instance, passing it all the data until ‚Äútoday‚Äù, and we set the model hyperparameters: <code translate="no">order=(1, 0, 0)</code> means that <em>p</em> = 1, <em>d</em> = 0, and 
<span class="keep-together"><em>q</em> = 0;</span> and <code translate="no">seasonal_order=(0, 1, 1, 7)</code> means that <em>P</em> = 0, <em>D</em> = 1, <em>Q</em> = 1, and 
<span class="keep-together"><em>s</em> = 7.</span> Notice that the <code translate="no">statsmodels</code> API differs a bit from Scikit-Learn‚Äôs API, since we pass the data to the model at construction time, instead of passing it to the 
<span class="keep-together"><code translate="no">fit()</code></span> method.</p>
</li>
<li>
<p>Next, we fit the model, and we use it to make a forecast for ‚Äútomorrow‚Äù, the 1st of June, 2019.</p>
</li>
</ul>

<p>The forecast is 427,759 passengers, when in fact there were 379,044. Yikes, we‚Äôre 12.9% off‚Äîthat‚Äôs pretty bad. It‚Äôs actually slightly worse than naive forecasting<a data-type="indexterm" data-primary="naive forecasting" id="id3102"/>, which forecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day? To check this, we can run the same code in a loop to make forecasts for every day in March, April, and May, and compute the MAE over that period:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">origin</code><code class="p">,</code> <code class="n">start_date</code><code class="p">,</code> <code class="n">end_date</code> <code class="o">=</code> <code class="s2">"2019-01-01"</code><code class="p">,</code> <code class="s2">"2019-03-01"</code><code class="p">,</code> <code class="s2">"2019-05-31"</code>
<code class="n">time_period</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">date_range</code><code class="p">(</code><code class="n">start_date</code><code class="p">,</code> <code class="n">end_date</code><code class="p">)</code>
<code class="n">rail_series</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">origin</code><code class="p">:</code><code class="n">end_date</code><code class="p">][</code><code class="s2">"rail"</code><code class="p">]</code><code class="o">.</code><code class="n">asfreq</code><code class="p">(</code><code class="s2">"D"</code><code class="p">)</code>
<code class="n">y_preds</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">today</code> <code class="ow">in</code> <code class="n">time_period</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">):</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">ARIMA</code><code class="p">(</code><code class="n">rail_series</code><code class="p">[</code><code class="n">origin</code><code class="p">:</code><code class="n">today</code><code class="p">],</code>  <code class="c1"># train on data up to "today"</code>
                  <code class="n">order</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code>
                  <code class="n">seasonal_order</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">7</code><code class="p">))</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>  <code class="c1"># note that we retrain the model every day!</code>
    <code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">forecast</code><code class="p">()</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">y_preds</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">y_pred</code><code class="p">)</code>

<code class="n">y_preds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">y_preds</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">time_period</code><code class="p">)</code>
<code class="n">mae</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_preds</code> <code class="o">-</code> <code class="n">rail_series</code><code class="p">[</code><code class="n">time_period</code><code class="p">])</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>  <code class="c1"># returns 32,040.7</code></pre>

<p>Ah, that‚Äôs much better! The MAE is about 32,041, which is significantly lower than the MAE we got with naive forecasting (42,143). So although the model is not perfect, it still beats naive forecasting by a large margin, on average.</p>

<p>At this point, you may be wondering how to pick good hyperparameters for the SARIMA model. There are several methods, but the simplest to understand and to get started with is the brute-force approach: just run a grid search. For each model you want to evaluate (i.e., each hyperparameter combination), you can run the preceding code example, changing only the hyperparameter values. Good <em>p</em>, <em>q</em>, <em>P</em>, and <em>Q</em> values are usually fairly small (typically 0 to 2, sometimes up to 5 or 6), and <em>d</em> and <em>D</em> are typically 0 or 1, sometimes 2. As for <em>s</em>, it‚Äôs just the main seasonal pattern‚Äôs period: in our case it‚Äôs 7 since there‚Äôs a strong weekly seasonality. The model with the lowest MAE wins. Of course, you can replace the MAE with another metric if it better matches your business objective. And that‚Äôs it!</p>
<div data-type="tip"><h6>Tip</h6>
<p>There are other more principled approaches to selecting good hyperparameters, based on analyzing<a data-type="indexterm" data-primary="autocorrelation function (ACF)" id="id3103"/><a data-type="indexterm" data-primary="ACF (autocorrelation function)" id="id3104"/><a data-type="indexterm" data-primary="partial autocorrelation function (PACF)" id="id3105"/><a data-type="indexterm" data-primary="PACF (partial autocorrelation function)" id="id3106"/> the <em>autocorrelation function</em> (ACF) and <em>partial autocorrelation function</em> (PACF),‚Å†<sup><a data-type="noteref" id="id3107-marker" href="ch13.html#id3107">6</a></sup> or minimizing the AIC or BIC metrics (introduced in <a data-type="xref" href="ch08.html#unsupervised_learning_chapter">Chapter¬†8</a>) to penalize models that use too many parameters and reduce the risk of overfitting the data, but grid search is a good place to start.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Preparing the Data for Machine Learning Models"><div class="sect2" id="id247">
<h2>Preparing the Data for Machine Learning Models</h2>

<p>Now that we have two baselines<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="data preparation for ML models" id="xi_timeseriesdataforecastingdatapreparationforMLmodels1338231_1"/><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="preparing data for ML models" id="xi_machinelearningMLpreparingdataforMLmodels1338231_1"/>, naive forecasting and SARIMA, let‚Äôs try to use the machine learning models we‚Äôve covered so far to forecast this time series, starting with a basic linear model. Our goal will be to forecast tomorrow‚Äôs ridership based on the ridership of the past 8 weeks of data (56 days). The inputs to our model will therefore be sequences (usually a single sequence per day once the model is in production), each containing 56 values from time steps <em>t</em>¬†‚Äì¬†55 to <em>t</em>. For each input sequence, the model will output a single value: the forecast for time step <em>t</em>¬†+¬†1.</p>

<p>But what will we use as training data? Well, here‚Äôs the trick: we will use every 56-day window from the past as training data, and the target for each window will be the value immediately following it. To do that, we need to create a custom dataset that will chop a given time series into all possible windows of a given length, each with its corresponding target:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">TimeSeriesDataset</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">series</code><code class="p">,</code> <code class="n">window_length</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">series</code> <code class="o">=</code> <code class="n">series</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code> <code class="o">=</code> <code class="n">window_length</code>

    <code class="k">def</code> <code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">)</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code>

    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="k">if</code> <code class="n">idx</code> <code class="o">&gt;=</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
            <code class="k">raise</code> <code class="ne">IndexError</code><code class="p">(</code><code class="s2">"dataset index out of range"</code><code class="p">)</code>
        <code class="n">end</code> <code class="o">=</code> <code class="n">idx</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code>  <code class="c1"># 1st index after window</code>
        <code class="n">window</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">[</code><code class="n">idx</code> <code class="p">:</code> <code class="n">end</code><code class="p">]</code>
        <code class="n">target</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">[</code><code class="n">end</code><code class="p">]</code>
        <code class="k">return</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code></pre>

<p>Let‚Äôs test this class by applying it to a simple time series containing the numbers 0 to 5. We could represent this series in 1D using [0, 1, 2, 3, 4, 5], but the RNN modules expect each sequence to be 2D, with a shape of [<em>sequence length</em>, <em>dimensionality</em>]. For univariate time series, the dimensionality is simply 1, so we represent the time series as [[0], [1], [2], [3], [4], [5]]. In the following code below, the <code translate="no">TimeSeriesDataset</code> contains all the windows of length 3, each with its corresponding target (i.e., the first value after the window):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">my_series</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="mi">4</code><code class="p">],</code> <code class="p">[</code><code class="mi">5</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">my_dataset</code> <code class="o">=</code> <code class="n">TimeSeriesDataset</code><code class="p">(</code><code class="n">my_series</code><code class="p">,</code> <code class="n">window_length</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code> <code class="ow">in</code> <code class="n">my_dataset</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="s2">"Window:"</code><code class="p">,</code> <code class="n">window</code><code class="p">,</code> <code class="s2">" Target:"</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">Window: tensor([[0], [1], [2]])  Target: tensor([3])</code>
<code class="go">Window: tensor([[1], [2], [3]])  Target: tensor([4])</code>
<code class="go">Window: tensor([[2], [3], [4]])  Target: tensor([5])</code></pre>

<p>It looks like our <code translate="no">TimeSeriesDataset</code> class works fine! Now we can create a <code translate="no">DataLoader</code> for this tiny dataset<a data-type="indexterm" data-primary="torch" data-secondary="utils.data.DataLoader()" id="id3108"/><a data-type="indexterm" data-primary="DataLoader(), PyTorch" id="id3109"/>, shuffling the windows and grouping them into batches of two:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">my_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">my_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">my_loader</code><code class="p">:</code>
<code class="gp">... </code>    <code class="nb">print</code><code class="p">(</code><code class="s2">"X:"</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="s2">" y:"</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">X: tensor([[[0], [1], [2]], [[2], [3], [4]]])  y: tensor([[3], [5]])</code>
<code class="go">X: tensor([[[1], [2], [3]]])  y: tensor([[4]])</code></pre>

<p>The first batch contains the windows [[0], [1], [2]] and [[2], [3], [4]], along with their respective targets [3] and [5]; and the second batch contains only one window [[1], [2], [3]], with its target [4]. Indeed, when the length of a dataset is not a multiple of the batch size, the last batch is shorter.</p>

<p>OK, now that we have a way to convert a time series into a dataset that we can use to train ML models, let‚Äôs go ahead and prepare our ridership dataset. First, we need to split our data into a training period, a validation period, and a test period. We will focus on the rail ridership for now. We will convert the data to 32-bit float tensors, and scale them down by a factor of one million to ensure the values end up near the 0‚Äì1 range; this plays nicely with the default weight initialization and learning rate. In this code, we use <code translate="no">df[["rail"]]</code> instead of <code translate="no">df["rail"]</code> to ensure the resulting tensor has a shape of [<em>series length</em>, 1] rather than [<em>series length</em>]:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">rail_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">FloatTensor</code><code class="p">(</code><code class="n">df</code><code class="p">[[</code><code class="s2">"rail"</code><code class="p">]][</code><code class="s2">"2016-01"</code><code class="p">:</code><code class="s2">"2018-12"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code> <code class="o">/</code> <code class="mf">1e6</code><code class="p">)</code>
<code class="n">rail_valid</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">FloatTensor</code><code class="p">(</code><code class="n">df</code><code class="p">[[</code><code class="s2">"rail"</code><code class="p">]][</code><code class="s2">"2019-01"</code><code class="p">:</code><code class="s2">"2019-05"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code> <code class="o">/</code> <code class="mf">1e6</code><code class="p">)</code>
<code class="n">rail_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">FloatTensor</code><code class="p">(</code><code class="n">df</code><code class="p">[[</code><code class="s2">"rail"</code><code class="p">]][</code><code class="s2">"2019-06"</code><code class="p">:]</code><code class="o">.</code><code class="n">values</code> <code class="o">/</code> <code class="mf">1e6</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When dealing with time series, you generally want to split across time. However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on. For example, if you have data about the financial health of 10,000 companies from 2001 to 2019, you might be able to split this data across the different companies. It‚Äôs very likely that many of these companies will be strongly correlated, though (e.g., whole economic sectors may go up or down jointly), and if you have correlated companies across the training set and the test set, your test set will not be as useful, as its measure of the generalization error will be optimistically biased.</p>
</div>

<p>Next, let‚Äôs use our <code translate="no">TimeSeriesDataset</code> class to create datasets for training, validation, and testing, and also create the corresponding data loaders. Since gradient descent expects the instances in the training set to be independent and identically distributed (IID)<a data-type="indexterm" data-primary="independent and identically distributed (IID), training instances as" id="id3110"/><a data-type="indexterm" data-primary="IID (independent and identically distributed), training instances as" id="id3111"/>, as we saw in <a data-type="xref" href="ch04.html#linear_models_chapter">Chapter¬†4</a>, we must set <code translate="no">shuffle</code> to <code translate="no">True</code>‚Äîthis will shuffle the windows, but not their contents:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">window_length</code> <code class="o">=</code> <code class="mi">56</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">TimeSeriesDataset</code><code class="p">(</code><code class="n">rail_train</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">TimeSeriesDataset</code><code class="p">(</code><code class="n">rail_valid</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">valid_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">valid_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">)</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="n">TimeSeriesDataset</code><code class="p">(</code><code class="n">rail_test</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">test_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">test_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">)</code></pre>

<p>And now we‚Äôre ready to build and train any regression model we want<a data-type="indexterm" data-startref="xi_timeseriesdataforecastingARMAmodelfamily1331066_1" id="id3112"/><a data-type="indexterm" data-startref="xi_timeseriesdataforecastingdatapreparationforMLmodels1338231_1" id="id3113"/><a data-type="indexterm" data-startref="xi_ARMAmodelfamily1331066_1" id="id3114"/><a data-type="indexterm" data-startref="xi_autoregressivemovingaverageARMAmodelfamily1331066_1" id="id3115"/><a data-type="indexterm" data-startref="xi_autoregressiveintegratedmovingaverageARIMAmodel13330105_1" id="id3116"/><a data-type="indexterm" data-startref="xi_ARIMAmodel13330105_1" id="id3117"/><a data-type="indexterm" data-startref="xi_SARIMAseasonalARIMAmodel1333274_1" id="id3118"/><a data-type="indexterm" data-startref="xi_seasonalARIMAmodelSARIMA1333274_1" id="id3119"/><a data-type="indexterm" data-startref="xi_machinelearningMLpreparingdataforMLmodels1338231_1" id="id3120"/>!</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Forecasting Using a Linear Model"><div class="sect2" id="id248">
<h2>Forecasting Using a Linear Model</h2>

<p>Let‚Äôs try a basic linear model first. We will use the Huber loss<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="with linear model" data-secondary-sortas="linear model" id="id3121"/><a data-type="indexterm" data-primary="linear models" data-secondary="forecasting time series" id="id3122"/><a data-type="indexterm" data-primary="Huber loss" id="id3123"/>, which usually works better than minimizing the MAE directly, as discussed in <a data-type="xref" href="ch09.html#ann_chapter">Chapter¬†9</a>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torchmetrics</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">window_length</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">loss_fn</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">HuberLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.003</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>
<code class="n">metric</code> <code class="o">=</code> <code class="n">torchmetrics</code><code class="o">.</code><code class="n">MeanAbsoluteError</code><code class="p">()</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># train the PyTorch model (e.g., using train() function from Chapter 10)</code></pre>

<p>Note that we must use an <code translate="no">nn.Flatten</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Flatten" id="id3124"/> before the <code translate="no">nn.Linear</code> layer, because the inputs have a shape of [<em>batch size</em>, <em>window length</em>, <em>dimensionality</em>], but the <code translate="no">nn.Linear</code> layer expects inputs of shape [<em>batch size</em>, <em>features</em>]. If you train this model, you will see that it reaches a validation MAE of 37,726 (your mileage may vary). That‚Äôs better than naive forecasting, but worse than the SARIMA model.<sup><a data-type="noteref" id="id3125-marker" href="ch13.html#id3125">7</a></sup></p>

<p>Can we do better with an RNN? Well, let‚Äôs see!</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Forecasting Using a Simple RNN"><div class="sect2" id="id249">
<h2>Forecasting Using a Simple RNN</h2>

<p>Let‚Äôs implement a simple RNN<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="with simple RNN" data-secondary-sortas="simple RNN" id="xi_timeseriesdataforecastingwithsimpleRNN1348329_1"/><a data-type="indexterm" data-primary="PyTorch" data-secondary="time series forecasting for RNN" id="xi_PyTorchtimeseriesforecastingforRNN1348329_1"/> containing a single recurrent layer (see <a data-type="xref" href="#rnn_layer_diagram">Figure¬†13-2</a>) plus a final <code translate="no">nn.Linear</code> layer that will take the last hidden state as input and output the model‚Äôs forecast:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SimpleRnnModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden_size</code> <code class="o">=</code> <code class="n">hidden_size</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">memory_cell</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">input_size</code> <code class="o">+</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Tanh</code><code class="p">()</code>
        <code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">batch_size</code><code class="p">,</code> <code class="n">window_length</code><code class="p">,</code> <code class="n">dimensionality</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code>
        <code class="n">X_time_first</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="n">H</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">X_t</code> <code class="ow">in</code> <code class="n">X_time_first</code><code class="p">:</code>
            <code class="n">XH</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">((</code><code class="n">X_t</code><code class="p">,</code> <code class="n">H</code><code class="p">),</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
            <code class="n">H</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">memory_cell</code><code class="p">(</code><code class="n">XH</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">H</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">univar_model</code> <code class="o">=</code> <code class="n">SimpleRnnModel</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Let‚Äôs go through this code:</p>

<ul>
<li>
<p>The constructor takes three arguments: the input size, the hidden size, and the output size. In our case, the input size is set to 1 when we create the model (on the very last line) since we are dealing with a univariate time series. The hidden size is the number of recurrent neurons. We set it to 32 in this example, but this is a hyperparameter you can tune. The output size is 1 since we‚Äôre only forecasting a single value.</p>
</li>
<li>
<p>The constructor first creates the memory cell, which will be used once per time step: it‚Äôs a sequential module composed of an <code translate="no">nn.Linear</code> layer and the tanh activation function. You can use another activation function here, but it‚Äôs common to use the tanh activation function because it tends to be more stable than other activation functions in RNNs.</p>
</li>
<li>
<p>Next, we create an <code translate="no">nn.Linear</code> layer that will be used to take the last hidden state and produce the final output. This is needed because the hidden state has one dimension per recurrent neuron (32 in this example), while the target has just one dimension since we‚Äôre dealing with a univariate time series and we‚Äôre only trying to forecast a single future value. Moreover, the tanh activation function only outputs values between ‚Äì1 and +1, while the values we need to forecast occasionally exceed +1.</p>
</li>
<li>
<p>The <code translate="no">forward()</code> method will be passed input batches produced by our data loader, so each batch will have a shape of [<em>batch size</em>, <em>window length</em>, <em>dimensionality</em>], with <em>dimensionality</em> = 1.</p>
</li>
<li>
<p>The hidden state <code translate="no">H</code> is initialized to zeros: for each input window, there‚Äôs one zero per recurrent neuron, so the hidden state‚Äôs shape is [<em>batch size</em>, <em>hidden_size</em>].</p>
</li>
<li>
<p>Next, we iterate over each time step. For this, we must swap the first two dimensions of <code translate="no">X</code> using <code translate="no">permute(0, 1)</code>. As a result, the input tensor <code translate="no">X_t</code> at each time step has a shape of [<em>batch size</em>, <em>dimensionality</em>].</p>
</li>
<li>
<p>At each time step, we want to feed both the current inputs <code translate="no">X_t</code> and the hidden state <code translate="no">H</code> to the memory cell. For this, we must first concatenate <code translate="no">X_t</code> and <code translate="no">H</code> along the first dimension, resulting in a tensor <code translate="no">XH</code> of shape [<em>batch size</em>, <em>input_size</em> + <em>hidden size</em>]. Then we can pass <code translate="no">XH</code> to the memory cell to get the new hidden state.</p>
</li>
<li>
<p>After the loop, <code translate="no">H</code> represents the final hidden state. We pass it through the output <code translate="no">nn.Linear</code> layer, and we get our final prediction of shape [<em>batch size</em>, <em>output size</em>].</p>
</li>
</ul>

<p>In short, this model initializes the hidden state to zeros, then it goes through each time step and applies the memory cell to both the current inputs and the last hidden state, which gives it the new hidden state. It repeats this process until the last time step, then it passes the last hidden state through a linear layer to get the actual forecasts. All of this is performed simultaneously for every sequence in the batch.</p>

<p>So that‚Äôs our first recurrent model! It‚Äôs a sequence-to-vector model. Since there‚Äôs a single output neuron in this case, the output vector for each input sequence has a size of 1.</p>

<p>Now if you move this model to the GPU, then train and evaluate it just like the previous one, you will find that its validation MAE reaches 30,659. That‚Äôs the best model we‚Äôve trained so far, and it even beats the SARIMA model; we‚Äôre doing pretty well!</p>
<div data-type="tip"><h6>Tip</h6>
<p>We‚Äôve only normalized the time series, without removing trend and seasonality, and yet the model still performs well. This is convenient, as it makes it possible to quickly search for promising models without worrying too much about preprocessing. However, to get the best performance, you may want to try making the time series more stationary, for example, using differencing.</p>
</div>

<p>PyTorch comes with an <code translate="no">nn.RNN</code> module that can greatly simplify the implementation of our <code translate="no">SimpleRnnModel</code>. The following implementation is (almost) equivalent to the previous one:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SimpleRnnModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">rnn</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">outputs</code><code class="p">,</code> <code class="n">last_state</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">rnn</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">outputs</code><code class="p">[:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">])</code></pre>

<p>The code is much shorter than before. Let‚Äôs go through it:</p>

<ul>
<li>
<p>In the constructor, we now create an <code translate="no">nn.RNN</code> module instead of building a memory cell. We specify the input size and the hidden size, just like we did earlier, and we also set <code translate="no">batch_first=True</code> because our input batches have the batch dimension first. If we didn‚Äôt set <code translate="no">batch_first=True</code>, the <code translate="no">nn.RNN</code> module would assume that the time dimension comes first (i.e., it would expect the input batches to have a shape of [<em>window length</em>, <em>batch size</em>, <em>dimensionality</em>] instead of [<em>batch size</em>, <em>window length</em>, <em>dimensionality</em>]).</p>
</li>
<li>
<p>The constructor also creates an output layer, exactly like in our previous 
<span class="keep-together">implementation.</span></p>
</li>
<li>
<p>In the <code translate="no">forward()</code> method, we pass the input batch directly to the <code translate="no">nn.RNN</code> module. This takes care of everything: internally, it initializes the hidden state with zeros, and it processes each time step using a simple memory cell based on a linear layer and an activation function (tanh by default), much like we did earlier.</p>
</li>
<li>
<p>Note that the <code translate="no">nn.RNN</code> module returns two things:</p>

<ul>
<li>
<p><code translate="no">outputs</code> is a tensor containing the outputs of the top recurrent layer at every time step. Right now we have a single recurrent layer, but in the next section we will see that the <code translate="no">nn.RNN</code> module supports multiple recurrent layers. Since we are dealing with a simple RNN, the outputs are just the hidden states of the top recurrent layer at each time step. The <code translate="no">outputs</code> tensor has a shape of [<em>batch size</em>, <em>window length</em>, <em>hidden size</em>] (if we didn‚Äôt set <code translate="no">batch_first=True</code>, then the first two dimensions would be swapped).</p>
</li>
<li>
<p><code translate="no">last_state</code> contains the hidden state of each recurrent layer after the very last time step. Its shape is [<em>number of layers</em>, <em>batch size</em>, <em>hidden size</em>]. In our case, there‚Äôs a single recurrent layer, so the size of the first dimension is 1.</p>
</li>
</ul>
</li>
<li>
<p>In the end, we take the last output (which is also the last state of the top recurrent layer) and we pass it through our <code translate="no">nn.Linear</code> output layer.</p>
</li>
</ul>

<p>If you train this model, you will get a similar result as before, but generally much faster because the <code translate="no">nn.RNN</code> module is well optimized. In particular, when using an Nvidia GPU, the <code translate="no">nn.RNN</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.RNN" id="id3126"/> leverages the cuDNN library which provides highly optimized implementations of various neural net architectures, including several RNN architectures.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code translate="no">nn.RNN</code> module uses two bias parameters: one for the inputs, and the other for the hidden states. It just adds them up, so this really doesn‚Äôt improve the model at all, but this extra parameter is required by the cuDNN library. This explains why you won‚Äôt get exactly the same results as before<a data-type="indexterm" data-startref="xi_timeseriesdataforecastingwithsimpleRNN1348329_1" id="id3127"/><a data-type="indexterm" data-startref="xi_PyTorchtimeseriesforecastingforRNN1348329_1" id="id3128"/>.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Forecasting Using a Deep RNN"><div class="sect2" id="id408">
<h2>Forecasting Using a Deep RNN</h2>

<p>It is quite common to stack multiple layers of cells<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="with deep RNN" data-secondary-sortas="deep RNN" id="id3129"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="deep RNN" id="id3130"/>, as shown in <a data-type="xref" href="#deep_rnn_diagram">Figure¬†13-10</a>. This gives you a <em>deep RNN</em>.</p>

<figure class="smallerseventy"><div id="deep_rnn_diagram" class="figure">
<img src="assets/hmls_1310.png" alt="Diagram showing a deep recurrent neural network (RNN) with stacked layers, unrolled through time to illustrate sequential data processing." width="1207" height="709"/>
<h6><span class="label">Figure 13-10. </span>A deep RNN (left) unrolled through time (right)</h6>
</div></figure>

<p class="pagebreak-before">Implementing a deep RNN with PyTorch is straightforward: just set the <code translate="no">num_layers</code> argument to the desired number of recurrent layers when creating the <code translate="no">nn.RNN</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.RNN" id="id3131"/>. For example, if you set <code translate="no">num_layers=3</code> when creating the <code translate="no">nn.RNN</code> module in the previous model‚Äôs constructor, you get a three-layer RNN (the rest of the code remains unchanged):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="bp">self</code><code class="o">.</code><code class="n">rnn</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">num_layers</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>

<p>If you train and evaluate this model, you will find that it reaches an MAE of 29,273. That‚Äôs our best model so far!</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Forecasting Multivariate Time Series"><div class="sect2" id="id250">
<h2>Forecasting Multivariate Time Series</h2>

<p>An important quality of neural networks<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="multivariate time series" id="xi_timeseriesdataforecastingmultivariatetimeseries1357640_1"/><a data-type="indexterm" data-primary="multivariate time series" id="xi_multivariatetimeseries1357640_1"/> is their flexibility: in particular, they can deal with multivariate time series with almost no change to their architecture. For example, let‚Äôs try to forecast the rail time series using both the rail and bus data as input. In fact, let‚Äôs also throw in the day type! Since we can always know in advance whether tomorrow is going to be a weekday, a weekend, or a holiday, we can shift the day type series one day into the future, so that the model is given tomorrow‚Äôs day type as input. For simplicity, we‚Äôll do this processing using Pandas:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">df_mulvar</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s2">"rail"</code><code class="p">,</code> <code class="s2">"bus"</code><code class="p">]]</code> <code class="o">/</code> <code class="mf">1e6</code>  <code class="c1"># use both rail &amp; bus series as input</code>
<code class="n">df_mulvar</code><code class="p">[</code><code class="s2">"next_day_type"</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"day_type"</code><code class="p">]</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># we know tomorrow's type</code>
<code class="n">df_mulvar</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">df_mulvar</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">float</code><code class="p">)</code>  <code class="c1"># one-hot encode day type</code></pre>

<p>Now <code translate="no">df_mulvar</code> is a DataFrame with five columns: the rail and bus data, plus three columns containing the one-hot encoding of the next day‚Äôs type (recall that there are three possible day types, <code translate="no">W</code>, <code translate="no">A</code>, and <code translate="no">U</code>). Next, we can proceed much like we did earlier. First we split the data into three periods, scale it down by a factor of one million, and convert it to tensors:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">mulvar_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">FloatTensor</code><code class="p">(</code><code class="n">df_mulvar</code><code class="p">[</code><code class="s2">"2016-01"</code><code class="p">:</code><code class="s2">"2018-12"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code> <code class="o">/</code> <code class="mf">1e6</code><code class="p">)</code>
<code class="n">mulvar_valid</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">FloatTensor</code><code class="p">(</code><code class="n">df_mulvar</code><code class="p">[</code><code class="s2">"2019-01"</code><code class="p">:</code><code class="s2">"2019-05"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code> <code class="o">/</code> <code class="mf">1e6</code><code class="p">)</code>
<code class="n">mulvar_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">FloatTensor</code><code class="p">(</code><code class="n">df_mulvar</code><code class="p">[</code><code class="s2">"2019-06"</code><code class="p">:]</code><code class="o">.</code><code class="n">values</code> <code class="o">/</code> <code class="mf">1e6</code><code class="p">)</code></pre>

<p>Then we need to create the PyTorch datasets. If we used the <code translate="no">TimeSeriesDataset</code> for this, the targets would include the next day‚Äôs rail and bus ridership, as well as the one-hot encoding of the following day type. Since we only want to predict the rail ridership for now, we must tweak the <code translate="no">TimeSeriesDataset</code> to keep only the first value in the target, which is the rail ridership. One way to do this is to create a new <code translate="no">MulvarTimeSeriesDataset</code> class that extends the <code translate="no">TimeSeriesDataset</code> class and tweaks the <code translate="no">__getitem__()</code> method to filter the target:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">MulvarTimeSeriesDataset</code><code class="p">(</code><code class="n">TimeSeriesDataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="n">window</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__getitem__</code><code class="p">(</code><code class="n">idx</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code><code class="p">[:</code><code class="mi">1</code><code class="p">]</code></pre>

<p class="pagebreak-before">Next, we can create the datasets and the data loaders, much like we did earlier:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">mulvar_train_set</code> <code class="o">=</code> <code class="n">MulvarTimeSeriesDataset</code><code class="p">(</code><code class="n">mulvar_train</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">mulvar_train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">mulvar_train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># create the datasets and data loaders for the validation and test sets</code></pre>

<p>If you look at the batches produced by the data loaders, you will find that the input shape is [32, 56, 5], and the target shape is [32, 1]. Perfect!</p>

<p>So we can finally create the RNN:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">mulvar_model</code> <code class="o">=</code> <code class="n">SimpleRnnModel</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">output_size</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">mulvar_model</code> <code class="o">=</code> <code class="n">mulvar_model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>Notice that this model is identical to the <code translate="no">univar_model</code> RNN we built earlier, except <code translate="no">input_size=5</code>: at each time step, the model now receives five inputs instead of one. This model actually reaches a validation MAE of 23,227. Now we‚Äôre making big progress!</p>

<p>In fact, it‚Äôs not too hard to make the RNN forecast both the rail and bus ridership. You just need to return <code translate="no">target[:2]</code> instead of <code translate="no">target[:1]</code> in the <code translate="no">MulvarTimeSeriesDataset</code> class, and set <code translate="no">output_size=2</code> when creating the <code translate="no">SimpleRnnModel</code>, that‚Äôs all there is to it!</p>

<p>As we discussed in <a data-type="xref" href="ch09.html#ann_chapter">Chapter¬†9</a>, using a single model for multiple related tasks often results in better performance than using a separate model for each task, since features learned for one task may be useful for the other tasks, and also because having to perform well across multiple tasks prevents the model from overfitting (it‚Äôs a form of regularization). However, it depends on the task, and in this particular case the multitask RNN that forecasts both the bus and the rail ridership doesn‚Äôt perform quite as well as dedicated models that forecast one or the other (using all five columns as input). Still, it reaches a validation MAE of 26,441 for rail and 26,178 for bus, which is still pretty good<a data-type="indexterm" data-startref="xi_timeseriesdataforecastingmultivariatetimeseries1357640_1" id="id3132"/><a data-type="indexterm" data-startref="xi_multivariatetimeseries1357640_1" id="id3133"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Forecasting Several Time Steps Ahead"><div class="sect2" id="id251">
<h2>Forecasting Several Time Steps Ahead</h2>

<p>So far we have only predicted the value<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="several time steps ahead" id="xi_timeseriesdataforecastingseveraltimestepsahead1363140_1"/> at the next time step, but we could just as easily have predicted the value several steps ahead by changing the targets appropriately (e.g., to predict the ridership 2 weeks from now, we could just change the targets to be the value 14 days ahead instead of 1 day ahead). But what if we want to predict the next 14 values with a single model?</p>

<p>The first option is to take the <code translate="no">univar_model</code> RNN we trained earlier for the rail time series, make it predict the next value, and add that value to the inputs, acting as if the predicted value had actually occurred. We then use the model again to predict the following value, and so on, as in the following code:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">n_steps</code> <code class="o">=</code> <code class="mi">14</code>
<code class="n">univar_model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">X</code> <code class="o">=</code> <code class="n">rail_valid</code><code class="p">[:</code><code class="n">window_length</code><code class="p">]</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">step_ahead</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_steps</code><code class="p">):</code>
        <code class="n">y_pred_one</code> <code class="o">=</code> <code class="n">univar_model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">([</code><code class="n">X</code><code class="p">,</code> <code class="n">y_pred_one</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)],</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

    <code class="n">Y_pred</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="n">n_steps</code><code class="p">:,</code> <code class="mi">0</code><code class="p">]</code></pre>

<p>In this code, we take the rail ridership of the first 56 days of the validation period, we add a batch dimension of size 1 using the <code translate="no">unsqueeze()</code> method (since our <code translate="no">univar_model</code> expects 3D inputs), then we move the tensor to the GPU. Now the shape of <code translate="no">X</code> is [1, 56, 1]. Then we repeatedly use the model to forecast the next value, and we append each forecast to the input series, along the time axis (<code translate="no">dim=1</code>). Since each prediction has a shape of [1, 1], we must use <code translate="no">unsqueeze()</code> again to add a batch dimension of size 1 before we can concatenate it to <code translate="no">X</code>. In the end, <code translate="no">X</code> has a shape of [1, 56 + 14, 1], and our final forecasts are the last 14 values of <code translate="no">X</code>. The resulting forecasts are plotted in <a data-type="xref" href="#forecast_ahead_plot">Figure¬†13-11</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If the model makes an error at one time step, then the forecasts for the following time steps are impacted as well: the errors tend to accumulate. So, it‚Äôs preferable to use this technique only for a small number of steps.</p>
</div>

<figure><div id="forecast_ahead_plot" class="figure">
<img src="assets/hmls_1311.png" alt="Line graph comparing actual vs. predicted data points over time, highlighting forecasts for 14 steps ahead made individually, with potential error accumulation." width="2267" height="904"/>
<h6><span class="label">Figure 13-11. </span>Forecasting 14 steps ahead, 1 step at a time</h6>
</div></figure>

<p>The second option is to train an RNN to predict the next 14 values in one shot. We can still use a sequence-to-vector model, but it will output 14 values instead of 1. However, we first need to change the targets to be vectors containing the next 14 values. For this, we can create the following class:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ForecastAheadDataset</code><code class="p">(</code><code class="n">TimeSeriesDataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">)</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code> <code class="o">-</code> <code class="mi">14</code> <code class="o">+</code> <code class="mi">1</code>

    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="n">end</code> <code class="o">=</code> <code class="n">idx</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code>  <code class="c1"># 1st index after window</code>
        <code class="n">window</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">[</code><code class="n">idx</code> <code class="p">:</code> <code class="n">end</code><code class="p">]</code>
        <code class="n">target</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">[</code><code class="n">end</code> <code class="p">:</code> <code class="n">end</code> <code class="o">+</code> <code class="mi">14</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>  <code class="c1"># 0 = rail ridership</code>
        <code class="k">return</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>I‚Äôve hardcoded the number 14, but in a real project you should make this configurable (e.g., just like the <code translate="no">window_length</code>).</p>
</div>

<p>This class inherits from the <code translate="no">TimeSeriesDataset</code> class and tweaks its <code translate="no">__len__()</code> and <code translate="no">__getitem__()</code> methods. The target is now a tensor containing the next 14 rail ridership values, rather than just the next value. We can once again create a training set, a validation set, and a test set, based on the multivariate time series we built earlier:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">ahead_train_set</code> <code class="o">=</code> <code class="n">ForecastAheadDataset</code><code class="p">(</code><code class="n">mulvar_train</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">ahead_train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">ahead_train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># create the datasets and data loaders for the validation and test sets</code></pre>

<p>Lastly, we can create a simple RNN, just like the <code translate="no">mulvar_model</code>, but with <code translate="no">output_size=14</code>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">ahead_model</code> <code class="o">=</code> <code class="n">SimpleRnnModel</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">output_size</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code>
<code class="n">ahead_model</code> <code class="o">=</code> <code class="n">ahead_model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>After training this model, you can predict the next 14 values at once, like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">ahead_model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">window</code> <code class="o">=</code> <code class="n">mulvar_valid</code><code class="p">[:</code><code class="n">window_length</code><code class="p">]</code>  <code class="c1"># shape [56, 5]</code>
    <code class="n">X</code> <code class="o">=</code> <code class="n">window</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>            <code class="c1"># shape [1, 56, 5]</code>
    <code class="n">Y_pred</code> <code class="o">=</code> <code class="n">ahead_model</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code>     <code class="c1"># shape [1, 14]</code></pre>

<p>This approach works quite well. Its forecasts for the next day are obviously better than its forecasts for 14 days into the future, but it doesn‚Äôt accumulate errors like the previous approach did. Now let‚Äôs see whether a sequence-to-sequence model can do even better<a data-type="indexterm" data-startref="xi_timeseriesdataforecastingseveraltimestepsahead1363140_1" id="id3134"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can combine both approaches to forecast many steps ahead: use a model that forecasts the next 14 days in one shot, then append the forecasts to the inputs and run the model again to get forecasts for the following 14 days, and so on.‚Å†<sup><a data-type="noteref" id="id3135-marker" href="ch13.html#id3135">8</a></sup></p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Forecasting Using a Sequence-to-Sequence Model"><div class="sect2" id="id252">
<h2>Forecasting Using a Sequence-to-Sequence Model</h2>

<p>Instead of training the model<a data-type="indexterm" data-primary="time series data, forecasting" data-secondary="with sequence-to-sequence model" data-secondary-sortas="sequence-to-sequence model" id="xi_timeseriesdataforecastingwithsequencetosequencemodel1370730_1"/><a data-type="indexterm" data-primary="sequence-to-sequence (seq2seq) network" id="xi_sequencetosequenceseq2seqnetwork1370730_1"/> to forecast the next 14 values only at the very last time step, we can train it to forecast the next 14 values at each and every time step. To be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1 to 14, then at time step 1 the model will forecast time steps 2 to 15, and so on. In other words, the targets are sequences of consecutive windows, shifted by one time step at each time step. The target for each input window is not a vector anymore, but a sequence of the same length as the inputs, containing a 14-dimensional vector at each step. Given an input batch of shape [<em>batch size</em>, <em>window length</em>, <em>input size</em>], the output will have a shape of [<em>batch size</em>, <em>window length</em>, <em>output_size</em>]. This is no longer a sequence-to-vector RNN, it‚Äôs a sequence-to-sequence (or <em>seq2seq</em>) RNN.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It may be surprising that the targets contain values that appear in the inputs (except for the last time step). Isn‚Äôt that cheating? Fortunately, not at all: at each time step, an RNN only knows about past time steps; it cannot look ahead. It is said<a data-type="indexterm" data-primary="causal model" id="id3136"/> to be a <em>causal</em> model.</p>
</div>

<p>You may be wondering why we would want to train a seq2seq model when we‚Äôre really only interested in forecasting future values, which are output by our model at the very last time step. And you‚Äôre right: after training, you can actually ignore all outputs except for the very last time step. The main advantage of this technique is that the loss will contain a term for the output of the RNN at each and every time step, not just for the output at the last time step. This means there will be many more error gradients flowing through the model, and they won‚Äôt have to flow through time as much since they will come from the output of each time step, not just the last one. This can both stabilize training and speed up convergence. Moreover, since the model must make predictions at each time step, it will see input sequences of varying lengths, which can reduce the risk of overfitting the model to the specific window length used during training. Well, at least that‚Äôs the hope! Let‚Äôs give this technique a try on the rail ridership time series. As usual, we first need to prepare the dataset:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="k">class</code> <code class="nc">Seq2SeqDataset</code><code class="p">(</code><code class="n">ForecastAheadDataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="n">end</code> <code class="o">=</code> <code class="n">idx</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">window_length</code>  <code class="c1"># 1st index after window</code>
        <code class="n">window</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">[</code><code class="n">idx</code> <code class="p">:</code> <code class="n">end</code><code class="p">]</code>
        <code class="n">target_period</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">series</code><code class="p">[</code><code class="n">idx</code> <code class="o">+</code> <code class="mi">1</code> <code class="p">:</code> <code class="n">end</code> <code class="o">+</code> <code class="mi">14</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>
        <code class="n">target</code> <code class="o">=</code> <code class="n">target_period</code><code class="o">.</code><code class="n">unfold</code><code class="p">(</code><code class="n">dimension</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">14</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code></pre>

<p>Our new <code translate="no">Seq2SeqDataset</code> class inherits from the <code translate="no">ForecastAheadDataset</code> class and overrides the <code translate="no">__getitem__()</code> method: the input window is defined just like before, but the target is now a sequence of consecutive windows, shifted by one time step at each time step. The <code translate="no">unfold()</code> method is where the magic happens: it takes a tensor and produces sliding blocks from it. For this, it repeatedly slides along the given dimension by the given number of steps and extracts a block of the given size. For example:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code><code class="o">.</code><code class="n">unfold</code><code class="p">(</code><code class="n">dimension</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="go">tensor([[0, 1, 2, 3],</code>
<code class="go">        [1, 2, 3, 4],</code>
<code class="go">        [2, 3, 4, 5]])</code></pre>

<p>Once again we must create a training set, a validation set, and a test set, as well as the corresponding data loaders:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">seq_train_set</code> <code class="o">=</code> <code class="n">Seq2SeqDataset</code><code class="p">(</code><code class="n">mulvar_train</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">seq_train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">seq_train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># create the datasets and data loaders for the validation and test sets</code></pre>

<p>And lastly, we can build the sequence-to-sequence model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">Seq2SeqRnnModel</code><code class="p">(</code><code class="n">SimpleRnnModel</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">outputs</code><code class="p">,</code> <code class="n">last_state</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">rnn</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">outputs</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">seq_model</code> <code class="o">=</code> <code class="n">Seq2SeqRnnModel</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">output_size</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code>
<code class="n">seq_model</code> <code class="o">=</code> <code class="n">seq_model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>We inherit from the <code translate="no">SimpleRnnModel</code> class, and we override the <code translate="no">forward()</code> method. Instead of applying the linear <code translate="no">self.output</code> layer only to the outputs of the last time step, as we did before, we now apply it to the outputs of every time step. It may surprise you that this works at all. So far, we have only applied <code translate="no">nn.Linear</code> layers to 2D inputs of shape [<em>batch size</em>, <em>features</em>], but here the <code translate="no">outputs</code> tensor has a shape of [<em>batch size</em>, <em>window length</em>, <em>hidden size</em>]: it‚Äôs 3D, not 2D! Luckily, this works fine as the <code translate="no">nn.Linear</code> layer will automatically be applied to each time step, so the model‚Äôs predictions will have a shape of [<em>batch size</em>, <em>window length</em>, <em>output size</em>]: just what we need.</p>

<p>Under the hood, the <code translate="no">nn.Linear</code> layer relies on <code translate="no">torch.matmul()</code> for matrix multiplication. This function efficiently supports multiplying arrays of more than two dimensions. For example, you can multiply an array of shape [2, 3, 5, 7] with an array of shape [2, 3, 7, 11]. Indeed, these two arrays can both be seen as 2 √ó 3 grids of matrices, and <code translate="no">torch.matmul()</code> simply multiplies the corresponding matrices in both grids. Since multiplying a 5 √ó 7 matrix with a 7 √ó 11 matrix produces a 5 √ó 11 matrix, the final result is a 2 √ó 3 grid of 5 √ó 11 matrices, represented as a tensor of shape [2, 3, 5, 11]. Broadcasting is also supported; for example, you can multiply an array of shape [10, 56, 32] with an array of shape [32, 14]: each of the ten 56 √ó 32 matrices in the first array will be multiplied by the same 32 √ó 14 matrix in the second array, and you will get a tensor of shape [10, 56, 14]. That‚Äôs what happens when you pass a 3D input to an <code translate="no">nn.Linear</code> layer.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Another way to get the exact same result is to replace the <code translate="no">nn.Linear</code> output layer with an <code translate="no">nn.Conv1d</code> layer using a kernel size of one (i.e., <code translate="no">Conv1d(32, 14, kernel_size=1)</code>). However, you would have to swap the last two dimensions of both the inputs and the outputs, treating the time dimension as a spatial dimension.</p>
</div>

<p>The training code is the same as usual. During training, all the model‚Äôs outputs are used, but after training, only the outputs of the very last time step matter, and the rest can be ignored (as mentioned earlier). For example, we can forecast the rail ridership for the next 14 days like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">seq_model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">some_window</code> <code class="o">=</code> <code class="n">mulvar_valid</code><code class="p">[:</code><code class="n">window_length</code><code class="p">]</code>  <code class="c1"># shape [56, 5]</code>
    <code class="n">X</code> <code class="o">=</code> <code class="n">some_window</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>            <code class="c1"># shape [1, 56, 5]</code>
    <code class="n">Y_preds</code> <code class="o">=</code> <code class="n">seq_model</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">))</code>           <code class="c1"># shape [1, 56, 14]</code>
    <code class="n">Y_pred</code> <code class="o">=</code> <code class="n">Y_preds</code><code class="p">[:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code>                     <code class="c1"># shape [1, 14]</code></pre>

<p>If you evaluate this model‚Äôs forecasts for <em>t</em>¬†+¬†1, you will find a validation MAE of 23,350, which is very good. Of course, the model is not as accurate for more distant forecasts. For example, the MAE for <em>t</em>¬†+¬†14 is 35,315.</p>

<p>Simple RNNs can be quite good at forecasting time series or handling other kinds of sequences, but they do not perform as well on long time series or sequences<a data-type="indexterm" data-startref="xi_timeseriesdataforecastingwithsequencetosequencemodel1370730_1" id="id3137"/><a data-type="indexterm" data-startref="xi_sequencetosequenceseq2seqnetwork1370730_1" id="id3138"/>. Let‚Äôs discuss why and see what we can do about it.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Handling Long Sequences"><div class="sect1" id="id253">
<h1>Handling Long Sequences</h1>

<p>To train an RNN on long sequences<a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="handling long sequences" id="xi_recurrentneuralnetworksRNNshandlinglongsequences1378034_1"/><a data-type="indexterm" data-primary="long sequences, training RNN on" id="xi_longsequencestrainingRNNon1378034_1"/>, we must run it over many time steps, making the unrolled RNN a very deep network. Just like any deep neural network, it may suffer from the unstable gradients problem, discussed in <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>: it may take forever to train, or training may be unstable. Moreover, when an RNN processes a long sequence, it will gradually forget the first inputs in the sequence. Let‚Äôs look at both these problems, starting with the unstable gradients problem.</p>








<section data-type="sect2" data-pdf-bookmark="Fighting the Unstable Gradients Problem"><div class="sect2" id="id254">
<h2>Fighting the Unstable Gradients Problem</h2>

<p>Many of the tricks we used in deep nets to alleviate the unstable gradients problem can also be used for RNNs: good parameter initialization, faster optimizers, dropout, and so on. However, nonsaturating activation functions (e.g., ReLU) may not help as much here<a data-type="indexterm" data-primary="vanishing and exploding gradients" data-secondary="unstable gradients problem" id="xi_vanishingandexplodinggradientsunstablegradientsproblem13783264_1"/><a data-type="indexterm" data-primary="ReLU (rectified linear units)" data-secondary="RNN unstable gradients problem" id="xi_ReLUrectifiedlinearunitsRNNunstablegradientsproblem13783264_1"/><a data-type="indexterm" data-primary="unstable gradients" id="xi_unstablegradients13783264_1"/>. In fact, they may actually lead the RNN to be even more unstable during training. Why? Well, suppose gradient descent updates the weights in a way that increases the outputs slightly at the first time step. Because the same weights are used at every time step, the outputs at the second time step may also be slightly increased, and those at the third, and so on until the outputs explode‚Äîand a nonsaturating activation function does not prevent that.</p>

<p>You can reduce this risk by using a smaller learning rate, or you can use a saturating activation function like the hyperbolic tangent<a data-type="indexterm" data-primary="hyperbolic tangent (htan)" id="id3139"/><a data-type="indexterm" data-primary="activation functions" data-secondary="hyperbolic tangent (htan)" id="id3140"/> (this explains why it‚Äôs the default).</p>

<p>In much the same way, the gradients themselves can explode. If you notice that training is unstable, you may want to monitor the size of the gradients and perhaps use gradient clipping<a data-type="indexterm" data-primary="gradient clipping" id="id3141"/>.</p>

<p>Moreover, batch normalization<a data-type="indexterm" data-primary="batch normalization (BN)" id="id3142"/><a data-type="indexterm" data-primary="BN (batch normalization)" id="id3143"/> cannot be used as efficiently with RNNs as with deep feedforward nets. In fact, you cannot use it between time steps, only between recurrent layers. To be more precise, it is technically possible to add a BN layer to a memory cell so that it will be applied at each time step (both on the inputs for that time step and on the hidden state from the previous step). However, this implies that the same BN layer will be used at each time step, with the same parameters, regardless of the actual scale and offset of the inputs and hidden state. In practice, this does not yield good results, as was demonstrated by C√©sar Laurent et al. in a <a href="https://homl.info/rnnbn">2015 paper</a>.‚Å†<sup><a data-type="noteref" id="id3144-marker" href="ch13.html#id3144">9</a></sup> The authors found that BN was slightly beneficial only when it was applied to the layer‚Äôs inputs, not to the hidden states. In other words, it was slightly better than nothing when applied between recurrent layers (i.e., vertically in <a data-type="xref" href="#deep_rnn_diagram">Figure¬†13-10</a>), but not within recurrent layers (i.e., horizontally).</p>

<p>Layer norm<a data-type="indexterm" data-primary="layer normalization (LN)" id="id3145"/><a data-type="indexterm" data-primary="LN (layer normalization)" id="id3146"/> (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>) tends to work a bit better than BN within recurrent layers. It is usually applied just before the activation function, at each time step. Sadly, PyTorch‚Äôs <code translate="no">nn.RNN</code> module does not support LN, so you have to implement the RNN‚Äôs loop manually (as we did earlier), and apply the <code translate="no">nn.LayerNorm</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.LayerNorm" id="id3147"/> at each iteration. This is not too hard, but you do lose the simplicity and speed of the <code translate="no">nn.RNN</code> module<a data-type="indexterm" data-primary="recurrent layer normalization" id="id3148"/>. For example, you can take the first version of our <code translate="no">SimpleRnnModel</code> class and add an <code translate="no">nn.LayerNorm</code> module to the memory cell, just before the tanh activation function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="bp">self</code><code class="o">.</code><code class="n">memory_cell</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">input_size</code> <code class="o">+</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">LayerNorm</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Tanh</code><code class="p">()</code>
<code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Layer norm does not always help; you just have to try it. In general, it works better in <em>gated RNNs</em> such as LSTM and GRU (discussed shortly). It is also more likely to help when the time series is preprocessed to remove any seasonality or trend.</p>
</div>

<p>Similarly, if you wish to apply dropout<a data-type="indexterm" data-primary="dropout in time series, custom RNN for" id="id3149"/> between each time step, you must write a custom RNN since the <code translate="no">nn.RNN</code> module does not support that. However, it does support adding a dropout layer after every recurrent layer: simply set the <code translate="no">dropout</code> hyperparameter to the desired dropout rate (it defaults to zero).</p>

<p>With these techniques, you can alleviate the unstable gradients problem and train an RNN much more efficiently. Now let‚Äôs look at how to deal with the short-term memory problem.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When forecasting time series, it is often useful to have some error bars along with your predictions<a data-type="indexterm" data-startref="xi_vanishingandexplodinggradientsunstablegradientsproblem13783264_1" id="id3150"/><a data-type="indexterm" data-startref="xi_ReLUrectifiedlinearunitsRNNunstablegradientsproblem13783264_1" id="id3151"/><a data-type="indexterm" data-startref="xi_unstablegradients13783264_1" id="id3152"/>. For this, one approach is to use MC dropout<a data-type="indexterm" data-primary="MC (Monte Carlo) dropout" id="id3153"/><a data-type="indexterm" data-primary="Monte Carlo (MC) dropout" id="id3154"/> (introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter¬†11</a>).</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Tackling the Short-Term Memory Problem"><div class="sect2" id="id255">
<h2>Tackling the Short-Term Memory Problem</h2>

<p>Due to the transformations<a data-type="indexterm" data-primary="short-term memory problem, RNNs" id="xi_shorttermmemoryproblemRNNs1381127_1"/> that the data goes through when traversing an RNN, some information is lost at each time step. After a while, the RNN‚Äôs state contains virtually no trace of the first inputs. This can be a showstopper. Imagine Dory the fish‚Å†<sup><a data-type="noteref" id="id3155-marker" href="ch13.html#id3155">10</a></sup> trying to translate a long sentence; by the time she‚Äôs finished reading it, she has no clue how it started. To tackle this problem, various types of cells with long-term memory have been introduced. They have proven so successful that the basic cells<a data-type="indexterm" data-primary="long short-term memory (LSTM) cell" id="xi_longshorttermmemoryLSTMcell13811619_1"/><a data-type="indexterm" data-primary="LSTM (long short-term memory) cell" id="xi_LSTMlongshorttermmemorycell13811619_1"/><a data-type="indexterm" data-primary="memory cells, RNNs" id="xi_memorycellsRNNs13811619_1"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="memory cells" id="xi_recurrentneuralnetworksRNNsmemorycells13811619_1"/> are not used much anymore. Let‚Äôs first look at the most popular of these long-term memory cells: the LSTM cell.</p>










<section data-type="sect3" data-pdf-bookmark="LSTM cells"><div class="sect3" id="id256">
<h3>LSTM cells</h3>

<p>The <em>long short-term memory</em> (LSTM) cell was <a href="https://homl.info/93">proposed in 1997</a>‚Å†<sup><a data-type="noteref" id="id3156-marker" href="ch13.html#id3156">11</a></sup> by Sepp Hochreiter and J√ºrgen Schmidhuber and gradually improved over the years by several researchers, such as <a href="https://homl.info/graves">Alex Graves</a>, <a href="https://homl.info/94">Ha≈üim Sak</a>,‚Å†<sup><a data-type="noteref" id="id3157-marker" href="ch13.html#id3157">12</a></sup> and <a href="https://homl.info/95">Wojciech Zaremba</a>.‚Å†<sup><a data-type="noteref" id="id3158-marker" href="ch13.html#id3158">13</a></sup> You can simply use the <code translate="no">nn.LSTM</code> module<a data-type="indexterm" data-primary="torch" data-secondary="nn.LSTM" id="id3159"/> instead of the <code translate="no">nn.RNN</code> module; it‚Äôs a drop-in replacement, and it usually performs much better: training converges faster, and the model detects longer-term patterns in the data.</p>

<p>So how does this magic work? Well, the LSTM architecture is shown in <a data-type="xref" href="#lstm_cell_diagram">Figure¬†13-12</a>. If you don‚Äôt look at what‚Äôs inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split into two vectors: <strong>h</strong><sub>(<em>t</em>)</sub> and <strong>c</strong><sub>(<em>t</em>)</sub> (‚Äúc‚Äù stands for ‚Äúcell‚Äù). You can think of <strong>h</strong><sub>(<em>t</em>)</sub> as the short-term state and <strong>c</strong><sub>(<em>t</em>)</sub> as the long-term state.</p>

<figure><div id="lstm_cell_diagram" class="figure">
<img src="assets/hmls_1312.png" alt="Diagram of an LSTM cell showing the flow of information through forget, input, and output gates, illustrating how short-term and long-term states are updated." width="1420" height="953"/>
<h6><span class="label">Figure 13-12. </span>An LSTM cell</h6>
</div></figure>

<p>Now let‚Äôs open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state <strong>c</strong><sub>(<em>t</em>‚Äì1)</sub> traverses the network from left to right, you can see that it first goes<a data-type="indexterm" data-primary="forget gate, LSTM" id="id3160"/> through a <em>forget gate</em>, dropping some memories, and then it adds some new memories via the addition operation (which adds the memories<a data-type="indexterm" data-primary="input gate, LSTM" id="id3161"/> that were selected by an <em>input gate</em>). The result <strong>c</strong><sub>(<em>t</em>)</sub> is sent straight out without any further transformation. So at each time step, some memories are dropped and some memories are added. Moreover, after the addition operation, the long-term state is copied and passed through the tanh function, and the result is filtered<a data-type="indexterm" data-primary="output gate, LSTM" id="id3162"/> by the <em>output gate</em>. This produces the short-term state <strong>h</strong><sub>(<em>t</em>)</sub> (which is equal to the cell‚Äôs output for this time step, <strong>y</strong><sub>(<em>t</em>)</sub>). Now let‚Äôs look at where new memories come from and how the gates work.</p>

<p>First, the current input vector <strong>x</strong><sub>(<em>t</em>)</sub> and the previous short-term state <strong>h</strong><sub>(<em>t</em>‚Äì1)</sub> are fed to four different fully connected layers. They all serve a different purpose:</p>

<ul>
<li>
<p>The main layer is the one that outputs <strong>g</strong><sub>(<em>t</em>)</sub>. It has the usual role of analyzing the current inputs <strong>x</strong><sub>(<em>t</em>)</sub> and the previous (short-term) state <strong>h</strong><sub>(<em>t</em>‚Äì1)</sub>. In a simple RNN cell, there is nothing other than this layer, and its output goes straight out to <strong>y</strong><sub>(<em>t</em>)</sub> and <strong>h</strong><sub>(<em>t</em>)</sub>. But in an LSTM cell, this layer‚Äôs output does not go straight out; instead its most important parts are stored in the long-term state (and the rest is dropped).</p>
</li>
<li>
<p>The three other layers<a data-type="indexterm" data-primary="gate controllers, LSTM" id="id3163"/> are <em>gate controllers</em>. Since they use the logistic activation function, the outputs range from 0 to 1. As you can see, the gate controllers‚Äô outputs are fed to element-wise multiplication operations: if they output 0s they close the gate, and if they output 1s they open it. Specifically:</p>

<ul>
<li>
<p>The <em>forget gate</em> (controlled by <strong>f</strong><sub>(<em>t</em>)</sub>) controls which parts of the long-term state should be erased.</p>
</li>
<li>
<p>The <em>input gate</em> (controlled by <strong>i</strong><sub>(<em>t</em>)</sub>) controls which parts of <strong>g</strong><sub>(<em>t</em>)</sub> should be added to the long-term state.</p>
</li>
<li>
<p>Finally, the <em>output gate</em> (controlled by <strong>o</strong><sub>(<em>t</em>)</sub>) controls which parts of the long-term state should be read and output at this time step, both to <strong>h</strong><sub>(<em>t</em>)</sub> and to <strong>y</strong><sub>(<em>t</em>)</sub>.</p>
</li>
</ul>
</li>
</ul>

<p>In short, an LSTM cell can learn to recognize an important input (that‚Äôs the role of the input gate), store it in the long-term state, preserve it for as long as it is needed (that‚Äôs the role of the forget gate), and extract it whenever it is needed (that‚Äôs the role of the output gate), all while being fully differentiable. This explains why these cells have been amazingly successful at capturing long-term patterns in time series, long texts, audio recordings, and more.</p>

<p><a data-type="xref" href="#lstm_equation">Equation 13-4</a> summarizes how to compute the cell‚Äôs long-term state, its short-term state, and its output at each time step for a single instance (the equations for a whole mini-batch are very similar).</p>
<div id="lstm_equation" data-type="equation"><h5><span class="label">Equation 13-4. </span>LSTM computations</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>œÉ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>i</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>i</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>i</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>œÉ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>f</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>f</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>f</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>œÉ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>o</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>o</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>o</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">tanh</mo>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>g</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow> </msub></mrow> <mo>‚ä∫</mo> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>g</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>‚äó</mo>
          <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mspace width="0.166667em"/>
          <mo>+</mo>
          <mspace width="0.166667em"/>
          <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>‚äó</mo>
          <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">y</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>=</mo>
          <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>‚äó</mo>
          <mo form="prefix">tanh</mo>
          <mrow>
            <mo>(</mo>
            <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></div>

<p>In this equation:</p>

<ul>
<li>
<p><strong>W</strong><sub><em>xi</em></sub>, <strong>W</strong><sub><em>xf</em></sub>, <strong>W</strong><sub><em>xo</em></sub>, and <strong>W</strong><sub><em>xg</em></sub> are the weight matrices of each of the four layers for their connection to the input vector <strong>x</strong><sub>(<em>t</em>)</sub>.</p>
</li>
<li>
<p><strong>W</strong><sub><em>hi</em></sub>, <strong>W</strong><sub><em>hf</em></sub>, <strong>W</strong><sub><em>ho</em></sub>, and <strong>W</strong><sub><em>hg</em></sub> are the weight matrices of each of the four layers for their connection to the previous short-term state <strong>h</strong><sub>(<em>t</em>‚Äì1)</sub>.</p>
</li>
<li>
<p><strong>b</strong><sub><em>i</em></sub>, <strong>b</strong><sub><em>f</em></sub>, <strong>b</strong><sub><em>o</em></sub>, and <strong>b</strong><sub><em>g</em></sub> are the bias terms for each of the four layers.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>Try replacing <code translate="no">nn.RNN</code> with <code translate="no">nn.LSTM</code> in the previous models and see what performance you can reach on the ridership dataset (a bit of hyperparameter tuning may be required).</p>
</div>

<p>Just like for simple RNNs, if you want to add layer normalization or dropout at each time step, you must implement the recurrent loop manually. One option is to use <a data-type="xref" href="#lstm_equation">Equation 13-4</a>, but a simpler option is to use the <code translate="no">nn.LSTMCell</code> module, which runs a single time step. For example, here is a simple implementation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">LstmModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden_size</code> <code class="o">=</code> <code class="n">hidden_size</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">memory_cell</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTMCell</code><code class="p">(</code><code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">batch_size</code><code class="p">,</code> <code class="n">window_length</code><code class="p">,</code> <code class="n">dimensionality</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code>
        <code class="n">X_time_first</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="n">H</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>
        <code class="n">C</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">device</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">X_t</code> <code class="ow">in</code> <code class="n">X_time_first</code><code class="p">:</code>
            <code class="n">H</code><code class="p">,</code> <code class="n">C</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">memory_cell</code><code class="p">(</code><code class="n">X_t</code><code class="p">,</code> <code class="p">(</code><code class="n">H</code><code class="p">,</code> <code class="n">C</code><code class="p">))</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">H</code><code class="p">)</code></pre>

<p>This is very similar to the first implementation of our <code translate="no">SimpleRnnModel</code>, but we are now using an <code translate="no">nn.LSTMCell</code> at each time step, and the hidden state is now split in two parts: the short-term <code translate="no">H</code> and the long-term <code translate="no">C</code>.</p>

<p>There are several variants of the LSTM cell. One particularly popular variant is the GRU cell, which we will look at now.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="GRU cells"><div class="sect3" id="id257">
<h3>GRU cells</h3>

<p>The <em>gated recurrent unit</em> (GRU) cell<a data-type="indexterm" data-primary="GRU (gated recurrent unit) cell" id="xi_GRUgatedrecurrentunitcell1399738_1"/><a data-type="indexterm" data-primary="gated recurrent unit (GRU) cell" id="xi_gatedrecurrentunitGRUcell1399738_1"/> (see <a data-type="xref" href="#gru_cell_diagram">Figure¬†13-13</a>) was proposed by Kyunghyun Cho et al. in a <a href="https://homl.info/97">2014 paper</a>‚Å†<sup><a data-type="noteref" id="id3164-marker" href="ch13.html#id3164">14</a></sup> that also introduced the encoder-decoder network we discussed earlier.</p>

<figure class="width-80"><div id="gru_cell_diagram" class="figure">
<img src="assets/hmls_1313.png" alt="Diagram of a GRU cell illustrating its components and flow of information including reset, update, and candidate activation functions." width="1256" height="1077"/>
<h6><span class="label">Figure 13-13. </span>GRU cell</h6>
</div></figure>

<p>The GRU cell is a simplified version of the LSTM cell, and it often performs just as well.‚Å†<sup><a data-type="noteref" id="id3165-marker" href="ch13.html#id3165">15</a></sup> These are the main simplifications:</p>

<ul>
<li>
<p>Both state vectors are merged into a single vector <strong>h</strong><sub>(<em>t</em>)</sub>.</p>
</li>
<li>
<p>A single gate controller <strong>z</strong><sub>(<em>t</em>)</sub> controls both the forget gate and the input gate. If the gate controller outputs a 1, the forget gate is open (=¬†1) and the input gate is closed (1¬†‚Äì¬†1¬†=¬†0). If it outputs a 0, the opposite happens. In other words, whenever a memory must be stored, the location where it will be stored is erased first.</p>
</li>
<li>
<p>There is no output gate; the full state vector is output at every time step. However, there is a new gate controller <strong>r</strong><sub>(<em>t</em>)</sub> that controls which part of the previous state will be shown to the main layer (<strong>g</strong><sub>(<em>t</em>)</sub>).</p>
</li>
</ul>

<p><a data-type="xref" href="#gru_equation">Equation 13-5</a> summarizes how to compute the cell‚Äôs state at each time step for a single instance.</p>
<div id="gru_equation" data-type="equation">
<h5><span class="label">Equation 13-5. </span>GRU computations</h5>
<math alttext="StartLayout 1st Row 1st Column Blank 2nd Column bold z Subscript left-parenthesis t right-parenthesis Baseline equals sigma left-parenthesis bold upper W Subscript x z Baseline Superscript upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline plus bold upper W Subscript h z Baseline Superscript upper T Baseline bold h Subscript left-parenthesis t minus 1 right-parenthesis Baseline plus bold b Subscript z Baseline right-parenthesis 2nd Row 1st Column Blank 2nd Column bold r Subscript left-parenthesis t right-parenthesis Baseline equals sigma left-parenthesis bold upper W Subscript x r Baseline Superscript upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline plus bold upper W Subscript h r Baseline Superscript upper T Baseline bold h Subscript left-parenthesis t minus 1 right-parenthesis Baseline plus bold b Subscript r Baseline right-parenthesis 3rd Row 1st Column Blank 2nd Column bold g Subscript left-parenthesis t right-parenthesis Baseline equals hyperbolic tangent left-parenthesis bold upper W Subscript x g Baseline Superscript upper T Baseline bold x Subscript left-parenthesis t right-parenthesis Baseline plus bold upper W Subscript h g Baseline Superscript upper T Baseline left-parenthesis r Subscript left-parenthesis t right-parenthesis Baseline circled-times bold h Subscript left-parenthesis t minus 1 right-parenthesis Baseline right-parenthesis plus bold b Subscript g Baseline right-parenthesis 4th Row 1st Column Blank 2nd Column bold h Subscript left-parenthesis t right-parenthesis Baseline equals bold z Subscript left-parenthesis t right-parenthesis Baseline circled-times bold h Subscript left-parenthesis t minus 1 right-parenthesis Baseline plus left-parenthesis 1 minus bold z Subscript left-parenthesis t right-parenthesis Baseline right-parenthesis circled-times bold g Subscript left-parenthesis t right-parenthesis EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <msub><mi>ùê≥</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>=</mo>
          <mi>œÉ</mi>
          <mrow>
            <mo>(</mo>
            <msup><mrow><msub><mi>ùêñ</mi> <mrow><mi>x</mi><mi>z</mi></mrow> </msub></mrow> <mtext>T</mtext> </msup>
            <msub><mi>ùê±</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>+</mo>
            <msup><mrow><msub><mi>ùêñ</mi> <mrow><mi>h</mi><mi>z</mi></mrow> </msub></mrow> <mtext>T</mtext> </msup>
            <msub><mi>ùê°</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
            <mo>+</mo>
            <msub><mi>ùêõ</mi> <mi>z</mi> </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <msub><mi>ùê´</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>=</mo>
          <mi>œÉ</mi>
          <mrow>
            <mo>(</mo>
            <msup><mrow><msub><mi>ùêñ</mi> <mrow><mi>x</mi><mi>r</mi></mrow> </msub></mrow> <mtext>T</mtext> </msup>
            <msub><mi>ùê±</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>+</mo>
            <msup><mrow><msub><mi>ùêñ</mi> <mrow><mi>h</mi><mi>r</mi></mrow> </msub></mrow> <mtext>T</mtext> </msup>
            <msub><mi>ùê°</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
            <mo>+</mo>
            <msub><mi>ùêõ</mi> <mi>r</mi> </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <msub><mi>ùê†</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>=</mo>
          <mtext>tanh</mtext>
          <mrow>
            <mo>(</mo>
            <msup><mrow><msub><mi>ùêñ</mi> <mrow><mi>x</mi><mi>g</mi></mrow> </msub></mrow> <mtext>T</mtext> </msup>
            <msub><mi>ùê±</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>+</mo>
            <msup><mrow><msub><mi>ùêñ</mi> <mrow><mi>h</mi><mi>g</mi></mrow> </msub></mrow> <mtext>T</mtext> </msup>
            <mrow>
              <mo>(</mo>
              <msub><mi>r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
              <mo>‚äó</mo>
              <msub><mi>ùê°</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
              <mo>)</mo>
            </mrow>
            <mo>+</mo>
            <msub><mi>ùêõ</mi> <mi>g</mi> </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <msub><mi>ùê°</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>=</mo>
          <msub><mi>ùê≥</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>‚äó</mo>
          <msub><mi>ùê°</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <msub><mi>ùê≥</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>)</mo>
          </mrow>
          <mo>‚äó</mo>
          <msub><mi>ùê†</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>PyTorch provides an <code translate="no">nn.GRU</code> layer; using it is just a matter<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3166"/> of replacing <code translate="no">nn.RNN</code> or <code translate="no">nn.LSTM</code> with <code translate="no">nn.GRU</code>. It also provides<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRUCell" id="id3167"/> an <code translate="no">nn.GRUCell</code> in case you want to create a custom RNN based on a GRU cell (just replace <code translate="no">nn.LSTMCell</code> with <code translate="no">nn.GRUCell</code> in the previous example, and get rid of <code translate="no">C</code>).</p>

<p>LSTM and GRU are one of the main reasons behind the success of RNNs. Yet while they can tackle much longer sequences than simple RNNs, they still have a fairly limited short-term memory, and they have a hard time learning long-term patterns in sequences of 100 time steps or more, such as audio samples, long time series, or long sentences. One way to solve this is to shorten the input sequences, for example, using 1D convolutional layers<a data-type="indexterm" data-startref="xi_GRUgatedrecurrentunitcell1399738_1" id="id3168"/><a data-type="indexterm" data-startref="xi_longshorttermmemoryLSTMcell13811619_1" id="id3169"/><a data-type="indexterm" data-startref="xi_gatedrecurrentunitGRUcell1399738_1" id="id3170"/><a data-type="indexterm" data-startref="xi_LSTMlongshorttermmemorycell13811619_1" id="id3171"/><a data-type="indexterm" data-startref="xi_memorycellsRNNs13811619_1" id="id3172"/><a data-type="indexterm" data-startref="xi_recurrentneuralnetworksRNNsmemorycells13811619_1" id="id3173"/>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Using 1D convolutional layers to process sequences"><div class="sect3" id="id258">
<h3>Using 1D convolutional layers to process sequences</h3>

<p>In <a data-type="xref" href="ch12.html#cnn_chapter">Chapter¬†12</a>, we saw that a 2D convolutional layer<a data-type="indexterm" data-primary="1D convolutional layers" data-primary-sortas="one d convolutional layers" id="xi_1Dconvolutionallayers13103157_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="convolutional layers" id="xi_convolutionalneuralnetworksCNNsconvolutionallayers13103157_1"/> works by sliding several fairly small kernels (or filters) across an image, producing multiple 2D feature maps (one per kernel). Similarly, a 1D convolutional layer slides several kernels across a sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a single very short sequential pattern (no longer than the kernel size). If you use 10 kernels, then the layer‚Äôs output will be composed of 10 1D sequences (all of the same length), or equivalently you can view this output as a single 10D sequence. This means that you can build a neural network composed of a mix of recurrent layers and 1D convolutional layers (or even 1D pooling layers). However, as mentioned earlier, you must swap the last two dimensions of the <code translate="no">nn.Conv1d</code> layer‚Äôs inputs and outputs, since the <code translate="no">nn.Conv1d</code> layer expects inputs of shape [<em>batch size</em>, <em>input features</em>, <em>sequence length</em>], and produces outputs of shape [<em>batch size</em>, <em>output features</em>, <em>sequence length</em>].</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you use a 1D convolutional layer with a stride<a data-type="indexterm" data-primary="strides" id="id3174"/> of 1 and <code translate="no">"same"</code> padding, then the output sequence will have the same length as the input sequence. But if you use <code translate="no">"valid"</code> padding or a stride greater than 1, then the output sequence will be shorter than the input sequence, so make sure you adjust the targets accordingly.</p>
</div>

<p>For example, the following model is composed of a 1D convolutional layer, followed by a GRU layer, and lastly a linear output layer, all of which input and output batches of sequences (i.e., 3D tensors). The <code translate="no">nn.Conv1d</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.Conv1d" id="id3175"/> downsamples the input sequences by a factor of 2, using a stride of 2. The kernel size is as large as the stride (larger, in fact), so all inputs will be used to compute the layer‚Äôs output, and therefore the model can learn to preserve the most useful information, dropping only the unimportant details. In the <code translate="no">forward()</code> method, we just chain the layers, but we permute the last two dimensions before and after the <code translate="no">nn.Conv1d</code> layer, and we ignore the hidden states returned by the <code translate="no">nn.GRU</code> layer<a data-type="indexterm" data-primary="torch" data-secondary="nn.GRU" id="id3176"/>:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DownsamplingModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv1d</code><code class="p">(</code><code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">stride</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">gru</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># treat time as a spatial dimension</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># swap back time &amp; features dimensions</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="n">Z</code><code class="p">,</code> <code class="n">_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gru</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">dseq_model</code> <code class="o">=</code> <code class="n">DownsamplingModel</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">output_size</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code>
<code class="n">dseq_model</code> <code class="o">=</code> <code class="n">dseq_model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>By shortening the time series, the convolutional layer helps the <code translate="no">GRU</code> layer detect longer patterns, so we can afford to double the window length to 112 days. Note that we must also crop off the first three time steps from the targets: indeed, the kernel‚Äôs size is 4, so the first output of the convolutional layer will be based on the input time steps 0 to 3, therefore the first forecasts must be for time steps 4 to 17 (instead of time steps 1 to 14). Moreover, we must downsample the targets by a factor of 2 because of the stride. For all this, we need a new <code translate="no">Dataset</code> class, so let‚Äôs create a subclass of the <code translate="no">Seq2SeqDataset</code> class:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DownsampledDataset</code><code class="p">(</code><code class="n">Seq2SeqDataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="n">window</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__getitem__</code><code class="p">(</code><code class="n">idx</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">window</code><code class="p">,</code> <code class="n">target</code><code class="p">[</code><code class="mi">3</code><code class="p">::</code><code class="mi">2</code><code class="p">]</code>  <code class="c1"># crop the first 3 targets and downsample</code>

<code class="n">window_length</code> <code class="o">=</code> <code class="mi">112</code>
<code class="n">dseq_train_set</code> <code class="o">=</code> <code class="n">DownsampledDataset</code><code class="p">(</code><code class="n">rail_train</code><code class="p">,</code> <code class="n">window_length</code><code class="p">)</code>
<code class="n">dseq_train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">dseq_train_set</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>  <code class="c1"># create the datasets and data loaders for the validation and test sets</code></pre>

<p>And now the model can be trained as usual. We‚Äôve successfully mixed convolutional layers and recurrent layers. But what if we used only 1D convolutional layers and dropped the recurrent layers entirely?</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="WaveNet"><div class="sect3" id="id259">
<h3>WaveNet</h3>

<p>In a <a href="https://homl.info/wavenet">2016 paper</a>,‚Å†<sup><a data-type="noteref" id="id3177-marker" href="ch13.html#id3177">16</a></sup> Aaron van den Oord and other DeepMind researchers introduced a novel architecture called <em>WaveNet</em>. They stacked 1D convolutional layers<a data-type="indexterm" data-primary="WaveNet" id="xi_WaveNet131077307_1"/><a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="WaveNet" id="xi_convolutionalneuralnetworksCNNsWaveNet131077307_1"/>, doubling the dilation rate (how spread apart each neuron‚Äôs inputs are) at every layer: the first convolutional layer gets a glimpse of just two time steps at a time, while the next one sees four time steps (its receptive field is four time steps long), the next one sees eight time steps, and so on (see <a data-type="xref" href="#wavenet_diagram">Figure¬†13-14</a>). This way, the lower layers learn short-term patterns, while the higher layers learn long-term patterns. Thanks to the doubling dilation rate, the network can process extremely large sequences very efficiently.</p>

<figure><div id="wavenet_diagram" class="figure">
<img src="assets/hmls_1314.png" alt="Diagram illustrating the WaveNet architecture with stacked 1D convolutional layers showing increasing dilation rates, demonstrating how the model processes time steps at varying scales for efficient audio generation." width="1390" height="565"/>
<h6><span class="label">Figure 13-14. </span>WaveNet architecture</h6>
</div></figure>

<p>The authors of the paper actually stacked 10 convolutional layers with dilation rates of 1, 2, 4, 8, ‚Ä¶‚Äã, 256, 512, then they stacked another group of 10 identical layers (also with dilation rates 1, 2, 4, 8, ‚Ä¶‚Äã, 256, 512), then again another identical group of 10 layers. They justified this architecture by pointing out that a single stack of 10 convolutional layers with these dilation rates will act like a super-efficient 
<span class="keep-together">convolutional</span> layer with a kernel of size 1,024 (except way faster, more powerful, and using significantly fewer parameters). They also left-padded the input sequences with a number of zeros equal to the dilation rate before every layer to preserve the same sequence length<a data-type="indexterm" data-primary="sequence length" id="id3178"/> throughout the network. Padding on the left rather than on both sides is important, as it ensures that the convolutional layer does not peek into the future when making predictions. This makes it a causal model.</p>

<p>Let‚Äôs implement a simplified WaveNet to tackle the same sequences<a data-type="indexterm" data-primary="gated activation units" id="id3179"/> as earlier.‚Å†<sup><a data-type="noteref" id="id3180-marker" href="ch13.html#id3180">17</a></sup> We will start by creating a custom <code translate="no">CausalConv1d</code> module that acts just like an <code translate="no">nn.Conv1d</code> module, except the inputs get padded on the left side by the appropriate amount to ensure the sequence preserves the same length:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">mport</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code> <code class="k">as</code> <code class="n">F</code>

<code class="k">class</code> <code class="nc">CausalConv1d</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Conv1d</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">padding</code> <code class="o">=</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">dilation</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">pad</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="p">(</code><code class="n">padding</code><code class="p">,</code> <code class="mi">0</code><code class="p">))</code>
        <code class="k">return</code> <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>In this code, we inherit from the <code translate="no">nn.Conv1d</code> class and we override the <code translate="no">forward()</code> method. In it, we calculate the size of the left-padding we need, and we pad the sequences using the <code translate="no">pad()</code> function before calling the base class‚Äôs <code translate="no">forward()</code> method. The <code translate="no">pad()</code> function takes two arguments: the tensor to pad (<code translate="no">X</code>), and a tuple of ints that indicates how much to pad to the left and right in the last dimension (i.e., the time dimension).</p>

<p>Now we‚Äôre ready to build the WaveNet model itself:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">WavenetModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="n">layers</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="k">for</code> <code class="n">dilation</code> <code class="ow">in</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code> <code class="o">*</code> <code class="mi">2</code><code class="p">:</code>
            <code class="n">conv</code> <code class="o">=</code> <code class="n">CausalConv1d</code><code class="p">(</code><code class="n">input_size</code><code class="p">,</code> <code class="n">hidden_size</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                                <code class="n">dilation</code><code class="o">=</code><code class="n">dilation</code><code class="p">)</code>
            <code class="n">layers</code> <code class="o">+=</code> <code class="p">[</code><code class="n">conv</code><code class="p">,</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()]</code>
            <code class="n">input_size</code> <code class="o">=</code> <code class="n">hidden_size</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">convs</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="o">*</code><code class="n">layers</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">convs</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">output</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">wavenet_model</code> <code class="o">=</code> <code class="n">WavenetModel</code><code class="p">(</code><code class="n">input_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">hidden_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">output_size</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code>
<code class="n">wavenet_model</code> <code class="o">=</code> <code class="n">wavenet_model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>

<p>In the constructor, we create eight <code translate="no">CausalConv1d</code> layers with various dilation rates (1, 2, 4, 8, then again 1, 2, 4, 8), each followed by the ReLU activation function. We chain all these modules in an <code translate="no">nn.Sequential</code> module <code translate="no">self.convs</code>. We also create the output <code translate="no">nn.Linear</code> layer. In the forward method, we permute the last two dimensions of the inputs, as we did earlier, we then pass them through the convolutional layers, then we permute the last two dimensions back to their original order, and we pass the result through the output layer. Thanks to the causal padding, every convolutional layer outputs a sequence of the same length as its input sequence, so the targets we use during training can be the full 112-day sequences; no need to crop them or downsample them. Thus, we can train the model using the data loaders we built for the <code translate="no">Seq2SeqModel</code> (i.e., <code translate="no">seq_train_loader</code> and <code translate="no">seq_valid_loader</code>).</p>

<p>The models we‚Äôve discussed in this section offer similar performance for the ridership forecasting task, but they may vary significantly depending on the task and the amount of available data. In the WaveNet paper, the authors achieved state-of-the-art performance on various audio tasks (hence the name of the architecture), including text-to-speech tasks, producing very realistic voices across several languages. They also used the model to generate music, one audio sample at a time. This feat is all the more impressive when you realize that a single second of audio can contain tens of thousands of time steps‚Äîeven LSTMs and GRUs cannot handle such long sequences.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If you evaluate our best Chicago ridership models on the test period, starting in 2020, you will find that they perform much worse than expected! Why is that? Well, that‚Äôs when the Covid-19 pandemic started, which greatly affected public transportation. As mentioned earlier, these models will only work well if the patterns they learned from the past continue in the future. In any case, before deploying a model to production, verify that it works well on recent data. And once it‚Äôs in production, make sure to monitor its performance regularly<a data-type="indexterm" data-startref="xi_recurrentneuralnetworksRNNshandlinglongsequences1378034_1" id="id3181"/><a data-type="indexterm" data-startref="xi_longsequencestrainingRNNon1378034_1" id="id3182"/><a data-type="indexterm" data-startref="xi_shorttermmemoryproblemRNNs1381127_1" id="id3183"/><a data-type="indexterm" data-startref="xi_1Dconvolutionallayers13103157_1" id="id3184"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsconvolutionallayers13103157_1" id="id3185"/><a data-type="indexterm" data-startref="xi_WaveNet131077307_1" id="id3186"/><a data-type="indexterm" data-startref="xi_convolutionalneuralnetworksCNNsWaveNet131077307_1" id="id3187"/>.</p>
</div>

<p>With that, you can now tackle all sorts of time series! In <a data-type="xref" href="ch14.html#nlp_chapter">Chapter¬†14</a>, we will continue to explore RNNs, and we will see how they can tackle various NLP tasks as well.</p>
</div></section>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="id260">
<h1>Exercises</h1>
<ol>
<li>
<p>Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?</p>
</li>
<li>
<p>How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?</p>
</li>
<li>
<p>How can you build a deep sequence-to-sequence RNN in PyTorch?</p>
</li>
<li>
<p>Suppose you have a daily univariate time series, and you want to forecast the next seven days using an RNN. Which architecture should you use?</p>
</li>
<li>
<p>What are the main difficulties when training RNNs? How can you handle them?</p>
</li>
<li>
<p>Can you sketch the LSTM cell‚Äôs architecture?</p>
</li>
<li>
<p>Why would you want to use 1D convolutional layers in an RNN?</p>
</li>
<li>
<p>Which neural network architecture could you use to classify videos?</p>
</li>
<li>
<p>Try to tweak the <code translate="no">Seq2SeqModel</code> model to forecast both rail and bus ridership for the next 14 days. The model will now need to predict 28 values instead of 14.</p>
</li>
<li>
<p>Download the <a href="https://homl.info/bach">Bach chorales</a> dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note‚Äôs index on a piano (except for the value 0, which means that no note is played). Train a model‚Äîrecurrent, convolutional, or 
<span class="keep-together">both‚Äîthat</span> can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out <a href="https://homl.info/coconet">Google‚Äôs Coconet model</a>, which was used for a nice Google doodle about Bach.</p>
</li>
<li>
<p>Train a classification model for the <a href="https://homl.info/quickdraw">QuickDraw dataset</a>, which contains millions of sketches of various objects. Start by downloading the simplified data for a few classes (e.g., <em>ant.ndjson</em>, <em>axe.ndjson</em>, and <em>bat.ndjson</em>). Each NDJSON file contains one JSON object per line, which you can parse using Python‚Äôs <code translate="no">json.loads()</code> function. This will give you a list of sketches, where each sketch is represented as a Python dictionary. In each dictionary, the <code translate="no">"drawing"</code> entry contains a list of pen strokes. You can convert this list to a 3D float tensor where the dimensions are [<em>strokes</em>, <em>x coordinates</em>, <em>y coordinates</em>]. Since an RNN takes a single sequence as input, you will need to concatenate all the strokes for each sketch into a single sequence. It‚Äôs best to add an extra feature to allow the RNN to know how far along each stroke it currently is (e.g., from 0 to 1). In other words, the model will receive a sequence where each time step has three features: the <em>x</em> and <em>y</em> coordinates of the pen, and the progress ratio along the current stroke.</p>
</li>
<li>
<p>Create a dataset containing short audio recordings of you saying ‚Äúyes‚Äù or ‚Äúno‚Äù, and train a binary classification RNN on it. For example, you could:</p>
<ol>
<li>
<p>Use an audio recording software such as Audacity to record yourself saying ‚Äúyes‚Äù as many times as your patience allows, with short pauses between each word. Create a similar recording for the word ‚Äúno‚Äù. Try to cover the various ways you might realistically pronounce these words in real life.</p>
</li>
<li>
<p>Load each WAV file using the <code translate="no">torchaudio.load()</code> function from the TorchAudio library. This will return a tensor containing the audio, as well as an integer indicating the number of samples per second. The audio tensor has a shape of [<em>channels</em>, <em>samples</em>]: one channel for mono, two for stereo. Convert stereo to mono by averaging over the channel dimension.</p>
</li>
<li>
<p>Chop each recording into individual words by splitting at the silences. You can do this using the <code translate="no">torchaudio.transforms.Vad</code> transform (Voice Activity Detection).</p>
</li>
<li>
<p>Since the sequences are so long, it‚Äôs hard to directly train an RNN on them, so it helps to convert the audio to a spectrogram first. For this, you can use the <code translate="no">torchaudio.transforms.MelSpectrogram</code> transform, which is well suited for voice. The output is a dramatically shorter sequence, with many more channels.</p>
</li>
<li>
<p>Now try building and training a binary classification RNN on your yes/no dataset! Consider sharing your dataset and model with the world (e.g., via the Hugging Face Hub)<a data-type="indexterm" data-startref="xi_recurrentneuralnetworksRNNs13322_1" id="id3188"/>.</p>
</li>

</ol>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter‚Äôs notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id3062"><sup><a href="ch13.html#id3062-marker">1</a></sup> Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather than the ReLU activation function. For example, see Vu Pham et al.‚Äôs <a href="https://homl.info/91">2013 paper</a> ‚ÄúDropout Improves Recurrent Neural Networks for Handwriting Recognition‚Äù. ReLU-based RNNs are also possible, as shown in Quoc V. Le et al.‚Äôs <a href="https://homl.info/92">2015 paper</a> ‚ÄúA Simple Way to Initialize Recurrent Networks of Rectified Linear Units‚Äù.</p><p data-type="footnote" id="id3066"><sup><a href="ch13.html#id3066-marker">2</a></sup> Michael I. Jordan, ‚ÄúAttractor Dynamics and Parallelism in a Connectionist Sequential Machine‚Äù, <em>Proceedings of the Eighth Annual Conference of the Cognitive Science Society</em> (1986).</p><p data-type="footnote" id="id3069"><sup><a href="ch13.html#id3069-marker">3</a></sup> Jeffrey L. Elman, ‚ÄúFinding Structure in Time‚Äù, <em>Cognitive Science</em>, Volume 14, Issue 2 (1990).</p><p data-type="footnote" id="id3076"><sup><a href="ch13.html#id3076-marker">4</a></sup> Nal Kalchbrenner and Phil Blunsom, ‚ÄúRecurrent Continuous Translation Models‚Äù, <em>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em> (2013): 1700‚Äì1709.</p><p data-type="footnote" id="id3084"><sup><a href="ch13.html#id3084-marker">5</a></sup> The latest data from the Chicago Transit Authority is available at the <a href="https://homl.info/ridership">Chicago Data Portal</a>.</p><p data-type="footnote" id="id3107"><sup><a href="ch13.html#id3107-marker">6</a></sup> For more details on the ACF-PACF approach, check out this very nice <a href="https://homl.info/arimatuning">post by Jason Brownlee</a>.</p><p data-type="footnote" id="id3125"><sup><a href="ch13.html#id3125-marker">7</a></sup> Note that the validation period starts on the 1st of January 2019, so the first prediction is for the 26th of February 2019, eight weeks later. When we evaluated the baseline models, we used predictions starting on the 1st of March instead, but this should be close enough.</p><p data-type="footnote" id="id3135"><sup><a href="ch13.html#id3135-marker">8</a></sup> We cannot use the <code translate="no">ahead_model</code> for this because it needs both the rail and bus ridership as input, but it only forecasts the rail ridership.</p><p data-type="footnote" id="id3144"><sup><a href="ch13.html#id3144-marker">9</a></sup> C√©sar Laurent et al., ‚ÄúBatch Normalized Recurrent Neural Networks‚Äù, <em>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</em> (2016): 2657‚Äì2661.</p><p data-type="footnote" id="id3155"><sup><a href="ch13.html#id3155-marker">10</a></sup> A character from the animated movies <em>Finding Nemo</em> and <em>Finding Dory</em> who has short-term memory loss.</p><p data-type="footnote" id="id3156"><sup><a href="ch13.html#id3156-marker">11</a></sup> Sepp Hochreiter and J√ºrgen Schmidhuber, ‚ÄúLong Short-Term Memory‚Äù, <em>Neural Computation</em> 9, no. 8 (1997): 1735‚Äì1780.</p><p data-type="footnote" id="id3157"><sup><a href="ch13.html#id3157-marker">12</a></sup> Ha≈üim Sak et al., ‚ÄúLong Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition‚Äù, arXiv preprint arXiv:1402.1128 (2014).</p><p data-type="footnote" id="id3158"><sup><a href="ch13.html#id3158-marker">13</a></sup> Wojciech Zaremba et al., ‚ÄúRecurrent Neural Network Regularization‚Äù, arXiv preprint arXiv:1409.2329 (2014).</p><p data-type="footnote" id="id3164"><sup><a href="ch13.html#id3164-marker">14</a></sup> Kyunghyun Cho et al., ‚ÄúLearning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation‚Äù, <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</em> (2014): 1724‚Äì1734.</p><p data-type="footnote" id="id3165"><sup><a href="ch13.html#id3165-marker">15</a></sup> See Klaus Greff et al., <a href="https://homl.info/98">‚ÄúLSTM: A Search Space Odyssey‚Äù</a>, <em>IEEE Transactions on Neural Networks and Learning Systems</em> 28, no. 10 (2017): 2222‚Äì2232. This paper seems to show that all LSTM variants perform roughly the same.</p><p data-type="footnote" id="id3177"><sup><a href="ch13.html#id3177-marker">16</a></sup> Aaron van den Oord et al., ‚ÄúWaveNet: A Generative Model for Raw Audio‚Äù, arXiv preprint arXiv:1609.03499 (2016).</p><p data-type="footnote" id="id3180"><sup><a href="ch13.html#id3180-marker">17</a></sup> The complete WaveNet uses a few more tricks, such as skip connections like in a ResNet, and <em>gated activation units</em> similar to those found in a GRU cell.</p></div></div></section></div></div></body></html>