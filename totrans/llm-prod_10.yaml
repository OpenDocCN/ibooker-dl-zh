- en: '11 Deploying an LLM on a Raspberry Pi: How low can you go?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setting up a Raspberry Pi server on your local network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting and quantizing a model to GGUF format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serving your model as a drop-in replacement to the OpenAI GPT model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to do next and how to make it better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The bitterness of poor quality remains long after the sweetness of low price
    is forgotten.*—Benjamin Franklin'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Welcome to one of our favorite projects on this list: serving an LLM on a device
    smaller than it should ever be served on. In this project, we will be pushing
    to the edge of this technology. By following along, you’ll be able to really flex
    everything you’ve learned in this book. In this project, we’ll deploy an LLM to
    a Raspberry Pi, which we will set up as an LLM Service you can query from any
    device on your home network. For all the hackers out there, this exercise should
    open the doors to many home projects. For everyone else, it’s a chance to solidify
    your understanding of the limitations of using LLMs and appreciate the community
    that has made this possible.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a practical project. In this chapter, we’ll dive into much more than
    LLMs, and there won’t be any model training or data focusing, so it is our first
    truly production-only project. What we’ll create will be significantly slower,
    less efficient, and less accurate than what you’re probably expecting, and that’s
    fine. Actually, it’s a wonderful learning experience. Understanding the difference
    between possible and useful is something many never learn until it smacks them
    across the face. An LLM running on a Raspberry Pi isn’t something you’ll want
    to deploy in an enterprise production system, but we will help you learn the principles
    behind it so you can eventually scale up to however large you’d like down the
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Setting up your Raspberry Pi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Serving and inferencing on a Raspberry Pi despite all odds is doable, although
    we generally don’t recommend doing so other than to show that you can, which is
    the type of warning that is the telltale sign of a fun project, like figuring
    out how many marshmallows you can fit in your younger brother’s mouth. Messing
    with Raspberry Pis by themselves is pretty fun in general, and we hope that this
    isn’t the first time you’ve played with one. Raspberry Pis make great, cheap servers
    for your home. You can use them for ad blocking (Pi-Hole is a popular library)
    or media streaming your own personal library with services like Plex and Jellyfin.
    There are lots of fun projects. Because it’s fully customizable, if you can write
    a functional Python script, you can likely run it on a Raspberry Pi server for
    your local network to consume, which is what we are going to do for our LLM server.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll just need three things to do this project: a Raspberry Pi with 8 GB
    of RAM, a MicroSD (at least 32 GB, but more is better), and a power supply. At
    the time of this writing, we could find several MicroSD cards with 1 TB of memory
    for $20, so hopefully, you get something much bigger than 32 GB. Anything else
    you purchase is just icing on the cake—for example, a case for your Pi. If you
    don’t have Wi-Fi, you’ll also need an ethernet cable to connect your Pi to your
    home network. We’ll show you how to remote into your Pi from your laptop once
    we get it up. In addition, if your laptop doesn’t come with a MicroSD slot, you’ll
    need some sort of adapter to connect it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Raspberry Pi itself, we will be using the Raspberry Pi 5 8 GB model
    for this project. If you’d like to follow along, the exact model we’re using can
    be found here: [https://mng.bz/KDZg](https://mng.bz/KDZg). For the model we’ll
    deploy, you’ll need a single-board computer with at least 8 GB of RAM to follow
    along. As a fun fact, we have been successful in deploying models to smaller Pis
    with only 4GB of RAM, and plenty of other single-board alternatives to the Raspberry
    Pi are available. If you choose a different board, though, it might be more difficult
    to follow along exactly, so do so only if you trust the company. Some alternatives
    we recommend include Orange Pi, Zima Board, and Jetson, but we won’t go over how
    to set these up.'
  prefs: []
  type: TYPE_NORMAL
- en: You won’t need to already know how to set up a Pi. We will walk you through
    all the steps, assuming this is your first Raspberry Pi project. A Pi is literally
    just hardware and an open sandbox for lots of projects, so we will first have
    to install an operating system (OS). After that, we’ll install the necessary packages
    and libraries, prepare our LLM, and finally serve it as a service you can ping
    from any computer in your home network and get generated text.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.1 Pi Imager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start off, Pis don’t usually come with an OS installed, and even if yours
    did, we’re going to change it. Common distributions like Rasbian OS or Ubuntu
    are too large and take too much RAM to run models at their fastest. To help us
    with this limitation, Raspberry Pi’s makers have released a free imaging software
    called the Pi Imager that you can download on your laptop from here: [https://www.raspberrypi.com/software/](https://www.raspberrypi.com/software/).
    If you already have the imager, we recommend updating it to a version higher than
    1.8 since we are using a Pi 5.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have it, plug the microSD into the computer where you’ve downloaded
    the Pi Imager program. (If you aren’t sure how to do this, search online for the
    USB 3.0 microSD Card Reader.) Open the imager and select the device; for us, that’s
    Raspberry Pi 5\. This selection will limit the OS options to those available for
    the Pi 5\. Then you can select the Raspberry Pi OS Lite 64-bit for your operating
    system. *Lite* is the keyword you are looking for, and you will likely have to
    find it in the Raspberry Pi OS (Other) subsection. Then select your microSD as
    your storage device. The actual name will vary depending on your setup. Figure
    11.1 shows an example of the Imager software with the correct settings. As a note,
    the Ubuntu Server is also a good operating system that would work for our project,
    and we’d recommend it. It’ll have a slightly different setup, so if you want to
    follow along, stick with a Raspberry Pi OS Lite.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 Raspberry Pi Imager set to the correct device, with the headless
    (Lite) operating system and the correct USB storage device selected
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: WARNING  And as a warning, make sure that you’ve selected the microSD to image
    the OS—please do not select your main hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are ready, navigate forward by selecting the Next button, and you should
    see a prompt asking about OS customizations, as shown in figure 11.2\. We will
    set this up, so click the Edit Settings button, and you should see a settings
    page.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Customizing our Raspberry Pi OS settings. Select Edit Settings.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 11.3 shows an example of the settings page. We’ll give the Pi server
    a hostname after the project, llmpi. We’ll set a username and password and configure
    the Wi-Fi settings to connect to our home network. This is probably the most important
    step, so make sure that you’re set up for the internet, either by setting up your
    Wi-Fi connection in settings or via ethernet.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 Example screenshot of the settings page with correct and relevant
    information
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Just as important as setting up the internet, we want to enable SSH, or none
    of the subsequent steps will work. To do this, go to the Services tab and select
    Enable SSH, as seen in figure 11.4\. We will use password authentication, so make
    sure you’ve set an appropriate username and password and are not leaving it to
    the default settings. You don’t want anyone with bad intentions to have super
    easy access to your Pi.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we are ready to image. Move forward through the prompts, and
    the imager will install the OS onto your SD card. This process can take a few
    minutes but is usually over pretty quickly. Once your SD has your OS on it, you
    can remove it safely from your laptop. Put the microSD card in your Pi, and turn
    it on! If everything was done correctly, your Pi should automatically boot up
    and connect to your Wi-Fi.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Make sure you select Enable SSH.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.1.2 Connecting to Pi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use our little Pi like a small server. What’s nice about our setup is
    that you won’t need to find an extra monitor or keyboard to plug into your Pi.
    Of course, this setup comes with the obvious drawback that we can’t see what the
    Pi is doing, nor do we have an obvious way to interact with it. Don’t worry; that’s
    why we set up SSH. Now we’ll show you how to connect to your Pi from your laptop.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we’ll need to do is find the Raspberry Pi’s IP address. An IP
    address is a numerical label to identify a computer on a network. The easiest
    way to see new devices that have connected to the internet you’re using is through
    the router’s software. See figure 11.5\. If you can access your router, you can
    go to its IP address in a browser. The IP address is typically 192.168.86.1 or
    192.168.0.1; the type of router usually sets this number and can often be found
    on the router itself. You’ll then need to log in to your router, where you can
    see all devices connected to your network.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 Example Google Home router interface with several devices listed
    to discover their IP addresses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you don’t have access to your router, which many people don’t, you’re not
    out of luck. The next easiest way is to ignore everything we said in the previous
    paragraph and connect your Pi to a monitor and keyboard. Run `$` `ifconfig` or
    `$` `ip` `a`, and then look for the `inet` parameter. These commands will output
    devices on your local network and their IP addresses. Figures 11.6 and 11.7 demonstrate
    running these commands and highlight what you are looking for. If you don’t have
    access to an extra monitor, well, things will get a bit tricky, but it’s still
    possible. However, we don’t recommend going down this path if you can avoid it.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 Example of running `ifconfig`. The IP address of our Pi (`inet`)
    is highlighted for clarity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/11-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 Example of running `ip` `a`. The IP address of our Pi (`inet`) is
    highlighted for clarity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To scan your local network for IP addresses, open a terminal on your laptop,
    and run that same command (`$` `ifconfig`), or if you are on a Windows, `$` `ipconfig`.
    If you don’t have `ifconfig`, you can install it with `$` `sudo` `apt` `install`
    `net-tools`. We didn’t mention this step before because it should have already
    been installed on your Pi.
  prefs: []
  type: TYPE_NORMAL
- en: If you already recognize which device the Pi is, that’s awesome! Just grab the
    `inet` parameter for that device. More likely, though, you won’t, and there are
    a few useful commands you can use if you know how. Use the command `$` `arp` `-a`
    to view the list of all IP addresses connected to your network and the command
    `$` `nslookup` `$IP_ADDRESS` to get the hostname for the computer at the IP address
    you pass in—you’d be looking for the hostname `raspberry`, but we’ll skip all
    that. We trust that if you know how to use these commands, you won’t be reading
    this section of the book. Instead, we’ll use caveman problem-solving, which means
    we’ll simply turn off the Pi, run our `$` `ifconfig` command again, and see what
    changes, specifically what disappears. When you turn it back on, your router might
    assign it a different IP address than last time, but you should still be able
    to `diff` the difference and find it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, we know that was potentially a lot just to get the IP address, but
    once you have it, the next step is easy. To SSH into it, you can run the `ssh`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Replace `username` with the username you created (it should be `pi` if you are
    following along with us), and replace the `0`s with the IP address of your Pi.
    Since this is the first time connecting to a brand-new device, you’ll be prompted
    to fingerprint to establish the connection and authenticity of the host. Then
    you’ll be prompted to put in a password. Enter the password you set in the imager
    before. If you didn’t set a password, it’s `pi` by default, but we trust you didn’t
    do that, right?
  prefs: []
  type: TYPE_NORMAL
- en: With that, you should be remotely connected to your Pi and see the Pi’s terminal
    reflected in your computer’s terminal, as shown in figure 11.8\. Nice job!
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 Terminal after a successfully secure shell into your Raspberry Pi.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.1.3 Software installations and updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our Pi is up and we’ve connected to it, we can start the installation.
    The first command is well known and will simply update our system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It can take a minute, but once that finishes running, congratulations! You
    now have a Raspberry Pi server on which you can run anything you want to at this
    point. It’s still a blank slate, so let’s change that and prepare it to run our
    LLM server. We first want to install any dependencies we need. Depending on your
    installation, this may include `g++` or `build-essentials`. We need just two:
    `git` and `pip`. Let’s start by installing them, which will make this whole process
    so much easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can clone the repo that will be doing the majority of the work here:
    Llama.cpp. Let’s clone the project into your Pi and build the project. To do that,
    run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A note on llama.cpp
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Llama.cpp, like many open source projects, is a project that is much more interested
    in making things work than necessarily following best engineering practices. Since
    you are cloning the repo in its current state, but we wrote these instructions
    in a previous state, you may run into problems we can’t prepare you for. Llama.cpp
    doesn’t have any form of versioning either. After cloning the repo, we recommend
    you run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This command will checkout the exact `git` `commit` we used so you can run everything
    in the exact same version of llama.cpp. We tested this on Mac, Windows 10, Ubuntu,
    Debian, and, of course, both a Raspberry Pi 4 and 5\. We don’t expect any problems
    on most systems with this version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the repo, we must complete a couple of tasks to prepare it.
    First, to keep our Pi clean, let’s create a virtual environment for our repo and
    activate it. Once we have our Python environment ready, we’ll install all the
    requirements. We can do so with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Llama.cpp is written in C++, which is a compiled language. That means we have
    to compile all the dependencies to run on our hardware and architecture. Let’s
    go ahead and build it. We do that with one simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A note on setting up
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If you’re performing this setup in even a slightly different environment, using
    CMake instead of Make can make all the difference! For example, even running on
    Ubuntu, we needed to use CMake to specify the compatible version of CudaToolkit
    and where that nvcc binary was stored in order to use CuBLAS instead of vanilla
    CPU to make use of a CUDA-integrated GPU. The original creator (Georgi Gerganov,
    aka ggerganov) uses CMake when building for tests because it requires more specifications
    than Make. For reference, here’s the CMake build command ggerganov currently uses;
    you can modify it as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we just need to get our model, and we’ll be ready to move forward. The
    model we’ve picked for this project is Llava-v1.6-Mistral-7B, which we will download
    using the `huggingface-cli`, like we’ve done in other chapters. Go ahead and run
    the following command to pull the LLaVA model, its accompanying tokenizer, and
    the config files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our model and tokenizer information, we’re ready to turn our
    LLM into something usable for devices as small as an Android phone or Raspberry
    Pi.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Preparing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a model, we need to standardize it so that the C++ code in
    the repo can interface with it in the best way. We will convert the model from
    the safetensor format, which we downloaded into .gguf. We’ve used GGUF models
    before, as they are extensible, quick to load, and contain all of the information
    about the model in a single model file. We also download the tokenizer information,
    which goes into our .gguf model file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once ready, we can convert our safetensor model to GGUF with the `convert.py`
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will convert all the weights into one .gguf checkpoint that is the
    same size on disk as all of the .safetensors files we downloaded combined. That’s
    now two copies of whatever we’ve downloaded, which is likely one too many if your
    microSD card is rather small. Once you have the .gguf checkpoint, we recommend
    you either delete or migrate the original model files somewhere off of the Pi
    to reclaim that memory, which could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our model is in the correct single-file format, we can make it smaller.
    Now memory constraints come into play. One reason we picked a 7B parameter model
    is that in the quantized `q4_K_M` format (we’ll talk about different llama.cpp-supported
    quantized formats later), it’s a little over 4 GB on disk, which is more than
    enough for the 8 GB Raspberry Pi to run effectively. Run the following command
    to quantize the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We won’t lie: it’ll be a bit of a waiting game while the quantization methodology
    is applied to all of the model weights, but when it’s finished, you’ll have a
    fresh quantized model ready to be served.'
  prefs: []
  type: TYPE_NORMAL
- en: Having trouble?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'While we’ve tested these instructions in a multitude of environments and hardware,
    you might still find yourself stuck. Here’s some troubleshooting advice you can
    try that has helped us out:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Redownload the model*. These models are large, and if your Pi had any internet
    connection problems during the download, you may have a corrupted model. You may
    try connecting with an ethernet cable instead of Wi-Fi if your connection is spotty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recompile your dependencies*. The easiest way to recomplie your dependencies
    is to run `make` `clean` and then `make` again. You might try using `cmake` or
    checking out different options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reboot your Pi*. Rebooting is a classic but tried-and-true solution, especially
    if you are dealing with memory problems (which we don’t have a lot of for the
    task at hand.). You can reboot while in SSH with `sudo` `reboot.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Run through these steps on your computer*. You’re likely to run into fewer
    problems on better hardware, and it can be useful to know what an easy path looks
    like before trying to make it work on an edge device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Download an already prepared model*. While we encourage you to go through
    the steps of converting and quantizing yourself, you can usually find most open
    source models already quantized to any and every format. So if you aren’t worried
    about finetuning it, you should be in luck. For us, we are in said luck.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you get stuck but want to keep moving forward, you can download a quantized
    version of the model with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 11.3 Serving the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re finally here, serving the model! With llama.cpp, creating a service for
    the model is incredibly easy, and we’ll get into some slightly more complex tricks
    in a bit, but for now, revel in what you’ve done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to use your Pi’s IP address, and the API key can be any random string
    to provide a small layer of security. That’s it! You now have an LLM running on
    a Raspberry Pi that can be queried from any computer on your local network. Note
    that the server can take a long time to boot up on your Pi, as it loads in the
    model. Don’t worry too much; give it time. Once ready, let’s test it out with
    a quick demo.
  prefs: []
  type: TYPE_NORMAL
- en: For this demo, let’s say you’ve already integrated an app pretty deeply with
    OpenAI’s Python package. In listing 11.1, we show you how to point this app to
    your Pi LLM service instead. We’ll continue to use OpenAI’s Python bindings and
    point it to our service instead. We do this by updating the `base_url` to our
    Pi’s IP address and using the same API key we set when we created the server.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that we’re calling the `gpt-3.5-turbo` model. OpenAI has different
    processes for calling different models. You can easily change that if you don’t
    like typing those letters, but it doesn’t really matter. You’ll just have to figure
    out how to change the script for whichever model you want to feel like you’re
    calling (again, you’re not actually calling ChatGPT).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 OpenAI but not ChatGPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You don’t need code to interact with your server. The server script comes with
    a built-in minimal GUI, and you can access it on your local network with a phone
    or your laptop by pointing a browser to your Pi’s IP address. Be sure to include
    the port 8080\. You can see an example of this in figure 11.9\.
  prefs: []
  type: TYPE_NORMAL
- en: This process will allow you to interface with the LLM API you’re running in
    a simple chat window. We encourage you to play around with it a bit. Since you’re
    running on a Raspberry Pi, the fastest you can expect this to go is about five
    tokens per second, and the slowest is, well, SLOW. You’ll immediately understand
    why normal people don’t put LLMs on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 Running an LLM on your Pi and interacting with it through the llama.cpp
    server
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At this point, you may be wondering why we were so excited about this project.
    We made a bunch of promises about what you’d learn, but this chapter is the shortest
    in the book, and the majority of what we did here was download other people’s
    repos and models. *Welcome to production.*
  prefs: []
  type: TYPE_NORMAL
- en: 'This is ultimately what most companies will ask you to do: download some model
    that someone heard about from a friend and put it on hardware that’s way too small
    and isn’t meant to run it. You should now be ready to hack together a prototype
    of exactly what they asked for within about 20 to 30 minutes. Being able to iterate
    quickly will allow you to go back and negotiate with more leverage, demonstrating
    why you need more hardware, data to train on, RAG, or any other system to make
    the project work. Building a rapid proof of concept and then scaling up to fit
    the project’s needs should be a key workflow for data scientists and ML engineers.'
  prefs: []
  type: TYPE_NORMAL
- en: One huge advantage of following the rapid proof of concept workflow demo-ed
    here is visibility. You can show that you can throw something amazing together
    extremely fast, which (if your product managers are good) should add a degree
    of trust when other goals are taking longer than expected. They’ve seen that if
    you want something bad in production, you can do that in a heartbeat. The good
    stuff that attracts and retains customers takes time with real investment into
    data and research.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve walked through the project once, let’s talk about ways to modify
    this project. For clarity, we chose to hold your hand and tell you exactly what
    commands to run so you could get your feet wet with guided assistance. Tutorials
    often end here, but real learning, especially projects in production, always goes
    a step further. So we want to give you ideas about how you can make this project
    your own, from choosing a different model to using different tooling.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.1 Using a better interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning a new tool is one of the most common tasks for someone in this field—and
    by that, we mean everything from data science to MLOps. While we’ve chosen to
    focus on some of the most popular and battle-tested tooling in this book—tools
    we’ve actually used in production— your company has likely chosen different tools.
    Even more likely, a new tool came out that everyone is talking about, and you
    want to try it out.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve talked a lot about llama.cpp and used it for pretty much everything in
    this project, including compiling, quantizing, serving, and even creating a frontend
    for our project. While the tool shines on the compiling and quantizing side, the
    other stuff was mostly added out of convenience. Let’s consider some other tools
    that can help give your project that extra pop or pizzazz.
  prefs: []
  type: TYPE_NORMAL
- en: To improve your project instantly, you might consider installing a frontend
    for the server like SillyTavern (not necessarily recommended; it’s just popular).
    A great frontend will turn “querying an LLM” into “chatting with an AI best friend,”
    shifting from a placid task to an exciting experience. Some tools we like for
    the job are KoboldCpp and Ollama, which were built to extend llama.cpp and make
    the interface simpler or more extensible. So they are perfect to extend this particular
    project. Oobabooga is another great web UI for text generation. All these tools
    offer lots of customization and ways to provide your users with unique experiences.
    They generally provide both a frontend and a server.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.2 Changing quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might consider doing this same project but on an older Pi with only 4 GB
    of memory, so you’ll need a smaller model. Maybe you want to do more than just
    serve an LLM with your Pi, so you need to shrink the model a bit more, or maybe
    you want to switch up the model entirely. Either way, you’ll need to dive a bit
    deeper down the quantization rabbit hole. Before, we quantized the model using
    `q4_K_M` format with the promise we’d explain it later. Well, now it’s later.
  prefs: []
  type: TYPE_NORMAL
- en: Llama.cpp offers many different quantization formats. To simplify the discussion,
    table 11.1 highlights a few of the more common quantization methods, along with
    how many bits each converts down to, the size of the resulting model, and the
    RAM required to run it for a 7B parameter model. This table should act as a quick
    reference to help you determine what size and level of performance you can expect.
    The general rule is that smaller quantization equals lower-quality performance
    and higher perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 Comparison of key attributes for different llama.cpp quantization
    methods for a 7B parameter model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Quant method | Bits | Size (GB) | Max RAM required (GB) | Use case | Params
    (billions) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Q2_K`  | 2  | 2.72  | 5.22  | Significant quality loss; not recommended
    for most purposes  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q3_K_S`  | 3  | 3.16  | 5.66  | Very small, high loss of quality  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q3_K_M`  | 3  | 3.52  | 6.02  | Very small, high loss of quality  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q3_K_L`  | 3  | 3.82  | 6.32  | Small, substantial quality loss  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q4_0`  | 4  | 4.11  | 6.61  | Legacy; small, very high loss of quality;
    prefer using `Q3_K_M`  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q4_K_S`  | 4  | 4.14  | 6.64  | Small, greater quality loss  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q4_K_M`  | 4  | 4.37  | 6.87  | Medium, balanced quality; recommended  |
    7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q5_0`  | 5  | 5.00  | 7.50  | Legacy; medium, balanced quality; prefer using
    `Q4_K_M`  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q5_K_S`  | 5  | 5.00  | 7.50  | Large, low loss of quality; recommended  |
    7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q5_K_M`  | 5  | 5.13  | 7.63  | large, very low loss of quality; recommended  |
    7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q6_K`  | 6  | 5.94  | 8.44  | Very large, extremely low loss of quality  |
    7  |'
  prefs: []
  type: TYPE_TB
- en: '| `Q8_0`  | 8  | 7.70  | 10.20  | Very large, extremely low loss of quality;
    not recommended  | 7  |'
  prefs: []
  type: TYPE_TB
- en: If you only have a 4 or 6 GB Pi, you’re probably looking at this table thinking,
    “Nope, time to give up.” But you’re not completely out of luck; your model will
    likely just run slower, and you’ll either need a smaller model than one of these
    7Bs—something with only, say, 1B or 3B parameters—or to quantize smaller to run.
    You’re really pushing the edge with such a small Pi, so `Q2_k` or `Q3_K_S` might
    work for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'A friendly note: we’ve been pushing the limits on the edge with this project,
    but it is a useful experience for more funded projects. When working on similar
    projects with better hardware, that better hardware will have its limits as to
    how large an LLM it can run. After all, there’s always a bigger model. Keep in
    mind that if you’re running with cuBLAS or any framework for utilizing a GPU,
    you’re constrained by the VRAM in addition to the RAM. For example, running with
    cuBLAS on a 3090 constrains you to 24 GB of VRAM. Using clever memory management
    (such as a headless OS to take up less RAM), you can load bigger models onto smaller
    devices and push the boundaries of what feels like it should be possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.3 Adding multimodality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There’s an entire dimension that we initially ignored so that it wouldn’t distract,
    but let’s talk about it now: LLaVA is actually multimodal! A multimodal model
    allows us to expand out from NLP to other sources like images, audio, and video.
    Pretty much every multimodal model is also an LLM at heart, as datasets of different
    modalities are labeled with natural language—for example, a text description of
    what is seen in an image. In particular, LLaVA, which stands for Large Language
    and Vision Assistant, allows us to give the model an input image and ask questions
    about it.'
  prefs: []
  type: TYPE_NORMAL
- en: A note about the llama server
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Remember when we said the llama.cpp project doesn’t follow many engineering
    best practices? Well, multimodality is one of them. The llama.cpp server at first
    supported multimodality, but many issues were soon added to the project. Instead
    of adding a feature and incrementing on it, the creator felt the original implementation
    was hacky and decided to remove it. One day, everything was working, and the next,
    it just disappeared altogether.
  prefs: []
  type: TYPE_NORMAL
- en: This change happened while we were writing this chapter—which was a headache
    in itself—but imagine what damage it could have caused when trying to run things
    in production. Unfortunately, this sudden change is par for the course when working
    on LLMs at this point in time, as there are very few stable dependencies you can
    rely on that are currently available. To reproduce what’s here and minimize debugging,
    we hope you check out the `git` `commit` mentioned earlier. The good news is that
    llama.cpp plans to continue to support multimodality, and another implementation
    will likely be ready to go soon—possibly by the time you read this chapter
  prefs: []
  type: TYPE_NORMAL
- en: We haven’t really talked about multimodality at all in this book, as many lessons
    from learning how to make LLMs work in production should transfer over to multimodal
    models. Regardless, we thought it’d be fun to show you how to deploy one.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve done most of the work already; however, llama.cpp has only converted the
    llama portion of the LLaVA model to .gguf. We need to add the vision portion back
    in. To test this, go to the GUI for your served model, and you’ll see an option
    to upload an image. If you do, you’ll get a helpful error, shown in figure 11.10,
    indicating that the server isn’t ready for multimodal serving.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 Our model isn’t ready yet; we need to provide a model projector.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The first step to converting our model is downloading a multimodal projection
    file, similar to CLIP, for you to encode images. Once we can encode the images,
    the model will know what to do with them since it’s already been trained for multimodal
    tasks. We aren’t going to go into the details of preparing the projection file;
    instead, we’ll show you where you can find it. Run the following command to download
    this file and then move it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using a different model or a homebrew, make sure you find or create
    a multimodal projection model to perform that function for you. It should feel
    intuitive as to why you’d need it: language models only read language. You can
    try finetuning and serializing images to strings instead of using a multimodal
    projection model; however, we don’t recommend doing so, as we haven’t seen good
    results from it. It increases the total amount of RAM needed to run these models,
    but not very much.'
  prefs: []
  type: TYPE_NORMAL
- en: Serving the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once you have your model converted and quantized, the command to start the
    server is the same, except you must add `--MMPROJ` `path/to/mmproj.gguf` to the
    end. This code will allow you to submit images to the model for tasks like performing
    optical character recognition (OCR), where we convert text in the image to actual
    text. Let’s do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now that our server knows what to do with images, let’s send it in a request.
    In line with the OpenAI API we used to chat with the language-only model before,
    another version shows you how to call a multimodal chat. The code is very similar
    to listing 11.1 since all we are doing is adding some image support. Like the
    last listing, we use the OpenAI API to access our LLM backend, but we will change
    the base URL to our model. The main difference is that we are serializing the
    image into a string so that it can be sent in the object with a couple of imports
    to facilitate that using the `encode_image` function. The only other big change
    is adding the encoded image to the content section of the messages we send.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 OpenAI but multimodal GPT-4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Replace with your server’s IP address and port.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set to the maximum dimension to allow (512=1 tile, 2048=max).'
  prefs: []
  type: TYPE_NORMAL
- en: Nothing too fancy or all that different from the many other times we’ve sent
    requests to servers. One little gotcha with this code that you should keep in
    mind is that the API will throw an error if you don’t use an API key, but if you
    don’t set one on the server, you can pass anything, and it won’t error out.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! We’ve now turned our language model into one that can also take
    images as input, and we have served it onto a Raspberry Pi and even queried it.
    At least, we hope you queried it because if you didn’t, let us tell you, it is
    very *slow*! When you run the multimodal server on the Pi, it will take dozens
    of minutes to encode and represent the image before even getting to the tokens
    per second that people generally use to measure the speed of generation. Once
    again, just because we can deploy these models to small devices doesn’t mean you’ll
    want to. This is the point where we’re going to recommend again that you should
    not actually be running this on a Pi, even in your house, if you want to actually
    get good use out of it.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.4 Serving the model on Google Colab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve done a couple of these exercises, how can we improve and extend
    this project for your production environment? The first improvement is obvious:
    hardware. Single-board RAM compute isn’t incredibly helpful when you have hundreds
    of customers; however, it is incredibly useful for testing, especially when you
    don’t want to waste money debugging production for your on-prem deployment. Other
    options for GPU support also exist, and luckily, all the previously discussed
    steps, minus the RPi setup, work on Google Colab’s free tier. Here are all of
    the setup steps that are different:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up llama.cpp:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Downloading from Hugging Face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Server command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Accessing the server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the steps are mostly the same, but because we are working in
    a Jupyter environment, some slight changes are necessary, as it’s often easier
    to run code directly instead of running a CLI command. We didn’t go into it, but
    Raspberry Pis can use `docker.io` and other packages to create docker images that
    you can use for responsible CI/CD. It’s a bit harder in a Google Colab environment.
    Also, keep in mind that Google won’t give you unlimited GPU time, and it goes
    so far as to monitor whether you have Colab open to turn off your free GPU “efficiently,”
    so make sure you’re only using those free resources for testing and debugging.
    No matter how you look at it, free GPUs are a gift, and we should be responsible
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also skip downloading the whole repo and running Make every time. You
    can use the llama.cpp Python bindings. And you can `pip` `install` with cuBLAS
    or NEON (for Mac GeForce Mx cards) to use hardware acceleration when pip installing
    with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This command abstracts most of the code in llama.cpp into easy-to-use Python
    bindings. Let’s now go through an example of how to use the Python bindings to
    make something easy to dockerize and deploy. Working with an API is slightly different
    from working with an LLM by itself, but luckily, LangChain comes in handy. Its
    whole library is built around working with the OpenAI API, and we use that API
    to access our own model!
  prefs: []
  type: TYPE_NORMAL
- en: In listing 11.3, we’ll combine what we know about the OpenAI API, llama.cpp
    Python bindings, and LangChain. We’ll start by setting up our environment variables,
    and then we’ll use the LangChain `ChatOpenAI` class and pretend that our server
    is GPT-3.5-turbo. Once we have those two things, we could be done, but we’ll extend
    by adding a sentence transformer and a prompt ready for RAG. If you have a dataset
    you’d like to use for RAG, now is the time to embed it and create a FAISS index.
    We’ll load your FAISS index and use it to help the model at inference time. Then,
    tokenize it with tiktoken to make sure we don’t overload our context length.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 OpenAI but not multimodal GPT-4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Replace with your server’s address and port.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Replace with your host IP.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This can be anything.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Again'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Embeddings for RAG'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Tokenization for checking context length quickly'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Change the prompt to be whatever you want.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Here’s a vectorΔB; feel free to drop in a replacement.'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 If you haven’t created a faiss or elasticsearch or usearch index, do it.'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 To keep track of chat history'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Searches the Vector ΔB'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Formats the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Sets up the actual LLM chain'
  prefs: []
  type: TYPE_NORMAL
- en: '#14 Δon’t overload your context length.'
  prefs: []
  type: TYPE_NORMAL
- en: '#15 Runs RAG with your API'
  prefs: []
  type: TYPE_NORMAL
- en: '#16 We’re just printing; do whatever you need to here.'
  prefs: []
  type: TYPE_NORMAL
- en: So here’s where many of our concepts really come together. The amazing thing
    is that you really can perform this inference and RAG on a Raspberry Pi; you don’t
    need a gigantic computer to get good, repeatable results. Compute layered on top
    of this helps immensely until you get to about 48 GB and can fit full versions
    of 7B and quantized versions of everything above that; all compute after that
    ends up getting only marginal gains currently. This field is advancing quickly,
    so look for new, quicker methods of inferencing larger models on smaller hardware.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve got our prototype project up and running. It’s easily extensible
    in pretty much any direction you’d like, and it conforms to industry standards
    and uses popular libraries. Add to this, make it better, and if you have expertise
    you feel isn’t being represented here, share it! This field is new, and interdisciplinary
    knowledge is how it will be pushed forward.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running the largest models on the smallest devices demands utilizing every memory-saving
    technique you can think of, like running a Lite operating system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hardest part of setting up a remote Pi for the first time is finding its
    IP address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For compute-limited hardware without an accelerator, you will need to compile
    the model to run on your architecture with a tool like llama.cpp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a memory-limited environment, quantization will be required for inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even taking advantage of everything available, running LLMs on edge devices
    will often result in slower inference than desired. Just because something is
    possible doesn’t make it practical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s API, along with all wrappers, can be used to access other models by
    pointing to a custom endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many open source tools are available to improve both the serving of models and
    the user interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower quantization equals higher perplexity, even with larger models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running multimodal models is also possible on a Raspberry Pi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same commands we ran on the Pi can be used to develop in Google Collab or
    another cloud provider with only slight modifications, making these projects more
    accessible than ever.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup and deployment are often much larger pieces to a successful project than
    preparing the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
