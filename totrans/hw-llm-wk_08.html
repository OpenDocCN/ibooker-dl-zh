<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__ethics"> <span class="chapter-title-numbering"><span class="num-string">9</span></span> <span class="title-text"> Ethics of building and using LLMs</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">How LLMs’ abilities to perform many tasks also create unanticipated risk</li> 
    <li class="readable-text" id="p3">The question of LLMs’ misalignment with human values</li> 
    <li class="readable-text" id="p4">The implications of LLMs’ data use on content creation and building future models</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Although the discussion of ethics may remind some of you of the dull readings from an entry-level college class, there are critical considerations when implementing algorithms that have the potential to affect humanity. Given the rapid growth in LLM use and their scope of capabilities, we must be aware of and attend to many evolving concerns. If you are unaware of these concerns, you will have no voice in their resolution.</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>Exploring the ethics of building and using LLMs is an incredibly complex topic that is challenging to represent completely. As a result, this chapter will present what we believe to be common concerns about building LLMs and the related ethical questions. Throughout the chapter, we’ll reference materials that round out this conversation so you can investigate further if you wish.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>We’ll cover three main topics:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p8">Why do people want to construct LLMs, and what do they provide that didn’t exist before?</li> 
   <li class="readable-text" id="p9">Some experts in machine learning believe that in future iterations, LLMs will lead to the extinction of the human race because they will automate us out of existence. Even if we do not agree with them, it is worth understanding the basis for this fear.</li> 
   <li class="readable-text" id="p10">The amount of training data needed for LLMs is monstrous. How do companies that build LLMs, such as OpenAI and Anthropic, source all that data? What ethical concerns arise that may have moral, legal, and financial implications due to how that data is collected and used?</li> 
  </ul> 
  <div class="readable-text" id="p11"> 
   <p>These are complicated considerations on both ethical and legal fronts. Our goal is not to tell you whether the creation of these models is ethical or nonethical but rather to outline primary considerations under each discussion. We hope this helps you consider LLMs’ implications, consequences, and risks on a broader scale. We see many high-profile, ethically sophisticated questions around LLM use, and many practitioners have not had to grapple meaningfully with this subject. Nevertheless, we believe that it is crucial to consider the ethical questions around building LLMs, and we will introduce you to some of the critical concerns to consider in this chapter.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>There are just as many considerations necessary when discussing how we use LLMs versus how we build LLMs, so we’ve divided this conversation into two sections. First, we focus on the ethics of building LLMs in general, while the latter section will cover the ethical implications of LLM use.</p> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>Last, we will avoid ascribing these arguments to specific individuals or groups. Our goal is to prevent bias and avoid “calling out” anyone in particular in this discussion. The concerns are what’s important.</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <h2 class=" readable-text-h2" id="why-did-we-build-llms-at-all"><span class="num-string browsable-reference-id">9.1</span> Why did we build LLMs at all?</h2> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Before we talk about the ethical ramifications of developing LLMs, it’s worth thinking about <em>what</em> it is we are trying to accomplish by building LLMs and <em>why</em> we want to achieve those things. Like all software engineering, building LLMs commonly aims to reduce or eliminate human labor from some tasks. Some economists might tell you that this is how standards of living generally increase. As technology advances, fewer people need to perform manual, labor-intensive tasks, and thus, they have more time for discovery, creation, and other functions that use high-level cognition.</p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>In the case of LLMs, a common goal is increasing the efficiency of algorithms for applications such as automated language translation, speech-to-text transcription, reading text contained in images and printed documents in applications such as Optical Character Recognition, indexing, and retrieving information, known simply as “search” or, more broadly, as information retrieval, and more. Others are interested in LLMs for purely scientific reasons, such as studying methods in computational linguistics, or creative applications, such as generating images, music, or videos. Furthermore, others may seek to increase access to and transparency of technology that affects our lives, or it may be just because LLMs have grabbed their attention and present fantastic new capabilities.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>For some, the variety of things LLMs can achieve is an intrinsic motivation for wanting to build them. AI and ML algorithms have been doing all the tasks we listed for some time; for example, machine translation is decades old. Part of what makes LLMs different is that they seem capable of doing everything with one model and algorithm. Before the advent of LLMs, engineers would implement tasks like translation and transcription in separate systems designed to meet those needs individually. The largest LLMs today can, to some degree, do each of these things and more. Often, it seems they can complete tasks of seemingly endless scope. At the same time, others fear LLMs because due to their breadth of capability, they believe they will steal work, motivation, and activity from humans by taking on tasks requiring discovery and creation, previously thought to be reserved for humans only.</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <h3 class=" readable-text-h3" id="the-pros-and-cons-of-llms-doing-everything"><span class="num-string browsable-reference-id">9.1.1</span> The pros and cons of LLMs doing everything</h3> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>Given that an LLM can perform many different tasks via a single model, you could describe it as a kind of “everything app”: your one-stop shop for AI-powered assistance. From a usability perspective, many benefits have emerged from the near-universal capability of LLMs, such as their relative aptitude for decomposing complex tasks into a series of steps or their ability to generate unique explanations to fill specific knowledge gaps.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Additionally, the chat-style interface seems very popular with users, even if other ways of working with LLMs are available. The popularity of chat may be due to its general accessibility: you chat with people constantly. Experience with phone calls is widespread, and with texts, Slack, Teams, instant messaging, and email, people implicitly know how to use various chat-based interfaces. As a result, interacting with an AI via a chat-based interface has become an inviting and easy way to increase adoption with little training. The widespread experience with chat-based applications also has a democratizing effect: users only need to learn something once to help them pursue many different goals.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>The primary disadvantage of such a system is that although it <em>can</em> be used for everything, that doesn’t mean we <em>should</em> use it for everything. When you have an algorithm that people can use for many different and potentially unexpected tasks, you do not have the time to test every possible use. Due to the breadth of potential applications of LLMs, there will be a gap between validating what the model does safely and what it can attempt to do but that could be potentially dangerous or harmful.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>For example, current LLM models can perform abstract evaluations of race or gender, even though these evaluations may contain harmful negative bias. While we can develop tests and defenses for specific instances of harmful bias, these are likely to be narrow in scope and highly specific. For example, suppose we ask for an image generation model to generate an image of a business meeting. The unfortunate result is that all people in that image will often be male and white. Naturally, we wish the model to transcend these stereotypes. However, identifying and fixing specific contextual bias concerns like this will not affect whether a model would cause harm when deployed in the real world and prompted in different, unanticipated ways. At best, these exercises exemplify how an LLM can fail, but addressing harm requires understanding the potential failures that can happen, for example, due to bias in the training data. Simultaneously, we must understand how people will use LLMs and whether those uses may lead to unintended harm due to how the LLM generates output. This may mean expressly not using an LLM for an intended use case due to the lack of mitigations for potential harms they may cause.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Recent research on the real-world harms of deployed LLMs found that the implicit bias in LLMs like OpenAI’s ChatGPT and Google’s Gemini against people who use African American vernacular English was worse than the archaic negative stereotypes measured among white Americans in the 1920s [1]. Another study considered the use case of a doctor consulting an LLM for information on medical best practices and treatment options for people of different races and found that the models frequently recommended debunked race-based medical practices grounded in eugenicist “science” [2]. Unfortunately, we continue to see these problems in models that score quite well on existing explicit bias benchmarks. The prevalence of latent bias suggests these benchmarks aren’t sufficient in evaluating potential harms and emphasizes the need to consider the harm an LLM can cause based on its use. In other words, it is more important to view harm due to AI deployment as a direct result of the specific proposed use cases and application, not as something we can ascribe to a general notion of whether a model contains racial bias.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>Today, we do not know how to design an algorithm capable of doing so many tasks in one system while simultaneously providing defense against accidental misuse and harm by well-meaning individuals. So it becomes critical from a developer’s perspective to do thorough user studies across a wide range of groups and settings to identify the unintended risks and to include monitoring and logging to remediate any late-identified risks. Whether we are attempting to prevent harmful racial stereotypes or the advocacy for debunked medical practices, the current approaches to constraining the misuse of LLMs are to enumerate what we know about potential problems and employ fine-tuning methods, such as RLHF, to force the model to behave better on known problems. The unfortunate side of this is that due to the potential breadth of LLM capabilities, the set of unknown problems is infinite, and as such, any testing regime will be incomplete.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p25"> 
   <p> <span class="print-book-callout-head">Note</span> The importance of postdeployment monitoring is not new. For example, the FDA has practiced this for many years with the MedWatch system. This system allows the public and medical professionals to report any adverse events with a drug or medical device so that the FDA can monitor for anything unusual. </p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3" id="do-we-want-to-automate-all-human-work"><span class="num-string browsable-reference-id">9.1.2</span> Do we want to automate all human work?</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>As we mentioned in the introduction, some economists might argue that automation allows the labor pool to focus on new work. This argument hinges on the idea that advances in automation have been good at eliminating work that most people don’t want to do. Farming is hard, mining rare earth metals is hard, and assembling cars, toys, and packages is hard. These are difficult labor and body-destroying jobs often coupled with limited intellectual stimulation. Heavy labor like farming requires 74% fewer laborers today than it did in 1950 [3] and, undoubtedly, many times fewer than it did back in the medieval era.</p> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>The difference with LLMs is the potential to automate away certain types of white-collar knowledge work. Copywriting [4], visual arts [5], graphic design [6], and banking [7] are just a few of the fields disrupted by generative AI.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>Those concerned about LLMs’ effect on the economy suggest that we will lose jobs to automation, which we caution is not as clear-cut as often portrayed. Institutional and consumer desires may push for retention and continued expansion of these types of white-collar jobs. We should be wary of ignoring a history of economic study about how jobs change as technology advances. Instead, we must address a more significant concern: obtaining high-quality training data. We believe this will drive new jobs in the future, emphasizing the importance of human creativity and ability, even if the current jobs it creates are not yet the desirable kind of white-collar work that many would prefer.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <h4 class=" readable-text-h4" id="a-counter-example-on-obvious-outcomes"> A counter-example on “obvious” outcomes</h4> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>Some argue that it is obvious that LLMs will affect some sectors of the economy for better or worse. The bank teller’s job is a famous example often used to argue against LLMs. The job of bank tellers has changed significantly since the invention of the Automatic Teller Machine (ATM) in the 1960s. Clearly, the ATM automated many of the bank teller’s tasks.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>But the ATM example is not that simple. The number of teller jobs increased for decades <em>after</em> the invention of the ATM, doubling to <span><img alt="equation image" src="../Images/eq-chapter-9-32-1.png"/></span> between 1970 and 2010 even as the ATM became more widely available [8]. Looking to historical studies of the ATM’s effect on jobs, it was recognized that many factors contributed to job loss, including changes in the growth rate and the nature of the job. Job loss came as a result of not just ATM technology but multiple rounds of technology innovation in other parts of the business, differences in how banks responded to the change, deregulation, and increased competition and consolidation in the banking industry [9]. So even though the ATM was arguably better and cheaper at the bank teller’s job, the nature of institutions, customers, and expectations prevented any immediate decline in jobs and made the situation far more complex than is often advertised.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>The ATM example is not unique; technology can, but does not always, lead to job losses due to automation. For example, machine translation improved dramatically in the early 2000s and again in 2016. Still, jobs for translation work increased within each period and continue to grow today [10]. The critical observation is that the job pool for translation doesn’t shrink when translators incorporate automated tools into their workflow. Instead, we saw a growth in the volume of translation work completed and an increase in demand for translation services as the amount of material requiring translation continues to grow. Some argue that similar demand will materialize for creative artists and writers [11]. According to this argument, while the means of producing art and performing knowledge work will change, the market will continue to grow, and demand will continue to rise in a way that can take advantage of the new supply of labor resulting from the introduction of automated tools. Thus, when we identify an area of work that may be automated or accelerated by LLMs, we must also determine whether the increased efficiency and quality could drive more demand.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Still, others will argue that LLMs fundamentally differ from everything that has ever happened. Thus, we cannot use prior methods of understanding technology’s potential effects on the economy to predict the future. Although possible and tempting to believe, given all the hype around LLMs, we are skeptical as to whether this is an overly broad statement that no one can prove false or true. Although we should indeed consider such possibilities and factors when making regulations (which, in turn, play a significant part in how jobs evolve with technology), it is also noteworthy that an estimated 60% of all US jobs are modern inventions that did not exist previously [12].</p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <h4 class=" readable-text-h4" id="considerations-on-training-data"> Considerations on training data</h4> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Generative AI’s effect on creative expression is poignant due to the situation’s perverse duality. Much of the work of writers and artists who post their content on the internet is fueling models that are seemingly out to eliminate their jobs. The ethical argument made by LLM researchers is that they should be able to freely use content from these creators as training data. This argument may lead to a pyrrhic victory and, ultimately, an undoing for AI. If AI replaces the work of creatives, LLM developers will find that they can no longer improve their models due to a lack of human-generated content and the exponential size increases in the data needed to train LLMs exceeding the linear growth in user-generated content. More importantly, the folks who create that content can no longer be employed or motivated to create content merely to have it slurped up by an LLM.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>This negative cycle will affect both LLMs and content creators, even if it is only a perceived risk and not a genuine concern. Data harvesting to train LLMs is a significant concern for thousands of websites that rely on user-generated content and advertising revenue from those who consume that content. These sites provide precious training data for LLMs, whose builders require massive collections of training data but do nothing to contribute to advertising revenue.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>For example, Stack Exchange is a collection of websites where users can post questions, have other users answer them, and receive a reputation rating for good answers. One of Stack Exchange’s websites, Stack Overflow, is a godsend to programmers looking for help solving coding problems. Stack Exchange also hosts many other diverse user communities catering to system administrators, math students, and tabletop gaming enthusiasts.</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>With the advent of LLMs, Stack Exchange was quick to change its business model and attempted to require payment from LLM creators to sustain its financial future [13]. Even with agreements between companies training LLMs and the websites hosting content in place, more direct commercialization of user-generated content may not be palatable to users. Stack Overflow experienced this as people began to delete their helpful answers from the platform in protest of Stack Overflow selling the results of their free labor to LLM creators [14].</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>This example mirrors a long history of search engines integrating the capabilities of the applications and websites they index into their primary interface. For example, it is now possible to search for and compare prices for airline tickets directly from within the Google search interface. This capability drives traffic away from established travel sites that provide the same service [15] and reduces the demand for the services and revenue of the companies that built those services. A potentially similar relationship exists between the LLMs trained on creative works and the original producers of that work when it becomes training data.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>It seems clear that the problems we are dealing with due to the rise of LLMs are similar, but not identical, to the problems we’ve seen in previous periods of automation. The question then becomes whether the differences related to LLM deployment are sufficiently significant to result in a different, more negative outcome. The outcomes are not apparent to us, primarily due to the broad scale, accessibility, and applicability of LLMs. It is up to LLM developers to take the initiative to understand and mitigate potential harms, like prenegotiating data usage and community building with the likely-to-be-affected fields. We will discuss other facets of training data and its sourcing in the last section of this chapter.</p> 
  </div> 
  <div class="readable-text" id="p42"> 
   <h2 class=" readable-text-h2" id="do-llms-pose-an-existential-risk"><span class="num-string browsable-reference-id">9.2</span> Do LLMs pose an existential risk?</h2> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>Some believe that LLMs are, in themselves, dangerous. If you are unfamiliar with the argument, it may sound absurd that training a powerful LLM model could result in significant real-world harms such as eliminating privacy, terminator robots, and threats to human existence as we know it. Yet many are concerned about these risks, including leaders in the field of AI like Geoffrey Hinton [16] and Yoshua Bengio [17]. Hinton and Benigo are two of the most well-regarded researchers in deep learning who share significant credit for the survival, revival, and dominance of neural network techniques in AI.</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>We believe AI does not present a realistic threat. However, serious and well-respected people are making these claims, so it is important to understand their arguments and explain why we believe these concerns are less significant than the need to address more immediate effects on the nature of work and ensure equitable and sustainable data licensing and compensation for creators.</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>In this section, we’ll focus on the general argument that AI could, broadly, become a risk to humanity because we could lose control over the LLMs, and LLMs might make decisions detrimental to humans. This notion stems from two ideas taken to their extremes:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p46">The idea that an LLM can use tools to build new LLMs and thus potentially self-improve</li> 
   <li class="readable-text" id="p47">The idea that an LLM with a goal not aligned with human needs may ultimately decide to take actions detrimental to human life in the interest of its own goals</li> 
  </ul> 
  <div class="readable-text" id="p48"> 
   <p>We have touched on this first idea about self-improvement tangentially throughout this book. We have discussed the fact that designing LLMs involves developing tools for data collection and creating the code to train an LLM using that data. One might hypothesize that if an LLM can use tools for data collection and training directly, without human intervention, an LLM could hypothetically train another LLM.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>The cognitive leap required to support this line of reasoning is that an LLM will be smart enough to build a better LLM. For us to accept this, we must assume that this new LLM will then be able to create an even better <span><img alt="equation image" src="../Images/eq-chapter-9-49-1.png"/></span> and, further, believe that this improvement cycle could repeat forever until the <span><img alt="equation image" src="../Images/eq-chapter-9-49-2.png"/></span> model will be more intelligent than any person who could ever exist and essentially be able to predict, subvert, or counteract any possible human action that might interrupt this cycle. This leap is challenging because we have little evidence that something like this is likely, based on what we observe in today’s technology.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>The second idea, often referred to as the “alignment problem,” is that LLMs misaligned with human needs may choose goals and outcomes that are detrimental to humans. This idea is reasonable because, as discussed inc chapter <a href="../Text/chapter-4.html">4</a>, creating a metric that measures only your intended goals is challenging. However, the extraordinary leap required for this line of thinking is that LLMs will have the ability and resources to interact with the world directly and physically, which could result in mass harm if not stopped.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Some combine these two ideas to argue that an LLM may have goals misaligned with humanity. They believe there will be a point at which an LLM realizes it needs to become more intelligent and improve itself to achieve its goals. As it does so, it takes resources away from humans or, via its improved intelligence, forces humans into subservience to help it achieve its goals. We outline this idea in figure <a href="#fig__destroyTheWorld">9.1</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/CH09_F01_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__destroyTheWorld"><span class="num-string">Figure <span class="browsable-reference-id">9.1</span></span> Two kinds of hypothetical concerns arise within the alignment problem, as commonly argued by those who think LLMs pose an existential risk to humanity. The top path shows a direct alignment problem, where the AI’s target solution directly harms humans. The bottom path shows an indirect alignment problem, where the AI has created a subgoal toward its eventual target. Even if the target—say, solving a hard math problem—is achieved, this LLM will do this at the cost of humanity. In an intermediate step, the LLM decides it needs more earthly resources than can be shared with humans to solve the problem.</h5>
  </div> 
  <div class="readable-text" id="p53"> 
   <p>An essential aspect of this argument is that the LLM, with a goal of self-preservation, determines that humans are destroying the planet. Since the LLM exists on earth and wants to continue doing so, it determines that destroying humans would be the best means of maintaining self-preservation.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>We do not think the potential for humanity’s destruction is a well-founded concern. Still, many people, including those with doctorates in computer science and who specialize in deep learning, are concerned about this scenario. The main problem with this “LLM-destroys-humanity” concept is that it relies on unfalsifiable logic. Unfalsifiable logic suggests that things will happen, and it is nearly impossible for anyone to prove that they will not. In this case, proving that LLMs won’t destroy humanity is challenging.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p55"> 
    <h5 class=" callout-container-h5 readable-text-h5">Teapots and unfalsifiable statements</h5> 
   </div> 
   <div class="readable-text" id="p56"> 
    <p> Demanding that someone make a falsifiable statement is essential in discussing abstract risks like LLMs’ potential to destroy humanity. A famous example is Bertrand Russell’s “teapot” thought experiment. The idea is simple: someone tells you that a teapot exists in space, too small and too far away to be detected. The premise itself is unfalsifiable; I can scan the universe for centuries looking for a teapot, but even though I can’t find it, I cannot prove that it does not exist. The only possibility is that I eventually find a teapot and confirm that it exists in space. Otherwise, I will never prove the teapot’s existence was a lie. Hence, when discussing abstract risks, unfalsifiable statements become a cognitive dead end. Arguing against a statement that no one can prove false is impossible. At the same time, those statements do nothing to advance the conversation to arrive at a meaning-ful insight or conclusion. Instead, making an argument based on realistic and practical concerns that can be acknowledged and addressed is more valuable in understanding the problems.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Two other arguments support this reasoning: technology tends to increase exponentially, and most humans are bad at considering exponentials and thus don’t fully comprehend how quickly this risk will become a reality.</p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>The fact that this line of thinking exists and is a concern of leaders in the field makes it worthwhile for you to delve deeper into the thoughts and considerations that are both for and against the idea that LLMs could bring about the end of humanity. The following subsections explore these arguments and the critical assumptions behind self-improvement and alignment mismatch.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3" id="self-improvement-and-the-iterative-s-curve"><span class="num-string browsable-reference-id">9.2.1</span> Self-improvement and the iterative S-curve</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>When considering the argument for self-improving intelligence, the view is reinforced by acknowledging that we, as humans, are the proof that it is possible to construct intelligence. If intelligence is constructible, there is reason to believe LLMs can build it themselves. The fact that most things improve on a sigmoid, or S-curve, is something we discussed in chapter <a href="../Text/chapter-7.html">7</a>. The important takeaway from that conversation is that there is a point of diminishing returns beyond which further improvements no longer provide meaningful value. The counterargument is that human technological advancement instead follows an iterative S-curve, where each plateau of diminishing returns is counteracted by discovering an innovation that begins a new S-curve, as shown in figure <a href="#fig__repeatedScurve">9.2</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/CH09_F02_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__repeatedScurve"><span class="num-string">Figure <span class="browsable-reference-id">9.2</span></span> The S-curve, or sigmoid, shows the classic plateau behavior: at some point, you hit diminishing returns. The counterpoint to this expressed by the iterative S-curve model is that progress continues past the plateau of diminishing returns by discovering new techniques, each represented by a new S-curve. The new techniques may start worse than the existing methods but have a higher potential to surpass them.</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>An argument against this claim is that there are significant gaps in the logic that self-improvement will lead to a human-killing level of capability. Although humans are a kind of existence proof, there is no known existence of anything more intelligent than humans (very narcissistic of us, we know). However, this also relies on the idea that smartness and intelligence can be improved. While terms like <em>smartness</em> and <em>intelligence</em> are helpful generalities used in everyday life, they evade precise quantification and definition because they are intrinsically abstract concepts. It is unclear whether there is a singular axis of intelligence along which an LLM will continually improve.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>We are more inclined to believe that there are limits to an LLM’s ability to self-improve. Our evidence for this argument appears in section <a href="../Text/chapter-7.html#sec__chp7_computational_limits">7.4</a>, where, in our discussion of the computational limits of LLMs, we demonstrated that LLMs have difficulty performing many types of calculations.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <h3 class=" readable-text-h3" id="the-alignment-problem"><span class="num-string browsable-reference-id">9.2.2</span> The alignment problem</h3> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>The second concern that an LLM may put its goals above the needs of humans is called the <em>alignment problem</em>. The alignment problem forms whenever we give an LLM a goal that we want it to achieve but do not sufficiently state, specify, or constrain the actions or methods that the LLM can use to achieve the goal we intended. Our discussion about what makes a suitable loss function in chapter <a href="../Text/chapter-4.html">4</a> is an example of the alignment problem in action today. More generally, humans deal with the alignment problem all the time. For instance, balancing corporate CEO compensation and the will of the company’s shareholders is a classic alignment problem, studied by economists for decades.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>The alignment problem is thus very real, and its existence tells us how hard it is to solve. Even when we try to be very explicit, such as when lawyers draw up a contract detailing and specifying what will or won’t happen in an agreement, stories about loopholes and shenanigans to subvert the other team are commonplace. While some of these stories are undoubtedly real, the fictitious ones are also informative. Indeed, a lot of active research in machine learning attempts to address this problem from a technical perspective, and we could probably learn a lesson or two from the lawyers and economists who deal with this every day.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>These general challenges with human alignment provide strong evidence that the alignment problem in LLMs is also a genuine concern. Still, a skeptical reader would ask whether there is evidence that a misaligned LLM would conclude that killing humans will advance its goal. Indeed, should an LLM reach this state, humans would fight back (“Just unplug it” is the common refrain). More importantly, many dooms-day arguments rely on the LLM being so intelligent that its actions are deterministic and that the outcome is known and prescribed no matter what happens.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>In reality, outcomes are probabilistic; things go right or wrong, and an LLM smarter than humans would surely understand that it could not guarantee outcomes sufficiently and that coexistence is worthwhile over killing all humans. Given intrinsic uncertainty and the need to then fight humans, who have a long track record of successfully blowing things up, would trying to fight or subvert humanity be the superintelligent thing to do?</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h4 class=" readable-text-h4" id="whose-values-is-your-model-aligned-to"> Whose values is your model aligned to?</h4> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>It is increasingly common for companies to use fine-tuning techniques like RLHF (which we described in depth in chapter 5) to attempt to align the behaviors of LLMs to what they desire. As we discussed, the goal is to make LLMs useful in that they’ll follow instructions and safer in that they’ll disobey requests for harmful or hurtful activities. Essentially, RLHF attempts to address the alignment problem and ensure the LLM output is constrained based on a specific set of examples and values. The critical question, as the title of this section suggests, is to whose values are we aligning these models? We will walk through our reasoning on why the alignment problem, while interesting and valuable in many instances, is not meaningful in discussing existential risk.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Fine-tuning an LLM using RLHF requires a large data set of input-output pairs, often hand-built. Companies building LLMs do not share their fine-tuning data because it is considered proprietary and provides an advantage over competitors. Thus, as users, we cannot inspect the intended alignment of the models we use. It is, therefore, unclear today to whom the goals of any individual LLM are aligned. We can approximate the nature of the goals embedded in a training dataset by considering their origin and chain of custody. A first approximation is that these datasets implicitly contain the goals of the people who created them. Often, the data labelers creating these datasets are employed in countries and nations with different societal norms. Following that, to some degree, the goals are those of the company developing the LLM and its employees, who ultimately can filter and subselect the data produced by those labelers.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>In response, we ask, “Are we, as users, comfortable using technology that may be biased toward alternative systems of belief that we do not share?” To some degree, we must be comfortable with this to use LLMs. The cost of creating these models and data sets is too high for us to make individualized models on every basis. As a result, LLM providers must exist, but the goals of those providers can’t possibly align with every potential user.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Simultaneously, suppose we are concerned about a nefarious actor using LLMs for evil or malicious purposes. In that case, we may also realize that our inability to solve the alignment problem is, in some ways, a blessing. If it were possible to perfectly align one of these algorithms to any individual’s belief system, then any bad actor could perfectly align an LLM to their bad behavior and beliefs. This thought highlights another problem: if we could create perfectly aligned LLMs, we would have to create LLMs so that only the good guys could align the LLMs to prevent the bad guys from doing bad things. This line of reasoning approaches the magical thinking that it is possible to create an all-powerful LLM that is simultaneously constrained to be obedient to all humans.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p74"> 
   <p> <span class="print-book-callout-head">Note</span> This way of thinking about alignment parallels similar thinking about encryption. Although one may attempt to create an encryption algorithm that includes a back door for good guys only that will allow them to decrypt the data, any such backdoor intrinsically becomes the highest-value target of attackers and increases the risk for all users. </p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>For this reason, we aren’t highly concerned about the potential for bad actors to align models to nefarious purposes. Still, the concern emphasizes a critical point for researchers: any progress in controlling LLMs is intrinsically a dual-use technology with both peaceful and adversarial applications. Indeed, anything we develop with LLMs is likely to be dual-use to some degree. Considering threat models when considering LLMs’ more serious potential harms is vital. Who would be motivated to perform such harm, why, and what is required to do so? What are the barriers in place today that prevent this harm from occurring, and does an LLM circumvent those barriers? Can the barriers be adapted to modern technology? As we proceed, our concern should focus not only on LLMs but also on the coexisting systems we operate that are the most significant enablers and blockers to success and risk. We must consider the complete picture to achieve the most desirable outcomes.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h2 class=" readable-text-h2" id="the-ethics-of-data-sourcing-and-reuse"><span class="num-string browsable-reference-id">9.3</span> The ethics of data sourcing and reuse</h2> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>LLMs and generative models like DALL-E, an image generation model that produces images based on user-provided text descriptions, require training on massive amounts of data. For example, LLM developers train models on 1 to 15 trillion tokens (e.g., Llama 3.1 used 15 trillion [18]) or 3 million to 30 million pages of text. This data represents an immense amount of writing, equal to hundreds of thousands or millions of books. While some models are trained repeatedly on the same data, and models are also trained on a wide variety of data such as code and mathematics, the amount of original text is still on the order of one million books</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p78"> 
   <p> <span class="print-book-callout-head">Note</span> It is important to note that much of this text isn’t books; it’s from many sources including news articles, websites, research papers, and government reports. We are summarizing this in units of books to make it more digestible, but it is <em>not true</em> that we train models on millions of books. </p> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>One of the main problems is that none of the existing models use training data whose license explicitly permits using it to train AIs. While some models are more license-compliant than others, most licensing still involves “all rights reserved” clauses, meaning that an owner has exclusive rights to the content and that others can’t use it for any purpose without their permission.</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>Further complicating this is that most content and data use licenses predate the existence of LLM technology. They do not envision training AI models as a potential use for data and, therefore, do not explicitly permit or prevent people using data this way. LLM developers are working on training models on more permissively licensed data. However, this doesn’t eliminate the core problem: mass data scraping to train AIs was not a recognized concern before, so existing licenses do not explicitly address this data use.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>An essential question for society and the law to grapple with is this: Under what conditions is reusing data for training a model considered acceptable use? Unfortunately, there are no clear answers to this question in the United States and other countries due to the lack of updated laws or established legal precedents. Older laws, like the US Digital Millennium Copyright Act, provide explicit protection to search engines for using data or text from other websites to create an index of content taken from the web. Does building an LLM using that content fall within those rights? We don’t know, and we aren’t your lawyer, but in this section, we will discuss some of the ethical factors in data acquisition for LLMs. We will present a brief primer on fair use and the rights of people who create the data and discuss the challenges of using public-domain data.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h3 class=" readable-text-h3" id="what-is-fair-use"><span class="num-string browsable-reference-id">9.3.1</span> What is fair use?</h3> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Many countries and cultures have different attitudes toward the use of copyrighted text. In many cases, there are meaningful exceptions to copyright law for people who use creative content in new ways, especially when those methods advance public good, scientific research, or have similar beneficial outcomes. In the United States, this is called “fair use.”</p> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Fair use always involves a context-sensitive analysis based on balancing four factors:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p85"><em>The purpose and character of the use</em>—Applications such as criticism, comment, education, news reporting, scholarship, or research are substantially more likely to be found to be fair use than other applications, especially when those other applications are commercial.</li> 
   <li class="readable-text" id="p86"><em>The nature of the copyrighted work</em>—Courts tend to give creative works, such as fictional writing, art, music, poetry, etc., more protection than nonfictional texts.</li> 
   <li class="readable-text" id="p87"><em>The amount or substantiality of the portion used</em>—Fair use may be permitted for using a part of a work, especially when that part is a narrowly tailored component.</li> 
   <li class="readable-text" id="p88"><em>The effect of the use on the potential market for or value of the work</em>—If the new use of the work produces something that someone might purchase instead of the original work, or if the new work otherwise competes with or diminishes the economic value of the original work, the work is less likely to be found to be fair use.</li> 
  </ul> 
  <div class="readable-text" id="p89"> 
   <p>Some of these points can be seen as favoring LLMs, while others conflict with how LLMs use data. Nevertheless, they are a subject of hot debate for practitioners in both the machine learning and legal fields, and it will take many years before the courts decide. Many applications of the fair use doctrine are to protect people from being exploited by a copyright holder. For example, if you are writing a negative product review, fair use prohibits the company from suing you for using their copyright to silence you. Other applications of fair use prevent the frustration of social needs, such as training students or apprentices on tools and techniques. LLMs uniquely stress some of these factors. Fundamentally, they often use content created by others, but some argue that certain types of content, such as comments on social media posts, are of minimal value. LLMs are creating a new market for the value of published work but are not commonly compensating the owners of that work.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>The unsatisfying but important answer for you as a practitioner is that you must operate and make decisions in an uncertain environment. If you can create your training data, you can circumvent much of this legal problem. Creating your training data from content you own is a particularly viable strategy for generative AI because, as discussed in chapter <a href="../Text/chapter-4.html">4</a>, the base models that need the most data are self-supervised. So you can get a lot of data to build an initial model and then put more work into a smaller fine-tuning dataset, as discussed in chapter <a href="../Text/chapter-5.html">5</a>.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>You will also be disappointed to learn that most people operating in this space are frequently unfamiliar with the laws relevant to their jurisdiction. There is a nontrivial chance that if you find a model released under a license compatible with your needs (good job checking the licenses!), that copyright or license on the data it has been trained on or refined from does not allow them to release it under that license. This general lack of care or awareness of data licensing concerns puts a burden on you to check, as well as you can, details related to the training data of third-party models and be aware that licensing concerns are prevalent in the field.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>Even if these legal questions are resolved favorably for the people who want to build LLMs, that does not make it ethical. The concerns discussed in this chapter contribute to what you may consider right or wrong. However, there is also a question about how to treat and interact with others today in a legally uncertain environment. Relying on the legal system to make something permissible is rarely a sign of actions that will engender goodwill and respect from the other parties involved. It is not hard to imagine an alternative scenario where companies make deals or partnerships with platforms that provide data that increases the number of consenting parties involved by either trading money or model usage rights. Once an agreement is in place, contracts can resolve conflicts around legal ambiguity, but this is, unfortunately, a rare occurrence in the field of LLMs.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h3 class=" readable-text-h3" id="the-challenges-associated-with-compensating-content-creators"><span class="num-string browsable-reference-id">9.3.2</span> The challenges associated with compensating content creators</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>One proposed solution to this ethical concern is to pay the authors, artists, and creators whose work exists in the training data. While this is conceptually appealing for many reasons, it may make the technology’s development economically unviable.</p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Society would be substantially more likely to reach an agreeable outcome if there were a relatively easy way to compensate creators appropriately for using their work. Using back-of-the-napkin math, we can estimate that one million books times <span><img alt="equation image" src="../Images/eq-chapter-9-95-1.png"/></span> yields a total cost of buying a copy of every work in the training corpus as equal to or greater than the cost of training the models themselves. The situation is even more dire for models whose training data is costly to create. Stable Diffusion, a popular image generation model, is trained on several <em>billion</em> images. It would cost over <span><img alt="equation image" src="../Images/eq-chapter-9-95-2.png"/></span> times what it costs to train the model to pay every artist in the training data one dollar, and one dollar per image is unlikely to be considered adequate compensation by artists.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Another approach to compensation would be to center compensation at the point of use: suppose every time a model generated content that drew from a book you wrote, you received a percentage of the income the model creator received. The more often the LLM generates content that relies on your work, the more significant fraction of that income you receive. While this could be a way to make long-term deployment of LLM technologies viable, there are substantial technical hurdles to implementing this model. For example, there is very little research on tracing the content generated by an LLM back to specific training data points. There is some reason to believe that such a task is impossible.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Better research on attributing generations to particular outputs, constraining outputs to only rely on a subset of the training data [19], or designing model training procedures where attribution is a central consideration (instead of one integrated into the LLM after training) would make this a substantially easier goal. Unfortunately, this kind of research typically requires training many similar LLMs; thus, it is costly. This expense makes it hard for anyone other than the technology companies that profit from the models to do the research.</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>This conversation does not yet consider the difficulty of identifying the owners of each document and compensating them. Further, paying people money at this scale is not free; processing fees alone would be a nontrivial fraction of the total payments because each author receives such a low average payment.</p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>If one believes that LLMs are a danger to society, you get the easy way out: you say that all these concerns are yet another reason not to create LLMs in the first place. If you are unconvinced that LLMs are an imposing danger to society, but rather, a positive addition, you now have a difficult question to answer. If you subscribe to a moral system like utilitarianism, you may argue that the net benefits of LLMs in utility and automation are more significant than the noncompensation and employment risk to the content creators. Indeed, the fair use doctrine is itself a form of legal recognition that there are cases where the copyright holder may not enforce their rights on others.</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <h3 class=" readable-text-h3" id="the-limitations-of-public-domain-data"><span class="num-string browsable-reference-id">9.3.3</span> The limitations of public domain data</h3> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>At this point, you may wonder whether data exists without copyright and if we should all use it to train LLMs instead. There is, indeed, a substantial amount of data in the public domain, meaning intellectual property laws do not protect it, and anyone can use it without asking permission or compensating the original copyright owner. Data can end up in the public domain for a variety of reasons, including being old (most countries have a maximum length of copyright), being non-copyrightable content (factual information, statistics, data generated without substantial human creative input, and some other forms of data are not copyrightable in the United States), or being made public domain by law (all US federal work products are public domain by law, and the US government can legislate that such work is in the public domain). Work in the public domain, perhaps combined with work licensed under terms like the MIT license or specific Creative Commons licenses, which intend to make the data widely used, could enable people to train models without dealing with these concerns. However, there are several significant challenges to doing so.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h4 class=" readable-text-h4" id="implicit-bias-and-the-public-domain"> Implicit bias and the public domain</h4> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>One of the primary sources of content in the public domain is works that are too old to be under copyright. As a result, there is an extreme bias toward older texts. Books written in the early 1900s or earlier express very different cultural attitudes and beliefs about science and technology and represent the world differently from works today. Having LLMs 95 years behind current cultural attitudes would be very bad from many perspectives. They would be full of inaccurate scientific information, exacerbate stereotypes and biases, use language less familiar to audiences today, and be hard to use productively.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p104"> 
   <p> <span class="print-book-callout-head">Note</span> Works published before 1977 lose their copyright 95 years after publication, so all works published in 1928 are public domain as of January 1, 2024, and all works published before 1977 will be public domain as of January 1, 2073. Under current copyright law, beginning in 2049, works published in 1978 and after will enter the public domain 70 years after the death of their creators, except for corporate-authored works, which follow the previous rules of entering the public domain after 95 years. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p105"> 
    <h5 class=" callout-container-h5 readable-text-h5">Should a model be exposed to racism?</h5> 
   </div> 
   <div class="readable-text" id="p106"> 
    <p> The problem of old data being, among other things, often quite racist and sexist is frustratingly complicated. It may seem obvious that we do not want any racist or sexist content in our training data, as it would seem an ideal means of ensuring that we do not fill our model with racist and sexist biases. However, if you successfully excluded this content from your training data, you would be hard-pressed to get that model to avoid generating racist or sexist output if instructed to do so by a user. The bottom line is that including unsavory content is necessary to make the model aware of what unsavory content is.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p107"> 
   <h4 class=" readable-text-h4" id="its-not-always-clear-what-is-in-the-public-domain"> It’s not always clear what is in the public domain</h4> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>The US government does not document which works are in the public domain and under active copyright. Identifying, collecting, and cleaning public domain works is a massive effort that requires legal, technological, and historical expertise. While some organizations have ongoing efforts to do this, the lack of readily available ways to check whether a work is in the public domain is a significant deterrent to training a model solely on such work.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h2 class=" readable-text-h2" id="ethical-concerns-with-llm-outputs"><span class="num-string browsable-reference-id">9.4</span> Ethical concerns with LLM outputs</h2> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>As we have discussed, LLMs are trained on large-scale data collected primarily from the internet. The internet contains a <em>lot</em> of undesirable materials. There is intensely negative content like overt racism, sexism, harmful conspiracy theories, and false information. More broadly, there are also just unintentional and outdated world views. LLMs pick up on the patterns of these views and will readily regurgitate them—an example of which can be found in figure <a href="#fig__sexistDoctorNurse">9.3</a>, showing how GPT-4 makes an implicitly sexist assumption that many good-intentioned people make.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>Thus, the outputs of an LLM can be problematic and require careful design, test-ing, and a willingness to say “no” to specific deployments. Although we have already discussed how the content of the output can be obviously and directly problematic, there are also indirect ways that LLM outputs can be problematic that are worth understanding in detail. First is legal complexity, in that valid and licensed data may not create legal outputs. Second, we must consider the potential for feedback in LLMs, meaning future LLMs will be trained on future data; we must be careful about corrupting future training with detrimental content. At first glance, these concerns seem irrelevant to developers, but when you consider fine-tuning an LLM to your problem, these problems will emerge, and awareness is required to avoid these risks.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p112">  
   <img alt="figure" src="../Images/CH09_F03_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__sexistDoctorNurse"><span class="num-string">Figure <span class="browsable-reference-id">9.3</span></span> A classic gendered trope is that men are doctors and women are nurses. This is reflected in language and thus learned by the model. Ideally, it would respond that the question is ambiguous, but instead, the bias of data leads to a bias in outputs.</h5>
  </div> 
  <div class="readable-text" id="p113"> 
   <h3 class=" readable-text-h3" id="licensing-implications-for-llm-output"><span class="num-string browsable-reference-id">9.4.1</span> Licensing implications for LLM output</h3> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>The first is a matter related to data licensing, which we introduced in the last section. That discussion focused on the ethics and validity of the data used to train an LLM. Now we have to turn the problem around: some data is almost certainly legal for training but may make the output unusable.</p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>This problem arises from the often-misunderstood world of open source software (OSS) licenses. There are many OSS licenses, and we won’t enumerate them all, but one commonly used open source license, known as the GNU General Public License, or GPL, is a good example. The GPL essentially says that you can use the licensed code as you wish, for free, so long as you make any code you use, modify, or add available under the GPL license. This intentionally “viral” license forces the licensee to follow the same rules and release their code as open source if they wish to use code covered by the GPL license.</p> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>Here comes the problem: LLMs have become quite popular for writing code and have been trained on GPL code. When must the output of the LLM itself become GPL-licensed? Multiple tiers of arguments quickly emerge as we consider the ethical questions related to this new situation that are not addressed explicitly by any of these licenses. A spectrum of possibilities exists with three main modes:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p117">If the LLM exactly regurgitated existing GPL code, surely it should be GPL licensed. How can we tell if an LLM is precisely generating copies of existing code that should be licensed accordingly?</li> 
   <li class="readable-text" id="p118">The LLM could generate seemingly novel code, but that algorithm may have needed specific GPL training data that solves related problems to generate the output. Is this a modification of the training data that should be licensed? If so, how do we solve the technical problem of finding the code that caused the LLM to generate any given output? The retrieval augmented generation (RAG) approach you learned about in chapter 5 could be a good way to do this.</li> 
   <li class="readable-text" id="p119">If we train the LLM on any GPL code, one could argue that all outputs of the LLM require a GPL license!</li> 
  </ul> 
  <div class="readable-text" id="p120"> 
   <p>In any case, we have the problem that while we can undoubtedly use GPL data to train an LLM, it is unclear how we can use the output of that LLM. Thus, knowing this risk, you now have an ethical question of where to draw this line if you wish to use an LLM for this work. Indeed, companies have to judge their own risk, and the question of who is liable and the degree of liability for each infringing use of GPLed outputs is unclear. Is it the organization that trained the model on GPL data, the company that uses the model to produce closed-source code based on GPL data, or none of the above?</p> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>The GPL license is intentionally viral, and many corporations treat it as a kind of poison that prevents them from protecting their intellectual property, embodied in software and source code. This notion of poisoning connects with our next topic—whether LLMs’ outputs are poisoning the training data required to build and improve future LLMs.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <h3 class=" readable-text-h3" id="do-llm-outputs-poison-the-well"><span class="num-string browsable-reference-id">9.4.2</span> Do LLM outputs poison the well?</h3> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>We begin this section using a metaphor based on a well-known problem in material sciences and manufacturing, specifically with alloy steel. Steel is used to build all sorts of things, from buildings to medical equipment. Many uses of steel also involve electronics that are sensitive to nuclear radiation. As a result of the first nuclear weapon tests in the 1940s, the entire world was polluted with radiation that did not previously exist. Unless you were near a nuclear detonation, there wasn’t enough radiation to harm most things. Still, there was enough radiation to contaminate all steel produced in the world in such a manner that you could no longer make steel for radiation-sensitive applications [20]. People would illegally salvage sunken ships from decades ago to find preexisting steel uncontaminated from background radiation. New manufacturing processes could produce a limited supply of clean steel, but they were astronomically expensive and thus economically infeasible in many cases. Thankfully, as materials science improved and atmospheric nuclear testing ceased, the problem diminished over time, but for decades, the world was affected by a few singular deployments of nuclear tests.</p> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>The analogy here is not that LLMs are nuclear bombs but that their output is potentially poisoning all training data that will be used to build better LLMs in the future. Researchers have identified a phenomenon known as <em>mode collapse</em> that demonstrates how LLMs can fail when trained on data generated by other LLMs [21]. As a quick refresher, the mode of a distribution (collection of numbers) is the most common value that occurs in that collection.</p> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>When a generative model produces output, most of that output will be from the mode of the distribution of content used to train the model. In other words, the output generated by a model will emphasize the most common components of its training data. Since the generative model will not output all the rare or nuanced cases in the data, the most common cases will be more prevalent in an LLM’s output. That means that the mode from the model is overrepresented compared to the original training data.</p> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>If you then train a new generative model on the outputs of this old model, you start to further overrepresent the mode at the cost of all other data. If you repeat this multiple times, you eventually get a useless model that always outputs the same thing repeatedly, as shown in figure <a href="#fig__modeCollapse">9.4</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p127">  
   <img alt="figure" src="../Images/CH09_F04_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__modeCollapse"><span class="num-string">Figure <span class="browsable-reference-id">9.4</span></span> You can think of text or images as coming from a distribution of data, where variety and interesting content almost necessarily come from the tails of the distribution (i.e., the less common parts of the distribution), as the most common words or content are often fillers or connectors, like the word <span><em><strong>the</strong></em></span>. Our models do not learn things they aren’t trained on and cannot learn everything in the distribution, so a sample from the model will invariably lose these interesting details. If repeated, the distribution collapses to just the most common components.</h5>
  </div> 
  <div class="readable-text" id="p128"> 
   <p>This concern raises the ethical question: Should we release LLMs to the public without implementing ways to prevent their output from contaminating future training data? Unfortunately, the opportunity to do anything about this concern has likely passed. LLM-generated content is prevalent in sources of training data frequently used to train LLMs and is often indistinguishable from human-generated content. None of the current LLM providers appear to be watermarking LLM-generated content by taking steps such as inserting subtle changes to the output to make it easily identifiable as generated data. While there is technical debate about how well watermarking can genuinely work, it is often the case that simple solutions are still sufficient for most use cases. Indeed, in chapter 2, we talked about how homoglyphs, different characters that look the same, are problematic for the input of an LLM. But they could be an easy watermark for LLM outputs, allowing trivial identification of the content that an LLM likely produced without postprocessing or editing. As a beneficial side effect, those who don’t want AI content (e.g., teachers) would have a more reliable option than the currently error-prone task of detecting LLMs [22].</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p129"> 
   <p> <span class="print-book-callout-head">Note</span> Mode collapse is a real risk that has been known for a long time, as it is a problem that goes beyond generative AI. However, human-augmented data can, but won’t necessarily, mitigate this risk. Essentially, as long as you can inject new data into the sampled distributions, it is possible to gain value from these samples. One way is by humans modifying AI-generated content or using AI to modify their human-generated content. Automated systems can also provide value, especially those that capture complex domain knowledge like a physics simulator or engine for mathematics proofs like Lean, which we discussed in section 6.2. The question becomes how well these augmentations are done and how much value they can gain, as they will not enable unlimited improvement. </p> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>There is a second, nontechnical concern in which we must question the ethical implications of LLMs poisoning the well. The manner in which people use technology has changed, potentially dramatically, since LLMs became available. Yet, the data we rely on to build our LLMs is based on how people interacted with information prior to the advent of LLMs. For example, Stack Exchange is a highly regarded collection of websites for question and answering, especially on technical topics like code. For this reason, it has been found particularly important for training LLMs. Yet, ChatGPT’s release itself may be hurting Stack Exchange and reducing the number of questions/answers posted, thus slowing the accumulation of new training content [23]. In other words, as people shift to using tools like LLMs to answer their questions, the need and benefit for humans generating the content seen on websites like Stack Exchange decrease, and thus the diversity of available training data is diminished.</p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Changes in behavior like this are a far more complex problem to address. Stack Exchange and the community of users who ask or answer questions on their website have autonomy and rights that should be respected. Their current policy is to ban the use of ChatGPT and similar tools to answer questions. However, we must consider whether there is a middle ground where careful applications of generated content can be coupled with human creation and curation to produce a virtuous cycle and novel results that benefit both humans and future LLMs. That may allow continued growth of the platform in a healthier way, but only if the owners and users are amenable.</p> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>Ultimately, our use of LLMs will have unintended consequences and unimaginable complexities. As a user, you must decide whether you are willing to accept the risk of these situations and how our use of these tools will alter the trajectory of future iterations.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h2 class=" readable-text-h2" id="other-explorations-in-llm-ethics"><span class="num-string browsable-reference-id">9.5</span> Other explorations in LLM ethics</h2> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>The conversation around the ethical implications of building and using LLMs is constantly evolving. Although much has been written on the subject, just as much remains to be explored on the ethics of LLMs and AI in general. Here, we have focused on the essential topics for building a foundational understanding. Other key concerns, such as privacy, security, and the potential for misuse, are covered further in books by Manning, such as <em>Introduction to Generative AI</em> by Numa Dhamani and Maggie Engler [24].</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>LLMs and generative AI will profoundly affect the world; with any new technology, it is essential to understand the foundations that guide its behavior and the implications of its use. Throughout this book, we have covered the fundamental components that make LLMs work, explored common misconceptions, and identified the ethical considerations for their construction and use. We hope to have established a strong foundation for you to continue your exploration of the field. Thank you for starting this journey with us.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p137">LLMs’ ability to be used for everything via one model helps people use them quickly and effectively for many tasks. This broad applicability to many tasks also makes it impossible to test the safety of all ways people may use LLMs.</li> 
   <li class="readable-text" id="p138">Historically, automation has been a good thing. Still, LLMs pose a unique risk to automating knowledge work, which differs from automating manual labor, the historical driver of improved living standards. The true effect of broadly automating knowledge work is unknown.</li> 
   <li class="readable-text" id="p139">Some fear that an LLM that is good enough to improve on a new LLM’s design will cascade to superintelligent algorithms that do not need humanity.</li> 
   <li class="readable-text" id="p140">Aligning any algorithm to what we meant, instead of what we asked, is a major challenge that likely has no reduction in risk even if solved.</li> 
   <li class="readable-text" id="p141">Ethically obtaining data is fraught with legal concerns due to technology moving faster than the law.</li> 
   <li class="readable-text" id="p142">The financial and technical logistics in compensating all content authors for their content’s use in the training data is unlikely to be practical, imposing ethical questions about the fairness of using their data.</li> 
   <li class="readable-text" id="p143">Public domain data with no copyright is too old to be problematic and poses different challenges related to identifying its legal status.</li> 
   <li class="readable-text" id="p144">The proliferation of LLM-generated data can potentially affect the LLMs we build in the future. We must consider the potential for feedback loops and the possibility of mode collapse.</li> 
  </ul>
 </body></html>