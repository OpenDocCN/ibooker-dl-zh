<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span> <span class="chapter-title-text">Clustering techniques</span></h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">Clustering techniques and salient use cases in the industry</li> &#13;
    <li class="readable-text" id="p3">Simple k-means, hierarchical, and density-based spatial clustering algorithms</li> &#13;
    <li class="readable-text" id="p4">Implementation of algorithms in Python </li> &#13;
    <li class="readable-text" id="p5">A case study on cluster analysis </li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p6"> &#13;
   <blockquote>&#13;
    <div>&#13;
     Simplicity is the ultimate sophistication. &#13;
     <div class=" quote-cite">&#13;
       —Leonardo da Vinci &#13;
     </div>&#13;
    </div>&#13;
   </blockquote> &#13;
  </div> &#13;
  <div class="readable-text" id="p7"> &#13;
   <p>Nature loves simplicity and teaches us to follow the same path. Most of the time, our decisions are simple choices. Simple solutions are easier to comprehend, less time-consuming, and painless to maintain and ponder over. The machine learning world is no different. An elegant machine learning solution is not the one that is the most complicated algorithm available but the one that solves the business problem. A robust machine learning solution is easy enough to readily decipher and pragmatic enough to implement. Clustering solutions are generally easier to understand. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p8"> &#13;
   <p>In the previous chapter, we defined unsupervised learning and discussed the various unsupervised algorithms available. We will cover each of those algorithms as we work through this book; in this second chapter, we focus on the first of these: clustering algorithms.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p9"> &#13;
   <p>We will define clustering first and then study the different types of clustering techniques. We will examine the mathematical foundation, accuracy measurements, and pros and cons of each algorithm. We will implement three of these algorithms using Python code on a dataset to complement theoretical knowledge. The chapter ends with the various use cases of clustering techniques in the pragmatic business scenario to prepare for the actual business world. This technique is followed throughout the book—we study the concepts first, implement the actual code to enhance the Python skills, and then dive into real-world business problems.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p10"> &#13;
   <p>We study basic clustering algorithms in this chapter, which are k-means clustering, hierarchical clustering, and density-based spatial clustering of applications with noise (DBSCAN) clustering. These clustering algorithms are generally the starting points whenever we want to study clustering. In the later chapters of the book, we will explore more complex algorithms like spectrum clustering, Gaussian mixture models, time series clustering, fuzzy clustering, and others. If you have a good understanding of k-means clustering, hierarchical clustering, and DBSCAN, you can skip to the next chapter. Still, it is advisable to read this chapter once—you might find something useful to refresh your concepts!</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p11"> &#13;
   <p>Let’s first understand what we mean by clustering. Good luck on your journey to master unsupervised learning–based clustering techniques!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p12"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.1</span> Technical toolkit </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p13"> &#13;
   <p>We use the latest version of Python in this chapter. A basic understanding of Python and code execution is expected. You are advised to refresh your knowledge of object-oriented programming and Python. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p14"> &#13;
   <p>Throughout the book, we use Jupyter Notebook to execute the code. Jupyter offers flexibility in execution and debugging. It is quite user-friendly and is platform or operating-system agnostic. So, if you are using Windows, macOS, or Linux, Jupyter should work just fine.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p15"> &#13;
   <p>All the datasets and code files are checked into the GitHub repository at <a href="https://mng.bz/lYq2">https://mng.bz/lYq2</a>. You need to install the following Python libraries to execute the code: <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, <code>scipy</code>, and <code>sklearn</code>. CPU is good enough for execution, but if you face some computing lags and would like to speed up the execution, switch to GPU or Google Collaboratory (Colab). Google Colab offers free computation for machine learning solutions. I recommend studying more about Google Colab and how to use it for training machine learning algorithms. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.2</span> Clustering </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p17"> &#13;
   <p>Consider this scenario: a group of children is asked to group the items in a room into different segments. Each child can use their own logic. Some might group the objects based on weight; other children might use material or color; while yet others might use all three: weight, material, and color. There are many permutations, and they depend on the parameters used for grouping. Here, a child is segmenting or clustering objects based on the chosen logic.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p18"> &#13;
   <p>Formally put, <em>clustering</em> is used to group objects with similar attributes in the same segments and objects with different attributes in different segments. The resultant clusters share similarities within themselves while they are more heterogeneous between each other. We can understand this better by looking at figure 2.1. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p19">  &#13;
   <img alt="figure" src="../Images/CH02_F01_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.1</span> Clustering is grouping objects with similar attributes into logical segments. The grouping is based on a similar trait shared by different observations, and hence they are gathered into a group. We are using shape as a variable for clustering here. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p20"> &#13;
   <p>Cluster analysis is not one individual algorithm or solution; rather it is used as a problem-solving mechanism in practical business scenarios. It is a class of algorithms under unsupervised learning and an iterative process following a logical approach and qualitative business inputs. It results in the generation of a thorough understanding of the data and the logical patterns in it, pattern discovery, and information retrieval. As an unsupervised approach, clustering does not need a target variable. It performs segmenting by analyzing underlying patterns in the dataset, which are generally multidimensional and, hence, difficult to analyze with traditional methods. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p21"> &#13;
   <p>Ideally, we want the clustering algorithms to have the following attributes:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p22"> The output clusters should be easy to explain and comprehend, usable, and make business sense. The number of clusters should not be too few or too many. For example, it is not ideal to have only two clusters, and the division is not clear and decisive. On the other hand, if we have 20 clusters, handling them will become a challenge. </li> &#13;
   <li class="readable-text" id="p23"> The algorithm should not be too sensitive to outliers or missing values or the noise in the dataset. Generally put, a good solution will be able to handle multiple data types. </li> &#13;
   <li class="readable-text" id="p24"> It is advisable for a data analyst/scientist to have a good grip on the business domain, although<em> </em>a good clustering solution may allow analysts with less domain understanding to train the clustering algorithm. </li> &#13;
   <li class="readable-text" id="p25"> The algorithm should be independent of the order of the input parameters. If the order matters, the clustering is biased on the order and hence will add more confusion to the process. </li> &#13;
   <li class="readable-text" id="p26"> As we generate new datasets continuously, the clusters should be scalable to newer training examples and should not be a time-consuming process. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p27"> &#13;
   <p>As one could imagine, the clustering output will depend on the attributes used for grouping. In figure 2.2, there can be two logical groupings for the same dataset, and both are equally valid. Hence, it is prudent that the attributes or <em>variables </em>for clustering are chosen wisely, and often that decision depends on the business problem at hand.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p28">  &#13;
   <img alt="figure" src="../Images/CH02_F02_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.2</span> Using different attributes for clustering results in different clusters for the same dataset. Hence, choosing the correct set of attributes defines the final set of results we will achieve.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p29"> &#13;
   <p>Along with the attributes used in clustering, the actual technique used also makes a big difference. There are quite a few (in fact, more than 80) clustering techniques. For the interested audience, we provide a list of all the clustering algorithms in the appendix.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p30"> &#13;
   <p>Clustering can be achieved using a variety of algorithms. These algorithms use different methodologies to define similarity between objects—for example, density-based clustering, centroid-based clustering, distribution-based methods, and others. Multiple techniques, such as Euclidean distance, Manhattan distance, etc., are available to measure the distance between objects. The choice of distance measurement leads to different similarity scores. We will study these similarity measurement parameters in a later section.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p31"> &#13;
   <p>At a high level, we can identify two broad clustering methods: <em>hard clustering</em> and <em>soft clustering </em>(see figure 2.3). When the decision is quite clear that an object belongs to a certain class or cluster, it is referred to as hard clustering. In hard clustering, an algorithm is quite sure of an object’s class. On the other hand, soft clustering assigns a likelihood score for an object belonging to a particular cluster. So, a soft clustering method will not put an object into a cluster; rather, an object can belong to multiple clusters. Soft clustering sometimes is also called <em>fuzzy</em> clustering.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p32">  &#13;
   <img alt="figure" src="../Images/CH02_F03_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.3</span> Hard clustering has distinct clusters, whereas in the case of soft clustering, a data point can belong to multiple clusters, and we get a likelihood score for a data point to belong to a cluster. The figure on the left is hard clustering, and the one on the right is soft clustering. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p33"> &#13;
   <p>We can broadly classify the clustering techniques as shown in table 2.1. The methods described are not the only ones available. We can have graph-based models, overlapping clustering, subspace models, etc. </p> &#13;
  </div> &#13;
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p34"> &#13;
   <h5 class=" browsable-container-h5"><span class="num-string">Table 2.1</span> Classification of clustering methodologies, brief descriptions, and examples </h5> &#13;
   <table> &#13;
    <thead> &#13;
     <tr> &#13;
      <th> &#13;
       <div>&#13;
         Serial no. &#13;
       </div> </th> &#13;
      <th> &#13;
       <div>&#13;
         Clustering methodology &#13;
       </div> </th> &#13;
      <th> &#13;
       <div>&#13;
         Brief description of the method &#13;
       </div> </th> &#13;
      <th> &#13;
       <div>&#13;
         Example &#13;
       </div> </th> &#13;
     </tr> &#13;
    </thead> &#13;
    <tbody> &#13;
     <tr> &#13;
      <td>  1 <br/> </td> &#13;
      <td>  Centroid-based clustering <br/> </td> &#13;
      <td>  Distance from a defined centroid <br/> </td> &#13;
      <td>  k-means <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  2 <br/> </td> &#13;
      <td>  Density-based models <br/> </td> &#13;
      <td>  Data points are connected in dense regions in a vector space <br/> </td> &#13;
      <td>  DBSCAN, OPTICS <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  3 <br/> </td> &#13;
      <td>  Connectivity-based clustering <br/> </td> &#13;
      <td>  Distance connectivity is the modus operandi <br/> </td> &#13;
      <td>  Hierarchical clustering, balanced iterative reducing and clustering using hierarchies <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  4 <br/> </td> &#13;
      <td>  Distribution models <br/> </td> &#13;
      <td>  Modeling is based on statistical distributions <br/> </td> &#13;
      <td>  Gaussian mixture models <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  5 <br/> </td> &#13;
      <td>  Deep learning models <br/> </td> &#13;
      <td>  Unsupervised neural network based <br/> </td> &#13;
      <td>  Self-organizing maps <br/> </td> &#13;
     </tr> &#13;
    </tbody> &#13;
   </table> &#13;
  </div> &#13;
  <div class="readable-text" id="p35"> &#13;
   <p>Generally, the six most popular algorithms used in clustering in the industry are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p36"> k-means clustering (with variants like k-medians, k-medoids) </li> &#13;
   <li class="readable-text" id="p37"> Agglomerative clustering or hierarchical clustering </li> &#13;
   <li class="readable-text" id="p38"> DBSCAN </li> &#13;
   <li class="readable-text" id="p39"> Spectral clustering </li> &#13;
   <li class="readable-text" id="p40"> Gaussian mixture models </li> &#13;
   <li class="readable-text" id="p41"> Balanced iterative reducing and clustering using hierarchies </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p42"> &#13;
   <p>Multiple other algorithms are available, like Chinese whisper, canopy clustering, SUBCLU, FLAME, and others. We will study the first three algorithms in this chapter and some of the advanced ones in subsequent chapters in the book. </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p43"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 2.1</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p44"> &#13;
    <p>Use these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p45"> DBSCAN clustering is a centroid-based clustering technique. True or False? </li> &#13;
    <li class="readable-text" id="p46"> Clustering is a supervised learning technique with a fixed target variable. True or False? </li> &#13;
    <li class="readable-text" id="p47"> What is the difference between hard clustering and soft clustering? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p48"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.3</span> Centroid-based clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p49"> &#13;
   <p>Centroid-based algorithms measure the similarity of the objects based on their distance to the centroid of the clusters (for more information on centroids, see the appendix). The distance is measured between a specific data point to the centroid for the cluster. The smaller the distance, the higher the similarity. We can understand the concept by looking at figure 2.4. The figure on the right side represents the respective centroids for each of the group of clusters. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p50"> &#13;
   <p><span class="print-book-callout-head">TIP </span> To get more clarity on the concept of centroid and other mathematical concepts, refer to the appendix.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p51">  &#13;
   <img alt="figure" src="../Images/CH02_F04_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.4</span> Centroid-based clustering methods create a centroid for the respective clusters, and the similarity is measured based on the distance from the centroid. In this case, we have five centroids; hence, we have five distinct clusters.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p52"> &#13;
   <p>In clustering, distance plays a central role as many algorithms use it as a metric to measure the similarity. In centroid-based clustering, distance is measured between points and between centroids. There are multiple ways to measure the distance. The most widely used are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p53"> <em>Euclidean distance</em><em> </em>—This is the most common distance metric used. It represents the straight-line distance between the two points in space and is the shortest path between the two points. For example, if we want to calculate the distance between points <em>P</em><sub>1</sub> and <em>P</em><sub>2</sub> where coordinates are (<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>) for <em>P</em><sub>1</sub> and (<em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>) for <em>P</em><sub>2</sub>, Euclidean distance is given by equation 2.1. The geometric representation is shown in figure 2.5: </li> &#13;
  </ul> &#13;
  <div class="browsable-container equation-container" id="p54"> &#13;
   <h5 class=" browsable-container-h5">(2.1)</h5> &#13;
   <p>Distance = √(<em>y</em><sub>2</sub> – <em>y</em><sub>1</sub>)<sub><sup>2</sup></sub> + (<em>x</em><sub>2</sub> – <em>x</em><sub>1</sub>)<sub><sup>2</sup></sub></p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p55"> <em>Chebyshev distance</em><em> </em>—Named after Russian mathematician Pafnuty Chebyshev, this is defined as the distance between two points such that their differences are maximum value along any coordinate dimension. Mathematically, we can represent Chebyshev distance in equation 2.2 and as shown in figure 2.5: </li> &#13;
  </ul> &#13;
  <div class="browsable-container equation-container" id="p56"> &#13;
   <h5 class=" browsable-container-h5">(2.2)</h5> &#13;
   <p>Chebyshev distance = max (|<em>y</em><sub>2</sub> – <em>y</em><sub>1</sub>|, |<em>x</em><sub>2</sub> – <em>x</em><sub>1</sub>|)</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p57">  &#13;
   <img alt="figure" src="../Images/CH02_F05_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.5</span> Euclidean distance, Chebyshev distance, Manhattan distance, and cosine distance are the primary distance metrics used. Note how the distance is different for two points using these metrics. In Euclidean distance, the direct distance is measured between two points, as shown by the first figure on the left. </h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p58"> <em>Manhattan distance</em><em> </em>—This is a very easy concept. It simply calculates the distance between two points in a grid-like path, and the distance is hence measured along the axes at right angles. Hence, sometimes it is also referred to as city block distance or the taxicab metric. Mathematically, we can represent the Manhattan distance in equation 2.3 and as shown in figure 2.5: </li> &#13;
  </ul> &#13;
  <div class="browsable-container equation-container" id="p59"> &#13;
   <h5 class=" browsable-container-h5">(2.3)</h5> &#13;
   <p>Manhattan distance = (|<em>y</em><sub>2</sub> – <em>y</em><sub>1</sub>| + |<em>x</em><sub>2</sub> – <em>x</em><sub>1</sub>|)</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p60"> &#13;
   <p>Manhattan distance is in L1 norm form while Euclidean distance is in L2 norm form. Refer to the appendix to study the L1 norm and L2 norm in detail. If we have a high number of dimensions or variables in the dataset, Manhattan distance is a better choice than Euclidean distance. This is due to the <em>curse of dimensionality,</em> which we will study in chapter 3.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p61"> <em>Cosine distance</em><em> </em>—Cosine distance is used to measure the similarity between two points in a vector-space diagram. In trigonometry, the cosine of 0 is 1 and the cosine of 90<sup>0</sup> is 0. Hence, if two points are similar to each other, the angle between them will be zero; hence, cosine will be 1, which means the two points are very similar to each other and vice versa. Mathematically, cosine similarity is shown in equation 2.4. If we want to measure the cosine between two vectors <em>A</em> and <em>B</em>, then cosine is </li> &#13;
  </ul> &#13;
  <div class="browsable-container equation-container" id="p62"> &#13;
   <h5 class=" browsable-container-h5">(2.4)</h5> &#13;
   <p>Cosine distance = (<em>A</em> . <em>B</em>) / (||<em>A</em>|| ||<em>B</em>||) </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p63"> &#13;
   <p><span class="print-book-callout-head">TIP </span> If you want to refresh your knowledge on the concepts of vector factorization, refer to the appendix.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p64"> &#13;
   <p>Other distance-measuring metrics, such as Hamming distance, Jaccard distance, and others, are available. Mostly, we use Euclidean distance in our pragmatic business problems, but other distance metrics are also used sometimes. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p65"> &#13;
   <p><span class="print-book-callout-head">Note </span> These distance metrics are true for other clustering algorithms too. I recommend testing the Python codes in the book with different distance metrics and comparing the performance.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p66"> &#13;
   <p>Now that we understand the various distance metrics, we proceed to study k-means clustering, which is the most widely used algorithm.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p67"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.3.1</span> K-means clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p68"> &#13;
   <p>K-means clustering is an easy and straightforward approach. It is arguably the most widely used clustering method to segment data points and create nonoverlapping clusters. We have to specify the number of clusters <em>k</em> we wish to create as an input, and the algorithm will associate each observation to exactly one of the k clusters. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p69"> &#13;
   <p><span class="print-book-callout-head">Note </span> K-means clustering is sometimes confused with the k-nearest neighbor (KNN) classifier. Although there is some relationship between the two, KNN is used for classification and regression problems.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p70"> &#13;
   <p>K-means clustering is quite an elegant approach and starts with some initial cluster centers and then iterates to assign each observation to the closest center. In the process, the centers are recalculated as the mean of points in the cluster. Let’s study the approach used in a step-by-step fashion by using the diagram in figure 2.6. For the sake of simplicity, we are assuming that there are three clusters in the dataset.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p71"> &#13;
   <p>The steps are as follows:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p72"> Let’s assume that we have all the data points, as shown in step 1. </li> &#13;
   <li class="readable-text" id="p73"> The three centers are initialized randomly, as shown by three squares: blue, red, and green. This input of three is the final number of clusters we wish to have. </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p74">  &#13;
   <img alt="figure" src="../Images/CH02_F06_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.6</span> Step 1 represents the raw dataset. In step 2, the algorithm initiates three random centroids as we have given the input of the number of clusters as three. In step 3, all the neighboring points of the centroids are assigned to the same cluster.</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p75"><span class="faux-ol-li-counter">3. </span> The distance of all the data points is calculated to the centers, and the points are assigned to the nearest center. Note that the points have attained blue, red, and green colors as they are nearest to those respective centers. (The colors are not distinguishable in the print version; hence we have grouped them together.) </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p76"><span class="faux-ol-li-counter">4. </span> The three centers are readjusted in this step. The centers are recalculated as the mean of the points in that cluster, as shown in figure 2.7. We can see that in step 4, the three squares have changed their respective positions as compared to step 3.<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p77">  &#13;
   <img alt="figure" src="../Images/CH02_F07_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.7</span> The centroids are recalculated in step 4. In step 5, the data points are again reassigned new centers. In step 6, the centroids are again readjusted as per the new calculations. </h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p78"><span class="faux-ol-li-counter">5. </span> The distance of all the data points is recalculated to the new centers and the points are reassigned to the nearest centers again. Note that two blue data points have become red while a red point has become green in this step. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p79"><span class="faux-ol-li-counter">6. </span> The centers are again readjusted as they were in step 4. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p80"><span class="faux-ol-li-counter">7. </span> The data points are again assigned a new cluster, as shown in figure 2.8. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p81"><span class="faux-ol-li-counter">8. </span> The process will continue until convergence is achieved. In other words, the process continues until there is no more reassignment of the data points; hence, we cannot improve the clustering further, and the final clustering is achieved. </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p82">  &#13;
   <img alt="figure" src="../Images/CH02_F08_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.8</span> The centroids are recalculated, and this process continues until we can no longer improve the clustering. Then the process stops, as shown in step 8.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p83"> &#13;
   <p>The objective of k-means clustering is to ensure that the within-cluster variation is as small as possible while the difference between clusters is as big as possible. In other words, the members of the same cluster are most similar to each other, while members in different clusters are dissimilar. Once the results no longer change, we can conclude that a local optimum has been reached, and clustering can stop. Hence, the final clusters are homogeneous within themselves while heterogeneous with each other. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p84"> &#13;
   <p>It is imperative to note two points here:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p85"> K-means clustering initializes the centers randomly; hence it finds a local optimum solution rather than a global optimum solution. Thus it is advisable to iterate the solution multiple times and choose the best output from all the results. By iteration, we mean to repeat the process multiple times, as in each of the iterations, the centroid chosen randomly will be different. </li> &#13;
   <li class="readable-text" id="p86"> We have to input the number of final clusters <em>k</em> we wish to have, and it changes the output drastically. A very small value of <em>k</em> relative to the data size will result in redundant clusters as they will not be of any use. In other words, if we have a very small value of <em>k</em> relative to big-sized data, data points with different characteristics will be cobbled together in a few groups. Having a very high value of <em>k</em> will create clusters that are different from each other minutely. </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p87"> &#13;
   <p>Moreover, having a very high number of clusters will be difficult to manage and refresh in the long run. Let’s study an example. If a telecom operator has 1 million subscribers, and if we take the number of clusters as two or three, the resultant cluster size will be very large. This can also lead to different customers classified in the same segment. On the other hand, if we take the number of clusters as 50 or 60, due to the sheer number of clusters, the output becomes unmanageable to use, analyze, and maintain. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p88"> &#13;
   <p>With different values of <em>k</em>, we get different results; hence, it is necessary that we understand how we can choose the optimum number of clusters for a dataset. Now let’s examine the process to measure the accuracy of clustering solutions. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p89"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.3.2</span> Measuring the accuracy of clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p90"> &#13;
   <p>One objective of clustering is to find the cleanest clusters. Theoretically (though not ideally), if we have the same number of clusters as the number of observations, the results will be completely accurate. In other words, if we have 1 million customers, the purest clustering will have 1 million clusters, wherein each customer is in a separate cluster. But it is not the best approach and is not a pragmatic solution either. Clustering intends to create a group of similar observations in one cluster, and we use the same principle to measure the accuracy of our solution. Other options include the following:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p91"> <em>Within the cluster sum of squares (WCSS) or cohesion</em>—This index measures the variability of the data points with respect to the distance they are from the centroid of the cluster. This metric is the average distance of each data point from the cluster’s centroid, which is repeated for each data point. If the value is too large, it shows there is a large data spread, whereas the smaller value indicates that the data points are quite similar and homogeneous and hence the cluster is compact.  </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p92"> &#13;
   <p>Sometimes, this intracluster distance is also referred to as <em>inertia</em> for that cluster. It is simply the summation of all the distances. The lower the value of inertia, the better the cluster is.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p93"> <em>Intercluster sum of squares</em>—This metric is used to measure the distance between centroids of all the clusters. To get it, we measure the distance between centroids of all the clusters and divide it by the number of clusters to get the average value. The bigger it is, the better the clustering is, indicating that clusters are heterogeneous and distinguishable from each other, as represented in figure 2.9.<span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p94">  &#13;
   <img alt="figure" src="../Images/CH02_F09_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.9</span> Intracluster vs. intercluster distance. Both are used to measure the purity of the final clusters and the performance of the clustering solution.</h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p95"> <em>Silhouette value</em>—This is one of the metrics used to measure the success of clustering. It ranges from –1 to +1, and a higher value is better. It measures how a data point is similar to other data points in its own cluster as compared to other clusters. As a first step, for each observation we calculate the average distance from all the data points in the same cluster; let’s call it <em>x</em><sub><em>i</em></sub>. Then we calculate the average distance from all the data points in the nearest cluster; let’s call it <em>y</em><sub><em>i</em></sub>.<sub> </sub>We will then calculate the coefficient by equation 2.5: </li> &#13;
  </ul> &#13;
  <div class="browsable-container equation-container" id="p96"> &#13;
   <h5 class=" browsable-container-h5">(2.5)</h5> &#13;
   <p>Silhouette coefficient = (<em>y</em><sub><em>i</em></sub> – <em>x</em><sub><em>i</em></sub>)/ max (<em>y</em><sub><em>i</em></sub>, <em>x</em><sub><em>i</em></sub>) </p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p97"> &#13;
   <p>If the value of coefficient is –1, it means that the observation is in the wrong cluster. If it is 0, the observation is very close to the neighboring clusters. If the value of coefficient is +1, it means that the observation is at a distance from the neighboring clusters. Hence, we would expect to get the highest value for the coefficient to have a good clustering solution.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p98"> <em>Dunn index</em>—This<em> </em>can also be used to measure the efficacy of the clustering. It uses the inter- and intradistance measurements defined in the previous intercluster sum of squares silhouette value sections and is given by equation 2.6: </li> &#13;
  </ul> &#13;
  <div class="browsable-container equation-container" id="p99"> &#13;
   <h5 class=" browsable-container-h5">(2.6)</h5> &#13;
   <p>Dunn index = min (intercluster distance)/max (intracluster distance)</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p100"> &#13;
   <p>Clearly, we would strive to maximize the value of Dunn index. To achieve it, the numerator should be as big as possible, implying that clusters are at a distance from each other, while the denominator should be as low as possible, signifying that the clusters are quite robust and close-packed. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p101"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.3.3</span> Finding the optimum value of k</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p102"> &#13;
   <p>Choosing the optimum number of clusters is not easy. As I said earlier, the finest clustering is when the number of clusters equals the number of observations, but as we studied in the last section, it is not practically possible. Hence, we should provide the number of clusters <em>k</em> as an input to the algorithm. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p103"> &#13;
   <p>Perhaps the most widely used method for finding the optimum value of <em>k</em> is the <em>elbow method. </em>In this method, we calculate within the cluster sum of squares or WCSS for different values of <em>k</em>. The process is the same as discussed in the last section. Then, WCSS is plotted on a graph against different values of <em>k</em>. Wherever we observe a kink or elbow, as shown in figure 2.10, we find the optimum number of clusters for the dataset. Notice the sharp edge.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p104">  &#13;
   <img alt="figure" src="../Images/CH02_F10_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.10</span> The elbow method to find the optimal number of clusters. The circle shows the kink. However, the final number of clusters is dependent on business logic, and often we merge/split clusters based on this. The ease of maintaining the clusters also plays a crucial role.</h5>&#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p105"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 2.2</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p106"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p107"> K-means clustering does not require the number of clusters as an input. True or False? </li> &#13;
    <li class="readable-text" id="p108"> KNN and k-means clustering are the same thing. True or False? </li> &#13;
    <li class="readable-text" id="p109"> Describe one possible process to find the optimal value of <em>k</em>. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p110"> &#13;
   <p>But this does not mean that it is the final number of clusters we suggest for the business problem. Based on the number of observations falling in each of the clusters, a few clusters might be combined or broken into subclusters. We also consider the computation cost required to create the clusters. The higher the number of clusters, the greater the computation cost and the time required. We can also find the optimum number of clusters using the silhouette coefficient we discussed earlier. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p111"> &#13;
   <p><span class="print-book-callout-head">Note </span> It is imperative that the business logic of merging a few clusters or breaking a few clusters is explored. Ultimately, the solution has to be implemented in real-world business scenarios. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p112"> &#13;
   <p>With this, we have examined the nuts and bolts of k-means clustering—the mathematical concepts and the process, the various distance metrics, and determining the best value of <em>k</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p113"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.3.4</span> Pros and cons of k-means clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p114"> &#13;
   <p>The k-means algorithm is quite a popular and widely implemented clustering solution. The solution offers the following advantages:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p115"> It is simple to comprehend and relatively easier to implement as compared to other algorithms. The distance measurement calculation makes it quite intuitive to understand even by users from nonstatistics backgrounds. </li> &#13;
   <li class="readable-text" id="p116"> If the number of dimensions is large, the k-means algorithm is faster than other clustering algorithms and creates tighter clusters. Hence, it is preferred if the number of dimensions is quite high. </li> &#13;
   <li class="readable-text" id="p117"> It quickly adapts to new observations and can generalize very well to clusters of various shapes and sizes. </li> &#13;
   <li class="readable-text" id="p118"> The solution produces results through a series of iterations of recalculations. Most of the time, the Euclidean distance metric is used, which makes it less computationally expensive. It also ensures that the algorithm surely converges and produces results. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p119"> &#13;
   <p>K-means is widely used for real-life business problems. Though there are clear advantages of k-means clustering, we do face certain challenges with the algorithm:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p120"> Choosing the optimum number of clusters is not easy. We should provide it as an input. With different values of <em>k</em>, the results will be completely different. The process of choosing the best value of <em>k</em> is explored in the previous section. </li> &#13;
   <li class="readable-text" id="p121"> The solution is dependent on the initial values of centroids. Since the centroids are initialized randomly, the output will be different with each iteration. Hence, it is advisable to run multiple versions of the solution and choose the best one. </li> &#13;
   <li class="readable-text" id="p122"> The algorithm is quite sensitive to outliers. Outliers can mess up the final results, and hence it is imperative that we treat outliers before starting with clustering. We can also implement other variants of the k-means algorithm, like k-modes clustering, to deal with the problem of outliers. We discuss dealing with outliers in section 11.4.4 of chapter 11. You can refer to it if you want to know how to deal with outliers. </li> &#13;
   <li class="readable-text" id="p123"> Since the basic principle of k-means clustering is to calculate the distance, the solution is not directly applicable to categorical variables. In other words, we cannot use categorical variables directly since we can calculate the distance between numeric values but cannot perform mathematical calculations on categorical variables. To resolve this, we can convert categorical variables to numeric ones using one-hot encoding. We discuss dealing with categorical variables in section 11.4.2 of chapter 11. You can refer to it if you want to know how to deal with categorical variables. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p124"> &#13;
   <p>Despite these problems, k-means clustering is one of the most used clustering solutions due to its simplicity and ease of implementation. There are different implementations of k-means algorithms like k-median, k-medoids, etc., which are sometimes used to resolve the problems faced:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p125"> As the name suggests, <em>k-median clustering</em> is based on the medians of the dataset as compared to the centroid in k-means. This increases the amount of computation time as the median can be found only after the data has been sorted. But at the same time, k-means is sensitive to outliers whereas k-median is less affected by them. </li> &#13;
   <li class="readable-text" id="p126"> <em>K-medoids clustering</em> is one of the variants of the k-means algorithm. Medoids are similar to means, except they are always from the same dataset and are implemented when it is difficult to get means, like with images. A medoid can be thought of as the most central point in a cluster that is least dissimilar to all the other members in the cluster. K-medoids choose the actual observations as the centers as compared to k-means, where the centroids may not even be part of the data. It is less sensitive to outliers as compared to the k-means clustering algorithm. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p127"> &#13;
   <p>There are other versions too, including k-means++, mini-batch k-means, and others. Generally, in the industry, k-means is used for most of the clustering solutions. You can explore other options like k-means++, mini-batch k-means, etc., if the results are not desirable or if the computation is taking a lot of time. Moreover, having different distance measurement metrics may produce different results for the k-means algorithm.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p128"> &#13;
   <p>This section concludes our discussion on the k-means clustering algorithm. It is time to hit the lab and develop actual Python code!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p129"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.3.5</span> K-means clustering implementation using Python</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p130"> &#13;
   <p>We will now create a Python solution for k-means clustering. In this case, we are using the dataset from the link at GitHub at <a href="https://mng.bz/lYq2">https://mng.bz/lYq2</a>. This dataset has information about the features of four models of cars. Based on the features of the car, we are going to group them into different clusters: </p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p131"> Import the libraries and the dataset into a dataframe. Here, <code>vehicles.csv</code> is the input data file. If the data file is not in the same folder as the Jupyter notebook, you would have to provide the complete path to the file. <code>dropna</code> is used to remove the missing values, if any: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p132"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import pandas as pd&#13;
vehicle_df = pd.read_csv('vehicle.csv').dropna()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p133"><span class="faux-ol-li-counter">2. </span> Perform some initial checks on the data, like shape, info, top five rows, distribution of classes, etc. This is to ensure that we have loaded the complete dataset and there is no corruption while loading the dataset. The <code>Shape</code> command will give the number of rows and columns in the data, <code>info</code> will describe all the variables and their respective types, and <code>head</code> will display the first five rows. The <code>value_counts</code> displays the distribution for the <code>class</code> variable. Or, in other words, <code>value_counts</code> returns the count of the unique values:  </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p134"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">vehicle_df.shape&#13;
vehicle_df.info()&#13;
vehicle_df.head()&#13;
pd.value_counts(vehicle_df['class'])</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p135"><span class="faux-ol-li-counter">3. </span> Generate two plots for the variable <code>class</code>. The dataset has more examples from <code>car</code> while for <code>bus</code> and <code>van</code> it is balanced data. I used the <code>matplotlib</code> library to plot these graphs. The outputs of the plots are as follows (see figure 2.11): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p136"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import matplotlib.pyplot as plt&#13;
%matplotlib inline&#13;
pd.value_counts(vehicle_df["class"]).plot(kind='bar')&#13;
pd.value_counts(vehicle_df['class']).hist(bins=300)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p137">  &#13;
   <img alt="figure" src="../Images/CH02_F11_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.11</span> Two plots for the variable <code>class</code></h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p138"><span class="faux-ol-li-counter">4. </span> Check for any missing data points in the dataset. There are no missing data points in our dataset, as we have already dealt with them: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p139"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">vehicle_df.isna().sum()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p140"> &#13;
   <p><span class="print-book-callout-head">Note </span> We cover the methods to deal with missing values in section 11.4.3 of chapter 11 as dropping the missing values is generally not the best approach.</p> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p141"><span class="faux-ol-li-counter">5. </span> Standardize the dataset. It is a good practice to standardize the dataset for clustering. It is important, as the different dimensions might be on a different scale, and one dimension may dominate the computation of distance if its values are naturally much larger than other dimensions. This is done using the <code>StandardScaler()</code> function. Refer to the appendix to examine different scaling techniques: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p142"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">vehicle_df_1 = vehicle_df.drop('class', axis=1)&#13;
from scipy.stats import zscore&#13;
vehicle_df_1_z = vehicle_df_1.apply(zscore)&#13;
from sklearn.preprocessing import StandardScaler&#13;
import umpy as np&#13;
sc = StandardScaler()&#13;
X_standard = sc.fit_transform(vehicle_df_1)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p143"><span class="faux-ol-li-counter">6. </span> Have a quick look at the dataset by generating a scatter plot. The plot displays the distribution of all the data points we have created as <code>X_standard</code> in the last step (see figure 2.12): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p144"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">plt.scatter(X_standard[:,0], X_standard[:,1])&#13;
plt.show()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p145">  &#13;
   <img alt="figure" src="../Images/CH02_F12_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.12</span> A scatter plot of the dataset</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p146"><span class="faux-ol-li-counter">7. </span> Perform k-means clustering. First, we have to select the optimum number of clusters using the elbow method. From the <code>sklearn</code> library, we import <code>KMeans</code>. In a <code>for</code> loop, we iterate for the values of clusters from 1 to 10. In other words, the algorithm will create between 1 and 10 clusters and will then generate the results for us to choose the optimal value of <em>k</em>. </li> &#13;
  </ol> &#13;
  <div class="readable-text list-body-item" id="p147"> &#13;
   <p>In the following code snippet, the model object contains the output of the k-means algorithm, which is then fit on the <code>X_standard</code> generated in the last step. Here, Euclidean distance has been used as a distance metric (see figure 2.13):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p148"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from sklearn.cluster import KMeans&#13;
from scipy.spatial.distance import cdist&#13;
clusters=range(1,10)&#13;
meanDistortions=[]&#13;
for k in clusters:&#13;
    model=KMeans(n_clusters=k)&#13;
    model.fit(X_standard)&#13;
    prediction=model.predict(X_standard)&#13;
    meanDistortions.append(sum(np.min(cdist(X_standard, &#13;
        model.cluster_centers_, 'euclidean'), axis=1)) / X_standard&#13;
                                                          .shape[0])&#13;
plt.plot(clusters, meanDistortions, 'bx-')&#13;
plt.xlabel('k')&#13;
plt.ylabel('Average distortion')&#13;
plt.title('Selecting k with the Elbow Method')<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p149">  &#13;
   <img alt="figure" src="../Images/CH02_F13_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.13</span> K-means clustering</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p150"><span class="faux-ol-li-counter">8. </span> As we can observe, the optimal number of clusters is three. It is the point where we can observe a sharp kink in the graph. We will continue with k-means clustering with the number of clusters as three. While there is nothing special about the number 3 here, it is best suited for this dataset. <code>random_state</code> is a parameter that is used to determine random numbers for centroid initialization. We set it to a value to make randomness deterministic. If you want to repeat the same results again, use the same random state number. It acts like a seed number: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p151"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">kmeans = KMeans(n_clusters=3, n_init = 15, random_state=2345) &#13;
kmeans.fit(X_standard)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p152"><span class="faux-ol-li-counter">9. </span> Get the <code>centroids</code> for the clusters: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p153"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">centroids = kmeans.cluster_centers_&#13;
centroids</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p154"><span class="faux-ol-li-counter">10. </span> Now we use the <code>centroids</code> so that they can be profiled by the <code>columns</code>: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p155"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">centroid_df = pd.DataFrame(centroids, columns = list(X_standard) )</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p156"><span class="faux-ol-li-counter">11. </span> We will now create a <code>dataframe</code> only for the purpose of creating the <code>labels</code>, and then we convert it into categorical variables: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p157"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">dataframe_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))&#13;
dataframe_labels['labels'] = dataframe_labels['labels'].astype('category')</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p158"><span class="faux-ol-li-counter">12. </span> In this step, we join the two <code>dataframes</code>: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p159"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">dataframe_labeled = vehicle_df_1.join(dataframe_labels)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p160"><span class="faux-ol-li-counter">13. </span> A <code>groupby</code> is done to create a data frame required for the analysis: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p161"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">dataframe_analysis = (dataframe_labeled.groupby(['labels'] , axis=0)).head(1234)&#13;
dataframe_labeled['labels'].value_counts()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p162"><span class="faux-ol-li-counter">14. </span> Now we create a visualization for the clusters we have defined. This is done using the <code>mpl_toolkits</code> library. The logic is simple to understand. The data points are colored as per the respective labels. The rest of the steps are related to the display of the plot by adjusting the label, title, ticks, etc. Since it is not possible to plot all 18 variables in the plot, we have chosen 3 variables to show in the plot (see figure 2.14): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p163"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from mpl_toolkits.mplot3d import Axes3D&#13;
fig = plt.figure(figsize=(8, 6))&#13;
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=60)&#13;
kmeans.fit(vehicle_df_1_z)&#13;
labels = kmeans.labels_&#13;
ax.scatter(vehicle_df_1_z.iloc[:, 0], vehicle_df_1_z.iloc[:, 1], vehicle_df_1_z.iloc[:, 3],c=labels.astype(np.float), edgecolor='k')&#13;
ax.w_xaxis.set_ticklabels([])&#13;
ax.w_yaxis.set_ticklabels([])&#13;
ax.w_zaxis.set_ticklabels([])&#13;
ax.set_xlabel('Length')&#13;
ax.set_ylabel('Height')&#13;
ax.set_zlabel('Weight')&#13;
ax.set_title('3D plot of KMeans Clustering on vehicles dataset')</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p164"> &#13;
   <p>We can also test the preceding code with multiple other values of <em>k</em>. We have created the code with different values of <em>k</em>. In the interest of space, we have put the code for testing with different values of <em>k</em> at the GitHub location. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p165">  &#13;
   <img alt="figure" src="../Images/CH02_F14_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.14</span> K-means clustering for the vehicles dataset</h5>&#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p166"> &#13;
   <p><span class="print-book-callout-head">Note </span> Exploratory data analysis holds the key to a robust machine learning solution and a successful project. In the subsequent chapters, we will create detailed exploratory data analyses for datasets.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p167"> &#13;
   <p>In the preceding example, we first did a small exploratory analysis of the dataset. This was followed by identifying the optimum number of clusters, which in this case comes out to be three. Then we implemented k-means clustering. You are expected to iterate the k-means solution with different initializations and compare the results, iterate with different values of <em>k</em>, and visualize to analyze the movements of data points. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p168"> &#13;
   <p>Centroid-based clustering is one of the most recommended solutions due to its less complicated logic, ease of implementation, flexibility, and trouble-free maintenance. Whenever we require clustering as a solution, mostly we start with creating a k-means clustering solution that acts as a benchmark. The algorithm is highly popular and generally one of the first solutions utilized for clustering. Then we test and iterate with other algorithms. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p169"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.4</span> Connectivity-based clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p170"> &#13;
   <p>“Birds of a feather flock together” is the principle followed in connectivity-based clusters. The core concept is that objects that are connected with each other are similar to each other. Hence, based on the connectivity between these objects, they are grouped into clusters. An example of such a representation is shown in figure 2.15, where we can iteratively group observations. As an example, we are initiating with all things, dividing into living and nonliving, and so on. Such representation is known as a <em>dendrogram. </em>Since there is a tree-like structure, connectivity-based clustering is sometimes referred to as hierarchical clustering. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p171">  &#13;
   <img alt="figure" src="../Images/CH02_F15_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.15</span> Hierarchical clustering utilizes grouping similar objects iteratively. Such representation is known as a dendrogram.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p172"> &#13;
   <p>Hierarchical clustering fits nicely into human intuition and, hence, is easy to comprehend. Unlike k-means clustering, in hierarchical clustering we do not have to input the number of final clusters, but the method does require a termination condition (i.e., when the clustering should stop). At the same time, hierarchical clustering does not suggest the optimum number of clusters. From the hierarchy/dendrogram generated, we have to choose the best number of clusters ourselves. We will explore this more when we create the Python code for it in subsequent sections.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p173"> &#13;
   <p>Figure 2.16 shows an example of hierarchical clustering. Here, the first node is the root, which is then iteratively split into nodes and subnodes. Whenever a node cannot be split further, it is called a terminal node or <em>leaf</em>.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p174">  &#13;
   <img alt="figure" src="../Images/CH02_F16_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.16</span> Hierarchical clustering has a root that splits into nodes and subnodes. A node that cannot be split further is called the leaf. In the bottom-up approach, a merging of the leaves will take place.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p175"> &#13;
   <p>Since there is more than one process or logic to merge the observations into clusters, we can generate a large number of dendrograms, which is given by equation 2.7:</p> &#13;
  </div> &#13;
  <div class="browsable-container equation-container" id="p176"> &#13;
   <h5 class=" browsable-container-h5">(2.7)</h5> &#13;
   <p>Number of dendrograms = (2<em>n</em> – 3)!/[2<sup>(</sup><sup><em>n</em></sup><sup>–2)</sup> (<em>n</em> – 2)!] </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p177"> &#13;
   <p>where <em>n</em> is the number of observations or the leaves. So, if we have only two observations, we can have only one dendrogram. If we have 5 observations, we can have 105 dendrograms. Hence, based on the number of observations, we can generate a lot of dendrograms. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p178"> &#13;
   <p>Hierarchical clustering can be further classified based on the process used to create the grouping of observations, which we explore next.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p179"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.4.1</span> Types of hierarchical clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p180"> &#13;
   <p>Based on the grouping strategy, hierarchical clustering can be subdivided into two types: <em>agglomerative</em> clustering and <em>divisive</em> clustering (see table 2.2).</p> &#13;
  </div> &#13;
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p181"> &#13;
   <h5 class=" browsable-container-h5"><span class="num-string">Table 2.2</span> Different types of hierarchical clustering</h5> &#13;
   <table> &#13;
    <thead> &#13;
     <tr> &#13;
      <th> &#13;
       <div>&#13;
         Serial no. &#13;
       </div> </th> &#13;
      <th> &#13;
       <div>&#13;
         Agglomerative clustering &#13;
       </div> </th> &#13;
      <th> &#13;
       <div>&#13;
         Divisive clustering &#13;
       </div> </th> &#13;
     </tr> &#13;
    </thead> &#13;
    <tbody> &#13;
     <tr> &#13;
      <td>  1 <br/> </td> &#13;
      <td>  Bottom-up approach. <br/> </td> &#13;
      <td>  Top-down approach. <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  2 <br/> </td> &#13;
      <td>  Each observation creates its own cluster and then merging takes place as the algorithm goes up. <br/> </td> &#13;
      <td>  We start with one cluster and then observations are iteratively split to create a tree-like structure. <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  3 <br/> </td> &#13;
      <td>  Greedy approach is followed to merge (the greedy approach is described below). <br/> </td> &#13;
      <td>  Greedy approach is followed to split. <br/> </td> &#13;
     </tr> &#13;
     <tr> &#13;
      <td>  4 <br/> </td> &#13;
      <td>  An observation will find the best pair to merge and the process completes when all the observations have merged with each other. <br/> </td> &#13;
      <td>  All the observations are taken at the start and then, based on division conditions, splitting takes place until all the observations are exhausted or the termination condition is met. <br/> </td> &#13;
     </tr> &#13;
    </tbody> &#13;
   </table> &#13;
  </div> &#13;
  <div class="readable-text" id="p182"> &#13;
   <p>Let’s explore the meaning of the greedy approach first. The greedy approach or greedy algorithm is any algorithm that makes the best choice at each step without considering the effect on future states. In other words, we live in the moment and choose the best option from the available choices at that moment. The current choice is independent of the future choices, and the algorithm will solve the subproblems later. The greedy approach may not provide the most optimal solution but generally provides a locally optimal solution that is close to the best solution in a reasonable amount of time. Hierarchical clustering follows this greedy approach while merging or splitting at a node. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p183"> &#13;
   <p>We next examine the steps followed in the hierarchical clustering approach:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p184"> As shown in figure 2.17, let us say we have five observations in our dataset: 1, 2, 3, 4, and 5. </li> &#13;
   <li class="readable-text" id="p185"> In this step, observations 1 and 2 are grouped into one and 4 and 5 are grouped into one; 3 is not grouped in any one. </li> &#13;
   <li class="readable-text" id="p186"> In this step, we group the output of 4,5 in the last step and observation 3 into one cluster. </li> &#13;
   <li class="readable-text" id="p187"> The output from step 3 is grouped with the output of 1,2 as a single cluster. </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p188">  &#13;
   <img alt="figure" src="../Images/CH02_F17_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.17</span> Steps followed in hierarchical clustering. From left to right, we have agglomerative clustering (merging of the nodes), while from right to left, we have divisive clustering (splitting of the nodes).</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p189"> &#13;
   <p>In this approach, from left to right, we have an agglomerative approach, and from right to left, a divisive approach is represented. In an agglomerative approach, we merge the observations, while in a divisive approach, we split the observations. We can use both agglomerative and divisive approaches for hierarchical clustering. Divisive clustering is an exhaustive approach and sometimes might take more time than the other. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p190"> &#13;
   <p>Similar to k-means clustering, the distance metric used to measure plays a significant role here. We are aware of and understand how to measure the distance between data points, but there are multiple methods to define that distance, which we study next. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p191"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.4.2</span> Linkage criterion for distance measurement</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p192"> &#13;
   <p>We can use Euclidean distance, Manhattan distance, Chebyshev distance, and others to measure the distance between two observations. At the same time, we can employ various methods to define that distance. Based on this input criterion, the resultant clusters will be different. The various methods to define the distance metric are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p193"> <em>Nearest neighbors or single linkages </em>use the distance between the two nearest points in different clusters. The distance between the closest neighbors in distinct clusters is calculated, and this is used to determine the next split/merging. It is done by an exhaustive search among all the pairs. </li> &#13;
   <li class="readable-text" id="p194"> <em>Farthest neighbor or complete linkage </em>is the opposite of the nearest neighbor approach. Here, instead of taking the nearest neighbors, we concentrate on the most distant neighbors in different clusters. In other words, the distance between the clusters is calculated by the greatest distance between two objects. </li> &#13;
   <li class="readable-text" id="p195"> <em>Group average linkage </em>calculates the average of the distances between all the possible pairs of objects in two different clusters. </li> &#13;
   <li class="readable-text" id="p196"> The <em>Ward linkage </em>method aims to minimize the variability of the clusters that are getting merged into one. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p197"> &#13;
   <p>We can use these options of distance metrics while we are developing the actual code for hierarchical clustering and compare the accuracies to determine the best distance metrics for the dataset. During algorithm training, the algorithm merges the observations, which will minimize the linkage criteria chosen. We can visualize the various linkages in figure 2.18.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p198"> &#13;
   <p><span class="print-book-callout-head">Note </span> Such inputs to the algorithm are referred to as hyperparameters. These are the values we feed to the algorithm to generate the results as per our requirement, and they act as our control on the algorithm. An example of a hyperparameter is <em>k</em> in k-means clustering. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p199">  &#13;
   <img alt="figure" src="../Images/CH02_F18_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.18</span> Single linkage is for closest neighbors (left); complete linkage is for farthest neighbors (center); and group average is for the average of the distance between clusters (right). </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p200"> &#13;
   <p>With this, we have understood the working mechanisms in hierarchical clustering. But we have still not addressed the mechanism to determine the optimum number of clusters using hierarchical clustering, which we examine next. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p201"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.4.3</span> Optimal number of clusters</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p202"> &#13;
   <p>Recall that in k-means clustering, we have to give the number of clusters as an input to the algorithm. We use the elbow method to determine the optimum number of clusters. In the case of hierarchical clustering, we do not have to specify the number of clusters to the algorithm, but we still have to identify the number of final clusters we wish to have. We use a dendrogram to answer that question.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p203"> &#13;
   <p>Let us assume that we have 10 data points in total at the bottom of the chart, as shown in figure 2.19. The clusters are merged iteratively until we get the one final cluster at the top. The height of the dendrogram at which two clusters get merged with each other represents the respective distance between the said clusters in the vector-space diagram. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p204">  &#13;
   <img alt="figure" src="../Images/CH02_F19_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.19</span> Dendrogram to identify the optimum number of clusters. The distance between X and Y is more than between A and B and between P and Q; hence, we choose that as the cut to create clusters and the number of clusters chosen is five. The x-axis represents the clusters, while the y-axis represents the distance (dissimilarity) between two clusters.</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p205"> &#13;
   <p>From a dendrogram, the number of clusters is given by the number of vertical lines being cut by a horizontal line. The <em>optimum</em> number of clusters is given by the number of vertical lines in the dendrogram cut by a horizontal line such that it intersects the tallest of the vertical lines. Or if the cut is shifted from one end of the vertical line to another, the length covered is the maximum. A dendrogram utilizes branches of clusters to show how closely various data points are related to each other. In a dendrogram, clusters that are located at the same height level are more closely related than clusters that are located at different height levels.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p206"> &#13;
   <p>In the example shown in figure 2.19, we have shown three potential cuts: AB, PQ, and XY. If we take a cut above AB, it will result in two very broad clusters, while below PQ will result in nine clusters that will become difficult to analyze further. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p207"> &#13;
   <p>Here, the distance between X and Y is more than between A and B and between P and Q. So we can conclude that the distance between X and Y is the maximum, and hence, we can finalize that as the best cut. This cut intersects at five distinct points; hence, we should have five clusters. The height of the cut in the dendrogram is similar to the value of <em>k</em> in k-means clustering. In k-means clustering, <em>k</em> determines the number of clusters. In hierarchical clustering, the best cut determines the number of clusters we wish to have.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p208"> &#13;
   <p>Similar to k-means clustering, the final number of clusters is not dependent on the choice from the algorithm only. Business acumen and pragmatic logic play a vital role in determining the final number of clusters. Recall that one of the important attributes of clusters is their usability, which we discussed in section 2.2. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p209"> &#13;
   <p>Sometimes we also use cophenetic correlation coefficient to measure how well the dendrogram represents the actual pairwise distance between the points. It compares the cophenetic distance, which is the height at which two points merged first in the dendrogram, with the original dissimilarity between the points.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p210"> &#13;
   <p>There is one more index known as the Calinski-Haranasz index. It measures the ratio of between-cluster dispersion to within-cluster dispersion. A higher value means better clustering, and hence we choose the optimal number of clusters to maximize this index.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p211"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 2.3</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p212"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p213"> What is the greedy approach used in hierarchical clustering? </li> &#13;
    <li class="readable-text" id="p214"> Complete linkage is used for finding distances for closest neighbors. True or False? </li> &#13;
    <li class="readable-text" id="p215"> What is the difference between group linkage and ward linkage? </li> &#13;
    <li class="readable-text" id="p216"> Describe the process to find the most optimal value of <em>k</em>. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p217"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.4.4</span> Pros and cons of hierarchical clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p218"> &#13;
   <p>Hierarchical clustering is a strong clustering technique and is quite popular, too. Similar to k-means, it also uses distance as a metric to measure similarity. At the same time, there are a few challenges with the algorithm. The advantages of hierarchical clustering are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p219"> Perhaps the biggest advantage of hierarchical clustering is the reproducibility of results. Recall in k-means clustering, the process starts with random initialization of centroids giving different results. In hierarchical clustering, we can reproduce the results. </li> &#13;
   <li class="readable-text" id="p220"> In hierarchical clustering, we do not have to input the number of clusters to segment the data. </li> &#13;
   <li class="readable-text" id="p221"> The implementation is easy to implement and comprehend. Since it follows a tree-like structure, it is explainable to users from nontechnical backgrounds. </li> &#13;
   <li class="readable-text" id="p222"> The dendrogram generated can be interpreted to give a very good understanding of the data with a visualization. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p223"> &#13;
   <p>At the same time, we do face some challenges with hierarchical clustering algorithms, which are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p224"> The biggest challenge we face with hierarchical clustering is the time taken to converge. The time complexity for k-means is linear, while for hierarchical clustering it is quadratic. For example, if we have “<em>n</em>” data points, then for k-means clustering the time complexity will be <em>O</em>(<em>n</em>), while for hierarchical clustering it is <em>O</em>(<em>n</em><sup>3</sup>). </li> &#13;
  </ul> &#13;
  <div class="readable-text print-book-callout" id="p225"> &#13;
   <p><span class="print-book-callout-head">TIP </span> Refer to the appendix if you want to study <em>O</em>(<em>n</em>).</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p226"> Since the time complexity is <em>O</em>(<em>n</em><sup>3</sup>), it is a time-consuming task. Moreover, the memory required to compute is at least <em>O</em>(<em>n</em><sup>2</sup>), making hierarchical clustering quite a time-consuming and memory-intensive process. And this is the problem even if the dataset is medium. The computation required might not be a challenge if we are using high-end processors, but it surely can be a concern for regular computers. </li> &#13;
   <li class="readable-text" id="p227"> The interpretation of dendrograms at times can be subjective; hence due diligence is required while interpreting dendrograms. The key to interpreting a dendrogram is to focus on the height at which any two data points are connected. It can be subjective, as different analysts can decipher different cuts and try to prove their methodology. Hence, it is advisable to interpret the results in the light of mathematics and marry the results with real-world business problems. </li> &#13;
   <li class="readable-text" id="p228"> Hierarchical clustering cannot undo the previous steps it has done. Even if we feel that a connection made is not proper and should be rolled back, there is no mechanism to remove the connection. </li> &#13;
   <li class="readable-text" id="p229"> The algorithm is very sensitive to outliers and messy datasets. The presence of outliers, NULL, missing values, duplicates, etc., makes a dataset messy. Hence the resultant output might not be proper or what we expected. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p230"> &#13;
   <p>But despite all the challenges, hierarchical clustering is one of the most widely used clustering algorithms. Generally, we create both k-means clustering and hierarchical clustering for the same dataset to compare the results of the two. If the number of clusters suggested and the distribution of respective clusters look similar, we get more confident about the clustering methodology used.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p231"> &#13;
   <p>We have covered the theoretical background of hierarchical clustering. It is time to take action and jump into Python for coding.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p232"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.4.5</span> Hierarchical clustering case study using Python</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p233"> &#13;
   <p>We will now create a Python solution for hierarchical clustering using the same dataset we used for k-means clustering:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p234"> Load the required libraries and dataset. For this, follow steps 1 to 6 we followed for the k-means algorithm.<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p235">  &#13;
   <img alt="figure" src="../Images/CH02_F20_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.20</span> Hierarchical clustering using average, ward, and complete linking methods (top to bottom, respectively)</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p236"><span class="faux-ol-li-counter">2. </span> Next, we create hierarchical clustering using three linkage methods: average, ward, and complete. Then the clusters will be plotted. The input to the method is the <code>X_Standard</code> variable, the linkage method used, and the distance metric. Then, using the <code>matplotlib</code> library, we plot the dendrogram. In the following code snippet, simply change the method from “average” to “ward” and “complete” and get the respective results (see figure 2.20): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p237"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from scipy.cluster.hierarchy import dendrogram, linkage&#13;
Z_df_average = linkage(X_standard, 'average', metric='euclidean')&#13;
Z_df_average.shape&#13;
plt.figure(figsize=(30, 12))&#13;
dendrogram(Z_df_average)&#13;
plt.show()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p238"><span class="faux-ol-li-counter">3. </span> We now want to choose the number of clusters we wish to have. For this purpose, let’s re-create the dendrogram by subsetting the last 10 merged clusters. We have chosen 10 as it is generally an optimal choice; I advise you to test with other values too (see figure 2.21):<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p239">  &#13;
   <img alt="figure" src="../Images/CH02_F21_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.21</span> A dendrogram subsetting the last 10 merged clusters</h5>&#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p240"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">dendrogram(&#13;
    Z_df_complete,&#13;
    truncate_mode='lastp',  p=10,)&#13;
plt.show()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p241"><span class="faux-ol-li-counter">4. </span> We observe that the most optimal distance is 10. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p242"><span class="faux-ol-li-counter">5. </span> Cluster the data into different groups. By using the logic described in the last section, the number of optimal clusters is going to be four: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p243"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from scipy.cluster.hierarchy import fcluster&#13;
hier_clusters = fcluster(Z_df_complete, max_distance, criterion='distance')&#13;
hier_clusters&#13;
len(set(hier_clusters))</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p244"><span class="faux-ol-li-counter">6. </span> Plot the distinct clusters using the <code>matplotlib</code> library. In the print version of the book, you will not see different colors. The output of the Python code will have the colors; I advise that you run the code to appreciate the output. The same output is available in the GitHub repository (see figure 2.22): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p245"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">plt.scatter(X_standard[:,0], X_standard[:,1], c=hier_clusters)  &#13;
plt.show()<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p246">  &#13;
   <img alt="figure" src="../Images/CH02_F22_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.22</span> A plot of the distinct clusters using the <code>matplotlib</code> library</h5>&#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p247"><span class="faux-ol-li-counter">7. </span> For different values of distance, the number of clusters will change, and the plot will look different. We are showing different results for distances of 5, 15, and 20 and different numbers of clusters generated for each iteration. Figure 2.23 shows that we get completely different results for different values of distances as we move from left to right. We should be cautious when we choose the value of the distance, and sometimes we might have to iterate a few times to get the best value.<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p248">  &#13;
   <img alt="figure" src="../Images/CH02_F23_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.23</span> The number of clusters using different values of distance</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p249"> &#13;
   <p>Using hierarchical clustering, we segment the data on the left side to the one on the right side of figure 2.24. The left side represents the raw data, while on the right, we have a representation of the clustered dataset. In the print version of the book, you won’t see the different colors. The output of the Python code will have the colors. The same output is available at the GitHub repository.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p250">  &#13;
   <img alt="figure" src="../Images/CH02_F24_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.24</span> Segmenting the data using hierarchical clustering</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p251"> &#13;
   <p>Hierarchical clustering is a robust method and is highly recommended. Along with k-means, it creates a great foundation for clustering-based solutions. Most of the time, at least these two techniques are used when we create clustering solutions, and then we move on to iterate with other methodologies.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p252"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.5</span> Density-based clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p253"> &#13;
   <p>We have studied k-means in the earlier sections. Recall how it uses a centroid-based method to assign a cluster to each of the data points. If an observation is an outlier, the outlier point pulls the centroid toward itself and is also assigned a cluster like a normal observation. These outliers do not necessarily bring information to the cluster and can affect other data points disproportionally but are still made a part of the cluster. Moreover, getting clusters of arbitrary shapes, as shown in figure 2.25, is a challenge with the k-means algorithm. Density-based clustering methods solve the problem.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p254">  &#13;
   <img alt="figure" src="../Images/CH02_F25_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.25</span> DBSCAN is highly recommended for irregular-shaped clusters. With k-means, we generally get spherical clusters; DBSCAN can resolve it.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p255"> &#13;
   <p>In the density-based method, the clusters are identified as the areas that have a higher density as compared to the rest of the dataset. In other words, given a vector-space diagram where the data points are represented, a cluster is defined by adjacent regions or neighboring regions of high-density points. This cluster will be separated from other clusters by regions of low-density points. The observations in the sparse areas or separating regions are considered noise or outliers in the dataset. A few examples of density-based clustering are shown in figure 2.25.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p256"> &#13;
   <p>We mentioned two terms: neighborhood and density. To understand density-based clustering, we will study these terms in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p257"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.5.1</span> Neighborhood and density</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p258"> &#13;
   <p>Imagine we represent data observations in a vector-space, and we have a point P. We now define the neighborhood for this point P. The representation is shown in figure 2.26. For a point P we have defined an <em class="obliqued">ε</em>—neighborhoods for it that are the points equidistant from P. In a 2D space, it is represented by a circle; in a 3D space it is a sphere; and for a <em>n</em>-dimensional space, it is <em>n</em>-sphere with center P and radius <em class="obliqued">ε</em>. This defines the concept of <em>neighborhood</em>.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p259">  &#13;
   <img alt="figure" src="../Images/CH02_F26_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.26</span> Representation of data points in a vector-space diagram. On the right-side we have a point P, and the circle drawn is of radius <em class="obliqued">ε</em>. So, for <em class="obliqued">ε</em> &gt; 0, the neighborhood of P is defined by the set of points that are at less than or equal to <em class="obliqued">ε</em> distance from the point P.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p260"> &#13;
   <p>Now let’s explore the term <em>density</em>. Recall density is mass divided by volume (mass/volume). The higher the mass, the higher the density, and the lower the mass, the lower the density. Conversely, the lower the volume, the higher the density, and vice versa.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p261"> &#13;
   <p>In the previous context, mass is the number of points in the neighborhood. In figure 2.26 we can observe the effect of <em class="obliqued">ε</em> on the number of data points or the mass. When it comes to volume, in the case of 2D space, volume is <span class="regular-symbol">π</span><em>r</em><sup>2</sup>, while for a sphere that is 3D, it is 4/3 <span class="regular-symbol">π</span><em>r</em><sup>3</sup>. For spheres of <em>n</em>-dimensions, we can calculate the respective volume as per the number of dimensions, which will be <span class="regular-symbol">π</span> times a numerical constant raised to the number of dimensions. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p262"> &#13;
   <p>So, in the two cases shown in figure 2.27, for a point P, we can get the number of points (mass) and volumes, and then we can calculate the respective densities. But the absolute values of these densities mean nothing to us; rather how they are similar (or different) from nearby areas is what’s important. The points that are in the same neighborhood and have similar densities can be grouped into one cluster. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p263">  &#13;
   <img alt="figure" src="../Images/CH02_F27_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.27</span> The effect of radius <em class="obliqued">ε</em>. On the left side, the number of points is more than on the right side. So the mass of the right side is less, since it contains a smaller number of data points.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p264"> &#13;
   <p>In an ideal case scenario, we wish to have highly dense clusters with a maximum number of points. In the two cases shown in figure 2.28, we have a less dense cluster depicted on the left and a high-dense one on the right. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p265">  &#13;
   <img alt="figure" src="../Images/CH02_F28_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.28</span> Denser clusters are preferred over less dense ones. Ideally, a dense cluster, with a maximum number of data points, is what we aim to achieve from clustering.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p266"> &#13;
   <p>From the preceding discussion, we can conclude that</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p267"> If we <em>increase</em> the value of <em class="obliqued">ε</em>, we will get a <em>higher</em> volume but not necessarily a higher number of points (mass). It depends on the distribution of the data points. </li> &#13;
   <li class="readable-text" id="p268"> If we <em>decrease</em> the value of <em class="obliqued">ε</em>, we will get a <em>lower</em> volume but not necessarily a lower number of points (mass). </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p269"> &#13;
   <p>These are the fundamental points we adhere to. Hence, it is imperative that we choose clusters that have high density and cover the maximum number of neighboring points. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p270"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.5.2</span> DBSCAN clustering</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p271"> &#13;
   <p>DBSCAN clustering is one of the highly recommended density-based algorithms. It clusters the data observations that are closely packed in a densely populated area but does not consider the outliers in low-density regions. Unlike k-means, we do not specify the number of clusters, and the algorithm is able to identify irregular-shaped clusters, whereas k-means generally proposes spherical-shaped clusters. Similar to hierarchical clustering, it works by connecting the data points but with the observations that satisfy the density criteria or the threshold value. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p272"> &#13;
   <p><span class="print-book-callout-head">Note </span> DBSCAN was proposed in 1996 by Martin Ester, Hans-Peter Kriegal, Jörg Sander, and Xiaowei Xu. The algorithm was given the Test of Time award in 2014 at ACM SIGKDD. The paper can be accessed at <a href="https://mng.bz/BXv1">https://mng.bz/BXv1</a>. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p273"> &#13;
   <p>DBSCAN works on the concepts of neighborhood we discussed in the last section. We will now dive deeper into the working methodology and building blocks of DBSCAN.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p274"> &#13;
   <h4 class=" readable-text-h4">nuts and bolts of DBSCAN clustering</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p275"> &#13;
   <p>Let’s now examine the core building blocks of DBSCAN clustering. We know it is a density-based clustering algorithm, and hence the neighborhood concept is applicable here. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p276"> &#13;
   <p>Say we have a few data observations that we need to cluster. We also locate a data point P. Then we can easily define two hyperparameter terms:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p277"> The radius of the neighborhood around P, known as <em class="obliqued">ε</em>, which we discussed in the last section. </li> &#13;
   <li class="readable-text" id="p278"> The minimum number of points we wish to have in the neighborhood of P or, in other words, the minimum number of points that are required to create a dense region. This is referred to as minimum points (minPts). It is one of the parameters we can input by applying a threshold on minPts. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p279"> &#13;
   <p>Based on these concepts, we can classify the observations into three broad categories: core points, border or reachable points, and outliers:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p280"> <em>Core points</em>—Any data point x can be termed as a core point if at least minPts are within <em class="obliqued">ε</em> distance of it (including x itself), shown as squares in figure 2.29. They are the building blocks of our clusters and are called core points. We use the same value of radius (<em class="obliqued">ε</em>) for each point and hence the <em>volume</em> of each neighborhood remains constant. But the number of points will vary and hence the <em>mass</em> varies. Consequently, the density varies as well. Since we put a threshold using minPts, we are putting a limit on density. So we can conclude that core points fulfill the minimum density threshold requirement. It is imperative to note that we can choose different values of <em class="obliqued">ε</em> and minPts to iterate and fine-tune the clusters.  </li> &#13;
   <li class="readable-text" id="p281"> <em>Border points or reachable points</em>—A point that is not a core point in the clusters is called a border point, shown as filled circles in figure 2.29. <span class="aframe-location"/> </li> &#13;
  </ul> &#13;
  <div class="browsable-container figure-container" id="p282">  &#13;
   <img alt="figure" src="../Images/CH02_F29_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.29</span> Core points are shown as squares; border points are shown as filled circles, while noise is unfilled circles. Together, these three are the building blocks for DBSCAN clustering. </h5>&#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p283"> &#13;
   <p>A point y is directly reachable from x if y is within <em class="obliqued">ε</em> distance of core point x. A point can only be approached from a core point, and it is the primary condition or rule to be followed. Only a core point can reach a noncore point, and the opposite is not true. In other words, a noncore point can only be reached by other core points; it cannot reach anyone else. In figure 2.29, border points are represented as dark circles. </p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p284"> &#13;
   <p>To understand the process better, we have to understand the term <em>density-reachable</em> or <em>connectedness</em>. In figure 2.30, we have two core points: X and Y. We can directly go from X to Y. Point Z is not in the neighborhood of X but is in the neighborhood of Y. So we cannot directly reach Z from X, but we can surely reach Z from X through Y or, in other words, using the neighborhood of Y, we can travel to Z from X. Conversely, we cannot go from Z to X since Z is the border point and, as described earlier, we cannot travel from a border point.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p285">  &#13;
   <img alt="figure" src="../Images/CH02_F30_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.30</span> X and Y are the core points, and we can travel from X to Y. Though Z is not in the immediate neighborhood of X, we can still reach Z from X through Y. This is the core concept of density-connected points used in DBSCAN clustering.</h5>&#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p286"> <em>Outliers</em>—All the other points are outliers. In other words, if it is not a core point or is not a reachable point, it is an outlier, shown as unfilled circles in figure 2.29. They are not assigned any cluster.  </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p287"> &#13;
   <h4 class=" readable-text-h4">steps in DBSCAN clustering</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p288"> &#13;
   <p>The steps in DBSCAN clustering are as follows:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p289"> We start with assigning values for <em class="obliqued">ε</em> and minPts required to create a cluster. </li> &#13;
   <li class="readable-text" id="p290"> We start with picking a random point, let’s say P, which is not yet given any label (i.e., it has not been analyzed and assigned any cluster). </li> &#13;
   <li class="readable-text" id="p291"> We then analyze the neighborhood for P. If it contains a sufficient number of points (i.e., higher than minPts), then the condition is met to start a cluster. If so, we tag the point P as the <em>core point</em>. If a point cannot be recognized as a core point, we will assign it the tag of <em>outlier</em> or <em>noise</em>. We should note this point can be made a part of a different cluster later. Then we go back to step 2. </li> &#13;
   <li class="readable-text" id="p292"> Once this core point P is found, we start creating the cluster by adding all directly reachable points from P and then increase this cluster size by adding more points directly reachable from P. Then we add all the points to the cluster, which can be included using the neighborhood by iterating through all these points. If we add an outlier point to the cluster, the tag of the outlier point is changed to a border point. </li> &#13;
   <li class="readable-text" id="p293"> This process continues until the density cluster is complete. We then find a new unassigned point and repeat the process. </li> &#13;
   <li class="readable-text" id="p294"> Once all the points have been assigned to a cluster or called an outlier, we stop our clustering process. </li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p295"> &#13;
   <p>There are iterations in the process. Then, once the clustering concludes, we utilize business logic to either merge or split a few clusters. </p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p296"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 2.4</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p297"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p298"> Compare and contrast the importance of DBSCAN clustering with respect to k-means clustering. </li> &#13;
    <li class="readable-text" id="p299"> A noncore point can reach a core point and vice versa is also true. True or False? </li> &#13;
    <li class="readable-text" id="p300"> Explain the significance of neighborhood and minPts. </li> &#13;
    <li class="readable-text" id="p301"> Describe the process to find the most optimal value of <em>k</em>. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p302"> &#13;
   <p>Now we are clear with the process of DBSCAN clustering. Before creating the Python solution, we will examine the advantages and disadvantages of the DBSCAN algorithm.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p303"> &#13;
   <h4 class=" readable-text-h4">pros and cons of DBSCAN clustering</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p304"> &#13;
   <p>DBSCAN has the following advantages: </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p305"> Unlike k-means, we need not specify the number of clusters to DBSCAN. </li> &#13;
   <li class="readable-text" id="p306"> The algorithm is quite a robust solution for unclean datasets. Unlike other algorithms, it can deal with outliers effectively. </li> &#13;
   <li class="readable-text" id="p307"> We can determine irregular-shaped clusters too. Arguably, this is the biggest advantage of DBSCAN clustering. </li> &#13;
   <li class="readable-text" id="p308"> Only the input of radius and minPts is required by the algorithm. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p309"> &#13;
   <p>DBSCAN has the following challenges:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p310"> The differentiation in clusters is sometimes not clear using DBSCAN. Depending on the order of processing the observations, a point can change its cluster. In other words, if a border point P is accessible by more than one cluster, P can belong to either cluster, which is dependent on the order of processing the data. </li> &#13;
   <li class="readable-text" id="p311"> If the difference in densities among different areas of the datasets is very big, then the optimum combination of <em class="obliqued">ε</em> and minPts will be difficult to determine, and hence DBSCAN will not generate effective results. </li> &#13;
   <li class="readable-text" id="p312"> The distance metric used plays a highly significant role in clustering algorithms, including DBSCAN. Arguably, the most common metric used is Euclidean distance, but if the number of dimensions is quite large, then it becomes a challenge to compute. </li> &#13;
   <li class="readable-text" id="p313"> The algorithm is very sensitive to different values of <em class="obliqued">ε</em> and minPts. Sometimes finding the most optimum value becomes a challenge. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p314"> &#13;
   <h4 class=" readable-text-h4">python solution for DBSCAN clustering</h4> &#13;
  </div> &#13;
  <div class="readable-text" id="p315"> &#13;
   <p>We will use the same dataset we have used for k-means and hierarchical clustering:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p316"> Load the libraries and dataset up to step 6 in the k-means algorithm. </li> &#13;
   <li class="readable-text" id="p317"> Import additional libraries: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p318"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from sklearn.cluster import DBSCAN &#13;
from sklearn.preprocessing import StandardScaler &#13;
from sklearn.preprocessing import normalize &#13;
from sklearn.neighbors import NearestNeighbors</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p319"> &#13;
   <p>Here we fit the model with a value for minimum distance and radius:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p320"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">db_default = DBSCAN(eps = 0.0375, min_samples = 6).fit(X_standard) &#13;
labels = db_default.labels_</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p321"> &#13;
   <p>The number of distinct clusters is 1:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p322"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">list(set(labels))</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p323"><span class="faux-ol-li-counter">3. </span> We are not getting any results for clustering here. In other words, there will not be any logical results of clustering since we have not provided the optimal values for minPts and <em class="obliqued">ε</em>. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p324"><span class="faux-ol-li-counter">4. </span> Now we will find the optimum values for <em class="obliqued">ε</em> (see figure 2.31). For this, we will calculate the distance to the nearest points for each point and then sort and plot the results. Wherever the curvature is maximum, it is the best value for <em class="obliqued">ε</em>. For minPts, generally minPts <span class="regular-symbol">≥</span> <em>d</em> + 1 where <em>d</em> is the number of dimensions in the dataset: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p325"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">neigh = NearestNeighbors(n_neighbors=2)&#13;
nbrs = neigh.fit(X_standard)&#13;
distances, indices = nbrs.kneighbors(X_standard)&#13;
distances = np.sort(distances, axis=0)&#13;
distances = distances[:,1]&#13;
plt.plot(distances)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p326">  &#13;
   <img alt="figure" src="../Images/CH02_F31_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.31</span> Finding the optimum value of <em class="obliqued">ε</em></h5>&#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p327"> &#13;
   <p><span class="print-book-callout-head">Note</span>  See the paper at <a href="https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf">https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf</a> for further study on how to choose the values of radius for DBSCAN. </p> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p328"><span class="faux-ol-li-counter">5. </span> The best value is coming up as 1.5, as observed in the point of defection. We will use it and set the minPts as 5, which is generally taken as a standard: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p329"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">db_default = DBSCAN(eps=1.5, min_samples=5)&#13;
db_default.fit(X_standard)&#13;
clusters = db_default.labels_</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p330"><span class="faux-ol-li-counter">6. </span> Now we can observe that we are getting more than one cluster: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p331"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">list(set(clusters))</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p332"><span class="faux-ol-li-counter">7. </span> Let’s plot the clusters (see figure 2.32). In the print version of the book, you will not see different colors. The output of the Python code will have the colors. The same output is available at the GitHub repository: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p333"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">colors = ['blue', 'red', 'orange', 'green', 'purple', 'black', 'brown', 'cyan', 'yellow', 'pink']&#13;
vectorizer = np.vectorize(lambda x: colors[x % len(colors)]) &#13;
plt.scatter(X_standard[:,0], X_standard[:,1], c=vectorizer(clusters))<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p334">  &#13;
   <img alt="figure" src="../Images/CH02_F32_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.32</span> Plotting the clusters</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p335"> &#13;
   <p>We have thus created a solution using DBSCAN. I advise you to compare the results from all three algorithms. In real-world scenarios, we test the solution with multiple algorithms, iterate with hyperparameters, and then choose the best solution.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p336"> &#13;
   <p>Density-based clustering is quite an efficient solution and, to a certain extent, is a very effective one too. It is heavily recommended if the shape of the clusters is suspected to be irregular. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p337"> &#13;
   <p>With this, we conclude our discussion on DBSCAN clustering. In the next section, we solve a business use case on clustering. In the case study, the focus is less on technical concepts and more on business understanding and solution generation.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p338"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.6</span> Case study using clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p339"> &#13;
   <p>We will now define a case study that employs clustering as one of the solutions. The objective of the case study is to give you a flavor of the practical and real-life business world. This case study–based approach is also followed in job-related interviews, wherein a case is discussed during the interview stage. I highly recommend you understand how we implement machine learning solutions in pragmatic business scenarios.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p340"> &#13;
   <p>A case study typically has a business problem, the dataset available, the various solutions that can be used, the challenges faced, and the final chosen solution. We also discuss the problems faced while implementing the solution in real-world business. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p341"> &#13;
   <p>So let’s start our case study on clustering using unsupervised learning. In the case study, we focus on the steps we take to solve the case study and not on the technical algorithms, as there can be multiple technical solutions to a particular problem. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p342"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.6.1</span> Business context </h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p343"> &#13;
   <p>The industry we are considering can be retail; telecom; banking, financial services, and insurance; aviation; healthcare; or any other industry that has a customer base. For any business, the objective is to generate more revenue for the business and ultimately increase the overall profit of the business. To increase revenue, the business would want to have increasingly more new customers. The business would also want the existing consumers to buy more and buy more often. So the business always strives to keep the consumers engaged and happy and to increase their transactional value with the business. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p344"> &#13;
   <p>For this to happen, the business must have a thorough understanding of its consumer base; it must know their tastes, price points, category preferences, affinity, preferred marketing/communication channels, etc. Once the business has examined and understood the consumer base minutely, then</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p345"> The product team can improve the product features as per the consumer’s need. </li> &#13;
   <li class="readable-text" id="p346"> The pricing team can improve the price of the products by aligning them to customers’ preferred prices. The prices can be customized for a customer, or loyalty discounts can be offered. </li> &#13;
   <li class="readable-text" id="p347"> The marketing team and customer relationship team can target the consumers with a customized offer. </li> &#13;
   <li class="readable-text" id="p348"> The teams can win back the consumers who are going to churn or stop buying from the business, can enhance their spending, increase the stickiness, and increase the customer lifetime value. </li> &#13;
   <li class="readable-text" id="p349"> Overall, different teams can align their offerings as per the understanding of the consumers generated. And the end consumer will be happier, more engaged, and more loyal to the business, leading to more fruitful consumer engagement. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p350"> &#13;
   <p>The business hence should dive deep into the consumers’ data and generate an understanding of the base. The customer data can look like that shown in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p351"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.6.2</span> Dataset for the analysis</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p352"> &#13;
   <p>We take as an example an apparel retailer that has a loyalty program and that saves the customer’s transaction details. The various (not exhaustive) data sources are shown in figure 2.33.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p353">  &#13;
   <img alt="figure" src="../Images/CH02_F33_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.33</span> Data sources for an apparel retail store</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p354"> &#13;
   <p>We can have store details, such as store ID, store name, city, area, number of employees, etc. We can have an item hierarchies table, which has all the details of the items like price, category, etc. Then we can have customer demographic details like age, gender, city, and customer transactional history. Clearly, by joining such tables, we will be able to create a master table that will have all the details in one place. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p355"> &#13;
   <p><span class="print-book-callout-head">Note</span>  I advise you to develop a good skill set for SQL. It is required in almost all of the domains related to data—be it data science, data engineering, or data visualization, SQL is ubiquitous. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p356"> &#13;
   <p>Figure 2.34 is an example of a master table. This is not an exhaustive list of variables, and the number of variables can be much larger than the ones shown. The master table has some raw variables like Revenue, Invoices, etc., and some derived variables like Average Transaction Value, Average Basket Size, etc.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p357">  &#13;
   <img alt="figure" src="../Images/CH02_F34_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.34</span> A master table</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p358"> &#13;
   <p>We could also take an example of a telecom operator. In that subscriber usage, call rate, revenue, days spent on the network, data usage, etc., will be the attributes we analyze. Hence, based on the business domain at hand, the datasets will change.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p359"> &#13;
   <p>Once we have the dataset, we generally create derived attributes from it. For example, the average transaction value attribute is total revenue divided by the number of invoices. We create such attributes in addition to the raw variables we already have. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p360"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.6.3</span> Suggested solutions</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p361"> &#13;
   <p>There can be multiple solutions to the problem, some of which we include in the following: </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p362"> We can create a dashboard to depict the major key performance indicators. This will allow us to analyze the history and take necessary actions based on it. But the dashboard will only show the information that we already know (to some extent). </li> &#13;
   <li class="readable-text" id="p363"> We can perform data analysis using some of techniques we used in the solutions in the earlier sections. This will solve a part of the problem and, moreover, it is difficult to consider multiple dimensions simultaneously. </li> &#13;
   <li class="readable-text" id="p364"> We can create predictive models to predict if the customers are going to shop in the coming months or are going to churn in the next <em>X</em> days, but this will not solve the problem completely. To be clear, “churn” here means that the customer no longer shops with the retailer in the next <em>X</em> days. Here, duration <em>X</em> is defined as per the business domain. For example, for the telecom domain, <em>X</em> will be less than in the insurance domain. This is due to the fact that people use mobile phones every day, whereas in the insurance domain, most customers pay the premium yearly. So customer interaction is less for insurance. </li> &#13;
   <li class="readable-text" id="p365"> We can create customer segmentation solutions wherein we group customers based on their historical trends and attributes. This is the solution we will use to solve this business problem. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p366"> &#13;
   <h3 class=" readable-text-h3"><span class="num-string">2.6.4</span> Solution for the problem </h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p367"> &#13;
   <p>Recall figure 1.9 in chapter 1, where we discussed the steps we follow in the machine learning algorithm. Everything starts with defining the business problem and then we move on to data discovery, preprocessing, etc. For our case study here, we will utilize a similar strategy. We have already defined the business problem; data discovery is done and we have completed the exploratory data analysis and the preprocessing of the data. To create a segmentation solution using clustering, follow these steps:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p368"> We start with finalizing the dataset we wish to feed to the clustering algorithms. We might have created some derived variables, treated some missing values or outliers, etc. In the case study, we would want to know the minimum/maximum/average values of transactions, invoices, items bought, etc. We would be interested to know the gender and age distribution. We also would like to know the mutual relationships between these variables, such as if women customers use the online mode more than male customers. All of these questions are answered as part of this step. </li> &#13;
  </ol> &#13;
  <div class="readable-text print-book-callout" id="p369"> &#13;
   <p><span class="print-book-callout-head">TIP </span> A Python Jupyter notebook is checked in at the GitHub repository, which provides detailed steps and code for the exploratory data analysis and data preprocessing. Check it out!</p> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p370"><span class="faux-ol-li-counter">2. </span> We create the first solution using k-means clustering followed by hierarchical clustering. For each of the algorithms, iterations are done by changing hyperparameters. In the case study, we will choose parameters like the number of visits, total revenue, distinct categories purchased, online/offline transactions ratio, gender, age, etc., as parameters for clustering. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p371"><span class="faux-ol-li-counter">3. </span> A final version of the algorithm and respective hyperparameters are chosen. The clusters are analyzed further in the light of business understanding. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p372"><span class="faux-ol-li-counter">4. </span> More often, the clusters are merged or broken, depending on the size of the observations and the nature of the attributes present in them. For example, if the total customer base is 1 million, it will be really hard to take action on a cluster of size 100. At the same time, it will be equally difficult to manage a cluster of size 700,000. </li> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p373"><span class="faux-ol-li-counter">5. </span> We then analyze the clusters we finally have. The clusters distribution is checked for the variables, their distinguishing factors are understood, and we give logical names to the clusters. We can expect to see such a clustering output as shown in figure 2.35. <span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p374">  &#13;
   <img alt="figure" src="../Images/CH02_F35_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.35</span> Segmentation based on a few dimensions like response, life stage, engagement, and spending patterns. The dimensions are not exhaustive, and in a real-world business problem, the number of dimensions can be higher.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p375"> &#13;
   <p>In the example clusters shown, we have depicted spending patterns, responsiveness to previous campaigns, life stage, and overall engagement as a few dimensions. Respective subdivisions of each of these dimensions are also shown. The clusters will be a logical combination of these dimensions. The actual dimensions can be much higher. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p376"> &#13;
   <p>The segmentation shown in figure 2.35 can be used for multiple domains and businesses. The parameters and attributes might change, the business context may be different, the extent of data available might vary—but the overall approach remains similar.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p377"> &#13;
   <p>In addition to the applications we saw in the last section, let’s examine a few use cases here:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p378"> Market research utilizes clustering to segment the groups of consumers into market segments; then the groups can be analyzed better in terms of their preferences. Product placement can be improved, pricing can be made tighter, and geography selection will be more scientific. </li> &#13;
   <li class="readable-text" id="p379"> In the bioinformatics and medical industry, clustering can be used to group genes into distinct categories. Groups of genes can be segmented and comparisons can be assessed by analyzing the attributes of the groups. </li> &#13;
   <li class="readable-text" id="p380"> Clustering is used as an effective data preprocessing step before we create algorithms using supervised learning solutions. It can also be used to reduce the data size by focusing on the data points belonging to a cluster. </li> &#13;
   <li class="readable-text" id="p381"> Clustering is utilized for pattern detection across both structured and unstructured datasets. We have already studied the case for a structured dataset. For text data, it can be used to group similar types of documents, journals, news, etc. We can also employ clustering to work and develop solutions for images. We will study unsupervised learning solutions for text and images in later chapters. </li> &#13;
   <li class="readable-text" id="p382"> As the algorithms work on similarity measurements, clustering can be used to segment the incoming dataset as fraud or genuine, which can be used to reduce the number of criminal activities. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p383"> &#13;
   <p>The use cases of clustering are many. We have discussed only the prominent ones. Clustering is one of the algorithms that changes the working methodologies and generates a lot of insights around the data. It is widely used across telecom; retail; banking, financial services, and insurance; aviation; and others.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p384"> &#13;
   <p>At the same time, there are a few problems with the algorithm. We next examine the common problems we face with clustering.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p385"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.7</span> Common challenges faced in clustering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p386"> &#13;
   <p>Clustering is not a completely straightforward solution without any challenges. Like any other solution in the world, clustering too has its share of problems. The most common challenges we face in clustering are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p387"> <em>Too much data</em><em> </em>—Sometimes the magnitude of the data is quite big, and there are a lot of dimensions available. In such a case, it becomes difficult to manage the dataset. The computation power might be limited, and like any project, there is finite time available. To overcome the problem, we can &#13;
    <ul> &#13;
     <li> Try to reduce the number of dimensions by finding the most significant variables by using a supervised learning-based regression approach or decision tree algorithm, etc. </li> &#13;
     <li> Reduce the number of dimensions by employing principal component analysis or singular value decomposition, etc. </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text" id="p388"> <em>A noisy dataset</em>—“Garbage in, garbage out” is a cliché that is true for clustering too. If the dataset is messy, it creates a lot of problems. The problems can include &#13;
    <ul> &#13;
     <li> Missing values (i.e., NULL, NA, ?, blanks, etc.). </li> &#13;
     <li> Outliers present in the dataset. </li> &#13;
     <li> Junk values like #€¶§^ etc., present in the dataset. </li> &#13;
     <li> Wrong entries made in the data. For example, if a name is entered in the Revenue field, it is an incorrect entry. </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p389"> &#13;
   <p>We discuss the steps and the process to resolve these problems in later chapters. In this chapter, we are examining how to work with categorical variables. </p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p390"> <em>Categorical variables</em><em> </em>—While discussing, recall the problem where k-means was not able to use categorical variables. We solve that problem next.  </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p391"> &#13;
   <p>To convert categorical variables into numeric ones, we can use <em>one-hot encoding</em>. This technique adds additional columns equal to the number of distinct classes as shown in the following figure. The variable city has unique values as London and New Delhi. We can observe that two additional columns have been created with 0 or 1 filled in for the values (see figure 2.36). <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p392">  &#13;
   <img alt="figure" src="../Images/CH02_F36_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.36</span> Using one-hot encoding to convert categorical variables into numeric ones</h5>&#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p393"> &#13;
   <p>Using one-hot encoding does not always ensure an effective and efficient solution. Imagine if the number of cities in this example is 100; then we will have 100 additional columns in the dataset, and most of the values will be filled in with 0. Hence, in such a situation, it is advisable to group a few values.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p394"> <em>Distance metrics</em><em> </em>—With different distance metrics, we might get different results. Though there is no “one size fits all,” Euclidean distance is most often used for measuring distance.  </li> &#13;
   <li class="readable-text" id="p395"> <em>Subjective interpretations</em><em> </em>—Interpretations for the clusters are quite subjective. By using different attributes, completely different clustering can be done for the same datasets. As discussed earlier, the focus should be on solving the business problem at hand. This holds the key to choosing the hyperparameters and the final algorithm. </li> &#13;
   <li class="readable-text" id="p396"> <em>Time-consuming</em><em> </em>—Since a lot of dimensions are dealt with simultaneously, sometimes converging the algorithm takes a lot of time.  </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p397"> &#13;
   <p>Despite all these challenges, clustering is a widely recognized and utilized technique. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p398"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.8</span> Concluding thoughts</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p399"> &#13;
   <p>Unsupervised learning is not an easy task. But it is certainly a very engaging one. It does not require any target variable, and the solution identifies the patterns itself, which is one of the biggest advantages of unsupervised learning algorithms. And the implementations are already having a great effect on the business world. We studied one of these solution classes called clustering in this chapter.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p400"> &#13;
   <p>Clustering is an unsupervised learning solution that is useful for pattern identifications, exploratory analysis, and, of course, segmenting the data points. Organizations heavily use clustering algorithms and proceed to the next level of understanding consumer data. Better prices can be offered, more relevant offers can be suggested, consumer engagement can be improved, and overall customer experience becomes better. After all, a happy consumer is the goal of any business. Clustering can be used not only for structured data but for text data, images, videos, and audio too. Due to its capability to find patterns across multiple datasets using a large number of dimensions, clustering is the go-to solution whenever we want to analyze multiple dimensions together.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p401"> &#13;
   <p>In this second chapter of this book, we introduced concepts of unsupervised-based clustering methods. We examined different types of clustering algorithms—k-means clustering, hierarchical clustering, and DBSCAN clustering—along with their mathematical concepts, respective use cases, and pros and cons with an emphasis on creating actual Python code for these datasets.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p402"> &#13;
   <p>In the following chapter, we will study dimensionality reduction techniques like principal component analysis and singular value decomposition. We will discuss the building blocks for techniques, their mathematical foundation, advantages and disadvantages, and use cases and perform actual Python implementation. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p403"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">2.9</span> Practical next steps and suggested readings</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p404"> &#13;
   <p>The following provides suggestions for what to do next and offers some helpful reading:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p405"> Get the online retail data from <a href="https://mng.bz/dXqo">https://mng.bz/dXqo</a>. This dataset contains all the online transactions occurring between January 12, 2010, and September 12, 2011, for a UK-based retailer. Apply the three algorithms described in the chapter to identify which customers the company should target and why. </li> &#13;
   <li class="readable-text" id="p406"> Get the IRIS dataset from <a href="https://www.kaggle.com/uciml/iris">https://www.kaggle.com/uciml/iris</a>. It includes three iris species with 50 samples, each having some properties of the flowers. Use k-means and DBSCAN and compare the results. </li> &#13;
   <li class="readable-text" id="p407"> Explore the dataset at UCI for clustering at <a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>. </li> &#13;
   <li class="readable-text buletless-item" id="p408"> Study the following papers on k-means clustering, hierarchical clustering, and DBSCAN clustering: &#13;
    <ul> &#13;
     <li> K-means clustering<br/> <a href="https://mng.bz/rKqJ">https://mng.bz/rKqJ</a><br/> <a href="https://mng.bz/VVEy">https://mng.bz/VVEy</a><br/> <a href="https://ieeexplore.ieee.org/document/1017616">https://ieeexplore.ieee.org/document/1017616</a> </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <ul> &#13;
   <li class=" buletless-item" style="list-style-type: none;"> &#13;
    <ul> &#13;
     <li class="readable-text" id="p409"> Hierarchical clustering<br/> <a href="https://ieeexplore.ieee.org/document/7100308">https://ieeexplore.ieee.org/document/7100308</a><br/> <a href="https://mng.bz/xKqd">https://mng.bz/xKqd</a><br/> <a href="https://mng.bz/AQno">https://mng.bz/AQno</a> </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <ul> &#13;
   <li class=" buletless-item" style="list-style-type: none;"> &#13;
    <ul> &#13;
     <li class="readable-text" id="p410"> DBSCAN clustering<br/> <a href="https://arxiv.org/pdf/1810.13105.pdf">https://arxiv.org/pdf/1810.13105.pdf</a><br/> <a href="https://ieeexplore.ieee.org/document/9356727">https://ieeexplore.ieee.org/document/9356727</a> </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p411"> &#13;
   <h2 class=" readable-text-h2">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p412"> Clustering is used for a variety of purposes across all industries, such as retail, telecom, finance, and pharma. Clustering solutions are implemented for customer and marketing segmentation to better understand the customer base, which further improves targeting. </li> &#13;
   <li class="readable-text" id="p413"> Clustering groups objects with similar attributes into segments, aiding in data understanding and pattern discovery without needing a target variable. </li> &#13;
   <li class="readable-text" id="p414"> Using clustering, we find the underlying patterns in a dataset and identify the natural groupings in the data. </li> &#13;
   <li class="readable-text" id="p415"> There can be multiple clustering techniques based on the methodology. A few examples are k-means clustering, hierarchical clustering, DBSCAN, and fuzzy clustering. </li> &#13;
   <li class="readable-text" id="p416"> Different clustering algorithms (k-means, hierarchical, DBSCAN) offer distinct pros and cons, and each is suitable for different data characteristics and purposes. </li> &#13;
   <li class="readable-text" id="p417"> Clustering is categorized into hard clustering, where objects belong to a single cluster, and soft clustering, where objects can belong to multiple clusters. </li> &#13;
   <li class="readable-text" id="p418"> Different clustering attributes and techniques, such as centroid-based, density-based, and distribution models, lead to varied clustering results. </li> &#13;
   <li class="readable-text" id="p419"> Effective clustering algorithms produce comprehensible, scalable, and independent clusters, handling outliers and multiple data types with minimal domain input. </li> &#13;
   <li class="readable-text" id="p420"> Distance metrics for clustering include Euclidean, Chebyshev, Manhattan, and cosine distances. </li> &#13;
   <li class="readable-text" id="p421"> Centroid-based clustering measures similarity based on the distance to the centroid of clusters. </li> &#13;
   <li class="readable-text" id="p422"> K-means clustering creates nonoverlapping clusters by specifying the number of clusters, <em>k</em>, and assigning data points to the nearest center iteratively. </li> &#13;
   <li class="readable-text" id="p423"> The elbow method is a common technique to determine the optimal number of clusters in k-means clustering. </li> &#13;
   <li class="readable-text" id="p424"> K-means is based on the centroid of the cluster. </li> &#13;
   <li class="readable-text" id="p425"> Hierarchical clustering creates clusters based on connectivity and does not require a predefined number of clusters. </li> &#13;
   <li class="readable-text" id="p426"> Hierarchical clustering can be agglomerative (bottom-up) or divisive (top-down) and uses linkage criteria to measure distances. </li> &#13;
   <li class="readable-text" id="p427"> DBSCAN identifies clusters based on point density and effectively distinguishes outliers. </li> &#13;
   <li class="readable-text" id="p428"> DBSCAN does not require specifying the number of clusters and is suited for irregular-shaped clusters. </li> &#13;
   <li class="readable-text" id="p429"> Measuring clustering accuracy involves metrics like WCSS, intercluster sum of squares, silhouette value, and the Dunn index. </li> &#13;
  </ul>&#13;
 </body></html>