- en: 7 Unsupervised learning for text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text data analysis: use cases and challenges'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing and cleaning text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector representation methods for text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis and text clustering using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI applications for text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everybody smiles in the same language.—George Carlin
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our world has so many languages. These languages are the most common medium
    of communication to express our thoughts and emotions. These words can be written
    into text. In this chapter, we explore the sorts of analysis we can do on text
    data. Text data falls under unstructured data and carries a lot of useful information
    and hence is a useful source of insights for businesses. We use natural language
    processing (NLP) to analyze the text data.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, to analyze text data, we have to make the data analysis-ready.
    Or, in very simple terms, since our algorithms and processors can only understand
    numbers, we have to represent the text data in numbers or *vectors*. We will explore
    all these steps in this chapter. Text data holds the key to quite a few important
    use cases, such as sentiment analysis, document categorization, and language translation,
    to name a few. We will cover the use cases using a case study and develop a Python
    solution on the same.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter starts with defining text data, sources of text data, and various
    use cases of text data. We will then move on to the steps and processes to clean
    and handle the text data. We cover the concepts of NLP, mathematical foundations,
    and methods to represent text data into vectors. We will create Python codes for
    the use cases. Toward the end, we share a case study on text data. Finally, we
    will also look into the generative AI-based (GenAI) solutions. We have not covered
    GenAI concepts yet in the book, as they are in part 3\. But here we introduce
    the concepts in the light of text data.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the seventh chapter, and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter Notebook as we
    have used so far. The codes and datasets used in this chapter have been checked
    in at the same GitHub location.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to install the following Python libraries for this chapter: `re`,
    `string`, `nltk`, `lxml`, `requests`, `pandas`, `textblob`, `matplotlib`, `sys`,
    `sklearn`, `scikitlearn`, and `warnings`. Along with these, you will need `numpy`
    and `pandas`. With libraries, we can use the algorithms very quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Text data is everywhere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall in the very first chapter of the book we explored structured and unstructured
    datasets. Unstructured data can be text, audio, image, or video. Examples of unstructured
    data and their respective sources are given in figure 7.1, where we explain the
    primary types of unstructured data—text, images, audio, and video—along with examples.
    The focus of this chapter is on text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 Unstructured data can be text, images, audio, or video. We deal with
    text data in this chapter. This list is not exhaustive.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Language is perhaps our greatest tool for communication. When in written form,
    this becomes text data. Today, thanks to widely accessible computers and smartphones,
    text is everywhere. It is generated by writing blogs and social media posts, tweets,
    comments, stories, reviews, chats, and comments, to name a few. Text data is generally
    much more direct than images and can be emotionally expressive. It is useful for
    businesses to unlock the potential of text data and derive insights from it. They
    can understand customers better, explore the business processes, and gauge the
    quality of services offered.
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever reviewed a product or a service on Amazon? You award stars to
    a product; at the same time, you can also input free text. Go to Amazon and look
    at some of the reviews. You might find some reviews have a good amount of text
    as the feedback. This text is useful for the product/service providers to enhance
    their offerings. Also, you might have participated in a few surveys that ask you
    to share your feedback. Moreover, with the advent of Alexa, Siri, Cortona, etc.,
    the voice command acts as an interface between humans and machines—which is again
    a rich source of data. Even the customer calls we make to a call center can be
    transcribed so that they become a source of text data. These calls can be recorded,
    and using speech-to-text conversion, we can generate a huge amount of text data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Use cases of text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all the use cases discussed in this section can implement unsupervised
    learning. Some require supervised learning too. Nevertheless, for your knowledge,
    we share both types of use cases, based on supervised learning and unsupervised
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sentiment analysis*—You might have participated in surveys or given your feedback
    on products/surveys. These surveys generate tons of text data. That text data
    can be analyzed, and we can determine whether the sentiment in the review is positive
    or negative. In simple words, sentiment analysis gauges the positivity or negativity
    of the text data. Hence, we can see the sentiment about a product or service in
    the minds of the customers. We can use both supervised and unsupervised learning
    for sentiment analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*News categorization or document categorization*—Look at the Google News web
    page and you will find that each news item has been categorized to sports, politics,
    science, business, or another category. Incoming news is classified based on the
    content of the news, which is the actual text. Imagine the thousands of documents
    that are sorted in this manner. In this use case, it is clear that machine learning
    is ideal, given the unfeasible amount of time and effort that would be required
    to sort such items manually. Supervised learning solutions work well for such
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Language translation*—Translation of text from one language to another is
    a very interesting use case. Using NLP, we can translate between languages. Language
    translation is very tricky, as different languages have different grammatical
    rules. Generally, deep learning–based solutions are the best fit for language
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spam filtering*—Email spam filters can be set up using NLP and supervised
    machine learning. A supervised learning algorithm can analyze incoming mail parameters
    and give a prediction if that email belongs to a spam folder or not. The prediction
    can be based on various parameters like sender email ID, subject line, body of
    the mail, attachments, time of mail, etc. Generally, supervised learning algorithms
    are used here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Part-of-speech tagging*—This is one of the popular use cases. It means that
    we can distinguish the nouns, adjectives, verbs, adverbs, etc., in a sentence.
    Named-entity recognition is also one of the famous applications of NLP. It involves
    identifying a person, place, organization, time, or number in a sentence. For
    example, John lives in London and works for Google. Named-entity recognition can
    generate understanding like [John][Person] lives in [London][Location] and works
    for [Google][organization].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sentence generation, captioning the images, speech-to-text or text-to-speech
    tasks, and handwriting recognition*—These are a few other significant and popular
    use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use cases listed here are not exhaustive. There are tons of other use cases
    that can be implemented using NLP. NLP is a very popular research field too. We
    share some significant papers at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You might have also heard about large language models (LLMs) like ChatGPT, Bard,
    and Claude. They are algorithms that process natural language inputs and predict
    the next word based on what they have already seen. With GenAI in the picture,
    a lot of the use cases can be solved by simply calling the API. ChatGPT can communicate
    like a human with memory and serves as customer support for many services. LLMs
    can summarize hundreds of pdf documents. You can even create applications that
    can be used for getting answers from multiple documents and websites. Certainly,
    GenAI has enhanced the power here.
  prefs: []
  type: TYPE_NORMAL
- en: While text data is very important, at the same time it is quite difficult to
    analyze. Remember, our computers and processors understand only numbers. So the
    text needs to be represented as numbers so we can perform mathematical and statistical
    calculations on it. Before diving into the preparation of text data, we cover
    some of the challenges we face while working on text datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Challenges with text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text is a difficult data type to work with. There are a large number of permutations
    to express the same thought. For example, I might ask, “Hey buddy, what is your
    age?” or “Hello there, may I know how old are you?”—they mean the same, right?
    The answer to both the questions is the same, and it is quite easy for humans
    to decipher, but it can be a daunting task for a machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most common challenges we face in this area are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Text data can be complex to handle. There can be a lot of junk characters like
    $^%*& present in the text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the advent of modern communications, we have started to use short forms
    of words; for example, “u” can be used for “you,” “brb” for “be right back,” and
    so on. Additionally, the challenge is where the same word might mean something
    different to different people, or misspelling a single letter can change the complete
    meaning of the word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language is changing, unbounded, and ever-evolving. It changes every day and
    new words are added to the language. If you do a simple Google search, you will
    find that quite a few words are added to the dictionary each year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The world has close to 6,500 languages, and each one carries its own unique
    characteristics. Each and every one completes our world. Each language follows
    its own rules and grammar, which are unique in usage and pattern. Even the writing
    can be different: some are written left to right, some right to left, and some
    even vertically. The same emotion might take fewer or more words in different
    languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The meaning of a word is dependent on the context. A word can be both an adjective
    and a noun, depending on the context. Consider these examples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “This book is a must-read” and “Please book a room for me.”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “Tommy” can be a name, but when used as “Tommy Hilfiger” its usage is completely
    changed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “Apple” is both a fruit and a company.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “April” is a month and can be a name too.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Look at one more example: “Mark traveled from the UK to France and is working
    with John over there. He misses his friends.” Humans can easily understand that
    “he” in the second sentence is Mark and not John, which might not be that simple
    for a machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be many synonyms for the same word, like “good” can be replaced by
    “positive,” “wonderful,” “superb,” or “exceptional” in different scenarios. Words
    like “studying,” “studious,” and “studies” are related to the same root word “study.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of text data can be daunting too. Managing a text dataset, storing
    it, cleaning it, and refreshing it is a herculean task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like any other machine learning project, text analytics follows the principles
    of machine learning, albeit the precise process is slightly different. Recall
    in chapter 1 we examined the process of a machine learning project, as shown in
    figure 7.2\. You are advised to refresh your memory on the process from chapter
    1 if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 The overall steps in a data science project are the same for text
    data. The preprocessing of text data is very different from the structured dataset.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Defining the business problem, data collection and monitoring, etc., remain
    the same. The major difference is in the processing of the text, which involves
    data cleaning, creation of features, representation of text data, etc. We will
    cover this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Note the three most effective use cases for the text data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is working on text data so tedious?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 7.5 Preprocessing the text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data, like any other data source, can be messy and noisy. We clean some
    of it in the data discovery phase and a lot of it in the preprocessing phase.
    At the same time, we should extract the features from our dataset. Some of the
    steps in the cleaning process are common and can be implemented on most text datasets.
    Some text datasets might require a customized approach. We start with cleaning
    the raw text data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Data cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with any form of data analysis, ensuring good data quality is vital. The
    cleaner the text data, the better the analysis. At the same time, preprocessing
    is not a straightforward task but rather is complex and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Text data must be cleaned as it contains a lot of junk characters, irrelevant
    words, noise and punctuation, URLs, etc. The primary ways of cleaning the text
    data are
  prefs: []
  type: TYPE_NORMAL
- en: '*Stopping word removal*—Out of all the words that are used in any language,
    there are some words that are most common. Stop words are the most common words
    in a vocabulary that carry less importance than key words. Examples are “is,”
    “an,” “the,” “a,” “be,” “has,” “had,” “it,” etc. Once we remove the stop words
    from the text, the dimensions of the data are reduced and hence the complexity
    of the solution is reduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can define a customized list of stop words and remove them that way, or there
    are standard libraries to remove the stop words.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, it is imperative that we understand the context very well
    while removing the stop words. For example, if we ask a question “is it raining?”
    then the answer “it is” is a complete answer in itself. When we are working with
    solutions where contextual information is important, we do not remove stop words.
  prefs: []
  type: TYPE_NORMAL
- en: '*Frequency-based removal of words*—Sometimes you might wish to remove the words
    that are most common in your text or that are very unique. The process is to get
    the frequency of the words in the text and then set a threshold of frequency.
    We can remove the most common ones. Or maybe you wish to remove the ones that
    have occurred only once/twice in the entire dataset. Based on the requirements,
    you will decide. At the same time, we should be cautious and observe due diligence
    while removing the words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Library-based cleaning*—This is done when we wish to clean the data using
    a predefined and customized library. We can create a repository of words that
    we do not want in our text and iteratively remove them from the text data. This
    approach allows us flexibility to implement the cleaning of our own choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Junk or unwanted characters*—Text data, particularly tweets, comments, etc.,
    might contain a lot of URLs, hashtags, numbers, punctuations, social media mentions,
    special characters, etc. We might need to clean them from the text. At the same
    time, we should be careful as some words that are not important for one domain
    might be required for a different domain. If data has been scraped from websites
    or HTML/XML sources, we need to get rid of all the HTML entities, punctuations,
    nonalphabet characters, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TIP  Always keep business context in mind while cleaning the text data.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, a lot of new types of expressions have entered the language—for
    example, lol, hahahaha, brb, rofl, etc. These expressions are to be converted
    to their original meanings. Even emojis like :-), ;-), etc., should be converted
    to their original meanings.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data encoding*—There are a few data encodings available like ISO/IEC, UTF-8,
    etc. Generally, UTF-8 is the most popular one. But it is not a hard and fast rule
    to always use UTF-8 only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lexicon normalization*—Depending on the context and usage, the same word might
    get represented in different ways. During lexicon normalization, we clean such
    ambiguities. The basic idea is to reduce the word to its root form. Hence, words
    that are derived from each other can be mapped to the central word, provided they
    have the same core meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 7.3 shows that the same word, “eat,” has been used in various forms.
    The root word is “eat,” but these different forms demonstrate the many different
    representations for “eat.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3 “Ate,” “eaten,” “eats,” and “eating” all have the same root word:
    “eat.” Stemming and lemmatization can be used to get the root word.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here, we wish to map all these words like “eating,” “eaten,” etc., to their
    central word, “eat,” as they have the same core meaning. There are two primary
    methods to work on this:'
  prefs: []
  type: TYPE_NORMAL
- en: Stemming is a basic rule-based approach for mapping a word to its core word.
    It removes “es,” “ing,” “ly,” “ed,” etc., from the end of the word. For example,
    studies will become “studi” and “studying” will become “study.” Being a rule-based
    approach, the output spellings might not always be accurate.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization is an organized approach that reduces words to their dictionary
    form. The *lemma* of a word is its dictionary or canonical form. For example,
    “eats,” “eating,” “eaten,” etc., all have the same root word “eat.” Lemmatization
    provides better results than stemming, but it takes more time.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only some of the methods to clean text data. These techniques will
    help, but business acumen is required to further make sense of the dataset. We
    will clean the text data using these approaches by developing a Python solution.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is cleaned, we start with the representation of data so that it
    can be processed by machine learning algorithms, which is our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Extracting features from the text dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have explored the concepts and techniques to clean up messy text data. Now
    we have cleaned the data, and it is ready to be used. The next step is to represent
    this data in a format that can be understood by our algorithms. As we know, our
    algorithms can only understand numbers.
  prefs: []
  type: TYPE_NORMAL
- en: A very simple technique to encode text data in a way that it can be useful for
    machine learning can be to simply perform one-hot encoding on our words and represent
    them in a matrix—but certainly not a scalable one if you have a complete document.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  One-hot encoding is covered in the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: The words can be first converted to lowercase and then sorted in alphabetical
    order. Then a numeric label can be assigned to them. Finally, words are converted
    to binary vectors. Let us understand using an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the text is “It is raining heavily,” we will use these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase the words so the output will be “it is raining heavily.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arrange them in alphabetical order. The result is heavily, is, it, raining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign place values to each word as heavily:0, is:1, it:2, raining:3\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Transform them into binary vectors as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[0\. 0\. 1\. 0.] #it'
  prefs: []
  type: TYPE_NORMAL
- en: '[0\. 1\. 0\. 0.] #is'
  prefs: []
  type: TYPE_NORMAL
- en: '[0\. 0\. 0\. 1.] #raining'
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. 0\. 0\. 0.]] #heavily'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we are able to represent each of the words in binary vectors,
    where 0 or 1 is the representation for each of the words. Though this approach
    is quite intuitive and simple to comprehend, it is pragmatically not possible
    when we have a massive corpus and vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Corpus refers to a collection of texts. It is Latin for “body.” It can
    be a body of written words or spoken words, which can be used to perform a linguistic
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, handling massive data sizes with so many dimensions will be computationally
    very expensive. The resulting matrix thus created will be very sparse too. Hence,
    we should consider other means and ways to represent our text data.
  prefs: []
  type: TYPE_NORMAL
- en: There are better alternatives than one-hot encoding. These techniques focus
    on the frequency of the word or the context in which the word is being used. This
    scientific method of text representation is much more accurate, robust, and explanatory.
    There are multiple such techniques like term frequency-inverse document frequency
    (TF-IDF), the bag of words approach, etc. We discuss a few of these techniques
    later in the chapter. First, we need to examine the important concept of tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is simply breaking a text or a set of text into individual tokens.
    It is the building block of NLP. Look at the example in figure 7.4, where we have
    created individual tokens for each word of the sentence. Tokenization is an important
    step as it allows us to assign unique identifiers or tokens to each of the words.
    Once we have allocated each word a specific token, the analysis becomes less complex.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 Tokenization can be used to break a sentence into different tokens
    of words.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tokens are usually used on individual words, but this is not always necessary.
    We are allowed to tokenize a word or the subwords or characters in a word. In
    the case of subwords, the same sentence can have subword tokens as rain-ing (i.e.,
    rain and ing as separate subtokens).
  prefs: []
  type: TYPE_NORMAL
- en: If we wish to perform tokenization at a character level, it can be r-a-i-n-i-n-g.
    In fact, in the first step of the one-hot encoding approach discussed in the last
    section, tokenization was done on the words. Tokenization at a character level
    might not always be used.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Tokenization is the building block for NPL solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have obtained the tokens, the tokens can be used to prepare a vocabulary.
    A vocabulary is the set of unique tokens in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple libraries for tokenization. *Regexp* tokenization uses the
    given pattern arguments to match the tokens or separators between the tokens.
    *Whitespace* tokenization treats any sequence of whitespace characters as a separator.
    Then we have *blankline,* which uses a sequence of blank lines as a separator.
    Finally, *wordpunct* tokenizes by matching a sequence of alphabetic characters
    and a sequence of nonalphabetic and nonwhitespace characters. We will perform
    tokenization when we create Python solutions for our text data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore more methods to represent text data. The first such method
    is the bag of words (BOW) approach.
  prefs: []
  type: TYPE_NORMAL
- en: 7.9 BOW approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, all the words in the corpus are used. In the BOW approach,
    the text data is tokenized for each word in the corpus, and then the respective
    frequency of each token is calculated. During this process, we disregard the grammar,
    the order, and the context of the word. We simply focus on the simplicity. Hence,
    we will represent each text (sentence or document) as a *bag of its own words*.
  prefs: []
  type: TYPE_NORMAL
- en: In the BOW approach for the entire document, we define the vocabulary of the
    corpus as all the unique words present in the corpus. Please note we use all the
    unique words in the corpus. If we want, we can also set a threshold (i.e., the
    upper and lower limit for the frequency of the words to be selected). Once we
    have the unique words, each of the sentences can be represented by a vector of
    the same dimension as the base vocabulary vector. This vector representation contains
    the frequency of each word of the sentence in the vocabulary. It might sound complicated,
    but it is actually a straightforward approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us understand this approach with an example. Let’s say that we have two
    sentences: “It is raining heavily” and “We should eat fruits.” To represent these
    two sentences, we calculate the frequency of each of the words in these sentences,
    as shown in figure 7.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 The frequency of each word has been calculated. In this example,
    we have two sentences.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now if we assume that the words in these two sentences represent the entire
    vocabulary, we can represent the first sentence as shown in figure 7.6\. Note
    that the table contains all the words, but the words that are not present in the
    sentence have received a value of 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 We are assuming that in the vocabulary only two sentences are present
    and the first sentence will be represented as shown.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this example, we examined how the BOW approach has been used to represent
    a sentence as a vector. But the BOW approach has not considered the order of the
    words or the context. It focuses only on the frequency of the word. Hence, it
    is a very fast approach to represent the data and is computationally less expensive
    compared to its peers. Since it is frequency based, it is commonly used for document
    classifications.
  prefs: []
  type: TYPE_NORMAL
- en: But, due to its pure frequency-based calculation and representation, solution
    accuracy using the BOW approach can take a hit. In language, the context of the
    word plays a significant role. As we have seen earlier, apple is both a fruit
    as well as a well-known brand and organization. That is why we have other advanced
    methods that consider more parameters than frequency alone. One such method is
    TF-IDF, which we will study next.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain tokenization in simple language as if you are explaining it to a person
    who does not know NLP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bag of words approach uses the context of the words and not frequency alone.
    True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lemmatization is a less rigorous approach than stemming. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 7.10 Term frequency and inverse document frequency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the BOW approach, we give importance to the frequency of a word only. But
    the words that have a higher frequency might not always offer meaningful information
    as compared to words that are rare but carry more importance. For example, say
    we have a collection of medical documents, and we wish to compare two words: “disease”
    and “diabetes.” Since the corpus consists of medical documents, the word “disease”
    is bound to be more frequent, while the word “diabetes” will be less frequent
    but more important to identify the documents that deal with diabetes. The term
    frequency and inverse document frequency (TF-IDF) approach allows us to resolve
    this problem and extract information on the more important words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In TF-IDF, we consider the relative importance of the word. TF means term frequency,
    and IDF means inverse document frequency. We can define TF-IDF in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: TF is the count of a term in the entire document (for example, the count of
    the word “a” in document “D”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IDF is the log of the ratio of total documents (*N*) in the entire corpus and
    the number of documents (*d**f*) that contain the word “a.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So the TF-IDF formula will give us the relative importance of a word in the
    entire corpus. The mathematical formula is the multiplication of TF and IDF and
    is given by equation 7.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch7-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: (7.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: where *N* is the total number of documents in the corpus, *tf*[*i*][,][*j*]is
    the frequency of the word in the document, and *df*[*i*] is the number of documents
    in the corpus that contain that word.
  prefs: []
  type: TYPE_NORMAL
- en: The concept might sound complex. Let’s understand this with an example. Say
    we have a collection of 1 million sports journals. These sports journals contain
    many articles of various lengths. We also assume that all the articles are in
    the English language only. So, let’s say, in these documents, we want to calculate
    the TF-IDF value for the words “ground” and “backhand.”
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we have a document of 100 words with the word “ground” appearing
    five times and “backhand” only twice. So the TF for ground is 5/100 = 0.05, and
    for backhand, it is 2/100 = 0.02.
  prefs: []
  type: TYPE_NORMAL
- en: We understand that the word “ground” is quite a common word in sports, while
    the word “backhand” will be used less often. Now we assume that “ground” appears
    in 100,000 documents out of 1 million documents while “backhand” appears only
    in 10\. So the IDF for “ground” is log (1,000,000/100,000) = log (10) = 1\. For
    “backhand” it will be log (1,000,000/10) = log (100,000) = 5.
  prefs: []
  type: TYPE_NORMAL
- en: To get the final values for “ground,” we multiply TF and IDF = 0.05 × 1 = 0.05\.
    To get the final values for “backhand,” we multiply TF and IDF = 0.02 × 5 = 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: We can observe in this case that the relative importance of the word “backhand”
    is more than the relative importance of the word “ground.” This is the advantage
    of TF-IDF over the frequency-based BOW approach. But TF-IDF takes more time to
    compute as compared to BOW, since all the TF and IDF have to be calculated. Nevertheless,
    TF-IDF offers a better and more mature solution as compared to the BOW approach
    in such cases. So, in scenarios where the relative importance of a word is in
    discussion, we can use TF-IDF. For example, if the task is to shortlist medical
    documents on cardiology, the importance of the word “angiogram” will be higher
    as it is much more related to cardiology.
  prefs: []
  type: TYPE_NORMAL
- en: We have so far covered BOW and the TF-IDF approach. But in neither of these
    approaches did we take the sequence of the words into consideration, which is
    covered in language models. We cover language models next.
  prefs: []
  type: TYPE_NORMAL
- en: 7.11 Language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models assign probabilities to the sequence of words. N-grams are the
    simplest in language models. We know that to analyze the text data, they must
    be converted to feature vectors. N-gram models create the feature vectors so that
    text can be represented in a format that can be analyzed further.
  prefs: []
  type: TYPE_NORMAL
- en: N-gram is a probabilistic language model. In an n-gram model, we calculate the
    probability of the *N*^(th) word given the sequence of (*N* – 1) words. To be
    more spe- cific, an n-gram model will predict the next word *x*[*i*] based on
    the words *x*[*i*][–(][*n–*][1][)], *x*[*i*][–(][*n–*][2][)]…*x*[*i*][–1]. If
    we wish to use the probability terms, we can represent them as the conditional
    probability of *x*[*i*] given the previous words, which can be represented as
    *P*(*x*[*i*] | *x*[*i*][–(][*n–*][1][)], *x*[*i*][–(][*n–*][2][)]…*x*[*i*][–1]).
    The probability is calculated by using the relative frequency of the sequence
    occurring in the text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  If the items are words, n-grams may be referred to as *shingles*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s study this using an example. We will take a sentence and then break down
    the meaning by using words in the sentence. Consider we have the sentence “It
    is raining heavily.” We show the respective representations of this sentence by
    using different values of *n* in figure 7.6\. You should note how the sequence
    of words and their respective combinations are getting changed for different values
    of *n*. If we wish to use *n* = 1 or a single word to make a prediction, the representation
    will be as shown in figure 7.7\. Note that each word is used separately here.
    They are referred to as *unigrams*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 Unigrams, bigrams, and trigrams can be used to represent the same
    sentence. The concept can be extended to n-grams too.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If we wish to use *n* = 2, the number of words used will become two. They are
    referred to as *bigrams*. If we use *n* = 3, the number of words becomes three,
    and they are referred to as *trigrams,* and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, if we have a unigram, it is a sequence of one word; for two words, it
    is a bigram; for three words, it is a trigram; and so on. So, a trigram model
    will approximate the probability of a word given all the previous words by using
    the conditional probability of only the preceding two words, whereas a bigram
    will do the same by considering only the preceding word. This is a valid assumption,
    indeed, that the probability of a word will depend only on the preceding word
    and is referred to as the *Markov* assumption. Generally, *n* > 1 is considered
    to be much more informative than unigrams. But obviously, the computation time
    will increase too.
  prefs: []
  type: TYPE_NORMAL
- en: The n-gram approach is very sensitive to the choice of *n*. It also depends
    significantly on the training corpus that has been used, which makes the probabilities
    heavily dependent on the training corpus. So, if an unknown word is encountered,
    it will be difficult for the model to work on that new word.
  prefs: []
  type: TYPE_NORMAL
- en: Next we create a Python example. We will show a few examples of text cleaning
    using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 7.12 Text cleaning using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few libraries you may need to install. We will show a few small
    code snippets. You are advised to use them as per the examples. We are also including
    the respective screenshots of the code snippets and their results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 1: Remove the blank spaces in the text. Import the library `re`; it is
    called the Regular Expression (`Regex`) expression. The text is “It is raining
    outside” with a lot of blank spaces in between (see figure 7.8):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 Removing the blank spaces
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Code 2: Now we will remove the punctuation in the text data (see figure 7.9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 Removing the punctuation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Code 3: Here is one more method to remove the punctuation (see figure 7.10):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 An alternative way to remove punctuation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Code 4: We will now remove the punctuation as well as convert the text to lowercase
    (see figure 7.11):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 Converting the text to lowercase
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Code 5: Tokenization is done here using the standard `nltk` library (see figure
    7.12):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 Tokenization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that in the output of the code, we have all the words, including the punctuation
    marks, as different tokens. If you wish to exclude the punctuation, you can clean
    the punctuation marks using the code snippets shared earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 6: Next comes the stop words. We will remove the stop words using the
    `nltk` library. After that, we tokenize the words (see figure 7.13):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 Removing stop words and tokenizing words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Code 7: We will now perform stemming on a text example. We use `nltk` library
    for it. The words are first tokenized, and then we apply stemming (see figure
    7.14):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 Tokenizing and then stemming the words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Code 8: We now perform lemmatization on a text example. We use the `nltk` library
    for it. The words are first tokenized, and then we apply lemmatization (see figure
    7.15):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 Tokenizing and then lemmatizing the words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Observe and compare the difference between the two outputs of stemming and lemmatization.
    For “studies” and “studying,” stemming generated the output as “studi” while lemmatization
    generated the correct output as “study.”
  prefs: []
  type: TYPE_NORMAL
- en: We have covered BOW, TF-IDF, and n-gram approaches so far. But in all these
    techniques, the relationship between words has been neglected. This relationship
    is used in word embeddings, our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: 7.13 Word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A word is characterized by the company it keeps.—John Rupert Firth
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So far we have studied several approaches, but all the techniques ignore the
    contextual relationship between words. Let’s take a closer look using an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we have 100,000 words in our vocabulary, starting from “aa” (the basaltic
    lava) to “zoom.” Now, if we perform one-hot encoding, all these words can be represented
    in a vector form. Each word will have a unique vector. For example, if the position
    of the word “king” is 21000, the vector will have a shape like the following vector,
    which has 1 at the 21,000th position and the rest of the values as 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few glaring problems with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of dimensions is very high, and it is complex to compute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is very sparse in nature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *n* new words have to be entered, the vocabulary increases by *n*, and hence
    each vector dimensionality increases by *n*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach ignores the relationship between words. We know that “ruler,”
    “king,” and “monarch” are sometimes used interchangeably. In the one-hot-encoding
    approach, any such relationships are ignored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we wish to perform language translation, or generate a chat-bot, we need
    to pass such knowledge to the machine learning solution. Word embeddings provide
    a solution to the problem. They convert the high-dimensional word features into
    lower dimensions while maintaining the contextual relationship. Word embeddings
    allow us to create much more generalized models. We can understand the meaning
    by looking at an example.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  In an LLM-enabled solution, you might not need to do a lot of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: In the example shown in figure 7.16, the relation of “man” to “woman” is similar
    to “king” to “queen”; “good” to “nice” is similar to “bad” to “awful”; or the
    relationship of “UK” to “London” is similar to “Japan” to “Tokyo.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F16_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 Word embeddings can be used to represent the relationships between
    words. For example, there is a relation from “men” to “women” that is similar
    to “king” to “queen” as both “men-women” and “king-queen” represent the male-female
    gender relationship.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In simple terms, using word embeddings, we can represent the words that have
    similar meanings. Word embeddings can be thought of as a class of techniques where
    we represent each of the individual words in a predefined vector space. Each of
    the words in the corpus is mapped to one vector. The distributed representation
    is understood based on the word’s usage. Hence, words that can be used similarly
    have similar representations. This allows the solution to capture the underlying
    meaning of the words and their relationships. Hence, the meaning of the word plays
    a significant role. This representation is more intelligent as compared to the
    BOW approach where each word is treated differently, irrespective of its usage.
    Also, the number of dimensions is fewer as compared to one-hot encoding. Each
    word is represented by 10s or 100s of dimensions, which is significantly less
    than the one-hot encoding approach where 1000s of dimensions are used for representation.
  prefs: []
  type: TYPE_NORMAL
- en: We cover the two most popular techniques—Word2Vec and global vectors for word
    representation (GloVe)—in the next section. The mathematical foundation for Word2Vec
    and GloVe are beyond the scope of this book. We provide an understanding of the
    working mechanism of the solutions and then develop Python code using Word2Vec
    and GloVe. This section is more technically involved, so if you are interested
    only in the application of the solutions, you can skip the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 7.14 Word2Vec and GloVe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word2Vec was first published in 2013\. It was developed by Tomas Mikolov and
    others at Google. We share the link to the paper at the end of the chapter. You
    are advised to study the paper thoroughly if you wish to learn about the more
    technical elements in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec is a group of models used to produce word embeddings. The input is
    a large corpus of text. The output is a vector space with a very large number
    of dimensions. In this output, each of the words in the corpus is assigned a unique
    and corresponding vector. The most important point is that the words that have
    a similar or common context in the corpus are located nearby in the vector space
    produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Word2Vec, the researchers introduced two different learning models—the continuous
    bag of words (CBOW) and the continuous skip-gram model:'
  prefs: []
  type: TYPE_NORMAL
- en: In CBOW, the model makes a prediction of the current word from a window of surrounding
    context words. So the CBOW model predicts a target word based on the context of
    the surrounding words in the text. Recall that in the BOW approach, the order
    of the words does not play any part. In contrast, in CBOW, the order of the words
    is significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The continuous skip-gram model uses the current word to predict the surrounding
    window of context words. While doing so, it allocates more weight to the neighboring
    words as compared to the distant words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GloVe is an unsupervised learning algorithm for generating vector representation
    for words. It was developed by Pennington and others at Stanford and launched
    in 2014\. It is a combination of two techniques: matrix factorization techniques
    and local context-based learning used in Word2Vec. GloVe can be used to find relationships
    like zip codes and cities, synonyms, etc. It generates a single set of vectors
    for words with the same morphological structure.'
  prefs: []
  type: TYPE_NORMAL
- en: Both Word2Vec and GloVe learn and understand vector representation of their
    words from the co-occurrence information. Co-occurrence means how frequently the
    words appear together in a large corpus. The prime difference is that Word2Vec
    is a prediction-based model, while GloVe is a frequency-based model. Word2Vec
    predicts the context given a word while GloVe learns the context by creating a
    co-occurrence matrix on how frequently a word appears in a given context.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: BOW is more rigorous than the TF-IDF approach. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Differentiate between Word2Vec and GloVe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now move to the case study and Python implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.15 Sentiment analysis case study with Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have discussed a lot of concepts on NLP and text data. In this section,
    we first explore a business case and then develop a Python solution based on it.
    Here we are working on sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Product reviews are a rich source of information—both for customers and organizations.
    Whenever we wish to buy any new product or service, we tend to look at the reviews
    by fellow customers. You might have reviewed products and services yourself. These
    reviews are available at Amazon and on blogs, surveys, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a case. A telecom operator receives complaints from its customers,
    reviews about the service, and comments about the overall experience. The streams
    can be product quality, pricing, onboarding experience, ease of registration,
    payment process, general reviews, customer service, etc. We want to determine
    the general context of the review—whether it is positive, negative, or neutral.
    The reviews include the number of stars allocated, actual text reviews, pros and
    cons about the product/service, attributes, etc. However, there are a few business
    problems—for instance,
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the number of stars received by a product/service is very high, while
    the actual reviews are quite negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The organizations and the product owners need to know which features are appreciated
    by the customers and which features are disliked by the customers. The team can
    then work on improving the features that are disliked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a need to gauge and keep an eye on the competition! The organizations
    need to know the attributes of the popular products of their competitors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The product owners want to better plan for the upcoming features they wish to
    release in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So the business teams will be able to answer these important questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are our customers’ satisfaction levels for the products and services?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the major pain points and dissatisfactions of the customers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What drives the customers’ engagement?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which services are complex and time-consuming, and which are the most liked
    services/products?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This business use case will drive the following business benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: The products and services that are most satisfactory and are the most liked
    should be continued.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ones that are not liked and are receiving a negative score should be improved
    and challenges mitigated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The respective teams, like finance, operations, complaints, CRM, etc., can be
    notified, and they can work individually to improve the customer experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The precise reasons for liking or disliking the services will be useful for
    the respective teams to work in the right direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, it will provide a benchmark to measure the Net Promoter Score for the
    customer base. The business can strive to enhance the overall customer experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We might want to represent these findings by means of a dashboard. This dashboard
    will be refreshed on a regular cycle, like monthly or quarterly.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this business problem, the teams can collect relevant data from websites,
    surveys, Amazon, blogs, etc. Then an analysis can be done on that dataset. It
    is relatively easy to analyze the structured data. In this example, we work on
    text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python Jupyter notebook is pushed to the GitHub location. You are advised
    to use the Jupyter notebook from the GitHub location as it contains more steps.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Define the tags. These tags are used to get the attributes of the product
    from the reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Make everything ready to extract the data. We create a dataframe to store
    the customer reviews. Then we iterate through all the reviews and extract the
    information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Iterate through the reviews and then fill in the details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Have a look at the output we generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Save the output to a path. You can give your own path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '7\. Load the data and analyze it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '8\. Look at the basic information about the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '9\. Look at the distribution of the stars given in the reviews. This will allow
    us to understand the reviews given by the customers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '10\. Make the text lowercase, and then remove the stop words and the words
    that have the highest frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '11\. Tokenize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '12\. Perform lemmatization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '13\. Append all the reviews to the string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '14\. Do the sentiment analysis. From `textblob`, we take the sentiment method.
    It generates polarity and subjectivity for a sentiment. Sentiment polarity for
    an element is the orientation of the sentiment in the expression; that is, it
    tells us if the text expresses a negative, positive, or neutral sentiment in the
    text. It subjectively measures and quantifies the amount of opinion and factual
    information in the text. If the subjectivity is high, it means that the text contains
    more opinion than facts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '15\. Save the sentiment to a .csv file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '16\. Allocate a meaning or a tag to the sentiment. We classify each of the
    scores from extremely satisfied to extremely dissatisfied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '17\. Look at the sentiment scores and plot them too. Finally, we merge them
    with the main dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Adds column polarityScore'
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, you not only scraped the reviews from the website but you
    also analyzed the dataset. If we compare the sentiments, we can see that the stars
    given to a product do not represent a true picture.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.17 compares the actual stars and the output from sentiment analysis.
    We can observe that 73% of customers have given five stars and 7% have given four
    stars, while in the sentiment analysis most of the reviews have been classified
    as neutral. This is the real power of sentiment analysis!
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH07_F17_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 Compare the original distribution of number of stars on the left
    side and the real results from the sentiment analysis on the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sentiment analysis is quite an important use case. It is very useful for business
    and product teams. The preceding code can be scaled to any such business problem
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: We now move to the second case study on document classification using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 7.16 Text clustering using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider this: we have a bunch of text datasets or documents, but they all
    are mixed up. We do not know which text belongs to which class. In this case,
    we will assume that we have two types of text datasets: one that has all the data
    related to football and one that is related to travel. We will develop a model
    that can segregate these two classes. To do that, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Create a dummy dataset. This text data has a few sentences we have written
    ourselves. There are two categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Use TF-IDF to vectorize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Do the clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Represent the centroids and print the outputs (see figure 7.18):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH07_F18_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 Printed output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can extend this example to other datasets too. Get the datasets from the
    internet and replicate the code in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: We have pushed the code to the GitHub location of the book. You are advised
    to use it. It is really an important source to represent text data.
  prefs: []
  type: TYPE_NORMAL
- en: 7.17 GenAI for text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAI solutions are a new kind of unsupervised solution. You surely have heard
    about ChatGPT and LLMs. They have revolutionized the world. GenAI for text data
    uses machine learning models to create human-like text. It is trained on large-scale
    data patterns and hence can generate a variety of content pieces—for example,
    essays, technical reports, and summaries of a book—and can act like a human chat
    interface. Even the complex translation of languages is made easy with GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI for text data involves the use of advanced algorithms, like transformers,
    to generate coherent, contextually appropriate text. These algorithms are trained
    on mammoth datasets. Imagine we feed tons of content present on the internet to
    the algorithms. By learning patterns and relationships between the words and the
    sentences, the grammar used, syntax, and semantics, they can create human-like
    responses. These models, such as OpenAI’s GPT or Google’s BERT, are very powerful
    for drafting emails with correct language and grammar, creating detailed reports,
    writing code modules in a language like Java/C++, and many other tasks. Using
    this power, content creators, writers and copyrighters, brand managers and marketers,
    and business owners can produce high-quality text in a much more scalable and
    efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its amazing potential, GenAI still has some areas in need of improvement.
    Sometimes it generates inaccurate information, also known as hallucinations. Ensuring
    that the output remains unbiased and ethical is another hurdle, as models can
    inadvertently reflect societal biases present in the data they were trained on.
    AI-generated text is increasingly being used in customer service, automating responses
    while still maintaining a personal tone. Researchers are also exploring its use
    in the healthcare and legal fields, where it can help with documentation and drafting.
    While GenAI is revolutionizing the way text is produced, the need for human oversight
    remains critical to ensure quality, accuracy, and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: 7.18 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Text data is one of the most useful datasets. A lot of intelligence is hidden
    in the texts: logs, blogs, reviews, posts, tweets, complaints, comments, articles,
    and so on—the sources of text data are many. Organizations are investing in setting
    up the infrastructure for accessing text data and storing it. Analyzing text data
    requires better processing powers and better machines than our standard laptops.
    It requires special skill sets and a deeper understanding of the concepts. NLP
    is an evolving field, and a lot of research is underway. At the same time, we
    cannot ignore the importance of sound business acumen and knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and machine learning are not easy. We have to understand a lot
    of concepts around data cleaning, exploration, representation, and modeling. But
    analyzing unstructured data might be even more complex than analyzing structured
    datasets. We worked on an images dataset in the last chapter, and in the current
    chapter, we worked on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Text data is one of the most difficult datasets to analyze. There are so many
    permutations and combinations for text data. Cleaning the text data is a difficult
    and complex task. In this chapter, we discussed a few important techniques to
    clean text data. We also covered some methods to represent text data in vector
    forms. You are advised to practice each of these methods and compare the performances
    by applying each of the techniques. We also introduced the concept of GenAI for
    text data.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we come to the end of chapter 7\. This also marks an end to part
    2\. In the next part, the complexity increases. We will be studying even deeper
    concepts of unsupervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 7.19 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the datasets from the following link. You will find a lot of text datasets
    here. You are advised to implement clustering and dimensionality reduction solutions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '50 Free Machine Learning Datasets: Natural Language Processing: [https://mng.bz/ZljO](https://mng.bz/ZljO)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will find a lot of useful datasets at Kaggle as well: [https://www.kaggle.com/datasets?search=text](https://www.kaggle.com/datasets?search=text)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go through the following research papers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient Estimation
    of Word Representations in Vector Space. [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington, J., Socher, R., and Manning, C. D. (2014). GloVe: Global Vectors
    for Word Representation. [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Das, B., and Chakraborty, S. (2018). An Improved Text Sentiment Classification
    Model Using TF-IDF and Next Word Negation. [https://arxiv.org/pdf/1806.06407.pdf](https://arxiv.org/pdf/1806.06407.pdf)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider these widely quoted papers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum, A., and Mitchell, T. (1998). Combining labeled and unlabeled data with
    co-training. [https://dl.acm.org/doi/10.1145/279943.279962](https://dl.acm.org/doi/10.1145/279943.279962)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Knight, K. (2009). Bayesian Inference with Tears. [https://mng.bz/RVp0](https://mng.bz/RVp0)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hofmann, T. (1999). Probabilistic latent semantic indexing. [https://dl.acm.org/doi/10.1145/312624.312649](https://dl.acm.org/doi/10.1145/312624.312649)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hindle, D., and Rooth, M. (1993). Structural Ambiguity and Lexical Relations.
    [https://aclanthology.org/J93-1005.pdf](https://aclanthology.org/J93-1005.pdf)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Collins and Singer. (1999). Unsupervised Models for Named Entity Classification.
    [https://aclanthology.org/W99-0613.pdf](https://aclanthology.org/W99-0613.pdf)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the comprehensive study on TF-IDF feature weighting: Das, M., Selvakumar,
    K., and Alphonse, J. P. A. (2023). A Comparative Study on TF-IDF Feature Weighting
    Method and its Analysis using Unstructured Dataset. [https://arxiv.org/abs/2308.04037](https://arxiv.org/abs/2308.04037)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data’s omnipresence in blogs, social media, surveys, and more, and its
    capacity to express emotions, emphasizes the importance of this form of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of text analysis include sentiment analysis, document categorization,
    language translation, spam filtering, and named-entity recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in text data include handling junk characters, multiple languages,
    evolving language, synonyms, and context-based meanings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing and cleaning involves removing stop words and unwanted characters
    and normalizing text through stemming and lemmatization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within text representation techniques, one-hot encoding is basic but not scalable;
    advanced techniques consider frequency and context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization involves breaking down text into tokens and is fundamental for
    creating analysis-ready datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BOW approach is a fast, frequency-based method that ignores word order and
    context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF weighs words based on importance over mere frequency, offering more insightful
    analysis than BOW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models and n-grams use word sequences for probabilistic predictions,
    with variations like unigrams, bigrams, and trigrams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python for text parsing illustrates cleaning and preprocessing text data using
    Python libraries like `nltk`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques like Word2Vec and GloVe maintain contextual relationships between
    words for better semantic understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec is prediction based, while GloVe is frequency based; both create compact
    and meaningful word representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs have revolutionized the entire landscape for text datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
