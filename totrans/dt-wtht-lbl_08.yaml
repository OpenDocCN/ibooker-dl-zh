- en: 7 Unsupervised learning for text data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 文本数据的无监督学习
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: 'Text data analysis: use cases and challenges'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据分析：用例和挑战
- en: Preprocessing and cleaning text data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据的预处理和清洗
- en: Vector representation methods for text data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据的向量表示方法
- en: Sentiment analysis and text clustering using Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python进行情感分析和文本聚类
- en: Generative AI applications for text data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据的生成式AI应用
- en: Everybody smiles in the same language.—George Carlin
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有人都用同一种语言微笑。——乔治·卡林
- en: Our world has so many languages. These languages are the most common medium
    of communication to express our thoughts and emotions. These words can be written
    into text. In this chapter, we explore the sorts of analysis we can do on text
    data. Text data falls under unstructured data and carries a lot of useful information
    and hence is a useful source of insights for businesses. We use natural language
    processing (NLP) to analyze the text data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的世界有如此多的语言。这些语言是我们表达思想和情感的最常见沟通媒介。这些词可以写成文本。在本章中，我们探讨可以对文本数据进行哪些分析。文本数据属于非结构化数据，并携带大量有用信息，因此是商业洞察的有用来源。我们使用自然语言处理（NLP）来分析文本数据。
- en: At the same time, to analyze text data, we have to make the data analysis-ready.
    Or, in very simple terms, since our algorithms and processors can only understand
    numbers, we have to represent the text data in numbers or *vectors*. We will explore
    all these steps in this chapter. Text data holds the key to quite a few important
    use cases, such as sentiment analysis, document categorization, and language translation,
    to name a few. We will cover the use cases using a case study and develop a Python
    solution on the same.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，为了分析文本数据，我们必须使数据分析就绪。或者用非常简单的话说，由于我们的算法和处理器只能理解数字，我们必须用数字或*向量*来表示文本数据。我们将在本章中探讨所有这些步骤。文本数据是许多重要用例的关键，例如情感分析、文档分类和语言翻译，仅举几例。我们将通过案例研究来涵盖这些用例，并在同一平台上开发Python解决方案。
- en: The chapter starts with defining text data, sources of text data, and various
    use cases of text data. We will then move on to the steps and processes to clean
    and handle the text data. We cover the concepts of NLP, mathematical foundations,
    and methods to represent text data into vectors. We will create Python codes for
    the use cases. Toward the end, we share a case study on text data. Finally, we
    will also look into the generative AI-based (GenAI) solutions. We have not covered
    GenAI concepts yet in the book, as they are in part 3\. But here we introduce
    the concepts in the light of text data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先定义文本数据、文本数据的来源以及文本数据的各种用例。然后，我们将继续讨论清洗和处理文本数据的步骤和过程。我们将涵盖NLP的概念、数学基础以及将文本数据表示为向量的方法。我们将为用例编写Python代码。在结尾部分，我们将分享一个关于文本数据的案例研究。最后，我们还将探讨基于生成式AI（GenAI）的解决方案。由于它们在第三部分有所涉及，本书中尚未涵盖GenAI的概念。但在这里，我们将根据文本数据介绍这些概念。
- en: Welcome to the seventh chapter, and all the very best!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第七章，祝大家一切顺利！
- en: 7.1 Technical toolkit
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 技术工具包
- en: We will continue to use the same version of Python and Jupyter Notebook as we
    have used so far. The codes and datasets used in this chapter have been checked
    in at the same GitHub location.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止相同的Python和Jupyter Notebook版本。本章中使用的代码和数据集已在相同的GitHub位置进行检查。
- en: 'You need to install the following Python libraries for this chapter: `re`,
    `string`, `nltk`, `lxml`, `requests`, `pandas`, `textblob`, `matplotlib`, `sys`,
    `sklearn`, `scikitlearn`, and `warnings`. Along with these, you will need `numpy`
    and `pandas`. With libraries, we can use the algorithms very quickly.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章，你需要安装以下Python库：`re`、`string`、`nltk`、`lxml`、`requests`、`pandas`、`textblob`、`matplotlib`、`sys`、`sklearn`、`scikitlearn`和`warnings`。除了这些，你还需要`numpy`和`pandas`。有了这些库，我们可以非常快速地使用算法。
- en: 7.2 Text data is everywhere
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 文本数据无处不在
- en: Recall in the very first chapter of the book we explored structured and unstructured
    datasets. Unstructured data can be text, audio, image, or video. Examples of unstructured
    data and their respective sources are given in figure 7.1, where we explain the
    primary types of unstructured data—text, images, audio, and video—along with examples.
    The focus of this chapter is on text data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一章，我们探讨了结构化和非结构化数据集。非结构化数据可以是文本、音频、图像或视频。图7.1中给出了非结构化数据及其相应来源的示例，其中我们解释了非结构化数据的主要类型——文本、图像、音频和视频——以及示例。本章的重点是文本数据。
- en: '![figure](../Images/CH07_F01_Verdhan.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F01_Verdhan.png)'
- en: Figure 7.1 Unstructured data can be text, images, audio, or video. We deal with
    text data in this chapter. This list is not exhaustive.
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 非结构化数据可以是文本、图像、音频或视频。在本章中，我们处理文本数据。这个列表并不全面。
- en: Language is perhaps our greatest tool for communication. When in written form,
    this becomes text data. Today, thanks to widely accessible computers and smartphones,
    text is everywhere. It is generated by writing blogs and social media posts, tweets,
    comments, stories, reviews, chats, and comments, to name a few. Text data is generally
    much more direct than images and can be emotionally expressive. It is useful for
    businesses to unlock the potential of text data and derive insights from it. They
    can understand customers better, explore the business processes, and gauge the
    quality of services offered.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 语言可能是我们沟通的最强大工具。当以书面形式存在时，它变成了文本数据。如今，得益于广泛可用的电脑和智能手机，文本无处不在。它通过撰写博客、社交媒体帖子、推文、评论、故事、评论等方式生成。文本数据通常比图像更直接，并且可以表达情感。对于企业来说，解锁文本数据的潜力并从中获得洞察力是有用的。他们可以更好地理解客户，探索业务流程，并评估所提供服务的质量。
- en: Have you ever reviewed a product or a service on Amazon? You award stars to
    a product; at the same time, you can also input free text. Go to Amazon and look
    at some of the reviews. You might find some reviews have a good amount of text
    as the feedback. This text is useful for the product/service providers to enhance
    their offerings. Also, you might have participated in a few surveys that ask you
    to share your feedback. Moreover, with the advent of Alexa, Siri, Cortona, etc.,
    the voice command acts as an interface between humans and machines—which is again
    a rich source of data. Even the customer calls we make to a call center can be
    transcribed so that they become a source of text data. These calls can be recorded,
    and using speech-to-text conversion, we can generate a huge amount of text data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾在亚马逊上评论过产品或服务？你给产品打星；同时，你也可以输入自由文本。去亚马逊看看一些评论。你可能会发现一些评论有大量的文本作为反馈。这些文本对产品/服务提供商来说很有用，可以帮助他们提升产品或服务。此外，你可能参与过一些要求你分享反馈的调查。此外，随着Alexa、Siri、Cortona等语音助手的出现，语音命令充当了人类与机器之间的接口——这又是一个丰富的数据来源。甚至我们打给客服中心的电话也可以转录，这样它们就变成了文本数据。这些电话可以被录音，并且使用语音转文本转换，我们可以生成大量的文本数据。
- en: 7.3 Use cases of text data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 文本数据的应用案例
- en: 'Not all the use cases discussed in this section can implement unsupervised
    learning. Some require supervised learning too. Nevertheless, for your knowledge,
    we share both types of use cases, based on supervised learning and unsupervised
    learning:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中讨论的并非所有用例都可以实现无监督学习。一些还需要监督学习。然而，为了增加你的知识，我们根据监督学习和无监督学习分享这两种类型的用例：
- en: '*Sentiment analysis*—You might have participated in surveys or given your feedback
    on products/surveys. These surveys generate tons of text data. That text data
    can be analyzed, and we can determine whether the sentiment in the review is positive
    or negative. In simple words, sentiment analysis gauges the positivity or negativity
    of the text data. Hence, we can see the sentiment about a product or service in
    the minds of the customers. We can use both supervised and unsupervised learning
    for sentiment analysis.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情感分析*——你可能参加过调查或对产品/调查提供反馈。这些调查产生了大量的文本数据。这些文本数据可以进行分析，我们可以确定评论中的情感是正面还是负面。简单来说，情感分析衡量文本数据的积极或消极。因此，我们可以看到客户对产品或服务的看法。我们可以使用监督学习和无监督学习进行情感分析。'
- en: '*News categorization or document categorization*—Look at the Google News web
    page and you will find that each news item has been categorized to sports, politics,
    science, business, or another category. Incoming news is classified based on the
    content of the news, which is the actual text. Imagine the thousands of documents
    that are sorted in this manner. In this use case, it is clear that machine learning
    is ideal, given the unfeasible amount of time and effort that would be required
    to sort such items manually. Supervised learning solutions work well for such
    problems.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*新闻分类或文档分类*——查看谷歌新闻网页，你会发现每条新闻都被分类到体育、政治、科学、商业或其他类别。进入的新闻是根据新闻内容进行分类的，即实际的文本。想象一下以这种方式排序的成千上万份文件。在这个用例中，很明显，鉴于手动排序此类项目所需的时间和精力不可行，机器学习是理想的。监督学习解决方案对这类问题效果良好。'
- en: '*Language translation*—Translation of text from one language to another is
    a very interesting use case. Using NLP, we can translate between languages. Language
    translation is very tricky, as different languages have different grammatical
    rules. Generally, deep learning–based solutions are the best fit for language
    translation.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言翻译*—将文本从一种语言翻译成另一种语言是一个非常有趣的用例。使用自然语言处理（NLP），我们可以在不同语言之间进行翻译。语言翻译非常棘手，因为不同的语言有不同的语法规则。通常，基于深度学习的解决方案最适合语言翻译。'
- en: '*Spam filtering*—Email spam filters can be set up using NLP and supervised
    machine learning. A supervised learning algorithm can analyze incoming mail parameters
    and give a prediction if that email belongs to a spam folder or not. The prediction
    can be based on various parameters like sender email ID, subject line, body of
    the mail, attachments, time of mail, etc. Generally, supervised learning algorithms
    are used here.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*垃圾邮件过滤*—可以使用自然语言处理（NLP）和监督机器学习来设置电子邮件垃圾邮件过滤器。一个监督学习算法可以分析传入的邮件参数，并预测该邮件是否属于垃圾邮件文件夹。预测可以基于各种参数，如发件人电子邮件ID、主题行、邮件正文、附件、邮件时间等。通常，在这里使用监督学习算法。'
- en: '*Part-of-speech tagging*—This is one of the popular use cases. It means that
    we can distinguish the nouns, adjectives, verbs, adverbs, etc., in a sentence.
    Named-entity recognition is also one of the famous applications of NLP. It involves
    identifying a person, place, organization, time, or number in a sentence. For
    example, John lives in London and works for Google. Named-entity recognition can
    generate understanding like [John][Person] lives in [London][Location] and works
    for [Google][organization].'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词性标注*—这是其中一个流行的用例。这意味着我们可以在句子中区分名词、形容词、动词、副词等。命名实体识别也是自然语言处理（NLP）的著名应用之一。它涉及在句子中识别人、地点、组织、时间或数字。例如，约翰住在伦敦，在谷歌工作。命名实体识别可以生成类似[John][Person]住在[London][Location]并为[Google][organization]工作的理解。'
- en: '*Sentence generation, captioning the images, speech-to-text or text-to-speech
    tasks, and handwriting recognition*—These are a few other significant and popular
    use cases.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*句子生成、图像标题、语音转文本或文本转语音任务以及手写识别*—这些都是一些其他重要且流行的用例。'
- en: The use cases listed here are not exhaustive. There are tons of other use cases
    that can be implemented using NLP. NLP is a very popular research field too. We
    share some significant papers at the end of the chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出的用例并不全面。还有许多其他可以用自然语言处理（NLP）实现的用例。自然语言处理（NLP）也是一个非常流行的研究领域。我们在本章末尾分享了一些重要的论文。
- en: You might have also heard about large language models (LLMs) like ChatGPT, Bard,
    and Claude. They are algorithms that process natural language inputs and predict
    the next word based on what they have already seen. With GenAI in the picture,
    a lot of the use cases can be solved by simply calling the API. ChatGPT can communicate
    like a human with memory and serves as customer support for many services. LLMs
    can summarize hundreds of pdf documents. You can even create applications that
    can be used for getting answers from multiple documents and websites. Certainly,
    GenAI has enhanced the power here.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也听说过像ChatGPT、Bard和Claude这样的大型语言模型（LLMs）。它们是处理自然语言输入并基于它们已经看到的预测下一个单词的算法。在通用人工智能（GenAI）的背景下，许多用例可以通过简单地调用API来解决。ChatGPT可以像有记忆的人类一样进行交流，并为许多服务提供客户支持。LLMs可以总结数百份PDF文档。你甚至可以创建可以用于从多个文档和网站上获取答案的应用程序。当然，通用人工智能在这里增强了这种能力。
- en: While text data is very important, at the same time it is quite difficult to
    analyze. Remember, our computers and processors understand only numbers. So the
    text needs to be represented as numbers so we can perform mathematical and statistical
    calculations on it. Before diving into the preparation of text data, we cover
    some of the challenges we face while working on text datasets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本数据非常重要，但同时分析起来也相当困难。记住，我们的计算机和处理器只理解数字。因此，文本需要被表示为数字，这样我们才能对其进行数学和统计计算。在深入准备文本数据之前，我们讨论了我们在处理文本数据集时面临的挑战。
- en: 7.4 Challenges with text data
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 文本数据面临的挑战
- en: Text is a difficult data type to work with. There are a large number of permutations
    to express the same thought. For example, I might ask, “Hey buddy, what is your
    age?” or “Hello there, may I know how old are you?”—they mean the same, right?
    The answer to both the questions is the same, and it is quite easy for humans
    to decipher, but it can be a daunting task for a machine.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是一种难以处理的数据类型。表达相同思想有多种排列组合。例如，我可能会问，“嘿，伙计，你多大了？”或者“你好，我可以知道你多大了？”——它们的意思是一样的，对吧？这两个问题的答案是一样的，对人类来说很容易理解，但对机器来说可能是一项艰巨的任务。
- en: 'Some of the most common challenges we face in this area are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，我们面临的一些最常见的挑战如下：
- en: Text data can be complex to handle. There can be a lot of junk characters like
    $^%*& present in the text.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据可能难以处理。文本中可能存在大量的垃圾字符，如$^%*&。
- en: With the advent of modern communications, we have started to use short forms
    of words; for example, “u” can be used for “you,” “brb” for “be right back,” and
    so on. Additionally, the challenge is where the same word might mean something
    different to different people, or misspelling a single letter can change the complete
    meaning of the word.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着现代通信的出现，我们开始使用单词的缩写形式；例如，“u”可以表示“你”，“brb”表示“马上回来”，等等。此外，挑战在于同一个词对不同的人来说可能有不同的含义，或者一个字母的拼写错误可能会改变整个词的意义。
- en: Language is changing, unbounded, and ever-evolving. It changes every day and
    new words are added to the language. If you do a simple Google search, you will
    find that quite a few words are added to the dictionary each year.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言正在变化，没有界限，并且不断进化。它每天都在变化，新的词汇被添加到语言中。如果你进行简单的谷歌搜索，你会发现每年都有很多词汇被添加到词典中。
- en: 'The world has close to 6,500 languages, and each one carries its own unique
    characteristics. Each and every one completes our world. Each language follows
    its own rules and grammar, which are unique in usage and pattern. Even the writing
    can be different: some are written left to right, some right to left, and some
    even vertically. The same emotion might take fewer or more words in different
    languages.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界上有近6500种语言，每一种都承载着其独特的特征。每一种语言都完善了我们的世界。每一种语言都遵循其自身的规则和语法，这些规则和模式在用法上都是独特的。甚至书写方式也可能不同：有的从左到右书写，有的从右到左书写，有的甚至垂直书写。同样的情感在不同的语言中可能需要更少或更多的词汇。
- en: 'The meaning of a word is dependent on the context. A word can be both an adjective
    and a noun, depending on the context. Consider these examples:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个词的意义取决于上下文。一个词可以是形容词也可以是名词，这取决于上下文。考虑以下例子：
- en: “This book is a must-read” and “Please book a room for me.”
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这本书是必读之作”和“请为我预订一个房间。”
- en: “Tommy” can be a name, but when used as “Tommy Hilfiger” its usage is completely
    changed.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Tommy”可以是一个名字，但用作“Tommy Hilfiger”时，其用法完全改变。
- en: “Apple” is both a fruit and a company.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “苹果”既是水果也是公司。
- en: “April” is a month and can be a name too.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “四月”是一个月份，也可以是一个名字。
- en: 'Look at one more example: “Mark traveled from the UK to France and is working
    with John over there. He misses his friends.” Humans can easily understand that
    “he” in the second sentence is Mark and not John, which might not be that simple
    for a machine.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看一个更例子：“马克从英国旅行到法国，并在那里与约翰一起工作。他想念他的朋友们。”人类可以轻易地理解第二个句子中的“他”指的是马克而不是约翰，这可能对机器来说并不那么简单。
- en: There can be many synonyms for the same word, like “good” can be replaced by
    “positive,” “wonderful,” “superb,” or “exceptional” in different scenarios. Words
    like “studying,” “studious,” and “studies” are related to the same root word “study.”
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于同一个词，可能有多个同义词，例如，“好”可以用“积极”、“精彩”、“出色”或“卓越”在不同的场景中替换。像“学习”、“勤奋”和“学习”这样的词与同一个词根“学习”相关。
- en: The size of text data can be daunting too. Managing a text dataset, storing
    it, cleaning it, and refreshing it is a herculean task.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据的大小也可能令人畏惧。管理文本数据集，存储它，清理它，并更新它是一项艰巨的任务。
- en: Like any other machine learning project, text analytics follows the principles
    of machine learning, albeit the precise process is slightly different. Recall
    in chapter 1 we examined the process of a machine learning project, as shown in
    figure 7.2\. You are advised to refresh your memory on the process from chapter
    1 if needed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他机器学习项目一样，文本分析遵循机器学习的原则，尽管精确的过程略有不同。回想一下第一章我们考察了机器学习项目的过程，如图7.2所示。如果你需要，建议你回顾一下第一章中的过程。
- en: '![figure](../Images/CH07_F02_Verdhan.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F02_Verdhan.png)'
- en: Figure 7.2 The overall steps in a data science project are the same for text
    data. The preprocessing of text data is very different from the structured dataset.
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 数据科学项目中整体步骤对于文本数据是相同的。文本数据的预处理与结构化数据集非常不同。
- en: Defining the business problem, data collection and monitoring, etc., remain
    the same. The major difference is in the processing of the text, which involves
    data cleaning, creation of features, representation of text data, etc. We will
    cover this in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 定义业务问题、数据收集和监控等保持不变。主要区别在于文本的处理，这涉及到数据清理、特征创建、文本数据表示等。我们将在下一节中介绍。
- en: Exercise 7.1
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习7.1
- en: 'Answer these questions to check your understanding:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Note the three most effective use cases for the text data.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意文本数据最有效的三个用例。
- en: Why is working on text data so tedious?
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么处理文本数据如此繁琐？
- en: 7.5 Preprocessing the text data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 预处理文本数据
- en: Text data, like any other data source, can be messy and noisy. We clean some
    of it in the data discovery phase and a lot of it in the preprocessing phase.
    At the same time, we should extract the features from our dataset. Some of the
    steps in the cleaning process are common and can be implemented on most text datasets.
    Some text datasets might require a customized approach. We start with cleaning
    the raw text data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据，就像任何其他数据源一样，可能会很混乱和嘈杂。我们在数据发现阶段清理一些，在预处理阶段清理很多。同时，我们应该从我们的数据集中提取特征。清理过程中的某些步骤是常见的，可以在大多数文本数据集中实现。某些文本数据集可能需要定制的方法。我们首先从清理原始文本数据开始。
- en: 7.6 Data cleaning
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 数据清理
- en: As with any form of data analysis, ensuring good data quality is vital. The
    cleaner the text data, the better the analysis. At the same time, preprocessing
    is not a straightforward task but rather is complex and time-consuming.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何形式的数据分析一样，确保良好的数据质量至关重要。文本数据越干净，分析就越好。同时，预处理不是一个简单的任务，而是一个复杂且耗时的任务。
- en: Text data must be cleaned as it contains a lot of junk characters, irrelevant
    words, noise and punctuation, URLs, etc. The primary ways of cleaning the text
    data are
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据必须被清理，因为它包含大量的垃圾字符、无关的词、噪声和标点符号、URL等。清理文本数据的主要方式包括
- en: '*Stopping word removal*—Out of all the words that are used in any language,
    there are some words that are most common. Stop words are the most common words
    in a vocabulary that carry less importance than key words. Examples are “is,”
    “an,” “the,” “a,” “be,” “has,” “had,” “it,” etc. Once we remove the stop words
    from the text, the dimensions of the data are reduced and hence the complexity
    of the solution is reduced.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*停用词去除*——在所有任何语言中使用的词中，有一些是最常见的。停用词是在词汇中最常见的词，它们的重要性低于关键词。例如，“是”、“一个”、“这”、“一个”、“是”、“有”、“曾经”、“它”等。一旦我们从文本中去除停用词，数据的维度就减少了，因此解决方案的复杂性也减少了。'
- en: We can define a customized list of stop words and remove them that way, or there
    are standard libraries to remove the stop words.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个定制的停用词列表并以此方式去除它们，或者有标准库可以去除停用词。
- en: At the same time, it is imperative that we understand the context very well
    while removing the stop words. For example, if we ask a question “is it raining?”
    then the answer “it is” is a complete answer in itself. When we are working with
    solutions where contextual information is important, we do not remove stop words.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在去除停用词时，我们必须非常了解上下文。例如，如果我们问一个问题“是否在下雨？”那么答案“是的”本身就是完整的答案。当我们处理上下文信息重要的解决方案时，我们不去除停用词。
- en: '*Frequency-based removal of words*—Sometimes you might wish to remove the words
    that are most common in your text or that are very unique. The process is to get
    the frequency of the words in the text and then set a threshold of frequency.
    We can remove the most common ones. Or maybe you wish to remove the ones that
    have occurred only once/twice in the entire dataset. Based on the requirements,
    you will decide. At the same time, we should be cautious and observe due diligence
    while removing the words.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于频率的词去除*——有时你可能希望去除你文本中最常见的词或者非常独特的词。这个过程是获取文本中词的频率，然后设置一个频率阈值。我们可以去除最常见的词。或者你可能希望去除在整个数据集中只出现一次/两次的词。根据需求，你将做出决定。同时，我们在去除词的时候应该谨慎并尽职。'
- en: '*Library-based cleaning*—This is done when we wish to clean the data using
    a predefined and customized library. We can create a repository of words that
    we do not want in our text and iteratively remove them from the text data. This
    approach allows us flexibility to implement the cleaning of our own choice.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于库的清理*——当我们希望使用预定义和定制的库来清理数据时，会进行此操作。我们可以创建一个包含我们不希望出现在文本中的单词的仓库，并迭代地从文本数据中移除它们。这种方法使我们能够灵活地实施我们自己的选择来清理。'
- en: '*Junk or unwanted characters*—Text data, particularly tweets, comments, etc.,
    might contain a lot of URLs, hashtags, numbers, punctuations, social media mentions,
    special characters, etc. We might need to clean them from the text. At the same
    time, we should be careful as some words that are not important for one domain
    might be required for a different domain. If data has been scraped from websites
    or HTML/XML sources, we need to get rid of all the HTML entities, punctuations,
    nonalphabet characters, and so on.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*垃圾或不想要的字符*——文本数据，尤其是推文、评论等，可能包含大量的URL、标签、数字、标点符号、社交媒体提及、特殊字符等。我们可能需要从文本中清理它们。同时，我们应该小心，因为在一个领域不重要的单词可能在另一个领域是必需的。如果数据是从网站或HTML/XML源抓取的，我们需要去除所有的HTML实体、标点符号、非字母字符等。'
- en: TIP  Always keep business context in mind while cleaning the text data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TIP  在清理文本数据时，始终牢记业务背景。
- en: As we know, a lot of new types of expressions have entered the language—for
    example, lol, hahahaha, brb, rofl, etc. These expressions are to be converted
    to their original meanings. Even emojis like :-), ;-), etc., should be converted
    to their original meanings.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，许多新的表达方式已经进入语言中——例如，lol，hahahaha，brb，rofl等。这些表达应该转换为它们的原始含义。甚至像:-) ，;-)
    这样的表情符号也应该转换为它们的原始含义。
- en: '*Data encoding*—There are a few data encodings available like ISO/IEC, UTF-8,
    etc. Generally, UTF-8 is the most popular one. But it is not a hard and fast rule
    to always use UTF-8 only.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据编码*——有一些数据编码可供选择，如ISO/IEC，UTF-8等。通常，UTF-8是最受欢迎的。但这并不是一个硬性规定，总是只能使用UTF-8。'
- en: '*Lexicon normalization*—Depending on the context and usage, the same word might
    get represented in different ways. During lexicon normalization, we clean such
    ambiguities. The basic idea is to reduce the word to its root form. Hence, words
    that are derived from each other can be mapped to the central word, provided they
    have the same core meaning.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词典规范化*——根据上下文和用法，同一个词可能会有不同的表示方式。在词典规范化过程中，我们清理这些歧义。基本思想是将单词还原到其词根形式。因此，来自彼此的单词可以映射到中心词，前提是它们有相同的核心意义。'
- en: Figure 7.3 shows that the same word, “eat,” has been used in various forms.
    The root word is “eat,” but these different forms demonstrate the many different
    representations for “eat.”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3显示，相同的单词“eat”被用于各种形式。词根是“eat”，但这些不同的形式展示了“eat”的许多不同表示形式。
- en: '![figure](../Images/CH07_F03_Verdhan.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F03_Verdhan.png)'
- en: 'Figure 7.3 “Ate,” “eaten,” “eats,” and “eating” all have the same root word:
    “eat.” Stemming and lemmatization can be used to get the root word.'
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 “Ate”，“eaten”，“eats”和“eating”都有相同的词根：“eat”。词干提取和词形还原都可以用来获取词根。
- en: 'Here, we wish to map all these words like “eating,” “eaten,” etc., to their
    central word, “eat,” as they have the same core meaning. There are two primary
    methods to work on this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们希望将这些像“eating”，“eaten”等单词映射到它们的中心词“eat”，因为它们有相同的核心意义。处理这个问题有两种主要方法：
- en: Stemming is a basic rule-based approach for mapping a word to its core word.
    It removes “es,” “ing,” “ly,” “ed,” etc., from the end of the word. For example,
    studies will become “studi” and “studying” will become “study.” Being a rule-based
    approach, the output spellings might not always be accurate.
  id: totrans-74
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取是一种基本的基于规则的将单词映射到其核心词的方法。它从单词的末尾移除“es”，“ing”，“ly”，“ed”等。例如，“studies”将变成“studi”，“studying”将变成“study”。作为一个基于规则的方法，输出拼写可能并不总是准确的。
- en: Lemmatization is an organized approach that reduces words to their dictionary
    form. The *lemma* of a word is its dictionary or canonical form. For example,
    “eats,” “eating,” “eaten,” etc., all have the same root word “eat.” Lemmatization
    provides better results than stemming, but it takes more time.
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原是一种有组织的步骤，它将单词还原到它们的词典形式。一个单词的*词元*是其词典或规范形式。例如，“eats”，“eating”，“eaten”等，都有相同的词根“eat”。词形还原比词干提取提供更好的结果，但它需要更多的时间。
- en: These are only some of the methods to clean text data. These techniques will
    help, but business acumen is required to further make sense of the dataset. We
    will clean the text data using these approaches by developing a Python solution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是清理文本数据的一些方法。这些技术会有所帮助，但还需要商业洞察力来进一步理解数据集。我们将通过开发Python解决方案来使用这些方法清理文本数据。
- en: Once the data is cleaned, we start with the representation of data so that it
    can be processed by machine learning algorithms, which is our next topic.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理完毕后，我们开始处理数据的表示，以便它可以被机器学习算法处理，这是我们下一个话题。
- en: 7.7 Extracting features from the text dataset
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 从文本数据集中提取特征
- en: We have explored the concepts and techniques to clean up messy text data. Now
    we have cleaned the data, and it is ready to be used. The next step is to represent
    this data in a format that can be understood by our algorithms. As we know, our
    algorithms can only understand numbers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了清理杂乱文本数据的概念和技术。现在我们已经清理了数据，它已经准备好被使用。下一步是将这些数据以算法可以理解的形式表示出来。正如我们所知，我们的算法只能理解数字。
- en: A very simple technique to encode text data in a way that it can be useful for
    machine learning can be to simply perform one-hot encoding on our words and represent
    them in a matrix—but certainly not a scalable one if you have a complete document.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常简单的将文本数据编码成对机器学习有用的方式是简单地对我们单词进行独热编码，并以矩阵的形式表示它们——但如果你有一个完整的文档，这肯定不是一个可扩展的方法。
- en: NOTE  One-hot encoding is covered in the appendix.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：独热编码在附录中有介绍。
- en: The words can be first converted to lowercase and then sorted in alphabetical
    order. Then a numeric label can be assigned to them. Finally, words are converted
    to binary vectors. Let us understand using an example.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将单词转换为小写，然后按字母顺序排序。然后为它们分配一个数字标签。最后，将单词转换为二进制向量。让我们用一个例子来理解。
- en: 'If the text is “It is raining heavily,” we will use these steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文本是“它正在下大雨”，我们将使用以下步骤：
- en: Lowercase the words so the output will be “it is raining heavily.”
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词转换为小写，以便输出结果为“it is raining heavily。”
- en: Arrange them in alphabetical order. The result is heavily, is, it, raining.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按字母顺序排列它们。结果是 heavily, is, it, raining。
- en: Assign place values to each word as heavily:0, is:1, it:2, raining:3\.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个单词分配一个位置值，例如 heavily:0, is:1, it:2, raining:3。
- en: 'Transform them into binary vectors as shown here:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们转换为如这里所示的二进制向量：
- en: '[0\. 0\. 1\. 0.] #it'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 1\. 0.] #it'
- en: '[0\. 1\. 0\. 0.] #is'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 1\. 0\. 0.] #is'
- en: '[0\. 0\. 0\. 1.] #raining'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 0\. 1.] #raining'
- en: '[1\. 0\. 0\. 0.]] #heavily'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. 0\. 0\. 0.]] #heavily'
- en: As we can see, we are able to represent each of the words in binary vectors,
    where 0 or 1 is the representation for each of the words. Though this approach
    is quite intuitive and simple to comprehend, it is pragmatically not possible
    when we have a massive corpus and vocabulary.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们能够用二进制向量表示每个单词，其中0或1是每个单词的表示。尽管这种方法非常直观且易于理解，但在我们有一个庞大的语料库和词汇表时，在实践上是不可能的。
- en: NOTE  Corpus refers to a collection of texts. It is Latin for “body.” It can
    be a body of written words or spoken words, which can be used to perform a linguistic
    analysis.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：语料库指的是一组文本。它在拉丁语中意为“身体”。它可以是一组书面文字或口头文字，可以用来进行语言分析。
- en: Moreover, handling massive data sizes with so many dimensions will be computationally
    very expensive. The resulting matrix thus created will be very sparse too. Hence,
    we should consider other means and ways to represent our text data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，处理具有如此多维度的海量数据将非常昂贵。因此创建的矩阵也将非常稀疏。因此，我们应该考虑其他方法和手段来表示我们的文本数据。
- en: There are better alternatives than one-hot encoding. These techniques focus
    on the frequency of the word or the context in which the word is being used. This
    scientific method of text representation is much more accurate, robust, and explanatory.
    There are multiple such techniques like term frequency-inverse document frequency
    (TF-IDF), the bag of words approach, etc. We discuss a few of these techniques
    later in the chapter. First, we need to examine the important concept of tokenization.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与独热编码相比，有更好的替代方案。这些技术关注单词的频率或单词被使用的上下文。这种文本表示的科学方法更加准确、健壮和具有解释性。有多个这样的技术，如词频-逆文档频率（TF-IDF）、词袋方法等。我们将在本章后面讨论这些技术中的几个。首先，我们需要检查分词这个重要概念。
- en: 7.8 Tokenization
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 分词
- en: Tokenization is simply breaking a text or a set of text into individual tokens.
    It is the building block of NLP. Look at the example in figure 7.4, where we have
    created individual tokens for each word of the sentence. Tokenization is an important
    step as it allows us to assign unique identifiers or tokens to each of the words.
    Once we have allocated each word a specific token, the analysis becomes less complex.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分词简单来说就是将文本或一组文本分解成单个标记。它是自然语言处理的基石。看看图7.4中的例子，我们为句子中的每个单词创建了单个标记。分词是一个重要的步骤，因为它允许我们为每个单词分配唯一的标识符或标记。一旦我们为每个单词分配了特定的标记，分析就会变得不那么复杂。
- en: '![figure](../Images/CH07_F04_Verdhan.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F04_Verdhan.png)'
- en: Figure 7.4 Tokenization can be used to break a sentence into different tokens
    of words.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 分词可以将句子分解成不同的单词标记。
- en: Tokens are usually used on individual words, but this is not always necessary.
    We are allowed to tokenize a word or the subwords or characters in a word. In
    the case of subwords, the same sentence can have subword tokens as rain-ing (i.e.,
    rain and ing as separate subtokens).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 标记通常用于单个单词，但这并不总是必要的。我们允许对单词或单词中的子词或字符进行分词。在子词的情况下，同一个句子可以有子词标记，如“rain-ing”（即“rain”和“ing”作为单独的子标记）。
- en: If we wish to perform tokenization at a character level, it can be r-a-i-n-i-n-g.
    In fact, in the first step of the one-hot encoding approach discussed in the last
    section, tokenization was done on the words. Tokenization at a character level
    might not always be used.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望在字符级别进行分词，那可能就是“r-a-i-n-i-n-g”。事实上，在上一个章节讨论的一热编码方法的第一步中，对单词进行了分词。在字符级别进行分词不一定总是需要。
- en: NOTE  Tokenization is the building block for NPL solutions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：分词是自然语言处理解决方案的基石。
- en: Once we have obtained the tokens, the tokens can be used to prepare a vocabulary.
    A vocabulary is the set of unique tokens in the corpus.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了标记，就可以使用这些标记来准备词汇表。词汇表是语料库中所有唯一标记的集合。
- en: There are multiple libraries for tokenization. *Regexp* tokenization uses the
    given pattern arguments to match the tokens or separators between the tokens.
    *Whitespace* tokenization treats any sequence of whitespace characters as a separator.
    Then we have *blankline,* which uses a sequence of blank lines as a separator.
    Finally, *wordpunct* tokenizes by matching a sequence of alphabetic characters
    and a sequence of nonalphabetic and nonwhitespace characters. We will perform
    tokenization when we create Python solutions for our text data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个库用于分词。*正则表达式*分词使用给定的模式参数来匹配标记或标记之间的分隔符。*空白符*分词将任何空白字符序列视为分隔符。然后我们有*空白行*，它使用空白行序列作为分隔符。最后，*单词和标点符号*通过匹配一系列字母字符和一系列非字母和非空白字符进行分词。当我们在文本数据中创建Python解决方案时，我们将执行分词操作。
- en: Next, we will explore more methods to represent text data. The first such method
    is the bag of words (BOW) approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨更多表示文本数据的方法。第一种方法是词袋（BOW）方法。
- en: 7.9 BOW approach
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.9 词袋方法
- en: As the name suggests, all the words in the corpus are used. In the BOW approach,
    the text data is tokenized for each word in the corpus, and then the respective
    frequency of each token is calculated. During this process, we disregard the grammar,
    the order, and the context of the word. We simply focus on the simplicity. Hence,
    we will represent each text (sentence or document) as a *bag of its own words*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，语料库中的所有单词都被使用。在词袋方法中，对语料库中的每个单词进行分词，然后计算每个标记的相应频率。在这个过程中，我们忽略了单词的语法、顺序和上下文。我们只关注简单性。因此，我们将每个文本（句子或文档）表示为其“自己的单词袋”。
- en: In the BOW approach for the entire document, we define the vocabulary of the
    corpus as all the unique words present in the corpus. Please note we use all the
    unique words in the corpus. If we want, we can also set a threshold (i.e., the
    upper and lower limit for the frequency of the words to be selected). Once we
    have the unique words, each of the sentences can be represented by a vector of
    the same dimension as the base vocabulary vector. This vector representation contains
    the frequency of each word of the sentence in the vocabulary. It might sound complicated,
    but it is actually a straightforward approach.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个文档的词袋方法中，我们将语料库的词汇定义为语料库中出现的所有唯一单词。请注意，我们使用语料库中的所有唯一单词。如果我们想，我们也可以设置一个阈值（即所选单词频率的上限和下限）。一旦我们有了唯一单词，每个句子都可以用一个与基本词汇向量相同维度的向量来表示。这种向量表示包含了句子中词汇中每个单词的频率。这可能听起来很复杂，但实际上这是一个简单直接的方法。
- en: 'Let us understand this approach with an example. Let’s say that we have two
    sentences: “It is raining heavily” and “We should eat fruits.” To represent these
    two sentences, we calculate the frequency of each of the words in these sentences,
    as shown in figure 7.5.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来理解这种方法。假设我们有两个句子：“It is raining heavily”和“We should eat fruits.”为了表示这两个句子，我们计算这些句子中每个单词的频率，如图7.5所示。
- en: '![figure](../Images/CH07_F05_Verdhan.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F05_Verdhan.png)'
- en: Figure 7.5 The frequency of each word has been calculated. In this example,
    we have two sentences.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 已计算每个单词的频率。在这个例子中，我们有两个句子。
- en: Now if we assume that the words in these two sentences represent the entire
    vocabulary, we can represent the first sentence as shown in figure 7.6\. Note
    that the table contains all the words, but the words that are not present in the
    sentence have received a value of 0.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们假设这两个句子中的单词代表整个词汇表，我们可以将第一个句子表示如图7.6所示。请注意，表中包含所有单词，但句子中不存在的单词已收到0的值。
- en: '![figure](../Images/CH07_F06_Verdhan.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F06_Verdhan.png)'
- en: Figure 7.6 We are assuming that in the vocabulary only two sentences are present
    and the first sentence will be represented as shown.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 我们假设词汇表中只有两个句子，第一个句子将按如下所示表示。
- en: In this example, we examined how the BOW approach has been used to represent
    a sentence as a vector. But the BOW approach has not considered the order of the
    words or the context. It focuses only on the frequency of the word. Hence, it
    is a very fast approach to represent the data and is computationally less expensive
    compared to its peers. Since it is frequency based, it is commonly used for document
    classifications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们考察了如何使用BOW方法将句子表示为一个向量。但是，BOW方法并没有考虑单词的顺序或上下文。它只关注单词的频率。因此，它是一种非常快速的数据表示方法，与同类方法相比，计算成本更低。由于它是基于频率的，它通常用于文档分类。
- en: But, due to its pure frequency-based calculation and representation, solution
    accuracy using the BOW approach can take a hit. In language, the context of the
    word plays a significant role. As we have seen earlier, apple is both a fruit
    as well as a well-known brand and organization. That is why we have other advanced
    methods that consider more parameters than frequency alone. One such method is
    TF-IDF, which we will study next.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于其纯粹基于频率的计算和表示，使用BOW方法的解决方案准确性可能会受到影响。在语言中，单词的上下文起着重要作用。正如我们之前看到的，苹果既是水果也是一个著名的品牌和组织。这就是为什么我们有其他更先进的方法，这些方法考虑的参数比频率更多。其中一种方法就是TF-IDF，我们将在下一节学习。
- en: Exercise 7.2
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 7.2
- en: 'Answer these questions to check your understanding:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Explain tokenization in simple language as if you are explaining it to a person
    who does not know NLP.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用简单的话解释分词，就像你向一个不知道NLP的人解释一样。
- en: The bag of words approach uses the context of the words and not frequency alone.
    True or False?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词袋模型方法使用单词的上下文，而不仅仅是频率。对还是错？
- en: Lemmatization is a less rigorous approach than stemming. True or False?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词形还原比词干提取方法不那么严格。对还是错？
- en: 7.10 Term frequency and inverse document frequency
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.10 术语频率和逆文档频率
- en: 'In the BOW approach, we give importance to the frequency of a word only. But
    the words that have a higher frequency might not always offer meaningful information
    as compared to words that are rare but carry more importance. For example, say
    we have a collection of medical documents, and we wish to compare two words: “disease”
    and “diabetes.” Since the corpus consists of medical documents, the word “disease”
    is bound to be more frequent, while the word “diabetes” will be less frequent
    but more important to identify the documents that deal with diabetes. The term
    frequency and inverse document frequency (TF-IDF) approach allows us to resolve
    this problem and extract information on the more important words.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在BOW方法中，我们只重视单词的频率。但是，频率较高的单词并不总是比罕见但更重要单词提供的信息更有意义。例如，假设我们有一组医学文档，我们希望比较两个单词：“疾病”和“糖尿病”。由于语料库由医学文档组成，单词“疾病”肯定更频繁，而单词“糖尿病”将较少出现但更重要，以识别处理糖尿病的文档。术语频率和逆文档频率（TF-IDF）方法使我们能够解决这个问题，并从更重要的单词中提取信息。
- en: 'In TF-IDF, we consider the relative importance of the word. TF means term frequency,
    and IDF means inverse document frequency. We can define TF-IDF in this way:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF-IDF中，我们考虑单词的相对重要性。TF代表术语频率，IDF代表逆文档频率。我们可以这样定义TF-IDF：
- en: TF is the count of a term in the entire document (for example, the count of
    the word “a” in document “D”).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF是整个文档中一个术语的计数（例如，单词“a”在文档“D”中的计数）。
- en: IDF is the log of the ratio of total documents (*N*) in the entire corpus and
    the number of documents (*d**f*) that contain the word “a.”
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IDF是整个语料库中总文档数(*N*)与包含单词“a”的文档数(*d**f*)之比的对数。
- en: 'So the TF-IDF formula will give us the relative importance of a word in the
    entire corpus. The mathematical formula is the multiplication of TF and IDF and
    is given by equation 7.1:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TF-IDF公式将给我们整个语料库中单词的相对重要性。数学公式是TF和IDF的乘积，由方程7.1给出：
- en: '![figure](../Images/verdhan-ch7-eqs-0x.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch7-eqs-0x.png)'
- en: (7.1)
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (7.1)
- en: where *N* is the total number of documents in the corpus, *tf*[*i*][,][*j*]is
    the frequency of the word in the document, and *df*[*i*] is the number of documents
    in the corpus that contain that word.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*N*是语料库中文档的总数，*tf*[*i*][,][*j*]是单词在文档中的频率，*df*[*i*]是语料库中包含该单词的文档数。
- en: The concept might sound complex. Let’s understand this with an example. Say
    we have a collection of 1 million sports journals. These sports journals contain
    many articles of various lengths. We also assume that all the articles are in
    the English language only. So, let’s say, in these documents, we want to calculate
    the TF-IDF value for the words “ground” and “backhand.”
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可能听起来很复杂。让我们用一个例子来理解这个概念。假设我们有一百万篇体育期刊的集合。这些体育期刊包含各种长度的文章。我们还假设所有文章都只使用英语。所以，在这些文档中，我们想要计算“地面”和“反手”这两个词的TF-IDF值。
- en: Let’s assume we have a document of 100 words with the word “ground” appearing
    five times and “backhand” only twice. So the TF for ground is 5/100 = 0.05, and
    for backhand, it is 2/100 = 0.02.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一篇包含100个单词的文档，其中“地面”这个词出现了5次，而“反手”只出现了2次。因此，“地面”的TF是5/100 = 0.05，而“反手”的TF是2/100
    = 0.02。
- en: We understand that the word “ground” is quite a common word in sports, while
    the word “backhand” will be used less often. Now we assume that “ground” appears
    in 100,000 documents out of 1 million documents while “backhand” appears only
    in 10\. So the IDF for “ground” is log (1,000,000/100,000) = log (10) = 1\. For
    “backhand” it will be log (1,000,000/10) = log (100,000) = 5.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，“地面”这个词在体育中相当常见，而“反手”这个词则不太常用。现在我们假设在100万份文档中有10万份包含“地面”，而“反手”只出现在10份中。因此，“地面”的IDF是log
    (1,000,000/100,000) = log (10) = 1。对于“反手”，它将是log (1,000,000/10) = log (100,000)
    = 5。
- en: To get the final values for “ground,” we multiply TF and IDF = 0.05 × 1 = 0.05\.
    To get the final values for “backhand,” we multiply TF and IDF = 0.02 × 5 = 0.1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到“地面”的最终值，我们需要将TF和IDF相乘，即0.05 × 1 = 0.05。要得到“反手”的最终值，我们需要将TF和IDF相乘，即0.02 ×
    5 = 0.1。
- en: We can observe in this case that the relative importance of the word “backhand”
    is more than the relative importance of the word “ground.” This is the advantage
    of TF-IDF over the frequency-based BOW approach. But TF-IDF takes more time to
    compute as compared to BOW, since all the TF and IDF have to be calculated. Nevertheless,
    TF-IDF offers a better and more mature solution as compared to the BOW approach
    in such cases. So, in scenarios where the relative importance of a word is in
    discussion, we can use TF-IDF. For example, if the task is to shortlist medical
    documents on cardiology, the importance of the word “angiogram” will be higher
    as it is much more related to cardiology.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以观察到“反手”这个词的相对重要性比“地面”这个词的相对重要性更高。这是TF-IDF相对于基于频率的BOW方法的优点。但是，与BOW相比，TF-IDF的计算需要更多的时间，因为所有TF和IDF都必须计算。尽管如此，TF-IDF在这种情况下提供了一个更好、更成熟的解决方案。因此，在讨论单词相对重要性的情况下，我们可以使用TF-IDF。例如，如果任务是筛选关于心脏病学的医学文档，那么“血管造影”这个词的重要性会更高，因为它与心脏病学的关系更为密切。
- en: We have so far covered BOW and the TF-IDF approach. But in neither of these
    approaches did we take the sequence of the words into consideration, which is
    covered in language models. We cover language models next.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了BOW和TF-IDF方法。但在这些方法中，我们都没有考虑单词的顺序，这是语言模型所涵盖的内容。我们将在下一节介绍语言模型。
- en: 7.11 Language models
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.11 语言模型
- en: Language models assign probabilities to the sequence of words. N-grams are the
    simplest in language models. We know that to analyze the text data, they must
    be converted to feature vectors. N-gram models create the feature vectors so that
    text can be represented in a format that can be analyzed further.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型为单词序列分配概率。N-gram 是语言模型中最简单的。我们知道，为了分析文本数据，它们必须被转换为特征向量。N-gram 模型创建特征向量，以便文本可以以可以进一步分析的形式表示。
- en: N-gram is a probabilistic language model. In an n-gram model, we calculate the
    probability of the *N*^(th) word given the sequence of (*N* – 1) words. To be
    more spe- cific, an n-gram model will predict the next word *x*[*i*] based on
    the words *x*[*i*][–(][*n–*][1][)], *x*[*i*][–(][*n–*][2][)]…*x*[*i*][–1]. If
    we wish to use the probability terms, we can represent them as the conditional
    probability of *x*[*i*] given the previous words, which can be represented as
    *P*(*x*[*i*] | *x*[*i*][–(][*n–*][1][)], *x*[*i*][–(][*n–*][2][)]…*x*[*i*][–1]).
    The probability is calculated by using the relative frequency of the sequence
    occurring in the text corpus.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 是一种概率语言模型。在 n-gram 模型中，我们计算给定 (*N* – 1) 个单词序列的第 *N* 个单词的概率。更具体地说，n-gram
    模型将根据单词 *x*[*i*][–(][*n–*][1][)]，*x*[*i*][–(][*n–*][2][)]…*x*[*i*][–1] 来预测下一个单词
    *x*[*i*]。如果我们希望使用概率术语，我们可以将它们表示为给定先前单词的 *x*[*i*] 的条件概率，可以表示为 *P*(*x*[*i*] | *x*[*i*][–(][*n–*][1][)],
    *x*[*i*][–(][*n–*][2][)]…*x*[*i*][–1])。概率是通过使用文本语料库中出现的序列的相对频率来计算的。
- en: NOTE  If the items are words, n-grams may be referred to as *shingles*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果项目是单词，n-gram 可以被称为 *shingles*。
- en: Let’s study this using an example. We will take a sentence and then break down
    the meaning by using words in the sentence. Consider we have the sentence “It
    is raining heavily.” We show the respective representations of this sentence by
    using different values of *n* in figure 7.6\. You should note how the sequence
    of words and their respective combinations are getting changed for different values
    of *n*. If we wish to use *n* = 1 or a single word to make a prediction, the representation
    will be as shown in figure 7.7\. Note that each word is used separately here.
    They are referred to as *unigrams*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来研究这个问题。我们将取一个句子，然后通过使用句子中的单词来分解其含义。考虑我们有一个句子“它正在下大雨。”我们通过在图 7.6 中使用不同的
    *n* 值来展示这个句子的相应表示。你应该注意，对于不同的 *n* 值，单词序列及其相应的组合是如何变化的。如果我们希望使用 *n* = 1 或单个单词来进行预测，表示将如图
    7.7 所示。注意，这里每个单词都是单独使用的。它们被称为 *unigrams*。
- en: '![figure](../Images/CH07_F07_Verdhan.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F07_Verdhan.png)'
- en: Figure 7.7 Unigrams, bigrams, and trigrams can be used to represent the same
    sentence. The concept can be extended to n-grams too.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.7 单词、二元组和三元组可以用来表示同一个句子。这个概念也可以扩展到 n-gram。
- en: If we wish to use *n* = 2, the number of words used will become two. They are
    referred to as *bigrams*. If we use *n* = 3, the number of words becomes three,
    and they are referred to as *trigrams,* and so on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望使用 *n* = 2，使用的单词数量将变为两个。它们被称为 *bigrams*。如果我们使用 *n* = 3，单词数量变为三个，它们被称为
    *trigrams*，依此类推。
- en: Hence, if we have a unigram, it is a sequence of one word; for two words, it
    is a bigram; for three words, it is a trigram; and so on. So, a trigram model
    will approximate the probability of a word given all the previous words by using
    the conditional probability of only the preceding two words, whereas a bigram
    will do the same by considering only the preceding word. This is a valid assumption,
    indeed, that the probability of a word will depend only on the preceding word
    and is referred to as the *Markov* assumption. Generally, *n* > 1 is considered
    to be much more informative than unigrams. But obviously, the computation time
    will increase too.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们有一个 unigram，它是一个单词的序列；对于两个单词，它是一个 bigram；对于三个单词，它是一个 trigram；依此类推。所以，一个
    trigram 模型将通过使用前两个单词的条件概率来近似给定所有先前单词的单词概率，而一个 bigram 将通过只考虑前一个单词来完成同样的工作。这确实是一个有效的假设，即单词的概率将只取决于前一个单词，这被称为
    *Markov* 假设。通常，*n* > 1 被认为比 unigrams 更有信息量。但显然，计算时间也会增加。
- en: The n-gram approach is very sensitive to the choice of *n*. It also depends
    significantly on the training corpus that has been used, which makes the probabilities
    heavily dependent on the training corpus. So, if an unknown word is encountered,
    it will be difficult for the model to work on that new word.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram 方法对 *n* 的选择非常敏感。它还显著依赖于所使用的训练语料库，这使得概率高度依赖于训练语料库。因此，如果遇到未知单词，模型在处理这个新单词时将非常困难。
- en: Next we create a Python example. We will show a few examples of text cleaning
    using Python.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个Python示例。我们将展示一些使用Python进行文本清理的示例。
- en: 7.12 Text cleaning using Python
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.12 使用Python进行文本清理
- en: 'There are a few libraries you may need to install. We will show a few small
    code snippets. You are advised to use them as per the examples. We are also including
    the respective screenshots of the code snippets and their results:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要安装几个库。我们将展示一些小的代码片段。建议您按照示例使用它们。我们还包括代码片段及其结果的相应截图：
- en: 'Code 1: Remove the blank spaces in the text. Import the library `re`; it is
    called the Regular Expression (`Regex`) expression. The text is “It is raining
    outside” with a lot of blank spaces in between (see figure 7.8):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 代码1：删除文本中的空白空间。导入库`re`；它被称为正则表达式（`Regex`）。文本是“外面在下雨”，其中包含很多空白空间（见图7.8）：
- en: '[PRE0]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![figure](../Images/CH07_F08_Verdhan.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F08_Verdhan.png)'
- en: Figure 7.8 Removing the blank spaces
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8 删除空白空间
- en: 'Code 2: Now we will remove the punctuation in the text data (see figure 7.9):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 代码2：现在我们将从文本数据中删除标点符号（见图7.9）：
- en: '[PRE1]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![figure](../Images/CH07_F09_Verdhan.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F09_Verdhan.png)'
- en: Figure 7.9 Removing the punctuation
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9 删除标点符号
- en: 'Code 3: Here is one more method to remove the punctuation (see figure 7.10):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 代码3：这里还有一种删除标点的方法（见图7.10）：
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![figure](../Images/CH07_F10_Verdhan.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F10_Verdhan.png)'
- en: Figure 7.10 An alternative way to remove punctuation
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10 删除标点的另一种方法
- en: 'Code 4: We will now remove the punctuation as well as convert the text to lowercase
    (see figure 7.11):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 代码4：我们现在将删除标点符号并将文本转换为小写（见图7.11）：
- en: '[PRE3]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![figure](../Images/CH07_F11_Verdhan.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F11_Verdhan.png)'
- en: Figure 7.11 Converting the text to lowercase
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11 将文本转换为小写
- en: 'Code 5: Tokenization is done here using the standard `nltk` library (see figure
    7.12):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 代码5：这里使用标准的`nltk`库进行标记化（见图7.12）：
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![figure](../Images/CH07_F12_Verdhan.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F12_Verdhan.png)'
- en: Figure 7.12 Tokenization
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.12 标记化
- en: Note that in the output of the code, we have all the words, including the punctuation
    marks, as different tokens. If you wish to exclude the punctuation, you can clean
    the punctuation marks using the code snippets shared earlier.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在代码的输出中，我们包含了所有的单词，包括标点符号，作为不同的标记。如果您希望排除标点符号，可以使用之前分享的代码片段进行清理。
- en: 'Code 6: Next comes the stop words. We will remove the stop words using the
    `nltk` library. After that, we tokenize the words (see figure 7.13):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 代码6：接下来是停用词。我们将使用`nltk`库来删除停用词。之后，我们对单词进行标记化（见图7.13）：
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![figure](../Images/CH07_F13_Verdhan.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F13_Verdhan.png)'
- en: Figure 7.13 Removing stop words and tokenizing words
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.13 删除停用词并进行单词标记化
- en: 'Code 7: We will now perform stemming on a text example. We use `nltk` library
    for it. The words are first tokenized, and then we apply stemming (see figure
    7.14):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 代码7：我们现在将对一个文本示例进行词干提取。我们使用`nltk`库来完成这项工作。首先对单词进行标记化，然后应用词干提取（见图7.14）：
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![figure](../Images/CH07_F14_Verdhan.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F14_Verdhan.png)'
- en: Figure 7.14 Tokenizing and then stemming the words
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.14 标记化然后词干提取单词
- en: 'Code 8: We now perform lemmatization on a text example. We use the `nltk` library
    for it. The words are first tokenized, and then we apply lemmatization (see figure
    7.15):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 代码8：我们现在对一个文本示例进行词形还原。我们使用`nltk`库来完成这项工作。首先对单词进行标记化，然后应用词形还原（见图7.15）：
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![figure](../Images/CH07_F15_Verdhan.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F15_Verdhan.png)'
- en: Figure 7.15 Tokenizing and then lemmatizing the words
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.15 标记化然后词形还原单词
- en: Observe and compare the difference between the two outputs of stemming and lemmatization.
    For “studies” and “studying,” stemming generated the output as “studi” while lemmatization
    generated the correct output as “study.”
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 观察并比较词干提取和词形还原两种输出的差异。对于“studies”和“studying”，词干提取生成了“studi”的输出，而词形还原生成了正确的“study”输出。
- en: We have covered BOW, TF-IDF, and n-gram approaches so far. But in all these
    techniques, the relationship between words has been neglected. This relationship
    is used in word embeddings, our next topic.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了BOW、TF-IDF和n-gram方法。但在所有这些技术中，都忽略了单词之间的关系。这种关系在词嵌入中得到了应用，这是我们下一个话题。
- en: 7.13 Word embeddings
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.13 词嵌入
- en: A word is characterized by the company it keeps.—John Rupert Firth
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个词的特点在于它所伴随的词。——约翰·鲁伯特·费思
- en: So far we have studied several approaches, but all the techniques ignore the
    contextual relationship between words. Let’s take a closer look using an example.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了多种方法，但所有技术都忽略了单词之间的上下文关系。让我们用一个例子来更仔细地看看。
- en: 'Imagine we have 100,000 words in our vocabulary, starting from “aa” (the basaltic
    lava) to “zoom.” Now, if we perform one-hot encoding, all these words can be represented
    in a vector form. Each word will have a unique vector. For example, if the position
    of the word “king” is 21000, the vector will have a shape like the following vector,
    which has 1 at the 21,000th position and the rest of the values as 0:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的词汇量中有100,000个单词，从“aa”（玄武岩熔岩）到“zoom。”现在，如果我们执行独热编码，所有这些单词都可以以向量形式表示。每个单词都将有一个唯一的向量。例如，如果单词“king”的位置是21,000，那么向量将具有以下形状，第21,000个位置为1，其余位置为0：
- en: '[PRE8]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are a few glaring problems with this approach:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有几个明显的问题：
- en: The number of dimensions is very high, and it is complex to compute.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度数量非常高，计算起来很复杂。
- en: The data is very sparse in nature.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据在本质上非常稀疏。
- en: If *n* new words have to be entered, the vocabulary increases by *n*, and hence
    each vector dimensionality increases by *n*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果必须输入*n*个新单词，词汇量将增加*n*，因此每个向量的维度也将增加*n*。
- en: This approach ignores the relationship between words. We know that “ruler,”
    “king,” and “monarch” are sometimes used interchangeably. In the one-hot-encoding
    approach, any such relationships are ignored.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法忽略了单词之间的关系。我们知道“ruler”、“king”和“monarch”有时可以互换使用。在独热编码方法中，任何此类关系都被忽略。
- en: If we wish to perform language translation, or generate a chat-bot, we need
    to pass such knowledge to the machine learning solution. Word embeddings provide
    a solution to the problem. They convert the high-dimensional word features into
    lower dimensions while maintaining the contextual relationship. Word embeddings
    allow us to create much more generalized models. We can understand the meaning
    by looking at an example.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望执行语言翻译或生成聊天机器人，我们需要将这些知识传递给机器学习解决方案。词嵌入提供了解决方案。它们将高维度的单词特征转换为低维度，同时保持上下文关系。词嵌入使我们能够创建更通用的模型。我们可以通过查看示例来理解意义。
- en: NOTE  In an LLM-enabled solution, you might not need to do a lot of these steps.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在启用LLM的解决方案中，你可能不需要执行很多这些步骤。
- en: In the example shown in figure 7.16, the relation of “man” to “woman” is similar
    to “king” to “queen”; “good” to “nice” is similar to “bad” to “awful”; or the
    relationship of “UK” to “London” is similar to “Japan” to “Tokyo.”
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.16所示的示例中，“man”与“woman”的关系类似于“king”与“queen”；“good”与“nice”的关系类似于“bad”与“awful”；或者“UK”与“London”的关系类似于“Japan”与“Tokyo”。
- en: '![figure](../Images/CH07_F16_Verdhan.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F16_Verdhan.png)'
- en: Figure 7.16 Word embeddings can be used to represent the relationships between
    words. For example, there is a relation from “men” to “women” that is similar
    to “king” to “queen” as both “men-women” and “king-queen” represent the male-female
    gender relationship.
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.16 词嵌入可以用来表示单词之间的关系。例如，存在从“men”到“women”的关系，这与“king”到“queen”的关系相似，因为“men-women”和“king-queen”都代表了男女性别关系。
- en: In simple terms, using word embeddings, we can represent the words that have
    similar meanings. Word embeddings can be thought of as a class of techniques where
    we represent each of the individual words in a predefined vector space. Each of
    the words in the corpus is mapped to one vector. The distributed representation
    is understood based on the word’s usage. Hence, words that can be used similarly
    have similar representations. This allows the solution to capture the underlying
    meaning of the words and their relationships. Hence, the meaning of the word plays
    a significant role. This representation is more intelligent as compared to the
    BOW approach where each word is treated differently, irrespective of its usage.
    Also, the number of dimensions is fewer as compared to one-hot encoding. Each
    word is represented by 10s or 100s of dimensions, which is significantly less
    than the one-hot encoding approach where 1000s of dimensions are used for representation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用词嵌入，我们可以表示具有相似意义的单词。词嵌入可以被视为一类技术，其中我们在预定义的向量空间中表示每个单独的单词。语料库中的每个单词都映射到一个向量。基于单词的使用情况来理解分布式表示。因此，可以类似使用的单词具有相似的表现。这使得解决方案能够捕捉单词及其关系的潜在意义。因此，单词的意义起着重要作用。这种表示比BOW方法更智能，在BOW方法中，每个单词都被单独对待，不考虑其使用情况。此外，与独热编码相比，维度数量更少。每个单词由10s或100s个维度表示，这比独热编码方法中用于表示的1000s个维度要少得多。
- en: We cover the two most popular techniques—Word2Vec and global vectors for word
    representation (GloVe)—in the next section. The mathematical foundation for Word2Vec
    and GloVe are beyond the scope of this book. We provide an understanding of the
    working mechanism of the solutions and then develop Python code using Word2Vec
    and GloVe. This section is more technically involved, so if you are interested
    only in the application of the solutions, you can skip the next section.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中介绍两种最流行的技术——Word2Vec和用于词表示的全局向量（GloVe）。Word2Vec和GloVe的数学基础超出了本书的范围。我们提供了对解决方案工作原理的理解，然后使用Word2Vec和GloVe开发Python代码。这一节涉及更多的技术性内容，所以如果您只对解决方案的应用感兴趣，可以跳过下一节。
- en: 7.14 Word2Vec and GloVe
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.14 Word2Vec和GloVe
- en: Word2Vec was first published in 2013\. It was developed by Tomas Mikolov and
    others at Google. We share the link to the paper at the end of the chapter. You
    are advised to study the paper thoroughly if you wish to learn about the more
    technical elements in detail.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec首次发表于2013年。它由Google的Tomas Mikolov等人开发。我们将在本章末尾分享论文的链接。如果您想详细了解更技术性的元素，建议您仔细研究这篇论文。
- en: Word2Vec is a group of models used to produce word embeddings. The input is
    a large corpus of text. The output is a vector space with a very large number
    of dimensions. In this output, each of the words in the corpus is assigned a unique
    and corresponding vector. The most important point is that the words that have
    a similar or common context in the corpus are located nearby in the vector space
    produced.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一组用于生成词嵌入的模型。输入是一个大型文本语料库。输出是一个具有非常多个维度的向量空间。在这个输出中，语料库中的每个词都被分配了一个独特且对应的向量。最重要的是，在语料库中具有相似或共同上下文的词在产生的向量空间中彼此靠近。
- en: 'In Word2Vec, the researchers introduced two different learning models—the continuous
    bag of words (CBOW) and the continuous skip-gram model:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在Word2Vec中，研究人员引入了两种不同的学习模型——连续词袋（CBOW）和连续跳字模型：
- en: In CBOW, the model makes a prediction of the current word from a window of surrounding
    context words. So the CBOW model predicts a target word based on the context of
    the surrounding words in the text. Recall that in the BOW approach, the order
    of the words does not play any part. In contrast, in CBOW, the order of the words
    is significant.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CBOW中，模型从周围上下文词的窗口中预测当前词。因此，CBOW模型根据文本中周围词的上下文来预测目标词。回想一下，在BOW方法中，词的顺序不起任何作用。相比之下，在CBOW中，词的顺序是重要的。
- en: The continuous skip-gram model uses the current word to predict the surrounding
    window of context words. While doing so, it allocates more weight to the neighboring
    words as compared to the distant words.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续跳字模型使用当前词来预测周围窗口的上下文词。在这个过程中，它相对于远距离词，给邻近词分配更多的权重。
- en: 'GloVe is an unsupervised learning algorithm for generating vector representation
    for words. It was developed by Pennington and others at Stanford and launched
    in 2014\. It is a combination of two techniques: matrix factorization techniques
    and local context-based learning used in Word2Vec. GloVe can be used to find relationships
    like zip codes and cities, synonyms, etc. It generates a single set of vectors
    for words with the same morphological structure.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe是一种用于生成词向量表示的无监督学习算法。它由斯坦福大学的Pennington等人开发，并于2014年推出。它是两种技术的结合：矩阵分解技术和Word2Vec中使用的基于局部上下文的学习。GloVe可以用来找到诸如邮编和城市、同义词等关系。它为具有相同形态结构的词生成单个向量集。
- en: Both Word2Vec and GloVe learn and understand vector representation of their
    words from the co-occurrence information. Co-occurrence means how frequently the
    words appear together in a large corpus. The prime difference is that Word2Vec
    is a prediction-based model, while GloVe is a frequency-based model. Word2Vec
    predicts the context given a word while GloVe learns the context by creating a
    co-occurrence matrix on how frequently a word appears in a given context.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec和GloVe都从它们的词的共现信息中学习和理解它们的词向量表示。共现意味着词在一个大型语料库中一起出现的频率。主要区别在于Word2Vec是一个基于预测的模型，而GloVe是一个基于频率的模型。Word2Vec在给定一个词的情况下预测上下文，而GloVe通过创建一个共现矩阵来学习一个词在给定上下文中出现的频率。
- en: Exercise 7.3
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习7.3
- en: 'Answer these questions to check your understanding:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以检查您的理解：
- en: BOW is more rigorous than the TF-IDF approach. True or False?
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BOW比TF-IDF方法更严格。对或错？
- en: Differentiate between Word2Vec and GloVe.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区分Word2Vec和GloVe。
- en: We will now move to the case study and Python implementation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转向案例研究和Python实现。
- en: 7.15 Sentiment analysis case study with Python implementation
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.15 使用Python实现的情感分析案例研究
- en: So far, we have discussed a lot of concepts on NLP and text data. In this section,
    we first explore a business case and then develop a Python solution based on it.
    Here we are working on sentiment analysis.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了许多关于自然语言处理和文本数据的概念。在本节中，我们首先探讨一个商业案例，然后基于此开发一个Python解决方案。在这里，我们正在进行情感分析。
- en: Product reviews are a rich source of information—both for customers and organizations.
    Whenever we wish to buy any new product or service, we tend to look at the reviews
    by fellow customers. You might have reviewed products and services yourself. These
    reviews are available at Amazon and on blogs, surveys, etc.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 产品评论是客户和组织的信息宝库。每当我们要购买任何新产品或服务时，我们往往会查看其他客户的评论。你可能自己已经评论过产品和服务。这些评论可以在亚马逊、博客、调查等地方找到。
- en: Let’s consider a case. A telecom operator receives complaints from its customers,
    reviews about the service, and comments about the overall experience. The streams
    can be product quality, pricing, onboarding experience, ease of registration,
    payment process, general reviews, customer service, etc. We want to determine
    the general context of the review—whether it is positive, negative, or neutral.
    The reviews include the number of stars allocated, actual text reviews, pros and
    cons about the product/service, attributes, etc. However, there are a few business
    problems—for instance,
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个案例。一家电信运营商收到了来自其客户的投诉、关于服务的评论以及关于整体体验的评论。这些流可以是产品质量、定价、入网体验、注册便捷性、支付流程、一般评论、客户服务等等。我们想要确定评论的一般背景——是正面、负面还是中性。这些评论包括分配的星级数量、实际文本评论、关于产品/服务的优缺点、属性等。然而，存在一些商业问题——例如，
- en: Sometimes the number of stars received by a product/service is very high, while
    the actual reviews are quite negative.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时产品/服务收到的星级数量非常高，而实际评论却相当负面。
- en: The organizations and the product owners need to know which features are appreciated
    by the customers and which features are disliked by the customers. The team can
    then work on improving the features that are disliked.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织和产品负责人需要知道哪些功能受到客户的喜爱，哪些功能不受客户的喜爱。然后团队可以针对不受欢迎的功能进行改进。
- en: There is a need to gauge and keep an eye on the competition! The organizations
    need to know the attributes of the popular products of their competitors.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要衡量和关注竞争！组织需要了解其竞争对手的流行产品的属性。
- en: The product owners want to better plan for the upcoming features they wish to
    release in the future.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品负责人希望更好地规划他们未来希望发布的即将到来的功能。
- en: 'So the business teams will be able to answer these important questions:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，业务团队将能够回答这些重要问题：
- en: What are our customers’ satisfaction levels for the products and services?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的产品和服务的客户满意度如何？
- en: What are the major pain points and dissatisfactions of the customers?
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的主要痛点和不满意是什么？
- en: What drives the customers’ engagement?
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么驱使了客户的参与度？
- en: Which services are complex and time-consuming, and which are the most liked
    services/products?
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些服务复杂且耗时，哪些是最受欢迎的服务/产品？
- en: 'This business use case will drive the following business benefits:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个业务用例将带来以下业务效益：
- en: The products and services that are most satisfactory and are the most liked
    should be continued.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最令人满意和最受欢迎的产品和服务应该继续进行。
- en: The ones that are not liked and are receiving a negative score should be improved
    and challenges mitigated.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不受欢迎且收到负面评分的产品应该得到改进，挑战得到缓解。
- en: The respective teams, like finance, operations, complaints, CRM, etc., can be
    notified, and they can work individually to improve the customer experience.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关团队，如财务、运营、投诉、CRM等，可以被通知，并且他们可以单独工作以改善客户体验。
- en: The precise reasons for liking or disliking the services will be useful for
    the respective teams to work in the right direction.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于喜欢或不喜欢服务的确切原因将有助于相关团队朝着正确的方向工作。
- en: Overall, it will provide a benchmark to measure the Net Promoter Score for the
    customer base. The business can strive to enhance the overall customer experience.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，它将为衡量客户基础的净推荐分数提供一个基准。企业可以努力提高整体客户体验。
- en: We might want to represent these findings by means of a dashboard. This dashboard
    will be refreshed on a regular cycle, like monthly or quarterly.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望通过仪表板来展示这些发现。这个仪表板将按照定期周期刷新，比如每月或每季度。
- en: To solve this business problem, the teams can collect relevant data from websites,
    surveys, Amazon, blogs, etc. Then an analysis can be done on that dataset. It
    is relatively easy to analyze the structured data. In this example, we work on
    text data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个商业问题，团队可以从网站、调查、亚马逊、博客等地方收集相关数据。然后可以对数据集进行分析。分析结构化数据相对容易。在这个例子中，我们处理文本数据。
- en: 'The Python Jupyter notebook is pushed to the GitHub location. You are advised
    to use the Jupyter notebook from the GitHub location as it contains more steps.
    The steps are as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Python Jupyter笔记本已推送到GitHub位置。建议您使用GitHub位置的Jupyter笔记本，因为它包含更多步骤。步骤如下：
- en: 'Import all the libraries:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有库：
- en: '[PRE9]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '2\. Define the tags. These tags are used to get the attributes of the product
    from the reviews:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 定义标签。这些标签用于从评论中获取产品的属性：
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '3\. Make everything ready to extract the data. We create a dataframe to store
    the customer reviews. Then we iterate through all the reviews and extract the
    information:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 准备好提取数据。我们创建一个数据框来存储客户评论。然后我们遍历所有评论并提取信息：
- en: '[PRE11]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '4\. Iterate through the reviews and then fill in the details:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 遍历评论并填写详细信息：
- en: '[PRE12]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '5\. Have a look at the output we generated:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 查看我们生成的输出：
- en: '[PRE13]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '6\. Save the output to a path. You can give your own path:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 将输出保存到路径。你可以指定自己的路径：
- en: '[PRE14]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '7\. Load the data and analyze it:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 加载数据并进行分析：
- en: '[PRE15]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '8\. Look at the basic information about the dataset:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 查看数据集的基本信息：
- en: '[PRE16]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '9\. Look at the distribution of the stars given in the reviews. This will allow
    us to understand the reviews given by the customers:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 9. 查看评论中给出的星级分布。这将使我们能够理解客户给出的评论：
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '10\. Make the text lowercase, and then remove the stop words and the words
    that have the highest frequency:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10. 将文本转换为小写，然后移除停用词和频率最高的词：
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '11\. Tokenize the data:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 11. 对数据进行分词：
- en: '[PRE19]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '12\. Perform lemmatization:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 12. 进行词元还原：
- en: '[PRE20]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '13\. Append all the reviews to the string:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 13. 将所有评论追加到字符串：
- en: '[PRE21]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '14\. Do the sentiment analysis. From `textblob`, we take the sentiment method.
    It generates polarity and subjectivity for a sentiment. Sentiment polarity for
    an element is the orientation of the sentiment in the expression; that is, it
    tells us if the text expresses a negative, positive, or neutral sentiment in the
    text. It subjectively measures and quantifies the amount of opinion and factual
    information in the text. If the subjectivity is high, it means that the text contains
    more opinion than facts:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 14. 进行情感分析。从`textblob`中获取情感方法。它为情感生成极性和主观性。一个元素的 sentiment 极性是情感在表达中的方向；也就是说，它告诉我们文本是否表达了负面的、正面的或中性的情感。它主观地衡量并量化文本中观点和事实信息量。如果主观性高，这意味着文本中包含的观点比事实多：
- en: '[PRE22]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '15\. Save the sentiment to a .csv file:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 15. 将情感保存到.csv文件：
- en: '[PRE23]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '16\. Allocate a meaning or a tag to the sentiment. We classify each of the
    scores from extremely satisfied to extremely dissatisfied:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 16. 为情感分配一个含义或标签。我们将每个评分从极度满意到极度不满意进行分类：
- en: '[PRE24]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '17\. Look at the sentiment scores and plot them too. Finally, we merge them
    with the main dataset:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 17. 查看情感评分并绘制它们。最后，我们将它们与主数据集合并：
- en: '[PRE25]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Adds column polarityScore'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 添加列polarityScore'
- en: In this case study, you not only scraped the reviews from the website but you
    also analyzed the dataset. If we compare the sentiments, we can see that the stars
    given to a product do not represent a true picture.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，你不仅从网站上抓取了评论，还分析了数据集。如果我们比较情感，我们可以看到产品得到的星级并不代表真实情况。
- en: Figure 7.17 compares the actual stars and the output from sentiment analysis.
    We can observe that 73% of customers have given five stars and 7% have given four
    stars, while in the sentiment analysis most of the reviews have been classified
    as neutral. This is the real power of sentiment analysis!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17比较了实际星级和情感分析的结果。我们可以观察到73%的客户给出了五星评价，7%给出了四星评价，而在情感分析中，大多数评论都被归类为中性。这正是情感分析的真实力量！
- en: '![figure](../Images/CH07_F17_Verdhan.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F17_Verdhan.png)'
- en: Figure 7.17 Compare the original distribution of number of stars on the left
    side and the real results from the sentiment analysis on the right.
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.17比较了左侧原始星级分布和右侧情感分析的真实结果。
- en: Sentiment analysis is quite an important use case. It is very useful for business
    and product teams. The preceding code can be scaled to any such business problem
    at hand.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一个非常重要的用例。它对商业和产品团队非常有用。前面的代码可以扩展到任何手头的商业问题。
- en: We now move to the second case study on document classification using Python.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向第二个案例研究，即使用Python进行文档分类。
- en: 7.16 Text clustering using Python
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.16 使用Python进行文本聚类
- en: 'Consider this: we have a bunch of text datasets or documents, but they all
    are mixed up. We do not know which text belongs to which class. In this case,
    we will assume that we have two types of text datasets: one that has all the data
    related to football and one that is related to travel. We will develop a model
    that can segregate these two classes. To do that, we follow these steps:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这种情况：我们有一批文本数据集或文档，但它们都是混合在一起的。我们不知道哪些文本属于哪个类别。在这种情况下，我们将假设我们有两种类型的文本数据集：一种包含所有与足球相关的数据，另一种与旅行相关。我们将开发一个可以将这两个类别分开的模型。为此，我们遵循以下步骤：
- en: 'Import all the libraries:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有库：
- en: '[PRE26]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '2\. Create a dummy dataset. This text data has a few sentences we have written
    ourselves. There are two categories:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 创建一个虚拟数据集。这些文本数据包含我们亲自写的几句话。有两个类别：
- en: '[PRE27]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '3\. Use TF-IDF to vectorize the data:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 使用TF-IDF对数据进行向量化：
- en: '[PRE28]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '4\. Do the clustering:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 进行聚类：
- en: '[PRE29]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '5\. Represent the centroids and print the outputs (see figure 7.18):'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 表示质心并打印输出（见图7.18）：
- en: '[PRE30]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![figure](../Images/CH07_F18_Verdhan.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F18_Verdhan.png)'
- en: Figure 7.18 Printed output
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.18 打印输出
- en: You can extend this example to other datasets too. Get the datasets from the
    internet and replicate the code in the preceding example.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此示例扩展到其他数据集。从互联网上获取数据集，并在前面的示例中复制代码。
- en: We have pushed the code to the GitHub location of the book. You are advised
    to use it. It is really an important source to represent text data.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将代码推送到本书的GitHub位置。建议您使用它。这确实是一个表示文本数据的重要来源。
- en: 7.17 GenAI for text data
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.17 GenAI用于文本数据
- en: GenAI solutions are a new kind of unsupervised solution. You surely have heard
    about ChatGPT and LLMs. They have revolutionized the world. GenAI for text data
    uses machine learning models to create human-like text. It is trained on large-scale
    data patterns and hence can generate a variety of content pieces—for example,
    essays, technical reports, and summaries of a book—and can act like a human chat
    interface. Even the complex translation of languages is made easy with GenAI.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI解决方案是一种新的无监督解决方案。你肯定听说过ChatGPT和LLMs。它们已经改变了世界。GenAI用于文本数据使用机器学习模型来创建类似人类的文本。它在大规模数据模式上训练，因此可以生成各种内容片段——例如，文章、技术报告和书籍摘要——并且可以像人类聊天界面一样操作。即使是复杂的多语言翻译，GenAI也能使其变得简单。
- en: GenAI for text data involves the use of advanced algorithms, like transformers,
    to generate coherent, contextually appropriate text. These algorithms are trained
    on mammoth datasets. Imagine we feed tons of content present on the internet to
    the algorithms. By learning patterns and relationships between the words and the
    sentences, the grammar used, syntax, and semantics, they can create human-like
    responses. These models, such as OpenAI’s GPT or Google’s BERT, are very powerful
    for drafting emails with correct language and grammar, creating detailed reports,
    writing code modules in a language like Java/C++, and many other tasks. Using
    this power, content creators, writers and copyrighters, brand managers and marketers,
    and business owners can produce high-quality text in a much more scalable and
    efficient manner.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI用于文本数据涉及使用高级算法，如transformers，来生成连贯、上下文适当的文本。这些算法在庞大的数据集上训练。想象一下，我们将互联网上存在的海量内容输入到算法中。通过学习单词和句子之间的模式和关系，使用的语法、句法和语义，它们可以创建类似人类的响应。这些模型，如OpenAI的GPT或Google的BERT，在撰写带有正确语言和语法的电子邮件、创建详细报告、用Java/C++等语言编写代码模块以及许多其他任务方面非常强大。利用这种力量，内容创作者、作家和版权所有者、品牌经理和营销人员以及企业主可以以更可扩展和高效的方式生产高质量的文本。
- en: Despite its amazing potential, GenAI still has some areas in need of improvement.
    Sometimes it generates inaccurate information, also known as hallucinations. Ensuring
    that the output remains unbiased and ethical is another hurdle, as models can
    inadvertently reflect societal biases present in the data they were trained on.
    AI-generated text is increasingly being used in customer service, automating responses
    while still maintaining a personal tone. Researchers are also exploring its use
    in the healthcare and legal fields, where it can help with documentation and drafting.
    While GenAI is revolutionizing the way text is produced, the need for human oversight
    remains critical to ensure quality, accuracy, and fairness.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GenAI具有惊人的潜力，但仍有一些需要改进的领域。有时它会生成不准确的信息，也称为幻觉。确保输出保持无偏见和道德是另一个挑战，因为模型可能会无意中反映它们在训练数据中存在的
    societal biases。AI生成的文本越来越多地用于客户服务，在保持个性化语调的同时自动化响应。研究人员也在探索其在医疗和法律领域的应用，它可以帮助文档编制和起草。虽然GenAI正在改变文本生成的方式，但人类监督的需求仍然至关重要，以确保质量、准确性和公平性。
- en: 7.18 Concluding thoughts
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.18 总结性思考
- en: 'Text data is one of the most useful datasets. A lot of intelligence is hidden
    in the texts: logs, blogs, reviews, posts, tweets, complaints, comments, articles,
    and so on—the sources of text data are many. Organizations are investing in setting
    up the infrastructure for accessing text data and storing it. Analyzing text data
    requires better processing powers and better machines than our standard laptops.
    It requires special skill sets and a deeper understanding of the concepts. NLP
    is an evolving field, and a lot of research is underway. At the same time, we
    cannot ignore the importance of sound business acumen and knowledge.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据是最有用的数据集之一。文本中隐藏着大量的智慧：日志、博客、评论、帖子、推文、投诉、评论、文章等等——文本数据的来源很多。组织正在投资建立访问和存储文本数据的基础设施。分析文本数据需要比我们标准笔记本电脑更好的处理能力和更好的机器。它需要特殊的技能集和对概念的深入理解。自然语言处理（NLP）是一个不断发展的领域，正在进行大量的研究。同时，我们也不能忽视良好的商业洞察力和知识的重要性。
- en: Data analysis and machine learning are not easy. We have to understand a lot
    of concepts around data cleaning, exploration, representation, and modeling. But
    analyzing unstructured data might be even more complex than analyzing structured
    datasets. We worked on an images dataset in the last chapter, and in the current
    chapter, we worked on text data.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和机器学习并不容易。我们必须理解围绕数据清理、探索、表示和建模的许多概念。但分析非结构化数据可能比分析结构化数据集更复杂。我们在上一章中处理了一个图像数据集，而在本章中，我们处理了文本数据。
- en: Text data is one of the most difficult datasets to analyze. There are so many
    permutations and combinations for text data. Cleaning the text data is a difficult
    and complex task. In this chapter, we discussed a few important techniques to
    clean text data. We also covered some methods to represent text data in vector
    forms. You are advised to practice each of these methods and compare the performances
    by applying each of the techniques. We also introduced the concept of GenAI for
    text data.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据是分析起来最困难的数据集之一。文本数据有如此多的排列组合。清理文本数据是一项困难和复杂的工作。在本章中，我们讨论了几种清理文本数据的重要技术。我们还介绍了一些将文本数据表示为向量形式的方法。建议您练习这些方法，并通过应用每种技术来比较性能。我们还介绍了针对文本数据的GenAI概念。
- en: With this, we come to the end of chapter 7\. This also marks an end to part
    2\. In the next part, the complexity increases. We will be studying even deeper
    concepts of unsupervised learning algorithms.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就结束了第7章。这也标志着第2部分的结束。在下一部分，复杂性会增加。我们将研究无监督学习算法的更深入概念。
- en: 7.19 Practical next steps and suggested readings
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.19 实践下一步行动和推荐阅读
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供了一些下一步行动的建议和一些有用的阅读材料：
- en: 'Get the datasets from the following link. You will find a lot of text datasets
    here. You are advised to implement clustering and dimensionality reduction solutions:'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从以下链接获取数据集。您在这里会发现很多文本数据集。建议您实现聚类和降维解决方案：
- en: '50 Free Machine Learning Datasets: Natural Language Processing: [https://mng.bz/ZljO](https://mng.bz/ZljO)'
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50个免费机器学习数据集：自然语言处理：[https://mng.bz/ZljO](https://mng.bz/ZljO)
- en: 'You will find a lot of useful datasets at Kaggle as well: [https://www.kaggle.com/datasets?search=text](https://www.kaggle.com/datasets?search=text)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在Kaggle上也会找到很多有用的数据集：[https://www.kaggle.com/datasets?search=text](https://www.kaggle.com/datasets?search=text)
- en: 'Go through the following research papers:'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看以下研究论文：
- en: Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient Estimation
    of Word Representations in Vector Space. [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). 在向量空间中高效估计词表示。[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)
- en: 'Pennington, J., Socher, R., and Manning, C. D. (2014). GloVe: Global Vectors
    for Word Representation. [https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington, J., Socher, R., and Manning, C. D. (2014). GloVe：全局词向量表示。[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)
- en: Das, B., and Chakraborty, S. (2018). An Improved Text Sentiment Classification
    Model Using TF-IDF and Next Word Negation. [https://arxiv.org/pdf/1806.06407.pdf](https://arxiv.org/pdf/1806.06407.pdf)
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das, B., and Chakraborty, S. (2018). 使用TF-IDF和下一词否定改进文本情感分类模型。[https://arxiv.org/pdf/1806.06407.pdf](https://arxiv.org/pdf/1806.06407.pdf)
- en: 'Consider these widely quoted papers:'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑以下广为人知的论文：
- en: Blum, A., and Mitchell, T. (1998). Combining labeled and unlabeled data with
    co-training. [https://dl.acm.org/doi/10.1145/279943.279962](https://dl.acm.org/doi/10.1145/279943.279962)
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum, A., and Mitchell, T. (1998). 结合有标签和无标签数据与协同训练。[https://dl.acm.org/doi/10.1145/279943.279962](https://dl.acm.org/doi/10.1145/279943.279962)
- en: Knight, K. (2009). Bayesian Inference with Tears. [https://mng.bz/RVp0](https://mng.bz/RVp0)
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knight, K. (2009). 哭泣的贝叶斯推理。[https://mng.bz/RVp0](https://mng.bz/RVp0)
- en: Hofmann, T. (1999). Probabilistic latent semantic indexing. [https://dl.acm.org/doi/10.1145/312624.312649](https://dl.acm.org/doi/10.1145/312624.312649)
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofmann, T. (1999). 概率潜在语义索引。[https://dl.acm.org/doi/10.1145/312624.312649](https://dl.acm.org/doi/10.1145/312624.312649)
- en: Hindle, D., and Rooth, M. (1993). Structural Ambiguity and Lexical Relations.
    [https://aclanthology.org/J93-1005.pdf](https://aclanthology.org/J93-1005.pdf)
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hindle, D., and Rooth, M. (1993). 结构歧义和词汇关系。[https://aclanthology.org/J93-1005.pdf](https://aclanthology.org/J93-1005.pdf)
- en: Collins and Singer. (1999). Unsupervised Models for Named Entity Classification.
    [https://aclanthology.org/W99-0613.pdf](https://aclanthology.org/W99-0613.pdf)
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collins和Singer. (1999). 命名实体分类的无监督模型。[https://aclanthology.org/W99-0613.pdf](https://aclanthology.org/W99-0613.pdf)
- en: 'See the comprehensive study on TF-IDF feature weighting: Das, M., Selvakumar,
    K., and Alphonse, J. P. A. (2023). A Comparative Study on TF-IDF Feature Weighting
    Method and its Analysis using Unstructured Dataset. [https://arxiv.org/abs/2308.04037](https://arxiv.org/abs/2308.04037)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看关于TF-IDF特征加权的全面研究：Das, M., Selvakumar, K., and Alphonse, J. P. A. (2023).
    TF-IDF特征加权方法比较研究及其在非结构化数据集上的分析。[https://arxiv.org/abs/2308.04037](https://arxiv.org/abs/2308.04037)
- en: Summary
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Text data’s omnipresence in blogs, social media, surveys, and more, and its
    capacity to express emotions, emphasizes the importance of this form of data.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据在博客、社交媒体、调查等领域的无处不在，以及其表达情感的能力，强调了这种形式数据的重要性。
- en: Applications of text analysis include sentiment analysis, document categorization,
    language translation, spam filtering, and named-entity recognition.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析的应用包括情感分析、文档分类、语言翻译、垃圾邮件过滤和命名实体识别。
- en: Challenges in text data include handling junk characters, multiple languages,
    evolving language, synonyms, and context-based meanings.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据面临的挑战包括处理垃圾字符、多种语言、演化的语言、同义词和基于上下文的意义。
- en: Data preprocessing and cleaning involves removing stop words and unwanted characters
    and normalizing text through stemming and lemmatization.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理和清洗包括去除停用词和不需要的字符，并通过词干提取和词形还原来规范化文本。
- en: Within text representation techniques, one-hot encoding is basic but not scalable;
    advanced techniques consider frequency and context.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本表示技术中，独热编码是基本的但不可扩展；高级技术考虑频率和上下文。
- en: Tokenization involves breaking down text into tokens and is fundamental for
    creating analysis-ready datasets.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词涉及将文本分解成标记，这是创建分析准备好的数据集的基础。
- en: The BOW approach is a fast, frequency-based method that ignores word order and
    context.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BOW方法是一种快速、基于频率的方法，它忽略了单词顺序和上下文。
- en: TF-IDF weighs words based on importance over mere frequency, offering more insightful
    analysis than BOW.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF根据重要性而非单纯频率来权衡单词，提供了比BOW更深入的分析。
- en: Language models and n-grams use word sequences for probabilistic predictions,
    with variations like unigrams, bigrams, and trigrams.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型和n-gram使用单词序列进行概率预测，包括单词、双词和三词等变体。
- en: Python for text parsing illustrates cleaning and preprocessing text data using
    Python libraries like `nltk`.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python进行文本解析展示了如何使用Python库如`nltk`进行文本数据的清理和预处理。
- en: Techniques like Word2Vec and GloVe maintain contextual relationships between
    words for better semantic understanding.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Word2Vec和GloVe的技术，通过维护词语之间的上下文关系，以实现更好的语义理解。
- en: Word2Vec is prediction based, while GloVe is frequency based; both create compact
    and meaningful word representations.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec是基于预测的，而GloVe是基于频率的；两者都创建了紧凑且具有意义的词语表示。
- en: LLMs have revolutionized the entire landscape for text datasets.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经彻底改变了文本数据集的整个格局。
