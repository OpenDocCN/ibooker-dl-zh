<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span></span> <span class="chapter-title-text">Training large language models: How to generate the generator</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Setting up a training environment and common libraries </li>
<li class="readable-text" id="p3">Applying various training techniques, including using advanced methodologies</li>
<li class="readable-text" id="p4">Tips and tricks to get the most out of training</li>
</ul>
</div>
<div class="readable-text" id="p5">
<blockquote>
<div>
     Be water, my friend. 
     <div class="quote-cite">
       —Bruce Lee 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p6">
<p>Are you ready to have some fun?! What do you mean the last four chapters weren’t fun? Well, I promise this one for sure will be. We’ve leveled up a lot and gained a ton of context that will prove invaluable now as we start to get our hands dirty. By training an LLM, we can create bots that can do amazing things and have unique personalities. Indeed, we can create new friends and play with them. In the last chapter, we showed you how to create a training dataset based on your Slack messages. Now we will show you how to take that dataset and create a persona of yourself. Finally, you will no longer have to talk to that one annoying coworker, and just like Gilfoyle, you can have your own AI Gilfoyle (<a href="https://youtu.be/IWIusSdn1e4">https://youtu.be/IWIusSdn1e4</a>). </p>
</div>
<div class="readable-text intended-text" id="p7">
<p>First things first, we’ll show you how to set up a training environment, as the process can be very resource-demanding, and without the proper equipment, you won’t be able to enjoy what comes next. We’ll then show you how to do the basics, like training from scratch and finetuning, after which we’ll get into some of the best-known methods to improve upon these processes, making them more efficient, faster, and cheaper. We’ll end the chapter with some tips and tricks we’ve acquired through our experience of training models in the field.</p>
</div>
<div class="readable-text" id="p8">
<h2 class="readable-text-h2" id="sigil_toc_id_80"><span class="num-string">5.1</span> Multi-GPU environments</h2>
</div>
<div class="readable-text" id="p9">
<p>Training is a resource-intensive endeavor. A model that only takes a single GPU to run inference on may take 10 times that many to train if, for nothing else, to parallelize your work and speed things up so you aren’t waiting for a thousand years for it to finish training. To really take advantage of what we want to teach you in this chapter, we’re first going to have to get you set up in an environment you can use as a playground. Later in the chapter, we’ll teach some resource-optimal strategies as well, but you’ll need to understand how to set up a multi-GPU env if you want to use the largest LLMs anyway.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>While you can learn a lot using smaller LLMs, what sets apart a pro from an amateur is often the ease and fluidity they have when working with larger models. And there’s a good reason for this since, on the whole, larger models outperform smaller models. If you want to work with the largest models, you’ll never be able to get started on your laptop. Even most customized gaming rigs with dual GPUs aren’t enough for inference, let alone training.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>To this end, we wanted to share with you a few methods to acquire access to a multi-GPU environment in the cloud, and then we will share the tools and libraries necessary to utilize them. The largest models do not fit in a single GPU, so without these environments and tools, you’ll be stuck playing on easy mode forever.</p>
</div>
<div class="readable-text" id="p12">
<h3 class="readable-text-h3" id="sigil_toc_id_81"><span class="num-string">5.1.1</span> Setting up</h3>
</div>
<div class="readable-text" id="p13">
<p>It should be pointed out up front that while multi-GPU environments are powerful, they are also expensive. When it comes to multi-GPUs, no services we know of offer a free tier or offering, but you can at least take comfort in knowing that paying per hour will be way cheaper than purchasing the rigs wholesale. Of course, if you can get your company to pay the bill, we recommend it, but it is still your responsibility to spin down and turn off any environment you create to avoid unnecessary charges.</p>
</div>
<div class="readable-text intended-text" id="p14">
<p>If your company is paying, it likely has chosen a hosted service that makes this whole process easy. For the rest of us, setting up a virtual machine (VM) in Google’s Compute Engine is one of the easiest methods. Once set up, we will then show you how to utilize it.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p15">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A note to the readers</h5>
</div>
<div class="readable-text" id="p16">
<p>For learning purposes, we use smaller models throughout this book in our code listings such that you can work with them on a single GPU either locally or using a service like Colab or Kaggle, which offers a free tier of a single GPU. While the listings could be run on CPU-only hardware, you won’t want to do it. Ultimately, there shouldn’t be any need to run these costly VMs throughout the book. However, you likely will still want to. Training with multiple GPUs is much faster, more efficient, and often necessary. We do encourage you to try larger LLM variations that require these bigger rigs, as the experience will be priceless. To make it easy, you should be able to recycle the code in this chapter for models and datasets much larger than what is presented, which will often just be a matter of changing a few lines.</p>
</div>
</div>
<div class="readable-text" id="p17">
<h4 class="readable-text-h4 sigil_not_in_toc">Google virtual machine</h4>
</div>
<div class="readable-text" id="p18">
<p>One of the easiest ways to create a multi-GPU environment is to set up a VM on Google’s cloud. To get started, you’ll need to create an account, create a Google Cloud Project (GCP), set up billing, and download the gcloud CLI. None of these steps are particularly hard, but be sure to follow the documentation found at <a href="https://cloud.google.com/sdk/docs/install-sdk">https://cloud.google.com/sdk/docs/install-sdk</a> for your operating system to install the SDK. The steps here also include the steps and how-tos for creating an account, project, and billing in the Before You Begin section if you don’t already have an account. </p>
</div>
<div class="readable-text intended-text" id="p19">
<p>For new accounts, Google offers a $300 credit to be used for pretty much anything on their GCP platform except GPUs. We hate to break this news, but sadly, there’s just no free lunch where we are going. So you’ll need to be sure to upgrade to a paid GCP tier. Don’t worry; just following along should only cost a couple of dollars, but if you are money conscious, we recommend reading the entire section first and then trying it out.</p>
</div>
<div class="readable-text intended-text" id="p20">
<p>After setting up your account, by default, GCP sets your GPU quotas to 0. Quotas are used to manage your costs. To increase your quotas, go to <a href="https://console.cloud.google.com/iam-admin/quotas">https://console.cloud.google.com/iam-admin/quotas</a>. You’ll be looking for the gpus_all_regions quota, and since we plan to use multiple GPUs, go ahead and submit a request to increase it to 2 or more. </p>
</div>
<div class="readable-text intended-text" id="p21">
<p>With all the prerequisites in place, we’ll get started by initializing and logging in. You’ll do this by running the following command in a terminal on your computer:</p>
</div>
<div class="browsable-container listing-container" id="p22">
<div class="code-area-container">
<pre class="code-area">$ gcloud init</pre>
</div>
</div>
<div class="readable-text" id="p23">
<p>You may have already done this step if you had to install the SDK, but if not, it will launch a web browser to help us log in and authorize us for the gcloud CLI, which allows us to select our project. We will be assuming you have just the one project, but if this isn’t your first rodeo and you have multiple projects, you’ll need to add the <code>--project</code> flag in all the subsequent commands.</p>
</div>
<div class="readable-text intended-text" id="p24">
<p>Next, we need to determine two things: the machine type (or which GPUs we want to use) and our container image. To pick a machine type, you can check out the different options at <a href="https://cloud.google.com/compute/docs/gpus">https://cloud.google.com/compute/docs/gpus</a>. For beginners, we highly recommend the NVIDIA L4 GPU, as it is an all-around fantastic machine. For our purposes, we’ll be using the g2-standard-24, which comes with two L4 GPUs and costs us about $2 per hour. This machine type isn’t in every region and zone, but you can find a region close to you at <a href="https://cloud.google.com/compute/docs/regions-zones">https://cloud.google.com/compute/docs/regions-zones</a>. We will be using the us-west1 region and us-west1-a zone.</p>
</div>
<div class="readable-text intended-text" id="p25">
<p>For the container image, we’ll save ourselves a lot of hassle by using one that has all the basics set up. Generally, this means creating your own, but Google has several prebuilt container images for deep learning, which are great to use or a great place to start as a base image to customize. These are all found in the <code>deeplearning-platform -release</code> project that they own. To check out the options available, you can run</p>
</div>
<div class="browsable-container listing-container" id="p26">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute images list --project deeplearning-platform-release 
    --format=”value(NAME)” --no-standard-images</pre>
</div>
</div>
<div class="readable-text print-book-callout" id="p27">
<p><span class="print-book-callout-head">NOTE</span>  You can learn more about the container image options here: <a href="https://cloud.google.com/deep-learning-vm/docs/images">https://cloud.google.com/deep-learning-vm/docs/images</a>.</p>
</div>
<div class="readable-text" id="p28">
<p>You can pick from Base, TensorFlow, and PyTorch compiled images, along with the CUDA and Python versions. We’ll be using <code>common-gpu-v20230925-debian-11-py310</code>, which is a simple image ready for GPU with a Debian Linux distribution and Python 3.10. Now that we have everything we need, we can create our VM! Go ahead and run the following commands to set up the VM:</p>
</div>
<div class="browsable-container listing-container" id="p29">
<div class="code-area-container">
<pre class="code-area">$ INSTANCE_NAME="g2-llminprod-example"
    $ gcloud compute instances create ${INSTANCE_NAME} --zone=us-west1-a 
    --machine-type=g2-standard-24 --image-project=deeplearning-platform-release
    --image=common-gpu-v20230925-debian-11-py310 --boot-disk-size=200GB --scopes
    cloud-platform --metadata=install-unattended-upgrades=False,install-nvidia-
    driver=True --maintenance-policy TERMINATE --restart-on-failure</pre>
</div>
</div>
<div class="readable-text" id="p30">
<p>The first command creates an environment variable to store the name of our VM since we’ll also be using it in several of the following commands. This name can be whatever you want it to be. The next command creates our VM instance. The first several flags (<code>zone</code>, <code>image</code>, <code>machine</code>) should make sense since we just spent the previous paragraphs preparing and gathering that information. The <code>boot-disk-size</code> sets the disk space for our VM and defaults to 200 GB, so it’s included here because it’s important to know for LLMs since they are large assets, and you will likely need to increase it—especially for LLMs that require multiple GPUs to run.</p>
</div>
<div class="readable-text intended-text" id="p31">
<p>The <code>scopes</code> flag is passed to set authorization. Current GCP best practices recommend setting it to <code>cloud-platform</code>, which determines authorization through OAuth and IAM roles. The <code>metadata</code> field isn’t required but is used here as a trick to ensure the NVIDIA drivers are installed. It is really useful if you are using these commands to create a shell script to automate this process. You should know that it will cause a small delay between when the VM is up and when you can actually SSH into it, as it won’t be responsive while it installs the drivers. If you don’t include it, the first time you SSH in through a terminal, it will ask you if you want to install it, so no harm done. However, if you access the VM through other methods (described in the next sections), you can run into problems. The last two commands are standard maintenance policies.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>Once that runs, you can verify the VM is up by running</p>
</div>
<div class="browsable-container listing-container" id="p33">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute instances describe ${INSTANCE_NAME}</pre>
</div>
</div>
<div class="readable-text" id="p34">
<p>This command will give you a lot of information about your instance that is worth looking over, including a status field that should read <code>'RUNNING'</code>. Once you’ve confirmed that it’s up, we will SSH into it. If this is your first time using gcloud to SSH, an SSH key will be generated automatically. Go ahead and run the following command:</p>
</div>
<div class="browsable-container listing-container" id="p35">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute ssh ${INSTANCE_NAME}</pre>
</div>
</div>
<div class="readable-text" id="p36">
<p>Your terminal will be shelled into our multi-GPU VM, and you are now in business. At this point, your VM is still just an empty shell, so you’ll want to bring in code. The easiest way to do this is to copy the files over with Secure Copy Protocol (SCP). You can do this for a single file or a whole directory. For example, assuming your project has a requirements.txt file and a subdirectory local-app-folder, from a new terminal, you can run the following commands:</p>
</div>
<div class="browsable-container listing-container" id="p37">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute scp requirements.txt ${INSTANCE_NAME}:~/requirements.txt
$ gcloud compute scp --recurse ~/local-app-folder/ 
${INSTANCE_NAME}:~/vm-app-folder</pre>
</div>
</div>
<div class="readable-text" id="p38">
<p>Overall, not too bad. Once you’ve gone through the process and set everything up, the next time you set up a VM, it will only be four commands (<code>create</code>, <code>describe</code>, <code>ssh</code>, <code>scp</code>) to get up and running. </p>
</div>
<div class="readable-text intended-text" id="p39">
<p>Of course, these instances cost good money, so the last command you’ll want to know before moving on is how to delete it:</p>
</div>
<div class="browsable-container listing-container" id="p40">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute instances delete ${INSTANCE_NAME} --quiet</pre>
</div>
</div>
<div class="readable-text" id="p41">
<p>For Linux power users, this code line is likely all you need, but for the rest of us plebs, shelling into a VM through a terminal is less than an ideal working environment. We’ll show you some tips and tricks to make the most of your remote machine.</p>
</div>
<div class="readable-text" id="p42">
<h4 class="readable-text-h4 sigil_not_in_toc">SSH through VS Code</h4>
</div>
<div class="readable-text" id="p43">
<p>For most devs, a terminal is fine, but what we really want is an IDE. Most IDEs offer remote SSH capabilities, but we’ll demonstrate with VS Code. The first step is to install the extension Remote-SSH (you can find the extension here: <a href="https://mng.bz/q0dE">https://mng.bz/q0dE</a>). Other extensions offer this capability, but Remote-SSH is maintained by Microsoft and has over 17 million installs, so it’s a great choice for beginners. </p>
</div>
<div class="readable-text intended-text" id="p44">
<p>Next, we are going to run a configuration command:</p>
</div>
<div class="browsable-container listing-container" id="p45">
<div class="code-area-container">
<pre class="code-area">$ gcloud compute config-ssh</pre>
</div>
</div>
<div class="readable-text" id="p46">
<p>Then, inside of VS Code, you can press F1 to open the command palette and run the Remote-SSH: Open SSH Host… command, and you should see your VM’s SSH address, which will look like l4-llm-example.us-west1-a.project-id-401501. If you don’t see it, something went wrong with the <code>config-ssh</code> command, and you likely need to run <code>gcloud</code> <code>init</code> again. Select the address, and a new VS Code window should pop up. In the bottom corner, you’ll see that it is connecting to your remote machine. And you are done! Easy. From here, you can use VS Code like you would when using it locally.</p>
</div>
<div class="readable-text" id="p47">
<h3 class="readable-text-h3" id="sigil_toc_id_82"><span class="num-string">5.1.2</span> Libraries</h3>
</div>
<div class="readable-text" id="p48">
<p>Although setting up hardware is important, none of it will work without the software packages that enable different points of hardware to communicate with each other effectively. With LLMs, the importance of the software is compounded. One author personally experienced having all hardware correctly configured and was pretty sure the software setup was likewise configured, only to start up training a model and be met with an estimated training time of over three years. After troubleshooting, the team realized this was because he had installed multiple versions of CUDA Toolkit, and PyTorch was looking at an incompatible (up-to-date) one instead of the one he had intended to use.</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>These software packages are about more than just using the CUDA low-level communication with your GPU; they’re about load-balancing, quantizing, and parallelizing your data as it runs through each computation to make sure it’s going as fast as possible while still enabling a certain level of fidelity for the matrices. You wouldn’t want to spend a long time making sure your embedding vectors are phenomenal representations just to have them distorted at run time. Thus, we present the four deep-learning libraries every practitioner should know for multi-GPU instances: DeepSpeed, Accelerate, BitsandBytes, and xFormers. At the time of this writing, all complementary features between these libraries are experimental, so feel free to mix and match. If you get a setup that utilizes all four at once to their full potential without erroring, drop it in a reusable container so fast.</p>
</div>
<div class="readable-text" id="p50">
<h4 class="readable-text-h4 sigil_not_in_toc">DeepSpeed</h4>
</div>
<div class="readable-text" id="p51">
<p>DeepSpeed is an optimization library for distributed deep learning. DeepSpeed is powered by Microsoft and implements various enhancements for speed in training and inference, like handling extremely long or multiple inputs in different modalities, quantization, caching weights and inputs, and, probably the hottest topic right now, scaling up to thousands of GPUs.</p>
</div>
<div class="readable-text intended-text" id="p52">
<p>Installation is fairly simple if you remember to always install the latest—but not nightly—version of PyTorch first. This means you also need to configure your CUDA Toolkit beforehand. Once you have that package, <code>pip</code> <code>install</code> <code>deepspeed</code> should get you right where you want to go unless, ironically, you use Microsoft’s other products. If you are on a Windows OS, there is only partial support, and there are several more steps you will need to follow to get it working for inference, not training, mode.</p>
</div>
<div class="readable-text" id="p53">
<h4 class="readable-text-h4 sigil_not_in_toc">Accelerate</h4>
</div>
<div class="readable-text" id="p54">
<p>From Hugging Face, Accelerate is made to help abstract the code for parallelizing and scaling to multiple GPUs away from you so that you can focus on the training and inference side. One huge advantage of Accelerate is that it adds only one import and two lines of code and changes two other lines, compared to a standard training loop in PyTorch in its vanilla implementation. Beyond that, Accelerate also has fairly easy CLI usage, allowing it to be automated along with Terraform or AWS CDK.</p>
</div>
<div class="readable-text intended-text" id="p55">
<p>Accelerate boasts compatibility over most environments, and as long as your environment is Python 3.8+ and PyTorch 1.10.0+ (CUDA compatibility first), you should be able to use Accelerate without problems. Once that’s done, <code>pip</code> <code>install</code> <code>accelerate</code> should get you there. Accelerate also has experimental support for DeepSpeed if you would like to get the benefits of both.</p>
</div>
<div class="readable-text" id="p56">
<h4 class="readable-text-h4 sigil_not_in_toc">BitsandBytes</h4>
</div>
<div class="readable-text" id="p57">
<p>If you don’t already know the name Tim Dettmers in this field, you should become acquainted pretty quickly. Not many people have done as much as he has to make CUDA-powered computing accessible. This package is made to help practitioners quantize models and perform efficient matrix multiplication for inference (and maybe training) within different bit sizes, all the way down to INT8. BitsandBytes has similar requirements and drawbacks to DeepSpeed: the requirements are Python 3.8+ and CUDA 10.0+ on Linux and Mac environments and partial support for Windows with a different package. </p>
</div>
<div class="readable-text intended-text" id="p58">
<p>You should have little trouble installing BitsandBytes, as <code>pip</code> <code>install</code> <code>bitsandbytes</code> should work for most use cases. If you find yourself on Windows, you’re in luck: <code>pip</code> <code>install</code> <code>bitsandbytes-windows</code> will work as well. If you want to use it with Hugging Face’s transformers or PyTorch, you will need to edit some minimum requirements stated within both of those packages, as the Windows version does not have the same version numbers as the regular package. BitsandBytes offers its own implementations of optimizers like Adam and NN layers like Linear to allow for that 8-bit boost to run deep learning apps on smaller devices at greater speed with a minimal drop in accuracy.</p>
</div>
<div class="readable-text" id="p59">
<h4 class="readable-text-h4 sigil_not_in_toc">xFormers</h4>
</div>
<div class="readable-text" id="p60">
<p>The most bleeding edge of the libraries we recommend for most use cases is xFormers, which is made for research and production. Following a (hopefully) familiar PyTorch-like pattern of independent building blocks for multiple modalities, xFormers takes it a step further and offers components that won’t be available in PyTorch for quite a while. One that we’ve used quite a lot is memory-efficient exact attention, which speeds up inference considerably. </p>
</div>
<div class="readable-text intended-text" id="p61">
<p>xFormers has more requirements than the other packages, and we’d like to stress once more that using one or more tools to keep track of your environment is strongly recommended. On Linux and Windows, you’ll need PyTorch 2.0.1, and <code>pip</code> <code>install</code> <code>-U</code> <code>xFormers</code> should work for you. That said, there are paths for installation with pretty much any other version of PyTorch, but the main ones are versions 1.12.1, 1.13.1, and 2.0.1.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>In table 5.1, we can see a heavily reduced breakdown of what each of these packages does and how it integrates with your code. Each package does similar things, but even when performing the same task, they will often perform those tasks differently or on different parts of your model or pipeline. There is some overlap between packages, and we’d encourage you to use all of them to see how they might benefit you. Now that you have an environment and a basic understanding of some of the tools we’ll be using, let’s move forward and see it in action. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p63">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 5.1</span> Comparison of optimization packages for ML</h5>
<table>
<thead>
<tr>
<th>
<div>
         Library 
       </div></th>
<th>
<div>
         Faster training or inference 
       </div></th>
<th>
<div>
         Code integration 
       </div></th>
<th>
<div>
         Lower accuracy 
       </div></th>
<th>
<div>
         Many GPUs 
       </div></th>
<th>
<div>
         Quantization 
       </div></th>
<th>
<div>
         Optimizations 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  DeepSpeed <br/></td>
<td>  Both <br/></td>
<td>  CLI <br/></td>
<td>  Depends <br/></td>
<td>  Yes <br/></td>
<td>  Supports <br/></td>
<td>  Caching, gradient checkpointing, memory management, scaling <br/></td>
</tr>
<tr>
<td>  Accelerate <br/></td>
<td>  Both <br/></td>
<td>  CLI and Code <br/></td>
<td>  Depends <br/></td>
<td>  Yes <br/></td>
<td>  Supports <br/></td>
<td>  Automation, compiling, parallelization <br/></td>
</tr>
<tr>
<td>  BitsandBytes <br/></td>
<td>  Both <br/></td>
<td>  Code <br/></td>
<td>  Always <br/></td>
<td>  NA <br/></td>
<td>  Yes but only <br/></td>
<td>  Quantization, quantized optimizers <br/></td>
</tr>
<tr>
<td>  xFormers <br/></td>
<td>  Training <br/></td>
<td>  Code <br/></td>
<td>  Depends <br/></td>
<td>  NA <br/></td>
<td>  Yes and more <br/></td>
<td>  Efficient attention, memory management <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p64">
<h2 class="readable-text-h2" id="sigil_toc_id_83"><span class="num-string">5.2</span> Basic training techniques</h2>
</div>
<div class="readable-text" id="p65">
<p>In training LLMs, the process typically starts with defining the architecture of the model, the nature and amount of data required, and the training objectives. We’ve already gone over these steps in the last chapter, so you should be well prepared already, but let’s look at a brief recap. The model architecture usually follows a variant of the Transformer architecture due to its effectiveness in capturing long-term dependencies and its parallelizable nature, making it amenable to large-scale computation. Data is the lifeblood of any LLM (or any ML model in general), which typically requires extensive corpora of diverse and representative text data. As the model’s purpose is to learn to predict the next word in a sequence, it’s crucial to ensure that the data covers a wide array of linguistic contexts.</p>
</div>
<div class="readable-text intended-text" id="p66">
<p>Because we’ll be going over various training techniques in this chapter, here’s a (super) quick rundown of the investments you’ll need for different types. For training from scratch, you’ll need VRAM greater than four times the number of billions of parameters to hold the model, along with the batches of training data. So to train a 1B parameter model from scratch, you’ll need at least 5 or 6 GB of VRAM, depending on your batch sizes and context length. Consider training a 70B parameter model like Llama 2 as an exercise. How much VRAM will you need to fit the model, along with a 32K token context limit? If you’re coming up with a number around 300 GB of VRAM, you’re right. For the finetuning techniques, you’ll need significantly fewer resources for a couple of reasons—namely, quantization and amount of data needed, meaning you no longer need 4× VRAM, but can use 2× or 1× with the correct setup.</p>
</div>
<div class="readable-text intended-text" id="p67">
<p>Unlike traditional ML models, LLMs are often trained in stages. Figure 5.1 shows the basic training life cycle of an LLM, starting from scratch, then finetuning, and finally prompting. The first step is creating our foundation model, where we take a large, often unrefined, dataset and train an empty shell of a model on it. This training will create a model that has seen such a large corpus of text that it appears to have a basic understanding of language. We can then take that foundation model and use transfer learning techniques, generally finetuning on a small, highly curated dataset to create a specialized LLM for expert tasks. Lastly, we use prompting techniques that, while not traditional training, allow us to goad the model to respond in a particular fashion or format, improving the accuracy of our results.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p68">
<img alt="figure" height="339" src="../Images/5-1.png" width="742"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.1</span> The training life cycle of an LLM. We start by creating a foundation model based on a large corpus of text, which we later finetune using a curated dataset for a specific task. We can then further improve the model by using the model itself and techniques like prompting to enhance or enlarge our curated dataset.</h5>
</div>
<div class="readable-text" id="p69">
<p>You’ll notice that the training life cycle is often a continuous loop—training models to understand language better and then using those models to improve our training datasets. Later in this chapter, we will go into more depth about other advanced training techniques that take advantage of this loop, like prompt tuning and RLHF. For now, let’s solidify our understanding of three basic steps.</p>
</div>
<div class="readable-text" id="p70">
<h3 class="readable-text-h3" id="sigil_toc_id_84"><span class="num-string">5.2.1</span> From scratch</h3>
</div>
<div class="readable-text" id="p71">
<p>Training an LLM is computationally intensive and can take several weeks or months even on high-performance hardware. This process feeds chunks of data (or “batches”) to the model and adjusts the weights based on the calculated loss. Over time, this iterative process of prediction and adjustment, also known as an epoch, leads the model to improve its understanding of the syntactic structures and complexities in the data. It’s worth noting that monitoring the training process is crucial to avoid overfitting, where the model becomes excessively tailored to the training data and performs poorly on unseen data. Techniques like early stopping, dropout, and learning rate scheduling are used to ensure the generalizability of the model, but they are not silver bullets. Remember, the ultimate goal is not just to minimize the loss on training data but to create a model that can understand and generate human-like text across a broad range of contexts.</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>Training an LLM from scratch is a complex process that begins with defining the model’s architecture. This decision should be guided by the specific task at hand, the size of the training dataset, and the available computational resources. The architecture, in simple terms, is a blueprint of the model that describes the number and arrangement of layers, the type of layers (like attention or feed-forward layers), and the connections between them. Modern LLMs typically employ a variant of the Transformer architecture, known for its scalability and efficiency in handling long sequences of data.</p>
</div>
<div class="readable-text intended-text" id="p73">
<p>Once the model’s architecture is set, the next step is to compile a large and diverse dataset for training. The quality and variety of data fed into the model largely dictate the model’s ability to understand and generate human-like text. A common approach is to use a large corpus of internet text, ensuring a wide-ranging mix of styles, topics, and structures. The data is then preprocessed and tokenized, converting the raw text into a numerical format that the model can learn from. During this tokenization process, the text is split into smaller units, or tokens, which could be as short as a single character or as long as a word.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>With a model and dataset ready, the next step is to initialize the model and set the learning objectives. The LLMs are trained using autoregressive semi-supervised learning techniques where the model learns to predict the next word in a sequence given the preceding words. The model’s weights are randomly initialized and then adjusted through backpropagation and optimization techniques such as Adam or Stochastic Gradient Descent based on the difference between the model’s predictions and the actual words in the training data. The aim is to minimize this difference, commonly referred to as the “loss,” to improve the model’s predictive accuracy.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>Training involves feeding the tokenized text into the model and adjusting the model’s internal parameters to minimize the loss. We said this once, but it bears repeating: this process is computationally demanding and may take weeks or even months to complete, depending on the model size and available hardware. After training, the model is evaluated on a separate validation dataset to ensure that it can generalize to unseen data. It is common to iterate on this process, finetuning the model parameters and adjusting the architecture as needed based on the model’s performance on the validation set.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p>Let’s explore training a brand-new transformer-based language model “from scratch,” meaning without any previously defined architecture, embeddings, or weights. Figure 5.2 shows this process. You shouldn’t have to train an LLM from scratch, nor would you normally want to, as it’s a very expensive and time-consuming endeavor; however, knowing how can help you immensely. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p77">
<img alt="figure" height="739" src="../Images/5-2.png" width="929"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.2</span> A simplified version of all the steps necessary to train a language model (large or otherwise) from scratch. You must have data, then define all of the model behavior, and only then proceed to train.</h5>
</div>
<div class="readable-text" id="p78">
<p>Listing 5.1 allows you to run through the motions without training an actual massive model, so feel free to explore with this code. For a more complex and complete example, check out Andrej Karpathy’s minGPT project here: <a href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a>. You should pay attention to some things when you review the listing. You might recall that we talked about tokenization and embeddings in the last chapter, so one thing to notice is that for simplicity, we will be using a character-based tokenizer. Before you run the code, can you predict whether this was a good or bad idea? Also, pay attention to how we use both Accelerate and BitsandBytes, which we introduced a little bit ago; you’ll see that these libraries come in mighty handy. Next, watch as we slowly build up the LLMs architecture, building each piece in a modular fashion and later defining how many of each piece is used and where to put them, almost like Legos. Finally, at the very end of the code, you’ll see a typical model training loop, splitting our data, running epochs in batches, and so forth.</p>
</div>
<div class="browsable-container listing-container" id="p79">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.1</span> An example of training from scratch </h5>
<div class="code-area-container">
<pre class="code-area">import os
import torch
from accelerate import Accelerator

import bitsandbytes as bnb  

class GPT(torch.nn.Module):     <span class="aframe-location"/> #1
    def __init__(self):
        super().__init__()
        self.token_embedding = torch.nn.Embedding(vocab_size, n_embed)
        self.positional_embedding = torch.nn.Embedding(block_size, n_embed)
        self.blocks = torch.nn.Sequential(
            *[Block(n_embed, n_head=n_head) for _ in range(n_layer)]
        )
        self.ln_f = torch.nn.LayerNorm(n_embed)
        self.lm_head = torch.nn.Linear(n_embed, vocab_size)

        self.apply(self._init_weights)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        tok_emb = self.token_embedding(idx)
        pos_emb = self.positional_embedding(torch.arange(T, device=device))
        x = tok_emb + pos_emb
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            loss = torch.nn.functional.cross_entropy(logits, targets)

        return logits, loss

    def _init_weights(self, module):
        if isinstance(module, torch.nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, torch.nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -block_size:]
            logits, loss = self(idx_cond)
            logits = logits[:, -1, :]
            probs = torch.nn.functional.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx


class Block(torch.nn.Module):         <span class="aframe-location"/> #2
    def __init__(self, n_embed, n_head):
        super().__init__()
        head_size = n_embed // n_head
        self.self_attention = MultiHeadAttention(n_head, head_size)
        self.feed_forward = FeedFoward(n_embed)
        self.ln1 = torch.nn.LayerNorm(n_embed)
        self.ln2 = torch.nn.LayerNorm(n_embed)

    def forward(self, x):
        x = x + self.self_attention(self.ln1(x))
        x = x + self.feed_forward(self.ln2(x))
        return x


class MultiHeadAttention(torch.nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = torch.nn.ModuleList(
            [Head(head_size) for _ in range(num_heads)]
        )
        self.projection = torch.nn.Linear(head_size * num_heads, n_embed)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.projection(out))
        return out


class Head(torch.nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = torch.nn.Linear(n_embed, head_size, bias=False)
        self.query = torch.nn.Linear(n_embed, head_size, bias=False)
        self.value = torch.nn.Linear(n_embed, head_size, bias=False)
        self.register_buffer(
            "tril", torch.tril(torch.ones(block_size, block_size))
        )

        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x):
        _, T, _ = x.shape
        k = self.key(x)
        q = self.query(x)
        attention = q @ k.transpose(-2, -1) * k.shape[-1] ** 0.5
        attention = attention.masked_fill(
            self.tril[:T, :T] == 0, float("-inf")
        )
        attention = torch.nn.functional.softmax(attention, dim=-1)
        attention = self.dropout(attention)

        v = self.value(x)
        out = attention @ v
        return out


class FeedFoward(torch.nn.Module):
    def __init__(self, n_embed):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(n_embed, 4 * n_embed),
            torch.nn.ReLU(),
            torch.nn.Linear(4 * n_embed, n_embed),
            torch.nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)


def encode(string):     <span class="aframe-location"/> #3
    return [utt2int[c] for c in string]


def decode(line):
    return "".join([int2utt[i] for i in line])


def get_batch(split):
    data = train_data if split == "train" else val_data
    idx = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i : i + block_size] for i in idx])
    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])
    x, y = x.to(device), y.to(device)
    return x, y


@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ["train", "val"]:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out


if __name__ == "__main__":      <span class="aframe-location"/> #4
    batch_size = 64  # Number of utterances at once     <span class="aframe-location"/> #5
    block_size = 256  # Maximum context window size
    max_iters = 5000
    eval_interval = 500
    learning_rate = 3e-4
    eval_iters = 200
    n_embed = 384
    n_head = 6
    n_layer = 6
    dropout = 0.2
    accelerator = Accelerator()
    device = accelerator.device
    doing_quantization = False  # Change to True if imported bitsandbytes

    with open("./data/crimeandpunishment.txt", "r", encoding="utf-8") as f: <span class="aframe-location"/> #6
        text = f.read()

    chars = sorted(list(set(text)))    <span class="aframe-location"/> #7
    vocab_size = len(chars)
    utt2int = {ch: i for i, ch in enumerate(chars)}
    int2utt = {i: ch for i, ch in enumerate(chars)}

    data = torch.tensor(encode(text), dtype=torch.long)
    n = int(0.9 * len(data))
    train_data = data[:n]
    val_data = data[n:]

    model = GPT().to(device)         <span class="aframe-location"/> #8
    print("Instantiated Model")
    print(
        sum(param.numel() for param in model.parameters()) / 1e6,
        "Model parameters",
    )

    optimizer = (
        torch.optim.AdamW(model.parameters(), lr=learning_rate)
        if not doing_quantization
        else bnb.optim.Adam(model.parameters(), lr=learning_rate)
    )
    print("Instantiated Optimizer")

    model, optimizer, train_data = accelerator.prepare(
        model, optimizer, train_data
    )
    print("Prepared model, optimizer, and data")

    # 
    for iter in range(max_iters):    <span class="aframe-location"/> #9
        print(f"Running Epoch {iter}")
        if iter % eval_interval == 0 or iter == max_iters - 1:
            losses = estimate_loss()
            print(
                f"| step {iter}: train loss {losses['train']:.4f} "
                "| validation loss {losses['val']:.4f} |"
            )

        xb, yb = get_batch("train")
        logits, loss = model(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        accelerator.backward(loss)
        optimizer.step()

    model_dir = "./models/scratchGPT/"     <span class="aframe-location"/> #10
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    model_path = model_dir + "model.pt"   <span class="aframe-location"/> #11
    torch.save(
        model.state_dict(),
        model_path,
    )

    loaded = GPT().load_state_dict(model_path)     <span class="aframe-location"/> #12

    context = torch.zeros((1, 1), dtype=torch.long, device=device)   <span class="aframe-location"/> #13
    print(decode(loaded.generate(context, max_new_tokens=500)[0].tolist()))</pre>
<div class="code-annotations-overlay-container">
     #1 Δefines the overall GPT architecture
     <br/>#2 Δefines the building blocks of the model
     <br/>#3 Helper functions for training
     <br/>#4 Trains the model
     <br/>#5 Parameters for our experiment
     <br/>#6 Δataset
     <br/>#7 Character-based pseudo-tokenization
     <br/>#8 Instantiates the model and looks at the parameters
     <br/>#9 Training block
     <br/>#10 Creates model directory
     <br/>#11 Saves the model
     <br/>#12 Loads the saved model
     <br/>#13 Tests the loaded model
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p80">
<p>In listing 5.1, we explored how the Lego blocks are put together for the GPT family of models and showed a training loop reminiscent of our exploration of language modeling in chapter 2. Beyond showing the first part of generative pretraining for models, this example also illustrates why character-based modeling, whether convolutional or otherwise, is weak for language modeling. Did you get it right? Yup, character-based modeling isn’t the best. Alphabets on their own do not contain enough information to produce statistically significant results, regardless of the tuning amount. From a linguistic standpoint, this is obvious, as alphabets and orthography, in general, are representations of meaning generated from humans, which is not intrinsically captured. </p>
</div>
<div class="readable-text intended-text" id="p81">
<p>Some of the ways to help with that information capture are increasing our tokenization capture window through word-, subword-, or sentence-level tokenization. We can also complete the pretraining before showing the model our task to allow it to capture as much approximate representation as possible. Next, we’ll show what benefits combining these two steps can have on our model’s performance.</p>
</div>
<div class="readable-text" id="p82">
<h3 class="readable-text-h3" id="sigil_toc_id_85"><span class="num-string">5.2.2</span> Transfer learning (finetuning)</h3>
</div>
<div class="readable-text" id="p83">
<p>Transfer learning is an essential approach in machine learning and a cornerstone of training LLMs. It’s predicated on the notion that we can reuse knowledge learned from one problem (the source domain) and apply it to a different but related problem (the target domain). In the context of LLMs, this typically means using a pretrained model, trained on a large, diverse dataset, and adapting it to a more specific task or domain.</p>
</div>
<div class="readable-text intended-text" id="p84">
<p>In the first step of transfer learning, an LLM is trained on a large, general-purpose corpus, such as the entirety of Wikipedia, books, or the internet. This pretraining stage allows the model to learn an extensive range of language patterns and nuances on a wide variety of topics. The goal here is to learn a universal representation of language that captures a broad understanding of syntax, semantics, and world knowledge. These models are often trained for many iterations and require significant computational resources, which is why it’s practical to use pretrained models provided by organizations like OpenAI or Hugging Face.</p>
</div>
<div class="readable-text intended-text" id="p85">
<p>After pretraining, the LLM is updated on a specific task or domain. This update process adapts the general-purpose language understanding of the model to a more specific task, such as sentiment analysis, text classification, or question answering. Updating usually requires significantly less computational resources than the initial pretraining phase because it involves training on a much smaller dataset specific to the task at hand. Through this process, the model is able to apply the vast knowledge it gained during pretraining to a specific task, often outperforming models trained from scratch on the same task. This process of transfer learning has led to many of the advances in NLP over recent years.</p>
</div>
<div class="readable-text" id="p86">
<h4 class="readable-text-h4 sigil_not_in_toc">Finetuning</h4>
</div>
<div class="readable-text" id="p87">
<p>There are several different transfer learning techniques, but when it comes to LLMs, the one everyone cares about is finetuning. Finetuning an LLM involves taking a pretrained model—that is, a model already trained on a large general corpus—and adapting it to perform a specific task or to understand a specific domain of data. </p>
</div>
<div class="readable-text intended-text" id="p88">
<p>This technique uses the fact that the base model has already learned a significant amount about the language, allowing you to reap the benefits of a large-scale model without the associated computational cost and time. The process of finetuning adapts the pre-existing knowledge of the model to a specific task or domain, making it more suitable for your specific use case. It’s like having a generalist who already understands the language well and then providing specialist training for a particular job. This approach is often more feasible for most users due to the significantly reduced computational requirements and training time compared to training a model from scratch.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>The first step in finetuning involves choosing a suitable pretrained model. This decision is guided by the specific task you want the model to perform and by the resources available to you. Keep in mind that this means setting a goal for the model’s behavior before training. Once the pretrained model has been chosen, it’s crucial to prepare the specific dataset you want the model to learn from. This data could be a collection of medical texts, for example, if you’re trying to finetune the model to understand medical language. The data must be preprocessed and tokenized in a way that’s compatible with the model’s pretraining.</p>
</div>
<div class="readable-text intended-text" id="p90">
<p>The finetuning process involves training the model on your specific dataset, but with a twist: instead of learning from scratch, the model’s existing knowledge is adjusted to better fit the new data. This finetuning is typically done with a smaller learning rate than in the initial training phase to prevent the model from forgetting its previously learned knowledge. After finetuning, the model is evaluated on a separate dataset to ensure it can generalize to unseen data in the specific domain. Similar to training from scratch, this process may involve several iterations to optimize the model’s performance. Finetuning offers a way to harness the power of LLMs for specific tasks or domains without the need for extensive resources or computation time. See figure 5.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p91">
<img alt="figure" height="279" src="../Images/5-3.png" width="387"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.3</span> Finetuning differs from training from scratch in that you don’t have to define model behavior, you can use the exact same training loop, and you have a fraction of the data requirement.</h5>
</div>
<div class="readable-text" id="p92">
<p>In listing 5.2, we show you how to finetune a GPT model. Notice how much less code there is in this listing than in listing 5.1. We don’t need to define an architecture or a tokenizer; we’ll just use those from the original model. Essentially, we get to skip ahead because weights and embeddings have already been defined.</p>
</div>
<div class="browsable-container listing-container" id="p93">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.2</span> An example of finetuning </h5>
<div class="code-area-container">
<pre class="code-area">import os
from transformers import (
    GPT2Tokenizer,
    GPT2LMHeadModel,
    GPT2Config,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)
from datasets import load_dataset

dataset = load_dataset("text", data_files="./data/crimeandpunishment.txt") <span class="aframe-location"/> #1
dataset = dataset.filter(lambda sentence: len(sentence["text"]) &gt; 1)
print(dataset["train"][0])

model_dir = "./models/betterGPT/"   <span class="aframe-location"/> #2
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
config = GPT2Config(      <span class="aframe-location"/> #3
    vocab_size=50261,
    n_positions=256,
    n_embd=768,
    activation_function="gelu",
)

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")   <span class="aframe-location"/> #4
special_tokens_dict = {
    "bos_token": "&lt;BOS&gt;",
    "eos_token": "&lt;EOS&gt;",
    "pad_token": "&lt;PAD&gt;",
    "mask_token": "&lt;MASK&gt;",
}
tokenizer.add_special_tokens(special_tokens_dict)

model = GPT2LMHeadModel.from_pretrained(                <span class="aframe-location"/> #5
    "gpt2", config=config, ignore_mismatched_sizes=True
)


def tokenize(batch):    <span class="aframe-location"/> #6
    return tokenizer(
        str(batch), padding="max_length", truncation=True, max_length=256
    )


tokenized_dataset = dataset.map(tokenize, batched=False)   <span class="aframe-location"/> #7
print(f"Tokenized: {tokenized_dataset['train'][0]}")

data_collator = DataCollatorForLanguageModeling(      <span class="aframe-location"/> #8
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)  # Masked Language Modeling - adds &lt;MASK&gt; tokens to guess the words

train_args = TrainingArguments(     <span class="aframe-location"/> #9
    output_dir=model_dir,
    num_train_epochs=1,
    per_device_train_batch_size=8,
    save_steps=5000,
    save_total_limit=2,
    report_to="none",
)

trainer = Trainer(     <span class="aframe-location"/> #10
    model=model,
    args=train_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
)

trainer.train()               <span class="aframe-location"/> #11
trainer.save_model(model_dir)
tokenizer.save_pretrained()

model = GPT2LMHeadModel.from_pretrained(model_dir)  <span class="aframe-location"/> #12
input = "To be or not"                                   <span class="aframe-location"/> #13
tokenized_inputs = tokenizer(input, return_tensors="pt")
out = model.generate(
    input_ids=tokenized_inputs["input_ids"],
    attention_mask=tokenized_inputs["attention_mask"],
    max_length=256,
    num_beams=5,
    temperature=0.7,
    top_k=50,
    top_p=0.90,
    no_repeat_ngram_size=2,
)
print(tokenizer.decode(out[0], skip_special_tokens=True))</pre>
<div class="code-annotations-overlay-container">
     #1 Loads and formats the dataset
     <br/>#2 Creates model directory to save to
     <br/>#3 Establishes our GPT-2 parameters (different from the paper and scratchGPT)
     <br/>#4 Instantiates our tokenizer and our special tokens
     <br/>#5 Instantiates our model from the config
     <br/>#6 Creates a tokenize function
     <br/>#7 Tokenizes our whole dataset (so we never have to do it again)
     <br/>#8 Creates a data collator to format the data for training
     <br/>#9 Establishes training arguments
     <br/>#10 Instantiates the Trainer
     <br/>#11 Trains and saves the model
     <br/>#12 Loads the saved model
     <br/>#13 Tests the saved model
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p94">
<p>Looking at listing 5.2 compared with listing 5.1, they have almost the exact same architecture (minus the activation function), and they’re training on exactly the same data. Yet, there’s a marked improvement with the finetuned GPT-2 model due to the lack of learned representation in the first model. Our pretrained model, along with subword BPE tokenization instead of character-based, helps the model figure out which units of statistically determined meaning are most likely to go together. You’ll notice, though, that GPT-2, even with pretraining, struggles to generate relevant longer narratives despite using a newer, better activation function.</p>
</div>
<div class="readable-text" id="p95">
<h4 class="readable-text-h4 sigil_not_in_toc">Finetuning OpenAI</h4>
</div>
<div class="readable-text" id="p96">
<p>We just trained a GPT model from scratch, and then we finetuned GPT-2, but we know many readers really want the power behind OpenAI’s larger GPT models. Despite being proprietary models, OpenAI has graciously created an API where we can finetune GPT-3 models. Currently, three models are available for finetuning with OpenAI’s platform, but it looks like it intends to extend that finetuning ability to all of its models on offer. OpenAI has written a whole guide, which you can find at <a href="http://platform.openai.com/">http://platform.openai.com/</a>, but once you have your dataset prepared in the necessary format, the code is pretty easy. Here are some snippets for various tasks:</p>
</div>
<div class="browsable-container listing-container" id="p97">
<div class="code-area-container">
<pre class="code-area">import os
from openai import OpenAI

client = OpenAI()
client.api_key = os.getenv("OPENAI_API_KEY")
client.files.create(
  file=open("mydata.jsonl", "rb"),
  purpose='fine-tune'
)</pre>
</div>
</div>
<div class="readable-text" id="p98">
<p>This first snippet uploads a training dataset in the correct format for the platform and specifies the purpose as finetuning, but doesn’t start the process yet. Next, you’ll need to create the finetuning job:</p>
</div>
<div class="browsable-container listing-container" id="p99">
<div class="code-area-container">
<pre class="code-area">client.fine_tuning.jobs.create(training_file="file-abc123", model="gpt-3.5-turbo")</pre>
</div>
</div>
<div class="readable-text" id="p100">
<p>This is where you specify which training file and which model you want to finetune. Once OpenAI’s training loop has completed, you’ll see the finetuned model’s name populated when you retrieve the job details. Now you can use that model the same way you would have used any of the vanilla ones for chat completion or anything else like this:</p>
</div>
<div class="browsable-container listing-container" id="p101">
<div class="code-area-container">
<pre class="code-area">completion = client.chat.completion.create(
  model="ft:gpt-3.5-turbo:my-org:custom_suffix:id",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)
print(completion.choices[0].message)</pre>
</div>
</div>
<div class="readable-text" id="p102">
<p>And that’s it for finetuning an OpenAI model! Very simple, doesn’t take too long, and as of March 2023, your data is private to you. Of course, you’ll be ceding all of the control of how that finetuning occurs over to OpenAI. If you’d like to do something beyond vanilla finetuning, you’ll need to do that yourself. In just a minute, we’ll go over those techniques you may consider, along with some more advanced processes that can help with more fine-grained models and more complex tasks.</p>
</div>
<div class="readable-text" id="p103">
<h3 class="readable-text-h3" id="sigil_toc_id_86"><span class="num-string">5.2.3</span> Prompting</h3>
</div>
<div class="readable-text" id="p104">
<p>One of the main reasons why LLMs are so powerful compared to traditional ML is because we can train them at run time. Give them a set of instructions and watch them follow them to the best of their ability. This technique is called prompting and is used in LLMs to guide the model’s output. In essence, the prompt is the initial input given to the model that provides it with context or instructions for what it should do. For example, “translate the following English text to French” and “summarize the following article” are prompts. In the context of LLMs, prompting becomes even more critical, as these models are not explicitly programmed to perform specific tasks but learn to respond to a variety of tasks based on the given prompt.</p>
</div>
<div class="readable-text intended-text" id="p105">
<p>Prompt engineering refers to the process of crafting effective prompts to guide the model’s behavior. The aim is to create prompts that lead the model to provide the most desirable or useful output. Prompt engineering can be more complex than it appears, as slight changes in how a prompt is phrased can lead to vastly different responses from the model. Some strategies for prompt engineering include being more explicit in the prompt, providing an example of the desired output, or rephrasing the prompt in different ways to get the best results. It’s a mixture of art and science, requiring a good understanding of the model’s capabilities and limitations.</p>
</div>
<div class="readable-text intended-text" id="p106">
<p>In this chapter, we are going to focus mainly on training and finetuning, the steps before deployment, but we would be remiss if we didn’t first mention prompting. We will talk about prompting in much more depth in chapter 7.</p>
</div>
<div class="readable-text" id="p107">
<h2 class="readable-text-h2" id="sigil_toc_id_87"><span class="num-string">5.3</span> Advanced training techniques</h2>
</div>
<div class="readable-text" id="p108">
<p>Now that you know how to do the basics, let’s go over some more advanced techniques. These techniques have been developed for a variety of reasons, such as improving generated text outputs, shrinking the model, providing continuous learning, speeding up training, and reducing costs. Depending on the needs of your organization, you may need to reach for a different training solution. While not a comprehensive list, the following techniques are often used and should be valuable tools as you prepare a production-ready model.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p109">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Classical ML training background </h5>
</div>
<div class="readable-text" id="p110">
<p>Going over some techniques to enhance your finetuning process requires a bit of background. We won’t be doing a full course in ML; however, in case this is your first exposure, you should know some classic learning paradigms that experiments tend to follow—supervised, unsupervised, adversarial, and reinforcement:</p>
</div>
<ul>
<li class="readable-text" id="p111"> Supervised learning involves collecting both the data to train on and the labels showcasing the expected output. </li>
<li class="readable-text" id="p112"> Unsupervised learning does not require labels, as the data is probed for similarity and grouped into clusters that are the closest comparison to each other. </li>
<li class="readable-text" id="p113"> Adversarial learning is what’s used to train a generative adversarial network. It involves two models, generally referred to as the Critic model and the Forger model. These two models essentially play a game against each other where the forger tries to copy some ideal output, and the critic tries to determine whether the forgery is the real thing. </li>
<li class="readable-text" id="p114"> Reinforcement learning (RL) opts for establishing a reward function instead of having predefined labels for the model to learn from. By measuring the model’s actions, it is given a reward based on that function instead.  </li>
</ul>
<div class="readable-text" id="p115">
<p>All LLMs must be trained using at least one of these, and they perform at a high level with all of them done correctly. The training techniques discussed in this chapter differ from those basic ones, ranging from adding some form of human input to the model to comparing outputs to changing how the model does matrix multiplication.</p>
</div>
</div>
<div class="readable-text" id="p116">
<h3 class="readable-text-h3" id="sigil_toc_id_88"><span class="num-string">5.3.1</span> Prompt tuning</h3>
</div>
<div class="readable-text" id="p117">
<p>We’ve gone over pragmatics before, but as a reminder, language models perform better when given real-world nonsemantic context pertaining to the tasks and expectations. Language modeling techniques all operate on the underlying assumption that the LM, given inputs and expected outputs, can divine the task to be done and do it in the best way within the number of parameters specified. </p>
</div>
<div class="readable-text intended-text" id="p118">
<p>While the idea of the model inferring both the task and the method of completing it from the data showed promise, it has been shown time and time again, from BERT to every T5 model and now to all LLMs, that providing your model with the expected task and relevant information for solving the task improves model performance drastically. As early as 2021, Google Research, DeepMind, and OpenAI had all published papers about prompt tuning, or giving a model pragmatic context during training. The benefits of prompt tuning are reducing the amount of data required for the model to converge during training and, even cooler, the ability to reuse a completely frozen language model for new tasks without retraining or fully finetuning. </p>
</div>
<div class="readable-text intended-text" id="p119">
<p>Because LLMs are so large (and getting larger), it is becoming increasingly difficult to share them and even more difficult to guarantee their performance on a given task, even one they are trained on. Prompt tuning can help nudge the model in the right direction without becoming a significant cost. Figure 5.4 shows this process.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p120">
<img alt="figure" height="302" src="../Images/5-4.png" width="619"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.4</span> Prompt tuning foregoes most finetuning to allow the majority of the foundation model’s language understanding ability to stay exactly the same and, instead, focuses on changing how the model responds to specific inputs.</h5>
</div>
<div class="readable-text" id="p121">
<p>Listing 5.3 shows how to prompt tune a smaller variant of the BLOOMZ model from Big Science. BLOOMZ was released as an early competitor in the LLM space but has ultimately struggled to garner attention or momentum in the community because of its inability to generate preferred outputs despite its mathematical soundness. Because prompt tuning doesn’t add much to the regular finetuning structure we used in listing 5.2, we’ll perform Parameter-Efficient Fine-Tuning (PEFT), which drastically reduces the memory requirements by determining which model parameters need changing the most.</p>
</div>
<div class="browsable-container listing-container" id="p122">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.3</span> An example of prompt tuning </h5>
<div class="code-area-container">
<pre class="code-area">import os
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    default_data_collator,
    get_linear_schedule_with_warmup,
)
from peft import (
    get_peft_model,
    PromptTuningInit,
    PromptTuningConfig,
    TaskType,
)
import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm


def preprocess_function(examples):         <span class="aframe-location"/> #1
    batch_size = len(examples[text_column])
    inputs = [
        f"{text_column} : {x} Label : " for x in examples[text_column]
    ]
    targets = [str(x) for x in examples[label_column]]
    model_inputs = tokenizer(inputs)
    labels = tokenizer(targets)

    for i in range(batch_size):
        sample_input_ids = model_inputs["input_ids"][i]
        label_input_ids = labels["input_ids"][i] + [tokenizer.pad_token_id]
        model_inputs["input_ids"][i] = sample_input_ids + label_input_ids
        labels["input_ids"][i] = [-100] * len(
            sample_input_ids
        ) + label_input_ids
        model_inputs["attention_mask"][i] = [1] * len(
            model_inputs["input_ids"][i]
        )
    for i in range(batch_size):
        sample_input_ids = model_inputs["input_ids"][i]
        label_input_ids = labels["input_ids"][i]
        model_inputs["input_ids"][i] = [tokenizer.pad_token_id] * (
            max_length - len(sample_input_ids)
        ) + sample_input_ids
        model_inputs["attention_mask"][i] = [0] * (
            max_length - len(sample_input_ids)
        ) + model_inputs["attention_mask"][i]
        labels["input_ids"][i] = [-100] * (
            max_length - len(sample_input_ids)
        ) + label_input_ids
        model_inputs["input_ids"][i] = torch.tensor(
            model_inputs["input_ids"][i][:max_length]
        )
        model_inputs["attention_mask"][i] = torch.tensor(
            model_inputs["attention_mask"][i][:max_length]
        )
        labels["input_ids"][i] = torch.tensor(
            labels["input_ids"][i][:max_length]
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


if __name__ == "__main__":        <span class="aframe-location"/> #2
    # Define training parameters
    device = "cuda"
    model_name_or_path = "bigscience/bloomz-560m"
    tokenizer_name_or_path = "bigscience/bloomz-560m"
    dataset_name = "twitter_complaints"
    text_column = "Tweet text"
    label_column = "text_label"
    max_length = 64
    lr = 3e-2
    num_epochs = 1
    batch_size = 8

    peft_config = PromptTuningConfig(     <span class="aframe-location"/> #3
        task_type=TaskType.CAUSAL_LM,
        prompt_tuning_init=PromptTuningInit.TEXT,
        num_virtual_tokens=8,
        prompt_tuning_init_text="Classify if the tweet "
        "is a complaint or not:",
        tokenizer_name_or_path=model_name_or_path,
    )
    checkpoint_name = (
        f"{dataset_name}_{model_name_or_path}"
        f"_{peft_config.peft_type}_{peft_config.task_type}_v1.pt".replace(
            "/", "_"
        )
    )
    dataset = load_dataset("ought/raft", dataset_name)   <span class="aframe-location"/> #4
    print(f"Dataset 1: {dataset['train'][0]}")

    classes = [         <span class="aframe-location"/> #5
        label.replace("_", " ")
        for label in dataset["train"].features["Label"].names
    ]
    dataset = dataset.map(
        lambda x: {"text_label": [classes[label] for label in x["Label"]]},
        batched=True,
        num_proc=1,
    )
    print(f"Dataset 2: {dataset['train'][0]}")

    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)   <span class="aframe-location"/> #6
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    target_max_length = max(
        [
            len(tokenizer(class_label)["input_ids"])
            for class_label in classes
        ]
    )
    print(f"Target Max Length: {target_max_length}")

    processed_datasets = dataset.map(          <span class="aframe-location"/> #7
        preprocess_function,
        batched=True,
        num_proc=1,
        remove_columns=dataset["train"].column_names,
        load_from_cache_file=False,
        desc="Running tokenizer on dataset",
    )

    train_dataset = processed_datasets["train"]    <span class="aframe-location"/> #8
    eval_dataset = processed_datasets["test"]

    train_dataloader = DataLoader(
        train_dataset,
        shuffle=True,
        collate_fn=default_data_collator,
        batch_size=batch_size,
        pin_memory=True,
    )
    eval_dataloader = DataLoader(
        eval_dataset,
        collate_fn=default_data_collator,
        batch_size=batch_size,
        pin_memory=True,
    )
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)  <span class="aframe-location"/> #9
    model = get_peft_model(model, peft_config)
    print(model.print_trainable_parameters())
    model = model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)   <span class="aframe-location"/> #10
    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=(len(train_dataloader) * num_epochs),
    )

    for epoch in range(num_epochs):   <span class="aframe-location"/> #11
        model.train()
        total_loss = 0
        for step, batch in enumerate(tqdm(train_dataloader)):
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_loss += loss.detach().float()
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        model.eval()
        eval_loss = 0
        eval_preds = []
        for step, batch in enumerate(tqdm(eval_dataloader)):
            batch = {k: v.to(device) for k, v in batch.items()}
            with torch.no_grad():
                outputs = model(**batch)
            loss = outputs.loss
            eval_loss += loss.detach().float()
            eval_preds.extend(
                tokenizer.batch_decode(
                    torch.argmax(outputs.logits, -1).detach().cpu().numpy(),
                    skip_special_tokens=True,
                )
            )

        eval_epoch_loss = eval_loss / len(eval_dataloader)
        eval_ppl = torch.exp(eval_epoch_loss)
        train_epoch_loss = total_loss / len(train_dataloader)
        train_ppl = torch.exp(train_epoch_loss)
        print(
            f"{epoch=}: {train_ppl=} {train_epoch_loss=} "
            f"{eval_ppl=} {eval_epoch_loss=}"
        )

    model_dir = "./models/PromptTunedPEFT"   <span class="aframe-location"/> #12
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    tokenizer.save_pretrained(model_dir)    <span class="aframe-location"/> #13
    model.save_pretrained(model_dir)

    with torch.no_grad():     <span class="aframe-location"/> #14
        inputs = tokenizer(
            f'{text_column} : {{"@nationalgridus I have no water and '
            "the bill is current and paid. Can you do something about "
            'this?"}} Label : ',
            return_tensors="pt",
        )

        inputs = {k: v.to(device) for k, v in inputs.items()}
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=10,
            eos_token_id=3,
        )
        print(
            tokenizer.batch_decode(
                outputs.detach().cpu().numpy(), skip_special_tokens=True
            )
        )</pre>
<div class="code-annotations-overlay-container">
     #1 Helper function to preprocess text; go ahead and skip to the training
     <br/>#2 Model prompt tuning
     <br/>#3 Δefines prompt tuning config; notice init_text
     <br/>#4 Loads Δataset
     <br/>#5 Labels the dataset
     <br/>#6 Loads tokenizer
     <br/>#7 Runs Tokenizer across dataset and preprocess
     <br/>#8 Prepares data loaders
     <br/>#9 Loads foundation model
     <br/>#10 Δefines optimizer
     <br/>#11 Training steps
     <br/>#12 Creates model directory to save to
     <br/>#13 Saving
     <br/>#14 Inference
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p123">
<p>Other than the changed setup, the main difference between listings 5.2 and 5.3 is simply prepending a prompt with some sort of instruction to the beginning of each input, reminiscent of the T5 training method that pioneered having a prepended task string before every input. Prompt tuning has emerged as a powerful technique for finetuning large language models to specific tasks and domains. By tailoring prompts to the desired output and optimizing them for improved performance, we can make our models more versatile and effective. However, as our LLMs continue to grow in scale and complexity, it becomes increasingly challenging to efficiently finetune them on specific tasks. This is where knowledge distillation comes into play, offering a logical next step. Knowledge distillation allows us to transfer the knowledge and expertise of these highly tuned models to smaller, more practical versions, enabling a wider range of applications and deployment scenarios. Together, prompt tuning and knowledge distillation form a dynamic duo in the arsenal of techniques for harnessing the full potential of modern LLMs.</p>
</div>
<div class="readable-text" id="p124">
<h3 class="readable-text-h3" id="sigil_toc_id_89"><span class="num-string">5.3.2</span> Finetuning with knowledge distillation</h3>
</div>
<div class="readable-text" id="p125">
<p>Knowledge distillation is an advanced technique that provides a more efficient path to finetuning an LLM. Rather than just finetuning an LLM directly, knowledge distillation involves transferring the knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student). The aim is to create a more compact model that retains the performance characteristics of the larger model but is more efficient in terms of resource usage. Figure 5.5 shows this process.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p126">
<img alt="figure" height="319" src="../Images/5-5.png" width="739"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.5</span> Knowledge distillation allows a smaller model to learn from a foundation model to replicate similar behavior with fewer parameters. The student model does not always learn the emergent qualities of the foundation model, so the dataset must be especially curated. The dotted line indicates a special relationship as the student model becomes the specialized LLM.</h5>
</div>
<div class="readable-text" id="p127">
<p>The first step in knowledge distillation is to select a pre-trained LLM as the teacher model. This could be any of the large models, such as Llama 2 70B or Falcon 180B, which have been trained on vast amounts of data. You also need to create or select a smaller model as the student. The student model might have a similar architecture to the teacher’s, but with fewer layers or reduced dimensionality to make it smaller and faster.</p>
</div>
<div class="readable-text intended-text" id="p128">
<p>Next, the student model is trained on the same task as the teacher model. However, instead of learning from the raw data directly, the student model learns to mimic the teacher model’s outputs. This training is typically done by adding a term to the loss function that encourages the student model’s predictions to be similar to the teacher model’s predictions. Thus, the student model not only learns from the task-specific labels but also benefits from the rich representations learned by the teacher model.</p>
</div>
<div class="readable-text intended-text" id="p129">
<p>Once the distillation process is complete, you’ll have a compact student model that can handle the specific tasks learned from the teacher model but at a fraction of the size and computational cost. The distilled model can then be further finetuned on a specific task or dataset if required. Through knowledge distillation, you can use the power of LLMs in situations where computational resources or response time are limited.</p>
</div>
<div class="readable-text intended-text" id="p130">
<p>In listing 5.4, we show how to perform finetuning with knowledge distillation using BERT and becoming DistilBERT. As opposed to regular finetuning, pay attention to the size and performance of the model. Both will drop; however, size will drop much faster than performance.</p>
</div>
<div class="browsable-container listing-container" id="p131">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.4</span> An example of knowledge distillation </h5>
<div class="code-area-container">
<pre class="code-area">import os
from transformers import (
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
)
from datasets import load_dataset, load_metric

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


def process(examples):
    tokenized_inputs = tokenizer(
        examples["sentence"], truncation=True, max_length=256
    )
    return tokenized_inputs


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_metric.compute(
        predictions=predictions, references=labels
    )
    return {
        "accuracy": acc["accuracy"],
    }


class DistillationTrainingArguments(TrainingArguments):
    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.alpha = alpha
        self.temperature = temperature


class DistillationTrainer(Trainer):
    def __init__(self, *args, teacher_model=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher = teacher_model
        self._move_model_to_device(self.teacher, self.model.device)  <span class="aframe-location"/> #1
        self.teacher.eval()

    def compute_loss(self, model, inputs, return_outputs=False):
        outputs_student = model(**inputs)     <span class="aframe-location"/> #2
        student_loss = outputs_student.loss
        with torch.no_grad():                        <span class="aframe-location"/> #3
            outputs_teacher = self.teacher(**inputs)

        assert (                 <span class="aframe-location"/> #4
            outputs_student.logits.size() == outputs_teacher.logits.size()
        )

        # Soften probabilities and compute distillation loss
        loss_function = nn.KLDivLoss(reduction="batchmean")
        loss_logits = loss_function(
            F.log_softmax(
                outputs_student.logits / self.args.temperature, dim=-1
            ),
            F.softmax(
                outputs_teacher.logits / self.args.temperature, dim=-1
            ),
        ) * (self.args.temperature**2)
        loss = (                              <span class="aframe-location"/> #5
            self.args.alpha * student_loss
            + (1.0 - self.args.alpha) * loss_logits
        )
        return (loss, outputs_student) if return_outputs else loss


if __name__ == "__main__":
    model_dir = "./models/KDGPT/"         <span class="aframe-location"/> #6
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    student_id = "gpt2"           <span class="aframe-location"/> #7
    teacher_id = "gpt2-medium"

    teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_id)
    student_tokenizer = AutoTokenizer.from_pretrained(student_id)

    sample = "Here's our sanity check."

    assert teacher_tokenizer(sample) == student_tokenizer(sample), (
        "Tokenizers need to have the same output! "
        f"{teacher_tokenizer(sample)} != {student_tokenizer(sample)}"
    )
    del teacher_tokenizer
    del student_tokenizer

    tokenizer = AutoTokenizer.from_pretrained(teacher_id)
    tokenizer.add_special_tokens({"pad_token": "[PAD]"})

    dataset_id = "glue"
    dataset_config = "sst2"
    dataset = load_dataset(dataset_id, dataset_config)

    tokenized_dataset = dataset.map(process, batched=True)
    tokenized_dataset = tokenized_dataset.rename_column("label", "labels")

    print(tokenized_dataset["test"].features)

    labels = tokenized_dataset["train"].features["labels"].names   <span class="aframe-location"/> #8
    num_labels = len(labels)
    label2id, id2label = dict(), dict()
    for i, label in enumerate(labels):
        label2id[label] = str(i)
        id2label[str(i)] = label

    training_args = DistillationTrainingArguments(   <span class="aframe-location"/> #9
        output_dir=model_dir,
        num_train_epochs=1,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        fp16=True,
        learning_rate=6e-5,
        seed=8855,
        Evaluation strategies
        evaluation_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        report_to="none",
        push_to_hub=False,      <span class="aframe-location"/> #10
        alpha=0.5,           <span class="aframe-location"/> #11
        temperature=4.0,
    )

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)   <span class="aframe-location"/> #12

    teacher_model = AutoModelForSequenceClassification.from_pretrained(  <span class="aframe-location"/> #13
        teacher_id,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id,
    )

    student_model = AutoModelForSequenceClassification.from_pretrained(  <span class="aframe-location"/> #14
        student_id,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id,
    )
    accuracy_metric = load_metric("accuracy")   <span class="aframe-location"/> #15

    trainer = DistillationTrainer(
        student_model,
        training_args,
        teacher_model=teacher_model,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )
    trainer.train()

    trainer.save_model(model_dir)</pre>
<div class="code-annotations-overlay-container">
     #1 Place teacher on same device as student
     <br/>#2 Computes student output
     <br/>#3 Computes teacher output
     <br/>#4 Asserts size
     <br/>#5 Returns weighted student loss
     <br/>#6 Creates model directory to save to
     <br/>#7 Δefines the teacher and student models
     <br/>#8 Creates label2id, id2label dicts for nice outputs for the model
     <br/>#9 Δefines training args
     <br/>#10 Pushes to hub parameters
     <br/>#11 Δistillation parameters
     <br/>#12 Δefines data_collator
     <br/>#13 Δefines model
     <br/>#14 Δefines student model
     <br/>#15 Δefines metrics and metrics function
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p132">
<p>Knowledge distillation, as exemplified by the provided <code>compute_loss</code> method, is a technique that enables the transfer of valuable insights from a teacher model to a more lightweight student model. In this process, the teacher model provides soft targets, offering probability distributions over possible outputs, which are then utilized to train the student model. The critical aspect of knowledge distillation lies in the alignment of these distributions, ensuring that the student model not only learns to mimic the teacher’s predictions but also gains a deeper understanding of the underlying data. This approach helps improve the student’s generalization capabilities and performance on various tasks, ultimately making it more efficient and adaptable.</p>
</div>
<div class="readable-text intended-text" id="p133">
<p>As we look forward, one logical progression beyond knowledge distillation is the incorporation of RLHF. While knowledge distillation enhances a model’s ability to make predictions based on existing data, RLHF allows the model to learn directly from user interactions and feedback. This dynamic combination not only refines the model’s performance further but also enables it to adapt and improve continuously. By incorporating human feedback, RL can help the model adapt to real-world scenarios, evolving its decision-making processes based on ongoing input, making it an exciting and natural evolution in the development of LLM systems.</p>
</div>
<div class="readable-text" id="p134">
<h3 class="readable-text-h3" id="sigil_toc_id_90"><span class="num-string">5.3.3</span> Reinforcement learning with human feedback</h3>
</div>
<div class="readable-text" id="p135">
<p>RLHF is a newer training technique developed to overcome one of the biggest challenges when it comes to RL: how to create reward systems that actually work. It sounds easy, but anyone who’s played around with RL knows how difficult it can be. Before AlphaStar, one author was building his own RL bot to play <em>StarCraft</em>, a war simulation game in space. </p>
</div>
<div class="readable-text print-book-callout" id="p136">
<p><span class="print-book-callout-head">NOTE</span>  Check out <a href="https://mng.bz/Dp4a">https://mng.bz/Dp4a</a> to learn more about AlphaStar.</p>
</div>
<div class="readable-text" id="p137">
<p>A simple reward system based on winning or losing was taking too long, so he decided to give it some reasonable intermediate rewards based on growing an army. However, this got blocked when it failed to build Pylons, a building required to increase army supply limits. So he gave it a reward to build Pylons. His bot quickly learned that it liked to build Pylons—so much so that it learned to almost win but not win, crippling its opponent so that it could keep building Pylons unharassed and for as long as it wanted.</p>
</div>
<div class="readable-text intended-text" id="p138">
<p>With a task like winning a game, even if it’s difficult, we can usually still come up with reasonable reward systems. But what about more abstract tasks, like teaching a robot how to do a backflip? These tasks get really difficult to design reward systems for, which is where RLHF comes in. What if instead of designing a system, we simply have a human make suggestions? A human knows what a backflip is, after all. The human will act like a tutor, picking attempts it likes more as the bot is training. That’s what RLHF is, and it works really well. Applied to LLMs, a human simply looks at generated responses to a prompt and picks which one they like more. See figure 5.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p139">
<img alt="figure" height="352" src="../Images/5-6.png" width="657"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.6</span> RLHF substitutes a loss function for a reward model and proximal policy optimization (PPO), allowing the model a much higher ceiling for learning trends within the data, including what is preferred as an output instead of what completes the task.</h5>
</div>
<div class="readable-text" id="p140">
<p>While very powerful, RLHF likely won’t stick around for very long. The reason is that it is incredibly computationally expensive for a result that is only incrementally better, especially a result that can be achieved and matched by higher-quality datasets with supervised learning approaches.</p>
</div>
<div class="readable-text intended-text" id="p141">
<p>There are some other problems with RLHF, such as that it requires hiring domain experts to evaluate and provide the human feedback. Not only can this get expensive, but it can also lead to privacy concerns since these reviewers would need to look at actual traffic and user interactions to grade them. To combat both of these concerns, you could try to outsource this directly to the users, asking for their feedback, but it may end up poisoning your data if your users have ill intent or are simply not experts in the subject matter, in which case they might upvote responses they like but that aren’t actually correct. This gets to the next problem: even experts have biases. RLHF doesn’t train a model to be more accurate or factually correct; it trains the model to generate human-acceptable answers.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>In production, RLHF has the advantage of allowing you to easily update your model on a continual basis. However, this is a two-edged sword, as it also increases the likelihood of your model degrading over time. OpenAI uses RLHF heavily, and it has led to many users complaining about their models, like GPT-4, becoming terrible in certain domains compared to when it first came out. One Stanford study found that GPT-4, when asked if a number was prime, used to get it right 98% of the time in March 2023, but three months later, in June 2023, it would only get it right 2% of the time.<a href="#footnote-245"><sup class="footnote-reference" id="footnote-source-1">1</sup></a> One reason is that the June model is much less verbose, opting to give a simple yes or no response. Humans like these responses. Getting straight to the point is often better, but LLMs tend to be better after they have had time to reason through the answer with techniques like chain of thought.</p>
</div>
<div class="readable-text intended-text" id="p143">
<p>With this in mind, RLHF is fantastic for applications where human-acceptable answers are the golden standard, and factually correct answers are less important—for example, a friendly chatbot or improving summarization tasks. These problems are intuitively syntactic in nature, essentially tasks that LLMs are already good at but which you want to refine by possibly creating a certain tone or personality.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>Another reason for RLHF degradation is due to data leakage. Data leakage is when your model is trained on the test or validation dataset you use to evaluate it. When this happens, you are essentially allowing the model to cheat, leading to overfitting and poor generalization. It’s just like how LeetCode interview questions lead tech companies to hire programmers who have lots of experience solving toy problems but don’t know how to make money or do their job.</p>
</div>
<div class="readable-text intended-text" id="p145">
<p>How does this happen? Well, simply. When you are running an LLM in production with RLHF, you know it’s going to degrade over time, so it’s best to run periodic evaluations to monitor the system. The more you run these evaluations, the more likely that one of the prompts will be picked up for human feedback and subsequent RL training. It could also happen by pure coincidence if your users happen to ask a question similar to a prompt in your evaluation dataset. Either way, without restrictions placed on RLHF (which generally are never done), it’s a self-defeating system.</p>
</div>
<div class="readable-text intended-text" id="p146">
<p>The really annoying aspect of continual updates through RLHF is that these updates ruin downstream engineering efforts, methods like prompting or retrieval-augmented generation (RAG). Engineering teams can take a lot of effort to dial in a process or procedure to query a model and then clean up responses, but all that work can easily be undermined if the underlying model is changing. As a result, many teams prefer a static model with periodic updates to one with continual updates.</p>
</div>
<div class="readable-text intended-text" id="p147">
<p>All that said, RLHF is still a powerful technique that may yield greater results later as it is optimized and refined. Also, it’s just really cool. We don’t recommend using RLHF, and we don’t have the space here to delve deeper; just know that it is a tool used by companies specializing in LLMs. For readers who want to understand RLHF better, we have included an in-depth example and code listing in appendix B.</p>
</div>
<div class="readable-text" id="p148">
<h3 class="readable-text-h3" id="sigil_toc_id_91"><span class="num-string">5.3.4</span> Mixture of experts</h3>
</div>
<div class="readable-text" id="p149">
<p>A mixture of experts (MoE) is functionally the same as any other model for training but contains a trick under the hood: sparsity. This gives the advantage of being able to train a bunch of models on a diverse set of data and tasks at once. You see, a MoE is exactly what it sounds like: an ensemble of identical models in the beginning. You can think of them as a group of freshman undergrads. Then, using some unsupervised grouping methods, such as k-means clustering, each of these experts “picks a major” during training. This allows the model only to activate some experts to answer particular inputs instead of all of them, or maybe the input is complex enough that it requires activating all of them. The point is that once training has completed, if it has been done on a representative-enough dataset, each of your experts will have a college degree in the major that they studied. Because the homogeneity of inputs is determined mathematically, those majors won’t always have a name that correlates to something you would major in at school, but we like to think of these as eccentric double minors or something of the sort. Maybe one of your experts majored in physics but double minored in advertising and Africana studies. It doesn’t really matter, but the major upside to designing an ensemble of models in this way is that you can effectively reduce computational requirements immensely while retaining specialization and training memory by only consulting the experts whose knowledge correlates with the tokenized input at inference time.</p>
</div>
<div class="readable-text intended-text" id="p150">
<p>In listing 5.5, we finetune a MoE model in much the same way as we did in listing 5.2 with GPT-2, thanks to Hugging Face’s API and Google’s Switch Transformer. Unlike the method we described in chapter 3, where we turned a feed-forward network into an MoE, we’ll start with an already created MoE and train it on our own dataset. Training an MoE is pretty simple now, unlike when they first came out. Very smart people performed so much engineering that we can give an oversimplified explanation of these models. Google created the Switch Transformer to combat two huge problems they had run into while trying to train LLMs: size and instability. Google engineers simplified the routing algorithm (how the model decides which experts to query for each input) and showed how to train models with lower quantizations (in this case, bfloat16) for the first time—quite an amazing feat and not one to take lightly, as GPT-4 is likely an MoE.</p>
</div>
<div class="browsable-container listing-container" id="p151">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.5</span> Example mixture of experts finetuning </h5>
<div class="code-area-container">
<pre class="code-area">import os
from transformers import (
    AutoTokenizer,
    SwitchTransformersForConditionalGeneration,
    SwitchTransformersConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset
import torch

dataset = load_dataset("text", data_files="./data/crimeandpunishment.txt") <span class="aframe-location"/> #1
dataset = dataset.filter(lambda sentence: len(sentence["text"]) &gt; 1)
print(f"Dataset 1: {dataset['train'][0]}")

model_dir = "./models/MoE/"         <span class="aframe-location"/> #2
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

tokenizer = AutoTokenizer.from_pretrained("google/switch-base-8")   <span class="aframe-location"/> #3

config = SwitchTransformersConfig(                  <span class="aframe-location"/> #4
    decoder_start_token_id=tokenizer.pad_token_id
)

model = SwitchTransformersForConditionalGeneration.from_pretrained(  <span class="aframe-location"/> #5
    "google/switch-base-8",
    config=config,
    device_map="auto",
    torch_dtype=torch.float16,
)


def tokenize(batch):    <span class="aframe-location"/> #6
    return tokenizer(
        str(batch), padding="max_length", truncation=True, max_length=256
    )


tokenized_dataset = dataset.map(tokenize, batched=False)       <span class="aframe-location"/> #7
print(f"Tokenized: {tokenized_dataset['train'][0]}")

data_collator = DataCollatorForLanguageModeling(           <span class="aframe-location"/> #8
    tokenizer=tokenizer, mlm=False, mlm_probability=0.0
)  # Causal Language Modeling - Does not use mask


train_args = TrainingArguments(   <span class="aframe-location"/> #9
    output_dir=model_dir,
    num_train_epochs=1,
    per_device_train_batch_size=8,
    save_steps=5000,
    save_total_limit=2,
    report_to="none",
)

trainer = Trainer(         <span class="aframe-location"/> #10
    model=model,
    args=train_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
)

trainer.train()                 <span class="aframe-location"/> #11
trainer.save_model(model_dir)
tokenizer.save_pretrained(model_dir)

model = SwitchTransformersForConditionalGeneration.from_pretrained(  <span class="aframe-location"/> #12
    model_dir,
    device_map="auto",
    torch_dtype=torch.float16,
)

input = "To be or not &lt;extra_id_0&gt; &lt;extra_id_0&gt;"      <span class="aframe-location"/> #13
tokenized_inputs = tokenizer(input, return_tensors="pt")
out = model.generate(
    input_ids=tokenized_inputs["input_ids"].to("cuda"),
    attention_mask=tokenized_inputs["attention_mask"],
    max_length=256,
    num_beams=5,
    temperature=0.7,
    top_k=50,
    top_p=0.90,
    no_repeat_ngram_size=2,
)
print(f"To be or not {tokenizer.decode(out[0], skip_special_tokens=True)}")</pre>
<div class="code-annotations-overlay-container">
     #1 Loads and formats the dataset
     <br/>#2 Creates model directory to save to
     <br/>#3 Instantiates our tokenizer
     <br/>#4 Establishes our SwitchTransformers config
     <br/>#5 Instantiates our model from the config
     <br/>#6 Creates a tokenize function
     <br/>#7 Tokenizes our whole dataset (so we never have to do it again)
     <br/>#8 Creates a data collator to format the data for training
     <br/>#9 Establishes training arguments
     <br/>#10 Instantiates the trainer
     <br/>#11 Trains and saves the model
     <br/>#12 Loads the saved model
     <br/>#13 Tests the saved model
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p152">
<p>In this script, an MoE model is finetuned using the Switch Transformer foundation model. MoE models are unique during finetuning because you typically update the task-specific parameters, such as the gating mechanism and the parameters of the experts, while keeping the shared parameters intact. This allows the MoE to use the expertise of the different experts for better task-specific performance. Finetuning MoE models differs from traditional finetuning because it requires handling the experts and gating mechanisms, which can be more complex than regular neural network architectures. In our case, we’re lucky that <code>trainer.train()</code> with the right config covers it for finetuning, and we can just bask in the work that Google did before us.</p>
</div>
<div class="readable-text intended-text" id="p153">
<p>A logical progression beyond MoE finetuning involves exploring Parameter-Efficient Fine-Tuning (PEFT) and low-rank adaptations (LoRA). PEFT aims to make the finetuning process more efficient by reducing the model’s size and computational demands, making it more suitable for resource-constrained scenarios. Techniques such as knowledge distillation, model pruning, quantization, and compression can be employed in PEFT to achieve this goal. In contrast, LoRA focuses on incorporating low-rank factorization methods into model architectures to reduce the number of parameters while maintaining or even enhancing model performance. These approaches are essential, as they enable the deployment of sophisticated models on devices with limited resources and in scenarios where computational efficiency is paramount.</p>
</div>
<div class="readable-text" id="p154">
<h3 class="readable-text-h3" id="sigil_toc_id_92"><span class="num-string">5.3.5</span> LoRA and PEFT</h3>
</div>
<div class="readable-text" id="p155">
<p>LoRA represents a significant breakthrough for machine learning in general. Taking advantage of a mathematical trick, LoRAs can change the output of a model without changing the original model weights or taking up significant space or cost, as shown in figure 5.7. The reason for the significance here is that it makes finetuning a separate model for many different tasks or domains much more feasible, as has already been seen in the diffusion space with text2image LoRAs popping up quite often for conditioning model output without significantly altering the base model’s abilities or style. Put simply, if you already like your model and would like to change it to do the exact same thing in a new domain without sacrificing what it was already good at on its own, an adapter might be the path for you, especially if you have multiple new domains that you don’t want bleeding into one another.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p156">
<img alt="figure" height="385" src="../Images/5-7.png" width="782"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.7</span> LoRA exemplifies the idea that you should only need to train and save the difference between where the foundation model is and where you want it to be. It does this through singular value decomposition (SVD).</h5>
</div>
<div class="readable-text" id="p157">
<p>To understand LoRAs, you need to first understand how models currently adjust weights. Since we aren’t going to go over a complete backpropagation tutorial here, we can abstract it as</p>
</div>
<div class="readable-text indented-paragraph equation-paragraph" id="p158">
<p>W = W + <span class="regular-symbol">Δ</span>W</p>
</div>
<div class="readable-text" id="p159">
<p>So if you have a model with 100 100-dimensional layers, your weights can be represented by a 100 × 100 matrix. The cool part comes with singular value decomposition (SVD), which has been used for compression by factoring a single matrix into three smaller matrices. We covered this topic in depth back in chapter 3 (see listing 3.2). So while we know the intuition for SVD with LLMs, what can we compress from that original formula?</p>
</div>
<div class="readable-text indented-paragraph equation-paragraph" id="p160">
<p><span class="regular-symbol">Δ</span>W = W<sub>a</sub> × W<sub>b</sub></p>
</div>
<div class="readable-text" id="p161">
<p>So if <span class="regular-symbol">Δ</span>W = 100 × 100, W<sub>a</sub> = 100 × c and W<sub>b</sub> = c × 100, where c &lt; 100. If c = 2, you can represent 10,000 elements using only 400 because when they’re multiplied together, they equal the 10,000 original elements. So the big question is, what does c equal for your task? The c-value is the “R” in LoRA, referring to the rank of the matrix of weights. There are algorithmic ways of determining that rank using eigenvectors and the like, but you can approximate a lot of it by knowing that a higher rank equals more complexity, meaning that the higher the number you use there, the closer you’ll get to original model accuracy, but the less memory you’ll save. If you think the task you’re finetuning the LoRA for isn’t as complex, reduce the rank.</p>
</div>
<div class="readable-text intended-text" id="p162">
<p>The next listing shows you how to combine creating a LoRA and then perform inference with both the LoRA and your base model.</p>
</div>
<div class="browsable-container listing-container" id="p163">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.6</span> Example LoRA and PEFT training </h5>
<div class="code-area-container">
<pre class="code-area">import os
from datasets import load_dataset
from transformers import (
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    TrainingArguments,
    Trainer,
)
from peft import (
    PeftModel,
    PeftConfig,
    get_peft_model,
    LoraConfig,
    TaskType,
)
import evaluate
import torch
import numpy as np

model_checkpoint = "meta-llama/Llama-2-7b-hf"
lr = 1e-3
batch_size = 16
num_epochs = 10

model_dir = "./models/LoRAPEFT"    <span class="aframe-location"/> #1
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

bionlp = load_dataset("tner/bionlp2004")

seqeval = evaluate.load("seqeval")

label_list = [
    "O",
    "B-DNA",
    "I-DNA",
    "B-protein",
    "I-protein",
    "B-cell_type",
    "I-cell_type",
    "B-cell_line",
    "I-cell_line",
    "B-RNA",
    "I-RNA",
]


def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(
        predictions=true_predictions, references=true_labels
    )
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }


tokenizer = AutoTokenizer.from_pretrained(
    model_checkpoint, add_prefix_space=True
)


def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    labels = []
    for i, label in enumerate(examples["tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


tokenized_bionlp = bionlp.map(tokenize_and_align_labels, batched=True)

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

id2label = {
    0: "O",
    1: "B-DNA",
    2: "I-DNA",
    3: "B-protein",
    4: "I-protein",
    5: "B-cell_type",
    6: "I-cell_type",
    7: "B-cell_line",
    8: "I-cell_line",
    9: "B-RNA",
    10: "I-RNA",
}
label2id = {
    "O": 0,
    "B-DNA": 1,
    "I-DNA": 2,
    "B-protein": 3,
    "I-protein": 4,
    "B-cell_type": 5,
    "I-cell_type": 6,
    "B-cell_line": 7,
    "I-cell_line": 8,
    "B-RNA": 9,
    "I-RNA": 10,
}

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint, num_labels=11, id2label=id2label, label2id=label2id
)

peft_config = LoraConfig(
    task_type=TaskType.TOKEN_CLS,
    inference_mode=False,
    r=16,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="all",
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
training_args = TrainingArguments(
    output_dir=model_dir,
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_bionlp["train"],
    eval_dataset=tokenized_bionlp["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

peft_model_id = "stevhliu/roberta-large-lora-token-classification"
config = PeftConfig.from_pretrained(model_dir)
inference_model = AutoModelForTokenClassification.from_pretrained(
    config.base_model_name_or_path,
    num_labels=11,
    id2label=id2label,
    label2id=label2id,
)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(inference_model, peft_model_id)

text = (
    "The activation of IL-2 gene expression and NF-kappa B through CD28 "
    "requires reactive oxygen production by 5-lipoxygenase."
)
inputs = tokenizer(text, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits
tokens = inputs.tokens()
predictions = torch.argmax(logits, dim=2)

for token, prediction in zip(tokens, predictions[0].numpy()):
    print((token, model.config.id2label[prediction]))</pre>
<div class="code-annotations-overlay-container">
     #1 Creates model directory to save to
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p164">
<p>Keep in mind that you still need to keep your base model, as shown in listing 5.6. The LoRA is run in addition to the foundation model; it sits on top and changes the weights at only the rank determined in the <code>LoraConfig</code> class (in this case, <code>16</code>). RoBERTa-Large was likely already decent at doing token classification on the bionlp dataset, but now, running with the LoRA on top, it’ll be even better. There are multiple types of LoRAs you can use, with QLoRA, QA-LoRA, and AWQ-LoRA all gaining popularity in different domains and tasks. With the transformers library, which can be controlled from the <code>LoraConfig</code>, we encourage you to experiment with different adaptation methods to find what works for your data and task.</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>The most attractive thing about LoRA is that the particular one we discussed here results in a file only 68 KB in size on disk and still has a significant performance boost. You could create LoRAs for each portion of your company that wants a model, one for the legal team that’s siloed so it doesn’t have to worry about any private data it is putting into it, one for your engineering team to help with code completion and answering questions about which data structures or algorithms to use, and one for anyone else. Because they’re so small, it’s suddenly much more feasible to store than the 1.45 GB (14.5 GB if we use Llama in fp16; it’s 28 GB in fp32) RoBERTa-Large model being finetuned a bunch of times. In the spirit of giving you more of these time- and space-saving tips, we’ll go over some things that aren’t mentioned anywhere else, but you may still get some use out of if the data science part of LLMs is what you are working with.</p>
</div>
<div class="readable-text" id="p166">
<h2 class="readable-text-h2" id="sigil_toc_id_93"><span class="num-string">5.4</span> Training tips and tricks</h2>
</div>
<div class="readable-text" id="p167">
<p>While this book isn’t focused on training and researching new models, we feel kind of bad telling you that finetuning models is an effective strategy for teaching LLMs correct guardrails based on your data and then just leaving you to figure out how to make it work on your own stuff. With this in mind, let’s look at some tried-and-true tips and tricks for both training and finetuning LLMs. These tips will help you with some of the least-intuitive parts of training LLMs that most practitioners (like us) had to learn the hard way.</p>
</div>
<div class="readable-text" id="p168">
<h3 class="readable-text-h3" id="sigil_toc_id_94"><span class="num-string">5.4.1</span> Training data size notes</h3>
</div>
<div class="readable-text" id="p169">
<p>First off, LLMs are notorious for overfitting. If you are considering training a foundation model, you need to consider the amount of data you have, which should be roughly 20× the number of parameters you’re trying to train.<a href="#footnote-246"><sup class="footnote-reference" id="footnote-source-2">2</sup></a> For example, if you’re training a 1B parameter model, you should train it on 20B tokens. If you have fewer tokens than that, you will run the risk of overfitting. </p>
</div>
<div class="readable-text intended-text" id="p170">
<p>If you already have a model and need to finetune it on your data, consider the inverse, where you should likely have ~0.000001× the number of tokens as a minimum (10K tokens for a 1B parameter model). We came up with this rule of thumb based on our experience, although it should be fairly intuitive. If you have fewer than 1/100,000 of your model parameters in tokens, finetuning likely won’t have much of an effect. In this case, you should consider another strategy that won’t cost as much, such as LoRA (which we just discussed), RAG (which we talk about in the next chapter), or a system that uses both.</p>
</div>
<div class="readable-text intended-text" id="p171">
<p>For both these examples, we’ve had the experience where a company we worked for hoped for great results with minimal data and was disappointed. One hoped to train an LLM from scratch with only ~1 million tokens while also disallowing open source datasets, and another wanted to finetune the model but only on a couple of hundred examples. Neither of these approaches were cost-efficient, nor did they create models that performed up to the standards the companies aimed for.</p>
</div>
<div class="readable-text" id="p172">
<h3 class="readable-text-h3" id="sigil_toc_id_95"><span class="num-string">5.4.2</span> Efficient training</h3>
</div>
<div class="readable-text" id="p173">
<p>We’ve so far focused on tools and methodologies for training, which should supercharge your ability to create the best and largest models your training system allows. However, other factors should be considered when setting up your training loops. In physics, the uncertainty principle shows that you can never perfectly know both the speed and position of a given particle. Machine learning’s uncertainty principle is that you can never perfectly optimize both your speed and your memory utilization. Improving speed comes at the cost of memory, and vice versa. Table 5.2 shows some choices you can make in training and their effects on speed and memory.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p174">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 5.2</span> Training choices to consider</h5>
<table>
<thead>
<tr>
<th>
<div>
         Method 
       </div></th>
<th>
<div>
         Improve speed 
       </div></th>
<th>
<div>
         Improves memory utilization 
       </div></th>
<th>
<div>
         Difficulty 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Batch size choice <br/></td>
<td>  Yes <br/></td>
<td>  Yes <br/></td>
<td>  Easy <br/></td>
</tr>
<tr>
<td>  Gradient accumulation <br/></td>
<td>  No <br/></td>
<td>  Yes <br/></td>
<td>  Medium <br/></td>
</tr>
<tr>
<td>  Gradient checkpointing <br/></td>
<td>  No <br/></td>
<td>  Yes <br/></td>
<td>  Medium <br/></td>
</tr>
<tr>
<td>  Mixed precision <br/></td>
<td>  Yes <br/></td>
<td>  No <br/></td>
<td>  Hard <br/></td>
</tr>
<tr>
<td>  Optimizer choice <br/></td>
<td>  Yes <br/></td>
<td>  Yes <br/></td>
<td>  Easy <br/></td>
</tr>
<tr>
<td>  Data preloading <br/></td>
<td>  Yes <br/></td>
<td>  No <br/></td>
<td>  Medium <br/></td>
</tr>
<tr>
<td>  Compiling <br/></td>
<td>  Yes <br/></td>
<td>  No <br/></td>
<td>  Easy <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p175">
<p>Carefully consider your options and what goal you’re working toward when setting up your training loop. For example, your batch size should be a power of 2 to hit maximum speed and memory efficiency. One author remembers working on getting an LLM to have a single-digit milli-second response time. The team was gearing up to serve millions of customers as fast as possible, and every millisecond counted. After using every trick in the book, I was able to achieve it, and I remember the huge feeling of accomplishment for finally getting that within the data science dev environment. Yet, it turned out that there was a hard batch size of 20 in the production environment. It was just a nice number picked out of a hat, and too many systems were built around this assumption; no one wanted to refactor. Software engineers, am I right? </p>
</div>
<div class="readable-text intended-text" id="p176">
<p>For the majority of these methods, the tradeoff is clear: if you go slower, you can fit a significantly larger model, but it will take way longer. Gradient accumulating and checkpointing can reduce memory usage by ~60%, but training will take much longer. The packages we talked about in section 5.1 can help mitigate these tradeoffs.</p>
</div>
<div class="readable-text" id="p177">
<h3 class="readable-text-h3" id="sigil_toc_id_96"><span class="num-string">5.4.3</span> Local minima traps</h3>
</div>
<div class="readable-text" id="p178">
<p>Local minima are hard to spot with LLMs and, as such, can be difficult to avoid. If you see your model converging early, be suspicious and judiciously test it before accepting the results. When you find that your model is converging early at a certain number of steps, one way to avoid it on subsequent runs is to save and load a checkpoint 100 or so steps before you see the errant behavior, turn your learning rate <em>way</em> down, train until you’re sure you’re past it, and then turn it back up and continue. Make sure to keep the previously saved checkpoint, and save a new checkpoint after that so that you have places to come back to in case things go wrong!</p>
</div>
<div class="readable-text intended-text" id="p179">
<p>You can probably tell that this is a frustrating occurrence that one author has run into before. He was so confused; he was working on a T5 XXL model, and around the 25K step mark, the model was converging and stopping early. He knew for a fact that it wasn’t actually converged; it was only 10% through the dataset! This happened two or three times, where he loaded up the checkpoint at around 20K steps and watched the exact same thing happen. It wasn’t until he loaded and turned the learning rate down that he finally saw the model improve past this point. Once he got through the patch of the local minimum, he turned it back up. This happened four more times throughout training this particular model, but since he knew what was happening, he was now able to avoid wasting lots of extra time. The lesson of the story? Use this rule of thumb: your LLM is not ready if it hasn’t trained on your full dataset.</p>
</div>
<div class="readable-text" id="p180">
<h3 class="readable-text-h3" id="sigil_toc_id_97"><span class="num-string">5.4.4</span> Hyperparameter tuning tips</h3>
</div>
<div class="readable-text" id="p181">
<p>Hyperparameter tuning isn’t something we’ve gone over extensively in this book, not because it’s not interesting but because it doesn’t help nearly as much as changing up your data, either getting more or cleaning it further. If you want to tune hyperparameters, Optuna is a great package, and you can get that ~1% boost in accuracy or F1 score that you really need. Otherwise, if you’re looking for a boost in a particular metric, try representing that metric more completely within your dataset and maybe use some statistical tricks like oversampling.</p>
</div>
<div class="readable-text intended-text" id="p182">
<p>While hyperparameter tuning is pretty cool mathematically, for LLMs, it’s not something that ever really needs to happen. If you need a boost in performance, you need more/better data, and tuning your hyperparameters will never match the performance boost you’d get quantizing the weights or performing any of the optimizations we’ve mentioned here or in chapter 3. The biggest performance boost we’ve ever gotten through tuning hyperparameters was about a 4% increase in F1, and we only did it because we wouldn’t be able to change our dataset for a couple of weeks at least.</p>
</div>
<div class="readable-text" id="p183">
<h3 class="readable-text-h3" id="sigil_toc_id_98"><span class="num-string">5.4.5</span> A note on operating systems</h3>
</div>
<div class="readable-text" id="p184">
<p>Windows is not the right OS to work professionally with LLMs without the Windows Subsystem for Linux. MacOS is great but lacks the hardware packages to really carry this load unless you know how to use an NVIDIA or AMD GPU with a Mac. If you are uncomfortable with Linux, you should take some time to familiarize yourself with it while your OS of choice catches up (if it ever does). A myriad of free online materials are available to help you learn about Bash, Linux, and the command line. Configuring the CUDA Toolkit and Nvidia drivers on Linux can make you want to pull your hair out, but it’s worth it compared to the alternatives. Along with this, learn about virtual environments, Docker, and cloud computing, like what’s in this chapter!</p>
</div>
<div class="readable-text intended-text" id="p185">
<p>All in all, Windows is easy in the beginning but frustrating in the long run. MacOS is also easy in the beginning but currently doesn’t work at all in the long run. Linux is incredibly frustrating in the beginning, but once you’re through that, it’s smooth sailing.</p>
</div>
<div class="readable-text" id="p186">
<h3 class="readable-text-h3" id="sigil_toc_id_99"><span class="num-string">5.4.6</span> Activation function advice</h3>
</div>
<div class="readable-text" id="p187">
<p>We’ve neglected to really dive into activation functions so far, not because they aren’t useful or cool but because you generally don’t need to tweak your activation functions unless you’re doing research science on model performance. If you take vanilla GPT-2 and give it a GeGLU activation instead of the GELU that it comes with, you will not get a significant boost in anything. In addition, you’ll need to redo your pretraining, as it pretrained with a different activation function. Activation functions help reduce some of the mathematical weaknesses of each layer, be they imaginary numbers from the quadratic attention, exploding and vanishing gradients, or maybe the researchers noticed positional encodings disappearing as they went through the model and changed a little bit. You can learn about activation functions, and we recommend doing so; in general, you can trust the papers that introduce new ones.</p>
</div>
<div class="readable-text intended-text" id="p188">
<p>We’ve come a long way in this chapter, discussing setting up an environment, training an LLM from scratch, and looking at a multitude of finetuning techniques. While we recognize there are still many aspects to this process that we did not touch on and that you need to learn on your own, you should be more than ready to create your own models. Now that you have a model, in the next chapter, we’ll discuss making it production-ready and creating an LLM service you can use to serve online inference.</p>
</div>
<div class="readable-text" id="p189">
<h2 class="readable-text-h2" id="sigil_toc_id_100">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p190"> Training is memory intensive, and you will need to master multi-GPU environments for many LLM training tasks. </li>
<li class="readable-text buletless-item" id="p191"> Model training has the same basic steps every time: 
    <ul>
<li> <em>Dataset preparation</em>—Acquire, clean, and curate your data. </li>
<li> <em>Model preparation</em>—Define model behavior, architecture, loss functions, etc. </li>
<li> <em>Training loop</em>—Initialization, tokenize, batch data, get predictions/loss, backpropagation, etc. </li>
</ul></li>
<li class="readable-text" id="p192"> Good data has a significantly greater effect on model performance than architecture or the training loop. </li>
<li class="readable-text" id="p193"> Finetuning is way easier than training from scratch because it requires much less data and resources. </li>
<li class="readable-text" id="p194"> Prompting allows us to train a model on a specific task after the fact, which is one of the reasons LLMs are so powerful compared to traditional ML. </li>
<li class="readable-text" id="p195"> Prompt tuning is a powerful way to focus your model to respond as a specialist to certain prompts. </li>
<li class="readable-text" id="p196"> Knowledge distillation is useful for training powerful smaller models that are efficient and adaptable. </li>
<li class="readable-text" id="p197"> RLHF is great at getting a model to respond in a way that pleases human evaluators but increases factually incorrect results. </li>
<li class="readable-text" id="p198"> Finetuning MoE models differs from traditional finetuning because it requires handling the experts and gating mechanisms. </li>
<li class="readable-text" id="p199"> LoRA is a powerful finetuning technique that adapts pretrained models to new tasks by creating tiny assets (low-rank matrices) that are fast to train, easy to maintain, and very cost-effective. </li>
<li class="readable-text" id="p200"> The quality and size of your data are two of the most important considerations for successfully training your model. </li>
<li class="readable-text" id="p201"> The major training tradeoff is speed for memory efficiency; if you go slower, you can fit a significantly larger model, but it will take way longer. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p202">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-245">[1]</span></a> L. Chen, M. Zaharia, and J. Zou, “How is ChatGPT’s behavior changing over time?,” arXiv.org, Jul. 18, 2023, <a href="https://arxiv.org/abs/2307.09009">https://arxiv.org/abs/2307.09009</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p203">
<p><a href="#footnote-source-2"><span class="footnote-definition" id="footnote-246">[2]</span></a> J. Hoffmann et al., “Training compute-optimal large language models,” arXiv:2203.15556 [cs], March 2022, <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>.</p>
</div>
</div></body></html>