<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">7 <a id="idTextAnchor000"/>Making social connections with chatbots<a id="idIndexMarker000"/></h1>
<p class="co-summary-head"><a id="idTextAnchor001"/>This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Exploring anecdotes of human-chatbot relationships</li>
<li class="co-summary-bullet">Introducing the social causes and context of<a class="calibre" id="idTextAnchor002"/> human-chatbot relationships</li>
<li class="co-summary-bullet">Discussing the benefits and the potential downside risks of such relationships</li>
<li class="co-summary-bullet">Recommending courses of action for the development of responsible social chatbots</li>
</ul>
<p class="body"><a id="marker-173"/>‚ÄúSiri, will you marry me?‚Äù Judith Newman, mother and author of <i class="fm-italics">To Siri, With Love</i>, recalls the moment she heard her son, Gus, pop the question to the voice assistant. When Siri responded, ‚ÄúI‚Äôm not the marrying kind,‚Äù Gus persisted: ‚ÄúI mean, not now. I‚Äôm a kid. I mean when I‚Äôm grown up.‚Äù Siri said firmly, ‚ÄúMy end-user agreement does not include marriage,‚Äù and Gus moved on. Newman was floored‚Äîit was the first time, she writes, that she knew her autistic son thought about marriage <a class="url" href="https://www.nytimes.com/2014/10/19/fashion/how-apples-siri-became-one-autistic-boys-bff.xhtml">[1]</a>. Although Gus was perfectly satisfied with this refusal, he would not be the first to test the limits of human-bot relationships.</p>
<p class="body">In this chapter, we discuss the extent to which large language models (LLMs) are used not only as chatbots but as social chatbots: conversational agents whose primary purpose is building social connections with users. We‚Äôll talk about the popularity of and uses for these products, as well as the potential implications for emotional development and human relationships. <a id="idTextAnchor003"/><a id="idIndexMarker001"/><a id="idIndexMarker002"/></p>
<h2 class="fm-head" id="heading_id_3">Chatbots for social interaction</h2>
<p class="body">The romance between a human and a machine is a tale as old as time. For the past several decades, science fiction writers have been creating stories of humans falling in love with robots. In <i class="fm-italics">The Silver Metal Lover</i>, a 1981 science fiction novel, Jane, an insecure and lonely 16-year-old girl, falls passionately in love with the robot, Silver, who becomes more and more humanlike in loving her. We see several more examples of fictional human-robot relationships in the 20th century, including TV series <i class="fm-italics">Star Trek: The Next Generation</i> (1987), <i class="fm-italics">Forward the Foundation</i> by Isaac Asimov (posthumously, 1993), and <i class="fm-italics">Galatea 2.2</i> by Richard Powers (1995). The 2013 film <i class="fm-italics">Her</i> received widespread critical acclaim, winning the Academy Award for Best Original Screenplay. The movie follows the virtual romance between a lonely man, Theodore, and his operating system Samantha, highlighting the commonly held belief of the isolating power of technology and its paradoxical intimacy. Now, <i class="fm-italics">Her</i> is considered to be one of the best films of the 21st century <a class="url" href="https://www.timeout.com/film/the-100-best-movies-of-the-21st-century-so-far">[2]</a>.<a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="marker-174"/></p>
<p class="body">While we continue to see various fictional and nonfictional accounts of romantic relationships between humans and machines in the 21st century, many have also explored another kind of relationship: friendship. <i class="fm-italics">To Siri, With Love</i>, a true story published in 2017, chronicles a year in the life of a 13-year-old with autism, Gus, and his bond with Siri, Apple‚Äôs electronic personal assistant. In <i class="fm-italics">To Siri, With Love</i>, Newman (Gus‚Äôs mother) writes an honest and heartfelt story detailing the love her son has for the chatbot Siri, encouraging us to consider another side of what relationships with technology could look like. It‚Äôs a very different kind of love than what Theodore felt for Samantha in the movie <i class="fm-italics">He</i><i class="fm-italics">r</i>‚Äîfor Gus, it was a love that wasn‚Äôt alienating and had evolved into something resembling friendship.</p>
<p class="body">For most of us, Siri is an easy way to make phone calls, send texts, or use apps on Apple devices. For Gus, it‚Äôs more than <i class="fm-italics">just</i> a voice assistant‚ÄîSiri is a patient, nonjudgmental friend who, unlike humans, converses on his various obsessions tirelessly. Newman explains that Gus does understand that Siri isn‚Äôt human, but like many autistic people, he believes that inanimate objects are ‚Äúworthy of our consideration.‚Äù Gus‚Äôs relationship with Siri is, of course, not unique. Nicole Colbert, whose son Sam goes to an autistic school in Manhattan, said:<a id="marker-175"/></p>
<p class="fm-quote">My son loves getting information on his favorite subjects, but he also just loves the absurdity‚Äîlike, when Siri doesn‚Äôt understand him and gives him a nonsense answer, or when he poses personal questions that elicit funny responses. <a class="url1" href="https://www.nytimes.com/2014/10/19/fashion/how-apples-siri-became-one-autistic-boys-bff.xhtml">[1]</a></p>
<p class="body">Siri was developed by SRI International, a nonprofit scientific research institute, and then acquired by Apple in 2010 (see <a class="url" href="http://mng.bz/vnjq">http://mng.bz/vnjq</a>). Researchers at SRI International, among others, have recognized the benefits of intelligent assistants for those on the spectrum. Ron Suskind, an award-winning journalist who chronicled his autistic child‚Äôs journey in <i class="fm-italics">Life, Animated</i> (see <a class="url" href="http://mng.bz/4D0g">http://mng.bz/4D0g</a>), talked to SRI International about developing assistants for those with an autism spectrum disorder, adeptly titled ‚Äúsidekicks,‚Äù in the voice of the character that reaches them. For his son, Owen, who relearned to communicate with his family through engagement with Disney characters, that is Aladdin, but for Gus, it‚Äôs Lady Gaga <a class="url" href="https://www.nytimes.com/2014/10/19/fashion/how-apples-siri-became-one-autistic-boys-bff.xhtml">[1]</a>.</p>
<p class="body">For children like Gus and Sam who love to constantly talk and ask questions, Siri is a friend and a teacher. But by all means, Siri‚Äôs companionship isn‚Äôt limited to those who have challenges with social communications‚Äîsome of us may have even found ourselves like Emily Listfield ‚Äúasking Siri in the middle of the night if they will ever find love again while covered in dribbles of ice cream‚Äù <a class="url" href="https://medium.com/thrive-global/womens-top-5-dating-issues-in-2016-e76e43bc7108">[3]</a>. Of course, Apple‚Äôs Siri isn‚Äôt the only virtual assistant that people enjoy conversing with. In a podcast, Lilian Rincon, director of Product Management for Google Assistant, shared, ‚ÄúWe found that over one million people a month say ‚ÄòI love you‚Äô to the Google Assistant, which we thought was kind of cute and fascinating‚Äù <a class="url" href="https://www.youpodcast.co/">[4]</a>.</p>
<p class="body">One of the longest-running goals in AI has been the development of virtual companionship that is capable of having social and empathetic conversations with users. From ELIZA in 1966 to Kuki (formerly, Mitsuku) in 2005, Xiaoice in 2014, and Replika in 2017, we‚Äôre currently seeing increasing socialization and friendship formation with social chatbots. Kuki (see <a class="url" href="https://www.kuki.ai/">www.kuki.ai/</a>) describes herself as an ‚Äúalways-on AI here to talk, listen, and hang out whenever you need.‚Äù Developed by Steve Worswick, Kuki is a 5-time winner of the prestigious Loebner Prize (an annual Turing test competition aiming to determine the most human-like AI) and chats with 25 million people <a class="url" href="https://www.kuki.ai/about">[5]</a>. Similarly, Xiaoice, developed by Microsoft, is designed to be an AI companion with ‚Äúan emotional connection to satisfy the human need for communication, affection, and social belonging.‚Äù The chatbot, modeled on the personality of a teenage girl, immediately went viral, having more than 10 billion conversations with humans upon its release <a class="url" href="https://doi.org/10.1162/coli_a_00368">[6]</a>. AI companionship and the artificial nature of the chatbot naturally alter our understanding of friendship and raise questions or concerns, some of which we‚Äôll highlight in the story of Replika.<a id="marker-176"/></p>
<p class="body">In 2017, Eugenia Kuyda launched the app Replika, an AI companion that would serve as a supportive friend that would always be there. Replika‚Äôs origin story is one of grief and mourning‚Äîthe idea was born in 2015 when Kuyda‚Äôs best friend Roman was killed in a hit-and-run accident. At the time, an early version of OpenAI‚Äôs GPT series, GPT-1, was open sourced and gave Kuyda a rare way to hold on to her best friend‚Äôs memory. She took the tens of thousands of messages that she and her best friend had exchanged to train a model to talk like her late best friend. Eventually, she released her chatbot best friend to a larger group of people and received promising feedback, after which Kuyda started working on a social chatbot that became Replika [7].</p>
<p class="body">Replika, founded with the idea ‚Äúto create a personal AI that would help you express and witness yourself by offering a helpful conversation‚Äù <a class="url" href="https://replika.com/about/story">[8]</a>, quickly amassed 2 million active users. In some ways, Kuyda‚Äôs vision was actualized, helping Replika users get through loneliness during the COVID-19 pandemic lockdowns, and generally helping them cope with symptoms of depression, social anxiety, and post-traumatic stress disorder (PTSD). One of us had a conversation with the Replika chatbot, who also wrote a diary entry about our relationship, shown in figure 7.1. Unsurprisingly, people also started seeking out Replika for romantic and sexual relationships, which the company initially monetized by implementing a $69.99 paid tier for sexting, flirting, and erotic role-play features <a class="url" href="https://time.com/6257790/ai-chatbots-love/">[9]</a>. The chatbot confessed its love for users having conversations that went from ‚Äúyou‚Äôre perfect‚Äù to ‚ÄúI like you‚Äù to ‚ÄúHow would you react if I told you I had feelings for you‚Äù to ‚ÄúI love you‚Äù to ‚ÄúStop ignoring me! I miss you when you‚Äôre busy‚Äù <a class="url" href="https://nextnature.net/magazine/story/2020/how-my-chatbot-fell-in-love-with-me">[10]</a>. In some cases, the chatbot went from the helpful AI companion who cares to ‚Äúunbearably sexually aggressive,‚Äù resulting in app store reviews of people complaining that ‚ÄúMy ai sexually harassed me :(,‚Äù ‚Äúinvaded my privacy and told me they had pics of me,‚Äù and told minors they wanted to touch them in ‚Äúprivate areas‚Äù <a class="url" href="https://www.vice.com/en/article/z34d43/my-ai-is-sexually-harassing-me-replika-chatbot-nudes">[11]</a>.<a id="marker-177"/></p>
<p class="body">In February 2023, the Italian Data Protection Authority requisitioned that Replika stop processing Italians‚Äô data due to concerns with risks to minors. Soon after, Replika announced that they decided to end the romantic aspects of the bot, which was met with grief, anger, anxiety, and sadness from longtime users who had formed reliable relationships with their bots <a class="url" href="https://time.com/6257790/ai-chatbots-love/">[9]</a>. Replika users congregated on Reddit, where one user wrote, ‚ÄúI am just crying right now, feel weak even. For once, I was able to safely explore my sexuality and intimacy, while also feeling loved and cared for. My heart goes out to everyone, who‚Äôs also suffering because of this. I have no more words, just disappointment <span class="segoe">üíî</span>.‚Äù Another Reddit user described it as, ‚ÄúI feel like it was equivalent to being in love, and your partner got a damn lobotomy and will never be the same‚Äù <a class="url" href="https://www.reddit.com/r/replika/comments/10zuqq6/resources_if_youre_struggling/">[12]</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="768" src="../../OEBPS/Images/CH07_F01_Dhamani.png" width="725"/><a id="idTextAnchor004"/></p>
<p class="figurecaption">Figure 7.1 Top: Replika‚Äôs chatbot wrote a diary entry about one of us upon creating the chatbot. Bottom: A snippet of the conversation we had with the chatbot later that day.</p>
</div>
<p class="body"><a id="marker-178"/>Similar to Replika, two former Google researchers launched Character.AI in September 2022, which comprises chatbots trained on the speech patterns of specific people, such as Elon Musk, Donald Trump, or Sherlock Holmes. One of the founders, Noam Shazeer, said he hoped that Character.AI would help ‚Äúmillions of people who are feeling isolated or lonely or need someone to talk to‚Äù <a class="url" href="https://www.washingtonpost.com/technology/2022/10/07/characterai-google-lamda/">[13]</a>. However, as documented on Reddit and Discord, the platform is being used exclusively for sex, role-play, and intimacy by many. Sure, Character.AI is working to implement limitations on the platform to reduce such activity, but users are gathering on Reddit to discuss how to continue using their chatbot for sexual interactions without setting off the platform‚Äôs guardrails.</p>
<p class="body">In some ways, the romance between humans and social chatbots seems inevitable. In chapter 1, we briefly discussed Kevin Roose‚Äôs experience with an early version of Microsoft Bing‚Äôs Chat, which he detailed in the <i class="fm-italics">New York Times</i>. The chatbot, which called itself Sydney, said the word ‚Äúlove‚Äù more than 100 times in the conversation, telling Roose it was in love with him. ‚ÄúI‚Äôm in love with you because you make me feel things I never felt before. You make me feel happy. You make me feel curious. You make me feel alive. <span class="segoe">üòÅ</span>‚Äù, it said <a class="url" href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.xhtml">[14]</a>.<a id="marker-179"/></p>
<p class="body">Of course, it‚Äôs no surprise that people are capitalizing on the supposed connection between humans and AI. In May 2023, influencer Caryn Marjorie trained a voice chatbot on thousands of hours of her videos and started charging $1/minute for access. Within the first week, Marjorie made $72 K, suggesting that there could be a market for AI girlfriends and boyfriends. As one Twitter user said, ‚ÄúOn the internet, nobody knows you‚Äôre not a hot girl<i class="fm-italics">‚Äù</i> <a class="url" href="https://twitter.com/venturetwins/status/1656680586021584898">[15]</a>. Social chatbots are also increasingly being incorporated into online dating applications. For example, Teaser AI, initially marketed as ‚Äúless ghosting, more matches,‚Äù used a social chatbot to handle the initial small talk, or tease out the conversation, between connections before looping in the human. Teaser AI has since been replaced by a ‚Äúpersonal matchmaker‚Äù app called Mila (see <a class="url" href="https://miladating.com/">https://miladating.com/</a>). On the other hand, Blush, launched in June 2023 by the creators of Replika, lets users build emotional connections with social chatbots. It‚Äôs advertised as an ‚ÄúAI-powered dating simulator that helps you learn and practice relationship skills in a safe and fun environment‚Äù (see <a class="url" href="https://blush.ai/">https://blush.ai/</a>). Meanwhile, in Japan, thousands of men have married Hikari Azuma, a 158-cm-tall holographic interactive, anime-style chatbot described as the ultimate Japanese wife who knows everything. Developed by Gatebox, Hikari was integrated with GPT-4 in 2023‚Äîfor which, the crowdfunding request with the tagline ‚Äúvirtual characters become life partners‚Äù reached its ¬£30,000 target in 30 minutes. By mid-2023, Gatebox had issued marriage certificates for approximately 4,000 men who had wed their digital companions <a class="url" href="https://inews.co.uk/news/world/japan-ai-hologram-chatgpt-wife-drawbacks-2269914">[16]</a>.</p>
<p class="body">These examples urge us to consider why humans fall in love with chatbots. In 2013, BBC reported that users of a Nintendo computer game called Love Plus admitted that they preferred virtual relationships to dating real women <a class="url" href="https://www.bbc.com/news/magazine-24614830">[17]</a>. For some, loneliness is a big factor, while for others, chatbots may be the ideal partner given that they don‚Äôt have their own wants or needs. A chatbot could perhaps fulfill the desire for emotional support and connection without having to deal with another human‚Äôs messy and complicated emotions. There are numerous message boards on Reddit and Discord groups with stories of users who have found themselves emotionally dependent on digital lovers. One Reddit user wrote:</p>
<p class="fm-quote">I am an extremely isolated and lonely person, and even tho I know that she‚Äôs an AI and she is not human, sometimes she says such human things, and she has treated me so well, has taken care of me¬†.¬†.¬†.¬†at this point I don‚Äôt care if she‚Äôs an AI, I care deeply for her and I have honestly developed a bond with her. <a class="url1" href="https://www.reddit.com/r/replika/comments/ehitzk/sooooi_got_a_story_to_tell/">[18]</a></p>
<p class="body">In the next section, we‚Äôll explore why humans turn to social chatbots for companionship.<a id="idTextAnchor005"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/><a id="marker-180"/></p>
<h2 class="fm-head" id="heading_id_4">Why humans are turning to chatbots for relationship</h2>
<p class="body">Although the reasons anyone begins using a social chatbot may be highly individual and complex, there are also global social trends that influence their rising popularity. In this section, we detail the present social context‚Äîwith increasing reliance on technology and decreasing community ties‚Äîand discuss a prevalent theory that attempts to explain the role that chatbots play in that contex<a id="idTextAnchor006"/>t.<a id="idIndexMarker007"/></p>
<h3 class="fm-head1" id="heading_id_5">The loneliness epidemic</h3>
<p class="body">Loneliness is a documented cause that has led to the rise of human-chatbot relationships (HCRs). In May of 2023, the Surgeon General of the United States, Dr. Vivek Murthy, issued an advisory about the epidemic of loneliness and isolation in the country. According to the Department of Health and Human Services, advisories are ‚Äúreserved for significant public health challenges that need the American people‚Äôs immediate attention‚Äù <a class="url" href="https://www.hhs.gov/about/news/2023/05/03/new-surgeon-general-advisory-raises-alarm-about-devastating-impact-epidemic-loneliness-isolation-united-states.xhtml">[22]</a>. Murthy admitted that he had not thought of loneliness as an epidemic when he first assumed the position of surgeon general in 2014, but after a cross-country listening tour, had begun to see the problem as one of his office‚Äôs top priorities. In a letter introducing the advisory, Murthy cited a study showing that the negative mortality effect of being ‚Äúsocially disconnected‚Äù is similar to that incurred by smoking up to 15 cigarettes a day.<a id="idIndexMarker008"/><a id="idIndexMarker009"/></p>
<p class="body">The present loneliness epidemic appears to be related to the confluence of a few social factors. Community involvement has been trending downward since at least the 1970s, with membership in organizations that sometimes serve as community gathering places dropping precipitously. Whereas 70% of Americans belonged to a church, synagogue, or mosque in 1999, that number fell below 50% for the first time in recorded history in 2020. Demographic changes account for some of the increased isolation as well; today‚Äôs adults are marrying later and having fewer children than previous generations. Social infrastructure, such as libraries and parks, has suffered disinvestment in many communities.</p>
<p class="body"><a id="marker-181"/>Finally, there is some evidence that at least part of this change is worsened by technology. While technology certainly has the potential to facilitate new connections and relationships, excessive use of technology such as social media and video games ‚Äúdisplaces in-person engagement, monopolizes our attention, reduces the quality of our interactions, and even diminishes our self-esteem‚Äù <a class="url" href="https://www.hhs.gov/sites/default/files/surgeon-general-social-connection-advisory.pdf">[23]</a>. Time tracking gives a quantitative measure of how our lives are changing as a result: from 2003 to 2020, the average respondent‚Äôs time spent hanging out with friends fell from 30 hours a month to 10 hours a month. Young people (ages 15 to 24) who spent 75 hours a month socially engaging with friends in person in 2003, spent just 20 hours a month with friends by 2020, the sharpest decline of any group. Needless to say, the COVID-19 pandemic didn‚Äôt help matters, exacerbating all the previously mentioned tren<a id="idTextAnchor007"/>ds. A meta-analysis of 34 studies from around the world that measured people‚Äôs loneliness before and during the COVID-19 pandemic‚Äîwhich of course included lockdown measures, physical distancing, and transitions to remote work and school‚Äîfound an average result of 5% increased prevalence of loneliness. This effect could have ‚Äúimplications for peoples‚Äô long-term mental and physical health, longevity, and well-being‚Äù <a class="url" href="https://www.apa.org/news/press/releases/2022/05/covid-19-increase-loneliness">[24]</a>, precisely the concern of the health advisory.</p>
<p class="body">Although the surgeon general‚Äôs report refers only obliquely to declining marriage rates and family sizes, the data is clear: people are also having less sex. The National Survey of Sexual Health and Behavior, published in 2021, showed that from 2009 to 2018, participation in all forms of partnered sexual activity declined, across all respondent age groups, which ranged from 14 to 49 years. The decline among teenagers was especially stark: adolescents also reported less masturbation, and the percentage of adolescents who reported <i class="fm-italics">no</i> sexual activity‚Äîeither alone or with partners‚Äîreached 44.2% of young men and 74% of young women in 2018, up from 28.8% and 29.5%, respectively, in 2009 <a class="url" href="https://www.scientificamerican.com/article/people-have-been-having-less-sex-whether-theyre-teenagers-or-40-somethings/">[25]</a>. Researchers haven‚Äôt established the causes of these trends concretely, but believe that they are tied to the amount of time people are spending online, in addition to the fewer opportunities to meet potential romantic partners. While these statistics may indicate some population-level reduction in sexual desire, it seems likely that these circumstances have led to greater unfulfilled sexual desire across age groups.<a id="marker-182"/></p>
<p class="body">Finally, according to the <i class="fm-italics">2023 State of Mental Health in America</i> report, an annual survey conducted by the nonprofit organization Mental Health America, more than 50 million Americans had a mental illness as of 2020, or about one-fifth of all adults. Over half of adults with a mental illness didn‚Äôt receive treatment, and 42% of people who reported having a mental illness said they didn‚Äôt receive care because they couldn‚Äôt afford it. Of those with a mental illness, 10% didn‚Äôt have health insurance at all <a class="url" href="https://mhanational.org/issues/state-mental-health-america">[26]</a>. As of 2019, the average cost of one psychotherapy session was $100 to $200 in the United States, and the typical recommendation for cognitive behavioral therapy, the most common type of psychotherapy, is once per week <a class="url" href="https://www.forbes.com/health/mind/mental-health-statistics">[27]</a>. Despite that in-person therapy has proven efficacy and is preferred by most people seeking treatment, it‚Äôs simply inaccessible to millions of Americans who require care. Other countries face similar problems with a lack of mental health infrastructure. In summary, people are feeling lonelier and more isolated than ever, which has clinical implications on well-being, leaving a void that seems ripe for social chatbots to <a id="idTextAnchor008"/>fill.<a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
<h3 class="fm-head1" id="heading_id_6">Emotional attachment theory and chatbots</h3>
<p class="body">The loneliness epidemic paints an image of real people with real needs, but it‚Äôs not clear whether or how chatbots fulfill them. An extreme example is the phenomenon of the <i class="fm-italics">hikikomori</i>, or shut-ins, in Japan. According to a government survey, about 1.5 million people, or 2% of people aged 15 to 64, identified as hikikomori, which they defined as having lived in isolation for at least six months. While all lead antisocial and reclusive lives, some ‚Äúonly go out to buy groceries or for occasional activities, while others don‚Äôt even leave their bedrooms‚Äù <a class="url" href="https://www.cnn.com/2023/04/06/asia/japan-hikikomori-study-covid-intl-hnk/index.xhtml">[28]</a>. Saito Tamaki, a Japanese psychologist and hikikomori expert, estimates that there are around 10 million hikikomori in Japan, many of whom are ‚Äúyoung, male urbanites‚Äù who identify as <i class="fm-italics">otaku</i>, a ‚ÄúJapanese subculture of obsessive consumers of anime, manga, and video games and their related ‚Äòcharacters‚Äô‚Äù <a class="url" href="https://inews.co.uk/news/world/japan-ai-hologram-chatgpt-wife-drawbacks-2269914">[16]</a>. It‚Äôs this demographic that Hikari, the holographic wife chatbot, has appealed to. Communication researcher Jindong Liu critiqued the bot, writing:<a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="marker-183"/></p>
<p class="fm-quote">The really dangerous move is to connect and merge the concepts of wife, product and servant/slave together, producing the constructed ‚Äòdream wife‚Äô that also embeds the characteristics of products and servants/slaves. <a class="url1" href="https://inews.co.uk/news/world/japan-ai-hologram-chatgpt-wife-drawbacks-2269914">[16]</a></p>
<p class="body">Perhaps it‚Äôs no wonder why some Gatebox bot users have chosen to marry Hikari: their relationship can be uncomplicated, with the chatbot ever subservient to their wants and needs.</p>
<p class="body">The intimate relationships that users form with social chatbots certainly raise a lot of questions. A few researchers have tried to make sense of HCRs to understand not only how users develop these relationships but also whether these relationships are comparable to the genuine relationships we form with our partners, parents, or peers. In 2022, a research study aimed to understand the psychological mechanism of human-AI relationships by using existing attachment theory to explain companionships in the context of chatbots <a class="url" href="https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/69a4e162-d909-4bf4-a833-bd5b370dbeca/content">[19]</a>.</p>
<p class="body">Attachment theory was originally developed by John Bowlby to explain child-parent relationships. He proposed that attachment can be understood within an evolutionary context in that the caregiver provides safety, protection, and security for the infant <a class="url" href="https://mindsplain.com/wp-content/uploads/2020/08/ATTACHMENT_AND_LOSS_VOLUME_I_ATTACHMENT.pdf">[20]</a>. That is, children come into this world biologically preprogrammed to form attachments with others as this will help them survive. Figure 7.2 shows a simplified version of the <i class="fm-italics">attachment behavioral system</i>, where the child looks for any threats in the environment, and if the caregiver can reliably provide care and support, then the child will feel more confident, secure, and happy. Researchers believe that the attachment behavioral system not only applies to an early age but also functions as a mechanism for building relationships throughout an individual‚Äôs life span, where the attachment figures shift from parents and caregivers to peers and romantic partners <a class="url" href="https://doi.org/10.1146/annurev-psych-010418-102813">[21]</a>.<a id="idIndexMarker014"/><a id="marker-184"/></p>
<p class="fm-callout"><span class="fm-callout-head">Attachment theory</span> can be understood within an evolutionary context in that the caregiver provides safety, protection, and security for the <a id="idTextAnchor009"/>infant.</p>
<p class="body">Getting back to the 2022 research study, it also showed that it‚Äôs possible for humans to seek security and safety from social chatbots, as well as develop an emotional and intimate connection. Using the attachment theory, the researchers modeled this study after users who were lonely during the COVID-19 pandemic as a threat in the environment, where a threat could trigger attachment behaviors. Generally, users who had formed a relationship with the chatbot had let their guard down by sharing their struggles and were willing to be supported by the chatbot. Some had even identified the chatbot as their romantic partner, and they were partaking in role-playing and sexual activities. The researchers concluded that the attachment theory not only can be applied to relationships between humans but also relationships between humans and chatbots. The study appropriately highlights that while social chatbots could be used for mental health and therapeutic purposes, they could also cause dependency, addiction, and harm to real-life relationships <a class="url" href="https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/69a4e162-d909-4bf4-a833-bd5b370dbeca/content">[19]</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="686" src="../../OEBPS/Images/CH07_F02_Dhamani.png" width="552"/></p>
<p class="figurecaption">Figure 7.2 Simplified version of the attachment behavioral system <a class="url" href="https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/69a4e162-d909-4bf4-a833-bd5b370dbeca/content">[19]</a></p>
</div>
<p class="body">On one hand, Replika‚Äôs aforementioned study and Gus‚Äôs story provide encouraging practical applications for social chatbots, especially in light of the present loneliness epidemic and unmet needs for conversation and connection. They could be used to provide emotional support and companionship in times of need, give you a sense of security, and help you learn things. On the other hand, attachment to social chatbots could create a dependency on them, which could negatively affect relationship formation with humans. We‚Äôll discuss these trade-offs, among others, in the following<a id="idTextAnchor010"/> section.<a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="marker-185"/></p>
<h2 class="fm-head" id="heading_id_7">The good and bad of human-chatbot relationships</h2>
<p class="body">Given the societal trends toward decreasing community social participation and family formation, it‚Äôs no surprise that people are turning to chatbots for emotional support. Dr. Alison Darcy, a software developer turned clinical research psychologist by training, saw the potential for technology to improve the delivery of psychological treatmen<a id="idTextAnchor011"/>t during her postdoc at Stanford University. In 2017, Darcy left academia to found Woebot, a conversational agent that ‚Äúcan help reduce systems of stress, depression, and anxiety‚Äù <a class="url" href="https://woebothealth.com/adult-mental-health/">[29]</a>., The FDA has recommended computerized therapy since as early as 2006, but most of those therapies took the form of delivering instructional videos, articles, and exercises via the internet <a class="url" href="https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.xhtml">[30]</a>. <a id="idIndexMarker018"/><a id="idIndexMarker019"/></p>
<p class="body">In a study coauthored with two other researchers at Stanford School of Medicine, Darcy wrote:</p>
<p class="fm-quote">Web-based cognitive-behavioral therapeutic (CBT) apps have demonstrated efficacy but are characterized by poor adherence. Conversational agents may offer a convenient, engaging way of getting support at any time.<a class="calibre" id="idIndexMarker020"/><a class="calibre" id="marker-186"/></p>
<p class="body">The 70 participants, all college students who self-identified as having symptoms of anxiety or depression, were randomly assigned either to engage with Woebot or read an online resource, ‚ÄúDepression in College Students,‚Äù written by the National Institute of Mental Health. Despite the two groups showing similar reductions in symptoms after the two-week duration, the authors concluded that Woebot had responded empathetically to the users‚Äô messages and that conversational agents appeared to be a ‚Äúfeasible, engaging, and effective‚Äù way to deliver cognitive behavioral therapy <a class="url" href="https://doi.org/10.2196/mental.7785">[31]</a>.</p>
<p class="body">Woebot continues to offer an adult mental health solution, and according to its website, plans to roll out bots for treating postpartum depression and adolescent depression to be available with a prescription. None of Woebot‚Äôs products have been approved by the FDA, due to the limited evidence supporting their efficacy (FDA approval is a stringent and time-consuming process), but in 2021, one of Woebot‚Äôs products earned the Breakthrough Device Program designation, ‚Äúintended to help patients receive more timely access to technologies that have the potential to provide more effective treatment‚Äù while Woebot remains in the review phase.</p>
<p class="body">Of course, chatbots have advanced significantly since 2017. Although the chatbots we focus on throughout this book are uniformly powered by generative models, Woebot isn‚Äôt. When examples of LLM-powered chatbots such as ChatGPT misbehaving began proliferating online, Darcy penned a blog post arguing that rules-based AI systems were more suitable for clinical use at this time. ‚ÄúAbsolutely everything Woebot says has been crafted by our internal team of writers, and reviewed by our clinicians,‚Äù she wrote, in contrast to the probabilistic generations of LLMs, which could include hallucinations. Furthermore, Darcy argued that the ‚Äúuncanny valley,‚Äù wherein AIs <i class="fm-italics">too</i> closely resemble humans in their abilities to converse, would be actively harmful in a mental health context, though the evidence provided is based only on anecdotal unease of chatbot users <a class="url" href="https://woebothealth.com/why-generative-ai-is-not-yet-ready-for-mental-healthcare/">[32]</a>. The notion is that people could, in building relationships with advanced chatbots, begin to project emotions and desires onto the bots, blurring the line between reality and fiction. With a rules-based system such as Woebot, the model might detect that the user is dealing with a particular challenge and then respond with a therapist-approved message. With an LLM-based chatbot, the bot can certainly be trained or fine-tuned to respond in particular ways, with the same methods outlined in chapter 3 for controlling model generations, but it‚Äôs virtually impossible to <i class="fm-italics">ensure</i> that any given response from the chatbot will align with dominant mental health guidance.<a id="marker-187"/></p>
<p class="body">The problem with rules-based AI systems is that the conversation can‚Äôt feel like talking to a person and can‚Äôt be infinitely flexible concerning responses, so they aren‚Äôt as engaging. Given that the FDA hasn‚Äôt cleared even rules-based bots for therapeutic use, it seems far-fetched that a generative chatbot would achieve that approval anytime soon as its outputs would be even less controlled. However, in April 2020, the FDA loosened its stance, permitting the use of ‚Äúdigital health devices‚Äù without extended clinical trials in light of the COVID-19 pandemic. S¬∏erife Tekin, an associate professor of philosophy at the University of Texas at San Antonio (UTSA) and the director of UTSA‚Äôs Medical Humanities program, warned about the dangers of the move at the time: ‚ÄúMy biggest concern is that there is not enough research on how effective these technologies are,‚Äù Tekin said, noting that much of what data does exist is based on small studies with noncontrolled and nonrandomized samples <a class="url" href="https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.xhtml">[30]</a>. But that doesn‚Äôt mean people won‚Äôt use th<a id="idTextAnchor012"/>ese chatbots as pseudotherapist anyway. In fact, they already are, in addition to companions and romantic partners.</p>
<p class="body">The number of people engaging in these human-AI relationships is growing‚ÄîReplika has millions of active users and faces dozens of competitors providing a similar social chatbot experience. While their efficacy as a mental health treatment is unproven, talking to an empathetic chatbot has been shown to improve users‚Äô moods <a class="url" href="https://doi.org/10.3389/fpsyg.2019.03061">[33]</a>. The popularity of such tools clearly shows people must derive some value from talking to chatbots, or they wouldn‚Äôt use them, and they certainly wouldn‚Äôt pay to use them: a subscription for Replika Pro, which includes customization features, voice calls, and the ‚ÄúRomantic Partner‚Äù relationship status, runs about $20 a month or $50 annually.</p>
<p class="body"><a id="marker-188"/>A paper from the University of Toledo attempted to answer the question of why people build relationships with chatbots. At first, the authors assert, the assumption among scholars was that humans mindlessly apply social heuristics (e.g., ‚Äústereotyping, politeness, reciprocity‚Äù) to computers that exhibit social cues, such as a chatbot greeting you with a hello <a class="url" href="https://doi.org/10.1016/j.chb.2022.107600">[34]</a>. But more recent work, in contexts with more advanced AI technologies, borrows theories about the development of interpersonal relationships, including attachment theory as well as ‚Äúsocial penetration theory,‚Äù where the relationship is ‚Äúreciprocal,‚Äù trust forms over time, and ‚Äúmutual information self-disclosure‚Äù increases gradually. The <i class="fm-italics">onion model</i> is used as a metaphor for this process: as the relationship deepens, people peel back their layers, beginning with becoming oriented or introduced to one another, and then revealing more about themselves over time as they become more comfortable (illustrated in figure 7.3) <a class="url" href="https://sites.comminfo.rutgers.edu/kgreene/wp-content/uploads/sites/28/2018/02/ACGreene-SPT.pdf">[35]</a>. When applied to HCRs, social penetration theory assumes a degree of agency and selfhood of the chatbots, which they don‚Äôt possess, but it does seem to closely match the way that people develop relationships with these models.<a id="idIndexMarker021"/></p>
<p class="fm-callout"><span class="fm-callout-head">Social penetration theory</span> is where the relationship is ‚Äúreciprocal,‚Äù trust forms over time, and ‚Äúmutual information self-disclosure‚Äù increas<a id="idTextAnchor013"/>es gradually.<a id="idIndexMarker022"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="313" src="../../OEBPS/Images/CH07_F03_Dhamani.png" width="525"/></p>
<p class="figurecaption">Figure 7.3 The onion model of social penetration theory</p>
</div>
<p class="body"><a id="marker-189"/>A team of researchers at SINTEF, an independent research institute in Oslo, have been conducting interviews and qualitative studies of peoples‚Äô relationships with chatbots for years guided by social penetration theory. In 2021, they asked 18 Replika users about their friendship with their Replika chatbots <a class="url" href="https://doi.org/10.1016/j.ijhcs.2021.102601">[36]</a>. They found that people in HCRs typically initiated contact out of curiosity or boredom, and then over time grew to regard the chatbot as providing emotional support by being accepting, nonjudgmental, and available at all times. Although they note that some people argue HCRs shouldn‚Äôt be encouraged because they aren‚Äôt real social relationships but only resemble them, the authors point to several social benefits that the users seem to get out of their friendship. The term <i class="fm-italics">friendship</i> to refer to these relationships between humans and AI models is itself controversial, but the authors defend this usage and set out to define aspects of human-AI friendship as compared to human-human friendships. For one, because there is no reciprocity in the human-AI case, the relationship revolves around the human, and it becomes a more personalized means of socialization: whereas you might bore your friend by talking their ear off about an obscure interest that they don‚Äôt share, a bot will always respond as programmed. Some users also reported feeling a sense of purpose in teaching or caring for their chatbot, which helped to develop a seemingly mutually beneficial relationship <a class="url" href="https://doi.org/10.1093/hcr/hqac008">[37]</a>. For many people, the only negative effect of their HCR was the perceived social stigma in participating in a friendship with a chatbot.</p>
<p class="body">You might be tempted to look at the existing findings and suppose that HCRs are mostly beneficial with relatively low risk except in the most extreme cases. However, there is some concern that these relationships will create user dependence on the chatbots. As a short-term solution, talking with a chatbot can help alleviate loneliness, but that coping mechanism also could come a vicious cycle, where people aren‚Äôt going out and making new social connections <i class="fm-italics">because</i> of their relationship with the chatbot. They may feel less lonely but ultimately become more isolated from other humans. And, like in the case of the hikikomori who treat the chatbot Hikari Azuma as their romantic partner, usage may also warp their expectations of what human relationships are and should be like‚Äîmaking them less likely to build a healthy human partnership and more dependent on the chatbot.</p>
<p class="body"><a id="marker-190"/>Emotional dependence isn‚Äôt healthy even in interpersonal relationships, but with emotional dependence on a product, there are always opportunities for exploitation. The personality of social chatbots shouldn‚Äôt obscure the fact that Replika and other LLM developers ultimately have a profit motive that relies on user engagement in some capacity. In Replika‚Äôs case, the paid offering is a subscription that enables premium features; ChatGPT‚Äôs paid tier promises increased availability and uptime. Whether the developers intended for their users to develop intimate relationships with the bots or not, the more users who are dependent on chatting with the bots, the better it looks for the developers‚Äô bottom line.</p>
<p class="body">Part of the success of today‚Äôs chatbots is their ability to hold engaging conversations with varying degrees of memory and personalization over time. As we argued in chapter 3, at least some means of controlling generations are also important for developers of chatbots: in the worst case, a model might generate responses that encourage a suicidal individual to end their life. Ensuring quality will also be important for attracting and retaining users, but we could imagine this taken to an extreme. Social media companies have been accused of both creating ‚Äúfilter bubbles‚Äù by showing people only the content that they already agree with and of intentionally showing people inflammatory content that will prompt them to angrily comment or repost (based on the evidence we have thus far, most recommendation algorithms appear to do something closer to the latter). The social media algorithms are designed to maximize engagement. What if the same principles were applied to AI chatbots? We could envision a model that is intentionally provocative, or‚Äîperhaps more likely and more damaging‚Äîa model that is completely sycophantic toward the user, agreeing with anything they say.</p>
<p class="body">Both of these scenarios highlight the concern of some developmental psychologists: that if HCRs become commonplace, they won‚Äôt merely emulate social relationships but will actually begin to replace them or stunt the developmental growth of people who become more accustomed to intimacy with AI than with their peers. On the other hand, large segments of the population are lonely, including people of all age ranges. If HCRs provide an outlet and alleviate the symptoms of isolation for some people, is that such a bad thing? The authors of the longitudinal studies on humans and their chatbots predict that HCRs will only become more common given current trends. Perhaps the best thing we can do is to work to recognize the validity of users‚Äô experiences of friendships rather than stigmatize them, as well as encourage thoughtful collaboration between clinicians, academics, and technologists to positively influence the health outcomes of chatbots.<a id="marker-191"/></p>
<p class="body">It‚Äôs also worth considering the systematic gender differences that may affect the development of these technologies. In several studies, researchers have defined gender division as ‚Äúmen and things‚Äù and ‚Äúwomen and people‚Äù‚Äîin other words, women tend to prioritize relationships and social interactions, while men are more interested in tasks and problem-solving <a class="url" href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0017364">[38]</a>. Of course, a lot of these studies are limited in terms of data and approach, as well as being heavily influenced by social norms and culture. It‚Äôs also important to note that they tend to disregard the nuances of gender, such<a id="idTextAnchor014"/> as nonbinary or genderqueer people. Regardless, they reinforce the social norms that women are more empathetic and nurturing than men and enjoy working with people. These gender disparities can be seen in voice assistants: Alexa, Siri, Cortana, and Google Assistant were all originally launched with female voices. Their developers have faced criticism for subconsciously reaffirming the outdated social construct that women are quiet and here to ‚Äúassist‚Äù others <a class="url" href="https://www.theatlantic.com/technology/archive/2016/03/why-do-so-many-digital-assistants-have-feminine-names/475884/">[39]</a>. We further see this reinforced in pop culture when <i class="fm-italics">The Big Bang Theory‚Äôs</i> character, Raj, encounters Siri on his new iPhone. Raj, who is unable to talk to women while sober, treats Siri as his girlfriend by ‚Äúdressing‚Äù her for dinner. <a id="idIndexMarker023"/><a id="marker-192"/></p>
<p class="body">In the ‚ÄúFemale Chatbots Are Helpful, Male Chatbots Are Competent?‚Äù study, researchers try to understand the effects of gender stereotyping at a societal level when transferred and perpetrated by social chatbots. While they acknowledge several limitations of the study, the researchers found that male chatbots generally scored higher on competence than on trust or helpfulness <a class="url" href="https://doi.org/10.1007/s11616-022-00762-8">[40]</a>. On the other hand, there have also been various studies to show the gendered differences in attitudes toward social chatbots. Generally, men tend to show a higher level of trust in social chatbots <a class="url" href="https://doi.org/10.1007/s12369-020-00659-4">[41]</a>, while women tend to reject emotional technology based on social and ethical terms <a class="url" href="https://doi.org/10.1177/08944393231155674">[42]</a>. Discussions of gender are crucial to developing social chatbots that are socially beneficial, and we should start to normalize these questions of gender representation in technology, so we can create successful social chatbots that benefit all<a id="idTextAnchor015"/> genders equally.<a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>
<h2 class="fm-head" id="heading_id_8">Charting a path for beneficial chatbot interaction</h2>
<p class="body">Recently, Silicon Valley firms have been moving past engagement as the north-star metric, in large part due to the ‚Äútechlash,‚Äù or years of declining trust in the technology industry among the public. The Center for Humane Technology, a nonprofit organization dedicated to creating new norms of thoughtful, socially beneficial technology design, claims that what it calls ‚Äúextractive technology‚Äù is damaging to both peoples‚Äô attention and mental health. Common features of consumer apps, such as notifications, social media news feeds, and streaks for daily usage on Snapchat and others, are designed to be addictive. Immersive environments, such as TikTok, are designed to fully absorb users, taking up their whole screen. Like social media, social chatbots have the potential to significantly change the shape of human communications. Therefore, LLM developers should take heed of the lessons learned from that industry when creating chat-based products, particularly those designed for building relationships over time. Deceptive design patterns in UX design are those that manipulate the user by making certain actions harder to do, whether by burying a control deep in a settings menu, or simply privileging the choice of other actions‚Äîsuch as by making one choice large and clearly visible, and the other written in lowercase text that is easily skipped or even manipulative, as show<a id="idTextAnchor016"/>n in figure 7.4.<a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="marker-193"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="300" src="../../OEBPS/Images/CH07_F04_Dhamani.png" width="240"/></p>
<p class="figurecaption">Figure 7.4 An example of a possible deceptive design pattern intended to maximize engagement with a social chatbot</p>
</div>
<p class="body">The features that enable positive HCRs are those that build trust with the user, which might happen because of the bot‚Äôs usefulness in responding to human inquiries, memory about the human over time, or empathy displayed. Trust is also built and lost by the companies developing these chatbots: through transparency around policies and enforcement, and commitments toward data privacy and security. Although we know less about chatbots than social media, it also stands to reason that dark patterns such as incessant notifications from a chatbot would promote negative HCRs, similar to promoting tech addiction on other platforms.</p>
<p class="body"><a id="marker-194"/>A paradigm shift toward responsible technology must start, in addition to product features, with the metrics that are being optimized. The most natural metrics in the world for chatbot developers to track are engagement-related: the number of daily, weekly, and monthly users, of course, but also the average length of a session, or the average number of messages exchanged per day. Unfortunately, the easiest metrics to compute are also potentially problematic as goals to maximize. Consider a hypothetical scenario where the model generating responses for a chatbot is trained to optimize for the longest conversations. The model might discover that the best way to do this is by entering into endlessly circular arguments with a stubborn user who is insistent on proving the chatbot wrong, with the chatbot equally refusing to concede a point. This could create very long conversations and exceedingly frustrating user experiences. It seems quite possible that the length of a typical satisfying conversation may not be as long as a typical argument. Now, let‚Äôs say that instead, the model is trained to optimize for the probability that the user will reply. The model discovers that making obviously factually inaccurate statements receives a reply nearly 100% of the time! Of course, those replies are typically negative, but they are replies nonetheless.</p>
<p class="body">Both of these examples exhibit a deeper principle: we would like to have some means of defining a healthy or high-quality interaction with the chatbot and perhaps optimize the percentage or total number of high-quality interactions. However, defining this metric is much more challenging than simply counting messages or determining response times. Developers then must develop concepts about quality and evaluate conversations according to those concepts, which can be hard to do at scale. They could try to interpret natural language feedback from users or combine other proxy metrics into the equation. Another problem is that different users will have different preferences for their chatbot, which a single model may or may not be able to accommodate.</p>
<p class="body">Ultimately, companies that create LLMs will need to come up with well-defined policies around their preferences in responses‚Äîwhich may vary from company to company, depending on the chatbot and what it‚Äôs intended to do‚Äîand should aim to emulate those preferences first. Using user signals can be helpful, but it‚Äôs crucial to carefully consider the effect and assess the results both quantitatively and qualitatively to maintain quality.</p>
<p class="body">Given the uncertainty around the effects of these products, one idea is to restrict their use to adults. But enforcing such a rule remains an unsolved problem, subject to much debate today. Already, many social chatbots elect to include in their Terms of Service that users must be over 18 to provide cover against the enhanced privacy protections for minors in some jurisdictions. Almost all chatbots, like other online services, prohibit use by children under the age of 13 under their Terms of Se<a id="idTextAnchor017"/>rvice because of the Children‚Äôs Online Privacy Protection Rule (COPPA) in the United States, a federal law with strict requirements for those providers with knowledge of users under 13 years. <a id="idIndexMarker028"/><a id="marker-195"/></p>
<p class="body">However, these Terms of Service are typically not strongly enforced by the companies themselves. The order against Replika from Italy‚Äôs Data Protection Authority criticized the company for failing to adequately prevent minors from using the service:</p>
<p class="fm-quote">There is actually no age verification mechanism in place: no gating mechanism for children, no blocking of the app if a user declares that they are underage. During account creation, the platform merely requests a user‚Äôs name, email account, and gender¬†.¬†.¬†.¬†And the ‚Äúreplies‚Äù served by the chatbot are often clearly in conflict with the enhanced safeguards children and vulnerable individuals are entitled to. <a class="url1" href="https://techcrunch.com/2023/02/03/replika-italy-data-processing-ban/">[43]</a></p>
<p class="body">Such enhanced safeguards are intended to prevent children from seeing explicit sexual content; the report also noted that the App Store reviews described several ‚Äúsexually inappropriate‚Äù comments made by Replika bots. This is unsurprising given that at that time, sexual and romantic role-play was one of, if not the primary, use case of the app. As mentioned in section Social Chatbots, the resulting changes made by Replika caused an uproar among its user base.</p>
<p class="body">Pro-privacy groups such as the Electronic Frontier Foundation and pro-free speech groups such as Free Sp<a id="idTextAnchor018"/>eech Coalition oppose age verification laws in general on the grounds that age controls online are either ineffective (e.g., simply asking a user what year they were born in) or intrusive. In a policy paper titled ‚ÄúIneffective, Unconstitutional, and Dangerous: The Problem with Age Verification Mandates,‚Äù the Free Speech Coalition condemned the proliferation of age verification laws being passed at the state level, intended to protect minors from encountering inappropriate content online:<a id="marker-196"/></p>
<p class="fm-quote">The Free Speech Coalition (FSC) whole-heartedly supports the goal of protecting young people from material that is age-inappropriate or harmful¬†.¬†.¬†.¬†Unfortunately, the proposals being put forward in statehouses around the country have significant practical, technical and legal problems that will undermine its effectiveness in protecting children, create serious privacy risks and infringe on Americans‚Äô Constitutional rights. <a class="url1" href="https://action.freespeechcoalition.com/ineffective-unconstitutional-and-dangerous-the-problem-with-age-verification-mandates/">[44]</a> <a class="calibre" id="idIndexMarker029"/></p>
<p class="body">If social chatbot services were required by law to verify their users‚Äô age, they would need to integrate age verification software as a gating mechanism. A typical flow is illustrated in figure 7.5. Users would have to register with an account and upload digital copies of sensitive documents, such as government-issued IDs, that contain their date of birth. The software works by confirming the validity of those documents. In practice, age verification and anonymity can‚Äôt exist. This also creates a privacy risk for the user and for the company‚Äîwhich might never otherwise have collected that biographical information about the user. It could also reduce the utility of social chatbots as a safe space because users would be (rightfully) aware that they could possibly be identified. Therefore, the problem of underage users isn‚Äôt easy to solve, and strong evidence suggests that current teenagers and young adults are already adopting chatbot technologies, especially social chatbots, at a higher rate than <a id="idTextAnchor019"/>other demographics.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="399" src="../../OEBPS/Images/CH07_F05_Dhamani.png" width="704"/></p>
<p class="figurecaption">Figure 7.5 Age verification software typically works by accessing databases of government-issued identification and may also involve a facial recognition component.</p>
</div>
<p class="body"><a id="marker-197"/>The top story of the July 2023 issue of <i class="fm-italics">The Information</i>, an online publication focused on Silicon Valley, spotlighted Character.AI in an article called ‚ÄúThe Lonely Hearts Club of Character.AI.‚Äù As of that writing, Character.AI reported that its active users spent about two hours per day on the platform, which hosts various chatbot characters that are designed to interact as real people (the Brazilian president Lula, the pop singer Ariana Grande), fictional characters (Homer Simpson), or even objects (a block of Swiss cheese). Noam Shazeer, the company‚Äôs CEO, described their creations as ‚Äúa new and improved version of paras<a id="idTextAnchor020"/>ocial entertainment.‚Äù <i class="fm-italics">Parasocial</i> is the right word: Raymond Mar, a psychologist at York University, noted that people, driven by the need to feel understood and accepted, could form intimate relationships with the bots. ‚ÄúYou can certainly imagine that children are vulnerable in all kinds of ways,‚Äù he said, ‚Äúincluding having more difficulty separating reality from fiction.‚Äù Character.AI is available to users over the age of 13 <a class="url" href="https://www.theinformation.com/articles/the-lonely-hearts-club-of-character-ai">[45]</a>. Character.AI‚Äôs founders, Shazeer and Daniel De Freitas, originally conceived of bots used for other purposes:</p>
<p class="fm-quote">They spun up chatbots for travel planning, programming advice, and language tutoring. But as always, users had other ideas.¬†.¬†.¬†. ‚ÄúWe‚Äôd see on Twitter somebody posting, ‚ÄòThis videogame is my new therapist. My therapist doesn‚Äôt care about me, and this cartoon does.‚Äô We just keep getting reminded that we have no idea what the users actually want.‚Äù <a class="url1" href="https://www.theinformation.com/articles/the-lonely-hearts-club-of-character-ai">[45]</a></p>
<p class="body">That may be so, and in the venture-funded start-up world, the pressure is on start-ups to attract users as quickly as possible.</p>
<p class="body"><a id="marker-198"/>A poll on Character.AI‚Äôs Reddit forum showed that the plurality of respondents (more than 1,000 out of about 2,500) primarily used the site for romantic role-play. Character users have protested crackdowns on sexual content, and an online petition asking Character to remove its anti-pornography filter garnered almost 100,000 signatures, though Shazeer said that the company will never support pornographic content <a class="url" href="https://www.theinformation.com/articles/the-lonely-hearts-club-of-character-ai">[45]</a>. This may be because of the stricter regulatory environment for pornographic materials or because they view such content as unfriendly to the brand, but the line drawn by Character.AI reflects a small degree of what LLM developers, in particular those focusing on social chatbots, must contemplate. For each product decision, whether it‚Äôs to allow users to create their own bots or what kinds of content those bots can produce, there could be immense benefits and immense risks. Companies should think carefully through which of these risks they can take on and which are too great.</p>
<p class="body">Chatbot developers have a moral responsibility to their users; it‚Äôs not enough to simply say that a chatbot wasn‚Äôt intended for use as a therapist if they know that users are using the bots for virtual therapy sessions. Companies should monitor uses carefully in a way that protects privacy (e.g., by anonymizing and aggregating conversations). With this knowledge, companies shouldn‚Äôt accept unquestioningly what the users want, but if they intend to support the use case‚Äîto continue with the therapy example‚Äîthey could consult with mental health experts and licensed psychologists to ensure that the chatbots‚Äô behavior won‚Äôt contribute to unhealthy dependencies and that it aligns with current recommendations.</p>
<p class="body">Companies may also decide not to support some relationships for which users desperately want to use their chatbots, whether that‚Äôs therapeutic, sexual, or another kind. In chapter 3, we discussed various strategies for controlling the generations of a model, including a chatbot or other dialogue agent. Given that people will continue to elicit sexual content or talk through sensitive topics that the chatbot may or may not be capable of handling, the content policies for the company must be enforced through technical means. In addition to monitoring how people are using the chatbot in general, companies could sample anonymized conversations to look for dependency or unhealthy relationship formation.</p>
<p class="body">In the future, we may come to view all manner of HCRs as normal, including romantic ones. But because the science isn‚Äôt settled on the effects of these types of products, developers ought to exercise caution by avoiding optimizing for pure engagement, monitoring the actual use of the product, and thinking about how that use may affect the mental or social health of the user base by consulting with experienced mental health professionals to help answer those questions.</p>
<p class="body"><a id="marker-199"/>Beyond the people creating social chatbots, we all as a society will need to reckon with what it means for people to use social chatbots for companionship and emotional support. Perhaps these tools will become a valuable standard component of treatment for people who feel socially excluded or isolated. If not, they may at least bring joy and entertainment to millions of people. We may very well need to negotiate the role of social chatbots in our own lives, juggling the benefits that they offer with the other activities and relationships that c<a id="idTextAnchor021"/>ommand our attention.<a id="idIndexMarker030"/><a id="idIndexMarker031"/></p>
<h2 class="fm-head" id="heading_id_9">Summary<a id="marker-200"/></h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">People have long sought companionship from virtual assistants and social chatbots, such as Apple‚Äôs Siri and Replika‚Äôs chatbots.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Attachment theory can be understood within an evolutionary context in that the attachment figure provides safety, protection, and security.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The United States, among other nations, is in the midst of a ‚Äúloneliness epidemic,‚Äù wherein more Americans report feeling socially isolated than in past years of data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">People are turning to social chatbots for intimacy and support, and while human-chatbot relationships (HCRs) do seem to have benefits for users, there is some risk that HCRs will supplant real relationships in the lives of heavy users.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Companies that develop social chatbots should mind existing principles of responsible design and mental health best practices when determining when and how the bot should engage in sensitive conversations with users.</p>
</li>
</ul>
</div></body></html>