<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conclusions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Conclusions</h1>
<blockquote>原文：<a href="https://deeplearningwithpython.io/chapters/chapter20_conclusion">https://deeplearningwithpython.io/chapters/chapter20_conclusion</a></blockquote>


<aside>
<p>This chapter covers
</p>
<ul>
<li>Important takeaways from this book</li>
<li>Resources for learning further and applying your skills in practice</li>
</ul>
</aside>

<p>We’ll start with a bird’s-eye view of what you should take away from this book.
This should refresh your memory regarding some of the concepts you’ve learned.
Then we’ll give you a short list of
resources and strategies for learning further about machine learning
and staying up to date with new advances.</p>
<p>Becoming an effective AI practitioner
is a journey, and finishing this book is merely your first step on it. I
want to make sure you realize this and are properly equipped to take the next
steps of this journey on your own.</p>
<h2 id="key-concepts-in-review">Key concepts in review</h2>
<p>This section briefly synthesizes key takeaways from this book. If you ever
need a quick refresher to help you recall what you’ve learned, you can read
these few pages.</p>
<h3 id="various-approaches-to-artificial-intelligence">Various approaches to artificial intelligence</h3>
<p>First, deep learning isn’t synonymous with artificial intelligence (AI), or even with machine
learning:</p>
<ul>
<li><em>Artificial intelligence</em> (AI) is an ancient, broad field that can
generally be understood as “all attempts to automate human cognitive processes.”
This can range from the very basic,
such as an Excel spreadsheet, to the very advanced, like a humanoid robot that
can walk and talk.</li>
</ul>
<ul>
<li><em>Machine learning</em> is a specific subfield of AI that aims at automatically
developing programs (called <em>models</em>) purely from exposure to training data.
This process of turning data into a program is called <em>learning</em>. Although
machine learning has been around for a long time, it only started to take off
in the 1990s, before becoming the dominant form of AI in the 2000s.</li>
</ul>
<ul>
<li><em>Deep learning</em> is one of many branches of machine learning, where the
models are long chains of geometric transformations, applied one after the other.
These operations are structured into modules called <em>layers</em>: deep learning
models are typically stacks of layers — or, more generally, graphs of layers.
These layers are parameterized by <em>weights</em>, which are the parameters learned
during training. The <em>knowledge</em> of a model is stored in its weights, and the
process of learning consists of finding “good values” for these weights —
values that minimize a <em>loss function</em>. Because the chain of geometric transformations
considered is differentiable, updating the weights to minimize the loss function
is done efficiently via <em>gradient descent</em>.</li>
</ul>
<ul>
<li><em>Generative AI</em> is a specific subset of deep learning, where models
are capable of generating text, images, videos, or sound. These models tend to
be very large — billions of parameters. They’re trained in a self-supervised
manner; that is, they’re trained to reconstruct artificially
missing or corrupted parts of an input —
for instance, denoising images, predicting the next word in a sentence, and so on.
This learning process enables the models to learn sophisticated “maps” (embedding manifolds)
of their input space, which can be used for sampling new inputs. These models have launched
AI into its “consumer” era with the rise of products like ChatGPT or Midjourney.</li>
</ul>
<p>Even though deep learning is just one among many approaches to machine
learning, it isn’t on an equal footing with the others. Deep learning is a
breakout success. Here’s why.</p>
<h3 id="what-makes-deep-learning-special-within-the-field-of-machine-learning">What makes deep learning special within the field of machine learning</h3>
<p>In the span of only a few years, deep
learning has achieved tremendous breakthroughs across a wide range of tasks
that have been historically perceived as extremely difficult for computers,
especially in the area of machine perception: extracting useful information
from images, videos, sound, and more. Given sufficient training data (in
particular, training data appropriately labeled by humans), deep learning makes it
possible to extract from perceptual data almost anything a human could.
Hence, it’s sometimes said that deep learning has “solved perception” —
although that’s true only for a fairly narrow definition of perception.</p>
<p>Due to its unprecedented technical successes, deep learning has singlehandedly
brought about the third and by far the largest <em>AI summer</em>: a period of
intense interest, investment, and hype in the field of AI. As this book is
being written, we’re in the middle of it. Whether this period will end in the
near future and what happens after it ends are topics of debate. One thing
is certain: in stark contrast with previous AI summers, deep learning has
provided enormous business value to both large and small technology companies
and has become a huge consumer success,
enabling human-level speech recognition, chatbot assistants, photorealistic image
generation, human-level machine translation, and more. The hype may
(and likely will) recede, but the sustained economic and technological impact
of deep learning will remain. In that sense, deep learning could be analogous
to the internet: it may be overly hyped up for a few years, but in the longer
term, it will still be a major revolution that will transform our economy and
our lives.</p>
<p>One reason I’m particularly optimistic about deep learning is that
even if we were to make no further technological progress in the next decade,
deploying existing algorithms to every applicable problem would be a game changer for most
industries. Deep learning is nothing short of a revolution, and progress is
currently happening at an incredibly fast rate due to an exponential
investment in resources and headcount. From where we stand, the future looks
bright, although short-term expectations are somewhat overoptimistic;
deploying deep learning to the full extent of its potential will likely
take multiple decades.</p>
<h3 id="how-to-think-about-deep-learning">How to think about deep learning</h3>
<p>The most surprising thing about deep learning is
how simple it is. Fifteen years ago, no one expected that we would achieve such
amazing results on machine-perception and natural language processing
problems by using simple parametric models trained with gradient descent.
Now it turns out that all you need is
sufficiently large parametric models trained with gradient descent on
sufficiently many examples. As Feynman once said about the universe, “It’s not
complicated,
it’s just a lot of it.”<sup class="footnote-link" id="footnote-link-1"><a href="#footnote-1">[1]</a></sup></p>
<p>In deep learning, everything is a vector; that is, everything is
a <em>point</em> in a <em>geometric space</em>. Model inputs (text, images, and so on)
and targets are first <em>vectorized</em> — turned into an initial input vector space
and target vector space. Each layer in a deep learning model operates one
simple geometric transformation on the data that goes through it. Together,
the chain of layers in the model forms one complex geometric transformation,
broken down into a series of simple ones. This complex transformation attempts
to map the input space to the target space, one point at a time. This
transformation is parameterized by the weights of the layers, which are
iteratively updated based on how well the model is currently performing. A key
characteristic of this geometric transformation is that it must be
<em>differentiable</em>, which is required for us to be able to learn its
parameters via gradient descent. Intuitively, this means the geometric
morphing from inputs to outputs must be smooth and continuous — a significant
constraint.</p>
<p>The entire process of applying this complex geometric transformation to the
input data can be visualized in 3D by imagining a person trying to uncrumple a
paper ball: the crumpled paper ball is the manifold of the input data that the
model starts with. Each movement operated by the person on the paper ball is
similar to a simple geometric transformation operated by one layer. The full
uncrumpling gesture sequence is the complex transformation of the entire
model. Deep learning models are mathematical machines for uncrumpling
complicated manifolds of high-dimensional data.</p>
<p>That’s the magic of deep learning — turning meaning into vectors, into geometric
spaces, and then incrementally learning complex geometric transformations that
map one space to another. All you need are spaces of sufficiently high
dimensionality to capture the full scope of the relationships found
in the original data.</p>
<p>The whole thing hinges on two core ideas:
that <em>meaning is derived from the pairwise relationship between things</em>
(between words in a language, between pixels in an image, and so on) and that
<em>these relationships can be captured by a distance function</em>.
But note that whether the brain implements meaning
via geometric spaces is an entirely separate question. Vector spaces are
efficient to work with from a computational standpoint, but different data
structures for intelligence can easily be envisioned — in particular, graphs.
Neural networks initially emerged from the idea of using graphs as a way to
encode meaning, which is why they’re named <em>neural networks</em>; the surrounding
field of research used to be called <em>connectionism</em>. Nowadays, the name <em>neural
network</em> exists purely for historical reasons — it’s an extremely misleading name
because they’re neither neural nor networks. In particular, neural networks
have hardly anything to do with the brain. A more appropriate name would have
been <em>layered representations learning</em> or <em>hierarchical representations learning</em>,
or maybe even deep <em>differentiable models</em> or <em>chained geometric transforms</em>,
to emphasize the fact that continuous geometric space manipulation is at their core.</p>
<h3 id="key-enabling-technologies">Key enabling technologies</h3>
<p>The technological revolution that’s
currently unfolding didn’t start with any single breakthrough invention.
Rather, like any other revolution, it’s the product of a vast accumulation of
enabling factors — slowly at first, and then suddenly. In the case of deep
learning, we can point out the following key factors:</p>
<ul>
<li>Incremental algorithmic innovations, first spread over two decades (starting
with backpropagation) and then happening increasingly faster as more research
effort was poured into deep learning after 2012. One major such
breakthrough was the Transformer architecture in 2017.</li>
</ul>
<ul>
<li>The availability of large amounts of image, video, and text data, which is a requirement
to realize that sufficiently large models trained on sufficiently large
data are all we need. This is, in turn, a by-product of the rise of the consumer
internet and Moore’s law applied to storage media. Today, state-of-the-art language models
are trained on a large fraction of the entire internet.</li>
</ul>
<ul>
<li>The availability of fast, highly parallel computation hardware at a low price,
especially the GPUs produced by NVIDIA — first gaming GPUs and then chips
designed from the ground up for deep learning. Early on, NVIDIA CEO Jensen
Huang took note of the deep learning boom and decided to bet the company’s
future on it, which paid off in a big way.</li>
</ul>
<ul>
<li>A complex stack of software layers that makes this computational power
available to humans: the CUDA language, frameworks like TensorFlow, JAX, and PyTorch that do
automatic differentiation, and Keras, which makes deep learning accessible to
most people.</li>
</ul>
<p>In the future, deep learning will not be used only by specialists such as researchers,
graduate students, and engineers with an academic profile; it will be a
tool in the toolbox of every developer, much like web technology today.
Everyone needs to build intelligent apps: just as every business today needs a
website, every product will need to intelligently make sense of user-generated
data. Bringing about this future will require us to build tools that make deep
learning radically easy to use and accessible to anyone with basic coding
abilities. Keras has been the first major step in that direction.</p>
<h3 id="the-universal-machine-learning-workflow">The universal machine learning workflow</h3>
<p>Having access to an extremely powerful tool
for creating models that map any input space to any target space is great, but
the difficult part of the machine learning workflow is often everything that
comes before designing and training such models (and, for production models,
what comes after, as well). Understanding the problem domain to be able
to determine what to attempt to predict, given what data, and how to measure
success is a prerequisite for any successful application of machine learning,
and it isn’t something that advanced tools like Keras and TensorFlow can help
you with. As a reminder, here’s a quick summary of the typical
machine learning workflow as described in chapter 6:</p>
<ul>
<li><em>Define the problem.</em> What data is available, and what are you trying to
  predict? Will you need to collect more data or hire people to manually label a
  dataset?</li>
<li><em>Identify a way to reliably measure success on your goal.</em> For simple tasks,
  this may be prediction accuracy, but in many cases, it will require
  sophisticated, domain-specific metrics.</li>
<li><em>Prepare the validation process that you’ll use to evaluate your models.</em> In
  particular, you should define a training set, a validation set, and a test
  set. The validation-set and test-set labels shouldn’t leak into the training
  data: for instance, with temporal prediction, the validation and test data
  should be posterior to the training data.</li>
<li><em>Vectorize the data by turning it into vectors and preprocessing it in a way that makes it more easily approachable by a neural network (normalization and so on).</em></li>
<li><em>Develop a first model that beats a trivial common-sense baseline, thus demonstrating that machine learning can work on your problem.</em> This may not
  always be the case!</li>
<li><em>Gradually refine your model architecture by tuning hyperparameters and adding regularization.</em> Make changes based on performance on the validation data only,
  not the test data or the training data. Remember that you should get your
  model to overfit (thus identifying a model capacity level that’s greater than
  you need) and only then begin to add regularization or downsize your model.
  Beware of validation-set overfitting when tuning hyperparameters — the fact
  that your hyperparameters may end up being overspecialized to the validation
  set. Avoiding this is the purpose of having a separate test set!</li>
<li><em>Deploy your final model in production — as a web API, as part of a JavaScript or C++ application, on an embedded device, etc.</em> Keep monitoring its performance
  on real-world data and use your findings to refine the next iteration of the model!</li>
</ul>
<h3 id="key-network-architectures">Key network architectures</h3>
<p>The families of network architectures that you should be familiar with after
reading this book are <em>densely connected networks</em>, <em>convolutional networks</em>,
<em>recurrent networks</em>, <em>Diffusion Models</em>, and <em>Transformers</em>. Each type of model
is meant for specific data modalities: a network architecture encodes
<em>assumptions</em> about the structure of the data — a <em>hypothesis space</em> within which
the search for a good model will proceed. Whether a given architecture will work
on a given problem depends entirely on the match between the structure of the
data and the assumptions of the network architecture.</p>
<p>These different network types can easily be combined to achieve larger
multimodal models, much as you combine LEGO bricks. In a way, deep learning
layers are LEGO bricks for information processing. Table 20.1 shows a quick
overview of the mapping between input and output modalities and the appropriate
network architectures.</p>
<figure id="table-20-1" class="table-figure"><table>
<thead>
<tr>
<th>Input</th>
<th>Output</th>
<th>Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vector data</td>
<td>Class probability, Regression value</td>
<td>Densely connected network</td>
</tr>
<tr>
<td>Timeseries data</td>
<td>Class probability, Regression value</td>
<td>RNN, Transformer</td>
</tr>
<tr>
<td>Images</td>
<td>Class probability, Regression value</td>
<td>ConvNet</td>
</tr>
<tr>
<td>Text</td>
<td>Class probability, Regression value</td>
<td>Transformer</td>
</tr>
<tr>
<td>Text, Images</td>
<td>Text</td>
<td>Transformer</td>
</tr>
<tr>
<td>Text, Images</td>
<td>Images</td>
<td>VAE, Diffusion Model</td>
</tr>
</tbody>
</table><figcaption>
<a href="#table-20-1">Table 20.1</a>: Model architectures for different data types
</figcaption>
</figure>

<p>Now let’s quickly review the specificities of each network architecture.</p>
<h4 id="densely-connected-networks">Densely connected networks</h4>
<p>A densely connected network is a stack of
<code>Dense</code> layers, meant to process vector data (where each sample is a vector
  of numerical or categorical attributes).
Such networks assume no specific structure in the input features: they’re called
<em>densely connected</em> because the units of a <code>Dense</code> layer are connected to
every other unit. The layer attempts to map relationships between any two
input features; this is unlike a 2D convolution layer, for instance, which
only looks at <em>local</em> relationships.</p>
<p>Densely connected networks are most commonly used for categorical data (for
example, where the input features are lists of attributes), such as the Boston
Housing Price dataset used in chapter 4. They’re also used as the final
classification or regression stage of most networks. For instance, the
ConvNets covered in chapter 8 typically end with one or two <code>Dense</code> layers,
and so do the recurrent networks in chapter 13.</p>
<p>Remember, to perform <em>binary classification</em>, end your stack of layers with a
<code>Dense</code> layer with a single unit and a <code>sigmoid</code> activation, and use
<code>binary_crossentropy</code> as the loss. Your targets should be either 0 or 1:</p>
<figure>
<pre><code class="language-python">import keras
from keras import layers

inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
</code></pre>
</figure>

<p>To perform <em>single-label categorical classification</em> (where each
sample has exactly one class, no
more), end your stack of layers with a <code>Dense</code> layer with a number of units
equal to the number of classes and a <code>softmax</code> activation.
If your targets are one-hot encoded, use <code>categorical_crossentropy</code>
as the loss; if they’re integers, use <code>sparse_categorical_ crossentropy</code>:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy")
</code></pre>
</figure>

<p>To perform <em>multilabel categorical classification</em> (where
   each sample can have several classes), end your stack of
layers with a <code>Dense</code> layer with a number of units equal to the number of
classes and a <code>sigmoid</code> activation, and use
<code>binary_crossentropy</code> as the loss. Your targets should be k-hot encoded:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
</code></pre>
</figure>

<p>To perform <em>regression</em> toward a vector of continuous values,
end your stack of layers with a <code>Dense</code> layer with a number of units equal to
the number of values you’re trying to predict (often a single one, such as the
price of a house) and no activation. Various losses can be used for
regression — most commonly <code>mean_squared_error</code> (MSE):</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_values)(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="mse")
</code></pre>
</figure>

<h4 id="convnets">ConvNets</h4>
<p>Convolution layers look at
spatially local patterns by applying the same geometric transformation to
different spatial locations (<em>patches</em>) in an input tensor. This results in
representations that are <em>translation invariant</em>, making convolution layers
highly data efficient and modular. This idea is applicable to spaces
of any dimensionality: 1D (continuous sequences), 2D (images), 3D (volumes), and so on.
You can use the <code>Conv1D</code> layer to process
sequences, the <code>Conv2D</code> layer to process images, and the <code>Conv3D</code> layer to
process volumes. As a leaner, more efficient alternative to convolution layers,
you can also use <em>depthwise separable convolution</em> layers, such as <code>SeparableConv2D</code>.</p>
<p><em>ConvNets</em>, or <em>convolutional networks</em>, consist of stacks of convolution and
max-pooling layers. The pooling layers let you spatially downsample the data,
which is required to keep feature maps to a reasonable size as the number of
features grows and to allow subsequent convolution layers to “see” a greater
spatial extent of the inputs. ConvNets are often ended with either a <code>Flatten</code>
operation or a global pooling layer, turning spatial feature maps into
vectors, followed by <code>Dense</code> layers to achieve
classification or regression.</p>
<p>Here’s a typical image-classification network (categorical classification, in
this case) using <code>SeparableConv2D</code> layers:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(height, width, channels))
x = layers.SeparableConv2D(32, 3, activation="relu")(inputs)
x = layers.SeparableConv2D(64, 3, activation="relu")(x)
x = layers.MaxPooling2D(2)(x)
x = layers.SeparableConv2D(64, 3, activation="relu")(x)
x = layers.SeparableConv2D(128, 3, activation="relu")(x)
x = layers.MaxPooling2D(2)(x)
x = layers.SeparableConv2D(64, 3, activation="relu")(x)
x = layers.SeparableConv2D(128, 3, activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy")
</code></pre>
</figure>

<p>When building a very deep ConvNet, it’s common to add
<em>batch normalization</em> layers as well as <em>residual connections</em>
— two architecture patterns that help
gradient information flow smoothly through the network.</p>
<h4 id="transformers">Transformers</h4>
<p>A Transformer looks at a set of vectors (such as word vectors)
and uses <em>neural attention</em>
to transform each vector into a representation that is aware of the <em>context</em>
provided by the other vectors in the set. When the set in question is an ordered
sequence, you can also use <em>positional encoding</em> to create Transformers
that can take into account both global context and word order, capable of
processing long text paragraphs much more effectively than RNNs or 1D ConvNets.</p>
<p>Transformers can be used for any set-processing or sequence-processing task,
including text classification, but they excel especially at
<em>sequence-to-sequence learning</em>, such as translating
paragraphs in a source language into a target language.</p>
<p>A sequence-to-sequence Transformer is made of two parts:</p>
<ul>
<li>A <code>TransformerEncoder</code> that turns
an input vector sequence into a context-aware, order-aware output vector sequence</li>
<li>A <code>TransformerDecoder</code> that takes the output of the <code>TransformerEncoder</code>, as
well as a target sequence, and predicts what should come next in the target sequence</li>
</ul>
<p>If you’re only processing a single sequence (or set) of vectors, you’d only
use the <code>TransformerEncoder</code>.</p>
<p>Following is a sequence-to-sequence Transformer for mapping
a source sequence to a target sequence (this setup could be used for machine translation
or question-answering, for instance):</p>
<figure>
<pre><code class="language-python">from keras_hub.layers import TokenAndPositionEmbedding
from keras_hub.layers import TransformerDecoder, TransformerEncoder

# Source sequence
encoder_inputs = keras.Input(shape=(src_seq_length,), dtype="int64")
x = TokenAndPositionEmbedding(vocab_size, src_seq_length, embed_dim)(
    encoder_inputs
)
encoder_outputs = TransformerEncoder(intermediate_dim=256, num_heads=8)(x)
# Target sequence so far
decoder_inputs = keras.Input(shape=(dst_seq_length,), dtype="int64")
x = TokenAndPositionEmbedding(vocab_size, dst_seq_length, embed_dim)(
    decoder_inputs
)
x = TransformerDecoder(intermediate_dim=256, num_heads=8)(x, encoder_outputs)
# Predictions for target sequence one step in the future
decoder_outputs = layers.Dense(vocab_size, activation="softmax")(x)
transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
transformer.compile(optimizer="adamw", loss="categorical_crossentropy")
</code></pre>
</figure>

<p>And this is a lone <code>TransformerEncoder</code> for binary classification of integer sequences:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(seq_length,), dtype="int64")
x = TokenAndPositionEmbedding(vocab_size, seq_length, embed_dim)(inputs)
x = TransformerEncoder(intermediate_dim=256, num_heads=8)(x)
x = layers.GlobalMaxPooling1D()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="adamw", loss="binary_crossentropy")
</code></pre>
</figure>

<h4 id="recurrent-neural-networks">Recurrent neural networks</h4>
<p><em>Recurrent neural networks</em> (RNNs) work by
processing sequences of inputs one timestep at a time and maintaining a state
throughout (a state is typically a vector or set of vectors).
They should be used preferentially over 1D
ConvNets in the case of sequences where patterns of interest aren’t invariant
by temporal translation (for instance, timeseries data where the recent past
is more important than the distant past).</p>
<p>Three RNN layers are available in Keras: <code>SimpleRNN</code>,
<code>GRU</code>, and <code>LSTM</code>. For most practical purposes, you should use either <code>GRU</code> or
<code>LSTM</code>. <code>LSTM</code> is the more powerful of the two but is also more expensive; you
can think of <code>GRU</code> as a simpler, cheaper alternative to it.</p>
<p>To stack multiple RNN layers on top of each other, each layer prior to
the last layer in the stack should return the full sequence of its outputs
(each input timestep will correspond to an output timestep); if you aren’t
stacking any further RNN layers, then it’s common to return only the last
output, which contains information about the entire sequence.</p>
<p>Following is a single RNN layer for binary classification of vector sequences:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(num_timesteps, num_features))
x = layers.LSTM(32)(inputs)
outputs = layers.Dense(num_classes, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
</code></pre>
</figure>

<p>And this is a stacked RNN layer for binary classification of vector sequences:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(num_timesteps, num_features))
x = layers.LSTM(32, return_sequences=True)(inputs)
x = layers.LSTM(32, return_sequences=True)(x)
x = layers.LSTM(32)(x)
outputs = layers.Dense(num_classes, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
</code></pre>
</figure>

<h2 id="limitations-of-deep-learning">Limitations of deep learning</h2>
<p>Building deep learning models is like playing with LEGO bricks:
layers can be plugged together to map essentially anything to anything, given
that you have appropriate training data available and that the mapping is
achievable via a continuous geometric transformation of reasonable complexity.</p>
<p>Here’s the catch, though — this mapping is often not learnable in a way
that will generalize. Deep learning models operate like vast, interpolative databases of patterns.
Their pattern-matching strength is also their core weakness:</p>
<ul>
<li><em>They fundamentally struggle to adapt to novelty.</em>
Because their parameters are fixed after training, they can only retrieve or replicate patterns similar to their training data.
Faced with inputs significantly outside this familiar distribution — no matter how simple the underlying task —
their performance degrades drastically, as they lack mechanisms for fluid generalization beyond their memorized experience.
This explains why even large models fail on novel tasks or simple variations of familiar problems — like ARC-AGI tasks.</li>
</ul>
<ul>
<li><em>They’re very sensitive to phrasing and other distractors.</em> Deep learning models exhibit high sensitivity to superficial
variations in input presentation, such as minor phrasing changes (consider prompt sensitivity in LLMs)
or imperceptible perturbations (consider adversarial examples in vision), indicating a lack of robust, human-like understanding.</li>
</ul>
<ul>
<li><em>They often can’t learn generalizable algorithms.</em> The continuous, geometric nature of deep learning models makes them fundamentally
ill-suited for learning exact, discrete, step-by-step algorithms, such as those that are the bread and butter of classical computer science.
Models approximate such processes through interpolation rather than implementing robust, generalizable procedures.</li>
</ul>
<p>You should always resist the temptation to anthropomorphize deep learning models. Their performance is
built on pointwise statistical patterns rather than human-like experiential grounding,
making it brittle when encountering deviations from training data.</p>
<p>The narrative that simply scaling up model size and training data would lead to general intelligence
has proven insufficient. While scaling enhances performance on benchmarks that amount to memorization tests,
it fails to address the fundamental limitations of deep learning, which stem from the core paradigm of fitting static,
interpolative curves to data. Five years of exponential scaling of base LLMs haven’t overcome these constraints because the underlying approach remains unchanged.</p>
<p>By 2024, this realization spurred a transition toward test-time adaptation (TTA),
where models perform search or fine-tuning during the inference phase to adapt to novel problems.
While TTA methods have yielded major breakthroughs, such as OpenAI’s o3 surpassing human baseline on ARC-AGI-1 in late 2024,
this performance has come at an extreme computational cost.
Efficient, human-like adaptation is still a completely open problem,
and the slightly harder ARC-AGI-2 benchmark remains completely unsolved as of today.
We still need further conceptual advances beyond mere scaling or brute-force search.</p>
<h2 id="what-might-lie-ahead">What might lie ahead</h2>
<p>Solving human-like fluid intelligence (and ARC-AGI-2) requires moving beyond the limitations inherent in current approaches.
While deep learning excels at <em>value-centric abstraction</em>, which enables pattern recognition and intuition,
it fundamentally lacks capabilities for <em>program-centric abstraction</em>, which underpins discrete reasoning, planning, and causal understanding.
Human intelligence seamlessly integrates both — future AI must do the same.</p>
<p>Future key developments may include</p>
<ul>
<li><em>Hybrid models</em> — Future models will likely integrate learned algorithmic modules (providing reasoning and symbolic manipulation)
with deep learning modules (providing perception and intuition).
These systems might learn to use programming primitives like control flow, variables, recursion, and complex data structures dynamically.</li>
</ul>
<ul>
<li><em>Deep-learning guided program search</em> — Program synthesis — automatically discovering executable code that meets specifications —
offers a route to program-centric abstraction. However, its reliance on inefficient discrete search is a major bottleneck.
A crucial advance will be using deep learning to guide this search, utilizing learned intuition about program structure to navigate vast, combinatorial spaces of programs
efficiently, much like human developers use experience and intuition to narrow down their choices.</li>
</ul>
<ul>
<li><em>Modular recombination and lifelong learning</em> — We’ll move away from monolithic, end-to-end models trained from scratch.
Instead, future AI systems will use massive libraries of reusable, modular components that can be repurposed across many problems, acquired from experience.
These libraries will feature both “geometric” (deep learning based) and “algorithmic” modules.
When faced with a new problem, such AI systems will fetch relevant modules and dynamically recombine them into a new model adapted
to the situation at hand. Whenever the system ends up developing a reusable component as a by-product of this problem-solving
loop, the new component would get added to the library, becoming available for every future task the system might encounter.</li>
</ul>
<p>Ultimately, developing AI that mirrors human-like fluid intelligence will require
blending continuous pattern recognition together with discrete, symbolic programs
and fully embracing the paradigm of on-the-fly adaptation.</p>
<h2 id="staying-up-to-date-in-a-fast-moving-field">Staying up to date in a fast-moving field</h2>
<p>As final parting words, I want to give you some pointers about how to keep
learning and updating your knowledge and skills after you’ve turned the last
page of this book. The field of modern deep learning, as we know it today, is
only a few years old, despite a long, slow prehistory stretching back decades.
With an exponential increase in financial resources and research headcount
since 2013, the field as a whole is now moving at a frenetic pace. What you’ve
learned in this book won’t stay relevant forever, and it isn’t all you’ll need
for the rest of your career.</p>
<p>Fortunately, there are plenty of free online resources that you can use to stay
up to date and expand your horizons. Here are a few.</p>
<h3 id="practice-on-real-world-problems-using-kaggle">Practice on real-world problems using Kaggle</h3>
<p>An effective way to acquire real-world experience is to try your hand at machine learning
competitions on Kaggle (<a href="https://kaggle.com">https://kaggle.com</a>). The only real way to learn is
through practice and actual coding — that’s the philosophy of this book, and
Kaggle competitions are the natural continuation of this. On Kaggle, you’ll
find an array of constantly renewed data science competitions, many of which
involve deep learning, prepared by companies interested in obtaining novel
solutions to some of their most challenging machine learning problems. Fairly
large monetary prizes are offered to top entrants.</p>
<p>By participating in a few competitions, maybe as part of a team, you’ll become
more familiar with the practical side of some of the advanced best practices
described in this book, especially hyperparameter tuning, avoiding
validation-set overfitting, and model ensembling.</p>
<h3 id="read-about-the-latest-developments-on-arxiv">Read about the latest developments on arXiv</h3>
<p>Deep learning research, in contrast with some other
scientific fields, takes place completely in the open. Papers are made
publicly and freely accessible as soon as they’re finalized, and a lot of
related software is open source. arXiv (<a href="https://arxiv.org">https://arxiv.org</a>) — pronounced
“archive” (the X stands for the Greek <em>chi</em>) — is an open access preprint server
for physics, mathematics, and computer science research papers. It has become
the de facto way to stay up to date on the cutting edge of machine learning
and deep learning. The large majority of deep learning researchers upload any
paper they write to arXiv shortly after completion. This allows them to plant
a flag and claim a specific finding without waiting for a conference
acceptance (which takes months), which is necessary given the fast pace of
research and the intense competition in the field. It also allows the field to
move extremely fast: all new findings are immediately available for all to see
and to build on.</p>
<p>An important downside is that the sheer quantity of new papers posted every day
on arXiv makes it impossible to even skim them all, and the fact that they
aren’t peer-reviewed makes it difficult to identify those that are both
important and high quality. It’s challenging, and becoming increasingly more so,
to find the signal in the noise. But some tools can help: in particular,
you can use Google Scholar (<a href="https://scholar.google.com">https://scholar.google.com</a>) to keep track of
publications by your favorite authors.</p>
<h3 id="explore-the-keras-ecosystem">Explore the Keras ecosystem</h3>
<p>With over 2.5 million users as of early 2025 and still growing,
Keras has a large ecosystem of tutorials, guides, and related open source projects:</p>
<ul>
<li>Your main reference for working with Keras is the online documentation
at <a href="https://keras.io">https://keras.io</a>. In particular, you’ll find extensive
developer guides at <a href="https://keras.io/guides">https://keras.io/guides</a>, and you’ll find dozens
of high-quality Keras code examples at <a href="https://keras.io/examples">https://keras.io/examples</a>.
Make sure to check them out!</li>
</ul>
<ul>
<li>The Keras source code can be found at <a href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</a>,
and Keras Hub can be found at <a href="https://github.com/keras-team/keras-hub">https://github.com/keras-team/keras-hub</a>.</li>
</ul>
<ul>
<li>You can follow François (@fchollet) and Matt (@mattdangerw) on X (formerly Twitter).</li>
</ul>
<h2 id="final-words">Final words</h2>
<p>This is the end of <em>Deep Learning with Python</em>! I hope you’ve learned a thing
or two about machine learning, deep learning, Keras, and maybe even cognition
in general. Learning is a lifelong journey, especially in the field of AI,
where we have far more unknowns on our hands than certitudes. So please go on
learning, questioning, and researching. Never stop. Because even given the
progress made so far, most of the fundamental questions in AI remain
unanswered. Many haven’t even been properly asked yet.</p>

&#13;

  <h3>Footnotes</h3>
  <ol>

    <li id="footnote-1">
      Richard Feynman, interview, <em>The World from Another Point of View</em>, Yorkshire Television, 1972.
      <a class="footnote-backlink" href="#footnote-link-1">[↩]</a>
    </li>

  </ol>
    
</body>
</html>