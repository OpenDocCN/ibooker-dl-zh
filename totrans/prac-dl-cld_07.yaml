- en: Chapter 7\. Practical Tools, Tips, and Tricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter contains material that we, your authors, have encountered during
    our professional work as well as while working on this book, primarily during
    experimentation. The material covered here doesn’t necessarily fit in any single
    chapter; rather, it’s material that deep learning practitioners could find useful
    on a day-to-day basis across a variety of tasks. In line with the “practical”
    theme, these questions cover a range of helpful pragmatic guidelines across topics
    including setting up an environment, training, model interoperability, data collection
    and labeling, code quality, managing experiments, team collaboration practices,
    privacy, and further exploration topics.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the fast-changing pace of the AI field, this chapter is a small subset
    of the “living” document hosted on the book’s Github repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    at *code/chapter-9*, where it is constantly evolving. If you have more questions
    or, even better, answers that might help other readers, feel free to tweet them
    [@PracticalDLBook](https://www.twitter.com/PracticalDLBook) or submit a pull request.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *I came across an interesting and useful Jupyter Notebook on GitHub.
    Making the code run will require cloning the repository, installing packages,
    setting up the environment, and more steps. Is there an instant way to run it
    interactively?*'
  prefs: []
  type: TYPE_NORMAL
- en: Simply enter the Git repository URL into Binder (*[mybinder.org](http://mybinder.org)*),
    which will turn it into a collection of interactive notebooks. Under the hood,
    it will search for a dependency file, like *requirements.txt* or *environment.yml*
    in the repository’s root directory. This will be used to build a Docker image,
    to help run the notebook interactively in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *What is the quickest way to get my deep learning setup running on a
    fresh Ubuntu machine with NVIDIA GPUs?*'
  prefs: []
  type: TYPE_NORMAL
- en: Life would be great if `pip install tensorflow-gpu` would solve everything.
    However, that’s far from reality. On a freshly installed Ubuntu machine, listing
    all the installation steps would take at least three pages and more than an hour
    to follow, including installing NVIDIA GPU drivers, CUDA, cuDNN, Python, TensorFlow,
    and other packages. And then it requires carefully checking the version interoperability
    between CUDA, cuDNN and TensorFlow. More often than not, this ends in a broken
    system. A world of pain to say the least!
  prefs: []
  type: TYPE_NORMAL
- en: 'Wouldn’t it be great if two lines could solve all of this effortlessly? Ask,
    and ye shall receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first line ensures that all the drivers are updated. The second line is
    brought to us by the Lambda Labs, a San Francisco–based deep learning hardware
    and cloud provider. The command sets up the Lambda Stack, which installs TensorFlow,
    Keras, PyTorch, Caffe, Caffe2, Theano, CUDA, cuDNN, and NVIDIA GPU drivers. Because
    the company needs to install the same deep learning packages on thousands of machines,
    it automated the process with a one-line command and then open sourced it so that
    others can also make use of it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *What is the fastest way to install TensorFlow on a Windows PC?*'
  prefs: []
  type: TYPE_NORMAL
- en: Install Anaconda Python 3.7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the command line, run `conda install tensorflow-gpu`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you do not have GPUs, run `conda install tensorflow`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One additional benefit of a CPU-based Conda installation is that it installs
    Intel MKL optimized TensorFlow, running faster than the version we get by using
    `pip install tensorflow`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I have an AMD GPU. Could I benefit from GPU speedups in TensorFlow
    on my existing system?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the majority of the deep learning world uses NVIDIA GPUs, there is
    a growing community of people running on AMD hardware with the help of the ROCm
    stack. Installation using the command line is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo apt install rocm-libs miopen-hip cxlactivitylogger`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo apt install wget python3-pip`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pip3 install --user tensorflow-rocm`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Q:** *Forget installation, where can I get preinstalled deep learning containers?*'
  prefs: []
  type: TYPE_NORMAL
- en: Docker is synonymous with setting up environments. Docker helps run isolated
    containers that are bundled with tools, libraries, and configuration files. There
    are several deep learning Docker containers available while selecting your virtual
    machine (VM) from major cloud providers AWS, Microsoft Azure, GCP, Alibaba, etc.)
    that are ready to start working. NVIDIA also freely provides NVIDIA GPU Cloud
    containers, which are the same high-performance containers used to break training
    speed records on the MLPerf benchmarks. You can even run these containers on your
    desktop machine.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *I don’t like having to stare at my screen constantly to check whether
    my training finished. Can I get a notification alert on my phone, instead?*'
  prefs: []
  type: TYPE_NORMAL
- en: Use [Knock Knock](https://oreil.ly/uX3qb), a Python library that, as the name
    suggests, notifies you when your training ends (or your program crashes) by sending
    alerts on email, Slack, or even Telegram! Best of all, it requires adding only
    two lines of code to your training script. No more opening your program a thousand
    times to check whether the training has finished.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I prefer graphics and visualizations over plain text. Can I get real-time
    visualizations for my training process?*'
  prefs: []
  type: TYPE_NORMAL
- en: FastProgress progress bar (originally developed for fast.ai by Sylvain Gugger)
    comes to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I conduct a lot of experiments iteratively and often lose track of
    what changed between each experiment as well as the effect of the change. How
    do I manage my experiments in a more organized manner?*'
  prefs: []
  type: TYPE_NORMAL
- en: Software development has had the ability to keep a historical log of changes
    through version control. Machine learning, unfortunately, did not have the same
    luxury. That’s changing now with tools like Weights and Biases, and Comet.ml.
    They allow you to keep track of multiple runs and to log training curves, hyperparameters,
    outputs, models, notes, and more with just two lines of code added to your Python
    script. Best of all, through the power of the cloud, you can conveniently track
    experiments even if you are away from the machine, and share the results with
    others.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *How do I check whether TensorFlow is using the GPU(s) on my machine?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following handy command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Q:** *I have multiple GPUs on my machine. I don’t want my training script
    to consume all of them. How do I restrict my script to run on only a specific
    GPU?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Use* `CUDA_VISIBLE_DEVICES=GPU_ID`. Simply prefix the training script command
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, write the following lines early on in your training script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`GPU_ID` can have values such as 0, 1, 2, and so on. You can see these IDs
    (along with GPU usage) using the `nvidia-smi` command. For assigning to multiple
    GPUs, use a comma-separated list of IDs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Sometimes it feels like there are too many knobs to adjust when training.
    Can it be done automatically, instead, to get the best accuracy?*'
  prefs: []
  type: TYPE_NORMAL
- en: There are many options for automated hyperparameter tuning, including Keras-specific
    Hyperas and Keras Tuner, and more generic frameworks such as Hyperopt and Bayesian
    optimization that perform extensive experimentation to maximize our objective
    (i.e., maximizing accuracy in our case) more intelligently than simple grid searches.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *ResNet and MobileNet work well enough for my use case. Is it possible
    to build a model architecture that can achieve even higher accuracy for my scenario?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three words: Neural Architecture Search (NAS). Let the algorithm find the best
    architecture for you. NAS can be accomplished through packages like Auto-Keras
    and AdaNet.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *How do I go about debugging my TensorFlow script?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is in the question: TensorFlow Debugger (`tfdbg)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *I want to quickly know the input and output layers of my model without
    writing code. How can I accomplish that?*'
  prefs: []
  type: TYPE_NORMAL
- en: Use Netron. It graphically shows your model, and on clicking any layer, provides
    details on the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I need to publish a research paper. Which tool should I use to draw
    my organic, free-range, gluten-free model architecture?*'
  prefs: []
  type: TYPE_NORMAL
- en: MS Paint, obviously! No, we’re just kidding. We are fans of NN-SVG as well as
    PlotNeuralNet for creating high-quality CNN diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Is there a one-stop shop for all models?*'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed! Explore *[PapersWithCode.com](http://PapersWithCode.com)*, *ModelZoo.co*,
    and *ModelDepot.io* for some inspiration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I’ve finished training my model. How can I make it available for others
    to use?*'
  prefs: []
  type: TYPE_NORMAL
- en: You can begin by making the model available for download from GitHub. And then
    list it on the model zoos mentioned in the previous answer. For even wider adoption,
    upload it to TensorFlow Hub (*tfhub.dev*).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the model, you should publish a “model card,” which is essentially
    like a résumé of the model. It’s a short report that details author information,
    accuracy metrics, and the dataset it was benchmarked on. Additionally, it provides
    guidance on potential biases and out-of-scope uses.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I have a model previously trained in framework X, but I need to use
    it in framework Y. Do I need to waste time retraining it in framework Y?*'
  prefs: []
  type: TYPE_NORMAL
- en: Nope. All you need is the power of the ONNX. For models not in the TensorFlow
    ecosystem, most major deep learning libraries support saving them in ONNX format,
    which can then be converted to the TensorFlow format. Microsoft’s MMdnn can help
    in this conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *Could I collect hundreds of images on a topic in a few minutes?*'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, you can collect hundreds of images in three minutes or less with a Chrome
    extension called Fatkun Batch Download Image. Simply search for a keyword in your
    favorite image search engine, filter images by the correct usage rights (e.g.,
    Public Domain), and press the Fatkun extension to download all images. See [Chapter 12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b),
    where we use it to build a Not Hotdog app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus tip: to download from a single website, search for a keyword followed
    by site:website_address. For example, “horse site:[flickr.com](http://flickr.com).”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Forget the browser. How do I scrape Google for images using the command
    line?*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`-k`, `-l`, and `-r` are shorthand for `keyword`, `limit` (number of images),
    and `usage_rights`, respectively. This is a powerful tool with many options for
    controlling and filtering what images to download from Google searches. Plus,
    instead of just loading the thumbnails shown by Google Images, it saves the original
    images linked by the search engine. For saving more than 100 images, install the
    `selenium` library along with `chromedriver`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Those were not enough for collecting images. I need more control. What
    other tools can help me download data in more custom ways beyond the search engine?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a GUI (no programming needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[ScrapeStorm.com](http://ScrapeStorm.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Easy GUI to identify rules for elements to extract
  prefs: []
  type: TYPE_NORMAL
- en: '[WebScraper.io](http://WebScraper.io)'
  prefs: []
  type: TYPE_NORMAL
- en: Chrome-based scraping extension, especially for extracting structured output
    from single websites
  prefs: []
  type: TYPE_NORMAL
- en: '[80legs.com](http://80legs.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based scalable scraper, for parallel, large tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Python-based programmatic tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Scrapy.org](http://Scrapy.org)'
  prefs: []
  type: TYPE_NORMAL
- en: For more programmable controls on scraping, this is one of the most famous scrapers.
    Compared to building your own naive scraper to explore websites, it offers throttling
    rate by domain, proxy, and IP; can handle *robots.txt*; offers flexibility in
    browser headers to show to web servers; and takes care of several possible edge
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: InstaLooter
  prefs: []
  type: TYPE_NORMAL
- en: A Python-based tool for scraping Instagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I have the images for the target classes, but now need images for the
    negative (not item/background) class. Any quick ways to build a big dataset of
    negative classes?*'
  prefs: []
  type: TYPE_NORMAL
- en: '[ImageN](https://oreil.ly/s4Nyk) offers 1,000 images—5 random images for 200
    ImageNet categories—which you can use as the negative class. If you need more,
    download a random sample programmatically from ImageNet.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *How can I search for a prebuilt dataset that suits my needs?*'
  prefs: []
  type: TYPE_NORMAL
- en: Try Google Dataset Search, *VisualData.io*, and *[DatasetList.com](http://DatasetList.com)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *For datasets like ImageNet, downloading, figuring out the format, and
    then loading them for training takes far too much time. Is there an easy way to
    read popular datasets?*'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets is a growing collection of datasets ready to use with TensorFlow.
    It includes ImageNet, COCO (37 GB), and Open Images (565 GB) among others. These
    datasets are exposed as `tf.data.Datasets`, along with performant code to feed
    them in your training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Training on the millions of ImageNet images will take a long, long
    time. Is there a smaller representative dataset I could try training on, to quickly
    experiment and iterate with?*'
  prefs: []
  type: TYPE_NORMAL
- en: Try [Imagenette](https://oreil.ly/NpYBe). Built by Jeremy Howard from fast.ai,
    this 1.4 GB dataset contains only 10 classes instead of 1,000.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *What are the largest readily available datasets that I could use for
    training?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tencent ML Images: 17.7 million images with 11,000 category labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open Images V4 (from Google): 9 million images in 19.7 K categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BDD100K (from UC Berkeley): Images from 100,000 driving videos, over 1,100
    hours'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YFCC100M (from Yahoo): 99.2 million images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q:** *What are some of the readily available large video datasets I could
    use?*'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Details** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| YouTube-8M | 6.1 million videos, 3,862 classes, 2.6 billion audio-visual
    features3.0 labels/video1.53 terabytes of randomly sampled videos |'
  prefs: []
  type: TYPE_TB
- en: '| Something Something(from Twenty Billion Neurons) | 221,000 videos in 174
    action classesFor example, “Pouring water into wine glass but missing so it spills
    next to it”Humans performing predefined actions with everyday objects |'
  prefs: []
  type: TYPE_TB
- en: '| Jester(from Twenty Billion Neurons) | 148,000 videos in 27 classesFor example,
    “Zooming in with two fingers”Predefined hand gestures in front of a webcam |'
  prefs: []
  type: TYPE_TB
- en: '**Q:** *Are those the largest labeled datasets ever assembled in the history
    of time?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nope! Companies like Facebook and Google curate their own private datasets
    that are much larger than the public ones we can play with:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Facebook: 3.5 billion Instagram images with noisy labels (first reported in
    2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google – JFT-300M: 300 million images with noisy labels (first reported in
    2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sadly, unless you’re an employee at one of these companies, you can’t really
    access these datasets. Nice recruiting tactic, we must say.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *How can I get help annotating data?*'
  prefs: []
  type: TYPE_NORMAL
- en: There are several companies out there that can assist with labeling different
    kinds of annotations. A few worth mentioning include SamaSource, Digital Data
    Divide, and iMerit, which employ people who otherwise have limited opportunities,
    eventually creating positive socioeconomic change through employment in underprivileged
    communities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Is there a versioning tool for datasets, like Git is for code?*'
  prefs: []
  type: TYPE_NORMAL
- en: Qri and Quilt can help version control our datasets, aiding in reproducibility
    of experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *What if I don’t have access to a large dataset for my unique problem?*'
  prefs: []
  type: TYPE_NORMAL
- en: Try to develop a synthetic dataset for training! For example, find a realistic
    3D model of the object of interest and place it in realistic environments using
    a 3D framework such as Unity. Adjust the lighting and camera position, zoom, and
    rotation to take snapshots of this object from many angles, generating an endless
    supply of training data. Alternatively, companies like AI.Reverie, CVEDIA, Neuromation,
    Cognata, Mostly.ai, and DataGen Tech provide realistic simulations for training
    needs. One big benefit of synthesized training data is that the labeling process
    is built into the synthesization process. After all, you would know what you are
    creating. This automatic labeling can save a lot of money and effort, compared
    to manual labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *How do I develop a more privacy-preserving model without going down
    the cryptography rabbit hole?*'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Encrypted might be the solution you’re looking for. It enables development
    using encrypted data, which is relevant, especially if you are on the cloud. Internally,
    lots of secure multiparty computation and homomorphic encryptions result in privacy-preserving
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *Can I keep my model under wraps from prying eyes?*'
  prefs: []
  type: TYPE_NORMAL
- en: Well, unless you are on the cloud, weights are visible and can be reverse engineered.
    Use the Fritz library for protecting your model’s IP when deployed on smartphones.
  prefs: []
  type: TYPE_NORMAL
- en: Education and Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *I want to become an AI expert. Beyond this book, where should I invest
    my time to learn more?*'
  prefs: []
  type: TYPE_NORMAL
- en: There are several resources on the internet to learn deep learning in depth.
    We highly recommend these video lectures from some of the best teachers, covering
    a variety of application areas from computer vision to natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Fast.ai (by Jeremy Howard and Rachel Thomas) features a free 14-video lecture
    series, taking a more learn-by-doing approach in PyTorch. Along with the course
    comes an ecosystem of tools and an active community that has led to many breakthroughs
    in the form of research papers and ready-to-use code (like three lines of code
    to train a state-of-the-art network using the fast.ai library).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning.ai (by Andrew Ng) features a five-course “Deep Learning Specialization.”
    It’s free of cost (although you could pay a small fee to get a certificate) and
    will solidify your theoretical foundation further. Dr. Ng’s first Coursera course
    on machine learning has taught more than two million students, and this series
    continues the tradition of highly approachable content loved by beginners and
    experts alike.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We would be remiss if we didn’t encourage you to note [O’Reilly’s Online Learning](http://oreilly.com)
    platform in this list. Helping more than two million users advance their careers,
    it contains hundreds of books, videos, live online trainings, and keynotes given
    by leading thinkers and practitioners at O’Reilly’s AI and data conferences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q:** *Where can I find interesting notebooks to learn from?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Seedbank is a collection of interactive machine learning examples. Built
    on top of Google Colaboratory, these Jupyter notebooks can be run instantly without
    any installations. Some interesting examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating audio with GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action recognition on video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating Shakespeare-esque text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio-style transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q:** *Where can I learn about the state of the art for a specific topic?*'
  prefs: []
  type: TYPE_NORMAL
- en: Considering how fast the state of the art moves in AI, SOTAWHAT is a handy command-line
    tool to search research papers for the latest models, datasets, tasks, and more.
    For example, to look up the latest results on ImageNet, use `sotawhat imagenet`
    on the command line. Additionally, [*paperswithcode.com/sota*](http://paperswithcode.com/sota)
    also features repositories for papers, their source code, and released models,
    along with an interactive visual timeline of benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I am reading a paper on Arxiv and I really like it. Do I need to write
    code from scratch?*'
  prefs: []
  type: TYPE_NORMAL
- en: Not at all! The ResearchCode Chrome extension makes it easy to find code when
    browsing *[arxiv.org](http://arxiv.org)* or Google Scholar. All it takes is a
    press of the extension button. You can also look up code without installing the
    extension on the *[ResearchCode.com](http://ResearchCode.com)* website.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *I don’t want to write any code, but I still want to interactively experiment
    with a model using my camera. How can I do that?*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Runway ML](https://runwayml.com) is an easy-to-use yet powerful GUI tool that
    allows you to download models (from the internet or your own) and use the webcam
    or other input, such as video files, to see the output interactively. This allows
    further combining and remixing outputs of models to make new creations. And all
    of this happens with just a few mouse clicks; hence, it’s attracting a large artist
    community!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q:** *8-1If I can test without code, can I train without code, too?*'
  prefs: []
  type: TYPE_NORMAL
- en: We discuss this in detail in [Chapter 8](part0010.html#9H5K3-13fa565533764549a6f0ab7f11eed62b)
    (web-based) and [Chapter 12](part0014.html#DB7S3-13fa565533764549a6f0ab7f11eed62b)
    (desktop-based). To keep it short, tools such as Microsoft’s [CustomVision.ai](http://CustomVision.ai),
    Google’s Cloud AutoML Vision, Clarifai, Baidu EZDL, and Apple’s Create ML provide
    drag-and-drop training capabilities. Some of these tools take as little as a few
    seconds to do the training.
  prefs: []
  type: TYPE_NORMAL
- en: One Last Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q:** *Tell me a great deep learning prank?*'
  prefs: []
  type: TYPE_NORMAL
- en: Print and hang poster shown in [Figure 7-1](part0009.html#satirical_poster_on_the_state_of_ai_from)
    from [*keras4kindergartners.com*](http://keras4kindergartners.com) near the watercooler,
    and watch people’s reactions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Satirical poster on the state of AI from keras4kindergartners.com](../images/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Satirical poster on the state of AI from [keras4kindergartners.com](http://keras4kindergartners.com)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
