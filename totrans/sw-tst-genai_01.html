<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">1 <a id="idTextAnchor001"/>Enhancing testing with large language models</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-3"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">How large language models can help us with testing</li>
<li class="co-summary-bullet">How to develop a mindset toward working with large language models</li>
</ul>
<p class="body">In this book, we will be using large language models (LLMs) to enhance and accelerate our testing activities—and what is a better way to introduce ourselves to this technology than to ask an LLM to create an introduction for this book:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Hello! I am ChatGPT, a language model designed by OpenAI to assist with a wide range of tasks and answer various questions.</p>
<p class="body-table">Regarding the book, “How AI can be used to help support various software testing activities,” I would recommend “AI-Driven Testing: Adding Intelligence to Your Software Testing Practice” by Julian Harty and Mahesh Sharma.</p>
<p class="body-table">This book explores the application of artificial intelligence (AI) techniques to software testing and how it can help improve the effectiveness and efficiency of testing processes. The authors cover various topics related to AI-driven testing, including machine learning, natural language processing, and computer vision, among others.</p>
<p class="body-table">Additionally, the book includes case studies, practical examples, and best practices for implementing AI-driven testing in different contexts. It is a valuable resource for software testers, developers, and anyone interested in leveraging AI to improve the quality of their software products.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">This artificially generated response, which reads as if it were written by a human, was created by sending a one-sentence instruction, known as a <i class="fm-italics">prompt,</i> to ChatGPT:<a id="idIndexMarker001"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">ChatGPT, please introduce yourself and a book about how AI can be used to help support various software testing activities.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Defining what LLMs do and determining the potential they offer are both easy and difficult. To gain the most benefit from these tools, it is necessary to find a balance between the two. At first glance, LLMs work by simply taking instructions from a user and responding with an answer using natural language. But this simple explanation doesn’t do justice to the potential an LLM can offer us as we test, nor does it explain the challenges that must be overcome to maximize their advantages. So, before we begin our journey to the world of LLMs and testing, let’s find out how LLMs can help us and what we need to be aware of to successfully use them.</p>
<h2 class="fm-head" id="heading_id_3">1.1 Recognizing the effect of AI tools on testing and development</h2>
<p class="body"><a id="marker-4"/>In the past, an individual who wanted to take advantage of AI was required to have the skills to develop, train, and deploy an AI model or have access to a team of experts to complete these tasks, all of which would make use of AI in everyday activities an expensive and exclusive endeavor. With the recent advances in AI and publicly available LLMs such as ChatGPT and Gemini, open source generative models, and fine-tuning and retrieval methods for generative AI, we’re now beginning to benefit from what some refer to as AI democratization.<a id="idIndexMarker002"/></p>
<p class="body">The barrier to integrating AI into our everyday work has dropped dramatically. Social media managers can now use LLMs to generate catchy and engaging copy, analysts can summarize unstructured data into clear and concise reports, and customer support agents can rapidly generate bespoke responses to customers with a few simple prompts. The use of LLM is not limited to data scientists and AI scholars anymore, and it is also advantageous for those of us who work in testing and software development.</p>
<p class="body">Good testing helps challenge assumptions and educate our teams on how our products truly behave in given situations. The more we test, the more we learn. But, as most professional testers will verify, there is never enough time to test everything we want. So, to be able to test more efficiently, we look for tools and techniques from automation to shift-left testing. LLMs offer another potential avenue to help us enhance the quality and scope of our testing so that we can discover and share more, which in turn can help our teams improve quality further.</p>
<p class="body">What makes LLMs so useful is that they summarize, transform, generate, and translate information in a way that is easy for humans to understand and that professionals responsible for testing can use for their testing needs—all of which are available through simple chat interfaces or APIs. LLMs can assist us in rapidly creating test automation or provide support as we carry out testing ourselves. And if we develop the right skills to identify when LLMs can help and use them sensibly, we begin to test faster, further, and more effectively. To help illustrate this concept and what we’ll be learning in this book, let’s take a look at some brief examples.</p>
<h3 class="fm-head1" id="heading_id_4">1.1.1 Data generation</h3>
<p class="body">Creating and managing test data can be one of the most complex aspects of testing. Creating realistic, useful, and anonymized data can make or break the success of testing, and doing it effectively can be a drain on resources. LLMs offer the ability to generate and transform data rapidly, speeding up the test data management process. By taking existing data and converting it to new formats or using it to generate new synthetic data, we can utilize LLMs to assist us with our test data requirements and get more time to drive testing forward.<a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="marker-5"/></p>
<h3 class="fm-head1" id="heading_id_5">1.1.2 Automated test building</h3>
<p class="body">Similarly, LLMs’ abilities to generate and transform can be used during the process of creating and maintaining automation. Although I would not advise having LLMs solely create automated tests for us, they can be used in targeted ways to help us create page objects, boilerplate classes, helper methods, and frameworks rapidly. By combining the knowledge of our products and our test design skills, we can identify the parts of the automation process that are algorithmic and structured in nature and use LLMs to speed up those segments of the automation process.<a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>
<h3 class="fm-head1" id="heading_id_6">1.1.3 Test design</h3>
<p class="body">Perhaps a less commonly discussed topic is how LLMs can help us in the process of identifying and designing tests. Similar to automated testing, the value of LLMs lies not in completely replacing our test design abilities but rather in augmenting them. We can use LLMs to overcome biases and blind spots to expand and suggest ideas based on current test design ideas we might have. We can also summarize and describe complex ideas in ways that make them more digestible for us to springboard test ideas from.<a id="idIndexMarker007"/><a id="idIndexMarker008"/></p>
<p class="body">We will explore such examples in this book and more to help us better appreciate when and where LLMs can be used and how to use them in a way that accelerates our testing. We’ll explore how to build prompts to support us in building quality production and automation code, rapidly creating test data, and enhancing our test design for both scripted and exploratory testing. We’ll also explore how we can fine-tune our own LLMs that will work as assistants to us in our testing, digesting domain knowledge and using it to help guide us toward building better-quality products.<a id="idIndexMarker009"/></p>
<h2 class="fm-head" id="heading_id_7">1.2 Delivering value with LLMs</h2>
<p class="body">Testing is a collaborative process, and all team members are responsible for testing. How we contribute to the testing process differs based on our role and experience, but we all participate. Thus, throughout this book, we’ll approach the use of LLMs with a critical mindset, discovering various ways in which we can use LLMs to help enhance the multiple types of testing we do. The intention is to give you the skills to identify and utilize LLMs to enhance and accelerate your testing, whether you are in a professional testing role or a developer who contributes to the testing process, all of which we can do by establishing some rules around the relationship between ourselves and the LLMs we want to use.<a id="idIndexMarker010"/></p>
<h3 class="fm-head1" id="heading_id_8">1.2.1 A model for delivering value</h3>
<p class="body">To get the most out of LLMs, we need to focus on three core principles this book has been structured around: the mindset, technique, and context (figure 1.1).<a id="marker-6"/><a id="idIndexMarker011"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" src="../../OEBPS/Images/CH01_F01_Winteringham2.png"/></p>
<p class="figurecaption">Figure 1.1 A model outlining the three tenets of success with generative AI</p>
</div>
<p class="body">We’ll explore these three core principles in depth in different parts of this book, starting with mindset. But to better appreciate why they are needed, let’s discuss each of them briefly to understand what they mean and why they are required.</p>
<p class="fm-head2">Mindset</p>
<p class="body">This is perhaps the most essential of the three principles, as having the correct mindset toward how we utilize LLMs can dramatically increase or decrease their value. Having the correct mindset means having a clear sense of the purpose and value of testing, the capabilities of LLMs, and how to build a relationship between both in which LLMs are used in focused, targeted ways.<a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>
<p class="fm-head2">Technique</p>
<p class="body">While understanding where to use LLMs is vital, we also need the ability to work with them in a way that maximizes their value. In the context of LLMs, this means learning how to create and curate instructions that clearly communicate what we want an LLM to do, as well as ensuring that it responds in a way that is useful and avoids risks of misinformation. The ecosystem and abilities around LLMs have grown extensively, meaning learning about other techniques, such as integrating with API platforms for LLMs, and AI agents can help us identify and create more advanced opportunities with LLMs.<a id="idIndexMarker014"/><a id="idIndexMarker015"/></p>
<p class="fm-head2">Context</p>
<p class="body">As we progress, you’ll notice how applicable the rule of garbage in, garbage out is to LLMs. If we prompt an LLM with a generalized, context-free request, we get a shallow, contextless response. Although technique can help us somewhat to maximize an LLM’s response, the final piece of the puzzle is being able to provide an LLM with enough context so that it can respond in a way relevant to our needs. As you’ll learn, there are different ways to approach this, such as retrieval-augmented generation (RAG) and fine-tuning, each having its challenges to consider and rewards to take advantage of.<a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
<p class="body">As mentioned, this book has been structured in such a way that the three principles are explored in depth to help us get the most from LLMs. So, let’s drill further into the concept of mindset and establish what a good mindset means before returning to technique and context later.</p>
<h3 class="fm-head1" id="heading_id_9">1.2.2 Using both human and AI abilities</h3>
<p class="body">Throughout this book, you’ll learn not just how to use LLMs but also how to establish a working practice in which our testing benefits from our abilities, as well as from LLMs. The value of any tool, whether it’s AI-based or not, comes not from its intrinsic features but rather from the relationship between the user and the tool. We can think of that relationship as an area of the effect model, as shown in figure 1.2.<a id="idIndexMarker018"/><a id="marker-7"/></p>
<p class="body">This diagram demonstrates that to increase our influence, a healthy relationship with tools that focus on expanding and enhancing our core abilities is required. Without this balance, we become too dependent on tools or ourselves. We can certainly rely on our abilities, but our area of effect will be smaller, bound by constraints such as time, attention, and biases. Basically, without tools, we can do only so much. But also we cannot defer all the work to tooling. Without an individual at the core, tools lack direction. Yes, tools can be autonomous and set off to run by themselves, but if they aren’t delivering feedback to a human, then no value is being extracted from them. The area of effect is missing its center. That’s why this book aims to teach you about how to strike a balance in the use of LLMs in testing, focusing as much on our abilities to identify opportunities for LLM use as the skills to get the most value of them.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre9" src="../../OEBPS/Images/CH01_F02_Winteringham2.png"/></p>
<p class="figurecaption">Figure 1.2 An outline of how tools enhance and extend the reach of testing</p>
</div>
<p class="body">To help us better understand this relationship, let’s consider an example in which we want to create tests for a file upload feature. First, we receive a user story like the following:<a id="marker-8"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td"/>
<td class="contenttable-0-td">
<p class="body-table-list-a">   •   As a paralegal, in order to store and share legal documents, I want to be able to securely upload documents.</p>
<p class="body-table-list-a">   •   Acceptance criteria:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Upload should support the following common file formats: PDF, DOCX, TXT</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   The maximum file size should be 20MB</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   When a document is uploaded it should give a progress update to the user</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Once uploaded, the document must initially be only accessible to the person who has uploaded it</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   A report of the upload and whether it was successful or not will be stored in the auditing feature</p>
</td>
</tr>
</tbody>
</table>
<p class="body">From this, we decide to simply rely on an LLM to generate the work for us—for example, sending a prompt like this:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Create tests for a file upload feature</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Sending this prompt to an LLM such as ChatGPT might return something like</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  Test for File Upload Functionality:</p>
<p class="body-table-ind-a">Test Case 1.1: Upload a Valid File</p>
<p class="body-table-ind-b">Input: Choose a file with a valid format (e.g., .jpg, .pdf).</p>
<p class="body-table-ind-b">Expected Outcome: The file is successfully uploaded, and a confirmation message is displayed.</p>
<p class="body-table-ind-a">Test Case 1.2: Attempt to Upload an Invalid File</p>
<p class="body-table-ind-b">Input: Choose a file with an invalid format (e.g., .exe, .xhtml).</p>
<p class="body-table-ind-b">Expected Outcome: An error message is displayed, indicating that the file format is not supported.<a id="marker-9"/></p>
<p class="body-table-list-a">   2.  Test for File Size Limit:</p>
<p class="body-table-ind-a">Test Case 2.1: Upload a File Below the Size Limit</p>
<p class="body-table-ind-b">Input: Choose a file smaller than the maximum allowed size.</p>
<p class="body-table-ind-b">Expected Outcome: The file is successfully uploaded, and a confirmation message is displayed.</p>
<p class="body-table-ind-a">Test Case 2.2: Attempt to Upload a File Exceeding the Size Limit</p>
<p class="body-table-ind-b">Input: Choose a file larger than the maximum allowed size.</p>
<p class="body-table-ind-b">Expected Outcome: An error message is displayed, indicating that the file exceeds the maximum size limit.</p>
<p class="body-table-list-a">   3.  Test for File Overwrite Protection:</p>
<p class="body-table-ind-a">Test Case 3.1: Upload a File with the Same Name as an Existing File</p>
<p class="body-table-ind-b">Input: Upload a file with a name that matches an existing file in the system.</p>
<p class="body-table-ind-b">Expected Outcome: An error message is displayed, indicating that a file with the same name already exists.</p>
<p class="body-table-list-a">   4.  Security Tests:</p>
<p class="body-table-ind-a">Test Case 4.1: Attempt to Upload a Malicious File (e.g., virus)</p>
<p class="body-table-ind-b">Input: Upload a known malicious file.</p>
<p class="body-table-ind-b">Expected Outcome: The system should detect the malicious file and prevent its upload, displaying a security warning.</p>
<p class="body-table-list-a">   5.  UI and User Experience Tests:</p>
<p class="body-table-ind-a">Test Case 5.1: Check for a User-Friendly Interface</p>
<p class="body-table-ind-b">Input: Use the file upload feature and evaluate the user interface for clarity and ease of use.</p>
<p class="body-table-ind-b">Expected Outcome: The interface should be intuitive, with clear instructions and feedback for users.</p>
<p class="body-table-ind-a">Test Case 5.2: Check for Proper Error Messages</p>
<p class="body-table-ind-b">Input: Attempt various erroneous actions during the file upload process.</p>
<p class="body-table-ind-b">Expected Outcome: Error messages should be descriptive, helping users understand and resolve issues.</p>
<p class="body-table-list-a">   6.  Integration Tests:</p>
<p class="body-table-ind-a">Test Case 6.1: Check File Upload with Other Features</p>
<p class="body-table-ind-b">Input: Test file uploads in conjunction with other features that depend on uploaded files (e.g., image processing, document storage).</p>
<p class="body-table-ind-b">Expected Outcome: Ensure the uploaded files are correctly processed and integrated with other system components.</p>
<p class="body-table-list-a">   7.  Performance Tests:</p>
<p class="body-table-ind-a">Test Case 7.1: Upload a Large Number of Files Concurrently</p>
<p class="body-table-ind-b">Input: Simulate multiple users uploading a large number of files simultaneously.</p>
<p class="body-table-ind-b">Expected Outcome: The system should handle concurrent uploads without significant performance degradation or crashes.</p>
<p class="body-table-list-a">   8.  Stress Tests:</p>
<p class="body-table-ind-a">Test Case 8.1: Upload Files Continuously for an Extended Period</p>
<p class="body-table-ind-b">Input: Continuously upload files for an extended period to assess system stability.</p>
<p class="body-table-ind-b">Expected Outcome: The system should not experience memory leaks, crashes, or performance degradation during this stress test.</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-10"/>At first glance, the response seems impressive. The LLM has provided us with a list of different tests to consider, which gives us the impression that we have everything we need to begin testing. However, if we start to consider the response more carefully, we can notice some problems:</p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Test case 1.1 suggests testing with valid formats but offers examples of formats our user story doesn’t support.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The suggestions from number 4 onward are shallow. For example, the Security test offers a test to insert a malicious file but nothing else. Surely, there would be other security problems to consider.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The expected outcomes are also very broad for each test case. For example, what does this response mean?</p>
</li>
</ul>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Error messages should be descriptive, helping users understand and resolve issues</p>
</td>
</tr>
</tbody>
</table>
<p class="body">LLMs are designed to give a response regardless of what they’ve been asked to do, and although that might sound useful, it becomes problematic when the instructions it has been given are vague. The example we’ve looked at isn’t being used to criticize the capabilities of LLMs but rather to help us appreciate that an LLM’s response will be as detailed or as generic as the prompt it has been given. This means the responses we’ve received from the LLM are broad and vague because our prompt is vague. By not giving much thought to what we want to ask an LLM to do, the absence of any context or detail in the prompt means what has been returned is pretty useless—a classic example of garbage in, garbage out.<a id="idIndexMarker019"/></p>
<p class="body"><a id="marker-11"/>This is what the area of the effect diagram from earlier attempts to define. To get value from LLMs, we need to design prompts that are clear, contextual, and specific so that we can extract more value from an LLM. Instead of deferring all responsibility to a tool to create the response we desire, we instead appreciate that our understanding of a context and the ability to distill that understanding into clear instructions are required to create a prompt that will maximize the response from an LLM, leading to better-written prompts, such as</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Act as a professional software tester and suggest test ideas for the feature delimited by three hashes. All test ideas should be focused on discovering risks that impact the quality characteristics: Functionality, Data Integrity and Security</p>
<p class="body-table">###</p>
<p class="body-table-list-a">   •   As a paralegal, in order to store and share legal documents, I want to be able to securely upload documents.</p>
<p class="body-table-list-a">   •   Acceptance criteria:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Upload should support the following common file formats: PDF, DOCX, TXT</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   The maximum file size should be 20MB</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   When a document is uploaded it should give a progress update to the user</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Once uploaded, the document must initially be only accessible to the person who has uploaded it</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   A report of the upload and whether it was successful or not will be stored in the auditing feature</p>
<p class="body-table">###</p>
</td>
</tr>
</tbody>
</table>
<p class="body">which, when sent to an LLM, returns a response like this:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Functionality:</p>
<p class="body-table-list-a">   •   Format Compatibility Testing:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Verify that the upload feature supports the following file formats: PDF, DOCX, and TXT.</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Attempt to upload files in various formats and ensure that the system handles them correctly.</p>
<p class="body-table-list-a">   •   File Size Limitation Testing:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test whether the system enforces a maximum file size limit of 20MB during the upload process.</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Attempt to upload files larger and smaller than 20MB to validate the system’s handling of these cases.</p>
<p class="body-table-list-a">   •   Progress Update Testing:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Check that users receive real-time progress updates while a document is being uploaded.</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Verify that these updates are clear and provide meaningful information to the user.</p>
<p class="body-table">Data Integrity:</p>
<p class="body-table-list-a">   •   Document Content Verification:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Upload files with known content and compare the content of the uploaded document with the original.</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Ensure that the content remains intact and no data corruption occurs during the upload process.</p>
<p class="body-table-list-a">   •   Document Privacy:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test that uploaded documents are initially only accessible to the person who uploaded them.</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Attempt to access the uploaded documents using another user’s account to ensure that access control is enforced.</p>
<p class="body-table-list-a">   •   Data Auditing Verification:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Confirm that the system accurately records all uploads in the auditing feature.</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Verify that the audit logs include information such as the timestamp, uploader, document name, and whether the upload was successful.</p>
<p class="body-table">Security:<a id="marker-12"/></p>
<p class="body-table-list-a">   •   File Type Security Testing:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Attempt to upload files with malicious content, such as scripts or malware, and ensure that the system detects and rejects them.</p>
<p class="body-table-list-a">   •   Access Control Testing:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Validate that the system properly enforces access control by ensuring that other users cannot access the documents uploaded by someone else.</p>
<p class="body-table-list-a">   •   Secure Data Transmission:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Ensure that the document upload process uses secure communication protocols (e.g., HTTPS) to protect data during transmission.</p>
<p class="body-table-list-a">   •   User Authentication and Authorization:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Verify that the system correctly authenticates and authorizes users before allowing them to upload or access documents.</p>
<p class="body-table-list-a">   •   Secure Data Storage:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Assess how the system stores uploaded documents to ensure that they are encrypted or adequately protected against unauthorized access or data breaches.</p>
<p class="body-table-list-a">   •   Session Management:</p>
<p class="body-table-list-b">   <span class="cambria">−</span>   Test how the system manages user sessions, ensuring that sessions are terminated after a period of inactivity to prevent unauthorized access.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">By creating a more focused and contextual prompt, we have created a response that offers a richer range of suggestions for our testing. This would not be achieved by an LLM alone but rather through the symbiosis of our skills to learn and frame our context into instructions that LLMs can take and rapidly expand on.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Activity 1.1</p>
<p class="fm-sidebar-text">Try out the sample prompts we’ve explored in this chapter and see what responses you receive. To familiarize yourself with an LLM, read appendix A, which shares how to set up and send a prompt to ChatGPT.</p>
</div>
<h3 class="fm-head1" id="heading_id_10">1.2.3 Being skeptical of LLMs</h3>
<p class="body">Although a lot can be said about the potential of LLMs, we should be wary of taking their abilities for granted. For example, consider our introduction to this book from ChatGPT. It confidently recommended to us that we should read the book <i class="fm-italics">AI-Driven Testing: Adding Intelligence to Your Software Testing Practice.</i> The problem is that this book doesn’t exist and was never written by Julian Harty and Mahesh Sharma. The LLM simply made up this title. (We’ll explore more about why this happens in chapter 2.)<a id="marker-13"/><a id="idIndexMarker020"/></p>
<p class="body">LLMs offer much potential, but they are not a solution for every problem, nor are they a single oracle of truth. We will explore further in Chapter 2 how LLMs use probability to determine responses and how an LLM comes to a solution is not the same way humans do, which highlights the second aspect of our area of effect model. We must use our skepticism to determine what is and isn’t of value from an LLM response.</p>
<p class="body">To blindly accept what an LLM output is, at best, putting us at risk of slowing our work down rather than accelerating it—and at worst, influencing us to carry out testing that can have a detrimental effect on the quality of our products. We must remind ourselves that we—not LLMs—are the ones who are leading the problem-solving activity. This can be difficult at times when working with tools that communicate in a way that feels so human, but to do so exposes us to the aforementioned risks. That’s why, in our area of effect model, we use our abilities to pick the elements from the LLM response that serve us well and reject and reevaluate how we instruct an LLM when it responds in a way that is not satisfactory.</p>
<p class="body">As we progress through the book and learn more about LLMs and how they can contribute to testing, we will keep our area of effect model in mind so that you, the reader, can develop the ability to use LLMs in testing in a way that is sober, considered, and valuable to you and your team.<a id="idIndexMarker021"/></p>
<h2 class="fm-head" id="heading_id_11">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Large language models (LLMs) work by taking a prompt we wrote and returning a response.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The popularity of LLMs stems from the ease with which they offer access to powerful AI algorithms.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs have helped many people in different roles and can also help us in testing.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We can use LLMs for a wide range of testing activities, from test design to automation.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We want to avoid overusing LLMs and must always be critical of how they work.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Success with LLMs comes from us appreciating the skills and abilities we bring to the process of using them.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If our prompts to LLMs are shallow and generic, their response will be the same.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Instead, we should use our skills to understand and frame a problem and use that to prompt LLMs to respond in the most valuable way possible.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We must also be skeptical of the responses we get from LLMs to ensure the responses we get from them are of value to us.<a id="marker-14"/></p>
</li>
</ul>
</div></body></html>