- en: Chapter 2\. End-to-End Machine Learning Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。端到端的机器学习项目
- en: 'In this chapter you will work through an example project end to end, pretending
    to be a recently hired data scientist at a real estate company. This example is
    fictitious; the goal is to illustrate the main steps of a machine learning project,
    not to learn anything about the real estate business. Here are the main steps
    we will walk through:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将通过一个示例项目端到端地工作，假装自己是一家房地产公司最近雇用的数据科学家。这个例子是虚构的；目标是说明机器学习项目的主要步骤，而不是了解房地产业务。以下是我们将要走过的主要步骤：
- en: Look at the big picture.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看大局。
- en: Get the data.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据。
- en: Explore and visualize the data to gain insights.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索和可视化数据以获得洞见。
- en: Prepare the data for machine learning algorithms.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为机器学习算法准备数据。
- en: Select a model and train it.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型并训练它。
- en: Fine-tune your model.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调您的模型。
- en: Present your solution.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 呈现解决方案。
- en: Launch, monitor, and maintain your system.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动、监控和维护您的系统。
- en: Working with Real Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用真实数据
- en: 'When you are learning about machine learning, it is best to experiment with
    real-world data, not artificial datasets. Fortunately, there are thousands of
    open datasets to choose from, ranging across all sorts of domains. Here are a
    few places you can look to get data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当您学习机器学习时，最好尝试使用真实世界的数据，而不是人工数据集。幸运的是，有成千上万的开放数据集可供选择，涵盖各种领域。以下是您可以查找数据的一些地方：
- en: 'Popular open data repositories:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的开放数据存储库：
- en: '[OpenML.org](https://openml.org)'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenML.org](https://openml.org)'
- en: '[Kaggle.com](https://kaggle.com/datasets)'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle.com](https://kaggle.com/datasets)'
- en: '[PapersWithCode.com](https://paperswithcode.com/datasets)'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PapersWithCode.com](https://paperswithcode.com/datasets)'
- en: '[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml)'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UC Irvine机器学习存储库](https://archive.ics.uci.edu/ml)'
- en: '[Amazon’s AWS datasets](https://registry.opendata.aws)'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[亚马逊的AWS数据集](https://registry.opendata.aws)'
- en: '[TensorFlow datasets](https://tensorflow.org/datasets)'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow数据集](https://tensorflow.org/datasets)'
- en: 'Meta portals (they list open data repositories):'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元门户网站（它们列出开放数据存储库）：
- en: '[DataPortals.org](https://dataportals.org)'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DataPortals.org](https://dataportals.org)'
- en: '[OpenDataMonitor.eu](https://opendatamonitor.eu)'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenDataMonitor.eu](https://opendatamonitor.eu)'
- en: 'Other pages listing many popular open data repositories:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他页面列出了许多流行的开放数据存储库：
- en: '[Wikipedia’s list of machine learning datasets](https://homl.info/9)'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[维基百科的机器学习数据集列表](https://homl.info/9)'
- en: '[Quora.com](https://homl.info/10)'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Quora.com](https://homl.info/10)'
- en: '[The datasets subreddit](https://reddit.com/r/datasets)'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据集的subreddit](https://reddit.com/r/datasets)'
- en: In this chapter we’ll use the California Housing Prices dataset from the StatLib
    repository⁠^([1](ch02.html#idm45720251177184)) (see [Figure 2-1](#california_housing_prices_plot)).
    This dataset is based on data from the 1990 California census. It is not exactly
    recent (a nice house in the Bay Area was still affordable at the time), but it
    has many qualities for learning, so we will pretend it is recent data. For teaching
    purposes I’ve added a categorical attribute and removed a few features.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用StatLib存储库中的加利福尼亚房价数据集（见[图2-1](#california_housing_prices_plot)）。该数据集基于1990年加利福尼亚人口普查数据。虽然不是最新的（在那个时候，旧金山湾区的漂亮房子仍然是负担得起的），但它具有许多学习的优点，因此我们将假装它是最新的数据。出于教学目的，我添加了一个分类属性并删除了一些特征。
- en: '![mls3 0201](assets/mls3_0201.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0201](assets/mls3_0201.png)'
- en: Figure 2-1\. California housing prices
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。加利福尼亚房屋价格
- en: Look at the Big Picture
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看大局
- en: Welcome to the Machine Learning Housing Corporation! Your first task is to use
    California census data to build a model of housing prices in the state. This data
    includes metrics such as the population, median income, and median housing price
    for each block group in California. Block groups are the smallest geographical
    unit for which the US Census Bureau publishes sample data (a block group typically
    has a population of 600 to 3,000 people). I will call them “districts” for short.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到机器学习房地产公司！您的第一个任务是使用加利福尼亚人口普查数据来建立该州房价模型。这些数据包括加利福尼亚每个街区组的人口、中位收入和中位房价等指标。街区组是美国人口普查局发布样本数据的最小地理单位（一个街区组通常有600到3000人口）。我会简称它们为“区”。
- en: Your model should learn from this data and be able to predict the median housing
    price in any district, given all the other metrics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型应该从这些数据中学习，并能够预测任何地区的房屋中位数价格，考虑到所有其他指标。
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Since you are a well-organized data scientist, the first thing you should do
    is pull out your machine learning project checklist. You can start with the one
    in [Appendix A](app01.html#project_checklist_appendix); it should work reasonably
    well for most machine learning projects, but make sure to adapt it to your needs.
    In this chapter we will go through many checklist items, but we will also skip
    a few, either because they are self-explanatory or because they will be discussed
    in later chapters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您是一个组织良好的数据科学家，您应该做的第一件事是拿出您的机器学习项目清单。您可以从[附录A](app01.html#project_checklist_appendix)开始；对于大多数机器学习项目来说，这应该运行得相当顺利，但请确保根据您的需求进行调整。在本章中，我们将逐个检查许多项目，但也会跳过一些，要么是因为它们是不言自明的，要么是因为它们将在后续章节中讨论。
- en: Frame the Problem
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建问题框架
- en: The first question to ask your boss is what exactly the business objective is.
    Building a model is probably not the end goal. How does the company expect to
    use and benefit from this model? Knowing the objective is important because it
    will determine how you frame the problem, which algorithms you will select, which
    performance measure you will use to evaluate your model, and how much effort you
    will spend tweaking it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 向老板提出问题的第一个问题是业务目标究竟是什么。构建模型可能不是最终目标。公司希望如何使用和从这个模型中受益？知道目标很重要，因为它将决定您如何构建问题，选择哪些算法，使用哪种性能度量来评估您的模型，以及您将花费多少精力来调整它。
- en: Your boss answers that your model’s output (a prediction of a district’s median
    housing price) will be fed to another machine learning system (see [Figure 2-2](#house_pricing_pipeline_diagram)),
    along with many other signals.⁠^([2](ch02.html#idm45720251160848)) This downstream
    system will determine whether it is worth investing in a given area. Getting this
    right is critical, as it directly affects revenue.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question to ask your boss is what the current solution looks like
    (if any). The current situation will often give you a reference for performance,
    as well as insights on how to solve the problem. Your boss answers that the district
    housing prices are currently estimated manually by experts: a team gathers up-to-date
    information about a district, and when they cannot get the median housing price,
    they estimate it using complex rules.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0202](assets/mls3_0202.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. A machine learning pipeline for real estate investments
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is costly and time-consuming, and their estimates are not great; in cases
    where they manage to find out the actual median housing price, they often realize
    that their estimates were off by more than 30%. This is why the company thinks
    that it would be useful to train a model to predict a district’s median housing
    price, given other data about that district. The census data looks like a great
    dataset to exploit for this purpose, since it includes the median housing prices
    of thousands of districts, as well as other data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this information, you are now ready to start designing your system.
    First, determine what kind of training supervision the model will need: is it
    a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement
    learning task? And is it a classification task, a regression task, or something
    else? Should you use batch learning or online learning techniques? Before you
    read on, pause and try to answer these questions for yourself.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Have you found the answers? Let’s see. This is clearly a typical supervised
    learning task, since the model can be trained with *labeled* examples (each instance
    comes with the expected output, i.e., the district’s median housing price). It
    is a typical regression task, since the model will be asked to predict a value.
    More specifically, this is a *multiple regression* problem, since the system will
    use multiple features to make a prediction (the district’s population, the median
    income, etc.). It is also a *univariate regression* problem, since we are only
    trying to predict a single value for each district. If we were trying to predict
    multiple values per district, it would be a *multivariate regression* problem.
    Finally, there is no continuous flow of data coming into the system, there is
    no particular need to adjust to changing data rapidly, and the data is small enough
    to fit in memory, so plain batch learning should do just fine.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the data were huge, you could either split your batch learning work across
    multiple servers (using the MapReduce technique) or use an online learning technique.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Select a Performance Measure
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your next step is to select a performance measure. A typical performance measure
    for regression problems is the *root mean square error* (RMSE). It gives an idea
    of how much error the system typically makes in its predictions, with a higher
    weight given to large errors. [Equation 2-1](#rmse_equation) shows the mathematical
    formula to compute the RMSE.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-1\. Root mean square error (RMSE)
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>RMSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mfenced separators="" open="(" close=")"><mi>h</mi><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow><mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>RMSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msup><mfenced separators="" open="(" close=")"><mi>h</mi><mrow><mo>(</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow><mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: 'Although the RMSE is generally the preferred performance measure for regression
    tasks, in some contexts you may prefer to use another function. For example, if
    there are many outlier districts. In that case, you may consider using the *mean
    absolute error* (MAE, also called the *average absolute deviation*), shown in
    [Equation 2-2](#mae_equation):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管均方根误差通常是回归任务的首选性能度量，但在某些情况下，您可能更喜欢使用另一个函数。例如，如果有许多异常值区域。在这种情况下，您可能考虑使用*平均绝对误差*（MAE，也称为*平均绝对偏差*），如[方程2-2](#mae_equation)所示：
- en: Equation 2-2\. Mean absolute error (MAE)
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程2-2. 平均绝对误差（MAE）
- en: <math display="block"><mrow><mtext>MAE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mfenced separators="" open="|" close="|"><mi>h</mi> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>-</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>MAE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <mfenced separators="" open="|" close="|"><mi>h</mi> <mrow><mo>(</mo> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>-</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: 'Both the RMSE and the MAE are ways to measure the distance between two vectors:
    the vector of predictions and the vector of target values. Various distance measures,
    or *norms*, are possible:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差（RMSE）和平均绝对误差（MAE）都是衡量两个向量之间距离的方法：预测向量和目标值向量。各种距离度量，或*范数*，都是可能的：
- en: 'Computing the root of a sum of squares (RMSE) corresponds to the *Euclidean
    norm*: this is the notion of distance we are all familiar with. It is also called
    the ℓ[2] *norm*, noted ∥ · ∥[2] (or just ∥ · ∥).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算平方和的根（RMSE）对应于*欧几里德范数*：这是我们都熟悉的距离概念。它也被称为ℓ[2] *范数*，表示为∥ · ∥[2]（或只是∥ · ∥）。
- en: Computing the sum of absolutes (MAE) corresponds to the ℓ[1] *norm*, noted ∥
    · ∥[1]. This is sometimes called the *Manhattan norm* because it measures the
    distance between two points in a city if you can only travel along orthogonal
    city blocks.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算绝对值之和（MAE）对应于ℓ[1] *范数*，表示为∥ · ∥[1]。这有时被称为*曼哈顿范数*，因为它测量了在城市中两点之间的距离，如果您只能沿着正交的城市街区行驶。
- en: More generally, the ℓ[*k*] *norm* of a vector **v** containing *n* elements
    is defined as ∥**v**∥[*k*] = (|*v*[1]|^(*k*) + |*v*[2]|^(*k*) + ... + |*v*[*n*]|^(*k*))^(1/*k*).
    ℓ[0] gives the number of nonzero elements in the vector, and ℓ[∞] gives the maximum
    absolute value in the vector.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更一般地，包含*n*个元素的向量**v**的ℓ[*k*] *范数*被定义为∥**v**∥[*k*] = (|*v*[1]|^(*k*) + |*v*[2]|^(*k*)
    + ... + |*v*[*n*]|^(*k*))^(1/*k*)。ℓ[0]给出向量中非零元素的数量，ℓ[∞]给出向量中的最大绝对值。
- en: The higher the norm index, the more it focuses on large values and neglects
    small ones. This is why the RMSE is more sensitive to outliers than the MAE. But
    when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs
    very well and is generally preferred.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 范数指数越高，就越关注大值并忽略小值。这就是为什么均方根误差（RMSE）比平均绝对误差（MAE）更容易受到异常值的影响。但是当异常值呈指数稀有性（如钟形曲线）时，均方根误差表现非常好，通常更受青睐。
- en: Check the Assumptions
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查假设
- en: Lastly, it is good practice to list and verify the assumptions that have been
    made so far (by you or others); this can help you catch serious issues early on.
    For example, the district prices that your system outputs are going to be fed
    into a downstream machine learning system, and you assume that these prices are
    going to be used as such. But what if the downstream system converts the prices
    into categories (e.g., “cheap”, “medium”, or “expensive”) and then uses those
    categories instead of the prices themselves? In this case, getting the price perfectly
    right is not important at all; your system just needs to get the category right.
    If that’s so, then the problem should have been framed as a classification task,
    not a regression task. You don’t want to find this out after working on a regression
    system for months.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，列出并验证迄今为止已经做出的假设（由您或他人）是一个很好的做法；这可以帮助您及早发现严重问题。例如，您的系统输出的区域价格将被馈送到下游机器学习系统中，您假设这些价格将被如此使用。但是，如果下游系统将价格转换为类别（例如，“便宜”，“中等”或“昂贵”），然后使用这些类别而不是价格本身呢？在这种情况下，完全准确地获取价格并不重要；您的系统只需要获取正确的类别。如果是这样，那么问题应该被定义为一个分类任务，而不是一个回归任务。您不希望在为回归系统工作数月后才发现这一点。
- en: Fortunately, after talking with the team in charge of the downstream system,
    you are confident that they do indeed need the actual prices, not just categories.
    Great! You’re all set, the lights are green, and you can start coding now!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在与负责下游系统的团队交谈后，您确信他们确实需要实际价格，而不仅仅是类别。太好了！您已经准备就绪，灯光是绿色的，现在可以开始编码了！
- en: Get the Data
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and
    walk through the code examples. As I mentioned in the preface, all the code examples
    in this book are open source and available [online](https://github.com/ageron/handson-ml3)
    as Jupyter notebooks, which are interactive documents containing text, images,
    and executable code snippets (Python in our case). In this book I will assume
    you are running these notebooks on Google Colab, a free service that lets you
    run any Jupyter notebook directly online, without having to install anything on
    your machine. If you want to use another online platform (e.g., Kaggle) or if
    you want to install everything locally on your own machine, please see the instructions
    on the book’s GitHub page.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是动手的时候了。毫不犹豫地拿起你的笔记本电脑，浏览代码示例。正如我在前言中提到的，本书中的所有代码示例都是开源的，可在[在线](https://github.com/ageron/handson-ml3)作为Jupyter笔记本使用，这些笔记本是交互式文档，包含文本、图像和可执行代码片段（在我们的情况下是Python）。在本书中，我假设您正在Google
    Colab上运行这些笔记本，这是一个免费服务，让您可以直接在线运行任何Jupyter笔记本，而无需在您的计算机上安装任何东西。如果您想使用另一个在线平台（例如Kaggle），或者如果您想在自己的计算机上本地安装所有内容，请参阅本书GitHub页面上的说明。
- en: Running the Code Examples Using Google Colab
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Colab运行代码示例
- en: 'First, open a web browser and visit [*https://homl.info/colab3*](https://homl.info/colab3):
    this will lead you to Google Colab, and it will display the list of Jupyter notebooks
    for this book (see [Figure 2-3](#google_colab_notebook_list)). You will find one
    notebook per chapter, plus a few extra notebooks and tutorials for NumPy, Matplotlib,
    Pandas, linear algebra, and differential calculus. For example, if you click *02_end_to_end_machine_learning_project.ipynb*,
    the notebook from [Chapter 2](#project_chapter) will open up in Google Colab (see
    [Figure 2-4](#notebook_in_colab)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开一个网络浏览器，访问[*https://homl.info/colab3*](https://homl.info/colab3)：这将带您到Google
    Colab，并显示本书的Jupyter笔记本列表（参见[图2-3](#google_colab_notebook_list)）。您将在每章找到一个笔记本，以及一些额外的笔记本和NumPy、Matplotlib、Pandas、线性代数和微积分的教程。例如，如果您点击*02_end_to_end_machine_learning_project.ipynb*，将会在Google
    Colab中打开[第2章](#project_chapter)的笔记本（参见[图2-4](#notebook_in_colab)）。
- en: A Jupyter notebook is composed of a list of cells. Each cell contains either
    executable code or text. Try double-clicking the first text cell (which contains
    the sentence “Welcome to Machine Learning Housing Corp.!”). This will open the
    cell for editing. Notice that Jupyter notebooks use Markdown syntax for formatting
    (e.g., `**bold**`, `*italics*`, `# Title`, `[url](link text)`, and so on). Try
    modifying this text, then press Shift-Enter to see the result.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本由一系列单元格组成。每个单元格包含可执行代码或文本。尝试双击第一个文本单元格（其中包含句子“欢迎来到机器学习房地产公司！”）。这将打开单元格进行编辑。请注意，Jupyter笔记本使用Markdown语法进行格式化（例如，`**粗体**`，`*斜体*`，`#
    标题`，`[url](链接文本)`等）。尝试修改这段文本，然后按Shift-Enter查看结果。
- en: '![mls3 0203](assets/mls3_0203.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0203](assets/mls3_0203.png)'
- en: Figure 2-3\. List of notebooks in Google Colab
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. Google Colab中的笔记本列表
- en: '![mls3 0204](assets/mls3_0204.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0204](assets/mls3_0204.png)'
- en: Figure 2-4\. Your notebook in Google Colab
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 在Google Colab中的笔记本
- en: Next, create a new code cell by selecting Insert → “Code cell” from the menu.
    Alternatively, you can click the + Code button in the toolbar, or hover your mouse
    over the bottom of a cell until you see + Code and + Text appear, then click +
    Code. In the new code cell, type some Python code, such as `print("Hello World")`,
    then press Shift-Enter to run this code (or click the ▷ button on the left side
    of the cell).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过选择插入→“代码单元格”菜单来创建一个新的代码单元格。或者，您可以在工具栏中点击+代码按钮，或者将鼠标悬停在单元格底部直到看到+代码和+文本出现，然后点击+代码。在新的代码单元格中，输入一些Python代码，例如`print("Hello
    World")`，然后按Shift-Enter运行此代码（或者点击单元格左侧的▷按钮）。
- en: If you’re not logged in to your Google account, you’ll be asked to log in now
    (if you don’t already have a Google account, you’ll need to create one). Once
    you are logged in, when you try to run the code you’ll see a security warning
    telling you that this notebook was not authored by Google. A malicious person
    could create a notebook that tries to trick you into entering your Google credentials
    so they can access your personal data, so before you run a notebook, always make
    sure you trust its author (or double-check what each code cell will do before
    running it). Assuming you trust me (or you plan to check every code cell), you
    can now click “Run anyway”.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未登录Google账户，现在将被要求登录（如果您尚未拥有Google账户，您需要创建一个）。一旦您登录，当您尝试运行代码时，您将看到一个安全警告，告诉您这个笔记本不是由Google编写的。一个恶意的人可能会创建一个试图欺骗您输入Google凭据的笔记本，以便访问您的个人数据，因此在运行笔记本之前，请务必确保信任其作者（或在运行之前仔细检查每个代码单元格将执行的操作）。假设您信任我（或者您计划检查每个代码单元格），现在可以点击“仍然运行”。
- en: 'Colab will then allocate a new *runtime* for you: this is a free virtual machine
    located on Google’s servers that contains a bunch of tools and Python libraries,
    including everything you’ll need for most chapters (in some chapters, you’ll need
    to run a command to install additional libraries). This will take a few seconds.
    Next, Colab will automatically connect to this runtime and use it to execute your
    new code cell. Importantly, the code runs on the runtime, *not* on your machine.
    The code’s output will be displayed under the cell. Congrats, you’ve run some
    Python code on Colab!'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Colab将为您分配一个新的*运行时*：这是位于Google服务器上的免费虚拟机，包含一堆工具和Python库，包括大多数章节所需的一切（在某些章节中，您需要运行一个命令来安装额外的库）。这将需要几秒钟。接下来，Colab将自动连接到此运行时，并使用它来执行您的新代码单元格。重要的是，代码在运行时上运行，*而不是*在您的计算机上。代码的输出将显示在单元格下方。恭喜，您已在Colab上运行了一些Python代码！
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'To insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed
    by A (to insert above the active cell) or B (to insert below). There are many
    other keyboard shortcuts available: you can view and edit them by typing Ctrl-M
    (or Cmd-M) then H. If you choose to run the notebooks on Kaggle or on your own
    machine using JupyterLab or an IDE such as Visual Studio Code with the Jupyter
    extension, you will see some minor differences—runtimes are called *kernels*,
    the user interface and keyboard shortcuts are slightly different, etc.—but switching
    from one Jupyter environment to another is not too hard.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要插入新的代码单元格，您也可以键入Ctrl-M（或macOS上的Cmd-M），然后按A（在活动单元格上方插入）或B（在下方插入）。还有许多其他可用的键盘快捷键：您可以通过键入Ctrl-M（或Cmd-M）然后H来查看和编辑它们。如果您选择在Kaggle上或使用JupyterLab或带有Jupyter扩展的IDE（如Visual
    Studio Code）在自己的机器上运行笔记本，您将看到一些细微差异——运行时称为*内核*，用户界面和键盘快捷键略有不同等等——但从一个Jupyter环境切换到另一个并不太困难。
- en: Saving Your Code Changes and Your Data
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存您的代码更改和数据
- en: You can make changes to a Colab notebook, and they will persist for as long
    as you keep your browser tab open. But once you close it, the changes will be
    lost. To avoid this, make sure you save a copy of the notebook to your Google
    Drive by selecting File → “Save a copy in Drive”. Alternatively, you can download
    the notebook to your computer by selecting File → Download → “Download .ipynb”.
    Then you can later visit [*https://colab.research.google.com*](https://colab.research.google.com)
    and open the notebook again (either from Google Drive or by uploading it from
    your computer).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对Colab笔记本进行更改，并且只要保持浏览器标签打开，这些更改将持续存在。但一旦关闭它，更改将丢失。为了避免这种情况，请确保通过选择文件→“在驱动器中保存副本”将笔记本保存到您的谷歌驱动器。或者，您可以通过选择文件→下载→“下载.ipynb”将笔记本下载到计算机。然后，您可以稍后访问[*https://colab.research.google.com*](https://colab.research.google.com)并再次打开笔记本（从谷歌驱动器或通过从计算机上传）。
- en: Warning
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Google Colab is meant only for interactive use: you can play around in the
    notebooks and tweak the code as you like, but you cannot let the notebooks run
    unattended for a long period of time, or else the runtime will be shut down and
    all of its data will be lost.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab仅用于交互使用：您可以在笔记本中玩耍并调整代码，但不能让笔记本在长时间内无人看管运行，否则运行时将关闭并丢失所有数据。
- en: If the notebook generates data that you care about, make sure you download this
    data before the runtime shuts down. To do this, click the Files icon (see step
    1 in [Figure 2-5](#save_data_google_colab)), find the file you want to download,
    click the vertical dots next to it (step 2), and click Download (step 3). Alternatively,
    you can mount your Google Drive on the runtime, allowing the notebook to read
    and write files directly to Google Drive as if it were a local directory. For
    this, click the Files icon (step 1), then click the Google Drive icon (circled
    in [Figure 2-5](#save_data_google_colab)) and follow the on-screen instructions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果笔记本生成了您关心的数据，请确保在运行时关闭之前下载这些数据。要做到这一点，请点击文件图标（参见[图2-5](#save_data_google_colab)中的步骤1），找到要下载的文件，点击其旁边的垂直点（步骤2），然后点击下载（步骤3）。或者，您可以在运行时挂载您的谷歌驱动器，使笔记本能够直接读写文件到谷歌驱动器，就像它是一个本地目录一样。为此，请点击文件图标（步骤1），然后点击谷歌驱动器图标（在[图2-5](#save_data_google_colab)中用圈圈圈出）并按照屏幕上的说明操作。
- en: '![mls3 0205](assets/mls3_0205.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0205](assets/mls3_0205.png)'
- en: Figure 2-5\. Downloading a file from a Google Colab runtime (steps 1 to 3),
    or mounting your Google Drive (circled icon)
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。从Google Colab运行时下载文件（步骤1至3），或挂载您的谷歌驱动器（圈圈图标）
- en: 'By default, your Google Drive will be mounted at */content/drive/MyDrive*.
    If you want to back up a data file, simply copy it to this directory by running
    `!cp /content/my_great_model /content/drive/MyDrive`. Any command starting with
    a bang (`!`) is treated as a shell command, not as Python code: `cp` is the Linux
    shell command to copy a file from one path to another. Note that Colab runtimes
    run on Linux (specifically, Ubuntu).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，您的谷歌驱动器将挂载在*/content/drive/MyDrive*。如果要备份数据文件，只需通过运行`!cp /content/my_great_model
    /content/drive/MyDrive`将其复制到此目录。任何以感叹号（`!`）开头的命令都被视为shell命令，而不是Python代码：`cp`是Linux
    shell命令，用于将文件从一个路径复制到另一个路径。请注意，Colab运行时在Linux上运行（具体来说是Ubuntu）。
- en: The Power and Danger of Interactivity
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互性的力量和危险
- en: 'Jupyter notebooks are interactive, and that’s a great thing: you can run each
    cell one by one, stop at any point, insert a cell, play with the code, go back
    and run the same cell again, etc., and I highly encourage you to do so. If you
    just run the cells one by one without ever playing around with them, you won’t
    learn as fast. However, this flexibility comes at a price: it’s very easy to run
    cells in the wrong order, or to forget to run a cell. If this happens, the subsequent
    code cells are likely to fail. For example, the very first code cell in each notebook
    contains setup code (such as imports), so make sure you run it first, or else
    nothing will work.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本是交互式的，这是一件好事：您可以逐个运行每个单元格，在任何时候停止，插入单元格，玩弄代码，返回并再次运行相同的单元格等等，我强烈鼓励您这样做。如果您只是逐个运行单元格而从不玩弄它们，您学习速度不会那么快。然而，这种灵活性是有代价的：很容易按错误的顺序运行单元格，或者忘记运行一个单元格。如果发生这种情况，后续的代码单元格很可能会失败。例如，每个笔记本中的第一个代码单元格包含设置代码（如导入），因此请确保首先运行它，否则什么都不会起作用。
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you ever run into a weird error, try restarting the runtime (by selecting
    Runtime → “Restart runtime” from the menu) and then run all the cells again from
    the beginning of the notebook. This often solves the problem. If not, it’s likely
    that one of the changes you made broke the notebook: just revert to the original
    notebook and try again. If it still fails, please file an issue on GitHub.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到奇怪的错误，请尝试重新启动运行时（通过选择运行时→“重新启动运行时”菜单）然后再次从笔记本开头运行所有单元格。这通常可以解决问题。如果不行，很可能是您所做的更改之一破坏了笔记本：只需恢复到原始笔记本并重试。如果仍然失败，请在GitHub上提交问题。
- en: Book Code Versus Notebook Code
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 书中代码与笔记本代码
- en: 'You may sometimes notice some little differences between the code in this book
    and the code in the notebooks. This may happen for several reasons:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您有时可能会注意到本书中的代码与笔记本中的代码之间存在一些小差异。这可能是由于以下几个原因：
- en: 'A library may have changed slightly by the time you read these lines, or perhaps
    despite my best efforts I made an error in the book. Sadly, I cannot magically
    fix the code in your copy of this book (unless you are reading an electronic copy
    and you can download the latest version), but I *can* fix the notebooks. So, if
    you run into an error after copying code from this book, please look for the fixed
    code in the notebooks: I will strive to keep them error-free and up-to-date with
    the latest library versions.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图书馆可能在您阅读这些文字时略有变化，或者尽管我尽力了，但书中可能存在错误。遗憾的是，我不能在您的这本书中神奇地修复代码（除非您正在阅读电子副本并且可以下载最新版本），但我可以修复笔记本。因此，如果您在从本书中复制代码后遇到错误，请查找笔记本中的修复代码：我将努力保持它们没有错误，并与最新的库版本保持同步。
- en: The notebooks contain some extra code to beautify the figures (adding labels,
    setting font sizes, etc.) and to save them in high resolution for this book. You
    can safely ignore this extra code if you want.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本包含一些额外的代码来美化图形（添加标签，设置字体大小等）并将它们保存为高分辨率以供本书使用。如果您愿意，可以安全地忽略这些额外的代码。
- en: 'I optimized the code for readability and simplicity: I made it as linear and
    flat as possible, defining very few functions or classes. The goal is to ensure
    that the code you are running is generally right in front of you, and not nested
    within several layers of abstractions that you have to search through. This also
    makes it easier for you to play with the code. For simplicity, there’s limited
    error handling, and I placed some of the least common imports right where they
    are needed (instead of placing them at the top of the file, as is recommended
    by the PEP 8 Python style guide). That said, your production code will not be
    very different: just a bit more modular, and with additional tests and error handling.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我优化了代码以提高可读性和简单性：我尽可能将其线性化和扁平化，定义了很少的函数或类。目标是确保您运行的代码通常就在您眼前，而不是嵌套在几层抽象中，需要搜索。这也使您更容易玩弄代码。为简单起见，我没有进行太多的错误处理，并且将一些不常见的导入放在需要它们的地方（而不是像PEP
    8 Python风格指南建议的那样将它们放在文件顶部）。也就是说，您的生产代码不会有太大的不同：只是更模块化，还有额外的测试和错误处理。
- en: OK! Once you’re comfortable with Colab, you’re ready to download the data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！一旦您熟悉了Colab，您就可以下载数据了。
- en: Download the Data
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: 'In typical environments your data would be available in a relational database
    or some other common data store, and spread across multiple tables/documents/files.
    To access it, you would first need to get your credentials and access authorizations⁠^([4](ch02.html#idm45720248484800))
    and familiarize yourself with the data schema. In this project, however, things
    are much simpler: you will just download a single compressed file, *housing.tgz*,
    which contains a comma-separated values (CSV) file called *housing.csv* with all
    the data.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型环境中，您的数据可能存储在关系数据库或其他常见数据存储中，并分布在多个表/文档/文件中。要访问它，您首先需要获取您的凭据和访问授权，并熟悉数据模式。然而，在这个项目中，情况要简单得多：您只需下载一个压缩文件*housing.tgz*，其中包含一个名为*housing.csv*的逗号分隔值（CSV）文件，其中包含所有数据。
- en: 'Rather than manually downloading and decompressing the data, it’s usually preferable
    to write a function that does it for you. This is useful in particular if the
    data changes regularly: you can write a small script that uses the function to
    fetch the latest data (or you can set up a scheduled job to do that automatically
    at regular intervals). Automating the process of fetching the data is also useful
    if you need to install the dataset on multiple machines.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动下载和解压数据相比，通常最好编写一个函数来执行此操作。特别是如果数据经常更改，这将非常有用：您可以编写一个小脚本，使用该函数获取最新数据（或者您可以设置定期自动执行此操作的计划任务）。自动获取数据的过程也很有用，如果您需要在多台机器上安装数据集。
- en: 'Here is the function to fetch and load the data:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于获取和加载数据的函数：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When `load_housing_data()` is called, it looks for the *datasets/housing.tgz*
    file. If it does not find it, it creates the *datasets* directory inside the current
    directory (which is */content* by default, in Colab), downloads the *housing.tgz*
    file from the *ageron/data* GitHub repository, and extracts its content into the
    *datasets* directory; this creates the *datasets*/*housing* directory with the
    *housing.csv* file inside it. Lastly, the function loads this CSV file into a
    Pandas DataFrame object containing all the data, and returns it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`load_housing_data()`时，它会查找*datasets/housing.tgz*文件。如果找不到该文件，它会在当前目录内创建*datasets*目录（默认为*/content*，在Colab中），从*ageron/data*
    GitHub存储库下载*housing.tgz*文件，并将其内容提取到*datasets*目录中；这将创建*datasets*/*housing*目录，并在其中包含*housing.csv*文件。最后，该函数将此CSV文件加载到一个包含所有数据的Pandas
    DataFrame对象中，并返回它。
- en: Take a Quick Look at the Data Structure
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速查看数据结构
- en: You start by looking at the top five rows of data using the DataFrame’s `head()`
    method (see [Figure 2-6](#housing_head_screenshot)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用DataFrame的`head()`方法来查看数据的前五行（请参见[图2-6](#housing_head_screenshot)）。
- en: '![mls3 0206](assets/mls3_0206.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0206](assets/mls3_0206.png)'
- en: Figure 2-6\. Top five rows in the dataset
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6。数据集中的前五行
- en: 'Each row represents one district. There are 10 attributes (they are not all
    shown in the screenshot): `longitude`, `latitude`, `housing_median_age`, `total_rooms`,
    `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`,
    and `ocean_proximity`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行代表一个地区。有10个属性（并非所有属性都显示在屏幕截图中）：`longitude`、`latitude`、`housing_median_age`、`total_rooms`、`total_bedrooms`、`population`、`households`、`median_income`、`median_house_value`和`ocean_proximity`。
- en: 'The `info()` method is useful to get a quick description of the data, in particular
    the total number of rows, each attribute’s type, and the number of non-null values:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`info()`方法对于快速获取数据的简要描述非常有用，特别是总行数、每个属性的类型和非空值的数量：'
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In this book, when a code example contains a mix of code and outputs, as is
    the case here, it is formatted like in the Python interpreter, for better readability:
    the code lines are prefixed with `>>>` (or `...` for indented blocks), and the
    outputs have no prefix.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，当代码示例包含代码和输出混合时，就像这里一样，它的格式类似于Python解释器，以便更好地阅读：代码行以`>>>`（或缩进块的情况下为`...`）为前缀，输出没有前缀。
- en: There are 20,640 instances in the dataset, which means that it is fairly small
    by machine learning standards, but it’s perfect to get started. You notice that
    the `total_bedrooms` attribute has only 20,433 non-null values, meaning that 207
    districts are missing this feature. You will need to take care of this later.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有20,640个实例，这意味着按照机器学习的标准来说，它相当小，但是非常适合入门。您会注意到`total_bedrooms`属性只有20,433个非空值，这意味着有207个地区缺少这个特征。您需要稍后处理这个问题。
- en: 'All attributes are numerical, except for `ocean_proximity`. Its type is `object`,
    so it could hold any kind of Python object. But since you loaded this data from
    a CSV file, you know that it must be a text attribute. When you looked at the
    top five rows, you probably noticed that the values in the `ocean_proximity` column
    were repetitive, which means that it is probably a categorical attribute. You
    can find out what categories exist and how many districts belong to each category
    by using the `value_counts()` method:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有属性都是数字的，除了`ocean_proximity`。它的类型是`object`，因此它可以保存任何类型的Python对象。但是由于您从CSV文件中加载了这些数据，您知道它必须是一个文本属性。当您查看前五行时，您可能会注意到`ocean_proximity`列中的值是重复的，这意味着它可能是一个分类属性。您可以使用`value_counts()`方法找出存在哪些类别以及每个类别包含多少个地区：
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s look at the other fields. The `describe()` method shows a summary of the
    numerical attributes ([Figure 2-7](#housing_describe_screenshot)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他字段。`describe()`方法显示了数字属性的摘要（[图2-7](#housing_describe_screenshot)）。
- en: '![mls3 0207](assets/mls3_0207.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0207](assets/mls3_0207.png)'
- en: Figure 2-7\. Summary of each numerical attribute
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7。每个数字属性的总结
- en: 'The `count`, `mean`, `min`, and `max` rows are self-explanatory. Note that
    the null values are ignored (so, for example, the `count` of `total_bedrooms`
    is 20,433, not 20,640). The `std` row shows the *standard deviation*, which measures
    how dispersed the values are.⁠^([5](ch02.html#idm45720248920160)) The `25%`, `50%`,
    and `75%` rows show the corresponding *percentiles*: a percentile indicates the
    value below which a given percentage of observations in a group of observations
    fall. For example, 25% of the districts have a `housing_median_age` lower than
    18, while 50% are lower than 29 and 75% are lower than 37\. These are often called
    the 25th percentile (or first *quartile*), the median, and the 75th percentile
    (or third quartile).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`count`、`mean`、`min`和`max`行是不言自明的。请注意，空值被忽略了（因此，例如，`total_bedrooms`的`count`是20,433，而不是20,640）。`std`行显示了*标准差*，它衡量了值的分散程度。⁠^([5](ch02.html#idm45720248920160))
    `25%`、`50%`和`75%`行显示了相应的*百分位数*：百分位数表示给定百分比的观察值中有多少落在一组观察值之下。例如，25%的地区的`housing_median_age`低于18，而50%低于29，75%低于37。这些通常被称为第25百分位数（或第一个*四分位数*）、中位数和第75百分位数（或第三四分位数）。'
- en: 'Another quick way to get a feel of the type of data you are dealing with is
    to plot a histogram for each numerical attribute. A histogram shows the number
    of instances (on the vertical axis) that have a given value range (on the horizontal
    axis). You can either plot this one attribute at a time, or you can call the `hist()`
    method on the whole dataset (as shown in the following code example), and it will
    plot a histogram for each numerical attribute (see [Figure 2-8](#attribute_histogram_plots)):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种快速了解您正在处理的数据类型的方法是为每个数字属性绘制直方图。直方图显示了具有给定值范围的实例数量（在水平轴上）（在垂直轴上）。您可以一次绘制一个属性，也可以在整个数据集上调用`hist()`方法（如下面的代码示例所示），它将为每个数字属性绘制一个直方图（参见[图2-8](#attribute_histogram_plots)）：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![mls3 0208](assets/mls3_0208.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0208](assets/mls3_0208.png)'
- en: Figure 2-8\. A histogram for each numerical attribute
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。每个数字属性的直方图
- en: 'Looking at these histograms, you notice a few things:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些直方图，您会注意到一些事情：
- en: First, the median income attribute does not look like it is expressed in US
    dollars (USD). After checking with the team that collected the data, you are told
    that the data has been scaled and capped at 15 (actually, 15.0001) for higher
    median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers
    represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000).
    Working with preprocessed attributes is common in machine learning, and it is
    not necessarily a problem, but you should try to understand how the data was computed.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，中位收入属性看起来不像是以美元（USD）表示的。在与收集数据的团队核实后，他们告诉您数据已经被缩放，并且对于更高的中位收入，已经被限制在15（实际上是15.0001），对于更低的中位收入，已经被限制在0.5（实际上是0.4999）。这些数字大致表示数万美元（例如，3实际上表示约30,000美元）。在机器学习中使用预处理的属性是很常见的，这并不一定是问题，但您应该尝试了解数据是如何计算的。
- en: 'The housing median age and the median house value were also capped. The latter
    may be a serious problem since it is your target attribute (your labels). Your
    machine learning algorithms may learn that prices never go beyond that limit.
    You need to check with your client team (the team that will use your system’s
    output) to see if this is a problem or not. If they tell you that they need precise
    predictions even beyond $500,000, then you have two options:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房屋中位年龄和房屋中位价值也被限制了。后者可能是一个严重的问题，因为它是您的目标属性（您的标签）。您的机器学习算法可能会学习到价格永远不会超过那个限制。您需要与客户团队（将使用您系统输出的团队）核实，看看这是否是一个问题。如果他们告诉您他们需要精确的预测，甚至超过50万美元，那么您有两个选择：
- en: Collect proper labels for the districts whose labels were capped.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集那些标签被限制的地区的正确标签。
- en: Remove those districts from the training set (and also from the test set, since
    your system should not be evaluated poorly if it predicts values beyond $500,000).
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练集中删除这些地区（也从测试集中删除，因为如果您的系统预测超过50万美元的值，它不应该被评估不良）。
- en: These attributes have very different scales. We will discuss this later in this
    chapter, when we explore feature scaling.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些属性具有非常不同的比例。当我们探索特征缩放时，我们将在本章后面讨论这一点。
- en: 'Finally, many histograms are *skewed right*: they extend much farther to the
    right of the median than to the left. This may make it a bit harder for some machine
    learning algorithms to detect patterns. Later, you’ll try transforming these attributes
    to have more symmetrical and bell-shaped distributions.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，许多直方图是*右偏的*：它们在中位数的右侧延伸得更远，而不是左侧。这可能会使一些机器学习算法更难检测到模式。稍后，您将尝试转换这些属性，使其具有更对称和钟形分布。
- en: You should now have a better understanding of the kind of data you’re dealing
    with.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该对您正在处理的数据有更好的了解。
- en: Warning
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Wait! Before you look at the data any further, you need to create a test set,
    put it aside, and never look at it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！在进一步查看数据之前，您需要创建一个测试集，将其放在一边，永远不要查看它。
- en: Create a Test Set
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个测试集
- en: 'It may seem strange to voluntarily set aside part of the data at this stage.
    After all, you have only taken a quick glance at the data, and surely you should
    learn a whole lot more about it before you decide what algorithms to use, right?
    This is true, but your brain is an amazing pattern detection system, which also
    means that it is highly prone to overfitting: if you look at the test set, you
    may stumble upon some seemingly interesting pattern in the test data that leads
    you to select a particular kind of machine learning model. When you estimate the
    generalization error using the test set, your estimate will be too optimistic,
    and you will launch a system that will not perform as well as expected. This is
    called *data snooping* bias.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段自愿设置一部分数据似乎有点奇怪。毕竟，您只是快速浏览了数据，而在决定使用什么算法之前，您肯定应该更多地了解它，对吧？这是真的，但您的大脑是一个惊人的模式检测系统，这也意味着它很容易过拟合：如果您查看测试集，您可能会在测试数据中发现一些看似有趣的模式，导致您选择特定类型的机器学习模型。当您使用测试集估计泛化误差时，您的估计将过于乐观，您将启动一个性能不如预期的系统。这被称为*数据窥探*偏差。
- en: 'Creating a test set is theoretically simple; pick some instances randomly,
    typically 20% of the dataset (or less if your dataset is very large), and set
    them aside:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建测试集在理论上很简单；随机选择一些实例，通常是数据集的20％（如果您的数据集非常大，则可以更少），并将它们放在一边：
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can then use this function like this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以像这样使用这个函数：
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Well, this works, but it is not perfect: if you run the program again, it will
    generate a different test set! Over time, you (or your machine learning algorithms)
    will get to see the whole dataset, which is what you want to avoid.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这样做是有效的，但并不完美：如果再次运行程序，它将生成不同的测试集！随着时间的推移，您（或您的机器学习算法）将看到整个数据集，这是您要避免的。
- en: One solution is to save the test set on the first run and then load it in subsequent
    runs. Another option is to set the random number generator’s seed (e.g., with
    `np.random.seed(42)`)⁠^([6](ch02.html#idm45720239285936)) before calling `np.random.permutation()`
    so that it always generates the same shuffled indices.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是在第一次运行时保存测试集，然后在后续运行中加载它。另一个选项是在调用`np.random.permutation()`之前设置随机数生成器的种子（例如，使用`np.random.seed(42)`）⁠^（[6]（ch02.html#idm45720239285936）），以便它始终生成相同的洗牌索引。
- en: However, both these solutions will break the next time you fetch an updated
    dataset. To have a stable train/test split even after updating the dataset, a
    common solution is to use each instance’s identifier to decide whether or not
    it should go in the test set (assuming instances have unique and immutable identifiers).
    For example, you could compute a hash of each instance’s identifier and put that
    instance in the test set if the hash is lower than or equal to 20% of the maximum
    hash value. This ensures that the test set will remain consistent across multiple
    runs, even if you refresh the dataset. The new test set will contain 20% of the
    new instances, but it will not contain any instance that was previously in the
    training set.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两种解决方案在获取更新的数据集后会失效。为了在更新数据集后仍然保持稳定的训练/测试分割，一个常见的解决方案是使用每个实例的标识符来决定是否应该放入测试集中（假设实例具有唯一且不可变的标识符）。例如，您可以计算每个实例标识符的哈希值，并且如果哈希值低于或等于最大哈希值的20％，则将该实例放入测试集中。这确保了测试集将在多次运行中保持一致，即使您刷新数据集。新的测试集将包含新实例的20％，但不会包含以前在训练集中的任何实例。
- en: 'Here is a possible implementation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可能的实现：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Unfortunately, the housing dataset does not have an identifier column. The
    simplest solution is to use the row index as the ID:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，住房数据集没有标识符列。最简单的解决方案是使用行索引作为ID：
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you use the row index as a unique identifier, you need to make sure that
    new data gets appended to the end of the dataset and that no row ever gets deleted.
    If this is not possible, then you can try to use the most stable features to build
    a unique identifier. For example, a district’s latitude and longitude are guaranteed
    to be stable for a few million years, so you could combine them into an ID like
    so:⁠^([7](ch02.html#idm45720242437568))
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用行索引作为唯一标识符，您需要确保新数据附加到数据集的末尾，并且永远不会删除任何行。如果这不可能，那么您可以尝试使用最稳定的特征来构建唯一标识符。例如，一个地区的纬度和经度保证在几百万年内保持稳定，因此您可以将它们组合成一个ID，如下所示：⁠^（[7]（ch02.html#idm45720242437568））
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Scikit-Learn provides a few functions to split datasets into multiple subsets
    in various ways. The simplest function is `train_test_split()`, which does pretty
    much the same thing as the `shuffle_and_split_data()` function we defined earlier,
    with a couple of additional features. First, there is a `random_state` parameter
    that allows you to set the random generator seed. Second, you can pass it multiple
    datasets with an identical number of rows, and it will split them on the same
    indices (this is very useful, for example, if you have a separate DataFrame for
    labels):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一些函数，以各种方式将数据集拆分为多个子集。最简单的函数是`train_test_split()`，它基本上与我们之前定义的`shuffle_and_split_data()`函数做的事情差不多，但有一些额外的功能。首先，有一个`random_state`参数，允许您设置随机生成器种子。其次，您可以传递多个具有相同行数的数据集，并且它将在相同的索引上拆分它们（例如，如果您有一个单独的标签DataFrame，这是非常有用的）：
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So far we have considered purely random sampling methods. This is generally
    fine if your dataset is large enough (especially relative to the number of attributes),
    but if it is not, you run the risk of introducing a significant sampling bias.
    When employees at a survey company decides to call 1,000 people to ask them a
    few questions, they don’t just pick 1,000 people randomly in a phone book. They
    try to ensure that these 1,000 people are representative of the whole population,
    with regard to the questions they want to ask. For example, the US population
    is 51.1% females and 48.9% males, so a well-conducted survey in the US would try
    to maintain this ratio in the sample: 511 females and 489 males (at least if it
    seems possible that the answers may vary across genders). This is called *stratified
    sampling*: the population is divided into homogeneous subgroups called *strata*,
    and the right number of instances are sampled from each stratum to guarantee that
    the test set is representative of the overall population. If the people running
    the survey used purely random sampling, there would be about a 10.7% chance of
    sampling a skewed test set with less than 48.5% female or more than 53.5% female
    participants. Either way, the survey results would likely be quite biased.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了纯随机抽样方法。如果您的数据集足够大（尤其是相对于属性数量），这通常是可以接受的，但如果不是，您就有可能引入显著的抽样偏差。当调查公司的员工决定打电话给1,000人询问一些问题时，他们不会仅仅在电话簿中随机挑选1,000人。他们会努力确保这1,000人在问题上代表整个人口。例如，美国人口中51.1%是女性，48.9%是男性，因此在美国进行良好的调查将尝试保持这个比例在样本中：511名女性和489名男性（至少如果可能的话，答案可能会因性别而有所不同）。这被称为*分层抽样*：人口被划分为称为*层*的同质子群，从每个层中抽取正确数量的实例以确保测试集代表整体人口。如果进行调查的人员使用纯随机抽样，那么抽取一个偏斜的测试集，女性参与者少于48.5%或多于53.5%的概率约为10.7%。无论哪种方式，调查结果可能会相当偏倚。
- en: 'Suppose you’ve chatted with some experts who told you that the median income
    is a very important attribute to predict median housing prices. You may want to
    ensure that the test set is representative of the various categories of incomes
    in the whole dataset. Since the median income is a continuous numerical attribute,
    you first need to create an income category attribute. Let’s look at the median
    income histogram more closely (back in [Figure 2-8](#attribute_histogram_plots)):
    most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000),
    but some median incomes go far beyond 6\. It is important to have a sufficient
    number of instances in your dataset for each stratum, or else the estimate of
    a stratum’s importance may be biased. This means that you should not have too
    many strata, and each stratum should be large enough. The following code uses
    the `pd.cut()` function to create an income category attribute with five categories
    (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000),
    category 2 from 1.5 to 3, and so on:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您与一些专家交谈过，他们告诉您，中位收入是预测中位房价的一个非常重要的属性。您可能希望确保测试集代表整个数据集中各种收入类别。由于中位收入是一个连续的数值属性，您首先需要创建一个收入类别属性。让我们更仔细地看一下中位收入直方图（回到[图2-8](#attribute_histogram_plots)）：大多数中位收入值聚集在1.5到6之间（即15,000美元至60,000美元），但有些中位收入远远超过6。对于每个层，您的数据集中应该有足够数量的实例，否则对层重要性的估计可能会有偏差。这意味着您不应该有太多层，并且每个层应该足够大。以下代码使用`pd.cut()`函数创建一个具有五个类别（从1到5标记）的收入类别属性；类别1范围从0到1.5（即15,000美元以下），类别2从1.5到3，依此类推：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'These income categories are represented in [Figure 2-9](#housing_income_cat_bar_plot):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些收入类别在[图2-9](#housing_income_cat_bar_plot)中表示：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you are ready to do stratified sampling based on the income category. Scikit-Learn
    provides a number of splitter classes in the `sklearn.model_selection` package
    that implement various strategies to split your dataset into a training set and
    a test set. Each splitter has a `split()` method that returns an iterator over
    different training/test splits of the same data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以根据收入类别进行分层抽样。Scikit-Learn在`sklearn.model_selection`包中提供了许多拆分器类，实现了各种策略来将数据集拆分为训练集和测试集。每个拆分器都有一个`split()`方法，返回相同数据的不同训练/测试拆分的迭代器。
- en: '![mls3 0209](assets/mls3_0209.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0209](assets/mls3_0209.png)'
- en: Figure 2-9\. Histogram of income categories
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9。收入类别直方图
- en: 'To be precise, the `split()` method yields the training and test *indices*,
    not the data itself. Having multiple splits can be useful if you want to better
    estimate the performance of your model, as you will see when we discuss cross-validation
    later in this chapter. For example, the following code generates 10 different
    stratified splits of the same dataset:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要准确，`split()`方法产生训练和测试*索引*，而不是数据本身。如果您想更好地估计模型的性能，拥有多个拆分可能是有用的，这将在本章后面讨论交叉验证时看到。例如，以下代码生成了同一数据集的10个不同的分层拆分：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For now, you can just use the first split:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用第一个拆分：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Or, since stratified sampling is fairly common, there’s a shorter way to get
    a single split using the `train_test_split()` function with the `stratify` argument:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，由于分层抽样相当常见，可以使用`train_test_split()`函数的`stratify`参数来更快地获得单个拆分：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s see if this worked as expected. You can start by looking at the income
    category proportions in the test set:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否按预期工作。您可以从测试集中查看收入类别的比例：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With similar code you can measure the income category proportions in the full
    dataset. [Figure 2-10](#compare_sampling_errors_screenshot) compares the income
    category proportions in the overall dataset, in the test set generated with stratified
    sampling, and in a test set generated using purely random sampling. As you can
    see, the test set generated using stratified sampling has income category proportions
    almost identical to those in the full dataset, whereas the test set generated
    using purely random sampling is skewed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的代码，您可以测量完整数据集中的收入类别比例。[图2-10](#compare_sampling_errors_screenshot)比较了整体数据集中的收入类别比例，使用分层抽样生成的测试集以及使用纯随机抽样生成的测试集。如您所见，使用分层抽样生成的测试集的收入类别比例几乎与完整数据集中的相同，而使用纯随机抽样生成的测试集则有所偏差。
- en: '![mls3 0210](assets/mls3_0210.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0210](assets/mls3_0210.png)'
- en: Figure 2-10\. Sampling bias comparison of stratified versus purely random sampling
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10。分层与纯随机抽样的抽样偏差比较
- en: 'You won’t use the `income_cat` column again, so you might as well drop it,
    reverting the data back to its original state:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您不会再使用`income_cat`列，因此可以将其删除，将数据恢复到原始状态：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We spent quite a bit of time on test set generation for a good reason: this
    is an often neglected but critical part of a machine learning project. Moreover,
    many of these ideas will be useful later when we discuss cross-validation. Now
    it’s time to move on to the next stage: exploring the data.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花了相当多的时间在测试集生成上，这是一个经常被忽视但至关重要的机器学习项目的关键部分。此外，当我们讨论交叉验证时，许多这些想法将在以后派上用场。现在是时候进入下一个阶段了：探索数据。
- en: Explore and Visualize the Data to Gain Insights
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化数据以获得洞见
- en: So far you have only taken a quick glance at the data to get a general understanding
    of the kind of data you are manipulating. Now the goal is to go into a little
    more depth.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您只是快速浏览了数据，以对您正在处理的数据类型有一个大致了解。现在的目标是深入一点。
- en: 'First, make sure you have put the test set aside and you are only exploring
    the training set. Also, if the training set is very large, you may want to sample
    an exploration set, to make manipulations easy and fast during the exploration
    phase. In this case, the training set is quite small, so you can just work directly
    on the full set. Since you’re going to experiment with various transformations
    of the full training set, you should make a copy of the original so you can revert
    to it afterwards:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保您已经将测试集放在一边，只探索训练集。此外，如果训练集非常庞大，您可能希望对探索集进行抽样，以便在探索阶段进行操作更加轻松和快速。在这种情况下，训练集相当小，因此您可以直接在完整集上工作。由于您将尝试对完整训练集进行各种转换，因此应该先复制原始数据，以便之后可以恢复到原始状态：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Visualizing Geographical Data
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化地理数据
- en: 'Because the dataset includes geographical information (latitude and longitude),
    it is a good idea to create a scatterplot of all the districts to visualize the
    data ([Figure 2-11](#bad_visualization_plot)):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集包含地理信息（纬度和经度），因此创建一个散点图来可视化所有地区是一个好主意（[图2-11](#bad_visualization_plot)）：
- en: '[PRE18]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![mls3 0211](assets/mls3_0211.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0211](assets/mls3_0211.png)'
- en: Figure 2-11\. A geographical scatterplot of the data
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11。数据的地理散点图
- en: 'This looks like California all right, but other than that it is hard to see
    any particular pattern. Setting the `alpha` option to `0.2` makes it much easier
    to visualize the places where there is a high density of data points ([Figure 2-12](#better_visualization_plot)):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来确实像加利福尼亚，但除此之外很难看出任何特定的模式。将`alpha`选项设置为`0.2`可以更容易地可视化数据点密度较高的地方（[图2-12](#better_visualization_plot)）：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that’s much better: you can clearly see the high-density areas, namely
    the Bay Area and around Los Angeles and San Diego, plus a long line of fairly
    high-density areas in the Central Valley (in particular, around Sacramento and
    Fresno).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在好多了：您可以清楚地看到高密度区域，即旧金山湾区、洛杉矶周围和圣迭戈周围，以及中央谷地（特别是萨克拉门托和弗雷斯诺周围）的一长串相当高密度区域。
- en: Our brains are very good at spotting patterns in pictures, but you may need
    to play around with visualization parameters to make the patterns stand out.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑非常擅长发现图片中的模式，但您可能需要调整可视化参数来突出这些模式。
- en: '![mls3 0212](assets/mls3_0212.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0212](assets/mls3_0212.png)'
- en: Figure 2-12\. A better visualization that highlights high-density areas
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12。一个更好的可视化，突出显示高密度区域
- en: Next, you look at the housing prices ([Figure 2-13](#housing_prices_scatterplot)).
    The radius of each circle represents the district’s population (option `s`), and
    the color represents the price (option `c`). Here you use a predefined color map
    (option `cmap`) called `jet`, which ranges from blue (low values) to red (high
    prices):⁠^([8](ch02.html#idm45720243740400))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将查看房屋价格（[图2-13](#housing_prices_scatterplot)）。每个圆的半径代表地区的人口（选项`s`），颜色代表价格（选项`c`）。在这里，您使用了一个预定义的颜色映射（选项`cmap`）称为`jet`，从蓝色（低值）到红色（高价格）：⁠^([8](ch02.html#idm45720243740400))
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This image tells you that the housing prices are very much related to the location
    (e.g., close to the ocean) and to the population density, as you probably knew
    already. A clustering algorithm should be useful for detecting the main cluster
    and for adding new features that measure the proximity to the cluster centers.
    The ocean proximity attribute may be useful as well, although in Northern California
    the housing prices in coastal districts are not too high, so it is not a simple
    rule.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图告诉你，房价与位置（例如靠近海洋）和人口密度密切相关，这可能是你已经知道的。聚类算法应该对检测主要集群和添加衡量到集群中心距离的新特征很有用。海洋接近度属性也可能有用，尽管在加利福尼亚北部，沿海地区的房价并不太高，所以这并不是一个简单的规则。
- en: '![mls3 0213](assets/mls3_0213.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0213](assets/mls3_0213.png)'
- en: 'Figure 2-13\. California housing prices: red is expensive, blue is cheap, larger
    circles indicate areas with a larger population'
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13\. 加利福尼亚房价：红色是昂贵的，蓝色是便宜的，较大的圆圈表示人口较多的地区
- en: Look for Correlations
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找相关性
- en: 'Since the dataset is not too large, you can easily compute the *standard correlation
    coefficient* (also called *Pearson’s r*) between every pair of attributes using
    the `corr()` method:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集不太大，你可以使用`corr()`方法轻松计算每对属性之间的*标准相关系数*（也称为*皮尔逊相关系数*）：
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now you can look at how much each attribute correlates with the median house
    value:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以看看每个属性与房屋中位价值的相关程度：
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The correlation coefficient ranges from –1 to 1\. When it is close to 1, it
    means that there is a strong positive correlation; for example, the median house
    value tends to go up when the median income goes up. When the coefficient is close
    to –1, it means that there is a strong negative correlation; you can see a small
    negative correlation between the latitude and the median house value (i.e., prices
    have a slight tendency to go down when you go north). Finally, coefficients close
    to 0 mean that there is no linear correlation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数的范围是-1到1。当它接近1时，意味着有很强的正相关性；例如，当中位收入上升时，中位房价往往会上涨。当系数接近-1时，意味着有很强的负相关性；你可以看到纬度和中位房价之间存在微弱的负相关性（即，当你向北走时，价格略有下降的趋势）。最后，接近0的系数意味着没有线性相关性。
- en: 'Another way to check for correlation between attributes is to use the Pandas
    `scatter_matrix()` function, which plots every numerical attribute against every
    other numerical attribute. Since there are now 11 numerical attributes, you would
    get 11² = 121 plots, which would not fit on a page—so you decide to focus on a
    few promising attributes that seem most correlated with the median housing value
    ([Figure 2-14](#scatter_matrix_plot)):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 检查属性之间的相关性的另一种方法是使用Pandas的`scatter_matrix()`函数，它将每个数值属性与其他每个数值属性绘制在一起。由于现在有11个数值属性，你将得到11²
    = 121个图，这些图无法放在一页上，所以你决定专注于一些看起来与房屋中位价值最相关的有希望的属性（[图2-14](#scatter_matrix_plot)）：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![mls3 0214](assets/mls3_0214.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0214](assets/mls3_0214.png)'
- en: Figure 2-14\. This scatter matrix plots every numerical attribute against every
    other numerical attribute, plus a histogram of each numerical attribute’s values
    on the main diagonal (top left to bottom right)
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14\. 这个散点矩阵将每个数值属性与其他每个数值属性绘制在一起，主对角线上是每个数值属性值的直方图（从左上到右下）
- en: The main diagonal would be full of straight lines if Pandas plotted each variable
    against itself, which would not be very useful. So instead, the Pandas displays
    a histogram of each attribute (other options are available; see the Pandas documentation
    for more details).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Pandas将每个变量与自身绘制在一起，主对角线将充满直线，这将没有太大用处。因此，Pandas显示了每个属性的直方图（还有其他选项可用；请参阅Pandas文档以获取更多详细信息）。
- en: 'Looking at the correlation scatterplots, it seems like the most promising attribute
    to predict the median house value is the median income, so you zoom in on their
    scatterplot ([Figure 2-15](#income_vs_house_value_scatterplot)):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 看着相关性散点图，似乎最有希望预测房屋中位价值的属性是中位收入，所以你放大了它们的散点图（[图2-15](#income_vs_house_value_scatterplot)）：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![mls3 0215](assets/mls3_0215.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0215](assets/mls3_0215.png)'
- en: Figure 2-15\. Median income versus median house value
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15\. 中位收入与中位房价
- en: 'This plot reveals a few things. First, the correlation is indeed quite strong;
    you can clearly see the upward trend, and the points are not too dispersed. Second,
    the price cap you noticed earlier is clearly visible as a horizontal line at $500,000\.
    But the plot also reveals other less obvious straight lines: a horizontal line
    around $450,000, another around $350,000, perhaps one around $280,000, and a few
    more below that. You may want to try removing the corresponding districts to prevent
    your algorithms from learning to reproduce these data quirks.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图揭示了一些事情。首先，相关性确实非常强；你可以清楚地看到上升趋势，点的分散程度也不大。其次，你之前注意到的价格上限在$500,000处清晰可见。但这个图还显示了其他不太明显的直线：一个在$450,000左右的水平线，另一个在$350,000左右，也许还有一个在$280,000左右，以及一些更低的线。你可能想尝试删除相应的地区，以防止你的算法学习复制这些数据怪癖。
- en: Warning
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The correlation coefficient only measures linear correlations (“as *x* goes
    up, *y* generally goes up/down”). It may completely miss out on nonlinear relationships
    (e.g., “as *x* approaches 0, *y* generally goes up”). [Figure 2-16](#correlation_coefficient_plots)
    shows a variety of datasets along with their correlation coefficient. Note how
    all the plots of the bottom row have a correlation coefficient equal to 0, despite
    the fact that their axes are clearly *not* independent: these are examples of
    nonlinear relationships. Also, the second row shows examples where the correlation
    coefficient is equal to 1 or –1; notice that this has nothing to do with the slope.
    For example, your height in inches has a correlation coefficient of 1 with your
    height in feet or in nanometers.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数只能测量线性相关性（“随着*x*的增加，*y*通常上升/下降”）。它可能完全忽略非线性关系（例如，“随着*x*接近0，*y*通常上升”）。[图2-16](#correlation_coefficient_plots)展示了各种数据集以及它们的相关系数。请注意，尽管底部行的所有图都具有相关系数为0，但它们的坐标轴显然*不*是独立的：这些是非线性关系的示例。此外，第二行显示了相关系数等于1或-1的示例；请注意，这与斜率无关。例如，您的身高以英寸为单位与以英尺或纳米为单位的身高的相关系数为1。
- en: '![mls3 0216](assets/mls3_0216.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0216](assets/mls3_0216.png)'
- en: 'Figure 2-16\. Standard correlation coefficient of various datasets (source:
    Wikipedia; public domain image)'
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-16\. 各种数据集的标准相关系数（来源：维基百科；公共领域图像）
- en: Experiment with Attribute Combinations
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试属性组合
- en: Hopefully the previous sections gave you an idea of a few ways you can explore
    the data and gain insights. You identified a few data quirks that you may want
    to clean up before feeding the data to a machine learning algorithm, and you found
    interesting correlations between attributes, in particular with the target attribute.
    You also noticed that some attributes have a skewed-right distribution, so you
    may want to transform them (e.g., by computing their logarithm or square root).
    Of course, your mileage will vary considerably with each project, but the general
    ideas are similar.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 希望前面的部分给您提供了一些探索数据和获得见解的方法。您发现了一些可能需要在将数据馈送到机器学习算法之前清理的数据怪癖，并且找到了属性之间的有趣相关性，特别是与目标属性相关的。您还注意到一些属性具有右偏分布，因此您可能希望对其进行转换（例如，通过计算其对数或平方根）。当然，每个项目的情况会有很大不同，但总体思路是相似的。
- en: 'One last thing you may want to do before preparing the data for machine learning
    algorithms is to try out various attribute combinations. For example, the total
    number of rooms in a district is not very useful if you don’t know how many households
    there are. What you really want is the number of rooms per household. Similarly,
    the total number of bedrooms by itself is not very useful: you probably want to
    compare it to the number of rooms. And the population per household also seems
    like an interesting attribute combination to look at. You create these new attributes
    as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备数据用于机器学习算法之前，您可能希望尝试各种属性组合。例如，一个地区的总房间数如果不知道有多少户，就不是很有用。您真正想要的是每户的房间数。同样，单独的卧室总数并不是很有用：您可能想要将其与房间数进行比较。每户人口也似乎是一个有趣的属性组合。您可以按照以下方式创建这些新属性：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And then you look at the correlation matrix again:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次查看相关矩阵：
- en: '[PRE26]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Hey, not bad! The new `bedrooms_ratio` attribute is much more correlated with
    the median house value than the total number of rooms or bedrooms. Apparently
    houses with a lower bedroom/room ratio tend to be more expensive. The number of
    rooms per household is also more informative than the total number of rooms in
    a district—obviously the larger the houses, the more expensive they are.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，不错！新的`bedrooms_ratio`属性与房屋中位数的相关性要比总房间数或卧室总数高得多。显然，卧室/房间比例较低的房屋往往更昂贵。每户房间数也比一个地区的总房间数更具信息量——显然，房屋越大，价格越高。
- en: 'This round of exploration does not have to be absolutely thorough; the point
    is to start off on the right foot and quickly gain insights that will help you
    get a first reasonably good prototype. But this is an iterative process: once
    you get a prototype up and running, you can analyze its output to gain more insights
    and come back to this exploration step.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这一轮探索不必绝对彻底；重点是要以正确的方式开始，并迅速获得有助于获得第一个相当不错原型的见解。但这是一个迭代过程：一旦您建立起一个原型并使其运行起来，您可以分析其输出以获得更多见解，并回到这一探索步骤。
- en: Prepare the Data for Machine Learning Algorithms
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习算法准备数据
- en: 'It’s time to prepare the data for your machine learning algorithms. Instead
    of doing this manually, you should write functions for this purpose, for several
    good reasons:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是为您的机器学习算法准备数据的时候了。您应该为此目的编写函数，而不是手动操作，有几个很好的理由：
- en: This will allow you to reproduce these transformations easily on any dataset
    (e.g., the next time you get a fresh dataset).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将使您能够轻松在任何数据集上重现这些转换（例如，下次获取新数据集时）。
- en: You will gradually build a library of transformation functions that you can
    reuse in future projects.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将逐渐构建一个转换函数库，可以在将来的项目中重复使用。
- en: You can use these functions in your live system to transform the new data before
    feeding it to your algorithms.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在实时系统中使用这些函数，在将新数据馈送到算法之前对其进行转换。
- en: This will make it possible for you to easily try various transformations and
    see which combination of transformations works best.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将使您能够轻松尝试各种转换，并查看哪种转换组合效果最好。
- en: 'But first, revert to a clean training set (by copying `strat_train_set` once
    again). You should also separate the predictors and the labels, since you don’t
    necessarily want to apply the same transformations to the predictors and the target
    values (note that `drop()` creates a copy of the data and does not affect `strat_train_set`):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，恢复到一个干净的训练集（再次复制`strat_train_set`）。您还应该分开预测变量和标签，因为您不一定希望对预测变量和目标值应用相同的转换（请注意，`drop()`会创建数据的副本，不会影响`strat_train_set`）：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Clean the Data
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理数据
- en: 'Most machine learning algorithms cannot work with missing features, so you’ll
    need to take care of these. For example, you noticed earlier that the `total_bedrooms`
    attribute has some missing values. You have three options to fix this:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Get rid of the corresponding districts.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get rid of the whole attribute.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the missing values to some value (zero, the mean, the median, etc.). This
    is called *imputation*.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can accomplish these easily using the Pandas DataFrame’s `dropna()`, `drop()`,
    and `fillna()` methods:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You decide to go for option 3 since it is the least destructive, but instead
    of the preceding code, you will use a handy Scikit-Learn class: `SimpleImputer`.
    The benefit is that it will store the median value of each feature: this will
    make it possible to impute missing values not only on the training set, but also
    on the validation set, the test set, and any new data fed to the model. To use
    it, first you need to create a `SimpleImputer` instance, specifying that you want
    to replace each attribute’s missing values with the median of that attribute:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Since the median can only be computed on numerical attributes, you then need
    to create a copy of the data with only the numerical attributes (this will exclude
    the text attribute `ocean_proximity`):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now you can fit the `imputer` instance to the training data using the `fit()`
    method:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `imputer` has simply computed the median of each attribute and stored the
    result in its `statistics_` instance variable. Only the `total_bedrooms` attribute
    had missing values, but you cannot be sure that there won’t be any missing values
    in new data after the system goes live, so it is safer to apply the `imputer`
    to all the numerical attributes:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now you can use this “trained” `imputer` to transform the training set by replacing
    missing values with the learned medians:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Missing values can also be replaced with the mean value (`strategy="mean"`),
    or with the most frequent value (`strategy="most_frequent"`), or with a constant
    value (`strategy="constant", fill_value=`…​). The last two strategies support
    non-numerical data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are also more powerful imputers available in the `sklearn.impute` package
    (both for numerical features only):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '`KNNImputer` replaces each missing value with the mean of the *k*-nearest neighbors’
    values for that feature. The distance is based on all the available features.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IterativeImputer` trains a regression model per feature to predict the missing
    values based on all the other available features. It then trains the model again
    on the updated data, and repeats the process several times, improving the models
    and the replacement values at each iteration.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse matrices)
    even when they are fed Pandas DataFrames as input.⁠^([11](ch02.html#idm45720247944688))
    So, the output of `imputer.transform(housing_num)` is a NumPy array: `X` has neither
    column names nor index. Luckily, it’s not too hard to wrap `X` in a DataFrame
    and recover the column names and index from `housing_num`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Handling Text and Categorical Attributes
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far we have only dealt with numerical attributes, but your data may also
    contain text attributes. In this dataset, there is just one: the `ocean_proximity`
    attribute. Let’s look at its value for the first few instances:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It’s not arbitrary text: there are a limited number of possible values, each
    of which represents a category. So this attribute is a categorical attribute.
    Most machine learning algorithms prefer to work with numbers, so let’s convert
    these categories from text to numbers. For this, we can use Scikit-Learn’s `OrdinalEncoder`
    class:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here’s what the first few encoded values in `housing_cat_encoded` look like:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You can get the list of categories using the `categories_` instance variable.
    It is a list containing a 1D array of categories for each categorical attribute
    (in this case, a list containing a single array since there is just one categorical
    attribute):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'One issue with this representation is that ML algorithms will assume that two
    nearby values are more similar than two distant values. This may be fine in some
    cases (e.g., for ordered categories such as “bad”, “average”, “good”, and “excellent”),
    but it is obviously not the case for the `ocean_proximity` column (for example,
    categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this
    issue, a common solution is to create one binary attribute per category: one attribute
    equal to 1 when the category is `"<1H OCEAN"` (and 0 otherwise), another attribute
    equal to 1 when the category is `"INLAND"` (and 0 otherwise), and so on. This
    is called *one-hot encoding*, because only one attribute will be equal to 1 (hot),
    while the others will be 0 (cold). The new attributes are sometimes called *dummy*
    attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical
    values into one-hot vectors:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示的一个问题是，机器学习算法会假设两个相邻的值比两个远离的值更相似。在某些情况下这可能没问题（例如，对于有序类别如“bad”、“average”、“good”和“excellent”），但显然不适用于`ocean_proximity`列（例如，类别0和4明显比类别0和1更相似）。为了解决这个问题，一个常见的解决方案是为每个类别创建一个二进制属性：当类别是`"<1H
    OCEAN"`时一个属性等于1（否则为0），当类别是`"INLAND"`时另一个属性等于1（否则为0），依此类推。这被称为*独热编码*，因为只有一个属性将等于1（热），而其他属性将等于0（冷）。新属性有时被称为*虚拟*属性。Scikit-Learn提供了一个`OneHotEncoder`类来将分类值转换为独热向量：
- en: '[PRE39]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By default, the output of a `OneHotEncoder` is a SciPy *sparse matrix*, instead
    of a NumPy array:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`OneHotEncoder`的输出是SciPy的*稀疏矩阵*，而不是NumPy数组：
- en: '[PRE40]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'A sparse matrix is a very efficient representation for matrices that contain
    mostly zeros. Indeed, internally it only stores the nonzero values and their positions.
    When a categorical attribute has hundreds or thousands of categories, one-hot
    encoding it results in a very large matrix full of 0s except for a single 1 per
    row. In this case, a sparse matrix is exactly what you need: it will save plenty
    of memory and speed up computations. You can use a sparse matrix mostly like a
    normal 2D array,⁠^([12](ch02.html#idm45720246357984)) but if you want to convert
    it to a (dense) NumPy array, just call the `toarray()` method:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵是包含大部分零的矩阵的非常高效的表示。实际上，它内部只存储非零值及其位置。当一个分类属性有数百或数千个类别时，对其进行独热编码会导致一个非常大的矩阵，除了每行一个单独的1之外，其他都是0。在这种情况下，稀疏矩阵正是你需要的：它将节省大量内存并加快计算速度。你可以像使用普通的2D数组一样使用稀疏矩阵，但如果你想将其转换为（密集的）NumPy数组，只需调用`toarray()`方法：
- en: '[PRE41]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`,
    in which case the `transform()` method will return a regular (dense) NumPy array
    directly.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，当创建`OneHotEncoder`时设置`sparse=False`，在这种情况下，`transform()`方法将直接返回一个常规（密集的）NumPy数组。
- en: 'As with the `OrdinalEncoder`, you can get the list of categories using the
    encoder’s `categories_` instance variable:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 与`OrdinalEncoder`一样，你可以使用编码器的`categories_`实例变量获取类别列表：
- en: '[PRE42]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Pandas has a function called `get_dummies()`, which also converts each categorical
    feature into a one-hot representation, with one binary feature per category:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas有一个名为`get_dummies()`的函数，它也将每个分类特征转换为一种独热表示，每个类别一个二进制特征：
- en: '[PRE43]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It looks nice and simple, so why not use it instead of `OneHotEncoder`? Well,
    the advantage of `OneHotEncoder` is that it remembers which categories it was
    trained on. This is very important because once your model is in production, it
    should be fed exactly the same features as during training: no more, no less.
    Look what our trained `cat_encoder` outputs when we make it transform the same
    `df_test` (using `transform()`, not `fit_transform()`):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很简单漂亮，那为什么不使用它而不是`OneHotEncoder`呢？嗯，`OneHotEncoder`的优势在于它记住了它训练过的类别。这非常重要，因为一旦你的模型投入生产，它应该被喂入与训练期间完全相同的特征：不多，也不少。看看我们训练过的`cat_encoder`在我们让它转换相同的`df_test`时输出了什么（使用`transform()`，而不是`fit_transform()`）：
- en: '[PRE44]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'See the difference? `get_dummies()` saw only two categories, so it output two
    columns, whereas `OneHotEncoder` output one column per learned category, in the
    right order. Moreover, if you feed `get_dummies()` a DataFrame containing an unknown
    category (e.g., `"<2H OCEAN"`), it will happily generate a column for it:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 看到区别了吗？`get_dummies()`只看到了两个类别，所以输出了两列，而`OneHotEncoder`按正确顺序输出了每个学习类别的一列。此外，如果你给`get_dummies()`一个包含未知类别（例如`"<2H
    OCEAN"`）的DataFrame，它会高兴地为其生成一列：
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'But `OneHotEncoder` is smarter: it will detect the unknown category and raise
    an exception. If you prefer, you can set the `handle_unknown` hyperparameter to
    `"ignore"`, in which case it will just represent the unknown category with zeros:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 但是`OneHotEncoder`更聪明：它会检测到未知类别并引发异常。如果你愿意，你可以将`handle_unknown`超参数设置为`"ignore"`，在这种情况下，它将只用零表示未知类别：
- en: '[PRE46]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Tip
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If a categorical attribute has a large number of possible categories (e.g.,
    country code, profession, species), then one-hot encoding will result in a large
    number of input features. This may slow down training and degrade performance.
    If this happens, you may want to replace the categorical input with useful numerical
    features related to the categories: for example, you could replace the `ocean_proximity`
    feature with the distance to the ocean (similarly, a country code could be replaced
    with the country’s population and GDP per capita). Alternatively, you can use
    one of the encoders provided by the `category_encoders` package on [GitHub](https://github.com/scikit-learn-contrib/category_encoders).
    Or, when dealing with neural networks, you can replace each category with a learnable,
    low-dimensional vector called an *embedding*. This is an example of *representation
    learning* (see Chapters [13](ch13.html#data_chapter) and [17](ch17.html#autoencoders_chapter)
    for more details).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'When you fit any Scikit-Learn estimator using a DataFrame, the estimator stores
    the column names in the `feature_names_in_` attribute. Scikit-Learn then ensures
    that any DataFrame fed to this estimator after that (e.g., to `transform()` or
    `predict()`) has the same column names. Transformers also provide a `get_feature_names_out()`
    method that you can use to build a DataFrame around the transformer’s output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Feature Scaling and Transformation
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most important transformations you need to apply to your data is
    *feature scaling*. With few exceptions, machine learning algorithms don’t perform
    well when the input numerical attributes have very different scales. This is the
    case for the housing data: the total number of rooms ranges from about 6 to 39,320,
    while the median incomes only range from 0 to 15\. Without any scaling, most models
    will be biased toward ignoring the median income and focusing more on the number
    of rooms.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common ways to get all attributes to have the same scale: *min-max
    scaling* and *standardization*.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As with all estimators, it is important to fit the scalers to the training
    data only: never use `fit()` or `fit_transform()` for anything else than the training
    set. Once you have a trained scaler, you can then use it to `transform()` any
    other set, including the validation set, the test set, and new data. Note that
    while the training set values will always be scaled to the specified range, if
    new data contains outliers, these may end up scaled outside the range. If you
    want to avoid this, just set the `clip` hyperparameter to `True`.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Min-max scaling (many people call this *normalization*) is the simplest: for
    each attribute, the values are shifted and rescaled so that they end up ranging
    from 0 to 1\. This is performed by subtracting the min value and dividing by the
    difference between the min and the max. Scikit-Learn provides a transformer called
    `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets you
    change the range if, for some reason, you don’t want 0–1 (e.g., neural networks
    work best with zero-mean inputs, so a range of –1 to 1 is preferable). It’s quite
    easy to use:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Standardization is different: first it subtracts the mean value (so standardized
    values have a zero mean), then it divides the result by the standard deviation
    (so standardized values have a standard deviation equal to 1). Unlike min-max
    scaling, standardization does not restrict values to a specific range. However,
    standardization is much less affected by outliers. For example, suppose a district
    has a median income equal to 100 (by mistake), instead of the usual 0–15\. Min-max
    scaling to the 0–1 range would map this outlier down to 1 and it would crush all
    the other values down to 0–0.15, whereas standardization would not be much affected.
    Scikit-Learn provides a transformer called `StandardScaler` for standardization:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Tip
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you want to scale a sparse matrix without converting it to a dense matrix
    first, you can use a `StandardScaler` with its `with_mean` hyperparameter set
    to `False`: it will only divide the data by the standard deviation, without subtracting
    the mean (as this would break sparsity).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'When a feature’s distribution has a *heavy tail* (i.e., when values far from
    the mean are not exponentially rare), both min-max scaling and standardization
    will squash most values into a small range. Machine learning models generally
    don’t like this at all, as you will see in [Chapter 4](ch04.html#linear_models_chapter).
    So *before* you scale the feature, you should first transform it to shrink the
    heavy tail, and if possible to make the distribution roughly symmetrical. For
    example, a common way to do this for positive features with a heavy tail to the
    right is to replace the feature with its square root (or raise the feature to
    a power between 0 and 1). If the feature has a really long and heavy tail, such
    as a *power law distribution*, then replacing the feature with its logarithm may
    help. For example, the `population` feature roughly follows a power law: districts
    with 10,000 inhabitants are only 10 times less frequent than districts with 1,000
    inhabitants, not exponentially less frequent. [Figure 2-17](#long_tail_plot) shows
    how much better this feature looks when you compute its log: it’s very close to
    a Gaussian distribution (i.e., bell-shaped).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0217](assets/mls3_0217.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Transforming a feature to make it closer to a Gaussian distribution
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another approach to handle heavy-tailed features consists in *bucketizing* the
    feature. This means chopping its distribution into roughly equal-sized buckets,
    and replacing each feature value with the index of the bucket it belongs to, much
    like we did to create the `income_cat` feature (although we only used it for stratified
    sampling). For example, you could replace each value with its percentile. Bucketizing
    with equal-sized buckets results in a feature with an almost uniform distribution,
    so there’s no need for further scaling, or you can just divide by the number of
    buckets to force the values to the 0–1 range.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: When a feature has a multimodal distribution (i.e., with two or more clear peaks,
    called *modes*), such as the `housing_median_age` feature, it can also be helpful
    to bucketize it, but this time treating the bucket IDs as categories, rather than
    as numerical values. This means that the bucket indices must be encoded, for example
    using a `OneHotEncoder` (so you usually don’t want to use too many buckets). This
    approach will allow the regression model to more easily learn different rules
    for different ranges of this feature value. For example, perhaps houses built
    around 35 years ago have a peculiar style that fell out of fashion, and therefore
    they’re cheaper than their age alone would suggest.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach to transforming multimodal distributions is to add a feature
    for each of the modes (at least the main ones), representing the similarity between
    the housing median age and that particular mode. The similarity measure is typically
    computed using a *radial basis function* (RBF)—any function that depends only
    on the distance between the input value and a fixed point. The most commonly used
    RBF is the Gaussian RBF, whose output value decays exponentially as the input
    value moves away from the fixed point. For example, the Gaussian RBF similarity
    between the housing age *x* and 35 is given by the equation exp(–*γ*(*x* – 35)²).
    The hyperparameter *γ* (gamma) determines how quickly the similarity measure decays
    as *x* moves away from 35\. Using Scikit-Learn’s `rbf_kernel()` function, you
    can create a new Gaussian RBF feature measuring the similarity between the housing
    median age and 35:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[Figure 2-18](#age_similarity_plot) shows this new feature as a function of
    the housing median age (solid line). It also shows what the feature would look
    like if you used a smaller `gamma` value. As the chart shows, the new age similarity
    feature peaks at 35, right around the spike in the housing median age distribution:
    if this particular age group is well correlated with lower prices, there’s a good
    chance that this new feature will help.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0218](assets/mls3_0218.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: Figure 2-18\. Gaussian RBF feature measuring the similarity between the housing
    median age and 35
  id: totrans-297
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So far we’ve only looked at the input features, but the target values may also
    need to be transformed. For example, if the target distribution has a heavy tail,
    you may choose to replace the target with its logarithm. But if you do, the regression
    model will now predict the *log* of the median house value, not the median house
    value itself. You will need to compute the exponential of the model’s prediction
    if you want the predicted median house value.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, most of Scikit-Learn’s transformers have an `inverse_transform()`
    method, making it easy to compute the inverse of their transformations. For example,
    the following code example shows how to scale the labels using a `StandardScaler`
    (just like we did for inputs), then train a simple linear regression model on
    the resulting scaled labels and use it to make predictions on some new data, which
    we transform back to the original scale using the trained scaler’s `inverse_transform()`
    method. Note that we convert the labels from a Pandas Series to a DataFrame, since
    the `StandardScaler` expects 2D inputs. Also, in this example we just train the
    model on a single raw input feature (median income), for simplicity:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This works fine, but a simpler option is to use a `TransformedTargetRegressor`.
    We just need to construct it, giving it the regression model and the label transformer,
    then fit it on the training set, using the original unscaled labels. It will automatically
    use the transformer to scale the labels and train the regression model on the
    resulting scaled labels, just like we did previously. Then, when we want to make
    a prediction, it will call the regression model’s `predict()` method and use the
    scaler’s `inverse_transform()` method to produce the prediction:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Custom Transformers
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although Scikit-Learn provides many useful transformers, you will need to write
    your own for tasks such as custom transformations, cleanup operations, or combining
    specific attributes.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'For transformations that don’t require any training, you can just write a function
    that takes a NumPy array as input and outputs the transformed array. For example,
    as discussed in the previous section, it’s often a good idea to transform features
    with heavy-tailed distributions by replacing them with their logarithm (assuming
    the feature is positive and the tail is on the right). Let’s create a log-transformer
    and apply it to the `population` feature:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The `inverse_func` argument is optional. It lets you specify an inverse transform
    function, e.g., if you plan to use your transformer in a `TransformedTargetRegressor`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Your transformation function can take hyperparameters as additional arguments.
    For example, here’s how to create a transformer that computes the same Gaussian
    RBF similarity measure as earlier:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Note that there’s no inverse function for the RBF kernel, since there are always
    two values at a given distance from a fixed point (except at distance 0). Also
    note that `rbf_kernel()` does not treat the features separately. If you pass it
    an array with two features, it will measure the 2D distance (Euclidean) to measure
    similarity. For example, here’s how to add a feature that will measure the geographic
    similarity between each district and San Francisco:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Custom transformers are also useful to combine features. For example, here’s
    a `FunctionTransformer` that computes the ratio between the input features 0 and
    1:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '`FunctionTransformer` is very handy, but what if you would like your transformer
    to be trainable, learning some parameters in the `fit()` method and using them
    later in the `transform()` method? For this, you need to write a custom class.
    Scikit-Learn relies on duck typing, so this class does not have to inherit from
    any particular base class. All it needs is three methods: `fit()` (which must
    return `self`), `transform()`, and `fit_transform()`.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get `fit_transform()` for free by simply adding `TransformerMixin`
    as a base class: the default implementation will just call `fit()` and then `transform()`.
    If you add `BaseEstimator` as a base class (and avoid using `*args` and `**kwargs`
    in your constructor), you will also get two extra methods: `get_params()` and
    `set_params()`. These will be useful for automatic hyperparameter tuning.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here’s a custom transformer that acts much like the `StandardScaler`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here are a few things to note:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn.utils.validation` package contains several functions we can use
    to validate the inputs. For simplicity, we will skip such tests in the rest of
    this book, but production code should have them.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-Learn pipelines require the `fit()` method to have two arguments `X`
    and `y`, which is why we need the `y=None` argument even though we don’t use `y`.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All Scikit-Learn estimators set `n_features_in_` in the `fit()` method, and
    they ensure that the data passed to `transform()` or `predict()` has this number
    of features.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fit()` method must return `self`.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This implementation is not 100% complete: all estimators should set `feature_names_in_`
    in the `fit()` method when they are passed a DataFrame. Moreover, all transformers
    should provide a `get_feature_names_out()` method, as well as an `inverse_transform()`
    method when their transformation can be reversed. See the last exercise at the
    end of this chapter for more details.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A custom transformer can (and often does) use other estimators in its implementation.
    For example, the following code demonstrates custom transformer that uses a `KMeans`
    clusterer in the `fit()` method to identify the main clusters in the training
    data, and then uses `rbf_kernel()` in the `transform()` method to measure how
    similar each sample is to each cluster center:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Tip
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can check whether your custom estimator respects Scikit-Learn’s API by passing
    an instance to `check_estimator()` from the `sklearn.utils.estimator_checks` package.
    For the full API, check out [*https://scikit-learn.org/stable/developers*](https://scikit-learn.org/stable/developers).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will see in [Chapter 9](ch09.html#unsupervised_learning_chapter), *k*-means
    is a clustering algorithm that locates clusters in the data. How many it searches
    for is controlled by the `n_clusters` hyperparameter. After training, the cluster
    centers are available via the `cluster_centers_` attribute. The `fit()` method
    of `KMeans` supports an optional argument `sample_weight`, which lets the user
    specify the relative weights of the samples. *k*-means is a stochastic algorithm,
    meaning that it relies on randomness to locate the clusters, so if you want reproducible
    results, you must set the `random_state` parameter. As you can see, despite the
    complexity of the task, the code is fairly straightforward. Now let’s use this
    custom transformer:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This code creates a `ClusterSimilarity` transformer, setting the number of
    clusters to 10\. Then it calls `fit_transform()` with the latitude and longitude
    of every district in the training set, weighting each district by its median house
    value. The transformer uses *k*-means to locate the clusters, then measures the
    Gaussian RBF similarity between each district and all 10 cluster centers. The
    result is a matrix with one row per district, and one column per cluster. Let’s
    look at the first three rows, rounding to two decimal places:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[Figure 2-19](#district_cluster_plot) shows the 10 cluster centers found by
    *k*-means. The districts are colored according to their geographic similarity
    to their closest cluster center. As you can see, most clusters are located in
    highly populated and expensive areas.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0219](assets/mls3_0219.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Gaussian RBF similarity to the nearest cluster center
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transformation Pipelines
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you can see, there are many data transformation steps that need to be executed
    in the right order. Fortunately, Scikit-Learn provides the `Pipeline` class to
    help with such sequences of transformations. Here is a small pipeline for numerical
    attributes, which will first impute then scale the input features:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The `Pipeline` constructor takes a list of name/estimator pairs (2-tuples)
    defining a sequence of steps. The names can be anything you like, as long as they
    are unique and don’t contain double underscores (`__`). They will be useful later,
    when we discuss hyperparameter tuning. The estimators must all be transformers
    (i.e., they must have a `fit_transform()` method), except for the last one, which
    can be anything: a transformer, a predictor, or any other type of estimator.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-339
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a Jupyter notebook, if you `import` `sklearn` and run `sklearn.​set_config(display="diagram")`,
    all Scikit-Learn estimators will be rendered as interactive diagrams. This is
    particularly useful for visualizing pipelines. To visualize `num_pipeline`, run
    a cell with `num_pipeline` as the last line. Clicking an estimator will show more
    details.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t want to name the transformers, you can use the `make_pipeline()`
    function instead; it takes transformers as positional arguments and creates a
    `Pipeline` using the names of the transformers’ classes, in lowercase and without
    underscores (e.g., `"simpleimputer"`):'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If multiple transformers have the same name, an index is appended to their names
    (e.g., `"foo-1"`, `"foo-2"`, etc.).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: When you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially
    on all the transformers, passing the output of each call as the parameter to the
    next call until it reaches the final estimator, for which it just calls the `fit()`
    method.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline exposes the same methods as the final estimator. In this example
    the last estimator is a `StandardScaler`, which is a transformer, so the pipeline
    also acts like a transformer. If you call the pipeline’s `transform()` method,
    it will sequentially apply all the transformations to the data. If the last estimator
    were a predictor instead of a transformer, then the pipeline would have a `predict()`
    method rather than a `transform()` method. Calling it would sequentially apply
    all the transformations to the data and pass the result to the predictor’s `predict()`
    method.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s call the pipeline’s `fit_transform()` method and look at the output’s
    first two rows, rounded to two decimal places:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As you saw earlier, if you want to recover a nice DataFrame, you can use the
    pipeline’s `get_feature_names_out()` method:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Pipelines support indexing; for example, `pipeline[1]` returns the second estimator
    in the pipeline, and `pipeline[:-1]` returns a `Pipeline` object containing all
    but the last estimator. You can also access the estimators via the `steps` attribute,
    which is a list of name/estimator pairs, or via the `named_steps` dictionary attribute,
    which maps the names to the estimators. For example, `num_pipeline["simpleimputer"]`
    returns the estimator named `"simpleimputer"`.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have handled the categorical columns and the numerical columns separately.
    It would be more convenient to have a single transformer capable of handling all
    columns, applying the appropriate transformations to each column. For this, you
    can use a `ColumnTransformer`. For example, the following `ColumnTransformer`
    will apply `num_pipeline` (the one we just defined) to the numerical attributes
    and `cat_pipeline` to the categorical attribute:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: First we import the `ColumnTransformer` class, then we define the list of numerical
    and categorical column names and construct a simple pipeline for categorical attributes.
    Lastly, we construct a `ColumnTransformer`. Its constructor requires a list of
    triplets (3-tuples), each containing a name (which must be unique and not contain
    double underscores), a transformer, and a list of names (or indices) of columns
    that the transformer should be applied to.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of using a transformer, you can specify the string `"drop"` if you want
    the columns to be dropped, or you can specify `"passthrough"` if you want the
    columns to be left untouched. By default, the remaining columns (i.e., the ones
    that were not listed) will be dropped, but you can set the `remainder` hyperparameter
    to any transformer (or to `"passthrough"`) if you want these columns to be handled
    differently.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Since listing all the column names is not very convenient, Scikit-Learn provides
    a `make_column_selector()` function that returns a selector function you can use
    to automatically select all the features of a given type, such as numerical or
    categorical. You can pass this selector function to the `ColumnTransformer` instead
    of column names or indices. Moreover, if you don’t care about naming the transformers,
    you can use `make_column_transformer()`, which chooses the names for you, just
    like `make_pipeline()` does. For example, the following code creates the same
    `ColumnTransformer` as earlier, except the transformers are automatically named
    `"pipeline-1"` and `"pipeline-2"` instead of `"num"` and `"cat"`:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now we’re ready to apply this `ColumnTransformer` to the housing data:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Great! We have a preprocessing pipeline that takes the entire training dataset
    and applies each transformer to the appropriate columns, then concatenates the
    transformed columns horizontally (transformers must never change the number of
    rows). Once again this returns a NumPy array, but you can get the column names
    using `preprocessing.get_feature_names_out()` and wrap the data in a nice DataFrame
    as we did before.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-361
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `OneHotEncoder` returns a sparse matrix and the `num_pipeline` returns a
    dense matrix. When there is such a mix of sparse and dense matrices, the `ColumnTransformer`
    estimates the density of the final matrix (i.e., the ratio of nonzero cells),
    and it returns a sparse matrix if the density is lower than a given threshold
    (by default, `sparse_threshold=0.3`). In this example, it returns a dense matrix.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Your project is going really well and you’re almost ready to train some models!
    You now want to create a single pipeline that will perform all the transformations
    you’ve experimented with up to now. Let’s recap what the pipeline will do and
    why:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Missing values in numerical features will be imputed by replacing them with
    the median, as most ML algorithms don’t expect missing values. In categorical
    features, missing values will be replaced by the most frequent category.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The categorical feature will be one-hot encoded, as most ML algorithms only
    accept numerical inputs.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few ratio features will be computed and added: `bedrooms_ratio`, `rooms_per_house`,
    and `people_per_house`. Hopefully these will better correlate with the median
    house value, and thereby help the ML models.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few cluster similarity features will also be added. These will likely be more
    useful to the model than latitude and longitude.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features with a long tail will be replaced by their logarithm, as most models
    prefer features with roughly uniform or Gaussian distributions.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All numerical features will be standardized, as most ML algorithms prefer when
    all features have roughly the same scale.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code that builds the pipeline to do all of this should look familiar to
    you by now:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'If you run this `ColumnTransformer`, it performs all the transformations and
    outputs a NumPy array with 24 features:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Select and Train a Model
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At last! You framed the problem, you got the data and explored it, you sampled
    a training set and a test set, and you wrote a preprocessing pipeline to automatically
    clean up and prepare your data for machine learning algorithms. You are now ready
    to select and train a machine learning model.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Train and Evaluate on the Training Set
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The good news is that thanks to all these previous steps, things are now going
    to be easy! You decide to train a very basic linear regression model to get started:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Done! You now have a working linear regression model. You try it out on the
    training set, looking at the first five predictions and comparing them to the
    labels:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Well, it works, but not always: the first prediction is way off (by over $200,000!),
    while the other predictions are better: two are off by about 25%, and two are
    off by less than 10%. Remember that you chose to use the RMSE as your performance
    measure, so you want to measure this regression model’s RMSE on the whole training
    set using Scikit-Learn’s `mean_squared_error()` function, with the `squared` argument
    set to `False`:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This is better than nothing, but clearly not a great score: the `median_housing_values`
    of most districts range between $120,000 and $265,000, so a typical prediction
    error of $68,628 is really not very satisfying. This is an example of a model
    underfitting the training data. When this happens it can mean that the features
    do not provide enough information to make good predictions, or that the model
    is not powerful enough. As we saw in the previous chapter, the main ways to fix
    underfitting are to select a more powerful model, to feed the training algorithm
    with better features, or to reduce the constraints on the model. This model is
    not regularized, which rules out the last option. You could try to add more features,
    but first you want to try a more complex model to see how it does.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'You decide to try a `DecisionTreeRegressor`, as this is a fairly powerful model
    capable of finding complex nonlinear relationships in the data (decision trees
    are presented in more detail in [Chapter 6](ch06.html#trees_chapter)):'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now that the model is trained, you evaluate it on the training set:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Wait, what!? No error at all? Could this model really be absolutely perfect?
    Of course, it is much more likely that the model has badly overfit the data. How
    can you be sure? As you saw earlier, you don’t want to touch the test set until
    you are ready to launch a model you are confident about, so you need to use part
    of the training set for training and part of it for model validation.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Better Evaluation Using Cross-Validation
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to evaluate the decision tree model would be to use the `train_​test_split()`
    function to split the training set into a smaller training set and a validation
    set, then train your models against the smaller training set and evaluate them
    against the validation set. It’s a bit of effort, but nothing too difficult, and
    it would work fairly well.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'A great alternative is to use Scikit-Learn’s *k_-fold cross-validation* feature.
    The following code randomly splits the training set into 10 nonoverlapping subsets
    called *folds*, then it trains and evaluates the decision tree model 10 times,
    picking a different fold for evaluation every time and using the other 9 folds
    for training. The result is an array containing the 10 evaluation scores:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Warning
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scikit-Learn’s cross-validation features expect a utility function (greater
    is better) rather than a cost function (lower is better), so the scoring function
    is actually the opposite of the RMSE. It’s a negative value, so you need to switch
    the sign of the output to get the RMSE scores.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the results:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Now the decision tree doesn’t look as good as it did earlier. In fact, it seems
    to perform almost as poorly as the linear regression model! Notice that cross-validation
    allows you to get not only an estimate of the performance of your model, but also
    a measure of how precise this estimate is (i.e., its standard deviation). The
    decision tree has an RMSE of about 66,868, with a standard deviation of about
    2,061\. You would not have this information if you just used one validation set.
    But cross-validation comes at the cost of training the model several times, so
    it is not always feasible.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: If you compute the same metric for the linear regression model, you will find
    that the mean RMSE is 69,858 and the standard deviation is 4,182\. So the decision
    tree model seems to perform very slightly better than the linear model, but the
    difference is minimal due to severe overfitting. We know there’s an overfitting
    problem because the training error is low (actually zero) while the validation
    error is high.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try one last model now: the `RandomForestRegressor`. As you will see
    in [Chapter 7](ch07.html#ensembles_chapter), random forests work by training many
    decision trees on random subsets of the features, then averaging out their predictions.
    Such models composed of many other models are called *ensembles*: they are capable
    of boosting the performance of the underlying model (in this case, decision trees).
    The code is much the same as earlier:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let’s look at the scores:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Wow, this is much better: random forests really look very promising for this
    task! However, if you train a `RandomForest` and measure the RMSE on the training
    set, you will find roughly 17,474: that’s much lower, meaning that there’s still
    quite a lot of overfitting going on. Possible solutions are to simplify the model,
    constrain it (i.e., regularize it), or get a lot more training data. Before you
    dive much deeper into random forests, however, you should try out many other models
    from various categories of machine learning algorithms (e.g., several support
    vector machines with different kernels, and possibly a neural network), without
    spending too much time tweaking the hyperparameters. The goal is to shortlist
    a few (two to five) promising models.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tune Your Model
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume that you now have a shortlist of promising models. You now need
    to fine-tune them. Let’s look at a few ways you can do that.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Grid Search
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One option would be to fiddle with the hyperparameters manually, until you find
    a great combination of hyperparameter values. This would be very tedious work,
    and you may not have time to explore many combinations.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, you can use Scikit-Learn’s `GridSearchCV` class to search for you.
    All you need to do is tell it which hyperparameters you want it to experiment
    with and what values to try out, and it will use cross-validation to evaluate
    all the possible combinations of hyperparameter values. For example, the following
    code searches for the best combination of hyperparameter values for the `RandomForestRegressor`:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Notice that you can refer to any hyperparameter of any estimator in a pipeline,
    even if this estimator is nested deep inside several pipelines and column transformers.
    For example, when Scikit-Learn sees `"preprocessing__geo__n_clusters"`, it splits
    this string at the double underscores, then it looks for an estimator named `"preprocessing"`
    in the pipeline and finds the preprocessing `ColumnTransformer`. Next, it looks
    for a transformer named `"geo"` inside this `ColumnTransformer` and finds the
    `ClusterSimilarity` transformer we used on the latitude and longitude attributes.
    Then it finds this transformer’s `n_clusters` hyperparameter. Similarly, `random_forest__max_features`
    refers to the `max_features` hyperparameter of the estimator named `"random_forest"`,
    which is of course the `RandomForest` model (the `max_features` hyperparameter
    will be explained in [Chapter 7](ch07.html#ensembles_chapter)).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-411
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Wrapping preprocessing steps in a Scikit-Learn pipeline allows you to tune
    the preprocessing hyperparameters along with the model hyperparameters. This is
    a good thing since they often interact. For example, perhaps increasing `n_clusters`
    requires increasing `max_features` as well. If fitting the pipeline transformers
    is computationally expensive, you can set the pipeline’s `memory` hyperparameter
    to the path of a caching directory: when you first fit the pipeline, Scikit-Learn
    will save the fitted transformers to this directory. If you then fit the pipeline
    again with the same hyperparameters, Scikit-Learn will just load the cached transformers.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two dictionaries in this `param_grid`, so `GridSearchCV` will first
    evaluate all 3 × 3 = 9 combinations of `n_clusters` and `max_features` hyperparameter
    values specified in the first `dict`, then it will try all 2 × 3 = 6 combinations
    of hyperparameter values in the second `dict`. So in total the grid search will
    explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the
    pipeline 3 times per combination, since we are using 3-fold cross validation.
    This means there will be a grand total of 15 × 3 = 45 rounds of training! It may
    take a while, but when it is done you can get the best combination of parameters
    like this:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: In this example, the best model is obtained by setting `n_clusters` to 15 and
    setting `max_features` to 8.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-416
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since 15 is the maximum value that was evaluated for `n_clusters`, you should
    probably try searching again with higher values; the score may continue to improve.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: You can access the best estimator using `grid_search.best_estimator_`. If `GridSearchCV`
    is initialized with `refit=True` (which is the default), then once it finds the
    best estimator using cross-validation, it retrains it on the whole training set.
    This is usually a good idea, since feeding it more data will likely improve its
    performance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation scores are available using `grid_search.cv_results_`. This is
    a dictionary, but if you wrap it in a DataFrame you get a nice list of all the
    test scores for each combination of hyperparameters and for each cross-validation
    split, as well as the mean test score across all splits:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The mean test RMSE score for the best model is 44,042, which is better than
    the score you got earlier using the default hyperparameter values (which was 47,019).
    Congratulations, you have successfully fine-tuned your best model!
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Randomized Search
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The grid search approach is fine when you are exploring relatively few combinations,
    like in the previous example, but `RandomizedSearchCV` is often preferable, especially
    when the hyperparameter search space is large. This class can be used in much
    the same way as the `GridSearchCV` class, but instead of trying out all possible
    combinations it evaluates a fixed number of combinations, selecting a random value
    for each hyperparameter at every iteration. This may sound surprising, but this
    approach has several benefits:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: If some of your hyperparameters are continuous (or discrete but with many possible
    values), and you let randomized search run for, say, 1,000 iterations, then it
    will explore 1,000 different values for each of these hyperparameters, whereas
    grid search would only explore the few values you listed for each one.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose a hyperparameter does not actually make much difference, but you don’t
    know it yet. If it has 10 possible values and you add it to your grid search,
    then training will take 10 times longer. But if you add it to a random search,
    it will not make any difference.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are 6 hyperparameters to explore, each with 10 possible values, then
    grid search offers no other choice than training the model a million times, whereas
    random search can always run for any number of iterations you choose.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each hyperparameter, you must provide either a list of possible values,
    or a probability distribution:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Scikit-Learn also has `HalvingRandomSearchCV` and `HalvingGridSearchCV` hyperparameter
    search classes. Their goal is to use the computational resources more efficiently,
    either to train faster or to explore a larger hyperparameter space. Here’s how
    they work: in the first round, many hyperparameter combinations (called “candidates”)
    are generated using either the grid approach or the random approach. These candidates
    are then used to train models that are evaluated using cross-validation, as usual.
    However, training uses limited resources, which speeds up this first round considerably.
    By default, “limited resources” means that the models are trained on a small part
    of the training set. However, other limitations are possible, such as reducing
    the number of training iterations if the model has a hyperparameter to set it.
    Once every candidate has been evaluated, only the best ones go on to the second
    round, where they are allowed more resources to compete. After several rounds,
    the final candidates are evaluated using full resources. This may save you some
    time tuning hyperparameters.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Methods
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to fine-tune your system is to try to combine the models that perform
    best. The group (or “ensemble”) will often perform better than the best individual
    model—just like random forests perform better than the individual decision trees
    they rely on—especially if the individual models make very different types of
    errors. For example, you could train and fine-tune a *k*-nearest neighbors model,
    then create an ensemble model that just predicts the mean of the random forest
    prediction and that model’s prediction. We will cover this topic in more detail
    in [Chapter 7](ch07.html#ensembles_chapter).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Best Models and Their Errors
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will often gain good insights on the problem by inspecting the best models.
    For example, the `RandomForestRegressor` can indicate the relative importance
    of each attribute for making accurate predictions:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Let’s sort these importance scores in descending order and display them next
    to their corresponding attribute names:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: With this information, you may want to try dropping some of the less useful
    features (e.g., apparently only one `ocean_proximity` category is really useful,
    so you could try dropping the others).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-438
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `sklearn.feature_selection.SelectFromModel` transformer can automatically
    drop the least useful features for you: when you fit it, it trains a model (typically
    a random forest), looks at its `feature_importances_` attribute, and selects the
    most useful features. Then when you call `transform()`, it drops the other features.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also look at the specific errors that your system makes, then try
    to understand why it makes them and what could fix the problem: adding extra features
    or getting rid of uninformative ones, cleaning up outliers, etc.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is also a good time to ensure that your model not only works well on average,
    but also on all categories of districts, whether they’re rural or urban, rich
    or poor, northern or southern, minority or not, etc. Creating subsets of your
    validation set for each category takes a bit of work, but it’s important: if your
    model performs poorly on a whole category of districts, then it should probably
    not be deployed until the issue is solved, or at least it should not be used to
    make predictions for that category, as it may do more harm than good.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate Your System on the Test Set
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After tweaking your models for a while, you eventually have a system that performs
    sufficiently well. You are ready to evaluate the final model on the test set.
    There is nothing special about this process; just get the predictors and the labels
    from your test set and run your `final_model` to transform the data and make predictions,
    then evaluate these predictions:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'In some cases, such a point estimate of the generalization error will not be
    quite enough to convince you to launch: what if it is just 0.1% better than the
    model currently in production? You might want to have an idea of how precise this
    estimate is. For this, you can compute a 95% *confidence interval* for the generalization
    error using `scipy.stats.t.interval()`. You get a fairly large interval from 39,275
    to 43,467, and your previous point estimate of 41,424 is roughly in the middle
    of it:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: If you did a lot of hyperparameter tuning, the performance will usually be slightly
    worse than what you measured using cross-validation. That’s because your system
    ends up fine-tuned to perform well on the validation data and will likely not
    perform as well on unknown datasets. That’s not the case in this example since
    the test RMSE is lower than the validation RMSE, but when it happens you must
    resist the temptation to tweak the hyperparameters to make the numbers look good
    on the test set; the improvements would be unlikely to generalize to new data.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the project prelaunch phase: you need to present your solution (highlighting
    what you have learned, what worked and what did not, what assumptions were made,
    and what your system’s limitations are), document everything, and create nice
    presentations with clear visualizations and easy-to-remember statements (e.g.,
    “the median income is the number one predictor of housing prices”). In this California
    housing example, the final performance of the system is not much better than the
    experts’ price estimates, which were often off by 30%, but it may still be a good
    idea to launch it, especially if this frees up some time for the experts so they
    can work on more interesting and productive tasks.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Launch, Monitor, and Maintain Your System
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perfect, you got approval to launch! You now need to get your solution ready
    for production (e.g., polish the code, write documentation and tests, and so on).
    Then you can deploy your model to your production environment. The most basic
    way to do this is just to save the best model you trained, transfer the file to
    your production environment, and load it. To save the model, you can use the `joblib`
    library like this:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Tip
  id: totrans-452
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s often a good idea to save every model you experiment with so that you can
    come back easily to any model you want. You may also save the cross-validation
    scores and perhaps the actual predictions on the validation set. This will allow
    you to easily compare scores across model types, and compare the types of errors
    they make.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your model is transferred to production, you can load it and use it. For
    this you must first import any custom classes and functions the model relies on
    (which means transferring the code to production), then load the model using `joblib`
    and use it to make predictions:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'For example, perhaps the model will be used within a website: the user will
    type in some data about a new district and click the Estimate Price button. This
    will send a query containing the data to the web server, which will forward it
    to your web application, and finally your code will simply call the model’s `predict()`
    method (you want to load the model upon server startup, rather than every time
    the model is used). Alternatively, you can wrap the model within a dedicated web
    service that your web application can query through a REST API⁠^([13](ch02.html#idm45720235925024))
    (see [Figure 2-20](#webservice_model_diagram)). This makes it easier to upgrade
    your model to new versions without interrupting the main application. It also
    simplifies scaling, since you can start as many web services as needed and load-balance
    the requests coming from your web application across these web services. Moreover,
    it allows your web application to use any programming language, not just Python.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0220](assets/mls3_0220.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. A model deployed as a web service and used by a web application
  id: totrans-458
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Another popular strategy is to deploy your model to the cloud, for example
    on Google’s Vertex AI (formerly known as Google Cloud AI Platform and Google Cloud
    ML Engine): just save your model using `joblib` and upload it to Google Cloud
    Storage (GCS), then head over to Vertex AI and create a new model version, pointing
    it to the GCS file. That’s it! This gives you a simple web service that takes
    care of load balancing and scaling for you. It takes JSON requests containing
    the input data (e.g., of a district) and returns JSON responses containing the
    predictions. You can then use this web service in your website (or whatever production
    environment you are using). As you will see in [Chapter 19](ch19.html#deployment_chapter),
    deploying TensorFlow models on Vertex AI is not much different from deploying
    Scikit-Learn models.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'But deployment is not the end of the story. You also need to write monitoring
    code to check your system’s live performance at regular intervals and trigger
    alerts when it drops. It may drop very quickly, for example if a component breaks
    in your infrastructure, but be aware that it could also decay very slowly, which
    can easily go unnoticed for a long time. This is quite common because of model
    rot: if the model was trained with last year’s data, it may not be adapted to
    today’s data.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: So, you need to monitor your model’s live performance. But how do you do that?
    Well, it depends. In some cases, the model’s performance can be inferred from
    downstream metrics. For example, if your model is part of a recommender system
    and it suggests products that the users may be interested in, then it’s easy to
    monitor the number of recommended products sold each day. If this number drops
    (compared to non-recommended products), then the prime suspect is the model. This
    may be because the data pipeline is broken, or perhaps the model needs to be retrained
    on fresh data (as we will discuss shortly).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: However, you may also need human analysis to assess the model’s performance.
    For example, suppose you trained an image classification model (we’ll look at
    these in [Chapter 3](ch03.html#classification_chapter)) to detect various product
    defects on a production line. How can you get an alert if the model’s performance
    drops, before thousands of defective products get shipped to your clients? One
    solution is to send to human raters a sample of all the pictures that the model
    classified (especially pictures that the model wasn’t so sure about). Depending
    on the task, the raters may need to be experts, or they could be nonspecialists,
    such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In
    some applications they could even be the users themselves, responding, for example,
    via surveys or repurposed captchas.⁠^([14](ch02.html#idm45720235915248))
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Either way, you need to put in place a monitoring system (with or without human
    raters to evaluate the live model), as well as all the relevant processes to define
    what to do in case of failures and how to prepare for them. Unfortunately, this
    can be a lot of work. In fact, it is often much more work than building and training
    a model.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'If the data keeps evolving, you will need to update your datasets and retrain
    your model regularly. You should probably automate the whole process as much as
    possible. Here are a few things you can automate:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Collect fresh data regularly and label it (e.g., using human raters).
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a script to train the model and fine-tune the hyperparameters automatically.
    This script could run automatically, for example every day or every week, depending
    on your needs.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write another script that will evaluate both the new model and the previous
    model on the updated test set, and deploy the model to production if the performance
    has not decreased (if it did, make sure you investigate why). The script should
    probably test the performance of your model on various subsets of the test set,
    such as poor or rich districts, rural or urban districts, etc.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should also make sure you evaluate the model’s input data quality. Sometimes
    performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning
    sensor sending random values, or another team’s output becoming stale), but it
    may take a while before your system’s performance degrades enough to trigger an
    alert. If you monitor your model’s inputs, you may catch this earlier. For example,
    you could trigger an alert if more and more inputs are missing a feature, or the
    mean or standard deviation drifts too far from the training set, or a categorical
    feature starts containing new categories.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Finally, make sure you keep backups of every model you create and have the process
    and tools in place to roll back to a previous model quickly, in case the new model
    starts failing badly for some reason. Having backups also makes it possible to
    easily compare new models with previous ones. Similarly, you should keep backups
    of every version of your datasets so that you can roll back to a previous dataset
    if the new one ever gets corrupted (e.g., if the fresh data that gets added to
    it turns out to be full of outliers). Having backups of your datasets also allows
    you to evaluate any model against any previous dataset.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, machine learning involves quite a lot of infrastructure. [Chapter 19](ch19.html#deployment_chapter)
    discusses some aspects of this, but it’s a very broad topic called *ML Operations*
    (MLOps), which deserves its own book. So don’t be surprised if your first ML project
    takes a lot of effort and time to build and deploy to production. Fortunately,
    once all the infrastructure is in place, going from idea to production will be
    much faster.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Try It Out!
  id: totrans-471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hopefully this chapter gave you a good idea of what a machine learning project
    looks like as well as showing you some of the tools you can use to train a great
    system. As you can see, much of the work is in the data preparation step: building
    monitoring tools, setting up human evaluation pipelines, and automating regular
    model training. The machine learning algorithms are important, of course, but
    it is probably preferable to be comfortable with the overall process and know
    three or four algorithms well rather than to spend all your time exploring advanced
    algorithms.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you have not already done so, now is a good time to pick up a laptop,
    select a dataset that you are interested in, and try to go through the whole process
    from A to Z. A good place to start is on a competition website such as [Kaggle](https://kaggle.com):
    you will have a dataset to play with, a clear goal, and people to share the experience
    with. Have fun!'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-474
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following exercises are based on this chapter’s housing dataset:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Try a support vector machine regressor (`sklearn.svm.SVR`) with various hyperparameters,
    such as `kernel="linear"` (with various values for the `C` hyperparameter) or
    `kernel="rbf"` (with various values for the `C` and `gamma` hyperparameters).
    Note that support vector machines don’t scale well to large datasets, so you should
    probably train your model on just the first 5,000 instances of the training set
    and use only 3-fold cross-validation, or else it will take hours. Don’t worry
    about what the hyperparameters mean for now; we’ll discuss them in [Chapter 5](ch05.html#svm_chapter).
    How does the best `SVR` predictor perform?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try replacing the `GridSearchCV` with a `RandomizedSearchCV`.
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try adding a `SelectFromModel` transformer in the preparation pipeline to select
    only the most important attributes.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try creating a custom transformer that trains a *k*-nearest neighbors regressor
    (`sklearn.neighbors.KNeighborsRegressor`) in its `fit()` method, and outputs the
    model’s predictions in its `transform()` method. Then add this feature to the
    preprocessing pipeline, using latitude and longitude as the inputs to this transformer.
    This will add a feature in the model that corresponds to the housing median price
    of the nearest districts.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Automatically explore some preparation options using `GridSearchCV`.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try to implement the `StandardScalerClone` class again from scratch, then add
    support for the `inverse_transform()` method: executing `scaler.​inverse_transform(scaler.fit_transform(X))`
    should return an array very close to `X`. Then add support for feature names:
    set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This
    attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()`
    method: it should have one optional `input_features=None` argument. If passed,
    the method should check that its length matches `n_features_in_`, and it should
    match `feature_names_in_` if it is defined; then `input_features` should be returned.
    If `input_features` is `None`, then the method should either return `feature_names_in_`
    if it is defined or `np.array(["x0", "x1", ...])` with length `n_features_in_`
    otherwise.'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch02.html#idm45720251177184-marker)) The original dataset appeared in
    R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions”, *Statistics
    & Probability Letters* 33, no. 3 (1997): 291–297.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch02.html#idm45720251160848-marker)) A piece of information fed to a
    machine learning system is often called a *signal*, in reference to Claude Shannon’s
    information theory, which he developed at Bell Labs to improve telecommunications.
    His theory: you want a high signal-to-noise ratio.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#idm45720251064784-marker)) Recall that the transpose operator
    flips a column vector into a row vector (and vice versa).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.html#idm45720248484800-marker)) You might also need to check legal
    constraints, such as private fields that should never be copied to unsafe data
    stores.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch02.html#idm45720248920160-marker)) The standard deviation is generally
    denoted *σ* (the Greek letter sigma), and it is the square root of the *variance*,
    which is the average of the squared deviation from the mean. When a feature has
    a bell-shaped *normal distribution* (also called a *Gaussian distribution*), which
    is very common, the “68-95-99.7” rule applies: about 68% of the values fall within
    1*σ* of the mean, 95% within 2*σ*, and 99.7% within 3*σ*.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch02.html#idm45720239285936-marker)) You will often see people set the
    random seed to 42\. This number has no special property, other than being the
    Answer to the Ultimate Question of Life, the Universe, and Everything.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch02.html#idm45720242437568-marker)) The location information is actually
    quite coarse, and as a result many districts will have the exact same ID, so they
    will end up in the same set (test or train). This introduces some unfortunate
    sampling bias.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch02.html#idm45720243740400-marker)) If you are reading this in grayscale,
    grab a red pen and scribble over most of the coastline from the Bay Area down
    to San Diego (as you might expect). You can add a patch of yellow around Sacramento
    as well.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch02.html#idm45720247972912-marker)) For more details on the design principles,
    see Lars Buitinck et al., “API Design for Machine Learning Software: Experiences
    from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238 (2013).'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch02.html#idm45720247957472-marker)) Some predictors also provide methods
    to measure the confidence of their predictions.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch02.html#idm45720247944688-marker)) By the time you read these lines,
    it may be possible to make all transformers output Pandas DataFrames when they
    receive a DataFrame as input: Pandas in, Pandas out. There will likely be a global
    configuration option for this: `sklearn.set_config(pandas_in_out=True)`.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch02.html#idm45720246357984-marker)) See SciPy’s documentation for more
    details.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch02.html#idm45720235925024-marker)) In a nutshell, a REST (or RESTful)
    API is an HTTP-based API that follows some conventions, such as using standard
    HTTP verbs to read, update, create, or delete resources (GET, POST, PUT, and DELETE)
    and using JSON for the inputs and outputs.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch02.html#idm45720235915248-marker)) A captcha is a test to ensure a
    user is not a robot. These tests have often been used as a cheap way to label
    training data.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
