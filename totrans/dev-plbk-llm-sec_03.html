<html><head></head><body><section data-pdf-bookmark="Chapter 3. Architectures and Trust Boundaries" data-type="chapter" epub:type="chapter"><div class="chapter" id="architectures_and_trust_boundaries">&#13;
      <h1><span class="label">Chapter 3. </span>Architectures and Trust Boundaries</h1>&#13;
      <p><a contenteditable="false" data-primary="architectures and trust boundaries" data-type="indexterm" id="ch03.html0"/>Unlike traditional web applications that rely on predefined algorithms and static databases, LLMs utilize massive neural networks to generate dynamic, context-aware responses. This seismic shift brings a unique set of security challenges, different from those seen in traditional web applications. While researchers have meticulously studied web applications and their vulnerabilities, the field of LLM security is still <span class="keep-together">relatively</span> nascent.</p>&#13;
      <p>This chapter aims to bridge this knowledge gap by dissecting the fundamental elements that set LLMs apart. We’ll start by exploring the building blocks of AI, neural networks, and how they relate to large language models. Then, we dive into the groundbreaking architecture that powers most LLMs today—the transformer model. Following this, we look into the various LLM-powered applications, such as chatbots and copilots.</p>&#13;
      <p><a contenteditable="false" data-primary="trust boundaries" data-secondary="defined" data-type="indexterm" id="id274"/>However, in addition to understanding the technology, security professionals must be aware of the new kinds of <em>trust boundaries</em> unique to LLMs—boundaries that demarcate areas of varying trustworthiness within an application. These include user prompts, uploaded content, training and test data, databases, plug-ins, and other boundary systems that we’ll detail later in the chapter.</p>&#13;
      <section data-pdf-bookmark="AI, Neural Networks, and Large Language Models: What’s the Difference? " data-type="sect1"><div class="sect1" id="ai_neural_networks_and_large_language_models_wh">&#13;
        <h1>AI, Neural Networks, and Large Language Models: <span class="keep-together">What’s the Difference?</span> </h1>&#13;
        <p><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="neural networks and LLMs compared to" data-type="indexterm" id="ch03.html1"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="AI/neural networks/LLMs compared" data-type="indexterm" id="ch03.html2"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="AI and neural networks compared to" data-type="indexterm" id="ch03.html3"/><a contenteditable="false" data-primary="neural networks" data-secondary="AI and LLMs compared to" data-type="indexterm" id="ch03.html4"/>Artificial intelligence, neural network, and LLM are terms often used interchangeably, but they represent different facets of a broader landscape of machine learning and computational intelligence. Let’s break down the differences to understand their unique roles in technology and security:</p>&#13;
        <dl>&#13;
          <dt>Artificial intelligence (AI)</dt>&#13;
          <dd>&#13;
            <p><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="defined" data-type="indexterm" id="id275"/><em>Artificial intelligence</em>, at its core, is a multidisciplinary field aimed at creating systems capable of performing tasks that would ordinarily require human intelligence. These tasks include problem-solving, perception, and language understanding. AI encompasses a wide range of technologies and methodologies, from rule-based systems to machine learning algorithms, serving as an umbrella term for multiple approaches to achieving artificial intelligence. It’s worth noting that the very definition of AI has been a moving target over the past few decades and continues to evolve as technology advances.</p>&#13;
          </dd>&#13;
          <dt>Neural networks</dt>&#13;
          <dd>&#13;
            <p><a contenteditable="false" data-primary="neural networks" data-secondary="defined" data-type="indexterm" id="id276"/><em>Neural networks</em> are one type of AI technology inspired by the human brain’s architecture. They are computational models designed to recognize patterns and make decisions based on the data they process. Neural networks can be simple, with a minimal number of layers (shallow neural networks), or highly complex, with multiple interconnected layers (deep neural networks). They are the backbone of many modern AI applications, including image recognition, natural language processing, and autonomous vehicles.</p>&#13;
          </dd>&#13;
          <dt>Large language models (LLMs)</dt>&#13;
          <dd>&#13;
            <p><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="defined" data-type="indexterm" id="id277"/><em>LLMs</em> represent a specific type of neural network. LLMs usually employ advanced forms of neural networks, such as transformer models, to analyze and produce text based on the training data their developers feed them. What sets them apart is their massive scale and specialization in handling linguistic tasks, which range from simple text completion to complex question answering and summarization.</p>&#13;
          </dd>&#13;
        </dl>&#13;
        <p>Understanding these distinctions is crucial for security professionals. Each layer—from broad AI technologies to specialized LLMs—introduces vulnerabilities and requires unique security measures. As we analyze the complexities of LLMs, recognizing their position in the broader AI landscape will be critical to discussing effectively safeguarding them. The rest of this book is centered on that discussion.<a contenteditable="false" data-primary="" data-startref="ch03.html4" data-type="indexterm" id="id278"/><a contenteditable="false" data-primary="" data-startref="ch03.html3" data-type="indexterm" id="id279"/><a contenteditable="false" data-primary="" data-startref="ch03.html2" data-type="indexterm" id="id280"/><a contenteditable="false" data-primary="" data-startref="ch03.html1" data-type="indexterm" id="id281"/></p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="The Transformer Revolution: Origins, Impact, and the LLM Connection" data-type="sect1"><div class="sect1" id="the_transformer_revolution_origins_impact_and_t">&#13;
        <h1>The Transformer Revolution: Origins, Impact, and <span class="keep-together">the LLM Connection</span></h1>&#13;
        <p><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="transformer architectures" data-type="indexterm" id="ch03.html5"/><a contenteditable="false" data-primary="transformer architectures" data-type="indexterm" id="ch03.html6"/>The transformer architecture is a pivotal milestone in the evolution of artificial intelligence, profoundly impacting the AI landscape and, by extension, LLMs. Let’s unravel the story of the transformer revolution—where it came from, when it happened, and the seismic shifts it brought to AI and LLMs.</p>&#13;
        <section class="pagebreak-before less_space" data-pdf-bookmark="Origins of the Transformer" data-type="sect2"><div class="sect2" id="origins_of_the_transformer">&#13;
          <h2>Origins of the Transformer</h2>&#13;
          <p><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="transformer architectures" data-tertiary="origins" data-type="indexterm" id="id282"/><a contenteditable="false" data-primary="transformer architectures" data-secondary="origins" data-type="indexterm" id="id283"/>The <a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-type="indexterm" id="id284"/><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="id285"/><a contenteditable="false" data-primary="neural networks" data-secondary="convolutional" data-type="indexterm" id="id286"/><a contenteditable="false" data-primary="neural networks" data-secondary="recurrent" data-type="indexterm" id="id287"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="id288"/><a contenteditable="false" data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="id289"/>transformer architecture was introduced in the groundbreaking research paper <a href="https://oreil.ly/lRNoH">“Attention Is All You Need”</a> by Ashish Vaswani et al., published in 2017. This paper proposed a novel approach to natural language processing (NLP) tasks, departing from the traditional models that relied heavily on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The transformer introduced a key innovation: the self-attention mechanism. This mechanism allowed the model to weigh the importance of different words in a sentence, enabling it to understand context more effectively.</p>&#13;
          <p>Before the emergence of transformers, the world of neural networks was replete with promise but often struggled to deliver on the lofty expectations. Traditional architectures like RNNs and CNNs enabled advanced AI capabilities but grappled with inherent limitations. These limitations stemmed from their inability to capture and utilize context effectively, particularly in natural language understanding.</p>&#13;
          <p>RNNs, while suitable for sequential data, faced challenges maintaining context over long sequences. They exhibited a form of “short-term memory,” which made them less adept at grasping intricate relationships and dependencies within lengthy texts or conversations. On the other hand, CNNs, renowned for their prowess in image recognition, needed help to extend their effectiveness to sequential data like language, where understanding context across words and sentences was paramount.</p>&#13;
          <p>This shortcoming in contextual understanding was the Achilles’ heel of traditional neural networks. They could only glimpse small portions of a text at a time, rendering them incapable of comprehending the broader narrative or nuances. It was akin to trying to understand a novel by reading only a few random sentences from its pages. The result was a gap between the promise of AI and its practical application, particularly in natural language understanding. It was this gap that the transformer architecture would bridge, unleashing a wave of progress and redefining the landscape of AI-driven language models.<a contenteditable="false" data-primary="" data-startref="ch03.html8" data-type="indexterm" id="id290"/><a contenteditable="false" data-primary="" data-startref="ch03.html7" data-type="indexterm" id="id291"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Transformer Architecture’s Impact on AI" data-type="sect2"><div class="sect2" id="transformer_architecture_s_impact_on_ai">&#13;
          <h2>Transformer Architecture’s Impact on AI</h2>&#13;
          <p><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="transformer architectures" data-tertiary="impact on AI" data-type="indexterm" id="ch03.html9"/><a contenteditable="false" data-primary="transformer architectures" data-secondary="impact on AI" data-type="indexterm" id="ch03.html10"/>Introducing the transformer architecture wasn’t just a milestone for natural language processing; it marked a paradigm shift across multiple domains within the AI landscape. While researchers initially used the transformer architecture to solve problems related to understanding and generating text, researchers and engineers quickly found that its capabilities extended far beyond that. Here are some areas where transformer architectures have made a considerable impact:</p>&#13;
          <dl>&#13;
            <dt class="pagebreak-before less_space">Natural language processing (NLP)</dt>&#13;
            <dd>&#13;
              <p><a contenteditable="false" data-primary="natural language processing (NLP)" data-type="indexterm" id="id292"/><a contenteditable="false" data-primary="NLP (natural language processing)" data-type="indexterm" id="id293"/>Of course, the first and most immediate impact was in NLP. Transformer models are now the backbone for various language tasks such as translation, summarization, question-answering, and sentiment analysis. They have set new performance benchmarks, sometimes surpassing human-level capabilities in specific tasks.</p>&#13;
            </dd>&#13;
            <dt>Computer vision</dt>&#13;
            <dd>&#13;
              <p>Interestingly, the transformer architecture also has applications in computer vision. While CNNs have been the gold standard for image-related tasks, transformer-based models like vision transformer (ViT) demonstrate competitive, if not superior, performance in tasks like image classification, object detection, and segmentation.</p>&#13;
            </dd>&#13;
            <dt>Speech recognition</dt>&#13;
            <dd>&#13;
              <p>The flexibility of transformer architectures has also made them a good fit for speech recognition. Combined with specialized models like the conformer, which fuses convolutional layers with transformer layers, they have set new standards for understanding spoken language.</p>&#13;
            </dd>&#13;
            <dt>Autonomous systems and self-driving cars</dt>&#13;
            <dd>&#13;
              <p>One of the most intriguing applications of transformers is autonomous systems, including self-driving cars. These vehicles require a high contextual understanding to navigate the world safely. Transformer models are at the heart of self-driving models from companies like Tesla.</p>&#13;
            </dd>&#13;
            <dt>Health care</dt>&#13;
            <dd>&#13;
              <p>In health care, transformer models are aiding in tasks ranging from drug discovery to the analysis of medical images. Their ability to sift through and interpret large amounts of data can speed up research and potentially lead to more accurate diagnoses.</p>&#13;
            </dd>&#13;
          </dl>&#13;
          <p>Therefore, the rise of the transformer architecture has been a tide that lifted all boats, revolutionizing not just one but multiple fields within AI. However, this versatility also brings unique security challenges across these various applications. As we look more deeply into LLM security, we’ll explore how the ubiquitous nature of transformer architectures necessitates a multifaceted approach to safeguarding AI systems<a contenteditable="false" data-primary="" data-startref="ch03.html10" data-type="indexterm" id="id294"/><a contenteditable="false" data-primary="" data-startref="ch03.html9" data-type="indexterm" id="id295"/>.<a contenteditable="false" data-primary="" data-startref="ch03.html6" data-type="indexterm" id="id296"/><a contenteditable="false" data-primary="" data-startref="ch03.html5" data-type="indexterm" id="id297"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Types of LLM-Based Applications" data-type="sect1"><div class="sect1" id="types_of_llm_based_applications">&#13;
        <h1>Types of LLM-Based Applications</h1>&#13;
        <p><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="types of LLM-based applications" data-type="indexterm" id="ch03.html11"/>Two common types of LLM-based applications are chatbots and copilots. Let’s briefly look at each to help you understand the breadth of applications in which developers use LLMs and give you context for understanding various architectural choices as you study further.</p>&#13;
        <p class="pagebreak-before less_space"><a contenteditable="false" data-primary="chatbots" data-type="indexterm" id="id298"/><em>Chatbots</em> are computer programs that can simulate conversations with humans, and they often power customer service applications, where they can answer questions and support customers. Chatbots also excel at entertainment applications like playing games or telling stories. Tay from <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a> is an example of an entertainment chatbot. Here are some more examples of LLM-based chatbots:</p>&#13;
        <ul>&#13;
          <li>&#13;
            <p>Sephora uses a chatbot to help customers find the right products for their skin type and needs.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>H&amp;M uses a chatbot to help customers find clothes and accessories that match their style.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Domino’s Pizza uses a chatbot to allow customers to order pizza via X (Twitter) or Facebook Messenger.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Fandango uses a chatbot to help customers find movie times and theaters nearby.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>JetBlue Airways uses a chatbot to answer customer questions about flights.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Amtrak uses a chatbot to help customers book tickets, check train status, and get answers to their questions.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>The Golden State Warriors use a chatbot to help fans purchase tickets, learn about upcoming games, and get news about the team.</p>&#13;
          </li>&#13;
        </ul>&#13;
        <p><a contenteditable="false" data-primary="copilots" data-type="indexterm" id="id299"/><em>Copilots</em> are AI systems that can assist humans with writing, coding, and research tasks. They can help users to generate ideas, identify errors, and improve their work. Copilots are still under development, but they have the potential to revolutionize the way we work and learn. Specific examples of LLM-based copilots are:</p>&#13;
        <ul>&#13;
          <li>&#13;
            <p>Grammarly and ProWritingAid help users improve their writing by identifying and correcting grammatical errors, suggesting style improvements, and providing feedback.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>GitHub Copilot, Google Gemini Code Assist, and AWS CodeWhisperer help programmers write code faster and more efficiently. They can generate code suggestions, translate between programming languages, and help to identify and debug errors.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Copilot for Microsoft 365 and Gemini for Google Workspace are AI-powered tools integrated into their respective office suites that help users to be more productive and creative in their work.</p>&#13;
          </li>&#13;
        </ul>&#13;
        <div class="less_space" data-type="note" epub:type="note"><h6>Note</h6>&#13;
          <p>A chatbot like ChatGPT can read and review a text block and then provide suggestions to improve it. However, the experience of using a copilot like Grammarly to do that is dramatically different and generally superior for that type of focused task.</p>&#13;
        </div>&#13;
        <p class="pagebreak-before less_space">Similarities between chatbots and copilots:</p>&#13;
        <ul>&#13;
          <li>&#13;
            <p>Both chatbots and copilots are LLM-based applications.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Both chatbots and copilots generate text.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Both chatbots and copilots assist humans with tasks.</p>&#13;
          </li>&#13;
        </ul>&#13;
        <p>Differences between chatbots and copilots:</p>&#13;
        <ul>&#13;
          <li>&#13;
            <p>Chatbots simulate conversation with humans, while copilots assist humans with specific tasks.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Chatbots often power customer service applications, while copilots assist in writing, coding, and research applications.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Chatbots are typically more interactive than copilots, while copilots focus more on completing tasks.</p>&#13;
          </li>&#13;
        </ul>&#13;
        <p>Keep these concepts in mind as we dig into the details of LLM architectures. Both application types share similar components, but you may make different decisions on implementing pieces based on the differing security considerations.<a contenteditable="false" data-primary="" data-startref="ch03.html11" data-type="indexterm" id="id300"/></p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="LLM Application Architecture" data-type="sect1"><div class="sect1" id="llm_application_architecture">&#13;
        <h1>LLM Application Architecture</h1>&#13;
        <p><a contenteditable="false" data-primary="application architecture, for LLM" data-type="indexterm" id="ch03.html12"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-type="indexterm" id="ch03.html13"/>Developers often consider LLMs standalone entities capable of impressive text generation and comprehension feats. However, in practice, an LLM is rarely isolated; it is a cog in the intricate machinery that constitutes an intelligent application. These applications are complex systems comprising multiple interconnected components, each playing a vital role in the overall functionality and performance of the application. Whether a conversational agent, an automated content generator, or a copilot for code, an LLM usually interacts with various elements such as users, databases, APIs, web pages, and even other machine learning models.</p>&#13;
        <p>Understanding the architecture of such composite systems is not just a matter of technical proficiency; it is crucial for effective security planning. The way these components interact introduces multiple trust and data flow layers, defining new security boundaries far removed from traditional web application security models. For instance, user inputs may not just be simple text fields but could include voice commands, images, or real-time collaborative editing. Similarly, an LLM’s outputs could be fed into other systems for further processing, introducing vulnerabilities and risks.</p>&#13;
        <p>In essence, the holistic view of an LLM-based application goes beyond securing the language model itself. It demands a comprehensive approach that considers the security of the entire architecture, from data ingestion and storage to model serving and user interaction. Only by understanding these intricacies can one formulate an <span class="keep-together">effective</span> strategy to safeguard an application against the myriad vulnerabilities such complex systems inherently possess.</p>&#13;
        <p>As we dig deeper into the subject in this chapter, we’ll dissect the various components that typically make up an LLM application, examine their roles, and explore the unique security challenges each presents. This understanding will be the foundation for a robust, multilayered approach to securing your LLM-based applications.</p>&#13;
        <p><a data-type="xref" href="#fig_1_typical_llm_application_dataflow_architecture">Figure 3-1</a> shows a highly simplified diagram to illustrate the components, relationships, and data flows in an application using an LLM. Subsequent chapters will expand on these areas.</p>&#13;
        <figure><div class="figure" id="fig_1_typical_llm_application_dataflow_architecture">&#13;
          <img alt="" src="assets/dpls_0301.png"/>&#13;
          <h6><span class="label">Figure 3-1. </span>Typical LLM application data-flow architecture</h6>&#13;
        </div></figure>&#13;
        <section data-pdf-bookmark="Trust Boundaries" data-type="sect2"><div class="sect2" id="trust_boundaries">&#13;
          <h2>Trust Boundaries</h2>&#13;
          <p><a contenteditable="false" data-primary="application architecture, for LLM" data-secondary="trust boundaries" data-type="indexterm" id="ch03.html14"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-tertiary="trust boundaries" data-type="indexterm" id="ch03.html15"/><a contenteditable="false" data-primary="trust boundaries" data-type="indexterm" id="ch03.html16"/>In application security, a <em>trust boundary</em> serves as an invisible, yet crucial, demarcation line that separates different components or entities based on their level of trustworthiness. These boundaries delineate areas where data or control flow changes from one level of trust to another—such as transitioning from user-controlled input to internal processing or moving from a secure internal database to a public-facing API. These boundaries act as checkpoints where developers should rigorously apply security measures like authentication, authorization, and data validation to prevent vulnerabilities.</p>&#13;
          <div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
            <p>Understanding trust boundaries is critical to threat modeling. Properly defining and recognizing these boundaries can be the difference between a secure system and one vulnerable to threats. </p>&#13;
          </div>&#13;
          <p class="pagebreak-before less_space"><a data-type="xref" href="#fig_2_llm_application_architecture_with_trust_boundaries">Figure 3-2</a> adds the trust boundaries to our architecture diagram.</p>&#13;
          <figure><div class="figure" id="fig_2_llm_application_architecture_with_trust_boundaries">&#13;
            <img alt="" src="assets/dpls_0302.png"/>&#13;
            <h6><span class="label">Figure 3-2. </span>LLM application architecture with trust boundaries</h6>&#13;
          </div></figure>&#13;
          <p>These boundaries, as depicted in the diagram, serve as gateways through which the LLM interfaces with diverse components—public data from the web, structured databases, spontaneous user interactions, or internally sourced training sets. Each delineated boundary highlights considerations we must make when considering data that flows into and out of the LLM. Here’s a quick summary; we’ll dive more deeply in the next section:</p>&#13;
          <dl>&#13;
            <dt>User interactions</dt>&#13;
            <dd>&#13;
              <p>You’ll need to consider safeguarding the model from potential adversarial or misleading inputs that users or systems might introduce. You’ll also need to worry about toxic, inaccurate, or sensitive data being output from the model and passed back to the user.</p>&#13;
            </dd>&#13;
            <dt>In-the-wild training data</dt>&#13;
            <dd>&#13;
              <p><a contenteditable="false" data-primary="training data" data-secondary="trust boundaries" data-type="indexterm" id="id301"/>LLMs are often trained on massive amounts of internet data. You need to consider this data untrusted and watch out for potential toxicity, bias, and adversarial data poisoning, which we’ll cover in <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>.</p>&#13;
            </dd>&#13;
            <dt>Internal test and training data</dt>&#13;
            <dd>&#13;
              <p>You may use internally curated data to fine-tune your model, which can significantly increase accuracy. But you must be wary of ingesting and exposing sensitive, confidential, or personally identifiable information. We’ll discuss this more in <a data-type="xref" href="ch05.html#can_your_llm_know_too_much">Chapter 5</a>.</p>&#13;
            </dd>&#13;
            <dt>External services</dt>&#13;
            <dd>&#13;
              <p>You must actively control how the LLM interfaces with connected services, like databases or APIs, from unauthorized interactions or data leaks. We’ll cover this more in <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>.</p>&#13;
            </dd>&#13;
            <dt>Public data access</dt>&#13;
            <dd>&#13;
              <p>Pulling data live from the web can be a powerful way to augment your application’s capabilities. However, you’ll need to consider this data untrusted and watch for issues like indirect prompt injection, which we’ll cover in <a data-type="xref" href="ch04.html#prompt_injection">Chapter 4</a>.</p>&#13;
            </dd>&#13;
          </dl>&#13;
          <p>Each point is a potential avenue of vulnerability, susceptible to exploitation if overlooked. In the evolving landscape of LLM applications, securing these trust boundaries is not just best practice—it’s essential to prevent unauthorized data access, mitigate data tampering, and avert system breaches. Recognizing these boundaries and their implications is the cornerstone of a resilient LLM security architecture. Now, let’s go into more detail on each area to ensure you have enough context to dive into the following chapters that detail the risk areas and mitigations.<a contenteditable="false" data-primary="" data-startref="ch03.html16" data-type="indexterm" id="id302"/><a contenteditable="false" data-primary="" data-startref="ch03.html15" data-type="indexterm" id="id303"/><a contenteditable="false" data-primary="" data-startref="ch03.html14" data-type="indexterm" id="id304"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="The Model" data-type="sect2"><div class="sect2" id="the_model_idTO9D8O">&#13;
          <h2>The Model</h2>&#13;
          <p><a contenteditable="false" data-primary="application architecture, for LLM" data-secondary="model" data-type="indexterm" id="ch03.html17"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-tertiary="model" data-type="indexterm" id="ch03.html18"/>The language model serves as the intellectual core of any LLM application, taking in data, generating responses, and driving interactions. Depending on the architecture and requirements, you may interact with the language model through a public API hosted by a third-party service or run a privately hosted model. For example, you can download versions of Meta’s powerful Llama model from GitHub or Hugging Face and run it locally.</p>&#13;
          <section data-pdf-bookmark="Public APIs: The convenience and the risks" data-type="sect3"><div class="sect3" id="public_apis_the_convenience_and_the_risks">&#13;
            <h3>Public APIs: The convenience and the risks</h3>&#13;
            <p>Utilizing a public API to access a language model offers convenience and lower up-front costs. Third parties manage and update these models, reducing your organization’s resource burden. However, the trade-off often comes in the form of higher risk of data exposure. When making a request to a third-party model, the data crosses a trust boundary, exiting your secure network and entering an external system. This process exposes you to risks around data confidentiality and, depending on the third party’s security measures, could make you vulnerable to data breaches.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Privately hosted models: More control, different risks" data-type="sect3"><div class="sect3" id="privately_hosted_models_more_control_different_r">&#13;
            <h3>Privately hosted models: More control, different risks</h3>&#13;
            <p>Opting for a privately hosted model gives you more control over your data, allowing you to manage trust boundaries more tightly. It also allows you to customize or fine-tune the model according to your needs. However, running a privately hosted model brings challenges, such as maintenance, updates, and ensuring that the model doesn’t contain vulnerabilities—essentially exposing you to potential supply chain risks. If you use an open source model, it becomes crucial to ensure its provenance and integrity to avoid embedded vulnerabilities or biases.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Risk considerations" data-type="sect3"><div class="sect3" id="risk_considerations">&#13;
            <h3>Risk considerations</h3>&#13;
            <p>Let’s look at some security considerations that depend on your choice of model and where it is deployed:</p>&#13;
            <dl>&#13;
              <dt>Sensitive data exposure</dt>&#13;
              <dd>&#13;
                <p>Public APIs may increase the risk of exposing sensitive information, while privately hosted models offer better control but require robust internal security measures.</p>&#13;
              </dd>&#13;
              <dt>Supply chain risk</dt>&#13;
              <dd>&#13;
                <p>The origins of your model, whether it’s a well-vetted public service or an open source download, are crucial. A compromised model can introduce vulnerabilities into your application, effectively acting as a back door for attacks. We’ll explore this more in <a data-type="xref" href="ch09.html#find_the_weakest_link">Chapter 9</a>.</p>&#13;
              </dd>&#13;
            </dl>&#13;
            <p>By carefully considering the model’s hosting environment, you can better assess the trade-offs and risks associated with sensitive data exposure and supply chain vulnerabilities. These considerations will guide you in establishing appropriate trust boundaries and security protocols tailored to your chosen model’s architecture.<a contenteditable="false" data-primary="" data-startref="ch03.html18" data-type="indexterm" id="id305"/><a contenteditable="false" data-primary="" data-startref="ch03.html17" data-type="indexterm" id="id306"/></p>&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="User Interaction" data-type="sect2"><div class="sect2" id="user_interaction">&#13;
          <h2>User Interaction</h2>&#13;
          <p><a contenteditable="false" data-primary="application architecture, for LLM" data-secondary="user interaction" data-type="indexterm" id="ch03.html19"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-tertiary="user interaction" data-type="indexterm" id="ch03.html20"/>While <em>user</em><em> </em><em>input</em> might suggest a one-way flow of information from the user into the application, the reality is often more nuanced. In the context of LLM applications, <em>user interaction</em> encapsulates both receiving input from the user and providing output back to the user. This bidirectional interaction is fundamental for creating an engaging and practical user experience, but also introduces a more complicated security landscape.</p>&#13;
          <p>Prompts are a vital element of user interaction. They are not merely requests for information but serve as a guide to how the user interacts with the LLM. A well-crafted prompt can direct the model to provide valuable and accurate information, while an ambiguous or poorly constructed one can lead to unclear or even misleading outputs. As a result, the management of prompts becomes a critical aspect of application security. For example, a carefully crafted prompt from a malicious user could trick the model into divulging information it shouldn’t or cause the model to generate harmful content. Returning to <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>, Tay fell victim to this when prompts from her 4chan hackers helped lead her astray.</p>&#13;
          <p>Given the importance of this bidirectional interaction, securing both inputs and outputs is crucial. On the input side, input validation, sanitation, and rate limiting measures are vital in mitigating vulnerabilities like injection attacks. On the output side, ensuring that the model’s responses are appropriately filtered and that your application does not leak sensitive information is equally vital. The nature of LLMs makes this even more challenging than it is with traditional applications, and we’ll discuss more techniques related to this later in the book.</p>&#13;
          <p>This interactive layer with the user creates a critical trust boundary in the application architecture. Any data crossing this boundary, whether going in or out, should be carefully managed to avoid security risks. Additional layers of protection include using encryption for sensitive outputs and employing real-time monitoring to flag potentially harmful or sensitive data flows. <a contenteditable="false" data-primary="" data-startref="ch03.html20" data-type="indexterm" id="id307"/><a contenteditable="false" data-primary="" data-startref="ch03.html19" data-type="indexterm" id="id308"/> We’ll discuss this more thoroughly in <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Training Data" data-type="sect2"><div class="sect2" id="training_data">&#13;
          <h2>Training Data</h2>&#13;
          <p><a contenteditable="false" data-primary="application architecture, for LLM" data-secondary="training data" data-type="indexterm" id="ch03.html21"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-tertiary="training data" data-type="indexterm" id="ch03.html22"/><a contenteditable="false" data-primary="training data" data-secondary="basics" data-type="indexterm" id="ch03.html23"/>Training data is the bedrock upon which LLMs build their understanding and capabilities. Whether used for initial training or subsequent fine-tuning, the nature and source of this data have significant implications for both the model’s performance and security posture. One crucial distinction is whether the data is internally sourced or culled from public or external sources (“in the wild”).</p>&#13;
          <p>Data generated or curated within an organization usually undergoes a more rigorous vetting than publicly sourced data. It is often aligned with the application’s specific requirements or use cases, making it generally more reliable and relevant. The controlled environment also allows for better implementation of security measures like encryption, access controls, and auditing. However, this data may contain sensitive or proprietary information, and the trust boundary here is closely tied to internal security protocols. A breach at this level could have serious ramifications, including data leakage or the corruption of the training set.</p>&#13;
          <p>Data sourced from public repositories or “the wild” introduces different challenges. While this data can offer diversity and scale, its reliability and safety are often not guaranteed. Such data could include misleading information, biases, or malicious inputs to compromise the model. The trust boundary here is more porous and extends to the external entities that generate or host this data: rigorous filtering, validation, and continuous monitoring become essential to mitigate risks and vulnerabilities. As we saw in <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>, Tay was digesting user prompts directly as training data. In this way, remnants of toxic prompts became part of her knowledge base, and then she began to spill poisonous output. Accepting unfiltered, untrusted user input into your training dataset is the simplest example of a failure to manage this critical <span class="keep-together">security</span> boundary.</p>&#13;
          <p><a contenteditable="false" data-primary="trust boundaries" data-secondary="with training data" data-type="indexterm" id="id309"/>For either internally sourced or public data, the concept of trust boundaries is critical. For internally sourced data, the boundary is often within the organization’s controlled environment, making it easier to enforce security measures. On the other hand, using external data effectively extends your trust boundary to include those external <span class="keep-together">sources,</span> which may not adhere to your security standards. Using external data for training necessitates additional layers of validation and security checks to ensure that unvetted data doesn’t compromise the integrity or security of the LLM application.</p>&#13;
          <p>Understanding the origins of your training data, the associated trust boundaries, and their respective security implications is crucial for safeguarding your LLM application. Comprehensive data governance policies must be in place to manage the lifecycle of your training data, regardless of its source.<a contenteditable="false" data-primary="" data-startref="ch03.html23" data-type="indexterm" id="id310"/><a contenteditable="false" data-primary="" data-startref="ch03.html22" data-type="indexterm" id="id311"/><a contenteditable="false" data-primary="" data-startref="ch03.html21" data-type="indexterm" id="id312"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Access to Live External Data Sources" data-type="sect2"><div class="sect2" id="access_to_live_external_data_sources">&#13;
          <h2>Access to Live External Data Sources</h2>&#13;
          <p><a contenteditable="false" data-primary="application architecture, for LLM" data-secondary="live external data sources access" data-type="indexterm" id="ch03.html24"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-tertiary="live external data sources access" data-type="indexterm" id="ch03.html25"/><a contenteditable="false" data-primary="external data sources, access to" data-type="indexterm" id="ch03.html26"/>Live external data sources bring an added dimension to the capabilities of LLM applications by enabling them to provide real-time information, context, or even third-party integrations. While access to live external data enhances the user experience and functional range, it introduces a new layer of complexity to the application’s <span class="keep-together">security</span> landscape.</p>&#13;
          <p>As an example of this, as of the writing of this chapter, OpenAI’s ChatGPT does not have direct access to the live web and is thus limited only to facts in its older training data. On the other hand, Google’s Bard (now called Gemini) does have access to live internet data for this test. Because of this, while the GPT-4 model is doubtlessly superior in reasoning capability, it fails at many basic tasks where Bard succeeds. <a data-type="xref" href="#fig_3_chatgpt_with_gtp_4_fails_to_answer_a_simple_questi">Figure 3-3</a> shows an interaction with ChatGPT. <a data-type="xref" href="#fig_4_bard_s_direct_access_to_internet_feeds_gives_it_an">Figure 3-4</a> shows the same interaction with Bard.</p>&#13;
          <figure><div class="figure" id="fig_3_chatgpt_with_gtp_4_fails_to_answer_a_simple_questi">&#13;
            <img alt="" src="assets/dpls_0303.png"/>&#13;
            <h6><span class="label">Figure 3-3. </span>ChatGPT with GTP-4 fails to answer a simple question due to limited access to external data</h6>&#13;
          </div></figure>&#13;
          <figure><div class="figure" id="fig_4_bard_s_direct_access_to_internet_feeds_gives_it_an">&#13;
            <img alt="" src="assets/dpls_0304.png"/>&#13;
            <h6><span class="label">Figure 3-4. </span>Bard’s direct access to internet feeds gives it an advantage</h6>&#13;
          </div></figure>&#13;
          <p>While accessing outside data sources such as websites, APIs, or third-party databases has advantages, it exposes the application to potential risks. The risks of ingesting untrusted external data sources can range from consuming false or harmful information from compromised websites to becoming a conduit for security threats like malware or unauthorized data access. The untrusted nature of these data sources makes them inherently less controllable than internal resources, thereby adding an additional layer of uncertainty and risk.</p>&#13;
          <p><a contenteditable="false" data-primary="trust boundaries" data-secondary="with public internet data" data-type="indexterm" id="id313"/>The concept of trust boundaries becomes especially pertinent when accessing public internet data. Unlike internal services, where you can uniformly apply security measures, external sources may adhere to security standards different from those of your organization. This differential in trust necessitates additional layers of validation, security checks, and monitoring to ensure that data crossing this boundary doesn’t compromise the system.<a contenteditable="false" data-primary="" data-startref="ch03.html26" data-type="indexterm" id="id314"/><a contenteditable="false" data-primary="" data-startref="ch03.html25" data-type="indexterm" id="id315"/><a contenteditable="false" data-primary="" data-startref="ch03.html24" data-type="indexterm" id="id316"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Access to Internal Services" data-type="sect2"><div class="sect2" id="access_to_internal_services">&#13;
          <h2>Access to Internal Services</h2>&#13;
          <p><a contenteditable="false" data-primary="application architecture, for LLM" data-secondary="internal services access" data-type="indexterm" id="ch03.html27"/><a contenteditable="false" data-primary="architectures and trust boundaries" data-secondary="application architecture" data-tertiary="internal services access" data-type="indexterm" id="ch03.html28"/><a contenteditable="false" data-primary="internal services, access to" data-type="indexterm" id="ch03.html29"/>Internal services like databases and internal APIs often serve as the backend support structure for LLM applications. They may house critical data from user profiles and logs to configuration settings and even vast data in SQL or vector databases. As a component that often interfaces with various other internal and external elements of the system, internal services represent a critical point in the application’s architecture, both functionally and from a security perspective.</p>&#13;
          <p class="pagebreak-before">These services often function within an organization’s controlled environment, enabling uniform application of security policies. However, just because these services are internal, you mustn’t fall victim to a false sense of security. They are still vulnerable to various threats, such as unauthorized access, data leaks, and internal threats from within the organization.</p>&#13;
          <p>Internal services such as databases, proprietary APIs, and backend systems often constitute the operational backbone for LLM applications. These resources typically reside within the organization’s secure network, providing trust and control that is harder to achieve with external services. However, this internal nature can paradoxically elevate the security risks involved, primarily if these services house the organization’s “crown jewels” of sensitive or valuable data<a contenteditable="false" data-primary="" data-startref="ch03.html29" data-type="indexterm" id="id317"/><a contenteditable="false" data-primary="" data-startref="ch03.html28" data-type="indexterm" id="id318"/><a contenteditable="false" data-primary="" data-startref="ch03.html27" data-type="indexterm" id="id319"/>.<a contenteditable="false" data-primary="" data-startref="ch03.html13" data-type="indexterm" id="id320"/><a contenteditable="false" data-primary="" data-startref="ch03.html12" data-type="indexterm" id="id321"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="conclusion">&#13;
        <h1>Conclusion</h1>&#13;
        <p>Securing LLM applications is an endeavor fraught with complexities, intricacies, and challenges that are significantly different from those of traditional web applications. This chapter has aimed to lay down the foundational knowledge required to navigate this complex landscape, focusing on three critical areas: distinguishing between artificial intelligence, neural networks, and large language models; understanding the pivotal role of transformer architectures; and diving deep into LLM application architecture, particularly the concept of trust boundaries. Knowing what sets LLMs apart helps us tailor our security strategies more effectively, going beyond general AI or machine learning frameworks.<a contenteditable="false" data-primary="" data-startref="ch03.html0" data-type="indexterm" id="id322"/></p>&#13;
      </div></section>&#13;
    </div></section></body></html>