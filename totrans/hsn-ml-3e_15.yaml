- en: Chapter 13\. Loading and Preprocessing Data with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#project_chapter), you saw that loading and preprocessing
    data is an important part of any machine learning project. You used Pandas to
    load and explore the (modified) California housing dataset—which was stored in
    a CSV file—and you applied Scikit-Learn’s transformers for preprocessing. These
    tools are quite convenient, and you will probably be using them often, especially
    when exploring and experimenting with data.
  prefs: []
  type: TYPE_NORMAL
- en: However, when training TensorFlow models on large datasets, you may prefer to
    use TensorFlow’s own data loading and preprocessing API, called *tf.data*. It
    is capable of loading and preprocessing data extremely efficiently, reading from
    multiple files in parallel using multithreading and queuing, shuffling and batching
    samples, and more. Plus, it can do all of this on the fly—it loads and preprocesses
    the next batch of data across multiple CPU cores, while your GPUs or TPUs are
    busy training the current batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: The tf.data API lets you handle datasets that don’t fit in memory, and it allows
    you to make full use of your hardware resources, thereby speeding up training.
    Off the shelf, the tf.data API can read from text files (such as CSV files), binary
    files with fixed-size records, and binary files that use TensorFlow’s TFRecord
    format, which supports records of varying sizes.
  prefs: []
  type: TYPE_NORMAL
- en: TFRecord is a flexible and efficient binary format usually containing protocol
    buffers (an open source binary format). The tf.data API also has support for reading
    from SQL databases. Moreover, many open source extensions are available to read
    from all sorts of data sources, such as Google’s BigQuery service (see [*https://tensorflow.org/io*](https://tensorflow.org/io)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras also comes with powerful yet easy-to-use preprocessing layers that can
    be embedded in your models: this way, when you deploy a model to production, it
    will be able to ingest raw data directly, without you having to add any additional
    preprocessing code. This eliminates the risk of mismatch between the preprocessing
    code used during training and the preprocessing code used in production, which
    would likely cause *training/serving skew*. And if you deploy your model in multiple
    apps coded in different programming languages, you won’t have to reimplement the
    same preprocessing code multiple times, which also reduces the risk of mismatch.'
  prefs: []
  type: TYPE_NORMAL
- en: As you will see, both APIs can be used jointly—for example, to benefit from
    the efficient data loading offered by tf.data and the convenience of the Keras
    preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first cover the tf.data API and the TFRecord format.
    Then we will explore the Keras preprocessing layers and how to use them with the
    tf.data API. Lastly, we will take a quick look at a few related libraries that
    you may find useful for loading and preprocessing data, such as TensorFlow Datasets
    and TensorFlow Hub. So, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: The tf.data API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole tf.data API revolves around the concept of a `tf.data.Dataset`: this
    represents a sequence of data items. Usually you will use datasets that gradually
    read data from disk, but for simplicity let’s create a dataset from a simple data
    tensor using `tf.data.Dataset.from_tensor_slices()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset`
    whose elements are all the slices of `X` along the first dimension, so this dataset
    contains 10 items: tensors 0, 1, 2, …​, 9\. In this case we would have obtained
    the same dataset if we had used `tf.data.Dataset.range(10)` (except the elements
    would be 64-bit integers instead of 32-bit integers).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can simply iterate over a dataset’s items like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The tf.data API is a streaming API: you can very efficiently iterate through
    a dataset’s items, but the API is not designed for indexing or slicing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset may also contain tuples of tensors, or dictionaries of name/tensor
    pairs, or even nested tuples and dictionaries of tensors. When slicing a tuple,
    a dictionary, or a nested structure, the dataset will only slice the tensors it
    contains, while preserving the tuple/dictionary structure. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Chaining Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have a dataset, you can apply all sorts of transformations to it by
    calling its transformation methods. Each method returns a new dataset, so you
    can chain transformations like this (this chain is illustrated in [Figure 13-1](#chaining_transformations_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first call the `repeat()` method on the original dataset,
    and it returns a new dataset that repeats the items of the original dataset three
    times. Of course, this will not copy all the data in memory three times! If you
    call this method with no arguments, the new dataset will repeat the source dataset
    forever, so the code that iterates over the dataset will have to decide when to
    stop.
  prefs: []
  type: TYPE_NORMAL
- en: Then we call the `batch()` method on this new dataset, and again this creates
    a new dataset. This one will group the items of the previous dataset in batches
    of seven items.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1301](assets/mls3_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. Chaining dataset transformations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, we iterate over the items of this final dataset. The `batch()` method
    had to output a final batch of size two instead of seven, but you can call `batch()`
    with `drop_remainder=True` if you want it to drop this final batch, such that
    all batches have the exact same size.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The dataset methods do *not* modify datasets—they create new ones. So make sure
    to keep a reference to these new datasets (e.g., with `dataset = ...`), or else
    nothing will happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also transform the items by calling the `map()` method. For example,
    this creates a new dataset with all batches multiplied by two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This `map()` method is the one you will call to apply any preprocessing to your
    data. Sometimes this will include computations that can be quite intensive, such
    as reshaping or rotating an image, so you will usually want to spawn multiple
    threads to speed things up. This can be done by setting the `num_parallel_calls`
    argument to the number of threads to run, or to `tf.data.AUTOTUNE`. Note that
    the function you pass to the `map()` method must be convertible to a TF function
    (see [Chapter 12](ch12.html#tensorflow_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to simply filter the dataset using the `filter()` method.
    For example, this code creates a dataset that only contains the batchs whose sum
    is greater than 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will often want to look at just a few items from a dataset. You can use
    the `take()` method for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Shuffling the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 4](ch04.html#linear_models_chapter), gradient descent
    works best when the instances in the training set are independent and identically
    distributed (IID). A simple way to ensure this is to shuffle the instances, using
    the `shuffle()` method. It will create a new dataset that will start by filling
    up a buffer with the first items of the source dataset. Then, whenever it is asked
    for an item, it will pull one out randomly from the buffer and replace it with
    a fresh one from the source dataset, until it has iterated entirely through the
    source dataset. At this point it will continue to pull out items randomly from
    the buffer until it is empty. You must specify the buffer size, and it is important
    to make it large enough, or else shuffling will not be very effective.⁠^([1](ch13.html#idm45720190533488))
    Just don’t exceed the amount of RAM you have, though even if you have plenty of
    it, there’s no need to go beyond the dataset’s size. You can provide a random
    seed if you want the same random order every time you run your program. For example,
    the following code creates and displays a dataset containing the integers 0 to
    9, repeated twice, shuffled using a buffer of size 4 and a random seed of 42,
    and batched with a batch size of 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you call `repeat()` on a shuffled dataset, by default it will generate a
    new order at every iteration. This is generally a good idea, but if you prefer
    to reuse the same order at each iteration (e.g., for tests or debugging), you
    can set `reshuffle_each_​itera⁠tion=False` when calling `shuffle()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a large dataset that does not fit in memory, this simple shuffling-buffer
    approach may not be sufficient, since the buffer will be small compared to the
    dataset. One solution is to shuffle the source data itself (for example, on Linux
    you can shuffle text files using the `shuf` command). This will definitely improve
    shuffling a lot! Even if the source data is shuffled, you will usually want to
    shuffle it some more, or else the same order will be repeated at each epoch, and
    the model may end up being biased (e.g., due to some spurious patterns present
    by chance in the source data’s order). To shuffle the instances some more, a common
    approach is to split the source data into multiple files, then read them in a
    random order during training. However, instances located in the same file will
    still end up close to each other. To avoid this you can pick multiple files randomly
    and read them simultaneously, interleaving their records. Then on top of that
    you can add a shuffling buffer using the `shuffle()` method. If this sounds like
    a lot of work, don’t worry: the tf.data API makes all this possible in just a
    few lines of code. Let’s go over how you can do this.'
  prefs: []
  type: TYPE_NORMAL
- en: Interleaving Lines from Multiple Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, suppose you’ve loaded the California housing dataset, shuffled it (unless
    it was already shuffled), and split it into a training set, a validation set,
    and a test set. Then you split each set into many CSV files that each look like
    this (each row contains eight input features plus the target median house value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also suppose `train_filepaths` contains the list of training filepaths
    (and you also have `valid_filepaths` and `test_filepaths`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you could use file patterns; for example, `train_filepaths =`
    `"datasets/housing/my_train_*.csv"`. Now let’s create a dataset containing only
    these filepaths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `list_files()` function returns a dataset that shuffles the
    filepaths. In general this is a good thing, but you can set `shuffle=False` if
    you do not want that for some reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can call the `interleave()` method to read from five files at a time
    and interleave their lines. You can also skip the first line of each file—which
    is the header row—using the `skip()` method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `interleave()` method will create a dataset that will pull five filepaths
    from the `filepath_dataset`, and for each one it will call the function you gave
    it (a lambda in this example) to create a new dataset (in this case a `TextLineDataset`).
    To be clear, at this stage there will be seven datasets in all: the filepath dataset,
    the interleave dataset, and the five `TextLineDataset`s created internally by
    the interleave dataset. When you iterate over the interleave dataset, it will
    cycle through these five `TextLineDataset`s, reading one line at a time from each
    until all datasets are out of items. Then it will fetch the next five filepaths
    from the `filepath_dataset` and interleave them the same way, and so on until
    it runs out of filepaths. For interleaving to work best, it is preferable to have
    files of identical length; otherwise the end of the longest file will not be interleaved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `interleave()` does not use parallelism; it just reads one line
    at a time from each file, sequentially. If you want it to actually read files
    in parallel, you can set the `interleave()` method’s `num_parallel_calls` argument
    to the number of threads you want (recall that the `map()` method also has this
    argument). You can even set it to `tf.data.AUTOTUNE` to make TensorFlow choose
    the right number of threads dynamically based on the available CPU. Let’s look
    at what the dataset contains now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These are the first rows (ignoring the header row) of five CSV files, chosen
    randomly. Looks good!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It’s possible to pass a list of filepaths to the `TextLineDataset` constructor:
    it will go through each file in order, line by line. If you also set the `num_parallel_reads`
    argument to a number greater than one, then the dataset will read that number
    of files in parallel and interleave their lines (without having to call the `interleave()`
    method). However, it will *not* shuffle the files, nor will it skip the header
    lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a housing dataset that returns each instance as a tensor containing
    a byte string, we need to do a bit of preprocessing, including parsing the strings
    and scaling the data. Let’s implement a couple custom functions that will perform
    this preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s walk through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the code assumes that we have precomputed the mean and standard deviation
    of each feature in the training set. `X_mean` and `X_std` are just 1D tensors
    (or NumPy arrays) containing eight floats, one per input feature. This can be
    done using a Scikit-Learn `StandardScaler` on a large enough random sample of
    the dataset. Later in this chapter, we will use a Keras preprocessing layer instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `parse_csv_line()` function takes one CSV line and parses it. To help with
    that, it uses the `tf.io.decode_csv()` function, which takes two arguments: the
    first is the line to parse, and the second is an array containing the default
    value for each column in the CSV file. This array (`defs`) tells TensorFlow not
    only the default value for each column, but also the number of columns and their
    types. In this example, we tell it that all feature columns are floats and that
    missing values should default to zero, but we provide an empty array of type `tf.float32`
    as the default value for the last column (the target): the array tells TensorFlow
    that this column contains floats, but that there is no default value, so it will
    raise an exception if it encounters a missing value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `tf.io.decode_csv()` function returns a list of scalar tensors (one per
    column), but we need to return a 1D tensor array. So we call `tf.stack()` on all
    tensors except for the last one (the target): this will stack these tensors into
    a 1D array. We then do the same for the target value: this makes it a 1D tensor
    array with a single value, rather than a scalar tensor. The `tf.io.decode_csv()`
    function is done, so it returns the input features and the target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the custom `preprocess()` function just calls the `parse_csv_line()`
    function, scales the input features by subtracting the feature means and then
    dividing by the feature standard deviations, and returns a tuple containing the
    scaled features and the target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s test this preprocessing function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Looks good! The `preprocess()` function can convert an instance from a byte
    string to a nice scaled tensor, with its corresponding label. We can now use the
    dataset’s `map()` method to apply the `preprocess()` function to each sample in
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Putting Everything Together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make the code more reusable, let’s put together everything we have discussed
    so far into another helper function; it will create and return a dataset that
    will efficiently load California housing data from multiple CSV files, preprocess
    it, shuffle it, and batch it (see [Figure 13-2](#input_pipeline_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use the `prefetch()` method on the very last line. This is important
    for performance, as you will see now.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1302](assets/mls3_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. Loading and preprocessing data from multiple CSV files
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prefetching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By calling `prefetch(1)` at the end of the custom `csv_reader_dataset()` function,
    we are creating a dataset that will do its best to always be one batch ahead.⁠^([2](ch13.html#idm45720189926208))
    In other words, while our training algorithm is working on one batch, the dataset
    will already be working in parallel on getting the next batch ready (e.g., reading
    the data from disk and preprocessing it). This can improve performance dramatically,
    as is illustrated in [Figure 13-3](#prefetching_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we also ensure that loading and preprocessing are multithreaded (by setting
    `num_parallel_calls` when calling `interleave()` and `map()`), we can exploit
    multiple CPU cores and hopefully make preparing one batch of data shorter than
    running a training step on the GPU: this way the GPU will be almost 100% utilized
    (except for the data transfer time from the CPU to the GPU⁠^([3](ch13.html#idm45720189876688))),
    and training will run much faster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1303](assets/mls3_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-3\. With prefetching, the CPU and the GPU work in parallel: as the
    GPU works on one batch, the CPU works on the next'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you plan to purchase a GPU card, its processing power and its memory size
    are of course very important (in particular, a large amount of RAM is crucial
    for large computer vision or natural language processing models). Just as important
    for good performance is the GPU’s *memory bandwidth*; this is the number of gigabytes
    of data it can get into or out of its RAM per second.
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is small enough to fit in memory, you can significantly speed
    up training by using the dataset’s `cache()` method to cache its content to RAM.
    You should generally do this after loading and preprocessing the data, but before
    shuffling, repeating, batching, and prefetching. This way, each instance will
    only be read and preprocessed once (instead of once per epoch), but the data will
    still be shuffled differently at each epoch, and the next batch will still be
    prepared in advance.
  prefs: []
  type: TYPE_NORMAL
- en: You have now learned how to build efficient input pipelines to load and preprocess
    data from multiple text files. We have discussed the most common dataset methods,
    but there are a few more you may want to look at, such as `concatenate()`, `zip()`,
    `window()`, `reduce()`, `shard()`, `flat_map()`, `apply()`, `unbatch()`, and `padded_batch()`.
    There are also a few more class methods, such as `from_generator()` and `from_​ten⁠sors()`,
    which create a new dataset from a Python generator or a list of tensors, respectively.
    Please check the API documentation for more details. Also note that there are
    experimental features available in `tf.data.experimental`, many of which will
    likely make it to the core API in future releases (e.g., check out the `CsvDataset`
    class, as well as the `make_csv_dataset()` method, which takes care of inferring
    the type of each column).
  prefs: []
  type: TYPE_NORMAL
- en: Using the Dataset with Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can use the custom `csv_reader_dataset()` function we wrote earlier
    to create a dataset for the training set, and for the validation set and the test
    set. The training set will be shuffled at each epoch (note that the validation
    set and the test set will also be shuffled, even though we don’t really need that):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can simply build and train a Keras model using these datasets. When
    you call the model’s `fit()` method, you pass `train_set` instead of `X_train,
    y_train`, and pass `validation_data=valid_set` instead of `validation_data=(X_valid,
    y_valid)`. The `fit()` method will take care of repeating the training dataset
    once per epoch, using a different random order at each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can pass a dataset to the `evaluate()` and `predict()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the other sets, the `new_set` will usually not contain labels. If it
    does, as is the case here, Keras will ignore them. Note that in all these cases,
    you can still use NumPy arrays instead of datasets if you prefer (but of course
    they need to have been loaded and preprocessed first).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to build your own custom training loop (as discussed in [Chapter 12](ch12.html#tensorflow_chapter)),
    you can just iterate over the training set, very naturally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, it is even possible to create a TF function (see [Chapter 12](ch12.html#tensorflow_chapter))
    that trains the model for a whole epoch. This can really speed up training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In Keras, the `steps_per_execution` argument of the `compile()` method lets
    you define the number of batches that the `fit()` method will process during each
    call to the `tf.function` it uses for training. The default is just 1, so if you
    set it to 50 you will often see a significant performance improvement. However,
    the `on_batch_*()` methods of Keras callbacks will only be called every 50 batches.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you now know how to build powerful input pipelines using the
    tf.data API! However, so far we’ve been using CSV files, which are common, simple,
    and convenient but not really efficient, and do not support large or complex data
    structures (such as images or audio) very well. So, let’s see how to use TFRecords
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are happy with CSV files (or whatever other format you are using), you
    do not *have* to use TFRecords. As the saying goes, if it ain’t broke, don’t fix
    it! TFRecords are useful when the bottleneck during training is loading and parsing
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: The TFRecord Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TFRecord format is TensorFlow’s preferred format for storing large amounts
    of data and reading it efficiently. It is a very simple binary format that just
    contains a sequence of binary records of varying sizes (each record is comprised
    of a length, a CRC checksum to check that the length was not corrupted, then the
    actual data, and finally a CRC checksum for the data). You can easily create a
    TFRecord file using the `tf.io.TFRecordWriter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can then use a `tf.data.TFRecordDataset` to read one or more TFRecord
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, a `TFRecordDataset` will read files one by one, but you can make
    it read multiple files in parallel and interleave their records by passing the
    constructor a list of filepaths and setting `num_parallel_reads` to a number greater
    than one. Alternatively, you could obtain the same result by using `list_files()`
    and `interleave()` as we did earlier to read multiple CSV files.
  prefs: []
  type: TYPE_NORMAL
- en: Compressed TFRecord Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It can sometimes be useful to compress your TFRecord files, especially if they
    need to be loaded via a network connection. You can create a compressed TFRecord
    file by setting the `options` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When reading a compressed TFRecord file, you need to specify the compression
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A Brief Introduction to Protocol Buffers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even though each record can use any binary format you want, TFRecord files
    usually contain serialized protocol buffers (also called *protobufs*). This is
    a portable, extensible, and efficient binary format developed at Google back in
    2001 and made open source in 2008; protobufs are now widely used, in particular
    in [gRPC](https://grpc.io), Google’s remote procedure call system. They are defined
    using a simple language that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This protobuf definition says we are using version 3 of the protobuf format,
    and it specifies that each `Person` object⁠^([4](ch13.html#idm45720189164144))
    may (optionally) have a `name` of type string, an `id` of type int32, and zero
    or more `email` fields, each of type string. The numbers `1`, `2`, and `3` are
    the field identifiers: they will be used in each record’s binary representation.
    Once you have a definition in a *.proto* file, you can compile it. This requires
    `protoc`, the protobuf compiler, to generate access classes in Python (or some
    other language). Note that the protobuf definitions you will generally use in
    TensorFlow have already been compiled for you, and their Python classes are part
    of the TensorFlow library, so you will not need to use `protoc`. All you need
    to know is how to *use* protobuf access classes in Python. To illustrate the basics,
    let’s look at a simple example that uses the access classes generated for the
    `Person` protobuf (the code is explained in the comments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In short, we import the `Person` class generated by `protoc`, we create an instance
    and play with it, visualizing it and reading and writing some fields, then we
    serialize it using the `SerializeToString()` method. This is the binary data that
    is ready to be saved or transmitted over the network. When reading or receiving
    this binary data, we can parse it using the `ParseFromString()` method, and we
    get a copy of the object that was serialized.⁠^([5](ch13.html#idm45720189071424))
  prefs: []
  type: TYPE_NORMAL
- en: 'You could save the serialized `Person` object to a TFRecord file, then load
    and parse it: everything would work fine. However, `ParseFromString()` is not
    a TensorFlow operation, so you couldn’t use it in a preprocessing function in
    a tf.data pipeline (except by wrapping it in a `tf.py_function()` operation, which
    would make the code slower and less portable, as you saw in [Chapter 12](ch12.html#tensorflow_chapter)).
    However, you could use the `tf.io.decode_proto()` function, which can parse any
    protobuf you want, provided you give it the protobuf definition (see the notebook
    for an example). That said, in practice you will generally want to use instead
    the predefined protobufs for which TensorFlow provides dedicated parsing operations.
    Let’s look at these predefined protobufs now.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Protobufs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main protobuf typically used in a TFRecord file is the `Example` protobuf,
    which represents one instance in a dataset. It contains a list of named features,
    where each feature can either be a list of byte strings, a list of floats, or
    a list of integers. Here is the protobuf definition (from TensorFlow’s source
    code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The definitions of `BytesList`, `FloatList`, and `Int64List` are straightforward
    enough. Note that `[packed = true]` is used for repeated numerical fields, for
    a more efficient encoding. A `Feature` contains either a `BytesList`, a `FloatList`,
    or an `Int64List`. A `Features` (with an `s`) contains a dictionary that maps
    a feature name to the corresponding feature value. And finally, an `Example` contains
    only a `Features` object.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why was `Example` even defined, since it contains no more than a `Features`
    object? Well, TensorFlow’s developers may one day decide to add more fields to
    it. As long as the new `Example` definition still contains the `features` field,
    with the same ID, it will be backward compatible. This extensibility is one of
    the great features of protobufs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how you could create a `tf.train.Example` representing the same person
    as earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is a bit verbose and repetitive, but you could easily wrap it inside
    a small helper function. Now that we have an `Example` protobuf, we can serialize
    it by calling its `SerializeToString()` method, then write the resulting data
    to a TFRecord file. Let’s write it five times to pretend we have several contacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Normally you would write much more than five `Example`s! Typically, you would
    create a conversion script that reads from your current format (say, CSV files),
    creates an `Example` protobuf for each instance, serializes them, and saves them
    to several TFRecord files, ideally shuffling them in the process. This requires
    a bit of work, so once again make sure it is really necessary (perhaps your pipeline
    works fine with CSV files).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a nice TFRecord file containing several serialized `Example`s,
    let’s try to load it.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and Parsing Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load the serialized `Example` protobufs, we will use a `tf.data.TFRecordDataset`
    once again, and we will parse each `Example` using `tf.io.parse_single_example()`.
    It requires at least two arguments: a string scalar tensor containing the serialized
    data, and a description of each feature. The description is a dictionary that
    maps each feature name to either a `tf.io.FixedLenFeature` descriptor indicating
    the feature’s shape, type, and default value, or a `tf.io.VarLenFeature` descriptor
    indicating only the type if the length of the feature’s list may vary (such as
    for the `"emails"` feature).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines a description dictionary, then creates a `TFRecordDataset`
    and applies a custom preprocessing function to it to parse each serialized `Example`
    protobuf that this dataset contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The fixed-length features are parsed as regular tensors, but the variable-length
    features are parsed as sparse tensors. You can convert a sparse tensor to a dense
    tensor using `tf.sparse.to_dense()`, but in this case it is simpler to just access
    its values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of parsing examples one by one using `tf.io.parse_single_example()`,
    you may want to parse them batch by batch using `tf.io.parse_example()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, a `BytesList` can contain any binary data you want, including any serialized
    object. For example, you can use `tf.io.encode_jpeg()` to encode an image using
    the JPEG format and put this binary data in a `BytesList`. Later, when your code
    reads the TFRecord, it will start by parsing the `Example`, then it will need
    to call `tf.io.decode_jpeg()` to parse the data and get the original image (or
    you can use `tf.io.decode_image()`, which can decode any BMP, GIF, JPEG, or PNG
    image). You can also store any tensor you want in a `BytesList` by serializing
    the tensor using `tf.io.serialize_tensor()` then putting the resulting byte string
    in a `BytesList` feature. Later, when you parse the TFRecord, you can parse this
    data using `tf.io.parse_tensor()`. See this chapter’s notebook at [*https://homl.info/colab3*](https://homl.info/colab3)
    for examples of storing images and tensors in a TFRecord file.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the `Example` protobuf is quite flexible, so it will probably
    be sufficient for most use cases. However, it may be a bit cumbersome to use when
    you are dealing with lists of lists. For example, suppose you want to classify
    text documents. Each document may be represented as a list of sentences, where
    each sentence is represented as a list of words. And perhaps each document also
    has a list of comments, where each comment is represented as a list of words.
    There may be some contextual data too, such as the document’s author, title, and
    publication date. TensorFlow’s `SequenceExample` protobuf is designed for such
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Lists of Lists Using the SequenceExample Protobuf
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the definition of the `SequenceExample` protobuf:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A `SequenceExample` contains a `Features` object for the contextual data and
    a `FeatureLists` object that contains one or more named `FeatureList` objects
    (e.g., a `FeatureList` named `"content"` and another named `"comments"`). Each
    `FeatureList` contains a list of `Feature` objects, each of which may be a list
    of byte strings, a list of 64-bit integers, or a list of floats (in this example,
    each `Feature` would represent a sentence or a comment, perhaps in the form of
    a list of word identifiers). Building a `SequenceExample`, serializing it, and
    parsing it is similar to building, serializing, and parsing an `Example`, but
    you must use `tf.io.parse_single_sequence_example()` to parse a single `SequenceExample`
    or `tf.io.parse_sequence_example()` to parse a batch. Both functions return a
    tuple containing the context features (as a dictionary) and the feature lists
    (also as a dictionary). If the feature lists contain sequences of varying sizes
    (as in the preceding example), you may want to convert them to ragged tensors
    using `tf.RaggedTensor.from_sparse()` (see the notebook for the full code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now that you know how to efficiently store, load, parse, and preprocess the
    data using the tf.data API, TFRecords, and protobufs, it’s time to turn our attention
    to the Keras preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: Keras Preprocessing Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparing your data for a neural network typically requires normalizing the
    numerical features, encoding the categorical features and text, cropping and resizing
    images, and more. There are several options for this:'
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing can be done ahead of time when preparing your training data
    files, using any tools you like, such as NumPy, Pandas, or Scikit-Learn. You will
    need to apply the exact same preprocessing steps in production, to ensure your
    production model receives preprocessed inputs similar to the ones it was trained
    on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you can preprocess your data on the fly while loading it with
    tf.data, by applying a preprocessing function to every element of a dataset using
    that dataset’s `map()` method, as we did earlier in this chapter. Again, you will
    need to apply the same preprocessing steps in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One last approach is to include preprocessing layers directly inside your model
    so it can preprocess all the input data on the fly during training, then use the
    same preprocessing layers in production. The rest of this chapter will look at
    this last approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras offers many preprocessing layers that you can include in your models:
    they can be applied to numerical features, categorical features, images, and text.
    We’ll go over the numerical and categorical features in the next sections, as
    well as basic text preprocessing, and we will cover image preprocessing in [Chapter 14](ch14.html#cnn_chapter)
    and more advanced text preprocessing in [Chapter 16](ch16.html#nlp_chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: The Normalization Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 10](ch10.html#ann_chapter), Keras provides a `Normalization`
    layer that we can use to standardize the input features. We can either specify
    the mean and variance of each feature when creating the layer or—more simply—pass
    the training set to the layer’s `adapt()` method before fitting the model, so
    the layer can measure the feature means and variances on its own before training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The data sample passed to the `adapt()` method must be large enough to be representative
    of your dataset, but it does not have to be the full training set: for the `Normalization`
    layer, a few hundred instances randomly sampled from the training set will generally
    be sufficient to get a good estimate of the feature means and variances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we included the `Normalization` layer inside the model, we can now deploy
    this model to production without having to worry about normalization again: the
    model will just handle it (see [Figure 13-4](#preprocessing_in_model_diagram)).
    Fantastic! This approach completely eliminates the risk of preprocessing mismatch,
    which happens when people try to maintain different preprocessing code for training
    and production but update one and forget to update the other. The production model
    then ends up receiving data preprocessed in a way it doesn’t expect. If they’re
    lucky, they get a clear bug. If not, the model’s accuracy just silently degrades.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1304](assets/mls3_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. Including preprocessing layers inside a model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Including the preprocessing layer directly in the model is nice and straightforward,
    but it will slow down training (only very slightly in the case of the `Normalization`
    layer): indeed, since preprocessing is performed on the fly during training, it
    happens once per epoch. We can do better by normalizing the whole training set
    just once before training. To do this, we can use the `Normalization` layer in
    a standalone fashion (much like a Scikit-Learn `StandardScaler`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can train a model on the scaled data, this time without a `Normalization`
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Good! This should speed up training a bit. But now the model won’t preprocess
    its inputs when we deploy it to production. To fix this, we just need to create
    a new model that wraps both the adapted `Normalization` layer and the model we
    just trained. We can then deploy this final model to production, and it will take
    care of both preprocessing its inputs and making predictions (see [Figure 13-5](#optimized_preprocessing_in_model_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 1305](assets/mls3_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Preprocessing the data just once before training using preprocessing
    layers, then deploying these layers inside the final model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we have the best of both worlds: training is fast because we only preprocess
    the data once before training begins, and the final model can preprocess its inputs
    on the fly without any risk of preprocessing mismatch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the Keras preprocessing layers play nicely with the tf.data API.
    For example, it’s possible to pass a `tf.data.Dataset` to a preprocessing layer’s
    `adapt()` method. It’s also possible to apply a Keras preprocessing layer to a
    `tf.data.Dataset` using the dataset’s `map()` method. For example, here’s how
    you could apply an adapted `Normalization` layer to the input features of each
    batch in a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, if you ever need more features than the Keras preprocessing layers
    provide, you can always write your own Keras layer, just like we discussed in
    [Chapter 12](ch12.html#tensorflow_chapter). For example, if the `Normalization`
    layer didn’t exist, you could get a similar result using the following custom
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s look at another Keras preprocessing layer for numerical features:
    the `Discretization` layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The Discretization Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Discretization` layer’s goal is to transform a numerical feature into
    a categorical feature by mapping value ranges (called bins) to categories. This
    is sometimes useful for features with multimodal distributions, or with features
    that have a highly non-linear relationship with the target. For example, the following
    code maps a numerical `age` feature to three categories, less than 18, 18 to 50
    (not included), and 50 or over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we provided the desired bin boundaries. If you prefer, you
    can instead provide the number of bins you want, then call the layer’s `adapt()`
    method to let it find the appropriate bin boundaries based on the value percentiles.
    For example, if we set `num_bins=3`, then the bin boundaries will be located at
    the values just below the 33rd and 66th percentiles (in this example, at the values
    10 and 37):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Category identifiers such as these should generally not be passed directly to
    a neural network, as their values cannot be meaningfully compared. Instead, they
    should be encoded, for example using one-hot encoding. Let’s look at how to do
    this now.
  prefs: []
  type: TYPE_NORMAL
- en: The CategoryEncoding Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When there are only a few categories (e.g., less than a dozen or two), then
    one-hot encoding is often a good option (as discussed in [Chapter 2](ch02.html#project_chapter)).
    To do this, Keras provides the `CategoryEncoding` layer. For example, let’s one-hot
    encode the `age_​cate⁠gories` feature we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'If you try to encode more than one categorical feature at a time (which only
    makes sense if they all use the same categories), the `CategoryEncoding` class
    will perform *multi-hot encoding* by default: the output tensor will contain a
    1 for each category present in *any* input feature. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: If you believe it’s useful to know how many times each category occurred, you
    can set `output_mode="count"` when creating the `CategoryEncoding` layer, in which
    case the output tensor will contain the number of occurrences of each category.
    In the preceding example, the output would be the same except for the second row,
    which would become `[0., 0., 2.]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that both multi-hot encoding and count encoding lose information, since
    it’s not possible to know which feature each active category came from. For example,
    both `[0, 1]` and `[1, 0]` are encoded as `[1., 1., 0.]`. If you want to avoid
    this, then you need to one-hot encode each feature separately and concatenate
    the outputs. This way, `[0, 1]` would get encoded as `[1., 0., 0., 0., 1., 0.]`
    and `[1, 0]` would get encoded as `[0., 1., 0., 1., 0., 0.]`. You can get the
    same result by tweaking the category identifiers so they don’t overlap. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In this output, the first three columns correspond to the first feature, and
    the last three correspond to the second feature. This allows the model to distinguish
    the two features. However, it also increases the number of features fed to the
    model, and thereby requires more model parameters. It’s hard to know in advance
    whether a single multi-hot encoding or a per-feature one-hot encoding will work
    best: it depends on the task, and you may need to test both options.'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can encode categorical integer features using one-hot or multi-hot encoding.
    But what about categorical text features? For this, you can use the `StringLookup`
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: The StringLookup Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the Keras `StringLookup` layer to one-hot encode a `cities` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We first create a `StringLookup` layer, then we adapt it to the data: it finds
    that there are three distinct categories. Then we use the layer to encode a few
    cities. They are encoded as integers by default. Unknown categories get mapped
    to 0, as is the case for “Montreal” in this example. The known categories are
    numbered starting at 1, from the most frequent category to the least frequent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conveniently, if you set `output_mode="one_hot"` when creating the `StringLookup`
    layer, it will output a one-hot vector for each category, instead of an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras also includes an `IntegerLookup` layer that acts much like the `StringLookup`
    layer but takes integers as input, rather than strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the training set is very large, it may be convenient to adapt the layer
    to just a random subset of the training set. In this case, the layer’s `adapt()`
    method may miss some of the rarer categories. By default, it would then map them
    all to category 0, making them indistinguishable by the model. To reduce this
    risk (while still adapting the layer only on a subset of the training set), you
    can set `num_oov_indices` to an integer greater than 1\. This is the number of
    out-of-vocabulary (OOV) buckets to use: each unknown category will get mapped
    pseudorandomly to one of the OOV buckets, using a hash function modulo the number
    of OOV buckets. This will allow the model to distinguish at least some of the
    rare categories. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Since there are five OOV buckets, the first known category’s ID is now 5 (`"Paris"`).
    But `"Foo"`, `"Bar"`, and `"Baz"` are unknown, so they each get mapped to one
    of the OOV buckets. `"Bar"` gets its own dedicated bucket (with ID 3), but sadly
    `"Foo"` and `"Baz"` happen to be mapped to the same bucket (with ID 4), so they
    remain indistinguishable by the model. This is called a *hashing collision*. The
    only way to reduce the risk of collision is to increase the number of OOV buckets.
    However, this will also increase the total number of categories, which will require
    more RAM and extra model parameters once the categories are one-hot encoded. So,
    don’t increase that number too much.
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea of mapping categories pseudorandomly to buckets is called the *hashing
    trick*. Keras provides a dedicated layer which does just that: the `Hashing` layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The Hashing Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each category, the Keras `Hashing` layer computes a hash, modulo the number
    of buckets (or “bins”). The mapping is entirely pseudorandom, but stable across
    runs and platforms (i.e., the same category will always be mapped to the same
    integer, as long as the number of bins is unchanged). For example, let’s use the
    `Hashing` layer to encode a few cities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The benefit of this layer is that it does not need to be adapted at all, which
    may sometimes be useful, especially in an out-of-core setting (when the dataset
    is too large to fit in memory). However, we once again get a hashing collision:
    “Tokyo” and “Montreal” are mapped to the same ID, making them indistinguishable
    by the model. So, it’s usually preferable to stick to the `StringLookup` layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now look at another way to encode categories: trainable embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding Categorical Features Using Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An embedding is a dense representation of some higher-dimensional data, such
    as a category, or a word in a vocabulary. If there are 50,000 possible categories,
    then one-hot encoding would produce a 50,000-dimensional sparse vector (i.e.,
    containing mostly zeros). In contrast, an embedding would be a comparatively small
    dense vector; for example, with just 100 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, embeddings are usually initialized randomly, and they are
    then trained by gradient descent, along with the other model parameters. For example,
    the `"NEAR BAY"` category in the California housing dataset could be represented
    initially by a random vector such as `[0.131, 0.890]`, while the `"NEAR OCEAN"`
    category might be represented by another random vector such as `[0.631, 0.791]`.
    In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter
    you can tweak.
  prefs: []
  type: TYPE_NORMAL
- en: Since these embeddings are trainable, they will gradually improve during training;
    and as they represent fairly similar categories in this case, gradient descent
    will certainly end up pushing them closer together, while it will tend to move
    them away from the `"INLAND"` category’s embedding (see [Figure 13-6](#embedding_diagram)).
    Indeed, the better the representation, the easier it will be for the neural network
    to make accurate predictions, so training tends to make embeddings useful representations
    of the categories. This is called *representation learning* (you will see other
    types of representation learning in [Chapter 17](ch17.html#autoencoders_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1306](assets/mls3_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Embeddings will gradually improve during training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Keras provides an `Embedding` layer, which wraps an *embedding matrix*: this
    matrix has one row per category and one column per embedding dimension. By default,
    it is initialized randomly. To convert a category ID to an embedding, the `Embedding`
    layer just looks up and returns the row that corresponds to that category. That’s
    all there is to it! For example, let’s initialize an `Embedding` layer with five
    rows and 2D embeddings, and use it to encode some categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, category 2 gets encoded (twice) as the 2D vector `[-0.04663396,
    0.01846724]`, while category 4 gets encoded as `[-0.02736737, -0.02768031]`. Since
    the layer is not trained yet, these encodings are just random.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An `Embedding` layer is initialized randomly, so it does not make sense to use
    it outside of a model as a standalone preprocessing layer unless you initialize
    it with pretrained weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to embed a categorical text attribute, you can simply chain a `StringLookup`
    layer and an `Embedding` layer, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the number of rows in the embedding matrix needs to be equal to the
    vocabulary size: that’s the total number of categories, including the known categories
    plus the OOV buckets (just one by default). The `vocabulary_size()` method of
    the `StringLookup` class conveniently returns this number.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example we used 2D embeddings, but as a rule of thumb embeddings typically
    have 10 to 300 dimensions, depending on the task, the vocabulary size, and the
    size of your training set. You will have to tune this hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting everything together, we can now create a Keras model that can process
    a categorical text feature along with regular numerical features and learn an
    embedding for each category (as well as for each OOV bucket):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This model takes two inputs: `num_input`, which contains eight numerical features
    per instance, plus `cat_input`, which contains a single categorical text input
    per instance. The model uses the `lookup_and_embed` model we created earlier to
    encode each ocean-proximity category as the corresponding trainable embedding.
    Next, it concatenates the numerical inputs and the embeddings using the `concatenate()`
    function to produce the complete encoded inputs, which are ready to be fed to
    a neural network. We could add any kind of neural network at this point, but for
    simplicity we just add a single dense output layer, and then we create the Keras
    `Model` with the inputs and output we’ve just defined. Next we compile the model
    and train it, passing both the numerical and categorical inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you saw in [Chapter 10](ch10.html#ann_chapter), since the `Input` layers
    are named `"num"` and `"cat"`, we could also have passed the training data to
    the `fit()` method using a dictionary instead of a tuple: `{"num": X_train_num,
    "cat": X_train_cat}`. Alternatively, we could have passed a `tf.data.Dataset`
    containing batches, each represented as `((X_batch_num, X_batch_cat), y_batch)`
    or as `({"num": X_batch_num, "cat": X_batch_cat}, y_batch)`. And of course the
    same goes for the validation data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One-hot encoding followed by a `Dense` layer (with no activation function and
    no biases) is equivalent to an `Embedding` layer. However, the `Embedding` layer
    uses way fewer computations as it avoids many multiplications by zero—the performance
    difference becomes clear when the size of the embedding matrix grows. The `Dense`
    layer’s weight matrix plays the role of the embedding matrix. For example, using
    one-hot vectors of size 20 and a `Dense` layer with 10 units is equivalent to
    using an `Embedding` layer with `input_dim=20` and `output_dim=10`. As a result,
    it would be wasteful to use more embedding dimensions than the number of units
    in the layer that follows the `Embedding` layer.
  prefs: []
  type: TYPE_NORMAL
- en: OK, now that you have learned how to encode categorical features, it’s time
    to turn our attention to text preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Text Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras provides a `TextVectorization` layer for basic text preprocessing. Much
    like the `StringLookup` layer, you must either pass it a vocabulary upon creation,
    or let it learn the vocabulary from some training data using the `adapt()` method.
    Let’s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The two sentences “Be good!” and “Question: be or be?” were encoded as `[2,
    1, 0, 0]` and `[6, 2, 1, 2]`, respectively. The vocabulary was learned from the
    four sentences in the training data: “be” = 2, “to” = 3, etc. To construct the
    vocabulary, the `adapt()` method first converted the training sentences to lowercase
    and removed punctuation, which is why “Be”, “be”, and “be?” are all encoded as
    “be” = 2\. Next, the sentences were split on whitespace, and the resulting words
    were sorted by descending frequency, producing the final vocabulary. When encoding
    sentences, unknown words get encoded as 1s. Lastly, since the first sentence is
    shorter than the second, it was padded with 0s.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `TextVectorization` layer has many options. For example, you can preserve
    the case and punctuation if you want, by setting `standardize=None`, or you can
    pass any standardization function you please as the `standardize` argument. You
    can prevent splitting by setting `split=None`, or you can pass your own splitting
    function instead. You can set the `output_sequence_length` argument to ensure
    that the output sequences all get cropped or padded to the desired length, or
    you can set `ragged=True` to get a ragged tensor instead of a regular tensor.
    Please check out the documentation for more options.
  prefs: []
  type: TYPE_NORMAL
- en: 'The word IDs must be encoded, typically using an `Embedding` layer: we will
    do this in [Chapter 16](ch16.html#nlp_chapter). Alternatively, you can set the
    `TextVectorization` layer’s `output_mode` argument to `"multi_hot"` or `"count"`
    to get the corresponding encodings. However, simply counting words is usually
    not ideal: words like “to” and “the” are so frequent that they hardly matter at
    all, whereas, rarer words such as “basketball” are much more informative. So,
    rather than setting `output_mode` to `"multi_hot"` or `"count"`, it is usually
    preferable to set it to `"tf_idf"`, which stands for *term-frequency* × *inverse-document-frequency*
    (TF-IDF). This is similar to the count encoding, but words that occur frequently
    in the training data are downweighted, and conversely, rare words are upweighted.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many TF-IDF variants, but the way the `TextVectorization` layer implements
    it is by multiplying each word count by a weight equal to log(1 + *d* / (*f* +
    1)), where *d* is the total number of sentences (a.k.a., documents) in the training
    data and *f* counts how many of these training sentences contain the given word.
    For example, in this case there are *d* = 4 sentences in the training data, and
    the word “be” appears in *f* = 3 of these. Since the word “be” occurs twice in
    the sentence “Question: be or be?”, it gets encoded as 2 × log(1 + 4 / (1 + 3))
    ≈ 1.3862944\. The word “question” only appears once, but since it is a less common
    word, its encoding is almost as high: 1 × log(1 + 4 / (1 + 1)) ≈ 1.0986123\. Note
    that the average weight is used for unknown words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach to text encoding is straightforward to use and it can give fairly
    good results for basic natural language processing tasks, but it has several important
    limitations: it only works with languages that separate words with spaces, it
    doesn’t distinguish between homonyms (e.g., “to bear” versus “teddy bear”), it
    gives no hint to your model that words like “evolution” and “evolutionary” are
    related, etc. And if you use multi-hot, count, or TF-IDF encoding, then the order
    of the words is lost. So what are the other options?'
  prefs: []
  type: TYPE_NORMAL
- en: One option is to use the [TensorFlow Text library](https://tensorflow.org/text),
    which provides more advanced text preprocessing features than the `TextVectorization`
    layer. For example, it includes several subword tokenizers capable of splitting
    the text into tokens smaller than words, which makes it possible for the model
    to more easily detect that “evolution” and “evolutionary” have something in common
    (more on subword tokenization in [Chapter 16](ch16.html#nlp_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Yet another option is to use pretrained language model components. Let’s look
    at this now.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pretrained Language Model Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [TensorFlow Hub library](https://tensorflow.org/hub) makes it easy to reuse
    pretrained model components in your own models, for text, image, audio, and more.
    These model components are called *modules*. Simply browse the [TF Hub repository](https://tfhub.dev),
    find the one you need, and copy the code example into your project, and the module
    will be automatically downloaded and bundled into a Keras layer that you can directly
    include in your model. Modules typically contain both preprocessing code and pretrained
    weights, and they generally require no extra training (but of course, the rest
    of your model will certainly require training).
  prefs: []
  type: TYPE_NORMAL
- en: For example, some powerful pretrained language models are available. The most
    powerful are quite large (several gigabytes), so for a quick example let’s use
    the `nnlm-en-dim50` module, version 2, which is a fairly basic module that takes
    raw text as input and outputs 50-dimensional sentence embeddings. We’ll import
    TensorFlow Hub and use it to load the module, then use that module to encode two
    sentences to vectors:^([8](ch13.html#idm45720186424416))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hub.KerasLayer` layer downloads the module from the given URL. This particular
    module is a *sentence encoder*: it takes strings as input and encodes each one
    as a single vector (in this case, a 50-dimensional vector). Internally, it parses
    the string (splitting words on spaces) and embeds each word using an embedding
    matrix that was pretrained on a huge corpus: the Google News 7B corpus (seven
    billion words long!). Then it computes the mean of all the word embeddings, and
    the result is the sentence embedding.⁠^([9](ch13.html#idm45720186382240))'
  prefs: []
  type: TYPE_NORMAL
- en: You just need to include this `hub_layer` in your model, and you’re ready to
    go. Note that this particular language model was trained on the English language,
    but many other languages are available, as well as multilingual models.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, the excellent open source [Transformers library by Hugging
    Face](https://huggingface.co/docs/transformers) also makes it easy to include
    powerful language model components inside your own models. You can browse the
    [Hugging Face Hub](https://huggingface.co/models), choose the model you want,
    and use the provided code examples to get started. It used to contain only language
    models, but it has now expanded to include image models and more.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to natural language processing in more depth in [Chapter 16](ch16.html#nlp_chapter).
    Let’s now look at Keras’s image preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: Image Preprocessing Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Keras preprocessing API includes three image preprocessing layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras.layers.Resizing` resizes the input images to the desired size. For
    example, `Resizing(height=100, width=200)` resizes each image to 100 × 200, possibly
    distorting the image. If you set `crop_to_aspect_ratio=True`, then the image will
    be cropped to the target image ratio, to avoid distortion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.keras.layers.Rescaling` rescales the pixel values. For example, `Rescal⁠ing​(scale=2/255,
    offset=-1)` scales the values from 0 → 255 to –1 → 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.keras.layers.CenterCrop` crops the image, keeping only a center patch of
    the desired height and width.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s load a couple of sample images and center-crop them. For
    this, we will use Scikit-Learn’s `load_sample_images()` function; this loads two
    color images, one of a Chinese temple and the other of a flower (this requires
    the Pillow library, which should already be installed if you are using Colab or
    if you followed the installation instructions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Keras also includes several layers for data augmentation, such as `RandomCrop`,
    `RandomFlip`, `RandomTranslation`, `RandomRotation`, `RandomZoom`, `RandomHeight`,
    `RandomWidth`, and `RandomContrast`. These layers are only active during training,
    and they randomly apply some transformation to the input images (their names are
    self-explanatory). Data augmentation will artificially increase the size of the
    training set, which often leads to improved performance, as long as the transformed
    images look like realistic (nonaugmented) images. We’ll cover image processing
    more closely in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under the hood, the Keras preprocessing layers are based on TensorFlow’s low-level
    API. For example, the `Normalization` layer uses `tf.nn.moments()` to compute
    both the mean and variance, the `Discretization` layer uses `tf.raw_ops.Bucketize()`,
    `CategoricalEncoding` uses `tf.math.bincount()`, `IntegerLookup` and `StringLookup`
    use the `tf.lookup` package, `Hashing` and `TextVectorization` use several ops
    from the `tf.strings` package, `Embedding` uses `tf.nn.embedding_lookup()`, and
    the image preprocessing layers use the ops from the `tf.image` package. If the
    Keras preprocessing API isn’t sufficient for your needs, you may occasionally
    need to use TensorFlow’s low-level API directly.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at another way to load data easily and efficiently in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Datasets Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [TensorFlow Datasets (TFDS)](https://tensorflow.org/datasets) project makes
    it very easy to load common datasets, from small ones like MNIST or Fashion MNIST
    to huge datasets like ImageNet (you will need quite a bit of disk space!). The
    list includes image datasets, text datasets (including translation datasets),
    audio and video datasets, time series, and much more. You can visit [*https://homl.info/tfds*](https://homl.info/tfds)
    to view the full list, along with a description of each dataset. You can also
    check out [Know Your Data](https://knowyourdata.withgoogle.com), which is a tool
    to explore and understand many of the datasets provided by TFDS.
  prefs: []
  type: TYPE_NORMAL
- en: 'TFDS is not bundled with TensorFlow, but if you are running on Colab or if
    you followed the installation instructions at [*https://homl.info/install*](https://homl.info/install),
    then it’s already installed. You can then import `tensorflow_datasets`, usually
    as `tfds`, then call the `tfds.load()` function, which will download the data
    you want (unless it was already downloaded earlier) and return the data as a dictionary
    of datasets (typically one for training and one for testing, but this depends
    on the dataset you choose). For example, let’s download MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then apply any transformation you want (typically shuffling, batching,
    and prefetching), and you’re ready to train your model. Here is a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `load()` function can shuffle the files it downloads: just set `shuffle_files=True`.
    However, this may be insufficient, so it’s best to shuffle the training data some
    more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that each item in the dataset is a dictionary containing both the features
    and the labels. But Keras expects each item to be a tuple containing two elements
    (again, the features and the labels). You could transform the dataset using the
    `map()` method, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: But it’s simpler to ask the `load()` function to do this for you by setting
    `as_supervised=True` (obviously this works only for labeled datasets).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, TFDS provides a convenient way to split the data using the `split`
    argument. For example, if you want to use the first 90% of the training set for
    training, the remaining 10% for validation, and the whole test set for testing,
    then you can set `split=["train[:90%]", "train[90%:]", "test"]`. The `load()`
    function will return all three sets. Here is a complete example, loading and splitting
    the MNIST dataset using TFDS, then using these sets to train and evaluate a simple
    Keras model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, you’ve reached the end of this quite technical chapter! You
    may feel that it is a bit far from the abstract beauty of neural networks, but
    the fact is deep learning often involves large amounts of data, and knowing how
    to load, parse, and preprocess it efficiently is a crucial skill to have. In the
    next chapter, we will look at convolutional neural networks, which are among the
    most successful neural net architectures for image processing and many other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why would you want to use the tf.data API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the benefits of splitting a large dataset into multiple files?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, how can you tell that your input pipeline is the bottleneck?
    What can you do to fix it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you save any binary data to a TFRecord file, or only serialized protocol
    buffers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you go through the hassle of converting all your data to the `Example`
    protobuf format? Why not use your own protobuf definition?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using TFRecords, when would you want to activate compression? Why not do
    it systematically?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data can be preprocessed directly when writing the data files, or within the
    tf.data pipeline, or in preprocessing layers within your model. Can you list a
    few pros and cons of each option?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name a few common ways you can encode categorical integer features. What about
    text?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the Fashion MNIST dataset (introduced in [Chapter 10](ch10.html#ann_chapter));
    split it into a training set, a validation set, and a test set; shuffle the training
    set; and save each dataset to multiple TFRecord files. Each record should be a
    serialized `Example` protobuf with two features: the serialized image (use `tf.io.serialize_tensor()`
    to serialize each image), and the label.⁠^([10](ch13.html#idm45720185796112))
    Then use tf.data to create an efficient dataset for each set. Finally, use a Keras
    model to train these datasets, including a preprocessing layer to standardize
    each input feature. Try to make the input pipeline as efficient as possible, using
    TensorBoard to visualize profiling data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this exercise you will download a dataset, split it, create a `tf.data.Dataset`
    to load it and preprocess it efficiently, then build and train a binary classification
    model containing an `Embedding` layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains
    50,000 movie reviews from the [Internet Movie Database (IMDb)](https://imdb.com).
    The data is organized in two directories, *train* and *test*, each containing
    a *pos* subdirectory with 12,500 positive reviews and a *neg* subdirectory with
    12,500 negative reviews. Each review is stored in a separate text file. There
    are other files and folders (including preprocessed bag-of-words versions), but
    we will ignore them in this exercise.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the test set into a validation set (15,000) and a test set (10,000).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use tf.data to create an efficient dataset for each set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a binary classification model, using a `TextVectorization` layer to preprocess
    each review.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an `Embedding` layer and compute the mean embedding for each review, multiplied
    by the square root of the number of words (see [Chapter 16](ch16.html#nlp_chapter)).
    This rescaled mean embedding can then be passed to the rest of your model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model and see what accuracy you get. Try to optimize your pipelines
    to make training as fast as possible.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use TFDS to load the same dataset more easily: `tfds.load("imdb_reviews")`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch13.html#idm45720190533488-marker)) Imagine a sorted deck of cards on
    your left: suppose you just take the top three cards and shuffle them, then pick
    one randomly and put it to your right, keeping the other two in your hands. Take
    another card on your left, shuffle the three cards in your hands and pick one
    of them randomly, and put it on your right. When you are done going through all
    the cards like this, you will have a deck of cards on your right: do you think
    it will be perfectly shuffled?'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch13.html#idm45720189926208-marker)) In general, just prefetching one
    batch is fine, but in some cases you may need to prefetch a few more. Alternatively,
    you can let TensorFlow decide automatically by passing `tf.data.AUTOTUNE` to `prefetch()`.
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch13.html#idm45720189876688-marker)) But check out the experimental `tf.data.experimental.prefetch_to_device()`
    function, which can prefetch data directly to the GPU. Any TensorFlow function
    or class with `experimental` in its name may change without warning in future
    versions. If an experimental function fails, try removing the word `experimental`:
    it may have been moved to the core API. If not, then please check the notebook,
    as I will ensure it contains up-to-date code.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch13.html#idm45720189164144-marker)) Since protobuf objects are meant
    to be serialized and transmitted, they are called *messages*.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch13.html#idm45720189071424-marker)) This chapter contains the bare minimum
    you need to know about protobufs to use TFRecords. To learn more about protobufs,
    please visit [*https://homl.info/protobuf*](https://homl.info/protobuf).
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch13.html#idm45720187048176-marker)) Tomáš Mikolov et al., “Distributed
    Representations of Words and Phrases and Their Compositionality”, *Proceedings
    of the 26th International Conference on Neural Information Processing Systems*
    2 (2013): 3111–3119.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch13.html#idm45720187037120-marker)) Malvina Nissim et al., “Fair Is
    Better Than Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv preprint
    arXiv:1905.09866 (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch13.html#idm45720186424416-marker)) TensorFlow Hub is not bundled with
    TensorFlow, but if you are running on Colab or if you followed the installation
    instructions at [*https://homl.info/install*](https://homl.info/install), then
    it’s already installed.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch13.html#idm45720186382240-marker)) To be precise, the sentence embedding
    is equal to the mean word embedding multiplied by the square root of the number
    of words in the sentence. This compensates for the fact that the mean of *n* random
    vectors gets shorter as *n* grows.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch13.html#idm45720185796112-marker)) For large images, you could use
    `tf.io.encode_jpeg()` instead. This would save a lot of space, but it would lose
    a bit of image quality.
  prefs: []
  type: TYPE_NORMAL
