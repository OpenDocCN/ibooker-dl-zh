- en: Chapter 13\. Loading and Preprocessing Data with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 13 章。使用 TensorFlow 加载和预处理数据
- en: In [Chapter 2](ch02.html#project_chapter), you saw that loading and preprocessing
    data is an important part of any machine learning project. You used Pandas to
    load and explore the (modified) California housing dataset—which was stored in
    a CSV file—and you applied Scikit-Learn’s transformers for preprocessing. These
    tools are quite convenient, and you will probably be using them often, especially
    when exploring and experimenting with data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](ch02.html#project_chapter)中，您看到加载和预处理数据是任何机器学习项目的重要部分。您使用 Pandas 加载和探索（修改后的）加利福尼亚房屋数据集——该数据集存储在
    CSV 文件中——并应用 Scikit-Learn 的转换器进行预处理。这些工具非常方便，您可能会经常使用它们，特别是在探索和实验数据时。
- en: However, when training TensorFlow models on large datasets, you may prefer to
    use TensorFlow’s own data loading and preprocessing API, called *tf.data*. It
    is capable of loading and preprocessing data extremely efficiently, reading from
    multiple files in parallel using multithreading and queuing, shuffling and batching
    samples, and more. Plus, it can do all of this on the fly—it loads and preprocesses
    the next batch of data across multiple CPU cores, while your GPUs or TPUs are
    busy training the current batch of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大型数据集上训练 TensorFlow 模型时，您可能更喜欢使用 TensorFlow 自己的数据加载和预处理 API，称为*tf.data*。它能够非常高效地加载和预处理数据，使用多线程和排队从多个文件中并行读取数据，对样本进行洗牌和分批处理等。此外，它可以实时执行所有这些操作——在
    GPU 或 TPU 正在训练当前批次数据时，它会在多个 CPU 核心上加载和预处理下一批数据。
- en: The tf.data API lets you handle datasets that don’t fit in memory, and it allows
    you to make full use of your hardware resources, thereby speeding up training.
    Off the shelf, the tf.data API can read from text files (such as CSV files), binary
    files with fixed-size records, and binary files that use TensorFlow’s TFRecord
    format, which supports records of varying sizes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data API 允许您处理无法放入内存的数据集，并充分利用硬件资源，从而加快训练速度。tf.data API 可以直接从文本文件（如 CSV 文件）、具有固定大小记录的二进制文件以及使用
    TensorFlow 的 TFRecord 格式的二进制文件中读取数据。
- en: TFRecord is a flexible and efficient binary format usually containing protocol
    buffers (an open source binary format). The tf.data API also has support for reading
    from SQL databases. Moreover, many open source extensions are available to read
    from all sorts of data sources, such as Google’s BigQuery service (see [*https://tensorflow.org/io*](https://tensorflow.org/io)).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord 是一种灵活高效的二进制格式，通常包含协议缓冲区（一种开源二进制格式）。tf.data API 还支持从 SQL 数据库中读取数据。此外，许多开源扩展可用于从各种数据源中读取数据，例如
    Google 的 BigQuery 服务（请参阅[*https://tensorflow.org/io*](https://tensorflow.org/io)）。
- en: 'Keras also comes with powerful yet easy-to-use preprocessing layers that can
    be embedded in your models: this way, when you deploy a model to production, it
    will be able to ingest raw data directly, without you having to add any additional
    preprocessing code. This eliminates the risk of mismatch between the preprocessing
    code used during training and the preprocessing code used in production, which
    would likely cause *training/serving skew*. And if you deploy your model in multiple
    apps coded in different programming languages, you won’t have to reimplement the
    same preprocessing code multiple times, which also reduces the risk of mismatch.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 还提供了强大而易于使用的预处理层，可以嵌入到您的模型中：这样，当您将模型部署到生产环境时，它将能够直接摄取原始数据，而无需您添加任何额外的预处理代码。这消除了训练期间使用的预处理代码与生产中使用的预处理代码之间不匹配的风险，这可能会导致*训练/服务偏差*。如果您将模型部署在使用不同编程语言编写的多个应用程序中，您不必多次重新实现相同的预处理代码，这也减少了不匹配的风险。
- en: As you will see, both APIs can be used jointly—for example, to benefit from
    the efficient data loading offered by tf.data and the convenience of the Keras
    preprocessing layers.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，这两个 API 可以联合使用——例如，从 tf.data 提供的高效数据加载和 Keras 预处理层的便利性中受益。
- en: In this chapter, we will first cover the tf.data API and the TFRecord format.
    Then we will explore the Keras preprocessing layers and how to use them with the
    tf.data API. Lastly, we will take a quick look at a few related libraries that
    you may find useful for loading and preprocessing data, such as TensorFlow Datasets
    and TensorFlow Hub. So, let’s get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先介绍 tf.data API 和 TFRecord 格式。然后我们将探索 Keras 预处理层以及如何将它们与 tf.data API
    一起使用。最后，我们将快速查看一些相关的库，您可能会发现它们在加载和预处理数据时很有用，例如 TensorFlow Datasets 和 TensorFlow
    Hub。所以，让我们开始吧！
- en: The tf.data API
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.data API
- en: 'The whole tf.data API revolves around the concept of a `tf.data.Dataset`: this
    represents a sequence of data items. Usually you will use datasets that gradually
    read data from disk, but for simplicity let’s create a dataset from a simple data
    tensor using `tf.data.Dataset.from_tensor_slices()`:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 tf.data API 围绕着 `tf.data.Dataset` 的概念展开：这代表了一系列数据项。通常，您会使用逐渐从磁盘读取数据的数据集，但为了简单起见，让我们使用
    `tf.data.Dataset.from_tensor_slices()` 从一个简单的数据张量创建数据集：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset`
    whose elements are all the slices of `X` along the first dimension, so this dataset
    contains 10 items: tensors 0, 1, 2, …​, 9\. In this case we would have obtained
    the same dataset if we had used `tf.data.Dataset.range(10)` (except the elements
    would be 64-bit integers instead of 32-bit integers).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`from_tensor_slices()` 函数接受一个张量，并创建一个 `tf.data.Dataset`，其中的元素是沿着第一维度的所有 `X`
    的切片，因此这个数据集包含 10 个项目：张量 0、1、2、…​、9。在这种情况下，如果我们使用 `tf.data.Dataset.range(10)`，我们将获得相同的数据集（除了元素将是
    64 位整数而不是 32 位整数）。'
- en: 'You can simply iterate over a dataset’s items like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以简单地迭代数据集的项目，如下所示：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The tf.data API is a streaming API: you can very efficiently iterate through
    a dataset’s items, but the API is not designed for indexing or slicing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data API 是一个流式 API：您可以非常高效地迭代数据集的项目，但该 API 不适用于索引或切片。
- en: 'A dataset may also contain tuples of tensors, or dictionaries of name/tensor
    pairs, or even nested tuples and dictionaries of tensors. When slicing a tuple,
    a dictionary, or a nested structure, the dataset will only slice the tensors it
    contains, while preserving the tuple/dictionary structure. For example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Chaining Transformations
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have a dataset, you can apply all sorts of transformations to it by
    calling its transformation methods. Each method returns a new dataset, so you
    can chain transformations like this (this chain is illustrated in [Figure 13-1](#chaining_transformations_diagram)):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we first call the `repeat()` method on the original dataset,
    and it returns a new dataset that repeats the items of the original dataset three
    times. Of course, this will not copy all the data in memory three times! If you
    call this method with no arguments, the new dataset will repeat the source dataset
    forever, so the code that iterates over the dataset will have to decide when to
    stop.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Then we call the `batch()` method on this new dataset, and again this creates
    a new dataset. This one will group the items of the previous dataset in batches
    of seven items.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1301](assets/mls3_1301.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. Chaining dataset transformations
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, we iterate over the items of this final dataset. The `batch()` method
    had to output a final batch of size two instead of seven, but you can call `batch()`
    with `drop_remainder=True` if you want it to drop this final batch, such that
    all batches have the exact same size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The dataset methods do *not* modify datasets—they create new ones. So make sure
    to keep a reference to these new datasets (e.g., with `dataset = ...`), or else
    nothing will happen.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also transform the items by calling the `map()` method. For example,
    this creates a new dataset with all batches multiplied by two:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This `map()` method is the one you will call to apply any preprocessing to your
    data. Sometimes this will include computations that can be quite intensive, such
    as reshaping or rotating an image, so you will usually want to spawn multiple
    threads to speed things up. This can be done by setting the `num_parallel_calls`
    argument to the number of threads to run, or to `tf.data.AUTOTUNE`. Note that
    the function you pass to the `map()` method must be convertible to a TF function
    (see [Chapter 12](ch12.html#tensorflow_chapter)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to simply filter the dataset using the `filter()` method.
    For example, this code creates a dataset that only contains the batchs whose sum
    is greater than 50:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will often want to look at just a few items from a dataset. You can use
    the `take()` method for that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Shuffling the Data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 4](ch04.html#linear_models_chapter), gradient descent
    works best when the instances in the training set are independent and identically
    distributed (IID). A simple way to ensure this is to shuffle the instances, using
    the `shuffle()` method. It will create a new dataset that will start by filling
    up a buffer with the first items of the source dataset. Then, whenever it is asked
    for an item, it will pull one out randomly from the buffer and replace it with
    a fresh one from the source dataset, until it has iterated entirely through the
    source dataset. At this point it will continue to pull out items randomly from
    the buffer until it is empty. You must specify the buffer size, and it is important
    to make it large enough, or else shuffling will not be very effective.⁠^([1](ch13.html#idm45720190533488))
    Just don’t exceed the amount of RAM you have, though even if you have plenty of
    it, there’s no need to go beyond the dataset’s size. You can provide a random
    seed if you want the same random order every time you run your program. For example,
    the following code creates and displays a dataset containing the integers 0 to
    9, repeated twice, shuffled using a buffer of size 4 and a random seed of 42,
    and batched with a batch size of 7:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you call `repeat()` on a shuffled dataset, by default it will generate a
    new order at every iteration. This is generally a good idea, but if you prefer
    to reuse the same order at each iteration (e.g., for tests or debugging), you
    can set `reshuffle_each_​itera⁠tion=False` when calling `shuffle()`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在打乱的数据集上调用`repeat()`，默认情况下它将在每次迭代时生成一个新的顺序。这通常是个好主意，但是如果您希望在每次迭代中重复使用相同的顺序（例如，用于测试或调试），可以在调用`shuffle()`时设置`reshuffle_each_​itera⁠tion=False`。
- en: 'For a large dataset that does not fit in memory, this simple shuffling-buffer
    approach may not be sufficient, since the buffer will be small compared to the
    dataset. One solution is to shuffle the source data itself (for example, on Linux
    you can shuffle text files using the `shuf` command). This will definitely improve
    shuffling a lot! Even if the source data is shuffled, you will usually want to
    shuffle it some more, or else the same order will be repeated at each epoch, and
    the model may end up being biased (e.g., due to some spurious patterns present
    by chance in the source data’s order). To shuffle the instances some more, a common
    approach is to split the source data into multiple files, then read them in a
    random order during training. However, instances located in the same file will
    still end up close to each other. To avoid this you can pick multiple files randomly
    and read them simultaneously, interleaving their records. Then on top of that
    you can add a shuffling buffer using the `shuffle()` method. If this sounds like
    a lot of work, don’t worry: the tf.data API makes all this possible in just a
    few lines of code. Let’s go over how you can do this.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个无法放入内存的大型数据集，这种简单的打乱缓冲区方法可能不够，因为缓冲区相对于数据集来说很小。一个解决方案是对源数据本身进行打乱（例如，在Linux上可以使用`shuf`命令对文本文件进行打乱）。这将显著改善打乱效果！即使源数据已经被打乱，通常也会希望再次打乱，否则每个时期将重复相同的顺序，模型可能会出现偏差（例如，由于源数据顺序中偶然存在的一些虚假模式）。为了进一步打乱实例，一个常见的方法是将源数据拆分为多个文件，然后在训练过程中以随机顺序读取它们。然而，位于同一文件中的实例仍然会相互靠近。为了避免这种情况，您可以随机选择多个文件并同时读取它们，交错它们的记录。然后在此基础上使用`shuffle()`方法添加一个打乱缓冲区。如果这听起来很费力，不用担心：tf.data
    API可以在几行代码中实现所有这些。让我们看看您可以如何做到这一点。
- en: Interleaving Lines from Multiple Files
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从多个文件中交错行
- en: 'First, suppose you’ve loaded the California housing dataset, shuffled it (unless
    it was already shuffled), and split it into a training set, a validation set,
    and a test set. Then you split each set into many CSV files that each look like
    this (each row contains eight input features plus the target median house value):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设您已经加载了加利福尼亚房屋数据集，对其进行了打乱（除非已经打乱），并将其分为训练集、验证集和测试集。然后将每个集合分成许多CSV文件，每个文件看起来像这样（每行包含八个输入特征加上目标中位房价）：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s also suppose `train_filepaths` contains the list of training filepaths
    (and you also have `valid_filepaths` and `test_filepaths`):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`train_filepaths`包含训练文件路径列表（您还有`valid_filepaths`和`test_filepaths`）：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, you could use file patterns; for example, `train_filepaths =`
    `"datasets/housing/my_train_*.csv"`. Now let’s create a dataset containing only
    these filepaths:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用文件模式；例如，`train_filepaths =` `"datasets/housing/my_train_*.csv"`。现在让我们创建一个仅包含这些文件路径的数据集：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By default, the `list_files()` function returns a dataset that shuffles the
    filepaths. In general this is a good thing, but you can set `shuffle=False` if
    you do not want that for some reason.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`list_files()`函数返回一个打乱文件路径的数据集。一般来说这是件好事，但是如果出于某种原因不想要这样，可以设置`shuffle=False`。
- en: 'Next, you can call the `interleave()` method to read from five files at a time
    and interleave their lines. You can also skip the first line of each file—which
    is the header row—using the `skip()` method):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以调用`interleave()`方法一次从五个文件中读取并交错它们的行。您还可以使用`skip()`方法跳过每个文件的第一行（即标题行）：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `interleave()` method will create a dataset that will pull five filepaths
    from the `filepath_dataset`, and for each one it will call the function you gave
    it (a lambda in this example) to create a new dataset (in this case a `TextLineDataset`).
    To be clear, at this stage there will be seven datasets in all: the filepath dataset,
    the interleave dataset, and the five `TextLineDataset`s created internally by
    the interleave dataset. When you iterate over the interleave dataset, it will
    cycle through these five `TextLineDataset`s, reading one line at a time from each
    until all datasets are out of items. Then it will fetch the next five filepaths
    from the `filepath_dataset` and interleave them the same way, and so on until
    it runs out of filepaths. For interleaving to work best, it is preferable to have
    files of identical length; otherwise the end of the longest file will not be interleaved.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`interleave()`方法将创建一个数据集，从`filepath_dataset`中提取五个文件路径，对于每个文件路径，它将调用您提供的函数（在本例中是lambda函数）来创建一个新的数据集（在本例中是`TextLineDataset`）。清楚地说，在这个阶段总共会有七个数据集：文件路径数据集、交错数据集以及交错数据集内部创建的五个`TextLineDataset`。当您迭代交错数据集时，它将循环遍历这五个`TextLineDataset`，从每个数据集中逐行读取，直到所有数据集都用完。然后它将从`filepath_dataset`中获取下一个五个文件路径，并以相同的方式交错它们，依此类推，直到文件路径用完。为了使交错效果最佳，最好拥有相同长度的文件；否则最长文件的末尾将不会被交错。'
- en: 'By default, `interleave()` does not use parallelism; it just reads one line
    at a time from each file, sequentially. If you want it to actually read files
    in parallel, you can set the `interleave()` method’s `num_parallel_calls` argument
    to the number of threads you want (recall that the `map()` method also has this
    argument). You can even set it to `tf.data.AUTOTUNE` to make TensorFlow choose
    the right number of threads dynamically based on the available CPU. Let’s look
    at what the dataset contains now:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`interleave()`不使用并行处理；它只是顺序地从每个文件中一次读取一行。如果您希望实际并行读取文件，可以将`interleave()`方法的`num_parallel_calls`参数设置为您想要的线程数（请记住，`map()`方法也有这个参数）。甚至可以将其设置为`tf.data.AUTOTUNE`，让TensorFlow根据可用的CPU动态选择正确的线程数。现在让我们看看数据集现在包含什么：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are the first rows (ignoring the header row) of five CSV files, chosen
    randomly. Looks good!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是随机选择的五个 CSV 文件的第一行（忽略标题行）。看起来不错！
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'It’s possible to pass a list of filepaths to the `TextLineDataset` constructor:
    it will go through each file in order, line by line. If you also set the `num_parallel_reads`
    argument to a number greater than one, then the dataset will read that number
    of files in parallel and interleave their lines (without having to call the `interleave()`
    method). However, it will *not* shuffle the files, nor will it skip the header
    lines.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将文件路径列表传递给 `TextLineDataset` 构造函数：它将按顺序遍历每个文件的每一行。如果还将 `num_parallel_reads`
    参数设置为大于一的数字，那么数据集将并行读取该数量的文件，并交错它们的行（无需调用 `interleave()` 方法）。但是，它不会对文件进行洗牌，也不会跳过标题行。
- en: Preprocessing the Data
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Now that we have a housing dataset that returns each instance as a tensor containing
    a byte string, we need to do a bit of preprocessing, including parsing the strings
    and scaling the data. Let’s implement a couple custom functions that will perform
    this preprocessing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个返回每个实例的住房数据集，其中包含一个字节字符串的张量，我们需要进行一些预处理，包括解析字符串和缩放数据。让我们实现一些自定义函数来执行这些预处理：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s walk through this code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步解释这段代码：
- en: First, the code assumes that we have precomputed the mean and standard deviation
    of each feature in the training set. `X_mean` and `X_std` are just 1D tensors
    (or NumPy arrays) containing eight floats, one per input feature. This can be
    done using a Scikit-Learn `StandardScaler` on a large enough random sample of
    the dataset. Later in this chapter, we will use a Keras preprocessing layer instead.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，代码假设我们已经预先计算了训练集中每个特征的均值和标准差。`X_mean` 和 `X_std` 只是包含八个浮点数的 1D 张量（或 NumPy
    数组），每个输入特征一个。可以使用 Scikit-Learn 的 `StandardScaler` 在数据集的足够大的随机样本上完成这个操作。在本章的后面，我们将使用
    Keras 预处理层来代替。
- en: 'The `parse_csv_line()` function takes one CSV line and parses it. To help with
    that, it uses the `tf.io.decode_csv()` function, which takes two arguments: the
    first is the line to parse, and the second is an array containing the default
    value for each column in the CSV file. This array (`defs`) tells TensorFlow not
    only the default value for each column, but also the number of columns and their
    types. In this example, we tell it that all feature columns are floats and that
    missing values should default to zero, but we provide an empty array of type `tf.float32`
    as the default value for the last column (the target): the array tells TensorFlow
    that this column contains floats, but that there is no default value, so it will
    raise an exception if it encounters a missing value.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_csv_line()` 函数接受一个 CSV 行并对其进行解析。为了帮助实现这一点，它使用 `tf.io.decode_csv()` 函数，该函数接受两个参数：第一个是要解析的行，第二个是包含
    CSV 文件中每列的默认值的数组。这个数组（`defs`）告诉 TensorFlow 不仅每列的默认值是什么，还告诉它列的数量和类型。在这个例子中，我们告诉它所有特征列都是浮点数，缺失值应默认为零，但我们为最后一列（目标）提供了一个空的
    `tf.float32` 类型的默认值数组：该数组告诉 TensorFlow 这一列包含浮点数，但没有默认值，因此如果遇到缺失值，它将引发异常。'
- en: 'The `tf.io.decode_csv()` function returns a list of scalar tensors (one per
    column), but we need to return a 1D tensor array. So we call `tf.stack()` on all
    tensors except for the last one (the target): this will stack these tensors into
    a 1D array. We then do the same for the target value: this makes it a 1D tensor
    array with a single value, rather than a scalar tensor. The `tf.io.decode_csv()`
    function is done, so it returns the input features and the target.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.io.decode_csv()` 函数返回一个标量张量列表（每列一个），但我们需要返回一个 1D 张量数组。因此，我们对除最后一个（目标）之外的所有张量调用
    `tf.stack()`：这将这些张量堆叠成一个 1D 数组。然后我们对目标值做同样的操作：这将使其成为一个包含单个值的 1D 张量数组，而不是标量张量。`tf.io.decode_csv()`
    函数完成后，它将返回输入特征和目标。'
- en: Finally, the custom `preprocess()` function just calls the `parse_csv_line()`
    function, scales the input features by subtracting the feature means and then
    dividing by the feature standard deviations, and returns a tuple containing the
    scaled features and the target.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，自定义的 `preprocess()` 函数只调用 `parse_csv_line()` 函数，通过减去特征均值然后除以特征标准差来缩放输入特征，并返回一个包含缩放特征和目标的元组。
- en: 'Let’s test this preprocessing function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试这个预处理函数：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Looks good! The `preprocess()` function can convert an instance from a byte
    string to a nice scaled tensor, with its corresponding label. We can now use the
    dataset’s `map()` method to apply the `preprocess()` function to each sample in
    the dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！`preprocess()` 函数可以将一个实例从字节字符串转换为一个漂亮的缩放张量，带有相应的标签。我们现在可以使用数据集的 `map()`
    方法将 `preprocess()` 函数应用于数据集中的每个样本。
- en: Putting Everything Together
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容放在一起
- en: 'To make the code more reusable, let’s put together everything we have discussed
    so far into another helper function; it will create and return a dataset that
    will efficiently load California housing data from multiple CSV files, preprocess
    it, shuffle it, and batch it (see [Figure 13-2](#input_pipeline_diagram)):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代码更具重用性，让我们将迄今为止讨论的所有内容放在另一个辅助函数中；它将创建并返回一个数据集，该数据集将高效地从多个 CSV 文件中加载加利福尼亚房屋数据，对其进行预处理、洗牌和分批处理（参见[图
    13-2](#input_pipeline_diagram)）：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that we use the `prefetch()` method on the very last line. This is important
    for performance, as you will see now.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在最后一行使用了 `prefetch()` 方法。这对性能很重要，你现在会看到。
- en: '![mls3 1302](assets/mls3_1302.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1302](assets/mls3_1302.png)'
- en: Figure 13-2\. Loading and preprocessing data from multiple CSV files
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 从多个 CSV 文件加载和预处理数据
- en: Prefetching
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预取
- en: By calling `prefetch(1)` at the end of the custom `csv_reader_dataset()` function,
    we are creating a dataset that will do its best to always be one batch ahead.⁠^([2](ch13.html#idm45720189926208))
    In other words, while our training algorithm is working on one batch, the dataset
    will already be working in parallel on getting the next batch ready (e.g., reading
    the data from disk and preprocessing it). This can improve performance dramatically,
    as is illustrated in [Figure 13-3](#prefetching_diagram).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'If we also ensure that loading and preprocessing are multithreaded (by setting
    `num_parallel_calls` when calling `interleave()` and `map()`), we can exploit
    multiple CPU cores and hopefully make preparing one batch of data shorter than
    running a training step on the GPU: this way the GPU will be almost 100% utilized
    (except for the data transfer time from the CPU to the GPU⁠^([3](ch13.html#idm45720189876688))),
    and training will run much faster.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1303](assets/mls3_1303.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-3\. With prefetching, the CPU and the GPU work in parallel: as the
    GPU works on one batch, the CPU works on the next'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you plan to purchase a GPU card, its processing power and its memory size
    are of course very important (in particular, a large amount of RAM is crucial
    for large computer vision or natural language processing models). Just as important
    for good performance is the GPU’s *memory bandwidth*; this is the number of gigabytes
    of data it can get into or out of its RAM per second.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is small enough to fit in memory, you can significantly speed
    up training by using the dataset’s `cache()` method to cache its content to RAM.
    You should generally do this after loading and preprocessing the data, but before
    shuffling, repeating, batching, and prefetching. This way, each instance will
    only be read and preprocessed once (instead of once per epoch), but the data will
    still be shuffled differently at each epoch, and the next batch will still be
    prepared in advance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: You have now learned how to build efficient input pipelines to load and preprocess
    data from multiple text files. We have discussed the most common dataset methods,
    but there are a few more you may want to look at, such as `concatenate()`, `zip()`,
    `window()`, `reduce()`, `shard()`, `flat_map()`, `apply()`, `unbatch()`, and `padded_batch()`.
    There are also a few more class methods, such as `from_generator()` and `from_​ten⁠sors()`,
    which create a new dataset from a Python generator or a list of tensors, respectively.
    Please check the API documentation for more details. Also note that there are
    experimental features available in `tf.data.experimental`, many of which will
    likely make it to the core API in future releases (e.g., check out the `CsvDataset`
    class, as well as the `make_csv_dataset()` method, which takes care of inferring
    the type of each column).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Using the Dataset with Keras
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can use the custom `csv_reader_dataset()` function we wrote earlier
    to create a dataset for the training set, and for the validation set and the test
    set. The training set will be shuffled at each epoch (note that the validation
    set and the test set will also be shuffled, even though we don’t really need that):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now you can simply build and train a Keras model using these datasets. When
    you call the model’s `fit()` method, you pass `train_set` instead of `X_train,
    y_train`, and pass `validation_data=valid_set` instead of `validation_data=(X_valid,
    y_valid)`. The `fit()` method will take care of repeating the training dataset
    once per epoch, using a different random order at each epoch:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, you can pass a dataset to the `evaluate()` and `predict()` methods:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unlike the other sets, the `new_set` will usually not contain labels. If it
    does, as is the case here, Keras will ignore them. Note that in all these cases,
    you can still use NumPy arrays instead of datasets if you prefer (but of course
    they need to have been loaded and preprocessed first).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to build your own custom training loop (as discussed in [Chapter 12](ch12.html#tensorflow_chapter)),
    you can just iterate over the training set, very naturally:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想构建自己的自定义训练循环（如[第12章](ch12.html#tensorflow_chapter)中讨论的），您可以很自然地遍历训练集：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In fact, it is even possible to create a TF function (see [Chapter 12](ch12.html#tensorflow_chapter))
    that trains the model for a whole epoch. This can really speed up training:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，甚至可以创建一个TF函数（参见[第12章](ch12.html#tensorflow_chapter)），用于整个时期训练模型。这可以真正加快训练速度：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In Keras, the `steps_per_execution` argument of the `compile()` method lets
    you define the number of batches that the `fit()` method will process during each
    call to the `tf.function` it uses for training. The default is just 1, so if you
    set it to 50 you will often see a significant performance improvement. However,
    the `on_batch_*()` methods of Keras callbacks will only be called every 50 batches.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，`compile()`方法的`steps_per_execution`参数允许您定义`fit()`方法在每次调用用于训练的`tf.function`时将处理的批次数。默认值只是1，因此如果将其设置为50，您通常会看到显着的性能改进。但是，Keras回调的`on_batch_*()`方法只会在每50批次时调用一次。
- en: Congratulations, you now know how to build powerful input pipelines using the
    tf.data API! However, so far we’ve been using CSV files, which are common, simple,
    and convenient but not really efficient, and do not support large or complex data
    structures (such as images or audio) very well. So, let’s see how to use TFRecords
    instead.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，您现在知道如何使用tf.data API构建强大的输入管道！然而，到目前为止，我们一直在使用常见、简单和方便但不是真正高效的CSV文件，并且不太支持大型或复杂的数据结构（如图像或音频）。因此，让我们看看如何改用TFRecords。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are happy with CSV files (or whatever other format you are using), you
    do not *have* to use TFRecords. As the saying goes, if it ain’t broke, don’t fix
    it! TFRecords are useful when the bottleneck during training is loading and parsing
    the data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对CSV文件（或者您正在使用的其他格式）感到满意，您不一定*必须*使用TFRecords。俗话说，如果它没有坏，就不要修理！当训练过程中的瓶颈是加载和解析数据时，TFRecords非常有用。
- en: The TFRecord Format
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TFRecord格式
- en: 'The TFRecord format is TensorFlow’s preferred format for storing large amounts
    of data and reading it efficiently. It is a very simple binary format that just
    contains a sequence of binary records of varying sizes (each record is comprised
    of a length, a CRC checksum to check that the length was not corrupted, then the
    actual data, and finally a CRC checksum for the data). You can easily create a
    TFRecord file using the `tf.io.TFRecordWriter` class:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord格式是TensorFlow存储大量数据并高效读取的首选格式。它是一个非常简单的二进制格式，只包含一系列大小不同的二进制记录（每个记录由长度、用于检查长度是否损坏的CRC校验和、实际数据，最后是数据的CRC校验和组成）。您可以使用`tf.io.TFRecordWriter`类轻松创建TFRecord文件：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And you can then use a `tf.data.TFRecordDataset` to read one or more TFRecord
    files:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`tf.data.TFRecordDataset`来读取一个或多个TFRecord文件：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: By default, a `TFRecordDataset` will read files one by one, but you can make
    it read multiple files in parallel and interleave their records by passing the
    constructor a list of filepaths and setting `num_parallel_reads` to a number greater
    than one. Alternatively, you could obtain the same result by using `list_files()`
    and `interleave()` as we did earlier to read multiple CSV files.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`TFRecordDataset`将逐个读取文件，但您可以使其并行读取多个文件，并通过传递文件路径列表给构造函数并将`num_parallel_reads`设置为大于1的数字来交错它们的记录。或者，您可以通过使用`list_files()`和`interleave()`来获得与我们之前读取多个CSV文件相同的结果。
- en: Compressed TFRecord Files
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩的TFRecord文件
- en: 'It can sometimes be useful to compress your TFRecord files, especially if they
    need to be loaded via a network connection. You can create a compressed TFRecord
    file by setting the `options` argument:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有时将TFRecord文件压缩可能很有用，特别是如果它们需要通过网络连接加载。您可以通过设置`options`参数创建一个压缩的TFRecord文件：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When reading a compressed TFRecord file, you need to specify the compression
    type:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取压缩的TFRecord文件时，您需要指定压缩类型：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A Brief Introduction to Protocol Buffers
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协议缓冲区简介
- en: 'Even though each record can use any binary format you want, TFRecord files
    usually contain serialized protocol buffers (also called *protobufs*). This is
    a portable, extensible, and efficient binary format developed at Google back in
    2001 and made open source in 2008; protobufs are now widely used, in particular
    in [gRPC](https://grpc.io), Google’s remote procedure call system. They are defined
    using a simple language that looks like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个记录可以使用您想要的任何二进制格式，但TFRecord文件通常包含序列化的协议缓冲区（也称为*protobufs*）。这是一个在2001年由谷歌开发的便携式、可扩展和高效的二进制格式，并于2008年开源；protobufs现在被广泛使用，特别是在[grpc](https://grpc.io)中，谷歌的远程过程调用系统。它们使用一个看起来像这样的简单语言进行定义：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This protobuf definition says we are using version 3 of the protobuf format,
    and it specifies that each `Person` object⁠^([4](ch13.html#idm45720189164144))
    may (optionally) have a `name` of type string, an `id` of type int32, and zero
    or more `email` fields, each of type string. The numbers `1`, `2`, and `3` are
    the field identifiers: they will be used in each record’s binary representation.
    Once you have a definition in a *.proto* file, you can compile it. This requires
    `protoc`, the protobuf compiler, to generate access classes in Python (or some
    other language). Note that the protobuf definitions you will generally use in
    TensorFlow have already been compiled for you, and their Python classes are part
    of the TensorFlow library, so you will not need to use `protoc`. All you need
    to know is how to *use* protobuf access classes in Python. To illustrate the basics,
    let’s look at a simple example that uses the access classes generated for the
    `Person` protobuf (the code is explained in the comments):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个protobuf定义表示我们正在使用protobuf格式的第3版，并且指定每个`Person`对象（可选）可能具有一个字符串类型的`name`、一个int32类型的`id`，以及零个或多个字符串类型的`email`字段。数字`1`、`2`和`3`是字段标识符：它们将在每个记录的二进制表示中使用。一旦你在*.proto*文件中有了一个定义，你就可以编译它。这需要使用protobuf编译器`protoc`在Python（或其他语言）中生成访问类。请注意，你通常在TensorFlow中使用的protobuf定义已经为你编译好了，并且它们的Python类是TensorFlow库的一部分，因此你不需要使用`protoc`。你只需要知道如何在Python中*使用*protobuf访问类。为了说明基础知识，让我们看一个简单的示例，使用为`Person`protobuf生成的访问类（代码在注释中有解释）：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In short, we import the `Person` class generated by `protoc`, we create an instance
    and play with it, visualizing it and reading and writing some fields, then we
    serialize it using the `SerializeToString()` method. This is the binary data that
    is ready to be saved or transmitted over the network. When reading or receiving
    this binary data, we can parse it using the `ParseFromString()` method, and we
    get a copy of the object that was serialized.⁠^([5](ch13.html#idm45720189071424))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们导入由`protoc`生成的`Person`类，创建一个实例并对其进行操作，可视化它并读取和写入一些字段，然后使用`SerializeToString()`方法对其进行序列化。这是准备保存或通过网络传输的二进制数据。当读取或接收这些二进制数据时，我们可以使用`ParseFromString()`方法进行解析，并获得被序列化的对象的副本。
- en: 'You could save the serialized `Person` object to a TFRecord file, then load
    and parse it: everything would work fine. However, `ParseFromString()` is not
    a TensorFlow operation, so you couldn’t use it in a preprocessing function in
    a tf.data pipeline (except by wrapping it in a `tf.py_function()` operation, which
    would make the code slower and less portable, as you saw in [Chapter 12](ch12.html#tensorflow_chapter)).
    However, you could use the `tf.io.decode_proto()` function, which can parse any
    protobuf you want, provided you give it the protobuf definition (see the notebook
    for an example). That said, in practice you will generally want to use instead
    the predefined protobufs for which TensorFlow provides dedicated parsing operations.
    Let’s look at these predefined protobufs now.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将序列化的`Person`对象保存到TFRecord文件中，然后加载和解析它：一切都会正常工作。然而，`ParseFromString()`不是一个TensorFlow操作，所以你不能在tf.data管道中的预处理函数中使用它（除非将其包装在`tf.py_function()`操作中，这会使代码变慢且不太可移植，正如你在[第12章](ch12.html#tensorflow_chapter)中看到的）。然而，你可以使用`tf.io.decode_proto()`函数，它可以解析任何你想要的protobuf，只要你提供protobuf定义（请参考笔记本中的示例）。也就是说，在实践中，你通常会希望使用TensorFlow提供的专用解析操作的预定义protobuf。现在让我们来看看这些预定义的protobuf。
- en: TensorFlow Protobufs
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Protobufs
- en: 'The main protobuf typically used in a TFRecord file is the `Example` protobuf,
    which represents one instance in a dataset. It contains a list of named features,
    where each feature can either be a list of byte strings, a list of floats, or
    a list of integers. Here is the protobuf definition (from TensorFlow’s source
    code):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord文件中通常使用的主要protobuf是`Example`protobuf，它表示数据集中的一个实例。它包含一个命名特征列表，其中每个特征可以是一个字节字符串列表、一个浮点数列表或一个整数列表。以下是protobuf定义（来自TensorFlow源代码）：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The definitions of `BytesList`, `FloatList`, and `Int64List` are straightforward
    enough. Note that `[packed = true]` is used for repeated numerical fields, for
    a more efficient encoding. A `Feature` contains either a `BytesList`, a `FloatList`,
    or an `Int64List`. A `Features` (with an `s`) contains a dictionary that maps
    a feature name to the corresponding feature value. And finally, an `Example` contains
    only a `Features` object.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`BytesList`、`FloatList`和`Int64List`的定义足够简单明了。请注意，对于重复的数值字段，使用`[packed = true]`进行更有效的编码。`Feature`包含一个`BytesList`、一个`FloatList`或一个`Int64List`。一个`Features`（带有`s`）包含一个将特征名称映射到相应特征值的字典。最后，一个`Example`只包含一个`Features`对象。'
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why was `Example` even defined, since it contains no more than a `Features`
    object? Well, TensorFlow’s developers may one day decide to add more fields to
    it. As long as the new `Example` definition still contains the `features` field,
    with the same ID, it will be backward compatible. This extensibility is one of
    the great features of protobufs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会定义`Example`，因为它只包含一个`Features`对象？嗯，TensorFlow的开发人员可能有一天决定向其中添加更多字段。只要新的`Example`定义仍然包含相同ID的`features`字段，它就是向后兼容的。这种可扩展性是protobuf的一个伟大特性。
- en: 'Here is how you could create a `tf.train.Example` representing the same person
    as earlier:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你如何创建一个代表同一个人的`tf.train.Example`：
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The code is a bit verbose and repetitive, but you could easily wrap it inside
    a small helper function. Now that we have an `Example` protobuf, we can serialize
    it by calling its `SerializeToString()` method, then write the resulting data
    to a TFRecord file. Let’s write it five times to pretend we have several contacts:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有点冗长和重复，但你可以很容易地将其包装在一个小的辅助函数中。现在我们有了一个`Example` protobuf，我们可以通过调用其`SerializeToString()`方法将其序列化，然后将生成的数据写入TFRecord文件。让我们假装写入五次，以假装我们有几个联系人：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Normally you would write much more than five `Example`s! Typically, you would
    create a conversion script that reads from your current format (say, CSV files),
    creates an `Example` protobuf for each instance, serializes them, and saves them
    to several TFRecord files, ideally shuffling them in the process. This requires
    a bit of work, so once again make sure it is really necessary (perhaps your pipeline
    works fine with CSV files).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您会写比五个`Example`更多的内容！通常情况下，您会创建一个转换脚本，从当前格式（比如CSV文件）读取数据，为每个实例创建一个`Example`
    protobuf，将它们序列化，并保存到几个TFRecord文件中，最好在此过程中对它们进行洗牌。这需要一些工作，所以再次确保这确实是必要的（也许您的流水线使用CSV文件运行良好）。
- en: Now that we have a nice TFRecord file containing several serialized `Example`s,
    let’s try to load it.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含多个序列化`Example`的漂亮TFRecord文件，让我们尝试加载它。
- en: Loading and Parsing Examples
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和解析示例
- en: 'To load the serialized `Example` protobufs, we will use a `tf.data.TFRecordDataset`
    once again, and we will parse each `Example` using `tf.io.parse_single_example()`.
    It requires at least two arguments: a string scalar tensor containing the serialized
    data, and a description of each feature. The description is a dictionary that
    maps each feature name to either a `tf.io.FixedLenFeature` descriptor indicating
    the feature’s shape, type, and default value, or a `tf.io.VarLenFeature` descriptor
    indicating only the type if the length of the feature’s list may vary (such as
    for the `"emails"` feature).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载序列化的`Example` protobufs，我们将再次使用`tf.data.TFRecordDataset`，并使用`tf.io.parse_single_example()`解析每个`Example`。它至少需要两个参数：包含序列化数据的字符串标量张量，以及每个特征的描述。描述是一个字典，将每个特征名称映射到`tf.io.FixedLenFeature`描述符，指示特征的形状、类型和默认值，或者`tf.io.VarLenFeature`描述符，仅指示特征列表的长度可能变化的类型（例如`"emails"`特征）。
- en: 'The following code defines a description dictionary, then creates a `TFRecordDataset`
    and applies a custom preprocessing function to it to parse each serialized `Example`
    protobuf that this dataset contains:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了一个描述字典，然后创建了一个`TFRecordDataset`，并对其应用了一个自定义预处理函数，以解析该数据集包含的每个序列化`Example`
    protobuf：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The fixed-length features are parsed as regular tensors, but the variable-length
    features are parsed as sparse tensors. You can convert a sparse tensor to a dense
    tensor using `tf.sparse.to_dense()`, but in this case it is simpler to just access
    its values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 固定长度的特征被解析为常规张量，但变长特征被解析为稀疏张量。您可以使用`tf.sparse.to_dense()`将稀疏张量转换为密集张量，但在这种情况下，更简单的方法是直接访问其值：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Instead of parsing examples one by one using `tf.io.parse_single_example()`,
    you may want to parse them batch by batch using `tf.io.parse_example()`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`tf.io.parse_example()`批量解析示例，而不是使用`tf.io.parse_single_example()`逐个解析它们：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Lastly, a `BytesList` can contain any binary data you want, including any serialized
    object. For example, you can use `tf.io.encode_jpeg()` to encode an image using
    the JPEG format and put this binary data in a `BytesList`. Later, when your code
    reads the TFRecord, it will start by parsing the `Example`, then it will need
    to call `tf.io.decode_jpeg()` to parse the data and get the original image (or
    you can use `tf.io.decode_image()`, which can decode any BMP, GIF, JPEG, or PNG
    image). You can also store any tensor you want in a `BytesList` by serializing
    the tensor using `tf.io.serialize_tensor()` then putting the resulting byte string
    in a `BytesList` feature. Later, when you parse the TFRecord, you can parse this
    data using `tf.io.parse_tensor()`. See this chapter’s notebook at [*https://homl.info/colab3*](https://homl.info/colab3)
    for examples of storing images and tensors in a TFRecord file.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`BytesList`可以包含您想要的任何二进制数据，包括任何序列化对象。例如，您可以使用`tf.io.encode_jpeg()`使用JPEG格式对图像进行编码，并将这些二进制数据放入`BytesList`中。稍后，当您的代码读取TFRecord时，它将从解析`Example`开始，然后需要调用`tf.io.decode_jpeg()`来解析数据并获取原始图像（或者您可以使用`tf.io.decode_image()`，它可以解码任何BMP、GIF、JPEG或PNG图像）。您还可以通过使用`tf.io.serialize_tensor()`对张量进行序列化，然后将生成的字节字符串放入`BytesList`特征中，将任何您想要的张量存储在`BytesList`中。稍后，当您解析TFRecord时，您可以使用`tf.io.parse_tensor()`解析这些数据。请参阅本章的笔记本[*https://homl.info/colab3*](https://homl.info/colab3)
    ，了解在TFRecord文件中存储图像和张量的示例。
- en: As you can see, the `Example` protobuf is quite flexible, so it will probably
    be sufficient for most use cases. However, it may be a bit cumbersome to use when
    you are dealing with lists of lists. For example, suppose you want to classify
    text documents. Each document may be represented as a list of sentences, where
    each sentence is represented as a list of words. And perhaps each document also
    has a list of comments, where each comment is represented as a list of words.
    There may be some contextual data too, such as the document’s author, title, and
    publication date. TensorFlow’s `SequenceExample` protobuf is designed for such
    use cases.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`Example` protobuf非常灵活，因此对于大多数用例来说可能已经足够了。但是，当您处理列表列表时，可能会有些繁琐。例如，假设您想对文本文档进行分类。每个文档可以表示为一个句子列表，其中每个句子表示为一个单词列表。也许每个文档还有一个评论列表，其中每个评论表示为一个单词列表。还可能有一些上下文数据，比如文档的作者、标题和发布日期。TensorFlow的`SequenceExample`
    protobuf就是为这种用例而设计的。
- en: Handling Lists of Lists Using the SequenceExample Protobuf
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SequenceExample Protobuf处理列表列表
- en: 'Here is the definition of the `SequenceExample` protobuf:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`SequenceExample` protobuf的定义：
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A `SequenceExample` contains a `Features` object for the contextual data and
    a `FeatureLists` object that contains one or more named `FeatureList` objects
    (e.g., a `FeatureList` named `"content"` and another named `"comments"`). Each
    `FeatureList` contains a list of `Feature` objects, each of which may be a list
    of byte strings, a list of 64-bit integers, or a list of floats (in this example,
    each `Feature` would represent a sentence or a comment, perhaps in the form of
    a list of word identifiers). Building a `SequenceExample`, serializing it, and
    parsing it is similar to building, serializing, and parsing an `Example`, but
    you must use `tf.io.parse_single_sequence_example()` to parse a single `SequenceExample`
    or `tf.io.parse_sequence_example()` to parse a batch. Both functions return a
    tuple containing the context features (as a dictionary) and the feature lists
    (also as a dictionary). If the feature lists contain sequences of varying sizes
    (as in the preceding example), you may want to convert them to ragged tensors
    using `tf.RaggedTensor.from_sparse()` (see the notebook for the full code):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that you know how to efficiently store, load, parse, and preprocess the
    data using the tf.data API, TFRecords, and protobufs, it’s time to turn our attention
    to the Keras preprocessing layers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Keras Preprocessing Layers
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preparing your data for a neural network typically requires normalizing the
    numerical features, encoding the categorical features and text, cropping and resizing
    images, and more. There are several options for this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing can be done ahead of time when preparing your training data
    files, using any tools you like, such as NumPy, Pandas, or Scikit-Learn. You will
    need to apply the exact same preprocessing steps in production, to ensure your
    production model receives preprocessed inputs similar to the ones it was trained
    on.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you can preprocess your data on the fly while loading it with
    tf.data, by applying a preprocessing function to every element of a dataset using
    that dataset’s `map()` method, as we did earlier in this chapter. Again, you will
    need to apply the same preprocessing steps in production.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One last approach is to include preprocessing layers directly inside your model
    so it can preprocess all the input data on the fly during training, then use the
    same preprocessing layers in production. The rest of this chapter will look at
    this last approach.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras offers many preprocessing layers that you can include in your models:
    they can be applied to numerical features, categorical features, images, and text.
    We’ll go over the numerical and categorical features in the next sections, as
    well as basic text preprocessing, and we will cover image preprocessing in [Chapter 14](ch14.html#cnn_chapter)
    and more advanced text preprocessing in [Chapter 16](ch16.html#nlp_chapter).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The Normalization Layer
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 10](ch10.html#ann_chapter), Keras provides a `Normalization`
    layer that we can use to standardize the input features. We can either specify
    the mean and variance of each feature when creating the layer or—more simply—pass
    the training set to the layer’s `adapt()` method before fitting the model, so
    the layer can measure the feature means and variances on its own before training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The data sample passed to the `adapt()` method must be large enough to be representative
    of your dataset, but it does not have to be the full training set: for the `Normalization`
    layer, a few hundred instances randomly sampled from the training set will generally
    be sufficient to get a good estimate of the feature means and variances.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we included the `Normalization` layer inside the model, we can now deploy
    this model to production without having to worry about normalization again: the
    model will just handle it (see [Figure 13-4](#preprocessing_in_model_diagram)).
    Fantastic! This approach completely eliminates the risk of preprocessing mismatch,
    which happens when people try to maintain different preprocessing code for training
    and production but update one and forget to update the other. The production model
    then ends up receiving data preprocessed in a way it doesn’t expect. If they’re
    lucky, they get a clear bug. If not, the model’s accuracy just silently degrades.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1304](assets/mls3_1304.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. Including preprocessing layers inside a model
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Including the preprocessing layer directly in the model is nice and straightforward,
    but it will slow down training (only very slightly in the case of the `Normalization`
    layer): indeed, since preprocessing is performed on the fly during training, it
    happens once per epoch. We can do better by normalizing the whole training set
    just once before training. To do this, we can use the `Normalization` layer in
    a standalone fashion (much like a Scikit-Learn `StandardScaler`):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we can train a model on the scaled data, this time without a `Normalization`
    layer:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Good! This should speed up training a bit. But now the model won’t preprocess
    its inputs when we deploy it to production. To fix this, we just need to create
    a new model that wraps both the adapted `Normalization` layer and the model we
    just trained. We can then deploy this final model to production, and it will take
    care of both preprocessing its inputs and making predictions (see [Figure 13-5](#optimized_preprocessing_in_model_diagram)):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![mls3 1305](assets/mls3_1305.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Preprocessing the data just once before training using preprocessing
    layers, then deploying these layers inside the final model
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we have the best of both worlds: training is fast because we only preprocess
    the data once before training begins, and the final model can preprocess its inputs
    on the fly without any risk of preprocessing mismatch.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the Keras preprocessing layers play nicely with the tf.data API.
    For example, it’s possible to pass a `tf.data.Dataset` to a preprocessing layer’s
    `adapt()` method. It’s also possible to apply a Keras preprocessing layer to a
    `tf.data.Dataset` using the dataset’s `map()` method. For example, here’s how
    you could apply an adapted `Normalization` layer to the input features of each
    batch in a dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Lastly, if you ever need more features than the Keras preprocessing layers
    provide, you can always write your own Keras layer, just like we discussed in
    [Chapter 12](ch12.html#tensorflow_chapter). For example, if the `Normalization`
    layer didn’t exist, you could get a similar result using the following custom
    layer:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, let’s look at another Keras preprocessing layer for numerical features:
    the `Discretization` layer.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The Discretization Layer
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Discretization` layer’s goal is to transform a numerical feature into
    a categorical feature by mapping value ranges (called bins) to categories. This
    is sometimes useful for features with multimodal distributions, or with features
    that have a highly non-linear relationship with the target. For example, the following
    code maps a numerical `age` feature to three categories, less than 18, 18 to 50
    (not included), and 50 or over:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this example, we provided the desired bin boundaries. If you prefer, you
    can instead provide the number of bins you want, then call the layer’s `adapt()`
    method to let it find the appropriate bin boundaries based on the value percentiles.
    For example, if we set `num_bins=3`, then the bin boundaries will be located at
    the values just below the 33rd and 66th percentiles (in this example, at the values
    10 and 37):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Category identifiers such as these should generally not be passed directly to
    a neural network, as their values cannot be meaningfully compared. Instead, they
    should be encoded, for example using one-hot encoding. Let’s look at how to do
    this now.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The CategoryEncoding Layer
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When there are only a few categories (e.g., less than a dozen or two), then
    one-hot encoding is often a good option (as discussed in [Chapter 2](ch02.html#project_chapter)).
    To do this, Keras provides the `CategoryEncoding` layer. For example, let’s one-hot
    encode the `age_​cate⁠gories` feature we just created:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If you try to encode more than one categorical feature at a time (which only
    makes sense if they all use the same categories), the `CategoryEncoding` class
    will perform *multi-hot encoding* by default: the output tensor will contain a
    1 for each category present in *any* input feature. For example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: If you believe it’s useful to know how many times each category occurred, you
    can set `output_mode="count"` when creating the `CategoryEncoding` layer, in which
    case the output tensor will contain the number of occurrences of each category.
    In the preceding example, the output would be the same except for the second row,
    which would become `[0., 0., 2.]`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that both multi-hot encoding and count encoding lose information, since
    it’s not possible to know which feature each active category came from. For example,
    both `[0, 1]` and `[1, 0]` are encoded as `[1., 1., 0.]`. If you want to avoid
    this, then you need to one-hot encode each feature separately and concatenate
    the outputs. This way, `[0, 1]` would get encoded as `[1., 0., 0., 0., 1., 0.]`
    and `[1, 0]` would get encoded as `[0., 1., 0., 1., 0., 0.]`. You can get the
    same result by tweaking the category identifiers so they don’t overlap. For example:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In this output, the first three columns correspond to the first feature, and
    the last three correspond to the second feature. This allows the model to distinguish
    the two features. However, it also increases the number of features fed to the
    model, and thereby requires more model parameters. It’s hard to know in advance
    whether a single multi-hot encoding or a per-feature one-hot encoding will work
    best: it depends on the task, and you may need to test both options.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Now you can encode categorical integer features using one-hot or multi-hot encoding.
    But what about categorical text features? For this, you can use the `StringLookup`
    layer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The StringLookup Layer
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the Keras `StringLookup` layer to one-hot encode a `cities` feature:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We first create a `StringLookup` layer, then we adapt it to the data: it finds
    that there are three distinct categories. Then we use the layer to encode a few
    cities. They are encoded as integers by default. Unknown categories get mapped
    to 0, as is the case for “Montreal” in this example. The known categories are
    numbered starting at 1, from the most frequent category to the least frequent.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Conveniently, if you set `output_mode="one_hot"` when creating the `StringLookup`
    layer, it will output a one-hot vector for each category, instead of an integer:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras also includes an `IntegerLookup` layer that acts much like the `StringLookup`
    layer but takes integers as input, rather than strings.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'If the training set is very large, it may be convenient to adapt the layer
    to just a random subset of the training set. In this case, the layer’s `adapt()`
    method may miss some of the rarer categories. By default, it would then map them
    all to category 0, making them indistinguishable by the model. To reduce this
    risk (while still adapting the layer only on a subset of the training set), you
    can set `num_oov_indices` to an integer greater than 1\. This is the number of
    out-of-vocabulary (OOV) buckets to use: each unknown category will get mapped
    pseudorandomly to one of the OOV buckets, using a hash function modulo the number
    of OOV buckets. This will allow the model to distinguish at least some of the
    rare categories. For example:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Since there are five OOV buckets, the first known category’s ID is now 5 (`"Paris"`).
    But `"Foo"`, `"Bar"`, and `"Baz"` are unknown, so they each get mapped to one
    of the OOV buckets. `"Bar"` gets its own dedicated bucket (with ID 3), but sadly
    `"Foo"` and `"Baz"` happen to be mapped to the same bucket (with ID 4), so they
    remain indistinguishable by the model. This is called a *hashing collision*. The
    only way to reduce the risk of collision is to increase the number of OOV buckets.
    However, this will also increase the total number of categories, which will require
    more RAM and extra model parameters once the categories are one-hot encoded. So,
    don’t increase that number too much.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea of mapping categories pseudorandomly to buckets is called the *hashing
    trick*. Keras provides a dedicated layer which does just that: the `Hashing` layer.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The Hashing Layer
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each category, the Keras `Hashing` layer computes a hash, modulo the number
    of buckets (or “bins”). The mapping is entirely pseudorandom, but stable across
    runs and platforms (i.e., the same category will always be mapped to the same
    integer, as long as the number of bins is unchanged). For example, let’s use the
    `Hashing` layer to encode a few cities:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The benefit of this layer is that it does not need to be adapted at all, which
    may sometimes be useful, especially in an out-of-core setting (when the dataset
    is too large to fit in memory). However, we once again get a hashing collision:
    “Tokyo” and “Montreal” are mapped to the same ID, making them indistinguishable
    by the model. So, it’s usually preferable to stick to the `StringLookup` layer.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now look at another way to encode categories: trainable embeddings.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Encoding Categorical Features Using Embeddings
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An embedding is a dense representation of some higher-dimensional data, such
    as a category, or a word in a vocabulary. If there are 50,000 possible categories,
    then one-hot encoding would produce a 50,000-dimensional sparse vector (i.e.,
    containing mostly zeros). In contrast, an embedding would be a comparatively small
    dense vector; for example, with just 100 dimensions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, embeddings are usually initialized randomly, and they are
    then trained by gradient descent, along with the other model parameters. For example,
    the `"NEAR BAY"` category in the California housing dataset could be represented
    initially by a random vector such as `[0.131, 0.890]`, while the `"NEAR OCEAN"`
    category might be represented by another random vector such as `[0.631, 0.791]`.
    In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter
    you can tweak.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Since these embeddings are trainable, they will gradually improve during training;
    and as they represent fairly similar categories in this case, gradient descent
    will certainly end up pushing them closer together, while it will tend to move
    them away from the `"INLAND"` category’s embedding (see [Figure 13-6](#embedding_diagram)).
    Indeed, the better the representation, the easier it will be for the neural network
    to make accurate predictions, so training tends to make embeddings useful representations
    of the categories. This is called *representation learning* (you will see other
    types of representation learning in [Chapter 17](ch17.html#autoencoders_chapter)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1306](assets/mls3_1306.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Embeddings will gradually improve during training
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Keras provides an `Embedding` layer, which wraps an *embedding matrix*: this
    matrix has one row per category and one column per embedding dimension. By default,
    it is initialized randomly. To convert a category ID to an embedding, the `Embedding`
    layer just looks up and returns the row that corresponds to that category. That’s
    all there is to it! For example, let’s initialize an `Embedding` layer with five
    rows and 2D embeddings, and use it to encode some categories:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As you can see, category 2 gets encoded (twice) as the 2D vector `[-0.04663396,
    0.01846724]`, while category 4 gets encoded as `[-0.02736737, -0.02768031]`. Since
    the layer is not trained yet, these encodings are just random.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An `Embedding` layer is initialized randomly, so it does not make sense to use
    it outside of a model as a standalone preprocessing layer unless you initialize
    it with pretrained weights.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to embed a categorical text attribute, you can simply chain a `StringLookup`
    layer and an `Embedding` layer, like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Note that the number of rows in the embedding matrix needs to be equal to the
    vocabulary size: that’s the total number of categories, including the known categories
    plus the OOV buckets (just one by default). The `vocabulary_size()` method of
    the `StringLookup` class conveniently returns this number.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example we used 2D embeddings, but as a rule of thumb embeddings typically
    have 10 to 300 dimensions, depending on the task, the vocabulary size, and the
    size of your training set. You will have to tune this hyperparameter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting everything together, we can now create a Keras model that can process
    a categorical text feature along with regular numerical features and learn an
    embedding for each category (as well as for each OOV bucket):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This model takes two inputs: `num_input`, which contains eight numerical features
    per instance, plus `cat_input`, which contains a single categorical text input
    per instance. The model uses the `lookup_and_embed` model we created earlier to
    encode each ocean-proximity category as the corresponding trainable embedding.
    Next, it concatenates the numerical inputs and the embeddings using the `concatenate()`
    function to produce the complete encoded inputs, which are ready to be fed to
    a neural network. We could add any kind of neural network at this point, but for
    simplicity we just add a single dense output layer, and then we create the Keras
    `Model` with the inputs and output we’ve just defined. Next we compile the model
    and train it, passing both the numerical and categorical inputs.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'As you saw in [Chapter 10](ch10.html#ann_chapter), since the `Input` layers
    are named `"num"` and `"cat"`, we could also have passed the training data to
    the `fit()` method using a dictionary instead of a tuple: `{"num": X_train_num,
    "cat": X_train_cat}`. Alternatively, we could have passed a `tf.data.Dataset`
    containing batches, each represented as `((X_batch_num, X_batch_cat), y_batch)`
    or as `({"num": X_batch_num, "cat": X_batch_cat}, y_batch)`. And of course the
    same goes for the validation data.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One-hot encoding followed by a `Dense` layer (with no activation function and
    no biases) is equivalent to an `Embedding` layer. However, the `Embedding` layer
    uses way fewer computations as it avoids many multiplications by zero—the performance
    difference becomes clear when the size of the embedding matrix grows. The `Dense`
    layer’s weight matrix plays the role of the embedding matrix. For example, using
    one-hot vectors of size 20 and a `Dense` layer with 10 units is equivalent to
    using an `Embedding` layer with `input_dim=20` and `output_dim=10`. As a result,
    it would be wasteful to use more embedding dimensions than the number of units
    in the layer that follows the `Embedding` layer.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: OK, now that you have learned how to encode categorical features, it’s time
    to turn our attention to text preprocessing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Text Preprocessing
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras provides a `TextVectorization` layer for basic text preprocessing. Much
    like the `StringLookup` layer, you must either pass it a vocabulary upon creation,
    or let it learn the vocabulary from some training data using the `adapt()` method.
    Let’s look at an example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The two sentences “Be good!” and “Question: be or be?” were encoded as `[2,
    1, 0, 0]` and `[6, 2, 1, 2]`, respectively. The vocabulary was learned from the
    four sentences in the training data: “be” = 2, “to” = 3, etc. To construct the
    vocabulary, the `adapt()` method first converted the training sentences to lowercase
    and removed punctuation, which is why “Be”, “be”, and “be?” are all encoded as
    “be” = 2\. Next, the sentences were split on whitespace, and the resulting words
    were sorted by descending frequency, producing the final vocabulary. When encoding
    sentences, unknown words get encoded as 1s. Lastly, since the first sentence is
    shorter than the second, it was padded with 0s.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `TextVectorization` layer has many options. For example, you can preserve
    the case and punctuation if you want, by setting `standardize=None`, or you can
    pass any standardization function you please as the `standardize` argument. You
    can prevent splitting by setting `split=None`, or you can pass your own splitting
    function instead. You can set the `output_sequence_length` argument to ensure
    that the output sequences all get cropped or padded to the desired length, or
    you can set `ragged=True` to get a ragged tensor instead of a regular tensor.
    Please check out the documentation for more options.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'The word IDs must be encoded, typically using an `Embedding` layer: we will
    do this in [Chapter 16](ch16.html#nlp_chapter). Alternatively, you can set the
    `TextVectorization` layer’s `output_mode` argument to `"multi_hot"` or `"count"`
    to get the corresponding encodings. However, simply counting words is usually
    not ideal: words like “to” and “the” are so frequent that they hardly matter at
    all, whereas, rarer words such as “basketball” are much more informative. So,
    rather than setting `output_mode` to `"multi_hot"` or `"count"`, it is usually
    preferable to set it to `"tf_idf"`, which stands for *term-frequency* × *inverse-document-frequency*
    (TF-IDF). This is similar to the count encoding, but words that occur frequently
    in the training data are downweighted, and conversely, rare words are upweighted.
    For example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are many TF-IDF variants, but the way the `TextVectorization` layer implements
    it is by multiplying each word count by a weight equal to log(1 + *d* / (*f* +
    1)), where *d* is the total number of sentences (a.k.a., documents) in the training
    data and *f* counts how many of these training sentences contain the given word.
    For example, in this case there are *d* = 4 sentences in the training data, and
    the word “be” appears in *f* = 3 of these. Since the word “be” occurs twice in
    the sentence “Question: be or be?”, it gets encoded as 2 × log(1 + 4 / (1 + 3))
    ≈ 1.3862944\. The word “question” only appears once, but since it is a less common
    word, its encoding is almost as high: 1 × log(1 + 4 / (1 + 1)) ≈ 1.0986123\. Note
    that the average weight is used for unknown words.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach to text encoding is straightforward to use and it can give fairly
    good results for basic natural language processing tasks, but it has several important
    limitations: it only works with languages that separate words with spaces, it
    doesn’t distinguish between homonyms (e.g., “to bear” versus “teddy bear”), it
    gives no hint to your model that words like “evolution” and “evolutionary” are
    related, etc. And if you use multi-hot, count, or TF-IDF encoding, then the order
    of the words is lost. So what are the other options?'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: One option is to use the [TensorFlow Text library](https://tensorflow.org/text),
    which provides more advanced text preprocessing features than the `TextVectorization`
    layer. For example, it includes several subword tokenizers capable of splitting
    the text into tokens smaller than words, which makes it possible for the model
    to more easily detect that “evolution” and “evolutionary” have something in common
    (more on subword tokenization in [Chapter 16](ch16.html#nlp_chapter)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Yet another option is to use pretrained language model components. Let’s look
    at this now.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Using Pretrained Language Model Components
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [TensorFlow Hub library](https://tensorflow.org/hub) makes it easy to reuse
    pretrained model components in your own models, for text, image, audio, and more.
    These model components are called *modules*. Simply browse the [TF Hub repository](https://tfhub.dev),
    find the one you need, and copy the code example into your project, and the module
    will be automatically downloaded and bundled into a Keras layer that you can directly
    include in your model. Modules typically contain both preprocessing code and pretrained
    weights, and they generally require no extra training (but of course, the rest
    of your model will certainly require training).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: For example, some powerful pretrained language models are available. The most
    powerful are quite large (several gigabytes), so for a quick example let’s use
    the `nnlm-en-dim50` module, version 2, which is a fairly basic module that takes
    raw text as input and outputs 50-dimensional sentence embeddings. We’ll import
    TensorFlow Hub and use it to load the module, then use that module to encode two
    sentences to vectors:^([8](ch13.html#idm45720186424416))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `hub.KerasLayer` layer downloads the module from the given URL. This particular
    module is a *sentence encoder*: it takes strings as input and encodes each one
    as a single vector (in this case, a 50-dimensional vector). Internally, it parses
    the string (splitting words on spaces) and embeds each word using an embedding
    matrix that was pretrained on a huge corpus: the Google News 7B corpus (seven
    billion words long!). Then it computes the mean of all the word embeddings, and
    the result is the sentence embedding.⁠^([9](ch13.html#idm45720186382240))'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: You just need to include this `hub_layer` in your model, and you’re ready to
    go. Note that this particular language model was trained on the English language,
    but many other languages are available, as well as multilingual models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, the excellent open source [Transformers library by Hugging
    Face](https://huggingface.co/docs/transformers) also makes it easy to include
    powerful language model components inside your own models. You can browse the
    [Hugging Face Hub](https://huggingface.co/models), choose the model you want,
    and use the provided code examples to get started. It used to contain only language
    models, but it has now expanded to include image models and more.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to natural language processing in more depth in [Chapter 16](ch16.html#nlp_chapter).
    Let’s now look at Keras’s image preprocessing layers.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Image Preprocessing Layers
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Keras preprocessing API includes three image preprocessing layers:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras.layers.Resizing` resizes the input images to the desired size. For
    example, `Resizing(height=100, width=200)` resizes each image to 100 × 200, possibly
    distorting the image. If you set `crop_to_aspect_ratio=True`, then the image will
    be cropped to the target image ratio, to avoid distortion.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.keras.layers.Rescaling` rescales the pixel values. For example, `Rescal⁠ing​(scale=2/255,
    offset=-1)` scales the values from 0 → 255 to –1 → 1.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.keras.layers.CenterCrop` crops the image, keeping only a center patch of
    the desired height and width.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s load a couple of sample images and center-crop them. For
    this, we will use Scikit-Learn’s `load_sample_images()` function; this loads two
    color images, one of a Chinese temple and the other of a flower (this requires
    the Pillow library, which should already be installed if you are using Colab or
    if you followed the installation instructions):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Keras also includes several layers for data augmentation, such as `RandomCrop`,
    `RandomFlip`, `RandomTranslation`, `RandomRotation`, `RandomZoom`, `RandomHeight`,
    `RandomWidth`, and `RandomContrast`. These layers are only active during training,
    and they randomly apply some transformation to the input images (their names are
    self-explanatory). Data augmentation will artificially increase the size of the
    training set, which often leads to improved performance, as long as the transformed
    images look like realistic (nonaugmented) images. We’ll cover image processing
    more closely in the next chapter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under the hood, the Keras preprocessing layers are based on TensorFlow’s low-level
    API. For example, the `Normalization` layer uses `tf.nn.moments()` to compute
    both the mean and variance, the `Discretization` layer uses `tf.raw_ops.Bucketize()`,
    `CategoricalEncoding` uses `tf.math.bincount()`, `IntegerLookup` and `StringLookup`
    use the `tf.lookup` package, `Hashing` and `TextVectorization` use several ops
    from the `tf.strings` package, `Embedding` uses `tf.nn.embedding_lookup()`, and
    the image preprocessing layers use the ops from the `tf.image` package. If the
    Keras preprocessing API isn’t sufficient for your needs, you may occasionally
    need to use TensorFlow’s low-level API directly.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at another way to load data easily and efficiently in TensorFlow.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Datasets Project
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [TensorFlow Datasets (TFDS)](https://tensorflow.org/datasets) project makes
    it very easy to load common datasets, from small ones like MNIST or Fashion MNIST
    to huge datasets like ImageNet (you will need quite a bit of disk space!). The
    list includes image datasets, text datasets (including translation datasets),
    audio and video datasets, time series, and much more. You can visit [*https://homl.info/tfds*](https://homl.info/tfds)
    to view the full list, along with a description of each dataset. You can also
    check out [Know Your Data](https://knowyourdata.withgoogle.com), which is a tool
    to explore and understand many of the datasets provided by TFDS.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'TFDS is not bundled with TensorFlow, but if you are running on Colab or if
    you followed the installation instructions at [*https://homl.info/install*](https://homl.info/install),
    then it’s already installed. You can then import `tensorflow_datasets`, usually
    as `tfds`, then call the `tfds.load()` function, which will download the data
    you want (unless it was already downloaded earlier) and return the data as a dictionary
    of datasets (typically one for training and one for testing, but this depends
    on the dataset you choose). For example, let’s download MNIST:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You can then apply any transformation you want (typically shuffling, batching,
    and prefetching), and you’re ready to train your model. Here is a simple example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `load()` function can shuffle the files it downloads: just set `shuffle_files=True`.
    However, this may be insufficient, so it’s best to shuffle the training data some
    more.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that each item in the dataset is a dictionary containing both the features
    and the labels. But Keras expects each item to be a tuple containing two elements
    (again, the features and the labels). You could transform the dataset using the
    `map()` method, like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: But it’s simpler to ask the `load()` function to do this for you by setting
    `as_supervised=True` (obviously this works only for labeled datasets).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, TFDS provides a convenient way to split the data using the `split`
    argument. For example, if you want to use the first 90% of the training set for
    training, the remaining 10% for validation, and the whole test set for testing,
    then you can set `split=["train[:90%]", "train[90%:]", "test"]`. The `load()`
    function will return all three sets. Here is a complete example, loading and splitting
    the MNIST dataset using TFDS, then using these sets to train and evaluate a simple
    Keras model:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Congratulations, you’ve reached the end of this quite technical chapter! You
    may feel that it is a bit far from the abstract beauty of neural networks, but
    the fact is deep learning often involves large amounts of data, and knowing how
    to load, parse, and preprocess it efficiently is a crucial skill to have. In the
    next chapter, we will look at convolutional neural networks, which are among the
    most successful neural net architectures for image processing and many other applications.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why would you want to use the tf.data API?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the benefits of splitting a large dataset into multiple files?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, how can you tell that your input pipeline is the bottleneck?
    What can you do to fix it?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you save any binary data to a TFRecord file, or only serialized protocol
    buffers?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you go through the hassle of converting all your data to the `Example`
    protobuf format? Why not use your own protobuf definition?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using TFRecords, when would you want to activate compression? Why not do
    it systematically?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data can be preprocessed directly when writing the data files, or within the
    tf.data pipeline, or in preprocessing layers within your model. Can you list a
    few pros and cons of each option?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name a few common ways you can encode categorical integer features. What about
    text?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the Fashion MNIST dataset (introduced in [Chapter 10](ch10.html#ann_chapter));
    split it into a training set, a validation set, and a test set; shuffle the training
    set; and save each dataset to multiple TFRecord files. Each record should be a
    serialized `Example` protobuf with two features: the serialized image (use `tf.io.serialize_tensor()`
    to serialize each image), and the label.⁠^([10](ch13.html#idm45720185796112))
    Then use tf.data to create an efficient dataset for each set. Finally, use a Keras
    model to train these datasets, including a preprocessing layer to standardize
    each input feature. Try to make the input pipeline as efficient as possible, using
    TensorBoard to visualize profiling data.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this exercise you will download a dataset, split it, create a `tf.data.Dataset`
    to load it and preprocess it efficiently, then build and train a binary classification
    model containing an `Embedding` layer:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains
    50,000 movie reviews from the [Internet Movie Database (IMDb)](https://imdb.com).
    The data is organized in two directories, *train* and *test*, each containing
    a *pos* subdirectory with 12,500 positive reviews and a *neg* subdirectory with
    12,500 negative reviews. Each review is stored in a separate text file. There
    are other files and folders (including preprocessed bag-of-words versions), but
    we will ignore them in this exercise.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the test set into a validation set (15,000) and a test set (10,000).
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use tf.data to create an efficient dataset for each set.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a binary classification model, using a `TextVectorization` layer to preprocess
    each review.
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an `Embedding` layer and compute the mean embedding for each review, multiplied
    by the square root of the number of words (see [Chapter 16](ch16.html#nlp_chapter)).
    This rescaled mean embedding can then be passed to the rest of your model.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model and see what accuracy you get. Try to optimize your pipelines
    to make training as fast as possible.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use TFDS to load the same dataset more easily: `tfds.load("imdb_reviews")`.'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch13.html#idm45720190533488-marker)) Imagine a sorted deck of cards on
    your left: suppose you just take the top three cards and shuffle them, then pick
    one randomly and put it to your right, keeping the other two in your hands. Take
    another card on your left, shuffle the three cards in your hands and pick one
    of them randomly, and put it on your right. When you are done going through all
    the cards like this, you will have a deck of cards on your right: do you think
    it will be perfectly shuffled?'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch13.html#idm45720189926208-marker)) In general, just prefetching one
    batch is fine, but in some cases you may need to prefetch a few more. Alternatively,
    you can let TensorFlow decide automatically by passing `tf.data.AUTOTUNE` to `prefetch()`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch13.html#idm45720189876688-marker)) But check out the experimental `tf.data.experimental.prefetch_to_device()`
    function, which can prefetch data directly to the GPU. Any TensorFlow function
    or class with `experimental` in its name may change without warning in future
    versions. If an experimental function fails, try removing the word `experimental`:
    it may have been moved to the core API. If not, then please check the notebook,
    as I will ensure it contains up-to-date code.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch13.html#idm45720189164144-marker)) Since protobuf objects are meant
    to be serialized and transmitted, they are called *messages*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch13.html#idm45720189071424-marker)) This chapter contains the bare minimum
    you need to know about protobufs to use TFRecords. To learn more about protobufs,
    please visit [*https://homl.info/protobuf*](https://homl.info/protobuf).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch13.html#idm45720187048176-marker)) Tomáš Mikolov et al., “Distributed
    Representations of Words and Phrases and Their Compositionality”, *Proceedings
    of the 26th International Conference on Neural Information Processing Systems*
    2 (2013): 3111–3119.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch13.html#idm45720187037120-marker)) Malvina Nissim et al., “Fair Is
    Better Than Sensational: Man Is to Doctor as Woman Is to Doctor”, arXiv preprint
    arXiv:1905.09866 (2019).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch13.html#idm45720186424416-marker)) TensorFlow Hub is not bundled with
    TensorFlow, but if you are running on Colab or if you followed the installation
    instructions at [*https://homl.info/install*](https://homl.info/install), then
    it’s already installed.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch13.html#idm45720186382240-marker)) To be precise, the sentence embedding
    is equal to the mean word embedding multiplied by the square root of the number
    of words in the sentence. This compensates for the fact that the mean of *n* random
    vectors gets shorter as *n* grows.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch13.html#idm45720185796112-marker)) For large images, you could use
    `tf.io.encode_jpeg()` instead. This would save a lot of space, but it would lose
    a bit of image quality.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
