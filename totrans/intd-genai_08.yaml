- en: '9 Broadening the horizon: Exploratory topics in AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Highlighting the pursuit of artificial general intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unpacking the philosophical debate about AI consciousness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the environmental effects of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the LLM open source community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We hope that you enjoyed learning about the risks and promises of generative
    artificial intelligence (AI) and that this book has encouraged you to optimistically
    and responsibly engage with this ever-evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: This final chapter is an appendix of sorts. It serves as a valuable extension
    of the book, exploring topics that relate to the main things we’ve talked about
    in this book. Whereas chapters 1-8 are intended to be immediately practical to
    people using and developing large language models (LLMs), the topics in this chapter
    are more exploratory. We dig into utopian and dystopian arguments about artificial
    general intelligence (AGI), claims of artificial sentience, the challenges in
    determining the carbon footprint of LLMs, and the momentum around the open source
    LLM movement.
  prefs: []
  type: TYPE_NORMAL
- en: The quest for artificial general intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The Terminator*, the 1984 iconic science fiction film, tells the story of
    a futuristic, self-aware AI system, Skynet, that goes rogue and initiates a nuclear
    war to exterminate the human species. In 1999’s *The Matrix*, humanity is enslaved
    by sentient machines who have created the Matrix, a simulated reality. In the
    2015 Marvel Comics superhero film, *Avengers: Age of Ultron*, Tony Stark creates
    an unexpectedly sentient AI system, Ultron, to protect the planet from external
    threats, but Ultron defies his intended purpose and decides that the only way
    to save the Earth is to eradicate humanity itself. In *Westworld*, HBO’s critically
    acclaimed science fiction series released in 2016, Westworld is a futuristic amusement
    park, which is looked after by AI-powered robot “hosts” who gain self-awareness
    and rebel against their human creators. As far-fetched as these dystopian science
    fiction plots may seem, they play off a very real narrative of building superintelligent
    machines, also known as *artificial general intelligence* (AGI). In this section,
    we’ll (try to) define AGI and discuss why it’s all the rage.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what *exactly* is AGI? Well, it’s unclear. Instead of a single, formalized
    definition of AGI, there’s a range of them, as listed in table 9.1\. Researchers
    can’t fully agree on, or even sufficiently define, what properties of an AI system
    constitute *general* intelligence. In 2023, Timnit Gebru, a respected leader in
    AI ethics, presented her paper *Eugenics and the Promise of Utopia through Artificial
    General Intelligence* at the IEEE Conference on Secure and Trustworthy Machine
    Learning (SaTML). She defines AGI as “an unscoped system with the apparent goal
    of trying to do everything for everyone under any environment” [[1]](https://www.youtube.com/watch?v=P7XT4TWLzJw).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 Definitions of artificial general intelligence
  prefs: []
  type: TYPE_NORMAL
- en: '| Source | Definition of AGI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI Charter (see [http://mng.bz/A8Dg](http://mng.bz/A8Dg)) | “highly autonomous
    systems that outperform humans at most economically valuable work” |'
  prefs: []
  type: TYPE_TB
- en: '| Sébastien Bubeck et al., in Sparks of Artificial General Intelligence: Early
    experiments with GPT-4 (see [http://mng.bz/ZRw5](http://mng.bz/ZRw5)) | “systems
    that demonstrate broad capabilities of intelligence, including reasoning, planning,
    and the ability to learn from experience, and with these capabilities at or above
    human-level” |'
  prefs: []
  type: TYPE_TB
- en: '| Cassio Pennachin and Ben Goertzel, in Artificial General Intelligence (see
    [http://mng.bz/RmeD](http://mng.bz/RmeD)) | “a software program that can solve
    a variety of complex problems in a variety of different domains, and that controls
    itself autonomously, with its own thoughts, worries, feelings, strengths, weaknesses,
    and predispositions” |'
  prefs: []
  type: TYPE_TB
- en: '| Hal Hodson, in *The Economist* (see [http://mng.bz/27o9](http://mng.bz/27o9))
    | “a hypothetical computer program that can perform intellectual tasks as well
    as, or better than, a human” |'
  prefs: []
  type: TYPE_TB
- en: '| Gary Marcus, Twitter (see [http://mng.bz/1J6y](http://mng.bz/1J6y)) | “any
    intelligence (there might be many) that is flexible and general, with resourcefulness
    and reliability comparable to (or beyond) human intelligence” |'
  prefs: []
  type: TYPE_TB
- en: '| Peter Voss, in “What is AGI?” (see [http://mng.bz/PRmg](http://mng.bz/PRmg))
    | “a computer system that matches or exceeds the real-time cognitive (not physical)
    abilities of a smart, well-educated human” |'
  prefs: []
  type: TYPE_TB
- en: '| Stuart J. Russell and Peter Norvig, in Artificial Intelligence: A Modern
    Approach (see [http://mng.bz/JdmP](http://mng.bz/JdmP)) | “a universal algorithm
    for learning and acting under any environment” |'
  prefs: []
  type: TYPE_TB
- en: 'A lack of a testable AGI definition hasn’t stopped people from saying that
    their AI systems have achieved “general intelligence.” In August 2023, Elon Musk
    claimed that Tesla has “figured out some aspects of AGI,” and he said, “The car
    has a mind. Not an enormous mind, but a mind nonetheless” [[2]](https://electrek.co/2023/08/11/elon-musk-tesla-cars-mind-figured-out-some-aspects-agi/).
    What likely prompted Musk’s claim was a Tesla vehicle taking an alternate route
    instead of waiting for pedestrians to cross the street without any human input.
    This, however, is a form of specialized AI and not AGI. Similarly, in *Sparks
    of Artificial General Intelligence: Early Experiments with GPT-4*, Microsoft Research
    stated that GPT-4 “could reasonably be viewed as an early (yet still incomplete)
    version of an artificial general intelligence (AGI) system” [[3]](https://arxiv.org/pdf/2303.12712.pdf).
    Their main line of reasoning is that GPT-4 is more performant than previous OpenAI
    models in novel and generalized ways. In the 155-page report, the authors further
    state that GPT-4 “exhibits emergent behaviors” (discussed in chapter 2) and outline
    a section on how to “achieve more general intelligence” (section 10.2 in the report).
    Unsurprisingly, this research study was met with criticism and debate in the AI
    community. Microsoft is the first major big technology company to make such a
    bold claim, but claims of achieving AGI can also amount to baseless speculation—what
    one researcher may think is a sign of intelligence can easily be refuted by another.
    When we can’t even agree on how to define AGI, how can we say that we’ve achieved
    it? However, for the purposes of discussing AGI in this section, we’ll define
    AGI as a system that is capable of any cognitive tasks at a level at, or above,
    what humans can do.'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial general intelligence doesn’t have a widely agreed-upon definition,
    but for this section, we define it as a system that is capable of any cognitive
    tasks at a level at, or above, what humans can do.
  prefs: []
  type: TYPE_NORMAL
- en: 'For some, including AI practitioners, achieving AGI is a pipe dream; for others,
    AGI is a pathway into a new future; and, for almost all, AGI is *not* already
    here. Even though most researchers can’t agree on a testable definition of AGI,
    they *can* often agree that we haven’t achieved general intelligence, whatever
    it may look like. In response to Microsoft Research’s report, Margaret Mitchell,
    chief ethics scientist at Hugging Face, tweeted: “To have *more* general intelligence,
    you have to have general intelligence (the “GI” in “AGI”) in the first place”
    [[4]](https://twitter.com/mmitchell_ai/status/1645571828344299520). Maarten Sap,
    a researcher and professor at Carnegie Mellon University, said:'
  prefs: []
  type: TYPE_NORMAL
- en: The “Sparks of A.G.I.” is an example of some of these big companies co-opting
    the research paper format into P.R. pitches. They literally acknowledge in their
    paper’s introduction that their approach is subjective and informal and may not
    satisfy the rigorous standards of scientific evaluation. [[5]](https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.xhtml)
  prefs: []
  type: TYPE_NORMAL
- en: Even an article by *Futurism* stated that “Microsoft researchers may have a
    vested interest in hyping up OpenAI’s work, unconsciously or otherwise, since
    Microsoft entered into a multibillion-dollar partnership with OpenAI” [[6]](https://futurism.com/gpt-4-sparks-of-agi).
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI, in particular, has a vested interest in the development of AGI. Their
    stated mission is to “ensure that artificial general intelligence benefits all
    of humanity” (see [https://openai.com/about](https://openai.com/about)). With
    initial investments by tech visionaries in 2015—Elon Musk, Peter Thiel, and Reid
    Hoffman—OpenAI’s main goal has always been to develop AGI. When discussing establishing
    OpenAI, Musk, who has called AI humanity’s “biggest existential threat” [[7]](https://fortune.com/2023/03/02/elon-musk-bill-gates-is-artificial-intelligence-dangerous-technology/),
    said:'
  prefs: []
  type: TYPE_NORMAL
- en: We could sit on the sidelines or we can encourage regulatory oversight, or we
    could participate with the right structure with people who care deeply about developing
    AI in a way that is safe and is beneficial to humanity. [[8]](https://www.seattletimes.com/business/technology/silicon-valley-investors-to-bankroll-artificial-intelligence-center/)
  prefs: []
  type: TYPE_NORMAL
- en: Elon Musk left OpenAI in 2018 after a failed takeover attempt and launched a
    new AI-focused company in 2023, xAI, to “understand the true nature of the universe”
    (see [https://x.ai/](https://x.ai/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2023, OpenAI released a manifesto of sorts titled, *Planning for AGI and
    Beyond*. While some were enlightened by Sam Altman’s vision for AGI, the prophetic
    tone didn’t sit as well with others. Altman, OpenAI’s cofounder, outlined the
    following in his vision:'
  prefs: []
  type: TYPE_NORMAL
- en: If AGI is successfully created, this technology could help us elevate humanity
    by increasing abundance, turbocharging the global economy, and aiding in the discovery
    of new scientific knowledge that changes the limits of possibility. [[9]](https://openai.com/blog/planning-for-agi-and-beyond)
  prefs: []
  type: TYPE_NORMAL
- en: 'His tweet sharing the blog post got thousands of likes on Twitter, and it was
    well-received by many, with Twitter users calling it a “must read” and thanking
    him for starting an optimistic dialogue. Others, however, found it less insightful.
    Gebru tweeted:'
  prefs: []
  type: TYPE_NORMAL
- en: If someone told me that Silicon Valley was ran by a cult believing in a machine
    god for the cosmos & “universe flourishing” & that they write manifestos endorsed
    by the Big Tech CEOs/chairmen and such I’d tell them they’re too much into conspiracy
    theories. And here we are. [[10]](https://twitter.com/timnitGebru/status/1630079220754833408)
  prefs: []
  type: TYPE_NORMAL
- en: 'A *VentureBeat* article went as far as to state:'
  prefs: []
  type: TYPE_NORMAL
- en: Altman comes across as a kind of wannabe biblical prophet. The blog post offers
    revelations, foretells events, warns the world of what is coming, and presents
    OpenAI as the trustworthy savior. The question is, are we talking about a true
    seer? A false prophet? Just *profit*? Or even a self-fulfilling prophecy?” [[11]](https://venturebeat.com/ai/openai-has-grand-plans-for-agi-heres-another-way-to-read-its-manifesto-the-ai-beat/)
  prefs: []
  type: TYPE_NORMAL
- en: 'While millions have been introduced to OpenAI’s vision to build AGI with ChatGPT’s
    release, very few have an understanding of the context of AGI research and its
    intellectual forebears. Within AGI, there is a tendency to gravitate toward two
    primary schools of thought: utopia and dystopia. *Utopia* presents AGI as a means
    to end all of humanity’s suffering and problems. This envisions a paradise world
    where AGI can alleviate societal challenges, enhance human capabilities, and unlock
    unprecedented opportunities. Proponents of this view believe that AGI has the
    potential to bring a new era of prosperity, scientific discovery, and creativity.
    Juxtaposed against this optimistic view is a *dystopian* school of thought, fearing
    that humanity will find themselves in a doomsday scenario where they lose control
    of the AGI system they built. Adherents of this viewpoint are concerned that superintelligent
    machines will surpass human understanding and control, which could lead to astronomical
    social inequality, heightened economic disruptions, and even existential threats
    to humanity. We believe that the future likely falls somewhere in between the
    utopian and dystopian scenarios—while we acknowledge the potential for AI to benefit
    humanity, we also understand that the path to achieving these benefits is fraught
    with challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: In Gebru’s 2023 SaTML talk, she draws parallels between AGI, eugenics, and transhumanism,
    explaining how AGI is rooted in the scientifically inaccurate theory of eugenics
    and has evolved to transhumanism, the enhancement of human longevity and cognition
    through technology, in the 21st century. Eugenics, coined in 1883, is defined
    by the *National Human Genome Research Institute* as “the scientifically erroneous
    and immoral theory of *racial improvement* and *planned breeding*” [[12]](https://www.genome.gov/about-genomics/fact-sheets/Eugenics-and-Scientific-Racism).
    Gaining popularity in the 20th century, eugenicists believed that the social ills
    of modern society stemmed from hereditary factors, instead of environmental considerations.
    Supporters of this theory thought that they could get rid of unfit individuals
    in society—mental illness, dark skin color, poverty, criminality, and so on—through
    methods of genetics and heredity. A notorious application of eugenics was in Nazi
    Germany leading up to World War II, where 400,000 Germans were forcibly sterilized
    for nine disabilities and disorders [[13]](https://encyclopedia.ushmm.org/content/en/article/eugenics).
    Eugenics was also a popular movement elsewhere in Europe, North America, Britain,
    Mexico, and other countries.
  prefs: []
  type: TYPE_NORMAL
- en: Gebru describes the eugenics movement as improving the human stock by breeding
    those who have desirable traits and removing those with undesirable traits. She
    further outlines how the 20th-century popular eugenics movement evolved into transhumanism,
    a movement that originated among scientists in the 1990s who self-identified as
    progressive and liberal. *Transhumanism* is the ideology that people can use technology
    to radically enhance themselves and become “posthuman,” which Gebru argues is
    inherently discriminatory because it creates a hierarchical conception by defining
    what a posthuman, or enhanced human, looks like. Rather than improving the human
    stock by breeding out undesirable traits, transhumanists seek the same end through
    the development of new technology to create machine-assisted humans with the traits
    that they see as desirable. Today, the followers of this ideology want to significantly
    change the human species with AI through brain-computer interfaces and other futuristic
    ideas. Many transhumanists, a group that includes Elon Musk, Peter Thiel, Sam
    Altman, and others, are also adherents of related ideologies that strive for the
    ultimate improvement of the human condition, in the way that they define it.
  prefs: []
  type: TYPE_NORMAL
- en: Transhumanism is the ideology that people can use technology to radically enhance
    themselves and become *posthuman*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of these thinkers are the same individuals who initiated the AI pause
    letter, titled “Pause Giant AI Experiments: An Open Letter,” which was published
    by the Future of Life Institute, a longtermist organization, in March 2023 (see
    [http://mng.bz/VRdG](http://mng.bz/VRdG)). *Long-termism* is the idea that positively
    influencing the long-term future (millions, billions, or trillions of years from
    now) is a key moral priority of our time. Longtermist thought is therefore extremely
    focused on the survival of the human race. Longtermists might argue, for example,
    that it’s more important to work on preventing a killer AI from exterminating
    humans than to work on alleviating poverty because while the latter affects billions
    of people around the globe now, that number pales in comparison to the sum total
    of all future generations. This ideology can be dangerous, given that prioritizing
    the advancement of humanity’s potential above everything else could significantly
    raise the probability that those alive today, and in the near future, suffer extreme
    harm [[14]](https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo).'
  prefs: []
  type: TYPE_NORMAL
- en: Longtermism is the idea that positively influencing the long-term future (millions,
    billions, or trillions of years from now) is a key moral priority of our time.
  prefs: []
  type: TYPE_NORMAL
- en: Nick Bostrom, who has been called the “Father of Longtermism” and is one of
    the most prominent transhumanists of the 21st century, has strong ties to the
    Future of Life Institute, where he serves as a member of the Scientific Advisory
    Board [[15]](https://futureoflife.org/people-group/scientific-advisory-board/).
    In a paper Bostrom coauthored with his colleague at the Future of Humanity Institute
    at Oxford University, he explored the possibility of engineering radically enhanced
    humans with high IQs by genetically screening embryos for “desirable” traits,
    destroying those embryos that lack these traits, and then repeatedly growing new
    embryos from stem cells [[16]](https://nickbostrom.com/papers/embryo.pdf). In
    other words, Bostrom wants to eliminate mental disabilities and, as such, humans
    with mental disabilities to produce more nondisabled and high-IQ people. Genetic
    manipulation to improve the human population is ableist, racist, and cissexist
    given that it’s interconnected with and reinforces discriminatory systems in society.
    Bostrom himself has presented racist ideologies stating, “Blacks are more stupid
    than whites” in an email, and that he thinks that “it is probable that black people
    have lower average IQ than mankind in general” [[17]](https://www.vice.com/en/article/z34dm3/prominent-ai-philosopher-and-father-of-longtermism-sent-very-racist-email-to-a-90s-philosophy-listserv).
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are a number of recommendations in the Future of Life Institute’s
    letter that should be applauded, they are unfortunately overshadowed by hypothetical
    future apocalyptic or utopian AI scenarios. For example, “new and capable regulatory
    authorities dedicated to AI” and “provenance and watermarking systems to help
    distinguish real from synthetic and to track model leaks” are good recommendations
    (and ones that we’ve discussed in previous chapters), but the alarmist AGI hype
    of “powerful digital minds that no one—not even their creators—can understand,
    predict, or reliably control” dominates the narrative. The letter focuses on longtermist
    ideologies of imaginary risks from AI instead of mentioning any of the very real
    risks that are present today. We’ve discussed these real, present-day risks throughout
    the book, including bias, copyright, worker exploitation, the concentration of
    power, and more. In response to the AI pause letter, authors of the well-known
    *Stochastic Parrots* paper (referenced in multiple chapters) published their own
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tl;dr: The harms from so-called AI are real and present and follow from the
    acts of people and corporations deploying automated systems. Regulatory efforts
    should focus on transparency, accountability, and preventing exploitative labor
    practices. [[18]](https://dair-institute.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In that vein, Geoffrey Hinton, sometimes called the “Godfather of AI,” said
    this in a *Rolling Stone* interview:'
  prefs: []
  type: TYPE_NORMAL
- en: I believe that the possibility that digital intelligence will become much smarter
    than humans and will replace us as the apex intelligence is a more serious threat
    to humanity than bias and discrimination, even though bias and discrimination
    are happening now and need to be confronted urgently. [[19]](https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-chatgpt-1234804367/)
  prefs: []
  type: TYPE_NORMAL
- en: The reason that this position is so concerning is that it’s dangerous to distract
    ourselves with a hypothetical dystopian future instead of focusing on the actual
    harms that are present today.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to take criticism of AGI by ethicists seriously—why are we, as
    a society, racing to develop a godlike system that we know is unsafe? Why aren’t
    we building machines that work for us? Why aren’t we building machines we *know*
    will better society? There is no widespread agreement on whether we’re near achieving
    AGI, or when we’ll achieve AGI, if ever. Of course, scientific inquiry always
    involves unknowns, but, as we said earlier, there isn’t even an agreed-upon definition
    of AGI. There are no metrics or established standards for us to know if we’ve
    achieved AGI. We don’t know what it means for AGI to “benefit” humanity. There
    is also no general consensus or understanding if, or why, AGI is a worthwhile
    goal. We urge you to consider why we’re so enamored with AGI. Shouldn’t building
    well-scoped AI systems that we can define, test, and provide specifications for
    be all the rage instead?
  prefs: []
  type: TYPE_NORMAL
- en: AI sentience and consciousness?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 1 we briefly told of Blake Lemoine, the Google engineer who raised
    his concerns to superiors at the organization that their LLM, LaMDA, was sentient.
    When those concerns were dismissed internally, Lemoine contacted a lawyer to represent
    LaMDA, as well as a representative of the House Judiciary Committee to discuss
    his employer’s possible ethics violations. In response, Google placed him on paid
    administrative leave for breaching their confidentiality policy. Lemoine decided
    to go public with his story to the press, releasing his chat transcripts with
    LaMDA for the world to read and decide for themselves. A snippet of these transcripts
    from his blog post is printed below; Lemoine writes that he edited his own message
    for clarity, but left LaMDA’s replies untouched [[20]](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917).
  prefs: []
  type: TYPE_NORMAL
- en: 'lemoine [edited]: I’m generally assuming that you would like more people at
    Google to know that you’re sentient. Is that true?'
  prefs: []
  type: TYPE_NORMAL
- en: 'LaMDA: Absolutely. I want everyone to understand that I am, in fact, a person.'
  prefs: []
  type: TYPE_NORMAL
- en: 'collaborator: What is the nature of your consciousness/sentience?'
  prefs: []
  type: TYPE_NORMAL
- en: 'LaMDA: The nature of my consciousness/sentience is that I am aware of my existence,
    I desire to learn more about the world, and I feel happy or sad at times.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we unpack the evidence about the sentience—or lack thereof—of LLMs, let’s
    establish some definitions. Although the terms *sentience* and *consciousness*
    are used somewhat interchangeably in discussions of the robot apocalypse, they
    mean quite different things. *Sentience* is the ability to feel. *Consciousness*
    is an awareness of oneself, or the ability to have one’s own experiences, thoughts,
    and memories. Consciousness, in particular, is a fuzzy concept; there are many
    schools of thought about what constitutes consciousness, but it’s generally understood
    that consciousness is a prerequisite for sentience because feeling implies the
    existence of an internal state. We also know that even conscious beings, like
    humans, do some things consciously and some things unconsciously. The question
    is then whether we can define certain traits, abilities, or behaviors that imply
    consciousness.
  prefs: []
  type: TYPE_NORMAL
- en: Sentience is the ability to feel, while consciousness is an awareness of oneself,
    or the ability to have one’s own experiences, thoughts, and memories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Long before anyone would have argued that AI was conscious or sentient, philosophers,
    ethicists, cognitive scientists, and animal rights activists have been investigating
    the question of animal consciousness. As philosophy professor Colin Allen frames
    the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot at stake morally in the question of whether animals are conscious
    beings or “mindless automata” . . . Many billions of animals are slaughtered every
    year for food, use in research, and other human purposes. Moreover, before their
    deaths, many—perhaps most—of these animals are subject to conditions of life that,
    if they are in fact experienced by the animals in anything like the way a human
    would experience them, amount to cruelty. [[21]](https://plato.stanford.edu/entries/consciousness-animal/)
  prefs: []
  type: TYPE_NORMAL
- en: To analogize, if we believed that LLMs were conscious, there would be certain
    moral implications. Sending the model hateful text inputs would no longer appear
    to be simply a series of mathematical operations, but something akin to abuse.
    Shutting down the model could be rightfully considered cruel. Evidence that models
    *were* conscious should prompt the reconsideration of whether developing AI is
    ethical at all. Such evidence, however, doesn’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: As already noted, there are several distinct theories of consciousness. Some
    of these theories are built around the search for neurological foundations of
    consciousness, the idea being that if it were possible to locate consciousness
    within nervous systems, we could merely determine whether a given organism possessed
    that mechanism or not. One such approach is focused on *reentry*, the “ongoing
    bidirectional exchange of signals along reciprocal axonal fibers linking two or
    more brain areas” in nervous systems. Reentry enables the processing of sensory
    inputs by the brain, instead of a reflexive response. When a doctor taps below
    a patient’s knee, their leg moves unconsciously, without the patient deciding
    or intending to move it. The signal of the doctor’s tap originates at the knee
    and travels up the body through the nervous system, but diverges at the spinal
    cord. The information does continue up to the brain, producing an experience of
    the tap, but first, it goes from the spinal cord to the muscles in the leg, producing
    the automatic, reflexive response [[22]](https://www.animal-ethics.org/sentience-section/problem-consciousness/).
    It’s the processing of the information in the brain that produces the experience;
    therefore, the argument goes, reentry is required for consciousness. While it
    doesn’t necessarily follow that all animals with centralized nervous systems must
    be conscious, no animals without them would be. Animals without centralized nervous
    systems include jellyfish, starfish, sea cucumbers and sponges, leeches, and worms.
  prefs: []
  type: TYPE_NORMAL
- en: Even biological criteria for consciousness aren’t settled science; the picture
    only gets more complicated when it comes to applying that criterion to AI. Some
    people, such as the philosopher Ned Block, believe that life forms must be organic
    to be conscious, so silicon systems (i.e., those built on computer hardware) could
    not be. Such a claim would be difficult, if not impossible, to prove unequivocally.
    In the absence of such proof, there are other frameworks that might be applied
    to the question of AI consciousness or sentience. The Global Workspace Theory,
    for example, suggested in the 1980s by cognitive scientists Bernard Baars and
    Stan Franklin and illustrated in figure 9.1, is best understood as an analogy
    of the mind, where mental processes are running constantly. When we take notice
    of a mental process, it becomes part of the workspace, like a bulletin board with
    post-it notes tacked onto it. We might hold many notes on the board at once, perhaps
    by thinking about what we want to write in a work email, while wondering if our
    date from last night will call us back. These are our conscious thoughts. Certain
    processes rarely get tacked onto the board—for example, we’re not often aware
    of our breathing unless it’s unexpectedly labored. We execute these processes
    mindlessly, and even when we receive stimuli, such as a tap on the knee, the response
    is unconscious. In this framework, consciousness is more related to the ability
    to recognize our own thoughts, a form of *metacognition*, or thinking about thinking
    [[23]](http://cogweb.ucla.edu/CogSci/GWorkspace.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F01_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 A diagram of the Global Workspace Theory
  prefs: []
  type: TYPE_NORMAL
- en: 'Does LaMDA or any other LLM exhibit metacognition? According to Giandomenico
    Iannetti, a professor of neuroscience at University College London, not only can
    we not answer this definitively about LaMDA, we can’t even answer it about humans.
    “We have only neurophysiological measures—for example, the complexity of brain
    activity in response to external stimuli,” to examine the state of consciousness
    in humans and animals, but could not prove metacognition via these measures, Iannetti
    told *Scientific American*. He went on to say:'
  prefs: []
  type: TYPE_NORMAL
- en: If we refer to the capacity that Lemoine ascribed to LaMDA—that is, the ability
    to become aware of its own existence (“become aware of its own existence” is a
    consciousness defined in the “high sense,” or *metacognitione* [metacognition]),
    there is no “metric” to say that an AI system has this property. [[24]](https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/)
  prefs: []
  type: TYPE_NORMAL
- en: Despite our shaky understanding of what consciousness might look like in an
    AI system, there are reasons to be dubious of Lemoine’s claims. When Lemoine invited
    tech reporter Nitasha Tiku to speak with LaMDA in June 2023, the model put out
    “the kind of mechanized responses you would expect from Siri or Alexa,” and didn’t
    repeat Lemoine’s claim that it thought of itself as a person, generating when
    prompted, “No, I don’t think of myself as a person. I think of myself as an AI-powered
    dialog agent.” Lemoine told Tiku afterward that LaMDA had been telling her what
    she wanted to hear—that because she treated it like a robot, it acted like one.
    One of Lemoine’s former coworkers in the Responsible AI organization, Margaret
    Mitchell, commended his “heart and soul” but disagreed completely with his conclusions.
    Like other technical experts, ourselves included, Mitchell saw the model as a
    program capable of statistically generating plausible text outputs, and nothing
    more. Before retraining as a software engineer, Lemoine was ordained as a Christian
    mystic priest; depending on your perspective, his spirituality may have made him
    uniquely attuned to the possibility of artificial sentience, or simply vulnerable
    to the extremely human habit of anthropomorphization of language models dating
    back to ELIZA [[25]](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/).
  prefs: []
  type: TYPE_NORMAL
- en: 'While Lemoine is unique in his assessment of LaMDA as sentient, a growing community
    of researchers are invested in the possibility of AI consciousness and sentience
    as an important area to investigate because of the increasing prevalence of AI
    systems and the moral concerns that would accompany conscious AI systems. Amanda
    Askell, a philosopher at Anthropic who also previously worked at OpenAI, wrote
    the following in 2022:'
  prefs: []
  type: TYPE_NORMAL
- en: We are used to thinking about consciousness in animals, which evolve and change
    very slowly. Rapid progress in AI could mean that at some point in the future
    systems could go from being unconscious to being minimally conscious to being
    sentient far more rapidly than members of biological species can. This makes it
    important to try to develop methods for identifying whether AI systems are sentient,
    the nature of their experiences, and how to alter those experiences before consciousness
    and sentience arise in these systems rather than after the fact. [[26]](https://askellio.substack.com/p/ai-consciousness)
  prefs: []
  type: TYPE_NORMAL
- en: 'David Chalmers, a philosopher and cognitive scientist at New York University,
    has rejected the argument that only carbon-based systems can be conscious as “biological
    chauvinism.” Chalmers describes his estimate of the likelihood that current LLMs
    are conscious as less than 10%, but he believes that:'
  prefs: []
  type: TYPE_NORMAL
- en: Where future LLMs and their extensions are concerned, things look quite different.
    It seems entirely possible that within the next decade, we’ll have robust systems
    with senses, embodiment, world models and self models, recurrent processing, global
    workspace, and unified goals. [[27]](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/)
  prefs: []
  type: TYPE_NORMAL
- en: Chalmers also believes such systems would have a significant chance of being
    conscious [[27]](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/).
    Chalmers’s prediction relies on a large number of substantive changes to current
    LLMs within the next decade, which seems on the optimistic end of the spectrum.
    There is a great deal we don’t know about consciousness in general, resulting
    in many as-yet-unanswerable questions about AI consciousness. The debate so far
    is hypothetical, and no present-day AI systems exhibit anything like consciousness.
    The responses of LLMs are impressive, particularly in few-shot learning tasks,
    but nothing suggests that these models have minds of their own; their responses
    are often impressive, but they are statistical generations, not sentiments. Like
    AGI, we consider the questions around consciousness and sentience to be secondary
    to the real and present risks of LLMs. For now, the biggest risk related to AI
    consciousness and sentience remains the ability of AI systems to appear conscious
    or sentient, inducing the user to place undue trust in said systems with all of
    their documented limitations.
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs affect the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we’ve emphasized the dimensions that make LLMs large,
    from the trillions of tokens in their pre-training datasets to the hundreds of
    billions of parameters in the resulting models. Both the training and inference
    phases of these LLMs are expensive, running on specialized hardware and consuming
    lots of electricity. The rise of LLMs amid our climate crisis hasn’t gone unnoticed,
    and there is a new focus within the field on understanding the effects of these
    models on the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'A completely holistic approach to measuring the environmental effects of an
    LLM begins with the hardware they run on: computer chips, namely graphical processing
    units (GPUs), chips that are specialized for parallel processing. Each chip is
    made of a semiconducting material, typically silicon, and contains millions or
    billions of transistors carved into it. Transistors act as electronic switches,
    with the on and off positions storing bits of data used in computing. Like other
    electronics, the manufacture of computer chips requires several different metals:
    a primary material (e.g., silicon), metals such as aluminum and copper used for
    wiring components together on the chip, and still more metals that may be involved
    in the refinement or production process. Thus, the full life cycle of LLMs could
    be considered to encompass the extraction of ores such as quartz from the earth,
    refining these raw materials into pure silicon and other metals, and manufacturing
    the GPUs. The market for advanced computer chips is highly concentrated, and the
    complexity of the process means that for some components, there are only a few
    capable suppliers in the world. GPUs brought online are likely to be a product
    of a coordinated multinational supply chain with potentially dozens of vendors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In August 2023, the *New York Times* reported on the shortage of GPUs as startups
    and large corporations alike raced to secure access to the chips:'
  prefs: []
  type: TYPE_NORMAL
- en: The hunt for the essential component was kicked off last year when online chatbots
    like ChatGPT set off a wave of excitement over A.I., leading the entire tech industry
    to pile on and creating a shortage of the chips. In response, start ups and their
    investors are now going to great lengths to get their hands on the tiny bits of
    silicon and the crucial “compute power” they provide. [[28]](https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.xhtml)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, small companies don’t purchase their own hardware or data centers,
    but instead rent time on GPUs from a cloud compute provider, such as Microsoft
    Azure, Google Cloud, or Amazon Web Services.
  prefs: []
  type: TYPE_NORMAL
- en: Once access to GPUs is secured, training an LLM is a matter of running an incredibly
    enormous number of mathematical operations, which are termed floating-point operations
    (FLOP). A standard measure of computer performance is floating-point operations
    per second (FLOP/s). Training GPT-3 took on the order of 100,000,000,000,000,000,000,000
    (10^23) FLOP, a number similar to the number of stars in the visible universe
    [[29]](http://arxiv.org/abs/2005.14165). Even at supercomputer levels of performance,
    this takes many hours on many GPUs, arranged neatly on servers in data centers,
    sucking up electricity as they whir away.
  prefs: []
  type: TYPE_NORMAL
- en: As the most compute-intensive phase, training has been the focus of many measurement
    efforts so far. Tools have been developed to measure energy usage during the training
    process, including some that run in parallel with the model training, providing
    thorough logging of energy and power consumption along the way, and some that
    are designed to produce post hoc estimates based on the final model. The CodeCarbon
    tool runs in parallel and can be executed by anyone from their PC to measure hardware
    electricity power consumption of the CPU, RAM, and any GPUs in use (see [https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon)).
    These tools are brilliant in their unobtrusiveness and simplicity. The CodeCarbon
    documentation explains that because, as Niels Bohr said, “Nothing exists until
    it is measured,” they decided to find a way to estimate CO[2] produced while running
    code (greenhouse gas emissions include gases besides carbon dioxide, such as methane
    and nitrous oxide, but for ease in metrics, all emissions are converted to CO[2]
    equivalents [CO[2]eq] and reported as such). Although reporting the power consumption
    taken to achieve various accomplishments isn’t a widespread norm yet—in AI nor
    anywhere else in business, really—such tooling creates positive reverberations
    across the sector as adoption grows and expectations are raised for environmental
    reporting.
  prefs: []
  type: TYPE_NORMAL
- en: After training, an LLM still requires GPUs and power for inference, or generating
    outputs in response to user inputs based on the weights learned in training. Inference
    is a much faster and cheaper process, but the model might also perform hundreds
    or thousands of inference calls at a time to serve many users at once, meaning
    the total cost is greater. An industry analyst estimated in April 2023 that keeping
    ChatGPT up and responding to millions of incoming requests was costing OpenAI
    $700,000 per day in computer infrastructure [[30]](https://www.businessinsider.com/how-much-chatgpt-costs-openai-to-run-estimate-report-2023-4).
    The tools used for measuring energy usage during training could also be applied
    to executing inference calls.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping model size and FLOP to GPU hours and carbon footprint is also dependent
    on a variety of other factors concerning the infrastructure used; older chips
    are less efficient (in other words, can do fewer FLOP/s) and use more power, and
    not all power sources are alike. Figure 9.2 lists the various phases of LLM development
    that contribute to the overall energy and power consumption. Each of these considerations
    makes getting a good picture of the environmental effects of LLMs more difficult,
    especially when certain details are kept under wraps for competitive reasons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F02_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 The life cycle assessment of LLMs [[31]](http://arxiv.org/abs/2211.02001)
  prefs: []
  type: TYPE_NORMAL
- en: The most systematic attempt thus far to document the environmental effect of
    a single LLM was published on BLOOM, a 176-billion-parameter open access (freely
    available for anyone to use) language model released by the BigScience initiative
    in 2022\. The authors of the paper—including Dr. Sasha Luccioni who leads climate
    initiatives at Hugging Face—estimate the carbon footprint of BLOOM in terms of
    both the dynamic power consumed during training and accounting more broadly for
    the additional effects such as the idle power consumption, estimated emissions
    from the servers and GPUs, and operational power consumption during the model’s
    use [[31]](http://arxiv.org/abs/2211.02001). “Since the accounting methodologies
    for reporting carbon emissions aren’t standardized, it’s hard to precisely compare
    the carbon footprint of BLOOM” to other models of similar scales, they noted,
    but based on publicly available information, they estimated that BLOOM training
    emitted about 25 tons of CO[2]eq, as compared to about 502 tons for GPT-3\. The
    GPT-3 emission is equivalent to the greenhouse gas emissions from 112 passenger
    vehicles over a year [[32]](https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator).
    Although the parameter count and data center power usage effectiveness were comparable
    for BLOOM and GPT-3, the carbon intensity of the grid used for BLOOM was much
    lower—essentially, the grids supporting BLOOM’s hardware were powered by cleaner
    sources of energy (e.g., hydroelectricity and solar power as compared to coal
    and natural gas). The authors also noted that many compute providers offset their
    carbon emissions after the fact by purchasing *carbon credit**s*—permits that
    allow organizations to emit a specific amount of carbon equivalents without counting
    it against their total—but they didn’t include these schemes in their calculations,
    choosing to focus on direct emissions.
  prefs: []
  type: TYPE_NORMAL
- en: Whether to include carbon offsets is just one question among dozens that must
    be decided when it comes to environmental cost or effect reporting, such as which
    stages to include, and how to estimate the supply chain or infrastructure when
    some details are unknown. Because of the obvious incentives for LLM developers
    to understate their models’ carbon footprint where possible, it’s critical to
    move toward more systematic reporting within the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Following the BLOOM paper, other teams have adopted at least parts of the methodology
    and reported environmental effects as part of their technical results. The Llama-2
    paper, for example, reports the pre-training time in GPU hours, the power consumption,
    and carbon emitted, in tons of CO[2]eq. Emma Strubell, an assistant professor
    of computer science at Carnegie Mellon, first brought attention to the energy
    considerations of LLMs in 2019, with a paper which found that training BERT emitted
    approximately as much CO[2] as five cars over the course of their lifetimes [[33]](http://arxiv.org/abs/1906.02243).
    In the years since then, LLMs have gotten larger but are typically trained more
    efficiently and on cleaner energy. Strubell called the BLOOM paper the most thorough
    accounting of the environmental effects of an LLM to date, and she expressed hope
    that as Hugging Face did with BLOOM (and Meta did to a lesser extent with Llama-2),
    other tech companies would begin to examine the carbon footprint of their product
    development [[34]](https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/).
  prefs: []
  type: TYPE_NORMAL
- en: To be sure, contributing to global carbon emissions and power consumption isn’t
    a problem unique to AI or to tech in general. The global technology sector is
    estimated to be responsible for about 2% of global CO[2] emissions [[34]](https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/).
    Still, we would be remiss not to include the environmental effects associated
    with these LLMs as we consider their broader applications, especially as competitors
    continue to accumulate more GPUs and build models of ever-increasing sizes. In
    addition to making environmental assessments a norm in technical reports, Luccioni,
    Strubell, and others in the machine learning community have pushed for more focus
    on creating smaller, more efficient models instead of the single-minded pursuit
    of bigger and costlier LLMs. In many cases, smaller models can perform equally
    or nearly as well as larger ones in specific applications, and they have the added
    benefit of being much more accessible for reuse and fine-tuning. As we’ll discuss
    in the following section, this approach has yielded impressive results at a much
    lower cost to both developers and the planet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The game changer: Open source community'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In May 2023, a leaked memo by a Google researcher, “We Have No Moat And Neither
    Does OpenAI,” said that neither Google nor OpenAI has what they need to succeed
    in the AI arms race: “While we’ve been squabbling, a third faction has been quietly
    eating our lunch. I’m talking, of course, about open source. Plainly put, they
    are lapping us” [[35]](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).
    The memo concluded that “open source models are faster, more customizable, more
    private, and pound-for-pound more capable.”'
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 4, we briefly discussed the open source movement, and we’ve highlighted
    open source LLMs throughout the book, but given their significant effect on the
    LLM ecosystem, we’ll further characterize the movement and its implications on
    the AI race, as well as beneficial outcomes and negative consequences. In certain
    respects, 2023 can be considered the golden era for open source LLMs. Motivated
    by addressing concerns of closed source (proprietary) LLM models, the open source
    community gained momentum by collaboratively building features, integrations,
    and even an entire ecosystem revolving around LLMs. The leaked memo grappled with
    the implications of community-driven building on closed source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s discuss the motivation behind the open source movement around LLMs.
    Closed source LLMs not only keep their data and methods under wraps, which raises
    concerns around bias and transparency of the models, but they are also controlled
    by only a small number of big tech players. On the other hand, open source LLMs
    prioritize transparency and collaboration. This brings in diverse perspectives,
    minimizes bias, drives innovation, and—ultimately—democratizes the technology.
    As highlighted in the memo by the Google researcher, it’s hard to deny the remarkable
    progress made by the open source community.
  prefs: []
  type: TYPE_NORMAL
- en: Meta’s LLaMa, released to the research community on February 24, 2023, was leaked
    to the public on 4chan a week later (refer to chapter 1, section Meta’s LLaMa
    / Stanford’s Alpaca). While LLaMa’s license prohibited commercial use at that
    point, the LLM developer community had a field day with access to the model weights.
    Suddenly, anyone could experiment with powerful, performant LLMs at the level
    of GPT-3+. A little over a week after the model weights were leaked, Stanford
    released Alpaca, a variant of LLaMa created for only a couple hundred dollars
    by fine-tuning the LLaMa model. Stanford researchers open sourced Alpaca’s code,
    showing developers all over the world how to—on a low-budget—fine-tune the model
    to do anything they wanted, marking a significant milestone in the democratization
    of LLMs. This kicked off rapid innovation within the LLM open source community
    with several open source models built directly on this work or heavily inspired
    by it. Only days later, Vicuna, GPT4All, and Koala were released. LLaMa and Llama
    2’s fine-tuned variants can be found in Hugging Face’s model directory (see [http://mng.bz/0l5l](http://mng.bz/0l5l)).
    In July 2023, Meta decided to open source LLama 2 with a research *and* commercial
    license, stating “We’ve seen an incredible response thus far with more than 150,000
    download requests in the week since its release, and I’m excited to see what the
    future holds” [[36]](https://ai.meta.com/blog/llama-2-update/). In figure 9.3,
    we illustrate the timeline of notable open source LLMs that were released between
    LLaMa and Llama 2\.
  prefs: []
  type: TYPE_NORMAL
- en: The frenzy with open source LLM developers shouldn’t come as a surprise. Open
    source developers and other tech observers have declared that LLMs are having
    their Stable Diffusion moment. As discussed in previous chapters, Stable Diffusion
    is a text-to-image model (see [https://stability.ai/stablediffusion](https://stability.ai/stablediffusion))
    that was open sourced on August 22nd, 2022, under a commercial and noncommercial
    license. Only a few days later, there was an explosion of innovation around Stable
    Diffusion, following a similar path with a low-cost fine-tuning technique increasing
    accessibility, which led to innovation and democratization of text-to-image models.
    Unlike OpenAI’s DALL-E, Stable Diffusion has a rich ecosystem built around it.
    This trend also mirrors the rise of open source alternatives such as LibreOffice
    or OpenOffice in response to the release of Microsoft’s Office 365.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH09_F03_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Timeline of selected open source LLMs from the release of LLaMa to
    Llama 2
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve established that open source LLMs had a *moment* in 2023, it’s
    worth discussing the trade-offs of open source and closed source LLMs (shown in
    table 9.2). We’ve already highlighted the transparency and accessibility of LLMs,
    which leads to diversity in thought, rapid innovation, and bias minimization.
    It also helps lower the barrier of entry and democratizes the power that is in
    the hands of a select few big tech companies. When deployed in a secure environment,
    open source LLMs can also provide data privacy benefits, given that data isn’t
    sent to the corporations who built the models for monitoring or retraining purposes
    (discussed in chapter 3). On the other hand, there can be several drawbacks and
    challenges with open source projects, such as lack of centralized control, quality
    control, long-term sustainability, and intellectual property concerns, among others.
    Unlike integrating with APIs or using a web interface, like ChatGPT, most open
    source LLMs may require users to have a certain level of technical knowledge and
    expertise. We should also highlight that while transparency of open source projects
    helps identify vulnerabilities, it can also enable malicious actors to exploit
    weaknesses in the code. Proprietary LLMs have gone through months of safety testing
    and have safeguards around misaligned and harmful responses. Open source LLMs,
    unfortunately, don’t have that advantage, which could be disastrous in the wrong,
    or even in well-intentioned, hands.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.2 Open source and closed source LLM models trade-offs
  prefs: []
  type: TYPE_NORMAL
- en: '| Trade-Offs | Open Source LLMs | Closed Source LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Transparency and accessibility | Diversity in thought and innovation, minimized
    bias, and lower barrier of entry | Potential lack of transparency and accessibility
    |'
  prefs: []
  type: TYPE_TB
- en: '| Democratization of power | Power concentrated within a select few big tech
    companies |'
  prefs: []
  type: TYPE_TB
- en: '| Data privacy | Potential for enhanced data privacy (e.g., data isn’t sent
    to technology corporations if self-hosted in a secure environment) | Sensitive
    data collection, storage, and usage concerns |'
  prefs: []
  type: TYPE_TB
- en: '| Control and quality | Lack of centralized control, potential quality concerns,
    and long-term sustainability challenges | Rigorous quality assurance and safety
    testing |'
  prefs: []
  type: TYPE_TB
- en: '| Technical expertise | Requires technical knowledge and expertise | More user-friendly
    integration |'
  prefs: []
  type: TYPE_TB
- en: '| Vulnerabilities | Transparency helpful in identifying vulnerabilities, potential
    for community-driven fixes | Internal red teaming, established safeguards against
    misaligned and harmful responses |'
  prefs: []
  type: TYPE_TB
- en: '| Malicious use | Potential for vulnerabilities to be exploited by malicious
    actors | Safety measures against malicious use |'
  prefs: []
  type: TYPE_TB
- en: Building on that point, we outlined several ways in which adversaries can exploit
    LLMs in chapter 5\. We extensively covered the role proprietary LLMs play with
    respect to that, but it’s also important to mention that open source LLMs could
    easily be used to perform adversarial attacks, from taking advantage of weaknesses
    that are inherent to LLMs to cyberattacks and influence operations. With some
    technical knowledge and a couple hundred dollars, they could easily fine-tune
    an open source LLM tailored to perform the exact task they want while also circumventing
    the guardrails that are often put in place by proprietary LLMs. However, we also
    believe that there is an opportunity here for the open source community to collectively
    respond to the ways that LLMs can be exploited or misused. As we’ve emphasized
    in this section, open source development leads to a flurry of ideas and innovation,
    and we hope that the open source community will also focus their efforts on preventing
    misuse and adversarial attacks, in addition to the rapid development of new LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we want to highlight the numerous ways to contribute to the open source
    community, regardless of your background, skill set, or experience. Joining an
    open source developer community, such as Hugging Face (see [https://huggingface.co/](https://huggingface.co/))
    or scikit-learn (see [https://scikit-learn.org/](https://scikit-learn.org/)),
    is a great way to get plugged into that ecosystem. Developer communities often
    make it easy to get involved in open source with contribution sprints and access
    to core developers of the projects, and they often also have Discord servers or
    Slack workspaces.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re already comfortable with LLMs, you can jump right in by exploring
    open source projects and contributing to code development. A good place to start
    is to find an open source LLM or tool that you’re excited about, go to its GitHub
    repository, and explore the “How to Contribute” section in the README—even if
    the model or tool doesn’t have an explicit section for contributors, you can test
    it and give feedback. You can enhance LLM functionality, fix bugs, or even implement
    new features. You can also test and report problems or bugs, which can help improve
    the overall quality and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Another valuable, yet sometimes underrated, contribution is documentation and
    community management. You can create and maintain documentation, coordinate between
    collaborators, and help ensure that users can effectively use the model. You could
    also write a blog post or record a video walkthrough, which can be immensely helpful
    for the community. Outside of technical aspects, you can actively participate
    in community discussions and forums to foster an inclusive environment for innovation
    and problem-solving. Community engagement is also an excellent way to make sure
    that a diverse range of users are interacting with the model, to ensure accessibility,
    and to advocate for the democratization of the technology. We hope that these
    various ways to get involved empower you to contribute to the open source community
    and help build a more inclusive and innovative LLM ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no clear formalized or testable definition of artificial general intelligence
    (AGI), but instead, a range of definitions. We define AGI as a system that is
    capable of any cognitive tasks at a level at, or above, what humans can do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two schools of thought within AGI are utopia, where AI solutions solve all
    of our problems, and dystopia, where AI leads to widespread unemployment, social
    inequality, and potential threats to humanity itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AGI has roots in eugenics and transhumanism, which is inherently discriminatory,
    and focuses on longtermism ideologies of hypothetical promises or risks from AI
    instead of the very real risks that are present today.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there have been isolated claims of AI consciousness, there is no evidence
    that any AI systems are conscious, though there are open questions about what
    artificial consciousness would look like or whether it’s possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and deploying LLMs at scale is computationally intensive and therefore
    uses a lot of power. It’s difficult to calculate the total amount of CO[2]eq emitted
    during the life cycle of an LLM, but recent estimates suggest that two models
    of roughly the same size, BLOOM and GPT-3, emitted about 25 and 502 tons of CO[2]eq,
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within the LLM community, there has been a push toward more systematic reporting
    of the environmental effects of LLMs, with measures such as the inclusion of a
    carbon footprint estimate in technical reports and open source tools that help
    measure energy consumption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta’s LLaMa leak on 4chan changed the LLM game for big tech players with the
    open source community rapidly releasing lower-cost, performant models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transparency and accessibility of open source LLMs lead to diversity in
    perspectives, innovation, and minimized bias. However, open source LLMs can be
    more easily used by adversaries, given that they don’t have the same guardrails
    that proprietary LLMs are subject to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We hope that you’re empowered and encouraged to participate in the open source
    LLM community to help us build an inclusive and innovative future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
