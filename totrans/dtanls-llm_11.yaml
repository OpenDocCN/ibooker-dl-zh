- en: 9 Optimizing cost and quality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 优化成本和质量
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Model choice and tuning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择和调整
- en: Prompt engineering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Fine-tuning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型微调
- en: Analyzing data with large language models is a great way to burn money quickly.
    If you’ve been using GPT-4 (or a similarly large model) for a while, you’ve probably
    noticed how fees pile up quickly, forcing you to recharge your account regularly.
    But do we always need to use the largest (and most expensive) model? Can’t we
    make smaller models perform almost as well? How can we get the most bang for our
    buck?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型分析数据是快速烧钱的好方法。如果你已经使用GPT-4（或类似的大型模型）了一段时间，你可能已经注意到费用是如何快速累积的，迫使你定期充值账户。但我们是否总是需要使用最大（也是最昂贵）的模型？我们能否让小型模型几乎以同样的效果运行？我们如何花最少的钱获得最大的效益？
- en: This chapter is about saving money when using language models on large data
    sets. Fortunately, we have quite a few options for doing so. First, we have lots
    of choices when it comes to large language models. Selecting a model that is as
    small (or, rather, as cheap) as possible while still performing well on our analysis
    task can go a long way toward balancing our budget. Second, models typically have
    various tuning parameters, allowing us to tune everything from the overall text
    generation strategy to the way specific tokens are (de-)prioritized. We want to
    optimize our settings there to turn small models into GPT-4 alternatives for certain
    tasks. Third, we can use prompt engineering to tweak the way we ask the model
    our questions, sometimes leading to surprisingly different results!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讲述在使用大型数据集上的语言模型时如何节省金钱。幸运的是，我们有很多选择来实现这一点。首先，在大型语言模型方面，我们有相当多的选择。选择一个尽可能小（或者说，更便宜）的模型，同时仍然能够很好地完成我们的分析任务，这可以在很大程度上帮助我们平衡预算。其次，模型通常具有各种调整参数，使我们能够从整体文本生成策略调整到特定标记的（去）优先级调整。我们希望在那里优化我们的设置，将小型模型转变为特定任务的GPT-4替代品。第三，我们可以使用提示工程来调整我们向模型提问的方式，有时会得到令人惊讶的不同结果！
- en: And finally, if none of these methods cut it, we can choose to create our own
    models, highly customized for only the task we care about. Of course, assuming
    we don’t want to spend millions on pretraining, we won’t start training new models
    from scratch. Instead, we will typically choose to fine-tune existing models with
    just a few hundred samples. That’s often enough to get significantly better performance
    than when using the base model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果上述方法都不奏效，我们可以选择创建自己的模型，这些模型高度定制化，仅针对我们关心的任务。当然，如果我们不想在预训练上花费数百万，我们就不会从头开始训练新模型。相反，我们通常会选择使用仅几百个样本对现有模型进行微调。这通常足以比使用基础模型获得显著更好的性能。
- en: Of course, what works best depends on the task we’re trying to solve, as well
    as on data properties. Fortunately, if we want to analyze large amounts of data,
    we can afford to spend a little money on trying different tuning options on a
    data sample. Chances are, this upfront investment will pay off once we analyze
    the entire data set! Throughout this chapter, we will apply all of these tuning
    options in an example scenario.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最佳方案取决于我们试图解决的问题以及数据属性。幸运的是，如果我们想分析大量数据，我们可以在数据样本上尝试不同的调整选项，花费一点钱。很可能，一旦我们分析了整个数据集，这种前期投资就会得到回报！在本章中，我们将在一个示例场景中应用所有这些调整选项。
- en: 9.1 Example scenario
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 示例场景
- en: You’re back at Banana and trying to classify user reviews. Users can leave free-form
    text reviews about their experiences with Banana products on the Banana website.
    You want to know whether those reviews are positive (i.e., the user was happy
    with the product) or negative (i.e., reading them will scare away potential customers!).
    Of course, you can use language models for that task (you saw that in chapter
    4). For instance, you can use GPT-4 (at the time of writing, this is OpenAI’s
    largest model for text processing). Provide GPT-4 with a review, together with
    instructions for how to classify it (including a description of possible class
    labels, such as “positive” and “negative”), and the output should be correct for
    most reviews.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你回到了Banana，试图对用户评论进行分类。用户可以在Banana网站上留下关于Banana产品体验的自由文本评论。你想要知道这些评论是积极的（即，用户对产品感到满意）还是消极的（即，阅读它们会吓跑潜在客户！）。当然，你可以使用语言模型来完成这项任务（你可以在第4章中看到）。例如，你可以使用GPT-4（在撰写本文时，这是OpenAI用于文本处理的最大模型）。向GPT-4提供一个评论，以及如何对其进行分类的说明（包括可能的标签描述，如“积极”和“消极”），输出应该对大多数评论都是正确的。
- en: 'However, analyzing data with GPT-4 costs about 6 cents per 1,000 tokens. That
    (6 cents) may not sound like much, but Banana receives thousands of product reviews
    every day! Let’s assume the average review contains about 100 tokens (about 400
    characters). Furthermore, let’s assume that Banana receives about 10,000 reviews
    per day. That means you collect 100 × 10,000 tokens per day: about 1 million tokens
    per day and 365 million tokens per year. How much does it cost to analyze one
    year’s worth of comments? About 365,000,000 × (0.06/1000) = 21,900 dollars.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用GPT-4分析数据每千个标记的成本约为6美分。这（6美分）可能听起来不多，但Banana每天都会收到数千条产品评论！让我们假设平均评论包含大约100个标记（大约400个字符）。此外，让我们假设Banana每天收到大约10,000条评论。这意味着你每天收集100
    × 10,000个标记：大约每天1百万个标记，每年3.65亿个标记。分析一年的评论需要多少钱？大约是365,000,000 × (0.06/1000) =
    21,900美元。
- en: That may put a bit of a dent in your budget! Can’t you get it cheaper? For example,
    at the time of writing, GPT-3.5 Turbo is priced at only around 0.0005 dollars
    per thousand tokens (tokens are priced differently depending on whether they are
    read or generated, but we will neglect that for now to simplify the calculations).
    That means only 365,000,000 × (0.0005/1000) = 182.5 dollars to analyze one year’s
    worth of comments. Much better! But to get satisfactory output quality, you may
    have to do a little extra work to ensure that you’re using the model in the best
    possible way.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会在你的预算上留下一些缺口！难道你不能以更低的价格获得它吗？例如，在撰写本文时，GPT-3.5 Turbo的价格仅为每千个标记大约0.0005美元（标记的价格因是否读取或生成而有所不同，但为了简化计算，我们暂时忽略这一点）。这意味着分析一年的评论只需365,000,000
    × (0.0005/1000) = 182.5美元。这要好得多！但为了获得令人满意的输出质量，你可能需要做一些额外的工作，以确保你以最佳方式使用该模型。
- en: Tip Instead of GPT-3.5 Turbo, you can also use alternative models such as GPT-4o
    mini (the model ID is `gpt-4o-mini`) in the following examples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：除了GPT-3.5 Turbo，你还可以在以下示例中使用其他模型，如GPT-4o mini（模型ID为`gpt-4o-mini`）。
- en: That’s what we will do in this example. Starting from the most naive implementation
    of our classifier, we will gradually refine our implementation and try all the
    various tuning options discussed in the introduction to this chapter!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们将在本例中做的事情。从我们分类器的最简单实现开始，我们将逐步改进我们的实现，并尝试本章介绍中讨论的所有各种调优选项！
- en: 9.2 Untuned classifier
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 未调优分类器
- en: 'Let’s begin with the base version of our classifier. Again, the goal is to
    take a review and decide whether it should be classified as positive (`pos`) or
    negative (`neg`). We will use the following prompt template to classify reviews:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们分类器的基版开始。再次强调，目标是取一个评论并决定它应该被分类为正面（`pos`）还是负面（`neg`）。我们将使用以下提示模板来分类评论：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this prompt template, `[Review]` is a placeholder that gets replaced with
    the actual review text. For example, after substitution, our prompt may look like
    this (the first two lines correspond to an abbreviated version of the review to
    classify, apparently a new movie streaming on Banana TV that doesn’t match the
    reviewer’s taste):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个提示模板中，`[Review]`是一个占位符，将被实际评论文本替换。例如，替换后，我们的提示可能看起来像这样（前两行对应于评论的缩略版，显然是Banana
    TV上的一部新电影流媒体，不符合评论者的口味）：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Ideally, if we send this prompt to a GPT model, we expect either `pos` or `neg`
    as the reply (in this specific case, we expect `neg`). Listing [9.1](#code__untuned)
    shows the complete Python code; we won’t spend too much time discussing it because
    it is similar to the classifiers we saw in chapter 4\. The `create_prompt` function
    (**1**) instantiates the prompt template for a specific review (stored in the
    input parameter `text`). The result is a prompt that we can send to our language
    model using the `call_llm` function (**2**). We call GPT-3.5 Turbo here (**3**)
    (saving costs). We also set `temperature` to `0`, which means we’re minimizing
    randomness when generating output. This means you should see the same results
    when running the code repeatedly. You may also notice that `call_llm` is a little
    longer in listing [9.1](#code__untuned) than the versions we have seen in previous
    listings. That’s because we retrieve not only the answer generated by our language
    model but also the number of tokens used (**4**). Counting the number of tokens
    will allow us to calculate the invocation costs on a data sample.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，如果我们向 GPT 模型发送这个提示，我们期望得到 `pos` 或 `neg` 作为回复（在这个特定情况下，我们期望 `neg`）。列表 [9.1](#code__untuned)
    展示了完整的 Python 代码；我们不会花太多时间讨论它，因为它与我们第 4 章中看到的分类器类似。`create_prompt` 函数（**1**）为特定评论实例化提示模板（存储在输入参数
    `text` 中）。结果是我们可以使用 `call_llm` 函数（**2**）发送给我们的语言模型的提示。在这里我们调用 GPT-3.5 Turbo（**3**）（节省成本）。我们还设置
    `temperature` 为 `0`，这意味着我们在生成输出时最小化随机性。这意味着当你重复运行代码时，你应该看到相同的结果。你也许还会注意到，列表 [9.1](#code__untuned)
    中的 `call_llm` 比我们之前看到的版本要长一些。那是因为我们不仅检索了我们的语言模型生成的答案，还检索了使用的标记数量（**4**）。计算标记数量将允许我们计算数据样本上的调用成本。
- en: 'Listing 9.1 Classifying reviews as positive or negative: base version'
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1 将评论分类为正面或负面：基础版本
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Generates prompts'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 生成提示'
- en: '#2 Invokes the LLM'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 调用语言模型'
- en: '#3 Generates an answer'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 生成答案'
- en: '#4 Extracts answer and token usage'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 提取答案和标记使用情况'
- en: '#5 Parses arguments'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 解析参数'
- en: '#6 Iterates over reviews'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 遍历评论'
- en: '#7 Classifies the review'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 分类评论'
- en: '#8 Updates counters'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 更新计数器'
- en: We will assume that reviews to classify are stored in a .csv file. We expect
    users to specify the path of that .csv file as a command-line argument (**5**).
    After reading the .csv file, we iterate over the reviews (**6**) in the order
    in which they appear in the input file. For each review, we extract the associated
    text (**7**) (we assume it’s stored in the `text` column), create a prompt for
    classification, and call the language model. The result is the answer text generated
    by the language model (hopefully it’s one of the two class labels, `pos` or `neg`),
    as well as the number of tokens used.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设要分类的评论存储在一个 .csv 文件中。我们期望用户指定该 .csv 文件的路径作为命令行参数（**5**）。在读取 .csv 文件后，我们按照它们在输入文件中出现的顺序遍历评论（**6**）。对于每条评论，我们提取相关的文本（**7**）（我们假设它存储在
    `text` 列），创建一个用于分类的提示，并调用语言模型。结果是语言模型生成的答案文本（希望它是两个类别标签之一，`pos` 或 `neg`），以及使用的标记数量。
- en: Our goal is to try different methods of querying a language model and compare
    the output quality and costs. To judge the output quality, we assume that the
    input .csv file contains not only the review text but also a ground-truth label.
    This means we assume that each review has already been associated with the correct
    class label, stored in the `sentiment` column (because our two class labels describe
    the sentiment of the review). After receiving the language model’s output, we
    compare the output to the ground truth (**8**) and update the number of correctly
    classified reviews (variable `nr_correct`). At the same time, we sum up the total
    number of tokens used (because processing fees are proportional to that) and store
    them in the counter called `nr_tokens`. After iterating over all reviews, listing
    [9.1](#code__untuned) prints out the final number of correct classifications and
    the number of tokens used.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是尝试不同的查询语言模型的方法，并比较输出质量和成本。为了判断输出质量，我们假设输入 .csv 文件不仅包含评论文本，还包含一个真实标签。这意味着我们假设每条评论都已经与正确的类别标签相关联，存储在
    `sentiment` 列（因为我们的两个类别标签描述了评论的情感）。在收到语言模型的输出后，我们将输出与真实标签（**8**）进行比较，并更新正确分类的评论数量（变量
    `nr_correct`）。同时，我们汇总使用的总标记数量（因为处理费用与它成正比）并将它们存储在名为 `nr_tokens` 的计数器中。遍历所有评论后，列表
    [9.1](#code__untuned) 打印出最终的分类正确数量和使用的标记数量。
- en: 9.3 Model tuning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 模型调整
- en: 'Let’s try it! You can find listing [9.1](#code__untuned) under Untuned Classifier
    on the book’s website. We reuse the movie reviews from chapter 4; search for the
    Reviews.csv link in the chapter 4 section. The file contains 10 reviews, along
    with the corresponding ground truth. Let’s assume that the code for listing [9.1](#code__untuned)
    and the reviews are stored in the same folder on disk. Open your terminal, switch
    to that folder, and run the following command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试！你可以在书籍网站上找到“未调优分类器”下的[9.1](#code__untuned)列表。我们重用了第4章中的电影评论；在第4章部分搜索“Reviews.csv”链接。该文件包含10篇评论，以及相应的ground
    truth。假设[9.1](#code__untuned)列表和评论存储在磁盘上的同一文件夹中。打开你的终端，切换到该文件夹，并运行以下命令：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Incorrect label'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 错误的标签'
- en: '#2 Nonexistent label'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 不存在的标签'
- en: The first 10 lines describe the results for each review. We have the label generated
    by the language model and then the ground-truth label (taken from the input file).
    At the end, we have the number of correctly classified reviews and the number
    of tokens used.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前十行描述了每个评论的结果。我们有了语言模型生成的标签和从输入文件中获取的ground-truth标签。最后，我们有正确分类的评论数量和使用的token数量。
- en: Out of 10 reviews, we classified 6 correctly. Well, at least that’s better than
    50%, but it’s still not a great result. What went wrong? Looking at output gives
    us some ideas. There are cases (**1**) where the language model simply picks the
    wrong class label. That’s not unexpected. However, there are also cases (**2**)
    where the language model picks a class label that doesn’t even exist! Granted,
    it’s not too far off (`negative` instead of `neg`), and that seems easy to fix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在10篇评论中，我们正确分类了6篇。嗯，至少比50%好，但这仍然不是一个很好的结果。出了什么问题？查看输出给我们一些想法。有些情况（**1**）中，语言模型简单地选择了错误的类别标签。这并不意外。然而，也有情况（**2**）中，语言模型选择了一个甚至不存在的类别标签！当然，这并不太离谱（`negative`而不是`neg`），这似乎很容易修复。
- en: We focus on the (probably) low-hanging fruit of making the language model generate
    only one of our two possible class labels. How do we do that? Enter the `logit_bias`
    parameter. The `logit_bias` parameter enables users to change the likelihood that
    certain tokens are selected (we briefly discussed this and other GPT parameters
    in chapter 3). In this specific case, we would like to significantly increase
    the probability of the tokens associated with our two class labels (`neg` and
    `pos`). The `logit_bias` parameter is specified as a Python dictionary, mapping
    token IDs to a bias. A positive bias means we want to increase the probability
    that the language model generates the corresponding token. A negative bias means
    we decrease the probability of generating the associated token.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于语言模型只生成我们两个可能的类别标签之一这个可能容易解决的问题。我们如何做到这一点？输入`logit_bias`参数。`logit_bias`参数允许用户更改某些token被选中的可能性（我们在第3章中简要讨论了这一点和其他GPT参数）。在这种情况下，我们希望显著增加与我们的两个类别标签（`neg`和`pos`）相关的token的概率。`logit_bias`参数指定为一个Python字典，将token
    ID映射到偏差。正偏差意味着我们希望增加语言模型生成相应token的概率。负偏差意味着我们降低生成相关token的概率。
- en: In this case, we want to increase the chances that GPT-3.5 selects one of the
    two tokens representing class labels. So we want to select a high bias for those
    two token IDs. Bias scores range from –100 to +100\. We will go with the maximum
    and assign a bias of +100 to the tokens representing class labels. First we need
    to find their token IDs. Language models represent text as a sequence of token
    IDs. To change token bias, we need to reference the IDs of the tokens we care
    about.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们希望增加GPT-3.5选择代表类别标签的两个token之一的机会。因此，我们希望为这两个token ID选择一个高偏差。偏差分数范围从-100到+100。我们将选择最大值，并将代表类别标签的token分配偏差+100。首先，我们需要找到它们的token
    ID。语言模型将文本表示为token ID的序列。要更改token偏差，我们需要参考我们关心的token的ID。
- en: A *tokenizer* is the component that transforms text into token IDs. You can
    find tokenizers for all GPT models at [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer).
    We’re using GPT-3.5, so select the one labeled GPT 3.5 & GPT-4\. Figure [9.1](#fig__gpttokenizer)
    shows the tokenizer web interface.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*tokenizer*是将文本转换为token ID的组件。你可以在[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)找到所有GPT模型的tokenizer。我们使用GPT-3.5，所以选择标记为GPT
    3.5 & GPT-4的那个。图[9.1](#fig__gpttokenizer)显示了tokenizer的Web界面。'
- en: 'We can enter text in the text box and click the Token IDs button to see the
    token IDs for our input text. Using the tokenizer, we learn that the token `pos`
    has ID 981 and the token `neg` has token ID 29875\. Now we’re ready to add a bias
    to our model invocation as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Defines the bias'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F01_Trummer.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1 GPT tokenizer at [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer):
    enter text to learn the associated token IDs.'
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Compared to the previous call (in listing [9.1](#code__untuned)), we add the
    logit bias (**1**) by mapping the IDs of the two tokens we’re interested in (`pos`
    with token ID 981 and `neg` with token ID 29875) to the highest possible bias
    value of 100\. That should fix the problem of generating tokens that do not correspond
    to class labels, right?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Warning The code described next causes problems and results in long running
    times and significant monetary fees. Do not try it without integrating the fix
    presented at the end of this section!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it to be sure. You can add the logit bias to the code from listing
    [9.1](#code__untuned). Alternatively, later in this chapter, we will present a
    tunable version of the classifier that will allow you to try different combinations
    of tuning parameters (including the logit bias). If you execute the classifier
    with biases added, you will likely see output similar to the following (actually,
    as executing the code takes a long time and incurs non-negligible costs, you may
    just want to trust me on this):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Nonexistent labels for each input:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Increased token usage'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Oh, no—not a single correct classification! What happened? Comparing generated
    “labels” to the ground truth reveals the problem (**1**): we’re only generating
    the two possible tokens (which is great!) but just way too many of them (which
    is not so great!). That increases token consumption (**2**) (note that the output
    length was limited for generating the example output; otherwise, token consumption
    would be much higher), but more importantly, it means our output does not correspond
    to any class label.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Why does the model generate so many tokens?
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We essentially restrict the model to generate text using only two tokens. Those
    are the two tokens we want to see in our output. However, we forgot to enable
    the model to generate any tokens that indicate the end of output! That is why
    the model cannot stop generating.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to fix this. We could, of course, add postprocessing
    to extract only the first token from the output generated by the language model.
    That would (mostly) fix our problem with the class labels. Look at the output,
    and you’ll see that using the first token leads to correct output in 7 of 10 cases.
    However, there is (another) problem with this approach: we’re paying to generate
    tokens that we don’t ultimately use! That’s clearly not what we want. So let’s
    tune our model even more by restricting the output length as well. All we need
    is a single token (this works only because our two possible class labels can be
    represented by a single token). That’s what the `max_tokens` parameter does. Let’s
    use it when calling our language model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题有多种方法。当然，我们可以添加后处理步骤来从语言模型生成的输出中仅提取第一个标记。这样（大多数情况下）就能解决我们关于类别标签的问题。看看输出，你会发现使用第一个标记在10个案例中有7个是正确的。然而，这种方法（还有）存在另一个问题：我们正在为最终不使用的标记付费！这显然不是我们想要的。所以让我们通过限制输出长度来进一步调整我们的模型。我们只需要一个标记（这之所以可行，是因为我们的两个可能的类别标签可以用一个标记来表示）。这就是`max_tokens`参数的作用。让我们在调用我们的语言模型时使用它：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Defines the bias'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义偏差'
- en: 'When you try it (which should be fast and not costly), you should see this
    output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试它（这应该既快又便宜）时，你应该看到以下输出：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Improves on the untuned classifier'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 改进了未调整的分类器'
- en: '#2 Reduces token usage'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 减少标记使用'
- en: 'Much better! We have improved the number of correctly handled cases from six
    (for the unturned version) to seven (**1**). That may not sound like much. However,
    thinking about the entire data set, it essentially means we have improved precision
    from 60% to 70%: that is, thousands more reviews will now be classified correctly!
    There is a caveat, of course. In reality, you should probably use a much larger
    sample. Due to random variations, the accuracy you observe on a sample may not
    be representative of the accuracy for the entire data set. To keep things simple
    (and your cost relatively low when trying it), we restrict ourselves to 10 samples
    here. As an additional bonus, our token consumption has again been reduced (**2**)
    (actually, the gap in token consumption, compared to a version without any output
    size bound, is likely to be much, much larger). Note that the two parameters discussed
    here are only a small subset of the available tuning options. You will find more
    details on relevant parameters in chapter 3\. Whenever you tune a model for a
    new task, be sure to consider all parameters that may be potentially relevant.
    Then try a few reasonable settings on a data sample to see which option performs
    best.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！我们将正确处理的案例数量从未调整版本中的六个提高到了七个（**1**）。这可能听起来不多。然而，从整个数据集的角度来看，这实际上意味着我们将精度从60%提高到了70%：也就是说，现在将有数千条评论将被正确分类！当然，这里有一个警告。在现实中，你可能需要使用一个更大的样本。由于随机变化，你在样本上观察到的准确性可能不代表整个数据集的准确性。为了简化问题（并且在你尝试时成本相对较低），我们在这里限制自己只使用10个样本。作为额外的奖励，我们的标记消耗再次减少了（**2**）（实际上，与没有任何输出大小限制的版本相比，标记消耗的差距可能要大得多）。请注意，这里讨论的两个参数只是可用调整选项的一小部分。你将在第3章中找到更多关于相关参数的详细信息。每次你为新的任务调整模型时，务必考虑所有可能相关的参数。然后在数据样本上尝试一些合理的设置，看看哪个选项表现最好。
- en: 9.4 Model selection
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 模型选择
- en: Let’s assume that we have maxed out our ability to get better performance by
    tuning our current model. What else can we do? We can, of course, select a different
    model. We saw a few GPT alternatives in the last chapter. If you can select a
    model specifically trained for the task you’re interested in (e.g., text classification),
    that’s often worth a look. Other factors that can influence your model choices
    are whether the data you plan to apply the model to is sensitive and whether sending
    that data to specific providers of language models is acceptable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经通过调整当前模型达到了性能提升的极限。我们还能做什么呢？当然，我们可以选择一个不同的模型。在上一章中，我们看到了一些GPT的替代品。如果你可以选择一个专门针对你感兴趣的任务（例如，文本分类）进行训练的模型，那么这通常值得一看。其他可能影响你模型选择的因素包括你打算应用模型的数据是否敏感，以及是否可以将这些数据发送给特定的语言模型提供商是否可接受。
- en: If you want to learn about the relative performance of different models, have
    a look at [https://crfm.stanford.edu/helm/lite/latest/](https://crfm.stanford.edu/helm/lite/latest/).
    This website contains the results of HELM, Stanford’s Holistic Evaluation of Language
    Models benchmark. The benchmark compares language models on various scenarios
    and contains results for specific tasks, as well as average performance, aggregated
    over various scenarios. You may want to check this out to get a sense of which
    models may be interesting to you. However, as various factors can influence a
    language model’s performance, it still pays to evaluate different models on the
    specific task you’re interested in.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解不同模型的相对性能，请查看 [https://crfm.stanford.edu/helm/lite/latest/](https://crfm.stanford.edu/helm/lite/latest/)。这个网站包含了
    HELM（斯坦福大学对语言模型的整体评估）的结果。该基准比较了在各种场景下的语言模型，并包含了特定任务的结果，以及在不同场景下的平均性能。你可能想查看一下，以了解哪些模型可能对你感兴趣。然而，由于各种因素都可能影响语言模型的表现，仍然值得在你感兴趣的特定任务上评估不同的模型。
- en: 'To keep things simple, let’s only consider GPT-4 as an alternative to GPT-3.5
    Turbo (which we used up to this point). Replace the name of the model in the language
    model invocation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们只考虑将 GPT-4 作为 GPT-3.5 Turbo（我们之前使用的模型）的替代品。在语言模型调用中替换模型的名称：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Defines the bias'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义了偏差'
- en: 'Running the resulting code should lead to the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 运行生成的代码应该产生以下输出：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Correct classification result'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 正确的分类结果'
- en: '#2 Best result so far'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 目前最佳结果'
- en: '#3 Same number of tokens'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 相同数量的标记'
- en: Compared to the prior version, GPT-4 manages to solve one more test case accurately
    (**1**)! That brings our accuracy to 80% (**2**), while our token consumption
    remains constant (**3**). That, by the way, is not guaranteed to be the case if
    we change the model. As different models may use different tokenizers, representing
    the same text may require a different number of tokens for different models. In
    this specific case, because GPT-4 and GPT-3.5 use the same tokenizer, the number
    of tokens does not change.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的版本相比，GPT-4 成功地准确解决了另一个测试案例（**1**）！这使我们的准确率达到了 80% （**2**），而我们的标记消耗保持不变（**3**）。顺便说一下，如果我们更改模型，这并不保证会发生。因为不同的模型可能使用不同的标记器，表示相同的文本可能需要不同数量的标记。在这个特定案例中，因为
    GPT-4 和 GPT-3.5 使用相同的标记器，所以标记的数量没有变化。
- en: Does that mean we’re paying the same amount of money? Not quite. Because GPT-4
    incurs much higher fees per token, we’re paying roughly 120 times more than before
    (the relative difference between the per-token prices of GPT-4 and GPT-3.5 Turbo).
    That’s why we’re trying to make GPT-3.5 perform as well as possible without resorting
    to GPT-4.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着我们支付了相同数量的金钱？并不完全是这样。因为 GPT-4 每个标记的收费要高得多，我们现在的支出大约是之前的 120 倍（GPT-4 和
    GPT-3.5 Turbo 每个标记价格的相对差异）。这就是为什么我们试图让 GPT-3.5 尽可能地表现良好，而不依赖于 GPT-4。
- en: 'Occasionally, during model selection and model tuning, it makes sense to look
    at the test data yourself. That gives you a better impression of the sweet spots
    and limitations of various models and enables you to judge whether the test cases
    on which your model performs badly are representative. For instance, the following
    review is solved correctly by GPT-4 but not by GPT-3.5:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择和模型调整过程中，有时亲自查看测试数据是有意义的。这让你对各种模型的优点和局限性有更好的印象，并使你能够判断你的模型表现不佳的测试案例是否具有代表性。例如，以下评论被
    GPT-4 正确解决，但 GPT-3.5 则没有：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This review contains positive (toward the end) as well as negative (the beginning)
    aspects. Although the final verdict is positive, we may conclude that spending
    more money to properly analyze borderline cases like that review is not worth
    it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇评论包含了积极（接近结尾）以及消极（开头）的方面。尽管最终结论是积极的，我们可能得出结论，为了正确分析像这篇评论这样的边缘案例而花费更多的钱是不值得的。
- en: 9.5 Prompt engineering
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 提示工程
- en: Setting aside options to swap models, what else can we do to improve performance
    with our model? One area we haven’t looked at yet is the definition of the prompt
    we use for classification. Changing the prompt template can have a significant
    effect on result quality. The fact that prompt tuning is often crucial has even
    led to the introduction of a dedicated term, *prompt engineering*, describing
    the process of searching for optimal prompt templates. What’s more, the challenges
    of prompt engineering have led to the creation of multiple platforms offering
    prompt templates for a plethora of different tasks. If you’re out of ideas for
    prompt variants, have a look at [https://promptbase.com/](https://promptbase.com/),
    [https://prompthero.com/](https://prompthero.com/), and similar platforms. The
    business model of such platforms is to enable users to buy and sell prompt templates
    that optimize the performance of specific models for specific tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了交换模型的选择，我们还能做些什么来提高我们模型的性能？我们还没有探讨的一个领域是我们用于分类的提示定义。改变提示模板可以对结果质量产生重大影响。提示调整通常至关重要的这一事实甚至导致了专门术语“提示工程”的引入，描述了搜索最佳提示模板的过程。更重要的是，提示工程带来的挑战导致了多个平台的创建，为各种不同的任务提供提示模板。如果你对提示变体没有想法，可以看看[https://promptbase.com/](https://promptbase.com/)，[https://prompthero.com/](https://prompthero.com/)和类似平台。这些平台的商业模式是使用户能够购买和销售针对特定任务优化特定模型性能的提示模板。
- en: 'Figuring out what prompt works best typically requires some experimentation.
    Next, we will focus on the basics and explore a classical technique to increase
    output quality by changing the prompt. We’re talking about few-shot learning here,
    which means we’re helping the model by giving it a few examples. That’s something
    we know from everyday life: it is often hard to understand a new task or approach
    based on a pure description alone. It is much better to see some examples to get
    the hang of it. For instance, in the previous sections, we could have just discussed
    the semantics of a few relevant model-tuning parameters. But isn’t it much better
    to see how they can be tuned in a concrete example scenario?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 确定哪个提示效果最佳通常需要一些实验。接下来，我们将关注基础知识，并探讨通过改变提示来提高输出质量的一种经典技术。我们在这里讨论的是少样本学习，这意味着我们通过给它一些示例来帮助模型。这是我们日常生活中知道的事情：仅基于纯描述很难理解一个新任务或方法。看到一些示例来掌握它要好得多。例如，在前面的章节中，我们只需讨论几个相关的模型调整参数的语义。但不是更好吗？在具体示例场景中看到它们是如何调整的？
- en: 'Of course it is. Language models “feel” the same way, and adding a few helpful
    examples can often improve their performance. So how do we show them examples?
    Easy: we specify those examples as part of the prompt. For instance, in our classification
    scenario, we want the language models to classify reviews. An example would be
    a review together with the reference class label.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当然是。语言模型“感觉”到同样的方式，添加一些有用的示例通常可以提高它们的性能。那么我们如何向它们展示示例呢？很简单：我们将这些示例指定为提示的一部分。例如，在我们的分类场景中，我们希望语言模型能够对评论进行分类。一个例子就是一个评论和参考类别标签。
- en: 'We will use the following prompt template to integrate a single sample into
    the prompt:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下提示模板将单个样本集成到提示中：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we replace the placeholders with the sample review, the sample review solution,
    and the review we’re interested in classifying, we get, for instance, the following
    prompt:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用样例评论、样例评论解决方案和我们要分类的评论替换占位符，我们得到的提示如下：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Sample review'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 样例评论'
- en: '#2 Instructions'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指令'
- en: '#3 Sample solution'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 样例解决方案'
- en: '#4 Review to classify'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 待分类评论'
- en: '#5 Instructions'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 指令'
- en: You see a sample review (**1**), instructions (**2**), and the reference class
    for the sample review (**3**). After that, you find the review we want to classify
    (**4**) and the classification instructions (again) (**5**), but no solution yet
    (of course not—that’s what we want the language model to generate). In this prompt,
    we provide exactly one example of a correctly solved task to the model. Doing
    so may help the model better understand what we’re asking it to do.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到一个样例评论（**1**），指令（**2**），以及样例评论的参考类别（**3**）。之后，你找到我们想要分类的评论（**4**）和分类指令（再次）(**5**)，但还没有解决方案（当然不是——这正是我们希望语言模型生成的）。在这个提示中，我们向模型提供了一个正确解决任务的示例。这样做可能有助于模型更好地理解我们要求它做什么。
- en: 'Of course, there are many options to provide samples in the prompt. We have
    chosen what is arguably the most straightforward solution: we use the same prompt
    structure twice for the two reviews. Because we’re using exactly the same structure,
    our prompt is slightly redundant: we repeat the task instructions (**2** and **5**),
    including the specification of the two possible class labels. Although we won’t
    do so here, it might be interesting to experiment and see whether you can integrate
    examples into the prompt in a different way, removing redundancies and reducing
    the prompt length (thereby reducing the number of tokens processed and, ultimately,
    processing fees).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在提示中提供样本有许多选项。我们选择了最简单直接的方法：我们为两个评论使用相同的提示结构两次。因为我们使用的是完全相同的结构，所以我们的提示稍微有些冗余：我们重复了任务指令（**2**
    和 **5**），包括两个可能的类别标签的指定。虽然我们在这里不会这样做，但尝试以不同的方式将示例整合到提示中，去除冗余并缩短提示长度（从而减少处理的令牌数量，最终减少处理费用）可能很有趣。
- en: 'Up to now, we have only considered adding a single example. But sometimes,
    seeing one example is not enough. That’s why it may make sense to add more than
    one example for the language model as well. Let’s assume that we have a few samples:
    reviews with associated class labels, stored in a data frame called `samples`.
    We can use the following code to generate prompts that integrate those samples:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了添加单个示例。但有时候，看到一个示例可能还不够。这就是为什么为语言模型添加多个示例也可能是有意义的。让我们假设我们有一些样本：带有相关类别标签的评论，存储在一个名为
    `samples` 的数据框中。我们可以使用以下代码生成整合这些样本的提示：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Creates a prompt for one review'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为一条评论创建提示'
- en: '#2 Generates a prompt for all reviews'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为所有评论生成提示'
- en: '#3 Integrates the samples'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 集成样本'
- en: '#4 Adds the review to classify'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 添加评论以进行分类'
- en: 'The `create_single_text_prompt` function (**1**) instantiates the following
    template for a single review:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_single_text_prompt` 函数（**1**）实例化以下模板，用于单个评论：'
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We use the same function to specify sample reviews, as well as to specify the
    review, along with the classification task that we want the language model to
    solve for us. If we specify a sample review, the `[Label]` placeholder will be
    replaced with the reference class label for the corresponding review. If we specify
    the task the language model should solve, we do not know the correct class label
    yet. In that case, we replace the `[Label]` placeholder with the empty string.
    It will be up to the language model to complete the prompt with the actual class
    label.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的函数来指定样本评论，以及指定我们希望语言模型为我们解决的问题的分类任务。如果我们指定样本评论，`[Label]` 占位符将被替换为对应评论的参考类别标签。如果我们指定语言模型应解决的问题，我们还没有正确的类别标签。在这种情况下，我们将
    `[Label]` 占位符替换为空字符串。这将由语言模型来完成，以实际类别标签完成提示。
- en: The `create_prompt` function (**2**) generates the complete prompt, considering
    all sample reviews, as well as the review we want to classify. First (**3**),
    it iterates over the sample reviews. We assume that our `samples` data frame stores
    review text in the `text` column and the associated class labels in the `sentiment`
    column. We add a prompt part (**4**) for the sample review using the `create_single_text_prompt`
    function (discussed earlier). Finally, we add instructions to classify the review
    we’re interested in.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_prompt` 函数（**2**）生成完整的提示，考虑所有样本评论以及我们想要分类的评论。首先（**3**），它遍历样本评论。我们假设我们的
    `samples` 数据框在 `text` 列中存储评论文本，在 `sentiment` 列中存储相关的类别标签。我们使用之前讨论过的 `create_single_text_prompt`
    函数（**4**）为样本评论添加一个提示部分。最后，我们添加指令来分类我们感兴趣的评论。'
- en: 'Let’s switch back to using GPT-3.5 Turbo. However, this time, we will use our
    new prompt-generation function. For the moment, we will restrict ourselves to
    a single example review in the prompt. On the book’s companion website, you can
    find training reviews with the correct class labels under Reviews Training, leading
    to the file train_reviews.csv. The reviews in this file do not overlap with those
    in the reviews.csv file (which we use to test our approach). Adding just the first
    review from train_reviews.csv as a sample to the prompts, you should now see the
    following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切换回使用 GPT-3.5 Turbo。然而，这次我们将使用我们新的提示生成函数。目前，我们将限制自己在提示中使用单个示例评论。在书的配套网站上，您可以在“评论训练”部分找到带有正确类别标签的训练评论，链接到文件
    train_reviews.csv。这个文件中的评论与 reviews.csv 文件中的评论（我们用来测试我们的方法）不重叠。将 train_reviews.csv
    中的第一条评论作为样本添加到提示中，现在您应该看到以下输出：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Equivalent to GPT-4 result'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 等同于 GPT-4 的结果'
- en: '#2 Token usage roughly doubles'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 标记使用量大约翻倍'
- en: Hooray! We have increased precision to 80% (**1**). That’s the same accuracy
    we got when using GPT-4 on the original prompts (without sample reviews). At the
    same time, our token usage has increased (**2**). More precisely, because we’re
    adding a second review to each prompt (i.e., we have one sample review and the
    review to classify), our token consumption has roughly doubled compared to the
    last version. However, compared to using GPT-4 on shorter prompts, our current
    approach is still about 60 times cheaper (because using GPT-4 is about 120 times
    more expensive than using GPT-3.5 Turbo).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 欢呼！我们已经将精度提高到 80%（**1**）。这与我们在原始提示（没有样本评论）上使用 GPT-4 所获得的精度相同。同时，我们的标记使用量也有所增加（**2**）。更准确地说，因为我们为每个提示添加了第二个评论（即，我们有一个样本评论和一个要分类的评论），与上一个版本相比，我们的标记消耗量大约翻倍。然而，与在较短的提示上使用
    GPT-4 相比，我们当前的方法仍然便宜约 60 倍（因为使用 GPT-4 大约比使用 GPT-3.5 Turbo 贵 120 倍）。
- en: 9.6 Tunable classifier
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 可调分类器
- en: Now that we have seen quite a few tuning options, you may be tempted to try
    new variations. For instance, do we still need to add bias (essentially restricting
    the output to the two possible class labels) if we’re adding samples? Can we get
    even better precision when using a larger model together with multiple samples
    in the prompt? Changing your code to try a new combination quickly becomes tedious.
    But no worries, we’ve got you covered! On the book’s website, you can find listing
    [9.2](#code__tunableClassifier) under Tunable Classifier. This implementation
    lets you try all the tuning variants by setting the right command-line parameters.
    We will quickly discuss the code, which integrates all the code variants discussed
    previously.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了很多调整选项，你可能想尝试新的变体。例如，如果我们添加了样本，是否还需要添加偏差（本质上限制输出为两个可能的类别标签）？当我们使用更大的模型并在提示中添加多个样本时，能否获得更高的精度？将你的代码更改为尝试新的组合很快就会变得繁琐。但不用担心，我们已经为你准备好了！在本书的网站上，你可以在“可调分类器”部分找到列表
    [9.2](#code__tunableClassifier)。这个实现允许你通过设置正确的命令行参数来尝试所有调整变体。我们将快速讨论代码，该代码整合了之前讨论的所有代码变体。
- en: Generating prompts (**1**) works as described in the last section. The `create_prompt`
    function takes the review text to classify and sample reviews as input. The sample
    reviews are added to the prompt, potentially supporting the language models in
    classifying the review we’re interested in. Note that we can still see how the
    language model performs without any samples (by not specifying any samples). Classification
    without any samples corresponds to a special case.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成提示（**1**）的工作方式如上一节所述。`create_prompt` 函数接受要分类的评论文本和样本评论作为输入。样本评论被添加到提示中，可能支持语言模型对感兴趣评论的分类。请注意，我们仍然可以在不指定任何样本的情况下看到语言模型的表现（通过不指定任何样本）。没有样本的分类对应于一个特殊情况。
- en: Listing 9.2 Tunable version of sentiment classifier
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2 情感分类器的可调版本
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Generates prompts with samples'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用样本生成提示'
- en: '#2 Calls language models with parameters'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用参数调用语言模型'
- en: '#3 Parses command-line parameters'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 解析命令行参数'
- en: '#4 Reads samples from disk'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 从磁盘读取样本'
- en: '#5 Classifies the review'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 分类评论'
- en: '#6 Updates the counters'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 更新计数器'
- en: '#7 Prints out the counters'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 打印计数器'
- en: 'Our `call_llm` function (**2**) integrates all the tuning parameters mentioned
    earlier. First is the name of the model to call (the `model` parameter). Second,
    we can specify the maximum number of output tokens (`max_tokens`). Finally, we
    can specify bias: tokens that should be prioritized when generating output. The
    `out_tokens` parameter allows users to specify a comma-separated list of token
    IDs to which we assign a high priority (essentially limiting output to one of
    these tokens). Although the model name is required, setting a value of `0` for
    the `max_tokens` parameter and the empty string for the `out_tokens` parameter
    allows us to avoid changing OpenAI’s default settings.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `call_llm` 函数（**2**）整合了之前提到的所有调整参数。首先是调用模型的名称（`model` 参数）。其次，我们可以指定最大输出标记数（`max_tokens`）。最后，我们可以指定偏差：在生成输出时应优先考虑的标记。`out_tokens`
    参数允许用户指定一个以逗号分隔的标记 ID 列表，我们将为这些标记分配高优先级（本质上限制输出为这些标记之一）。尽管模型名称是必需的，但将 `max_tokens`
    参数设置为 `0` 和将 `out_tokens` 参数设置为空字符串允许我们避免更改 OpenAI 的默认设置。
- en: 'The tunable classifier uses quite a few command-line parameters (**3**). Let’s
    discuss them in the order in which you need to specify them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可调分类器使用相当多的命令行参数（**3**）。让我们按照你需要指定的顺序来讨论它们：
- en: '`file_path`—Path to the .csv file containing reviews used to evaluate our language
    model'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`file_path`—包含用于评估我们的语言模型的评论的.csv文件的路径'
- en: '`model`—Name of the language model we want to use (e.g., `gpt-3.5-turbo`)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`—我们想要使用的语言模型的名称（例如，`gpt-3.5-turbo`)'
- en: '`max_tokens`—Maximum number of output tokens to generate per input review'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_tokens`—每个输入评论生成输出标记的最大数量'
- en: '`out_tokens`—A comma-separated list of tokens to prioritize when generating
    output'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_tokens`—在生成输出时优先考虑的标记的逗号分隔列表'
- en: '`nr_samples`—Number of review samples with solutions to integrate into each
    prompt'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nr_samples`—要整合到每个提示中的评论样本数量'
- en: '`sample_path`—Path to the .csv file containing reviews with correct class labels
    to use as samples (this can be empty if the `nr_samples` parameter is set to `0`)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_path`—包含带有正确类别标签的评论的.csv文件的路径，用作样本（如果将`nr_samples`参数设置为`0`，则可以为空）'
- en: Warning Limiting the number of output tokens is almost always a good idea. In
    particular, you should do it whenever biasing output toward specific tokens without
    including any of the “stop” tokens (indicating the end of output).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 限制输出标记的数量几乎总是个好主意。特别是，你应该在将输出偏向特定标记而不包括任何“停止”标记（表示输出结束）时这样做。
- en: After parsing input parameters, the classifier reads samples from disk (**4**)
    and classifies reviews (**5**) while updating counters (**6**) that are ultimately
    printed (**7**).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析输入参数后，分类器从磁盘读取样本（**4**）并对评论进行分类（**5**），同时更新最终打印的计数器（**6**）。
- en: 'Let’s see how we can simulate all the different versions of our classifier
    that we have discussed so far. Using the following invocation should give us the
    untuned version of our classifier, assuming that the file reviews.csv is located
    in the same directory as the code itself:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何模拟我们迄今为止讨论的所有不同版本的分类器。使用以下调用应该会给出我们未调整的分类器版本，假设reviews.csv文件位于代码本身所在的目录中：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we don’t specify any tokens to prioritize (we specify the empty string),
    don’t restrict the output length (setting it to `0` means no restrictions), and
    set the number of samples in the prompt to `0` (which means we can set the path
    to the file containing samples to the empty string as well).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有指定任何优先考虑的标记（我们指定空字符串），不限制输出长度（将其设置为`0`表示没有限制），并将提示中的样本数量设置为`0`（这意味着我们也可以将包含样本的文件路径设置为空字符串）。
- en: 'The following command, on the other hand, will give us the version that restricts
    the output length while prioritizing the tokens that correspond to our class labels:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，以下命令将给出一个限制输出长度同时优先考虑对应于我们类别标签的标记的版本：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can get the last version we discussed, using one sample per prompt
    while tuning the model as before, via the following command (assuming the file
    train_reviews.csv is located in the same repository as the code):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过以下命令获取我们讨论的最后版本，每次提示使用一个样本，同时像以前一样调整模型（假设train_reviews.csv文件位于与代码相同的存储库中）：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Feel free to try new combinations that we haven’t discussed!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试我们尚未讨论的新组合吧！
- en: 9.7 Fine-tuning
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 微调
- en: So far, we have done everything in our power to squeeze the best performance
    out of existing models. Those models have been trained for tasks that are, perhaps,
    similar but not *exactly* like the one we’re interested in. Wouldn’t it be nice
    to get a model customized specifically for our task? That is possible when using
    fine-tuning. Let’s see how to implement fine-tuning with OpenAI’s models in practice.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经尽我们所能从现有模型中榨取最佳性能。这些模型已经针对可能与我们感兴趣的任务相似但不完全相同的目标进行了训练。如果能够得到一个专门针对我们任务的定制模型，那岂不是很好？使用微调就可以实现这一点。让我们看看如何在实践中使用OpenAI的模型实现微调。
- en: Fine-tuning means we take an existing model, such as OpenAI’s GPT-3.5 Turbo
    model, and specialize it for a task we’re interested in. Of course, in principle,
    we could train our model from scratch. But that is typically prohibitively expensive,
    and in addition, we usually don’t find enough task-specific training data to sustain
    a large model during training. That’s why it is much better to rely on fine-tuning.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 微调意味着我们取一个现有的模型，例如OpenAI的GPT-3.5 Turbo模型，并使其专门针对我们感兴趣的任务。当然，从原则上讲，我们可以从头开始训练我们的模型。但那通常成本过高，而且此外，我们通常找不到足够的特定任务训练数据来在训练期间维持一个大模型。这就是为什么依赖微调要好得多。
- en: Fine-tuning is typically the last thing we try when maximizing performance for
    a specific task. The reason is that fine-tuning requires a certain upfront investment
    in terms of time and money. During fine-tuning, we pay OpenAI to create a customized
    version of one of its base models just for our task. The price is based on the
    size of the training data and the number of times that training data is read (i.e.,
    the number of *epochs*). For example, at the time of writing, fine-tuning GPT-3.5
    Turbo costs about 0.8 cents per 1,000 tokens of training data and epoch. Also,
    after fine-tuning, we pay to use the fine-tuned model. The price per token is
    higher for the fine-tuned model than for the base version. That makes sense as,
    at least in theory, the fine-tuned model should perform better for our specific
    task.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 微调通常是我们为了最大化特定任务的性能而尝试的最后一件事。原因是微调需要在时间和金钱上做出一定的前期投资。在微调过程中，我们支付OpenAI为其基础模型之一创建一个定制的版本，专门用于我们的任务。价格基于训练数据的大小和训练数据被读取的次数（即，*epoch*的数量）。例如，在撰写本文时，微调GPT-3.5
    Turbo的费用大约是每1,000个训练数据token和epoch 0.8美分。此外，微调后，我们还需要支付使用微调模型的费用。与基础版本相比，微调模型的每token价格更高。这在理论上是有道理的，因为至少在理论上，微调模型应该在我们的特定任务上表现更好。
- en: One possible advantage of fine-tuning is that we improve the accuracy of the
    model output. Another possible advantage is that we may be able to shorten our
    prompts. When using a generic model, the prompt needs to contain a description
    of the task to perform (along with all relevant data). On the other hand, our
    fine-tuned model should be specialized to perform a single task and perform well
    on it. If the model only needs to do one task, in principle it should be possible
    to leave the task description out of the prompt because it is implicit. Besides
    the task description, we can leave out other information that is helpful for a
    generic model but not required for a specialized one. For instance, it may be
    necessary to integrate samples into the prompt for the generic model to obtain
    reasonable output quality, whereas that is unnecessary for the fine-tuned version.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的一个可能优势是提高模型输出的准确性。另一个可能的优势是，我们可能能够缩短我们的提示。当使用通用模型时，提示需要包含要执行的任务的描述（以及所有相关数据）。另一方面，我们的微调模型应该专门用于执行单个任务，并且在该任务上表现良好。如果模型只需要执行一个任务，原则上应该可以省略任务描述，因为它已经是隐含的。除了任务描述之外，我们还可以省略对通用模型有帮助但不是专门模型所必需的其他信息。例如，对于通用模型来说，可能需要将样本集成到提示中，以获得合理的输出质量，而对于微调版本来说，这可能是多余的。
- en: 'In our specific scenario, we want to map reviews to a class label (based on
    the underlying sentiment of the review author). Previously, we specified the classification
    task as part of the prompt (and even provided some helpful examples). Now, perhaps,
    when fine-tuning a model, we can leave out those instructions. More precisely,
    we may no longer need to use prompts like the following (a prompt containing sample
    reviews (**1**) with instructions (**2**) and sample solutions (**3**), along
    with the review to classify (**4**) and corresponding instructions (**5**)):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特定场景中，我们希望将评论映射到类标签（基于评论作者的潜在情感）。之前，我们将分类任务作为提示的一部分进行了指定（甚至提供了一些有用的示例）。现在，也许在微调模型时，我们可以省略这些指令。更准确地说，我们可能不再需要使用以下提示（包含样本评论（**1**）、指令（**2**）和样本解决方案（**3**），以及要分类的评论（**4**）和相应的指令（**5**））：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Sample review'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 样本评论'
- en: '#2 Instructions'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指令'
- en: '#3 Sample solution'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 样本解决方案'
- en: '#4 Review to classify'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 分类评论'
- en: '#5 Instructions'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 指令'
- en: 'Instead, we can assume that the model implicitly knows that it should classify
    reviews and which class labels are available. Under that assumption, we can simplify
    the prompt to this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以假设模型隐含地知道它应该对评论进行分类以及哪些类标签是可用的。基于这个假设，我们可以将提示简化为以下内容：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This prompt merely states the review that we want to classify. We assume that
    all other task-specific information (such as instructions and samples) is already
    implicitly known to the model. As you certainly noticed, this prompt is much shorter
    than the previous version. That means we *may* save money when using the fine-tuned
    model instead of the base version. On the other hand, keep in mind that using
    the fine-tuned model is more expensive per token than using the base version.
    We postpone the corresponding calculations to later. But first, let’s see whether
    we can even make such concise prompts work in practice via fine-tuning.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示仅仅陈述了我们想要分类的评论。我们假设所有其他特定任务的信息（例如指令和样本）模型已经隐含地知道。正如你肯定注意到的，这个提示比之前的版本要短得多。这意味着当我们使用微调模型而不是基础版本时，我们可能会节省一些钱。另一方面，请记住，使用微调模型比使用基础版本每token的成本更高。我们将相应的计算推迟到以后。但首先，让我们看看我们是否可以通过微调使这样的简洁提示在实践上发挥作用。
- en: 9.8 Generating training data
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 生成训练数据
- en: First we have to generate our training data for fine-tuning. We will use the
    reviews with associated class labels contained in the file train_reviews.csv,
    available on the companion website under Review Training. OpenAI expects training
    data for fine-tuning in a very specific format. Before we can fine-tune, we need
    to transform our .csv data into the required format.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须生成我们的微调训练数据。我们将使用包含在文件train_reviews.csv中的评论及其关联的标签，该文件可在配套网站上的“评论训练”部分找到。OpenAI期望微调的训练数据具有一个非常特定的格式。在我们能够微调之前，我们需要将我们的.csv数据转换为所需的格式。
- en: 'Training data for fine-tuning OpenAI’s chat models generally takes the form
    of successful interactions with the model (i.e., examples where the model produces
    the output we ideally want it to produce). In the case of OpenAI’s chat models,
    such interactions are described via message histories. Each message is described
    by a Python dictionary object. For instance, the following describes a successful
    completion, given the earlier example review as input:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 微调OpenAI的聊天模型的数据通常采用与模型成功交互的形式（即，模型产生我们理想中想要产生的输出的例子）。在OpenAI的聊天模型的情况下，这种交互通过消息历史来描述。每条消息都由一个Python字典对象描述。例如，以下描述了一个成功完成，给定之前的示例评论作为输入：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is a negative review (i.e., the review author does not want to recommend
    the movie), and therefore, we ideally want the model to generate a message that
    contains the single token `neg`. That’s the interaction depicted here.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个负面评论（即，评论作者不想推荐这部电影），因此，我们理想中希望模型生成包含单个token `neg` 的消息。这就是这里描述的交互。
- en: To make fine-tuning worth it, you typically want to use at least 50 samples
    and up to a few thousand samples. Using more samples for fine-tuning can improve
    performance but is also more expensive. On the other hand, this is a one-time
    fee because you can reuse the same fine-tuned model for a potentially large data
    set (and the usage fees for the fine-tuned model do not depend on the amount of
    training data used for fine-tuning). The example file (reviews_train.csv) contains
    100 samples and is therefore within the range of data sizes where fine-tuning
    may become useful.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使微调变得值得，你通常至少需要使用50个样本，最多几千个样本。使用更多样本进行微调可以提高性能，但成本也更高。另一方面，这是一个一次性费用，因为你可以为可能的大型数据集重复使用相同的微调模型（并且微调模型的费用不取决于用于微调的训练数据量）。示例文件（reviews_train.csv）包含100个样本，因此它位于微调可能变得有用的数据大小范围内。
- en: 'OpenAI expects data for fine-tuning in JSON-lines format (such files typically
    have the suffix .jsonl). Files that comply with this format essentially contain
    one Python dictionary in each line. In this case, each line describes one successful
    interaction with the model (using the same format as in the previous example).
    To handle JSON-lines files more easily from Python, we will use the `jsonlines`
    library. As a first step, go to the terminal and install the library using the
    following command:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI期望微调数据以JSON-line格式（这类文件通常有.jsonl后缀）。符合此格式的文件实际上每行包含一个Python字典。在这种情况下，每行描述了与模型的一次成功交互（使用与上一个示例相同的格式）。为了更容易地从Python处理JSON-line文件，我们将使用`jsonlines`库。作为第一步，前往终端并使用以下命令安装库：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now we can use the library to transform our .csv data into the format required
    by OpenAI. Listing [9.3](#code__createTrainingData) uses the `get_samples` function
    (**1**) to prepare samples in the required format. The input is a pandas DataFrame
    (`df` parameter) containing the training samples in the usual format (we assume
    that the `text` column contains the reviews and the `sentiment` column contains
    the associated class labels). We turn each sample into a successful message exchange
    with the model. First, we create the message sent by the user (**2**), which only
    includes the review text. Second, we create the desired answer message to generate
    by the model (associated with the “assistant” role) (**3**). The full set of training
    samples is a list of message exchanges, each prepared in the previously mentioned
    format.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用库将我们的 .csv 数据转换为 OpenAI 所需的格式。列表 [9.3](#code__createTrainingData) 使用
    `get_samples` 函数（**1**）准备所需格式的样本。输入是一个包含训练样本的 pandas DataFrame（`df` 参数），这些样本以通常的格式表示（我们假设
    `text` 列包含评论，而 `sentiment` 列包含相关的类标签）。我们将每个样本转换为与模型成功消息交换。首先，我们创建用户发送的消息（**2**），它仅包括评论文本。其次，我们创建模型要生成的期望答案消息（与“助手”角色相关）（**3**）。完整的训练样本集是一系列消息交换，每个都以前述格式准备。
- en: Listing 9.3 Generating training data for fine-tuning
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3 为微调生成训练数据
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Generates training data'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 生成训练数据'
- en: '#2 Creates a user message'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建用户消息'
- en: '#3 Creates an assistant message'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建助手消息'
- en: '#4 Parses the command-line arguments'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 解析命令行参数'
- en: '#5 Stores the training data in new format'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 以新格式存储训练数据'
- en: Listing [9.3](#code__createTrainingData) expects as input a path to the .csv
    file with training samples, as well as the path to the output file (**4**). The
    output file follows the JSON-lines format, so we ideally assign an output path
    ending with .jsonl. After transforming the input .csv file into the fine-tuning
    format, we use the `jsonlines` library to write the transformed samples into the
    JSON-lines file (**5**).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [9.3](#code__createTrainingData) 预期输入一个包含训练样本 .csv 文件的路径，以及输出文件的路径（**4**）。输出文件遵循
    JSON-lines 格式，因此我们理想地分配一个以 .jsonl 结尾的输出路径。在将输入 .csv 文件转换为微调格式后，我们使用 `jsonlines`
    库将转换后的样本写入 JSON-lines 文件（**5**）。
- en: 'As usual, you don’t need to enter the code for this listing. You can find it
    on the website under Prepare Fine-Tuning. Run it from the terminal using the following
    command (we assume that the file train_reviews.csv is located in the same repository
    as the code):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，您不需要为此列表输入代码。您可以在网站上的“准备微调”部分找到它。使用以下命令在终端中运行它（我们假设 train_reviews.csv
    文件位于与代码相同的存储库中）：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You may want to manually inspect the train_reviews.jsonl file that was (hopefully)
    generated by running this command. You should see one training sample on each
    line, represented as a Python dictionary.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要手动检查由运行此命令（希望）生成的 train_reviews.jsonl 文件。您应该看到每行一个训练样本，以 Python 字典的形式表示。
- en: 9.9 Starting a fine-tuning job
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.9 开始微调作业
- en: Now that we have our training data in the right format, we can create a fine-tuning
    job on OpenAI’s platform. Of course, because the model is stored only on OpenAI’s
    platform, we cannot do the fine-tuning ourselves. Instead, we send our training
    data to OpenAI and request to use that data to create a customized model. To create
    a customized model, we must first choose a base model. In this case, we will start
    from the GPT-3.5 Turbo model (which makes it easier to compare with the results
    we have obtained so far).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将训练数据格式化正确，我们可以在 OpenAI 的平台上创建一个微调作业。当然，因为模型仅存储在 OpenAI 的平台上，我们无法自行进行微调。相反，我们将我们的训练数据发送给
    OpenAI，并请求使用这些数据创建一个定制模型。要创建一个定制模型，我们首先必须选择一个基础模型。在这种情况下，我们将从 GPT-3.5 Turbo 模型开始（这使得与迄今为止获得的结果进行比较更容易）。
- en: 'We can create a fine-tuning job using the following code snippet (assuming
    that `in_path` is the path to the file containing training data):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码片段创建微调作业（假设 `in_path` 是包含训练数据的文件的路径）：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `reply` object will contain a Python object with metadata about our fine-tuning
    job (assuming the job creation succeeds). Most importantly, we get the ID of the
    job we just created in the `reply.id` field. Fine-tuning jobs typically take a
    while (around 15 minutes is typical for the fine-tuning job we describe here).
    That means we have to wait until our fine-tuned model has been created. The job
    ID allows us to verify the status of our fine-tuning job and retrieve the ID of
    the freshly created model once it is available. We can retrieve status information
    about our fine-tuning job using the following piece of Python code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`reply` 对象将包含有关我们微调作业的元数据的 Python 对象（假设作业创建成功）。最重要的是，我们在 `reply.id` 字段中获取了我们刚刚创建的作业的
    ID。微调作业通常需要一段时间（我们描述的微调作业通常需要大约 15 分钟）。这意味着我们必须等待我们的微调模型被创建。作业 ID 允许我们验证微调作业的状态，并在模型可用时检索新创建的模型的
    ID。我们可以使用以下 Python 代码检索关于微调作业的状态信息：'
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `reply.status` field reports the status of the fine-tuning job, which will
    eventually reach the value `succeeded`. After that has happened, we can retrieve
    the ID of the fine-tuned model in `reply.fine_tuned_model`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`reply.status` 字段报告微调作业的状态，最终将达到 `succeeded` 值。在那之后，我们可以在 `reply.fine_tuned_model`
    中检索微调模型的 ID。'
- en: Listing [9.4](#code__fineTune) starts the fine-tuning process, waits until the
    corresponding job finishes, and finally prints out the ID of the generated model.
    Given a path to a file containing training data, the code first uploads the file
    containing training data (**1**). It retrieves the file ID assigned by OpenAI
    and uses it to create a fine-tuning job (**2**). Then, we iterate until the fine-tuning
    job completes successfully (**3**). In each iteration, we print out a timer (measuring
    seconds since the start of the fine-tuning job) and check for status updates with
    regard to the job (**4**). Finally, we retrieve the model ID and print it (**5**).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [9.4](#code__fineTune) 开始微调过程，等待相应的作业完成，并最终打印出生成的模型的 ID。给定包含训练数据的文件路径，代码首先上传包含训练数据的文件（**1**）。它检索
    OpenAI 分配的文件 ID，并使用它来创建微调作业（**2**）。然后，我们迭代直到微调作业成功完成（**3**）。在每次迭代中，我们打印出一个计时器（测量自微调作业开始以来的秒数）并检查作业的状态更新（**4**）。最后，我们检索模型
    ID 并打印它（**5**）。
- en: Listing 9.4 Fine-tuning a GPT model using training data
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.4 使用训练数据微调 GPT 模型
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Uploads training data to OpenAI'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将训练数据上传到 OpenAI'
- en: '#2 Creates a fine-tuning job'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建微调作业'
- en: '#3 Iterates until the job completes'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 循环直到作业完成'
- en: '#4 Gets the job status'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取作业状态'
- en: '#5 Retrieves the ID of the fine-tuned model'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 获取微调模型的 ID'
- en: 'You can find the code on the website under Start Fine-Tuning. Run it using
    the following command (where train_reviews.jsonl is the previously generated file):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在“开始微调”网页上找到代码。使用以下命令运行它（其中 train_reviews.jsonl 是之前生成的文件）：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If you run the script to completion, you will see output such as the following
    (this is, of course, just part of the output; dots represent missing lines):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行脚本直到完成，你将看到如下输出（这当然只是输出的一部分；点代表缺失的行）：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: After printing out the job ID, we receive regular updates on the job status,
    typically proceeding from `validating_files` to `running` to (hopefully) `succeeded`.
    The problem is that the job may take a while to finish (for the previous example,
    about 14 minutes). If you don’t want to run the script continuously (e.g., to
    switch off your computer), you can interrupt the script after the fine-tuning
    job has started (you will know because the script prints out the job ID at that
    point). The fine-tuning job will proceed as planned on OpenAI’s servers. Depending
    on your setup, you may even receive an email notifying you once the job has finished.
    Otherwise, you can periodically run this script.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在打印出作业 ID 后，我们会定期收到关于作业状态的更新，通常是从 `validating_files` 到 `running`，然后（希望）到 `succeeded`。问题是作业可能需要一段时间才能完成（对于前面的例子，大约需要
    14 分钟）。如果你不想连续运行脚本（例如，为了关闭电脑），你可以在微调作业开始后中断脚本（你将知道因为脚本在那个点打印出了作业 ID）。微调作业将在 OpenAI
    的服务器上按计划进行。根据你的设置，你甚至可能会收到一封电子邮件通知你作业已完成。否则，你可以定期运行此脚本。
- en: Listing 9.5 Checking for the status of fine-tuning jobs
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5 检查微调作业的状态
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Retrieves and prints the job metadata'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取并打印作业元数据'
- en: Given the job ID (retrieved from the output of listing [9.4](#code__fineTune)),
    the script retrieves and prints the job metadata (**1**), including the job status
    and the ID of the resulting model (after the job has finished successfully).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 给定工作ID（从列出[9.4](#code__fineTune)的输出中检索），脚本检索并打印工作元数据（**1**），包括工作状态和成功完成工作后的结果模型ID。
- en: 9.10 Using the fine-tuned model
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.10 使用微调模型
- en: 'Congratulations! You have created a specialized model, fine-tuned to the task
    (review classification) you care about. How can you use it? Fortunately, doing
    so is straightforward using the OpenAI library. Instead of specifying the name
    of one of the standard models (e.g., `gpt-3.5-turbo`), we now specify the ID of
    our fine-tuned model, like so (replace the placeholder `[Fine-tuned model ID]`
    with the actual model ID):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经创建了一个专门化的模型，针对你关心的任务（审查分类）进行了微调。你该如何使用它？幸运的是，使用OpenAI库这样做很简单。我们不再指定标准模型之一（例如，`gpt-3.5-turbo`）的名称，而是现在指定我们微调模型的ID，如下所示（将占位符`[Fine-tuned
    model ID]`替换为实际的模型ID）：
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As before, we assume that the `prompt` variable contains the prompt text. The
    prompts, however, differ for our fine-tuned model. Previously, we described the
    classification task, along with the review text. Now we have trained our custom
    model to map the review text alone to an appropriate class. That means our prompt-generation
    function simplifies to the following (in fact, you might argue that creating a
    dedicated function is no longer required):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们假设`prompt`变量包含提示文本。然而，对于我们的微调模型，提示有所不同。以前，我们描述了分类任务，包括审查文本。现在我们已训练我们的自定义模型，将审查文本单独映射到适当的类别。这意味着我们的提示生成函数简化为以下内容（实际上，你可能会争论创建专用函数不再需要）：
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Instead of generating multipart prompts, we return the review text to classify.
    You may want to find out what happens when using the simplified prompt with the
    original model (`gpt-3.5-turbo`). You will see output like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是生成多部分提示，而是将审查文本返回以进行分类。你可能想了解使用简化提示与原始模型（`gpt-3.5-turbo`）时会发生什么。你将看到如下输出：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Clearly, the model gets confused about our intentions—that is, what we expect
    it to do with the input reviews. Instead of generating correct class labels, it
    writes elaborate analyses commenting on the primary points raised in the reviews.
    This is not unexpected. Imagine if someone handed you a review without any further
    instructions. How would you know that the person wanted you to classify the review,
    let alone the correct labels of the possible classes? It would be almost impossible
    to do so, and the same applies to language models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型对我们的意图感到困惑——也就是说，我们期望它如何处理输入的审查。它没有生成正确的类别标签，而是写下了详尽的评论，评论了审查中提出的主要观点。这是意料之中的。想象一下，如果有人给你一份没有进一步指示的审查，你会知道那个人想让你分类审查，更不用说可能的类别的正确标签了？这几乎是不可能的，语言模型也是如此。
- en: 'However, if we switch to our fine-tuned model and provide the same prompts
    as input, we will get the following output instead:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们切换到我们的微调模型并使用相同的提示作为输入，我们将得到以下输出而不是：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#1 Improved accuracy'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 提高准确性'
- en: '#2 Lower token consumption'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 降低令牌消耗'
- en: Note that even without setting any tuning parameters (or providing any samples
    in the prompt), we now get an accuracy of 70% (**1**), rather than the 60% in
    our original version! Also, the number of tokens used is reduced by about 200
    compared to the initial version (**2**). This is because we omit the instructions
    (and class labels) in each prompt.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使没有设置任何微调参数（或在提示中提供任何样本），我们现在得到的准确率是70%（**1**），而不是原始版本的60%！而且，与初始版本相比，使用的令牌数量减少了大约200（**2**）。这是因为我们在每个提示中省略了指令（和类别标签）。
- en: 'Okay! We have seen that we can fine-tune a model to classify reviews accurately
    while reducing the prompt size. But the question remains: Was it worth it? Let’s
    do some calculations to find that out. We set aside the cost of generating the
    fine-tuned model because we only have to do that once (and in our example scenario,
    we assume that we want to analyze one year’s worth of reviews). Without fine-tuning,
    we can achieve the same accuracy (70%) with the generic model when exploiting
    tuning parameters (setting bias and a limit on the number of output tokens). In
    that case, we use 2,228 tokens for our 10 sample reviews. After fine-tuning, we
    only use 2,085 tokens for our sample reviews. However, with the generic model,
    we pay 0.05 cents per 1,000 input tokens. On the other hand, for the fine-tuned
    model, we pay 0.3 cents per 1,000 tokens. That means our cost per token is six
    times higher after fine-tuning! The moderate decrease in the number of tokens
    processed does not amortize the higher fees per token in this specific scenario.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！我们已经看到，我们可以微调一个模型以准确分类评论，同时减少提示大小。但问题仍然存在：这是否值得？让我们做一些计算来找出答案。我们暂时不考虑微调模型的成本，因为我们只需要做一次（在我们的示例场景中，我们假设我们想要分析一年的评论）。在不进行微调的情况下，当利用调整参数（设置偏差和输出标记数的限制）时，我们可以使用通用模型达到相同的准确率（70%）。在这种情况下，我们为我们的10个样本评论使用了2,228个标记。微调后，我们只为我们的样本评论使用了2,085个标记。然而，对于通用模型，我们每1,000个输入标记支付0.05美分。另一方面，对于微调模型，我们每1,000个标记支付0.3美分。这意味着微调后我们的每标记成本提高了六倍！在这个特定场景中，处理标记数的适度减少并不能抵消每标记更高费用的增加。
- en: In general, fine-tuning can be very helpful in increasing quality and possibly
    reducing costs. However, be aware that it comes with various overheads. Before
    using a fine-tuned model in production, evaluate it experimentally, do your calculations,
    and make sure it is worth it!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，微调在提高质量和可能降低成本方面非常有帮助。然而，请注意，它伴随着各种开销。在生产中使用微调模型之前，请进行实验评估，进行计算，并确保它是值得的！
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Tuning parameter settings can influence model performance and cost.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整参数设置可以影响模型性能和成本。
- en: Consider limiting output length and introducing token logit bias.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑限制输出长度并引入标记对数偏差。
- en: Do not always use the largest available model, as doing so increases cost.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要总是使用可用的最大模型，因为这样做会增加成本。
- en: Identify the best model for your task by evaluating it on samples.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在样本上评估来确定最适合您任务的模型。
- en: The design of the prompt can have a significant effect on performance.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示的设计可以对性能产生重大影响。
- en: Include samples of correctly solved tasks in the prompt for few-shot learning.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中包含正确解决的任务样本，以进行少样本学习。
- en: Fine-tuning allows you to specialize base models to the tasks you care about.
    It may allow you to reduce prompt size due to specialization.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调允许您将基础模型专门化到您关心的任务上。它可能允许您通过专门化来减少提示大小。
- en: Fine-tuning incurs overhead proportional to the amount of data trained on. It
    also increases the cost per token when you use the resulting model.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调产生的开销与训练数据量成正比。当您使用生成的模型时，它还会增加每标记的成本。
