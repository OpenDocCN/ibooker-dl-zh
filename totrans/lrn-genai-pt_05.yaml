- en: 4 Image generation with generative adversarial networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用生成对抗网络进行图像生成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Designing a generator by mirroring steps in the discriminator network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过镜像判别网络中的步骤来设计生成器
- en: How a 2D convolutional operation works on an image
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D卷积操作在图像上的工作原理
- en: How a 2D transposed convolutional operation inserts gaps between the output
    values and generates feature maps of a higher resolution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D转置卷积操作如何在输出值之间插入间隔并生成更高分辨率的特征图
- en: Building and training generative adversarial networks to generate grayscale
    and color images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练生成对抗网络以生成灰度和彩色图像
- en: 'You have successfully generated an exponential growth curve and a sequence
    of integers that are all multiples of 5 in chapter 3\. Now that you understand
    how generative adversarial networks (GANs) work, you are ready to apply the same
    skills to generate many other forms of content, such as high-resolution color
    images and realistic-sounding music. However, this may be easier said than done
    (you know what they say: the devil is in the details). For example, exactly how
    can we make the generator conjure up realistic images out of thin air? That’s
    the question we’re going to tackle in this chapter.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，你已经成功生成了一个指数增长曲线和一系列都是5的倍数的整数。现在，既然你已经理解了生成对抗网络（GANs）的工作原理，你就可以应用相同的技能来生成许多其他形式的内容，例如高分辨率彩色图像和听起来逼真的音乐。然而，这说起来容易做起来难（你知道他们怎么说：魔鬼藏在细节里）。例如，我们究竟如何让生成器凭空创造出逼真的图像呢？这正是本章将要解决的问题。
- en: A common approach for the generator to create images from scratch is to mirror
    steps in the discriminator network. In the first project in this chapter, your
    goal is to create grayscale images of clothing items such as coats, shirts, sandals,
    and so on. You learn to mirror the layers in the discriminator network when designing
    a generator network. In this project, only dense layers are used in both the generator
    and discriminator networks. Each neuron in a dense layer is connected to every
    neuron in the previous and next layer. For this reason, dense layers are also
    called fully connected layers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器从零开始创建图像的常见方法是通过镜像判别网络中的步骤。在本章的第一个项目中，你的目标是创建如外套、衬衫、凉鞋等服装物品的灰度图像。你将学习在设计生成器网络时如何镜像判别网络中的层。在这个项目中，生成器和判别网络都只使用了密集层。密集层中的每个神经元都与前一层的每个神经元和后一层的每个神经元相连。因此，密集层也被称为全连接层。
- en: In the second project in this chapter, your goal is to create high-resolution
    color images of anime faces. Like in the first project, the generator mirrors
    the steps in the discriminator network to conjure up images. However, high-resolution
    color images in this project contain many more pixels than the low-resolution
    grayscale images in the first project. If we use dense layers only, the number
    of parameters in the model increases enormously. This, in turn, makes learning
    slow and ineffective. We, therefore, turn to convolutional neural networks (CNNs).
    In CNNs, each neuron in a layer is connected only to a small region of the input.
    This local connectivity reduces the number of parameters, making the network more
    efficient. CNNs require fewer parameters than fully connected networks of similar
    size, leading to faster training times and lower computational costs. CNNs are
    also generally more effective at capturing spatial hierarchies in image data because
    they treat images as multidimensional objects instead of 1D vectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二个项目中，你的目标是创建动漫面孔的高分辨率彩色图像。就像在第一个项目中一样，生成器通过镜像判别网络中的步骤来生成图像。然而，在这个项目中，高分辨率彩色图像包含的像素比第一个项目中的低分辨率灰度图像要多得多。如果我们只使用密集层，模型的参数数量会急剧增加。这反过来又使得学习变得缓慢且无效。因此，我们转向卷积神经网络（CNNs）。在CNNs中，一个层中的每个神经元仅与输入的小区域相连。这种局部连接减少了参数数量，使得网络更加高效。与相同大小的全连接网络相比，CNNs需要的参数更少，这导致了更快的训练时间和更低的计算成本。CNNs通常在捕捉图像数据中的空间层次结构方面也更为有效，因为它们将图像视为多维对象，而不是一维向量。
- en: To prepare you for the second project, we’ll show you how convolutional operations
    work and how they downsample the input images and extract spatial features in
    them. You’ll also learn concepts such as filter size, stride, and zero-padding
    and how they affect the degree of downsampling in CNNs. While the discriminator
    network uses convolutional layers, the generator mirrors these layers by using
    transposed convolutional layers (also known as deconvolution or upsampling layers).
    You’ll learn how transposed convolutional layers are used for upsampling to generate
    high-resolution feature maps.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你为第二个项目做好准备，我们将向你展示卷积操作是如何工作的，以及它们如何对输入图像进行下采样并从中提取空间特征。你还将学习诸如滤波器大小、步长和零填充等概念，以及它们如何影响CNN中的下采样程度。虽然判别器网络使用卷积层，但生成器通过使用转置卷积层（也称为反卷积或上采样层）来镜像这些层。你将学习转置卷积层是如何用于上采样以生成高分辨率特征图的。
- en: To summarize, you’ll learn how to mirror the steps in the discriminator network
    to create images from scratch in this chapter. In addition, you’ll learn how convolutional
    layers and transposed convolutional layers work. After this chapter, you’ll use
    convolutional layers and transposed convolutional layers to create high-resolution
    images in other settings later in this book (such as in feature transfers when
    training a CycleGAN to convert blond hair to black hair or in a variational autoencoder
    [VAE] to generate high-resolution human face images).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你将在本章学习如何镜像判别器网络的步骤，从头开始创建图像。此外，你还将学习卷积层和转置卷积层的工作原理。在本章之后，你将使用卷积层和转置卷积层在本书的其他设置中创建高分辨率图像（例如，在训练CycleGAN将金发转换为黑发时进行特征转换，或在变分自编码器[VAE]中生成高分辨率人脸图像）。
- en: 4.1 GANs to generate grayscale images of clothing items
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 使用GAN生成服装物品的灰度图像
- en: Our goal in the first project is to train a model to generate grayscale images
    of clothing items such as sandals, t-shirts, coats, and bags.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一个项目中的目标是训练一个模型，生成如凉鞋、T恤、外套和包等服装物品的灰度图像。
- en: When you use GANs to generate images, you’ll always start by obtaining training
    data. You’ll then create a discriminator network from scratch. You’ll mirror steps
    in the discriminator network when creating a generator network. Finally, you’ll
    train the GANs and use the trained model for image generation. Let’s see how that
    works with a simple project that creates grayscale images of clothing items.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用GAN生成图像时，你将始终从获取训练数据开始。然后，你将从头创建一个判别器网络。在创建生成器网络时，你将镜像判别器网络中的步骤。最后，你将训练GAN并使用训练好的模型进行图像生成。让我们通过一个简单的项目来看看这是如何工作的，该项目生成服装物品的灰度图像。
- en: 4.1.1 Training samples and the discriminator
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 训练样本和判别器
- en: The steps involved with preparing the training data are similar to what we have
    done in chapter 2, with a few exceptions that I’ll highlight later. To save time,
    I’ll skip the steps you have seen before in chapter 2 and refer you to the book’s
    GitHub repository. Follow the steps in the Jupyter Notebook for this chapter in
    the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    so that you create a data iterator with batches.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 准备训练数据所涉及的步骤与我们在第二章中做的类似，但有几点例外，我稍后会强调。为了节省时间，我将跳过你在第二章中已经看到的步骤，并指导你参考书籍的GitHub仓库。按照书籍GitHub仓库中该章节的Jupyter
    Notebook中的步骤进行操作（[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)），以便你创建一个带有批次的迭代器。
- en: There are 60,000 images in the training set. In chapter 2, we split the training
    set further into a train set and a validation set. We used the loss in the validation
    set to determine whether the parameters had converged so that we could stop training.
    However, GANs are trained using a different approach compared to traditional supervised
    learning models (such as the classification models you have seen in chapter 2).
    Since the quality of the generated samples improves throughout training, the discriminator’s
    task becomes more and more difficult. The loss from the discriminator network
    is not a good indicator of the quality of the model. The usual way of measuring
    the performance of GANs is through visual inspection to assess the quality and
    realism of generated images. We can potentially compare the quality of generated
    samples with training samples and use methods such as the Inception Score to evaluate
    the performance of GANs (See, for example, “Pros and Cons of GAN Evaluation Measures,”
    by Ali Borji, 2018, for a survey on various GAN evaluation methods; [https://arxiv.org/abs/1802.03446](https://arxiv.org/abs/1802.03446)).
    However, researchers have documented the weaknesses of these measures (“A Note
    on the Inception Score,” by Shane Barratt and Rishi Sharma, 2018, demonstrates
    that the inception score fails to provide useful guidance when comparing models;
    [https://arxiv.org/abs/1801.01973](https://arxiv.org/abs/1801.01973)). In this
    chapter, we’ll use visual inspections to check the quality of generated samples
    periodically and determine when to stop training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集中有60,000张图像。在第2章中，我们将训练集进一步分为训练集和验证集。我们使用验证集中的损失来确定参数是否收敛，以便我们可以停止训练。然而，与传统的监督学习模型（如第2章中看到的分类模型）相比，GANs的训练采用不同的方法。由于生成的样本质量在整个训练过程中不断提高，判别器的任务变得越来越困难。判别器网络的损失并不是模型质量的良好指标。衡量GANs性能的通常方法是通过视觉检查来评估生成图像的质量和逼真度。我们可以将生成样本的质量与训练样本进行比较，并使用诸如Inception
    Score等方法来评估GANs的性能（例如，参见Ali Borji于2018年撰写的“GAN评估措施的优缺点”，对各种GAN评估方法进行了调查；[https://arxiv.org/abs/1802.03446](https://arxiv.org/abs/1802.03446)）。然而，研究人员已经记录了这些措施的弱点（“关于Inception
    Score的笔记”，由Shane Barratt和Rishi Sharma于2018年撰写，表明在比较模型时，inception score无法提供有用的指导；[https://arxiv.org/abs/1801.01973](https://arxiv.org/abs/1801.01973)）。在本章中，我们将定期进行视觉检查以检查生成样本的质量，并确定何时停止训练。
- en: The discriminator network is a binary classifier, which is similar to the binary
    classifier for clothing items we discussed in chapter 2\. Here the discriminator’s
    job is to classify the samples into either real or fake.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络是一个二元分类器，这与我们在第2章中讨论的服装项目的二元分类器相似。在这里，判别器的任务是分类样本为真实或假。
- en: 'We use PyTorch to create the following discriminator neural network D:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PyTorch创建以下判别器神经网络D：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① The first fully connected layer has 784 inputs and 1,024 outputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ① 第一个全连接层有784个输入和1,024个输出。
- en: ② The last fully connected layer has 256 inputs and 1 output.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ② 最后一个全连接层有256个输入和1个输出。
- en: 'The input size is 784 because each grayscale image has a size of 28 × 28 pixels
    in the training set. Because dense layers take only 1D inputs, we flatten the
    images before feeding them to the model. The output layer has just one neuron
    in it: the output of the discriminator D is a single value. We use the sigmoid
    activation function to squeeze the output to the range [0, 1] so that it can be
    interpreted as the probability, p, that the sample is real. With complementary
    probability 1 – p, the sample is fake.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小为784，因为训练集中每个灰度图像的大小为28 × 28像素。由于密集层只接受1D输入，我们在将图像输入模型之前将其展平。输出层只有一个神经元：判别器D的输出是一个单一值。我们使用sigmoid激活函数将输出挤压到[0,
    1]的范围内，以便它可以解释为样本是真实的概率，p。使用互补概率1 – p，样本是假的。
- en: Exercise 4.1
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.1
- en: Modify the discriminator D so that the numbers of outputs in the first three
    layers are 1,000, 500, and 200 instead of 1,024, 512, and 256\. Make sure the
    number of outputs in a layer matches the number of inputs in the next layer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 修改判别器D，使其前三个层的输出数量分别为1,000、500和200，而不是1,024、512和256。确保一个层的输出数量与下一层的输入数量相匹配。
- en: 4.1.2 A generator to create grayscale images
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 生成灰度图像的生成器
- en: While the discriminator network is fairly easy to create, how to create a generator
    so that it can conjure up realistic images is a different matter. A common approach
    is to mirror the layers used in the discriminator network to create a generator,
    as shown in the following listing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然创建判别器网络相对简单，但如何创建一个生成器以生成逼真的图像则是另一回事。一种常见的方法是镜像判别器网络中使用的层来创建生成器，如下面的列表所示。
- en: Listing 4.1 Designing a generator by mirroring layers in the discriminator
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1通过镜像判别器中的层来设计生成器
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① The first layer in the generator is symmetric to the last layer in the discriminator.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成器的第一层与判别器的最后一层是对称的。
- en: ② The second layer in the generator is symmetric to the second to last layer
    in the discriminator (numbers of inputs and outputs have switched positions).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成器的第二层与判别器的倒数第二层是对称的（输入和输出的位置已交换）。
- en: ③ The third layer in the generator is symmetric to the third to last layer in
    the discriminator.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 生成器的第三层与判别器的倒数第三层是对称的。
- en: ④ The last layer in the generator is symmetric to the first layer in the discriminator.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 生成器的最后一层与判别器的第一层是对称的。
- en: ⑤ Uses Tanh() activation so the output is between –1 and 1, the same as values
    in images
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用Tanh()激活函数，使得输出值在-1和1之间，与图像中的值相同
- en: 'Figure 4.1 provides a diagram of the architecture of generator and discriminator
    networks in the GAN to generate grayscale images of clothing items. As shown in
    the top right corner of figure 4.1, a flattened grayscale image from the training
    set, which contains 28 × 28 = 784 pixels, goes through four dense layers sequentially
    in the discriminator network, and the output is the probability that the image
    is real. To create an image, the generator uses the same four dense layers but
    in reverse order: it obtains a 100-value random noise vector from the latent space
    (bottom left in figure 4.1) and feeds the vector through the four dense layers.
    In each layer, the numbers of *inputs* and *outputs* in the discriminator are
    reversed and used as the numbers of *outputs* and *inputs* in the generator. Finally,
    the generator comes up with a 784-value tensor, which can be reshaped into a 28
    × 28 grayscale image (top left).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1展示了GAN生成服装物品灰度图像的生成器和判别器网络的架构图。如图4.1右上角所示，来自训练集的平面灰度图像，包含28 × 28 = 784个像素，在判别器网络中依次通过四个密集层，输出是图像为真实的概率。为了创建图像，生成器使用相同的四个密集层，但顺序相反：它从潜在空间（图4.1左下角）获得一个100值的随机噪声向量，并将向量通过四个密集层。在每个层中，判别器中的输入和输出数量被反转，并用作生成器的输出和输入数量。最后，生成器得到一个784值的张量，可以被重塑为一个28
    × 28的灰度图像（图4.1左上角）。
- en: '![](../../OEBPS/Images/CH04_F01_Liu.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F01_Liu.png)'
- en: 'Figure 4.1 Designing a generator network to create clothing items by mirroring
    the layers in the discriminator network. The right side of the diagram shows the
    discriminator network, which contains four dense layers. To design a generator
    that can conjure up clothing items from thin air, we mirror the layers in the
    discriminator network. Specifically, as shown on the left half of the figure,
    the generator has four similar dense layers in it but in reverse order: the first
    layer in the generator mirrors the last layer in the discriminator, the second
    layer in the generator mirrors the second to last layer in the discriminator,
    and so on. Further, in each of the top three layers, the numbers of inputs and
    outputs in the discriminator are reversed and used as the numbers of outputs and
    inputs in the generator.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1通过镜像判别器网络中的层来设计生成器网络以创建服装物品。图例的右侧显示了判别器网络，其中包含四个密集层。为了设计一个可以从无中生有地创造出服装物品的生成器，我们镜像了判别器网络中的层。具体来说，如图的左侧所示，生成器中有四个类似的密集层，但顺序相反：生成器的第一层镜像判别器的最后一层，生成器的第二层镜像判别器的倒数第二层，以此类推。此外，在顶部三个层中，判别器中的输入和输出数量被反转，并用作生成器的输出和输入数量。
- en: 'The left side of figure 4.1 is the generator network, while the right side
    is the discriminator network. If you compare the two networks, you’ll notice how
    the generator mirrors the layers used in the discriminator. Specifically, the
    generator has four similar dense layers in it but in reverse order: the first
    layer in the generator mirrors the last layer in the discriminator, the second
    layer in the generator mirrors the second to last layer in the discriminator,
    and so on. The number of outputs of the generator is 784, with values between
    -1 and 1 after the `Tanh()` activation, and this matches the input to the discriminator
    network.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1的左侧是生成器网络，而右侧是判别器网络。如果你比较这两个网络，你会注意到生成器如何镜像判别器中使用的层。具体来说，生成器中有四个相似的密集层，但顺序相反：生成器中的第一层反映了判别器中的最后一层，生成器中的第二层反映了判别器中的倒数第二层，以此类推。生成器的输出数量为784，经过`Tanh()`激活后的值介于-1和1之间，这与判别器网络的输入相匹配。
- en: Exercise 4.2
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.2
- en: Modify the generator G so that the numbers of outputs in the first three layers
    are 1,000, 500, and 200 instead of 1,024, 512, and 256\. Make sure that the modified
    generator mirrors the layers used in the modified discriminator in exercise 4.1.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 修改生成器G，使其前三个层的输出数量分别为1,000、500和200，而不是1,024、512和256。确保修改后的生成器与修改后的判别器在练习4.1中使用的层相匹配。
- en: 'As in GAN models we have seen in chapter 3, the loss function is the binary
    cross-entropy loss since the discriminator D is performing a binary classification
    problem. We’ll use the Adam optimizer for both the discriminator and the generator,
    with a learning rate of 0.0001:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们在第3章中看到的GAN模型，损失函数是二元交叉熵损失，因为判别器D执行的是二元分类问题。我们将使用Adam优化器来优化判别器和生成器，学习率为0.0001：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we’ll train the GANs we just created by using the clothing item images
    in the training dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用训练数据集中的服装物品图像来训练我们刚刚创建的GANs。
- en: 4.1.3 Training GANs to generate images of clothing items
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 训练GANs生成服装物品的图像
- en: The training process is similar to what we have done in chapter 3 when training
    GANs to generate an exponential growth curve or to generate a sequence of numbers
    that are all multiples of 5\.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程与我们第3章训练GANs生成指数增长曲线或生成所有都是5的倍数的数字序列时所做的类似。
- en: Unlike in chapter 3, we’ll solely rely on visual inspections to determine whether
    the model is well-trained. For that purpose, we define a `see_output()` function
    to visualize the fake images created by the generator periodically.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与第3章不同，我们将仅依靠视觉检查来确定模型是否训练良好。为此，我们定义了一个 `see_output()` 函数，以定期可视化生成器创建的假图像。
- en: 'NOTE Interested readers can check this GitHub repository to learn how to implement
    the inception score in PyTorch to evaluate GANs: [https://github.com/sbarratt/inception-score-pytorch](https://github.com/sbarratt/inception-score-pytorch).
    However, the repository doesn’t recommend using the inception score to evaluate
    generative models due to its ineffectiveness.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：感兴趣的读者可以查看这个GitHub仓库，了解如何在PyTorch中实现 inception score 以评估 GANs：[https://github.com/sbarratt/inception-score-pytorch](https://github.com/sbarratt/inception-score-pytorch)。然而，该仓库不建议使用inception
    score来评估生成模型，因为它效果不佳。
- en: Listing 4.2 Defining a function to visualize the generated clothing items
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 定义一个函数来可视化生成的服装物品
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Generates 32 fake images
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成32张假图像
- en: ② Plots them in a 4 × 8 grid
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将它们绘制在 4 × 8 的网格中
- en: ③ Shows the ith image
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 显示第i张图像
- en: ④ Calls the see_output() function to visualize the generated images before training
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在训练前调用 see_output() 函数来可视化生成的图像
- en: If you run the preceding code cell, you’ll see 32 images that look like snowflake
    statics on a TV screen, as shown in figure 4.2\. They don’t look like clothing
    items at all because we haven’t trained the generator yet.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码单元，你会看到32张看起来像电视屏幕上的雪花静态图像，如图4.2所示。它们根本不像服装物品，因为我们还没有训练生成器。
- en: '![](../../OEBPS/Images/CH04_F02_Liu.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F02_Liu.png)'
- en: Figure 4.2 Output from the GAN model to generate clothing items before training.
    Since the model is not trained, the generated images are nothing like the images
    in the training set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 GAN模型在训练前生成服装物品的输出。由于模型尚未训练，生成的图像与训练集中的图像毫不相像。
- en: 'To train the GAN model, we define a few functions: `train_D_on_real()`, `train_D_on_fake()`,
    and `train_G()`. They are similar to those defined in chapter 3\. Go to the Jupypter
    Notebook for this chapter in the book’s GitHub repository and see what minor modifications
    we have made.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练 GAN 模型，我们定义了几个函数：`train_D_on_real()`、`train_D_on_fake()` 和 `train_G()`。它们与第
    3 章中定义的类似。请访问本书 GitHub 仓库中本章的 Jupyter Notebook，查看我们做了哪些细微的修改。
- en: Now we are ready to train the model. We iterate through all batches in the training
    dataset. For each batch of data, we first train the discriminator using the real
    samples. After that, the generator creates a batch of fake samples, and we use
    them to train the discriminator again. Finally, we let the generator create a
    batch of fake samples again, but this time, we use them to train the generator
    instead. We train the model for 50 epochs, as shown in the following listing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已准备好训练模型。我们遍历训练数据集中的所有批次。对于每个数据批次，我们首先使用真实样本训练判别器。之后，生成器创建一个假样本批次，我们再次使用它们来训练判别器。最后，我们让生成器再次创建一个假样本批次，但这次我们使用它们来训练生成器。我们训练模型
    50 个周期，如下所示。
- en: Listing 4.3 Training GANs for clothing item generation
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 训练 GAN 以生成服装项目
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Trains the discriminator using real samples
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用真实样本训练判别器
- en: ② Trains the discriminator using fake samples
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用假样本训练判别器
- en: ③ Trains the generator
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 训练生成器
- en: ④ Visualizes generated samples after every 10 epochs
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 每 10 个周期后可视化生成的样本
- en: 'The training takes about 10 minutes if you are using GPU training. Otherwise,
    it may take an hour or so, depending on the hardware configuration on your computer.
    Or you can download the trained model from my website: [https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip](https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip).
    Unzip it after downloading.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 GPU 训练，训练大约需要 10 分钟。否则，可能需要一个小时左右，具体取决于你电脑的硬件配置。或者你可以从我的网站上下载训练好的模型：[https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip](https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip)。下载后请解压。
- en: 'After every 10 epochs of training, you can visualize the generated clothing
    items, as shown in figure 4.3\. After just 10 epochs of training, the model can
    already generate clothing items that clearly can pass as real: you can tell what
    they are. The first three items in the first row in figure 4.3 are clearly a coat,
    a dress, and a pair of trousers, for example. As training progresses, the quality
    of the generated images becomes better and better.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在每 10 个周期的训练后，你可以可视化生成的服装项目，如图 4.3 所示。仅仅经过 10 个周期的训练，模型就能生成可以明显以真品身份出现的服装项目：你可以辨认出它们。图
    4.3 第一行前三个项目显然是一件外套、一件连衣裙和一条裤子，例如。随着训练的进行，生成的图像质量越来越好。
- en: '![](../../OEBPS/Images/CH04_F03_Liu.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F03_Liu.png)'
- en: Figure 4.3 Clothing items generated by an image GAN model after 10 epochs of
    training
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 经过 10 个周期训练的图像 GAN 模型生成的服装项目
- en: 'As we do in all GANs, we discard the discriminator and save the trained generator
    to generate samples later:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在所有 GAN 中所做的那样，我们丢弃判别器并保存训练好的生成器以供以后生成样本：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We have now saved the generator in the local folder. To use the generator,
    we load up the model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已将生成器保存在本地文件夹中。要使用生成器，我们加载模型：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The generator is now loaded. We can use it to generate clothing items:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器现在已加载。我们可以用它来生成服装项目：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The generated clothing items are shown in figure 4.4\. As you can see, the clothing
    items are fairly close to those in the training set.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的服装项目如图 4.4 所示。如图所示，服装项目与训练集中的项目相当接近。
- en: '![](../../OEBPS/Images/CH04_F04_Liu.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F04_Liu.png)'
- en: Figure 4.4 Clothing items generated by a trained image GAN model (after 50 epochs)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 经过 50 个周期训练的图像 GAN 模型生成的服装项目
- en: Now that you have learned how to create grayscale images by using GANs, you’ll
    learn how to generate high-resolution color images by using deep convolutional
    GAN (DCGAN) in the remaining sections of this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何使用 GAN 创建灰度图像，你将在本章剩余部分学习如何使用深度卷积 GAN (DCGAN) 生成高分辨率彩色图像。
- en: 4.2 Convolutional layers
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 卷积层
- en: To create high-resolution color images, we need more sophisticated techniques
    than simple fully connected neural networks. Specifically, we’ll use CNNs, which
    are particularly effective for processing data with a grid-like topology, such
    as images. They are distinct from fully connected (dense) layers in a couple of
    ways. First, in CNNs, each neuron in a layer is connected only to a small region
    of the input. This is based on the understanding that in image data, local groups
    of pixels are more likely to be related to each other. This local connectivity
    reduces the number of parameters, making the network more efficient. Second, CNNs
    use the concept of shared weights—the same weights are used across different regions
    of the input. This is akin to sliding a filter across the entire input space.
    This filter detects specific features (e.g., edges or textures) regardless of
    their position in the input, leading to the property of translation invariance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建高分辨率彩色图像，我们需要比简单的全连接神经网络更复杂的技巧。具体来说，我们将使用卷积神经网络（CNNs），它们在处理具有网格状拓扑结构的数据（如图像）方面特别有效。它们在几个方面与全连接（密集）层不同。首先，在CNNs中，每一层的每个神经元仅连接到输入的小区域。这是基于这样的理解：在图像数据中，局部像素群更有可能相互关联。这种局部连接性减少了参数数量，使网络更高效。其次，CNNs使用共享权重——相同的权重被用于输入的不同区域。这类似于在整个输入空间上滑动一个过滤器。这个过滤器检测特定的特征（例如，边缘或纹理），而不管它们在输入中的位置如何，从而导致了平移不变性的特性。
- en: Due to their structure, CNNs are more efficient for image processing. They require
    fewer parameters than fully connected networks of similar size, leading to faster
    training times and lower computational costs. They are also generally more effective
    at capturing spatial hierarchies in image data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的结构，CNNs在图像处理方面更高效。它们所需的参数比类似大小的全连接网络少，导致训练时间更快和计算成本更低。它们通常在捕获图像数据中的空间层次结构方面也更为有效。
- en: 'Convolutional layers and transposed convolutional layers are two fundamental
    building blocks in CNNs, commonly used in image processing and computer vision
    tasks. They have different purposes and characteristics: convolutional layers
    are used for feature extraction. They apply a set of learnable filters (also known
    as kernels) to the input data to detect patterns and features at different spatial
    scales. These layers are essential for capturing hierarchical representations
    of the input data. In contrast, transposed convolutional layers are used for upsampling
    or generating high-resolution feature maps.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和转置卷积层是CNNs中的两个基本构建块，常用于图像处理和计算机视觉任务。它们有不同的目的和特性：卷积层用于特征提取。它们将一组可学习的过滤器（也称为核）应用于输入数据，以检测不同空间尺度上的模式和特征。这些层对于捕获输入数据的层次表示至关重要。相比之下，转置卷积层用于上采样或生成高分辨率特征图。
- en: In this section, you’ll learn how convolutional operations work and how kernel
    size, stride, and zero-padding affect convolutional operations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习卷积操作的工作原理以及核大小、步长和零填充如何影响卷积操作。
- en: 4.2.1 How do convolutional operations work?
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 卷积操作是如何工作的？
- en: Convolutional layers use filters to extract spatial patterns on the input data.
    A convolutional layer is capable of automatically detecting a large number of
    patterns and associating them with the target label. Therefore, convolutional
    layers are commonly used in image classification tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层使用过滤器从输入数据中提取空间模式。一个卷积层能够自动检测大量模式并将它们与目标标签关联起来。因此，卷积层通常用于图像分类任务。
- en: Convolutional operations involve applying a filter to an input image to produce
    a feature map. This process involves using element-wise multiplication of the
    filter with the input image and summing the results. The weights in the filter
    are the same as the filter moves on the input image to scan different areas. Figure
    4.5 shows a numerical example of how convolutional operations work. The left column
    is the input image, and the second column is a filter (a 2 × 2 matrix). Convolutional
    operations (the third column) involve sliding the filter over the input image,
    multiplying corresponding elements, and summing them up (the last column).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作涉及将一个过滤器应用于输入图像以生成一个特征图。这个过程包括使用过滤器与输入图像的逐元素乘法，并将结果相加。过滤器中的权重在过滤器在输入图像上移动以扫描不同区域时保持不变。图4.5展示了卷积操作的工作原理的数值示例。左侧列是输入图像，第二列是一个过滤器（一个2×2矩阵）。卷积操作（第三列）涉及将过滤器在输入图像上滑动，乘以相应的元素，并将它们相加（最后一列）。
- en: '![](../../OEBPS/Images/CH04_F05_Liu.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F05_Liu.png)'
- en: Figure 4.5 A numerical example of how convolutional operations work, with stride
    equal to 1 and no padding
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 卷积操作如何工作的数值示例，步长为1，无填充
- en: 'To gain a deep understanding of exactly how convolutional operations work,
    let’s implement the convolutional operations in PyTorch in parallel so that you
    can verify the numbers as shown in figure 4.5\. First, let’s create a PyTorch
    tensor to represent the input image in the figure:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入理解卷积操作的确切工作原理，让我们在PyTorch中并行实现卷积操作，以便您可以验证如图4.5所示的数字。首先，让我们创建一个PyTorch张量来表示图中的输入图像：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① The four values in the shape of the image, (1, 1, 3, 3), are the number of
    images in the batch, number of color channels, image height, and image width,
    respectively.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ① 图像形状中的四个值（1, 1, 3, 3）分别是批处理中的图像数量、颜色通道数量、图像高度和图像宽度。
- en: The image is reshaped so that it has a dimension of (1, 1, 3, 3), indicating
    that there is just one observation in the batch, and the image has just one color
    channel. The height and the width of the image are both 3 pixels.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被重塑，使其具有维度（1, 1, 3, 3），表示批处理中只有一个观察值，图像只有一个颜色通道。图像的高度和宽度都是3像素。
- en: 'Let’s represent the 2 × 2 filter, as shown in the second column of figure 4.5,
    by creating a 2D convolutional layer in PyTorch:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在PyTorch中创建一个2 × 2滤波器，如图4.5的第二列所示，来表示它：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Initiates a 2D convolutional layer
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化一个2D卷积层
- en: ② Extracts the randomly initialized weights and bias in the layer
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从层中提取随机初始化的权重和偏置
- en: A 2D convolutional layer takes several arguments. The `in_channels` argument
    is the number of channels in the input image. This value is 1 for grayscale images
    and 3 for color images since color images have three color channels (red, green,
    and blue [RGB]). The `out_channels` is the number of channels after the convolutional
    layer, which can take any number based on how many features you want to extract
    from the image. The `kernel_size` argument controls the size of the kernel; for
    example, `kernel_size=3` means the filter has a shape of 3 × 3, and `kernel_size=4`
    means the filter has a shape of 4 × 4\. We set the kernel size to 2 so the filter
    has a shape of 2 × 2\.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 2D卷积层需要几个参数。`in_channels`参数是输入图像中的通道数。对于灰度图像，这个值是1，对于彩色图像，这个值是3，因为彩色图像有三个颜色通道（红色、绿色和蓝色[RGB]）。`out_channels`是卷积层之后的通道数，可以根据你想从图像中提取多少特征来取任何数值。`kernel_size`参数控制内核的大小；例如，`kernel_size=3`表示滤波器的大小为3
    × 3，而`kernel_size=4`表示滤波器的大小为4 × 4。我们将内核大小设置为2，因此滤波器的大小为2 × 2。
- en: A 2D convolutional layer also has several optional arguments. The `stride` argument
    specifies how many pixels to move to the right or down each time the filter moves
    along the input image. The `stride` argument has a default value of 1\. A higher
    value of stride leads to more downsampling of the image. The `padding` argument
    means how many rows of zeros to add to four sides of the input image, with a default
    value of 0\. The `bias` argument indicates whether to add a learnable bias as
    the parameter, with a default value of `True`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2D卷积层也有几个可选参数。`stride`参数指定每次滤波器在输入图像上移动时向右或向下移动多少像素。`stride`参数的默认值为1。更大的步长值会导致图像的更多下采样。`padding`参数表示在输入图像的四边添加多少行零，默认值为0。`bias`参数表示是否将可学习的偏置作为参数添加，默认值为`True`。
- en: 'The preceding 2D convolutional layer has one input channel, one output channel,
    with a kernel size of 2 × 2, and a stride of 1\. When the convolutional layer
    is created, the weights and the bias in it are randomly initialized. You will
    see the following output as the weights and the bias of this convolutional layer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的2D卷积层有一个输入通道，一个输出通道，内核大小为2 × 2，步长为1。当创建卷积层时，其中的权重和偏置是随机初始化的。您将看到以下输出作为此卷积层的权重和偏置：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To make our example easier to follow, we’ll replace the weights and the bias
    with whole numbers:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的例子更容易理解，我们将权重和偏置替换为整数：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Handpicks weights and bias
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ① 手动选择权重和偏置
- en: ② Replaces the weights and bias in the convolutional layer with our handpicked
    numbers
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将卷积层中的权重和偏置替换为我们手动选择的数字
- en: ③ Prints out the new weights and bias in the convolutional layer
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出卷积层中的新权重和偏置
- en: 'Since we are not learning the parameters in the convolutional layer, `torch.no_grad()`
    is used to disable gradient calculation, which reduces memory consumption and
    speeds up computations. Now the convolutional layer has weights and the bias that
    we have chosen. They also match the numbers in figure 4.5\. The output from the
    preceding code cell is:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不在卷积层中学习参数，使用 `torch.no_grad()` 来禁用梯度计算，这减少了内存消耗并加快了计算速度。现在卷积层具有我们选择的权重和偏置，它们也匹配图
    4.5 中的数字。前面代码单元格的输出为：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we apply the preceding convolutional layer on the 3 × 3 image we mentioned,
    what is the output? Let’s find out:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将前面提到的 3 × 3 图像应用于卷积层，输出是什么？让我们来看看：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output is
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output has a shape of (1, 1, 2, 2), with four values in it: 7, 14, 54,
    and 50\. These numbers match those in figure 4.5.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的形状为 (1, 1, 2, 2)，其中包含四个值：7、14、54 和 50。这些数字与图 4.5 中的数字相匹配。
- en: But how exactly does the convolutional layer generate this output through the
    filter? We’ll explain in detail next.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但卷积层究竟是如何通过滤波器生成这个输出的？我们将在下文中详细解释。
- en: The input image is a 3 × 3 matrix, and the filter is a 2 × 2 matrix. When the
    filter scans over the image, it first covers the four pixels in the top left corner
    of the image, which have values `[[1, 1], [0, 1]]`, as shown in the first row
    in Figure 4.5\. The filter has values `[[1,2],[3,4]]`. The convolution operation
    finds the sum of the element-wise multiplication of the two tensors (in this case,
    one tensor is the filter and the other is the covered area). In other words, the
    convolution operation performs element-wise multiplication in each of the four
    cells and then adds up the values in the four cells. Therefore, the output from
    scanning the top left corner is
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像是一个 3 × 3 矩阵，滤波器是一个 2 × 2 矩阵。当滤波器在图像上扫描时，它首先覆盖图像左上角四个像素，这些像素的值为 `[[1, 1],
    [0, 1]]`，如图 4.5 的第一行所示。滤波器的值为 `[[1,2],[3,4]]`。卷积操作找到两个张量（在这种情况下，一个张量是滤波器，另一个是被覆盖的区域）逐元素乘积的和。换句话说，卷积操作在每个四个单元格中执行逐元素乘法，然后将四个单元格中的值相加。因此，扫描左上角得到的输出是
- en: 1 × 1 × 1 × 2 + 0 × 3 + 1 × 4 = 7.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 × 1 × 2 + 0 × 3 + 1 × 4 = 7。
- en: 'This explains why the top left corner of the output has a value of 7\. Similarly,
    when the filter is applied to the top right corner of the image, the covered area
    is `[[1,1],[1,2]]`. The output is therefore:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么输出左上角的值为 7。同样，当滤波器应用于图像的右上角时，覆盖区域为 `[[1,1],[1,2]]`。因此，输出为：
- en: 1 × 1 + 1 × 2 + 1 × 3 + 2 × 4 = 14.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 + 1 × 2 + 1 × 3 + 2 × 4 = 14。
- en: This explains why the top right corner of the output has a value of 14\.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么输出右上角的值为 14。
- en: Exercise 4.3
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 4.3
- en: What are the values in the covered area when the filter is applied to the bottom
    right corner of the image? Explain why the bottom right corner of the output has
    a value of 50.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当滤波器应用于图像右下角时，覆盖区域的值是什么？解释为什么输出右下角的值为 50。
- en: 4.2.2 How do stride and padding affect convolutional operations?
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 步长和填充如何影响卷积操作？
- en: Stride and zero padding are two important concepts in the context of convolutional
    operations. They play a crucial role in determining the dimensions of the output
    feature map and the way the filter interacts with the input data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 步长和零填充是卷积操作中的两个重要概念。它们在确定输出特征图的维度以及滤波器与输入数据交互的方式中起着至关重要的作用。
- en: Stride refers to the number of pixels by which the filter moves across the input
    image. When the stride is 1, the filter moves 1 pixel at a time. A larger stride
    means the filter jumps over more pixels as it slides over the image. Increasing
    the stride reduces the spatial dimensions of the output feature map.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 步长指的是滤波器在输入图像上移动的像素数。当步长为 1 时，滤波器每次移动 1 个像素。较大的步长意味着滤波器在滑动图像时跳过了更多的像素。增加步长会减少输出特征图的空間维度。
- en: Zero padding involves adding layers of zeros around the border of the input
    image before applying the convolutional operation. Zero padding allows control
    over the spatial dimensions of the output feature map. Without padding, the dimensions
    of the output will be smaller than the input. By adding padding, you can preserve
    the dimensions of the input.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 零填充涉及在应用卷积操作之前在输入图像的边缘添加零层。零填充允许控制输出特征图的空間维度。没有填充，输出的维度将小于输入。通过添加填充，可以保留输入的维度。
- en: 'Let’s use an example to show how stride and padding work. The following code
    cell redefines the 2D convolutional layer:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来展示步长和填充是如何工作的。下面的代码单元重新定义了2D卷积层：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Changes the stride from 1 to 2
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将步长从1改为2
- en: ② Changes the padding from 0 to 1
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将填充从0改为1
- en: The output is
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `padding=1` argument adds one row of 0s around the input image, so the padded
    image now has a size of 5 × 5 instead of 3 × 3.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`padding=1`参数在输入图像周围添加一行0，因此填充后的图像现在的大小为5 × 5而不是3 × 3。'
- en: 'When the filter scans over the padded image, it first covers the top left corner,
    which has values `[[0, 0], [0, 1]]`. The filter has values `[[1,2],[3,4]]`. Therefore,
    the output from scanning the top left corner is:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当过滤器扫描填充后的图像时，它首先覆盖左上角，其值为`[[0, 0], [0, 1]]`。过滤器的值为`[[1,2],[3,4]]`。因此，扫描左上角的输出为：
- en: 0 × 1+0 × 2+0 × 3+1 × 4=4
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 0 × 1+0 × 2+0 × 3+1 × 4=4
- en: 'This explains why the top left corner of the output has a value of 4\. Similarly,
    when the filter slides two pixels down to the bottom left corner of the image,
    the covered area is `[[0,0],[0,8]]`. The output is therefore:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么输出图像的左上角有一个值为4。同样，当过滤器向下滑动两个像素到达图像的左下角时，覆盖的区域是`[[0,0],[0,8]]`。因此，输出如下：
- en: 0 × 1+0 × 2+0 × 3+8 × 4=32
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 0 × 1+0 × 2+0 × 3+8 × 4=32
- en: This explains why the bottom left corner of the output has a value of 32\.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么输出图像的左下角有一个值为32。
- en: 4.3 Transposed convolution and batch normalization
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 转置卷积和批量归一化
- en: Transposed convolutional layers are also known as deconvolution or upsampling
    Layers. They are used for upsampling or generating high-resolution feature maps.
    They are often employed in generative models like GANs and VAEs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积层也被称为反卷积或上采样层。它们用于上采样或生成高分辨率特征图。它们通常在生成模型如GAN和VAE中使用。
- en: Transposed convolutional layers apply a filter to the input data, but unlike
    standard convolution, they increase the spatial dimensions by inserting gaps between
    the output values, which effectively “upscales” the feature maps. This process
    generates feature maps of a higher resolution. Transposed convolutional layers
    help increase the spatial resolution, which is useful in image generation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积层将过滤器应用于输入数据，但与标准卷积不同，它们通过在输出值之间插入间隔来增加空间维度，从而有效地“上采样”特征图。这个过程生成了更高分辨率的特征图。转置卷积层有助于提高空间分辨率，这在图像生成中很有用。
- en: Strides can be used in transposed convolution layers to control the amount of
    upsampling. The greater the value of the stride, the more upsampling the transposed
    convolution layer has on the input data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 步长可以在转置卷积层中使用来控制上采样的量。步长的值越大，转置卷积层对输入数据的上采样就越多。
- en: Two-dimensional batch normalization is a technique used in neural networks,
    particularly CNNs, to stabilize and speed up the training process. It addresses
    several problems, including saturation, vanishing gradients, and exploding gradients,
    which are common challenges in deep learning. In this section, you’ll look at
    some examples so you have a deeper understanding of how it works. You’ll use it
    when creating GANs to generate high-resolution color images in the next section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 二维批量归一化是一种在神经网络中使用的技巧，特别是在CNN中，用于稳定和加速训练过程。它解决了包括饱和、消失梯度和梯度爆炸在内的几个问题，这些都是深度学习中常见的挑战。在本节中，你将查看一些示例，以便更深入地了解其工作原理。你将在下一节创建GAN时生成高分辨率彩色图像时使用它。
- en: Vanishing and exploding gradients in deep learning
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的消失和梯度爆炸
- en: The vanishing gradient problem occurs in deep neural networks when the gradients
    of the loss function with respect to the network parameters become exceedingly
    small during backpropagation. This results in very slow updates to the parameters,
    hindering the learning process, especially in the early layers of the network.
    Conversely, the exploding gradient problem happens when these gradients become
    excessively large, leading to unstable updates and causing the model parameters
    to oscillate or diverge to very large values. Both problems impede the effective
    training of deep neural networks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度问题发生在深度神经网络中，当在反向传播过程中损失函数相对于网络参数的梯度变得极其小时。这导致参数更新非常缓慢，阻碍了学习过程，尤其是在网络的早期层。相反，梯度爆炸问题发生在这些梯度变得过大时，导致更新不稳定，并使模型参数振荡或发散到非常大的值。这两个问题都阻碍了深度神经网络的
    有效训练。
- en: 4.3.1 How do transposed convolutional layers work?
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 转置卷积层是如何工作的？
- en: Contrary to convolutional layers, transposed convolutional layers upsample and
    fill in gaps in an image to generate features and increase resolution by using
    kernels (i.e., filters). The output is usually larger than the input in a transposed
    convolutional layer. Therefore, transposed convolutional layers are essential
    tools when it comes to generating high-resolution images. To show you exactly
    how 2D transposed convolutional operations work, let’s use a simple example and
    a figure. Suppose you have a very small 2 × 2 input image, as shown in the left
    column in figure 4.6.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积层相反，转置卷积层通过使用核（即滤波器）在图像中上采样并填充间隙来生成特征并增加分辨率。在转置卷积层中，输出通常比输入大。因此，转置卷积层是生成高分辨率图像时的必要工具。为了向您展示2D转置卷积操作的确切工作方式，让我们用一个简单的例子和一张图来说明。假设你有一个非常小的2
    × 2输入图像，如图4.6的左侧列所示。
- en: '![](../../OEBPS/Images/CH04_F06_Liu.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F06_Liu.png)'
- en: Figure 4.6 A numerical example of how transposed convolutional operations work
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 转置卷积操作的一个数值示例
- en: 'The input image has the following values in it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像中的值为：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You want to upsample the image so that it has a higher resolution. You can
    create a 2D transposed convolutional layer in PyTorch:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要上采样图像，使其具有更高的分辨率。你可以在PyTorch中创建一个2D转置卷积层：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① A transposed convolutional layer with one input channel, one output channel,
    a kernel size of 2, and a stride of 2
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个具有一个输入通道、一个输出通道、核大小为2、步长为2的转置卷积层
- en: ② Replaces the weights and bias in the transposed convolutional layer with handpicked
    values
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ② 用挑选的值替换转置卷积层中的权重和偏置
- en: This 2D transposed convolutional layer has one input channel, one output channel,
    with a kernel size of 2 × 2 and a stride of 2\. The 2 × 2 filter is shown in the
    second column in figure 4.6\. We replaced the randomly initialized weights and
    the bias in the layer with our handpicked whole numbers so it’s easy to follow
    the calculations. The `state_dict()` method in the preceding code listing returns
    the parameters in a deep neural network.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个2D转置卷积层有一个输入通道，一个输出通道，核大小为2 × 2，步长为2。2 × 2滤波器如图4.6的第二列所示。我们用我们挑选的整数替换了层中的随机初始化权重和偏置，这样便于跟踪计算。前述代码列表中的
    `state_dict()` 方法返回深度神经网络中的参数。
- en: 'When the transposed convolutional layer is applied to the 2 × 2 image we mentioned
    earlier, what is the output? Let’s find out:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当将转置卷积层应用于我们之前提到的2 × 2图像时，输出是什么？让我们来找出答案：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output is
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output has a shape of (1, 1, 4, 4), meaning we have upsampled a 2 × 2 image
    to a 4 × 4 image. How does the transposed convolutional layer generate the preceding
    output through the filter? We’ll explain in detail next.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的形状为 (1, 1, 4, 4)，这意味着我们将一个2 × 2的图像上采样到一个4 × 4的图像。转置卷积层是如何通过滤波器生成前面的输出的？我们将在下文中详细解释。
- en: The image is a 2 × 2 matrix, and the filter is also a 2 × 2 matrix. When the
    filter is applied to the image, each element in the image multiplies with the
    filter and goes to the output. The top left value in the image is 1, and we multiply
    it with the values in the filter, `[[2, 3], [4, 5]]`, and this leads to the four
    values in the top left block of the output matrix `transoutput`, with values `[[2,
    3], [4, 5]]`, as shown at the top right corner in figure 4.6\. Similarly, the
    bottom left value in the image is 2, and we multiply it with the values in the
    filter, `[[2, 3], [4, 5]]`, and this leads to the four values in the bottom left
    block of the output matrix `transoutput`, `[[4, 6], [8, 10]]`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是一个2 × 2矩阵，滤波器也是一个2 × 2矩阵。当滤波器应用于图像时，图像中的每个元素与滤波器相乘，并传递到输出。图像左上角的值是1，我们将其与滤波器中的值
    `[[2, 3], [4, 5]]` 相乘，这导致输出矩阵 `transoutput` 左上角块中的四个值，值为 `[[2, 3], [4, 5]]`，如图4.6右上角所示。同样，图像左下角的值是2，我们将其与滤波器中的值
    `[[2, 3], [4, 5]]` 相乘，这导致输出矩阵 `transoutput` 左下角块中的四个值，值为 `[[4, 6], [8, 10]]`。
- en: Exercise 4.4
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.4
- en: If an image has values `[[10, 10], [15, 20]]` in it, what is the output after
    you apply the 2D transposed convolutional layer `transconv` to the image? Assume
    `transconv` has values `[[2, 3], [4, 5]]` in it. Assume a kernel size of 2 and
    a stride size of 2\.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个图像中包含值 `[[10, 10], [15, 20]]`，在你对该图像应用2D转置卷积层 `transconv` 后，输出是什么？假设 `transconv`
    中的值为 `[[2, 3], [4, 5]]`。假设核大小为2，步长大小为2。
- en: 4.3.2 Batch normalization
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 批标准化
- en: Two-dimensional batch normalization is a standard technique in modern deep learning
    frameworks and has become a crucial component for effectively training deep neural
    networks. You’ll see it quite often later in this book.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 二维批量归一化是现代深度学习框架中的标准技术，并已成为有效训练深度神经网络的关键组件。你将在本书后面的内容中经常看到它。
- en: In 2D batch normalization, normalization is performed independently for each
    feature channel by adjusting and scaling values in the channel so they have a
    mean of 0 and a variance of 1\. A feature channel refers to one of the dimensions
    in a multidimensional tensor in CNNs used to represent different aspects or features
    of the input data. For example, they can represent color channels like red, green,
    or blue. The normalization ensures that the distribution of the inputs to layers
    deep in the network remains more stable during training. This stability arises
    because the normalization process reduces the internal covariate shift, which
    is the change in the distribution of network activations due to the update of
    weights in lower layers. It also helps to address the vanishing or exploding gradient
    problems by keeping the inputs in an appropriate range to prevent gradients from
    becoming too small (vanishing) or too large (exploding).^([1](#footnote-000))
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维批量归一化中，通过调整和缩放通道中的值，使其具有0均值和1方差，对每个特征通道独立进行归一化。特征通道是指CNN中多维张量中的一个维度，用于表示输入数据的各个方面或特征。例如，它们可以表示颜色通道，如红色、绿色或蓝色。归一化确保了在训练过程中，网络深层输入的分布保持更稳定。这种稳定性源于归一化过程减少了内部协变量偏移，这是由于较低层权重的更新导致的网络激活分布的变化。它还有助于通过保持输入在适当的范围内，防止梯度变得太小（消失）或太大（爆炸）来解决问题。[1](#footnote-000)
- en: 'Here’s how the 2D batch normalization works: for each feature channel, we first
    calculate the mean and variance of all observations within the channel. We then
    normalize the values for each feature channel using the mean and variance obtained
    earlier (by subtracting the mean from each observation and then dividing the difference
    by the standard deviation). This ensures that the values in each channel have
    a mean of 0 and a standard deviation of 1 after normalization, which helps stabilize
    and speed up training. It also helps maintain stable gradients during backpropagation,
    which further aids in training deep neural networks.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是二维批量归一化工作原理的说明：对于每个特征通道，我们首先计算该通道内所有观察值的均值和方差。然后，我们使用之前获得的均值和方差（通过从每个观察值中减去均值，然后除以标准差）对每个特征通道的值进行归一化。这确保了归一化后每个通道的值具有0均值和1标准差，这有助于稳定和加速训练。它还有助于在反向传播过程中保持稳定的梯度，这进一步有助于训练深度神经网络。
- en: Let’s use a concrete example to show how the 2D batch normalization works.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个具体的例子来说明二维批量归一化是如何工作的。
- en: 'Suppose that you have a three-channel input with a size of 64 × 64\. You pass
    the input through a 2D convolutional layer with three output channels as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个大小为64 × 64的三通道输入。你将输入通过一个有三个输出通道的2D卷积层，如下所示：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Fixes the random state so results are reproducible
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ① 固定随机状态，以确保结果可重复
- en: ② Creates a 3-channel input
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个3通道输入
- en: ③ Creates a 2D convolutional layer
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个二维卷积层
- en: ④ Passes the input through the convolutional layer
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将输入通过卷积层传递
- en: The output from the preceding code cell is
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码单元格的输出是
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We have created a three-channel input and passed it through a 2D convolutional
    layer with three output channels. The processed input has three channels with
    a size of 64 × 64 pixels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个三通道输入，并通过一个有三个输出通道的2D卷积层传递。处理后的输入有三个通道，大小为64 × 64像素。
- en: 'Let’s look at the mean and standard deviation of the pixels in each of the
    three output channels:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看三个输出通道中每个通道的像素均值和标准差：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The average values of the pixels in each output channel are not 0; the standard
    deviations of pixels in each output channel are not 1\. Now, we perform a 2D batch
    normalization:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出通道中像素的平均值不是0；每个输出通道中像素的标准差不是1。现在，我们执行二维批量归一化：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we have the following output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有以下输出：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The average values of pixels in each output channel are now practically 0 (or
    a very small number that is close to 0); the standard deviations of pixels in
    each output channel are now a number close to 1\. That’s what batch normalization
    does: it normalizes observations in each feature channel so that values in each
    feature channel have 0 mean and unit standard deviation.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出通道的像素平均值现在实际上为0（或一个非常接近0的小数）；每个输出通道的像素标准差现在是一个接近1的数。这就是批量归一化的作用：它对每个特征通道中的观测值进行归一化，使得每个特征通道中的值具有0均值和单位标准差。
- en: 4.4 Color images of anime faces
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 动漫面孔的彩色图像
- en: In this second project, you’ll learn how to create high-resolution color images.
    The training steps in this project are similar to the first project, with the
    exception that the training data are color images of anime faces. Further, the
    discriminator and generator neural networks are more sophisticated. We’ll use
    2D convolutional and 2D transposed convolutional layers in the two networks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第二个项目中，你将学习如何创建高分辨率彩色图像。这个项目的训练步骤与第一个项目类似，不同之处在于训练数据是动漫面孔的彩色图像。此外，判别器和生成器神经网络更加复杂。我们将在两个网络中使用2D卷积和2D转置卷积层。
- en: 4.4.1 Downloading anime faces
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 下载动漫面孔
- en: You can download the training data from Kaggle [https://mng.bz/1a9R](https://mng.bz/1a9R),
    which contains 63,632 color images of anime faces. You need to set up a free Kaggle
    account to log in first. Extract the data from the zip file and put them in a
    folder on your computer. For example, I placed everything in the zip file in /files/anime/
    on my computer. As a result, all anime face images are in /files/anime/images/.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从Kaggle [https://mng.bz/1a9R](https://mng.bz/1a9R)下载训练数据，其中包含63,632张动漫面孔的彩色图像。你需要先设置一个免费的Kaggle账户进行登录。从zip文件中提取数据，并将它们放在你电脑上的一个文件夹中。例如，我将zip文件中的所有内容都放在了电脑上的/files/anime/文件夹中。因此，所有动漫面孔图像都在/files/anime/images/文件夹中。
- en: 'Define the path name so you can use it later to load the images in Pytorch:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 定义路径名，以便以后可以在Pytorch中加载图像：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Change the name of the path depending on where you have saved the images on
    your computer. Note that the `ImageFolder()` class uses the directory name of
    the images to identify the class the images belong to. As a result, the final
    /images/ directory is not included in `anime_path` that we define earlier.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你在电脑上保存图像的位置更改路径名。请注意，`ImageFolder()`类使用图像的目录名来识别图像所属的类别。因此，我们之前定义的`anime_path`中不包括最终的/images/目录。
- en: 'Next, we use the `ImageFolder()` class in Torchvision `datasets` package to
    load the dataset:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用Torchvision `datasets`包中的`ImageFolder()`类来加载数据集：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ① Changes image size to 64 × 64
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将图像大小调整为64 × 64
- en: ② Converts images to PyTorch tensors
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将图像转换为PyTorch张量
- en: ③ Normalizes image values to [-1, 1] in all three color channels
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在所有三个颜色通道中将图像值归一化到[-1, 1]
- en: ④ Loads the data and transforms images
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 加载数据并转换图像
- en: We perform three different transformations when loading up the images from the
    local folder. First, we resize all images to 64 pixels in height and 64 pixels
    in width. Second, we convert the images to PyTorch tensors with values in the
    range [0, 1] by using the `ToTensor()` class. Finally, we use the `Normalize()`
    class to deduct 0.5 from the value and divide the difference by 0.5\. As a result,
    the image data are now between –1 and 1.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在从本地文件夹加载图像时，我们执行了三种不同的转换。首先，我们将所有图像的高度和宽度都调整为64像素。其次，我们使用`ToTensor()`类将图像转换为PyTorch张量，其值在[0,
    1]范围内。最后，我们使用`Normalize()`类从值中减去0.5，并将差值除以0.5。因此，图像数据现在在-1和1之间。
- en: 'We can now put the training data in batches:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将训练数据放入批次中：
- en: '[PRE29]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The training dataset is now in batches, with a batch size of 128.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集现在是批次的，批大小为128。
- en: 4.4.2 Channels-first color images in PyTorch
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 PyTorch中的通道优先彩色图像
- en: 'PyTorch uses a so-called channels-first approach when handling color images.
    This means the shape of images in PyTorch are (number_channels, height, width).
    In contrast, in other Python libraries such as TensorFlow or Matplotlib, a channels-last
    approach is used: a color image has a shape of (height, width, number_channels)
    instead.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在处理彩色图像时使用所谓的通道优先方法。这意味着PyTorch中图像的形状是（通道数，高度，宽度）。相比之下，在其他Python库（如TensorFlow或Matplotlib）中，使用的是通道后方法：彩色图像的形状为（高度，宽度，通道数）。
- en: 'Let’s look at an example image in our dataset and print out the shape of the
    image:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据集中的一张示例图像，并打印出图像的形状：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output is
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The shape of the first image is 3 × 64 × 64\. This means the image has three
    color channels (RGB). The height and width of the image are both 64 pixels.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图像的形状是 3 × 64 × 64。这意味着图像有三个颜色通道（RGB）。图像的高度和宽度都是 64 像素。
- en: 'When we plot the images in Matplotlib, we need to convert them to channels-last
    by using the `permute()` method in PyTorch:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 Matplotlib 中绘制图像时，我们需要使用 PyTorch 中的 `permute()` 方法将它们转换为通道最后：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that we need to multiply the PyTorch tensor representing the image by 0.5
    and then add 0.5 to it to convert the values from the range [–1, 1] to the range
    [0, 1]. You’ll see a plot of an anime face after running the preceding code cell.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要将代表图像的 PyTorch 张量乘以 0.5，然后加上 0.5，以将值从范围 [–1, 1] 转换为范围 [0, 1]。运行前面的代码单元后，您将看到一个动漫人脸的绘图。
- en: 'Next, we define a function `plot_images()` to visualize 32 images in four rows
    and eight columns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数 `plot_images()` 来可视化四行八列的 32 张图像：
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ① Defines a function to visualize 32 images
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个函数来可视化 32 张图像
- en: ② Places them in a 4 × 8 grid
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将它们放置在 4 × 8 的网格中
- en: ③ Obtains a batch of images
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 获取一批图像
- en: ④ Calls the function to visualize the images
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 调用函数来可视化图像
- en: You’ll see a plot of 32 anime faces in a 4 × 8 grid after running the preceding
    code cell, as shown in figure 4.7\.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码单元后，您将看到一个 4 × 8 网格中的 32 张动漫人脸的绘图，如图 4.7 所示。
- en: '![](../../OEBPS/Images/CH04_F07_Liu.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F07_Liu.png)'
- en: Figure 4.7 Examples from the anime faces training dataset
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 动漫人脸训练数据集的示例
- en: 4.5 Deep convolutional GAN
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 深度卷积 GAN
- en: 'In this section, you’ll create a DCGAN model so that we can train it to generate
    anime face images. As usual, the GAN model consists of a discriminator network
    and a generator network. However, the networks are more sophisticated than the
    ones we have seen before: we’ll use convolutional layers, transposed convolutional
    layers, and batch normalization layers in these networks.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将创建一个 DCGAN 模型，以便我们可以训练它生成动漫人脸图像。像往常一样，GAN 模型由判别器网络和生成器网络组成。然而，这些网络比我们之前看到的更复杂：在这些网络中，我们将使用卷积层、转置卷积层和批量归一化层。
- en: We’ll start with the discriminator network. After that, I’ll explain how the
    generator network mirrors the layers in the discriminator network to conjure up
    realistic color images. You’ll then train the model with the data you prepared
    earlier in this chapter and use the trained model to generate novel images of
    anime face images.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从判别器网络开始。之后，我将解释生成器网络如何镜像判别器网络中的层来生成逼真的彩色图像。然后，您将使用本章前面准备的数据训练模型，并使用训练好的模型生成动漫人脸图像的新颖图像。
- en: 4.5.1 Building a DCGAN
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 构建 DCGAN
- en: As in previous GAN models we have seen, the discriminator is a binary classifier
    to classify samples into real or fake. However, different from the networks we
    have used so far, we’ll use convolutional layers and batch normalizations. The
    high-resolution color images in this project have too many parameters, and if
    we use dense layers only, it’s difficult to train the model effectively. The structure
    of the discriminator neural network is shown in the following listing.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前看到的 GAN 模型一样，判别器是一个二元分类器，用于将样本分类为真实或伪造。然而，与迄今为止我们使用的网络不同，我们将使用卷积层和批量归一化。本项目中的高分辨率彩色图像参数太多，如果我们只使用密集层，就难以有效地训练模型。判别器神经网络的结构如下所示。
- en: Listing 4.4 A discriminator in DCGAN
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 DCGAN 中的判别器
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Passes the image through a 2D convolutional layer
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将图像通过一个二维卷积层
- en: ② Applies the LeakyReLU activation on outputs of the first convolutional layer
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在第一个卷积层的输出上应用 LeakyReLU 激活函数
- en: ③ Performs 2D batch normalization on outputs of the second convolutional layer
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 对第二个卷积层的输出执行二维批量归一化
- en: ④ The output is a single value between 0 and 1, which can be interpreted as
    the probability that an image is real.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 输出是一个介于 0 和 1 之间的单个值，可以解释为图像是真实的概率。
- en: 'The input to the discriminator network is a color image with three color channels.
    The first 2D convolutional layer is `Conv2d(3, 64, 4, 2, 1, bias=False)`: this
    means the input has three channels and the output has 64 channels; the kernel
    size is 4; the stride is 2; and the padding is 1\. Each of the 2D convolutional
    layers in the network takes an image and applies filters to extract spatial features.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的输入是一个具有三个颜色通道的彩色图像。第一个二维卷积层是 `Conv2d(3, 64, 4, 2, 1, bias=False)`：这意味着输入有三个通道，输出有
    64 个通道；核大小为 4；步长为 2；填充为 1。网络中的每个二维卷积层都接受一个图像并应用滤波器以提取空间特征。
- en: 'Starting from the second 2D convolutional layer, we apply 2D batch normalization
    (which I explained in the last section) and LeakyReLU activation (which I’ll explain
    later) on the output. The LeakyReLU activation function is a modified version
    of ReLU. It allows the output to have a slope for values below zero. Specifically,
    the LeakyReLU function is defined as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从第二个2D卷积层开始，我们在输出上应用2D批量归一化（我在上一节中解释过）和LeakyReLU激活（我将在后面解释）。LeakyReLU激活函数是ReLU的修改版。它允许输出在零以下的值具有斜率。具体来说，LeakyReLU函数的定义如下：
- en: '![](../../OEBPS/Images/CH04_F07_Liu_EQ01.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F07_Liu_EQ01.png)'
- en: where β is a constant between 0 and 1\. The LeakyReLU activation function is
    commonly used to address the sparse gradients problem (when most gradients become
    zero or near-zero). Training DCGANs is one such case. When the input to a neuron
    is negative, the output of ReLU is zero, and the neuron becomes inactive. LeakyReLU
    returns a small negative value, not zero, for negative inputs. This helps keep
    the neurons active and learning, maintaining a better gradient flow and leading
    to faster convergence of model parameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其中β是介于0和1之间的常数。LeakyReLU激活函数通常用于解决稀疏梯度问题（当大多数梯度变为零或接近零时）。训练DCGANs就是这样一种情况。当神经元的输入为负时，ReLU的输出为零，神经元变得不活跃。LeakyReLU对负输入返回一个小的负值，而不是零。这有助于保持神经元活跃和学习，保持更好的梯度流动，并导致模型参数更快地收敛。
- en: We’ll use the same approach when building the generator for clothing item generation.
    We’ll mirror the layers used in the discriminator in DCGAN to create a generator,
    as shown in the following listing.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建服装生成器的生成器时，我们将采用相同的方法。我们将使用DCGAN中判别器中使用的层来创建一个生成器，如下面的列表所示。
- en: Listing 4.5 Designing a generator in DCGAN
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5 在DCGAN中设计生成器
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① The first layer in the generator is modeled after the last layer in the discriminator.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ①生成器中的第一层是模仿判别器中的最后一层。
- en: ② The second layer in the generator is symmetric to the second to last layer
    in the discriminator (numbers of inputs and outputs have switched positions).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ②生成器中的第二层与判别器中的倒数第二层对称（输入和输出的数量位置已交换）。
- en: ③ The last layer in the generator is symmetric to the first layer in the discriminator.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ③生成器中的最后一层与判别器中的第一层对称。
- en: ④ Uses the Tanh() activation to squeeze values in the output layer to the range
    [–1, 1] because the images in the training set have values between –1 and 1
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ④使用Tanh()激活函数将输出层的值挤压到[–1, 1]的范围内，因为训练集中的图像值介于–1和1之间
- en: 'As shown in figure 4.8, to create an image, the generator uses five 2D transposed
    convolutional layers: they are symmetric to the five 2D convolutional layers in
    the discriminator. For example, the last layer, `ConvTranspose2d(64, 3, 4, 2,
    1, bias=False)`, is modeled after the first layer in the discriminator, `Conv2d(3,
    64, 4, 2, 1, bias=False)`. The numbers of *input* and *output* channels in `Conv2d`
    are reversed and used as the numbers of *output* and *input* channels in `ConvTranspose2d`.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如图4.8所示，为了创建图像，生成器使用了五个2D转置卷积层：它们与判别器中的五个2D卷积层对称。例如，最后一层`ConvTranspose2d(64,
    3, 4, 2, 1, bias=False)`是模仿判别器的第一层`Conv2d(3, 64, 4, 2, 1, bias=False)`。`Conv2d`中的`*input*`和`*output*`通道的数量被反转，并用作`ConvTranspose2d`中的`*output*`和`*input*`通道的数量。
- en: '![](../../OEBPS/Images/CH04_F08_Liu.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F08_Liu.png)'
- en: Figure 4.8 Designing a generator network in DCGAN to create anime faces by mirroring
    the layers in the discriminator network. The right side of the diagram shows the
    discriminator network, which contains five 2D convolutional layers. To design
    a generator that can conjure up anime faces out of thin air, we mirror the layers
    in the discriminator network. Specifically, as shown on the left half of the figure,
    the generator has five 2D transposed convolutional layers, symmetric to the 2D
    convolutional layers in the discriminator. Further, in each of the top four layers,
    the numbers of `input` and `output` channels in the discriminator are reversed
    and used as the numbers of `output` and `input` channels in the generator.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 在DCGAN中设计生成器网络以通过镜像判别器网络中的层来创建动漫面孔。图的右侧显示了判别器网络，其中包含五个2D卷积层。为了设计一个能够凭空创造出动漫面孔的生成器，我们镜像了判别器网络中的层。具体来说，如图的左侧所示，生成器有五个2D转置卷积层，与判别器中的2D卷积层对称。此外，在顶部四个层中，判别器中的`input`和`output`通道的数量被反转，并用作生成器中的`output`和`input`通道的数量。
- en: The number of input channels in the first 2D transposed convolutional layer
    is 100\. This is because the generator obtains a 100-value random noise vector
    from the latent space (bottom left of figure 4.8) and feeds it to the generator.
    The number of output channels in the last 2D transposed convolutional layer in
    the generator is 3 because the output is an image with three color channels (RGB).
    We apply the Tanh activation function to the output of the generator to squeeze
    all values to the range [–1, 1] because the training images all have values between
    –1 and 1.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层2D转置卷积层的输入通道数为100。这是因为生成器从潜在空间（图4.8的左下角）获取一个100值的随机噪声向量并将其馈送到生成器。生成器中最后2D转置卷积层的输出通道数为3，因为输出是一个具有三个颜色通道（RGB）的图像。我们对生成器的输出应用Tanh激活函数，将所有值挤压到范围[–1,
    1]，因为训练图像的所有值都在–1和1之间。
- en: 'As usual, the loss function is binary cross-entropy loss. The discriminator
    is trying to maximize the accuracy of the binary classification: identify a real
    sample as real and a fake sample as fake. The generator, on the other hand, is
    trying to minimize the probability that the fake sample is being identified as
    fake.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，损失函数是二元交叉熵损失。判别器试图最大化二元分类的准确性：将真实样本识别为真实，将伪造样本识别为伪造。另一方面，生成器试图最小化伪造样本被识别为伪造的概率。
- en: 'We’ll use the Adam optimizer for both the discriminator and the generator and
    set the learning rate to 0.0002:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Adam优化器来训练判别器和生成器，并将学习率设置为0.0002：
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You have seen the Adam optimizer in chapter 2 but with default values of betas.
    Here, we select betas that are different from the default values. The betas in
    the Adam optimizer play crucial roles in stabilizing and speeding up the convergence
    of the training process. They do this by controlling how much emphasis is placed
    on recent versus past gradient information (beta1) and by adapting the learning
    rate based on the certainty of the gradient information (beta2). These parameters
    are typically fine-tuned based on the specific characteristics of the problem
    being solved.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第二章中已经看到了Adam优化器，但这里我们选择了与默认值不同的beta。在这里，我们选择了不同的beta。Adam优化器中的beta在稳定和加速训练过程方面起着至关重要的作用。它们通过控制对最近和过去梯度信息的重视程度（beta1）以及根据梯度信息的确定性调整学习率（beta2）来实现这一点。这些参数通常根据要解决的问题的具体特征进行微调。
- en: 4.5.2 Training and using DCGAN
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 训练和使用DCGAN
- en: 'The training process for DCGAN is similar to what we have done for other GAN
    models, such as those used in chapter 3 and earlier in this chapter. Since we
    don’t know the true distribution of anime face images, we’ll rely on visualization
    techniques to determine when the training is complete. Specifically, we define
    a `test_epoch()` function to visualize the anime faces created by the generator
    after each epoch of training:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN的训练过程与我们之前在其他GAN模型中做过的类似，例如第三章和本章早些时候使用的模型。由于我们不知道动漫人脸图像的真实分布，我们将依靠可视化技术来确定何时训练完成。具体来说，我们定义了一个`test_epoch()`函数，用于在每次训练周期后可视化生成器创建的动漫人脸：
- en: '[PRE37]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ① Obtains 32 random noise vectors from the latent space
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从潜在空间获取32个随机噪声向量
- en: ② Generates 32 anime face images
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成32张动漫人脸图像
- en: ③ Plots the generated images in a 4 × 8 grid
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在4 × 8网格中绘制生成的图像
- en: ④ Calls the function to generate images before training the model
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在训练模型之前调用生成图像的函数
- en: If you run the preceding code cell, you’ll see 32 images that look like snowflake
    statics on a TV screen. They don’t look like anime faces at all because we haven’t
    trained the generator yet.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码单元，你会看到32张看起来像电视屏幕上的雪花静态的图像。它们根本不像动漫人脸，因为我们还没有训练生成器。
- en: We define three functions, `train_D_on_real()`, `train_D_on_fake()`, and `train_G()`,
    similar to those we used to train the GANs to generate grayscale images of clothing
    items earlier in this chapter. Go to the Jupypter Notebook for this chapter in
    the book’s GitHub repository and familiarize yourself with the functions. They
    train the discriminator with real images. They then train the discriminator with
    fake images; finally, they train the generator.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了三个函数，`train_D_on_real()`、`train_D_on_fake()`和`train_G()`，类似于我们之前在本章中用于训练GAN以生成服装物品灰度图像的函数。请访问本书GitHub仓库中本章的Jupyter
    Notebook，熟悉这些函数。它们使用真实图像训练判别器。然后，它们使用伪造图像训练判别器；最后，它们训练生成器。
- en: 'Next, we train the model for 20 epochs:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练模型20个周期：
- en: '[PRE38]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The training takes about 20 minutes if you are using GPU training. Otherwise,
    it may take 2 to 3 hours, depending on the hardware configuration on your computer.
    Alternatively, you can download the trained model from my website: [https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip](https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用GPU进行训练，训练时间大约需要20分钟。否则，根据您电脑上的硬件配置，可能需要2到3小时。另外，您可以从我的网站上下载训练好的模型：[https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip](https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip)。
- en: After every epoch of training, you can visualize the generated anime faces.
    After just one epoch of training, the model can already generate color images
    that look like anime faces, as shown in figure 4.9\. As training progresses, the
    quality of the generated images becomes better and better.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期之后，您可以可视化生成的动漫人脸。经过仅一个周期的训练后，模型已经可以生成看起来像动漫人脸的彩色图像，如图4.9所示。随着训练的进行，生成的图像质量会越来越好。
- en: '![](../../OEBPS/Images/CH04_F09_Liu.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F09_Liu.png)'
- en: Figure 4.9 Generated images in DCGAN after one epoch of training
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 DCGAN经过一个周期训练后的生成图像
- en: 'We’ll discard the discriminator and save the trained generator in the local
    folder:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将丢弃判别器，并将训练好的生成器保存在本地文件夹中：
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To use the trained generator, we load up the model and use it to generate 32
    images:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用训练好的生成器，我们加载模型并使用它生成32张图像：
- en: '[PRE40]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The generated anime faces are shown in figure 4.10\. The generated images bear
    a close resemblance to the ones in the training set shown in figure 4.7.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的动漫人脸图像如图4.10所示。生成的图像与图4.7中显示的训练集图像非常相似。
- en: '![](../../OEBPS/Images/CH04_F10_Liu.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F10_Liu.png)'
- en: Figure 4.10 Generated anime face images by the trained generator in DCGAN
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 由训练好的DCGAN生成器生成的动漫人脸图像
- en: 'You may have noticed that the hair colors of the generated images are different:
    some are black, some are red, and some are blond. You may wonder: Can we tell
    the generator to create images with a certain characteristic, such as black hair
    or red hair? The answer is yes. You’ll learn a couple of different methods to
    select characteristics in generated images in GANs in chapter 5\.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到生成的图像中的发色不同：有的是黑色，有的是红色，有的是金色。您可能会想：我们能否让生成器创建具有特定特征（如黑色头发或红色头发）的图像？答案是肯定的。您将在第5章中学习几种不同的方法来在GANs中选取生成图像的特征。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: To conjure up realistic-looking images out of thin air, the generator mirrors
    layers used in the discriminator network.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要从无中生有地生成看起来逼真的图像，生成器会镜像判别器网络中使用的层。
- en: While it’s feasible to generate grayscale images by using just fully connected
    layers, to generate high-resolution color images, we need to use CNNs.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然仅使用全连接层就可以生成灰度图像是可行的，但要生成高分辨率彩色图像，我们需要使用卷积神经网络（CNNs）。
- en: Two-dimensional convolutional layers are used for feature extraction. They apply
    a set of learnable filters (also known as kernels) to the input data to detect
    patterns and features at different spatial scales. These layers are essential
    for capturing hierarchical representations of the input data.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维卷积层用于特征提取。它们将一组可学习的滤波器（也称为核）应用于输入数据，以检测不同空间尺度上的模式和特征。这些层对于捕获输入数据的层次表示至关重要。
- en: Two-dimensional transposed convolutional layers (also known as deconvolution
    or upsampling layers) are used for upsampling or generating high-resolution feature
    maps. They apply a filter to the input data. However, unlike standard convolution,
    they increase the spatial dimensions by inserting gaps between the output values,
    which effectively “upscales” the feature maps. This process generates feature
    maps of a higher resolution.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维转置卷积层（也称为反卷积或上采样层）用于上采样或生成高分辨率特征图。它们对输入数据应用一个滤波器。然而，与标准卷积不同，它们通过在输出值之间插入间隔来增加空间维度，从而有效地“上采样”特征图。这个过程生成了更高分辨率的特征图。
- en: Two-dimensional batch normalization is a technique commonly used in deep learning
    and neural networks to improve the training and performance of CNNs and other
    models that work with 2D data, such as images. It normalizes the values for each
    feature channel, so they have a mean of 0 and a standard deviation of 1, which
    helps stabilize and speed up training.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维批量归一化是一种在深度学习和神经网络中常用的技术，用于提高CNNs和其他处理2D数据（如图像）的模型训练和性能。它对每个特征通道的值进行归一化，使它们具有0的均值和1的标准差，这有助于稳定和加速训练。
- en: '* * *'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: '^([1](#footnote-000-backlink))  Sergey Ioffe, Christian Szegedy, 2015, “Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift.” [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink))  谢尔盖·约费，克里斯蒂安·塞格迪，2015年，“批量归一化：通过减少内部协变量偏移来加速深度网络训练。”
    [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).
