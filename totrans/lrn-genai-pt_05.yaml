- en: 4 Image generation with generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Designing a generator by mirroring steps in the discriminator network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a 2D convolutional operation works on an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How a 2D transposed convolutional operation inserts gaps between the output
    values and generates feature maps of a higher resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training generative adversarial networks to generate grayscale
    and color images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have successfully generated an exponential growth curve and a sequence
    of integers that are all multiples of 5 in chapter 3\. Now that you understand
    how generative adversarial networks (GANs) work, you are ready to apply the same
    skills to generate many other forms of content, such as high-resolution color
    images and realistic-sounding music. However, this may be easier said than done
    (you know what they say: the devil is in the details). For example, exactly how
    can we make the generator conjure up realistic images out of thin air? That’s
    the question we’re going to tackle in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: A common approach for the generator to create images from scratch is to mirror
    steps in the discriminator network. In the first project in this chapter, your
    goal is to create grayscale images of clothing items such as coats, shirts, sandals,
    and so on. You learn to mirror the layers in the discriminator network when designing
    a generator network. In this project, only dense layers are used in both the generator
    and discriminator networks. Each neuron in a dense layer is connected to every
    neuron in the previous and next layer. For this reason, dense layers are also
    called fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: In the second project in this chapter, your goal is to create high-resolution
    color images of anime faces. Like in the first project, the generator mirrors
    the steps in the discriminator network to conjure up images. However, high-resolution
    color images in this project contain many more pixels than the low-resolution
    grayscale images in the first project. If we use dense layers only, the number
    of parameters in the model increases enormously. This, in turn, makes learning
    slow and ineffective. We, therefore, turn to convolutional neural networks (CNNs).
    In CNNs, each neuron in a layer is connected only to a small region of the input.
    This local connectivity reduces the number of parameters, making the network more
    efficient. CNNs require fewer parameters than fully connected networks of similar
    size, leading to faster training times and lower computational costs. CNNs are
    also generally more effective at capturing spatial hierarchies in image data because
    they treat images as multidimensional objects instead of 1D vectors.
  prefs: []
  type: TYPE_NORMAL
- en: To prepare you for the second project, we’ll show you how convolutional operations
    work and how they downsample the input images and extract spatial features in
    them. You’ll also learn concepts such as filter size, stride, and zero-padding
    and how they affect the degree of downsampling in CNNs. While the discriminator
    network uses convolutional layers, the generator mirrors these layers by using
    transposed convolutional layers (also known as deconvolution or upsampling layers).
    You’ll learn how transposed convolutional layers are used for upsampling to generate
    high-resolution feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, you’ll learn how to mirror the steps in the discriminator network
    to create images from scratch in this chapter. In addition, you’ll learn how convolutional
    layers and transposed convolutional layers work. After this chapter, you’ll use
    convolutional layers and transposed convolutional layers to create high-resolution
    images in other settings later in this book (such as in feature transfers when
    training a CycleGAN to convert blond hair to black hair or in a variational autoencoder
    [VAE] to generate high-resolution human face images).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 GANs to generate grayscale images of clothing items
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal in the first project is to train a model to generate grayscale images
    of clothing items such as sandals, t-shirts, coats, and bags.
  prefs: []
  type: TYPE_NORMAL
- en: When you use GANs to generate images, you’ll always start by obtaining training
    data. You’ll then create a discriminator network from scratch. You’ll mirror steps
    in the discriminator network when creating a generator network. Finally, you’ll
    train the GANs and use the trained model for image generation. Let’s see how that
    works with a simple project that creates grayscale images of clothing items.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Training samples and the discriminator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The steps involved with preparing the training data are similar to what we have
    done in chapter 2, with a few exceptions that I’ll highlight later. To save time,
    I’ll skip the steps you have seen before in chapter 2 and refer you to the book’s
    GitHub repository. Follow the steps in the Jupyter Notebook for this chapter in
    the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    so that you create a data iterator with batches.
  prefs: []
  type: TYPE_NORMAL
- en: There are 60,000 images in the training set. In chapter 2, we split the training
    set further into a train set and a validation set. We used the loss in the validation
    set to determine whether the parameters had converged so that we could stop training.
    However, GANs are trained using a different approach compared to traditional supervised
    learning models (such as the classification models you have seen in chapter 2).
    Since the quality of the generated samples improves throughout training, the discriminator’s
    task becomes more and more difficult. The loss from the discriminator network
    is not a good indicator of the quality of the model. The usual way of measuring
    the performance of GANs is through visual inspection to assess the quality and
    realism of generated images. We can potentially compare the quality of generated
    samples with training samples and use methods such as the Inception Score to evaluate
    the performance of GANs (See, for example, “Pros and Cons of GAN Evaluation Measures,”
    by Ali Borji, 2018, for a survey on various GAN evaluation methods; [https://arxiv.org/abs/1802.03446](https://arxiv.org/abs/1802.03446)).
    However, researchers have documented the weaknesses of these measures (“A Note
    on the Inception Score,” by Shane Barratt and Rishi Sharma, 2018, demonstrates
    that the inception score fails to provide useful guidance when comparing models;
    [https://arxiv.org/abs/1801.01973](https://arxiv.org/abs/1801.01973)). In this
    chapter, we’ll use visual inspections to check the quality of generated samples
    periodically and determine when to stop training.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator network is a binary classifier, which is similar to the binary
    classifier for clothing items we discussed in chapter 2\. Here the discriminator’s
    job is to classify the samples into either real or fake.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use PyTorch to create the following discriminator neural network D:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① The first fully connected layer has 784 inputs and 1,024 outputs.
  prefs: []
  type: TYPE_NORMAL
- en: ② The last fully connected layer has 256 inputs and 1 output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input size is 784 because each grayscale image has a size of 28 × 28 pixels
    in the training set. Because dense layers take only 1D inputs, we flatten the
    images before feeding them to the model. The output layer has just one neuron
    in it: the output of the discriminator D is a single value. We use the sigmoid
    activation function to squeeze the output to the range [0, 1] so that it can be
    interpreted as the probability, p, that the sample is real. With complementary
    probability 1 – p, the sample is fake.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.1
  prefs: []
  type: TYPE_NORMAL
- en: Modify the discriminator D so that the numbers of outputs in the first three
    layers are 1,000, 500, and 200 instead of 1,024, 512, and 256\. Make sure the
    number of outputs in a layer matches the number of inputs in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 A generator to create grayscale images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the discriminator network is fairly easy to create, how to create a generator
    so that it can conjure up realistic images is a different matter. A common approach
    is to mirror the layers used in the discriminator network to create a generator,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 Designing a generator by mirroring layers in the discriminator
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① The first layer in the generator is symmetric to the last layer in the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: ② The second layer in the generator is symmetric to the second to last layer
    in the discriminator (numbers of inputs and outputs have switched positions).
  prefs: []
  type: TYPE_NORMAL
- en: ③ The third layer in the generator is symmetric to the third to last layer in
    the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: ④ The last layer in the generator is symmetric to the first layer in the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Uses Tanh() activation so the output is between –1 and 1, the same as values
    in images
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1 provides a diagram of the architecture of generator and discriminator
    networks in the GAN to generate grayscale images of clothing items. As shown in
    the top right corner of figure 4.1, a flattened grayscale image from the training
    set, which contains 28 × 28 = 784 pixels, goes through four dense layers sequentially
    in the discriminator network, and the output is the probability that the image
    is real. To create an image, the generator uses the same four dense layers but
    in reverse order: it obtains a 100-value random noise vector from the latent space
    (bottom left in figure 4.1) and feeds the vector through the four dense layers.
    In each layer, the numbers of *inputs* and *outputs* in the discriminator are
    reversed and used as the numbers of *outputs* and *inputs* in the generator. Finally,
    the generator comes up with a 784-value tensor, which can be reshaped into a 28
    × 28 grayscale image (top left).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1 Designing a generator network to create clothing items by mirroring
    the layers in the discriminator network. The right side of the diagram shows the
    discriminator network, which contains four dense layers. To design a generator
    that can conjure up clothing items from thin air, we mirror the layers in the
    discriminator network. Specifically, as shown on the left half of the figure,
    the generator has four similar dense layers in it but in reverse order: the first
    layer in the generator mirrors the last layer in the discriminator, the second
    layer in the generator mirrors the second to last layer in the discriminator,
    and so on. Further, in each of the top three layers, the numbers of inputs and
    outputs in the discriminator are reversed and used as the numbers of outputs and
    inputs in the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The left side of figure 4.1 is the generator network, while the right side
    is the discriminator network. If you compare the two networks, you’ll notice how
    the generator mirrors the layers used in the discriminator. Specifically, the
    generator has four similar dense layers in it but in reverse order: the first
    layer in the generator mirrors the last layer in the discriminator, the second
    layer in the generator mirrors the second to last layer in the discriminator,
    and so on. The number of outputs of the generator is 784, with values between
    -1 and 1 after the `Tanh()` activation, and this matches the input to the discriminator
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.2
  prefs: []
  type: TYPE_NORMAL
- en: Modify the generator G so that the numbers of outputs in the first three layers
    are 1,000, 500, and 200 instead of 1,024, 512, and 256\. Make sure that the modified
    generator mirrors the layers used in the modified discriminator in exercise 4.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in GAN models we have seen in chapter 3, the loss function is the binary
    cross-entropy loss since the discriminator D is performing a binary classification
    problem. We’ll use the Adam optimizer for both the discriminator and the generator,
    with a learning rate of 0.0001:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll train the GANs we just created by using the clothing item images
    in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Training GANs to generate images of clothing items
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training process is similar to what we have done in chapter 3 when training
    GANs to generate an exponential growth curve or to generate a sequence of numbers
    that are all multiples of 5\.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in chapter 3, we’ll solely rely on visual inspections to determine whether
    the model is well-trained. For that purpose, we define a `see_output()` function
    to visualize the fake images created by the generator periodically.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE Interested readers can check this GitHub repository to learn how to implement
    the inception score in PyTorch to evaluate GANs: [https://github.com/sbarratt/inception-score-pytorch](https://github.com/sbarratt/inception-score-pytorch).
    However, the repository doesn’t recommend using the inception score to evaluate
    generative models due to its ineffectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 Defining a function to visualize the generated clothing items
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates 32 fake images
  prefs: []
  type: TYPE_NORMAL
- en: ② Plots them in a 4 × 8 grid
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shows the ith image
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calls the see_output() function to visualize the generated images before training
  prefs: []
  type: TYPE_NORMAL
- en: If you run the preceding code cell, you’ll see 32 images that look like snowflake
    statics on a TV screen, as shown in figure 4.2\. They don’t look like clothing
    items at all because we haven’t trained the generator yet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 Output from the GAN model to generate clothing items before training.
    Since the model is not trained, the generated images are nothing like the images
    in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the GAN model, we define a few functions: `train_D_on_real()`, `train_D_on_fake()`,
    and `train_G()`. They are similar to those defined in chapter 3\. Go to the Jupypter
    Notebook for this chapter in the book’s GitHub repository and see what minor modifications
    we have made.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to train the model. We iterate through all batches in the training
    dataset. For each batch of data, we first train the discriminator using the real
    samples. After that, the generator creates a batch of fake samples, and we use
    them to train the discriminator again. Finally, we let the generator create a
    batch of fake samples again, but this time, we use them to train the generator
    instead. We train the model for 50 epochs, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 Training GANs for clothing item generation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Trains the discriminator using real samples
  prefs: []
  type: TYPE_NORMAL
- en: ② Trains the discriminator using fake samples
  prefs: []
  type: TYPE_NORMAL
- en: ③ Trains the generator
  prefs: []
  type: TYPE_NORMAL
- en: ④ Visualizes generated samples after every 10 epochs
  prefs: []
  type: TYPE_NORMAL
- en: 'The training takes about 10 minutes if you are using GPU training. Otherwise,
    it may take an hour or so, depending on the hardware configuration on your computer.
    Or you can download the trained model from my website: [https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip](https://gattonweb.uky.edu/faculty/lium/gai/fashion_gen.zip).
    Unzip it after downloading.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After every 10 epochs of training, you can visualize the generated clothing
    items, as shown in figure 4.3\. After just 10 epochs of training, the model can
    already generate clothing items that clearly can pass as real: you can tell what
    they are. The first three items in the first row in figure 4.3 are clearly a coat,
    a dress, and a pair of trousers, for example. As training progresses, the quality
    of the generated images becomes better and better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Clothing items generated by an image GAN model after 10 epochs of
    training
  prefs: []
  type: TYPE_NORMAL
- en: 'As we do in all GANs, we discard the discriminator and save the trained generator
    to generate samples later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now saved the generator in the local folder. To use the generator,
    we load up the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The generator is now loaded. We can use it to generate clothing items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The generated clothing items are shown in figure 4.4\. As you can see, the clothing
    items are fairly close to those in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F04_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 Clothing items generated by a trained image GAN model (after 50 epochs)
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to create grayscale images by using GANs, you’ll
    learn how to generate high-resolution color images by using deep convolutional
    GAN (DCGAN) in the remaining sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Convolutional layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create high-resolution color images, we need more sophisticated techniques
    than simple fully connected neural networks. Specifically, we’ll use CNNs, which
    are particularly effective for processing data with a grid-like topology, such
    as images. They are distinct from fully connected (dense) layers in a couple of
    ways. First, in CNNs, each neuron in a layer is connected only to a small region
    of the input. This is based on the understanding that in image data, local groups
    of pixels are more likely to be related to each other. This local connectivity
    reduces the number of parameters, making the network more efficient. Second, CNNs
    use the concept of shared weights—the same weights are used across different regions
    of the input. This is akin to sliding a filter across the entire input space.
    This filter detects specific features (e.g., edges or textures) regardless of
    their position in the input, leading to the property of translation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: Due to their structure, CNNs are more efficient for image processing. They require
    fewer parameters than fully connected networks of similar size, leading to faster
    training times and lower computational costs. They are also generally more effective
    at capturing spatial hierarchies in image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional layers and transposed convolutional layers are two fundamental
    building blocks in CNNs, commonly used in image processing and computer vision
    tasks. They have different purposes and characteristics: convolutional layers
    are used for feature extraction. They apply a set of learnable filters (also known
    as kernels) to the input data to detect patterns and features at different spatial
    scales. These layers are essential for capturing hierarchical representations
    of the input data. In contrast, transposed convolutional layers are used for upsampling
    or generating high-resolution feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll learn how convolutional operations work and how kernel
    size, stride, and zero-padding affect convolutional operations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 How do convolutional operations work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional layers use filters to extract spatial patterns on the input data.
    A convolutional layer is capable of automatically detecting a large number of
    patterns and associating them with the target label. Therefore, convolutional
    layers are commonly used in image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional operations involve applying a filter to an input image to produce
    a feature map. This process involves using element-wise multiplication of the
    filter with the input image and summing the results. The weights in the filter
    are the same as the filter moves on the input image to scan different areas. Figure
    4.5 shows a numerical example of how convolutional operations work. The left column
    is the input image, and the second column is a filter (a 2 × 2 matrix). Convolutional
    operations (the third column) involve sliding the filter over the input image,
    multiplying corresponding elements, and summing them up (the last column).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F05_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 A numerical example of how convolutional operations work, with stride
    equal to 1 and no padding
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain a deep understanding of exactly how convolutional operations work,
    let’s implement the convolutional operations in PyTorch in parallel so that you
    can verify the numbers as shown in figure 4.5\. First, let’s create a PyTorch
    tensor to represent the input image in the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① The four values in the shape of the image, (1, 1, 3, 3), are the number of
    images in the batch, number of color channels, image height, and image width,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The image is reshaped so that it has a dimension of (1, 1, 3, 3), indicating
    that there is just one observation in the batch, and the image has just one color
    channel. The height and the width of the image are both 3 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s represent the 2 × 2 filter, as shown in the second column of figure 4.5,
    by creating a 2D convolutional layer in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Initiates a 2D convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts the randomly initialized weights and bias in the layer
  prefs: []
  type: TYPE_NORMAL
- en: A 2D convolutional layer takes several arguments. The `in_channels` argument
    is the number of channels in the input image. This value is 1 for grayscale images
    and 3 for color images since color images have three color channels (red, green,
    and blue [RGB]). The `out_channels` is the number of channels after the convolutional
    layer, which can take any number based on how many features you want to extract
    from the image. The `kernel_size` argument controls the size of the kernel; for
    example, `kernel_size=3` means the filter has a shape of 3 × 3, and `kernel_size=4`
    means the filter has a shape of 4 × 4\. We set the kernel size to 2 so the filter
    has a shape of 2 × 2\.
  prefs: []
  type: TYPE_NORMAL
- en: A 2D convolutional layer also has several optional arguments. The `stride` argument
    specifies how many pixels to move to the right or down each time the filter moves
    along the input image. The `stride` argument has a default value of 1\. A higher
    value of stride leads to more downsampling of the image. The `padding` argument
    means how many rows of zeros to add to four sides of the input image, with a default
    value of 0\. The `bias` argument indicates whether to add a learnable bias as
    the parameter, with a default value of `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding 2D convolutional layer has one input channel, one output channel,
    with a kernel size of 2 × 2, and a stride of 1\. When the convolutional layer
    is created, the weights and the bias in it are randomly initialized. You will
    see the following output as the weights and the bias of this convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To make our example easier to follow, we’ll replace the weights and the bias
    with whole numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Handpicks weights and bias
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces the weights and bias in the convolutional layer with our handpicked
    numbers
  prefs: []
  type: TYPE_NORMAL
- en: ③ Prints out the new weights and bias in the convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are not learning the parameters in the convolutional layer, `torch.no_grad()`
    is used to disable gradient calculation, which reduces memory consumption and
    speeds up computations. Now the convolutional layer has weights and the bias that
    we have chosen. They also match the numbers in figure 4.5\. The output from the
    preceding code cell is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we apply the preceding convolutional layer on the 3 × 3 image we mentioned,
    what is the output? Let’s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output has a shape of (1, 1, 2, 2), with four values in it: 7, 14, 54,
    and 50\. These numbers match those in figure 4.5.'
  prefs: []
  type: TYPE_NORMAL
- en: But how exactly does the convolutional layer generate this output through the
    filter? We’ll explain in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: The input image is a 3 × 3 matrix, and the filter is a 2 × 2 matrix. When the
    filter scans over the image, it first covers the four pixels in the top left corner
    of the image, which have values `[[1, 1], [0, 1]]`, as shown in the first row
    in Figure 4.5\. The filter has values `[[1,2],[3,4]]`. The convolution operation
    finds the sum of the element-wise multiplication of the two tensors (in this case,
    one tensor is the filter and the other is the covered area). In other words, the
    convolution operation performs element-wise multiplication in each of the four
    cells and then adds up the values in the four cells. Therefore, the output from
    scanning the top left corner is
  prefs: []
  type: TYPE_NORMAL
- en: 1 × 1 × 1 × 2 + 0 × 3 + 1 × 4 = 7.
  prefs: []
  type: TYPE_NORMAL
- en: 'This explains why the top left corner of the output has a value of 7\. Similarly,
    when the filter is applied to the top right corner of the image, the covered area
    is `[[1,1],[1,2]]`. The output is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 × 1 + 1 × 2 + 1 × 3 + 2 × 4 = 14.
  prefs: []
  type: TYPE_NORMAL
- en: This explains why the top right corner of the output has a value of 14\.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.3
  prefs: []
  type: TYPE_NORMAL
- en: What are the values in the covered area when the filter is applied to the bottom
    right corner of the image? Explain why the bottom right corner of the output has
    a value of 50.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 How do stride and padding affect convolutional operations?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stride and zero padding are two important concepts in the context of convolutional
    operations. They play a crucial role in determining the dimensions of the output
    feature map and the way the filter interacts with the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Stride refers to the number of pixels by which the filter moves across the input
    image. When the stride is 1, the filter moves 1 pixel at a time. A larger stride
    means the filter jumps over more pixels as it slides over the image. Increasing
    the stride reduces the spatial dimensions of the output feature map.
  prefs: []
  type: TYPE_NORMAL
- en: Zero padding involves adding layers of zeros around the border of the input
    image before applying the convolutional operation. Zero padding allows control
    over the spatial dimensions of the output feature map. Without padding, the dimensions
    of the output will be smaller than the input. By adding padding, you can preserve
    the dimensions of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use an example to show how stride and padding work. The following code
    cell redefines the 2D convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Changes the stride from 1 to 2
  prefs: []
  type: TYPE_NORMAL
- en: ② Changes the padding from 0 to 1
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `padding=1` argument adds one row of 0s around the input image, so the padded
    image now has a size of 5 × 5 instead of 3 × 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the filter scans over the padded image, it first covers the top left corner,
    which has values `[[0, 0], [0, 1]]`. The filter has values `[[1,2],[3,4]]`. Therefore,
    the output from scanning the top left corner is:'
  prefs: []
  type: TYPE_NORMAL
- en: 0 × 1+0 × 2+0 × 3+1 × 4=4
  prefs: []
  type: TYPE_NORMAL
- en: 'This explains why the top left corner of the output has a value of 4\. Similarly,
    when the filter slides two pixels down to the bottom left corner of the image,
    the covered area is `[[0,0],[0,8]]`. The output is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: 0 × 1+0 × 2+0 × 3+8 × 4=32
  prefs: []
  type: TYPE_NORMAL
- en: This explains why the bottom left corner of the output has a value of 32\.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Transposed convolution and batch normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transposed convolutional layers are also known as deconvolution or upsampling
    Layers. They are used for upsampling or generating high-resolution feature maps.
    They are often employed in generative models like GANs and VAEs.
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolutional layers apply a filter to the input data, but unlike
    standard convolution, they increase the spatial dimensions by inserting gaps between
    the output values, which effectively “upscales” the feature maps. This process
    generates feature maps of a higher resolution. Transposed convolutional layers
    help increase the spatial resolution, which is useful in image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Strides can be used in transposed convolution layers to control the amount of
    upsampling. The greater the value of the stride, the more upsampling the transposed
    convolution layer has on the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Two-dimensional batch normalization is a technique used in neural networks,
    particularly CNNs, to stabilize and speed up the training process. It addresses
    several problems, including saturation, vanishing gradients, and exploding gradients,
    which are common challenges in deep learning. In this section, you’ll look at
    some examples so you have a deeper understanding of how it works. You’ll use it
    when creating GANs to generate high-resolution color images in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients in deep learning
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing gradient problem occurs in deep neural networks when the gradients
    of the loss function with respect to the network parameters become exceedingly
    small during backpropagation. This results in very slow updates to the parameters,
    hindering the learning process, especially in the early layers of the network.
    Conversely, the exploding gradient problem happens when these gradients become
    excessively large, leading to unstable updates and causing the model parameters
    to oscillate or diverge to very large values. Both problems impede the effective
    training of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 How do transposed convolutional layers work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to convolutional layers, transposed convolutional layers upsample and
    fill in gaps in an image to generate features and increase resolution by using
    kernels (i.e., filters). The output is usually larger than the input in a transposed
    convolutional layer. Therefore, transposed convolutional layers are essential
    tools when it comes to generating high-resolution images. To show you exactly
    how 2D transposed convolutional operations work, let’s use a simple example and
    a figure. Suppose you have a very small 2 × 2 input image, as shown in the left
    column in figure 4.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F06_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 A numerical example of how transposed convolutional operations work
  prefs: []
  type: TYPE_NORMAL
- en: 'The input image has the following values in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You want to upsample the image so that it has a higher resolution. You can
    create a 2D transposed convolutional layer in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ① A transposed convolutional layer with one input channel, one output channel,
    a kernel size of 2, and a stride of 2
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces the weights and bias in the transposed convolutional layer with handpicked
    values
  prefs: []
  type: TYPE_NORMAL
- en: This 2D transposed convolutional layer has one input channel, one output channel,
    with a kernel size of 2 × 2 and a stride of 2\. The 2 × 2 filter is shown in the
    second column in figure 4.6\. We replaced the randomly initialized weights and
    the bias in the layer with our handpicked whole numbers so it’s easy to follow
    the calculations. The `state_dict()` method in the preceding code listing returns
    the parameters in a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the transposed convolutional layer is applied to the 2 × 2 image we mentioned
    earlier, what is the output? Let’s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output has a shape of (1, 1, 4, 4), meaning we have upsampled a 2 × 2 image
    to a 4 × 4 image. How does the transposed convolutional layer generate the preceding
    output through the filter? We’ll explain in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: The image is a 2 × 2 matrix, and the filter is also a 2 × 2 matrix. When the
    filter is applied to the image, each element in the image multiplies with the
    filter and goes to the output. The top left value in the image is 1, and we multiply
    it with the values in the filter, `[[2, 3], [4, 5]]`, and this leads to the four
    values in the top left block of the output matrix `transoutput`, with values `[[2,
    3], [4, 5]]`, as shown at the top right corner in figure 4.6\. Similarly, the
    bottom left value in the image is 2, and we multiply it with the values in the
    filter, `[[2, 3], [4, 5]]`, and this leads to the four values in the bottom left
    block of the output matrix `transoutput`, `[[4, 6], [8, 10]]`.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.4
  prefs: []
  type: TYPE_NORMAL
- en: If an image has values `[[10, 10], [15, 20]]` in it, what is the output after
    you apply the 2D transposed convolutional layer `transconv` to the image? Assume
    `transconv` has values `[[2, 3], [4, 5]]` in it. Assume a kernel size of 2 and
    a stride size of 2\.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Batch normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two-dimensional batch normalization is a standard technique in modern deep learning
    frameworks and has become a crucial component for effectively training deep neural
    networks. You’ll see it quite often later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In 2D batch normalization, normalization is performed independently for each
    feature channel by adjusting and scaling values in the channel so they have a
    mean of 0 and a variance of 1\. A feature channel refers to one of the dimensions
    in a multidimensional tensor in CNNs used to represent different aspects or features
    of the input data. For example, they can represent color channels like red, green,
    or blue. The normalization ensures that the distribution of the inputs to layers
    deep in the network remains more stable during training. This stability arises
    because the normalization process reduces the internal covariate shift, which
    is the change in the distribution of network activations due to the update of
    weights in lower layers. It also helps to address the vanishing or exploding gradient
    problems by keeping the inputs in an appropriate range to prevent gradients from
    becoming too small (vanishing) or too large (exploding).^([1](#footnote-000))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the 2D batch normalization works: for each feature channel, we first
    calculate the mean and variance of all observations within the channel. We then
    normalize the values for each feature channel using the mean and variance obtained
    earlier (by subtracting the mean from each observation and then dividing the difference
    by the standard deviation). This ensures that the values in each channel have
    a mean of 0 and a standard deviation of 1 after normalization, which helps stabilize
    and speed up training. It also helps maintain stable gradients during backpropagation,
    which further aids in training deep neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a concrete example to show how the 2D batch normalization works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that you have a three-channel input with a size of 64 × 64\. You pass
    the input through a 2D convolutional layer with three output channels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ① Fixes the random state so results are reproducible
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a 3-channel input
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a 2D convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: ④ Passes the input through the convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: The output from the preceding code cell is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We have created a three-channel input and passed it through a 2D convolutional
    layer with three output channels. The processed input has three channels with
    a size of 64 × 64 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the mean and standard deviation of the pixels in each of the
    three output channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The average values of the pixels in each output channel are not 0; the standard
    deviations of pixels in each output channel are not 1\. Now, we perform a 2D batch
    normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The average values of pixels in each output channel are now practically 0 (or
    a very small number that is close to 0); the standard deviations of pixels in
    each output channel are now a number close to 1\. That’s what batch normalization
    does: it normalizes observations in each feature channel so that values in each
    feature channel have 0 mean and unit standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Color images of anime faces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this second project, you’ll learn how to create high-resolution color images.
    The training steps in this project are similar to the first project, with the
    exception that the training data are color images of anime faces. Further, the
    discriminator and generator neural networks are more sophisticated. We’ll use
    2D convolutional and 2D transposed convolutional layers in the two networks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Downloading anime faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can download the training data from Kaggle [https://mng.bz/1a9R](https://mng.bz/1a9R),
    which contains 63,632 color images of anime faces. You need to set up a free Kaggle
    account to log in first. Extract the data from the zip file and put them in a
    folder on your computer. For example, I placed everything in the zip file in /files/anime/
    on my computer. As a result, all anime face images are in /files/anime/images/.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the path name so you can use it later to load the images in Pytorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Change the name of the path depending on where you have saved the images on
    your computer. Note that the `ImageFolder()` class uses the directory name of
    the images to identify the class the images belong to. As a result, the final
    /images/ directory is not included in `anime_path` that we define earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use the `ImageFolder()` class in Torchvision `datasets` package to
    load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: ① Changes image size to 64 × 64
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts images to PyTorch tensors
  prefs: []
  type: TYPE_NORMAL
- en: ③ Normalizes image values to [-1, 1] in all three color channels
  prefs: []
  type: TYPE_NORMAL
- en: ④ Loads the data and transforms images
  prefs: []
  type: TYPE_NORMAL
- en: We perform three different transformations when loading up the images from the
    local folder. First, we resize all images to 64 pixels in height and 64 pixels
    in width. Second, we convert the images to PyTorch tensors with values in the
    range [0, 1] by using the `ToTensor()` class. Finally, we use the `Normalize()`
    class to deduct 0.5 from the value and divide the difference by 0.5\. As a result,
    the image data are now between –1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now put the training data in batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The training dataset is now in batches, with a batch size of 128.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Channels-first color images in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PyTorch uses a so-called channels-first approach when handling color images.
    This means the shape of images in PyTorch are (number_channels, height, width).
    In contrast, in other Python libraries such as TensorFlow or Matplotlib, a channels-last
    approach is used: a color image has a shape of (height, width, number_channels)
    instead.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example image in our dataset and print out the shape of the
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The shape of the first image is 3 × 64 × 64\. This means the image has three
    color channels (RGB). The height and width of the image are both 64 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we plot the images in Matplotlib, we need to convert them to channels-last
    by using the `permute()` method in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to multiply the PyTorch tensor representing the image by 0.5
    and then add 0.5 to it to convert the values from the range [–1, 1] to the range
    [0, 1]. You’ll see a plot of an anime face after running the preceding code cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function `plot_images()` to visualize 32 images in four rows
    and eight columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a function to visualize 32 images
  prefs: []
  type: TYPE_NORMAL
- en: ② Places them in a 4 × 8 grid
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtains a batch of images
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calls the function to visualize the images
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see a plot of 32 anime faces in a 4 × 8 grid after running the preceding
    code cell, as shown in figure 4.7\.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F07_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Examples from the anime faces training dataset
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Deep convolutional GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you’ll create a DCGAN model so that we can train it to generate
    anime face images. As usual, the GAN model consists of a discriminator network
    and a generator network. However, the networks are more sophisticated than the
    ones we have seen before: we’ll use convolutional layers, transposed convolutional
    layers, and batch normalization layers in these networks.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the discriminator network. After that, I’ll explain how the
    generator network mirrors the layers in the discriminator network to conjure up
    realistic color images. You’ll then train the model with the data you prepared
    earlier in this chapter and use the trained model to generate novel images of
    anime face images.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Building a DCGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As in previous GAN models we have seen, the discriminator is a binary classifier
    to classify samples into real or fake. However, different from the networks we
    have used so far, we’ll use convolutional layers and batch normalizations. The
    high-resolution color images in this project have too many parameters, and if
    we use dense layers only, it’s difficult to train the model effectively. The structure
    of the discriminator neural network is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 A discriminator in DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: ① Passes the image through a 2D convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: ② Applies the LeakyReLU activation on outputs of the first convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: ③ Performs 2D batch normalization on outputs of the second convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: ④ The output is a single value between 0 and 1, which can be interpreted as
    the probability that an image is real.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input to the discriminator network is a color image with three color channels.
    The first 2D convolutional layer is `Conv2d(3, 64, 4, 2, 1, bias=False)`: this
    means the input has three channels and the output has 64 channels; the kernel
    size is 4; the stride is 2; and the padding is 1\. Each of the 2D convolutional
    layers in the network takes an image and applies filters to extract spatial features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from the second 2D convolutional layer, we apply 2D batch normalization
    (which I explained in the last section) and LeakyReLU activation (which I’ll explain
    later) on the output. The LeakyReLU activation function is a modified version
    of ReLU. It allows the output to have a slope for values below zero. Specifically,
    the LeakyReLU function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F07_Liu_EQ01.png)'
  prefs: []
  type: TYPE_IMG
- en: where β is a constant between 0 and 1\. The LeakyReLU activation function is
    commonly used to address the sparse gradients problem (when most gradients become
    zero or near-zero). Training DCGANs is one such case. When the input to a neuron
    is negative, the output of ReLU is zero, and the neuron becomes inactive. LeakyReLU
    returns a small negative value, not zero, for negative inputs. This helps keep
    the neurons active and learning, maintaining a better gradient flow and leading
    to faster convergence of model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the same approach when building the generator for clothing item generation.
    We’ll mirror the layers used in the discriminator in DCGAN to create a generator,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Designing a generator in DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: ① The first layer in the generator is modeled after the last layer in the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: ② The second layer in the generator is symmetric to the second to last layer
    in the discriminator (numbers of inputs and outputs have switched positions).
  prefs: []
  type: TYPE_NORMAL
- en: ③ The last layer in the generator is symmetric to the first layer in the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Uses the Tanh() activation to squeeze values in the output layer to the range
    [–1, 1] because the images in the training set have values between –1 and 1
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in figure 4.8, to create an image, the generator uses five 2D transposed
    convolutional layers: they are symmetric to the five 2D convolutional layers in
    the discriminator. For example, the last layer, `ConvTranspose2d(64, 3, 4, 2,
    1, bias=False)`, is modeled after the first layer in the discriminator, `Conv2d(3,
    64, 4, 2, 1, bias=False)`. The numbers of *input* and *output* channels in `Conv2d`
    are reversed and used as the numbers of *output* and *input* channels in `ConvTranspose2d`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F08_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 Designing a generator network in DCGAN to create anime faces by mirroring
    the layers in the discriminator network. The right side of the diagram shows the
    discriminator network, which contains five 2D convolutional layers. To design
    a generator that can conjure up anime faces out of thin air, we mirror the layers
    in the discriminator network. Specifically, as shown on the left half of the figure,
    the generator has five 2D transposed convolutional layers, symmetric to the 2D
    convolutional layers in the discriminator. Further, in each of the top four layers,
    the numbers of `input` and `output` channels in the discriminator are reversed
    and used as the numbers of `output` and `input` channels in the generator.
  prefs: []
  type: TYPE_NORMAL
- en: The number of input channels in the first 2D transposed convolutional layer
    is 100\. This is because the generator obtains a 100-value random noise vector
    from the latent space (bottom left of figure 4.8) and feeds it to the generator.
    The number of output channels in the last 2D transposed convolutional layer in
    the generator is 3 because the output is an image with three color channels (RGB).
    We apply the Tanh activation function to the output of the generator to squeeze
    all values to the range [–1, 1] because the training images all have values between
    –1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, the loss function is binary cross-entropy loss. The discriminator
    is trying to maximize the accuracy of the binary classification: identify a real
    sample as real and a fake sample as fake. The generator, on the other hand, is
    trying to minimize the probability that the fake sample is being identified as
    fake.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the Adam optimizer for both the discriminator and the generator and
    set the learning rate to 0.0002:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You have seen the Adam optimizer in chapter 2 but with default values of betas.
    Here, we select betas that are different from the default values. The betas in
    the Adam optimizer play crucial roles in stabilizing and speeding up the convergence
    of the training process. They do this by controlling how much emphasis is placed
    on recent versus past gradient information (beta1) and by adapting the learning
    rate based on the certainty of the gradient information (beta2). These parameters
    are typically fine-tuned based on the specific characteristics of the problem
    being solved.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Training and using DCGAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process for DCGAN is similar to what we have done for other GAN
    models, such as those used in chapter 3 and earlier in this chapter. Since we
    don’t know the true distribution of anime face images, we’ll rely on visualization
    techniques to determine when the training is complete. Specifically, we define
    a `test_epoch()` function to visualize the anime faces created by the generator
    after each epoch of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: ① Obtains 32 random noise vectors from the latent space
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates 32 anime face images
  prefs: []
  type: TYPE_NORMAL
- en: ③ Plots the generated images in a 4 × 8 grid
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calls the function to generate images before training the model
  prefs: []
  type: TYPE_NORMAL
- en: If you run the preceding code cell, you’ll see 32 images that look like snowflake
    statics on a TV screen. They don’t look like anime faces at all because we haven’t
    trained the generator yet.
  prefs: []
  type: TYPE_NORMAL
- en: We define three functions, `train_D_on_real()`, `train_D_on_fake()`, and `train_G()`,
    similar to those we used to train the GANs to generate grayscale images of clothing
    items earlier in this chapter. Go to the Jupypter Notebook for this chapter in
    the book’s GitHub repository and familiarize yourself with the functions. They
    train the discriminator with real images. They then train the discriminator with
    fake images; finally, they train the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train the model for 20 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The training takes about 20 minutes if you are using GPU training. Otherwise,
    it may take 2 to 3 hours, depending on the hardware configuration on your computer.
    Alternatively, you can download the trained model from my website: [https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip](https://gattonweb.uky.edu/faculty/lium/gai/anime_gen.zip).'
  prefs: []
  type: TYPE_NORMAL
- en: After every epoch of training, you can visualize the generated anime faces.
    After just one epoch of training, the model can already generate color images
    that look like anime faces, as shown in figure 4.9\. As training progresses, the
    quality of the generated images becomes better and better.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F09_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 Generated images in DCGAN after one epoch of training
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll discard the discriminator and save the trained generator in the local
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the trained generator, we load up the model and use it to generate 32
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The generated anime faces are shown in figure 4.10\. The generated images bear
    a close resemblance to the ones in the training set shown in figure 4.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F10_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 Generated anime face images by the trained generator in DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed that the hair colors of the generated images are different:
    some are black, some are red, and some are blond. You may wonder: Can we tell
    the generator to create images with a certain characteristic, such as black hair
    or red hair? The answer is yes. You’ll learn a couple of different methods to
    select characteristics in generated images in GANs in chapter 5\.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To conjure up realistic-looking images out of thin air, the generator mirrors
    layers used in the discriminator network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While it’s feasible to generate grayscale images by using just fully connected
    layers, to generate high-resolution color images, we need to use CNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-dimensional convolutional layers are used for feature extraction. They apply
    a set of learnable filters (also known as kernels) to the input data to detect
    patterns and features at different spatial scales. These layers are essential
    for capturing hierarchical representations of the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-dimensional transposed convolutional layers (also known as deconvolution
    or upsampling layers) are used for upsampling or generating high-resolution feature
    maps. They apply a filter to the input data. However, unlike standard convolution,
    they increase the spatial dimensions by inserting gaps between the output values,
    which effectively “upscales” the feature maps. This process generates feature
    maps of a higher resolution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-dimensional batch normalization is a technique commonly used in deep learning
    and neural networks to improve the training and performance of CNNs and other
    models that work with 2D data, such as images. It normalizes the values for each
    feature channel, so they have a mean of 0 and a standard deviation of 1, which
    helps stabilize and speed up training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](#footnote-000-backlink))  Sergey Ioffe, Christian Szegedy, 2015, “Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift.” [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  prefs: []
  type: TYPE_NORMAL
