<html><head></head><body><section data-pdf-bookmark="Chapter 2. Introduction to Computer Vision" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch02_introduction_to_computer_vision_1748548889076080">
      <h1><span class="label">Chapter 2. </span>Introduction to Computer Vision</h1>
      <p><a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a> introduced the basics of how machine learning works. You saw how to get started with programming using neural networks to match data to labels, and from there, you saw how to infer the rules that can be used to distinguish items. </p>
      <p>In this chapter, we’ll consider the next logical step, which is to apply these concepts to computer vision. In this process, a model learns how to recognize content in pictures so it can “see” what’s in them. You’ll work with a popular dataset of clothing items and build a model that can differentiate between them and thus “see” the difference between different types of clothing.</p>
      <section data-pdf-bookmark="How Computer Vision Works" data-type="sect1"><div class="sect1" id="ch02_how_computer_vision_works_1748548889076295">
        <h1>How Computer Vision Works</h1>
        <p><em>Computer vision</em> is the ability<a contenteditable="false" data-primary="computer vision" data-secondary="about" data-type="indexterm" id="id626"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="challenges of computer vision" data-type="indexterm" id="id627"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="challenges of computer vision" data-type="indexterm" id="id628"/> of a computer to recognize items beyond just storing their pixels. For example, consider items of clothing that might look like those in <a data-type="xref" href="#ch02_figure_1_1748548889066336">Figure 2-1</a>. They’re very complex, with lots of different varieties of the same item. Take a look at the two shoes—they’re very different, but they’re still shoes! </p>
        <figure><div class="figure" id="ch02_figure_1_1748548889066336">
          <img src="assets/aiml_0201.png"/>
          <h6><span class="label">Figure 2-1. </span>Clothing examples</h6>
        </div></figure>
        <p>There are a number of different recognizable clothing items here. You understand the difference between a shirt, a coat, and a dress, and you fundamentally know what each of these items are—but how would you explain all that to somebody who has never seen clothing? How about a shoe? There are two shoes in this image, but given the major differences between them, how would you explain to someone what makes them both shoes? This is another area where the rules-based programming we spoke about in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a> can fall apart. Sometimes, it’s just unfeasible to describe something with rules. </p>
        <p>Of course, computer vision is no exception to this issue. But consider how you learned to recognize all these items—by seeing lots of different examples and gaining experience with how they’re used. Can a computer learn the same way? The answer is yes, but with limitations. Throughout the rest of this chapter, we’ll take a look at an example of how to teach a computer to recognize items of clothing using a well-known dataset called Fashion MNIST.</p>
      </div></section>
      <section data-pdf-bookmark="The Fashion MNIST Database" data-type="sect1"><div class="sect1" id="ch02_the_fashion_mnist_database_1748548889076368">
        <h1>The Fashion MNIST Database</h1>
        <p>One of the foundational datasets<a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-type="indexterm" id="ch2mnist2"/><a contenteditable="false" data-primary="Fashion MNIST database" data-type="indexterm" id="ch2mnist4"/><a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST database" data-type="indexterm" id="ch2mnist5"/><a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="Fashion MNIST database" data-type="indexterm" id="ch2mnist6"/> for learning and benchmarking algorithms is the Modified National Institute of Standards and Technology (MNIST) database, which was created by Yann LeCun, Corinna Cortes, and Christopher Burges. This dataset consists of images of 70,000 handwritten digits from 0 to 9, and the images are 28 × 28 grayscale. </p>
        <p class="pagebreak-before"><a href="https://oreil.ly/f-mnist">Fashion MNIST</a> is designed to be a drop-in replacement for MNIST that has the same number of records, the same image dimensions, and the same number of classes. Rather than images of the digits 0 through 9, Fashion MNIST contains images of 10 different types of clothing.</p>
        <p>You can see an example<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="example of dataset contents" data-type="indexterm" id="id629"/><a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST database" data-tertiary="example of dataset contents" data-type="indexterm" id="id630"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="example of dataset contents" data-type="indexterm" id="id631"/> of the dataset contents in <a data-type="xref" href="#ch02_figure_2_1748548889066375">Figure 2-2</a>, in which three lines are dedicated to each clothing item type.</p>
        <figure><div class="figure" id="ch02_figure_2_1748548889066375">
          <img src="assets/aiml_0202.png"/>
          <h6><span class="label">Figure 2-2. </span>Exploring the Fashion MNIST dataset</h6>
        </div></figure>
        <p>Fashion MNIST has a nice variety of clothing, including shirts, trousers, dresses, and lots of types of shoes! <a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="pixel values in 0 to 255 range" data-type="indexterm" id="id632"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="pixel values in 0 to 255 range" data-type="indexterm" id="id633"/>Also, as you may notice, it’s monochrome, so each picture consists of a certain number of pixels with values between 0 and 255. This makes the dataset simpler to manage. </p>
        <p>You can see a close-up of a particular image from the dataset in <a data-type="xref" href="#ch02_figure_3_1748548889066397">Figure 2-3</a>.</p>
        <figure><div class="figure" id="ch02_figure_3_1748548889066397">
          <img src="assets/aiml_0203.png"/>
          <h6><span class="label">Figure 2-3. </span>Close-up of an image in the Fashion MNIST dataset</h6>
        </div></figure>
        <p>Like any image, this one is a rectangular grid of pixels. In this case, the grid size is 28 × 28, and <a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="pixel values in 0 to 255 range" data-type="indexterm" id="id634"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="pixel values in 0 to 255 range" data-type="indexterm" id="id635"/>each pixel is a value between 0 and 255, so it is represented by a square in grayscale. To make it easier to see, I have expanded it so that it looks pixelated.<a contenteditable="false" data-primary="" data-startref="ch2mnist2" data-type="indexterm" id="id636"/><a contenteditable="false" data-primary="" data-startref="ch2mnist4" data-type="indexterm" id="id637"/><a contenteditable="false" data-primary="" data-startref="ch2mnist5" data-type="indexterm" id="id638"/><a contenteditable="false" data-primary="" data-startref="ch2mnist6" data-type="indexterm" id="id639"/></p> 
 <p>Let’s now take a look at how you can use these pixel values with the functions we saw previously.</p>
      </div></section>
      <section data-pdf-bookmark="Neurons for Vision" data-type="sect1"><div class="sect1" id="ch02_neurons_for_vision_1748548889076419">
        <h1>Neurons for Vision</h1>
        <p>In <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>, you saw a<a contenteditable="false" data-primary="neurons" data-secondary="computer vision" data-type="indexterm" id="ch2neuco"/><a contenteditable="false" data-primary="computer vision" data-secondary="neurons for vision" data-type="indexterm" id="ch2neuco2"/> very simple scenario in which a machine was given a set of <em>x</em> and <em>y</em> values and it learned that the relationship between them was <em>y</em> = 2<em>x</em> – 1. This was done using a very simple neural network with one layer and one neuron. If you were to draw that visually, it might look like <a data-type="xref" href="#ch02_figure_4_1748548889066415">Figure 2-4</a>.</p>
        <figure><div class="figure" id="ch02_figure_4_1748548889066415">
          <img alt="" src="assets/aiml_0204.png"/>
          <h6><span class="label">Figure 2-4. </span>A single neuron learning a linear relationship</h6>
        </div></figure>
        <p>Each of our images is a set of 784 values (28 × 28) <a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="pixel values in 0 to 255 range" data-type="indexterm" id="id640"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="pixel values in 0 to 255 range" data-type="indexterm" id="id641"/>between 0 and 255. They can be our <em>x</em>. We also know that we have 10 different types of images in our dataset, so let’s consider them to be our <em>y</em>. Now, we want to learn what the function looks like in which <em>y</em> is a function of <em>x</em>. </p>
        <p>Given that we have 784 <em>x</em> values per image and our <em>y</em> is going to be between 0 and 9, a simple equation like <em>y</em> = <em>mx</em> + <em>c</em> isn’t going to be enough to solve the problem. That’s because there’s a large variety of possible values and the equation can only plot values on a line. </p>
        <p>But what we <em>can</em> do is have<a contenteditable="false" data-primary="neurons" data-secondary="computer vision" data-tertiary="parameters" data-type="indexterm" id="id642"/><a contenteditable="false" data-primary="computer vision" data-secondary="neurons for vision" data-tertiary="parameters" data-type="indexterm" id="id643"/><a contenteditable="false" data-primary="parameters" data-secondary="neurons for computer vision" data-type="indexterm" id="id644"/> several neurons working together. Each neuron will learn <em>parameters</em>, and when we have a combined function of all of these parameters working together, we can see whether we can match that pattern to our desired answer (see <a data-type="xref" href="#ch02_figure_5_1748548889066432">Figure 2-5</a>). </p>
        <figure><div class="figure" id="ch02_figure_5_1748548889066432">
          <img src="assets/aiml_0205.png"/>
          <h6><span class="label">Figure 2-5. </span>Extending our pattern for a more complex example</h6>
        </div></figure>
        <p>The gray boxes at the top of this diagram can be considered the pixels in the image, which are our <em>X</em> values. When we train the neural network, we load the pixels into a layer of neurons—<a data-type="xref" href="#ch02_figure_5_1748548889066432">Figure 2-5</a> shows them being loaded into the first neuron, but the values are loaded into just each of them. Also, consider each neuron’s weight and bias (<em>w</em> and <em>b</em>) to be randomly initialized. Then, when we sum up the values of the output of each neuron, we’re going to get a value. We’ll do this for <em>every</em> neuron in the output layer, so neuron 0 will contain the value of the probability that the pixels will add up to label 0, neuron 1 will contain the value of the probability that the pixels will add up to label 1, etc.</p>
        <p>Over time, we want to match that value to the desired output―which, for this image, is the number 9, which is also the label for the ankle boot that was shown in <a data-type="xref" href="#ch02_figure_3_1748548889066397">Figure 2-3</a>. So, in other words, this neuron should have the largest value of all of the output neurons.</p>
        <p>Given that there are 10 labels, a random initialization should get the right answer about 10% of the time. <a contenteditable="false" data-primary="loss functions" data-secondary="computer vision" data-type="indexterm" id="id645"/>From that, the loss function and optimizer can do their job epoch by epoch to tweak the internal parameters of each neuron to improve on that 10%. <a contenteditable="false" data-primary="computer vision" data-secondary="neurons for vision" data-tertiary="learning to “see”" data-type="indexterm" id="id646"/><a contenteditable="false" data-primary="neurons" data-secondary="computer vision" data-tertiary="learning to “see”" data-type="indexterm" id="id647"/>And thus, over time, the computer will learn to “see” what makes a shoe a shoe or a dress a dress. You’ll see this process of improvement when you run the code and your neural network effectively learns to distinguish the different items.<a contenteditable="false" data-primary="" data-startref="ch2neuco" data-type="indexterm" id="id648"/><a contenteditable="false" data-primary="" data-startref="ch2neuco2" data-type="indexterm" id="id649"/></p>
      </div></section>
      <section data-pdf-bookmark="Designing the Neural Network" data-type="sect1"><div class="sect1" id="ch02_designing_the_neural_network_1748548889076469">
        <h1>Designing the Neural Network</h1>
        <p>Let’s take the example we just<a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="designing the neural network" data-type="indexterm" id="ch2dsgn"/><a contenteditable="false" data-primary="neural networks" data-secondary="Fashion MNIST database model" data-tertiary="designing the neural network" data-type="indexterm" id="ch2dsgn2"/><a contenteditable="false" data-primary="layers" data-secondary="Sequential defining" data-tertiary="multiple layers" data-type="indexterm" id="id650"/><a contenteditable="false" data-primary="Sequential" data-secondary="multiple layers defined" data-type="indexterm" id="id651"/> walked through and explore what it looks like in code. First, we’ll look at the design of the neural network that was shown in <a data-type="xref" href="#ch02_figure_5_1748548889066432">Figure 2-5</a>:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>
    <code class="n">nn</code><code class="o">.</code><code class="n">LogSoftmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="p">)</code></pre>
        <p>If you remember, in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a> we had a <code>Sequential</code> model to specify that we had many layers. In that case, we had only one layer, but now we’re using it to define multiple layers.</p>
        <p>The first layer, a <code>Linear</code>, is a<a contenteditable="false" data-primary="Sequential" data-secondary="Linear layers defined" data-type="indexterm" id="ch2lnr"/><a contenteditable="false" data-primary="linear layers" data-secondary="multiple layers" data-type="indexterm" id="ch2lnr2"/><a contenteditable="false" data-primary="layers" data-secondary="linear layers" data-tertiary="multiple layers" data-type="indexterm" id="ch2lnr3"/> layer of neurons that learn a linear relationship between their inputs and their outputs. As before, when using a <code>Linear</code>, you give two parameters: the input shape and the output shape. Conveniently, the output shape is effectively the number of neurons you want in this layer, and we’re specifying that we want 128 of them. The <em>input</em> shape is defined as (28 × 28), which is the size of the data coming into the network, and as you saw earlier, this is the dimension of a Fashion MNIST image.</p>
        <p>The input is shown as the middle<a contenteditable="false" data-primary="layers" data-secondary="hidden layers" data-type="indexterm" id="id652"/><a contenteditable="false" data-primary="hidden layers" data-type="indexterm" id="id653"/> layer in <a data-type="xref" href="#ch02_figure_5_1748548889066432">Figure 2-5</a>, and you’ll often hear such layers described as <em>hidden layers</em>. The term <em>hidden</em> just means that there’s no direct interface to that layer. This takes a little bit of getting used to—the middle layer is the first layer that you <em>define</em>, and in a diagram like <a data-type="xref" href="#ch02_figure_5_1748548889066432">Figure 2-5</a>, you can see that it’s in the middle of the diagram. This is because we also drew the data “coming in” to this layer. <a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="flattening into 1-D array" data-type="indexterm" id="id654"/><a contenteditable="false" data-primary="layers" data-secondary="rectangular data flattened to 1-D array" data-type="indexterm" id="id655"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="flattening into 1-D array" data-type="indexterm" id="id656"/><a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST database" data-tertiary="flattening into 1-D array" data-type="indexterm" id="id657"/><a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="flattening data into 1-D array" data-type="indexterm" id="id658"/>One other thing to note is that image data from datasets like Fashion MNIST is usually rectangular in shape, but a layer doesn’t recognize that, so it will need to be “flattened” into a 1-D array, as shown across the top of <a data-type="xref" href="#ch02_figure_5_1748548889066432">Figure 2-5</a>. You’ll see the code for that in a moment.</p>
        <p>With this first <code>Linear</code>, we’re asking for 128 neurons to have their internal parameters randomly initialized. Often, the question I’ll get asked at this point is “Why 128?” This is entirely arbitrary—there’s no fixed rule for the number of neurons to use. As you design the layers, you want to pick the appropriate number of values to enable your model to actually learn. <a contenteditable="false" data-primary="neurons" data-secondary="speed versus accuracy of learning" data-type="indexterm" id="id659"/><a contenteditable="false" data-primary="accuracy" data-secondary="speed versus accuracy of learning" data-type="indexterm" id="id660"/><a contenteditable="false" data-primary="overfitting" data-type="indexterm" id="id661"/><a contenteditable="false" data-primary="training" data-secondary="overfitting" data-type="indexterm" id="id662"/>More neurons means it will run more slowly, as it has to learn more parameters. More neurons could also lead to a network that is great at recognizing the training data but not so good at recognizing data that it hasn’t previously seen. (This is known as <em>overfitting</em>, and we’ll discuss it later in this chapter). On the other hand, fewer neurons means that the model might not have sufficient parameters to learn. </p>
        <p>You will need to explore this trade-off between speed of learning and accuracy of learning and do some experimentation over time to pick the right values. <a contenteditable="false" data-primary="hyperparameters" data-secondary="hyperparameter tuning" data-type="indexterm" id="id663"/><a contenteditable="false" data-primary="hyperparameters" data-type="indexterm" id="id664"/><a contenteditable="false" data-primary="parameters" data-secondary="hyperparameters versus" data-type="indexterm" id="id665"/><a contenteditable="false" data-primary="hyperparameters" data-secondary="parameters versus" data-type="indexterm" id="id666"/>This process is typically called <em>hyperparameter tuning</em>. In ML, a <em>hyperparameter</em> is a value that is used to control the training, as opposed to the internal values of the neurons that get trained/learned, which are referred to as <em>parameters</em>.</p>
        <p>When you’re defining a neural network with PyTorch and using the <code>Sequential</code>, you don’t just define the layers of the network and what types of neurons they may use. You can also define functions that execute on the data while it flows between the neural network layers. <a contenteditable="false" data-primary="activation functions" data-type="indexterm" id="id667"/><a contenteditable="false" data-primary="activation functions" data-secondary="ReLU" data-type="indexterm" id="id668"/><a contenteditable="false" data-primary="ReLU (rectified linear unit) activation function" data-type="indexterm" id="id669"/>These are typically called <em>activation functions</em>, and an activation function is the next thing you see specified in the code as <code>nn.ReLU()</code>. An activation function is code that will execute on each neuron in the layer. PyTorch supports a number of activation functions out of the box, and a very common one in middle layers is <code>ReLU</code>, which stands for <em>rectified linear unit</em>. It’s a simple function that returns a value only if it’s greater than 0. In this case, we don’t want negative values being passed to the next layer to potentially impact the summing function, so instead of writing a lot of <code>if-then</code> code, we can simply activate the layer with <code>ReLU</code>.</p>
        <p>Finally, there’s another <code>Linear</code> layer, which will be the <em>output layer</em>. If you look at the defined shape of (128, 10) and think of it through that “input size, output size” framework, you’ll see that it has 128 “inputs” (i.e., the number of neurons in the layer above) and 10 “outputs.” What are these 10? Recall that Fashion MNIST has 10 classes of clothing. Each of these neurons is effectively assigned one class, and it will end up with a probability that the input pixels match that class, so our job is to determine which one has the highest value. You might wonder how these assignments happen: where is the code that says one neuron is for a shoe and another is for a shirt? To answer that question, recall the <em>y</em> = 2<em>x</em> ‒ 1 example in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>, where we had a set of input data and a set of known, correct answers that is sometimes called the <em>ground truth</em>. Fashion MNIST will work in the same way. When training the network, we provide the input images <em>and</em> their known answers as a set of what we want the output neurons to look like. Thus, the network will “learn” that when it sees a shoe, the output neurons that don’t represent that shoe should have a zero value and the ones that do should have a “1” value.</p>
        <p>We <em>could</em> also loop through the output neurons to find the highest value, but the <code>LogSoftmax</code> activation function does that for us. </p>
        <p>So now, when we train our neural network, we have two goals. We want to be able to feed in a 28 × 28–pixel array, and we want the neurons in the middle layer to have weights and biases (<em>w</em> and <em>B</em> values) that, when combined, will match those pixels to one of the 10 output values.<a contenteditable="false" data-primary="" data-startref="ch2dsgn" data-type="indexterm" id="id670"/><a contenteditable="false" data-primary="" data-startref="ch2dsgn2" data-type="indexterm" id="id671"/><a contenteditable="false" data-primary="" data-startref="ch2lnr" data-type="indexterm" id="id672"/><a contenteditable="false" data-primary="" data-startref="ch2lnr2" data-type="indexterm" id="id673"/><a contenteditable="false" data-primary="" data-startref="ch2lnr3" data-type="indexterm" id="id674"/></p>
      </div></section>
      <section data-pdf-bookmark="The Complete Code" data-type="sect1"><div class="sect1" id="ch02_the_complete_code_1748548889076520">
        <h1>The Complete Code</h1>
        <p>Now that we’ve explored the architecture of the neural network, let’s look at the complete code for training a model with the Fashion MNIST data.<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="complete code for training a model" data-type="indexterm" id="ch2coco"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="complete code for training a model" data-type="indexterm" id="ch2coco2"/><a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="complete code for training a model" data-type="indexterm" id="ch2coco3"/></p>
        <p>Here’s the complete code:<a contenteditable="false" data-primary="DataLoader" data-secondary="Fashion MNIST data" data-type="indexterm" id="id675"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.optim</code> <code class="k">as</code> <code class="nn">optim</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">datasets</code><code class="p">,</code> <code class="n">transforms</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code>
 
<code class="c1"># Load the dataset</code>
<code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code><code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">()])</code>
 
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> 
                             <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
<code class="n">test_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> 
                             <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
 
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> 
                          <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">test_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">test_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> 
                          <code class="n">shuffle</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
 
<code class="c1"># Define the model</code>
<code class="k">class</code> <code class="nc">FashionMNISTModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">FashionMNISTModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">LogSoftmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">logits</code>
 
<code class="n">model</code> <code class="o">=</code> <code class="n">FashionMNISTModel</code><code class="p">()</code>
 
<code class="c1"># Define the loss function and optimizer</code>
<code class="n">loss_function</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">NLLLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>
 
<code class="c1"># Train the model</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="n">dataloader</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">):</code>
    <code class="n">size</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataloader</code><code class="o">.</code><code class="n">dataset</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">batch</code><code class="p">,</code> <code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">dataloader</code><code class="p">):</code>
        <code class="c1"># Compute prediction and loss</code>
        <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
 
        <code class="c1"># Backpropagation</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
 
        <code class="k">if</code> <code class="n">batch</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">loss</code><code class="p">,</code> <code class="n">current</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="n">batch</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"loss: </code><code class="si">{</code><code class="n">loss</code><code class="si">:</code><code class="s2">&gt;7f</code><code class="si">}</code>  <code class="w"/>
                    <code class="p">[{</code><code class="n">current</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}</code><code class="o">/</code><code class="p">{</code><code class="n">size</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}]</code><code class="s2">")</code><code class="w"/>
 
<code class="c1"># Training process</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">5</code>
<code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch </code><code class="si">{</code><code class="n">t</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="se">\n</code><code class="s2">-------------------------------"</code><code class="p">)</code>
    <code class="n">train</code><code class="p">(</code><code class="n">train_loader</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_function</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Done!"</code><code class="p">)</code></pre>
        <p>Let’s walk through this piece by piece.<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="complete code for training a model" data-tertiary="Fashion MNIST database loaded" data-type="indexterm" id="id676"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="complete code for training a model" data-tertiary="Fashion MNIST database loaded" data-type="indexterm" id="id677"/> First, let’s consider where the data comes from. In the torchvision  library, there’s a datasets collection, and we can load Fashion MNIST from that, addressed as follows:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code></pre>
        <p>So, in our first block of code, you’ll see these:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">train_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> 
                             <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
<code class="n">test_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> 
                             <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code></pre>
        <p>Now, you might wonder why<a contenteditable="false" data-primary="datasets" data-secondary="training and testing" data-type="indexterm" id="id678"/><a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="two datasets" data-type="indexterm" id="id679"/> we’re using <em>two</em> datasets. It’s simple: one is for training, and one is for testing. The idea here is also simple: if you train a neural network on a set of data, it can become an expert on <em>that</em> set of data, but it may not be effective at understanding or classifying <em>other</em> data that it previously has not seen. In the case of Fashion MNIST, it might become excellent at understanding the difference between a subset of shoes and shirts, but it will do poorly when new data is presented to it. So, it’s good practice to always hold back a little of your data and <em>not</em> train the neural <span class="keep-together">network</span> with it. In this case, Fashion MNIST has 70,000 items of data, but only 60,000 of them are used to train the network and the other 10,000 are used to test it. If you look at the preceding code carefully, you’ll see that the difference between the two lines is the <code>train=</code> parameter. For the first one, the training set the parameter is set to True. For the other, it’s set to False.</p>
        <p>You’ll also see the <code>transform</code> parameter in the datasets. It specifies a transformation to apply to the data, which was defined like this:<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="transform parameter" data-type="indexterm" id="id680"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="transform parameter" data-type="indexterm" id="id681"/><a contenteditable="false" data-primary="datasets" data-secondary="transform parameter" data-type="indexterm" id="id682"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code><code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">()])</code></pre>
        <p>Neural networks typically<a contenteditable="false" data-primary="neural networks" data-secondary="normalized values" data-type="indexterm" id="id683"/><a contenteditable="false" data-primary="normalized values for neural networks" data-type="indexterm" id="id684"/><a contenteditable="false" data-primary="neural networks" data-secondary="normalized values" data-tertiary="transforming 0-255 values" data-type="indexterm" id="id685"/><a contenteditable="false" data-primary="normalized values for neural networks" data-secondary="transforming 0-255 values" data-type="indexterm" id="id686"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="pixel values in 0 to 255 range" data-tertiary="normalizing" data-type="indexterm" id="id687"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="pixel values in 0 to 255 range" data-tertiary="normalizing" data-type="indexterm" id="id688"/> work with <em>normalized</em> values (i.e., those between 0 and 1). However, the pixels in our image are in the range of 0–255, and the values indicate their color depth, with 0 being black, 255 being white, and everything in between being shades of gray. To prepare the data for the neural network, we should map these shades to values between 0 and 1. The preceding code will automatically do that for you in PyTorch, so applying this <code>transform</code> parameter as you’re loading the code will then map the pixel values from the [0, 255] integer range to a [0, 1] floating-point range and load them into an array that’s suitable for the neural network (aka a <span class="keep-together">Tensor).</span> </p>
        <p>Our job will be to fit the training images to the training labels in a manner that’s similar to how we fit <em>y</em> to <em>x </em>in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>. </p>
        <p>The math for <a href="https://oreil.ly/6d_Po">why normalized data is better for training neural networks</a> is beyond<a contenteditable="false" data-primary="normalized values for neural networks" data-secondary="information online" data-type="indexterm" id="id689"/><a contenteditable="false" data-primary="online resources" data-secondary="normalized values for neural networks information" data-type="indexterm" id="id690"/> the scope of this book, but bear in mind that when you’re training a neural network in PyTorch, normalization will improve performance. Often, your network will not learn and will have massive errors when dealing with nonnormalized data. You’ll recall that the <em>y</em> = 2<em>x</em> – 1 example from <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a> didn’t require the data to be normalized because it was very simple, but for fun, try training it with different values of <em>x</em> and <em>y</em> where <em>x</em> is much larger—and you’ll see it quickly fail!</p>
        <p>Next, we define the neural network that makes up our model,<a contenteditable="false" data-primary="neural networks" data-secondary="Fashion MNIST database model" data-tertiary="designing the neural network" data-type="indexterm" id="id691"/> as discussed earlier, but we’ll flesh it out with a bit more detail—including the flattening layers and how we want the “forward” pass to work in the model.</p>
        <p>Here’s the code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define the model</code>
<code class="k">class</code> <code class="nc">FashionMNISTModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">FashionMNISTModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Flatten</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">28</code><code class="o">*</code><code class="mi">28</code><code class="p">,</code> <code class="mi">128</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>
            <code class="n">nn</code><code class="o">.</code><code class="n">LogSoftmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">flatten</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">linear_relu_stack</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">logits</code>
 
<code class="n">model</code> <code class="o">=</code> <code class="n">FashionMNISTModel</code><code class="p">()</code></pre>
        <p>Some key things to note here are that the <code>FashionMNISTModel</code> class subclasses <span class="keep-together"><code>nn.Module</code></span>, which gives you the ability to override its <code>forward</code> method. We use this method when data is passing forward through the network. <a contenteditable="false" data-primary="loss functions" data-secondary="loss.backward function" data-type="indexterm" id="id692"/><a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="id693"/>Remember back in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a> when we saw the <code>loss.backward()</code> call that did backpropagation and changed the parameters of the network? You’ll frequently encounter that same pattern when training models with PyTorch. You’ll define functions to execute as the data moves <em>forward</em> through the network, and then you’ll define others to execute as the gradients that we calculate from the loss move <em>backward</em> through the network. </p>
        <p>So, if we look at the <code>init</code> for the class, we define two methods: <code>flatten</code>, which  <span class="keep-together"> is set to <code>nn.FLatten()</code></span> (a built-in function to flatten the 2D image to 1D), and <code>lin⁠ear​_relu_stack</code>, which is set to the sequence of layers and operations (often abbreviated to <em>ops</em>) that define the behavior of the network.</p>
        <p>In <code>forward</code>, we then simply define how these work. First, we flatten our data, <code>x</code>, by calling <code>self.flatten</code>, and then the results will be passed into <code>linear_relu_stack</code> to get the results. <a contenteditable="false" data-primary="logits" data-type="indexterm" id="id694"/><a contenteditable="false" data-primary="Softmax layer" data-secondary="logits" data-type="indexterm" id="id695"/><a contenteditable="false" data-primary="layers" data-secondary="Softmax layer" data-tertiary="logits" data-type="indexterm" id="id696"/>The results are called <em>logits</em>, which are log probabilities (as defined by <code>LogSoftmax</code>) that indicate the confidence the model has that each class is the correct classification.</p>
        <p>To learn from our data,<a contenteditable="false" data-primary="optimizers" data-secondary="Adam" data-type="indexterm" id="id697"/><a contenteditable="false" data-primary="loss functions" data-secondary="computer vision" data-type="indexterm" id="id698"/><a contenteditable="false" data-primary="Adam optimizer" data-type="indexterm" id="id699"/> we need a loss function to calculate how good or bad our current “guess” is, and we also need an optimizer to figure out the next set of parameters for an improved guess.</p>
        <p>Here’s an example of how to define both:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define the loss function and optimizer</code>
<code class="n">loss_function</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">NLLLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code></pre>
        <p>First, let’s look at the loss function. It’s defined as <code>nn.NLLLoss(),</code> which stands for “Negative Log Likelihood Loss.” Don’t worry—nobody expects you to understand what that means at this point! Ultimately, as you work through learning how to do ML, you’ll learn about different loss functions, and you’ll experiment with which ones work well in particular scenarios. In this case, given that the output logits are log probabilities, I chose this loss function because it works particularly well for this scenario. As mentioned, over time, you’ll learn a lot more about the library of loss <span class="keep-together">functions,</span> and you can choose the best ones for your scenario. But for now, just go with the flow and use this one! </p>
        <p>For the optimizer, I’ve opted<a contenteditable="false" data-primary="optimizers" data-secondary="Adam" data-type="indexterm" id="id700"/><a contenteditable="false" data-primary="Adam optimizer" data-type="indexterm" id="id701"/> to use the <code>Adam</code> optimization algorithm. It’s similar to the stochastic gradient descent that we used for the <em>y</em> = 2<em>x</em> – 1 model in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>, but it’s generally faster and more accurate. As with the loss function, you’ll learn more about optimization algorithms over time, and you’ll be able to choose from the menu of optimizers that fit your scenario best. One important thing here is to note that I’ve passed in <code>model.parameters()</code> as a parameter to this. This parameter passes all the trainable parameters in the model to the optimizer so that it can adjust them to help minimize the loss calculated by the loss function.</p>
        <p>Now, let’s get down to the specifics and explore what the code we use for training the network looks like:<a contenteditable="false" data-primary="neural networks" data-secondary="Fashion MNIST database model" data-tertiary="code for loss" data-type="indexterm" id="id702"/><a contenteditable="false" data-primary="training" data-secondary="Fashion MNIST database model" data-tertiary="code for loss" data-type="indexterm" id="id703"/><a contenteditable="false" data-primary="learning process" data-secondary="Fashion MNIST database model" data-tertiary="code for loss" data-type="indexterm" id="id704"/><a contenteditable="false" data-primary="training" data-secondary="Fashion MNIST database model" data-tertiary="code for loss" data-type="indexterm" id="id705"/><a contenteditable="false" data-primary="loss functions" data-secondary="computer vision" data-tertiary="code for loss" data-type="indexterm" id="id706"/><a contenteditable="false" data-primary="prediction" data-secondary="Fashion MNIST model" data-type="indexterm" id="id707"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Train the model</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="n">dataloader</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">):</code>
    <code class="n">size</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataloader</code><code class="o">.</code><code class="n">dataset</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">batch</code><code class="p">,</code> <code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">dataloader</code><code class="p">):</code>
        <code class="c1"># Compute prediction and loss</code>
        <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
 
        <code class="c1"># Backpropagation</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
 
        <code class="k">if</code> <code class="n">batch</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">loss</code><code class="p">,</code> <code class="n">current</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="n">batch</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"loss: </code><code class="si">{</code><code class="n">loss</code><code class="si">:</code><code class="s2">&gt;7f</code><code class="si">}</code><code class="s2">  [</code><code class="si">{</code><code class="n">current</code><code class="si">:</code><code class="s2">&gt;5d</code><code class="si">}</code><code class="s2">/</code><code class="si">{</code><code class="n">size</code><code class="si">:</code><code class="s2">&gt;5d</code><code class="si">}</code><code class="s2">]"</code><code class="p">)</code></pre>
        <p>While some of this will look familiar because it builds on the simple neural network from <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>, there are a few new concepts here, given that we’re using much more data. First, you’ll see that we get the <code>size</code> of the dataset. We simply use this to report on progress, as shown in the very last line.</p>
        <p>Then, we call <code>model.train</code> to explicitly set the model into training mode. PyTorch has optimizations that occur during training that are beyond the scope of this chapter. (To take advantage of them, you’ll switch the model between training and inference modes.) Note that this is more a property of the model than a method, but the method syntax is there. Sorry if it’s a little confusing!</p>
        <p>Next up is this interesting line:<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="data loader" data-type="indexterm" id="id708"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="data loader" data-type="indexterm" id="id709"/><a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST database" data-tertiary="data loader" data-type="indexterm" id="id710"/></p>
        <pre data-code-language="python" data-type="programlisting">    <code class="k">for</code> <code class="n">batch</code><code class="p">,</code> <code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">dataloader</code><code class="p">):</code></pre>
        <p class="pagebreak-before less_space">Let’s explore this in a little more detail. We made the Fashion MNIST dataset available to our code by using a data loader. There are 60,000 records available for training, each of which is 784 pixels. That’s a lot of data, and you don’t necessarily need all of it in memory at once. The idea of a <code>batch</code> is to take a chunk of that data—which, by default, is 64 items—and work with it. Enumerating the data loader gives us that, so we’ll train with 938 batches, 937 of 64, and the last one of 32 because you can’t evenly divide 60,000 by 64! </p>
        <p>Now, for each batch, we’ll go through the same loop that we saw for the previous example. We’ll get the predictions from the model, calculate the loss, backpropagate the gradients from the loss function, and optimize with new parameters.</p>
        <p>We’ll also use the term <em>epoch</em> for a training cycle<a contenteditable="false" data-primary="training" data-secondary="epoch for training cycle" data-type="indexterm" id="id711"/><a contenteditable="false" data-primary="epoch for training cycle" data-type="indexterm" id="id712"/> with <em>all</em> of the data (i.e., every batch). We can then output the status of the training every one hundred batches so as not to overload the output console!</p>
        <p>So, to train the network for five epochs, we can use code like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Training process</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">5</code>
<code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch </code><code class="si">{</code><code class="n">t</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="se">\n</code><code class="s2">-------------------------------"</code><code class="p">)</code>
    <code class="n">train</code><code class="p">(</code><code class="n">train_loader</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_function</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Done!"</code><code class="p">)</code></pre>
        <p>This will simply call the train function we specified five times—putting the network through the training loop by calculating the predictions, figuring out the loss, optimizing the parameters, and repeating five times.<a contenteditable="false" data-primary="" data-startref="ch2coco" data-type="indexterm" id="id713"/><a contenteditable="false" data-primary="" data-startref="ch2coco2" data-type="indexterm" id="id714"/><a contenteditable="false" data-primary="" data-startref="ch2coco3" data-type="indexterm" id="id715"/></p>
      </div></section>
      <section data-pdf-bookmark="Training the Neural Network" data-type="sect1"><div class="sect1" id="ch02_training_the_neural_network_1748548889076570">
        <h1>Training the Neural Network</h1>
        <p>Once you’ve executed the code,<a contenteditable="false" data-primary="learning process" data-secondary="Fashion MNIST database model" data-tertiary="test set inference and accuracy" data-type="indexterm" id="ch2la"/><a contenteditable="false" data-primary="neural networks" data-secondary="Fashion MNIST database model" data-tertiary="test set inference and accuracy" data-type="indexterm" id="ch2la2"/><a contenteditable="false" data-primary="training" data-secondary="Fashion MNIST database model" data-tertiary="test set inference and accuracy" data-type="indexterm" id="ch2la3"/><a contenteditable="false" data-primary="loss functions" data-secondary="computer vision" data-tertiary="test set inference and accuracy" data-type="indexterm" id="ch2la4"/><a contenteditable="false" data-primary="accuracy" data-secondary="Fashion MNIST database model" data-type="indexterm" id="ch2la5"/><a contenteditable="false" data-primary="inference" data-secondary="Fashion MNIST database model" data-type="indexterm" id="ch2la6"/><a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="test set inference and accuracy" data-type="indexterm" id="ch2la7"/><a contenteditable="false" data-primary="testing" data-secondary="inference and accuracy" data-type="indexterm" id="ch2la8"/> you’ll see the network train epoch by epoch. Then, after running the training, you’ll see something at the end that looks like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Epoch</code> <code class="mi">5</code>
<code class="o">-------------------------------</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.429329</code>  <code class="p">[</code>    <code class="mi">0</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.348756</code>  <code class="p">[</code> <code class="mi">6400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.237481</code>  <code class="p">[</code><code class="mi">12800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.336960</code>  <code class="p">[</code><code class="mi">19200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.435592</code>  <code class="p">[</code><code class="mi">25600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.272769</code>  <code class="p">[</code><code class="mi">32000</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.362881</code>  <code class="p">[</code><code class="mi">38400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.202799</code>  <code class="p">[</code><code class="mi">44800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.354268</code>  <code class="p">[</code><code class="mi">51200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">loss</code><code class="p">:</code> <code class="mf">0.205381</code>  <code class="p">[</code><code class="mi">57600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Done</code><code class="err">!</code></pre>
        <p>You can see here that over time, the loss has gone down. For example, in my case, the loss value at the end of the first epoch was .345, and by the end of the fifth epoch, it was .205. This data shows us that the network is learning.</p>
        <p>But how can we tell how <em>accurately</em> it’s learning? Note that loss and accuracy, while related, don’t have a direct linear relationship—for example, we can’t say that if loss is 20%, then accuracy is 80%. So, we need to go a little deeper.</p>
        <p>Recall that when we were getting the data, we got <em>two</em> datasets: one for training and one for testing. Here’s a great place where we can write code to pass the test data through our network and evaluate how accurate the network is at predicting answers. We already know the correct answers, so we could do inference on all 10,000 test records, get the answers that the model predicts, and then check them against the ground truth for accuracy.</p>
        <p>Here’s the code:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Function to test the model</code>
<code class="k">def</code> <code class="nf">test</code><code class="p">(</code><code class="n">dataloader</code><code class="p">,</code> <code class="n">model</code><code class="p">):</code>
    <code class="n">size</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataloader</code><code class="o">.</code><code class="n">dataset</code><code class="p">)</code>
    <code class="n">num_batches</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataloader</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>  <code class="c1"># Set the model to evaluation mode</code>
    <code class="n">test_loss</code><code class="p">,</code> <code class="n">correct</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="k">for</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">dataloader</code><code class="p">:</code>
            <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
            <code class="n">test_loss</code> <code class="o">+=</code> <code class="n">loss_function</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
            <code class="n">correct</code> <code class="o">+=</code> <code class="p">(</code><code class="n">pred</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code> <code class="o">==</code> 
                        <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">type</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">float</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
    <code class="n">test_loss</code> <code class="o">/=</code> <code class="n">num_batches</code>
    <code class="n">correct</code> <code class="o">/=</code> <code class="n">size</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Test Error: </code><code class="se">\n</code><code class="s2"> Accuracy: </code><code class="si">{</code><code class="p">(</code><code class="mi">100</code><code class="o">*</code><code class="n">correct</code><code class="p">)</code><code class="si">:</code><code class="s2">&gt;0.1f</code><code class="si">}</code><code class="s2">%, </code><code class="w"/>
            <code class="n">Avg</code> <code class="n">loss</code><code class="p">:</code> <code class="p">{</code><code class="n">test_loss</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">8</code><code class="n">f</code><code class="p">}</code> \<code class="n">n</code><code class="s2">")</code><code class="w"/>
 
<code class="c1"># Evaluate the model</code>
<code class="n">test</code><code class="p">(</code><code class="n">test_loader</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code></pre>
        <p>There are a few things to note in this code. First is the <code>model.eval()</code> line, which indicates that we are switching the model from training mode to inference mode. Similarly, <code>torch.no_grad()</code>will turn off gradient calculation in PyTorch to speed up inference. We’re no longer <em>training</em> the model, so we don’t need to do all the loss function backpropagation and optimization. We can just turn that off.</p>
        <p>Then, as it does during training,<a contenteditable="false" data-primary="prediction" data-secondary="Fashion MNIST model" data-type="indexterm" id="ch2pred"/> the network just goes through every item in the data loader, gets the prediction for that item, and checks its correctness with this line:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">correct</code> <code class="o">+=</code> <code class="p">(</code><code class="n">pred</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code> <code class="o">==</code>  <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">type</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">float</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code></pre>
        <p>That’s a bit of a mouthful, so let’s break it down.</p>
        <p>First, the <code>pred</code> value will give us the prediction from the network. The network outputs 10 values, each of which includes the probability of the class it represents being the correct one. Calling <code>argmax</code> on this will give us which one had the biggest value (i.e., the one with the probability closest to 1). The <em>y</em> value is the correct answer. For example, if we get a prediction, the neuron with the highest value is the sixth one, and <em>y</em> = 6, so we know we have a correct answer. Also, because we’re dealing in batches, we want to count each time <code>pred.argmax(1) == y</code> for this batch, hence, the <code>sum()</code>. </p>
        <p>Therefore, our accuracy value will be the sum of correct items divided by the total number of items. So, when you run this code after training the model, you should see output like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Test</code> <code class="n">Error</code><code class="p">:</code>
 <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">86.9</code><code class="o">%</code><code class="p">,</code> <code class="n">Avg</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.366243</code></pre>
        <p>Remarkably, after running the neural network for only five epochs, we can see that it is 86.9% accurate on data it hadn’t previously seen!</p>
        <p>At this point, you may be thinking that it’s really nice to see the accuracy of the model on the test set, but you may also be asking why we’ve only reported loss on the training—why not also report accuracy there? It seems silly to finish training the model by only looking at minimizing loss and <em>then</em> to figure out the accuracy. And you’d be right!</p>
        <p>Fortunately, updating the model training code to <em>also</em> report on accuracy is pretty easy to do. Here’s a function called <code>get_accuracy()</code> that you can use during training:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Function to calculate accuracy</code>
<code class="k">def</code> <code class="nf">get_accuracy</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">labels</code><code class="p">):</code>
    <code class="n">_</code><code class="p">,</code> <code class="n">predictions</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">correct</code> <code class="o">=</code> <code class="p">(</code><code class="n">predictions</code> <code class="o">==</code> <code class="n">labels</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
    <code class="n">accuracy</code> <code class="o">=</code> <code class="n">correct</code> <code class="o">/</code> <code class="n">labels</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">accuracy</code></pre>
        <p>Then, in your training loop, you can simply call this function after the loss function call like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">batch</code><code class="p">,</code> <code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">dataloader</code><code class="p">):</code>
    <code class="c1"># Compute prediction and loss</code>
    <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
    <code class="n">accuracy</code> <code class="o">=</code> <code class="n">get_accuracy</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
 
    <code class="c1"># Backpropagation</code></pre>
        <p>And when you’re reporting on the output of the training, you can use the accuracy metric like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">if</code> <code class="n">batch</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
    <code class="n">current</code> <code class="o">=</code> <code class="n">batch</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
    <code class="n">avg_loss</code> <code class="o">=</code> <code class="n">total_loss</code> <code class="o">/</code> <code class="p">(</code><code class="n">batch</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">avg_accuracy</code> <code class="o">=</code> <code class="n">total_accuracy</code> <code class="o">/</code> <code class="p">(</code><code class="n">batch</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="mi">100</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Batch </code><code class="si">{</code><code class="n">batch</code><code class="si">}</code><code class="s2">, Loss: </code><code class="si">{</code><code class="n">avg_loss</code><code class="si">:</code><code class="s2">&gt;7f</code><code class="si">}</code><code class="s2">, </code><code class="w"/>
            <code class="n">Accuracy</code><code class="p">:</code> <code class="p">{</code><code class="n">avg_accuracy</code><code class="p">:</code><code class="o">&gt;</code><code class="mf">0.2</code><code class="n">f</code><code class="p">}</code><code class="o">%</code> 
                      <code class="p">[{</code><code class="n">current</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}</code><code class="o">/</code><code class="p">{</code><code class="n">size</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}]</code><code class="s2">")</code><code class="w"/></pre>
        <p>Running this will give you output a bit like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Epoch</code> <code class="mi">5</code>
<code class="o">-------------------------------</code>
<code class="n">Batch</code> <code class="mi">0</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.177518</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.31</code><code class="o">%</code> <code class="p">[</code>    <code class="mi">0</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">100</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.304973</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.89</code><code class="o">%</code> <code class="p">[</code> <code class="mi">6400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">200</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.311628</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.51</code><code class="o">%</code> <code class="p">[</code><code class="mi">12800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">300</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.307373</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.63</code><code class="o">%</code> <code class="p">[</code><code class="mi">19200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">400</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.309722</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.67</code><code class="o">%</code> <code class="p">[</code><code class="mi">25600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">500</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.310240</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.60</code><code class="o">%</code> <code class="p">[</code><code class="mi">32000</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">600</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.306988</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.70</code><code class="o">%</code> <code class="p">[</code><code class="mi">38400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">700</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.308556</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.64</code><code class="o">%</code> <code class="p">[</code><code class="mi">44800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">800</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.309518</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.67</code><code class="o">%</code> <code class="p">[</code><code class="mi">51200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">900</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.311487</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">88.59</code><code class="o">%</code> <code class="p">[</code><code class="mi">57600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Done</code><code class="err">!</code></pre>
        <p>Now, you’re probably wondering why the accuracy for the test data (86.9%) is <em>lower</em> than the accuracy for the training data (88.59%). This is very common, and when you think about it, it makes sense: the neural network only really knows how to match the inputs it has been trained on with the outputs for those values. Our hope is that given enough data, the network will be able to generalize from the examples it has seen and thus “learn” what a shoe or a dress looks like. But there will always be examples of items that it hasn’t seen that are also different enough from what it has seen to confuse it.</p>
        <p>For example, if you grew up only ever seeing sneakers, then that’s what a shoe looks like to you. So, when you first see a high-heeled shoe, you might be a little confused. From your experience, it’s probably a shoe, but you don’t know for sure. That’s exactly what a neural network “thinks” when it “sees” inputs that are different enough from what it’s been trained on.<a contenteditable="false" data-primary="" data-startref="ch2la" data-type="indexterm" id="id716"/><a contenteditable="false" data-primary="" data-startref="ch2la2" data-type="indexterm" id="id717"/><a contenteditable="false" data-primary="" data-startref="ch2la3" data-type="indexterm" id="id718"/><a contenteditable="false" data-primary="" data-startref="ch2la4" data-type="indexterm" id="id719"/><a contenteditable="false" data-primary="" data-startref="ch2la5" data-type="indexterm" id="id720"/><a contenteditable="false" data-primary="" data-startref="ch2la6" data-type="indexterm" id="id721"/><a contenteditable="false" data-primary="" data-startref="ch2la7" data-type="indexterm" id="id722"/><a contenteditable="false" data-primary="" data-startref="ch2la8" data-type="indexterm" id="id723"/><a contenteditable="false" data-primary="" data-startref="ch2pred" data-type="indexterm" id="id724"/></p>
      </div></section>
      <section data-pdf-bookmark="Exploring the Model Output" data-type="sect1"><div class="sect1" id="ch02_exploring_the_model_output_1748548889076624">
        <h1>Exploring the Model Output</h1>
        <p>Now that we’ve trained the model<a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="exploring model output" data-type="indexterm" id="ch2exp"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="exploring model output" data-type="indexterm" id="ch2exp2"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="exploring model output" data-type="indexterm" id="ch2exp3"/> and gotten a good gauge of its accuracy by using the test set, let’s explore it a little. Here’s a function we can use to predict a single image:</p>
        <pre data-code-language="python" data-type="programlisting"> <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
 
<code class="k">def</code> <code class="nf">predict_single_image</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">label</code><code class="p">,</code> <code class="n">model</code><code class="p">):</code>
    <code class="c1"># Set the model to evaluation mode</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
 
<code class="c1"># Unsqueeze image as the model expects a batch dimension</code>
    <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
 
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">prediction</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">prediction</code><code class="p">)</code>
        <code class="n">predicted_label</code> <code class="o">=</code> <code class="n">prediction</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
 
    <code class="c1"># Display the image and predictions</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Predicted: </code><code class="si">{</code><code class="n">predicted_label</code><code class="si">}</code><code class="s1">, Actual: </code><code class="si">{</code><code class="n">label</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
 
    <code class="k">return</code> <code class="n">predicted_label</code>
 
<code class="c1"># Choose an image from the test set</code>
<code class="n">image</code><code class="p">,</code> <code class="n">label</code> <code class="o">=</code> <code class="n">test_dataset</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>  <code class="c1"># Change index to test different images</code>
 
<code class="c1"># Predict the class for the chosen image</code>
<code class="n">predicted_label</code> <code class="o">=</code> <code class="n">predict_single_image</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">label</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"The model predicted </code><code class="si">{</code><code class="n">predicted_label</code><code class="si">}</code><code class="s2">, and the actual label is </code><code class="si">{</code><code class="n">label</code><code class="si">}</code><code class="s2">."</code><code class="p">)</code></pre>
        <p>Let’s start with this code, which should look familiar to you now that you’ve seen the previous accuracy calculation code:</p>
        <pre data-code-language="python" data-type="programlisting">    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">prediction</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">prediction</code><code class="p">)</code>
        <code class="n">predicted_label</code> <code class="o">=</code> <code class="n">prediction</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code></pre>
        <p>Here, we get the <code>image</code>, send it to the <code>model</code>, get back a <code>prediction</code>, and <code>print</code> it out. Then, we get the <code>argmax</code> of that to show the label. Here’s an example output of the <code>prediction</code>:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">tensor</code><code class="p">([[</code><code class="err">–</code><code class="mf">12.4290</code><code class="p">,</code> <code class="err">–</code><code class="mf">16.0639</code><code class="p">,</code> <code class="err">–</code><code class="mf">14.3148</code><code class="p">,</code> <code class="err">–</code><code class="mf">16.2861</code><code class="p">,</code> <code class="err">–</code><code class="mf">13.1672</code><code class="p">,</code>  <code class="err">–</code><code class="mf">4.5377</code><code class="p">,</code> <code class="err">–</code><code class="mf">13.6284</code><code class="p">,</code> 
         <code class="err">–</code><code class="mf">1.3124</code><code class="p">,</code>  <code class="err">–</code><code class="mf">8.9946</code><code class="p">,</code>  <code class="err">–</code><code class="mf">0.3285</code><code class="p">]])</code></pre>
        <p>These numbers may seem vague, but ultimately, our goal is simply to look for the biggest one! The <code>Softmax</code> function gets the <code>log()</code> of the value, where <code>log(1)</code> is zero and the log of any value less than one is a negative value. As you look through the list, you’ll notice that the value closest to 0 (–0.3285) is the very last one. This indicates that the function believes the class for this image should be class number 9. (There are 10 classes in Fashion MNIST, which are numbered 0 through 9.)</p>
        <p>Fashion MNIST’s class number 9 is “Ankle Boot,” so I’ve also included the code to render the image in <a data-type="xref" href="#ch02_figure_6_1748548889066448">Figure 2-6</a>.</p>
        <p>Also, as we can see, this is an example of where the model got the prediction right. The ground truth was that it’s label 9, and the prediction was for number 9. Drawing the image so that we mere humans can compare the two also gives us an ankle boot!</p>
                <figure><div class="figure" id="ch02_figure_6_1748548889066448">
          <img src="assets/aiml_0206.png"/>
          <h6><span class="label">Figure 2-6. </span>Exploring the output of the predictive model</h6>
        </div></figure>
        <p>Now, try a few different values for yourself and see if you can find anywhere the model gets it wrong.<a contenteditable="false" data-primary="" data-startref="ch2exp" data-type="indexterm" id="id725"/><a contenteditable="false" data-primary="" data-startref="ch2exp2" data-type="indexterm" id="id726"/><a contenteditable="false" data-primary="" data-startref="ch2exp3" data-type="indexterm" id="id727"/></p>
      </div></section>
      <section data-pdf-bookmark="Overfitting" data-type="sect1"><div class="sect1" id="ch02_overfitting_1748548889076667">
        <h1>Overfitting</h1>
        <p>In the last example, we trained<a contenteditable="false" data-primary="computer vision" data-secondary="overfitting" data-type="indexterm" id="id728"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="overfitting" data-type="indexterm" id="id729"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="overfitting" data-type="indexterm" id="id730"/><a contenteditable="false" data-primary="overfitting" data-type="indexterm" id="id731"/><a contenteditable="false" data-primary="training" data-secondary="overfitting" data-type="indexterm" id="id732"/><a contenteditable="false" data-primary="testing" data-secondary="inference and accuracy" data-tertiary="overfitting" data-type="indexterm" id="id733"/> for only five epochs. That is, we went through the entire training loop of having the neurons randomly initialized and checked against their labels, then that performance was measured by the loss function and updated by the optimizer five times. And the results we got were pretty good: 88.59% accuracy on the training set and 86.5% on the test set. So what happens if we train for longer?</p>
        <p>Next, try updating it to train for 50 epochs instead of 5. In my case, I got the following accuracy figures on the training set:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Epoch</code> <code class="mi">50</code>
<code class="o">-------------------------------</code>
<code class="n">Batch</code> <code class="mi">0</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.077159</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.88</code><code class="o">%</code> <code class="p">[</code>    <code class="mi">0</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">100</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.094825</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.57</code><code class="o">%</code> <code class="p">[</code> <code class="mi">6400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">200</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.093598</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.67</code><code class="o">%</code> <code class="p">[</code><code class="mi">12800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">300</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.095906</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.54</code><code class="o">%</code> <code class="p">[</code><code class="mi">19200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">400</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.096683</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.48</code><code class="o">%</code> <code class="p">[</code><code class="mi">25600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">500</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.101872</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.31</code><code class="o">%</code> <code class="p">[</code><code class="mi">32000</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">600</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.103130</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.22</code><code class="o">%</code> <code class="p">[</code><code class="mi">38400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">700</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.103901</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.17</code><code class="o">%</code> <code class="p">[</code><code class="mi">44800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">800</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.104216</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.15</code><code class="o">%</code> <code class="p">[</code><code class="mi">51200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">900</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.104010</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.15</code><code class="o">%</code> <code class="p">[</code><code class="mi">57600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Done</code><code class="err">!</code></pre>
        <p>This is particularly exciting because we’re doing much better: we’re getting 96.15% accuracy! </p>
        <p>However, for the test set, accuracy reached 89.2%:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Test</code> <code class="n">Error</code><code class="p">:</code>
 <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">89.2</code><code class="o">%</code><code class="p">,</code> <code class="n">Avg</code> <code class="n">loss</code><code class="p">:</code> <code class="mf">0.433885</code></pre>
        <p>So, we got a big improvement over the training set and a smaller one over the test set. This might suggest that training our network for much longer would lead to much better results—but that’s not always the case. The network is doing much better with the training data, but the model is not necessarily a better model. In fact, the divergence in the accuracy numbers shows that the model might have become overspecialized to the training data, in a process that’s often called <em>overfitting</em>. As you build more neural networks, this problem is something to watch out for—and as you go through this book, you’ll learn a number of techniques to avoid it!</p>
      </div></section>
      <section data-pdf-bookmark="Early Stopping" data-type="sect1"><div class="sect1" id="ch02_early_stopping_1748548889076713">
        <h1>Early Stopping</h1>
        <p>In each of the cases so far,<a contenteditable="false" data-primary="computer vision" data-secondary="introduction" data-tertiary="training until desired accuracy" data-type="indexterm" id="ch2untl"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="training until desired accuracy" data-type="indexterm" id="ch2untl2"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="training until desired accuracy" data-type="indexterm" id="ch2untl3"/><a contenteditable="false" data-primary="accuracy" data-secondary="Fashion MNIST database model" data-tertiary="training until desired accuracy" data-type="indexterm" id="ch2untl4"/><a contenteditable="false" data-primary="training" data-secondary="Fashion MNIST database model" data-tertiary="training until desired accuracy" data-type="indexterm" id="ch2untl5"/> we’ve hardcoded the number of epochs we’re training for. While that works, we might want to train until we reach the desired accuracy instead of constantly trying different numbers of epochs and training and retraining until we get to our desired value. So, for example, if we want to train until the model is at 95% accuracy on the training set, and if we want to do it without knowing in advance how many epochs it will take. . .how can we do it?</p>
        <p>Given that we’ve updated our code to check the accuracy as the model trained and to print it out, now, all we have to do is check that accuracy and end the training if it’s above a certain amount—such as 95% (or 0.95 when normalized). For example, we can do this:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">if</code> <code class="n">batch</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
    <code class="n">current</code> <code class="o">=</code> <code class="n">batch</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
    <code class="n">avg_loss</code> <code class="o">=</code> <code class="n">total_loss</code> <code class="o">/</code> <code class="p">(</code><code class="n">batch</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">avg_accuracy</code> <code class="o">=</code> <code class="n">total_accuracy</code> <code class="o">/</code> <code class="p">(</code><code class="n">batch</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="mi">100</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Batch </code><code class="si">{</code><code class="n">batch</code><code class="si">}</code><code class="s2">, Loss: </code><code class="si">{</code><code class="n">avg_loss</code><code class="si">:</code><code class="s2">&gt;7f</code><code class="si">}</code><code class="s2">, </code><code class="w"/>
           <code class="n">Accuracy</code><code class="p">:</code> <code class="p">{</code><code class="n">avg_accuracy</code><code class="p">:</code><code class="o">&gt;</code><code class="mf">0.2</code><code class="n">f</code><code class="p">}</code><code class="o">%</code> <code class="p">[{</code><code class="n">current</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}</code><code class="o">/</code><code class="p">{</code><code class="n">size</code><code class="p">:</code><code class="o">&gt;</code><code class="mi">5</code><code class="n">d</code><code class="p">}]</code><code class="s2">")</code><code class="w"/>

<code class="c1"># Early stopping condition</code>
<code class="k">if</code> <code class="n">avg_accuracy</code> <code class="o">&gt;=</code> <code class="mi">95</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Reached 95</code><code class="si">% a</code><code class="s2">ccuracy, stopping training."</code><code class="p">)</code>
    <code class="k">return</code> <code class="kc">True</code>  <code class="c1"># Stop training</code></pre>
        <p>Note that if we use this code inside the <code>if batch % 100 == 0</code> block, we can break the training loop before all batches in a particular epoch have been processed. It’s better to do this check at the end of the epoch, so we need to be sure to place the <code>if avg_accuracy &gt;= 95</code> in the right place! </p>
        <p>Now, when we’re training, at the end of every epoch, the average accuracy for the epoch will be calculated—and if it hits 95%, the training will stop. Previously, I had trained the model for 50 epochs to get 96.15% accuracy, but with this early stopping, where I’ve defined 95% as “good enough,” you can see that the model stopped training after only 37 epochs. Interestingly, accuracy was 94.99% for a couple of epochs before that, so I might have been able to stop even earlier! </p>
        <p>This process of <em>early stopping</em> is very powerful in helping you save time as you evaluate different model architectures for solving specific problems. It helps you train your model until it’s “good enough,” instead of having a fixed training loop. For example, the process can look like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">Epoch</code> <code class="mi">36</code>
<code class="o">-------------------------------</code>
<code class="n">Batch</code> <code class="mi">0</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.098307</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.88</code><code class="o">%</code> <code class="p">[</code>    <code class="mi">0</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">100</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.119195</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.45</code><code class="o">%</code> <code class="p">[</code> <code class="mi">6400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">200</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.127049</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.20</code><code class="o">%</code> <code class="p">[</code><code class="mi">12800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">300</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.126001</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.34</code><code class="o">%</code> <code class="p">[</code><code class="mi">19200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">400</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.127823</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.25</code><code class="o">%</code> <code class="p">[</code><code class="mi">25600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">500</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.131262</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.11</code><code class="o">%</code> <code class="p">[</code><code class="mi">32000</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">600</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.135573</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">94.95</code><code class="o">%</code> <code class="p">[</code><code class="mi">38400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">700</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.135920</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">94.95</code><code class="o">%</code> <code class="p">[</code><code class="mi">44800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">800</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.135125</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">94.99</code><code class="o">%</code> <code class="p">[</code><code class="mi">51200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">900</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.134854</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">94.99</code><code class="o">%</code> <code class="p">[</code><code class="mi">57600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Epoch</code> <code class="mi">37</code>
<code class="o">-------------------------------</code>
<code class="n">Batch</code> <code class="mi">0</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.104421</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">96.88</code><code class="o">%</code> <code class="p">[</code>    <code class="mi">0</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">100</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.122693</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.34</code><code class="o">%</code> <code class="p">[</code> <code class="mi">6400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">200</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.124787</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.26</code><code class="o">%</code> <code class="p">[</code><code class="mi">12800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">300</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.127841</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.16</code><code class="o">%</code> <code class="p">[</code><code class="mi">19200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">400</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.130558</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.05</code><code class="o">%</code> <code class="p">[</code><code class="mi">25600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">500</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.131684</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.00</code><code class="o">%</code> <code class="p">[</code><code class="mi">32000</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">600</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.132620</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">94.95</code><code class="o">%</code> <code class="p">[</code><code class="mi">38400</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">700</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.132498</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.01</code><code class="o">%</code> <code class="p">[</code><code class="mi">44800</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">800</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.132462</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.05</code><code class="o">%</code> <code class="p">[</code><code class="mi">51200</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Batch</code> <code class="mi">900</code><code class="p">,</code> <code class="n">Loss</code><code class="p">:</code> <code class="mf">0.133915</code><code class="p">,</code> <code class="n">Accuracy</code><code class="p">:</code> <code class="mf">95.03</code><code class="o">%</code> <code class="p">[</code><code class="mi">57600</code><code class="o">/</code><code class="mi">60000</code><code class="p">]</code>
<code class="n">Reached</code> <code class="mi">95</code><code class="o">%</code> <code class="n">accuracy</code><code class="p">,</code> <code class="n">stopping</code> <code class="n">training</code><code class="o">.</code></pre>
        <p>This process can save you a lot of time you would otherwise spend manually checking on the network to see if it’s learning appropriately.<a contenteditable="false" data-primary="" data-startref="ch2untl" data-type="indexterm" id="id734"/><a contenteditable="false" data-primary="" data-startref="ch2untl2" data-type="indexterm" id="id735"/><a contenteditable="false" data-primary="" data-startref="ch2untl3" data-type="indexterm" id="id736"/><a contenteditable="false" data-primary="" data-startref="ch2untl4" data-type="indexterm" id="id737"/><a contenteditable="false" data-primary="" data-startref="ch2untl5" data-type="indexterm" id="id738"/> </p>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch02_summary_1748548889076756">
        <h1>Summary</h1>
        <p>In <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>, you learned about how ML is based on fitting features to labels through sophisticated pattern matching with a neural network. In this chapter, you took that to the next level by going beyond a single neuron and learning how to create your first (very basic) computer vision neural network. The network was somewhat limited because of the data: all the images were 28 × 28 grayscale, with the item of clothing centered in the frame. This is a good start, but it’s a very controlled scenario. </p>
        <p>To do better at vision, you may need the computer to learn features of an image instead of learning merely the raw pixels. You can do that with a process called <em>convolutions</em>, and in the next chapter, you’ll learn how to define convolutional neural networks to understand the contents of images.</p>
      </div></section>
    </div></section></body></html>