<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">11</span></span> <span class="chapter-title-text">Deploying an LLM on a Raspberry Pi: How low can you go?</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Setting up a Raspberry Pi server on your local network</li>
<li class="readable-text" id="p3">Converting and quantizing a model to GGUF format</li>
<li class="readable-text" id="p4">Serving your model as a drop-in replacement to the OpenAI GPT model</li>
<li class="readable-text" id="p5">What to do next and how to make it better</li>
</ul>
</div>
<div class="readable-text" id="p6">
<blockquote>
<div>
<em>The bitterness of poor quality remains long after the sweetness of low price is forgotten.</em>
<div class="quote-cite">
       —Benjamin Franklin 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p7">
<p>Welcome to one of our favorite projects on this list: serving an LLM on a device smaller than it should ever be served on. In this project, we will be pushing to the edge of this technology. By following along, you’ll be able to really flex everything you’ve learned in this book. In this project, we’ll deploy an LLM to a Raspberry Pi, which we will set up as an LLM Service you can query from any device on your home network. For all the hackers out there, this exercise should open the doors to many home projects. For everyone else, it’s a chance to solidify your understanding of the limitations of using LLMs and appreciate the community that has made this possible. </p>
</div>
<div class="readable-text intended-text" id="p8">
<p>This is a practical project. In this chapter, we’ll dive into much more than LLMs, and there won’t be any model training or data focusing, so it is our first truly production-only project. What we’ll create will be significantly slower, less efficient, and less accurate than what you’re probably expecting, and that’s fine. Actually, it’s a wonderful learning experience. Understanding the difference between possible and useful is something many never learn until it smacks them across the face. An LLM running on a Raspberry Pi isn’t something you’ll want to deploy in an enterprise production system, but we will help you learn the principles behind it so you can eventually scale up to however large you’d like down the line.</p>
</div>
<div class="readable-text" id="p9">
<h2 class="readable-text-h2" id="sigil_toc_id_170"><span class="num-string">11.1</span> Setting up your Raspberry Pi</h2>
</div>
<div class="readable-text" id="p10">
<p>Serving and inferencing on a Raspberry Pi despite all odds is doable, although we generally don’t recommend doing so other than to show that you can, which is the type of warning that is the telltale sign of a fun project, like figuring out how many marshmallows you can fit in your younger brother’s mouth. Messing with Raspberry Pis by themselves is pretty fun in general, and we hope that this isn’t the first time you’ve played with one. Raspberry Pis make great, cheap servers for your home. You can use them for ad blocking (Pi-Hole is a popular library) or media streaming your own personal library with services like Plex and Jellyfin. There are lots of fun projects. Because it’s fully customizable, if you can write a functional Python script, you can likely run it on a Raspberry Pi server for your local network to consume, which is what we are going to do for our LLM server.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>You’ll just need three things to do this project: a Raspberry Pi with 8 GB of RAM, a MicroSD (at least 32 GB, but more is better), and a power supply. At the time of this writing, we could find several MicroSD cards with 1 TB of memory for $20, so hopefully, you get something much bigger than 32 GB. Anything else you purchase is just icing on the cake—for example, a case for your Pi. If you don’t have Wi-Fi, you’ll also need an ethernet cable to connect your Pi to your home network. We’ll show you how to remote into your Pi from your laptop once we get it up. In addition, if your laptop doesn’t come with a MicroSD slot, you’ll need some sort of adapter to connect it.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>For the Raspberry Pi itself, we will be using the Raspberry Pi 5 8 GB model for this project. If you’d like to follow along, the exact model we’re using can be found here: <a href="https://mng.bz/KDZg">https://mng.bz/KDZg</a>. For the model we’ll deploy, you’ll need a single-board computer with at least 8 GB of RAM to follow along. As a fun fact, we have been successful in deploying models to smaller Pis with only 4GB of RAM, and plenty of other single-board alternatives to the Raspberry Pi are available. If you choose a different board, though, it might be more difficult to follow along exactly, so do so only if you trust the company. Some alternatives we recommend include Orange Pi, Zima Board, and Jetson, but we won’t go over how to set these up.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>You won’t need to already know how to set up a Pi. We will walk you through all the steps, assuming this is your first Raspberry Pi project. A Pi is literally just hardware and an open sandbox for lots of projects, so we will first have to install an operating system (OS). After that, we’ll install the necessary packages and libraries, prepare our LLM, and finally serve it as a service you can ping from any computer in your home network and get generated text.</p>
</div>
<div class="readable-text" id="p14">
<h3 class="readable-text-h3" id="sigil_toc_id_171"><span class="num-string">11.1.1</span> Pi Imager</h3>
</div>
<div class="readable-text" id="p15">
<p>To start off, Pis don’t usually come with an OS installed, and even if yours did, we’re going to change it. Common distributions like Rasbian OS or Ubuntu are too large and take too much RAM to run models at their fastest. To help us with this limitation, Raspberry Pi’s makers have released a free imaging software called the Pi Imager that you can download on your laptop from here: <a href="https://www.raspberrypi.com/software/">https://www.raspberrypi.com/software/</a>. If you already have the imager, we recommend updating it to a version higher than 1.8 since we are using a Pi 5.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>Once you have it, plug the microSD into the computer where you’ve downloaded the Pi Imager program. (If you aren’t sure how to do this, search online for the USB 3.0 microSD Card Reader.) Open the imager and select the device; for us, that’s Raspberry Pi 5. This selection will limit the OS options to those available for the Pi 5. Then you can select the Raspberry Pi OS Lite 64-bit for your operating system. <em>Lite</em> is the keyword you are looking for, and you will likely have to find it in the Raspberry Pi OS (Other) subsection. Then select your microSD as your storage device. The actual name will vary depending on your setup. Figure 11.1<span class="aframe-location"/> shows an example of the Imager software with the correct settings. As a note, the Ubuntu Server is also a good operating system that would work for our project, and we’d recommend it. It’ll have a slightly different setup, so if you want to follow along, stick with a Raspberry Pi OS Lite. </p>
</div>
<div class="browsable-container figure-container" id="p17">
<img alt="figure" height="708" src="../Images/11-1.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.1</span> Raspberry Pi Imager set to the correct device, with the headless (Lite) operating system and the correct USB storage device selected</h5>
</div>
<div class="readable-text print-book-callout" id="p18">
<p><span class="print-book-callout-head">WARNING</span>  And as a warning, make sure that you’ve selected the microSD to image the OS—please do not select your main hard drive.</p>
</div>
<div class="readable-text" id="p19">
<p>Once you are ready, navigate forward by selecting the Next button, and you should see a prompt asking about OS customizations, as shown in figure 11.2. We will set this up, so click the Edit Settings button, and you should see a settings page. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p20">
<img alt="figure" height="347" src="../Images/11-2.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.2</span> Customizing our Raspberry Pi OS settings. Select Edit Settings.</h5>
</div>
<div class="readable-text" id="p21">
<p>Figure 11.3 shows an example of the settings page. We’ll give the Pi server a hostname after the project, llmpi. We’ll set a username and password and configure the Wi-Fi settings to connect to our home network. This is probably the most important step, so make sure that you’re set up for the internet, either by setting up your Wi-Fi connection in settings or via ethernet.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p22">
<img alt="figure" height="1192" src="../Images/11-3.png" width="1020"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.3</span> Example screenshot of the settings page with correct and relevant information</h5>
</div>
<div class="readable-text intended-text" id="p23">
<p>Just as important as setting up the internet, we want to enable SSH, or none of the subsequent steps will work. To do this, go to the Services tab and select Enable SSH, as seen in figure 11.4. We will use password authentication, so make sure you’ve set an appropriate username and password and are not leaving it to the default settings. You don’t want anyone with bad intentions to have super easy access to your Pi.</p>
</div>
<div class="readable-text intended-text" id="p24">
<p>At this point, we are ready to image. Move forward through the prompts, and the imager will install the OS onto your SD card. This process can take a few minutes but is usually over pretty quickly. Once your SD has your OS on it, you can remove it safely from your laptop. Put the microSD card in your Pi, and turn it on! If everything was done correctly, your Pi should automatically boot up and connect to your Wi-Fi.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p25">
<img alt="figure" height="326" src="../Images/11-4.png" width="1041"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.4</span> Make sure you select Enable SSH.</h5>
</div>
<div class="readable-text" id="p26">
<h3 class="readable-text-h3" id="sigil_toc_id_172"><span class="num-string">11.1.2</span> Connecting to Pi</h3>
</div>
<div class="readable-text" id="p27">
<p>We will use our little Pi like a small server. What’s nice about our setup is that you won’t need to find an extra monitor or keyboard to plug into your Pi. Of course, this setup comes with the obvious drawback that we can’t see what the Pi is doing, nor do we have an obvious way to interact with it. Don’t worry; that’s why we set up SSH. Now we’ll show you how to connect to your Pi from your laptop.</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>The first thing we’ll need to do is find the Raspberry Pi’s IP address. An IP address is a numerical label to identify a computer on a network. The easiest way to see new devices that have connected to the internet you’re using is through the router’s software. See figure 11.5. If you can access your router, you can go to its IP address in a browser. The IP address is typically 192.168.86.1 or 192.168.0.1; the type of router usually sets this number and can often be found on the router itself. You’ll then need to log in to your router, where you can see all devices connected to your network. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p29">
<img alt="figure" height="2048" src="../Images/11-5.png" width="946"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.5</span> Example Google Home router interface with several devices listed to discover their IP addresses</h5>
</div>
<div class="readable-text" id="p30">
<p>If you don’t have access to your router, which many people don’t, you’re not out of luck. The next easiest way is to ignore everything we said in the previous paragraph and connect your Pi to a monitor and keyboard. Run <code>$</code> <code>ifconfig</code> or <code>$</code> <code>ip</code> <code>a</code>, and then look for the <code>inet</code> parameter. These commands will output devices on your local network and their IP addresses. Figures 11.6 and 11.7 demonstrate running these commands and highlight what you are looking for. If you don’t have access to an extra monitor, well, things will get a bit tricky, but it’s still possible. However, we don’t recommend going down this path if you can avoid it.<span class="aframe-location"/><span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p31">
<img alt="figure" height="481" src="../Images/11-6.png" width="362"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.6</span> Example of running <code>ifconfig</code>. The IP address of our Pi (<code>inet</code>) is highlighted for clarity.</h5>
</div>
<div class="browsable-container figure-container" id="p32">
<img alt="figure" height="399" src="../Images/11-7.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.7</span> Example of running <code>ip</code> <code>a</code>. The IP address of our Pi (<code>inet</code>) is highlighted for clarity.</h5>
</div>
<div class="readable-text" id="p33">
<p>To scan your local network for IP addresses, open a terminal on your laptop, and run that same command (<code>$</code> <code>ifconfig</code>), or if you are on a Windows, <code>$</code> <code>ipconfig</code>. If you don’t have <code>ifconfig</code>, you can install it with <code>$</code> <code>sudo</code> <code>apt</code> <code>install</code> <code>net-tools</code>. We didn’t mention this step before because it should have already been installed on your Pi. </p>
</div>
<div class="readable-text intended-text" id="p34">
<p>If you already recognize which device the Pi is, that’s awesome! Just grab the <code>inet</code> parameter for that device. More likely, though, you won’t, and there are a few useful commands you can use if you know how. Use the command <code>$</code> <code>arp</code> <code>-a</code> to view the list of all IP addresses connected to your network and the command <code>$</code> <code>nslookup</code> <code>$IP_ADDRESS</code> to get the hostname for the computer at the IP address you pass in—you’d be looking for the hostname <code>raspberry</code>, but we’ll skip all that. We trust that if you know how to use these commands, you won’t be reading this section of the book. Instead, we’ll use caveman problem-solving, which means we’ll simply turn off the Pi, run our <code>$</code> <code>ifconfig</code> command again, and see what changes, specifically what disappears. When you turn it back on, your router might assign it a different IP address than last time, but you should still be able to <code>diff</code> the difference and find it.</p>
</div>
<div class="readable-text intended-text" id="p35">
<p>Alright, we know that was potentially a lot just to get the IP address, but once you have it, the next step is easy. To SSH into it, you can run the <code>ssh</code> command:</p>
</div>
<div class="browsable-container listing-container" id="p36">
<div class="code-area-container">
<pre class="code-area">$ ssh username@0.0.0.0</pre>
</div>
</div>
<div class="readable-text" id="p37">
<p>Replace <code>username</code> with the username you created (it should be <code>pi</code> if you are following along with us), and replace the <code>0</code>s with the IP address of your Pi. Since this is the first time connecting to a brand-new device, you’ll be prompted to fingerprint to establish the connection and authenticity of the host. Then you’ll be prompted to put in a password. Enter the password you set in the imager before. If you didn’t set a password, it’s <code>pi</code> by default, but we trust you didn’t do that, right?</p>
</div>
<div class="readable-text intended-text" id="p38">
<p>With that, you should be remotely connected to your Pi and see the Pi’s terminal reflected in your computer’s terminal, as shown in figure 11.8. Nice job!<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p39">
<img alt="figure" height="505" src="../Images/11-8.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.8</span> Terminal after a successfully secure shell into your Raspberry Pi.</h5>
</div>
<div class="readable-text" id="p40">
<h3 class="readable-text-h3" id="sigil_toc_id_173"><span class="num-string">11.1.3</span> Software installations and updates</h3>
</div>
<div class="readable-text" id="p41">
<p>Now that our Pi is up and we’ve connected to it, we can start the installation. The first command is well known and will simply update our system:</p>
</div>
<div class="browsable-container listing-container" id="p42">
<div class="code-area-container">
<pre class="code-area">$ sudo apt update &amp;&amp; sudo apt upgrade -y</pre>
</div>
</div>
<div class="readable-text" id="p43">
<p>It can take a minute, but once that finishes running, congratulations! You now have a Raspberry Pi server on which you can run anything you want to at this point. It’s still a blank slate, so let’s change that and prepare it to run our LLM server. We first want to install any dependencies we need. Depending on your installation, this may include <code>g++</code> or <code>build-essentials</code>. We need just two: <code>git</code> and <code>pip</code>. Let’s start by installing them, which will make this whole process so much easier: </p>
</div>
<div class="browsable-container listing-container" id="p44">
<div class="code-area-container">
<pre class="code-area">$ sudo apt install git-all python3-pip</pre>
</div>
</div>
<div class="readable-text" id="p45">
<p>Next, we can clone the repo that will be doing the majority of the work here: Llama.cpp. Let’s clone the project into your Pi and build the project. To do that, run the following commands:</p>
</div>
<div class="browsable-container listing-container" id="p46">
<div class="code-area-container">
<pre class="code-area">$ git clone https://github.com/ggerganov/llama.cpp.git  
$ cd llama.cpp</pre>
</div>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p47">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A note on llama.cpp</h5>
</div>
<div class="readable-text" id="p48">
<p>Llama.cpp, like many open source projects, is a project that is much more interested in making things work than necessarily following best engineering practices. Since you are cloning the repo in its current state, but we wrote these instructions in a previous state, you may run into problems we can’t prepare you for. Llama.cpp doesn’t have any form of versioning either. After cloning the repo, we recommend you run</p>
</div>
<div class="browsable-container listing-container" id="p49">
<div class="code-area-container">
<pre class="code-area">$ git checkout 306d34be7ad19e768975409fc80791a274ea0230</pre>
</div>
</div>
<div class="readable-text" id="p50">
<p>This command will checkout the exact <code>git</code> <code>commit</code> we used so you can run everything in the exact same version of llama.cpp. We tested this on Mac, Windows 10, Ubuntu, Debian, and, of course, both a Raspberry Pi 4 and 5. We don’t expect any problems on most systems with this version.</p>
</div>
</div>
<div class="readable-text" id="p51">
<p>Now that we have the repo, we must complete a couple of tasks to prepare it. First, to keep our Pi clean, let’s create a virtual environment for our repo and activate it. Once we have our Python environment ready, we’ll install all the requirements. We can do so with the following commands: </p>
</div>
<div class="browsable-container listing-container" id="p52">
<div class="code-area-container">
<pre class="code-area">$ python3 -m venv .venv
$ source .venv/bin/activate
$ pip install -r requirements.txt</pre>
</div>
</div>
<div class="readable-text" id="p53">
<p>Llama.cpp is written in C++, which is a compiled language. That means we have to compile all the dependencies to run on our hardware and architecture. Let’s go ahead and build it. We do that with one simple command:</p>
</div>
<div class="browsable-container listing-container" id="p54">
<div class="code-area-container">
<pre class="code-area">$ make</pre>
</div>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p55">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A note on setting up</h5>
</div>
<div class="readable-text" id="p56">
<p>If you’re performing this setup in even a slightly different environment, using CMake instead of Make can make all the difference! For example, even running on Ubuntu, we needed to use CMake to specify the compatible version of CudaToolkit and where that nvcc binary was stored in order to use CuBLAS instead of vanilla CPU to make use of a CUDA-integrated GPU. The original creator (Georgi Gerganov, aka ggerganov) uses CMake when building for tests because it requires more specifications than Make. For reference, here’s the CMake build command ggerganov currently uses; you can modify it as needed:</p>
</div>
<div class="browsable-container listing-container" id="p57">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ cmake .. -DLLAMA_NATIVE=OFF -DLLAMA_BUILD_SERVER=ON -DLLAMA_CURL=ON 
<span class="">↪</span> --DLLAMA_CUBLAS=ON -DCUDAToolkit_ROOT=/usr/local/cuda 
<span class="">↪</span> --DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc 
<span class="">↪</span> --DCMAKE_CUDA_ARCHITECTURES=75 -DLLAMA_FATAL_WARNINGS=OFF 
<span class="">↪</span> --DLLAMA_ALL_WARNINGS=OFF -DCMAKE_BUILD_TYPE=Release</pre>
</div>
</div>
</div>
<div class="readable-text" id="p58">
<p>Next, we just need to get our model, and we’ll be ready to move forward. The model we’ve picked for this project is Llava-v1.6-Mistral-7B, which we will download using the <code>huggingface-cli</code>, like we’ve done in other chapters. Go ahead and run the following command to pull the LLaVA model, its accompanying tokenizer, and the config files: </p>
</div>
<div class="browsable-container listing-container" id="p59">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ pip install -U huggingface_hub
$ huggingface-cli download liuhaotian/llava-v1.6-mistral-7b --local-dir 
<span class="">↪</span> ./models/llava --local-dir-use-symlinks False</pre>
</div>
</div>
<div class="readable-text" id="p60">
<p>Now that we have our model and tokenizer information, we’re ready to turn our LLM into something usable for devices as small as an Android phone or Raspberry Pi.</p>
</div>
<div class="readable-text" id="p61">
<h2 class="readable-text-h2" id="sigil_toc_id_174"><span class="num-string">11.2</span> Preparing the model</h2>
</div>
<div class="readable-text" id="p62">
<p>Now that we have a model, we need to standardize it so that the C++ code in the repo can interface with it in the best way. We will convert the model from the safetensor format, which we downloaded into .gguf. We’ve used GGUF models before, as they are extensible, quick to load, and contain all of the information about the model in a single model file. We also download the tokenizer information, which goes into our .gguf model file.</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>Once ready, we can convert our safetensor model to GGUF with the <code>convert.py</code> script:</p>
</div>
<div class="browsable-container listing-container" id="p64">
<div class="code-area-container">
<pre class="code-area">$ python3 convert.py ./models/llava/ --skip-unknown</pre>
</div>
</div>
<div class="readable-text" id="p65">
<p>This code will convert all the weights into one .gguf checkpoint that is the same size on disk as all of the .safetensors files we downloaded combined. That’s now two copies of whatever we’ve downloaded, which is likely one too many if your microSD card is rather small. Once you have the .gguf checkpoint, we recommend you either delete or migrate the original model files somewhere off of the Pi to reclaim that memory, which could look like this:</p>
</div>
<div class="browsable-container listing-container" id="p66">
<div class="code-area-container">
<pre class="code-area">$ find -name './models/llava/model-0000*-of-00004.safetensors' -exec  
rm {} \;</pre>
</div>
</div>
<div class="readable-text" id="p67">
<p>Once our model is in the correct single-file format, we can make it smaller. Now memory constraints come into play. One reason we picked a 7B parameter model is that in the quantized <code>q4_K_M</code> format (we’ll talk about different llama.cpp-supported quantized formats later), it’s a little over 4 GB on disk, which is more than enough for the 8 GB Raspberry Pi to run effectively. Run the following command to quantize the model:</p>
</div>
<div class="browsable-container listing-container" id="p68">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ ./quantize ./models/llava/ggml-model-f16.gguf ./models/llava/llava-
<span class="">↪</span> v1.6-mistral-7b-q4_k_m.gguf Q4_K_M</pre>
</div>
</div>
<div class="readable-text" id="p69">
<p>We won’t lie: it’ll be a bit of a waiting game while the quantization methodology is applied to all of the model weights, but when it’s finished, you’ll have a fresh quantized model ready to be served.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p70">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Having trouble? </h5>
</div>
<div class="readable-text" id="p71">
<p>While we’ve tested these instructions in a multitude of environments and hardware, you might still find yourself stuck. Here’s some troubleshooting advice you can try that has helped us out:</p>
</div>
<ul>
<li class="readable-text" id="p72"> <em>Redownload the model</em>. These models are large, and if your Pi had any internet connection problems during the download, you may have a corrupted model. You may try connecting with an ethernet cable instead of Wi-Fi if your connection is spotty. </li>
<li class="readable-text" id="p73"> <em>Recompile your dependencies</em>. The easiest way to recomplie your dependencies is to run <code>make</code> <code>clean</code> and then <code>make</code> again. You might try using <code>cmake</code> or checking out different options. </li>
<li class="readable-text" id="p74"> <em>Reboot your Pi</em>. Rebooting is a classic but tried-and-true solution, especially if you are dealing with memory problems (which we don’t have a lot of for the task at hand.). You can reboot while in SSH with <code>sudo</code> <code>reboot.</code> </li>
<li class="readable-text" id="p75"> <em>Run through these steps on your computer</em>. You’re likely to run into fewer problems on better hardware, and it can be useful to know what an easy path looks like before trying to make it work on an edge device. </li>
<li class="readable-text" id="p76"> <em>Download an already prepared model</em>. While we encourage you to go through the steps of converting and quantizing yourself, you can usually find most open source models already quantized to any and every format. So if you aren’t worried about finetuning it, you should be in luck. For us, we are in said luck. </li>
</ul>
<div class="readable-text" id="p77">
<p>If you get stuck but want to keep moving forward, you can download a quantized version of the model with the following command:</p>
</div>
<div class="browsable-container listing-container" id="p78">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ huggingface-cli download cjpais/llava-1.6-mistral-7b-gguf --local-dir 
<span class="">↪</span> ./models/llava --local-dir-use-symlinks False --include *Q4_K_M*</pre>
</div>
</div>
</div>
<div class="readable-text" id="p79">
<h2 class="readable-text-h2" id="sigil_toc_id_175"><span class="num-string">11.3</span> Serving the model</h2>
</div>
<div class="readable-text" id="p80">
<p>We’re finally here, serving the model! With llama.cpp, creating a service for the model is incredibly easy, and we’ll get into some slightly more complex tricks in a bit, but for now, revel in what you’ve done:</p>
</div>
<div class="browsable-container listing-container" id="p81">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$./server -m ./models/llava/llava-v1.6-mistral-7b-q4_k_m.gguf --host
<span class="">↪</span> $PI_IP_ADDRESS --api-key $API_KEY</pre>
</div>
</div>
<div class="readable-text" id="p82">
<p>Be sure to use your Pi’s IP address, and the API key can be any random string to provide a small layer of security. That’s it! You now have an LLM running on a Raspberry Pi that can be queried from any computer on your local network. Note that the server can take a long time to boot up on your Pi, as it loads in the model. Don’t worry too much; give it time. Once ready, let’s test it out with a quick demo.</p>
</div>
<div class="readable-text intended-text" id="p83">
<p>For this demo, let’s say you’ve already integrated an app pretty deeply with OpenAI’s Python package. In listing 11.1, we show you how to point this app to your Pi LLM service instead. We’ll continue to use OpenAI’s Python bindings and point it to our service instead. We do this by updating the <code>base_url</code> to our Pi’s IP address and using the same API key we set when we created the server.</p>
</div>
<div class="readable-text intended-text" id="p84">
<p>Also, notice that we’re calling the <code>gpt-3.5-turbo</code> model. OpenAI has different processes for calling different models. You can easily change that if you don’t like typing those letters, but it doesn’t really matter. You’ll just have to figure out how to change the script for whichever model you want to feel like you’re calling (again, you’re not actually calling ChatGPT). </p>
</div>
<div class="browsable-container listing-container" id="p85">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.1</span> OpenAI but not ChatGPT </h5>
<div class="code-area-container">
<pre class="code-area">import openai

client = openai.OpenAI(
    base_url="http://0.0.0.0:8080/v1",  # replace with your pi's ip address
    api_key="1234",  # replace with your server's api key
)

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "system",
            "content": "You are Capybara, an AI assistant. Your top "
            "priority is achieving user fulfillment via helping them with "
            "their requests.",
        },
        {
            "role": "user",
            "content": "Building a website can be done in 10 simple steps:",
        },
    ],
)

print(completion.choices[0].message)</pre>
</div>
</div>
<div class="readable-text" id="p86">
<p>You don’t need code to interact with your server. The server script comes with a built-in minimal GUI, and you can access it on your local network with a phone or your laptop by pointing a browser to your Pi’s IP address. Be sure to include the port 8080. You can see an example of this in figure 11.9. </p>
</div>
<div class="readable-text intended-text" id="p87">
<p>This process will allow you to interface with the LLM API you’re running in a simple chat window. We encourage you to play around with it a bit. Since you’re running on a Raspberry Pi, the fastest you can expect this to go is about five tokens per second, and the slowest is, well, SLOW. You’ll immediately understand why normal people don’t put LLMs on edge devices.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p88">
<img alt="figure" height="936" src="../Images/11-9.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.9</span> Running an LLM on your Pi and interacting with it through the llama.cpp server</h5>
</div>
<div class="readable-text" id="p89">
<p>At this point, you may be wondering why we were so excited about this project. We made a bunch of promises about what you’d learn, but this chapter is the shortest in the book, and the majority of what we did here was download other people’s repos and models. <em>Welcome to production.</em></p>
</div>
<div class="readable-text intended-text" id="p90">
<p>This is ultimately what most companies will ask you to do: download some model that someone heard about from a friend and put it on hardware that’s way too small and isn’t meant to run it. You should now be ready to hack together a prototype of exactly what they asked for within about 20 to 30 minutes. Being able to iterate quickly will allow you to go back and negotiate with more leverage, demonstrating why you need more hardware, data to train on, RAG, or any other system to make the project work. Building a rapid proof of concept and then scaling up to fit the project’s needs should be a key workflow for data scientists and ML engineers.</p>
</div>
<div class="readable-text intended-text" id="p91">
<p>One huge advantage of following the rapid proof of concept workflow demo-ed here is visibility. You can show that you can throw something amazing together extremely fast, which (if your product managers are good) should add a degree of trust when other goals are taking longer than expected. They’ve seen that if you want something bad in production, you can do that in a heartbeat. The good stuff that attracts and retains customers takes time with real investment into data and research.</p>
</div>
<div class="readable-text" id="p92">
<h2 class="readable-text-h2" id="sigil_toc_id_176"><span class="num-string">11.4</span> Improvements</h2>
</div>
<div class="readable-text" id="p93">
<p>Now that we’ve walked through the project once, let’s talk about ways to modify this project. For clarity, we chose to hold your hand and tell you exactly what commands to run so you could get your feet wet with guided assistance. Tutorials often end here, but real learning, especially projects in production, always goes a step further. So we want to give you ideas about how you can make this project your own, from choosing a different model to using different tooling.</p>
</div>
<div class="readable-text" id="p94">
<h3 class="readable-text-h3" id="sigil_toc_id_177"><span class="num-string">11.4.1</span> Using a better interface</h3>
</div>
<div class="readable-text" id="p95">
<p>Learning a new tool is one of the most common tasks for someone in this field—and by that, we mean everything from data science to MLOps. While we’ve chosen to focus on some of the most popular and battle-tested tooling in this book—tools we’ve actually used in production— your company has likely chosen different tools. Even more likely, a new tool came out that everyone is talking about, and you want to try it out.</p>
</div>
<div class="readable-text intended-text" id="p96">
<p>We’ve talked a lot about llama.cpp and used it for pretty much everything in this project, including compiling, quantizing, serving, and even creating a frontend for our project. While the tool shines on the compiling and quantizing side, the other stuff was mostly added out of convenience. Let’s consider some other tools that can help give your project that extra pop or pizzazz.</p>
</div>
<div class="readable-text intended-text" id="p97">
<p>To improve your project instantly, you might consider installing a frontend for the server like SillyTavern (not necessarily recommended; it’s just popular). A great frontend will turn “querying an LLM” into “chatting with an AI best friend,” shifting from a placid task to an exciting experience. Some tools we like for the job are KoboldCpp and Ollama, which were built to extend llama.cpp and make the interface simpler or more extensible. So they are perfect to extend this particular project. Oobabooga is another great web UI for text generation. All these tools offer lots of customization and ways to provide your users with unique experiences. They generally provide both a frontend and a server.</p>
</div>
<div class="readable-text" id="p98">
<h3 class="readable-text-h3" id="sigil_toc_id_178"><span class="num-string">11.4.2</span> Changing quantization</h3>
</div>
<div class="readable-text" id="p99">
<p>You might consider doing this same project but on an older Pi with only 4 GB of memory, so you’ll need a smaller model. Maybe you want to do more than just serve an LLM with your Pi, so you need to shrink the model a bit more, or maybe you want to switch up the model entirely. Either way, you’ll need to dive a bit deeper down the quantization rabbit hole. Before, we quantized the model using <code>q4_K_M</code> format with the promise we’d explain it later. Well, now it’s later. </p>
</div>
<div class="readable-text intended-text" id="p100">
<p>Llama.cpp offers many different quantization formats. To simplify the discussion, table 11.1 highlights a few of the more common quantization methods, along with how many bits each converts down to, the size of the resulting model, and the RAM required to run it for a 7B parameter model. This table should act as a quick reference to help you determine what size and level of performance you can expect. The general rule is that smaller quantization equals lower-quality performance and higher perplexity.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p101">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 11.1</span> Comparison of key attributes for different llama.cpp quantization methods for a 7B parameter model</h5>
<table>
<thead>
<tr>
<th>
<div>
         Quant method 
       </div></th>
<th>
<div>
         Bits 
       </div></th>
<th>
<div>
         Size (GB) 
       </div></th>
<th>
<div>
         Max RAM required (GB) 
       </div></th>
<th>
<div>
         Use case 
       </div></th>
<th>
<div>
         Params (billions) 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>Q2_K</code> <br/></td>
<td>  2 <br/></td>
<td>  2.72 <br/></td>
<td>  5.22 <br/></td>
<td>  Significant quality loss; not recommended for most purposes <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q3_K_S</code> <br/></td>
<td>  3 <br/></td>
<td>  3.16 <br/></td>
<td>  5.66 <br/></td>
<td>  Very small, high loss of quality <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q3_K_M</code> <br/></td>
<td>  3 <br/></td>
<td>  3.52 <br/></td>
<td>  6.02 <br/></td>
<td>  Very small, high loss of quality <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q3_K_L</code> <br/></td>
<td>  3 <br/></td>
<td>  3.82 <br/></td>
<td>  6.32 <br/></td>
<td>  Small, substantial quality loss <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q4_0</code> <br/></td>
<td>  4 <br/></td>
<td>  4.11 <br/></td>
<td>  6.61 <br/></td>
<td>  Legacy; small, very high loss of quality; prefer using <code>Q3_K_M</code> <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q4_K_S</code> <br/></td>
<td>  4 <br/></td>
<td>  4.14 <br/></td>
<td>  6.64 <br/></td>
<td>  Small, greater quality loss <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q4_K_M</code> <br/></td>
<td>  4 <br/></td>
<td>  4.37 <br/></td>
<td>  6.87 <br/></td>
<td>  Medium, balanced quality; recommended <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q5_0</code> <br/></td>
<td>  5 <br/></td>
<td>  5.00 <br/></td>
<td>  7.50 <br/></td>
<td>  Legacy; medium, balanced quality; prefer using <code>Q4_K_M</code> <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q5_K_S</code> <br/></td>
<td>  5 <br/></td>
<td>  5.00 <br/></td>
<td>  7.50 <br/></td>
<td>  Large, low loss of quality; recommended <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q5_K_M</code> <br/></td>
<td>  5 <br/></td>
<td>  5.13 <br/></td>
<td>  7.63 <br/></td>
<td>  large, very low loss of quality; recommended <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q6_K</code> <br/></td>
<td>  6 <br/></td>
<td>  5.94 <br/></td>
<td>  8.44 <br/></td>
<td>  Very large, extremely low loss of quality <br/></td>
<td>  7 <br/></td>
</tr>
<tr>
<td> <code>Q8_0</code> <br/></td>
<td>  8 <br/></td>
<td>  7.70 <br/></td>
<td>  10.20 <br/></td>
<td>  Very large, extremely low loss of quality; not recommended <br/></td>
<td>  7 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p102">
<p>If you only have a 4 or 6 GB Pi, you’re probably looking at this table thinking, “Nope, time to give up.” But you’re not completely out of luck; your model will likely just run slower, and you’ll either need a smaller model than one of these 7Bs—something with only, say, 1B or 3B parameters—or to quantize smaller to run. You’re really pushing the edge with such a small Pi, so <code>Q2_k</code> or <code>Q3_K_S</code> might work for you.</p>
</div>
<div class="readable-text intended-text" id="p103">
<p>A friendly note: we’ve been pushing the limits on the edge with this project, but it is a useful experience for more funded projects. When working on similar projects with better hardware, that better hardware will have its limits as to how large an LLM it can run. After all, there’s always a bigger model. Keep in mind that if you’re running with cuBLAS or any framework for utilizing a GPU, you’re constrained by the VRAM in addition to the RAM. For example, running with cuBLAS on a 3090 constrains you to 24 GB of VRAM. Using clever memory management (such as a headless OS to take up less RAM), you can load bigger models onto smaller devices and push the boundaries of what feels like it should be possible.</p>
</div>
<div class="readable-text" id="p104">
<h3 class="readable-text-h3" id="sigil_toc_id_179"><span class="num-string">11.4.3</span> Adding multimodality</h3>
</div>
<div class="readable-text" id="p105">
<p>There’s an entire dimension that we initially ignored so that it wouldn’t distract, but let’s talk about it now: LLaVA is actually multimodal! A multimodal model allows us to expand out from NLP to other sources like images, audio, and video. Pretty much every multimodal model is also an LLM at heart, as datasets of different modalities are labeled with natural language—for example, a text description of what is seen in an image. In particular, LLaVA, which stands for Large Language and Vision Assistant, allows us to give the model an input image and ask questions about it. </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p106">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A note about the llama server</h5>
</div>
<div class="readable-text" id="p107">
<p>Remember when we said the llama.cpp project doesn’t follow many engineering best practices? Well, multimodality is one of them. The llama.cpp server at first supported multimodality, but many issues were soon added to the project. Instead of adding a feature and incrementing on it, the creator felt the original implementation was hacky and decided to remove it. One day, everything was working, and the next, it just disappeared altogether.</p>
</div>
<div class="readable-text" id="p108">
<p>This change happened while we were writing this chapter—which was a headache in itself—but imagine what damage it could have caused when trying to run things in production. Unfortunately, this sudden change is par for the course when working on LLMs at this point in time, as there are very few stable dependencies you can rely on that are currently available. To reproduce what’s here and minimize debugging, we hope you check out the <code>git</code> <code>commit</code> mentioned earlier. The good news is that llama.cpp plans to continue to support multimodality, and another implementation will likely be ready to go soon—possibly by the time you read this chapter</p>
</div>
</div>
<div class="readable-text" id="p109">
<p>We haven’t really talked about multimodality at all in this book, as many lessons from learning how to make LLMs work in production should transfer over to multimodal models. Regardless, we thought it’d be fun to show you how to deploy one.</p>
</div>
<div class="readable-text" id="p110">
<h4 class="readable-text-h4 sigil_not_in_toc">Updating the model</h4>
</div>
<div class="readable-text" id="p111">
<p>We’ve done most of the work already; however, llama.cpp has only converted the llama portion of the LLaVA model to .gguf. We need to add the vision portion back in. To test this, go to the GUI for your served model, and you’ll see an option to upload an image. If you do, you’ll get a helpful error, shown in figure 11.10, indicating that the server isn’t ready for multimodal serving.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p112">
<img alt="figure" height="324" src="../Images/11-10.png" width="873"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 11.10</span> Our model isn’t ready yet; we need to provide a model projector.</h5>
</div>
<div class="readable-text" id="p113">
<p>The first step to converting our model is downloading a multimodal projection file, similar to CLIP, for you to encode images. Once we can encode the images, the model will know what to do with them since it’s already been trained for multimodal tasks. We aren’t going to go into the details of preparing the projection file; instead, we’ll show you where you can find it. Run the following command to download this file and then move it:</p>
</div>
<div class="browsable-container listing-container" id="p114">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-
<span class="">↪</span> gguf/resolve/main/mmproj-model-f16.gguf
$ mv mmproj-model-f16.gguf ./models/llava/mmproj.gguf</pre>
</div>
</div>
<div class="readable-text" id="p115">
<p>If you are using a different model or a homebrew, make sure you find or create a multimodal projection model to perform that function for you. It should feel intuitive as to why you’d need it: language models only read language. You can try finetuning and serializing images to strings instead of using a multimodal projection model; however, we don’t recommend doing so, as we haven’t seen good results from it. It increases the total amount of RAM needed to run these models, but not very much.</p>
</div>
<div class="readable-text" id="p116">
<h4 class="readable-text-h4 sigil_not_in_toc">Serving the model</h4>
</div>
<div class="readable-text" id="p117">
<p>Once you have your model converted and quantized, the command to start the server is the same, except you must add <code>--MMPROJ</code> <code>path/to/mmproj.gguf</code> to the end. This code will allow you to submit images to the model for tasks like performing optical character recognition (OCR), where we convert text in the image to actual text. Let’s do that now:</p>
</div>
<div class="browsable-container listing-container" id="p118">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$./server -m ./models/llava/llava-v1.6-mistral-7b-q4_k_m.gguf --host 
<span class="">↪</span> $PI_IP_ADDRESS --api-key $API_KEY --MMPROJ ./models/llava/mmproj.gguf</pre>
</div>
</div>
<div class="readable-text" id="p119">
<p>Now that our server knows what to do with images, let’s send it in a request. In line with the OpenAI API we used to chat with the language-only model before, another version shows you how to call a multimodal chat. The code is very similar to listing 11.1 since all we are doing is adding some image support. Like the last listing, we use the OpenAI API to access our LLM backend, but we will change the base URL to our model. The main difference is that we are serializing the image into a string so that it can be sent in the object with a couple of imports to facilitate that using the <code>encode_image</code> function. The only other big change is adding the encoded image to the content section of the messages we send.</p>
</div>
<div class="browsable-container listing-container" id="p120">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.2</span> OpenAI but multimodal GPT-4</h5>
<div class="code-area-container">
<pre class="code-area">import openai

import base64
from io import BytesIO
from PIL import Image


def encode_image(image_path, max_image=512):
    with Image.open(image_path) as img:
        width, height = img.size
        max_dim = max(width, height)
        if max_dim &gt; max_image:
            scale_factor = max_image / max_dim
            new_width = int(width * scale_factor)
            new_height = int(height * scale_factor)
            img = img.resize((new_width, new_height))

        buffered = BytesIO()
        img.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return img_str


client = openai.OpenAI(
    base_url="http://0.0.0.0:1234/v1",  
    api_key="1234",    <span class="aframe-location"/> #1
)
image_file = "myImage.jpg"
max_size = 512  
encoded_string = encode_image(image_file, max_size)     <span class="aframe-location"/> #2

completion = client.chat.completions.with_raw_response.create(
    model="gpt-4-vision-preview",
    messages=[
        {
            "role": "system",
            "content": "You are an expert at analyzing images with computer vision. In case of error,\nmake a full report of the cause of: any issues in receiving, understanding, or describing images",
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Building a website can be done in 10 simple steps:",
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{encoded_string}"
                    },
                },
            ],
        },
    ],
    max_tokens=500,
)

chat = completion.parse()
print(chat.choices[0].message.content)</pre>
<div class="code-annotations-overlay-container">
     #1 Replace with your server’s IP address and port.
     <br/>#2 Set to the maximum dimension to allow (512=1 tile, 2048=max).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p121">
<p>Nothing too fancy or all that different from the many other times we’ve sent requests to servers. One little gotcha with this code that you should keep in mind is that the API will throw an error if you don’t use an API key, but if you don’t set one on the server, you can pass anything, and it won’t error out.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>And that’s it! We’ve now turned our language model into one that can also take images as input, and we have served it onto a Raspberry Pi and even queried it. At least, we hope you queried it because if you didn’t, let us tell you, it is very <em>slow</em>! When you run the multimodal server on the Pi, it will take dozens of minutes to encode and represent the image before even getting to the tokens per second that people generally use to measure the speed of generation. Once again, just because we can deploy these models to small devices doesn’t mean you’ll want to. This is the point where we’re going to recommend again that you should not actually be running this on a Pi, even in your house, if you want to actually get good use out of it. </p>
</div>
<div class="readable-text" id="p123">
<h3 class="readable-text-h3" id="sigil_toc_id_180"><span class="num-string">11.4.4</span> Serving the model on Google Colab</h3>
</div>
<div class="readable-text" id="p124">
<p>Now that we’ve done a couple of these exercises, how can we improve and extend this project for your production environment? The first improvement is obvious: hardware. Single-board RAM compute isn’t incredibly helpful when you have hundreds of customers; however, it is incredibly useful for testing, especially when you don’t want to waste money debugging production for your on-prem deployment. Other options for GPU support also exist, and luckily, all the previously discussed steps, minus the RPi setup, work on Google Colab’s free tier. Here are all of the setup steps that are different:</p>
</div>
<ol>
<li class="readable-text" id="p125"> Setting up llama.cpp: </li>
</ol>
<div class="browsable-container listing-container" id="p126">
<div class="code-area-container code-area-with-html">
<pre class="code-area">!git clone https://github.com/ggerganov/llama.cpp &amp;&amp; cd 
<span class="">↪</span> llama.cpp &amp;&amp; make -j LLAMA_CUBLAS=1</pre>
</div>
</div>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p127"><span class="faux-ol-li-counter">2. </span> Downloading from Hugging Face: </li>
</ol>
<div class="browsable-container listing-container" id="p128">
<div class="code-area-container code-area-with-html">
<pre class="code-area">import os
os.environ[“HF_HUB_ENABLE_HF_TRANSFER”] = “1”
!huggingface-cli download repo/model_name name_of_downloaded_
<span class="">↪</span> model --local-dir . --local-dir-use-symlinks False</pre>
</div>
</div>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p129"><span class="faux-ol-li-counter">3. </span> Server command: </li>
</ol>
<div class="browsable-container listing-container" id="p130">
<div class="code-area-container">
<pre class="code-area">!./server -m content/model/path --log-disable --port 1337</pre>
</div>
</div>
<ol class="faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p131"><span class="faux-ol-li-counter">4. </span> Accessing the server: </li>
</ol>
<div class="browsable-container listing-container" id="p132">
<div class="code-area-container">
<pre class="code-area">from .googlecolab.output import eval_js
print(eval_js(“google.colab.kernel.proxyPort(1337)”))</pre>
</div>
</div>
<div class="readable-text" id="p133">
<p>As you can see, the steps are mostly the same, but because we are working in a Jupyter environment, some slight changes are necessary, as it’s often easier to run code directly instead of running a CLI command. We didn’t go into it, but Raspberry Pis can use <code>docker.io</code> and other packages to create docker images that you can use for responsible CI/CD. It’s a bit harder in a Google Colab environment. Also, keep in mind that Google won’t give you unlimited GPU time, and it goes so far as to monitor whether you have Colab open to turn off your free GPU “efficiently,” so make sure you’re only using those free resources for testing and debugging. No matter how you look at it, free GPUs are a gift, and we should be responsible with them.</p>
</div>
<div class="readable-text intended-text" id="p134">
<p>You can also skip downloading the whole repo and running Make every time. You can use the llama.cpp Python bindings. And you can <code>pip</code> <code>install</code> with cuBLAS or NEON (for Mac GeForce Mx cards) to use hardware acceleration when pip installing with this command:</p>
</div>
<div class="browsable-container listing-container" id="p135">
<div class="code-area-container">
<pre class="code-area">$ CMAKE_ARGS=”-DLLAMA_CUBLAS=on” FORCE_CMAKE=1 pip install llama-cpp-python</pre>
</div>
</div>
<div class="readable-text" id="p136">
<p>This command abstracts most of the code in llama.cpp into easy-to-use Python bindings. Let’s now go through an example of how to use the Python bindings to make something easy to dockerize and deploy. Working with an API is slightly different from working with an LLM by itself, but luckily, LangChain comes in handy. Its whole library is built around working with the OpenAI API, and we use that API to access our own model!</p>
</div>
<div class="readable-text intended-text" id="p137">
<p>In listing 11.3, we’ll combine what we know about the OpenAI API, llama.cpp Python bindings, and LangChain. We’ll start by setting up our environment variables, and then we’ll use the LangChain <code>ChatOpenAI</code> class and pretend that our server is GPT-3.5-turbo. Once we have those two things, we could be done, but we’ll extend by adding a sentence transformer and a prompt ready for RAG. If you have a dataset you’d like to use for RAG, now is the time to embed it and create a FAISS index. We’ll load your FAISS index and use it to help the model at inference time. Then, tokenize it with tiktoken to make sure we don’t overload our context length.</p>
</div>
<div class="browsable-container listing-container" id="p138">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 11.3</span> OpenAI but not multimodal GPT-4</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">import os
from langchain.chains import LLMChain
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from sentence_transformers import SentenceTransformer
import numpy as np
from datasets import load_dataset
import tiktoken

os.environ["OPENAI_API_KEY"] = "Your API Key"
os.environ[
    "OPENAI_API_BASE"
] = "http://0.0.0.0:1234/v1"    <span class="aframe-location"/> #1
os.environ[
    "OPENAI_API_HOST"
] = "http://0.0.0.0:1234"     <span class="aframe-location"/> #2

llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",     <span class="aframe-location"/> #3
    temperature=0.25,
    openai_api_base=os.environ["OPENAI_API_BASE"],    <span class="aframe-location"/> #4
    openai_api_key=os.environ["OPENAI_API_KEY"],
    max_tokens=500,
    n=1,
)

embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")  <span class="aframe-location"/> #5
tiktoker = tiktoken.encoding_for_model("gpt-3.5-turbo")     <span class="aframe-location"/> #6

prompt_template = """Below is an instruction     <span class="aframe-location"/> #7
that describes a task, 
paired with an input that provides further context. 
Write a response that appropriately completes the request.
    ###Instruction:
    You are an expert python developer.
    Given a question, some conversation history, 
and the closest code snippet we could find for 
the request, give your best suggestion for how 
to write the code needed to answer the User's question.

    ###Input:

    #Question: {question}

    #Conversation History: {conversation_history}

    Code Snippet:
    {code_snippet}


    ###Response:
    """

vectorDB = load_dataset(    <span class="aframe-location"/> #8
    "csv", data_files="your dataset with embeddings.csv", split="train"
)
try:                                                          <span class="aframe-location"/> #9
    vectorDB.load_faiss_index("embeddings", "my_index.faiss")
except:
    print(
    """No faiss index, run vectorDB.add_faiss_index(column='embeddings')
    and vectorDB.save_faiss_index('embeddings', 'my_index.faiss')"""
    )

message_history = []     <span class="aframe-location"/> #10

query = "How can I train an LLM from scratch?"     <span class="aframe-location"/> #11
embedded = embedder.encode(query)
q = np.array(embedded, dtype=np.float32)
_, retrieved_example = vectorDB.get_nearest_examples("embeddings", q, k=1)

formatted_prompt = PromptTemplate(     <span class="aframe-location"/> #12
    input_variables=["question", "conversation_history", "code_snippet"],
    template=prompt_template,
)
chain = LLMChain(llm=llm, prompt=formatted_prompt)   <span class="aframe-location"/> #13

num_tokens = len(          <span class="aframe-location"/> #14
    tiktoker.encode(f"{prompt_template},\n" + "\n".join(message_history) +
<span class="">↪</span> query)
    )
)
while num_tokens &gt;= 4000:
    message_history.pop(0)
    num_tokens = len(
    tiktoker.encode(f"{prompt_template},\n" + "\n".join(message_history) +
<span class="">↪</span> query)
        )
    )
res = chain.run(      <span class="aframe-location"/> #15
    {
    "question": query,
    "conversation_history": message_history,
    "code_snippet": "",
    }
)
message_history.append(f"User: {query}\nLlama: {res}")

print(res)     <span class="aframe-location"/> #16</pre>
<div class="code-annotations-overlay-container">
     #1 Replace with your server’s address and port.
     <br/>#2 Replace with your host IP.
     <br/>#3 This can be anything.
     <br/>#4 Again
     <br/>#5 Embeddings for RAG
     <br/>#6 Tokenization for checking context length quickly
     <br/>#7 Change the prompt to be whatever you want.
     <br/>#8 Here’s a vectorΔB; feel free to drop in a replacement.
     <br/>#9 If you haven’t created a faiss or elasticsearch or usearch index, do it.
     <br/>#10 To keep track of chat history
     <br/>#11 Searches the Vector ΔB
     <br/>#12 Formats the prompt
     <br/>#13 Sets up the actual LLM chain
     <br/>#14 Δon’t overload your context length.
     <br/>#15 Runs RAG with your API
     <br/>#16 We’re just printing; do whatever you need to here.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p139">
<p>So here’s where many of our concepts really come together. The amazing thing is that you really can perform this inference and RAG on a Raspberry Pi; you don’t need a gigantic computer to get good, repeatable results. Compute layered on top of this helps immensely until you get to about 48 GB and can fit full versions of 7B and quantized versions of everything above that; all compute after that ends up getting only marginal gains currently. This field is advancing quickly, so look for new, quicker methods of inferencing larger models on smaller hardware.</p>
</div>
<div class="readable-text intended-text" id="p140">
<p>With that, we’ve got our prototype project up and running. It’s easily extensible in pretty much any direction you’d like, and it conforms to industry standards and uses popular libraries. Add to this, make it better, and if you have expertise you feel isn’t being represented here, share it! This field is new, and interdisciplinary knowledge is how it will be pushed forward.</p>
</div>
<div class="readable-text" id="p141">
<h2 class="readable-text-h2" id="sigil_toc_id_181">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p142"> Running the largest models on the smallest devices demands utilizing every memory-saving technique you can think of, like running a Lite operating system. </li>
<li class="readable-text" id="p143"> The hardest part of setting up a remote Pi for the first time is finding its IP address. </li>
<li class="readable-text" id="p144"> For compute-limited hardware without an accelerator, you will need to compile the model to run on your architecture with a tool like llama.cpp. </li>
<li class="readable-text" id="p145"> In a memory-limited environment, quantization will be required for inference. </li>
<li class="readable-text" id="p146"> Even taking advantage of everything available, running LLMs on edge devices will often result in slower inference than desired. Just because something is possible doesn’t make it practical. </li>
<li class="readable-text" id="p147"> OpenAI’s API, along with all wrappers, can be used to access other models by pointing to a custom endpoint. </li>
<li class="readable-text" id="p148"> Many open source tools are available to improve both the serving of models and the user interface. </li>
<li class="readable-text" id="p149"> Lower quantization equals higher perplexity, even with larger models. </li>
<li class="readable-text" id="p150"> Running multimodal models is also possible on a Raspberry Pi. </li>
<li class="readable-text" id="p151"> The same commands we ran on the Pi can be used to develop in Google Collab or another cloud provider with only slight modifications, making these projects more accessible than ever. </li>
<li class="readable-text" id="p152"> Setup and deployment are often much larger pieces to a successful project than preparing the model. </li>
</ul>
</div></body></html>