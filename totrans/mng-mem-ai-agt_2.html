<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Long-Term Memory: Building Persistent Learning Agents"><div class="chapter" id="ch02_long_term_memory_building_persistent_learning_age_1758256567757093">
      <h1><span class="label">Chapter 2. </span>Long-Term Memory: Building Persistent Learning Agents</h1>
      <p>There’s no single, universal definition for the different types of memory in agent systems—every company seems to have its own take. For example, Anthropic, OpenAI, and Google all use slightly different terminology and approaches. But the more interesting question is: how does an agent actually decide what counts as procedural, semantic, or episodic memory? Or put another way: what gets treated as short-term, long-term, or contextual memory?</p>
      <p>Along these lines, semantic caching might play a big role. Sometimes short-term memories can be promoted to long-term if they’re accessed frequently enough. Conversely, long-term memories that aren’t used much might get summarized, become less detailed, or even be dropped from the system altogether. These are the kinds of trade-offs and decisions that go into designing agent memory. Ultimately, it all comes down to how the system is built to manage and retain information. Since it’s nearly impossible to program an agent to handle every scenario, we rely on constraints and parameters to guide what gets treated as episodic, semantic, or procedural memory.</p>
      <section data-type="sect1" class="pagebreak-before" data-pdf-bookmark="Types of Long-Term Memory"><div class="sect1" id="ch02_types_of_long_term_memory_1758256567757177">
        <h1 class="less_space">Types of Long-Term Memory</h1>
        <p>The industry has converged on three primary types of long-term memory, although implementations vary significantly:</p>
        <dl>
          <dt>Episodic memory </dt>
          <dd>
            <p>Stores specific past experiences and events, functioning like human autobiographical memory. Companies typically implement this through RAG systems on conversation histories, extracting relevant chunks instead of keeping the full history.<sup><a data-type="noteref" id="id64-marker" href="ch02.html#id64">1</a></sup> The common approach uses few-shot example prompting where agents learn from past sequences, with key events, actions, and outcomes logged in structured formats.<sup><a data-type="noteref" id="id65-marker" href="ch02.html#id65">2</a></sup></p>
          </dd>
          <dt>Semantic memory </dt>
          <dd>
            <p>Maintains structured factual knowledge—facts, definitions, rules—implemented through knowledge bases, symbolic AI, or vector embeddings. LLMs extract information from conversations, storing it as user or entity profiles that are retrieved and inserted into system prompts to influence future responses.</p>
          </dd>
          <dt>Procedural memory </dt>
          <dd>
            <p>The least common area, but a growing one, which stores skills, rules, and learned behaviors for automatic task performance. This combines LLM weights, agent code, and system prompts, with some agents updating their own prompts through “reflection” or metaprompting.</p>
          </dd>
        </dl>
        <p>The field of agentic AI is moving away from rigid definitions of memory toward more flexible hybrid approaches, where memories can transition between types based on usage patterns and importance scoring. As Rowan Trollope, CEO of Redis, observes, this mirrors human memory consolidation. Just as our REM cycle compresses information from short-term to long-term memory as we sleep, the goal is to build agents to engage in similar processes.<sup><a data-type="noteref" id="id66-marker" href="ch02.html#id66">3</a></sup></p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Long-Term Memory in Frameworks"><div class="sect1" id="ch02_long_term_memory_in_frameworks_1758256567757230">
        <h1>Long-Term Memory in Frameworks</h1>
        <p>The practical implementation of long-term memory varies dramatically across frameworks, with each taking a different philosophical approach to the challenge:</p>
        <dl>
          <dt>LangGraph Stores </dt>
          <dd>
            <p>From the creators of LangChain, a framework for LLM and agent development, this system organizes memory in namespaces as JSON documents with unique identifiers, supporting semantic facts, user preferences, episodic examples, and procedural system prompts. The LangMem SDK adds tools for extracting information from conversations, optimizing prompts, and maintaining persistent memory.<sup><a data-type="noteref" id="id67-marker" href="ch02.html#id67">4</a></sup></p>
          </dd>
          <dt>Mem0 (Memory-Zero) </dt>
          <dd>
            <p>This framework extracts key facts from interactions and updates long-term memory selectively. Rather than storing complete chat histories, it maintains concise entries to reduce memory usage and improve retrieval speed. Users can add a graph-based extension, Mem0g, that maps relationships between entities for additional context.<sup><a data-type="noteref" id="id68-marker" href="ch02.html#id68">5</a></sup></p>
          </dd>
          <dt>Redis Semantic Caching (LangCache) </dt>
          <dd>
            <p>This framework addresses the repetitive nature of agent queries through semantic caching. The implementation includes configurable search criteria, a REST API, and user-specific security features. At the time of writing, LangCache is in private preview, but it should soon be available to the general public.<sup><a data-type="noteref" id="id69-marker" href="ch02.html#id69">6</a></sup></p>
          </dd>
          <dt>ADK MemoryService (Google’s Agent Development Kit) </dt>
          <dd>
            <p>This framework provides a BaseMemoryService interface with two primary functions: adding completed sessions to storage and searching stored information. Developers can choose between InMemoryService for RAM-based keyword searches (nonpersistent) or VertexAIMemoryBankService for production environments with persistent semantic search.</p>
          </dd>
        </dl>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Technologies and Solutions for Advanced Memory Management"><div class="sect1" id="ch02_technologies_and_solutions_for_advanced_memory_man_1758256567757281">
        <h1>Technologies and Solutions for Advanced <span class="keep-together">Memory Management</span></h1>
        <p>The landscape of agent memory technologies reflects a fundamental tension: the need for both sophistication and simplicity, performance and flexibility, innovation and reliability. Each solution represents a different philosophy about how agents should <span class="keep-together">remember</span>.</p>
        <p>Each framework reflects different priorities and philosophies:</p>
        <dl>
          <dt>LangGraph </dt>
          <dd>
            <p>Seeks simplicity with its document store approach. Easy integration and namespace organization make it ideal for rapid prototyping and standard agent workflows, though it lacks the sophistication of specialized solutions.</p>
          </dd>
          <dt>ADK MemoryService </dt>
          <dd>
            <p>Provides enterprise-grade reliability with clear interfaces and Vertex AI integration. The trade-off is its limitation to the <span class="keep-together">Google</span> ecosystem—perfect for Google Cloud deployments but less flexible for multicloud architectures.</p>
          </dd>
          <dt>Vector database alternatives </dt>
          <dd>
            <p>Each serves varying levels of demand. Pinecone offers managed services that are excellent for scale. Weaviate provides open source with hybrid search capabilities. Qdrant focuses on performance with advanced filtering. Chroma remains lightweight and developer friendly. Pgvector leverages familiar PostgreSQL tooling for teams already invested in that ecosystem. And Redis leverages its ubiquity as a caching system by adding RediSearch capabilities for indexing and search. </p>
          </dd>
        </dl>
        <p>The choice of a vector database isn’t simple (<a data-type="xref" href="#ch02_figure_1_1758256567754182">Figure 2-1</a>). Do you need the raw performance of Redis? The intelligent abstraction of Mem0? The simplicity of LangGraph? The enterprise features of ADK? Your answer shapes your agent’s memory architecture and your overall system design.</p>
        <figure><div id="ch02_figure_1_1758256567754182" class="figure">
          <img alt="Diagram illustrating the agent memory architecture, showing the process from input processing with named-entity recognition, through embedding and scoring, to retrieval from a vector database using cosine similarity for efficient memory storage and contextual retrieval." src="assets/mmai_0201.png" width="1265" height="1546"/>
          <h6><span class="label">Figure 2-1. </span>Scoring and retrieval outline</h6>
        </div></figure>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Enhancing Memory Accuracy with Named-Entity Recognition"><div class="sect1" id="ch02_enhancing_memory_accuracy_with_named_entity_recogn_1758256567757331">
        <h1>Enhancing Memory Accuracy with <span class="keep-together">Named-Entity Recognition</span></h1>
        <p>Named-entity recognition (NER) transforms the fuzzy world of natural language into the structured precision that computers—and agents—can reliably work with. In agent memory systems, NER isn’t just a nice-to-have feature; it’s becoming essential for accurate, retrievable memory.</p>
        <section data-type="sect2" data-pdf-bookmark="The NER Pipeline and Its Role in Memory"><div class="sect2" id="ch02_the_ner_pipeline_and_its_role_in_memory_1758256567757382">
          <h2>The NER Pipeline and Its Role in Memory</h2>
          <p>NER is a technique that helps computers pick out and label important things—such as people, places, organizations, or dates—from plain text. Think of it as a way for an agent to read a sentence and automatically highlight the names, companies, or locations mentioned, turning messy language into structured data.</p>
          <p>Why does this matter for agents? When an agent can reliably identify and tag entities in conversations or documents, it can organize and retrieve information much more effectively. For example, an agent might use NER to keep track of which people were mentioned in a meeting, pull up all previous discussions about a specific project, or link related facts across different conversations. This structured approach makes memory retrieval more accurate and allows agents to answer questions like “What did John say about the budget?” or “When was the last time we discussed Paris?” with much greater precision.</p>
          <p>Modern NER systems are impressively accurate, especially when fine-tuned for conversational AI.<sup><a data-type="noteref" id="id70-marker" href="ch02.html#id70">7</a></sup> Some agents go even further, using NER-enhanced pipelines to store entities as metadata, combine semantic and entity-based search, and maintain relationships between entities over time. This turns fuzzy, unstructured memory into something agents can more easily reason about and use.</p>
          <p>NER can undergo the following three phases:</p>
          <dl>
            <dt>Entity extraction</dt>
            <dd>
              <p>Identifies people, locations, organizations, dates, and custom entities, tagging each with confidence scores and linking entities across conversation turns</p>
            </dd>
            <dt>Memory storage enhancement</dt>
            <dd>
              <p>Stores entities as structured metadata alongside embeddings, enables hybrid search combining semantic and entity-based approaches, and creates entity-centric memory indexes</p>
            </dd>
            <dt>Retrieval improvement</dt>
            <dd>
              <p>Allows for querying memories by specific entities, filtering results by entity type, and maintaining entity relationships over time</p>
            </dd>
          </dl>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Improving Memory Retrieval and Accuracy with Structured Data"><div class="sect2" id="ch02_improving_memory_retrieval_and_accuracy_with_struc_1758256567757447">
          <h2>Improving Memory Retrieval and Accuracy with Structured Data</h2>
          <p>NER isn’t just about labeling words—it’s about making memory search smarter and more precise. For example, if you ask an agent, “What did John say about the budget?” NER lets the agent filter its memory for both the person (John) and the topic (budget) instead of just doing a broad keyword search. This can shrink the search space dramatically and make answers more relevant.<sup><a data-type="noteref" id="id71-marker" href="ch02.html#id71">8</a></sup></p>
          <p>Beyond simple filtering, NER helps agents build knowledge graphs—connecting people, places, and things and tracking how they relate over time. This reduces confusion (such as distinguishing “Apple” the company from “apple” the fruit) and helps agents resolve pronouns or ambiguous references.</p>
          <p>In practice, many production systems already use NER for better memory: Redis Agent Memory Server, Mem0g, and LangChain all include entity recognition or entity memory modules. As the field evolves, NER has begun to expand to handle not just text but also images and audio as well as to adapt to new domains with minimal training.</p>
          <p>Structured data from NER turns vague, fuzzy memory into something agents can actually use and reason about—making them more helpful and accurate in real-world scenarios.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="ch02_summary_1758256567757517">
        <h1>Summary</h1>
        <p>Before moving into economic considerations, let’s recap what we’ve learned about memory so far. </p>
        <p>Agent memory is the process of processing, interpreting, storing, and retrieving data dynamically. This simple statement, which opened our exploration, has big implications for how we build and think about AI agents. Throughout this chapter, we’ve seen how this idea shapes everything about agent memory systems—from the way we draw inspiration from human cognition to the nuts and bolts of real-world implementations.</p>
        <p>The real challenge isn’t just technical: it’s about embracing the unpredictability and flexibility that agents require. Agents aren’t just databases with a fancy interface; they’re dynamic, adaptive systems that need to balance remembering, forgetting, and learning on <span class="keep-together">the fly</span>.</p>
        <p>If there’s one takeaway, it’s that building agent memory isn’t about stuffing more data into bigger context windows or chasing the latest database. It’s about designing systems that can manage information intelligently—knowing what to keep, what to compress, and what to let go. The future of agent memory will be shaped by how well we can give our agents the ability to adapt, prioritize, and make sense of the information they encounter.</p>
        <p>In the end, memory is just data—but when managed well, it becomes knowledge, and when applied dynamically, it becomes intelligence. That’s the real promise of agent memory systems: turning raw data into something agents and humans can actually use, one interaction at a time.</p>
      </div></section>
    <div data-type="footnotes"><p data-type="footnote" id="id64"><sup><a href="ch02.html#id64-marker">1</a></sup> Cole Stryker, “What Is AI Agent Memory?” IBM, accessed September 8, 2025, <a href="https://www.ibm.com/think/topics/ai-agent-memory"><em class="hyperlink">https://www.ibm.com/think/topics/ai-agent-memory</em></a>.</p><p data-type="footnote" id="id65"><sup><a href="ch02.html#id65-marker">2</a></sup> “Memory: Agent Development Kit,” Google ADK Documentation, <a href="https://google.github.io/adk-docs/sessions/memory" class="orm:hideurl"><em class="hyperlink">https://google.​git⁠hub.​io/adk-docs/sessions/memory</em></a>.</p><p data-type="footnote" id="id66"><sup><a href="ch02.html#id66-marker">3</a></sup> Rowan Trollope, personal interview, July 11, 2025.</p><p data-type="footnote" id="id67"><sup><a href="ch02.html#id67-marker">4</a></sup> “Launching Long-Term Memory Support in LangGraph,” LangChain Blog, October 8, 2024, <a href="https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/"><em class="hyperlink">https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/</em></a>; “LangGraph Memory Concepts,” LangChain Documentation, <a href="https://langchain-ai.github.io/langgraph/concepts/memory/"><em class="hyperlink">https://langchain-ai.github.io/langgraph/concepts/memory/</em></a>; “LangMem SDK for Agent Long-Term Memory,” LangChain Blog, February 18, 2025, <a href="https://blog.langchain.com/langmem-sdk-launch/"><em class="hyperlink">https://blog.langchain.com/langmem-sdk-launch/</em></a>.</p><p data-type="footnote" id="id68"><sup><a href="ch02.html#id68-marker">5</a></sup> Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav, “Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory,” arXiv:2504.19413, 2025.</p><p data-type="footnote" id="id69"><sup><a href="ch02.html#id69-marker">6</a></sup> Rowan Trollope, “Introducing LangCache and Vector Sets, Simple Solutions for High-Performing AI Apps,” Redis Blog, April 8, 2025, <a href="https://redis.io/blog/spring-release-2025/"><em class="hyperlink">https://redis.io/blog/spring-release-2025/</em></a>; Jim Allen Wallace, “Semantic Caching for Faster, Smarter LLM Apps,” Redis Blog, July 9, 2024, <a href="https://redis.io/blog/what-is-semantic-caching/"><em class="hyperlink">https://redis.io/blog/what-is-semantic-caching/</em></a>; Rowan Trollope, personal interview, July 11, 2025.</p><p data-type="footnote" id="id70"><sup><a href="ch02.html#id70-marker">7</a></sup> Imed Keraghel, Stanislas Morbieu, and Mohamed Nadif, “Recent Advances in Named Entity Recognition: A Comprehensive Survey and Comparative Study,” arXiv:2401.10825, 2024.</p><p data-type="footnote" id="id71"><sup><a href="ch02.html#id71-marker">8</a></sup> “Entity-Based Memory Retrieval in Agent Systems,” in <em>Proceedings of AAAI Conference on Artificial Intelligence</em>, 2024.</p></div></div></section></div></div></body></html>