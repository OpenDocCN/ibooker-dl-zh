- en: Appendix B. References and Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: B.1 Chapter 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Custom-built LLMs are able to outperform general-purpose LLMs as a team at
    Bloomberg showed via a version of GPT pretrained on finance data from scratch.
    The custom LLM outperformed ChatGPT on financial tasks while maintaining good
    performance on general LLM benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*BloombergGPT: A Large Language Model for Finance* (2023) by Wu *et al.*, [https://arxiv.org/abs/2303.17564](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Existing LLMs can be adapted and finetuned to outperform general LLMs as well,
    which teams from Google Research and Google DeepMind showed in a medical context:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Towards Expert-Level Medical Question Answering with Large Language Models*
    (2023) by Singhal *et al.*, [https://arxiv.org/abs/2305.09617](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that proposed the original transformer architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Attention Is All You Need* (2017) by Vaswani *et al.*, [https://arxiv.org/abs/1706.03762](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original encoder-style transformer, called BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: '*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*
    (2018) by Devlin *et al.*, [https://arxiv.org/abs/1810.04805](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper describing the decoder-style GPT-3 model, which inspired modern LLMs
    and will be used as a template for implementing an LLM from scratch in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Language Models are Few-Shot Learners* (2020) by Brown *et al.*, [https://arxiv.org/abs/2005.14165](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original vision transformer for classifying images, which illustrates that
    transformer architectures are not only restricted to text inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*
    (2020) by Dosovitskiy *et al.*, [https://arxiv.org/abs/2010.11929](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two experimental (but less popular) LLM architectures that serve as examples
    that not all LLMs need to be based on the transformer architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '*RWKV: Reinventing RNNs for the Transformer Era* (2023) by Peng *et al.*, [https://arxiv.org/abs/2305.13048](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hyena Hierarchy: Towards Larger Convolutional Language Models (2023)* by Poli
    *et al.,* [https://arxiv.org/abs/2302.10866](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mamba: Linear-Time Sequence Modeling with Selective State Spaces* (2023) by
    Gu and Dao, [https://arxiv.org/abs/2312.00752](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta AI''s model is a popular implementation of a GPT-like model that is openly
    available in contrast to GPT-3 and ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Llama 2: Open Foundation and Fine-Tuned Chat Models* (2023) by Touvron *et
    al.*, [https://arxiv.org/abs/2307.09288](abs.html)[1](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For readers interested in additional details about the dataset references in
    section 1.5, this paper describes the publicly available *The Pile* dataset curated
    by Eleuther AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Pile: An 800GB Dataset of Diverse Text for Language Modeling* (2020) by
    Gao *et al.*, [https://arxiv.org/abs/2101.00027](abs.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following paper provides the reference for InstructGPT for finetuning GPT-3,
    which was mentioned in section 1.6 and will be discussed in more detail in chapter
    7:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Training Language Models to Follow Instructions with Human Feedback* (2022)
    by *Ouyang et al.*, [https://arxiv.org/abs/2203.02155](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B.2 Chapter 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Readers who are interested in discussion and comparison of embedding spaces
    with latent spaces and the general notion of vector representations can find more
    information in the first chapter of my book Machine Learning Q and AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine Learning Q and AI* (2023) by Sebastian Raschka, [https://leanpub.com/machine-learning-q-and-ai](machine-learning-q-and-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following paper provides more in-depth discussions of how how byte pair
    encoding is used as a tokenization method:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Machine Translation of Rare Words with Subword Units (2015) by Sennrich
    at al., [https://arxiv.org/abs/1508.07909](abs.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for the byte pair encoding tokenizer used to train GPT-2 was open-sourced
    by OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/openai/gpt-2/blob/master/src/encoder.py](src.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI provides an interactive web UI to illustrate how the byte pair tokenizer
    in GPT models works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://platform.openai.com/tokenizer](platform.openai.com.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For readers interested in coding and training a BPE tokenizer from the ground
    up, Andrej Karpathy''s GitHub repository `minbpe` offers a minimal and readable
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: A minimal implementation of a BPE tokenizer, [https://github.com/karpathy/minbpe](karpathy.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Readers who are interested in studying alternative tokenization schemes that
    are used by some other popular LLMs can find more information in the SentencePiece
    and WordPiece papers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer
    for Neural Text Processing (2018) by Kudo and Richardson, [https://aclanthology.org/D18-2012/](D18-2012.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast WordPiece Tokenization (2020) by Song et al., [https://arxiv.org/abs/2012.15524](abs.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B.3 Chapter 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Readers interested in learning more about Bahdanau attention for RNN and language
    translation can find detailed insights in the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Neural Machine Translation by Jointly Learning to Align and Translate* (2014)
    by Bahdanau, Cho, and Bengio, [https://arxiv.org/abs/1409.0473](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The concept of self-attention as scaled dot-product attention was introduced
    in the original transformer paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Attention Is All You Need* (2017) by Vaswani et al., [https://arxiv.org/abs/1706.03762](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FlashAttentio*n is a highly efficient implementation of self-attention mechanism,
    which accelerates the computation process by optimizing memory access patterns.
    FlashAttention is mathematically the same as the standard self-attention mechanism
    but optimizes the computational process for efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '*FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awarenes*s
    (2022) by Dao *et al.*, [https://arxiv.org/abs/2205.14135](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning*
    (2023) by Dao, [https://arxiv.org/abs/2307.08691](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch implements a function for self-attention and causal attention that
    supports FlashAttention for efficiency. This function is beta and subject to change:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scaled_dot_product_attention` documentation: [https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html](generated.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch also implements an efficient `MultiHeadAttention` class based on the
    `scaled_dot_product` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MultiHeadAttention` documentation: [https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html](generated.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout is a regularization technique used in neural networks to prevent overfitting
    by randomly dropping units (along with their connections) from the neural network
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dropout: A Simple Way to Prevent Neural Networks from Overfitting* (2014)
    by Srivastava *et al.*, [https://jmlr.org/papers/v15/srivastava14a.html](v15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While using the multi-head attention based on scaled-dot product attention
    remains the most common variant of self-attention in practice, authors found that
    it''s possible to also achieve good performance without the value weight matrix
    and projection layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Simplifying Transformer Blocks* (2023) by He and Hofmann, [https://arxiv.org/abs/2311.01906](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B.4 Chapter 4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The layer normalization paper, titled "Layer Normalization," introduces a technique
    that stabilizes the hidden state dynamics neural networks by normalizing the summed
    inputs to the neurons within a hidden layer, significantly reducing training time
    compared to previously published methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Layer Normalization* (2016) by Ba, Kiros, and Hinton, [https://arxiv.org/abs/1607.06450](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post-LayerNorm, used in the original Transformer model, applies layer normalization
    after the self-attention and feed forward networks. In contrast, Pre-LayerNorm,
    as adopted in models like GPT-2 and newer LLMs, applies layer normalization before
    these components, which can lead to more stable training dynamics and has been
    shown to improve performance in some cases, as discussed in the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*On Layer Normalization in the Transformer Architecture* (2020) by Xiong *et
    al.*, [https://arxiv.org/abs/2002.04745](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ResiDual: Transformer with Dual Residual Connections* (2023) by Tie *et al.*,
    [https://arxiv.org/abs/2304.14802](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A popular variant of LayerNorm used in modern LLMs is RMSNorm due to its improved
    computing efficiency. This variant simplifies the normalization process by normalizing
    the inputs using only the root mean square of the inputs, without subtracting
    the mean before squaring. This means it does not center the data before computing
    the scale. RMSNorm is described in more detail in the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Root Mean Square Layer Normalization* (2019) by Zhang and Sennrich, [https://arxiv.org/abs/1910.07467](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GELU (Gaussian Error Linear Unit) activation function combines the properties
    of both the classic ReLU activation function and the normal distribution''s cumulative
    distribution function to model layer outputs, allowing for stochastic regularization
    and non-linearities in deep learning models, as introduced in the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gaussian Error Linear Units (GELUs)* (2016) by Hendricks and Gimpel, [https://arxiv.org/abs/1606.08415](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GPT-2 paper introduced a series of transformer-based LLMs with varying
    sizes—124M, 355M, 774M, and 1.5B parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Language Models are Unsupervised Multitask Learners* (2019) by Radford *et
    al.*, [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](better-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI''s GPT-3 uses fundamentally the same architecture as GPT-2, except that
    the largest version (175 billion) is 100x larger than the largest GPT-2 model
    and has been trained on much more data. Interested readers can refer to the official
    GPT-3 paper by OpenAI and the technical overview by Lambda Labs, which calculates
    that training GPT-3 on a single RTX 8000 consumer GPU would take 665 years:'
  prefs: []
  type: TYPE_NORMAL
- en: Language Models are Few-Shot Learners (2023) by Brown et al., [https://arxiv.org/abs/2005.14165](abs.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI''s GPT-3 Language Model: A Technical Overview, [https://lambdalabs.com/blog/demystifying-gpt-3](blog.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NanoGPT is a code repository with a minimalist yet efficient implementation
    of a GPT-2 model, similar to the model implemented in this book. While the code
    in this book is different from nanoGPT, this repository inspired the reorganization
    of a large GPT Python parent class implementation into smaller submodules:'
  prefs: []
  type: TYPE_NORMAL
- en: NanoGPT, a repository for training medium-sized GPTs, [https://github.com/karpathy/nanoGPT](karpathy.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An informative blog post showing that most of the computation in LLMs is spent
    in the feed forward layers rather than attention layers when the context size
    is smaller than 32,000 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '"In the long (context) run" by Harm de Vries, [https://www.harmdevries.com/post/context-length/](context-length.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B.5 Chapter 5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A video lecture by the author detailing the loss function and applying a log
    transformation to make it easier to handle for mathematical optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: L8.2 Logistic Regression Loss Function, [https://www.youtube.com/watch?v=GxJe0DZvydM](www.youtube.com.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following two papers detail the dataset, hyperparameter, and architecture
    details used for pretraining LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling
    (2023) by Biderman *et al.*, [https://arxiv.org/abs/2304.01373](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OLMo: Accelerating the Science of Language Models (2024) by Groeneveld *et
    al.*, [https://arxiv.org/abs/2402.00838](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following supplementary code available for this book contains instructions
    for preparing 60,000 public domain books from Project Gutenberg for LLM training:'
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining GPT on the Project Gutenberg Dataset, [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg](ch05.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chapter 5 discusses the pretraining of LLMs, and Appendix D covers more advanced
    training functions, such as linear warmup and cosine annealing. The following
    paper finds that similar techniques can be successfully applied to continue pretraining
    already pretrained LLMs, along with additional tips and insights:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple and Scalable Strategies to Continually Pre-train Large Language Models
    (2024) by Ibrahim *et al.*, [https://arxiv.org/abs/2403.08763](abs.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BloombergGPT is an example of a domain-specific large language model (LLM)
    created by training on both general and domain-specific text corpora, specifically
    in the field of finance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'BloombergGPT: A Large Language Model for Finance (2023) by Wu *et al.*, [https://arxiv.org/abs/2303.17564](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GaLore is a recent research project that aims to make LLM pretraining more efficient.
    The required code change boils down to just replacing PyTorch's `AdamW` optimizer
    in the training function with the `GaLoreAdamW` optimizer provided by the `galore-torch`
    Python package.
  prefs: []
  type: TYPE_NORMAL
- en: 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection (2024)
    by Zhao *et al.*, [https://arxiv.org/abs/2403.03507](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GaLore code repository, [https://github.com/jiaweizzhao/GaLore](jiaweizzhao.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following papers and resources share openly available, large-scale pretraining
    datasets for LLMs that consist of hundreds of gigabytes to terabytes of text data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research
    by Soldaini *et al.* 2024, [https://arxiv.org/abs/2402.00159](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Pile: An 800GB Dataset of Diverse Text for Language Modeling by Gao et
    al. 2020, [https://arxiv.org/abs/2101.00027](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web
    Data, and Web Data Only, by Penedo *et al.* (2023) [https://arxiv.org/abs/2306.01116](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RedPajama by Together AI, [https://github.com/togethercomputer/RedPajama-Data](togethercomputer.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that originally introduced top-k sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Neural Story Generation by Fan *et al.* (2018), [https://arxiv.org/abs/1805.04833](abs.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beam search (not cover in chapter 5) is an alternative decoding algorithm that
    generates output sequences by keeping only the top-scoring partial sequences at
    each step to balance efficiency and quality:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Diverse Beam Sea*rch: Decoding Diverse Solutions from Neural Sequence Models
    by Vijayakumar* et al. (2016), [https://arxiv.org/abs/1610.02424](abs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
