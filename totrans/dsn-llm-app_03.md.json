["```py\n!pip install datasets\nfrom datasets import dataset\nrealnewslike = load_dataset(\"allenai/c4\", \"realnewslike\",\n                            streaming=True, split=\"train\")\nfor i, example in enumerate(realnewslike):\n    if \"Iceland\" in example[\"text\"]:\n        print(example)\n    if i == 10000:  # Limit to 10,000 iterations for demonstration\n        break\n```", "```py\n!pip install justext\n\nimport requests\nimport justext\n\nresponse =\n  requests.get(\"https://en.wikipedia.org/wiki/Toronto_Transit_Commission\")\ntext = justext.justext(response.content, justext.get_stoplist(\"English\"))\nfor content in text:\n  if content.is_boilerplate:\n    print(content.text)\n```", "```py\nJump to content\nMain menu\nMain menu\nNavigation\nMain page\nContents\nCurrent events\nRandom article\nAbout Wikipedia\nContact us\nDonate\nContribute\nHelp\nLearn to edit\n…\n```", "```py\n!pip install langdetect\nfrom langdetect import detect_langs()\ndetect_langs(\"\"\"Pag-uwi ko galing sa paaralan, sobrang pagod ako dahil sa dami\nng aking ginawa sa buong araw. Ang traffic din sa kalsada, nakaka-stress\ntalaga! Pero nang makarating ako sa aking tahanan, nabuhayan ako ng loob dahil\nsa masarap na amoy ng ulam na inihanda ni nanay. Excited na akong kumain\nkasama ang aking pamilya at i-share ang mga kwento ko tungkol sa aking mga\nkaibigan, guro, at mga natutunan ko sa school. After dinner, magre-relax muna\nako habang nanonood ng TV, and then magre-review ng lessons bago matulog. Ito\nang routine ko pag-uwi mula sa school, at masaya ako na dumating sa bahay namay\nnaghihintay na pamilya na handang makinig at suportahan ako sa aking\npag-aaral.\"\"\")\n```", "```py\n[tl:0.9999984631271781]\n```", "```py\ndetect_langs(\"\"\"After a long day at school, pagod na pagod talaga ako. The\ntraffic on the way home didn't help, nakakastress na nga! But upon arriving\nhome, I felt a sense of relief dahil sa welcoming atmosphere and the delicious\naroma of the ulam na inihanda ni Mommy. Excited na akong mag-share ng\nexperiences ko today with my family during dinner, kasama ang mga kwento about\nmy friends, teachers, and interesting lessons sa school. After eating, it's\ntime for me to chill while watching some TV shows, and then review my lessons\nbago ako matulog. This is my daily routine pag-uwi galing school, and I am\ngrateful na may loving family ako na handang makinig at supportahan ako sa\naking educational journey.\"\"\")\n```", "```py\n[en:0.9999954357601804]\n```", "```py\ndetect_langs('I love you too.')\n```", "```py\n[sk:0.8571379760844766, en:0.14285726700161824]\n```", "```py\nfrom model import KenlmModel\nmodel = KenlmModel.from_pretrained(\"wikipedia\", \"en\")\nmodel.get_perplexity(\"She was a shriveling bumblebee, and he was a bumbling `banshee``,` `but` `they` `accepted` `a` `position` `at` `Gringotts` `because` `of` `their` `love` `for`\n`maple` `syrup``\")`\n```", "```py```", "```py```", "``` ssn_pattern = r\"(?!000|666|333)0*(?:[0-6][0-9][0-9]|[0-7][0-6][0-9]| `[``0``-``7``][``0``-``7``][``0``-``2``])[``-`\\ `](``?!``00``)[``0``-``9``]{``2``}[``-`\\ `](``?!``0000``)[``0``-``9``]{``4``}``\"` ```", "````` ```py`Note that detection is not the same as validation. Not all nine-digit numbers of the form XXX-XX-XXXX are SSNs! Validation is the process of checking if a sequence of characters maps to a valid identifier. For example, the Canadian equivalent of SSN, the social insurance number (SIN) contains a checksum digit that can be used to validate it:    ``` from stdnum.ca import sin sin_pattern = re.compile(r\"\\d{3}[-\\ ]\\d{3}[-\\ ]\\d{3}\", flags=re.X) for match in sin_pattern.findall(text):     if sin.is_valid(match):          print(match) ```py    The `is_valid()` function uses the [Luhn checksum algorithm](https://oreil.ly/i34BW) to validate if the sequence of digits maps to a valid SIN. The same algorithm is also used to validate credit cards. Here is the [regex](https://oreil.ly/6uTq-) for detecting credit card numbers:    ``` from stdnum import luhn cc_base_pattern =  r\"\\b \\d (?:\\d[ -]?){14} \\d \\b\" cc_full_pattern = r\"\"\"4[0-9]{12}(?:[0-9]{3})? |  (?:5[1-5][0-9]{2}|222[1-9]|22[3-9][0-9]|2[3-6][0-9]{2}|27[01][0-9]|  2720)[0-9]{12} |  3[47][0-9]{13} |  3(?:0[0-5]|[68][0-9])[0-9]{11} |  6(?:011|5[0-9]{2})[0-9]{12} |  (?:2131|1800|35\\d{3})\\d{11}\"\"\" ```py    The regular expression for detecting email address is as follows:    ``` email_pattern = r\"[\\w\\.=-]+ @ [\\w\\.-]+ \\. [\\w]{2,3}\" ```py    ###### Note    Removing structured PII data while keeping the number of false positives low is hard enough, but detecting and remediating unstructured data is even harder. Due to the complexity of this task and the uncertainty about its impact on the resulting model performance, we decided to not run the Transformer model–based PII pipeline over the ROOTS dataset for training the BLOOM model.```` ```py``  `````", "```py`### PII remediation    Once PII has been detected, it can be remediated. [Figure 2-8](#PII-remediation-options) depicts one of the remediation schemes.  ![PII Remediation Options](assets/dllm_0208.png)  ###### Figure 2-8\\. PII remediation options    Here is a nonexhaustive list of remediation options:    Replace with a special token      For example, a valid phone number can be replaced by the string `<phone` `number>`.      Replace with a random token of the same entity type      For example, replace the name “Clarietta Richards” with “Natasha Bridges,” or any other name.      Replace with a shuffled token      Entities detected across the dataset can be shuffled.      Remove entire document/data source      If the amount of PII detected in a single document or data source is higher than a specific threshold, it is probably best to remove it. For example, *pastebin.com* is said to contain a lot of inadvertently placed PII and is recommended to be not included in training datasets.      Each of these techniques can have a varied effect on the model’s downstream performance. How does replacing tokens affect training perplexity? Are downstream tasks like NER negatively affected when tuned on the resulting model? How does replacement by special tokens compare to replacement with random tokens? This is a relatively underexplored topic, and all these questions are still open.    [Faker](https://oreil.ly/K4QI_) is an excellent library for facilitating random token replacement. It supports random token generation for a variety of PII types including names, addresses, credit card numbers, and phone numbers. One danger in using random tokens is that the replacement process can alter the demographic distribution of the dataset, for example, if the replacement names were all or mostly Anglo-Saxon names. Faker has localization support to enable replacement with fake data from the same geography/culture. Let’s explore the library in more detail:    ```", "```py    This code generates 12-digit fake Aadhaar IDs, which are the Indian equivalent of Social Security numbers. Note that the generated IDs are all invalid but still follow the same format. Similarly:    ```", "```py    generates fake but representative addresses for the selected locale.    ###### Note    Removing PII from training datasets is only one of several solutions to prevent data leakage from models. One promising technique is [differential privacy](https://oreil.ly/TRbsf), which introduces randomness in the inputs or outputs to provide theoretical guarantees for privacy preservation. In neural networks, differential privacy is implemented using the [DP-SGD](https://oreil.ly/DVQkl) algorithm, which involves gradient clipping and noise addition at the end of each update. However, differential privacy significantly slows training, negatively affects model performance, and disproportionately impacts minority groups in the dataset in terms of model utility degradation. Apart from differential privacy, other methods include adversarial training, [model unlearning](https://oreil.ly/_AV3V), [retroactive censoring, and “memfree” decoding](https://oreil.ly/5p0z3).```", "```py``  `` `## Training Set Decontamination    Training set decontamination is a crucial data preprocessing step that helps improve LLM evaluations. A pre-training dataset is said to be contaminated if it contains data from the benchmark test sets used to evaluate its performance. Contamination can happen if the test datasets were constructed from web text, or if the dataset was uploaded on the web after creation. There are two types of contamination:^([1](ch02.html#id625))    Input and label contamination      In this setting, both the questions (inputs) and answers (target labels) exist in the pre-training dataset.      Input contamination      In this setting, only the inputs are present in the pre-training dataset but not the target labels. We will describe the effects of input contamination and how we can leverage it for positive use in [Chapter 7](ch07.html#ch07).      [OpenAI](https://oreil.ly/d7pHK) addressed training set contamination in GPT-3 by finding 13-gram overlaps between text in the test/validation set and the train set, and removing 200 characters before and after the matched texts. The n-gram matching approach is the most commonly used method for decontamination.    However, [Yang et al.](https://oreil.ly/JjtHS) note that contamination can also happen if a rephrased or translation of the benchmark data is present in the training dataset. This makes data contamination very challenging to detect and remove. Most benchmark results continue to be overstated due to this problem.    ## Data Mixtures    Pre-training datasets contain data from a wide variety of domains. The final dataset is prepared such that these domains are represented in optimal proportions. For example, Wikipedia, academic texts, and smaller subsets were [upsampled](https://oreil.ly/hpHdw) by up to three times in The Pile dataset. More involved techniques like [DoReMi](https://oreil.ly/5z9u1) and [RegMix](https://oreil.ly/VWyzt) are also used to calculate the right data mixture. Meta noted that for [Llama 3](https://oreil.ly/fMOrb), it empirically arrived at a data mixture where 50% of the tokens are about general knowledge, 25% are about math and reasoning, 17% represent code, and the remaining are non-English tokens.    ###### Note    Many pre-training datasets these days include code, even if the model is not intended for generating code. [Aryabumi et al.](https://oreil.ly/Vm0lH) have shown that including code in pre-training data significantly improves performance on downstream tasks that do not involve generating code.    Now that we have discussed all the important data collection and preprocessing steps for preparing a pre-training dataset, let’s see how individual datasets differ in terms of the preprocessing steps they have undergone.    ###### Tip    [DataTrove](https://oreil.ly/lDFm2) by Hugging Face is a full-fledged pre-training dataset preprocessing pipeline code repository. You can go through the repo to understand how the concepts introduced in the chapter are implemented at scale.    [Table 2-2](#table2-pretraining-datasets) provides a list of the popular pre-training datasets and the kind of preprocessing they went through.      Table 2-2\\. Pretraining datasets and their preprocessing pipeline   | Name | Extraction and cleaning | Quality filtering | Deduplication | Language identification | Models trained with this dataset | | --- | --- | --- | --- | --- | --- | | C4 | Remove pages containing word in blocklist, remove code, remove short lines and pages | - | Deduplication of 3-sentence spans | langdetect | T5, FLAN-T5, UL2, Llama | | The Pile | justext library for text extraction | fasttext classifier | Document level, with MinHashLSH | pycld2 | GPT-NeoX, GPT-J, Cerebras-GPT, StableLM, Pythia | | CCNet | - | Perplexity filtering | Paragraph-level deduplication | fasttext |  | | RedPajama | CCNet pipeline | Classifier distinguishing between Wikipedia text and random C4 text | Paragraph-level deduplication (for Common Crawl) | fasttext | Red Pajama-INCITE, MPT | | CleanPajama | Low-length filter, NFC normalization | - | MinHashLSH | - | - | | RefinedWeb | URL filtering by blocklists, trafilatura library for text extraction, repetitive content removal | - | Fuzzy document-level deduplication with MinHash, exact sequence-level deduplication | fasttext | Falcon | | ROOTS | Removal of documents with low ratio of closed class words, high ratio of blocklist words, high ratio of character/word repetition | Perplexity filtering | SimHash, Suffix Array | fasttext | BLOOM |` `` ```", "```py ```", "```py`  ```"]