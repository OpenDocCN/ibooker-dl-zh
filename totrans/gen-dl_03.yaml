- en: Chapter 1\. Generative Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is a general introduction to the field of generative modeling.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: We will start with a gentle theoretical introduction to generative modeling
    and see how it is the natural counterpart to the more widely studied discriminative
    modeling. We will then establish a framework that describes the desirable properties
    that a good generative model should have. We will also lay out the core probabilistic
    concepts that are important to know, in order to fully appreciate how different
    approaches tackle the challenge of generative modeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This will lead us naturally to the penultimate section, which lays out the six
    broad families of generative models that dominate the field today. The final section
    explains how to get started with the codebase that accompanies this book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: What Is Generative Modeling?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative modeling can be broadly defined as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Generative modeling is a branch of machine learning that involves training a
    model to produce new data that is similar to a given dataset.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What does this mean in practice? Suppose we have a dataset containing photos
    of horses. We can *train* a generative model on this dataset to capture the rules
    that govern the complex relationships between pixels in images of horses. Then
    we can *sample* from this model to create novel, realistic images of horses that
    did not exist in the original dataset. This process is illustrated in [FigureÂ 1-1](#generative_model).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0101.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. A generative model trained to generate realistic photos of horses
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In order to build a generative model, we require a dataset consisting of many
    examples of the entity we are trying to generate. This is known as the *training
    data*, and one such data point is called an *observation*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Each observation consists of many *features*. For an image generation problem,
    the features are usually the individual pixel values; for a text generation problem,
    the features could be individual words or groups of letters. It is our goal to
    build a model that can generate new sets of features that look as if they have
    been created using the same rules as the original data. Conceptually, for image
    generation this is an incredibly difficult task, considering the vast number of
    ways that individual pixel values can be assigned and the relatively tiny number
    of such arrangements that constitute an image of the entity we are trying to generate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A generative model must also be *probabilistic* rather than *deterministic*,
    because we want to be able to sample many different variations of the output,
    rather than get the same output every time. If our model is merely a fixed calculation,
    such as taking the average value of each pixel in the training dataset, it is
    not generative. A generative model must include a random component that influences
    the individual samples generated by the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we can imagine that there is some unknown probabilistic distribution
    that explains why some images are likely to be found in the training dataset and
    other images are not. It is our job to build a model that mimics this distribution
    as closely as possible and then sample from it to generate new, distinct observations
    that look as if they could have been included in the original training set.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Generative Versus Discriminative Modeling
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to truly understand what generative modeling aims to achieve and why
    this is important, it is useful to compare it to its counterpart, *discriminative
    modeling*. If you have studied machine learning, most problems you will have faced
    will have most likely been discriminative in nature. To understand the difference,
    letâ€™s look at an example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a dataset of paintings, some painted by Van Gogh and some by
    other artists. With enough data, we could train a discriminative model to predict
    if a given painting was painted by Van Gogh. Our model would learn that certain
    colors, shapes, and textures are more likely to indicate that a painting is by
    the Dutch master, and for paintings with these features, the model would upweight
    its prediction accordingly. [FigureÂ 1-2](#discriminative_model) shows the discriminative
    modeling processâ€”note how it differs from the generative modeling process shown
    in [FigureÂ 1-1](#generative_model).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0102.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. A discriminative model trained to predict if a given image is painted
    by Van Gogh
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When performing discriminative modeling, each observation in the training data
    has a *label*. For a binary classification problem such as our artist discriminator,
    Van Gogh paintings would be labeled 1 and nonâ€“Van Gogh paintings labeled 0\. Our
    model then learns how to discriminate between these two groups and outputs the
    probability that a new observation has label 1â€”i.e., that it was painted by Van
    Gogh.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, generative modeling doesnâ€™t require the dataset to be labeled because
    it concerns itself with generating entirely new images, rather than trying to
    predict a label of a given image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s define these types of modeling formally, using mathematical notation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Generative Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we can also build a generative model to model the conditional probability
    <math alttext="p left-parenthesis bold x vertical-bar y right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>ğ±</mi> <mo>|</mo> <mi>y</mi> <mo>)</mo></mrow></math> â€”the probability
    of seeing an observation <math alttext="bold x"><mi>ğ±</mi></math> with a specific
    label <math alttext="y"><mi>y</mi></math> .
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: For example, if our dataset contains different types of fruit, we could tell
    our generative model to specifically generate an image of an apple.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: An important point to note is that even if we were able to build a perfect discriminative
    model to identify Van Gogh paintings, it would still have no idea how to create
    a painting that looks like a Van Gogh. It can only output probabilities against
    existing images, as this is what it has been trained to do. We would instead need
    to train a generative model and sample from this model to generate images that
    have a high chance of belonging to the original training dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The Rise of Generative Modeling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until recently, discriminative modeling has been the driving force behind most
    progress in machine learning. This is because for any discriminative problem,
    the corresponding generative modeling problem is typically much more difficult
    to tackle. For example, it is much easier to train a model to predict if a painting
    is by Van Gogh than it is to train a model to generate a Van Goghâ€“style painting
    from scratch. Similarly, it is much easier to train a model to predict if a page
    of text was written by Charles Dickens than it is to build a model to generate
    a set of paragraphs in the style of Dickens. Until recently, most generative challenges
    were simply out of reach and many doubted that they could ever be solved. Creativity
    was considered a purely human capability that couldnâ€™t be rivaled by AI.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: However, as machine learning technologies have matured, this assumption has
    gradually weakened. In the last 10 years many of the most interesting advancements
    in the field have come through novel applications of machine learning to generative
    modeling tasks. For example, [FigureÂ 1-3](#face_generation) shows the striking
    progress that has already been made in facial image generation since 2014.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0103.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Face generation using generative modeling has improved significantly
    over the last decade (adapted from [Brundage et al., 2018](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf))^([1](ch01.xhtml#idm45387027355040))
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As well as being easier to tackle, discriminative modeling has historically
    been more readily applicable to practical problems across industry than generative
    modeling. For example, a doctor may benefit from a model that predicts if a given
    retinal image shows signs of glaucoma, but wouldnâ€™t necessarily benefit from a
    model that can generate novel pictures of the back of an eye.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ›´å®¹æ˜“å¤„ç†å¤–ï¼Œè¾¨åˆ«å»ºæ¨¡åœ¨å†å²ä¸Šæ¯”ç”Ÿæˆå»ºæ¨¡æ›´å®¹æ˜“åº”ç”¨äºè·¨è¡Œä¸šçš„å®é™…é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒåŒ»ç”Ÿå¯èƒ½ä¼šä»ä¸€ä¸ªå¯ä»¥é¢„æµ‹ç»™å®šè§†ç½‘è†œå›¾åƒæ˜¯å¦æ˜¾ç¤ºé’å…‰çœ¼è¿¹è±¡çš„æ¨¡å‹ä¸­å—ç›Šï¼Œä½†ä¸ä¸€å®šä¼šä»ä¸€ä¸ªå¯ä»¥ç”Ÿæˆçœ¼ç›èƒŒé¢çš„æ–°é¢–å›¾ç‰‡çš„æ¨¡å‹ä¸­å—ç›Šã€‚
- en: However, this is also starting to change, with the proliferation of companies
    offering generative services that target specific business problems. For example,
    it is now possible to access APIs that generate original blog posts given a particular
    subject matter, produce a variety of images of your product in any setting you
    desire, or write social media content and ad copy to match your brand and target
    message. There are also clear positive applications of generative AI for industries
    such as game design and cinematography, where models trained to output video and
    music are beginning to add value.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¹Ÿå¼€å§‹å‘ç”Ÿå˜åŒ–ï¼Œéšç€è¶Šæ¥è¶Šå¤šçš„å…¬å¸æä¾›é’ˆå¯¹ç‰¹å®šä¸šåŠ¡é—®é¢˜çš„ç”ŸæˆæœåŠ¡ã€‚ä¾‹å¦‚ï¼Œç°åœ¨å¯ä»¥è®¿é—®APIï¼Œæ ¹æ®ç‰¹å®šä¸»é¢˜ç”ŸæˆåŸåˆ›åšå®¢æ–‡ç« ï¼Œç”Ÿæˆæ‚¨äº§å“åœ¨ä»»ä½•æ‚¨æƒ³è¦çš„ç¯å¢ƒä¸­çš„å„ç§å›¾ç‰‡ï¼Œæˆ–è€…æ’°å†™ç¤¾äº¤åª’ä½“å†…å®¹å’Œå¹¿å‘Šæ–‡æ¡ˆä»¥åŒ¹é…æ‚¨çš„å“ç‰Œå’Œç›®æ ‡ä¿¡æ¯ã€‚ç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨æ¸¸æˆè®¾è®¡å’Œç”µå½±åˆ¶ä½œç­‰è¡Œä¸šä¹Ÿæœ‰æ˜æ˜¾çš„ç§¯æåº”ç”¨ï¼Œè®­ç»ƒç”¨äºè¾“å‡ºè§†é¢‘å’ŒéŸ³ä¹çš„æ¨¡å‹å¼€å§‹å¢åŠ ä»·å€¼ã€‚
- en: Generative Modeling and AI
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå»ºæ¨¡å’Œäººå·¥æ™ºèƒ½
- en: As well as the practical uses of generative modeling (many of which are yet
    to be discovered), there are three deeper reasons why generative modeling can
    be considered the key to unlocking a far more sophisticated form of artificial
    intelligence that goes beyond what discriminative modeling alone can achieve.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç”Ÿæˆå»ºæ¨¡çš„å®é™…ç”¨é€”ï¼ˆå…¶ä¸­è®¸å¤šå°šæœªè¢«å‘ç°ï¼‰ï¼Œè¿˜æœ‰ä¸‰ä¸ªæ›´æ·±å±‚æ¬¡çš„åŸå› ï¼Œå¯ä»¥è®¤ä¸ºç”Ÿæˆå»ºæ¨¡æ˜¯è§£é”ä¸€ç§æ›´å¤æ‚å½¢å¼çš„äººå·¥æ™ºèƒ½çš„å…³é”®ï¼Œè¶…è¶Šäº†è¾¨åˆ«å»ºæ¨¡å•ç‹¬å¯ä»¥å®ç°çš„èŒƒå›´ã€‚
- en: Firstly, purely from a theoretical point of view, we shouldnâ€™t limit our machine
    training to simply categorizing data. For completeness, we should also be concerned
    with training models that capture a more complete understanding of the data distribution,
    beyond any particular label. This is undoubtedly a more difficult problem to solve,
    due to the high dimensionality of the space of feasible outputs and the relatively
    small number of creations that we would class as belonging to the dataset. However,
    as we shall see, many of the same techniques that have driven development in discriminative
    modeling, such as deep learning, can be utilized by generative models too.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä»ç†è®ºè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ä¸åº”è¯¥å°†æœºå™¨è®­ç»ƒä»…é™äºç®€å•åœ°å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†å®Œæ•´æ€§ï¼Œæˆ‘ä»¬è¿˜åº”è¯¥å…³æ³¨è®­ç»ƒèƒ½å¤Ÿæ•æ‰æ•°æ®åˆ†å¸ƒæ›´å®Œæ•´ç†è§£çš„æ¨¡å‹ï¼Œè¶…è¶Šä»»ä½•ç‰¹å®šæ ‡ç­¾ã€‚è¿™æ— ç–‘æ˜¯ä¸€ä¸ªæ›´éš¾è§£å†³çš„é—®é¢˜ï¼Œå› ä¸ºå¯è¡Œè¾“å‡ºç©ºé—´çš„ç»´åº¦å¾ˆé«˜ï¼Œæˆ‘ä»¬å°†å½’ç±»ä¸ºæ•°æ®é›†çš„åˆ›ä½œæ•°é‡ç›¸å¯¹è¾ƒå°‘ã€‚ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œè®¸å¤šæ¨åŠ¨è¾¨åˆ«å»ºæ¨¡å‘å±•çš„ç›¸åŒæŠ€æœ¯ï¼Œå¦‚æ·±åº¦å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥è¢«ç”Ÿæˆæ¨¡å‹åˆ©ç”¨ã€‚
- en: Secondly, as we shall see in [ChapterÂ 12](ch12.xhtml#chapter_world_models),
    generative modeling is now being used to drive progress in other fields of AI,
    such as reinforcement learning (the study of teaching agents to optimize a goal
    in an environment through trial and error). Suppose we want to train a robot to
    walk across a given terrain. A traditional approach would be to run many experiments
    where the agent tries out different strategies in the terrain, or a computer simulation
    of the terrain. Over time the agent would learn which strategies are more successful
    than others and therefore gradually improve. A challenge with this approach is
    that it is fairly inflexible because it is trained to optimize the policy for
    one particular task. An alternative approach that has recently gained traction
    is to instead train the agent to learn a *world model* of the environment using
    a generative model, independent of any particular task. The agent can quickly
    adapt to new tasks by testing strategies in its own world model, rather than in
    the real environment, which is often computationally more efficient and does not
    require retraining from scratch for each new task.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨[ç¬¬12ç« ](ch12.xhtml#chapter_world_models)ä¸­çœ‹åˆ°ï¼Œç”Ÿæˆå»ºæ¨¡ç°åœ¨è¢«ç”¨äºæ¨åŠ¨å…¶ä»–é¢†åŸŸçš„äººå·¥æ™ºèƒ½è¿›æ­¥ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡è¯•é”™æ¥æ•™å¯¼ä»£ç†ä¼˜åŒ–ç¯å¢ƒä¸­çš„ç›®æ ‡ï¼‰ã€‚å‡è®¾æˆ‘ä»¬æƒ³è®­ç»ƒä¸€ä¸ªæœºå™¨äººåœ¨ç»™å®šåœ°å½¢ä¸Šè¡Œèµ°ã€‚ä¼ ç»Ÿæ–¹æ³•æ˜¯åœ¨ç¯å¢ƒä¸­è¿è¡Œè®¸å¤šå®éªŒï¼Œå…¶ä¸­ä»£ç†å°è¯•ä¸åŒçš„ç­–ç•¥ï¼Œæˆ–è€…åœ¨åœ°å½¢çš„è®¡ç®—æœºæ¨¡æ‹Ÿä¸­å°è¯•ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä»£ç†å°†å­¦ä¼šå“ªäº›ç­–ç•¥æ¯”å…¶ä»–ç­–ç•¥æ›´æˆåŠŸï¼Œå› æ­¤é€æ¸æ”¹è¿›ã€‚è¿™ç§æ–¹æ³•çš„æŒ‘æˆ˜åœ¨äºå®ƒç›¸å½“åƒµåŒ–ï¼Œå› ä¸ºå®ƒè¢«è®­ç»ƒæ¥ä¼˜åŒ–ä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ã€‚æœ€è¿‘å¼€å§‹æµè¡Œçš„å¦ä¸€ç§æ–¹æ³•æ˜¯ï¼Œä»£æ›¿è®­ç»ƒä»£ç†äººå­¦ä¹ ç¯å¢ƒçš„*ä¸–ç•Œæ¨¡å‹*ï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œç‹¬ç«‹äºä»»ä½•ç‰¹å®šä»»åŠ¡ã€‚ä»£ç†å¯ä»¥é€šè¿‡åœ¨è‡ªå·±çš„ä¸–ç•Œæ¨¡å‹ä¸­æµ‹è¯•ç­–ç•¥ï¼Œè€Œä¸æ˜¯åœ¨çœŸå®ç¯å¢ƒä¸­æµ‹è¯•ï¼Œæ¥å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œè¿™é€šå¸¸åœ¨è®¡ç®—ä¸Šæ›´æœ‰æ•ˆï¼Œå¹¶ä¸”ä¸éœ€è¦ä¸ºæ¯ä¸ªæ–°ä»»åŠ¡ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒã€‚
- en: Finally, if we are to truly say that we have built a machine that has acquired
    a form of intelligence that is comparable to a humanâ€™s, generative modeling must
    surely be part of the solution. One of the finest examples of a generative model
    in the natural world is the person reading this book. Take a moment to consider
    what an incredible generative model you are. You can close your eyes and imagine
    what an elephant would look like from any possible angle. You can imagine a number
    of plausible different endings to your favorite TV show, and you can plan your
    week ahead by working through various futures in your mindâ€™s eye and taking action
    accordingly. Current neuroscientific theory suggests that our perception of reality
    is not a highly complex discriminative model operating on our sensory input to
    produce predictions of what we are experiencing, but is instead a generative model
    that is trained from birth to produce simulations of our surroundings that accurately
    match the future. Some theories even suggest that the output from this generative
    model is what we directly perceive as reality. Clearly, a deep understanding of
    how we can build machines to acquire this ability will be central to our continued
    understanding of the workings of the brain and general artificial intelligence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Our First Generative Model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this in mind, letâ€™s begin our journey into the exciting world of generative
    modeling. To begin with, weâ€™ll look at a toy example of a generative model and
    introduce some of the ideas that will help us to work through the more complex
    architectures that we will encounter later in the book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Hello World!
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s start by playing a generative modeling game in just two dimensions. I
    have chosen a rule that has been used to generate the set of points <math alttext="bold
    upper X"><mi>ğ—</mi></math> in [FigureÂ 1-4](#world_map_points). Letâ€™s call this
    rule <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    . Your challenge is to choose a different point <math alttext="bold x equals left-parenthesis
    x 1 comma x 2 right-parenthesis"><mrow><mi>ğ±</mi> <mo>=</mo> <mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>
    in the space that looks like it has been generated by the same rule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![A set of two points in 2-dimensions](Images/gdl2_0104.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. A set of points in two dimensions, generated by an unknown rule
    <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Where did you choose? You probably used your knowledge of the existing data
    points to construct a mental model, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , of whereabouts in the space the point is more likely to be found. In this respect,
    <math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    is an *estimate* of <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    . Perhaps you decided that <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    should look like [FigureÂ 1-5](#world_map_model)â€”a rectangular box where points
    may be found, and an area outside of the box where there is no chance of finding
    any points.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0105.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. The orange box, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , is an estimate of the true data-generating distribution, <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To generate a new observation, you can simply choose a point at random within
    the box, or more formally, *sample* from the distribution <math alttext="p Subscript
    m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    . Congratulations, you have just built your first generative model! You have used
    the training data (the black points) to construct a model (the orange region)
    that you can easily sample from to generate other points that appear to belong
    to the training set.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s now formalize this thinking into a framework that can help us understand
    what generative modeling is trying to achieve.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The Generative Modeling Framework
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can capture our motivations and goals for building a generative model in
    the following framework.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s now reveal the true data-generating distribution, <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    , and see how the framework applies to this example. As we can see from [FigureÂ 1-6](#world_map_model_data),
    the data-generating rule is simply a uniform distribution over the land mass of
    the world, with no chance of finding a point in the sea.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0106.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. The orange box, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , is an estimate of the true data-generating distribution, <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    (the gray area)
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clearly, our model, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , is an oversimplification of <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> . We can inspect
    points A, B, and C to understand the successes and failures of our model in terms
    of how accurately it mimics <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> :'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Point A is an observation that is generated by our model but does not appear
    to have been generated by <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> as itâ€™s in
    the middle of the sea.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Point B could never have been generated by <math alttext="p Subscript m o d
    e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    as it sits outside the orange box. Therefore, our model has some gaps in its ability
    to produce observations across the entire range of potential possibilities.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Point C is an observation that could be generated by <math alttext="p Subscript
    m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    and also by <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    .
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite its shortcomings, the model is easy to sample from, because it is simply
    a uniform distribution over the orange box. We can easily choose a point at random
    from inside this box, in order to sample from it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can certainly say that our model is a simple representation of the
    underlying complex distribution that captures some of the underlying high-level
    features. The true distribution is separated into areas with lots of land mass
    (continents) and those with no land mass (the sea). This is a high-level feature
    that is also true of our model, except we have one large continent, rather than
    many.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: This example has demonstrated the fundamental concepts behind generative modeling.
    The problems we will be tackling in this book will be far more complex and high-dimensional,
    but the underlying framework through which we approach the problem will be the
    same.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Representation Learning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is worth delving a little deeper into what we mean by learning a *representation*
    of the high-dimensional data, as it is a topic that will recur throughout this
    book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you wanted to describe your appearance to someone who was looking for
    you in a crowd of people and didnâ€™t know what you looked like. You wouldnâ€™t start
    by stating the color of pixel 1 of a photo of you, then pixel 2, then pixel 3,
    etc. Instead, you would make the reasonable assumption that the other person has
    a general idea of what an average human looks like, then amend this baseline with
    features that describe groups of pixels, such as *I have very blond hair* or *I
    wear glasses*. With no more than 10 or so of these statements, the person would
    be able to map the description back into pixels to generate an image of you in
    their head. The image wouldnâ€™t be perfect, but it would be a close enough likeness
    to your actual appearance for them to find you among possibly hundreds of other
    people, even if theyâ€™ve never seen you before.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: This is the core idea behind *representation learning*. Instead of trying to
    model the high-dimensional sample space directly, we describe each observation
    in the training set using some lower-dimensional *latent space* and then learn
    a mapping function that can take a point in the latent space and map it to a point
    in the original domain. In other words, each point in the latent space is a *representation*
    of some high-dimensional observation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean in practice? Letâ€™s suppose we have a training set consisting
    of grayscale images of biscuit tins ([FigureÂ 1-7](#biscuit_tins)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0107.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. The biscuit tin dataset
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To us, it is obvious that there are two features that can uniquely represent
    each of these tins: the height and width of the tin. That is, we can convert each
    image of a tin to a point in a latent space of just two dimensions, even though
    the training set of images is provided in high-dimensional pixel space. Notably,
    this means that we can also produce images of tins that do not exist in the training
    set, by applying a suitable mapping function <math alttext="f"><mi>f</mi></math>
    to a new point in the latent space, as shown in [FigureÂ 1-8](#biscuit_tin_generation).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Realizing that the original dataset can be described by the simpler latent space
    is not so easy for a machineâ€”it would first need to establish that height and
    width are the two latent space dimensions that best describe this dataset, then
    learn the mapping function <math alttext="f"><mi>f</mi></math> that can take a
    point in this space and map it to a grayscale biscuit tin image. Machine learning
    (and specifically, deep learning) gives us the ability to train machines that
    can find these complex relationships without human guidance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0108.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. The 2D latent space of biscuit tins and the function <math alttext="f"><mi>f</mi></math>
    that maps a point in the latent space back to the original image domain
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the benefits of training models that utilize a latent space is that we
    can perform operations that affect high-level properties of the image by manipulating
    its representation vector within the more manageable latent space. For example,
    it is not obvious how to adjust the shading of every single pixel to make an image
    of a biscuit tin *taller*. However, in the latent space, itâ€™s simply a case of
    increasing the *height* latent dimension, then applying the mapping function to
    return to the image domain. We shall see an explicit example of this in the next
    chapter, applied not to biscuit tins but to faces.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The concept of encoding the training dataset into a latent space so that we
    can sample from it and decode the point back to the original domain is common
    to many generative modeling techniques, as we shall see in later chapters of this
    book. Mathematically speaking, *encoder-decoder* techniques try to transform the
    highly nonlinear *manifold* on which the data lies (e.g., in pixel space) into
    a simpler latent space that can be sampled from, so that it is likely that any
    point in the latent space is the representation of a well-formed image, as shown
    in [FigureÂ 1-9](#manifold).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒæ•°æ®é›†ç¼–ç åˆ°ä¸€ä¸ªæ½œåœ¨ç©ºé—´ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä»ä¸­è¿›è¡Œé‡‡æ ·å¹¶å°†ç‚¹è§£ç å›åŸå§‹åŸŸçš„æ¦‚å¿µå¯¹äºè®¸å¤šç”Ÿæˆå»ºæ¨¡æŠ€æœ¯æ˜¯å¸¸è§çš„ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ä¹¦çš„åç»­ç« èŠ‚ä¸­çœ‹åˆ°ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œ*ç¼–ç å™¨-è§£ç å™¨*æŠ€æœ¯è¯•å›¾å°†æ•°æ®æ‰€åœ¨çš„é«˜åº¦éçº¿æ€§*æµå½¢*ï¼ˆä¾‹å¦‚ï¼Œåœ¨åƒç´ ç©ºé—´ä¸­ï¼‰è½¬æ¢ä¸ºä¸€ä¸ªæ›´ç®€å•çš„æ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥ä»ä¸­è¿›è¡Œé‡‡æ ·ï¼Œå› æ­¤å¾ˆå¯èƒ½æ½œåœ¨ç©ºé—´ä¸­çš„ä»»ä½•ç‚¹éƒ½æ˜¯ä¸€ä¸ªè‰¯å¥½å½¢æˆçš„å›¾åƒçš„è¡¨ç¤ºï¼Œå¦‚[å›¾1-9](#manifold)æ‰€ç¤ºã€‚
- en: '![](Images/gdl2_0109.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0109.png)'
- en: Figure 1-9\. The *dog* manifold in high-dimensional pixel space is mapped to
    a simpler latent space that can be sampled from
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾1-9\. åœ¨é«˜ç»´åƒç´ ç©ºé—´ä¸­çš„*ç‹—*æµå½¢è¢«æ˜ å°„åˆ°ä¸€ä¸ªæ›´ç®€å•çš„æ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥ä»ä¸­è¿›è¡Œé‡‡æ ·
- en: Core Probability Theory
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒæ¦‚ç‡ç†è®º
- en: We have already seen that generative modeling is closely connected to statistical
    modeling of probability distributions. Therefore, it now makes sense to introduce
    some core probabilistic and statistical concepts that will be used throughout
    this book to explain the theoretical background of each model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°ç”Ÿæˆå»ºæ¨¡ä¸æ¦‚ç‡åˆ†å¸ƒçš„ç»Ÿè®¡å»ºæ¨¡å¯†åˆ‡ç›¸å…³ã€‚å› æ­¤ï¼Œç°åœ¨å¼•å…¥ä¸€äº›æ ¸å¿ƒæ¦‚ç‡å’Œç»Ÿè®¡æ¦‚å¿µæ˜¯æœ‰æ„ä¹‰çš„ï¼Œè¿™äº›æ¦‚å¿µå°†è´¯ç©¿æœ¬ä¹¦ï¼Œç”¨æ¥è§£é‡Šæ¯ä¸ªæ¨¡å‹çš„ç†è®ºèƒŒæ™¯ã€‚
- en: If you have never studied probability or statistics, donâ€™t worry. To build many
    of the deep learning models that we shall see later in this book, it is not essential
    to have a deep understanding of statistical theory. However, to gain a full appreciation
    of the task that we are trying to tackle, itâ€™s worth trying to build up a solid
    understanding of basic probabilistic theory. This way, you will have the foundations
    in place to understand the different families of generative models that will be
    introduced later in this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»æœªå­¦ä¹ è¿‡æ¦‚ç‡æˆ–ç»Ÿè®¡å­¦ï¼Œä¸ç”¨æ‹…å¿ƒã€‚ä¸ºäº†æ„å»ºæœ¬ä¹¦åé¢å°†çœ‹åˆ°çš„è®¸å¤šæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸å¿…å¯¹ç»Ÿè®¡ç†è®ºæœ‰æ·±å…¥çš„ç†è§£ã€‚ç„¶è€Œï¼Œä¸ºäº†å……åˆ†ç†è§£æˆ‘ä»¬è¯•å›¾è§£å†³çš„ä»»åŠ¡ï¼Œå€¼å¾—å°è¯•å»ºç«‹å¯¹åŸºæœ¬æ¦‚ç‡ç†è®ºçš„æ‰å®ç†è§£ã€‚è¿™æ ·ï¼Œæ‚¨å°†æœ‰åŸºç¡€æ¥ç†è§£æœ¬ç« åé¢å°†ä»‹ç»çš„ä¸åŒç±»å‹çš„ç”Ÿæˆæ¨¡å‹ã€‚
- en: 'As a first step, we shall define five key terms, linking each one back to our
    earlier example of a generative model that models the world map in two dimensions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰äº”ä¸ªå…³é”®æœ¯è¯­ï¼Œå°†æ¯ä¸ªæœ¯è¯­ä¸æˆ‘ä»¬ä¹‹å‰åœ¨äºŒç»´ä¸–ç•Œåœ°å›¾ä¸­å»ºæ¨¡çš„ç”Ÿæˆæ¨¡å‹çš„ä¾‹å­è”ç³»èµ·æ¥ï¼š
- en: Sample space
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬ç©ºé—´
- en: The *sample space* is the complete set of all values an observation <math alttext="bold
    x"><mi>ğ±</mi></math> can take.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ ·æœ¬ç©ºé—´*æ˜¯è§‚å¯Ÿå€¼ <math alttext="bold x"><mi>ğ±</mi></math> å¯ä»¥å–çš„æ‰€æœ‰å€¼çš„å®Œæ•´é›†åˆã€‚'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: In our previous example, the sample space consists of all points of latitude
    and longitude <math alttext="bold x equals left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>ğ±</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow></math> on the world map. For example, <math
    alttext="bold x"><mi>ğ±</mi></math> = (40.7306, â€“73.9352) is a point in the sample
    space (New York City) that belongs to the true data-generating distribution. <math
    alttext="bold x"><mi>ğ±</mi></math> = (11.3493, 142.1996) is a point in the sample
    space that does not belong to the true data-generating distribution (itâ€™s in the
    sea).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œæ ·æœ¬ç©ºé—´åŒ…æ‹¬ä¸–ç•Œåœ°å›¾ä¸Šæ‰€æœ‰çš„çº¬åº¦å’Œç»åº¦ç‚¹ <math alttext="bold x equals left-parenthesis
    x 1 comma x 2 right-parenthesis"><mrow><mi>ğ±</mi> <mo>=</mo> <mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>ã€‚ä¾‹å¦‚ï¼Œ<math
    alttext="bold x"><mi>ğ±</mi></math> = (40.7306, â€“73.9352) æ˜¯æ ·æœ¬ç©ºé—´ï¼ˆçº½çº¦å¸‚ï¼‰ä¸­å±äºçœŸå®æ•°æ®ç”Ÿæˆåˆ†å¸ƒçš„ç‚¹ã€‚<math
    alttext="bold x"><mi>ğ±</mi></math> = (11.3493, 142.1996) æ˜¯æ ·æœ¬ç©ºé—´ä¸­ä¸å±äºçœŸå®æ•°æ®ç”Ÿæˆåˆ†å¸ƒçš„ç‚¹ï¼ˆåœ¨æµ·é‡Œï¼‰ã€‚
- en: Probability density function
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡å¯†åº¦å‡½æ•°
- en: A *probability density function* (or simply *density function*) is a function
    <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></math> that maps a point <math alttext="bold x"><mi>ğ±</mi></math>
    in the sample space to a number between 0 and 1\. The integral of the density
    function over all points in the sample space must equal 1, so that it is a well-defined
    probability distribution.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¦‚ç‡å¯†åº¦å‡½æ•°*ï¼ˆæˆ–ç®€ç§°*å¯†åº¦å‡½æ•°*ï¼‰æ˜¯ä¸€ä¸ªå°†æ ·æœ¬ç©ºé—´ä¸­çš„ç‚¹ <math alttext="bold x"><mi>ğ±</mi></math> æ˜ å°„åˆ°0åˆ°1ä¹‹é—´çš„æ•°å­—çš„å‡½æ•°
    <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></math>ã€‚å¯†åº¦å‡½æ•°åœ¨æ ·æœ¬ç©ºé—´ä¸­æ‰€æœ‰ç‚¹ä¸Šçš„ç§¯åˆ†å¿…é¡»ç­‰äº1ï¼Œä»¥ä¾¿å®ƒæ˜¯ä¸€ä¸ªæ˜ç¡®å®šä¹‰çš„æ¦‚ç‡åˆ†å¸ƒã€‚'
- en: Note
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: In the world map example, the density function of our generative model is 0
    outside of the orange box and constant inside of the box, so that the integral
    of the density function over the entire sample space equals 1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸–ç•Œåœ°å›¾çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆæ¨¡å‹çš„å¯†åº¦å‡½æ•°åœ¨æ©™è‰²æ¡†ä¹‹å¤–ä¸º0ï¼Œåœ¨æ¡†å†…ä¸ºå¸¸æ•°ï¼Œä½¿å¾—å¯†åº¦å‡½æ•°åœ¨æ•´ä¸ªæ ·æœ¬ç©ºé—´ä¸Šçš„ç§¯åˆ†ç­‰äº1ã€‚
- en: While there is only one true density function <math alttext="p Subscript d a
    t a Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> that is assumed to have generated the
    observable dataset, there are infinitely many density functions <math alttext="p
    Subscript m o d e l Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> that we can use to estimate <math alttext="p
    Subscript d a t a Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> .
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åªæœ‰ä¸€ä¸ªçœŸå®çš„å¯†åº¦å‡½æ•°<math alttext="p Subscript d a t a Baseline left-parenthesis bold
    x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>è¢«å‡å®šä¸ºç”Ÿæˆå¯è§‚æµ‹æ•°æ®é›†ï¼Œä½†æœ‰æ— é™å¤šä¸ªå¯†åº¦å‡½æ•°<math
    alttext="p Subscript m o d e l Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>å¯ä»¥ç”¨æ¥ä¼°è®¡<math alttext="p Subscript d a
    t a Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>ã€‚
- en: Parametric modeling
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°å»ºæ¨¡
- en: '*Parametric modeling* is a technique that we can use to structure our approach
    to finding a suitable <math alttext="p Subscript m o d e l Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> . A *parametric model*
    is a family of density functions <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> that can be described using a finite
    number of parameters, <math alttext="theta"><mi>Î¸</mi></math> .'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*å‚æ•°å»ºæ¨¡*æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥æ„å»ºæˆ‘ä»¬å¯»æ‰¾é€‚å½“<math alttext="p Subscript m o d e l Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>çš„æ–¹æ³•ã€‚*å‚æ•°æ¨¡å‹*æ˜¯ä¸€ç»„å¯†åº¦å‡½æ•°<math
    alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>ï¼Œå¯ä»¥ç”¨æœ‰é™æ•°é‡çš„å‚æ•°<math
    alttext="theta"><mi>Î¸</mi></math>æ¥æè¿°ã€‚'
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: 'If we assume a uniform distribution as our model family, then the set all possible
    boxes we could draw on [FigureÂ 1-5](#world_map_model) is an example of a parametric
    model. In this case, there are four parameters: the coordinates of the bottom-left
    <math alttext="left-parenthesis theta 1 comma theta 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>Î¸</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> and top-right <math alttext="left-parenthesis theta 3
    comma theta 4 right-parenthesis"><mrow><mo>(</mo> <msub><mi>Î¸</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>Î¸</mi> <mn>4</mn></msub> <mo>)</mo></mrow></math> corners
    of the box.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å‡è®¾å‡åŒ€åˆ†å¸ƒä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹æ—ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨[å›¾1-5](#world_map_model)ä¸Šç»˜åˆ¶çš„æ‰€æœ‰å¯èƒ½æ¡†çš„é›†åˆæ˜¯å‚æ•°æ¨¡å‹çš„ä¸€ä¸ªç¤ºä¾‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ‰å››ä¸ªå‚æ•°ï¼šæ¡†çš„å·¦ä¸‹è§’åæ ‡<math
    alttext="left-parenthesis theta 1 comma theta 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>Î¸</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math>å’Œå³ä¸Šè§’<math alttext="left-parenthesis theta 3 comma theta
    4 right-parenthesis"><mrow><mo>(</mo> <msub><mi>Î¸</mi> <mn>3</mn></msub> <mo>,</mo>
    <msub><mi>Î¸</mi> <mn>4</mn></msub> <mo>)</mo></mrow></math>çš„åæ ‡ã€‚
- en: Thus, each density function <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> in this parametric model (i.e., each
    box) can be uniquely represented by four numbers, <math alttext="theta equals
    left-parenthesis theta 1 comma theta 2 comma theta 3 comma theta 4 right-parenthesis"><mrow><mi>Î¸</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>Î¸</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></math> .
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ä¸ªå‚æ•°æ¨¡å‹ä¸­çš„æ¯ä¸ªå¯†åº¦å‡½æ•°<math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>ï¼ˆå³æ¯ä¸ªæ¡†ï¼‰å¯ä»¥ç”¨å››ä¸ªæ•°å­—<math alttext="theta equals
    left-parenthesis theta 1 comma theta 2 comma theta 3 comma theta 4 right-parenthesis"><mrow><mi>Î¸</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>Î¸</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>Î¸</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></math>å”¯ä¸€è¡¨ç¤ºã€‚
- en: Likelihood
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æ€§
- en: 'The *likelihood* <math alttext="script upper L left-parenthesis theta vertical-bar
    bold x right-parenthesis"><mrow><mi>â„’</mi> <mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></math> of a parameter set <math alttext="theta"><mi>Î¸</mi></math>
    is a function that measures the plausibility of <math alttext="theta"><mi>Î¸</mi></math>
    , given some observed point <math alttext="bold x"><mi>ğ±</mi></math> . It is defined
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°é›†Î¸çš„*å¯èƒ½æ€§*<math alttext="script upper L left-parenthesis theta vertical-bar
    bold x right-parenthesis"><mrow><mi>â„’</mi> <mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ±</mi>
    <mo>)</mo></mrow></math>æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè¡¡é‡ç»™å®šä¸€äº›è§‚å¯Ÿç‚¹<math alttext="bold x"><mi>ğ±</mi></math>çš„Î¸çš„åˆç†æ€§ã€‚å®ƒçš„å®šä¹‰å¦‚ä¸‹ï¼š
- en: <math alttext="script upper L left-parenthesis theta vertical-bar bold x right-parenthesis
    equals p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>â„’</mi>
    <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script upper L left-parenthesis theta vertical-bar bold x right-parenthesis
    equals p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>â„’</mi>
    <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ±</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: 'That is, the likelihood of <math alttext="theta"><mi>Î¸</mi></math> given some
    observed point <math alttext="bold x"><mi>ğ±</mi></math> is defined to be the value
    of the density function parameterized by <math alttext="theta"><mi>Î¸</mi></math>
    , at the point <math alttext="bold x"><mi>ğ±</mi></math> . If we have a whole dataset
    <math alttext="bold upper X"><mi>ğ—</mi></math> of independent observations, then
    we can write:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šä¸€äº›è§‚å¯Ÿç‚¹<math alttext="bold x"><mi>ğ±</mi></math>çš„Î¸çš„å¯èƒ½æ€§è¢«å®šä¹‰ä¸ºç”±<math alttext="theta"><mi>Î¸</mi></math>å‚æ•°åŒ–çš„å¯†åº¦å‡½æ•°åœ¨ç‚¹<math
    alttext="bold x"><mi>ğ±</mi></math>å¤„çš„å€¼ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªå®Œæ•´çš„ç‹¬ç«‹è§‚æµ‹æ•°æ®é›†<math alttext="bold upper
    X"><mi>ğ—</mi></math>ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: <math alttext="script upper L left-parenthesis theta vertical-bar bold upper
    X right-parenthesis equals product Underscript bold x element-of bold upper X
    Endscripts p Subscript theta Baseline left-parenthesis bold x right-parenthesis"
    display="block"><mrow><mi>â„’</mi> <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ—</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ</mo> <mrow><mi>ğ±</mi><mo>âˆˆ</mo><mi>ğ—</mi></mrow></munder>
    <msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script upper L left-parenthesis theta vertical-bar bold upper
    X right-parenthesis equals product Underscript bold x element-of bold upper X
    Endscripts p Subscript theta Baseline left-parenthesis bold x right-parenthesis"
    display="block"><mrow><mi>â„’</mi> <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ—</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ</mo> <mrow><mi>ğ±</mi><mo>âˆˆ</mo><mi>ğ—</mi></mrow></munder>
    <msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: In the world map example, an orange box that only covered the left half of the
    map would have a likelihood of 0â€”it couldnâ€™t possibly have generated the dataset,
    as we have observed points in the right half of the map. The orange box in [FigureÂ 1-5](#world_map_model)
    has a positive likelihood, as the density function is positive for all data points
    under this model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸–ç•Œåœ°å›¾ç¤ºä¾‹ä¸­ï¼Œä¸€ä¸ªåªè¦†ç›–åœ°å›¾å·¦åŠéƒ¨åˆ†çš„æ©™è‰²æ¡†çš„ä¼¼ç„¶ä¸º0â€”å®ƒä¸å¯èƒ½ç”Ÿæˆæ•°æ®é›†ï¼Œå› ä¸ºæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ°å›¾å³åŠéƒ¨åˆ†çš„ç‚¹ã€‚åœ¨[å›¾1-5](#world_map_model)ä¸­çš„æ©™è‰²æ¡†å…·æœ‰æ­£çš„ä¼¼ç„¶ï¼Œå› ä¸ºåœ¨è¯¥æ¨¡å‹ä¸‹æ‰€æœ‰æ•°æ®ç‚¹çš„å¯†åº¦å‡½æ•°éƒ½ä¸ºæ­£ã€‚
- en: 'Since the product of a large number of terms between 0 and 1 can be quite computationally
    difficult to work with, we often use the *log-likelihood* â„“ instead:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº0åˆ°1ä¹‹é—´å¤§é‡é¡¹çš„ä¹˜ç§¯å¯èƒ½ä¼šå¯¼è‡´è®¡ç®—ä¸Šçš„å›°éš¾ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨*log-likelihood* â„“ä»£æ›¿ï¼š
- en: <math alttext="script l left-parenthesis theta vertical-bar bold upper X right-parenthesis
    equals sigma-summation Underscript bold x element-of bold upper X Endscripts log
    p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>â„“</mi>
    <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ—</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>âˆ‘</mo> <mrow><mi>ğ±</mi><mo>âˆˆ</mo><mi>ğ—</mi></mrow></munder> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script l left-parenthesis theta vertical-bar bold upper X right-parenthesis
    equals sigma-summation Underscript bold x element-of bold upper X Endscripts log
    p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>â„“</mi>
    <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ—</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>âˆ‘</mo> <mrow><mi>ğ±</mi><mo>âˆˆ</mo><mi>ğ—</mi></mrow></munder> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
- en: There are statistical reasons why the likelihood is defined in this way, but
    we can also see that this definition intuitively makes sense. The likelihood of
    a set of parameters <math alttext="theta"><mi>Î¸</mi></math> is defined to be the
    probability of seeing the data if the true data-generating distribution was the
    model parameterized by <math alttext="theta"><mi>Î¸</mi></math> .
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ç»Ÿè®¡åŸå› è§£é‡Šä¸ºä»€ä¹ˆä¼¼ç„¶ä»¥è¿™ç§æ–¹å¼å®šä¹‰ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°è¿™ç§å®šä¹‰åœ¨ç›´è§‰ä¸Šæ˜¯æœ‰é“ç†çš„ã€‚ä¸€ç»„å‚æ•°Î¸çš„ä¼¼ç„¶è¢«å®šä¹‰ä¸ºå¦‚æœçœŸå®çš„æ•°æ®ç”Ÿæˆåˆ†å¸ƒæ˜¯ç”±Î¸å‚æ•°åŒ–çš„æ¨¡å‹ï¼Œåˆ™çœ‹åˆ°æ•°æ®çš„æ¦‚ç‡ã€‚
- en: Warning
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: Note that the likelihood is a function of the *parameters*, not the data. It
    should *not* be interpreted as the probability that a given parameter set is correctâ€”in
    other words, it is not a probability distribution over the parameter space (i.e.,
    it doesnâ€™t sum/integrate to 1, with respect to the parameters).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¼¼ç„¶æ˜¯*å‚æ•°*çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯æ•°æ®ã€‚å®ƒä¸åº”è¯¥è¢«è§£é‡Šä¸ºç»™å®šå‚æ•°é›†æ­£ç¡®çš„æ¦‚ç‡â€”æ¢å¥è¯è¯´ï¼Œå®ƒä¸æ˜¯å‚æ•°ç©ºé—´ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼ˆå³ï¼Œå®ƒä¸ä¼šå¯¹å‚æ•°æ±‚å’Œ/ç§¯åˆ†ä¸º1ï¼‰ã€‚
- en: It makes intuitive sense that the focus of parametric modeling should be to
    find the optimal value <math alttext="ModifyingAbove theta With caret"><mover
    accent="true"><mi>Î¸</mi> <mo>^</mo></mover></math> of the parameter set that maximizes
    the likelihood of observing the dataset <math alttext="bold upper X"><mi>ğ—</mi></math>
    .
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°åŒ–å»ºæ¨¡çš„é‡ç‚¹åº”è¯¥æ˜¯æ‰¾åˆ°æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„æ•°æ®é›†ğ—çš„å¯èƒ½æ€§çš„å‚æ•°é›†çš„æœ€ä¼˜å€¼^Î¸ã€‚
- en: Maximum likelihood estimation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§ä¼¼ç„¶ä¼°è®¡
- en: '*Maximum likelihood estimation* is the technique that allows us to estimate
    <math alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>Î¸</mi>
    <mo>^</mo></mover></math> â€”the set of parameters <math alttext="theta"><mi>Î¸</mi></math>
    of a density function <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>Î¸</mi></msub> <mrow><mo>(</mo>
    <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math> that is most likely to explain some
    observed data <math alttext="bold upper X"><mi>ğ—</mi></math> . More formally:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€å¤§ä¼¼ç„¶ä¼°è®¡*æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒå…è®¸æˆ‘ä»¬ä¼°è®¡^Î¸â€”â€”å¯†åº¦å‡½æ•°pÎ¸ï¼ˆğ±ï¼‰çš„å‚æ•°é›†Î¸æœ€æœ‰å¯èƒ½è§£é‡Šä¸€äº›è§‚å¯Ÿåˆ°çš„æ•°æ®ğ—ã€‚æ›´æ­£å¼åœ°è¯´ï¼š'
- en: <math alttext="ModifyingAbove theta With caret equals arg max Underscript bold
    x Endscripts script l left-parenthesis theta vertical-bar bold upper X right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>Î¸</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mrow><mo form="prefix">arg</mo><mo movablelimits="true" form="prefix">max</mo></mrow>
    <mi>ğ±</mi></munder> <mi>â„“</mi> <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ—</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove theta With caret equals arg max Underscript bold
    x Endscripts script l left-parenthesis theta vertical-bar bold upper X right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>Î¸</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mrow><mo form="prefix">arg</mo><mo movablelimits="true" form="prefix">max</mo></mrow>
    <mi>ğ±</mi></munder> <mi>â„“</mi> <mrow><mo>(</mo> <mi>Î¸</mi> <mo>|</mo> <mi>ğ—</mi>
    <mo>)</mo></mrow></mrow></math>
- en: <math alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>Î¸</mi>
    <mo>^</mo></mover></math> is also called the *maximum likelihood estimate* (MLE).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^Î¸ä¹Ÿè¢«ç§°ä¸º*æœ€å¤§ä¼¼ç„¶ä¼°è®¡*ï¼ˆMLEï¼‰ã€‚
- en: Note
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: In the world map example, the MLE is the smallest rectangle that still contains
    all of the points in the training set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸–ç•Œåœ°å›¾ç¤ºä¾‹ä¸­ï¼ŒMLEæ˜¯ä»ç„¶åŒ…å«è®­ç»ƒé›†ä¸­æ‰€æœ‰ç‚¹çš„æœ€å°çŸ©å½¢ã€‚
- en: 'Neural networks typically *minimize* a loss function, so we can equivalently
    talk about finding the set of parameters that *minimize the negative log-likelihood*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œé€šå¸¸*æœ€å°åŒ–*æŸå¤±å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç­‰æ•ˆåœ°è®¨è®ºæ‰¾åˆ°*æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶*çš„å‚æ•°é›†ï¼š
- en: <math alttext="ModifyingAbove theta With caret equals arg min Underscript theta
    Endscripts left-parenthesis minus script l left-parenthesis theta vertical-bar
    bold upper X right-parenthesis right-parenthesis equals arg min Underscript theta
    Endscripts left-parenthesis minus log p Subscript theta Baseline left-parenthesis
    bold upper X right-parenthesis right-parenthesis" display="block"><mrow><mover
    accent="true"><mi>Î¸</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>Î¸</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mi>â„“</mi> <mo>(</mo> <mi>Î¸</mi> <mo>|</mo>
    <mi>ğ—</mi> <mo>)</mo></mfenced> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>Î¸</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ—</mi> <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove theta With caret equals arg min Underscript theta
    Endscripts left-parenthesis minus script l left-parenthesis theta vertical-bar
    bold upper X right-parenthesis right-parenthesis equals arg min Underscript theta
    Endscripts left-parenthesis minus log p Subscript theta Baseline left-parenthesis
    bold upper X right-parenthesis right-parenthesis" display="block"><mrow><mover
    accent="true"><mi>Î¸</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>Î¸</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mi>â„“</mi> <mo>(</mo> <mi>Î¸</mi> <mo>|</mo>
    <mi>ğ—</mi> <mo>)</mo></mfenced> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>Î¸</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ—</mi> <mo>)</mo></mrow></mfenced></mrow></math>
- en: Generative modeling can be thought of as a form of maximum likelihood estimation,
    where the parameters <math alttext="theta"><mi>Î¸</mi></math> are the weights of
    the neural networks contained in the model. We are trying to find the values of
    these parameters that maximize the likelihood of observing the given data (or
    equivalently, minimize the negative log-likelihood).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå»ºæ¨¡å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å½¢å¼ï¼Œå…¶ä¸­å‚æ•°Î¸æ˜¯æ¨¡å‹ä¸­åŒ…å«çš„ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚æˆ‘ä»¬è¯•å›¾æ‰¾åˆ°è¿™äº›å‚æ•°çš„å€¼ï¼Œä»¥æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„ç»™å®šæ•°æ®çš„å¯èƒ½æ€§ï¼ˆæˆ–ç­‰æ•ˆåœ°ï¼Œæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰ã€‚
- en: However, for high-dimensional problems, it is generally not possible to directly
    calculate <math alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
    â€”it is *intractable*. As we shall see in the next section, different families
    of generative models take different approaches to tackling this problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¯¹äºé«˜ç»´é—®é¢˜ï¼Œé€šå¸¸ä¸å¯èƒ½ç›´æ¥è®¡ç®—pÎ¸ï¼ˆğ±ï¼‰â€”å®ƒæ˜¯*éš¾ä»¥è®¡ç®—*çš„ã€‚æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œä¸åŒç±»å‹çš„ç”Ÿæˆæ¨¡å‹é‡‡å–ä¸åŒçš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: Generative Model Taxonomy
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ¨¡å‹åˆ†ç±»
- en: 'While all types of generative models ultimately aim to solve the same task,
    they all take slightly different approaches to modeling the density function <math
    alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>Î¸</mi></msub> <mrow><mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></mrow></math>
    . Broadly speaking, there are three possible approaches:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ‰€æœ‰ç±»å‹çš„ç”Ÿæˆæ¨¡å‹æœ€ç»ˆéƒ½æ—¨åœ¨è§£å†³ç›¸åŒçš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬åœ¨å¯¹å¯†åº¦å‡½æ•°pÎ¸ï¼ˆğ±ï¼‰å»ºæ¨¡æ—¶é‡‡å–äº†ç•¥æœ‰ä¸åŒçš„æ–¹æ³•ã€‚å¹¿ä¹‰ä¸Šè¯´ï¼Œæœ‰ä¸‰ç§å¯èƒ½çš„æ–¹æ³•ï¼š
- en: Explicitly model the density function, but constrain the model in some way,
    so that the density function is tractable (i.e., it can be calculated).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ˜¾å¼åœ°å¯¹å¯†åº¦å‡½æ•°å»ºæ¨¡ï¼Œä½†ä»¥æŸç§æ–¹å¼é™åˆ¶æ¨¡å‹ï¼Œä½¿å¾—å¯†åº¦å‡½æ•°æ˜¯å¯è®¡ç®—çš„ã€‚
- en: Explicitly model a tractable approximation of the density function.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implicitly model the density function, through a stochastic process that directly
    generates data.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are shown in [FigureÂ 1-10](#gm_taxonomy) as a taxonomy, alongside the
    six families of generative models that we will explore in [PartÂ II](part02.xhtml#part_methods)
    of this book. Note that these families are not mutually exclusiveâ€”there are many
    examples of models that are hybrids between two different kinds of approaches.
    You should think of the families as different general approaches to generative
    modeling, rather than explicit model architectures.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0110.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 1-10\. A taxonomy of generative modeling approaches
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first split that we can make is between models where the probability density
    function <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>ğ±</mi> <mo>)</mo></mrow></math> is modeled *explicitly* and those
    where it is modeled *implicitly*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '*Implicit density models* do not aim to estimate the probability density at
    all, but instead focus solely on producing a stochastic process that directly
    generates data. The best-known example of an implicit generative model is a *generative
    adversarial network*. We can further split *explicit density models* into those
    that directly optimize the density function (tractable models) and those that
    only optimize an approximation of it.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '*Tractable models* place constraints on the model architecture, so that the
    density function has a form that makes it easy to calculate. For example, *autoregressive
    models* impose an ordering on the input features, so that the output can be generated
    sequentiallyâ€”e.g., word by word, or pixel by pixel. *Normalizing flow models*
    apply a series of tractable, invertible functions to a simple distribution, in
    order to generate more complex distributions.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '*Approximate density models* include *variational autoencoders*, which introduce
    a latent variable and optimize an approximation of the joint density function.
    *Energy-based models* also utilize approximate methods, but do so via Markov chain
    sampling, rather than variational methods. *Diffusion models* approximate the
    density function by training a model to gradually denoise a given image that has
    been previously corrupted.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: A common thread that runs through all of the generative model family types is
    *deep learning*. Almost all sophisticated generative models have a deep neural
    network at their core, because they can be trained from scratch to learn the complex
    relationships that govern the structure of the data, rather than having to be
    hardcoded with information a priori. Weâ€™ll explore deep learning in [ChapterÂ 2](ch02.xhtml#chapter_deep_learning),
    with practical examples of how to get started building your own deep neural networks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The Generative Deep Learning Codebase
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final section of this chapter will get you set up to start building generative
    deep learning models by introducing the codebase that accompanies this book.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many of the examples in this book are adapted from the excellent open source
    implementations that are available through [the Keras website](https://oreil.ly/1UTwa).
    I highly recommend you check out this resource, as new models and examples are
    constantly being added.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Cloning the Repository
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, youâ€™ll first need to clone the Git repository. *Git* is an open
    source version control system and will allow you to copy the code locally so that
    you can run the notebooks on your own machine, or in a cloud-based environment.
    You may already have this installed, but if not, follow the [instructions relevant
    to your operating system](https://oreil.ly/tFOdN).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'To clone the repository for this book, navigate to the folder where you would
    like to store the files and type the following into your terminal:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`You should now be able to see the files in a folder on your machine.`  `##
    Using Docker'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: The codebase for this book is intended to be used with *Docker*, a free containerization
    technology that makes getting started with a new codebase extremely easy, regardless
    of your architecture or operating system. If you have never used Docker, donâ€™t
    worryâ€”there is a description of how to get started in the *README* file in the
    book repository.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Running on a GPU
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you donâ€™t have access to your own GPU, thatâ€™s also no problem! All of the
    examples in this book will train on a CPU, though this will take longer than if
    you use a GPU-enabled machine. There is also a section in the *README* about setting
    up a Google Cloud environment that gives you access to a GPU on a pay-as-you-go
    basis.`  `# Summary
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduced the field of generative modeling, an important branch
    of machine learning that complements the more widely studied discriminative modeling.
    We discussed how generative modeling is currently one of the most active and exciting
    areas of AI research, with many recent advances in both theory and applications.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: We started with a simple toy example and saw how generative modeling ultimately
    focuses on modeling the underlying distribution of the data. This presents many
    complex and interesting challenges, which we summarized into a framework for understanding
    the desirable properties of any generative model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We then walked through the key probabilistic concepts that will help to fully
    understand the theoretical foundations of each approach to generative modeling
    and laid out the six different families of generative models that we will explore
    in [PartÂ II](part02.xhtml#part_methods) of this book. We also saw how to get started
    with the *Generative Deep Learning* codebase, by cloning the repository.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In [ChapterÂ 2](ch02.xhtml#chapter_deep_learning), we will begin our exploration
    of deep learning and see how to use Keras to build models that can perform discriminative
    modeling tasks. This will give us the necessary foundation to tackle generative
    deep learning problems in later chapters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch01.xhtml#idm45387027355040-marker)) Miles Brundage et al., â€œThe Malicious
    Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,â€ February
    20, 2018, [*https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf*](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf).`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
