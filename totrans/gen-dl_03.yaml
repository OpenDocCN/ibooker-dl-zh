- en: Chapter 1\. Generative Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。生成建模
- en: This chapter is a general introduction to the field of generative modeling.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是对生成建模领域的一般介绍。
- en: We will start with a gentle theoretical introduction to generative modeling
    and see how it is the natural counterpart to the more widely studied discriminative
    modeling. We will then establish a framework that describes the desirable properties
    that a good generative model should have. We will also lay out the core probabilistic
    concepts that are important to know, in order to fully appreciate how different
    approaches tackle the challenge of generative modeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个温和的理论介绍生成建模开始，看看它是更广泛研究的判别建模的自然对应。然后我们将建立一个描述一个好的生成模型应该具有的理想特性的框架。我们还将阐明重要的概率概念，以便充分理解不同方法如何应对生成建模的挑战。
- en: This will lead us naturally to the penultimate section, which lays out the six
    broad families of generative models that dominate the field today. The final section
    explains how to get started with the codebase that accompanies this book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自然地引导我们到倒数第二部分，其中描述了如今主导该领域的六个广泛的生成模型家族。最后一部分解释了如何开始使用本书附带的代码库。
- en: What Is Generative Modeling?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是生成建模？
- en: 'Generative modeling can be broadly defined as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 生成建模可以被广泛定义如下：
- en: Generative modeling is a branch of machine learning that involves training a
    model to produce new data that is similar to a given dataset.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成建模是机器学习的一个分支，涉及训练一个模型来生成类似于给定数据集的新数据。
- en: What does this mean in practice? Suppose we have a dataset containing photos
    of horses. We can *train* a generative model on this dataset to capture the rules
    that govern the complex relationships between pixels in images of horses. Then
    we can *sample* from this model to create novel, realistic images of horses that
    did not exist in the original dataset. This process is illustrated in [Figure 1-1](#generative_model).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这在实践中意味着什么？假设我们有一个包含马照片的数据集。我们可以在这个数据集上*训练*一个生成模型，以捕捉马图片中像素之间复杂关系的规则。然后我们可以从这个模型中*采样*，创建出原始数据集中不存在的新颖、逼真的马的图片。这个过程在[图1-1](#generative_model)中有所说明。
- en: '![](Images/gdl2_0101.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0101.png)'
- en: Figure 1-1\. A generative model trained to generate realistic photos of horses
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。一个生成模型被训练来生成逼真的马的照片
- en: In order to build a generative model, we require a dataset consisting of many
    examples of the entity we are trying to generate. This is known as the *training
    data*, and one such data point is called an *observation*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个生成模型，我们需要一个包含我们试图生成的实体的许多示例的数据集。这被称为*训练数据*，一个这样的数据点被称为*观察*。
- en: Each observation consists of many *features*. For an image generation problem,
    the features are usually the individual pixel values; for a text generation problem,
    the features could be individual words or groups of letters. It is our goal to
    build a model that can generate new sets of features that look as if they have
    been created using the same rules as the original data. Conceptually, for image
    generation this is an incredibly difficult task, considering the vast number of
    ways that individual pixel values can be assigned and the relatively tiny number
    of such arrangements that constitute an image of the entity we are trying to generate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个观察结果由许多*特征*组成。对于图像生成问题，特征通常是单个像素值；对于文本生成问题，特征可以是单个单词或字母组合。我们的目标是构建一个能够生成新特征集的模型，看起来就好像它们是使用与原始数据相同的规则创建的。从概念上讲，对于图像生成来说，这是一个非常困难的任务，考虑到单个像素值可以被分配的方式数量庞大，而构成我们试图生成的实体图像的这种排列的数量相对较少。
- en: A generative model must also be *probabilistic* rather than *deterministic*,
    because we want to be able to sample many different variations of the output,
    rather than get the same output every time. If our model is merely a fixed calculation,
    such as taking the average value of each pixel in the training dataset, it is
    not generative. A generative model must include a random component that influences
    the individual samples generated by the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个生成模型必须是*概率*的，而不是*确定*的，因为我们希望能够对输出进行许多不同的变化采样，而不是每次都得到相同的输出。如果我们的模型仅仅是一个固定的计算，比如在训练数据集中每个像素的平均值，那么它就不是生成的。一个生成模型必须包括一个随机组件，影响模型生成的个体样本。
- en: In other words, we can imagine that there is some unknown probabilistic distribution
    that explains why some images are likely to be found in the training dataset and
    other images are not. It is our job to build a model that mimics this distribution
    as closely as possible and then sample from it to generate new, distinct observations
    that look as if they could have been included in the original training set.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以想象存在某种未知的概率分布，解释为什么一些图像可能在训练数据集中找到，而其他图像则不可能。我们的工作是构建一个尽可能模拟这个分布的模型，然后从中采样生成新的、不同的观察结果，看起来就好像它们可能已经包含在原始训练集中。
- en: Generative Versus Discriminative Modeling
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成模型与判别模型
- en: In order to truly understand what generative modeling aims to achieve and why
    this is important, it is useful to compare it to its counterpart, *discriminative
    modeling*. If you have studied machine learning, most problems you will have faced
    will have most likely been discriminative in nature. To understand the difference,
    let’s look at an example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正理解生成建模的目标以及为什么这很重要，将其与其对应的*判别建模*进行比较是有用的。如果你学过机器学习，你所面对的大多数问题很可能是判别性的。为了理解区别，让我们看一个例子。
- en: Suppose we have a dataset of paintings, some painted by Van Gogh and some by
    other artists. With enough data, we could train a discriminative model to predict
    if a given painting was painted by Van Gogh. Our model would learn that certain
    colors, shapes, and textures are more likely to indicate that a painting is by
    the Dutch master, and for paintings with these features, the model would upweight
    its prediction accordingly. [Figure 1-2](#discriminative_model) shows the discriminative
    modeling process—note how it differs from the generative modeling process shown
    in [Figure 1-1](#generative_model).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组绘画数据，一些是梵高画的，一些是其他艺术家的。有了足够的数据，我们可以训练一个判别模型来预测一幅给定的画是不是梵高画。我们的模型会学习到某些颜色、形状和纹理更有可能表明一幅画是荷兰大师的作品，对于具有这些特征的画作，模型会相应地增加其预测权重。[图1-2](#discriminative_model)展示了判别建模过程——注意它与[图1-1](#generative_model)中展示的生成建模过程的不同之处。
- en: '![](Images/gdl2_0102.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0102.png)'
- en: Figure 1-2\. A discriminative model trained to predict if a given image is painted
    by Van Gogh
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2。一个训练有素的判别模型，用于预测一幅给定图像是否是梵高的画。
- en: When performing discriminative modeling, each observation in the training data
    has a *label*. For a binary classification problem such as our artist discriminator,
    Van Gogh paintings would be labeled 1 and non–Van Gogh paintings labeled 0\. Our
    model then learns how to discriminate between these two groups and outputs the
    probability that a new observation has label 1—i.e., that it was painted by Van
    Gogh.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行判别建模时，训练数据中的每个观察都有一个*标签*。对于像我们的艺术家判别器这样的二元分类问题，梵高的画作将被标记为1，非梵高的画作将被标记为0。然后我们的模型学习如何区分这两组，并输出一个新观察具有标签1的概率——即它是不是梵高画的概率。
- en: In contrast, generative modeling doesn’t require the dataset to be labeled because
    it concerns itself with generating entirely new images, rather than trying to
    predict a label of a given image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，生成建模不需要数据集被标记，因为它关注的是生成全新的图像，而不是试图预测给定图像的标签。
- en: 'Let’s define these types of modeling formally, using mathematical notation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式定义这些类型的建模，使用数学符号：
- en: Conditional Generative Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件生成模型
- en: Note that we can also build a generative model to model the conditional probability
    <math alttext="p left-parenthesis bold x vertical-bar y right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>𝐱</mi> <mo>|</mo> <mi>y</mi> <mo>)</mo></mrow></math> —the probability
    of seeing an observation <math alttext="bold x"><mi>𝐱</mi></math> with a specific
    label <math alttext="y"><mi>y</mi></math> .
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以构建一个生成模型来建模条件概率<math alttext="p left-parenthesis bold x vertical-bar
    y right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>𝐱</mi> <mo>|</mo> <mi>y</mi>
    <mo>)</mo></mrow></math>——观察到具有特定标签<math alttext="y"><mi>y</mi></math>的观察<math
    alttext="bold x"><mi>𝐱</mi></math>的概率。
- en: For example, if our dataset contains different types of fruit, we could tell
    our generative model to specifically generate an image of an apple.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的数据集包含不同类型的水果，我们可以告诉我们的生成模型专门生成一个苹果的图像。
- en: An important point to note is that even if we were able to build a perfect discriminative
    model to identify Van Gogh paintings, it would still have no idea how to create
    a painting that looks like a Van Gogh. It can only output probabilities against
    existing images, as this is what it has been trained to do. We would instead need
    to train a generative model and sample from this model to generate images that
    have a high chance of belonging to the original training dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的观点是，即使我们能够构建一个完美的判别模型来识别梵高的画作，它仍然不知道如何创作一幅看起来像梵高的画。它只能输出针对现有图像的概率，因为这是它被训练做的事情。我们需要训练一个生成模型，并从这个模型中采样，以生成具有高概率属于原始训练数据集的图像。
- en: The Rise of Generative Modeling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成建模的崛起
- en: Until recently, discriminative modeling has been the driving force behind most
    progress in machine learning. This is because for any discriminative problem,
    the corresponding generative modeling problem is typically much more difficult
    to tackle. For example, it is much easier to train a model to predict if a painting
    is by Van Gogh than it is to train a model to generate a Van Gogh–style painting
    from scratch. Similarly, it is much easier to train a model to predict if a page
    of text was written by Charles Dickens than it is to build a model to generate
    a set of paragraphs in the style of Dickens. Until recently, most generative challenges
    were simply out of reach and many doubted that they could ever be solved. Creativity
    was considered a purely human capability that couldn’t be rivaled by AI.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，判别建模一直是机器学习中取得进展的主要驱动力。这是因为对于任何判别问题，相应的生成建模问题通常更难解决。例如，训练一个模型来预测一幅画是否是梵高的比起从头开始训练一个模型生成梵高风格的画作要容易得多。同样，训练一个模型来预测一段文本是否是查尔斯·狄更斯写的比起构建一个模型生成狄更斯风格的段落要容易得多。直到最近，大多数生成挑战都是难以实现的，许多人怀疑它们是否能够被解决。创造力被认为是一种纯粹的人类能力，无法被人工智能所匹敌。
- en: However, as machine learning technologies have matured, this assumption has
    gradually weakened. In the last 10 years many of the most interesting advancements
    in the field have come through novel applications of machine learning to generative
    modeling tasks. For example, [Figure 1-3](#face_generation) shows the striking
    progress that has already been made in facial image generation since 2014.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着机器学习技术的成熟，这种假设逐渐削弱。在过去的10年中，该领域中最有趣的进展之一是通过将机器学习应用于生成建模任务的新颖应用。例如，[图1-3](#face_generation)展示了自2014年以来在面部图像生成方面已经取得的显著进展。
- en: '![](Images/gdl2_0103.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0103.png)'
- en: Figure 1-3\. Face generation using generative modeling has improved significantly
    over the last decade (adapted from [Brundage et al., 2018](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf))^([1](ch01.xhtml#idm45387027355040))
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3。使用生成建模进行人脸生成在过去十年中取得了显著进展（改编自[Brundage et al., 2018](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf))^([1](ch01.xhtml#idm45387027355040))
- en: As well as being easier to tackle, discriminative modeling has historically
    been more readily applicable to practical problems across industry than generative
    modeling. For example, a doctor may benefit from a model that predicts if a given
    retinal image shows signs of glaucoma, but wouldn’t necessarily benefit from a
    model that can generate novel pictures of the back of an eye.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更容易处理外，辨别建模在历史上比生成建模更容易应用于跨行业的实际问题。例如，医生可能会从一个可以预测给定视网膜图像是否显示青光眼迹象的模型中受益，但不一定会从一个可以生成眼睛背面的新颖图片的模型中受益。
- en: However, this is also starting to change, with the proliferation of companies
    offering generative services that target specific business problems. For example,
    it is now possible to access APIs that generate original blog posts given a particular
    subject matter, produce a variety of images of your product in any setting you
    desire, or write social media content and ad copy to match your brand and target
    message. There are also clear positive applications of generative AI for industries
    such as game design and cinematography, where models trained to output video and
    music are beginning to add value.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这也开始发生变化，随着越来越多的公司提供针对特定业务问题的生成服务。例如，现在可以访问API，根据特定主题生成原创博客文章，生成您产品在任何您想要的环境中的各种图片，或者撰写社交媒体内容和广告文案以匹配您的品牌和目标信息。生成人工智能在游戏设计和电影制作等行业也有明显的积极应用，训练用于输出视频和音乐的模型开始增加价值。
- en: Generative Modeling and AI
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成建模和人工智能
- en: As well as the practical uses of generative modeling (many of which are yet
    to be discovered), there are three deeper reasons why generative modeling can
    be considered the key to unlocking a far more sophisticated form of artificial
    intelligence that goes beyond what discriminative modeling alone can achieve.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成建模的实际用途（其中许多尚未被发现），还有三个更深层次的原因，可以认为生成建模是解锁一种更复杂形式的人工智能的关键，超越了辨别建模单独可以实现的范围。
- en: Firstly, purely from a theoretical point of view, we shouldn’t limit our machine
    training to simply categorizing data. For completeness, we should also be concerned
    with training models that capture a more complete understanding of the data distribution,
    beyond any particular label. This is undoubtedly a more difficult problem to solve,
    due to the high dimensionality of the space of feasible outputs and the relatively
    small number of creations that we would class as belonging to the dataset. However,
    as we shall see, many of the same techniques that have driven development in discriminative
    modeling, such as deep learning, can be utilized by generative models too.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从理论角度来看，我们不应该将机器训练仅限于简单地对数据进行分类。为了完整性，我们还应该关注训练能够捕捉数据分布更完整理解的模型，超越任何特定标签。这无疑是一个更难解决的问题，因为可行输出空间的维度很高，我们将归类为数据集的创作数量相对较少。然而，正如我们将看到的，许多推动辨别建模发展的相同技术，如深度学习，也可以被生成模型利用。
- en: Secondly, as we shall see in [Chapter 12](ch12.xhtml#chapter_world_models),
    generative modeling is now being used to drive progress in other fields of AI,
    such as reinforcement learning (the study of teaching agents to optimize a goal
    in an environment through trial and error). Suppose we want to train a robot to
    walk across a given terrain. A traditional approach would be to run many experiments
    where the agent tries out different strategies in the terrain, or a computer simulation
    of the terrain. Over time the agent would learn which strategies are more successful
    than others and therefore gradually improve. A challenge with this approach is
    that it is fairly inflexible because it is trained to optimize the policy for
    one particular task. An alternative approach that has recently gained traction
    is to instead train the agent to learn a *world model* of the environment using
    a generative model, independent of any particular task. The agent can quickly
    adapt to new tasks by testing strategies in its own world model, rather than in
    the real environment, which is often computationally more efficient and does not
    require retraining from scratch for each new task.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，正如我们将在[第12章](ch12.xhtml#chapter_world_models)中看到，生成建模现在被用于推动其他领域的人工智能进步，如强化学习（通过试错来教导代理优化环境中的目标）。假设我们想训练一个机器人在给定地形上行走。传统方法是在环境中运行许多实验，其中代理尝试不同的策略，或者在地形的计算机模拟中尝试。随着时间的推移，代理将学会哪些策略比其他策略更成功，因此逐渐改进。这种方法的挑战在于它相当僵化，因为它被训练来优化一个特定任务的策略。最近开始流行的另一种方法是，代替训练代理人学习环境的*世界模型*，使用生成模型，独立于任何特定任务。代理可以通过在自己的世界模型中测试策略，而不是在真实环境中测试，来快速适应新任务，这通常在计算上更有效，并且不需要为每个新任务从头开始重新训练。
- en: Finally, if we are to truly say that we have built a machine that has acquired
    a form of intelligence that is comparable to a human’s, generative modeling must
    surely be part of the solution. One of the finest examples of a generative model
    in the natural world is the person reading this book. Take a moment to consider
    what an incredible generative model you are. You can close your eyes and imagine
    what an elephant would look like from any possible angle. You can imagine a number
    of plausible different endings to your favorite TV show, and you can plan your
    week ahead by working through various futures in your mind’s eye and taking action
    accordingly. Current neuroscientific theory suggests that our perception of reality
    is not a highly complex discriminative model operating on our sensory input to
    produce predictions of what we are experiencing, but is instead a generative model
    that is trained from birth to produce simulations of our surroundings that accurately
    match the future. Some theories even suggest that the output from this generative
    model is what we directly perceive as reality. Clearly, a deep understanding of
    how we can build machines to acquire this ability will be central to our continued
    understanding of the workings of the brain and general artificial intelligence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们真的要说我们已经建立了一台获得了与人类相媲美的智能形式的机器，生成建模肯定是解决方案的一部分。自然界中最好的生成模型之一就是正在阅读本书的人。花点时间考虑一下你是一个多么不可思议的生成模型。你可以闭上眼睛想象大象从任何可能的角度看起来是什么样子。你可以想象你最喜欢的电视节目有许多不同的可能结局，你可以通过在脑海中思考各种未来并相应地采取行动来计划下周。当前的神经科学理论表明，我们对现实的感知并不是一个高度复杂的辨别模型，它根据我们的感官输入产生我们正在经历的预测，而是一个从出生开始就接受训练以产生准确匹配未来的环境的模型。一些理论甚至暗示，这个生成模型的输出就是我们直接感知为现实的东西。显然，深入了解我们如何构建机器来获得这种能力将是我们继续了解大脑运作和普遍人工智能的核心。
- en: Our First Generative Model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个生成模型
- en: With this in mind, let’s begin our journey into the exciting world of generative
    modeling. To begin with, we’ll look at a toy example of a generative model and
    introduce some of the ideas that will help us to work through the more complex
    architectures that we will encounter later in the book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，让我们开始我们激动人心的生成建模之旅。首先，我们将看一个生成模型的玩具示例，并介绍一些将帮助我们理解本书后面将遇到的更复杂架构的想法。
- en: Hello World!
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你好，世界！
- en: Let’s start by playing a generative modeling game in just two dimensions. I
    have chosen a rule that has been used to generate the set of points <math alttext="bold
    upper X"><mi>𝐗</mi></math> in [Figure 1-4](#world_map_points). Let’s call this
    rule <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    . Your challenge is to choose a different point <math alttext="bold x equals left-parenthesis
    x 1 comma x 2 right-parenthesis"><mrow><mi>𝐱</mi> <mo>=</mo> <mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>
    in the space that looks like it has been generated by the same rule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在只有两个维度中玩一个生成建模游戏开始。我选择了一个用于生成点集<math alttext="bold upper X"><mi>𝐗</mi></math>的规则，如[图1-4](#world_map_points)所示。让我们称这个规则为<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>。你的挑战是选择一个不同的点<math
    alttext="bold x equals left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>𝐱</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow></math>，看起来像是由相同规则生成的。
- en: '![A set of two points in 2-dimensions](Images/gdl2_0104.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![二维空间中的两个点集](Images/gdl2_0104.png)'
- en: Figure 1-4\. A set of points in two dimensions, generated by an unknown rule
    <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4。由未知规则生成的二维空间中的点集<math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
- en: Where did you choose? You probably used your knowledge of the existing data
    points to construct a mental model, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , of whereabouts in the space the point is more likely to be found. In this respect,
    <math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    is an *estimate* of <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    . Perhaps you decided that <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    should look like [Figure 1-5](#world_map_model)—a rectangular box where points
    may be found, and an area outside of the box where there is no chance of finding
    any points.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何选择的？你可能利用现有数据点的知识构建了一个心理模型<math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>，来估计点更有可能出现的位置。在这方面，<math
    alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>是<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>的*估计*。也许你决定<math
    alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>应该看起来像[图1-5](#world_map_model)——一个矩形框，点可能出现在其中，框外则不可能找到任何点。
- en: '![](Images/gdl2_0105.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0105.png)'
- en: Figure 1-5\. The orange box, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , is an estimate of the true data-generating distribution, <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。橙色框，<math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>，是真实数据生成分布<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>的估计
- en: To generate a new observation, you can simply choose a point at random within
    the box, or more formally, *sample* from the distribution <math alttext="p Subscript
    m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    . Congratulations, you have just built your first generative model! You have used
    the training data (the black points) to construct a model (the orange region)
    that you can easily sample from to generate other points that appear to belong
    to the training set.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成一个新的观察结果，您可以简单地在框内随机选择一个点，或者更正式地，从分布<math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>中*采样*。恭喜，您刚刚构建了您的第一个生成模型！您已经使用训练数据（黑点）构建了一个模型（橙色区域），您可以轻松地从中进行采样以生成其他似乎属于训练集的点。
- en: Let’s now formalize this thinking into a framework that can help us understand
    what generative modeling is trying to achieve.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这种思维形式化为一个框架，可以帮助我们理解生成建模试图实现的目标。
- en: The Generative Modeling Framework
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成建模框架
- en: We can capture our motivations and goals for building a generative model in
    the following framework.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下框架中捕捉建立生成模型的动机和目标。
- en: Let’s now reveal the true data-generating distribution, <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    , and see how the framework applies to this example. As we can see from [Figure 1-6](#world_map_model_data),
    the data-generating rule is simply a uniform distribution over the land mass of
    the world, with no chance of finding a point in the sea.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们揭示真实的数据生成分布<math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>，看看这个框架如何适用于这个例子。正如我们从[图1-6](#world_map_model_data)中看到的，数据生成规则只是世界陆地的均匀分布，没有机会在海洋中找到一个点。
- en: '![](Images/gdl2_0106.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0106.png)'
- en: Figure 1-6\. The orange box, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , is an estimate of the true data-generating distribution, <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    (the gray area)
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6。橙色框<math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>是真实数据生成分布<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>的估计（灰色区域）
- en: 'Clearly, our model, <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    , is an oversimplification of <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> . We can inspect
    points A, B, and C to understand the successes and failures of our model in terms
    of how accurately it mimics <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> :'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们的模型<math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>是对<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>的过度简化。我们可以检查点A、B和C，以了解我们的模型在多大程度上准确模拟了<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>的成功和失败：
- en: Point A is an observation that is generated by our model but does not appear
    to have been generated by <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> as it’s in
    the middle of the sea.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点A是由我们的模型生成的观察结果，但似乎并不是由<math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>生成的，因为它位于海洋中间。
- en: Point B could never have been generated by <math alttext="p Subscript m o d
    e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    as it sits outside the orange box. Therefore, our model has some gaps in its ability
    to produce observations across the entire range of potential possibilities.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点B永远不可能由<math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>生成，因为它位于橙色框外。因此，我们的模型在产生观察结果的整个潜在可能性范围内存在一些缺陷。
- en: Point C is an observation that could be generated by <math alttext="p Subscript
    m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    and also by <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    .
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点C是一个观察结果，可以由<math alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>和<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>生成。
- en: Despite its shortcomings, the model is easy to sample from, because it is simply
    a uniform distribution over the orange box. We can easily choose a point at random
    from inside this box, in order to sample from it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型存在缺陷，但由于它只是一个橙色框内的均匀分布，因此很容易从中进行采样。我们可以轻松地随机选择框内的一个点，以便从中进行采样。
- en: Also, we can certainly say that our model is a simple representation of the
    underlying complex distribution that captures some of the underlying high-level
    features. The true distribution is separated into areas with lots of land mass
    (continents) and those with no land mass (the sea). This is a high-level feature
    that is also true of our model, except we have one large continent, rather than
    many.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以肯定地说，我们的模型是对捕捉一些基本高级特征的底层复杂分布的简单表示。真实分布被分为有大量陆地（大陆）和没有陆地（海洋）的区域。这也是我们模型的一个高级特征，除了我们只有一个大陆，而不是很多。
- en: This example has demonstrated the fundamental concepts behind generative modeling.
    The problems we will be tackling in this book will be far more complex and high-dimensional,
    but the underlying framework through which we approach the problem will be the
    same.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了生成建模背后的基本概念。我们在本书中将要解决的问题将会更加复杂和高维，但我们处理问题的基本框架将是相同的。
- en: Representation Learning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示学习
- en: It is worth delving a little deeper into what we mean by learning a *representation*
    of the high-dimensional data, as it is a topic that will recur throughout this
    book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 值得深入探讨一下我们所说的学习高维数据的*表示*是什么意思，因为这是本书中将会反复出现的一个主题。
- en: Suppose you wanted to describe your appearance to someone who was looking for
    you in a crowd of people and didn’t know what you looked like. You wouldn’t start
    by stating the color of pixel 1 of a photo of you, then pixel 2, then pixel 3,
    etc. Instead, you would make the reasonable assumption that the other person has
    a general idea of what an average human looks like, then amend this baseline with
    features that describe groups of pixels, such as *I have very blond hair* or *I
    wear glasses*. With no more than 10 or so of these statements, the person would
    be able to map the description back into pixels to generate an image of you in
    their head. The image wouldn’t be perfect, but it would be a close enough likeness
    to your actual appearance for them to find you among possibly hundreds of other
    people, even if they’ve never seen you before.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想向一个在人群中寻找你并不知道你长什么样的人描述你的外貌。你不会从描述你照片的像素1的颜色开始，然后是像素2，然后是像素3，依此类推。相反，你会合理地假设对方对一个普通人的外貌有一个大致的概念，然后用描述像素组的特征来修正这个基线，比如*我有很金黄的头发*或*我戴眼镜*。只需不超过10条这样的描述，对方就能将描述映射回像素，生成一个你的形象。这个形象可能不完美，但足够接近你的实际外貌，让对方在可能有数百人的人群中找到你，即使他们以前从未见过你。
- en: This is the core idea behind *representation learning*. Instead of trying to
    model the high-dimensional sample space directly, we describe each observation
    in the training set using some lower-dimensional *latent space* and then learn
    a mapping function that can take a point in the latent space and map it to a point
    in the original domain. In other words, each point in the latent space is a *representation*
    of some high-dimensional observation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*表示学习*的核心思想。我们不是直接对高维样本空间建模，而是使用一些较低维度的*潜在空间*来描述训练集中的每个观察，并学习一个能够将潜在空间中的点映射到原始域中的点的映射函数。换句话说，潜在空间中的每个点都是某个高维观察的*表示*。
- en: What does this mean in practice? Let’s suppose we have a training set consisting
    of grayscale images of biscuit tins ([Figure 1-7](#biscuit_tins)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这在实践中意味着什么？假设我们有一个由饼干罐的灰度图像组成的训练集([图1-7](#biscuit_tins))。
- en: '![](Images/gdl2_0107.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0107.png)'
- en: Figure 1-7\. The biscuit tin dataset
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7。饼干罐数据集
- en: 'To us, it is obvious that there are two features that can uniquely represent
    each of these tins: the height and width of the tin. That is, we can convert each
    image of a tin to a point in a latent space of just two dimensions, even though
    the training set of images is provided in high-dimensional pixel space. Notably,
    this means that we can also produce images of tins that do not exist in the training
    set, by applying a suitable mapping function <math alttext="f"><mi>f</mi></math>
    to a new point in the latent space, as shown in [Figure 1-8](#biscuit_tin_generation).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，很明显有两个特征可以唯一代表这些罐子：罐子的高度和宽度。也就是说，我们可以将每个罐子的图像转换为一个仅有两个维度的潜在空间中的点，即使训练集中提供的图像是在高维像素空间中的。值得注意的是，这意味着我们也可以通过将适当的映射函数<math
    alttext="f"><mi>f</mi></math>应用于潜在空间中的新点，生成训练集中不存在的罐子的图像，如[图1-8](#biscuit_tin_generation)所示。
- en: Realizing that the original dataset can be described by the simpler latent space
    is not so easy for a machine—it would first need to establish that height and
    width are the two latent space dimensions that best describe this dataset, then
    learn the mapping function <math alttext="f"><mi>f</mi></math> that can take a
    point in this space and map it to a grayscale biscuit tin image. Machine learning
    (and specifically, deep learning) gives us the ability to train machines that
    can find these complex relationships without human guidance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 意识到原始数据集可以用更简单的潜在空间来描述对于机器来说并不容易——它首先需要确定高度和宽度是最能描述这个数据集的两个潜在空间维度，然后学习能够将这个空间中的点映射到灰度饼干罐图像的映射函数<math
    alttext="f"><mi>f</mi></math>。机器学习（特别是深度学习）赋予我们训练机器能够找到这些复杂关系的能力，而无需人类的指导。
- en: '![](Images/gdl2_0108.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0108.png)'
- en: Figure 1-8\. The 2D latent space of biscuit tins and the function <math alttext="f"><mi>f</mi></math>
    that maps a point in the latent space back to the original image domain
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8。饼干罐的2D潜在空间和将潜在空间中的点映射回原始图像域的函数<math alttext="f"><mi>f</mi></math>
- en: One of the benefits of training models that utilize a latent space is that we
    can perform operations that affect high-level properties of the image by manipulating
    its representation vector within the more manageable latent space. For example,
    it is not obvious how to adjust the shading of every single pixel to make an image
    of a biscuit tin *taller*. However, in the latent space, it’s simply a case of
    increasing the *height* latent dimension, then applying the mapping function to
    return to the image domain. We shall see an explicit example of this in the next
    chapter, applied not to biscuit tins but to faces.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 利用潜在空间训练模型的一个好处是，我们可以通过在更易管理的潜在空间内操作其表示向量，影响图像的高级属性。例如，要调整每个像素的阴影以使饼干罐的图像*更高*并不明显。然而，在潜在空间中，只需增加*高度*潜在维度，然后应用映射函数返回到图像域。我们将在下一章中看到一个明确的例子，不是应用于饼干罐而是应用于人脸。
- en: The concept of encoding the training dataset into a latent space so that we
    can sample from it and decode the point back to the original domain is common
    to many generative modeling techniques, as we shall see in later chapters of this
    book. Mathematically speaking, *encoder-decoder* techniques try to transform the
    highly nonlinear *manifold* on which the data lies (e.g., in pixel space) into
    a simpler latent space that can be sampled from, so that it is likely that any
    point in the latent space is the representation of a well-formed image, as shown
    in [Figure 1-9](#manifold).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练数据集编码到一个潜在空间中，以便我们可以从中进行采样并将点解码回原始域的概念对于许多生成建模技术是常见的，我们将在本书的后续章节中看到。从数学上讲，*编码器-解码器*技术试图将数据所在的高度非线性*流形*（例如，在像素空间中）转换为一个更简单的潜在空间，可以从中进行采样，因此很可能潜在空间中的任何点都是一个良好形成的图像的表示，如[图1-9](#manifold)所示。
- en: '![](Images/gdl2_0109.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0109.png)'
- en: Figure 1-9\. The *dog* manifold in high-dimensional pixel space is mapped to
    a simpler latent space that can be sampled from
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 在高维像素空间中的*狗*流形被映射到一个更简单的潜在空间，可以从中进行采样
- en: Core Probability Theory
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概率理论
- en: We have already seen that generative modeling is closely connected to statistical
    modeling of probability distributions. Therefore, it now makes sense to introduce
    some core probabilistic and statistical concepts that will be used throughout
    this book to explain the theoretical background of each model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到生成建模与概率分布的统计建模密切相关。因此，现在引入一些核心概率和统计概念是有意义的，这些概念将贯穿本书，用来解释每个模型的理论背景。
- en: If you have never studied probability or statistics, don’t worry. To build many
    of the deep learning models that we shall see later in this book, it is not essential
    to have a deep understanding of statistical theory. However, to gain a full appreciation
    of the task that we are trying to tackle, it’s worth trying to build up a solid
    understanding of basic probabilistic theory. This way, you will have the foundations
    in place to understand the different families of generative models that will be
    introduced later in this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从未学习过概率或统计学，不用担心。为了构建本书后面将看到的许多深度学习模型，不必对统计理论有深入的理解。然而，为了充分理解我们试图解决的任务，值得尝试建立对基本概率理论的扎实理解。这样，您将有基础来理解本章后面将介绍的不同类型的生成模型。
- en: 'As a first step, we shall define five key terms, linking each one back to our
    earlier example of a generative model that models the world map in two dimensions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将定义五个关键术语，将每个术语与我们之前在二维世界地图中建模的生成模型的例子联系起来：
- en: Sample space
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 样本空间
- en: The *sample space* is the complete set of all values an observation <math alttext="bold
    x"><mi>𝐱</mi></math> can take.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*样本空间*是观察值 <math alttext="bold x"><mi>𝐱</mi></math> 可以取的所有值的完整集合。'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In our previous example, the sample space consists of all points of latitude
    and longitude <math alttext="bold x equals left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>𝐱</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow></math> on the world map. For example, <math
    alttext="bold x"><mi>𝐱</mi></math> = (40.7306, –73.9352) is a point in the sample
    space (New York City) that belongs to the true data-generating distribution. <math
    alttext="bold x"><mi>𝐱</mi></math> = (11.3493, 142.1996) is a point in the sample
    space that does not belong to the true data-generating distribution (it’s in the
    sea).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，样本空间包括世界地图上所有的纬度和经度点 <math alttext="bold x equals left-parenthesis
    x 1 comma x 2 right-parenthesis"><mrow><mi>𝐱</mi> <mo>=</mo> <mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>。例如，<math
    alttext="bold x"><mi>𝐱</mi></math> = (40.7306, –73.9352) 是样本空间（纽约市）中属于真实数据生成分布的点。<math
    alttext="bold x"><mi>𝐱</mi></math> = (11.3493, 142.1996) 是样本空间中不属于真实数据生成分布的点（在海里）。
- en: Probability density function
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 概率密度函数
- en: A *probability density function* (or simply *density function*) is a function
    <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></math> that maps a point <math alttext="bold x"><mi>𝐱</mi></math>
    in the sample space to a number between 0 and 1\. The integral of the density
    function over all points in the sample space must equal 1, so that it is a well-defined
    probability distribution.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率密度函数*（或简称*密度函数*）是一个将样本空间中的点 <math alttext="bold x"><mi>𝐱</mi></math> 映射到0到1之间的数字的函数
    <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></math>。密度函数在样本空间中所有点上的积分必须等于1，以便它是一个明确定义的概率分布。'
- en: Note
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the world map example, the density function of our generative model is 0
    outside of the orange box and constant inside of the box, so that the integral
    of the density function over the entire sample space equals 1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在世界地图的例子中，我们生成模型的密度函数在橙色框之外为0，在框内为常数，使得密度函数在整个样本空间上的积分等于1。
- en: While there is only one true density function <math alttext="p Subscript d a
    t a Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> that is assumed to have generated the
    observable dataset, there are infinitely many density functions <math alttext="p
    Subscript m o d e l Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> that we can use to estimate <math alttext="p
    Subscript d a t a Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> .
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然只有一个真实的密度函数<math alttext="p Subscript d a t a Baseline left-parenthesis bold
    x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>被假定为生成可观测数据集，但有无限多个密度函数<math
    alttext="p Subscript m o d e l Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>可以用来估计<math alttext="p Subscript d a
    t a Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>。
- en: Parametric modeling
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数建模
- en: '*Parametric modeling* is a technique that we can use to structure our approach
    to finding a suitable <math alttext="p Subscript m o d e l Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> . A *parametric model*
    is a family of density functions <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> that can be described using a finite
    number of parameters, <math alttext="theta"><mi>θ</mi></math> .'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*参数建模*是一种技术，我们可以用来构建我们寻找适当<math alttext="p Subscript m o d e l Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>的方法。*参数模型*是一组密度函数<math
    alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>，可以用有限数量的参数<math
    alttext="theta"><mi>θ</mi></math>来描述。'
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If we assume a uniform distribution as our model family, then the set all possible
    boxes we could draw on [Figure 1-5](#world_map_model) is an example of a parametric
    model. In this case, there are four parameters: the coordinates of the bottom-left
    <math alttext="left-parenthesis theta 1 comma theta 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>θ</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> and top-right <math alttext="left-parenthesis theta 3
    comma theta 4 right-parenthesis"><mrow><mo>(</mo> <msub><mi>θ</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>θ</mi> <mn>4</mn></msub> <mo>)</mo></mrow></math> corners
    of the box.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设均匀分布作为我们的模型族，那么我们可以在[图1-5](#world_map_model)上绘制的所有可能框的集合是参数模型的一个示例。在这种情况下，有四个参数：框的左下角坐标<math
    alttext="left-parenthesis theta 1 comma theta 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>θ</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math>和右上角<math alttext="left-parenthesis theta 3 comma theta
    4 right-parenthesis"><mrow><mo>(</mo> <msub><mi>θ</mi> <mn>3</mn></msub> <mo>,</mo>
    <msub><mi>θ</mi> <mn>4</mn></msub> <mo>)</mo></mrow></math>的坐标。
- en: Thus, each density function <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> in this parametric model (i.e., each
    box) can be uniquely represented by four numbers, <math alttext="theta equals
    left-parenthesis theta 1 comma theta 2 comma theta 3 comma theta 4 right-parenthesis"><mrow><mi>θ</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>θ</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>θ</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></math> .
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个参数模型中的每个密度函数<math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>（即每个框）可以用四个数字<math alttext="theta equals
    left-parenthesis theta 1 comma theta 2 comma theta 3 comma theta 4 right-parenthesis"><mrow><mi>θ</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>θ</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>θ</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></math>唯一表示。
- en: Likelihood
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性
- en: 'The *likelihood* <math alttext="script upper L left-parenthesis theta vertical-bar
    bold x right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></math> of a parameter set <math alttext="theta"><mi>θ</mi></math>
    is a function that measures the plausibility of <math alttext="theta"><mi>θ</mi></math>
    , given some observed point <math alttext="bold x"><mi>𝐱</mi></math> . It is defined
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数集θ的*可能性*<math alttext="script upper L left-parenthesis theta vertical-bar
    bold x right-parenthesis"><mrow><mi>ℒ</mi> <mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></math>是一个函数，用于衡量给定一些观察点<math alttext="bold x"><mi>𝐱</mi></math>的θ的合理性。它的定义如下：
- en: <math alttext="script upper L left-parenthesis theta vertical-bar bold x right-parenthesis
    equals p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>ℒ</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script upper L left-parenthesis theta vertical-bar bold x right-parenthesis
    equals p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>ℒ</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: 'That is, the likelihood of <math alttext="theta"><mi>θ</mi></math> given some
    observed point <math alttext="bold x"><mi>𝐱</mi></math> is defined to be the value
    of the density function parameterized by <math alttext="theta"><mi>θ</mi></math>
    , at the point <math alttext="bold x"><mi>𝐱</mi></math> . If we have a whole dataset
    <math alttext="bold upper X"><mi>𝐗</mi></math> of independent observations, then
    we can write:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，给定一些观察点<math alttext="bold x"><mi>𝐱</mi></math>的θ的可能性被定义为由<math alttext="theta"><mi>θ</mi></math>参数化的密度函数在点<math
    alttext="bold x"><mi>𝐱</mi></math>处的值。如果我们有一个完整的独立观测数据集<math alttext="bold upper
    X"><mi>𝐗</mi></math>，那么我们可以写成：
- en: <math alttext="script upper L left-parenthesis theta vertical-bar bold upper
    X right-parenthesis equals product Underscript bold x element-of bold upper X
    Endscripts p Subscript theta Baseline left-parenthesis bold x right-parenthesis"
    display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐗</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∏</mo> <mrow><mi>𝐱</mi><mo>∈</mo><mi>𝐗</mi></mrow></munder>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script upper L left-parenthesis theta vertical-bar bold upper
    X right-parenthesis equals product Underscript bold x element-of bold upper X
    Endscripts p Subscript theta Baseline left-parenthesis bold x right-parenthesis"
    display="block"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐗</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∏</mo> <mrow><mi>𝐱</mi><mo>∈</mo><mi>𝐗</mi></mrow></munder>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the world map example, an orange box that only covered the left half of the
    map would have a likelihood of 0—it couldn’t possibly have generated the dataset,
    as we have observed points in the right half of the map. The orange box in [Figure 1-5](#world_map_model)
    has a positive likelihood, as the density function is positive for all data points
    under this model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在世界地图示例中，一个只覆盖地图左半部分的橙色框的似然为0—它不可能生成数据集，因为我们观察到地图右半部分的点。在[图1-5](#world_map_model)中的橙色框具有正的似然，因为在该模型下所有数据点的密度函数都为正。
- en: 'Since the product of a large number of terms between 0 and 1 can be quite computationally
    difficult to work with, we often use the *log-likelihood* ℓ instead:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于0到1之间大量项的乘积可能会导致计算上的困难，我们通常使用*log-likelihood* ℓ代替：
- en: <math alttext="script l left-parenthesis theta vertical-bar bold upper X right-parenthesis
    equals sigma-summation Underscript bold x element-of bold upper X Endscripts log
    p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>ℓ</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐗</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>𝐱</mi><mo>∈</mo><mi>𝐗</mi></mrow></munder> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script l left-parenthesis theta vertical-bar bold upper X right-parenthesis
    equals sigma-summation Underscript bold x element-of bold upper X Endscripts log
    p Subscript theta Baseline left-parenthesis bold x right-parenthesis" display="block"><mrow><mi>ℓ</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐗</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>𝐱</mi><mo>∈</mo><mi>𝐗</mi></mrow></munder> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
- en: There are statistical reasons why the likelihood is defined in this way, but
    we can also see that this definition intuitively makes sense. The likelihood of
    a set of parameters <math alttext="theta"><mi>θ</mi></math> is defined to be the
    probability of seeing the data if the true data-generating distribution was the
    model parameterized by <math alttext="theta"><mi>θ</mi></math> .
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有统计原因解释为什么似然以这种方式定义，但我们也可以看到这种定义在直觉上是有道理的。一组参数θ的似然被定义为如果真实的数据生成分布是由θ参数化的模型，则看到数据的概率。
- en: Warning
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note that the likelihood is a function of the *parameters*, not the data. It
    should *not* be interpreted as the probability that a given parameter set is correct—in
    other words, it is not a probability distribution over the parameter space (i.e.,
    it doesn’t sum/integrate to 1, with respect to the parameters).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，似然是*参数*的函数，而不是数据。它不应该被解释为给定参数集正确的概率—换句话说，它不是参数空间上的概率分布（即，它不会对参数求和/积分为1）。
- en: It makes intuitive sense that the focus of parametric modeling should be to
    find the optimal value <math alttext="ModifyingAbove theta With caret"><mover
    accent="true"><mi>θ</mi> <mo>^</mo></mover></math> of the parameter set that maximizes
    the likelihood of observing the dataset <math alttext="bold upper X"><mi>𝐗</mi></math>
    .
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化建模的重点应该是找到最大化观察到的数据集𝐗的可能性的参数集的最优值^θ。
- en: Maximum likelihood estimation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计
- en: '*Maximum likelihood estimation* is the technique that allows us to estimate
    <math alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>θ</mi>
    <mo>^</mo></mover></math> —the set of parameters <math alttext="theta"><mi>θ</mi></math>
    of a density function <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> that is most likely to explain some
    observed data <math alttext="bold upper X"><mi>𝐗</mi></math> . More formally:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*最大似然估计*是一种技术，它允许我们估计^θ——密度函数pθ（𝐱）的参数集θ最有可能解释一些观察到的数据𝐗。更正式地说：'
- en: <math alttext="ModifyingAbove theta With caret equals arg max Underscript bold
    x Endscripts script l left-parenthesis theta vertical-bar bold upper X right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>θ</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mrow><mo form="prefix">arg</mo><mo movablelimits="true" form="prefix">max</mo></mrow>
    <mi>𝐱</mi></munder> <mi>ℓ</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐗</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove theta With caret equals arg max Underscript bold
    x Endscripts script l left-parenthesis theta vertical-bar bold upper X right-parenthesis"
    display="block"><mrow><mover accent="true"><mi>θ</mi> <mo>^</mo></mover> <mo>=</mo>
    <munder><mrow><mo form="prefix">arg</mo><mo movablelimits="true" form="prefix">max</mo></mrow>
    <mi>𝐱</mi></munder> <mi>ℓ</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>|</mo> <mi>𝐗</mi>
    <mo>)</mo></mrow></mrow></math>
- en: <math alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>θ</mi>
    <mo>^</mo></mover></math> is also called the *maximum likelihood estimate* (MLE).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^θ也被称为*最大似然估计*（MLE）。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the world map example, the MLE is the smallest rectangle that still contains
    all of the points in the training set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在世界地图示例中，MLE是仍然包含训练集中所有点的最小矩形。
- en: 'Neural networks typically *minimize* a loss function, so we can equivalently
    talk about finding the set of parameters that *minimize the negative log-likelihood*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常*最小化*损失函数，因此我们可以等效地讨论找到*最小化负对数似然*的参数集：
- en: <math alttext="ModifyingAbove theta With caret equals arg min Underscript theta
    Endscripts left-parenthesis minus script l left-parenthesis theta vertical-bar
    bold upper X right-parenthesis right-parenthesis equals arg min Underscript theta
    Endscripts left-parenthesis minus log p Subscript theta Baseline left-parenthesis
    bold upper X right-parenthesis right-parenthesis" display="block"><mrow><mover
    accent="true"><mi>θ</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>θ</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mi>ℓ</mi> <mo>(</mo> <mi>θ</mi> <mo>|</mo>
    <mi>𝐗</mi> <mo>)</mo></mfenced> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>θ</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove theta With caret equals arg min Underscript theta
    Endscripts left-parenthesis minus script l left-parenthesis theta vertical-bar
    bold upper X right-parenthesis right-parenthesis equals arg min Underscript theta
    Endscripts left-parenthesis minus log p Subscript theta Baseline left-parenthesis
    bold upper X right-parenthesis right-parenthesis" display="block"><mrow><mover
    accent="true"><mi>θ</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>θ</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mi>ℓ</mi> <mo>(</mo> <mi>θ</mi> <mo>|</mo>
    <mi>𝐗</mi> <mo>)</mo></mfenced> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo
    movablelimits="true" form="prefix">min</mo></mrow> <mi>θ</mi></munder> <mfenced
    separators="" open="(" close=")"><mo>-</mo> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow></mfenced></mrow></math>
- en: Generative modeling can be thought of as a form of maximum likelihood estimation,
    where the parameters <math alttext="theta"><mi>θ</mi></math> are the weights of
    the neural networks contained in the model. We are trying to find the values of
    these parameters that maximize the likelihood of observing the given data (or
    equivalently, minimize the negative log-likelihood).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成建模可以被看作是一种最大似然估计的形式，其中参数θ是模型中包含的神经网络的权重。我们试图找到这些参数的值，以最大化观察到的给定数据的可能性（或等效地，最小化负对数似然）。
- en: However, for high-dimensional problems, it is generally not possible to directly
    calculate <math alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
    —it is *intractable*. As we shall see in the next section, different families
    of generative models take different approaches to tackling this problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于高维问题，通常不可能直接计算pθ（𝐱）—它是*难以计算*的。正如我们将在下一节中看到的，不同类型的生成模型采取不同的方法来解决这个问题。
- en: Generative Model Taxonomy
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型分类
- en: 'While all types of generative models ultimately aim to solve the same task,
    they all take slightly different approaches to modeling the density function <math
    alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
    . Broadly speaking, there are three possible approaches:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所有类型的生成模型最终都旨在解决相同的任务，但它们在对密度函数pθ（𝐱）建模时采取了略有不同的方法。广义上说，有三种可能的方法：
- en: Explicitly model the density function, but constrain the model in some way,
    so that the density function is tractable (i.e., it can be calculated).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显式地对密度函数建模，但以某种方式限制模型，使得密度函数是可计算的。
- en: Explicitly model a tractable approximation of the density function.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显式地建模密度函数的可处理近似。
- en: Implicitly model the density function, through a stochastic process that directly
    generates data.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直接生成数据的随机过程隐式建模密度函数。
- en: These are shown in [Figure 1-10](#gm_taxonomy) as a taxonomy, alongside the
    six families of generative models that we will explore in [Part II](part02.xhtml#part_methods)
    of this book. Note that these families are not mutually exclusive—there are many
    examples of models that are hybrids between two different kinds of approaches.
    You should think of the families as different general approaches to generative
    modeling, rather than explicit model architectures.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在[图1-10](#gm_taxonomy)中显示为分类法，与本书[第II部分](part02.xhtml#part_methods)中将探索的六种生成模型家族并列。请注意，这些家族并不是相互排斥的—有许多模型是两种不同方法的混合体。您应该将这些家族视为生成建模的不同一般方法，而不是显式模型架构。
- en: '![](Images/gdl2_0110.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0110.png)'
- en: Figure 1-10\. A taxonomy of generative modeling approaches
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10\. 生成建模方法的分类法
- en: The first split that we can make is between models where the probability density
    function <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math> is modeled *explicitly* and those
    where it is modeled *implicitly*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行的第一个分割是在概率密度函数 <math alttext="p left-parenthesis bold x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></math> 被*显式*建模和被*隐式*建模的模型之间。
- en: '*Implicit density models* do not aim to estimate the probability density at
    all, but instead focus solely on producing a stochastic process that directly
    generates data. The best-known example of an implicit generative model is a *generative
    adversarial network*. We can further split *explicit density models* into those
    that directly optimize the density function (tractable models) and those that
    only optimize an approximation of it.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐式密度模型*并不旨在估计概率密度，而是专注于产生直接生成数据的随机过程。隐式生成模型的最著名例子是*生成对抗网络*。我们可以进一步将*显式密度模型*分为直接优化密度函数（可处理模型）和仅优化其近似值的模型。'
- en: '*Tractable models* place constraints on the model architecture, so that the
    density function has a form that makes it easy to calculate. For example, *autoregressive
    models* impose an ordering on the input features, so that the output can be generated
    sequentially—e.g., word by word, or pixel by pixel. *Normalizing flow models*
    apply a series of tractable, invertible functions to a simple distribution, in
    order to generate more complex distributions.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*可处理模型*对模型架构施加约束，使得密度函数具有易于计算的形式。例如，*自回归模型*对输入特征进行排序，以便可以按顺序生成输出—例如，逐字或逐像素。*归一化流模型*将一系列可处理、可逆函数应用于简单分布，以生成更复杂的分布。'
- en: '*Approximate density models* include *variational autoencoders*, which introduce
    a latent variable and optimize an approximation of the joint density function.
    *Energy-based models* also utilize approximate methods, but do so via Markov chain
    sampling, rather than variational methods. *Diffusion models* approximate the
    density function by training a model to gradually denoise a given image that has
    been previously corrupted.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*近似密度模型*包括*变分自动编码器*，引入潜变量并优化联合密度函数的近似值。*基于能量的模型*也利用近似方法，但是通过马尔可夫链采样，而不是变分方法。*扩散模型*通过训练模型逐渐去噪给定的先前损坏的图像来近似密度函数。'
- en: A common thread that runs through all of the generative model family types is
    *deep learning*. Almost all sophisticated generative models have a deep neural
    network at their core, because they can be trained from scratch to learn the complex
    relationships that govern the structure of the data, rather than having to be
    hardcoded with information a priori. We’ll explore deep learning in [Chapter 2](ch02.xhtml#chapter_deep_learning),
    with practical examples of how to get started building your own deep neural networks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 贯穿所有生成模型家族类型的共同主题是*深度学习*。几乎所有复杂的生成模型都以深度神经网络为核心，因为它们可以从头开始训练，学习控制数据结构的复杂关系，而不必事先硬编码信息。我们将在[第2章](ch02.xhtml#chapter_deep_learning)中探讨深度学习，提供实际示例，帮助您开始构建自己的深度神经网络。
- en: The Generative Deep Learning Codebase
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式深度学习代码库
- en: The final section of this chapter will get you set up to start building generative
    deep learning models by introducing the codebase that accompanies this book.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一节将帮助您开始构建生成式深度学习模型，介绍伴随本书的代码库。
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Many of the examples in this book are adapted from the excellent open source
    implementations that are available through [the Keras website](https://oreil.ly/1UTwa).
    I highly recommend you check out this resource, as new models and examples are
    constantly being added.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的许多示例都改编自[Keras网站](https://oreil.ly/1UTwa)上提供的优秀开源实现。我强烈建议您查看这个资源，因为不断添加新模型和示例。
- en: Cloning the Repository
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克隆存储库
- en: To get started, you’ll first need to clone the Git repository. *Git* is an open
    source version control system and will allow you to copy the code locally so that
    you can run the notebooks on your own machine, or in a cloud-based environment.
    You may already have this installed, but if not, follow the [instructions relevant
    to your operating system](https://oreil.ly/tFOdN).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您首先需要克隆Git存储库。*Git*是一个开源版本控制系统，可以让您将代码本地复制，以便在自己的计算机上或在基于云的环境中运行笔记本。您可能已经安装了这个，但如果没有，请按照与您操作系统相关的[说明](https://oreil.ly/tFOdN)进行操作。
- en: 'To clone the repository for this book, navigate to the folder where you would
    like to store the files and type the following into your terminal:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要克隆本书的存储库，请导航到您想要存储文件的文件夹，并在终端中输入以下内容：
- en: '[PRE0]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`You should now be able to see the files in a folder on your machine.`  `##
    Using Docker'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`您现在应该能够在计算机上的文件夹中看到文件。`  `## 使用Docker'
- en: The codebase for this book is intended to be used with *Docker*, a free containerization
    technology that makes getting started with a new codebase extremely easy, regardless
    of your architecture or operating system. If you have never used Docker, don’t
    worry—there is a description of how to get started in the *README* file in the
    book repository.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有自己的GPU，也没有问题！本书中的所有示例都将在CPU上训练，尽管这将比在启用GPU的机器上使用时间更长。*README*中还有关于设置Google
    Cloud环境的部分，该环境可以让您按需使用GPU。
- en: Running on a GPU
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在GPU上运行
- en: If you don’t have access to your own GPU, that’s also no problem! All of the
    examples in this book will train on a CPU, though this will take longer than if
    you use a GPU-enabled machine. There is also a section in the *README* about setting
    up a Google Cloud environment that gives you access to a GPU on a pay-as-you-go
    basis.`  `# Summary
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '# 总结'
- en: This chapter introduced the field of generative modeling, an important branch
    of machine learning that complements the more widely studied discriminative modeling.
    We discussed how generative modeling is currently one of the most active and exciting
    areas of AI research, with many recent advances in both theory and applications.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了生成建模领域，这是机器学习的一个重要分支，它补充了更广泛研究的判别建模。我们讨论了生成建模目前是人工智能研究中最活跃和令人兴奋的领域之一，近年来在理论和应用方面取得了许多进展。
- en: We started with a simple toy example and saw how generative modeling ultimately
    focuses on modeling the underlying distribution of the data. This presents many
    complex and interesting challenges, which we summarized into a framework for understanding
    the desirable properties of any generative model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的玩具示例开始，看到生成建模最终关注的是对数据的基础分布进行建模。这带来了许多复杂和有趣的挑战，我们将这些总结为了一个框架，以理解任何生成模型的理想特性。
- en: We then walked through the key probabilistic concepts that will help to fully
    understand the theoretical foundations of each approach to generative modeling
    and laid out the six different families of generative models that we will explore
    in [Part II](part02.xhtml#part_methods) of this book. We also saw how to get started
    with the *Generative Deep Learning* codebase, by cloning the repository.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码库旨在与*Docker*一起使用，这是一种免费的容器化技术，可以使您轻松开始使用新的代码库，而不受架构或操作系统的限制。如果您从未使用过Docker，不用担心——书籍存储库中有如何开始的描述在*README*文件中。
- en: In [Chapter 2](ch02.xhtml#chapter_deep_learning), we will begin our exploration
    of deep learning and see how to use Keras to build models that can perform discriminative
    modeling tasks. This will give us the necessary foundation to tackle generative
    deep learning problems in later chapters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml#chapter_deep_learning)中，我们将开始探索深度学习，并看到如何使用Keras构建可以执行判别建模任务的模型。这将为我们提供必要的基础，以便在后面的章节中解决生成式深度学习问题。
- en: '^([1](ch01.xhtml#idm45387027355040-marker)) Miles Brundage et al., “The Malicious
    Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” February
    20, 2018, [*https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf*](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf).`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后讨论了关键的概率概念，这将有助于充分理解每种生成建模方法的理论基础，并列出了我们将在本书的[第II部分](part02.xhtml#part_methods)中探索的六种不同的生成模型系列。我们还看到了如何开始使用*Generative
    Deep Learning*代码库，方法是克隆存储库。
