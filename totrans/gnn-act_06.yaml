- en: 5 Graph autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distinguishing between discriminative and generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying autoencoders and variational autoencoders to graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building graph autoencoders with PyTorch Geometric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over-squashing and graph neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Link prediction and graph generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we’ve covered how classical deep learning architectures can be extended
    to work on graph-structured data. In chapter 3, we considered convolutional graph
    neural networks (GNNs), which apply the convolutional operator to identify patterns
    within the data. In chapter 4, we explored the attention mechanism and how this
    can be used to improve performance for graph-learning tasks such as node classification.
  prefs: []
  type: TYPE_NORMAL
- en: Both convolutional GNNs and attention GNNs are examples of *discriminative*
    *models*, as they learn to discriminate between different instances of data, such
    as whether a photo is of a cat or a dog. In this chapter, we introduce the topic
    of *generative* *models* and explore them through two of the most common architectures,
    autoencoders and variational autoencoders (VAEs). Generative models aim to learn
    the entire dataspace rather than separating boundaries *within* the dataspace,
    as do discriminative models. For example, a generative model learns how to generate
    images of cats and dogs (learning to reproduce aspects of a cat or dog, rather
    than learning just the features that separates two or more classes, such as the
    pointed ears of a cat or the long ears of a spaniel).
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll discover, discriminative models learn to separate boundaries in dataspace,
    whereas generative models learn to model the dataspace itself. By approximating
    the dataspace, we can sample from a generative model to create new examples of
    our training data. In the preceding example, we can use our generative model to
    make new images of a cat or dog, or even some hybrid version that has features
    of both. This is a very powerful tool and important knowledge for both beginner
    and established data scientists. In recent years, deep generative models, generative
    models that use artificial neural networks, have shown amazing ability in many
    language and vision tasks. For example, the family of DALL-E models are able to
    generate new images from text prompts while models such as OpenAI’s GPT models
    have dramatically changed the capabilities of chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to extend generative architectures to act
    on graph-structured data, leading to graph autoencoders (GAEs) and variational
    graph autoencoders (VGAEs). These models are distinct from previous chapters,
    which focused on discriminative models. As we’ll see, generative models model
    the entire dataspace and can be combined with discriminative models for downstream
    machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the power of generative approaches to learning tasks, we return
    to the Amazon Product Co-Purchaser Network introduced in chapter 3\. However,
    in chapter 3, you learned how to predict what category an item might belong to
    given its position in the network. In this chapter, we’ll show how to predict
    where an item should be placed in the network, given its description. This is
    known as *edge* (or link) *prediction* and comes up frequently, for example, when
    designing recommendation systems. We’ll put our understanding of GAEs to work
    here to perform edge prediction, building a model that can predict when nodes
    in a graph are connected. We’ll also discuss the problems of over-squashing, a
    specific consideration for GNNs, and how we can apply a GNN to generate potential
    chemical graphs.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should know the basics of when and where to
    use generative models of graphs (rather than discriminative ones) and how to implement
    them when we need to.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/4aGQ](https://mng.bz/4aGQ)). Colab links and data from this chapter
    can be accessed in the same location.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1 Generative models: Learning how to generate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classic example of deep learning is, given a set of labeled images, how to
    train models to *learn* what label to give to new and unseen images. If we consider
    the example of a set of images of boats and airplanes, we want our model to distinguish
    between these different images. If we then pass the model a new image, we want
    our model to correctly identify this as, for example, a boat. Discriminative models
    learn to discriminate between classes based on their specific target labels. Both
    convolutional architectures (discussed in chapter 3) and attention-based architectures
    (covered in chapter 4) are typically used to create discriminative models. However,
    as we’ll see, they can also be incorporated into generative models. To understand
    this, we first have to understand the difference between discriminative and generative
    modeling approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Generative and discriminative models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described in previous chapters, the original dataset that we use to train
    a model is referred to as our *training data,* and the labels that we seek to
    predict are our *training targets*. The unseen data is our *test data*, and we
    want to learn the *target labels* (from training) to classify the test data. Another
    way to describe this is using conditional probability. We want our models to return
    the probability of some target, Y, given an instance of data, X. We can write
    this as P(Y|X), where the vertical bar means that Y is “conditioned” on X.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve said, discriminative models learn to discriminate *between* classes.
    This is equivalent to learning the separating boundaries of the data in the dataspace.
    In contrast, generative models learn to model the dataspace itself. They capture
    the entire distribution of data in the dataspace, and, when presented with a new
    example, they tell us how likely the new example is. Using the language of probability,
    we say that they model the *joint probability* between data and targets, P(X,Y).
    A typical example of a generative model might be a model that is used to predict
    the next word in a sentence (e.g., the autocomplete feature in many modern mobile
    phones). The generative model assigns a probability to each possible next word
    and returns those words that have the highest probability. Discriminative models
    can tell you how likely a word has some specific sentiment, while a generative
    model will suggest a word to use.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our image example, a generative model approximates the overall
    distribution of images. This can be seen in figure 5.1, where the generative model
    has learned where the points are positioned in the dataspace (rather than how
    they are separated). This means that generative models must learn more complicated
    correlations in the data than their discriminative counterparts. For example,
    a generative model learns that “airplanes have wings” and “boats appear near water.”
    On the other hand, discriminative models just have to learn the difference between
    “boat” and “not boat.” They can do this by looking for telltale signs such as
    a mast, keel, or boom in the image. They can then largely ignore the rest of the
    image. As a result, generative models can be more computationally expensive to
    train and can require larger network architectures. (In section 5.5, we’ll describe
    over-squashing, which is a particular problem for large GNNs.)
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Comparison of generative and discriminative tasks. On the left, the
    discriminative model learns to separate different images of boats and airplanes.
    On the right, the generative model attempts to learn the entire dataspace, which
    allows for new synthetic examples to be created such as a boat in the sky or an
    airplane on water.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.1.2 Synthetic data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given that discriminative models are computationally cheaper to train and more
    robust to outliers than generative models, you might wonder why we want to use
    a generative model at all. Generative models, however, are efficient tools when
    labeling data is relatively expensive but generating datasets is easy to do. For
    example, generative models are increasingly being used in drug discovery where
    they generate new candidate drugs that might match certain properties, such as
    the ability to reduce the effects of some disease. In a sense, generative models
    attempt to learn how to create synthetic data, which allows us to create new data
    instances. For example, none of the people shown in figure 5.2 exist and were
    instead created by sampling from the dataspace, approximated using a generative
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2 Figure showing synthetic faces (Source: [1])'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Synthetic examples created by generative models can be used to augment a dataset,
    which is expensive to collect. Rather than taking lots of pictures of faces under
    every condition, we can use generative models to create new data examples (e.g.,
    a person wearing a hat, glasses, and a mask) to increase our dataset to contain
    tricky edge cases. These synthetic examples can then be used to further improve
    our other models (e.g., one that identifies when someone is wearing a mask). However,
    when introducing synthetic data, we must also be careful about introducing other
    biases or noise into our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, discriminative models are often used downstream of generative models.
    This is because generative models are typically trained in a “self-supervised”
    way, without relying on data labels. They learn to compress (or *encode)* complex
    high-dimensional data to lower dimensions. These low-dimensional representations
    can be used to better tease out underlying patterns within our data. This is known
    as *dimension reduction* and can be helpful in clustering data or in classification
    tasks. Later, we’ll see how generative models can separate graphs into different
    classes without ever seeing their labels. In cases where annotating each data
    point is expensive, generative models can be huge cost savers. Let’s get on to
    meeting our first generative GNN model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Graph autoencoders for link prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the fundamental and popular models for deep generative models is the
    autoencoder. The reason the autoencoder framework is so widely used is because
    it’s incredibly adaptive. Just as the attention mechanisms in chapter 3 can be
    used to improve on many different models, autoencoders can be combined with many
    different models, including different types of GNNs. Once the autoencoder structure
    is understood, the encoder and decoder can be replaced with any type of neural
    network, including different GNNs such as the graph convolutional network (GCN)
    and GraphSAGE architectures from chapter 2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we need to take care when applying autoencoders to graph-based data.
    When reconstructing our data, we also have to reconstruct our adjacency matrix.
    In this section, we’ll look at implementing a GAE using the Amazon Products dataset
    from chapter 3 [2]. We’ll build a GAE for the task of link prediction, which is
    a common problem when working with graphs. This allows us to reconstruct the adjacency
    matrix and is especially useful when we’re dealing with a dataset that has missing
    data. We’ll follow this process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create both an encoder and decoder.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the encoder to create a latent space to sample from.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the training and testing loop by including a loss suitable for constructing
    a generative model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data as a graph, with edge lists and node features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model, passing the edge data to compute the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the model using the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.2.1 Review of the Amazon Products dataset from chapter 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 3, we learned about the Amazon Products dataset with co-purchaser
    information. This dataset contains information about a range of different items
    that were purchased, details about who purchased them and how, and categories
    for the items, which were the labels in chapter 3\. We’ve already learned about
    how we can turn this tabular dataset into a graph structure and, by doing so,
    make our learning algorithms more efficient and more powerful. We’ve also already
    used some dimension reduction without realizing it. Principal component analysis
    (PCA) was applied to the Amazon Products dataset to create the features. Each
    product description was converted into numerical values using the bag-of-words
    algorithm, and PCA is then applied to reduce the (now numerical) description to
    100 features.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’re going to revisit the Amazon Products dataset but with
    a different aim in mind. We’re going to use our dataset to learn link predictions.
    Essentially, this means learning the *relations between* nodes in our graph. This
    has many use cases, such as predicting what movies or TV shows users would like
    to watch next, suggesting new connections on social media platforms, or even predicting
    customers who are more likely to default on credit. Here, we’re going to use it
    to predict which products in the Amazon Electronics dataset should be connected
    together, as we show in figure 5.3\. For further details about link prediction,
    check out section 5.5 at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 The Amazon Electronics dataset, where different products such as
    cameras and lenses are connected based on whether they have been bought together
    in the past
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As with all data science projects, it’s worth first taking a look at the dataset
    and understanding what the problem is. We start by loading the data, the same
    way as we did in chapter 3, which we show in listing 5.1\. The data is preprocessed
    and labeled so it can be loaded using NumPy. Further details on the dataset can
    be found in [2].
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Loading the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding output prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With the data loaded, we can next look at some basic statistics and details
    of the data. We’re interested in edge or link prediction, so it’s worth understanding
    how many different edges exist. We might also want to know how many components
    there are and the average degree to understand how connected our graph is. We
    show the code to calculate this in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Exploratory data analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This is only possible because the adjacency matrix is undirected.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Ratio of actual edges to possible edges'
  prefs: []
  type: TYPE_NORMAL
- en: We also plot the distribution of the degree to see how connections vary, as
    shown in the following listing and in figure 5.4\.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Plotting the graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets row indices for each nonzero value'
  prefs: []
  type: TYPE_NORMAL
- en: We find that there are 7,650 nodes, more than 143,000 edges, and an overall
    density of 0.0049\. Therefore, our graph is medium size (~10,000 nodes) but very
    sparse (density much less than 0.05). We see that the majority of nodes have a
    low degree (less than 10), but that there is a second peak of edges with a higher
    degree (around 30) and a longer tail. In total, we see very few nodes with a high
    degree, which we would expect given the low density of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 Degree distribution for the Amazon Electronics co-purchaser graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.2.2 Defining a graph autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’ll use a generative model, the autoencoder, to estimate and predict
    links in the Amazon Electronics dataset. In doing so, we’re in good company, as
    link prediction was the problem that GAEs were applied to when first published
    by Kipf and Welling in 2012 [3]. In their seminal paper, they introduced the GAE
    and its variational extension, which we’ll be discussing shortly, and then applied
    these models to three classic benchmarks in graph deep learning, the Cora dataset,
    CiteSeer, and PubMed. Today, most graph deep learning libraries make it very easy
    to create and begin training GAEs, as these have become one of the most popular
    graph-based deep generative models. We’ll look at the steps required to build
    one in more detail in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The GAE model is similar to a typical autoencoder. The only difference is that
    each individual layer of our network is a GNN, such as a GCN or GraphSAGE network.
    In figure 5.5, we show a schematic for a GAE’s architecture. Broadly, we’ll be
    taking our edge data and compressing it into a low-dimensional representation
    using an encoder network.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Schematic for the GAE showing the key elements of the model, such
    as the encoder, latent space, and decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first thing we need to define for our GAE is the encoder, which will take
    our data and transform it into a latent representation. The code snippet for implementing
    the encoder is given in listing 5.4\. We first import our libraries and then build
    a GNN where each layer is progressively smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Graph encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads GCNConv models from PyG'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the encoder layer and initializes it with a predefined size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Defines each of the encoder layer networks'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward pass for the encoder with edge data'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we also have to make sure that our forward pass can return the edge
    data from our graph because we’ll be using our autoencoder to reconstruct the
    graph from the latent space. To put this another way, the autoencoder will be
    learning how to reconstruct the adjacency matrix from a low-dimensional representation
    of our feature space. This means it’s also learning to predict edges from new
    data. To do this, we need to make the autoencoder structure learn to reconstruct
    edges, specifically by changing the decoder. Here, we’ll use the inner product
    to predict edges from the latent space. This is shown in listing 5.5\. (To understand
    why we use the inner product, see the technical details in section 5.5.)
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Graph decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines the decoder layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 States the shape and size of the decoder (which, again, is the reverse of
    the encoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Forward pass for the decoder'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re ready to combine both encoder and decoder together in the GAE class,
    which contains both submodels (see listing 5.6). Note that we don’t initialize
    the decoder with any input or output sizes now as this is just applying the inner
    product to the output of our encoder with the edge data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Graph autoencoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines the encoder for the GAE'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the decoder'
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch Geometric (PyG), the GAE model can be made even easier by just importing
    the GAE class, which automatically builds both decoder and autoencoder once passed
    to the encoder. We’ll use this functionality when we build a VGAE later in the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Training a graph autoencoder to perform link prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having built our GAE, we can proceed to use this to perform edge prediction
    for the sub models Amazon Products dataset. The overall framework will follow
    a typical deep learning problem format, where we first load the data, prepare
    the data, and split this data into train, test, and validation datasets; define
    our training parameters; and then train and test our model. These steps are shown
    in figure 5.6\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Overall steps for training our model for link prediction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We begin by loading the dataset and preparing it for our learning algorithms,
    which we’ve already done in listing 5.1\. For us to use the PyG models for GAE
    and VGAE, we need to construct an edge index from the adjacency matrix, which
    is easily done using one of PyG’s utility functions, `to_edge_index`, as we describe
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Construct Edge Index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the to_edge_index from the PyG utility libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Converts the adjacency matrix to an edge index and edge attribute vector'
  prefs: []
  type: TYPE_NORMAL
- en: We then load the PyG libraries and convert our data into a PyG data object.
    We can also apply transformations to our dataset, where the features and adjacency
    matrix are loaded as in chapter 3\. First, we normalize our features and then
    split our dataset into training, testing, and validation sets based on the edges
    or links of the graph, as shown in listing 5.8\. This is a vital step when carrying
    out link prediction to ensure we correctly split our data. In the code, we’ve
    used 5% of the data for validation and 10% for test data, noting that our graph
    is undirected. Here, we don’t add any negative training samples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Convert to a PyG object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Converts our data to a PyG data object'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Transforms our data and splits the links into train, test, and validation
    sets'
  prefs: []
  type: TYPE_NORMAL
- en: With everything in place, we can now apply GAE to the Amazon Products dataset.
    First, we define our model, as well as our optimizer and our loss. We apply the
    binary cross-entropy loss to the predicted values from the decoder and compare
    against our true edge index to see whether our model has reconstructed the adjacency
    matrix correctly, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9 Define the model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Specifies the shape of our encoder'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines a GAE with the correct shape'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Our loss now is binary cross-entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to use a binary cross-entropy loss because we want to calculate
    the probabilities that each edge is a true edge, where true edges correspond to
    the ones that aren’t being hidden and don’t need to be predicted (i.e., the positive
    samples). The encoder learns to compress the edge data but doesn’t change the
    number of edges, whereas the decoder learns to predict edges. In a sense, we’re
    combining both the discriminative and generative steps here. Therefore, the binary
    cross-entropy gives us a probability where there is likely to be an edge between
    these nodes. It’s binary, as either an edge should exist (label 1) or shouldn’t
    (label 0). We can compare all of those edges that have a binary cross-entropy
    probability greater than 0.5 to the actual true edges in each epoch of our training
    loop, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 Training function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Encodes graph into latent representation'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Performs a new round of negative sampling'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Combines new negative samples with the edge label index'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Generates edge predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Combines edge labels with 0s for negative samples'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Computes loss and backpropagates'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we first encoded our graph into a latent representation. We then perform
    a round of negative sampling, with new samples drawn for each epoch. Negative
    sampling takes a random subset of nonexistent labels rather than existing positive
    ones during training to account for the class imbalance between real labels and
    nonexistent ones. Once we have these new negative samples, we concatenate them
    with our original edge labels index and pass these to our decoder to get a reconstructed
    graph. Finally, we concatenate our true edge labels with the 0 labels for our
    negative edges and compute the loss between our predicted edges and our true edges.
    Note that we’re not doing batch learning here; instead, we’re choosing to train
    on all data during each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Our test function, shown in listing 5.11, is much simpler than our training
    function as it doesn’t have to perform any negative sampling. Instead, we just
    use the true and predicted edges and return a Receiver Operating Characteristic
    (ROC)/Area Under the Curve (AUC) score to measure the accuracy of our model. Recall
    that the ROC/AUC curves will range between 0 and 1, and a perfect model, whose
    predictions are 100% correct, will have an AUC of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Test function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Encodes the graph into a latent representation'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Decodes the graph using the full edge label index'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calculates the overall ROC/AUC score'
  prefs: []
  type: TYPE_NORMAL
- en: At each time step, we’ll calculate the overall success of a model using all
    our edge data from our validation data. After training is complete, we then use
    the test data to calculate the final test accuracy, as shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 Training loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Performs a training step'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Tests our updated model on validation data'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Tests our final model on test data'
  prefs: []
  type: TYPE_NORMAL
- en: We find that after 200 epochs, we achieve an accuracy of more than 83%. Even
    better, when we then use our test set to see how well our model is able to predict
    edges, we get an accuracy of 86%. We can interpret our model performance as being
    able to suggest a meaningful item to the purchaser 86% of the time, assuming that
    all future data is the same as our current dataset. This is a great result and
    demonstrates how useful GNNs are for recommender systems. We can also use our
    model to better understand how the dataset is structured or apply additional classification
    and feature engineering tasks by exploring our newly constructed latent space.
    Next, we’re going to learn about one of the most common extensions to the graph
    autoencoder model—the VGAE.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Variational graph autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autoencoders map data onto discrete points in the latent space. To sample outside
    of the training dataset and generate new synthetic data, we can interpolate between
    these discrete points. This is exactly the process that we described in figure
    5.1, where we generated unseen combinations of data such as a flying boat. However,
    autoencoders are deterministic, where each input maps to a specific point in the
    latent space. This can lead to sharp discontinuities when sampling, which can
    affect performance for data generation resulting in synthetic data that doesn’t
    reproduce the original dataset as well. To improve our generative process, we
    need to ensure that our latent space is well-structured, or *regular.* In figure
    5.7, for example, we show how to use the Kullback-Liebler divergence (KL divergence)
    to restructure the latent space to improve reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 Regular spaces are continuous and compact, but data regions may become
    less separated. Alternatively, high reconstruction loss typically means data is
    well separated, but the latent space might be less covered leading to worse generative
    samples. Here, KL Divergence refers to the Kullback-Liebler divergence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The KL divergence is a measure of how one probability distribution differs from
    another. It calculates how much “extra information” is needed to encode values
    from one distribution (the original data distribution) into another (the latent
    space). On the left, the data groups (*x*[*i*]) don’t overlap much, which means
    the KL divergence is higher. On the right, there is more overlap (similarity)
    between the different data groups, meaning the KL divergence is lower. When building
    a more regular latent space that has a high KL divergence, we can get very good
    reconstruction but poor interpolation, while we get the opposite for low KL divergence.
    More details on this are provided in section 5.5.
  prefs: []
  type: TYPE_NORMAL
- en: '*Regular* means that the space fulfills two properties: continuity and compactness.
    *Continuity* means that nearby points in the latent space are decoded into approximately
    similar things, while *compactness* means that any point in the latent space should
    lead to a meaningful decoded representation. These terms, approximately similar
    and meaningful, have precise definitions, which you can read more about in *Learn
    Generative AI with PyTorch* (Manning, 2024; [https://mng.bz/AQBg](https://mng.bz/AQBg)).
    However, for this chapter, all you need to know is that these properties make
    it easier to sample from the latent space, resulting in cleaner generated samples
    and potentially higher model accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: When we regularize a latent space, we use variational methods that model the
    entire dataspace in terms of probability distributions (or densities). As we’ll
    see, the main benefit of using variational methods is that the latent space is
    well structured. However, variational methods don’t necessarily guarantee higher
    performance, so it’s often important to test both the autoencoder and the variational
    counterpart when using these types of models. This can be done by either looking
    at the reconstruction score (e.g., mean squared error) on the test dataset, applying
    some dimension reduction method to the latent encodings (e.g., t-SNE or Uniform
    Manifold Approximation and Projection [UMAP]), or using task-specific measures
    (e.g., the Inception Score for images or ROUGE/METEOR for text generation). Specifically
    for graphs, measures such as the maximum mean discrepancy (MMD), graph statistics,
    or graph kernel methods can all be used to compare against different synthetically
    generated graph copies.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll go into more detail on what it means to model
    a dataspace as a probability density and how we can transform our graph autoencoder
    into a VGAE with just a few lines. These depend on some key probabilistic machine
    learning concepts such as the KL divergence and the reparameterization trick,
    which we give an overview of in section 5.5\. For more of a deep dive into these
    concepts, we recommend *Probabilistic Deep Learning* (Manning, 2020). Let’s build
    a VGAE architecture and apply it to the same Amazon Products dataset as before.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Building a variational graph autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The VGAE architecture is similar to the GAE model. The main difference is that
    the output of a *variational* graph encoder is generated by sampling from a probability
    density. We can characterize density in terms of its mean and variance. Therefore,
    the output of the encoder will now be the mean and variance for each dimension
    of our previous space. The decoder then takes this sampled latent representation
    and decodes it to appear like the input data. This can be seen in figure 5.8,
    where the high-level model is that we now extend our previous autoencoder to output
    mean and variance rather than point estimates from the latent space. This allows
    our model to make probabilistic samples from the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Structure of a general VAE, where we now sample from a probability
    density in the latent space rather than a point estimate as with typical autoencoders.
    VGAEs extend the VAE architecture to apply to graph-structured data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We have to adapt our architecture and also change our loss to include an additional
    term for regularizing the latent space. Listing 5.13 provides a code snippet for
    the VGAE. The similarities between listing 5.4 and the `VariationalGCNEncoder`
    layer in listing 5.13 include that we’ve doubled the dimensionality of our latent
    space and now return the mean and the log variance from our encoder at the end
    of our forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13 `VariationalGCNEncoder`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Adds in mean and log variance variables to sample from'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Forward pass returns mean and log variance variables'
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed the GAE, we learned that the decoder uses the inner product
    to return the adjacency matrix, or edge list. Previously we explicitly implemented
    the inner dot product. However, in PyG, this functionality is built in. To build
    a VGAE structure, we can call the `VGAE` function, shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14 Variational graph autoencoder (`VGAE`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uses the VGAE function from the PyG library to build the autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: This functionality makes it much simpler to build a VGAE, where the VGAE function
    in PyG takes care of the reparameterization trick. Now that we have our VGAE model,
    the next thing we need to do is amend the training and testing functions to include
    the KL divergence loss. The training function is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15 Training function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 As we are using the PyG VGAE function, we need to use the encode and decode
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Adds in the regularizing term of the loss given by the KL divergence'
  prefs: []
  type: TYPE_NORMAL
- en: This is the same training loop that we used in listing 5.12 to train our GAE
    model. The only differences are that we include an additional term to our loss
    that minimizes the KL divergence and we change the `encoder` and `decoder` method
    calls to `encode` and `decode` (which we also need to update in our test function).
    Otherwise, the training remains unchanged. Note that thanks to the added PyG functionality,
    these changes are considerably less involved than when we made the changes in
    PyTorch earlier. However, going through each of those extra steps gives us more
    intuition about the underlying architecture for a GAE.
  prefs: []
  type: TYPE_NORMAL
- en: We can now apply our VGAE to the Amazon Products dataset and use this to perform
    edge prediction, which yields an overall test accuracy of 88%. This is slightly
    higher than our accuracy for GAE. It’s important to note that VGAEs won’t necessarily
    give higher accuracy. As a result, you should always try a GAE as well as a VGAE
    and run careful model validation when using this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 When to use a variational graph autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given that the accuracy for the VGAE was similar to the GAE, it’s important
    to realize the limitations of both methods. In general, GAEs and VGAEs are great
    models to use when you want to build a generative model or where you want to use
    one aspect of your data to learn another aspect. For example, we might want to
    make a graph-based model for pose prediction. We can use both GAE and VGAE architectures
    to predict future poses based on video footage. (We’ll see a similar example in
    later chapters.) When we do so, we’re using the GAE/VGAE to learn a graph of the
    body, conditioned on what the future positions of each body part will be. However,
    if we’re specifically interested in generating new data, such as new chemical
    graphs for drug discovery, VGAEs are often better as the latent space is more
    structured.
  prefs: []
  type: TYPE_NORMAL
- en: In general, GAEs are great for specific reconstruction tasks such as link prediction
    or node classification, while VGAEs are better for where the tasks require a larger
    or more diverse range of synthetic samples, such as where you want to generate
    entirely new subgraphs or small graphs. VGAEs are also often better suited for
    when the underlying dataset is noisy, compared to GAEs which are faster and more
    suitable for graph data with clear structure. Finally, note that VGAEs are less
    prone to overfitting due to their variational approach, and they may generalize
    better as a result. As always, your choice of architecture depends on the problem
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ve learned about two examples of generative models, the
    GAE and VGAE models, and how to implement these models to work with graph-structured
    data. To better understand how to use this model class, we applied our models
    to an edge prediction task. However, this is only one step in applying a generative
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In many instances where we require a generative model, we use successive layers
    of autoencoders to further reduce the dimensionality of our system and increase
    our reconstruction power. In the context of drug discovery and chemical science,
    GAEs allow us to reconstruct the adjacency matrix (as we did here) as well as
    reconstruct types of molecules and even the number of molecules. GAEs are used
    frequently in many sciences and industries. Now you have the tools to try them
    out too.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll demonstrate how to use the VGAE to generate new graphs
    with specific qualities, such as novel molecules that have a high property indicating
    usefulness as a potential drug candidate.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Generating graphs using GNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve considered how to use a generative model of our graph to estimate
    edges between nodes. However, sometimes we’re also interested in generating not
    just a node or an edge but the entire graph. This can be particularly important
    when trying to understand or predict graph-level data. In this example, we’ll
    do exactly that by using our GAE and VGAEs to generate new potential molecules
    to synthesize, which have certain properties.
  prefs: []
  type: TYPE_NORMAL
- en: One of the fields that GNNs have had the largest effect on has been drug discovery,
    especially for the identification of new molecules or potential drugs. In 2020,
    a new antibiotic was proposed that was discovered using a GNN, and, in 2021, a
    new method for identifying carcinogens in food was published that also made use
    of GNNs. Since then, there have been many other papers that use GNNs as tools
    to accelerate the drug discovery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Molecular graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re going to be considering small molecules that have previously been screened
    for drugs, as described in the ZINC dataset of around 250,000 individual molecules.
    Each molecule in this dataset has additional data including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified Molecular Input Line Entry System (SMILES)* —A description of
    the molecular structure or the molecular *graph* in ASCII format.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**   *Important properties* —Synthetic accessibility score (SAS), water-octanol
    partition coefficient (`logP`), and, most importantly, a measure of the quantitative
    estimate of druglikeness (QED), which highlights how likely this molecule could
    be as a potential drug.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To make this dataset usable by our GNN models, we need to convert this into
    a suitable graph structure. Here, we’re going to be using PyG for defining our
    model and running our deep learning routines. Therefore, we first download the
    data and then convert the dataset into graph objects using NetworkX. We download
    our dataset in listing 5.16, which generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Listing 5.16 Create a molecular graph dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In listing 5.17, we define a function to convert the SMILES into small graphs,
    which we then use to create a PyG dataset. We also add some additional information
    to each object in our dataset, such as the number of heavy atoms that we can use
    for further data exploration. Here, we use the recursive SMILES depth-first search
    (DFS) toolkit (RDKit) package ([www.rdkit.org/docs/index.xhtml](http://www.rdkit.org/docs/index.xhtml)),
    which is a great open source tool for cheminformatics.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17 Create the molecular graph dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: A random sample from our dataset is shown in figure 5.9, which highlights how
    varied our molecular graphs are and their small size, where each one has less
    than 100 nodes and edges.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Example molecular graphs with quantitative estimate of druglikeness
    (QED)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.4.2 Identifying new drug candidates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In figure 5.10, we start to see how QED can vary with different molecular structures.
    One of the main obstacles to drug discovery is the number of different potential
    combinations of molecules and how to know which ones to synthesize and then test
    for drug efficacy. This is far before the stage of introducing the drug to human,
    animal (in vivo), or sometimes even cellular (in vitro) trials. Even evaluating
    things such as a molecule’s solubility can be a challenge if we use the molecular
    graph alone. Here, we’re going to be focusing on predicting the molecules’ QED,
    to see which ones are most likely to have potential use as a drug. To give an
    example of how the QED can vary, see figure 5.10, which has four molecules with
    high (~0.95) and low (~0.12) QED. We can see some qualitative differences between
    these molecules, such as the increased number of strong bonds for those with low
    QED. However, estimating the QED directly from the graph is a challenge. To help
    us with this task, we’ll use a GNN to both generate and evaluate new potential
    drugs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 Molecules with high QED (top) and low QED (bottom)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our work will be based on two important papers that demonstrated how generative
    models can be effective tools for identifying new molecules (Gómez-Bombarelli
    et al. [4] and De Cao et al. [5]). Specifically, Gómez-Bombarelli et al. showed
    that by constructing a smooth representation of the dataspace, which is the latent
    space we described earlier in this chapter, it’s possible to optimize to find
    new candidates with specific properties of interest. This work borrows heavily
    from an equivalent implementation in the Keras library, outlined in a posting
    by Victor Basu [6]. Figure 5.11 reproduces the basic idea from [5].
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 Example of how a graph autoencoder that is trained to re-create
    small graphs can also be used to make property predictions. The property prediction
    is applied in the latent space and creates a learned gradient of a specific graph
    property—in our case, the QED value.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 5.11, we can see that the underlying model structure is an autoencoder,
    just like the ones we’ve been discussing in this chapter. Here, we pass the SMILES
    of the molecule as input to the encoder, and this is then used to construct the
    latent space of different molecular representations. This is shown as regions
    with different colors representing different groups of molecules. Then, the decoder
    is designed to faithfully translate the latent space back into the original molecule.
    This is similar to the autoencoder structure that we showed earlier in figure
    5.5.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside the latent space, we now also have an additional function, which is
    going to predict the property of the molecule. In figure 5.11, the property we’ll
    predict is also the property we’re optimizing for. Therefore, by learning how
    to encode both the molecule and the property, which in our case is QED, into the
    latent space, we can optimize drug discovery to generate new candidate molecules
    with a high QED.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we’ll use the VGAE. This model includes two losses: a reconstruction
    loss that measures the difference between the original input data passed to the
    encoder and the output from the decoder, as well as a measure of the structure
    of the latent space, where we use the KL divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with these two loss functions, we’ll add one more function: a property
    prediction loss. The property prediction loss estimates the MSE between predicted
    and actual properties after running the latent representation through a property
    prediction model, as shown in the middle of figure 5.11\.'
  prefs: []
  type: TYPE_NORMAL
- en: To train our GNN, we adapt the training loop provided earlier in listing 5.15
    to include these individual losses. This is shown in listing 5.18\. Here, we have
    the reconstruction loss as the binary cross-entropy (BCE) for the adjacency matrix,
    while the property prediction loss considers only QED and can be based on the
    MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18 Loss for molecule graph generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Reconstruction loss'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Property prediction loss'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 KL divergence loss'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 VGAEs for generating graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have both our training data and loss, we can start to think about
    the model. Overall, this model will be similar to the ones discussed earlier in
    the chapter, both GAE and VGAE. However, we need to make some subtle changes to
    our model to ensure that it’s well applied to the problem at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a heterogenous GCN to account for different edge types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the decoder to generate the entire graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce a property prediction layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Heterogeneous GCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The small graphs that we’re generating will have different edge types that connect
    the nodes of our graphs. Specifically, we can have a different number of bonds
    between the atoms such as a single bond, double bond, triple bond, or even *aromatic
    bonds*, which relate to molecules that are formed into a ring. Graphs with more
    than one edge type are known as heterogeneous graphs, so we’ll need to make our
    GNN applicable to heterogeneous graphs.
  prefs: []
  type: TYPE_NORMAL
- en: So far, all the graphs we’ve been considering have been homogenous (only one
    edge type). In listing 5.19, we show how the GCN, which we discussed in chapter
    3, can be adapted to heterogeneous graphs. Here, we explicitly map out some of
    the different features for heterogeneous graphs. However, it’s important to note
    that many GNN packages already support models for heterogeneous graphs out of
    the box. For example, PyG has a specific class of models known as `HeteroConv`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.19 shows the code to create a heterogenous GCN. This builds off the
    message-passing class in PyG, which is fundamental to all GNN models. We also
    use the PyTorch `Parameter` class to create a new subset of parameters that are
    specific to the different edge types (relations). Finally, we also specify here
    that the aggregation operation in the message-passing framework is based on summation
    (`'add'`). If you’re interested, feel free to try other aggregation operations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.19 Heterogenous GCN
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 "Add" aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Parameter for weights'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 edge_type is used to select the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 x_j has shape [E, in_channels], and edge_type has shape [E].'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Select the corresponding weights.'
  prefs: []
  type: TYPE_NORMAL
- en: With the preceding GNN, we can then compose our encoder as a combination of
    these individual GNN layers. This is shown in listing 5.20, where we follow the
    same logic as when we defined our edge encoder (refer to listing 5.13), except
    that we now switch out our GCN layers for the heterogeneous GCN layers. As we
    have different edge types, we must now also specify the number of different types
    (relations) as well as passing the specific edge type into the forward function
    for our graph encoder. Again, we return both log variance and mean to ensure that
    the latent space is constructed using distributions rather than point samples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.20 Small graph encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Heterogeneous GCNs'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Forward pass GCNs'
  prefs: []
  type: TYPE_NORMAL
- en: Graph decoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our previous examples, we used GAEs to generate and predict edges between
    nodes in a single graph. However, we’re now interested in using our autoencoder
    to generate entire graphs. Therefore, we no longer just consider the inner product
    decoder to account for the presence of an edge in the graph but instead decode
    both the adjacency matrix and feature matrix for each small molecular graph. This
    is shown in listing 5.21\.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.21 Small graph decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generates the adjacency matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Symmetrizes the adjacency matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies softmax'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Generates features'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Applies softmax'
  prefs: []
  type: TYPE_NORMAL
- en: The majority of this code is typical for decoder style networks. We begin with
    a small network that matches the dimension for the latent space created using
    the encoder. We then progressively increase the size of the graph through subsequent
    layers of the network. Here, we can use simple linear networks, where we include
    network dropout for performance. At the final layer, we reshape the decoder output
    into the adjacency and feature matrices. We also ensure that the adjacency matrix
    is symmetric before applying softmax. We symmetrize the adjacency matrix by adding
    it to its transpose and dividing by 2\. This ensures that node `i` is connected
    to `j` and that `j` is also connected to `i`. We then apply softmax to normalize
    the adjacency matrix, ensuring all outgoing edges from each node sum to 1\. There
    are other choices we could make here such as using the maximum value, applying
    a threshold, or using the sigmoid function instead of softmax. In general, averaging
    + softmax is a good approach.
  prefs: []
  type: TYPE_NORMAL
- en: Property prediction layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All that’s left is to combine both encoder and decoder networks into a final
    model that can be used for molecular graph generation, as shown in listing 5.22\.
    Overall, this follows the same steps as in listing 5.14 earlier, where we define
    both our encoder and decoder as well as use the reparameterization trick. The
    only difference is that we also include a simple linear network to predict the
    property of the graphs, in this case, the QED. This is applied on the latent representation
    (`z`), after being reparameterized.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.22 VGAE for molecular graph generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output for the model is then both the mean and log variance, which are passed
    to the KL divergence; the reconstructed adjacency matrix and feature matrix, passed
    to the reconstruction loss; and the predicted QED values, which are used in the
    prediction loss. Using these, we can then compute the loss for our network and
    backpropagate the loss through the network weights to refine the generated graphs
    to have specifically high QED values. Next, we show how to achieve just that in
    our training and test loops.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.4 Generating molecules using a GNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we discussed all the individual parts needed to use
    a GNN to generate molecules. We’ll now bring the different elements together and
    demonstrate how to use a GNN to create novel graphs that are optimized for a specific
    property. In figure 5.12, we show the steps to generate molecules with a GNN that
    have a high QED. These steps include creating suitable graphs to represent small
    molecules, passing these through our autoencoder, predicting specific molecular
    features such as QED, and then repeating those steps until we’re able to recreate
    new novel molecular graphs with specific features.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 Steps to generate molecules with a GNN
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The key element that remains is to combine our loss functions with our adapted
    VGAE model. This is shown in listing 5.23, which defines our training loop. This
    is similar to previous training loops that you’ve seen in earlier chapters and
    examples. The main idea is that our model is used to predict some property of
    the graph. However, here we’re predicting the entire graph, as defined in the
    predicted adjacency matrix (`pred_adj`) and the predicted feature matrix (`pred_feat`).
  prefs: []
  type: TYPE_NORMAL
- en: The output from our model and the real data are passed to our method for calculating
    the loss, which contains the reconstruction loss, KL divergence loss, and property
    prediction loss. Finally, we compute the gradient penalty, which acts as a further
    regularizer for our model (and defined in more detail in section 5.5). With both
    loss and gradient calculated, we backpropagate through our model, step our optimizer
    forwards, and return the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.23 Training function for molecule graph generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Compute losses'
  prefs: []
  type: TYPE_NORMAL
- en: During training time, we find that the model loss decreases, demonstrating that
    the model is effectively learning how to reproduce novel molecules. We show some
    of these molecules in figure 5.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 Small molecule graphs generated using a GNN
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To better understand the distribution of the predicted QED property in our latent
    space, we apply our encoder to a new subset of data and look at the first two
    axes of the data as represented in the latent space, as shown in figure 5.14\.
    Here, we can see that the latent space has been constructed to cluster molecules
    with higher QED together. Therefore, by sampling from regions around this area,
    we can identify new molecules to test. Future work will be needed to verify our
    results, but as a first step toward the discovery of new molecules, we’ve demonstrated
    that a GNN model may well be used to propose new and potentially valuable drug
    candidates.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 Latent space of drug molecules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this chapter, we’ve focused on generative tasks rather than classical discriminative
    models. We’ve shown generative models, such as GAEs and VGAEs, can be used for
    edge prediction, learning to identify connections between nodes where information
    is potentially not available. We then went on to show that generative GNNs can
    be used to discover not just unknown parts of a graph, such as a node or edge,
    but also entirely new and complicated graphs, when we applied our GNNs to generate
    new small molecules with a high QED. These results highlight that GNNs are vital
    tools for those working in chemistry, life sciences, and many other disciplines
    that deal with many individual graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we’ve learned that GNNs are extremely useful for both discriminative
    and generative tasks. Here, we consider the topic of small molecule graphs, but
    GNNs have also been applied to knowledge graphs and small social clusters. In
    the next chapter, we’ll look at how we can learn to generate graphs that are consistent
    over time by combining generative GNNs with temporal encodings. In that spirit,
    we take a further step forward and learn how GNNs can be taught how to walk.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep generative models use artificial neural networks to model the dataspace.
    One of the classic examples of a deep generative model is the autoencoder. Autoencoders
    contain two key components, the encoder and the decoder, both represented by neural
    networks. They learn how to take data and encode (compress) it into a low dimensional
    representation as well as decode (uncompress) it again. Figure 5.15 shows a basic
    autoencoder taking an image as input and compressing it (step 1). This results
    in the low dimensional representation, or latent space (step 2). The autoencoder
    then reconstructs the image (step 3), and the process is repeated until the reconstruction
    error between input image (x) and output image (x*) is as small as possible. The
    autoencoder is the basic idea behind GAEs and VGAEs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 Structure of an autoencoder [9]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.5.1 Understanding link prediction tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Link prediction is a common problem in graph-based learning, especially in situations
    where we have incomplete knowledge of our data. This might be because the graph
    changes over time, for example, where we expect new customers to use an e-commerce
    service, and we want a model that can give the best suggested products to buy
    at that time. Alternatively, it may be costly to acquire this knowledge, for example,
    if we want our model to predict which combinations of drugs lead to specific disease
    outcomes. Finally, our data may contain incorrect or purposefully hidden details,
    such as fake accounts on a social media platform. Link prediction allows us to
    infer relations *between* nodes in our graph. Essentially, this means creating
    a model that predicts when and how nodes are connected, as shown in figure 5.16\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 Schematic explaining how link prediction is performed in practice.
    Subsections of the input graph (subgraphs) are passed to the GNN with different
    links missing, and the model learns to predict when to recreate a link.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For link prediction, a model will take pairs of nodes as input and predict whether
    these nodes are connected (whether they should be *linked)*. To train a model,
    we’ll also need ground-truth targets. We generate these by hiding a subset of
    links within the graph. These hidden links become the missing data that we’ll
    learn to infer, which are known as *negative samples*. However, we also need a
    way to encode the information about pairs of nodes. Both of these parts can be
    solved simultaneously using GAEs, as autoencoders both encode information about
    the edge as well as predict whether an edge exists.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 The inner product decoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inner product decoders are used for graphs because we want to reconstruct the
    adjacency matrix from the latent representation of our feature data. The GAE learns
    how to rebuild a graph (to infer the edges) given a latent representation of the
    nodes. The inner product in high dimensional space calculates the distance between
    two positions. We use the inner product, rescaled by the sigmoid function, to
    gain a probability for an edge between nodes. Essentially, we use the distance
    between points in the latent space as a probability that a node will be connected
    when decoded. This allows us to build a decoder that takes samples from our latent
    space and returns probabilities of whether an edge exists, namely, to perform
    edge prediction, as shown in figure 5.17.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 Overall steps for training our model for link prediction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The inner product decoder works by taking the latent representation of our data
    and applying the inner product of this data using the passed edge index of our
    data. We then apply the sigmoid function to this value, which returns a matrix
    where each value represents the probability that there is an edge between the
    two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing the latent space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Put plainly, the KL divergence tells us how much worse we would be doing if
    we used the wrong probability density when estimating a probability of something.
    Suppose we have two coins and want to guess how well one coin (which we know is
    fair) matches the other coin (which we don’t know is fair). We’re trying to use
    the coin with the known probability to predict the probability for the coin with
    unknown probability. If it’s a good predictor (the unknown coin actually is fair),
    then the KL divergence will be zero. The probability densities of both coins are
    the same; however, if we find that the coin is a bad predictor, then the KL divergence
    will be large. This is because the two probability densities will be far from
    each other. In figure 5.18, we can see this explicitly. We’re trying to model
    the unknown probability density Q(z) using the conditional probability density
    P(Z|X). As the densities overlap, the KL divergence here will be low.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 KL divergence calculates the degree to which two probability densities
    are distinct. High KL divergence means that they are well separated, whereas low
    KL divergence means they are not.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Practically, we convert an autoencoder to a VGAE by introducing the KL divergence
    in the loss. Our intention here is to both minimize the discrepancy between our
    encoder and decoder as in the autoencoder loss, as well as minimize the difference
    between the probability distribution given by our encoder and the “true” distribution
    that was used to generate our data. This is done by adding the KL divergence to
    the loss. For many standard VGAE, this is given by
  prefs: []
  type: TYPE_NORMAL
- en: (5.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/2.png)'
  prefs: []
  type: TYPE_IMG
- en: where (*p*||*q*) denotes the divergence of probability *p* with respect to probability
    *q*. The term *m* is the mean value of the latent features, and log(var) is the
    logarithm of the variance. We use this in the loss function whenever we build
    a VGAE, ensuring that the forward pass returns both the mean and variance to our
    decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Over-squashing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve discussed how GNNs can be used to find out information about a node by
    propagating node and edge representations through message passing. These are used
    to make embeddings of individual nodes or edges, which help guide the model to
    perform some specific tasks. In this chapter, we discussed how to construct a
    model that constructs latent representations by propagating all of the embeddings
    created by the message-passing layers into a latent space. Both perform dimension
    reduction and representation learning of graph-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: However, GNNs have a specific limitation in how much information they can use
    to make representations. GNNs suffer from something known as *over-squashing*,
    which refers to how information that is spread many hops across the graph (i.e.,
    message passing) causes a considerable drop in performance. This is because the
    neighborhood that each node receives information from, also known as its receptive
    field, grows exponentially with the number of layers of the GNN. As more information
    is aggregated through message passing across these layers, the important signals
    from distant nodes become diluted compared to the information coming from nearer
    nodes. This causes the node representations to become similar or more homogenous,
    and eventually to converge to the same representations, also known as over-smoothing,
    which we discussed in chapter 4\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirical evidence has shown that this can start to occur with as few as three
    or four layers [7], as you can see in figure 5.19\. This highlights one of the
    key differences between GNNs and other deep learning architectures: we rarely
    want to make a very deep model with many layers stacked on top of each other.
    For models with many layers, other methods are often also introduced to ensure
    long-range information is included such as skip connections or attention mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19 Visualization of over-squashing (Source: Alon and Yahav [7])'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the previous example in this chapter, we spoke about using GNNs for drug
    discovery. Here, we considered an example where the graphs were relatively small.
    However, when graphs become larger, there is an increasing risk that long-range
    interactions become important. This is particularly true in chemistry and biology,
    where nodes at extreme ends of a graph can have an outsized influence on the overall
    properties of the graph. In the context of chemistry, these might be two atoms
    that are either ends of a large molecule and which decide the overall properties
    of the molecule such as its toxicity. The range of interactions or information
    flow that we need to consider to effectively model a problem is known as the *problem
    radius*. When designing a GNN, we need to make sure that the number of layers
    is at least as large as the problem radius.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are several methods for addressing over-squashing for GNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that not too many layers are stacked together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add in new “fake” edges between nodes that are very far/many hops apart or introduce
    a single node that is attached to all other nodes so that the problem radius is
    reduced to 2\.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use sampling, such as GraphSAGE, which samples from the neighborhood or introduces
    skip connections, which similarly skip some local neighbors. For sampling methods,
    it’s important to balance the loss of local information with the gain of more
    long-range information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these methods are highly problem specific, and you should think carefully
    about the type of interactions between nodes in your graph when deciding whether
    long-range interactions are important. For example, in the next chapter, we consider
    motion prediction where the head has likely little influence on the foot compared
    to the knee. Alternatively, molecular graphs as described in this chapter will
    likely have large influences from more distant nodes. Therefore, the most important
    part in resolving problems such as over-squashing is making sure you have a solid
    understanding of both your problem and data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discriminative models learn to separate data classes, while generative models
    learn to model the entire dataspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative models are often used to perform dimension reduction. Principal component
    analysis (PCA) is a form of linear dimension reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders contain two key components, the encoder and the decoder, both represented
    by neural networks. They learn how to take data and encode (compress) it into
    a low dimensional representation as well as decode (uncompress) it again. For
    autoencoders, the low dimensional representation is known as the latent space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs extend autoencoders to have a regularizing term in the loss. This regularizing
    term is typically the Kullback-Liebler (KL) divergence, which measures the difference
    between two distributions—the learned latent distribution and a prior distribution.
    The latent space of VAEs is more structured and continuous, where each point represents
    a probability density rather than a fixed-point encoding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders and variational autoencoders (VAEs) can also be applied to graphs.
    These are, respectively, graph autoencoders (GAE) and variational graph autoencoders
    (VGAE). They are similar to typical autoencoders and VAEs, but the decoder element
    is typically the dot product applied to the edge list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAEs and VGAEs are useful for edge prediction tasks. They can help us predict
    where there might be hidden edges in our graph.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
