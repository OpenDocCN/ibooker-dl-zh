- en: 'Chapter 12\. Magic Wand: Training a Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。魔杖：训练模型
- en: In [Chapter 11](ch11.xhtml#chapter_magic_wand_application), we used a 20 KB
    pretrained model to interpret raw accelerometer data, using it to identify which
    of a set of gestures was performed. In this chapter, we show you how this model
    was trained, and then we talk about how it actually works.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.xhtml#chapter_magic_wand_application)中，我们使用了一个20 KB的预训练模型来解释原始加速度计数据，用它来识别执行了一组手势中的哪一个。在本章中，我们将向您展示这个模型是如何训练的，然后我们将讨论它的实际工作原理。
- en: Our wake-word and person detection models both required large amounts of data
    to train. This is mostly due to the complexity of the problems they were trying
    to solve. There are a huge number of different ways in which a person can say
    “yes” or “no”—think of all the variations of accent, intonation, and pitch that
    make someone’s voice unique. Similarly, a person can appear in an image in an
    infinite variety of ways; you might see their face, their whole body, or a single
    hand, and they could be standing in any possible pose.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的唤醒词和人员检测模型都需要大量数据进行训练。这主要是由于它们试图解决的问题的复杂性。一个人说“是”或“不”有很多不同的方式——想想所有使某人的声音独特的口音、语调和音调的变化。同样，一个人在图像中出现的方式有无限多种可能；你可能看到他们的脸、整个身体或一个手，他们可能站在任何可能的姿势中。
- en: So that it can accurately classify such a diversity of valid inputs, a model
    needs to be trained on an equally diverse set of training data. This is why our
    datasets for wake-word and person detection training were so large, and why training
    takes so long.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够准确分类如此多样的有效输入，模型需要在同样多样的训练数据集上进行训练。这就是为什么我们的唤醒词和人员检测训练数据集如此庞大，以及为什么训练需要如此长时间。
- en: Our magic wand gesture recognition problem is a lot simpler. In this case, rather
    than trying to classify a huge range of natural voices or human appearances and
    poses, we’re attempting to understand the differences between three specific and
    deliberately selected gestures. Although there’ll be some variation in the way
    different people perform each gesture, we’re hoping that our users will strive
    to perform the gestures as correctly and uniformly as possible.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的魔杖手势识别问题要简单得多。在这种情况下，我们并不是试图对广泛范围的自然声音或人类外貌和姿势进行分类，而是试图理解三种特定和故意选择的手势之间的差异。虽然不同人执行每个手势的方式会有一些变化，但我们希望我们的用户会尽可能正确和统一地执行这些手势。
- en: This means that there’ll be a lot less variation in our expected valid inputs,
    which makes it a lot easier to train an accurate model without needing vast amounts
    of data. In fact, the dataset we’ll be using to train the model contains only
    around 150 examples for each gesture and is only 1.5 MB in size. It’s exciting
    to think about how a useful model can be trained on such a small dataset, because
    obtaining sufficient data is often the most difficult part of a machine learning
    project.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们期望的有效输入变化会少得多，这使得在不需要大量数据的情况下训练准确的模型变得更加容易。事实上，我们将用来训练模型的数据集每种手势只包含大约150个示例，总大小仅为1.5
    MB。想到一个有用的模型可以在如此小的数据集上训练，真是令人兴奋，因为获得足够的数据通常是机器学习项目中最困难的部分。
- en: In the first part of this chapter, you’ll learn how to train the original model
    used in the magic wand application. In the second part, we’ll talk about how this
    model actually works. And finally, you’ll see how you can capture your own data
    and train a new model that recognizes different gestures.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，您将学习如何训练魔杖应用程序中使用的原始模型。在第二部分，我们将讨论这个模型的实际工作原理。最后，您将看到如何捕获自己的数据并训练一个识别不同手势的新模型。
- en: Training a Model
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: To train our model, we use training scripts located in the TensorFlow repository.
    You can find them in [*magic_wand/train*](https://oreil.ly/LhZGT).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们使用了位于TensorFlow存储库中的训练脚本。您可以在[*magic_wand/train*](https://oreil.ly/LhZGT)中找到它们。
- en: 'The scripts perform the following tasks:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本执行以下任务：
- en: Prepare raw data for training.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练准备原始数据。
- en: Generate synthetic data.^([1](ch12.xhtml#idm46473549700952))
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成数据。^([1](ch12.xhtml#idm46473549700952))
- en: Split the data for training, validation, and testing.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练、验证和测试集。
- en: Perform data augmentation.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行数据增强。
- en: Define the model architecture.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型架构。
- en: Run the training process.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行训练过程。
- en: Convert the model into the TensorFlow Lite format.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型转换为TensorFlow Lite格式。
- en: To make life easy, the scripts are accompanied by a Jupyter notebook which demonstrates
    how to use them. You can run the notebook in Colaboratory (Colab) on a GPU runtime.
    With our tiny dataset, training will take only a few minutes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化生活，这些脚本附带了一个Jupyter笔记本，演示了如何使用它们。您可以在Colaboratory（Colab）上的GPU运行时中运行笔记本。使用我们的小数据集，训练只需要几分钟。
- en: To begin, let’s walk through the training process in Colab.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在Colab中走一遍训练过程。
- en: Training in Colab
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Colab中进行训练
- en: Open the Jupyter notebook at [*magic_wand/train/train_magic_wand_model.ipynb*](https://oreil.ly/2BLtj)
    and click the “Run in Google Colab” button, as shown in [Figure 8-1](ch08.xhtml#run_in_google_colab_2).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 打开[*magic_wand/train/train_magic_wand_model.ipynb*](https://oreil.ly/2BLtj)中的Jupyter笔记本，并单击“在Google
    Colab中运行”按钮，如[图8-1](ch08.xhtml#run_in_google_colab_2)所示。
- en: '![The ''Run in Google Colab'' button](Images/timl_0403.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![在Google Colab中运行按钮](Images/timl_0403.png)'
- en: Figure 12-1\. The “Run in Google Colab” button
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1。在Google Colab中运行按钮
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: As of this writing, there’s a bug in GitHub that results in intermittent error
    messages when displaying Jupyter notebooks. If you see the message “Sorry, something
    went wrong. Reload?” when trying to access the notebook, follow the instructions
    in [“Building Our Model”](ch04.xhtml#ch4_building_our_model).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，GitHub存在一个错误，导致在显示Jupyter笔记本时会出现间歇性错误消息。如果在尝试访问笔记本时看到消息“抱歉，出了点问题。重新加载？”，请按照[“构建我们的模型”](ch04.xhtml#ch4_building_our_model)中的说明操作。
- en: 'This notebook walks through the process of training the model. It includes
    the following steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本将演示训练模型的过程。它包括以下步骤：
- en: Installing dependencies
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装依赖项
- en: Downloading and preparing the data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载和准备数据
- en: Loading TensorBoard to visualize the training process
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载TensorBoard以可视化训练过程
- en: Training the model
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Generating a C source file
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成C源文件
- en: Enable GPU Training
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用GPU训练
- en: Training this model should be very quick, but it will be even faster if we use
    a GPU runtime. To enable this option, go to Colab’s Runtime menu and choose “Change
    runtime type,” as illustrated in [Figure 12-2](#change_runtime_type_2).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型应该非常快，但如果我们使用GPU运行时会更快。要启用此选项，请转到Colab的运行时菜单，并选择“更改运行时类型”，如图12-2所示。
- en: This opens the “Notebook settings” dialog box shown in [Figure 12-3](#notebook_settings_2).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开图12-3所示的“笔记本设置”对话框。
- en: From the “Hardware accelerator” drop-down list, select GPU, as depicted in [Figure 12-4](#hardware_accelerator_2),
    and then click SAVE.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从“硬件加速器”下拉列表中选择GPU，如图12-4所示，然后点击保存。
- en: You’re now ready to run the notebook.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好运行笔记本了。
- en: '![The ''Change runtime type'' option in Colab](Images/timl_0802.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![在Colab中更改运行时类型的选项](Images/timl_0802.png)'
- en: Figure 12-2\. The “Change runtime type” option in Colab
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2。在Colab中更改运行时类型的选项
- en: '![The ''Notebook settings'' box](Images/timl_0803.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本设置框](Images/timl_0803.png)'
- en: Figure 12-3\. The “Notebook settings” dialog box
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3。笔记本设置对话框
- en: '![The ''Hardware accelerator'' dropdown](Images/timl_0804.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![硬件加速器下拉列表](Images/timl_0804.png)'
- en: Figure 12-4\. The “Hardware accelerator” drop-down list
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4。硬件加速器下拉列表
- en: Install dependencies
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装依赖项
- en: The first step is to install the required dependencies. In the “Install dependencies”
    section, run the cells to install the correct versions of TensorFlow and grab
    a copy of the training scripts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装所需的依赖项。在“安装依赖项”部分，运行单元格安装正确版本的TensorFlow并获取训练脚本的副本。
- en: Prepare the data
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: Next, in the “Prepare the data” section, run the cells to download the dataset
    and split it into training, validation, and test sets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在“准备数据”部分，运行单元格下载数据集并将其分割为训练、验证和测试集。
- en: 'The first cell downloads and extracts the dataset into the training scripts’
    directory. The dataset consists of four directories, one for each gesture (“wing,”
    “ring,” and “slope”) plus a “negative” directory for data that represents no distinct
    gesture. Each directory contains files that represent raw data resulting from
    the capture process for the gesture being performed:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个单元格下载并提取数据集到训练脚本目录。数据集包括四个目录，一个用于每个手势（“wing”，“ring”和“slope”），另一个“negative”目录用于表示没有明显手势的数据。每个目录包含代表手势执行过程中捕获的原始数据的文件：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are 10 files for each gesture, which we’ll walk through later on. Each
    file contains a gesture being demonstrated by a named individual, with the last
    part of the filename corresponding to their user ID. For example, the file *output_slope_dengyl.txt*
    contains data for the “slope” gesture being demonstrated by a user whose ID is
    `dengyl`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个手势有10个文件，我们稍后会详细介绍。每个文件包含一个由命名个体演示的手势，文件名的最后部分对应其用户ID。例如，文件*output_slope_dengyl.txt*包含了用户ID为`dengyl`的用户演示“slope”手势的数据。
- en: 'There are approximately 15 individual performances of a given gesture in each
    file, one accelerometer reading per row, with each performance being prefixed
    by the row `-,-,-`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件中大约有15次给定手势的表演，每行一个加速度计读数，每次表演都以行`-,-,-`开头：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each performance consists of a log of up to a few seconds’ worth of data, with
    25 rows per second. The gesture itself occurs at some point within that window,
    with the device being held still for the remainder of the time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每次表演包括几秒钟的数据日志，每秒25行。手势本身发生在该窗口内的某个时间点，设备在其余时间内保持静止。
- en: 'Due to the way the measurements were captured, the files also contain some
    garbage characters. Our first training script, [*data_prepare.py*](https://oreil.ly/SCZe9),
    which is run in our second training cell, will clean up this dirty data:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于测量数据的捕获方式，文件中还包含一些垃圾字符。我们的第一个训练脚本[*data_prepare.py*](https://oreil.ly/SCZe9)，将在第二个训练单元格中运行，将清理这些脏数据：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This script is designed to read the raw data files from their folders, ignore
    any garbage characters, and write them in a sanitized form to another location
    within the training scripts’ directory (*data/complete_data*). Cleaning up messy
    data sources is a common task when training machine learning models given that
    it’s very common for errors, corruption, and other issues to creep into large
    datasets.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本旨在从文件夹中读取原始数据文件，忽略任何垃圾字符，并将它们以经过清理的形式写入到训练脚本目录内的另一个位置（*data/complete_data*）。清理混乱的数据源是训练机器学习模型时的常见任务，因为大型数据集很容易出现错误、损坏和其他问题。
- en: In addition to cleaning the data, the script generates some *synthetic data*.
    This is a term for data that is generated algorithmically, rather than being captured
    from the real world. In this case, the `generate_negative_data()` function in
    *data_prepare.py* creates synthetic data that is equivalent to movement of the
    accelerometer that doesn’t correspond to any particular gesture. This data is
    used to train our “unknown” category.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了清理数据，脚本还生成了一些*合成数据*。这是指通过算法生成的数据，而不是从现实世界中捕获的数据。在这种情况下，*data_prepare.py*中的`generate_negative_data()`函数创建了相当于加速度计移动但不对应任何特定手势的合成数据。这些数据用于训练我们的“未知”类别。
- en: Because creating synthetic data is much faster than capturing real-world data,
    it’s useful to help augment our training process. However, real-world variation
    is unpredictable, so it’s not often possible to create an entire dataset from
    synthetic data. In our case, it’s helpful for making our “unknown” category more
    robust, but it wouldn’t be helpful for classifying the known gestures.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成合成数据比捕获现实世界数据要快得多，因此有助于增强我们的训练过程。然而，现实世界的变化是不可预测的，因此往往不可能完全使用合成数据创建整个数据集。在我们的情况下，这有助于使我们的“未知”类别更加健壮，但对于分类已知手势并不有用。
- en: 'The next script to run in the second cell is [*data_split_person.py*](https://oreil.ly/1U0FW):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个单元格中运行的下一个脚本是[*data_split_person.py*](https://oreil.ly/1U0FW)：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This script splits the data into training, validation, and test sets. Because
    our data is labeled with the person who created it, we’re able to use one set
    of people’s data for training, another set for validation, and a final set for
    test. The data is split as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本将数据分成训练、验证和测试集。因为我们的数据带有创建者的标签，我们可以使用一个人的数据进行训练，另一个人的数据进行验证，最后一个人的数据进行测试。数据分割如下：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use six people’s data for training, two for validation, and two for testing.
    In addition, we mix in our negative data, which isn’t associated with a particular
    user. Our total data is split between the three sets at a ratio of roughly 60%/20%/20%,
    which is pretty standard for machine learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用六个人的数据进行训练，两个用于验证，两个用于测试。此外，我们混合了与特定用户无关的负面数据。我们的总数据在三个集合之间以大约60%/20%/20%的比例分配，这对于机器学习来说是相当标准的。
- en: In splitting by person, we’re trying to ensure that our model will be able to
    generalize to new data. Because the model will be validated and tested on data
    from individuals who were not included in the training dataset, the model will
    need to be robust against individual variations in how each person performs each
    gesture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按个人分割，我们试图确保我们的模型能够推广到新数据。因为模型将在未包含在训练数据集中的个体数据上进行验证和测试，所以模型需要对每个人执行每个手势的方式的个体变化具有鲁棒性。
- en: It’s also possible to split the data randomly, instead of by person. In this
    case, the training, validation, and testing datasets would each contain some samples
    of each gesture from every single individual. The resulting model will have been
    trained on data from every single person rather than just six, so it will have
    had more exposure to people’s varying gesturing styles.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以随机分割数据，而不是按个人分割。在这种情况下，训练、验证和测试数据集将分别包含每个个体的每个手势的一些样本。由此产生的模型将被训练在每个人的数据上，而不仅仅是六个人，因此它将更多地接触到人们不同的手势风格。
- en: However, because the validation and training sets also contain data from every
    individual, we’d have no way of testing whether the model is able to generalize
    to new gesturing styles that it has not seen before. A model developed in this
    way might report higher accuracy during validation and testing, but it would not
    be guaranteed to work as well with new data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于验证和训练集也包含来自每个个体的数据，我们无法测试模型是否能够推广到之前未见过的新手势风格。以这种方式开发的模型可能在验证和测试过程中报告更高的准确性，但不能保证在新数据上的表现同样出色。
- en: Make sure you’ve run both cells in the “Prepare the data” section before continuing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您已经运行了“准备数据”部分中的两个单元格。
- en: Load TensorBoard
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载TensorBoard
- en: 'After the data has been prepared, we can run the next cell to load TensorBoard,
    which will help us monitor the training process:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好后，我们可以运行下一个单元格来加载TensorBoard，这将帮助我们监视训练过程：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training logs will be written to the *logs/scalars* subdirectory of the training
    scripts’ directory, so we pass this in to TensorBoard.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志将被写入训练脚本目录下的*logs/scalars*子目录中，因此我们将其传递给TensorBoard。
- en: Begin training
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始训练
- en: 'After TensorBoard has loaded, it’s time to begin training. Run the following
    cell:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard加载完成后，现在是开始训练的时候了。运行以下单元格：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The script [*train.py*](https://oreil.ly/S3w0X) sets up the model architecture,
    loads the data using [*data_load.py*](https://oreil.ly/aCZgu), and begins the
    training process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本[*train.py*](https://oreil.ly/S3w0X)设置了模型架构，使用[*data_load.py*](https://oreil.ly/aCZgu)加载数据，并开始训练过程。
- en: As the data is loaded, *load_data.py* also performs data augmentation using
    code defined in [*data_augmentation.py*](https://oreil.ly/zL6wm). The function
    `augment_data()` takes data representing a gesture and creates a number of new
    versions of it, each modified slightly from the original. The modifications include
    shifting and warping the datapoints in time, adding random noise, and increasing
    the amount of acceleration. This augmented data is used alongside the original
    data to train the model, helping make the most of our small dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据加载时，*load_data.py*还使用[*data_augmentation.py*](https://oreil.ly/zL6wm)中定义的代码执行数据增强。函数`augment_data()`接受表示手势的数据，并创建一些稍微修改的新版本，每个版本都与原始数据略有不同。修改包括在时间上移动和扭曲数据点，添加随机噪声，以及增加加速度的量。这些增强数据与原始数据一起用于训练模型，有助于充分利用我们的小数据集。
- en: 'As training ramps up, you’ll see some output appearing below the cell you just
    ran. There’s a lot there, so let’s pick out the most noteworthy parts. First,
    Keras generates a nice table that shows the architecture of our model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的加速，您将看到一些输出出现在您刚刚运行的单元格下方。那里有很多内容，让我们挑出最值得注意的部分。首先，Keras生成了一个漂亮的表格，显示了我们模型的架构：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It tells us all the layers that are used, along with their shapes and their
    numbers of parameters—which is another term for weights and biases. You can see
    that our model uses `Conv2D` layers, as it’s a convolutional model. Not shown
    in this table is the fact that our model’s input shape is `(None, 128, 3)`. We’ll
    look more closely at the model’s architecture later.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它告诉我们所使用的所有层，以及它们的形状和参数数量——这是权重和偏差的另一个术语。您可以看到我们的模型使用了`Conv2D`层，因为它是一个卷积模型。在这个表中没有显示的是我们模型的输入形状是`(None,
    128, 3)`。我们稍后会更仔细地查看模型的架构。
- en: 'The output will also show us an estimate of the model’s size:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还将显示模型大小的估计：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This represents the amount of memory that will be taken up by the model’s trainable
    parameters. It doesn’t include the extra space required to store the model’s execution
    graph, so our actual model file will be slightly larger, but it gives us an idea
    of the correct order of magnitude. This will definitely qualify as a tiny model!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了模型可训练参数所占用的内存量。它不包括存储模型执行图所需的额外空间，因此我们的实际模型文件会稍微大一些，但这给我们一个正确数量级的概念。这绝对可以被称为一个微小模型！
- en: 'You’ll eventually see the training process itself begin:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最终您将看到训练过程本身开始：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: At this point, you can take a look at TensorBoard to see the training process
    moving along.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可以查看TensorBoard，以查看训练过程的进行情况。
- en: Evaluate the results
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'When training is complete, we can look at the cell’s output for some useful
    information. First, we can see that the validation accuracy in our final epoch
    looks very promising at 0.9743, and the loss is nice and low, too:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以查看单元格的输出以获取一些有用的信息。首先，我们可以看到我们最终时期的验证准确率非常有希望，为0.9743，损失也很低：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is great, especially as we’re using a per-person data split, meaning our
    validation data is from a completely different set of individuals. However, we
    can’t just rely on our validation accuracy to evaluate our model. Because the
    model’s hyperparameters and architecture were hand-tuned on the validation dataset,
    we might have overfit it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这很棒，特别是因为我们使用了按人员数据拆分，这意味着我们的验证数据来自完全不同的一组个体。然而，我们不能仅仅依靠我们的验证准确性来评估我们的模型。因为模型的超参数和架构是在验证数据集上手动调整的，我们可能已经过度拟合了。
- en: 'To get a better understanding of our model’s final performance, we can evaluate
    it against our test dataset by calling Keras’s `model.evaluate()` function. The
    next line of output shows the results of this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解我们模型的最终性能，我们可以通过调用Keras的`model.evaluate()`函数来评估它与我们的测试数据集的表现。下一行输出显示了这个结果：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Although not as amazing as the validation numbers, the model shows a good-enough
    accuracy of 0.9323, with a loss that is still low. The model will predict the
    correct class 93% of the time, which should be fine for our purposes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管验证数字没有那么惊人，但模型显示了一个足够好的准确率为0.9323，损失仍然很低。该模型将在93%的时间内预测正确的类别，这对我们的目的应该是可以接受的。
- en: 'The next few lines show the *confusion matrix* for the results, calculated
    by the [`tf.math.confusion_matrix()`](https://oreil.ly/xlIKj) function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几行显示了结果的*混淆矩阵*，由[`tf.math.confusion_matrix()`](https://oreil.ly/xlIKj)函数计算：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A confusion matrix is a helpful tool for evaluating the performance of classification
    models. It shows how well the predicted class of each input in the test dataset
    agrees with its actual value.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是评估分类模型性能的有用工具。它显示了测试数据集中每个输入的预测类别与其实际值的一致程度。
- en: 'Each column of the confusion matrix corresponds to a predicted label, in order
    (“wing,” “ring,” “slope,” then “unknown”). Each row, from the top down, corresponds
    to the actual label. From our confusion matrix, we can see that the vast majority
    of predictions agree with the actual labels. We can also see the specific places
    where confusion is occurring: most significantly, a fair number of inputs were
    misclassified as “unknown,” especially those belonging to the “ring” category.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵的每一列对应于一个预测标签，依次为“wing”，“ring”，“slope”，然后“unknown”。从上到下，每一行对应于实际标签。从我们的混淆矩阵中，我们可以看到绝大多数预测与实际标签一致。我们还可以看到混淆发生的具体位置：最显著的是，相当多的输入被错误分类为“unknown”，特别是属于“ring”类别的输入。
- en: The confusion matrix gives us an idea of where our model’s weak points are.
    In this case, it informs us that it might be beneficial to obtain more training
    data for the “ring” gesture in order to help the model better learn the differences
    between “ring” and “unknown.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵让我们了解模型的弱点在哪里。在这种情况下，它告诉我们，为了帮助模型更好地学习“ring”和“unknown”之间的差异，获取更多的“ring”手势的训练数据可能是有益的。
- en: 'The final thing that *train.py* does is convert the model to TensorFlow Lite
    format, in both floating-point and quantized variations. The following output
    reveals the sizes of each variant:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*train.py*的最后一步是将模型转换为TensorFlow Lite格式，包括浮点和量化变体。以下输出显示了每个变体的大小：'
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our 20 KB model shrinks down to 8.8 KB after quantization. This is a *very*
    tiny model, and a great result.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的20 KB模型在量化后缩小到8.8 KB。这是一个*非常*小的模型，是一个很好的结果。
- en: Create a C array
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个C数组
- en: 'The next cell, in the “Create a C source file” section, transforms this into
    a C source file. Run this cell to see the output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在“创建C源文件”部分中的下一个单元格将其转换为C源文件。运行此单元格以查看输出：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can copy and paste the contents of this file into our project so that we
    can use the newly trained model in our application. Later, you’ll learn how to
    collect new data and teach the application to understand new gestures. For now,
    let’s keep moving.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此文件的内容复制粘贴到我们的项目中，以便我们可以在我们的应用程序中使用新训练的模型。稍后，您将学习如何收集新数据并教导应用程序理解新的手势。现在，让我们继续前进。
- en: Other Ways to Run the Scripts
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行脚本的其他方法
- en: If you’d prefer not to use Colab, or you’re making changes to the model training
    scripts and would like to test them out locally, you can easily run the scripts
    from your own development machine. You can find the instructions in [*README.md*](https://oreil.ly/6-KPf).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用Colab，或者您正在更改模型训练脚本并希望在本地测试它们，您可以轻松地从自己的开发机器上运行这些脚本。您可以在[*README.md*](https://oreil.ly/6-KPf)中找到说明。
- en: Next up, we walk through how the model itself works.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍模型本身的工作原理。
- en: How the Model Works
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的工作原理
- en: 'So far, we’ve established that our model is a convolutional neural network
    (CNN) and that it transforms a sequence of 128 three-axis accelerometer readings,
    representing around five seconds of time, into an array of four probabilities:
    one for each gesture, and one for “unknown.”'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经确定我们的模型是一个卷积神经网络（CNN），它将表示大约五秒时间的128个三轴加速度计读数序列转换为四个概率数组：一个用于每个手势，一个用于“unknown”。
- en: CNNs are used when the relationships between adjacent values contain important
    information. In the first part of our explanation, we’ll take a look at our data
    and learn why a CNN is well suited to making sense of it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当相邻值之间的关系包含重要信息时，CNNs被用来。在我们解释的第一部分中，我们将查看我们的数据并了解为什么CNN非常适合理解它。
- en: Visualizing the Input
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化输入
- en: In our time-series accelerometer data, adjacent accelerometer readings give
    us clues about the device’s motion. For example, if acceleration on one axis changes
    rapidly from zero to positive, then back to zero, the device might have begun
    motion in that direction. [Figure 12-5](#gesture_single_axis_acceleration_graph)
    shows a hypothetical example of this.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的时间序列加速度计数据中，相邻的加速度计读数给我们关于设备运动的线索。例如，如果一个轴上的加速度从零迅速变为正值，然后再回到零，那么设备可能已经开始朝着那个方向运动。[图12-5](#gesture_single_axis_acceleration_graph)展示了这种假设性示例。
- en: '![A graph of accelerometer values for a single axis of a device being moved](Images/timl_1205.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![显示设备单轴加速度计值的图表](Images/timl_1205.png)'
- en: Figure 12-5\. Accelerometer values for a single axis of a device being moved
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5. 设备单轴加速度计值
- en: Any given gesture is composed of a series of motions, one after the other. For
    example, consider our “wing” gesture, shown in [Figure 12-6](#gesture_wing_gesture).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 任何给定的手势由一系列运动组成，一个接着一个。例如，考虑我们的“翼”手势，如[图12-6](#gesture_wing_gesture)所示。
- en: '![Diagram showing the ''wing'' gesture](Images/timl_1206.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![显示“翼”手势的图表](Images/timl_1206.png)'
- en: Figure 12-6\. The “wing” gesture
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6. “翼”手势
- en: The device is first moved down and to the right, then up and to the right, then
    down and to the right, then up and to the right again. [Figure 12-7](#gesture_wing_gesture_graph)
    shows a sample of real data captured during the “wing” gesture, measured in milli-Gs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 设备首先向下和向右移动，然后向上和向右移动，然后再向下和向右移动，然后再向上和向右移动。[图12-7](#gesture_wing_gesture_graph)显示了在“翼”手势期间捕获的实际数据样本，以毫G为单位测量。
- en: '![A graph of accelerometer values during the ''wing'' gesture](Images/timl_1207.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![在“翼”手势期间加速度计值的图表](Images/timl_1207.png)'
- en: Figure 12-7\. Accelerometer values during the “wing” gesture
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7. “翼”手势期间的加速度计值
- en: By looking at this graph and breaking it down into its component parts, we can
    understand which gesture is being made. From the z-axis acceleration, it’s very
    clear that the device is being moved up and down in the way we would expect given
    the “wing” gesture’s shape. More subtly, we can see how the acceleration on the
    x-axis correlates with the z-axis changes in a way that indicates the device’s
    motion across the width of the gesture. Meanwhile, we can observe that the y-axis
    remains mostly stable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这个图表并将其分解为其组成部分，我们可以理解正在进行的手势。从z轴加速度来看，很明显设备正在上下移动，这符合我们对“翼”手势形状的预期。更微妙的是，我们可以看到x轴上的加速度如何与z轴的变化相关联，表明设备在手势的宽度方向上移动。同时，我们可以观察到y轴基本保持稳定。
- en: Similarly, a CNN with multiple layers is able to learn how to discern each gesture
    through its telltale component parts. For example, a network might learn to distinguish
    an up-and-down motion, and that two of them, when combined with the appropriate
    z- and y-axis movements, indicates a “wing” gesture.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，具有多层的CNN能够学习如何通过其特征组件部分来辨别每个手势。例如，网络可能学会区分上下运动，并且当与适当的z轴和y轴运动结合时，表示“翼”手势的两个运动。
- en: To do this, a CNN learns a series of *filters*, arranged in layers. Each filter
    learns to spot a particular type of feature in the data. When it notices this
    feature, it passes this high-level information to the next layer of the network.
    For example, one filter in the first layer of the network might learn to spot
    something simple, like a period of upward acceleration. When it identifies such
    a structure, it passes this information to the next layer of the network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，CNN学习一系列*滤波器*，排列在层中。每个滤波器学会在数据中发现特定类型的特征。当它注意到这个特征时，它将这个高级信息传递给网络的下一层。例如，网络的第一层中的一个滤波器可能学会发现一些简单的东西，比如一个向上加速的周期。当它识别到这样的结构时，它将这些信息传递给网络的下一层。
- en: Subsequent layers of filters learn how the outputs of earlier, simpler filters
    are composed together to form larger structures. For example, a series of four
    alternating upward and downward accelerations might fit together to represent
    the “W” shape in our “wing” gesture.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 后续的滤波器层学习如何将早期更简单的滤波器的输出组合在一起形成更大的结构。例如，一系列四个交替的向上和向下的加速度可能组合在一起表示我们“翼”手势中的“W”形状。
- en: In this process, the noisy input data is progressively transformed into a high-level,
    symbolic representation. Subsequent layers of our network can analyze this symbolic
    representation to guess which gesture was performed.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，嘈杂的输入数据逐渐转化为高级符号表示。我们网络的后续层可以分析这个符号表示，猜测执行了哪个手势。
- en: In the next section, we walk through the actual model architecture and see how
    it maps onto this process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将详细介绍实际的模型架构，并看看它如何映射到这个过程中。
- en: Understanding the Model Architecture
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解模型架构
- en: 'The architecture of our model is defined in [*train.py*](https://oreil.ly/vxT1v),
    in the `build_cnn()` function. This function uses the Keras API to define a model,
    layer by layer:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的架构在[*train.py*](https://oreil.ly/vxT1v)中定义，在`build_cnn()`函数中。这个函数使用Keras
    API逐层定义模型：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is a sequential model, meaning the output of each layer is passed directly
    into the next one. Let’s walk through the layers one by one and explore what’s
    going on. The first layer is a `Conv2D`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序模型，意味着每一层的输出直接传递到下一层。让我们逐层走过并探索正在发生的事情。第一层是一个`Conv2D`：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is a convolutional layer; it directly receives our network’s input, which
    is a sequence of raw accelerometer data. The input’s shape is provided in the
    `input_shape` argument. It’s set to `(seq_length, 3, 1)`, where `seq_length` is
    the total number of accelerometer measurements that are passed in (128 by default).
    Each measurement is composed of three values, representing the x-, y-, and z-axes.
    The input is visualized in [Figure 12-8](#gesture_model_input_diagram).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个卷积层；它直接接收我们网络的输入，这是一系列原始加速度计数据。输入的形状在`input_shape`参数中提供。它设置为`(seq_length,
    3, 1)`，其中`seq_length`是传入的加速度计测量的总数（默认为128）。每个测量由三个值组成，表示x轴、y轴和z轴。输入在[图12-8](#gesture_model_input_diagram)中可视化。
- en: '![A diagram of the model''s input](Images/timl_1208.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. The model’s input
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The job of our convolutional layer is to take this raw data and extract some
    basic features that can be interpreted by subsequent layers. The arguments to
    the `Conv2D()` function determine how many features will be extracted. The arguments
    are described in the [`tf.keras.layers.Conv2D()` documentation](https://oreil.ly/hqXJF).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The first argument determines how many filters the layer will have. During training,
    each filter learns to identify a particular feature in the raw data—for example,
    one filter might learn to identify the telltale signs of an upward motion. For
    each filter, the layer outputs a *feature map* that shows where the feature it
    has learned occurs within the input.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The layer defined in our code has eight filters, meaning that it will learn
    to recognize and output eight different types of high-level features from the
    input data. You can see this reflected in the output shape, `(batch_size, 128,
    3, 8)`, which has eight *feature channels* in its final dimension, one for each
    feature. The value in each channel indicates the degree to which a feature was
    present in that location of the input.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in [Chapter 8](ch08.xhtml#chapter_training_micro_speech), convolutional
    layers slide a window across the data and decide whether a given feature is present
    in that window. The second argument to `Conv2D()` is where we provide the dimensions
    of this window. In our case, it’s `(4, 3)`. This means that the features for which
    our filters are hunting span four consecutive accelerometer measurements and all
    three axes. Because the window spans four measurements, each filter analyzes a
    small snapshot of time, meaning it can generate features that represent a change
    in acceleration over time. You can see how this works in [Figure 12-9](#gesture_model_convolutional_window_diagram).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a convolution window overlaid on the data](Images/timl_1209.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 12-9\. A convolution window overlaid on the data
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `padding` argument determines how the window will be moved across the data.
    When `padding` is set to `"same"`, the layer’s output will have the same length
    (128) and width (3) as the input. Because every movement of the filter window
    results in a single output value, the `"same"` argument means the window must
    be moved three times across the data, and 128 times down it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Because the window has a width of 3, this means it must start by overhanging
    the lefthand side of the data. The empty spaces, where the filter window doesn’t
    cover an actual value, are *padded* with zeros. To move a total of 128 times down
    the length of the data, the filter must also overhang the top of the data. You
    can see how this works in Figures [12-10](#gesture_model_convolutional_window_padding)
    and [12-11](#gesture_model_convolutional_window_padding_2).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as the convolution window has moved across all the data, using each
    filter to create eight different feature maps, the output will be passed to our
    next layer, `MaxPool2D`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![A diagram of a convolution window moving across the data](Images/timl_1210.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. The convolution window in its first position, necessitating padding
    on the top and left sides
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A diagram of a convolution window moving across the data](Images/timl_1211.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 12-11\. The same convolution window having moved to its second position,
    requiring padding only on the top
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This `MaxPool2D` layer takes the output of the previous layer, a `(128, 3, 8)`
    tensor, and shrinks it down to a `(42, 1, 8)` tensor—a third of its original size.
    It does this by looking at a window of input data and then selecting the largest
    value in the window and propagating only that value to the output. The process
    is then repeated with the next window of data. The argument provided to the [`MaxPool2D()`](https://oreil.ly/HZo0q)
    function, `(3, 3)`, specifies that a 3 × 3 window should be used. By default,
    the window is always moved so that it contains entirely new data. [Figure 12-12](#gesture_model_max_pooling)
    shows how this process works.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`MaxPool2D`层接收前一层的输出，一个`(128, 3, 8)`张量，并将其缩小为一个`(42, 1, 8)`张量——原始大小的三分之一。它通过查看输入数据的窗口，然后选择窗口中的最大值，并将仅该值传播到输出中来实现这一点。然后，该过程会重复下一个数据窗口。提供给[`MaxPool2D()`](https://oreil.ly/HZo0q)函数的参数`(3,
    3)`指定使用一个3×3的窗口。默认情况下，窗口总是移动，以包含全新的数据。[图12-12](#gesture_model_max_pooling)展示了这个过程是如何工作的。
- en: '![A diagram of max pooling at work](Images/timl_1212.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![一个最大池化工作的示意图](Images/timl_1212.png)'
- en: Figure 12-12\. Max pooling at work
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-12。最大池化的工作
- en: Note that although the diagram shows a single value for each element, our data
    actually has eight feature channels per element.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管图中每个元素只显示了一个值，但我们的数据实际上每个元素有八个特征通道。
- en: But why do we need to shrink our input like this? When used for classification,
    the goal of a CNN is to transform a big, complex input tensor into a small, simple
    output. The `MaxPool2D` layer helps make this happen. It boils down the output
    of our first convolutional layer into a concentrated, high-level representation
    of the relevant information that it contains.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么我们需要像这样缩小我们的输入呢？当用于分类时，CNN的目标是将一个大而复杂的输入张量转换为一个小而简单的输出。`MaxPool2D`层有助于实现这一目标。它将我们第一个卷积层的输出浓缩成一个集中的、高级别的表示，其中包含的相关信息。
- en: By concentrating the information, we begin to strip out things that aren’t relevant
    to the task of identifying which gesture was contained within the input. Only
    the most significant features, which were maximally represented in the first convolutional
    layer’s output, are preserved. It’s interesting to note that even though our original
    input had three accelerometer axes for each measurement, a combination of `Conv2D`
    and `MaxPool2D` has now merged these together into a single value.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过集中信息，我们开始剥离那些与识别输入中包含的手势无关的内容。只有在第一个卷积层的输出中最大程度地表示的最重要的特征被保留下来。有趣的是，即使我们的原始输入每次测量都有三个加速度计轴，但`Conv2D`和`MaxPool2D`的组合现在已经将它们合并成一个单一的值。
- en: 'After we’ve shrunk our data down, it goes through a [`Dropout` layer](https://oreil.ly/JuQtU):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将数据缩小后，它经过了一个[`Dropout`层](https://oreil.ly/JuQtU)：
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `Dropout` layer randomly sets some of a tensor’s values to zero during training.
    In this case, by calling `Dropout(0.1)`, we set 10% of the values to zero, entirely
    obliterating that data. This might seem like a strange thing to do, so let’s explain.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout`层在训练期间会随机将张量的一些值设为零。在这种情况下，通过调用`Dropout(0.1)`，我们将10%的值设为零，完全消除了这些数据。这可能看起来像是一种奇怪的做法，让我们解释一下。'
- en: '*Dropout* is a regularization technique. As mentioned earlier in the book,
    *regularization* is the process of improving machine learning models so that they
    are less likely to overfit their training data. Dropout is a simple but effective
    way to limit overfitting. By randomly removing some data between one layer and
    the next, we force the neural network to learn how to cope with unexpected noise
    and variation. Adding dropout between layers is a common and effective practice.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout*是一种正则化技术。正如本书前面提到的，*正则化*是改进机器学习模型的过程，使其不太可能过度拟合训练数据。Dropout是一种简单但有效的限制过拟合的方法。通过在一层和下一层之间随机删除一些数据，我们迫使神经网络学习如何应对意外的噪音和变化。在层之间添加dropout是一种常见且有效的做法。'
- en: The dropout layer is only active during training. During inference, it has no
    effect; all of the data is allowed through.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout层只在训练期间激活。在推断期间，它没有任何效果；所有数据都被允许通过。
- en: 'After the `Dropout` layer, we again feed the data through a `MaxPool2D` layer
    and a `Dropout` layer:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Dropout`层之后，我们再次通过一个`MaxPool2D`层和一个`Dropout`层传递数据：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This layer has 16 filters and a window size of `(4, 1)`. These numbers are part
    of the model’s hyperparameters, and they were chosen in an iterative process while
    the model was being developed. Designing an effective architecture is a process
    of trial and error, and these magic numbers are what was arrived at after a lot
    of experimentation. It’s unlikely that you’ll ever select the exact right values
    the first time around.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层有16个过滤器和一个窗口大小为`(4, 1)`。这些数字是模型的超参数的一部分，在模型开发过程中通过迭代过程选择。设计一个有效的架构是一个反复试验的过程，这些神奇的数字是在经过大量实验后得出的。你不太可能第一次就选择到完全正确的值。
- en: Like the first convolutional layer, this one also learns to spot patterns in
    adjacent values that contain meaningful information. Its output is an even higher-level
    representation of the content of a given input. The features it recognizes are
    compositions of the features identified by our first convolutional layer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个卷积层一样，这一层也学会了发现包含有意义信息的相邻值的模式。它的输出是给定输入内容的更高级表示。它识别的特征是我们第一个卷积层识别的特征的组合。
- en: 'After this convolutional layer, we do another `MaxPool2D` and `Dropout`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个卷积层之后，我们再做一次`MaxPool2D`和`Dropout`：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This continues the process of distilling the original input down to a smaller,
    more manageable representation. The output, with a shape of `(14, 1, 16)`, is
    a multidimensional tensor that symbolically represents only the most significant
    structures contained within the input data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这继续了将原始输入精炼为更小、更易管理的表示的过程。输出的形状为`(14, 1, 16)`，是一个多维张量，象征性地表示了输入数据中只包含的最重要的结构。
- en: If we wanted to, we could continue with the process of convolution and pooling.
    The number of layers in a CNN is just another hyperparameter that we can tune
    during model development. However, during the development of this model, we found
    that two convolutional layers was sufficient.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，我们可以继续卷积和池化的过程。CNN中的层数只是我们可以在模型开发过程中调整的另一个超参数。然而，在开发这个模型的过程中，我们发现两个卷积层已经足够了。
- en: 'Up until this point, we’ve been running our data through convolutional layers,
    which care only about the relationships between adjacent values—we haven’t really
    been considering the bigger picture. However, because we now have high-level representations
    of the major features contained within our input, we can “zoom out” and study
    them in aggregate. To do so, we flatten our data and feed it into a `Dense` layer
    (also known as a *fully connected layer*):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在通过卷积层运行我们的数据，这些层只关心相邻值之间的关系——我们并没有真正考虑更大的整体情况。然而，由于我们现在有了包含在我们输入中的主要特征的高级表示，我们可以“放大”并以总体方式研究它们。为此，我们将我们的数据展平并将其输入到一个`Dense`层（也称为*全连接层*）中：
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The [`Flatten` layer](https://oreil.ly/TUIZc) is used to transform a multidimensional
    tensor into one with a single dimension. In this case, our `(14, 1, 16)` tensor
    is squished down into a single dimension with shape `(224)`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[`Flatten`层](https://oreil.ly/TUIZc)用于将多维张量转换为具有单个维度的张量。在这种情况下，我们的`(14, 1,
    16)`张量被压缩成一个形状为`(224)`的单个维度。'
- en: 'It’s then fed into a [`Dense` layer](https://oreil.ly/FbpDB) with 16 neurons.
    This is one of the most basic tools in the deep learning toolbox: a layer where
    every input is connected to every neuron. By considering all of our data, all
    at once, this layer can learn the meanings of various combinations of inputs.
    The output of this `Dense` layer will be a set of 16 values representing the content
    of the original input in a highly compressed form.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其输入到具有16个神经元的[`Dense`层](https://oreil.ly/FbpDB)中。这是深度学习工具箱中最基本的工具之一：每个输入都连接到每个神经元的层。通过一次考虑所有数据，这一层可以学习各种输入组合的含义。这个`Dense`层的输出将是一组16个值，代表原始输入的内容以高度压缩的形式。
- en: 'Our final task is to shrink these 16 values down into 4 classes. To do this,
    we first add some more dropout and then a final `Dense` layer:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后任务是将这16个值缩小为4个类。为此，我们首先添加一些更多的dropout，然后添加一个最终的`Dense`层：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This layer has four neurons; one representing each class of gesture. Each of
    them is connected to all 16 of the outputs from the previous layer. During training,
    each neuron will learn the combination of previous-layer activations that correspond
    to the gesture it represents.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层有四个神经元；每个代表一个手势类。它们中的每一个都连接到前一层的所有16个输出。在训练过程中，每个神经元将学习与其代表的手势相对应的前一层激活的组合。
- en: The layer is configured with a `"softmax"` activation function, which results
    in the layer’s output being a set of probabilities that sum to 1\. This output
    is what we see in the model’s output tensor.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该层配置了一个`"softmax"`激活函数，导致该层的输出是一组总和为1的概率。这个输出是我们在模型输出张量中看到的。
- en: This type of model architecture—a combination of convolutional and fully connected
    layers—is very useful in classifying time-series sensor data like the measurements
    we obtain from our accelerometer. The model learns to identify the high-level
    features that represent the “fingerprint” of a particular class of input. It’s
    small, runs fast, and doesn’t take long to train. This architecture will be a
    valuable tool in your belt as an embedded machine learning engineer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型架构——卷积和全连接层的组合——在分类时间序列传感器数据方面非常有用，比如我们从加速度计获取的测量数据。该模型学习识别代表特定输入类的“指纹”的高级特征。它小巧、运行快速，训练时间不长。这种架构将是您作为嵌入式机器学习工程师的宝贵工具。
- en: Training with Your Own Data
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用您自己的数据进行训练
- en: In this section, we’ll show you how to train your own, custom model that recognizes
    new gestures. We’ll walk through how to capture accelerometer data, modify the
    training scripts to incorporate it, train a new model, and integrate it into the
    embedded application.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何训练自己的自定义模型，以识别新的手势。我们将逐步介绍如何捕获加速度计数据，修改训练脚本以将其纳入，训练新模型，并将其集成到嵌入式应用程序中。
- en: Capturing Data
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 捕获数据
- en: To obtain training data, we can use a simple program to log accelerometer data
    to the serial port while gestures are being performed.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取训练数据，我们可以使用一个简单的程序在手势执行时将加速度计数据记录到串行端口。
- en: SparkFun Edge
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SparkFun Edge
- en: The fastest way to get started is by modifying one of the examples in the [SparkFun
    Edge Board Support Package (BSP)](https://oreil.ly/z4eHX). First, follow SparkFun’s
    [“Using SparkFun Edge Board with Ambiq Apollo3 SDK”](https://oreil.ly/QqKPa) guide
    to set up the Ambiq SDK and SparkFun Edge BSP.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 快速入门的最快方法是修改[SparkFun Edge Board Support Package (BSP)](https://oreil.ly/z4eHX)中的一个示例。首先，按照SparkFun的[“使用Ambiq
    Apollo3 SDK与SparkFun Edge Board”](https://oreil.ly/QqKPa)指南设置Ambiq SDK和SparkFun
    Edge BSP。
- en: After you’ve downloaded the SDK and BSP, you’ll need to tweak the example code
    so it does what we want.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下载SDK和BSP后，您需要调整示例代码以使其符合我们的要求。
- en: 'First, open the file *AmbiqSuite-Rel2.2.0/boards/SparkFun_Edge_BSP/examples/example1_edge_test/src/tf_adc/tf_adc.c*
    in your text editor of choice. Find the call to `am_hal_adc_samples_read()`, on
    line 61 of the file:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在您选择的文本编辑器中打开文件*AmbiqSuite-Rel2.2.0/boards/SparkFun_Edge_BSP/examples/example1_edge_test/src/tf_adc/tf_adc.c*。找到文件第61行的`am_hal_adc_samples_read()`调用：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Change its second parameter to `true` so that the entire function call looks
    like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将其第二个参数更改为`true`，使整个函数调用看起来像这样：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, you’ll need to modify the file *AmbiqSuite-Rel2.2.0/boards/SparkFun_Edge_BSP/examples/example1_edge_test/src/main.c*.
    Find the `while` loop on line 51:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要修改文件*AmbiqSuite-Rel2.2.0/boards/SparkFun_Edge_BSP/examples/example1_edge_test/src/main.c*。找到第51行的`while`循环：
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Change the code to add the following extra line:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 更改代码以添加以下额外行：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now find this line a little further along in the `while` loop:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在`while`循环中稍后找到这行：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Delete the original line and replace it with the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The program will now output data in the format expected by the training scripts.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Next, follow the instructions in [SparkFun’s guide](https://oreil.ly/BPJMG)
    to build the `example1_edge_test` example application and flash it to the device.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Logging data
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After you’ve built and flashed the example code, follow these instructions to
    capture some data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'First, open a new terminal window. Then run the following command to begin
    logging all of the terminal’s output to a file named *output.txt*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, in the same window, use `screen` to connect to the device:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Measurements from the accelerometer will be shown on the screen and saved to
    *output.txt* in the same comma-delimited format expected by the training scripts.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: You should aim to capture multiple performances of the same gesture in a single
    file. To start capturing a single performance of a gesture, press the button marked
    `RST`. The characters `-,-,-` will be written to the serial port; this output
    is used by the training scripts to identify the start of a gesture performance.
    After you’ve performed the gesture, press the button marked `14` to stop logging
    data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’ve logged the same gesture a number of times, exit `screen` by pressing
    Ctrl- A, immediately followed by the K key, and then the Y key. After you’ve exited
    `screen`, enter the following command to stop logging data to *output.txt*:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You now have a file, *output.txt*, which contains data for one person performing
    a single gesture. To train an entirely new model, you should aim to collect a
    similar amount of data as in the original dataset, which contains around 15 performances
    of each gesture by 10 people.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t care about your model working for people other than yourself, you
    can probably get away with capturing only your own performances. That said, the
    more variation in performances you can collect, the better.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'For compatibility with the training scripts, you should rename your captured
    data files in the following format:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For example, data for a hypothetical “triangle” gesture made by “Daniel” would
    have the following name:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The training scripts will expect the data to be organized in directories for
    each gesture name; for example:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You’ll also need to provide data for the “unknown” category, in a directory
    named *negative*. In this case, you can just reuse the data files from the original
    dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Note that because the model architecture is designed to output probabilities
    for four classes (three gestures plus “unknown”), you should provide three gestures
    of your own. If you want to train on more or fewer gestures, you’ll need to change
    the training scripts and adjust the model architecture.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the Training Scripts
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a model with your new gestures, you need to make some changes to the
    training scripts.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'First, replace all of the gesture names within the following files:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[*data_load.py*](https://oreil.ly/1Tplr)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*data_prepare.py*](https://oreil.ly/O7eym)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*data_split.py*](https://oreil.ly/w8ORq)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, replace all of the person names within the following files:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[*data_prepare.py*](https://oreil.ly/3swnY)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*data_split_person.py*](https://oreil.ly/xhVh7)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if you have a different number of person names (the original dataset
    has 10) and you want to split the data by person during training, you’ll need
    to decide on a new split. If you have data from only a few people, it won’t be
    possible to split by person during training, so don’t worry about *data_split_person.py*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a new model, copy your data files directories into the training scripts’
    directory and follow the process we walked through earlier in this chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: If you have data from only a few people, you should split the data randomly
    rather than per person. To do this, run *data_split.py* instead of *data_split_person.py*
    when preparing for training.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Because you’re training on new gestures, it’s worth playing with the model’s
    hyperparameters to obtain the best accuracy. For example, you can see whether
    you get better results by training for more or fewer epochs, or with a different
    arrangement of layers or number of neurons, or with different convolutional hyperparameters.
    You can use TensorBoard to monitor your progress.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你正在训练新手势，值得尝试调整模型的超参数以获得最佳准确性。例如，你可以尝试训练更多或更少的epochs，或者使用不同排列的层或神经元数量，或者使用不同的卷积超参数来查看是否能获得更好的结果。你可以使用TensorBoard来监视你的进展。
- en: Once you have a model with acceptable accuracy, you’ll need to make a few changes
    to the project to make sure it works.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有一个准确度可接受的模型，你需要对项目进行一些更改以确保它正常运行。
- en: Using the New Model
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用新模型
- en: First, you’ll need to copy the new model’s data, as formatted by `xxd -i`, into
    *magic_wand_model_data.cc*. Make sure you also update the value of `g_magic_wand_model_data_len`
    to match the number output by `xxd`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要将由`xxd -i`格式化的新模型数据复制到*magic_wand_model_data.cc*中。确保你还更新`g_magic_wand_model_data_len`的值，以匹配`xxd`输出的数字。
- en: Next, in the array `should_continuous_count`, you’ll need to update the values
    in *accelerometer_handler.cc* that specify the number of continuous predictions
    required for each gesture. The value corresponds to how long the gesture takes
    to perform. Given that the original “wing” gesture requires a continuous count
    of 15, estimate how long your new gestures will take relative to that, and update
    the values in the array. You can tune these values iteratively until you get the
    most reliable performance.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在数组`should_continuous_count`中，你需要更新*accelerometer_handler.cc*中指定每个手势所需的连续预测次数的值。该值对应于手势执行所需的时间。鉴于原始的“翅膀”手势需要连续计数为15，估算一下你的新手势相对于那个需要多长时间，然后更新数组中的值。你可以通过迭代调整这些值，直到获得最可靠的性能。
- en: Finally, update the code in *output_handler.cc* to print the correct names for
    your new gestures. When this is done, you can build your code and flash your device.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，更新*output_handler.cc*中的代码以打印你的新手势的正确名称。完成后，你可以构建你的代码并刷写你的设备。
- en: Wrapping Up
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve taken our deepest dive yet into the architecture of a
    typical embedded machine learning model. This type of convolutional model is a
    powerful tool for classifying time-series data, and you’ll come across it often.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了典型嵌入式机器学习模型的架构。这种卷积模型是对时间序列数据进行分类的强大工具，你将经常遇到它。
- en: By now, you hopefully have an understanding of what embedded machine learning
    applications look like, and how their application code works together with models
    to understand the world around them. As you build your own projects, you’ll begin
    to put together a toolbox of familiar models that you can use to solve different
    problems.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，希望你已经了解了嵌入式机器学习应用程序的外观，以及它们的应用代码如何与模型一起工作来理解周围的世界。当你构建自己的项目时，你将开始组建一个熟悉模型的工具箱，可以用来解决不同的问题。
- en: Learning Machine Learning
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习机器学习
- en: This book is intended to provide a gentle introduction to the possibilities
    of embedded machine learning, but it’s not a complete reference on machine learning
    itself. If you’d like to dig deeper into building your own models, there are some
    amazing and highly accessible resources that are suitable for students of all
    backgrounds and will give you a running start.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在提供对嵌入式机器学习可能性的初步介绍，但它并不是机器学习本身的完整参考资料。如果你想深入了解如何构建自己的模型，有一些令人惊叹且易于访问的资源适合各种背景的学生，并将为你提供一个良好的起点。
- en: 'Here are some of our favorites, which will build on what you’ve learned here:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些我们喜欢的内容，将建立在你在这里学到的基础上：
- en: François Chollet’s [*Deep Learning with Python*](https://oreil.ly/PFF3r) (Manning)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François Chollet的[*Python深度学习*](https://oreil.ly/PFF3r)（Manning）
- en: Aurélien Géron’s [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*,
    2nd Edition](https://oreil.ly/M5KrN) (O’Reilly)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aurélien Géron的[*使用Scikit-Learn、Keras和TensorFlow进行实践机器学习，第二版*](https://oreil.ly/M5KrN)（O’Reilly）
- en: Deeplearning.ai’s [Deep Learning Specialization](https://oreil.ly/xKQMP) and
    [TensorFlow in Practice](https://oreil.ly/4q7HY) courses
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deeplearning.ai的[深度学习专项](https://oreil.ly/xKQMP)和[TensorFlow实践](https://oreil.ly/4q7HY)课程
- en: Udacity’s [Intro to TensorFlow for Deep Learning](https://oreil.ly/YJlYd) course
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Udacity的[深度学习TensorFlow入门](https://oreil.ly/YJlYd)课程
- en: What’s Next
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来是什么
- en: The remaining chapters of this book take a deeper dive into the tools and workflows
    of embedded machine learning. You’ll learn how to think about designing your own
    TinyML applications, how to optimize models and application code to run well on
    low-powered devices, how to port existing machine learning models to embedded
    devices, and how to debug embedded machine learning code. We’ll also address some
    high-level concerns, like deployment, privacy, and security.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的剩余章节将更深入地探讨嵌入式机器学习的工具和工作流程。你将学习如何思考设计自己的TinyML应用程序，如何优化模型和应用代码以在低功耗设备上运行良好，如何将现有的机器学习模型移植到嵌入式设备上，以及如何调试嵌入式机器学习代码。我们还将解决一些高层次的问题，如部署、隐私和安全性。
- en: But first, let’s learn a bit more about TensorFlow Lite, the framework that
    powers all of the examples in this book.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们更多地了解一下TensorFlow Lite，这是本书中所有示例的框架动力源。
- en: ^([1](ch12.xhtml#idm46473549700952-marker)) This is a new term, which we’ll
    talk about later.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.xhtml#idm46473549700952-marker)) 这是一个新术语，我们稍后会谈论。
